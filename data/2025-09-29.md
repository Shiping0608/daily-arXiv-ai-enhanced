<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 11]
- [math.AP](#math.AP) [Total: 12]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [cs.DC](#cs.DC) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [math.CV](#math.CV) [Total: 1]
- [hep-ex](#hep-ex) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 2]
- [cs.LG](#cs.LG) [Total: 4]
- [math.PR](#math.PR) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Towards provable energy-stable overset grid methods using sub-cell summation-by-parts operators](https://arxiv.org/abs/2509.21442)
*Jan Glaubitz,Joshua Lampert,Andrew R. Winters,Jan Nordström*

Main category: math.NA

TL;DR: Developed sub-cell summation-by-parts operators to create provably conservative and energy-stable overset grid methods, resolving longstanding stability issues.


<details>
  <summary>Details</summary>
Motivation: Overset grid methods for complex geometries have practical and theoretical stability challenges, especially at high orders.

Method: Introduced sub-cell summation-by-parts (SBP) operators that mimic integration by parts at sub-cell level, creating a discrete counterpart to continuous overset domain analysis.

Result: Developed provably conservative and energy-stable overset grid methods.

Conclusion: Resolved longstanding stability issues in overset grid methods through novel sub-cell SBP operators.

Abstract: Overset grid methods handle complex geometries by overlapping simpler,
geometry-fitted grids to cover the original, more complex domain. However,
ensuring their stability -- particularly at high orders -- remains a practical
and theoretical challenge. In this work, we address this gap by developing a
discrete counterpart to the recent well-posedness analysis of Kopriva, Gassner,
and Nordstr\"om for continuous overset domain initial-boundary-value problems.
To this end, we introduce the novel concept of sub-cell summation-by-parts
(SBP) operators. These discrete derivative operators mimic integration by parts
at a sub-cell level. By exploiting this sub-cell SBP property, we develop
provably conservative and energy-stable overset grid methods, thereby resolving
longstanding stability issues in the field.

</details>


### [2] [Conforming lifting and adjoint consistency for the Discrete de Rham complex of differential forms](https://arxiv.org/abs/2509.21449)
*Daniele A. Di Pietro,Jérôme Droniou,Silvano Pitassi*

Main category: math.NA

TL;DR: The paper introduces conforming liftings for Discrete de Rham (DDR) methods to address challenges from non-conformity, enabling optimal bounds on consistency errors in integration-by-parts formulas on polytopal meshes.


<details>
  <summary>Details</summary>
Motivation: DDR methods provide compatible approximations on polytopal meshes but face analytical challenges due to non-conformity. The work aims to solve these issues by designing conforming liftings that are right-inverse of interpolators.

Method: Design conforming liftings on DDR spaces using finite element spaces on a simplicial submesh of the polytopal mesh. Analysis is carried out in the polytopal exterior calculus framework for unified proofs.

Result: Using conforming liftings, the authors obtain an optimal bound on the residual in the global integration-by-parts formula in terms of mesh size, addressing consistency errors from non-conformity.

Conclusion: Conforming liftings effectively resolve analytical challenges in DDR methods, providing control over functions and enabling optimal error bounds for integration-by-parts formulas on general polytopal meshes.

Abstract: Discrete de Rham (DDR) methods provide non-conforming but compatible
approximations of the continuous de Rham complex on general polytopal meshes.
Owing to the non-conformity, several challenges arise in the analysis of these
methods. In this work, we design conforming liftings on the DDR spaces, that
are right-inverse of the interpolators and can be used to solve some of these
challenges. We illustrate this by tackling the question of the global
integration-by-part formula. By non-conformity of the discrete complex, this
formula involves a residual -- which can be interpreted as a consistency error
on the adjoint of the discrete exterior derivative -- on which we obtain, using
the conforming lifting, an optimal bound in terms of the mesh size. Our
analysis is carried out in the polytopal exterior calculus framework, which
allows for unified proofs for all the spaces and operators in the DDR complex.
Moreover, the liftings are explicitly constructed in finite element spaces on a
simplicial submesh of the underlying polytopal mesh, which gives more control
on the resulting functions (e.g., discrete trace and inverse inequalities).

</details>


### [3] [Fast summation of Stokes potentials using a new kernel-splitting in the DMK framework](https://arxiv.org/abs/2509.21471)
*Ludvig af Klinteberg,Leslie Greengard,Shidong Jiang,Anna-Karin Tornberg*

Main category: math.NA

TL;DR: A new efficient kernel splitting method using prolate spheroidal wave functions for biharmonic Green's function, enabling faster Ewald summation and DMK algorithms for Stokes and elastic interactions.


<details>
  <summary>Details</summary>
Motivation: Classical Ewald methods use Gaussian-based kernel splitting, which is inefficient for biharmonic Green's functions. More efficient splitting methods are needed for faster computation of Stokeslet, stresslet, and elastic kernels.

Method: Developed kernel splitting using zeroth-order prolate spheroidal wave functions for biharmonic Green's function. Created DMK (dual-space multilevel kernel-splitting) algorithm that is adaptive and linear-scaling for both uniform and nonuniform point distributions.

Result: The DMK algorithm demonstrates efficient performance in numerical examples for both 2D and 3D cases, working in free space and periodic cubes with linear scaling.

Conclusion: PSWF-based kernel splitting provides more efficient alternatives to Gaussian-based methods for biharmonic-related kernels, enabling faster computational methods for Stokes and elastic interactions.

Abstract: Classical Ewald methods for Coulomb and Stokes interactions rely on
``kernel-splitting," using decompositions based on Gaussians to divide the
resulting potential into a near field and a far field component. Here, we show
that a more efficient splitting for the scalar biharmonic Green's function can
be derived using zeroth-order prolate spheroidal wave functions (PSWFs), which
in turn yields new efficient splittings for the Stokeslet, stresslet, and
elastic kernels, since these Green's tensors can all be derived from the
biharmonic kernel. This benefits all fast summation methods based on kernel
splitting, including FFT-based Ewald summation methods, that are suitable for
uniform point distributions, and DMK-based methods that allow for nonuniform
point distributions. The DMK (dual-space multilevel kernel-splitting) algorithm
we develop here is fast, adaptive, and linear-scaling, both in free space and
in a periodic cube. We demonstrate its performance with numerical examples in
two and three dimensions.

</details>


### [4] [An Adaptive CUR Algorithm and its Application to Reduced-Order Modeling of Random PDEs](https://arxiv.org/abs/2509.21480)
*Grishma Palkar,Hessam Babaee*

Main category: math.NA

TL;DR: Proposes a cross oversampling algorithm for CUR matrix approximation that improves robustness by augmenting the intersection with additional sampled columns and rows, with adaptive oversampling selection.


<details>
  <summary>Details</summary>
Motivation: Standard CUR algorithms suffer from accuracy loss in some settings, limiting their robustness for applications like reduced-order modeling of nonlinear matrix differential equations.

Method: Cross oversampling algorithm that augments the intersection with additional sampled columns and rows, with adaptive selection of oversampling entries based on efficiently computable indicators.

Result: Error analysis demonstrates improved robustness, and performance is validated for time integration of nonlinear stochastic PDEs on low-rank matrix manifolds.

Conclusion: The proposed oversampling approach enhances CUR algorithm robustness while maintaining efficiency for nonlinear matrix approximation problems.

Abstract: Certain classes of CUR algorithms, also referred to as cross or
pseudoskeleton algorithms, are widely used for low-rank matrix approximation
when direct access to all matrix entries is costly. Their key advantage lies in
constructing a rank-r approximation by sampling only r columns and r rows of
the target matrix. This property makes them particularly attractive for
reduced-order modeling of nonlinear matrix differential equations, where
nonlinear operations on low-rank matrices can otherwise produce high-rank or
even full-rank intermediates that must subsequently be truncated to rank $r$.
CUR cross algorithms bypass the intermediate step and directly form the
rank-$r$ matrix. However, standard cross algorithms may suffer from loss of
accuracy in some settings, limiting their robustness and broad applicability.
In this work, we propose a cross oversampling algorithm that augments the
intersection with additional sampled columns and rows. We provide an error
analysis demonstrating that the proposed oversampling improves robustness. We
also present an algorithm that adaptively selects the number of oversampling
entries based on efficiently computable indicators. We demonstrate the
performance of the proposed CUR algorithm for time integration of several
nonlinear stochastic PDEs on low-rank matrix manifolds.

</details>


### [5] [Micro-macro kinetic flux-vector splitting schemes for the multidimensional Boltzmann-ES-BGK equation](https://arxiv.org/abs/2509.21832)
*James A. Rossmanith,Preeti Sar*

Main category: math.NA

TL;DR: Developed a finite volume method using micro-macro decomposition for solving Boltzmann equations with ES-BGK collision operator, extending previous work to 2D and implementing parallel MPI scaling.


<details>
  <summary>Details</summary>
Motivation: To create efficient numerical methods for kinetic Boltzmann equations that maintain accuracy across spatial/temporal scales while reducing computational costs compared to direct kinetic methods, particularly for low/intermediate Knudsen numbers.

Method: Finite volume method with micro-macro decomposition of distribution function; macro portion uses fluid model with moment closure from micro heat flux; micro portion uses orthogonal projector; implicit time discretization for collision operator; kinetic flux vector splitting for macro transport; upwind differencing for micro transport; extended to 2D, ES-BGK operator, and reflecting wall boundaries.

Result: Successfully applied to various 1D and 2D test cases; implemented parallel MPI version with demonstrated weak and strong scaling performance across multiple processors.

Conclusion: The developed method provides an efficient and accurate approach for solving kinetic equations with ES-BGK collision operator, maintaining correct transport coefficients while reducing computational requirements through micro-macro decomposition and parallel implementation.

Abstract: The kinetic Boltzmann equation models gas dynamics over a wide range of
spatial and temporal scales. Simplified versions of the full Boltzmann
collision operator, such as the classical Bhatnagar-Gross-Krook and the closely
related Ellipsoidal-Statistical-BGK operators, can dramatically decrease the
computational costs of numerical solving kinetic equations. Classical BGK
yields incorrect transport coefficients (relative to the full Boltzmann
collision operator) at low Knudsen numbers, whereas ES-BGK captures them
correctly. In this work, we develop a finite volume method using a micro-macro
decomposition of the distribution function, which requires a smaller velocity
mesh relative to direct kinetic methods for low and intermediate Knudsen
numbers. The macro portion of the model is a fluid model with a moment closure
provided from the heat flux tensor calculated from the micro portion. The micro
portion is obtained by applying to the original kinetic equation a projector
into the orthogonal complement of the null space of the collision operator -
this projector depends on the macro portion. In particular, we extend the
technique of Bennoune, Lemou, and Mieussens [Uniformly stable schemes for the
Boltzmann equation preserving the compressible Navier-Stokes asymptotics, J.
Comput. Phys. (2008)] to two-space dimensions, the ES-BGK collision operator,
and problems with reflecting wall boundary conditions. As it appears in both
the micro and macro equations, the collision operator is handled via L-stable
implicit time discretizations. At the same time, the remaining transport terms
are computed via kinetic flux vector splitting (for macro) and upwind
differencing (for micro). The resulting scheme is applied to various test cases
in 1D and 2D. The 2D version of the code is parallelized via MPI, and we
present weak and strong scaling studies with varying numbers of processors.

</details>


### [6] [Fast Rank Adaptive CUR via a Recycled Small Sketch](https://arxiv.org/abs/2509.21963)
*Nathaniel Pritchard,Taejun Park,Yuji Nakatsukasa,Per-Gunnar Martinsson*

Main category: math.NA

TL;DR: IterativeCUR is a new adaptive algorithm for low-rank matrix approximation that uses CUR decomposition, offering significant speed improvements over existing methods while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional SVD-based approaches for low-rank matrix approximations have computational limitations, and CUR decomposition can better preserve matrix structures like sparsity and facilitate data interpretation, but existing CUR methods need improvement in efficiency and adaptivity.

Method: IterativeCUR uses randomized numerical linear algebra techniques with a single small sketch that is successively downdated, providing an adaptive approach that takes desired tolerance as input rather than requiring an a priori rank guess.

Result: The algorithm achieves up to 4x speed-up over state-of-the-art pivoting-on-sketch approaches and up to 40x speed-up over rank-adaptive randomized SVD approaches, with asymptotic complexity of O(mn + (m+n)r² + r³) for an m×n matrix of numerical rank r.

Conclusion: IterativeCUR is a highly competitive low-rank approximation method that combines the structural preservation benefits of CUR decomposition with superior computational efficiency compared to existing approaches.

Abstract: The computation of accurate low-rank matrix approximations is central to
improving the scalability of various techniques in machine learning,
uncertainty quantification, and control. Traditionally, low-rank approximations
are constructed using SVD-based approaches such as truncated SVD or
RandomizedSVD. Although these SVD approaches -- especially RandomizedSVD --
have proven to be very computationally efficient, other low-rank approximation
methods can offer even greater performance. One such approach is the CUR
decomposition, which forms a low-rank approximation using direct row and column
subsets of a matrix. Because CUR uses direct matrix subsets, it is also often
better able to preserve native matrix structures like sparsity or
non-negativity than SVD-based approaches and can facilitate data interpretation
in many contexts. This paper introduces IterativeCUR, which draws on previous
work in randomized numerical linear algebra to build a new algorithm that is
highly competitive compared to prior work: (1) It is adaptive in the sense that
it takes as an input parameter the desired tolerance, rather than an a priori
guess of the numerical rank. (2) It typically runs significantly faster than
both existing CUR algorithms and techniques such as RandomizedSVD, in
particular when these methods are run in an adaptive rank mode. Its asymptotic
complexity is $\mathcal{O}(mn + (m+n)r^2 + r^3)$ for an $m\times n$ matrix of
numerical rank $r$. (3) It relies on a single small sketch from the matrix that
is successively downdated as the algorithm proceeds.
  We demonstrate through extensive experiments that IterativeCUR achieves up to
$4\times$ speed-up over state-of-the-art pivoting-on-sketch approaches with no
loss of accuracy, and up to $40\times$ speed-up over rank-adaptive randomized
SVD approaches.

</details>


### [7] [A Parallel-in-Time Combination Method for Parabolic Problems](https://arxiv.org/abs/2509.22156)
*Michael Griebel,Marc Alexander Schweitzer,Lukas Troska*

Main category: math.NA

TL;DR: A parallel solver for high-dimensional parabolic problems combining MGRIT for time, sparse grids for space, and domain decomposition, achieving excellent scalability up to 6D.


<details>
  <summary>Details</summary>
Motivation: To solve parabolic problems with many space dimensions efficiently using parallel computing methods.

Method: Combines MGRIT (multigrid reduction-in-time) from XBraid for time parallelism, sparse grid combination method for spatial discretization, and domain decomposition via space-filling curves.

Result: Extremely fast and embarrassingly parallel solver with excellent speedup and scale-up, demonstrated on heat equation, chemical master equation, and stochastic differential equations.

Conclusion: The approach is highly effective for parabolic problems with up to six space dimensions, showing superior parallelization properties.

Abstract: In this article, we present a parallel discretization and solution method for
parabolic problems with a higher number of space dimensions. It consists of a
parallel-in-time approach using the multigrid reduction-in-time algorithm MGRIT
with its implementation in the library XBraid, the sparse grid combination
method for discretizing the resulting elliptic problems in space, and a domain
decomposition method for each of the subproblems in the combination method
based on the space-filling curve approach. As a result, we obtain an extremely
fast and embarrassingly parallel solver with excellent speedup and scale-up
qualities, which is perfectly suited for parabolic problems with up to six
space dimensions. We describe our new parallel approach and show its superior
parallelization properties for the heat equation, the chemical master equation
and some exemplary stochastic differential equations.

</details>


### [8] [Well-balanced high-order method for non-conservative hyperbolic PDEs with source terms: application to one-dimensional blood flow equations with gravity](https://arxiv.org/abs/2509.22190)
*Chiara Colombo,Caterina Dalmaso,Lucas O. Müller,Annunziato Siviglia*

Main category: math.NA

TL;DR: A well-balanced finite volume method for non-conservative hyperbolic PDEs with source terms, using high-order spatial reconstruction and explicit time evolution.


<details>
  <summary>Details</summary>
Motivation: To develop accurate numerical methods that preserve stationary solutions for hyperbolic PDEs with source terms, particularly in applications like blood flow modeling.

Method: Combines high-order spatial reconstruction based on generalized Riemann problems with a well-balanced space-time evolution operator for explicit time stepping.

Result: Method preserves stationary solutions up to machine precision, achieves high-order accuracy, and performs well on complex networks like 86 arteries under various conditions.

Conclusion: The proposed method is effective, efficient, and well-balanced for solving hyperbolic PDEs with source terms, demonstrating practical utility in biomedical applications.

Abstract: The present work proposes a well-balanced finite volume-type numerical method
for the solution of non-conservative hyperbolic partial differential equations
(PDEs) with source terms. The method is characterized, first, by the use of a
recently introduced high-order spatial reconstruction, based on generalized
Riemann problem information from the previous time level. Such reconstruction
is well-balanced up to order three, compact, efficient and easy to implement.
Second, the method incorporates a well-balanced space-time evolution operator,
which allows for well-balanced fully explicit time evolution. The accuracy and
efficiency of the method are assessed on both a scalar problem (Burgers'
equation) and a nonlinear PDE system (hyperbolized one-dimensional blood flow
equations with gravity and friction, and with variable mechanical and
geometrical properties). The well-balanced property is verified by showing that
numerically-determined stationary solutions are preserved up to machine
precision. The order of accuracy in space and time is validated through
empirical convergence rate studies. Additionally, the performance of the method
is assessed on a network of 86 arteries, under both stationary and transient
conditions.

</details>


### [9] [Square-Domain Area-Preserving Parameterization for Genus-Zero and Genus-One Closed Surfaces](https://arxiv.org/abs/2509.22269)
*Shu-Yung Liu,Mei-Heng Yueh*

Main category: math.NA

TL;DR: A numerical framework for seamless parameterization of genus-zero and genus-one closed surfaces onto a unit square domain using slicing and area-preserving mapping.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing parameterization methods that require multiple charts or non-planar domains for closed surfaces.

Method: Slice the surface using shortest-path or Reeb graph method, then map onto unit square using globally convergent algorithm that minimizes weighted variance of per-triangle area ratios for area preservation.

Result: High accuracy and efficiency demonstrated on benchmark models, enabling applications like geometry images with accurate surface reconstructions.

Conclusion: The proposed method provides an effective framework for seamless parameterization of closed surfaces with practical applications in surface reconstruction.

Abstract: The parameterization of closed surfaces typically requires either multiple
charts or a non-planar domain to achieve a seamless global mapping. In this
paper, we propose a numerical framework for the seamless parameterization of
genus-zero and genus-one closed simplicial surfaces onto a unit square domain.
The process begins by slicing the surface with either the shortest-path or the
Reeb graph method. The sliced surface is then mapped onto the unit square using
a globally convergent algorithm that minimizes the weighted variance of
per-triangle area ratios to achieve area preservation. Numerical experiments on
benchmark models demonstrate that our method achieves high accuracy and
efficiency. Furthermore, the proposed method enables applications such as
geometry images, producing accurate and high-quality surface reconstructions.

</details>


### [10] [Universal Solution to Kronecker Product Decomposition](https://arxiv.org/abs/2509.22373)
*Daizhan Cheng*

Main category: math.NA

TL;DR: The paper presents a universal method for Kronecker product decomposition (KPD) of vectors, matrices, and hypermatrices using monic decomposition algorithm, swap matrices, and permutation matrices.


<details>
  <summary>Details</summary>
Motivation: To provide a general solution framework for KPD problems across different mathematical objects (vectors, matrices, hypermatrices) with verifiable necessary and sufficient conditions.

Method: Uses monic decomposition algorithm (MDA) with projections, swap matrices for matrix rearrangement, and permutation matrices for hypermatrix conversion to vector KPD problems.

Result: Established necessary and sufficient conditions for KPD solvability and developed algorithms for both approximate and finite sum decompositions that work universally across vectors, matrices, and hypermatrices.

Conclusion: The proposed universal KPD solving method successfully handles decomposition problems for various mathematical structures with proven conditions and demonstrated effectiveness through numerical examples.

Abstract: This paper provides a general solution for the Kronecker product
decomposition (KPD) of vectors, matrices, and hypermatrices. First, an
algorithm, namely, monic decomposition algorithm (MDA), is reviewed. It
consists of a set of projections from a higher dimension Euclidian space to its
factor-dimension subspaces. It is then proved that the KPD of vectors is
solvable, if and only if, the project mappings provide the required decomposed
vectors. Hence it provides an easily verifiable necessary and sufficient
condition for the KPD of vectors. Then an algorithm is proposed to calculate
the least square error approximated decomposition. Using it finite times a
finite sum (precise) KPD of any vectors can be obtained. Then the swap matrix
is used to make the elements of a matrix re-arranging, and then provides a
method to convert the KPD of matrices to its corresponding KPD of vectors. It
is proved that the KPD of a matrix is solvable, if and only if, the KPD of its
corresponding vector is solvable. In this way, the necessary and sufficient
condition, and the followed algorithms for approximate and finite sum KPDs for
matrices are also obtained. Finally, the permutation matrix is introduced and
used to convert the KPD of any hypermatrix to KPD of its corresponding vector.
Similarly to matrix case, the necessary and sufficient conditions for
solvability and the techniques for vectors and matrices are also applicable for
hypermatrices, though some additional algorithms are necessary. Several
numerical examples are included to demonstrate this universal KPD solving
method.

</details>


### [11] [The discretizations of the derivative by the continuous Galerkin and the discontinuous Galerkin methods are exactly the same](https://arxiv.org/abs/2509.22587)
*Bernardo Cockburn*

Main category: math.NA

TL;DR: The paper reveals that continuous and discontinuous Galerkin methods share the same derivative discretization, leading to a new post-processing technique for DG solutions that enhances convergence without extra computation.


<details>
  <summary>Details</summary>
Motivation: To establish a connection between continuous and discontinuous Galerkin methods in ODE frameworks and leverage this relationship to improve DG solution accuracy.

Method: Developed an elementwise post-processing technique that adds a scaled left-Radau polynomial (degree k+1) multiplied by solution jumps at interval boundaries to the DG approximation.

Result: The post-processed solution becomes continuous and achieves convergence order k+2 for k>0 (one order higher than original DG), while maintaining the same order for k=0.

Conclusion: The discovered link between Galerkin methods enables efficient post-processing that significantly improves DG solution accuracy and continuity without additional computational cost.

Abstract: In the framework of ODEs, we uncover a new link between the continuous
Galerkin method (see Math. Comp. (1972), 26 (118 and 120), 415-426 and 881-891)
and the discontinuous Galerkin method (see Mathematical Aspects of Finite
elements in PDEs, (1974), 89-123), namely, that the discretizations of the
derivative by these two methods are the same. A direct consequence of this
result is the construction of a new elementwise post-processing of the
approximate solution provided by the Discontinuous Galerkin method. When the DG
method uses polynomials of degree $k\ge0$, the post-processing consists in
adding, to the DG approximate solution, the (scaled) left-Radau polynomial of
degree $k+1$ multiplied by the jump of the approximate solution at the left
boundary of the interval. No extra computation is required. The resulting new
approximation is continuous and, for $k>0$, converges with order $k+2$, that
is, with one order more than the original discontinuous Galerkin approximation.
For $k=0$, the order remains the same.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [12] [An inverse problem for linear system of dispersive equations](https://arxiv.org/abs/2509.21524)
*Deissy Marcela Pizo,Juan Carlos Muñoz Grajales*

Main category: math.AP

TL;DR: This paper presents an inverse problem approach to identify the linear velocity coefficient in Benjamin-Bona-Mahony-type equations using a restricted minimization problem and numerical optimization with L-BFGS-B algorithm.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of identifying linear velocity coefficients in water wave models that account for dispersion and topography effects in shallow channels.

Method: Reformulate the inverse problem as a restricted minimization problem with regularization, then solve using L-BFGS-B algorithm implemented in Dolfin-Adjoint-Python-SciPy libraries.

Result: Established local stability for the minimization problem and demonstrated effective identification of linear velocity coefficients through numerical simulations in Boussinesq-type systems.

Conclusion: The proposed method successfully identifies linear velocity coefficients in water wave models, with proven local stability and practical effectiveness shown through numerical experiments.

Abstract: This paper addresses the inverse problem of identifying the linear velocity
coefficient in a linear system governed by two Benjamin-Bona-Mahony-type
equations, which model the displacement of water waves propagating along the
surface of a shallow channel, incorporating effects of dispersion and
topography. To solve this, we reformulate the inverse problem as a restricted
minimization problem (RMP), aimed at optimizing a suitably regularized
objective functional. We use numerical techniques, specifically the iterative
L-BFGS-B algorithm implemented in the Dolfin-Adjoint-Python-SciPy libraries, to
solve the RMP effectively. Following methodologies similar to those in Pipicano
et al., we establish a local stability result for the RMP. Additionally,
through numerical simulations, we demonstrate the effectiveness of the proposed
identification method in determining the linear velocity coefficient in
Boussinesq-type systems.

</details>


### [13] [Multiple solutions to the nonlinear Schrödinger equation with a partial confinement](https://arxiv.org/abs/2509.21806)
*Liying Shan,Wei Shuai,Leyun Wu*

Main category: math.AP

TL;DR: The paper studies multiple solutions to the nonlinear Schrödinger equation with partial confinement, relevant to Bose-Einstein condensate dynamics, including ground states, sign-changing solutions, and novel saddle-type nodal solutions.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of Bose-Einstein condensates through the nonlinear Schrödinger equation with partial confinement, and to investigate the existence and symmetry properties of various solution types.

Method: Developed innovative techniques including the method of moving planes and the Hopf lemma specifically adapted for nonlinear Schrödinger equations with partial confinement.

Result: Verified existence of positive ground state solutions, proved nonexistence of least energy sign-changing solutions, discovered saddle-type nodal solutions with nodal domains intersecting at the origin, and revealed symmetry properties of these solutions.

Conclusion: The study provides comprehensive understanding of solution types for partially confined NLS, with novel findings about saddle-type nodal solutions and development of specialized analytical techniques for this class of equations.

Abstract: We consider multiple solutions to the nonlinear Schr\"odinger equation (NLS)
with a partial confinement, which is physically relevant to dynamics of the
Bose-Einstein condensate. Our study not only verifies the existence of positive
ground state solutions and the nonexistence of least energy sign-changing
solutions but also sheds light on the symmetry associated with these solutions.
A novel finding is the existence of saddle type nodal solutions with their
nodal domains intersecting at the origin. Furthermore, we have developed some
innovative techniques such as the method of moving planes and the Hopf lemma
for nonlinear Schr\"odinger equations with partial confinement.

</details>


### [14] [Logarithmic evolutions in solutions to the convection-diffusion equation of Burgers type](https://arxiv.org/abs/2509.21909)
*Masakazu Yamamoto*

Main category: math.AP

TL;DR: This paper analyzes the convection-diffusion equation of Burgers type, showing that nonlinearity creates logarithmic time development in solutions with decay rates varying by spatial dimension parity.


<details>
  <summary>Details</summary>
Motivation: To understand how nonlinearity affects the asymptotic behavior of Burgers-type convection-diffusion equations and investigate dimensional parity effects on solution symmetry.

Method: Analysis of the initial value problem for Burgers-type convection-diffusion equation, examining asymptotic profiles and comparing with Navier-Stokes equations.

Result: Nonlinearity creates logarithmic time development; decay rates differ by dimension parity - even dimensions show expected decay rates while odd dimensions decay faster than expected.

Conclusion: Dimensional parity affects nonlinearity symmetry in Burgers-type equations, with implications for understanding bilinear problems and similar Navier-Stokes equations.

Abstract: In this paper, the initial value problem of the convection-diffusion equation
of Burgers type is treated. In the asymptotic profile of solutions, the
nonlinearity of the equation is reflected. A characteristic component derived
from nonlinearity develops logarithmically over time, and the rate of decay
varies depending on the spatial dimensions. The rate of this component
predicted from the scale of the equation. In even dimensions, the logarithmic
component decays at the expected rate, but in odd dimensions, it decays much
faster than expected. These results suggest that there are differences in the
symmetry of nonlinearity depending on the parity of dimensions. This
interpretation is supported by comparison with similar Navier--Stokes
equations. The Burgers type is applicable as an indicator for considering
several bilinear problems.

</details>


### [15] [Quantitative periodic homogenization of parabolic equations with large drift and potential](https://arxiv.org/abs/2509.22003)
*Kshitij Sinha*

Main category: math.AP

TL;DR: The paper studies homogenization rates for parabolic problems with large drift and potential terms, showing the solution decomposes into three multiplicative components.


<details>
  <summary>Details</summary>
Motivation: To understand homogenization behavior for parabolic equations with significant lower order terms (drift and potential) in periodic settings.

Method: Decompose solution into three parts: time function, ground-state of exponential cell eigenvalue problem, and solution to parabolic equation with zero effective drift. Derive L² convergence rates.

Result: Established that the solution factors into three multiplicative components and obtained L² convergence rates in the homogenization limit.

Conclusion: The three-part decomposition provides a framework for analyzing homogenization of parabolic problems with large lower order terms, with explicit convergence rates.

Abstract: This work aims to study the rates in the context of periodic homogenization
of parabolic problems with large lower order terms (both drift and potential).
We demonstrate that the solution is a product of three terms: (i) a function of
time, (ii) the ground-state of an exponential cell eigenvalue problem and (iii)
the solution to a parabolic equation with zero effective drift. For the latter,
we derive $\mathrm L^2$ rates in the homogenization limit.

</details>


### [16] [Initial boundary value problems for 3-d Navier-Stokes equations with hyperbolic heat conduction](https://arxiv.org/abs/2509.22029)
*Yuxi Hu,Reinhard Racke*

Main category: math.AP

TL;DR: Existence of global small solutions for 3D compressible Navier-Stokes equations with hyperbolic heat conduction in spherical symmetry, with justification of relaxation and vanishing viscosity limits.


<details>
  <summary>Details</summary>
Motivation: To study compressible fluid flow with hyperbolic heat conduction (Cattaneo-Christov model) instead of classical Fourier law, focusing on spherical symmetry for mathematical tractability.

Method: Analysis of initial boundary value problem for 3D compressible Navier-Stokes equations with Cattaneo-Christov heat conduction, using uniform a priori estimates and spherical symmetry reduction.

Result: Established existence of uniform global small solutions and rigorously justified both relaxation limit (to Fourier law) and vanishing viscosity limit.

Conclusion: The hyperbolic heat conduction model admits global small solutions in spherical symmetry, with valid relaxation and vanishing viscosity limits that connect to classical models.

Abstract: We study an initial boundary value problem for the 3-dimensional compressible
Navier-Stokes equations with hyperbolic heat conduction, where the classical
Fourier law is replaced by the Cattaneo-Christov constitutive relation. We
focus on spherically symmetric solutions. We establish the existence of uniform
global small solutions to the resulting system. Furthermore, based on uniform a
priori estimates, we rigorously justify both the relaxation limit and the
vanishing viscosity limit.

</details>


### [17] [A two-point phase recovering from holographic data on a single plane](https://arxiv.org/abs/2509.22048)
*Roman Novikov,Vladimir Sivkin*

Main category: math.AP

TL;DR: The paper presents two-point local formulas for recovering radiation solutions from holographic intensity data on a distant hyperplane, with error decaying as distance increases.


<details>
  <summary>Details</summary>
Motivation: To develop efficient methods for reconstructing radiation solutions from intensity measurements (holographic data) in exterior regions, which has applications in wave propagation and imaging.

Method: Uses two-point local formulas applied to the intensity of total solutions (plane wave + radiation solution) on a distant hyperplane, leveraging the Helmholtz equation in exterior regions.

Result: Successfully recovers radiation solutions restricted to the hyperplane with decaying error as distance increases, and provides numerical implementation demonstrating the approach.

Conclusion: The proposed two-point formulas effectively reconstruct radiation solutions from holographic intensity data with accuracy improving with distance, validated through numerical experiments.

Abstract: We consider a plane wave, a radiation solution, and the sum of these
solutions (total solution) for the Helmholtz equation in an exterior region in
$\mathbb{R}^d,$ $d\geq 2$. In this region, we consider a hyperplane $X$ with
sufficiently large distance $s$ from the origin in ${\mathbb R}^d.$ We give
two-point local formulas for approximate recovering the radiation solution
restricted to the plane $X$ from the intensity of the total solution at $X$,
that is, from holographic data. The recovering is given in terms of the
far-field pattern of the radiation solution with a decaying error term as $s
\to +\infty.$ A numerical implementation is also presented.

</details>


### [18] [Optimal Control of a Navier-Stokes-Cahn-Hilliard System for Membrane-fluid Interaction](https://arxiv.org/abs/2509.22069)
*Andrea Signori,Hao Wu*

Main category: math.AP

TL;DR: Analysis of optimal control for a 2D Navier-Stokes-Cahn-Hilliard system modeling fluid-membrane interaction, focusing on existence of optimal controls and derivation of optimality conditions.


<details>
  <summary>Details</summary>
Motivation: To study optimal control problems for fluid-membrane interaction systems, where fluid dynamics (Navier-Stokes) is coupled with membrane deformation (Cahn-Hilliard) through a phase-field variable.

Method: Introduce external forcing term as control variable, minimize tracking-type cost functional, establish existence of optimal control, and derive first-order necessary optimality conditions using adjoint system analysis.

Result: Proved existence of optimal control and derived rigorous first-order necessary optimality conditions by establishing sufficient regularity for solutions of the adjoint system.

Conclusion: Successfully formulated and analyzed optimal control problem for coupled fluid-membrane system, providing mathematical foundation for control applications in such complex physical systems.

Abstract: We consider an optimal control problem for a two-dimensional
Navier-Stokes-Cahn-Hilliard system arising in the modeling of fluid-membrane
interaction. The fluid dynamics is governed by the incompressible Navier-Stokes
equations, which are nonlinearly coupled with a sixth-order Cahn-Hilliard type
equation representing the deformation of a flexible membrane through a
phase-field variable. Building on the previously established existence and
uniqueness of global strong solutions for the coupled system, we introduce an
external forcing term acting on the fluid as the control variable. Then we seek
to minimize a tracking-type cost functional, demonstrating the existence of an
optimal control and deriving the associated first-order necessary optimality
conditions. A key issue is to establish sufficient regularity for solutions of
the adjoint system, which is crucial for the rigorous derivation of optimality
conditions in the fluid dynamic setting.

</details>


### [19] [An Inverse Problem for the Prescribed Mean Curvature](https://arxiv.org/abs/2509.22078)
*Tony Liimatainen,Janne Nurminen*

Main category: math.AP

TL;DR: The paper proves that in 2D, the source function H in the prescribed mean curvature equation is uniquely determined by the Dirichlet-to-Neumann map, representing the first treatment of inverse source problems for quasilinear equations.


<details>
  <summary>Details</summary>
Motivation: To extend the study of inverse problems for minimal surfaces by considering the inverse source problem for the prescribed mean curvature equation, which has applications in geometric analysis and PDE theory.

Method: Uses the higher order linearization method and develops a novel approach to decouple the resulting system of nonlinear algebraic and geometric equations, leveraging a Liouville type uniqueness result for conformal mappings.

Result: Proves that in two dimensions, the source function H is uniquely determined by the associated Dirichlet-to-Neumann map.

Conclusion: This work establishes the first inverse source result for quasilinear equations and provides a methodological framework for handling such problems through geometric and algebraic decoupling techniques.

Abstract: We extend the study of inverse problems for minimal surfaces by considering
the inverse source problem for the prescribed mean curvature equation
\begin{equation*}
  \nabla \cdot \left[ \frac{\nabla u}{(1 + |\nabla u|^2)^{1/2}} \right] = H(x).
\end{equation*} We prove that in two dimensions, the source function $H$ is
uniquely determined by the associated Dirichlet-to-Neumann map. A notable
feature of this problem is that although the equation is posed on an Euclidean
domain, its linearization yields an anisotropic conductivity equation where the
coefficient matrix corresponds to a Riemannian metric $g$ depending on the
background solution. This work represents the first treatments of inverse
source problems for quasilinear equations.
  The proof relies on the higher order linearization method. The main
methodological contribution is the development of an approach to decouple the
resulting system of nonlinear algebraic and geometric equations, which enables
the complete determination of the source term from boundary measurements. The
decoupling uses a Liouville type uniqueness result for conformal mappings.

</details>


### [20] [The moving patch model with fractional diffusion](https://arxiv.org/abs/2509.22234)
*Sebastián Flores-Sepúlveda,Gabrielle Nornberg,Alexander Quaas*

Main category: math.AP

TL;DR: This paper analyzes a reaction-diffusion equation with nonlocal diffusion in an advective environment, focusing on population dynamics in bounded advantageous patches with surrounding hostile regions.


<details>
  <summary>Details</summary>
Motivation: To understand how populations evolve in environments with localized favorable conditions and nonlocal dispersal mechanisms, particularly how advection affects survival and extinction.

Method: Derived optimal solvability criteria for traveling waves using eigenvalue analysis of linearized elliptic operators, then applied these criteria to study long-term behavior of parabolic solutions.

Result: Established that when population survives without advection (c=0), there exist two critical speeds c* and c** such that persistence occurs for |c| < c* and extinction for |c| > c**.

Conclusion: The study provides mathematical criteria for population persistence versus extinction in advective environments with nonlocal diffusion, quantifying the impact of environmental velocity on species survival.

Abstract: In this paper we study the following one-dimensional reaction-diffusion
problem $$ u_t+(-\Delta)^s u=f(x-c t, u) \;\:\textrm{ in } \mathbb{R}\times
(0,+\infty), $$ where $s>\frac{1}{2}$, $c \in \mathbb{R}$ is a prescribed
velocity, and $f$ is of KPP type, which describes the evolution of a population
in an advective environment subjected to nonlocal diffusion. We suppose the
environment is such that it is only advantageous in a bounded ``patch", outside
of which the species dies at an asymptotically constant rate.
  We first derive an optimal solvability criteria for the corresponding
traveling waves problem $$\Delta^s u+c u^{\prime}+f(x, u)=0 \;\:\textrm{ in }
\mathbb{R},$$ through the first eigenvalue of the associated linearized
elliptic operator with drift. Then we use this criteria to establish the long
time behavior of the solution to the parabolic problem, for any continuous
bounded nonnegative initial data, leading the species either through their
extinction or survival. Moreover, assuming that for $c=0$ the population
survives, we show that there exist two positive critical speeds $c^{*}$ and
$c^{**}$ such that for all $|c| <c^{*}$ the population persists, whereas and it
perishes for $|c| >c^{**}$.

</details>


### [21] [Remarks on the reinforcement of the spectrum of an elliptic problem with Robin boundary condition](https://arxiv.org/abs/2509.22305)
*Emanuele Cristoforoni,Federico Villone*

Main category: math.AP

TL;DR: Study of spectral properties of elliptic operators on domains with thin boundary layers, showing convergence to limit operator spectrum as layer thickness vanishes.


<details>
  <summary>Details</summary>
Motivation: To understand how the spectrum of elliptic operators behaves when the domain includes a thin boundary layer that shrinks to zero, which has applications in mathematical physics and boundary value problems.

Method: Mathematical analysis of differential elliptic operators on H^1 spaces with thin layers, using asymptotic analysis and spectral theory to study the limit as the layer thickness parameter ε approaches 0.

Result: Proved that the spectrum converges to the spectrum of a differential elliptic operator in H^1(Ω) as ε→0, and established first-order asymptotic development for the spectrum.

Conclusion: The spectral properties of elliptic operators with thin boundary layers converge to those of the limiting operator on the main domain, with quantifiable asymptotic behavior as the layer thickness vanishes.

Abstract: We investigate the spectral properties of a differential elliptic operator on
$H^1(\bar{\Omega}\cup \Sigma)$, where $\Omega$ is a smooth domain surrounded by
a layer $\Sigma$. The thickness of the layer is given by $\varepsilon h$, where
$h$ is a positive function defined on the boundary $\partial \Omega$ and
$\varepsilon$ is the ellipticity constant of the operator in $\Sigma$. We prove
that, in the limit for $\varepsilon$ going to $0$, the spectrum converges to
the spectrum of a differential elliptic operator in $H^1(\Omega)$, and we
investigate a first-order asymptotic development.

</details>


### [22] [Openness of shape optimizers in higher-order and non-scalar problems](https://arxiv.org/abs/2509.22487)
*Rupert L. Frank*

Main category: math.AP

TL;DR: The paper proves that quasi-open sets minimizing the first eigenvalues of polyharmonic, Lamé and Stokes operators with Dirichlet boundary conditions are actually open sets, removing previous dimensional restrictions.


<details>
  <summary>Details</summary>
Motivation: To establish that eigenvalue-minimizing sets for higher-order and non-scalar operators are open, extending previous results that had dimensional limitations.

Method: Uses Campanato theory, which is well-suited for analyzing higher-order and non-scalar operators in the context of eigenvalue minimization problems.

Result: Shows that quasi-open sets minimizing the first eigenvalues of polyharmonic, Lamé and Stokes operators with Dirichlet boundary conditions are open sets.

Conclusion: The study successfully removes dimensional restrictions from earlier works and demonstrates that eigenvalue-minimizing sets for these operators are open, using Campanato theory as an effective analytical tool.

Abstract: We consider the first eigenvalues of the polyharmonic, Lam\'e and Stokes
operators with Dirichlet boundary conditions on sets of given finite measure.
It is shown that a quasi-open set for which this eigenvalue is minimal is open.
This removes dimensional restrictions in earlier works. We use Campanato
theory, which works well in the present higher order or non-scalar setting.

</details>


### [23] [Rigidity of critical points of hydrophobic capillary functionals](https://arxiv.org/abs/2509.22532)
*Antonio De Rosa,Robin Neumayer,Reinaldo Resende*

Main category: math.AP

TL;DR: Rigidity of volume-preserving critical points of capillary energy in half space for contact angles 90°-120°, extending to 90°-180° under additional conditions.


<details>
  <summary>Details</summary>
Motivation: To establish rigidity theorems for capillary energy critical points without structural or regularity assumptions on finite perimeter sets.

Method: Mathematical analysis of sets of finite perimeter, proving rigidity through geometric measure theory and capillary energy minimization.

Result: Proved rigidity for contact angles 90°-120°, extended to 90°-180° when tangential capillary boundary is null; established anisotropic counterpart with density bounds.

Conclusion: Critical points of capillary energy exhibit rigidity in specified contact angle ranges, with extensions under additional geometric conditions.

Abstract: We prove the rigidity, among sets of finite perimeter, of volume-preserving
critical points of the capillary energy in the half space, in the case where
the prescribed interior contact angle is between $90^\circ$ and $120^\circ$. No
structural or regularity assumption is required on the finite perimeter sets.
Assuming that the ``tangential'' part of the capillary boundary is
$\mathcal{H}^n$-null, this rigidity theorem extends to the full hydrophobic
regime of interior contact angles between $90^\circ$ and $180^\circ$.
Furthermore, we establish the anisotropic counterpart of this theorem under the
assumption of lower density bounds.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [24] [Fracture-Driven Single Bubble Grows and Migration Model in Aquatic Muds](https://arxiv.org/abs/2509.22439)
*Regina Katsman*

Main category: physics.comp-ph

TL;DR: Model of buoyancy-driven methane bubble growth in muddy aquatic sediments that integrates solid mechanics with solute exchange dynamics and fracture mechanics.


<details>
  <summary>Details</summary>
Motivation: To understand methane bubble growth and migration in sediments, which is important for greenhouse gas emissions and sediment fracturing processes.

Method: Combines Linear Elastic Fracture Mechanics (LEFM) with solute exchange dynamics and gas conservation, using an advanced meshing strategy for computational efficiency.

Result: Developed a comprehensive model that simulates bubble elastic expansion, differential fracturing, and evolving bubble shape/size in cohesive sediments.

Conclusion: This model serves as a foundational tool for upscaling single bubble characteristics to effective gassy medium theories, improving accuracy in acoustic applications and methane emission evaluations.

Abstract: Methane (CH$_4$) is the most prevalent hydrocarbon and a significant
greenhouse gas found in the atmosphere. Buoyancy-driven CH$_4$ bubble growth
and migration within muddy aquatic sediments are closely associated with
sediment fracturing. This paper presents a model of buoyancy-driven CH$_4$
single bubble growth in fine-grained cohesive (muddy) aquatic sediment. * Solid
mechanics model component simulates bubble elastic expansion caused by solute
supply from the surrounding mud, followed by differential fracturing of the mud
by the evolving bubble front, a process governed by the principles of Linear
Elastic Fracture Mechanics (LEFM). This differential fracturing controls the
evolving shape and size of the bubble. * The model integrates the LEFM with the
dynamics of solute exchange between the bubble and the surrounding mud,
alongside the conservation of CH$_4$ gas within the bubble. * An advanced
meshing strategy allows balancing between the geometry resolution and the
amount of mesh elements, thereby optimizing for both solution accuracy and
computational efficiency. This model is intended to be a foundational tool for
proper upscaling of single bubble characteristics to effective gassy medium
theories. This will enhance the accuracy of the acoustic applications and could
contribute to evaluation of overall CH$_4$ emission from the aquatic muds.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [25] [Electrostatic waves in astrophysical Druyvesteyn plasmas: I. Langmuir waves](https://arxiv.org/abs/2509.21506)
*Simon Tischmann,Rudi Gaelzer,Dustin Schröder,Marian Lazar,Horst Fichtner*

Main category: physics.plasm-ph

TL;DR: This paper introduces a new modeling approach using generalized Druyvesteyn distributions to study linear waves in non-equilibrium plasmas, providing an alternative to Kappa distributions that can capture both high-energy tails and low-energy flat-tops observed in various astrophysical environments.


<details>
  <summary>Details</summary>
Motivation: Astrophysical plasmas are often in non-equilibrium states with non-Maxwellian velocity distributions, as evidenced by direct measurements in solar wind, solar corona, and planetary environments. Current models like Kappa distributions have limitations in capturing the full complexity of these distributions.

Method: The authors use generalized Druyvesteyn distributions to model non-equilibrium plasmas, derive the corresponding dispersion relation for longitudinal waves in terms of a newly introduced Druyvesteyn dispersion function, numerically compute dispersion curves and damping rates for isotropic cases, and provide analytical approximations for weak damping limits.

Result: The study successfully develops a new modeling framework that can reproduce both high-energy tails and low-energy flat-tops of velocity distributions observed in various astrophysical contexts, such as electrons associated with Earth's bow shock, interplanetary shocks, and solar transition region.

Conclusion: This work provides a new quantitative modeling tool for treating a variety of non-Maxwellian plasmas, offering an alternative approach that better captures the complex characteristics of velocity distributions in astrophysical environments beyond what traditional Kappa distributions can achieve.

Abstract: Plasmas in various astrophysical systems are in non-equilibrium states as
evidenced by direct in-situ measurements in the solar wind, solar corona and
planetary environments as well as by indirect observations of various sources
of waves and emissions. Specific are non-Maxwellian velocity distributions with
suprathermal tails, for whose description the most-used are the Kappa
(power-law) distributions. With this paper we introduce a modeling alternative
for linear waves in plasmas described by another non-equilibrium model, namely
the generalized Druyvesteyn distribution. This can reproduce not only the
high-energy tails, but also the low-energy flat-tops of velocity distributions,
like those of electrons associated with the Earth's bow shock and
interplanetary shocks or of electrons in the solar transition region. We derive
the corresponding dispersion relation for longitudinal waves in terms of the
newly introduced Druyvsteyn dispersion function, numerically compute, for the
isotropic case, the dispersion curves as well as damping rates, and provide
analytical approximation in the limit of weak damping. Thereby, we provide a
new modeling tool that facilitates the quantitative treatment of a variety of
non-Maxwellian plasmas.

</details>


### [26] [Generation of directed electron beams by tight focusing of an ultrashort IR laser in a near-critical plasma](https://arxiv.org/abs/2509.21645)
*Marianna Lytova,François Fillion-Gourdeau,Simon Vallières,Sylvain Fourmaux,Stéphane Payeur,François Légaré,Steve MacLean*

Main category: physics.plasm-ph

TL;DR: Optimizing laser-driven electron acceleration in ambient air using laser and gas parameters to achieve multi-MeV electrons with low divergence for medical and imaging applications.


<details>
  <summary>Details</summary>
Motivation: To control and optimize electron beams generated by laser acceleration in ambient air, exploring strategies using laser polarization and gas parameters for practical applications.

Method: Theoretical analysis comparing linearly, circularly, and radially polarized laser pulses in near-critical plasmas, studying acceleration efficiency through maximum kinetic energy and electron count, and scaling to different gas densities.

Result: Linearly and circularly polarized pulses are more efficient than radially polarized ones; linearly polarized pulses produce lower divergence electron beams; optimal conditions found at 1.5μm wavelength with a_0≥15; noble gases (Ne, Ar) identified as suitable media.

Conclusion: This acceleration scheme enables multi-MeV electrons with low divergence using millijoule-class high-repetition rate lasers, making it promising for medical sciences and ultrafast imaging applications.

Abstract: Recent studies have demonstrated the possibility of accelerating electrons to
MeV energies in ambient air using tightly focused laser configurations. In this
article, we explore possible strategies to control and optimize the resulting
electron beams using laser and gas parameters. Our theoretical analysis shows
that in near-critical plasmas, linearly and circularly polarized pulses are
more efficient than radially polarized pulses for electron acceleration. In
addition, electron beams obtained from linearly polarized pulses have lower
divergence angles. By studying the efficiency of the acceleration process -
characterized by the maximum kinetic energy of electrons and their total number
- we identify optimal conditions in ambient air at wavelength lambda_0
approximately 1.5 micrometers and a_0 >= 15. We also scale our results to
lower-density air and demonstrate that some noble gases (Ne, Ar) are suitable
media for accelerating electrons. Our investigations show that this
acceleration scheme enables multi-MeV electrons with low divergence using
millijoule-class high-repetition rate lasers, making it a promising candidate
for applications in medical sciences and ultrafast imaging.

</details>


### [27] [Saturation of the kinetic ballooning instability due to the electron parallel nonlinearity](https://arxiv.org/abs/2509.21680)
*Yang Chen,Scott E. Parker*

Main category: physics.plasm-ph

TL;DR: Implementation of electron parallel nonlinearity (EPN) in GEM gyrokinetic PIC code shows strong impact on heat transport above KBM threshold, linked to electron radial motion from magnetic fluttering.


<details>
  <summary>Details</summary>
Motivation: To study the effects of electron parallel nonlinearity on plasma turbulence and heat transport in gyrokinetic simulations, particularly for kinetic ballooning modes.

Method: Implemented EPN in the GEM gyrokinetic Particle-in-Cell turbulence code and applied it to the Cyclone Base Case scenario.

Result: EPN has strong effect on saturated heat transport above KBM threshold, with evidence showing this is due to electron radial motion from magnetic fluttering creating fine structures in velocity space.

Conclusion: Electron parallel nonlinearity significantly influences plasma turbulence and heat transport through mechanisms involving magnetic fluttering and velocity space structures.

Abstract: The electron parallel nonlinearity (EPN) is implemented in the gyrokinetic
Particle-in-Cell turbulence code GEM [Y. Chen and S. E. Parker, J. Comp. Phys.
220, 839 (2007)]. Application to the Cyclone Base Case reveals a strong effect
of EPN on the saturated heat transport above the kinetic ballooning mode
threshold. Evidence is provided to show that the strong effect is associated
with the electron radial motion due to magnetic fluttering, which turns fine
structures of the KBM eigenmode in radius into fine structures in velocity and
increases the magnitude of the EPN term in the kinetic equation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [28] [Redesigning GROMACS Halo Exchange: Improving Strong Scaling with GPU-initiated NVSHMEM](https://arxiv.org/abs/2509.21527)
*Mahesh Doijade,Andrey Alekseenko,Ania Brown,Alan Gray,Szilárd Páll*

Main category: cs.DC

TL;DR: GPU-initiated communication redesign of GROMACS halo-exchange using NVSHMEM, achieving up to 2x performance improvement through better communication-computation overlap.


<details>
  <summary>Details</summary>
Motivation: GROMACS is latency-sensitive with sub-millisecond iteration rates, and MPI's CPU-centric nature introduces latencies that hinder GPU utilization and scalability on heterogeneous supercomputers.

Method: NVSHMEM-based GPU kernel-initiated redesign of domain decomposition halo-exchange algorithm, with fused data packing/communication, hardware latency-hiding, kernel fusion across communication phases, and asynchronous copy engine over NVLink.

Result: 1.5x improvement in intra-node scaling, 2x in multi-node scaling over NVLink, and 1.3x multi-node over NVLink+InfiniBand.

Conclusion: GPU-initiated communication provides significant benefits for strong-scaling latency-sensitive applications like molecular dynamics simulations.

Abstract: Improving time-to-solution in molecular dynamics simulations often requires
strong scaling due to fixed-sized problems. GROMACS is highly
latency-sensitive, with peak iteration rates in the sub-millisecond, making
scalability on heterogeneous supercomputers challenging. MPI's CPU-centric
nature introduces additional latencies on GPU-resident applications' critical
path, hindering GPU utilization and scalability. To address these limitations,
we present an NVSHMEM-based GPU kernel-initiated redesign of the GROMACS domain
decomposition halo-exchange algorithm. Highly tuned GPU kernels fuse data
packing and communication, leveraging hardware latency-hiding for fine-grained
overlap. We employ kernel fusion across overlapped data forwarding
communication phases and utilize the asynchronous copy engine over NVLink to
optimize latency and bandwidth. Our GPU-resident formulation greatly increases
communication-computation overlap, improving GROMACS strong scaling performance
across NVLink by up to 1.5x (intra-node) and 2x (multi-node), and up to 1.3x
multi-node over NVLink+InfiniBand. This demonstrates the profound benefits of
GPU-initiated communication for strong-scaling a broad range of
latency-sensitive applications.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [29] [A regret minimization approach to fixed-point iterations](https://arxiv.org/abs/2509.21653)
*Joon Kwon*

Main category: math.OC

TL;DR: A conversion scheme that transforms regret-minimizing algorithms into fixed point iterations with convergence guarantees, extending classical Krasnoselskii-Mann iterations and enabling new adaptive methods.


<details>
  <summary>Details</summary>
Motivation: To develop a general framework for converting regret-minimizing algorithms into fixed point iterations, extending beyond classical methods and enabling adaptive convergence guarantees.

Method: Proposed conversion scheme that transforms regret minimizing algorithms into fixed point iterations, with specific focus on converting Online Gradient Descent (recovering classical Krasnoselskii-Mann) and AdaGrad family algorithms.

Result: The approach yields new simple iterations for finding fixed points of non-self operators and provides fixed point iterations with adaptive guarantees. Numerical experiments show faster convergence of AdaGrad-based iterations compared to classical Krasnoselskii-Mann iterations.

Conclusion: The conversion scheme successfully bridges regret minimization and fixed point theory, providing a unified framework that extends classical methods and enables adaptive algorithms with improved convergence properties.

Abstract: We propose a conversion scheme that turns regret minimizing algorithms into
fixed point iterations, with convergence guarantees following from regret
bounds. The resulting iterations can be seen as a grand extension of the
classical Krasnoselskii--Mann iterations, as the latter are recovered by
converting the Online Gradient Descent algorithm. This approach yields new
simple iterations for finding fixed points of non-self operators. We also focus
on converting algorithms from the AdaGrad family of regret minimizers, and thus
obtain fixed point iterations with adaptive guarantees of a new kind. Numerical
experiments on various problems demonstrate faster convergence of AdaGrad-based
fixed point iterations over Krasnoselskii--Mann iterations.

</details>


### [30] [Spiking Neural Networks: a theoretical framework for Universal Approximation and training](https://arxiv.org/abs/2509.21920)
*Umberto Biccari*

Main category: math.OC

TL;DR: This paper provides rigorous mathematical analysis of Spiking Neural Networks (SNNs) with Leaky Integrate-and-Fire neurons, proving universal approximation capabilities and analyzing spike timing dynamics across layers.


<details>
  <summary>Details</summary>
Motivation: SNNs are biologically-inspired and energy-efficient alternatives to traditional neural networks, but their theoretical foundations remain poorly understood. The authors aim to establish rigorous mathematical understanding of SNN capabilities and limitations.

Method: The authors develop mathematical analysis of SNN architecture with LIF neurons and threshold-reset dynamics. They use constructive encoding of target values via spike timing and analyze the interplay between idealized δ-driven dynamics and smooth Gaussian-regularized models.

Result: The paper proves a universal approximation theorem showing SNNs can approximate continuous functions to arbitrary accuracy. It also analyzes spike time behavior across layers, proving well-posedness of hybrid dynamics and deriving conditions for stable, decreasing, or increasing spike counts due to resonance or overlapping inputs.

Conclusion: The results provide principled theoretical foundation for understanding both the expressive power and dynamical constraints of SNNs, offering theoretical guarantees for their use in classification and signal processing tasks.

Abstract: Spiking Neural Networks (SNNs) are widely regarded as a biologically-inspired
and energy-efficient alternative to classical artificial neural networks. Yet,
their theoretical foundations remain only partially understood. In this work,
we develop a rigorous mathematical analysis of a representative SNN
architecture based on Leaky Integrate-and-Fire (LIF) neurons with
threshold-reset dynamics. Our contributions are twofold. First, we establish a
universal approximation theorem showing that SNNs can approximate continuous
functions on compact domains to arbitrary accuracy. The proof relies on a
constructive encoding of target values via spike timing and a careful interplay
between idealized $\delta$-driven dynamics and smooth Gaussian-regularized
models. Second, we analyze the quantitative behavior of spike times across
layers, proving well-posedness of the hybrid dynamics and deriving conditions
under which spike counts remain stable, decrease, or in exceptional cases
increase due to resonance phenomena or overlapping inputs. Together, these
results provide a principled foundation for understanding both the expressive
power and the dynamical constraints of SNNs, offering theoretical guarantees
for their use in classification and signal processing tasks.

</details>


### [31] [A dynamical formulation of multi-marginal optimal transport](https://arxiv.org/abs/2509.22494)
*Brendan Pass,Yair Shenfeld*

Main category: math.OC

TL;DR: A primal-dual dynamical formulation for multi-marginal optimal transport with (semi-)convex costs, extending beyond classical approaches and enabling convex optimization methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations of classical dynamical approaches like Benamou-Brenier, which don't cover certain cost functions in multi-marginal optimal transport problems.

Method: Developed a primal-dual dynamical formulation that yields a convex optimization problem, allowing use of convex optimization tools and proximal splitting methods.

Result: The formulation applies to cost functions not covered by classical approaches and enables finding quasi-Monge solutions for translation-invariant costs.

Conclusion: The proposed dynamical formulation provides a convex optimization framework for multi-marginal optimal transport, extending applicability beyond classical methods and demonstrating practical utility through numerical implementation.

Abstract: We present a primal-dual dynamical formulation of the multi-marginal optimal
transport problem for (semi-)convex cost functions. Even in the two-marginal
setting, this formulation applies to cost functions not covered by the
classical dynamical approach of Benamou-Brenier. Our dynamical formulation
yields a convex optimization problem, enabling the use of convex optimization
tools to find quasi-Monge solutions of the static multi-marginal problem for
translation-invariant costs. We illustrate our results numerically with
proximal splitting methods.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [32] [Enhancing Molecular Dipole Moment Prediction with Multitask Machine Learning](https://arxiv.org/abs/2509.22435)
*William Colglazier,Nicholas Lubbers,Sergei Tretiak,Anders M. N. Niklasson,Maksim Kulichenko*

Main category: physics.chem-ph

TL;DR: Multitask learning using dipole magnitudes and Mulliken charges improves molecular dipole prediction by 30%, showing that auxiliary data with limited quantitative reliability can enhance model accuracy through qualitative physical insights.


<details>
  <summary>Details</summary>
Motivation: To improve molecular dipole moment predictions by leveraging auxiliary data (Mulliken charges) that lack quantitative accuracy but contain qualitative physical information about charge distribution, testing whether lower quality labels can enhance model performance.

Method: Multitask machine learning strategy that simultaneously trains on quantum dipole magnitudes (primary target) and inexpensive Mulliken atomic charges (auxiliary task), with Mulliken charges given small weight in the loss function.

Result: Including Mulliken charges with small weight in loss function yields up to 30% improvement in dipole prediction accuracy, enabling the model to learn more physically grounded representations of charge distributions.

Conclusion: Even auxiliary data of limited quantitative reliability can provide valuable qualitative physical insights, strengthening ML models for molecular properties by improving both accuracy and consistency of predictions.

Abstract: We present a multitask machine learning strategy for improving the prediction
of molecular dipole moments by simultaneously training on quantum dipole
magnitudes and inexpensive Mulliken atomic charges. With dipole magnitudes as
the primary target and assuming only scalar dipole values are available without
vector components we examine whether incorporating lower quality labels that do
not quantitatively reproduce the target property can still enhance model
accuracy. Mulliken charges were chosen intentionally as an auxiliary task,
since they lack quantitative accuracy yet encode qualitative physical
information about charge distribution. Our results show that including Mulliken
charges with a small weight in the loss function yields up to a 30% improvement
in dipole prediction accuracy. This multitask approach enables the model to
learn a more physically grounded representation of charge distributions,
thereby improving both the accuracy and consistency of dipole magnitude
predictions. These findings highlight that even auxiliary data of limited
quantitative reliability can provide valuable qualitative physical insights,
ultimately strengthening the predictive power of machine learning models for
molecular properties.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [33] [Breakdown of Kolmogorov Scaling and Modified Energy Transfer in Bubble-Laden Turbulence](https://arxiv.org/abs/2509.22324)
*Andrea Montessori,Marco Lauricella,Aritra Mukherjee,Luca Brandt*

Main category: physics.flu-dyn

TL;DR: Study shows dispersed bubbles in turbulence preserve global energy cascade but cause phase-specific deviations, with gas phase showing non-classical scaling while liquid phase remains classical.


<details>
  <summary>Details</summary>
Motivation: To understand how dispersed bubble phases affect turbulence structure and energy transfer mechanisms in multiphase flows, particularly bubble-laden flows relevant to environmental, atmospheric, and industrial applications.

Method: High-resolution high-performance simulations using lattice Boltzmann method to analyze forced homogeneous and isotropic turbulence with dispersed bubble phase.

Result: Global energy cascade follows Kolmogorov predictions, but phase-specific analysis reveals gas phase exhibits significant deviations with nearly flat spectrum at low wave numbers and k^{-3} scaling at intermediate scales, while liquid phase maintains classical turbulence structure up to 24% gas volume fractions.

Conclusion: Despite dispersed phase presence, global energy transfer remains universal, but phase-specific interactions modify turbulent dynamics at small scales, providing insights for understanding multiphase turbulence in various applications.

Abstract: We investigate the effect of a dispersed bubble phase on forced homogeneous
and isotropic turbulence using high-resolution high-performance simulations
based on the lattice Boltzmann method. While the classical Kolmogorov energy
cascade is largely preserved when considering the system as a whole, a
phase-specific analysis reveals striking deviations from the classical
turbulence scaling. In particular, the gas phase exhibits significant
departures from Kolmogorov's predictions, whereas the continuous liquid phase
retains a turbulence structure consistent with classical expectations up to 24%
in gas volume fractions. These findings suggest that, despite the presence of a
dispersed phase, the global energy transfer remains close to a universal
behavior. At the same time, phase-specific interactions are shown to introduce
modifications to the turbulent dynamics at small scales. In particular, the
gas-phase exhibits a nearly flat spectrum at low wave numbers followed by a
k^{-3} scaling at intermediate scales pointing to the presence of patterns of
localized bursts uniformly distributed between two finite wavelengths.Our
results aim at deepening the understanding of multiphase turbulence,
particularly in the context of energy transfer mechanisms and phase
interactions in bubble-laden flows. This study provides a framework for future
investigations into the fundamental properties of multiphase turbulence and its
implications for environmental, atmospheric, and industrial flows.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [34] [Dimer-driven multiple reentrant localization with composite potential](https://arxiv.org/abs/2509.21338)
*Pei-Jie Chang,Dong Ruan,Gui-Lu Long*

Main category: cond-mat.dis-nn

TL;DR: The paper investigates reentrant localization transitions in 1D lattices with periodic potentials and quasi-periodic modulations, showing multiple localization transitions stabilized by dimer structures.


<details>
  <summary>Details</summary>
Motivation: To understand how reentrant localization persists under more general conditions beyond simple dimerized hopping and staggered disorder, specifically with additional periodic potentials and modified quasi-periodic modulations.

Method: Used eigenstate-based indicators and experimentally accessible dynamical observables to study localization in 1D lattices with periodic potential and quasi-periodic modulation, systematically varying periodicity parameter α and quasi-periodic frequency β.

Result: Identified robust multiple reentrant localization transitions uniquely stabilized by dimer structure, where competition between onsite periodic potential and quasi-periodic modulation is most pronounced. The behavior disappears for any deviation from dimer configuration.

Conclusion: The interplay between competing factors (periodic potential and quasi-periodic modulation) drives multiple reentrant localization transitions, with dimer structure playing an essential role in stabilizing these transitions.

Abstract: Recent studies have revealed reentrant localization transitions in
quasi-periodic one-dimensional lattices, where the competition between
dimerized hopping and staggered disorder plays a central role. Yet the extent
to which such reentrant localization persists under more general conditions,
such as additional periodic potentials, modified quasi-periodic modulations
remains unclear. Here we investigate localization phenomena in a
one-dimensional lattice subject to a periodic potential and an additional
quasi-periodic modulation. Using both eigenstate-based indicators and
experimentally accessible dynamical observables, we identify robust reentrant,
or multiple, localization transitions. We show that these transitions are
uniquely stabilized by the dimer structure of the unit cell, where the
competition between the onsite periodic potential and the quasi-periodic
modulation becomes most pronounced. By systematically varying the periodicity
parameter $\alpha$ and the quasi-periodic frequency $\beta$, we find that the
robust multiple reentrant localization behavior disappears for any deviation
from the dimer configuration, confirming its essential role. Our results
suggest that the interplay between these competing factors drives the multiple
reentrant localization transitions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [35] [MORPH: Shape-agnostic PDE Foundation Models](https://arxiv.org/abs/2509.21670)
*Mahindra Singh Rautela,Alexander Most,Siddharth Mansingh,Bradley C. Love,Ayan Biswas,Diane Oyen,Earl Lawrence*

Main category: cs.CV

TL;DR: MORPH is a shape-agnostic, autoregressive foundation model for PDEs that handles heterogeneous spatiotemporal datasets of varying dimensions and fields through a convolutional vision transformer architecture with component-wise convolution, inter-field cross-attention, and axial attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: To create a flexible foundation model that can handle the heterogeneous and multimodal nature of scientific observations in PDEs, addressing challenges of varying data dimensionality, resolutions, and mixed scalar/vector fields.

Method: Built on convolutional vision transformer backbone with three key components: (1) component-wise convolution for joint processing of scalar/vector channels, (2) inter-field cross-attention for information propagation between physical fields, (3) axial attention for computational efficiency. Pretrained on diverse PDE datasets and evaluated with fine-tuning and LoRA adapters.

Result: MORPH outperforms models trained from scratch in both zero-shot and full-shot generalization, matching or surpassing strong baselines and state-of-the-art models across extensive evaluations.

Conclusion: MORPH presents a flexible and powerful backbone for learning from heterogeneous scientific observations, charting a path toward scalable and data-efficient scientific machine learning.

Abstract: We introduce MORPH, a shape-agnostic, autoregressive foundation model for
partial differential equations (PDEs). MORPH is built on a convolutional vision
transformer backbone that seamlessly handles heterogeneous spatiotemporal
datasets of varying data dimensionality (1D--3D) at different resolutions,
multiple fields with mixed scalar and vector components. The architecture
combines (i) component-wise convolution, which jointly processes scalar and
vector channels to capture local interactions, (ii) inter-field
cross-attention, which models and selectively propagates information between
different physical fields, (iii) axial attentions, which factorizes full
spatiotemporal self-attention along individual spatial and temporal axes to
reduce computational burden while retaining expressivity. We pretrain multiple
model variants on a diverse collection of heterogeneous PDE datasets and
evaluate transfer to a range of downstream prediction tasks. Using both
full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH
outperforms models trained from scratch in both zero-shot and full-shot
generalization. Across extensive evaluations, MORPH matches or surpasses strong
baselines and recent state-of-the-art models. Collectively, these capabilities
present a flexible and powerful backbone for learning from heterogeneous and
multimodal nature of scientific observations, charting a path toward scalable
and data-efficient scientific machine learning.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [36] [Enhanced Axion-Photon Conversion via Seeded Photons instead of Resonant Cavities for Short-Pulse Axion Detection](https://arxiv.org/abs/2509.21417)
*Xiangyan An,Min Chen,Jianglai Liu,Zhengming Sheng,Jie Zhang*

Main category: hep-ph

TL;DR: A seeded axion-photon conversion scheme enhances LSW experiment sensitivity by using coherent seed EM fields to amplify axion-induced signals through constructive interference, potentially achieving orders-of-magnitude higher photon yields.


<details>
  <summary>Details</summary>
Motivation: To improve sensitivity of light-shining-through-wall experiments for axion detection, especially when resonant cavities are impractical, by overcoming limitations of standard unseeded approaches.

Method: Inject a small, coherent seed EM field into the axion-photon conversion region, allowing axion-induced EM fields to constructively interfere with the seed field. Derived conversion probability from axion-modified Maxwell equations and analyzed phase-dependent interference effects.

Result: The seeded scheme can achieve orders-of-magnitude higher photon yield per axion compared to standard LSW setups, potentially surpassing resonance-enhanced experiments in certain parameter regimes.

Conclusion: This seeded approach offers a promising route to extend laboratory axion search reach, particularly when resonant cavities are impractical, with significant sensitivity improvements through constructive interference amplification.

Abstract: We propose a seeded axion-photon conversion scheme to enhance the sensitivity
of light-shining-through-wall (LSW) experiments for axion detection, where the
axions are generated from short pulse lasers and the usual resonant cavity is
not applicable. By injecting a small, coherent seed electromagnetic field (EM
field) into the axion-photon conversion region, the axion-induced EM field can
constructively interfere with the seed field, amplifying the regenerated photon
number beyond the unseeded case. Starting from the axion-modified Maxwell
equations, we derive the axion-photon conversion probability and show how an
initial seed field boosts the final photon amplitude. The relative phase
between the seed field and the axion field can cause constructive or
destructive interference, though the measurable quantity is the net photon
number variation. We evaluate the expected signal enhancement, signal-to-noise
considerations (including seed shot noise), and the potential improvement in
coupling sensitivity. Compared to a standard LSW setup, the seeded scheme can
achieve orders-of-magnitude higher photon yield per axion, potentially
surpassing resonance-enhanced experiments in certain parameter regimes. This
approach offers a promising route to extend the reach of laboratory axion
searches, especially for the case where the resonant cavities are impractical.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [37] [Stationary and stable varifolds with singularities](https://arxiv.org/abs/2509.21508)
*Camillo De Lellis,Jonas Hirsch,Luca Spolaor*

Main category: math.DG

TL;DR: Construction of minimal m-dimensional immersions in R^{m+1} with C^{m-1,α} metrics, featuring catenoidal necks or floating disks converging to isolated multiplicity 2 singular flat points.


<details>
  <summary>Details</summary>
Motivation: To study minimal immersions with specific geometric structures (catenoidal necks or floating disks) that converge to singular flat points, exploring the behavior of minimal surfaces near singularities.

Method: Construct minimal m-dimensional immersions in R^{m+1} spaces with C^{m-1,α} metrics, where the immersions contain sequences of catenoidal necks or floating disks.

Result: Successfully constructed minimal immersions where the catenoidal necks or floating disks converge to isolated, multiplicity 2 singular flat points.

Conclusion: The construction demonstrates the existence of minimal immersions with specific geometric features converging to singular points, contributing to understanding minimal surface behavior near singularities.

Abstract: We construct minimal $m$-dimensional immersions in $\mathbb{R}^{m+1}$,
equipped with a $C^{m-1, \alpha}$ metric, $\alpha\in [0,1)$, with a sequence of
catenoidal necks or floating disks converging to an isolated, multiplicity $2$,
singular flat point.

</details>


### [38] [Noncollapsing for Curvature Flows with Inhomogeneous Speeds](https://arxiv.org/abs/2509.21830)
*Weimin Sheng,Ye Zhu*

Main category: math.DG

TL;DR: The paper establishes exterior noncollapsing estimates for fully nonlinear curvature flows of hypersurfaces in Euclidean space, addressing the challenge of nonlinear evolution equations for exscribed curvature.


<details>
  <summary>Details</summary>
Motivation: To extend noncollapsing estimates to fully nonlinear curvature flows where the speed function is nonlinear, unlike previous linearized approaches, and to refine interior estimates for inverse-concave speed functions.

Method: The authors study hypersurfaces evolving by symmetric, monotone increasing, 1-homogeneous, positive speed functions F composed with modulating functions Ψ. They assume F is convex or inverse-concave and Ψ satisfies corresponding structural conditions.

Result: Exterior noncollapsing estimates are established for the flow, overcoming the difficulty of nonlinear evolution equations for exscribed curvature. For inverse-concave F, interior estimates are refined using Andrews and Langford's argument.

Conclusion: The paper successfully extends noncollapsing theory to fully nonlinear curvature flows and provides refined interior estimates for inverse-concave speed functions, advancing the understanding of geometric evolution equations.

Abstract: We study closed, embedded hypersurfaces in Euclidean space evolving by fully
nonlinear curvature flows, whose speed is given by a symmetric, monotone
increasing, $1$-homogeneous, positive underlying speed function $F$ composed
with a modulating function $\Psi$. Under the assumption that $F$ is convex or
inverse-concave and that $\Psi$ satisfies the corresponding structural
conditions, we establish exterior noncollapsing estimates for the flow. The
main difficulty stems from the nonlinearity of the evolution equation satisfied
in the viscosity sense by the exscribed curvature, whereas in previous works it
is a solution to the linearized flow. Moreover, in the case where $F$ is
inverse-concave, we refine Andrews and Langford's argument for the interior
case.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [39] [A Quantum Algorithm for Nonlinear Electromagnetic Fluid Dynamics via Koopman-von Neumann Linearization](https://arxiv.org/abs/2509.22503)
*Hayato Higuchi,Yuki Ito,Kazuki Sakamoto,Keisuke Fujii,Akimasa Yoshikawa*

Main category: quant-ph

TL;DR: Quantum algorithm for plasma simulation using Koopman-von Neumann linearization and Hamiltonian simulation via quantum singular value transformation, achieving polynomial speedup over classical methods.


<details>
  <summary>Details</summary>
Motivation: To overcome computational resource limitations in plasma simulations that scale polynomially with spatial grids, enabling large-scale plasma modeling.

Method: Map nonlinear electromagnetic fluid dynamics to Schrödinger equation using Koopman-von Neumann linearization, then evolve system via Hamiltonian simulation with quantum singular value transformation.

Result: Achieves O(sN_x polylog(N_x)T) time complexity (vs classical O(sN_x^s(T^{5/4}+TN_x))) and exponential space complexity reduction from O(sN_x^s) to O(s polylog(N_x)). Validated with Kelvin-Helmholtz instability simulation.

Conclusion: Quantum computing provides viable pathway to overcome computational barriers in multiscale plasma modeling, with practical feasibility demonstrated.

Abstract: To simulate plasma phenomena, large-scale computational resources have been
employed in developing high-precision and high-resolution plasma simulations.
One of the main obstacles in plasma simulations is the requirement of
computational resources that scale polynomially with the number of spatial
grids, which poses a significant challenge for large-scale modeling. To address
this issue, this study presents a quantum algorithm for simulating the
nonlinear electromagnetic fluid dynamics that govern space plasmas. We map it,
by applying Koopman-von Neumann linearization, to the Schr\"{o}dinger equation
and evolve the system using Hamiltonian simulation via quantum singular value
transformation. Our algorithm scales $O \left(s N_x \, \mathrm{polylog} \left(
N_x \right) T \right)$ in time complexity with $s$, $N_x$, and $T$ being the
spatial dimension, the number of spatial grid points per dimension, and the
evolution time, respectively. Comparing the scaling $O \left( s N_x^s
\left(T^{5/4}+T N_x\right) \right)$ for the classical method with the finite
volume scheme, this algorithm achieves polynomial speedup in $N_x$. The space
complexity of this algorithm is exponentially reduced from $O\left( s N_x^s
\right)$ to $O\left( s \, \mathrm{polylog} \left( N_x \right) \right)$.
Numerical experiments validate that accurate solutions are attainable with
smaller $m$ than theoretically anticipated and with practical values of $m$ and
$R$, underscoring the feasibility of the approach. As a practical
demonstration, the method accurately reproduces the Kelvin-Helmholtz
instability, underscoring its capability to tackle more intricate nonlinear
dynamics. These results suggest that quantum computing can offer a viable
pathway to overcome the computational barriers of multiscale plasma modeling.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [40] [Instability of the halocline at the North Pole](https://arxiv.org/abs/2509.21378)
*Christian Puntini*

Main category: physics.geo-ph

TL;DR: The paper analyzes the stability of near-inertial Pollard waves in the Arctic Ocean halocline, finding they become linearly unstable when wave steepness exceeds a specific computable threshold.


<details>
  <summary>Details</summary>
Motivation: To address stability issues of near-inertial Pollard waves in the Arctic Ocean halocline region, building on previous work by Puntini (2025a).

Method: Adopts the short-wavelength instability approach, reducing stability analysis to studying ODEs along fluid trajectories.

Result: Near-inertial Pollard waves become linearly unstable when their steepness exceeds a specific threshold that can be computed using the explicit dispersion relation.

Conclusion: The explicit dispersion relation enables easy computation of the instability threshold based on water column physical properties.

Abstract: In this paper we address the issue of stability for the near-inertial Pollard
waves, as a model for the halocline in the region of the Arctic Ocean centered
around the North Pole, derived in Puntini (2025a). Adopting the
short-wavelength instability approach, the stability of such flows reduces to
study the stability of a system of ODEs along fluid trajectories, leading to
the result that, when the steepness of the near-inertial Pollard waves exceeds
a specific threshold, those waves are linearly unstable. The explicit
dispersion relation of the model allows to easily compute such threshold,
knowing the physical properties of the water column.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [41] [Extremal polynomials for the Rogosinski--Szegő estimates of the third coefficient of nonnegative sine polynomials](https://arxiv.org/abs/2509.22238)
*Dmitriy Dmitrishin,Alexander Stokolos,Walter Trebels*

Main category: math.CV

TL;DR: Sharp upper and lower bounds for coefficient a₃ in normalized sine-polynomials and typically real polynomials, with identification of extremal polynomials.


<details>
  <summary>Details</summary>
Motivation: Previous work by Rogosinski and Szegő provided bounds for a₃ but couldn't construct the extremal polynomials. This paper aims to find these extremal polynomials.

Method: Uses Fejér's method with Chebyshev polynomials of the second kind and their derivatives, working in the framework of normalized typically real polynomials on the unit disc.

Result: Regained sharp upper and lower estimates for a₃ and identified extremal polynomials. For odd N, extremizers are unique; for even N, there's a one-parameter family.

Conclusion: Successfully constructed the extremal polynomials that attain the bounds for coefficient a₃, solving the problem left open by previous research.

Abstract: In the class of normalized sine-polynomials $S(t),$ non-negative on
$[0,\pi],$ W.Rogosinski and G.Szeg\H{o} 1950 considered a number of extremal
problems and proved, among other things, sharp upper and lower estimates for
the coefficient $a_3.$ Their proof is based on the Luk\'acs representation of
non-negative algebraic polynomials. This method does not lead to the
construction of polynomials attaining the extreme values.
  We consider the corresponding problem in the framework of normalized
typically real polynomials $P(z)$ on the unit disc in $\mathbb C.$ By
L.Fej\'er's method with the additional use of the Chebyshev polynomials of the
second kind and their derivatives, we regain the sharp upper and lower
estimates for $a_3$ and identify the extremal polynomials. The corresponding
statements for sine polynomials follow by the observation
$S(t)=\text{Im}\{P(e^{it})\}$. For odd $N$ the extremizers are unique, for even
$N$ there is a one-parameter family of extremizers.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [42] [step2point dataset: Detailed shower simulation for data representation studies](https://arxiv.org/abs/2509.22340)
*Anna Zaborowska,Peter McKeown*

Main category: hep-ex

TL;DR: The paper presents a simulation dataset for studying optimal data representations of electromagnetic and hadronic showers in calorimeters, aiming to develop tools for creating inputs for generative model-based surrogate simulators.


<details>
  <summary>Details</summary>
Motivation: To enable the study and development of optimal data representations for calorimeter shower simulations, with the goal of creating general tools that can translate detailed simulation outputs into formats suitable for generative surrogate models.

Method: The authors created and published a detailed simulation dataset containing electromagnetic and hadronic shower data from calorimeters, which can be used to construct and analyze different data representation approaches.

Result: A comprehensive simulation dataset is made available that allows researchers to study various data representation methods for calorimeter shower simulations.

Conclusion: This dataset serves as a foundation for developing optimal data representations that can be used as inputs for surrogate simulators based on generative models, potentially improving the efficiency of calorimeter simulation workflows.

Abstract: This dataset contains a detailed simulation output that allows the
construction and study of different data representations for electromagnetic
and hadronic showers in calorimeters. It is published so that optimal data
representations can be studied, with the ultimate goal of constructing a
general tool that takes detailed simulation output and translates it into an
optimal representation that can serve as the input to surrogate simulators
based on generative models.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [43] [Understanding the oxidation of pure Tungsten in air and its impact on the lifecycle of a fusion power plant](https://arxiv.org/abs/2509.22031)
*R. Li,G. Álvarez,A. Ipakchi,L. Cupertino-Malheiros,M. R. Gilbert,E. Martínez-Pañeda,E. Prestat*

Main category: physics.app-ph

TL;DR: Study of tungsten oxidation and oxide sublimation effects on fusion power plant lifecycle, revealing void/crack formation above 600°C that creates dust and spalling issues.


<details>
  <summary>Details</summary>
Motivation: To assess the impact of tungsten oxidation and oxide sublimation on fusion power plant lifecycle, particularly for maintenance and waste-handling scenarios.

Method: Oxidized pure W at 400-1050°C for 1-70 hours, characterized using electron microscopy and Raman spectroscopy to analyze oxide layer structure.

Result: Found void/crack formation above 600°C leading to dust/spalling; identified three-layer oxide structure: inner WO2 (30-50nm), middle WO2.72 (10-20μm), outer WO2.9/WO3 phases.

Conclusion: Oxidation-induced microstructure affects parabolic-to-linear kinetics and potentially impacts tritium permeation and detritiation efficiency in fusion components.

Abstract: The oxidation of pure W and the sublimation of W oxide have been investigated
to assess their impact on the lifecycle of a fusion power plant. Pure W has
been oxidised at temperatures between 400 and 1050C and for durations ranging
between 1 and 70 h. The formation of voids and cracks has been observed at
temperatures above 600C, leading to the formation of dust or oxide spalling,
which could be problematic in maintenance and waste-handling scenarios of a
fusion power plant. Preferential oxidation taking place at the edge of the
specimen was characterised, and its impact is discussed in relation to
component design. Characterisation using electron microscopy and Raman
spectroscopy revealed that the oxide scale is formed of three main layers: the
inner layer is 30-50 nm thick WO2 oxide, the middle layer is a 10-20 um of
WO_2.72 and the outer layer is formed of WO2.9/WO3 phases - whose thickness
varies according to the total thickness of the oxide scale. The observed
microstructure is discussed in relation to the parabolic-to-linear kinetics and
its potential impact on tritium permeation and detritiation efficiency.

</details>


### [44] [Tunable Transmissive Metagratings Using Single Layer Cylindrical Plasma Discharges](https://arxiv.org/abs/2509.22172)
*Mohammad G. H. Alijani,Alessio Monti,Stefano Vellucci,Mirko Barbuto,Alessandro Toscano,Filiberto Bilotti*

Main category: physics.app-ph

TL;DR: A reconfigurable transmissive metagrating using plasma discharges achieves beam steering from -41° to 41° with high transmission efficiency above 80-90%.


<details>
  <summary>Details</summary>
Motivation: To develop a tunable metagrating that enables effective directional control of electromagnetic waves with high efficiency and low reflection.

Method: Single-layer metagrating with unit-cells containing two side-by-side core-shell cylinders (tunable plasma core, high-index dielectric shell), modeled using free-electron plasma permittivity with adjustable plasma frequency.

Result: Main transmission lobe can be switched between -41°, 0° and 41° by tuning plasma frequencies, with transmission efficiency above 80% at broadside and 90% in steered direction.

Conclusion: The proposed plasma-based metagrating provides effective directional control with low reflection and high power efficiency through tunable plasma frequency adjustment.

Abstract: In this paper, we propose a novel single-layer reconfigurable transmissive
metagrating based on plasma discharges. Each unit-cell consists of two
side-by-side core-shell cylinders, with a tunable plasma core and a high-index
dielectric shell. The structure is modeled using a free-electron plasma
permittivity with adjustable plasma frequency. Analytical and numerical results
show that the main transmission lobe can be switched between -41{\deg}, 0{\deg}
and 41{\deg} by tuning the plasma frequencies. Transmission efficiency remains
above 80% at broadside and 90% in the steered direction. This tunability
enables effective directional control with low reflection and high overall
power efficiency.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Shoot from the HIP: Hessian Interatomic Potentials without derivatives](https://arxiv.org/abs/2509.21624)
*Andreas Burger,Luca Thiede,Nikolaj Rønne,Varinia Bernales,Nandita Vijaykumar,Tejs Vegge,Arghya Bhowmik,Alan Aspuru-Guzik*

Main category: cs.LG

TL;DR: HIP is a deep learning model that directly predicts molecular Hessians (second derivatives of potential energy) without using automatic differentiation or finite differences, achieving significant speed and accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Molecular Hessians are computationally expensive to calculate with traditional quantum mechanical methods and neural networks, and they scale poorly with system size, limiting their practical use in computational chemistry tasks.

Method: Construct SE(3)-equivariant, symmetric Hessians from irreducible representation (irrep) features up to degree l=2 computed during message passing in graph neural networks, enabling direct prediction without differentiation.

Result: HIP Hessians are one to two orders of magnitude faster, more accurate, more memory efficient, easier to train, and enable more favorable scaling with system size compared to existing methods.

Conclusion: The direct prediction approach for Hessians demonstrates superior performance across various computational chemistry tasks including transition state search, geometry optimization, zero-point energy corrections, and vibrational analysis.

Abstract: Fundamental tasks in computational chemistry, from transition state search to
vibrational analysis, rely on molecular Hessians, which are the second
derivatives of the potential energy. Yet, Hessians are computationally
expensive to calculate and scale poorly with system size, with both quantum
mechanical methods and neural networks. In this work, we demonstrate that
Hessians can be predicted directly from a deep learning model, without relying
on automatic differentiation or finite differences. We observe that one can
construct SE(3)-equivariant, symmetric Hessians from irreducible
representations (irrep) features up to degree $l$=2 computed during message
passing in graph neural networks. This makes HIP Hessians one to two orders of
magnitude faster, more accurate, more memory efficient, easier to train, and
enables more favorable scaling with system size. We validate our predictions
across a wide range of downstream tasks, demonstrating consistently superior
performance for transition state search, accelerated geometry optimization,
zero-point energy corrections, and vibrational analysis benchmarks. We
open-source the HIP codebase and model weights to enable further development of
the direct prediction of Hessians at https://github.com/BurgerAndreas/hip

</details>


### [46] [Reparameterizing 4DVAR with neural fields](https://arxiv.org/abs/2509.21751)
*Jaemin Oh*

Main category: cs.LG

TL;DR: Neural field-based 4DVAR reformulation enables parallel-in-time optimization for weather prediction, removing sequential dependencies and incorporating physics constraints through neural networks.


<details>
  <summary>Details</summary>
Motivation: Classical 4DVAR is computationally intensive and difficult to optimize due to time-sequential dependencies, requiring a more efficient approach.

Method: Represent spatiotemporal state as continuous function parameterized by neural network, enabling parallel optimization with physics-informed loss constraints.

Result: Produces more stable initial condition estimates without spurious oscillations compared to baseline 4DVAR, tested on 2D Navier-Stokes equations.

Conclusion: Neural reparameterization framework works without ground-truth data, broadening applicability to settings with limited reference information.

Abstract: Four-dimensional variational data assimilation (4DVAR) is a cornerstone of
numerical weather prediction, but its cost function is difficult to optimize
and computationally intensive. We propose a neural field-based reformulation in
which the full spatiotemporal state is represented as a continuous function
parameterized by a neural network. This reparameterization removes the
time-sequential dependency of classical 4DVAR, enabling parallel-in-time
optimization in parameter space. Physical constraints are incorporated directly
through a physics-informed loss, simplifying implementation and reducing
computational cost. We evaluate the method on the two-dimensional
incompressible Navier--Stokes equations with Kolmogorov forcing. Compared to a
baseline 4DVAR implementation, the neural reparameterized variants produce more
stable initial condition estimates without spurious oscillations. Notably,
unlike most machine learning-based approaches, our framework does not require
access to ground-truth states or reanalysis data, broadening its applicability
to settings with limited reference information.

</details>


### [47] [GenUQ: Predictive Uncertainty Estimates via Generative Hyper-Networks](https://arxiv.org/abs/2509.21605)
*Tian Yu Yen,Reese E. Jones,Ravi G. Patel*

Main category: cs.LG

TL;DR: GenUQ introduces a measure-theoretic approach to uncertainty quantification for operator learning that uses generative hyper-networks to produce parameter distributions without requiring likelihood construction, outperforming other UQ methods in three test problems.


<details>
  <summary>Details</summary>
Motivation: Traditional likelihood-based UQ methods for operator learning are limited when stochastic operators produce actions where likelihoods are difficult or impossible to construct, creating a need for alternative UQ approaches.

Method: GenUQ uses a generative hyper-network model that produces parameter distributions consistent with observed data, avoiding the need for likelihood construction through a measure-theoretic approach to uncertainty quantification.

Result: GenUQ outperforms other UQ methods in three example problems: recovering a manufactured operator, learning the solution operator to a stochastic elliptic PDE, and modeling failure location of porous steel under tension.

Conclusion: The measure-theoretic generative approach provides an effective alternative to likelihood-based UQ methods for operator learning, enabling uncertainty quantification in cases where traditional methods fail.

Abstract: Operator learning is a recently developed generalization of regression to
mappings between functions. It promises to drastically reduce expensive
numerical integration of PDEs to fast evaluations of mappings between
functional states of a system, i.e., surrogate and reduced-order modeling.
Operator learning has already found applications in several areas such as
modeling sea ice, combustion, and atmospheric physics. Recent approaches
towards integrating uncertainty quantification into the operator models have
relied on likelihood based methods to infer parameter distributions from noisy
data. However, stochastic operators may yield actions from which a likelihood
is difficult or impossible to construct. In this paper, we introduce, GenUQ, a
measure-theoretic approach to UQ that avoids constructing a likelihood by
introducing a generative hyper-network model that produces parameter
distributions consistent with observed data. We demonstrate that GenUQ
outperforms other UQ methods in three example problems, recovering a
manufactured operator, learning the solution operator to a stochastic elliptic
PDE, and modeling the failure location of porous steel under tension.

</details>


### [48] [Fast-Forward Lattice Boltzmann: Learning Kinetic Behaviour with Physics-Informed Neural Operators](https://arxiv.org/abs/2509.22411)
*Xiao Xue,Marco F. P. ten Eikelder,Mingyang Gao,Xiaoyuan Cheng,Yiming Yang,Yi He,Shuo Wang,Sibo Cheng,Yukun Hu,Peter V. Coveney*

Main category: cs.LG

TL;DR: A physics-informed neural operator framework for the lattice Boltzmann equation that enables long-term prediction without step-by-step integration, bypassing computational bottlenecks of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional lattice Boltzmann equation (LBE) solving is computationally intensive due to strict time-step restrictions from collision kernels, limiting practical applications.

Method: Physics-informed neural operator framework incorporating moment-matching constraints and global equivariance, enabling discretization-invariant predictions and kinetic super-resolution without explicit collision kernel solving.

Result: Demonstrated robustness across complex flow scenarios including von Karman vortex shedding, ligament breakup, and bubble adhesion, with models trained on coarse lattices generalizing to finer ones.

Conclusion: Establishes a new data-driven pathway for modeling kinetic systems that is collision model-agnostic and overcomes computational limitations of traditional LBE methods.

Abstract: The lattice Boltzmann equation (LBE), rooted in kinetic theory, provides a
powerful framework for capturing complex flow behaviour by describing the
evolution of single-particle distribution functions (PDFs). Despite its
success, solving the LBE numerically remains computationally intensive due to
strict time-step restrictions imposed by collision kernels. Here, we introduce
a physics-informed neural operator framework for the LBE that enables
prediction over large time horizons without step-by-step integration,
effectively bypassing the need to explicitly solve the collision kernel. We
incorporate intrinsic moment-matching constraints of the LBE, along with global
equivariance of the full distribution field, enabling the model to capture the
complex dynamics of the underlying kinetic system. Our framework is
discretization-invariant, enabling models trained on coarse lattices to
generalise to finer ones (kinetic super-resolution). In addition, it is
agnostic to the specific form of the underlying collision model, which makes it
naturally applicable across different kinetic datasets regardless of the
governing dynamics. Our results demonstrate robustness across complex flow
scenarios, including von Karman vortex shedding, ligament breakup, and bubble
adhesion. This establishes a new data-driven pathway for modelling kinetic
systems.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [49] [Kolmogorov equations for stochastic Volterra processes with singular kernels](https://arxiv.org/abs/2509.21608)
*Ioannis Gasteratos,Alexandre Pannier*

Main category: math.PR

TL;DR: The paper establishes connections between fully nonlinear Stochastic Volterra Equations (SVEs) with singular kernels and Markovian SPDEs, develops novel Itô formulas, and proves that SVE solutions solve Fokker-Planck equations.


<details>
  <summary>Details</summary>
Motivation: To rigorously link solutions of SVEs with singular convolution kernels to Markovian SPDEs and develop stochastic calculus tools for analyzing their probabilistic properties.

Method: Using stochastic calculus in Hilbert spaces, reproducing kernel properties, and establishing invariance/smoothing properties for transport-type SPDEs on a carefully chosen Hilbert space H₁.

Result: Proved two novel Itô formulas, showed SVE solution laws solve Fokker-Planck equations, and established backward Kolmogorov equations for conditional expectations. For power-law kernels, obtained well-posedness for all H with additive noise and H>1/4 with multiplicative noise.

Conclusion: The analysis provides a comprehensive framework connecting SVEs with singular kernels to Markovian SPDEs, enabling probabilistic analysis through backward/forward Kolmogorov equations and novel stochastic calculus tools.

Abstract: We associate backward and forward Kolmogorov equations to a class of fully
nonlinear Stochastic Volterra Equations (SVEs) with convolution kernels $K$
that are singular at the origin. Working on a carefully chosen Hilbert space
$\mathcal{H}_1$, we rigorously establish a link between solutions of SVEs and
Markovian mild solutions of a Stochastic Partial Differential Equation (SPDE)
of transport-type. Then, we obtain two novel It\^o formulae for functionals of
mild solutions and, as a byproduct, show that their laws solve corresponding
Fokker-Planck equations. Finally, we introduce a natural notion of "singular"
directional derivatives along $K$ and prove that (conditional) expectations of
SVE solutions can be expressed in terms of the unique solution to a backward
Kolmogorov equation on $\mathcal{H}_1$. Our analysis relies on stochastic
calculus in Hilbert spaces, the reproducing kernel property of the state space
$\mathcal{H}_1,$ as well as crucial invariance and smoothing properties that
are specific to the SPDEs of interest. In the special case of singular
power-law kernels, our conditions guarantee well-posedness of the backward
equation either for all values of the Hurst parameter $H,$ when the noise is
additive, or for all $H>1/4$ when the noise is multiplicative.

</details>
