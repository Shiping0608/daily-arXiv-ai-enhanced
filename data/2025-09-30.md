<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 29]
- [math.AP](#math.AP) [Total: 41]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 5]
- [math.DS](#math.DS) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [physics.optics](#physics.optics) [Total: 2]
- [math-ph](#math-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [math.PR](#math.PR) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [stat.ML](#stat.ML) [Total: 2]
- [cs.NI](#cs.NI) [Total: 3]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 2]
- [math.DG](#math.DG) [Total: 5]
- [math.SP](#math.SP) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [An unfitted HDG discretization for a model problem in shape optimization](https://arxiv.org/abs/2509.22724)
*Esteban Henríquez,Tonatiuh Sánchez-Vizuet,Manuel Solano*

Main category: math.NA

TL;DR: An unfitted HDG discretization method for shape optimization that uses fixed non-geometry conforming meshes with high-order transfer to handle curved boundaries, avoiding remeshing while achieving high accuracy with coarse meshes.


<details>
  <summary>Details</summary>
Motivation: To develop a shape optimization method that avoids constant remeshing during optimization while maintaining high accuracy in boundary description, particularly for curved boundaries that arise during optimization.

Method: Unfitted HDG discretization with fixed shape-regular non-geometry conforming meshes and high-order transfer technique for curved boundaries. Includes rigorous analysis of well-posedness and a priori error analysis.

Result: Method achieves high-resolution boundary approximations with grids containing up to 100 times fewer elements than interpolatory techniques. Numerical examples demonstrate convergence and efficiency of transfer path method.

Conclusion: The proposed approach successfully enables accurate shape optimization without remeshing, using coarse computational meshes while maintaining high boundary resolution through the transfer technique.

Abstract: We apply an unfitted HDG discretization to a model problem in shape
optimization. The method proposed uses a fixed, shape regular, non-geometry
conforming mesh and a high order transfer technique to deal with the curved
boundaries arising in the optimization process. The use of this strategy avoids
the need for constant remeshing and enables a highly accurate description of
the domain using a coarse computational mesh. We develop a rigorous analysis of
the well-posedness of the problems that arise from the optimality conditions,
and provide an a priori error analysis for the resulting discrete schemes.
Numerical examples with manufactured problems are provided demonstrating the
convergence of the scheme and the efficiency of the transfer path method. The
approach proposed yields high resolution approximations of the boundary using
grids with as few as 100 times less elements than an interpolatory technique.

</details>


### [2] [Accelerating High-Fidelity Fixed Point Schemes with On-the-fly Reduced Order Modeling](https://arxiv.org/abs/2509.22846)
*Philippe-André Luneau,Jean Deteix*

Main category: math.NA

TL;DR: A method for accelerating fixed point schemes for PDEs using on-the-fly reduced-order modeling without offline training or precomputed basis.


<details>
  <summary>Details</summary>
Motivation: To accelerate fixed point schemes for PDE-related problems while maintaining high fidelity, avoiding the need for offline training phases and precomputed reduced bases.

Method: Train a reduced-order model on-the-fly with an error criterion based on error propagation, allowing the surrogate model to adapt during iterations for general fixed point problems with complex dependence structures.

Result: Significant speedups achieved while maintaining high fidelity compared to reference solutions.

Conclusion: The proposed algorithm effectively accelerates fixed point schemes for PDEs through adaptive on-the-fly reduced-order modeling with proven convergence guarantees.

Abstract: A general method for accelerating fixed point schemes for problems related to
partial differential equations is presented in this article. The speedup is
obtained by training a reduced-order model on-the-fly, removing the need to do
an offline training phase and any dependence to a precomputed reduced basis
(e.g. a fixed geometry or mesh). The surrogate model can adapt itself along the
iterations because of an error criterion based on error propagation, ensuring
the high fidelity of the converged result. Convergence results are given for a
general class of fixed point problems with complex dependence structures
between multiple auxiliary linear systems. The proposed algorithm is applied to
the solution of a system of coupled partial differential equations. The
speedups obtained are significant, and the output of the method can be
considered high-fidelity when compared to the reference solution.

</details>


### [3] [Mathematical and numerical modeling of coupled oxygen dynamics and neuronal electrophysiology](https://arxiv.org/abs/2509.22863)
*Francesco Daniele,Caterina B. Leimer Saglio,Stefano Pagani,Paola F. Antonietti*

Main category: math.NA

TL;DR: This paper develops a multiscale computational model to study how oxygen deprivation (ischemia) affects neuronal excitability through cell swelling and impaired transport mechanisms.


<details>
  <summary>Details</summary>
Motivation: Understanding how reduced oxygen supply shapes neuronal excitability is crucial for explaining brain function in pathological conditions like ischemia, which impairs ionic pumps and causes cellular swelling.

Method: A monodomain model coupled with neuronal ionic and metabolic models, using a volumetric variation law that links cell swelling to local oxygen concentration and blood flow reduction. Numerical implementation uses p-adaptive discontinuous Galerkin method on polygonal grids with Crank-Nicolson time integration.

Result: The model successfully captures how subclinical and severe ischemia affect brain electrophysiology and metabolic concentrations through tortuosity-driven transport impairment caused by cellular swelling.

Conclusion: The developed multiscale model provides a computational framework to study the interplay between oxygen supply and neuronal excitability in ischemic conditions, offering insights into pathological brain function.

Abstract: Understanding how oxygen supply shapes neuronal excitability is crucial for
explaining brain function in pathological scenarios, such as ischemia. This
condition is caused by a reduced blood supply, leading to oxygen and other
metabolites deprivation; this energy deficit impairs ionic pumps and causes
cellular swelling. In this work, this phenomenon is modeled through a
volumetric variation law that links cell swelling to local oxygen concentration
and the percentage of blood flow reduction. The swelling law ties volume
changes to local oxygen and the degree of blood-flow depression, providing a
simple pathway from hypoxia to tortuosity-driven transport impairment. To study
the interplay between oxygen supply and excitability in brain tissue, we employ
a monodomain model coupled with specific neuronal ionic and metabolic models
that characterize ion and metabolite concentration dynamics. From a numerical
point of view, suitable space- and time-adaptive schemes are employed to
capture the sharp and fast-traveling wavefronts representing action potentials
more accurately. This multiscale model is discretized in space with the
high-order p-adaptive discontinuous Galerkin method on polygonal and polyhedral
grids (PolyDG) and integrated in time with a Crank-Nicolson scheme. We
numerically investigate different pathological scenarios on a two-dimensional,
idealized square domain discretized with a polygonal grid, analyzing how
subclinical and severe ischemia can affect brain electrophysiology and
metabolic concentrations.

</details>


### [4] [Superconvergence of High-order Magnus Quantum Algorithms](https://arxiv.org/abs/2509.22897)
*Di Fang,Jiaqi Zhang*

Main category: math.NA

TL;DR: Quantum Magnus algorithms for Hamiltonian simulation exhibit superconvergence: order p methods achieve order 2p convergence when applied to Schrödinger equation simulation in the interaction picture.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous mathematical proof that the observed superconvergence behavior in quantum Magnus algorithms extends to general high-order methods, establishing the theoretical foundation for these quantum simulation techniques.

Method: Combines techniques from semiclassical analysis and Weyl calculus to analyze quantum Magnus algorithms applied to time-dependent Hamiltonian simulation in the interaction picture.

Result: Proved that a quantum Magnus algorithm of order p achieves superconvergence of order 2p when simulating the Schrödinger equation in the interaction picture.

Conclusion: The work establishes rigorous mathematical foundations for quantum Magnus algorithms, demonstrating their superior convergence properties and offering new analytical perspectives for quantum time-dependent Hamiltonian simulation.

Abstract: The Magnus expansion has long been a celebrated subject in numerical
analysis, leading to the development of many useful classical integrators. More
recently, it has been discovered to be a powerful tool for designing quantum
algorithms for Hamiltonian simulation in quantum computing. In particular,
surprising superconvergence behavior has been observed for quantum Magnus
algorithms applied to the simulation of the Schr\"odinger equation, with the
first- and second-order methods exhibiting doubled convergence order. In this
work, we provide a rigorous proof that such superconvergence extends to general
high-order quantum Magnus algorithms. Specifically, we show that a quantum
Magnus algorithm of order $p$ achieves the superconvergence of order $2p$ in
time when applying to the Schr\"odinger equation simulation in the interaction
picture. Our analysis combines techniques from semiclassical analysis and Weyl
calculus, offering a new perspective on the mathematical foundations of quantum
algorithms for time-dependent Hamiltonian simulation.

</details>


### [5] [A Virtual Element Method on polyhedra with curved faces](https://arxiv.org/abs/2509.23005)
*Daniele Prada,Franco Brezzi,L. Donatella Marini*

Main category: math.NA

TL;DR: Construction of conforming Virtual Element Method (VEM) approximations for domains with curved boundaries/interfaces in 2D and 3D, supporting Dirichlet and Neumann boundary conditions with optimal convergence rates.


<details>
  <summary>Details</summary>
Motivation: To develop VEM approximations that can handle curved boundaries and interfaces in both 2D and 3D domains, while supporting non-homogeneous boundary conditions and maintaining optimal convergence properties.

Method: Virtual Element Method with local spaces of degree k that satisfy the patch test when exact solution is a polynomial of degree k. The approach handles curved boundaries and interfaces through conforming approximations.

Result: Achieves optimal convergence rates for degree k ≥ 1. The method is theoretically analyzed for 2D case and numerically validated for both 2D and 3D domains.

Conclusion: The proposed VEM approach successfully handles curved boundaries and interfaces while maintaining optimal convergence properties and satisfying the patch test for polynomial solutions.

Abstract: In this paper we construct conforming Virtual Element approximations on
domains with curved boundary and/or internal curved interfaces, both in two and
three dimensions. Our approach allows to impose both Dirichlet and Neumann
non-homogeneous boundary conditions, and provides, for degree of accuracy $k
\geq 1$, optimal convergence rates. Whenever the exact solution is a polynomial
of degree $k$, local spaces of degree $k$ ensure satisfaction of the patch
test. The proposed method is theoretically analyzed in the two-dimensional
case, whereas it is numerically validated both in two and three dimensions.

</details>


### [6] [Dynamical feedback control with operator learning for the Vlasov-Poisson system](https://arxiv.org/abs/2509.23063)
*Jingcheng Lu,Li Wang,Jeff Calder*

Main category: math.NA

TL;DR: A dynamic feedback control strategy for the Vlasov-Poisson system using neural operators and cancellation-based methods to suppress plasma instabilities over long time horizons.


<details>
  <summary>Details</summary>
Motivation: To address the need for instantaneous control of instabilities in plasma fusion over extended time periods.

Method: Two approaches: 1) Learning control operators using neural networks with low-rank architecture trained via adjoint state method; 2) A novel cancellation-based strategy that removes destabilizing electric field components without training.

Result: The controller effectively suppresses instabilities beyond training horizons, ensures perturbation decay over infinite time, and demonstrates strong robustness under noisy feedback in numerical experiments.

Conclusion: The proposed methods provide effective control strategies for plasma fusion instabilities, with both neural operator and cancellation-based approaches showing promising results in one- and multidimensional settings.

Abstract: To meet the demands of instantaneous control of instabilities over long time
horizons in plasma fusion, we design a dynamic feedback control strategy for
the Vlasov-Poisson system by constructing an operator that maps state
perturbations to an external control field. In the first part of the paper, we
propose learning such an operator using a neural network. Inspired by optimal
control theory for linearized dynamics, we introduce a low-rank neural operator
architecture and train it via adjoint state method. The resulting controller is
effective at suppressing instabilities well beyond the training time horizon.
To generalize control across varying initial data, we further introduce a novel
cancellation-based control strategy that removes the destabilizing component of
the electric field. This approach naturally defines an operator without
requiring any training, ensures perturbation decay over infinite time, and
demonstrates strong robustness under noisy feedback. Numerical experiments
confirm the effectiveness of the method in both one- and multidimensional
settings.

</details>


### [7] [A FFT-based GMRES for fast solving of Poisson equation in concatenated geometry](https://arxiv.org/abs/2509.23180)
*Zichao Jiang,Jiacheng Lian,Zhuolin Wang*

Main category: math.NA

TL;DR: A novel domain decomposition method extends FFT-based Poisson solvers to complex geometries using Schur complement coupling and FFT-based preconditioning for GMRES, achieving 40x speedup.


<details>
  <summary>Details</summary>
Motivation: FFT-based Poisson solvers are efficient but limited to simple geometries due to separability requirements. This work aims to extend their applicability to complex composite domains.

Method: Domain decomposition with Schur complement coupling at interfaces, using FFT-based preconditioner for GMRES that approximates the inverse of the block operator without Schur complement.

Result: Achieves second-order accuracy on cross-shaped domains and 40x speedup over sparse GMRES for 10^5 grid points while maintaining optimal complexity per iteration.

Conclusion: The method successfully extends FFT-based solvers to complex geometries with significant performance improvements while preserving numerical accuracy.

Abstract: Fast Fourier Transform (FFT)-based solvers for the Poisson equation are
highly efficient, exhibiting $O(N\log N)$ computational complexity and
excellent parallelism. However, their application is typically restricted to
simple, regular geometries due to the separability requirement of the
underlying discrete operators. This paper introduces a novel domain
decomposition method that extends the applicability of FFT-based solvers to
complex composite domains geometries constructed from multiple sub-regions. The
method transforms the global problem into a system of sub-problems coupled
through Schur complements at the interfaces. A key challenge is that the Schur
complement disrupts the matrix structure required for direct FFT-based
inversion. To overcome this, we develop a FFT-based preconditioner to
accelerate the Generalized Minimal Residual (GMRES) method for the interface
system. The central innovation is a novel preconditioner based on the inverse
of the block operator without the Schur complement, which can be applied
efficiently using the FFT-based solver. The resulting preconditioned iteration
retains an optimal complexity for each step. Numerical experiments on a
cross-shaped domain demonstrate that the proposed solver achieves the expected
second-order accuracy of the underlying finite difference scheme. Furthermore,
it exhibits significantly improved computational performance compared to a
classic sparse GMRES solver based on Eigen libeary. For a problem with $10^5$
grid points, our method achieves a speedup of over 40 times.

</details>


### [8] [On the rotating nonlinear Klein-Gordon equation with multiscale effects: structure-preserving methods and applications to vortex dynamics](https://arxiv.org/abs/2509.23191)
*Meng Li,Chunyan Niu,Huifei Wang,Junjun Wang*

Main category: math.NA

TL;DR: Development of structure-preserving Galerkin finite element methods for the rotating nonlinear Klein-Gordon equation, addressing conservation challenges with rotational terms through conservation-adjusting techniques.


<details>
  <summary>Details</summary>
Motivation: The RKG equation models rotating galaxies and cosmic superfluids with highly oscillatory multiscale behavior, but rotational terms prevent traditional FEMs from conserving both energy and charge simultaneously.

Method: Structure-preserving Galerkin finite element methods using conservation-adjusting technique for both conforming and nonconforming FEMs, with rigorous convergence analysis.

Result: Unconditional optimal and high-order accuracy error estimates established, validated by numerical experiments showing accuracy, efficiency, and robustness. Simulations demonstrate vortex dynamics across relativistic regimes.

Conclusion: The proposed structure-preserving schemes successfully handle rotational challenges in RKG equations, enabling accurate simulation of vortex creation, relativistic effects, and vortex interactions.

Abstract: We study numerical methods for the rotating nonlinear Klein-Gordon (RKG)
equation, a fundamental model in relativistic quantum physics, which exhibits
highly oscillatory multiscale behavior due to the presence of a small parameter
{\epsilon}. The RKG equation models rotating galaxies under the Minkowski
metric and also provides an effective description of phenomena such as cosmic
superfluids. This work focuses on the development and rigorous analysis of
structure-preserving Galerkin finite element methods (FEMs) for the RKG
equation. A central challenge is that the rotational terms prevent traditional
nonconforming FEMs from simultaneously conserving energy and charge. By
employing a conservation-adjusting technique, we construct a consistent
structure-preserving algorithm applicable to both conforming and nonconforming
FEMs. Moreover, we provide a comprehensive convergence analysis, establishing
unconditional optimal and high-order accuracy error estimates. These
theoretical results are further validated through extensive numerical
experiments, which demonstrate the accuracy, efficiency, and robustness of the
structure-preserving schemes. Finally, simulations of vortex dynamics, ranging
from the relativistic to the nonrelativistic regimes, are presented to
illustrate vortex creation, relativistic effects on bound states, and
interactions of vortex pairs.

</details>


### [9] [A Besov-based integration-by-parts method for the incompressible Navier-Stokes equations](https://arxiv.org/abs/2509.23192)
*Xinyu Cheng,Zhaonan Luo,Sheng Wang*

Main category: math.NA

TL;DR: This paper presents a novel numerical analysis framework for incompressible Navier-Stokes equations using Besov spaces, establishing stability and convergence of a semi-implicit time-stepping scheme with precise error estimates in B^0_∞,1 and B^0_∞,2 spaces.


<details>
  <summary>Details</summary>
Motivation: To develop sharper, more localized error bounds for Navier-Stokes equations than classical Sobolev spaces provide, leveraging the advantages of Besov spaces for nonlinear fluid PDE analysis.

Method: Uses a semi-implicit time-stepping scheme with detailed analysis in Besov spaces B^0_∞,1 and B^0_∞,2, employing an integration-by-parts technique to handle the nonlinear advection term by transferring derivatives onto test functions.

Result: Established stability and convergence of the scheme with refined error estimates that directly link convergence to the critical regularity of the continuous solution, providing better bounds than classical Sobolev space approaches.

Conclusion: Besov spaces offer significant advantages for numerical analysis of nonlinear fluid PDEs, enabling sharper error estimates and more effective handling of low regularity spaces in Navier-Stokes equations.

Abstract: This note introduces a novel numerical analysis framework for the
incompressible Navier-Stokes equations based on Besov spaces. The key
contribution of this note is to establish the stability and convergence of a
semi-implicit time-stepping scheme by deriving precise error estimates in the
$B^0_{\infty,1}$ and $B^0_{\infty,2}$ spaces. Another contribution of our
analysis is the detailed treatment of the $B^0_{\infty,2}$ case, where a
crucial integration-by-parts technique is employed to adeptly handle the
nonlinear advection term. This technique allows for a refined estimate that
effectively transfers derivatives onto the test functions, mitigating the
inherent analytical challenges posed by the low regularity of these spaces. Our
results provide sharper, more localized error bounds than in classical Sobolev
spaces, directly linking the scheme's convergence to the critical regularity of
the continuous solution. This work underscores the advantage of Besov spaces
for the numerical analysis of nonlinear fluid PDEs.

</details>


### [10] [An Accelerated Newton-GMRES Method for Multilinear PageRank](https://arxiv.org/abs/2509.23374)
*Maryam Boubekraoui,Ridwane Tahiri*

Main category: math.NA

TL;DR: Accelerated Newton-GMRES method with vector extrapolation for efficient multilinear PageRank computation in large-scale networks.


<details>
  <summary>Details</summary>
Motivation: Traditional Newton methods for multilinear PageRank are too computationally expensive for large-scale applications due to solving large linear systems at each iteration.

Method: Combines Newton-GMRES with Krylov subspace techniques to approximate Newton steps without forming large Jacobian matrices, and uses vector extrapolation methods (MPE, RRE, AA) to improve convergence.

Result: Significantly outperforms classical Newton-based solvers in efficiency, robustness, and scalability on synthetic and real-world data.

Conclusion: The proposed accelerated Newton-GMRES with vector extrapolation provides an effective solution for large-scale multilinear PageRank problems.

Abstract: Modeling complex multiway relationships in large-scale networks is becoming
more and more challenging in data science. The multilinear PageRank problem,
arising naturally in the study of higher-order Markov chains, is a powerful
framework for capturing such interactions, with applications in web ranking,
recommendation systems, and social network analysis. It extends the classical
Google PageRank model to a tensor-based formulation, leading to a nonlinear
system that captures multi-way dependencies between states. Newton-based
methods can achieve local quadratic convergence for this problem, but they
require solving a large linear system at each iteration, which becomes too
costly for large-scale applications. To address this challenge, we present an
accelerated Newton-GMRES method that leverages Krylov subspace techniques to
approximate the Newton step without explicitly forming the large Jacobian
matrix. We further employ vector extrapolation methods, including Minimal
Polynomial Extrapolation (MPE), Reduced Rank Extrapolation (RRE), and Anderson
Acceleration (AA), to improve the convergence rate and enhance numerical
stability. Extensive experiments on synthetic and real-world data demonstrate
that the proposed approach significantly outperforms classical Newton-based
solvers in terms of efficiency, robustness, and scalability.

</details>


### [11] [Penalized Weighted Trace Minimization for Optimal Control Device Design and Placement](https://arxiv.org/abs/2509.23477)
*James Cheung*

Main category: math.NA

TL;DR: New analytical framework for well-posedness of constrained optimization in infinite-dimensional linear quadratic control systems, focusing on optimal control device design and placement.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for determining when constrained optimization problems in optimal control device design have unique solutions, particularly in infinite-dimensional settings.

Method: Proves well-posedness of the 'strong form' of time-independent operator-valued Riccati equation, then uses trace-class operator analysis and Lagrange multiplier formalism to analyze operator-valued Riccati equation-constrained optimization problems.

Result: Establishes conditions under which unique solutions exist for two important classes of penalized trace minimization problems related to optimal control device placement and design.

Conclusion: The framework provides fundamental mathematical tools to ensure well-posedness and existence of unique solutions in optimal control device design problems within infinite-dimensional linear quadratic control systems.

Abstract: In this paper, we present a new analytical framework for determining the
well-posedness of constrained optimization problems that arise in the study of
optimal control device design and placement within the context of infinite
dimensional linear quadratic control systems. We first prove the well-posedness
of the newly minted "strong form" of the time-independent operator-valued
Riccati equation. This form of the equation then enables the use of trace-class
operator analysis and the Lagrange multiplier formalism to analyze
operator-valued Riccati equation-constrained optimization problems. Using this
fundamental result, we then determine the conditions under which there exists
unique solutions to two important classes of penalized trace minimization
problems for optimal control device placement and design.

</details>


### [12] [Energy minimization for skyrmions on planar thin films](https://arxiv.org/abs/2509.23495)
*Giovanni Di Fratta,Michael Innerberger,Dirk Praetorius,Valeriy Slastikov*

Main category: math.NA

TL;DR: The paper develops algorithms for minimizing an energy functional in micromagnetic and liquid crystal thin films, featuring non-convex anti-symmetric exchange and anisotropy terms, with proven weak convergence and empirical validation.


<details>
  <summary>Details</summary>
Motivation: To address energy minimization problems in micromagnetic and liquid crystal theory on thin films, particularly those involving non-convex anti-symmetric exchange and anisotropy terms.

Method: Devised an algorithm for continuous energy minimization with weak convergence proof, and presented a numerical algorithm for energy minimization with rigorous convergence analysis for special cases.

Result: Showed weak convergence of subsequences to Euler-Lagrange equation solutions, and empirically demonstrated that the numerical algorithm converges to correct solutions without user parameters.

Conclusion: The proposed algorithms effectively solve the energy minimization problem with theoretical guarantees and practical performance, validated through benchmark problems.

Abstract: We consider an energy functional that arises in micromagnetic and liquid
crystal theory on thin films. In particular, our energy comprises a non-convex
term that models anti-symmetric exchange as well as an anisotropy term. We
devise an algorithm for energy minimization in the continuous case and show
weak convergence of a subsequence towards a solution of the corresponding
Euler--Lagrange equation. Furthermore, an algorithm for numerical energy
minimization is presented. We show empirically that this numerical algorithm
converges to the correct solutions for a benchmark problem without the need for
user-supplied parameters, and present a rigorous convergence analysis for
important special cases.

</details>


### [13] [Multi-order Runge-Kutta methods or how to numerically solve initial value problems of any order](https://arxiv.org/abs/2509.23513)
*Petronijevic Loris*

Main category: math.NA

TL;DR: The paper introduces multi-order Runge-Kutta methods that directly solve higher-order initial value problems without rewriting them as first-order systems, achieving greater accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional methods rewrite higher-order initial value problems as first-order systems, but direct treatment allows for significantly greater accuracy.

Method: Introduces multi-order Runge-Kutta methods designed to solve initial value problems of arbitrary order directly.

Result: Establishes fundamental properties including convergence, order of consistency, and linear stability. Analyzes the structure of the approximation system to define explicit methods and understand implicit methods.

Conclusion: Multi-order Runge-Kutta methods provide a more accurate alternative to traditional approaches by directly solving higher-order problems without system rewriting.

Abstract: When one wishes to numerically solve an initial value problem, it is
customary to rewrite it as an equivalent first-order system to which a method,
usually from the class of Runge-Kutta methods, is applied. Directly treating
higher-order initial value problems without such rewriting, however, allows for
significantly greater accuracy. We therefore introduce a new generalization of
Runge-Kutta methods, called multi-order Runge-Kutta methods, designed to solve
initial value problems of arbitrary order. We establish fundamental properties
of these methods, including convergence, order of consistency, and linear
stability. We also analyze the structure of the system satisfied by the
approximations of a method, which enables us to provide a proper definition of
explicit methods and to gain a finer understanding of implicit methods.

</details>


### [14] [Asymptotic Error Analysis of Gauss Quadrature for Nonsmooth Functions](https://arxiv.org/abs/2509.23532)
*Pei Liu*

Main category: math.NA

TL;DR: Derives an asymptotic error formula for Gauss-Legendre quadrature on functions with limited regularity, identifying optimal convergence rates and leading coefficients to enable error minimization and acceleration.


<details>
  <summary>Details</summary>
Motivation: To address the lack of uniformly valid approximations for Legendre functions near [-1,1] and provide precise error analysis for quadrature on functions with limited regularity.

Method: Uses contour-integral representation of remainder term, approximates integrand with displaced singularities, and derives uniform asymptotic expansions of Legendre functions along the contour.

Result: Obtains error formula identifying optimal convergence rate and leading coefficient expressed in terms of cos((2n+1)φ), enabling quadrature size optimization and error correction.

Conclusion: The analysis provides accurate error characterization for Gauss-Legendre quadrature on singular functions, validated by numerical experiments with power and logarithmic singularities.

Abstract: We derive an asymptotic error formula for Gauss--Legendre quadrature applied
to functions with limited regularity, using the contour-integral representation
of the remainder term. To address the absence of uniformly valid approximations
of Legendre functions near $[-1,1]$, we approximate the integrand by smoother
functions with singularities displaced from the interval and then obtain
asymptotic expansions of the Legendre functions that hold uniformly along the
contour. The resulting error formula identifies not only the optimal
convergence rate but also the leading coefficient, expressed in terms of
$\cos((2n+1)\phi)$, where $n$ is the number of quadrature points and
$\cos(\phi)$ locates the singularity. This characterization enables both the
selection of quadrature sizes that minimize the leading error and the use of
the error formula as a correction term to accelerate convergence. Applications
to functions with power and logarithmic singularities are presented, and
numerical experiments confirm the accuracy of the analysis.

</details>


### [15] [Performance and Numerical Aspects of Decompositional Factorizations with FP64 Floating-Point Emulation in INT8](https://arxiv.org/abs/2509.23565)
*Piotr Luszczek,Vijay Gadepally,LaToya Anderson,William Arcand,David Bestor,William Bergeron,Alex Bonn,Daniel J. Burrill,Chansup Byun,Michael Houle,Matthew Hubbell,Hayden Jananthan,Michael Jones,Peter Michaleas,Guillermo Morales,Julia Mullen,Andrew Prout,Albert Reuther,Antonio Rosa,Charles Yee,Jeremy Kepner*

Main category: math.NA

TL;DR: Split integer emulation of FP64 using fixed-point tensor cores faces numerical issues in dense linear solvers, with performance and numerical profiles tested on NVIDIA Hopper GPUs.


<details>
  <summary>Details</summary>
Motivation: Modern hardware accelerators include lower-precision data formats that offer performance gains and energy savings, but introduce numerical issues not present in standard floating-point formats.

Method: Split integer emulation of FP64 performed only by fixed-point tensor core units, with extensive numerical tests on dense linear solvers and scaling input sizes.

Result: The study shows the effect of extended numerical range of matrix entries and provides performance and numerical profiles on NVIDIA Hopper GPUs.

Conclusion: While mixed precision offers performance benefits, split integer emulation of FP64 faces practical numerical challenges in dense linear solver applications that need to be addressed.

Abstract: Mixing precisions for performance has been an ongoing trend as the modern
hardware accelerators started including new, and mostly lower-precision, data
formats. The advantage of using them is a great potential of performance gain
and energy savings. The disadvantage are the numerical issues not present in
the standard-mandated floating-point formats. Split integer emulation of FP64
takes this to an extreme with the computation performed only by fixed-point
tensor core units. We present the new issues the emulation faces for practical
cases involving dense linear solver. We show extensive numerical tests
indicating the effect of extended numerical range of matrix entries. We also
scaled the input sizes to study the performance and numerical profiles on the
NVIDIA Hopper GPUs.

</details>


### [16] [A space-time generalized finite difference method for solving the transient Stokes/Parabolic interface problem in the moving system](https://arxiv.org/abs/2509.23702)
*Yanan Xing,Haibiao Zheng*

Main category: math.NA

TL;DR: A space-time generalized finite difference method (ST-GFDM) is developed for solving transient Stokes/Parabolic moving interface problems, treating time as a third spatial dimension to handle complex interface shapes and movements.


<details>
  <summary>Details</summary>
Motivation: To address fluid-structure interaction problems with complex moving interfaces by developing a method that can handle irregular movements, complex shapes, and interface deformations more effectively.

Method: The ST-GFDM treats time as a third spatial dimension, transforming 2D time-dependent problems into 3D interface problems. It uses locally dense nodes in small subdomains and transforms interface problems into coupled sub-problems for improved performance.

Result: The method demonstrates good performance in handling various complex scenarios including irregular moving directions, complex interface shapes, and interface translation/deformation. Five examples verify the method's simplicity, accuracy, high efficiency, and stability.

Conclusion: ST-GFDM is an effective approach for solving Stokes/Parabolic moving interface problems, offering advantages in dealing with complex interface geometries and movements while maintaining computational efficiency and stability.

Abstract: In this paper, a space-time generalized finite difference method (ST-GFDM) is
proposed to solve the transient Stokes/Parabolic moving interface problem which
is a type of fluid-structure interaction problem. The ST-GFDM considers the
time dimension as the third space dimension, and the 2D time-dependent
Stokes/Parabolic moving interface problem can be seen as a 3D interface problem
where the interface is formed by the initial interface shape and the moving
trajectory. The GFDM has an advantage in dealing with interface problems with
complex interface shape and moving interface in the ST domain. More irregular
moving direction, more complex interface shape, and the translation and
deformation of interface are analyzed to show the advantage of the ST-GFDM. The
interface problem can be transformed into coupled sub-problems and locally
dense nodes is used when the subdomain is too small to satisfy the needs of the
numbers of the nodes to improve the performance of the ST-GFDM. Five examples
are provided to verify the existence of the good performance of the ST-GFDM for
Stokes/Parabolic moving interface problems, including those of the simplicity,
accuracy, high efficiency and stability.

</details>


### [17] [Finite Element Complexes with Traces Structures: A unified framework for cohomology and bounded interpolation](https://arxiv.org/abs/2509.23788)
*Jun Hu,Yizhou Liang,Ting Lin*

Main category: math.NA

TL;DR: A unified framework for finite element complexes with extra smoothness, using trace structure to derive bubble complexes and ensure correct cohomology and L² bounded interpolation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extra smoothness in nonstandard finite element complexes (Stokes, Hessian, Elasticity, divdiv) compared to standard finite element exterior calculus.

Method: Introduces trace structure to derive bubble complexes in different dimensions (vertices, edges, faces), and shows that exactness of all bubble complexes ensures correct cohomology.

Result: Demonstrates that if bubble complexes in all dimensions are exact, the finite element has correct cohomology and L² bounded interpolation can be constructed.

Conclusion: Provides a unified framework for handling finite element complexes with extra smoothness, enabling correct cohomology and bounded interpolation through exact bubble complexes.

Abstract: This paper considers the cohomology and bounded interpolation of nonstandard
finite element complexes, e.g. Stokes, Hessian, Elasticity, divdiv. Compared to
the standard finite element exterior calculus, the main challenge is the
existence of extra smoothness. This paper provides a unified framework for
finite element complexes with extra smoothness. The trace structure is
introduced to derive the bubble complexes in different dimensions (vertices,
edges, faces). It is shown that if the bubble complexes in different dimensions
are all exact, then the finite element has the correct cohomology. Moreover,
the $L^2$ bounded interpolation can be constructed.

</details>


### [18] [Stabilizing the singularity swap quadrature for near-singular line integrals](https://arxiv.org/abs/2509.23881)
*David Krantz,Alex H. Barnett,Anna-Karin Tornberg*

Main category: math.NA

TL;DR: Singularity swap quadrature (SSQ) suffers from catastrophic cancellation for certain kernels. The paper introduces target-specific translated bases to incorporate near-vanishing kernel behavior, achieving near machine precision with minimal extra cost.


<details>
  <summary>Details</summary>
Motivation: SSQ is effective for potential evaluation but suffers from catastrophic cancellation when kernels have both near-vanishing numerators and strong singularities, particularly in double layer potentials and tensorial kernels.

Method: Introduces target-specific translated monomial and Fourier bases that explicitly incorporate the near-vanishing behavior of kernel numerators, combined with stable evaluation of the dominant constant term.

Result: Achieves close to machine precision for prototype integrals, with up to ten orders of magnitude lower error than standard SSQ at extremely close evaluation distances.

Conclusion: The proposed method effectively addresses precision loss in SSQ without significant additional computational cost, enabling high-accuracy evaluations even at very close distances.

Abstract: Singularity swap quadrature (SSQ) is an effective method for the evaluation
at nearby targets of potentials due to densities on curves in three dimensions.
While highly accurate in most settings, it is known to suffer from catastrophic
cancellation when the kernel exhibits both near-vanishing numerators and strong
singularities, as arises with scalar double layer potentials or tensorial
kernels in Stokes flow or linear elasticity. This precision loss turns out to
be tied to the interpolation basis, namely monomial (for open curves) or
Fourier (closed curves). We introduce a simple yet powerful remedy:
target-specific translated monomial and Fourier bases that explicitly
incorporate the near-vanishing behavior of the kernel numerator. We combine
this with a stable evaluation of the constant term which now dominates the
integral, significantly reducing cancellation. We show that our approach
achieves close to machine precision for prototype integrals, and up to ten
orders of magnitude lower error than standard SSQ at extremely close evaluation
distances, without significant additional computational cost.

</details>


### [19] [Best weighted approximation of some kernels on the real axis](https://arxiv.org/abs/2509.23890)
*Stanislav Chaichenko,Viktor Savchuk,Andrii Shidlich*

Main category: math.NA

TL;DR: Exact calculation of best weighted polynomial approximation for specific kernel forms in mean square metric


<details>
  <summary>Details</summary>
Motivation: To determine precise values and polynomials for optimal weighted polynomial approximations of complex kernel functions

Method: Mathematical analysis of kernels with form (A+Bt)/(t²+λ²)^(s+1) where A,B are complex constants, λ>0, s∈ℕ, using mean square metric

Result: Obtained exact value and polynomial expressions for the best weighted polynomial approximation

Conclusion: Successfully derived analytical solutions for weighted polynomial approximations of the specified kernel family

Abstract: We calculate the exact value and find the polynomial of the best weighted
polynomial approximation of kernels of the form $\frac
{A+Bt}{(t^2+\lambda^2)^{s+1}}$, where $A$ and $B$ are fixed complex numbers,
$\lambda>0$, $s\in {\mathbb N}$, in the mean square metric.

</details>


### [20] [Mixed-Derivative Total Variation](https://arxiv.org/abs/2509.23995)
*Vincent Guillemet,Michael Unser*

Main category: math.NA

TL;DR: The paper introduces a new convex regularization combining TV and mixed derivatives norms, showing extreme points are axis-aligned polygon indicators, enabling exact pixel-based discretization for inverse problems.


<details>
  <summary>Details</summary>
Motivation: To develop regularization methods for inverse problems that can be exactly discretized on pixel grids while maintaining continuous-domain properties.

Method: Proposed convex combination of TV term and M(R^2) norm of mixed derivatives, analyzed extreme points of unit ball, and constructed exact discretization using tensor products of first-order B-splines.

Result: Extreme points are indicator functions of polygons with edges aligned to coordinate axes; exact discretization achieved; denoising problem has unique solution that solves continuous-domain IP.

Conclusion: The proposed regularization enables exact pixel-based discretization while preserving continuous-domain properties, providing a practical framework for solving inverse problems.

Abstract: The formulation of norms on continuous-domain Banach spaces with exact
pixel-based discretization is advantageous for solving inverse problems (IPs).
In this paper, we investigate a new regularization that is a convex combination
of a TV term and the $\M(\R^2)$ norm of mixed derivatives. We show that the
extreme points of the corresponding unit ball are indicator functions of
polygons whose edges are aligned with either the $x_1$- or $x_2$-axis. We then
apply this result to construct a new regularization for IPs, which can be
discretized exactly by tensor products of first-order B-splines, or
equivalently, pixels. Furthermore, we exactly discretize the loss of the
denoising problem on its canonical pixel basis and prove that it admits a
unique solution, which is also a solution to the underlying continuous-domain
IP.

</details>


### [21] [Analytical and Numerical Approaches for Finding Functional Iterates and Roots](https://arxiv.org/abs/2509.24049)
*Sanay Nesargi,Gregory Roudenko*

Main category: math.NA

TL;DR: The paper investigates solutions to f(f(x)) = e^x (half-iterate of exponential), constructs non-elementary solutions using Lambert W function, tetration, and Abel's equation, and develops computational methods for fractional iterates.


<details>
  <summary>Details</summary>
Motivation: To find solutions to the functional equation f(f(x)) = e^x, which represents finding a half-iterate of the exponential function, and extend this to fractional iteration theory for broader applications.

Method: Uses Lambert W function, tetration, Abel's functional equation, power-series expansion near x=1, numerical approximation of super-logarithm, and optimization techniques including genetic algorithms.

Result: Successfully constructed non-elementary solutions, developed computational procedures for fractional iterates including half-iterate of e^x, validated against Kneser's tetration, and extended methods to other analytic functions.

Conclusion: Fractional iterates exist for many analytic functions and can be computed with practical accuracy, opening applications in dynamical systems and functional analysis.

Abstract: We investigate solutions to the functional equation $f(f(x)) = e^x$, which
can be interpreted as the problem of finding a half iterate of the exponential
map. While no elementary solution exists, we construct and analyze
non-elementary solutions using methods based on the Lambert W function,
tetration, and Abel's functional equation. We examine structural properties of
possible solutions, including monotonicity, injectivity, and behavior across
different intervals, and provide a piecewise-defined framework that extends to
the entire real domain. Building on this, we introduce the super-logarithm and
its inverse, the super-root, as analytic tools for defining fractional iterates
of $e^x$. Using a power-series expansion near $x = 1$, we numerically
approximate the super-logarithm and demonstrate a procedure for computing
fractional iterates, including the half-iterate of the exponential function.
Our approach is validated by comparisons to known constructions such as
Kneser's tetration, with an emphasis on computational feasibility and numerical
stability. Finally, we explore the broader landscape of fractional iteration,
showing that similar techniques can be applied to other functions beyond $e^x$.
Through numerical approximations and series-based methods using genetic
algorithms and other optimization techniques, we confirm that fractional
iterates not only exist for many analytic functions but can be computed with
practical accuracy, opening pathways to further applications in dynamical
systems and functional analysis.

</details>


### [22] [Numerical and analytical modeling of heat equation in current-carrying conductors using the heat equation implemented using Finite-JAX](https://arxiv.org/abs/2509.24162)
*Arturo Rodriguez,Christopher Harris,Avinash Potluri,Noah L. Estrada,Alan M. Hernandez,Vyom Kumar,Francisco O. Aguirre Ortega,Vineeth Vijaya Kumar*

Main category: math.NA

TL;DR: Modeling heat distribution in current-carrying wires using finite-difference methods with JAX implementation, validated against analytical solutions.


<details>
  <summary>Details</summary>
Motivation: Resistive heating in conductors affects structural integrity and reliability of electronic systems, requiring accurate temperature prediction.

Method: Used classical heat conduction equation with finite-difference discretization in JAX, explicit time integration, and Dirichlet boundary conditions.

Result: Numerical predictions converge to analytical solutions with expected error reduction rates, validating the approach.

Conclusion: The heat equation provides rigorous mathematical foundation for modeling resistive heating in conductors with verified numerical methods.

Abstract: Current-carrying conductors inevitably experience resistive heating due to
the finite electrical conductivity of the material. The resulting temperature
distribution within the wire has essential implications for structural
integrity, efficiency, and long-term reliability of electronic and power
systems. In this work, we model the spatiotemporal evolution of heat in a
current-carrying wire using the classical heat conduction equation. In a
two-dimensional formulation, heat transport is considered both along and across
the conductor. The governing partial differential equation is discretized using
finite-difference methods implemented using Finite-JAX under appropriate
initial and boundary conditions, including the Dirichlet condition relevant to
practical scenarios. Time integration is performed using the explicit scheme,
and stability constraints are systematically examined. To assess the accuracy
of the numerical approach, we compare the computed temperature fields with the
exact analytical solution of the heat equation for canonical geometry. Results
show that the numerical prediction converges toward the analytical solution,
with error norms decreasing at the expected order of accuracy. This study
demonstrates how the heat equation provides a rigorous mathematical foundation
for modeling resistive heating in conductors.

</details>


### [23] [Spectral equivalence of unsymmetric kernel matrices and applications](https://arxiv.org/abs/2509.24561)
*Tizian Wenzel,Armin Iske*

Main category: math.NA

TL;DR: This paper extends stability analysis from symmetric to unsymmetric kernel matrices by establishing spectral equivalence with their symmetric versions, particularly for finitely smooth kernels and convolutional kernels over domains.


<details>
  <summary>Details</summary>
Motivation: While symmetric kernel matrices have been well-studied for stability properties, unsymmetric kernel matrices lack similar theoretical foundations. The authors aim to extend stability analysis to unsymmetric cases, generalizing previous work on translational invariant kernels to more practical finitely smooth kernels.

Method: The authors establish a spectral equivalence between unsymmetric kernel matrices and their unshifted symmetric versions under small shifts. This approach allows them to derive stability properties for unsymmetric matrices by leveraging known results about symmetric matrices.

Result: The paper derives novel lower bounds for the smallest eigenvalue of kernel matrices in terms of the separation distance of data points. These bounds apply to convolutional kernels over domains, which are no longer translational invariant but remain practically important.

Conclusion: The work successfully extends stability analysis from symmetric to unsymmetric kernel matrices, providing theoretical foundations for practical applications involving finitely smooth kernels and convolutional kernels over domains, with explicit bounds on condition numbers.

Abstract: Symmetric kernel matrices are a well researched topic in the literature of
kernel based approximation. In particular stability properties in terms of
lower bounds on the smallest eigenvalue of such symmetric kernel matrices are
thoroughly investigated, as they play a fundamental role in theory and
practice. In this work, we focus on unsymmetric kernel matrices and derive
stability properties under small shifts by establishing a spectral equivalence
to their unshifted, symmetric versions. This extends and generalizes results
for translational invariant kernels upon Quak et al [SIAM Journal on Math.
Analysis, 1993] and Sivakumar and Ward [Numerische Mathematik, 1993], however
focussing instead on finitely smooth kernels. As applications, we consider
convolutional kernels over domains, which are no longer translational
invariant, but which are still an important class of kernels for applications.
For these, we derive novel lower bounds for the smallest eigenvalue of the
kernel matrices in terms of the separation distance of the data points, and
thus derive stability bounds in terms of the condition number.

</details>


### [24] [Universal $L_2$-approximation using median lattice algorithms](https://arxiv.org/abs/2509.24582)
*Zexin Pan,Takashi Goda,Peter Kritzer*

Main category: math.NA

TL;DR: A universal median lattice-based algorithm for multivariate L2-approximation in weighted Korobov spaces that eliminates the need for prior knowledge of smoothness and weights, achieving near-optimal error rates.


<details>
  <summary>Details</summary>
Motivation: Existing median lattice-based algorithms require prior knowledge of smoothness and weights of the Korobov space, which limits their practical applicability. The goal is to develop a universal algorithm that works without such prior information.

Method: The proposed universal median lattice-based algorithm uses randomly shifted, randomly chosen rank-1 lattice rules to estimate coefficients via median approximations, but eliminates the need for constructing hyperbolic cross index sets based on known smoothness and weights.

Result: The algorithm achieves L2-approximation error arbitrarily close to the optimal rate for individual functions in Korobov spaces with arbitrary smoothness and downward-closed weights, though with slightly deteriorated tractability properties compared to the non-universal version.

Conclusion: A universal median lattice-based algorithm is successfully developed that works without prior knowledge of smoothness and weights, maintaining near-optimal approximation rates while sacrificing some tractability.

Abstract: We study the problem of multivariate $L_2$-approximation of functions in a
weighted Korobov space using a median lattice-based algorithm recently proposed
by the authors. In the original work, the algorithm requires knowledge of the
smoothness and weights of the Korobov space to construct the hyperbolic cross
index set, where each coefficient is estimated via the median of approximations
obtained from randomly shifted, randomly chosen rank-1 lattice rules. In this
paper, we introduce a \emph{universal median lattice-based algorithm}, which
eliminates the need for any prior information on smoothness and weights.
Although the tractability property of the algorithm slightly deteriorates, we
prove that, for individual functions in the Korobov space with arbitrary
smoothness and (downward-closed) weights, it achieves an $L_2$-approximation
error arbitrarily close to the optimal rate with respect to the number of
function evaluations.

</details>


### [25] [Coupling Physics Informed Neural Networks with External Solvers](https://arxiv.org/abs/2509.24615)
*Rahul Halder,Giovanni Stabile,Gianluigi Rozza*

Main category: math.NA

TL;DR: The paper proposes modifying physics-based loss terms in PINNs to incorporate numerical residuals from external forward solvers, addressing challenges with gradient computation when coupling with discretized governing equations.


<details>
  <summary>Details</summary>
Motivation: PINNs face difficulties when coupling with external forward solvers due to inability to directly access discretized forms of governing equations and include them in computational graphs, creating challenges for gradient-based optimization.

Method: Modify physics-based loss term to account for residual from external solver and compute required derivatives for optimization machinery. Demonstrate on benchmark full-order and reduced-order systems.

Result: The proposed methodology successfully addresses the gradient computation challenge when PINNs are coupled with external forward solvers using discretized governing equations.

Conclusion: The modified physics-based loss approach enables effective coupling of PINNs with external solvers by properly handling numerical residuals and derivative computations for optimization.

Abstract: The current work aims to incorporate physics-based loss in Physics Informed
Neural Network (PINN) directly using the numerical residual obtained from the
governing equation in any dicretized forward solver. PINN's major difficulties
in coupling with external forward solvers arise from the inability to access
the discretized form (Finite difference, finite volume, finite element, etc.)
of the governing equation directly through the network and to include them in
its computational graph. This poses a significant challenge to conventional
automatic-differentiation-based derivative computation of physics-based loss
terms concerning the neural network hyperparameters if gradient-based
optimization techniques are adopted. Therefore, we propose modifying the
physics-based loss term to account for the residual arising from the external
solver and to compute the derivative required for the optimization machinery.
The proposed methodologies are demonstrated on benchmark full-order and
reduced-order systems.

</details>


### [26] [An Efficient Finite Element Method for Multi-dimensional Nonlocal Laplacian on Uniform Grids](https://arxiv.org/abs/2509.24809)
*Changtao Sheng,Huiyuan Li,Huifang Yuan,Li-Lian Wang*

Main category: math.NA

TL;DR: A semi-analytic approach for computing the stiffness matrix of nonlocal Laplacian on uniform grids using B-splines and FFT, with automatic reduction to local case as interaction radius approaches zero.


<details>
  <summary>Details</summary>
Motivation: Computing stiffness matrices for nonlocal Laplacian on unstructured meshes is difficult due to nonlocality and potential singularities.

Method: Use C^0-piecewise linear FEM on uniform grids, leverage connection between FE bases and B-splines to reduce 2d-dimensional integrals to d-dimensional balls, employ spherical coordinates for singularity analysis, and utilize FFT for efficient matrix-vector multiplication.

Result: Nonlocal stiffness matrix has block-Toeplitz structure enabling FFT-based computation, and automatically reduces to local stiffness matrix as interaction radius δ→0+.

Conclusion: The semi-analytic approach provides efficient computation for uniform grids and can integrate with grid-overlay technique for arbitrary bounded domains.

Abstract: Computing the stiffness matrix for the finite element discretization of the
nonlocal Laplacian on unstructured meshes is difficult, because the operator is
nonlocal and can even be singular. In this paper, we focus on the
$C^0$-piecewise linear finite element method (FEM) for the nonlocal Laplacian
on uniform grids within a $d$-dimensional rectangular domain. By leveraging the
connection between FE bases and B-splines (having attractive convolution
properties), we can reduce the involved $2d$-dimensional integrals for the
stiffness matrix entries into integrations over $d$-dimensional balls with
explicit integrands involving cubic B-splines and the kernel functions, which
allows for explicit study of the singularities and accurate evaluations of such
integrals in spherical coordinates. We show the nonlocal stiffness matrix has a
block-Toeplitz structure, so the matrix-vector multiplication can be
implemented using fast Fourier transform (FFT). In addition, when the
interaction radius $\delta\to 0^+,$ the nonlocal stiffness matrix automatically
reduces to the local one. Although our semi-analytic approach on uniform grids
cannot be extended to general domains with unstructured meshes, the resulting
solver can seamlessly integrate with the grid-overlay (Go) technique for the
nonlocal Laplacian on arbitrary bounded domains.

</details>


### [27] [A posteriori existence of strong solutions to the Navier-Stokes equations in 3D](https://arxiv.org/abs/2509.25105)
*Aaron Brunk,Jan Giesselmann,Tabea Tscherpel*

Main category: math.NA

TL;DR: A posteriori existence criterion for 3D Navier-Stokes equations using numerical solutions to verify strong solution existence on finite time intervals.


<details>
  <summary>Details</summary>
Motivation: Global existence of strong solutions to 3D incompressible Navier-Stokes equations is an open problem; a posteriori methods provide rigorous verification using numerical solutions.

Method: Uses conditional stability estimates in L² and L³ spaces, based on Iskauriaza-Serëgin-Shverak blow-up criterion, applied to mixed finite elements with implicit Euler time discretization.

Result: Develops computable a posteriori criterion involving negative Sobolev norms of residual, allowing verification of strong solution existence without extra assumptions.

Conclusion: The method enables rigorous verification of strong solution existence on short time intervals using numerical approximations, potentially extending beyond theoretical capabilities with sufficient computational resources.

Abstract: Global existence of strong solutions to the three-dimensional incompressible
Navier-Stokes equations remains an open problem. A posteriori existence results
offer a way to rigorously verify the existence of strong solutions by ruling
out blow-up on a certain time interval, using only numerical solutions. In this
work we present such a result for the Navier-Stokes equations subject to
periodic boundary conditions, which makes use of a version of the celebrated
blow-up criterion in the critical space $L^\infty(L^3)$ by Iskauriaza,
Ser\"egin and Shverak (2003). Our approach is based on a conditional stability
estimate in $L^2$ and $L^3$. The a posteriori criterion that, if satisfied,
verifies existence of strong solutions, involves only negative Sobolev norms of
the residual. We apply the criterion to numerical approximations computed with
mixed finite elements and an implicit Euler time discretisation. A posteriori
error estimates allow us to derive a fully computable criterion without
imposing any extra assumptions on the solution. While limited to short time
intervals, with sufficient computational resources in principle the criterion
might allow for a verification over longer time intervals than what can be
achieved by theoretical means.

</details>


### [28] [Diffuse Domain Methods with Dirichlet Boundary Conditions](https://arxiv.org/abs/2509.25115)
*Luke Benfield,Andreas Dedner*

Main category: math.NA

TL;DR: New Diffuse Domain Methods for Dirichlet boundary conditions using mixed formulations and Nitsche's method, with improved accuracy demonstrated on Navier-Stokes problems.


<details>
  <summary>Details</summary>
Motivation: Solving PDEs on complex domains requires fitted meshes, which is computationally challenging. The Diffuse Domain Method offers an alternative by reformulating problems on simple domains using phase-field functions.

Method: Developed new DDM methods from mixed formulations to transform Dirichlet conditions into natural boundary conditions, and created coercive formulations using Nitsche's method with proofs of coercivity.

Result: Numerical experiments show improved accuracy and reveal the balance between L² and H¹ errors. Demonstrated practical effectiveness on incompressible Navier-Stokes equations for benchmark fluid dynamics problems.

Conclusion: The new DDM methods provide effective alternatives for solving PDEs with Dirichlet boundary conditions on complex domains, with proven coercivity and demonstrated practical performance.

Abstract: The solution of partial differential equations (PDEs) on complex domains
often presents a significant computational challenge by requiring the
generation of fitted meshes. The Diffuse Domain Method (DDM) is an alternative
which reformulates the problem on a larger, simple domain where the complex
geometry is represented by a smooth phase-field function.
  This paper introduces and analyses several new DDM methods for solving
problems with Dirichlet boundary conditions. We derive two new methods from the
mixed formulation of the governing equations. This approach transforms the
essential Dirichlet conditions into natural boundary conditions. Additionally,
we develop coercive formulations based on Nitsche's method, and provide proofs
of coercivity for all new and key existing approximations.
  Numerical experiments demonstrate the improved accuracy of the new methods,
and reveal the balance between $L^2$ and $H^1$ errors. The practical
effectiveness of this approach is demonstrated through the simulation of the
incompressible Navier-Stokes equations on a benchmark fluid dynamics problems.

</details>


### [29] [A bound-preserving multinumerics scheme for steady-state convection-diffusion equations](https://arxiv.org/abs/2509.25181)
*Maurice S. Fabien*

Main category: math.NA

TL;DR: A novel adaptive coupling method combining cell-centered finite volume (FV) and discontinuous Galerkin (DG) methods for convection-diffusion equations, with automatic domain partitioning to ensure bound-preserving solutions.


<details>
  <summary>Details</summary>
Motivation: DG methods are stable and resolve sharp layers in convection-dominated regimes but can produce spurious oscillations and are computationally expensive, while FV methods are monotone and inexpensive but low-order. The goal is to combine their strengths while mitigating their weaknesses.

Method: Domain is divided into disjoint FV and DG regions coupled through interface terms. An adaptive partitioning strategy automatically selects subdomains: when solution's cell average violates bounds, switch to FV on a small neighborhood of that element. This process repeats until all cell averages are bound-preserving, then standard conservative limiters are applied.

Result: The adaptive technique effectively combines DG's high-order accuracy and sharp layer resolution with FV's monotonicity and computational efficiency, producing bound-preserving solutions.

Conclusion: The proposed adaptive FV-DG coupling method successfully addresses the limitations of both methods, providing an effective solution for convection-diffusion problems with verified performance on standard benchmarks.

Abstract: We solve the convection-diffusion equation using a coupling of cell-centered
finite volume (FV) and discontinuous Galerkin (DG) methods. The domain is
divided into disjoint regions assigned to FV or DG, and the two methods are
coupled through an interface term. DG is stable and resolves sharp layers in
convection-dominated regimes, but it can produce sizable spurious oscillations
and is computationally expensive; FV (two-point flux) is low-order and
monotone, but inexpensive. We propose a novel adaptive partitioning strategy
that automatically selects FV and DG subdomains: whenever the solution's cell
average violates the bounds, we switch to FV on a small neighborhood of that
element. Viewed as a natural analog of $p$-adaptivity, this process is repeated
until all cell averages are bound-preserving (up to some specified tolerance).
Thereafter, standard conservative limiters may be applied to ensure the full
solution is bound-preserving. Standard benchmarks confirm the effectiveness of
the adaptive technique.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [30] [Homogenization of a semilinear elliptic problem](https://arxiv.org/abs/2509.22859)
*Thuyen Dang,Yuliya Gorb,Silvia Jiménez Bolaños*

Main category: math.AP

TL;DR: Homogenization of semilinear elliptic equations with discontinuous coefficients using two-scale convergence


<details>
  <summary>Details</summary>
Motivation: To address homogenization problems where the coefficients of second-order differential operators may be discontinuous, which is common in heterogeneous materials and composite structures

Method: Two-scale convergence technique to derive the homogenized equation, with proofs of existence, uniqueness, and a priori estimates for fine-scale solutions

Result: Successfully established existence and uniqueness of fine-scale solutions, derived homogenized equation, and provided corrector results for semilinear elliptic equations with discontinuous coefficients

Conclusion: The two-scale convergence method is effective for homogenizing semilinear elliptic equations even when coefficients are discontinuous, providing rigorous mathematical foundation for such problems

Abstract: We consider the homogenization of a semilinear elliptic equation where the
coefficients of the second-order differential operator may be discontinuous. We
establish the existence and uniqueness of the fine-scale solution, followed by
an a priori estimate. The homogenized equation is derived using two-scale
convergence, and a corrector result is also provided

</details>


### [31] [Existence result for a system of two semilinear coupled Poisson equations with asymptotically linear nonlinearities](https://arxiv.org/abs/2509.22899)
*Ablanvi Songo*

Main category: math.AP

TL;DR: Existence of solutions for semilinear coupled Poisson equations with asymptotically linear nonlinearities without Ambrosetti-Rabinowitz condition.


<details>
  <summary>Details</summary>
Motivation: To establish solution existence for coupled Poisson equations without relying on the restrictive Ambrosetti-Rabinowitz condition or its variants.

Method: Uses a generalized saddle point theorem developed by Colin and the author.

Result: Proves existence of at least one solution to the system of two semilinear coupled Poisson equations.

Conclusion: The generalized saddle point theorem provides an effective approach for proving solution existence in systems with asymptotically linear nonlinearities, bypassing traditional Ambrosetti-Rabinowitz requirements.

Abstract: We establish the existence of at least one solution to a system of two
semilinear coupled Poisson equations with asymptotically linear nonlinearities,
without imposing the Ambrosetti-Rabinowitz condition or any of its refinements.
The proof relies on a generalized saddle point theorem due to Colin and the
author \cite{CS}.

</details>


### [32] [The effect of "very fast" dispersal on two species competition with drift](https://arxiv.org/abs/2509.22924)
*Erin Ellefsen,Rana Parshad*

Main category: math.AP

TL;DR: A novel model shows that while faster dispersers typically win in competition, 'much faster' dispersers can actually lose to slower competitors under certain initial conditions, contrary to classical theory.


<details>
  <summary>Details</summary>
Motivation: To challenge classical ecological theory that predicts faster dispersers always win in competition, and explore how extreme dispersal differences affect competitive outcomes in fragmented habitats.

Method: Developed a mathematical model using p-Laplacian operator to represent 'much faster' dispersing species competing with slower dispersers, with analytical proofs for global existence of weak solutions in the regime 3/2 < p < 2, supplemented by numerical simulations.

Result: Proved that while faster dispersers generally win, 'much faster' dispersers can lose to slower competitors for certain initial data, demonstrating a counterintuitive outcome that contradicts classical predictions.

Conclusion: The findings reveal complex competitive dynamics where extreme dispersal advantages can become disadvantages, with important implications for biodiversity conservation, refuge design, and biological control strategies in the context of habitat fragmentation and climate change.

Abstract: Classical theory predicts that for two competing populations subject to a
constant downstream drift, the faster disperser will competitively exclude the
slower disperser. In the current work, we consider a novel model of a "much
faster" dispersing species, modeled via a $p$-Laplacian operator, competing
with a slower disperser. We prove global existence of weak solutions to this
model for any positive initial condition, in the regime $\frac{3}{2} < p <2$.
Counterintuitively, we show that while the faster disperser always wins - the
"much faster" disperser could actually lose, for certain initial data. Several
numerical simulations are conducted to confirm our analytical findings. Our
results have implications for biodiversity, refuge design, and improved
biological control, driven by habitat fragmentation and climate change.

</details>


### [33] [A convergence rate result for front tracking approximations of conservation laws with discontinuous flux](https://arxiv.org/abs/2509.22952)
*Shyam Sundar Ghoshal,John D Towers*

Main category: math.AP

TL;DR: This paper analyzes a front tracking algorithm for scalar conservation laws with flux discontinuities, proving convergence rates of at least 1/2 for general fluxes and 1 for strictly monotone or continuous fluxes.


<details>
  <summary>Details</summary>
Motivation: To expand the class of discontinuous-flux conservation laws for which front tracking error estimates exist, using a new analytical method that doesn't rely on the Kuznetsov lemma.

Method: Front tracking algorithm analysis for scalar conservation laws with single spatial flux discontinuity (two-flux problem), requiring only smooth fluxes with finitely many flux crossings but no convexity or unimodality assumptions.

Result: Proved convergence rate of at least 1/2 for general fluxes, and convergence rate of 1 for strictly increasing/decreasing fluxes or continuous fluxes (no discontinuity).

Conclusion: The paper successfully expands the applicability of front tracking error estimates to broader classes of discontinuous-flux conservation laws using a novel analytical approach.

Abstract: We consider the initial value problem for a scalar conservation law in one
space dimension with a
  single spatial flux discontinuity, the so-called two-flux problem. We prove
that a well-known
  front tracking algorithm has a convergence rate of at least one-half. The
fluxes are required to be smooth, but are not required to be
  convex or concave, monotone, or even unimodal. We require that there are no
more than
  finitely many flux crossings, but we do not require that they satisfy the
so-called crossing condition.
  If both fluxes are strictly increasing or strictly decreasing then our
analysis yields a
  convergence rate of one, in agreement with a recent result. Similarly, if the
fluxes
  are equal, i.e., there is no flux discontinuity, we obtain a convergence rate
of one
  in this case also, in agreement with a classical result.
  The novelty of this paper is that the class of discontinuous-flux
conservation laws
  for which there is a front tracking error estimate is expanded, and that the
method of analysis is new;
  we do not use the Kuznetsov lemma which is commonly used for this type of
analysis.

</details>


### [34] [On parameterized nonlocal-fractional transmission problems and associated function spaces](https://arxiv.org/abs/2509.22987)
*Qiang Du,Zhaolong Han,Tadele Mengesha,James M. Scott,Xiaochuan Tian*

Main category: math.AP

TL;DR: Mathematical formulation and analysis of seamlessly coupled nonlocal models with transmission conditions across interfaces, derived from parameterized energy families.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze coupled nonlocal models that seamlessly connect different interaction ranges across domain interfaces, providing rigorous mathematical foundations.

Method: Derived from variation of parameterized energy families combining fractional Dirichlet energy and finite-range nonlocal Dirichlet energy, with rigorous formulation and well-posedness analysis.

Result: Presented rigorous mathematical formulation and established well-posedness of the coupled nonlocal models, investigated limiting behavior in various parameter regimes.

Conclusion: Successfully developed and analyzed a family of seamlessly coupled nonlocal models with transmission conditions, providing mathematical foundation for interface problems in nonlocal modeling.

Abstract: In this paper, we consider a family of seamlessly coupled nonlocal models
associated with transmission conditions across an interface. The models are
derived from the variation of a parameterized family of energies consisting of
a fractional type Dirichlet energy on one subdomain and a nonlocal Dirichlet
energy involving a finite range of interactions on another subdomain. We
present the rigorous mathematical formulation and its well-posedness. We also
investigate the behavior of the model in various limiting regimes.

</details>


### [35] [Stability of standing waves for all frequencies to nonlinear Schrödinger equations with potentials in one dimension](https://arxiv.org/abs/2509.23016)
*Noriyoshi Fukaya,Masahiro Ikeda,Hiroaki Kikuchi*

Main category: math.AP

TL;DR: The paper proves orbital stability of standing waves for 1D nonlinear Schrödinger equations with potentials in L²-subcritical and critical cases for all frequencies.


<details>
  <summary>Details</summary>
Motivation: The presence of potentials breaks scale invariance, making it difficult to apply existing abstract stability theory directly without perturbative arguments, leaving little known about orbital stability for all frequencies in non-scale-invariant settings.

Method: The authors employ the approach of Noris, Tavares, and Verzini (2014) to overcome the difficulty of applying the Grillakis-Shatah-Strauss abstract theory directly.

Result: Standing waves are shown to be orbitally stable for all frequencies in the L²-subcritical and critical cases.

Conclusion: The paper successfully establishes orbital stability of standing waves across all frequencies in non-scale-invariant settings with potentials, overcoming previous limitations in the field.

Abstract: In this paper, we study the orbital stability of standing waves for
one-dimensional nonlinear Schr\"odinger equations with potentials. We show that
the standing waves are orbitally stable for all frequencies in the $L^{2}$-
subcritical and critical cases. Since the presence of potentials breaks the
scale invariance of the equations, it is a delicate problem to apply the
abstract theory of Grillakis, Shatah, and Strauss (1987) directly without a
perturbative argument. For this reason, little is known about the orbital
stability of standing waves for \textit{all} frequencies in the
non-scale-invariant setting. We overcome this difficulty by employing the
approach of Noris, Tavares, and Verzini (2014).

</details>


### [36] [Global boundedness of solutions of degenerate and non-uniform parabolic equations](https://arxiv.org/abs/2509.23064)
*Thuyen Dang,Duong Minh Duc*

Main category: math.AP

TL;DR: The paper establishes global boundedness of weak solutions to parabolic equations with unbounded coefficients and non-uniform ellipticity, without requiring Muckenhoupt-type conditions.


<details>
  <summary>Details</summary>
Motivation: To extend regularity theory for parabolic equations beyond the classical assumptions of bounded coefficients, strict/uniform ellipticity, and Muckenhoupt-type conditions, allowing for more general coefficient behavior.

Method: Analysis of weak solutions to parabolic equations ∂u/∂t - Lu = f, where L is an elliptic operator with potentially unbounded coefficients that may not be strictly or uniformly elliptic, using techniques that don't rely on Muckenhoupt conditions.

Result: Proves global boundedness of weak solutions u, even when the coefficients are unbounded and the operator is not strictly/uniformly elliptic, and applies to solutions that vanish on part of the boundary (A×(0,T))∪(Ω×{0}) and are free elsewhere.

Conclusion: The paper successfully extends regularity results for parabolic equations to include cases with unbounded coefficients and non-uniform ellipticity, providing a more general framework for analyzing weak solutions without traditional restrictive assumptions.

Abstract: Let $2 \le N\in\mathbb{N}$, $\Omega$ be a bounded open in $\mathbb{R}^{N}$,
$T\in (0,\infty)$, $Q=\Omega\times (0,T)$, $u$ be a weak solution of parabolic
equation $\displaystyle \frac{\partial u}{\partial t} -Lu= f$, where $L$ is an
elliptic operator on a space of functions on $Q$. The coefficients of $L$ may
not be bounded, not strictly nor uniformly elliptic, and not of Muckenhoupt
type. We obtain global boundedness of $u$. Our result can be applied to $u$,
which may vanish on $(A\times (0,T))\cup (\Omega\times \{0\})$ of the boundary
of $Q$ and is free outside this set.

</details>


### [37] [Robin Problems of Elliptic Equations on Rough Domains: Hölder Regularity, Green's Functions, and Harmonic Measures](https://arxiv.org/abs/2509.23073)
*Jiayi Wang,Dachun Yang,Sibei Yang*

Main category: math.AP

TL;DR: The paper establishes existence, uniqueness, and regularity results for Robin boundary value problems with measurable coefficients on domains with Ahlfors regular boundaries, extending previous work by weakening assumptions on the boundary coefficient β.


<details>
  <summary>Details</summary>
Motivation: To extend the results of David et al. by weakening their assumption that β is a positive constant, allowing β to be a non-negative measurable function with certain integrability conditions and positive lower bounds on measurable sets.

Method: Analysis of Robin boundary value problems using techniques from elliptic PDE theory, harmonic analysis, and geometric measure theory on domains with Ahlfors regular boundaries.

Result: Proved existence and uniqueness of weak solutions, global Hölder regularity, boundary Harnack inequality, existence and estimates of Green's functions, and mutual absolute continuity between harmonic measure and surface measure.

Conclusion: The results successfully extend previous work by allowing more general boundary coefficients β while maintaining key regularity properties and quantitative characterizations of solutions.

Abstract: Let $n\ge 2$ and $s\in (n-2,n)$. Assume that $\Omega\subset \mathbb{R}^n$ is
a one-sided bounded non-tangentially accessible domain with $s$-Ahlfors regular
boundary and $\sigma$ is the surface measure on the boundary of $\Omega$,
denoted by $\partial \Omega$. Let $\beta$ be a non-negative measurable function
on $\partial \Omega$ satisfying $\beta\in L^{q_0}(\partial
\Omega,\sigma)~\text{with}~ q_0 \in(\frac{s}{s+2-n},\infty]~\text{and}\
\beta\ge a_0~\text{on}~E_0\subset \partial \Omega, $ where $a_0$ is a given
positive constant and $E_0\subset \partial \Omega$ is a $\sigma$-measurable set
with $\sigma(E_0)>0$. In this article, for any $f\in L^p(\partial
\Omega,\sigma)$ with $p\in(s/(s+2-n),\infty]$, we obtain the existence and
uniqueness, the global H\"older regularity, and the boundary Harnack inequality
of the weak solution to the Robin problem $$\begin{cases} -\mathrm{div}(A\nabla
u) = 0~~&\text{in}~\Omega,\\ A\nabla u\cdot \boldsymbol{\nu}+\beta u =
f~~&\text{on}~\partial \Omega, \end{cases} $$ where the coefficient matrix $A$
is real-valued, bounded and measurable and satisfies the uniform ellipticity
condition and where $\boldsymbol{\nu}$ denotes the outward unit normal to
$\partial\Omega$. Furthermore, we establish the existence, upper bound
pointwise estimates, and the H\"older regularity of Green's functions
associated with this Robin problem. As applications, we further prove that the
harmonic measure associated with this Robin problem is mutually absolutely
continuous with respect to the surface measure $\sigma$ and also provide a
quantitative characterization of mutual absolute continuity at small scales.
These results extend the corresponding results established by David et al.
[arXiv: 2410.23914] via weakening their assumption that $\beta$ is a given
positive constant.

</details>


### [38] [Stochastic diffusive energy balance climate model with a multiplicative noise modeling the Solar variability](https://arxiv.org/abs/2509.23153)
*Gregorio Díaz,Jesús Ildefonso Díaz*

Main category: math.AP

TL;DR: Existence, uniqueness, and comparison of solutions for a nonlinear stochastic parabolic PDE modeling solar variability with multiplicative Wiener noise in a 1D Energy Balance Model, using a hybrid co-albedo function combining features of Sellers and Budyko models.


<details>
  <summary>Details</summary>
Motivation: To develop a more realistic climate model that incorporates solar variability as stochastic noise and combines the advantages of both Sellers and Budyko albedo models for better detection of polar ice caps.

Method: Introduces a hybrid co-albedo nonlinear term that is continuous (like Sellers model) but has infinite derivative at u=-10°C (like Budyko model), and applies the method of successive approximations despite the lack of differentiability.

Result: Successfully proves existence, uniqueness, and comparison of solutions for the nonlinear stochastic parabolic PDE with multiplicative Wiener cylindrical noise.

Conclusion: The hybrid co-albedo approach effectively combines advantages of both Sellers and Budyko models, and the method of successive approximations works satisfactorily despite the non-differentiable function.

Abstract: We prove the existence, uniqueness, and comparison of solutions for a
nonlinear stochastic parabolic partial differential equation that includes the
Solar variability in terms of a multiplicative Wiener cylindrical noise in the
term of the absorbed radiative energy in a simplified diffusive one-dimensional
Energy Balance Model. We introduce a hybrid co-albedo nonlinear term, which has
the advantages of both the Sellers model, as it is a continuous function, and
the Budyko model, as it has an infinite derivative at $u=-10^C$ (the
temperature at which ice is white), allowing the location of the polar ice caps
to be easily detected. We show that, despite the lack of differentiability of
this function, the method of successive approximations can be satisfactorily
applied.

</details>


### [39] [Necessary and Sufficient Conditions for the Maz'ya-Shaposhnikova Formula in (Fractional) Sobolev Spaces](https://arxiv.org/abs/2509.23226)
*Elisa Davoli,Giovanni Di Fratta,Rossella Giorgio,Andrea Pinamonti*

Main category: math.AP

TL;DR: The paper establishes necessary and sufficient conditions for the convergence of nonlocal functionals to local norms as the interaction scale vanishes, generalizing the Maz'ya-Shaposhnikova theorem to broader kernel classes.


<details>
  <summary>Details</summary>
Motivation: To determine the weakest moment-type assumptions on nonlocal kernels that ensure convergence of nonlocal functionals to local L^p norms, extending classical results to non-symmetric and non-homogeneous kernels.

Method: Analyzes asymptotic behavior of nonlocal functionals using mass-escape conditions and short-range attenuation effects, employing density arguments to extend results from smooth functions to Sobolev spaces.

Result: Convergence occurs when kernels satisfy two optimal conditions: mass-escape and vanishing p-moments near origin, valid for both integer-order and fractional Sobolev spaces.

Conclusion: The framework recovers and extends the Maz'ya-Shaposhnikova theorem, providing necessary and sufficient conditions for convergence that apply to general kernel families beyond the classical fractional case.

Abstract: We investigate the asymptotic behavior, as $\varepsilon \to 0$, of nonlocal
functionals $$ \mathcal{F}_{\varepsilon}(u) =
\iint_{\mathbb{R}^N\times\mathbb{R}^N}
  \rho_{\varepsilon}(y-x)\,|u(x)-u(y)|^p\,dx\,dy,\qquad u\in
L^p(\mathbb{R}^N),\quad 1\leqslant p<\infty, $$ associated with a general
family of nonnegative measurable kernels
$\{\rho_{\varepsilon}\}_{\varepsilon>0}$. Our primary aim is to single out the
weakest moment-type assumptions on the family
$\{\rho_{\varepsilon}\}_{\varepsilon>0}$ that are necessary and sufficient for
the pointwise convergence $$ \lim_{\varepsilon\to
0}\mathcal{F}_{\varepsilon}(u)=2\|u\|_{L^p}^p $$ to hold for every $u$ in a
prescribed subspace of $L^p(\mathbb{R}^N)$. In the canonical smooth regime of
compactly supported functions ($u\in C_c^{\infty}(\mathbb{R}^N)$) we show that
convergence occurs when two optimal conditions are satisfied: (i) a mass-escape
condition, and (ii) a short-range attenuation effect, expressed by the
vanishing as $\varepsilon\to 0$ of the kernels' $p$-moments in any fixed
neighborhood of the origin. This general framework recovers the classical
Maz'ya--Shaposhnikova theorem for fractional-type kernels and extends the
convergence result to a much broader class of interaction profiles, which may
be non-symmetric and non-homogeneous. Using a density argument that preserves
the moment assumptions, we prove that the same necessary and sufficient
conditions remain valid in the integer-order Sobolev setting ($u\in
W^{1,p}(\mathbb{R}^N)$). Finally, by adapting the method to fractional Sobolev
spaces $W^{s,p}(\mathbb{R}^N)$ with $s\in(0,1)$, we recover the
Maz'ya-Shaposhnikova formula and extend it under analogous abstract conditions
on the family $\{\rho_{\varepsilon}\}_{\varepsilon>0}$.

</details>


### [40] [Global energy minimizers for a diffusion-aggregation model on sphere](https://arxiv.org/abs/2509.23227)
*Razvan C. Fetecau,Hansol Park,Vishnu Vaidya*

Main category: math.AP

TL;DR: Study of ground states and equilibria for a free energy functional on sphere with competing entropy (nonlinear diffusion) and nonlocal interaction terms, analyzing transitions based on interaction strength and diffusion exponent ranges.


<details>
  <summary>Details</summary>
Motivation: To understand phase transitions in systems with competing spreading (entropy) and aggregation (nonlocal interaction) forces, particularly generalizing the Onsager free energy with dipolar potential used in polymer orientation studies.

Method: Analysis of a free energy functional consisting of entropy (nonlinear diffusion) and quadratic nonlocal interaction potential on sphere, examining transitions in equilibria and global minimizers as interaction strength varies across different diffusion exponent ranges.

Result: The study reveals qualitatively different behaviors of equilibria and ground states depending on the diffusion exponent range, with transitions occurring as the strength of nonlocal attractive interactions changes.

Conclusion: The free energy functional exhibits rich transition behavior between spreading and aggregation states, with diffusion exponent playing a crucial role in determining the qualitative nature of equilibria and ground states.

Abstract: We investigate the ground states of a free energy functional on sphere. The
energy consists of an entropy and a nonlocal interaction term that are in
competition with each other, as they favour spreading and aggregation,
respectively. Specifically, the entropy corresponds to slow nonlinear diffusion
and the interaction term is modeled by a quadratic interaction potential. We
investigate the transitions that occur in the equilibria and the global
minimizers of the energy, in terms of the strength of the nonlocal attractive
interactions. We consider separately various ranges of the diffusion exponent,
which give qualitatively different behaviours of equilibria and ground states.
In terms of applications, we note that the energy we consider here is a
generalization to nonlinear diffusion of the Onsager free energy with dipolar
potential, used to study phase transitions in polymer orientation.

</details>


### [41] [Analytic smoothing effect for a model of Kolmogorov-Prandtl type equations](https://arxiv.org/abs/2509.23272)
*Xiao-Dong Cao,Chao-Jiang Xu,Yan Xu*

Main category: math.AP

TL;DR: The paper analyzes the Cauchy problem for Kolmogorov-Prandtl type equations, proving existence of unique classical solutions for small perturbations of constant states with Sobolev initial data, and showing analyticity for positive times.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for Kolmogorov-Prandtl type equations by proving well-posedness and regularity properties of solutions.

Method: Analysis of the Cauchy problem for Kolmogorov-Prandtl equations using Sobolev space theory and analytic function methods.

Result: Proves existence and uniqueness of classical solutions for small perturbations of constant states with Sobolev initial data, and demonstrates that solutions become analytic for any positive time.

Conclusion: The Kolmogorov-Prandtl type equations are well-posed for small perturbations and exhibit enhanced regularity (analyticity) over time, providing important mathematical insights into these fluid dynamics models.

Abstract: In this note, we consider the Cauchy problem for a model of
Kolmogorov-Prandtl type equations. For the small perturbation of a constant
state, we prove that it admits a unique classical solution with the initial
datum in Sobolev space, and it is analytic for any positive times.

</details>


### [42] [Large Deflections of A Flow-Driven Cantilever with Kutta-Joukowski Flow Conditions](https://arxiv.org/abs/2509.23306)
*Maria Deliyianni,Irena Lasiecka,Justin T. Webster*

Main category: math.AP

TL;DR: Analysis of a flow-structure system modeling airflow over a cantilevered beam with nonlinear dynamics, including semigroup theory for linearization and local-in-time strong solutions for the full nonlinear system.


<details>
  <summary>Details</summary>
Motivation: Study flow-beam interactions relevant to flight systems and piezoelectric energy harvesters, addressing challenges like unbounded flow domain, lack of interface trace regularity, and dynamic boundary conditions.

Method: Linearize the model and construct a viable semigroup with flow regularity theory, then use semigroup perturbation with higher regularity nonlinear beam estimates.

Result: Successfully produced a semigroup for the linearized model and obtained local-in-time strong solutions for the nonlinear dynamics.

Conclusion: The approach provides a rigorous mathematical framework for analyzing complex flow-structure interactions with nonlinear beam dynamics and dynamic boundary conditions.

Abstract: We consider a canonical flow-structure system modeling airflow over a
cantilevered beam. Flow-beam interactions arise in flight systems as well as
alternative energy technologies, such as piezoelectric energy harvesters. A
potential flow, given through a hyperbolic equation, captures the airflow
interacting with a beam clamped on one end and free on the other. The dynamic
coupling occurs through an impermeability condition across the beam; in the
wake the Kutta-Joukowski flow condition is imposed. Several challenges arise in
the analysis, including the unboundedness of the flow domain, lack of interface
trace regularity, and flow conditions which give rise to a dynamic and mixed
boundary value problem. Additionally, we consider a recent nonlinear model
capturing the cantilever large deflections through the effects of
inextensibility. We produce a viable underlying semigroup for the model's
linearization, which includes a flow regularity theory. Then, exploiting higher
regularity nonlinear estimates for the beam, we utilize a semigroup
perturbation to obtain local-in-time strong solutions for the nonlinear
dynamics.

</details>


### [43] [Existence and numerical approximation of solutions of a Schrödinger equation with derivative in the nonlinear term](https://arxiv.org/abs/2509.23414)
*Juan Carlos Muñoz Grajales,Deissy Marcela Pizo*

Main category: math.AP

TL;DR: Study of a Schrödinger-type equation with derivative nonlinearity and diffusion effects, including local well-posedness analysis, Fourier spectral numerical scheme, and asymptotic behavior investigation.


<details>
  <summary>Details</summary>
Motivation: The equation models physical phenomena like low-order magnetization in ferromagnetic nanocables and collision of ferromagnetic solitons in weakly ferromagnetic media.

Method: Established local well-posedness for the Cauchy problem, analyzed convergence and error order of Fourier spectral numerical scheme, and investigated asymptotic behavior through analytical and numerical approaches.

Result: Developed theoretical framework for local well-posedness and provided numerical scheme with convergence analysis for periodic solutions.

Conclusion: The paper provides comprehensive analysis of the Schrödinger-type equation with derivative nonlinearity, combining theoretical well-posedness results with practical numerical methods for solution approximation.

Abstract: In this paper, we study a Schr\"odinger-type equation featuring a derivative
in the nonlinear term and incorporating diffusion effects. This type of
equation arises in various physical applications, such as modeling low-order
magnetization in ferromagnetic nanocables and describing the collision of
ferromagnetic solitons in weakly ferromagnetic media. We establish a local
well-posedness result for the Cauchy problem associated with this model and
analyze the convergence and error order of a Fourier spectral numerical scheme
for approximating its solutions in the periodic setting. Additionally, we
investigate the behavior of solutions in certain asymptotic regimes, both
analytically and through numerical experiments, by examining limiting cases of
the model's parameters.

</details>


### [44] [Dispersive equations with invariant measures](https://arxiv.org/abs/2509.23466)
*Nicola Garofalo*

Main category: math.AP

TL;DR: Study of Schrödinger equations with friction and invariant measures, focusing on the Cauchy problem with the Ornstein-Uhlenbeck operator as a model for new degenerate dispersive equations.


<details>
  <summary>Details</summary>
Motivation: To stimulate interest in a new class of possibly degenerate dispersive equations that cannot be treated by existing mathematical physics theory.

Method: Analysis of the Cauchy problem for the Schrödinger equation with the Ornstein-Uhlenbeck operator as a model case.

Result: The paper presents a model equation that highlights limitations in current theory for degenerate dispersive equations.

Conclusion: The Ornstein-Uhlenbeck Schrödinger equation serves as an important model case that reveals gaps in existing mathematical framework for degenerate dispersive PDEs.

Abstract: In mathematical physics it is of interest to study Schr\"odinger equations
with friction and possessing an invariant measure. The focus of this paper is
the Cauchy problem for the Schr\"odinger equation $\p_t f - i \mathscr L f =
0$, where $\mathscr L = \Delta - \sa x,\nabla\da$ is the Ornstein-Uhlenbeck
operator. We use this as a model to stimulate interest in a new class of
possibly degenerate dispersive equations which cannot be treated by the
existing theory.

</details>


### [45] [On the Interplay Between Hodge Projections and Bounded Harmonic Functions on Manifolds with Ends](https://arxiv.org/abs/2509.23478)
*Dangyang He,Adam Sikora*

Main category: math.AP

TL;DR: Study of L^p-boundedness of Hodge projection on manifolds with ends, focusing on connections to Riesz transform, bounded harmonic functions, and L^2 harmonic one-forms.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between Hodge projection boundedness and harmonic analysis on manifolds with ends, particularly connections to Riesz transforms and harmonic function spaces.

Method: Mathematical analysis of Hodge projection properties in the context of manifolds with ends, examining connections to Riesz transform and harmonic function theory.

Result: Established connections between L^p-boundedness of Hodge projection and the structure of L^2 harmonic one-forms, which relates to bounded harmonic functions.

Conclusion: The boundedness properties of Hodge projection are fundamentally linked to harmonic analysis structures on manifolds with ends, particularly through harmonic one-forms and bounded harmonic functions.

Abstract: We investigate the $L^p$-boundedness of the Hodge projection in the setting
of manifolds with ends. We examine its relationship to the Riesz transform and
the space of bounded harmonic functions. In particular, we explore how the
$L^p$-boundedness of the Hodge projection is connected to the structure of
$L^2$ harmonic one-forms and, subsequently, to the space of bounded harmonic
functions.

</details>


### [46] [Unifying local and nonlocal corrosion frameworks: A convergent nonlocal extension of the KKS phase-field model](https://arxiv.org/abs/2509.23503)
*Christian J. Cyron,Marvin Fritz,Alexander Hermann,Tobias Köppl,Arman Shojaei,Stewart Silling*

Main category: math.AP

TL;DR: A nonlocal extension of the KKS phase-field corrosion model that bridges local and nonlocal approaches by replacing gradient operators with integral operators over a finite interaction horizon.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between local phase-field models and nonlocal modeling approaches like peridynamics, creating a unified framework that incorporates intrinsic length scales.

Method: Replace classical gradient operators with integral operators over finite interaction horizon, define nonlocal free energy, derive evolution equations (nonlocal Allen-Cahn and Cahn-Hilliard equations), establish well-posedness using Galerkin approximations and energy estimates.

Result: Successfully developed a nonlocal KKS model with proven well-posedness and convergence to local counterpart as interaction horizon approaches zero. Numerical experiments demonstrate nonlocality effects and confirm theoretical convergence.

Conclusion: The nonlocal extension effectively unifies local and nonlocal corrosion modeling perspectives, providing a rigorous mathematical framework that bridges phase-field and peridynamic approaches while maintaining convergence to classical models.

Abstract: We introduce a nonlocal extension of the Kim-Kim-Suzuki (KKS) phase-field
corrosion model aimed at bridging local and nonlocal corrosion modeling
approaches, such as phase-field and peridynamic frameworks. In this
formulation, classical gradient operators are replaced with integral operators
defined over a finite interaction horizon, naturally embedding an intrinsic
length scale that aligns with nonlocal theories like peridynamics. Under
precise assumptions on function spaces and kernel functions, we define a
nonlocal free energy that integrates a standard bulk free energy density with a
nonlocal interaction term. Through differentiation in an appropriate Hilbert
space, we derive evolution equations, yielding a nonlocal Allen--Cahn equation
for the phase-field and a nonlocal Cahn--Hilliard-type equation for the
concentration. The latter is expressed as a gradient flow in a metric induced
by the inverse of the nonlocal operator, mirroring the classical \(H^{-1}\)
metric for conserved dynamics. We establish the well-posedness of these
equations using Galerkin approximations, uniform energy estimates, and
compactness arguments. Furthermore, we prove the convergence of the nonlocal
model to its local KKS counterpart as the interaction horizon approaches zero,
effectively unifying local and nonlocal perspectives. Numerical experiments,
implemented via finite difference spatial discretization and explicit
time-stepping, demonstrate the effects of nonlocality and confirm the
theoretical convergence, reinforcing the connection between the two modeling
paradigms.

</details>


### [47] [Global dynamics of damped Euler systems with exterior potentials](https://arxiv.org/abs/2509.23538)
*Young-Pil Choi,Houzhi Tang,Weiyuan Zou*

Main category: math.AP

TL;DR: Global well-posedness for 3D isothermal Euler equations with damping: large damping allows arbitrary initial data, small data yields algebraic decay, and pressureless case shows finite-time blowup with insufficient damping.


<details>
  <summary>Details</summary>
Motivation: To understand the global behavior and long-time dynamics of the three-dimensional isothermal Euler equations with linear damping and exterior potential, particularly the effects of damping strength and pressure on solution behavior.

Method: Combined parabolic comparison principle with scaled high-order energy estimates for large damping; used spectral analysis and frequency decomposition for small data; constructed weighted functional for pressureless case.

Result: For large damping: global well-posedness for arbitrary initial data with uniform density bounds. For small data: global classical solutions with sharp algebraic decay rates. For pressureless case: finite-time blowup possible with insufficient damping.

Conclusion: Damping strength and pressure presence critically affect solution behavior - large damping ensures global existence, while pressureless systems can blow up with insufficient damping, revealing fundamental differences between pressured and pressureless cases.

Abstract: We study the three-dimensional isothermal Euler equations with linear damping
and an exterior potential. For sufficiently large damping, we prove global
well-posedness for arbitrarily large initial data by combining a parabolic
comparison principle with scaled high-order energy estimates ensuring uniform
density bounds. In the small-data regime with arbitrary damping, we establish
global classical solutions and derive sharp algebraic decay rates via spectral
analysis and frequency decomposition, and further prove their optimality under
a mild non-degeneracy condition. Finally, for the pressureless damped system,
we construct a weighted functional showing that solutions can blow up in finite
time when the damping is insufficient, highlighting a qualitative difference
from the pressured case.

</details>


### [48] [Wave packet decompositions and sharp bilinear estimates for rough Hamiltonian flows](https://arxiv.org/abs/2509.23551)
*Robert Schippa,Daniel Tataru*

Main category: math.AP

TL;DR: Proves bilinear L^p estimates for rough dispersive evolutions with non-degeneracy and transversality assumptions, generalizing sharp Fourier extension estimates for cone and paraboloid.


<details>
  <summary>Details</summary>
Motivation: To extend sharp Fourier extension estimates from classical cases (cone and paraboloid) to more general rough dispersive evolutions satisfying non-degeneracy and transversality conditions.

Method: Uses wave packet decomposition with space-time and space-time frequency localization, and constructs refined wave packet parametrix for dispersive equations with C^{1,1}-coefficients using FBI transform.

Result: Obtains bilinear estimates for solutions to dispersive equations with C^{1,1} coefficients when solutions interact transversely.

Conclusion: Successfully generalizes Fourier extension theory to rough dispersive evolutions through wave packet methods and FBI transform techniques.

Abstract: The goal of this paper is to prove bilinear $L^p$ estimates for rough
dispersive evolutions satisfying non-degeneracy and transversality assumptions.
The estimates generalize the sharp Fourier extension estimates for the cone and
the paraboloid. To this end, we require a wave packet decomposition with
localization properties in space-time and space-time frequencies. Secondly, we
construct a refined wave packet parametrix for dispersive equations with
$C^{1,1}$-coefficients by using the FBI transform. As a consequence, we obtain
bilinear estimates for solutions to dispersive equations with $C^{1,1}$
coefficients provided that the solutions interact transversely.

</details>


### [49] [Pullback $V$-Attractors of the Stochastic Calmed 3$D$ Navier-Stokes Equations](https://arxiv.org/abs/2509.23553)
*Yawen Duan,Anhui Gu*

Main category: math.AP

TL;DR: The paper studies a calmed version of 3D rotational Navier-Stokes equations with additive noise, establishing global well-posedness and proving existence of pullback attractors.


<details>
  <summary>Details</summary>
Motivation: To investigate stochastic 3D rotational Navier-Stokes equations with additive noise and establish the existence of pullback attractors for the calmed system.

Method: Uses Ornstein-Uhlenbeck process to transform the equation into a random system, applies Galerkin approximation for global well-posedness, and proves pullback flattening property for attractor existence.

Result: Established global well-posedness of solutions for the calmed system, demonstrated existence of closed measurable pullback absorbing set, and obtained existence of pullback attractor in V.

Conclusion: The calmed 3D rotational Navier-Stokes equations with additive noise possess a pullback attractor, providing important insights into the long-term behavior of stochastic fluid systems.

Abstract: In this paper, we investigate a calmed version of the 3$D$ rotational
Navier-Stokes equations driven by additive noise. First, we use the
Ornstein-Uhlenbeck process to transform the equation into a random one. By
using the Galerkin approximation, we establish the global well-posedness of
solutions for the calmed system. Then, we demonstrate the existence of a
closed, measurable $\mathcal{D}_V$-pullback absorbing set. Finally, by proving
the pullback flattening property, we obtain the existence of a
$\mathcal{D}_V$-pullback attractor in \(V\).

</details>


### [50] [Infinite-peakon solutions of the Camassa-Holm equation](https://arxiv.org/abs/2509.23826)
*Xiang-Ke Chang,Jonathan Eckhardt,Aleksey Kostenko*

Main category: math.AP

TL;DR: The paper develops an inverse spectral transform method for low regularity solutions of the Camassa-Holm equation using the classical moment problem and generalized indefinite strings, focusing on solutions with infinitely many peakons and analyzing both determinate and indeterminate moment problem cases.


<details>
  <summary>Details</summary>
Motivation: To study conservative low regularity solutions to the Camassa-Holm equation, particularly those involving infinitely many peaked solitons (peakons), and develop a systematic approach using inverse spectral methods.

Method: Exploits the classical moment problem in the framework of generalized indefinite strings to develop the inverse spectral transform method, identifying solutions amenable to this approach and providing explicit formulas.

Result: Shows that solutions can be completely recovered when the moment problem is determinate (as infinite superpositions of peakons), and explores indeterminate cases. Applies results to analyze long-time behavior for three types of initial data.

Conclusion: The inverse spectral transform method provides a powerful framework for analyzing Camassa-Holm equation solutions, particularly peakon configurations, with applications to understanding long-time behavior across different spectral scenarios.

Abstract: We study a class of (conservative) low regularity solutions to the
Camassa-Holm equation on the line by exploiting the classical moment problem
(in the framework of generalized indefinite strings) to develop the inverse
spectral transform method. In particular, we identify explicitly the solutions
that are amenable to this approach, which include solutions made up of
infinitely many peaked solitons (peakons). We determine which part of the
solution can be recovered from the moments of the underlying spectral measure
and provide explicit formulas. We show that the solution can be recovered
completely if the corresponding moment problem is determinate, in which case
the solution is a (potentially infinite) superposition of peakons. However, we
also explore the situation when the underlying moment problem is indeterminate.
  As an application, our results are then used to investigate the long-time
behavior of solutions. We will demonstrate this on three exemplary cases of
solutions with: (i) discrete underlying spectrum (one may choose for this
initial data corresponding to Al-Salam-Carlitz II polynomials; notice that they
correspond to an indeterminate moment problem); (ii) step-like initial data
associated with the Laguerre polynomials, and (iii) asymptotically eventually
periodic initial data associated with the Jacobi polynomials.

</details>


### [51] [Observability of Schrödinger propagators on tori in rough settings](https://arxiv.org/abs/2509.23965)
*Nicolas Burq,Hui Zhu*

Main category: math.AP

TL;DR: The paper establishes a criterion that proves the observability conjecture for Schrödinger propagators on the 1D torus, provides new measurable observation sets, and reduces the general conjecture to verifying weaker integrability bounds for free Schrödinger waves.


<details>
  <summary>Details</summary>
Motivation: To address the conjecture that Schrödinger propagators with bounded potentials on arbitrary dimensional tori are observable from any space-time set of positive Lebesgue measure.

Method: Develops a criterion that proves the conjecture for the 1D torus and reduces the general case to verifying certain integrability bounds for free Schrödinger waves, which are weaker than Bourgain's conjectured periodic Strichartz estimates.

Result: Successfully proves the observability conjecture for the 1D torus, identifies new examples of measurable observation sets, and provides a reduction framework for the general case.

Conclusion: The work makes significant progress on the observability conjecture by establishing a criterion that works for the 1D case and reduces the general problem to verifying weaker integrability bounds, offering a promising approach for future research.

Abstract: On a torus of arbitrary dimension, it is conjectured that Schr\"odinger
propagators with bounded potentials are observable from any space-time set of
positive Lebesgue measure. We establish a criterion that proves the conjecture
on the one-dimensional torus, yields new examples of measurable observation
sets, and, more significantly, reduces the general conjecture to verifying
certain integrability bounds for free Schr\"odinger waves. These bounds are far
weaker than Bourgain's conjectured periodic Strichartz estimates, yet remain
highly nontrivial.

</details>


### [52] [Strong stability and the Schiffer Conjecture for the fluid-elastic semigroup](https://arxiv.org/abs/2509.23989)
*Karoline Disser*

Main category: math.AP

TL;DR: Extension of Avalos-Triggiani's fluid-elastic semigroup analysis: decomposes dynamics into stable part and pressure wave for 'bad domains', shows bad domain characterization is equivalent to Schiffer problem, suggesting balls are the only bad domains.


<details>
  <summary>Details</summary>
Motivation: To extend previous results on coupled Stokes-Lamé system by analyzing dynamics for 'bad domains' where strong stability fails, and connect domain characterization to geometric analysis problems.

Method: Prove decomposition of dynamics into strongly stable part and pressure wave solution for bad domains, establish equivalence between bad domain characterization and Schiffer problem.

Result: Complete characterization of long-time behavior for all domains, with bad domains exhibiting pressure wave component; equivalence to Schiffer problem strengthens conjecture that only balls are bad domains.

Conclusion: The analysis fully characterizes semigroup behavior, connects fluid-structure interaction to geometric analysis, and provides insights for nonlinear systems.

Abstract: In a series of papers, Avalos and Triggiani established the fluid-elastic
semigroup for the coupled Stokes-Lam\'e system modelling the coupled dynamics
of a linearly elastic structure immersed in a viscous Newtonian fluid. They
analyzed the spectrum of its generator and proved that the semigroup is
strongly stable, if the domain of the structure satisfies a geometric
condition, i.e. it is not a bad domain. We extend these results in two
directions: first, for bad domains, we prove a decomposition of the dynamics
into a strongly stable part and a pressure wave, a special solution of the
Dirichlet-Lam\'e system, that can be determined from the initial values. This
fully characterizes the long-time behaviour of the semigroup. Secondly, we show
that the characterization of bad domains is equivalent to the Schiffer problem.
This strengthens the conjecture that balls are the only bad domains and
establishes a direct connection to geometric analysis. We also discuss
implications for associated nonlinear systems.

</details>


### [53] [Global Analysis of the {Gray--Scott} Model with Fractional--Classical Diffusion](https://arxiv.org/abs/2509.24019)
*Md Shah Alam*

Main category: math.AP

TL;DR: Global existence and uniform bounds for nonnegative solutions of Gray-Scott reaction-diffusion system with mixed local-nonlocal diffusion operators.


<details>
  <summary>Details</summary>
Motivation: To understand pattern formation in reaction-diffusion systems with combined local and nonlocal diffusion operators, which exhibit different qualitative behavior from purely local models.

Method: Using semigroup methods and duality estimates to analyze the mixed diffusion system on domains in R^n.

Result: Proved global existence of componentwise nonnegative solutions and established uniform-in-time bounds for the system.

Conclusion: Mixed diffusion models show different pattern formation characteristics compared to purely local diffusion, as demonstrated through numerical simulations.

Abstract: We analyze the Gray--Scott reaction--diffusion system on
$\Omega\subset\mathbb{R}^n$ ($n\ge 1$) with mixed diffusion combining local and
nonlocal operators. Using semigroup methods and duality estimates, we prove
global existence of componentwise nonnegative solutions and establish
uniform-in-time bounds. Numerical simulations illustrate pattern formation and
highlight qualitative differences between the purely local and mixed-diffusion
models.

</details>


### [54] [Application of Onsager's Variational Principle to the Navier-Stokes Equations](https://arxiv.org/abs/2509.24033)
*Hamid A. Said*

Main category: math.AP

TL;DR: Proposes an L²-based approach for studying energy equalities in 3D Navier-Stokes equations, with a new sufficient condition involving limits of minimizer sequences.


<details>
  <summary>Details</summary>
Motivation: Motivated by Onsager's principle of least dissipation of energy (1931) to study global and local energy equalities.

Method: L²-based approach using sequences of minimizers to establish sufficient conditions for energy equalities.

Result: Energy equalities are attained if the limit of the minimizer sequence is non-vanishing; vanishing case shows turbulence-driven energy transfer dominance.

Conclusion: The approach provides new insights into energy conservation in Navier-Stokes equations, particularly highlighting turbulence effects in the vanishing case.

Abstract: In this note we propose a basic $L^2$-based approach for studying the global
and local energy equalities of the incompressible 3D Navier-Stokes equations in
the standard energy class on $\mathbb{T}^3 \times (0,T]$. Motivated by L.
Onsager's principle of least dissipation of energy (1931), we give a new
sufficient condition for the energy equalities in terms of the limit of a
sequence of minimizers. In particular, we show that the equalities are attained
if this limit is non-vanishing. We observe that the indeterminacy in the
vanishing case is reminiscent of turbulence-driven energy transfer dominating
other transport processes.

</details>


### [55] [A new minimax principle and application to the p-Laplace equation](https://arxiv.org/abs/2509.24268)
*Xu-Jia Wang,Xinyue Zhang*

Main category: math.AP

TL;DR: A new minimax principle is introduced to prove existence of multi-peak solutions for the p-Laplace Neumann problem with nonlinearity u^(q-1) - u^(p-1) in bounded domains.


<details>
  <summary>Details</summary>
Motivation: To establish existence of multi-peak solutions for the p-Laplace equation with Neumann boundary conditions, which are solutions with multiple concentration points.

Method: Developed a new minimax principle applied to peak functions in Sobolev space W^{1,p}(Ω), combining variational methods, topological degree theory, and gradient flow techniques.

Result: Successfully proved the existence of multi-peak solutions for the p-Laplace Neumann problem with the specified parameter range 1<p<n and p<q<np/(n-p).

Conclusion: The introduced minimax principle provides an effective framework for proving existence of multi-peak solutions in nonlinear elliptic equations with Neumann boundary conditions.

Abstract: We introduce a new minimax principle to prove the existence of multi-peak
solutions to the Neumann problem of the $p$-Laplace equation $$ -\varepsilon^p
\Delta_p u = u^{q-1} - u^{p-1} \ \ \text{in}\ \Omega,$$ where $\Om$ is a
bounded domain in $\mathbb{R}^n$ with smooth boundary, $1<p<n$ and $p<q<
\frac{np}{n-p}$. The minimax principle will be applied to the set of peak
functions, which is a subset of the Sobolev space $W^{1,p} (\Omega)$. The
argument is based on a combination of variational method, topological degree
theory, and gradient flow.

</details>


### [56] [Finite number of traces for Mumford-Shah minimizers in dimension 2](https://arxiv.org/abs/2509.24426)
*Camille Labourie,Antoine Lemenant*

Main category: math.AP

TL;DR: A Mumford-Shah minimizer in 2D can only have three maximum limit values near the singular set.


<details>
  <summary>Details</summary>
Motivation: To answer a question raised by E. De Giorgi about the behavior of Mumford-Shah minimizers near singular points.

Method: Using tools developed in the early 2000's by G. David, A. Bonnet, and J.-C. Léger.

Result: Proved that in dimension 2, a Mumford-Shah minimizer can only admit three maximum limit values when approaching the singular set.

Conclusion: The paper resolves De Giorgi's question by establishing the three-value limit property for 2D Mumford-Shah minimizers.

Abstract: In this short note, we answer a question raised by E. De Giorgi, showing that
a Mumford-Shah minimizer in dimension 2 can only admit three maximum limit
values as approaching the singular set. This result stems from tools developed
in the early 2000's by G. David, A. Bonnet, and J.-C. L\'eger.

</details>


### [57] [Harnack inequality for a class of degenerate fully nonlinear pseudo-$p$-Laplacian equations](https://arxiv.org/abs/2509.24442)
*Sun-Sig Byun,Hongsoo Kim*

Main category: math.AP

TL;DR: Interior Hölder regularity and Harnack inequalities for viscosity solutions of degenerate fully nonlinear pseudo-p-Laplacian equations in nondivergence form.


<details>
  <summary>Details</summary>
Motivation: To establish regularity results for a class of degenerate fully nonlinear equations that exhibit coordinatewise degeneracy, which are important in nonlinear PDE theory.

Method: Adaptation of the sliding paraboloid method with anisotropic functions specifically designed to handle the coordinatewise degeneracy structure.

Result: Successfully proved interior Hölder regularity and Harnack inequalities for viscosity solutions of these degenerate equations.

Conclusion: The sliding paraboloid method with anisotropic functions provides an effective approach for analyzing regularity properties of degenerate fully nonlinear pseudo-p-Laplacian equations in nondivergence form.

Abstract: We prove interior H\"older regularity and Harnack inequalities for viscosity
solutions of a class of degenerate fully nonlinear pseudo-$p$-Laplacian
equations in nondivergence form. Our main approach is an adaptation of the
sliding paraboloid method with anisotropic functions tailored to the
coordinatewise degeneracy.

</details>


### [58] [Existence, Nonexistence, and Symmetry of Positive Solutions for Fractional Laplacian Problems](https://arxiv.org/abs/2509.24454)
*Haipeng Lu,Mei Yu*

Main category: math.AP

TL;DR: Existence and properties of solutions for fractional Laplacian problems using mountain pass theorem and moving planes method, with symmetry results and nonexistence in supercritical cases.


<details>
  <summary>Details</summary>
Motivation: To study the properties of solutions to elliptic and parabolic problems involving the fractional Laplacian, particularly focusing on existence, symmetry, and asymptotic behavior.

Method: Applied mountain pass theorem to prove existence of bounded classical positive solutions in subcritical regime, and used method of moving planes to establish symmetry/monotonicity properties.

Result: Proved existence of bounded classical positive solutions in subcritical regime, established their symmetry/monotonicity in first variable, showed nonexistence in supercritical/negative exponent cases, analyzed asymptotic behavior at infinity.

Conclusion: The developed approaches are extendable to broader classes of nonlocal elliptic and parabolic equations with general operators and nonlinearities, providing insights for real-world applications.

Abstract: This paper studies the properties of solutions to a class of elliptic and
parabolic problems involving the fractional Laplacian. By applying the mountain
pass theorem, we prove the existence of bounded classical positive solutions in
the subcritical regime. Moreover, using the method of moving planes, we
establish that these solutions are symmetric or monotone in the first variable.
In contrast, we show that no such solutions exist in the supercritical or
negative exponent cases. An analysis of the asymptotic behavior of solutions at
infinity provides further insight into their profiles, which supports
applications to real-world problems. The approaches developed in this work can
also be extended to a wider range of nonlocal elliptic and parabolic equations,
including those with more general operators and nonlinearities.

</details>


### [59] [Convergence of graph Dirichlet energies and graph Laplacians on intersecting manifolds of varying dimensions](https://arxiv.org/abs/2509.24458)
*Leon Bungert,Dejan Slepčev*

Main category: math.AP

TL;DR: The paper studies Γ-convergence of graph Dirichlet energies and spectral convergence of graph Laplacians on unions of intersecting manifolds with different dimensions, showing that normalized graph Laplacians adapt to all dimensions while unnormalized ones only capture the highest dimension.


<details>
  <summary>Details</summary>
Motivation: Real-world data often consists of parts or classes with different intrinsic dimensions, creating a challenge for machine learning methods to adapt to such varied dimensionalities.

Method: The authors investigate Γ-convergence of both unnormalized and normalized graph Dirichlet energies on unions of intersecting manifolds, and establish related spectral convergence of graph Laplacians.

Result: Unnormalized graph Dirichlet energy and its Laplacian only capture variations within the highest-dimensional manifold, while normalized Dirichlet energy converges to a tensorized Dirichlet energy that adapts to all dimensions simultaneously.

Conclusion: Normalized graph Laplacians are better suited for data with mixed intrinsic dimensions as they adapt to all dimensional components, unlike unnormalized Laplacians which focus only on the highest dimension.

Abstract: We study $\Gamma$-convergence of graph Dirichlet energies and spectral
convergence of graph Laplacians on unions of intersecting manifolds of
potentially different dimensions. Our investigation is motivated by problems of
machine learning, as real-world data often consist of parts or classes with
different intrinsic dimensions. An important challenge is to understand which
machine learning methods adapt to such varied dimensionalities. We investigate
the standard unnormalized and the normalized graph Dirichlet energies. We show
that the unnormalized energy and its associated graph Laplacian asymptotically
only sees the variations within the manifold of the highest dimension. On the
other hand, we prove that the normalized Dirichlet energy converges to a
(tensorized) Dirichlet energy on the union of manifolds that adapts to all
dimensions simultaneously. We also establish the related spectral convergence
and present a few numerical experiments to illustrate our findings.

</details>


### [60] [A Source Identification Problem for the Bi-Parabolic Equation Containing a Poly-harmonic Operator](https://arxiv.org/abs/2509.24470)
*Dang Duc Trong,Bui Thanh Duy,Nguyen Dang Minh*

Main category: math.AP

TL;DR: This paper addresses the source identification problem for bi-parabolic equations with poly-harmonic operators, aiming to determine f from perturbed data of ψ and u(T) using truncation regularization on unbounded domains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the source identification problem for bi-parabolic equations, which has limited existing literature despite being explored in some publications. The authors aim to provide valuable insights by relaxing conditions on ψ and working on unbounded domains.

Method: The method employs a truncation regularization technique to solve the inverse source problem for the bi-parabolic equation (∂t + 𝔄)²u(t) = ψ(t)f, where 𝔄 is a poly-harmonic operator. The approach works with perturbed data of ψ and u(T) on unbounded domains.

Result: The paper presents results for determining the source term f from perturbed measurements, demonstrating that their approach provides valuable insights into solving bi-parabolic source identification problems under relaxed conditions.

Conclusion: The authors conclude that their regularization method successfully addresses the source identification problem for bi-parabolic equations with poly-harmonic operators, offering new perspectives by working with relaxed conditions on ψ and considering unbounded domains.

Abstract: In this paper, we address the source identification problem for the
bi-parabolic equation involving a operator. Specifically, we investigate the
equation $(\partial_t +\frak A)^2u(t)=\psi(t)f$, where $\frak A$ denotes a
poly-harmonic operator. Given the perturbed data of $\psi$ and $u(T)$ (where
$T>0$), our objective is to determine $f$. Although several scientific
publications have explored regularization techniques for bi-parabolic problems,
the existing literature remains limited. By relaxing certain conditions on the
function $\psi$ and employing a truncation regularization method while
considering the problem on an unbounded domain, we believe our results provide
valuable insights.

</details>


### [61] [$H^\infty$-calculus for Stokes operators on rough and on unbounded domains](https://arxiv.org/abs/2509.24501)
*Peer Christian Kunstmann,Patrick Tolksdorf*

Main category: math.AP

TL;DR: Overview of boundedness results for H∞-calculus of Stokes operator in rough/unbounded domains using abstract comparison principle.


<details>
  <summary>Details</summary>
Motivation: To establish bounded H∞-calculus for Stokes operator in various domain types (rough, unbounded, bounded) with different boundary conditions.

Method: Uses abstract comparison principle by Kunstmann and Weis as basis, applies it to Stokes operator with no-slip boundary conditions in bounded Lipschitz domains and Neumann conditions in bounded convex domains.

Result: Achieved bounded H∞-calculus for Stokes operator: shorter proof for unbounded domains, new result for Neumann boundary conditions in bounded convex domains.

Conclusion: Abstract comparison principle provides unified approach for establishing bounded H∞-calculus of Stokes operator across different domain types and boundary conditions.

Abstract: In this article, we give an overview on known as well as new results on the
boundedness of the $H^{\infty}$-calculus of the Stokes operator in rough as
well as in unbounded (smoother) domains. We present a special case of an
abstract comparison principle due to Kunstmann and Weis
(\cite{KuW:Hinfty-Stokes}) that serves as the basis for all considerations.
Subsequently, we show how this result can be applied to arrive at a bounded
$H^{\infty}$-calculus for the Stokes operator. We sketch the proof for no slip
boundary conditions in bounded Lipschitz domains which was given
in~\cite{KuW:Hinfty-Stokes}. For unbounded domains this approach yields a
shorter proof compared to previous arguments. Moreover, we further establish
the boundedness of the $H^{\infty}$-calculus for the Stokes operator with
Neumann type boundary conditions in bounded convex domains which is entirely
new.

</details>


### [62] [On the Serrin's problem with Robin boundary conditions](https://arxiv.org/abs/2509.24557)
*Nunzia Gavitone,Riccardo Molinarolo*

Main category: math.AP

TL;DR: Serrin's rigidity result for Robin boundary conditions in torsion problems


<details>
  <summary>Details</summary>
Motivation: Study symmetry of solutions to torsion problems with Robin boundary conditions under specific domain and parameter assumptions

Method: Mathematical analysis of torsion problems with Robin boundary conditions, imposing extra conditions on domain boundary

Result: Proved Serrin's rigidity result under suitable assumptions on domain regularity and Robin parameter

Conclusion: Symmetry properties hold for solutions when appropriate boundary conditions and domain regularity are satisfied

Abstract: Let $\Omega \subset \mathbb{R}^N$, $N\ge 2$, be an open, connected, bounded
set with $C^2$ boundary. In this paper we consider the torsion problem with
Robin boundary conditions and we study the symmetry of the solutions when
suitable extra conditions are imposed on the boundary of $\Omega$. In
particular, we prove the Serrin's rigidity result under suitable assumptions on
the domain and on the Robin parameter.

</details>


### [63] [Quantitative regularity for minimizing intrinsic fractional harmonic maps](https://arxiv.org/abs/2509.24587)
*Y. -Y. Wang,C. -L. Xiang,G. -F. Zheng*

Main category: math.AP

TL;DR: The paper studies compactness and regularity theory of minimizing intrinsic fractional harmonic mappings, improving dimension estimates of singular sets.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive regularity theory for minimizing intrinsic fractional harmonic mappings introduced by Moser and Roberts.

Method: Uses modified Luckhaus lemma for compactness, quantitative stratification theory for volume estimates of singular sets, and combines these for global regularity estimates.

Result: Achieved improved dimension estimates of singular sets for minimizing intrinsic fractional harmonic mappings.

Conclusion: The combined approach of compactness and quantitative stratification theory successfully leads to enhanced regularity results and better understanding of singular sets.

Abstract: In this note, we study compactness and regularity theory of minimizing
intrinsic fractional harmonic mappings introduced by Moser and Roberts. Based
on the partial regularity theory of Moser and Roberts, we first use the
modified Luckhaus lemma of Roberts to deduce compactness of these mappings, and
then develop volume estimates of singular sets by the quantitative
stratification theory of Cheeger and Naber. Combining these two results lead to
a global regularity estimates which, in turn, allow us to obtain an improvement
of the dimension estimate of singular sets.

</details>


### [64] [On a Differential Model for Sandpiles Growing in a Silo](https://arxiv.org/abs/2509.24618)
*Graziano Crasta,Annalisa Malusa*

Main category: math.AP

TL;DR: Analysis of sandpile growth PDE system: characterizes long-term behavior, provides convergence conditions, shows finite-time convergence not guaranteed, and fully characterizes equilibrium profiles.


<details>
  <summary>Details</summary>
Motivation: To understand the mathematical properties of sandpile growth in containers under vertical sources, particularly focusing on convergence behavior and equilibrium states.

Method: Analysis of boundary value problem for PDE system describing sandpile growth, using mathematical characterization and counterexamples.

Result: Established sufficient conditions for finite-time convergence to equilibrium, demonstrated that stable configurations may not be reached in finite time even with time-independent sources, and provided complete characterization of equilibrium profiles.

Conclusion: The paper provides comprehensive mathematical understanding of sandpile growth dynamics, including convergence conditions and equilibrium characterization, with important implications for understanding granular material behavior.

Abstract: We discuss some features of a boundary value problem for a system of PDEs
that describes the growth of a sandpile in a container under the action of a
vertical source. In particular, we characterize the long-term behavior of the
profiles, and we provide a sufficient condition on the vertical source that
guarantees the convergence to the equilibrium in a finite time. We show by
counterexamples that a stable configuration may not be reached in a finite
time, in general, even if the source is time-independent. Finally, we provide a
complete characterization of the equilibrium profiles.

</details>


### [65] [Observability estimates for the Schrödinger equation on the equilateral triangle](https://arxiv.org/abs/2509.24642)
*Paul Alphonse,David Lafontaine*

Main category: math.AP

TL;DR: The paper proves observability estimates for the Schrödinger equation on an equilateral triangle with Neumann and Dirichlet boundary conditions, without requiring geometric control conditions on rough localization functions.


<details>
  <summary>Details</summary>
Motivation: This is the first result of its kind on a non-toric domain in the compact setting, addressing observability for the Schrödinger equation on geometric domains beyond toric ones.

Method: The strategy uses Pinsky's tiling argument to deduce results from observability estimates on rational twisted tori, employing propagation of singularities adapted from Burq and Zworski's arguments, and deriving Strichartz estimates from Zygmund inequalities.

Result: Observability estimates are established for both Neumann and Dirichlet boundary conditions on the equilateral triangle, and sharp constant Strichartz estimates are provided on rational twisted tori and the equilateral triangle.

Conclusion: The analysis successfully extends observability results to non-toric domains using tiling arguments and propagation techniques, providing new insights into Schrödinger equation behavior on geometric domains.

Abstract: We prove observability estimates for the Schr\"odinger equation posed on the
equilateral triangle in the plane, under both Neumann and Dirichlet boundary
conditions. No geometric control condition is required on the rough
localization functions that we consider. This is the first result of this kind
on a non-toric domain in the compact setting. Our strategy is to exploit
Pinsky's tiling argument to deduce this result from observability estimates on
rational twisted tori. These are obtained via propagation of singularities,
adapting arguments from Burq and Zworski. The later require Strichartz
estimates on such twisted rational tori, that we derive from Zygmund
inequalities in the same geometric setting, also providing the sharp constant.
Strichartz estimates on the equilateral triangle are also derived from this
analysis.

</details>


### [66] [A coarse-graining theory for elliptic operators and homogenization in high contrast](https://arxiv.org/abs/2509.24887)
*Scott Armstrong,Tuomo Kuusi*

Main category: math.AP

TL;DR: A coarse-graining theory for divergence-form elliptic operators is developed, showing homogenization occurs within C log²(1+Θ) scales in high-contrast regimes, with iterable ellipticity across scales.


<details>
  <summary>Details</summary>
Motivation: To develop a rigorous framework for analyzing divergence-form elliptic operators across multiple scales, particularly in high-contrast regimes where standard homogenization theory may not apply directly.

Method: Construction of coarse-grained matrices on spatial blocks that encode scale-dependent ellipticity, transmitting precise information across scales while maintaining elliptic estimates. The approach provides a renormalization group analysis framework.

Result: Under simplifying assumptions, homogenization is proven to occur within at most C log²(1+Θ) dyadic length scales in high-contrast regimes, where Θ is the ellipticity contrast.

Conclusion: The scale-local notion of ellipticity is genuinely iterable across arbitrarily many scales, establishing a rigorous foundation for renormalization group analysis in elliptic homogenization problems.

Abstract: We review a coarse-graining theory for divergence-form elliptic operators.
The construction centers on a pair of coarse-grained matrices defined on
spatial blocks that encode a scale-dependent notion of ellipticity, transmit
precise information from small to large scales, and yield coarse-grained
counterparts of standard elliptic estimates. Under simplifying assumptions, we
give a complete proof of the result of [arXiv:2405.10732] that homogenization
is reached within at most $C\log^2(1+\Theta)$ dyadic length scales in the
high-contrast regime, where $\Theta$ is the ellipticity contrast. We argue that
this scale-local notion of ellipticity is genuinely iterable across arbitrarily
many scales, providing a framework for a rigorous renormalization group
analysis.

</details>


### [67] [Traveling wave solutions for the generalized Burgers-Fisher equation](https://arxiv.org/abs/2509.24909)
*Razvan Gabriel Iagar,Ariel Sánchez*

Main category: math.AP

TL;DR: The paper classifies traveling wave solutions to the generalized Burgers-Fisher equation, establishing existence and uniqueness for all speeds c ∈ ℝ, and characterizes their asymptotic behavior at ±∞ based on parameter relationships.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze and classify traveling wave solutions of the generalized Burgers-Fisher equation, particularly focusing on how wave speed c and nonlinear parameters affect solution behavior and asymptotic properties.

Method: Mathematical analysis of traveling wave solutions in the form u(x,t)=f(x+ct), using techniques from partial differential equations and dynamical systems to study existence, uniqueness, and asymptotic behavior with respect to wave speed c and parameter relationships.

Result: Proved existence and uniqueness of traveling waves for all c ∈ ℝ. Identified a unique critical speed c* ∈ (0,∞) for soliton solutions. Established parameter-dependent relationships: c* < kn when n < p+q+1, c* > kn when n > p+q+1. Characterized asymptotic behavior based on speed ranges relative to c* and kn.

Conclusion: The generalized Burgers-Fisher equation exhibits rich traveling wave dynamics with speed-dependent asymptotic behavior. The convective term (k ≠ 0) enables traveling waves with u(x,t)→1 as t→∞ for speeds c ∈ (0,c*), distinguishing it from the non-convective case.

Abstract: Traveling wave solutions, in the form $u(x,t)=f(x+ct)$, to the generalized
Burgers-Fisher equation $$ \partial_tu=u_{xx}+k(u^n)_x+u^p-u^q, \quad
(x,t)\in\mathbb{R}\times(0,\infty), $$ with $n\geq2$, $p>q\geq1$ and $k>0$, are
classified with respect to their speed $c\in(-\infty,\infty)$ and the behavior
at $\pm\infty$. The existence and uniqueness of traveling waves with any speed
$c\in\mathbb{R}$ is established and their behavior as $x\to\pm\infty$ is
described. In particular, it is shown that there exists a unique
$c^*\in(0,\infty)$ such that there exists a unique soliton $f^*$ with speed
$c^*$ and such that $$
\lim\limits_{\xi\to-\infty}f^*(\xi)=\lim\limits_{\xi\to\infty}f^*(\xi)=0, \quad
\xi=x+ct. $$ Moreover, if $n<p+q+1$ then $c^*<kn$ and if $n>p+q+1$ then
$c^*>kn$. For $c<\min\{c^*,kn\}$, any traveling wave with speed $c$ satisfies
$\lim\limits_{\xi\to-\infty}f(\xi)=0$ and $\lim\limits_{\xi\to\infty}f(\xi)=1$,
while for $c>\max\{c^*,kn\}$ any traveling wave with speed $c$ satisfies
$\lim\limits_{\xi\to-\infty}f(\xi)=1$ and $\lim\limits_{\xi\to\infty}f(\xi)=0$.
In particular, for any speed $c\in(0,c^*)$, there are traveling wave solutions
$u$ with speed $c$ such that $u(x,t)\to1$ as $t\to\infty$, in contrast to the
non-convective case $k=0$.

</details>


### [68] [Sharp behavior of semilinear damped wave equations driven by mixed local-nonlocal operators](https://arxiv.org/abs/2509.24940)
*Wenhui Chen,Tuan Anh Dao*

Main category: math.AP

TL;DR: The paper studies a semilinear damped wave equation with mixed local-nonlocal operator, determining the critical exponent p_crit=1+2min{1,σ}/n that separates global existence from finite-time blow-up, and analyzes asymptotic profiles and lifespan estimates.


<details>
  <summary>Details</summary>
Motivation: To understand how mixed local-nonlocal operators affect the qualitative behavior of solutions to damped wave equations, particularly regarding critical phenomena, large-time behavior, and blow-up dynamics.

Method: The authors investigate the Cauchy problem for the semilinear damped wave equation with mixed operator L_{a,b}=-aΔ+b(-Δ)^σ, using mathematical analysis to determine critical exponents, establish asymptotic profiles, and derive lifespan estimates.

Result: Found critical exponent p_crit=1+2min{1,σ}/n; for p>p_crit, established anomalous diffusion (σ∈(0,1)) or classical diffusion (σ∈(1,∞)) asymptotic profiles; for 1<p≤p_crit, derived sharp lifespan bounds for blow-up solutions.

Conclusion: Mixed operators fundamentally govern solution behavior through max{1,σ} or min{1,σ}, revealing their crucial influence on critical phenomena, large-time behavior, and blow-up dynamics in damped wave equations.

Abstract: This paper investigates the Cauchy problem for the semilinear damped wave
equation $u_{tt}+\mathcal{L}_{a,b}u+u_t=|u|^p$ with the mixed local-nonlocal
operator $\mathcal{L}_{a,b}:=-a\Delta+b(-\Delta)^{\sigma}$, where
$a,b\in\mathbb{R}_+$ and $\sigma\in(0,1)\cup (1,+\infty)$. We determine the
critical exponent for this problem being
$p_{\mathrm{crit}}=1+\frac{2\min\{1,\sigma\}}{n}$, which sharply separates
global in-time existence and finite-time blow-up of solutions. Furthermore, for
the super-critical case $p>p_{\mathrm{crit}}$, we establish the asymptotic
profiles of global in-time solutions, showing the anomalous diffusion when
$\sigma\in(0,1)$ and the classical diffusion when $\sigma\in(1,+\infty)$,
together with the sharp decay estimates. For solutions blowing up in finite
time when $1<p\leqslant p_{\mathrm{crit}}$, we derive the sharp estimates for
upper and lower bounds of lifespan. Our results reveal the crucial influence of
mixed operators on the qualitative properties of solutions, fundamentally
governing their critical phenomena, large-time behavior and blow-up dynamics,
via $\max\{1,\sigma\}$ or $\min\{1,\sigma\}$.

</details>


### [69] [On domains of elliptic operators with distributional coefficients](https://arxiv.org/abs/2509.24950)
*Immanuel Zachhuber*

Main category: math.AP

TL;DR: The paper constructs domains for singular elliptic operators using paracontrolled distributions theory, establishing rigorous definitions and showing convergence of regularized operators.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of defining domains for singular elliptic operators with rough coefficients, leveraging recent advances in singular SPDEs and paracontrolled distributions.

Method: Uses paracontrolled distributions theory to construct continuous change of variables Θ that transforms the singular operator A into a more regular form, and analyzes convergence of regularized operators A_ε.

Result: Successfully constructs a domain for the singular elliptic operator A and proves norm resolvent convergence of regularized operators A_ε to the limiting closed operator.

Conclusion: The paracontrolled distributions framework provides a powerful method to rigorously define and analyze singular elliptic operators, with applications to sub-critical noise problems.

Abstract: In this note we show how one can use recently gained insights from the study
of singular SPDEs, more particularly the study of singular operators via the
theory of Paracontrolled Distributions, to construct domains for (singular)
elliptic operators. Formally we consider \[ A (u) \text{"$=$''} (1 - \Delta) u
+ \nabla V \cdot \nabla u + \xi u +
  {{div} (\rho u)}, \] where $V \in \mathcal{C}^{\delta}$, $\xi \in
\mathcal{C}^{- 2 + \delta}$, $\rho \in \mathcal{C}^{- 1 + \delta},{div}\rho =
0$ and which satisfy a structural assumption that is notably satisfied when
$\xi$ is a "sub-critical noise". We also show that under this assumption, one
can construct a continuous change of variables $\Theta$ which satisfies \[ A
\Theta - (1 - \Delta) \in \mathcal{L} (H^2 ; H^{\delta'}) \] which allows us to
define $A$ rigorously and parametrise a domain. Moreover, for suitably
regularised operators \[ A_{\varepsilon} (u) := (1 - \Delta) u + \nabla
V_{\varepsilon} \cdot
  \nabla u + (\xi_{\varepsilon} + c_{\varepsilon}) u +
  {{div} (\rho_{\varepsilon} \cdot u)}, \] we show that for a strongly
converging regularised change of variables $\Theta_{\varepsilon} \rightarrow
\Theta$ we have \[ A_{\varepsilon} \Theta_{\varepsilon} \rightarrow A \Theta
\text{ in }
  \mathcal{L} (H^2 ; L^2) \] which in particular implies norm resolvent
convergence to a limiting closed operator.

</details>


### [70] [Nonuniqueness of Leray-Hopf solutions to the unforced incompressible 3D Navier-Stokes Equation](https://arxiv.org/abs/2509.25116)
*Thomas Hou,Yixuan Wang,Changhe Yang*

Main category: math.AP

TL;DR: First rigorous computer-assisted proof of nonuniqueness for Leray-Hopf solutions to 3D Navier-Stokes equations by constructing self-similar solutions and showing instability through linear analysis.


<details>
  <summary>Details</summary>
Motivation: To address the long-standing open problem of nonuniqueness in Leray-Hopf solutions for 3D incompressible Navier-Stokes equations, providing the first computer-assisted rigorous proof.

Method: Construct self-similar Leray-Hopf solution, analyze linearized operator stability, develop numerical method for high-precision candidate solutions, decompose operator into coercive part plus compact perturbation, use finite-rank approximation and computer-assisted proofs to verify invertibility.

Result: Successfully demonstrated existence of unstable eigenpair, proving nonuniqueness and yielding infinitely many Leray-Hopf solutions.

Conclusion: The paper provides the first rigorous computer-assisted proof of nonuniqueness for Leray-Hopf solutions in 3D Navier-Stokes equations, establishing a framework for such proofs in fluid dynamics.

Abstract: The nonuniqueness of Leray-Hopf solutions to the unforced incompressible 3D
Navier-Stokes equations is one of the central open problems in mathematical
fluid dynamics. In this paper, we provide, to our knowledge, the first rigorous
computer-assisted proof demonstrating such nonuniqueness. Inspired by earlier
works in this area, we construct a Leray-Hopf solution in the self-similar
setting and then establish the existence of a second solution by analyzing the
stability of the linearized operator around this profile, showing that it
corresponds to an unstable perturbation. To achieve this, we develop an
innovative numerical method that computes candidate solutions with high
precision and propose a framework for rigorously establishing exact solutions
in a neighborhood of these candidates. A key step is to decompose the
linearized operator into a coercive part plus a compact perturbation, which is
further approximated by a finite-rank operator up to a small error. The
invertibility of the linearized operator restricted to the image of this
finite-rank approximation is then rigorously verified using computer-assisted
proofs. This certifies the existence of an unstable eigenpair and,
consequently, yields a second solution - indeed, infinitely many Leray-Hopf
solutions.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [71] [A Comparison of Surrogate Constitutive Models for Viscoplastic Creep Simulation of HT-9 Steel](https://arxiv.org/abs/2509.22667)
*Pieterjan Robbe,Andre Ruybalid,Arun Hegde,Christophe Bonneville,Habib N Najm,Laurent Capolungo,Cosmin Safta*

Main category: physics.comp-ph

TL;DR: Developed two local surrogate models (piecewise response surface and mixture of experts) for viscoplastic response of steel to replace computationally expensive mechanistic models, with mixture of experts showing superior accuracy.


<details>
  <summary>Details</summary>
Motivation: Complex mechanistic microstructure-informed constitutive models for polycrystals have prohibitive computational costs, especially for optimization and uncertainty quantification tasks requiring numerous evaluations.

Method: Created two surrogate models: piecewise response surface method and mixture of experts model, using training data from viscoplastic self-consistent (VPSC) simulations of HT-9 steel creep behavior.

Result: The mixture of experts model outperformed the piecewise response surface method in accuracy for predicting viscoplastic material behavior, as measured by defined test metrics.

Conclusion: Data-driven surrogate models provide an effective balance between accuracy and computational efficiency for constitutive modeling, with mixture of experts approach showing particular promise for complex material behavior.

Abstract: Mechanistic microstructure-informed constitutive models for the mechanical
response of polycrystals are a cornerstone of computational materials science.
However, as these models become increasingly more complex - often involving
coupled differential equations describing the effect of specific deformation
modes - their associated computational costs can become prohibitive,
particularly in optimization or uncertainty quantification tasks that require
numerous model evaluations. To address this challenge, surrogate constitutive
models that balance accuracy and computational efficiency are highly desirable.
Data-driven surrogate models, that learn the constitutive relation directly
from data, have emerged as a promising solution. In this work, we develop two
local surrogate models for the viscoplastic response of a steel: a piecewise
response surface method and a mixture of experts model. These surrogates are
designed to adapt to complex material behavior, which may vary with material
parameters or operating conditions. The surrogate constitutive models are
applied to creep simulations of HT-9 steel, an alloy of considerable interest
to the nuclear energy sector due to its high tolerance to radiation damage,
using training data generated from viscoplastic self-consistent (VPSC)
simulations. We define a set of test metrics to numerically assess the accuracy
of our surrogate models for predicting viscoplastic material behavior, and show
that the mixture of experts model outperforms the piecewise response surface
method in terms of accuracy.

</details>


### [72] [Interplay of Variance Reduction and Population Control in Monte Carlo Neutron Transport](https://arxiv.org/abs/2509.22943)
*Jordan Northrop,Ilham Variansyah,Todd Palmer,Camille Palmer*

Main category: physics.comp-ph

TL;DR: This study examines how steady-state variance reduction techniques perform when extended to time-dependent Monte Carlo neutron transport simulations, and how they interact with population control methods.


<details>
  <summary>Details</summary>
Motivation: Monte Carlo methods are accurate for neutron transport but computationally expensive. While variance reduction helps steady-state simulations and population control is essential for time-dependent problems, combining them can create algorithmic conflicts that need investigation.

Method: Conducted simulations using various combinations of variance reduction and population control techniques across multiple test problems to assess their performance and interactions.

Result: Using weight windows with weight-based combing for population control significantly hinders performance, while pairing weight windows with uniform combing provides necessary efficiencies. Further performance gains were achieved by replacing steady-state weight windows with time-dependent versions.

Conclusion: Careful algorithm selection is crucial for effective large-scale time-dependent simulations. The combination of variance reduction and population control techniques significantly impacts simulation performance, with specific pairings (like weight windows with uniform combing) being particularly effective.

Abstract: Monte Carlo methods are widely used for neutron transport simulations at
least partly because of the accuracy they bring to the modeling of these
problems. However, the computational burden associated with the slow
convergence rate of Monte Carlo poses a significant challenge to running
large-scale simulations. The continued improvement in high-performance
computing capabilities has put exascale time-dependent Monte Carlo neutron
transport simulations within reach. Variance reduction techniques have become
an essential component to the efficiency of steady-state simulations, and
population control techniques are an integral part of time-dependent
simulations, but combining them can create algorithmic conflicts. This study
investigates the performance of steady-state variance reduction techniques when
extended to time-dependent problems and examines how variance reduction and
population control techniques combine to impact the effectiveness of
time-dependent simulations. Simulations were conducted using various
combinations of these techniques across multiple test problems to assess their
performance. While this study does not examine all possible variance reduction
and population control combinations, the findings emphasize the importance of
carefully selecting algorithms to simulate large-scale time-dependent problems
effectively. Notably, using weight windows with weight-based combing for
population control can significantly hinder simulation performance, whereas
pairing weight windows with uniform combing can provide the efficiencies
necessary for successfully computing the results of massive problems. Further
performance gains were observed when steady-state weight windows were replaced
with time-dependent versions.

</details>


### [73] [Symmetry-preserving random batch Ewald method for constant-potential simulation of electrochemical systems](https://arxiv.org/abs/2509.24742)
*Weihang Gao,Qi Zhou,Qianru Zhang,Zhenli Xu*

Main category: physics.comp-ph

TL;DR: A symmetry-preserving random batch Ewald (SRBE) algorithm is proposed to efficiently compute charge fluctuations in electrochemical systems, achieving significant computational speedup while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Constant potential molecular dynamics simulation is important for electrochemical applications, but calculating charge fluctuation on electrodes remains computationally expensive.

Method: The SRBE algorithm deterministically computes low-frequency components perpendicular to electrodes and approximates remaining components using random batch sampling, reducing charge and force fluctuations while preserving symmetry in anisotropic systems.

Result: Numerical experiments show high accuracy in capturing dynamic charging processes and equilibrium electric double layer structures, with parallel efficiency improvements up to two orders of magnitude compared to conventional FFT-based algorithms.

Conclusion: The SRBE method has strong potential for enabling large-scale electrochemical simulations and broad applicability to practical problems in the field.

Abstract: Constant potential molecular dynamics simulation plays important role for
applications of electrochemical systems, yet the calculation of charge
fluctuation on electrodes remains a computational bottleneck. We propose a
highly scalable, symmetry-preserving random batch Ewald (SRBE) algorithm to
address this challenge. The SRBE algorithm deterministically computes the
low-frequency components along the direction perpendicular to electrodes, while
efficiently approximating the remaining components using random batch sampling.
This approach simultaneously reduces charge and force fluctuations while
satisfying the symmetry-preserving mean field condition in anisotropic systems
with large aspect ratios. Numerical experiments on electrode/ionic liquid
systems validate the high accuracy of the SRBE method in capturing dynamic
charging processes and equilibrium electric double layer structures. The SRBE
method achieves parallel efficiency improvements of up to two orders of
magnitude compared with conventional FFT-based algorithms. These findings
highlight its strong potential for enabling large-scale electrochemical
simulations and its broad applicability to practical problems in the field.

</details>


### [74] [FESTIM v2.0: Upgraded framework for multi-species hydrogen transport and enhanced performance](https://arxiv.org/abs/2509.24760)
*James Dark,Rémi Delaporte-Mathurin,Jørgen S. Dokken Huihua Yang,Chirag Khurana,Kaelyn Dunnell,Gabriele Ferrero,Vladimir Kulagin,Samuele Meschini*

Main category: physics.comp-ph

TL;DR: FESTIM v2.0 is an open-source finite element framework for modeling hydrogen isotope transport in materials, featuring enhanced physics capabilities and migration to DOLFINx for improved performance.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible and extensible tool for simulating hydrogen transport processes in materials, addressing the need for comprehensive modeling of diffusion, trapping, and surface interactions.

Method: The framework adopts a modular structure supporting multi-species transport, advanced trapping/reaction schemes, isotope exchange, decay, and advection. It features generalized interface/boundary conditions and interoperability with external solvers for multiphysics workflows.

Result: FESTIM v2.0 broadens both physical scope and software infrastructure, enabling coupling with fluid dynamics and neutron transport codes through improved performance and sustainability via migration to DOLFINx.

Conclusion: FESTIM v2.0 serves as a versatile platform for investigating hydrogen transport across scientific and engineering applications, positioning it as a comprehensive tool for materials research.

Abstract: FESTIM is an open-source finite element framework for modelling the transport
of hydrogen isotopes in materials. It provides a flexible and extensible tool
for simulating diffusion, trapping, surface interactions, and other processes
that govern hydrogen behaviour. This paper presents FESTIM v2.0, a major
release that broadens both the physical scope and the software infrastructure
of the framework. On the physics side, the formulation adopts a modular
structure that supports multi-species transport, advanced trapping and reaction
schemes, isotope exchange, decay, and advection. Interface and boundary
conditions have been generalised, and interoperability with external solvers
enables multiphysics workflows, including coupling with fluid dynamics and
neutron transport codes. On the software side, FESTIM v2.0 has been migrated to
DOLFINx, the next-generation FEniCS platform, providing improved performance,
interoperability, and long-term sustainability. Taken together, these advances
position FESTIM v2.0 as a versatile platform for investigating hydrogen
transport in materials across scientific and engineering applications.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [75] [On the mechanism of Pedestal Relaxation Events -- Insights gained by turbulence simulations with GRILLIX](https://arxiv.org/abs/2509.22948)
*Christoph Pitzal,Andreas Stegmeir,Tim Happel,Kaiyu Zhang,Konrad Eder,Wladimir Zholobenko,Philipp Ulbl,Manuel Herschel,Frank Jenko,The ASDEX Upgrade Team*

Main category: physics.plasm-ph

TL;DR: Global fluid simulations identify Micro-Tearing Modes as the mechanism behind Pedestal Relaxation Events in I-mode discharges, showing good agreement with linear theory predictions.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying mechanism of Pedestal Relaxation Events in I-mode discharges, which show similarities to ELMs but appear to have different triggering mechanisms.

Method: Global trans-collisional fluid simulations of I-mode discharge using GRILLIX, analyzing mode properties and comparing with linear theory in simplified slab geometry.

Result: Simulations reproduced experimentally observed PRE characteristics and identified Micro-Tearing Modes as the triggering mechanism, with excellent agreement between simulation results and linear theory growth-rate estimates.

Conclusion: MTMs are responsible for triggering PREs, and the study provides insights into simulating low collisionality regimes with trans-collisional fluid models while highlighting challenges with Landau-fluid closure implementation.

Abstract: Pedestal Relaxation Events (PREs) appear in I-mode discharges close to the
I-H transition. Although they show certain similarities with Edge Localised
Modes (ELMs), i.e. periodic energy ejections, the underlying mechanism seems to
be very different from the mechanism responsible for ELMs. In this manuscript,
we present global trans-collisional fluid simulations of an I-mode discharge in
ASDEX Upgrade using GRILLIX. We observe multiple PREs during the simulation,
which reproduce a range of experimentally observed PRE characteristics.
Furthermore, a detailed analysis of various mode properties in our simulation
allows us to pinpoint the underlying mechanism responsible for triggering PREs
to Micro-Tearing Modes (MTMs). The system is analysed dynamically by evaluating
density and electron temperature gradient lengths at the OMP position, where
the MTM is located and grows over time. The path taken by the system in
gradient length space is compared to a growth-rate estimate calculated by
linear theory in simplified slab geometry, providing excellent agreement.
Building on these insights, we sketch a qualitative picture of a full PRE
cycle. Finally, we discuss the influence of the recently implemented
Landau-fluid closure and the challenges of simulating low collisionality
regimes with trans-collisional fluid models, like the one employed by GRILLIX.

</details>


### [76] [Control of kinetic plasma instabilities by laser fields](https://arxiv.org/abs/2509.23080)
*Nicolas Crouseilles,Lukas Einkemmer,Qin Li,Uri Shumlak,Yukun Yue*

Main category: physics.plasm-ph

TL;DR: This paper studies laser control of kinetic plasma instabilities, showing linear theory cannot stabilize longitudinal instabilities but nonlinear effects enable effective control through PDE-constrained optimization.


<details>
  <summary>Details</summary>
Motivation: To investigate whether external electromagnetic fields from lasers can control kinetic plasma instabilities, addressing limitations of linear stabilization approaches.

Method: Derived dispersion relation for reduced Vlasov-Maxwell system by extending Penrose condition, analyzed decoupling into longitudinal and transverse parts, and used PDE-constrained optimization to determine control parameters.

Result: Found that dispersion relation decouples into classic Penrose condition (longitudinal) and laser-influenced transverse part. Linear theory cannot stabilize longitudinal instabilities, but nonlinear coupling enables effective control.

Conclusion: While linear theory shows limitations for longitudinal instability control, nonlinear effects provide a pathway for effective laser-based control of plasma instabilities through optimized external field parameters.

Abstract: We study the possibility of controlling kinetic plasma instabilities by using
lasers to apply external electromagnetic fields. We derive the dispersion
relation for the corresponding mathematical description, a reduced
Vlasov--Maxwell system, by extending the well-known Penrose condition. It is
observed that, under very mild assumptions, the dispersion relation decouples
into two parts. The first part is identical to the classic Penrose condition
for the Vlasov--Poisson system, while the second part describes the influence
of the laser on the transverse dynamics (e.g. a Weibel instability). In
particular, this means that the longitudinal dynamics (e.g. a two-stream
instability) can not be stabilized in this manner as far as linear theory is
concerned. We show, however, that nonlinear effects can be used to couple the
two parts and achieve effective control. This is done by determining the
control parameters (i.e. the form of the external electric and magnetic fields)
by solving a PDE-constrained optimization problem.

</details>


### [77] [Programmable Focal Elongation and Shaping of High-Intensity Laser Pulses using Adaptive Optics](https://arxiv.org/abs/2509.23294)
*P. Blum,A. Puchert,E. Archer,S. Jalas,S. W. Jolly,J. Osterhoff,W. P. Leemans,M. Kirchen,A. R. Maier,R. J. Shalloo*

Main category: physics.plasm-ph

TL;DR: A method for programmatic structuring of laser pulse intensity distribution in the focal region using standard optics, enabling extended focal lengths and control over longitudinal intensity for plasma acceleration applications.


<details>
  <summary>Details</summary>
Motivation: Controlling laser intensity distribution is essential for optimizing plasma waveguides and enabling advanced plasma acceleration techniques like dephasingless wakefield acceleration.

Method: Programmatic structuring of high-intensity focal region using standard off-axis parabolic mirror, extending focal length beyond Rayleigh length and controlling longitudinal intensity distribution.

Result: Theoretical framework validated through numerical simulations and experimental measurements, demonstrated in existing plasma accelerator system with available hardware.

Conclusion: This technique shows potential for multi-GeV laser plasma acceleration and flying foci generation, benefiting from improved programmatic structuring of high-intensity laser pulses.

Abstract: Controlling the intensity distribution of laser pulses in the focal region is
essential for optimizing optically generated plasma waveguides and enabling
advanced plasma acceleration techniques, including dephasingless wakefield
acceleration. Here, we present a method for programmatic structuring of the
high-intensity focal region of a standard off-axis parabolic mirror, extending
the length of this region well beyond the Rayleigh length and enabling control
over the longitudinal intensity distribution. The theoretical framework is
validated through numerical simulations and experimental measurements. Further,
we demonstrate the use of this technique in an existing plasma accelerator
system using readily available hardware components. Finally, we illustrate the
potential application of this method to multi-GeV laser plasma acceleration and
the generation of flying foci, research areas which would significantly benefit
from improved programmatic structuring of high-intensity laser pulses.

</details>


### [78] [Flow Crossover and Parallel Outflow during Collisionless Magnetic Reconnection](https://arxiv.org/abs/2509.24513)
*Theerasarn Pianpanit,Kittipat Malakit,Pakkapawn Prapan,David Ruffolo,Peera Pongkitiwanichakul,Piyawat Suetrong,Michael A. Shay,Paul A. Cassak*

Main category: physics.plasm-ph

TL;DR: Particle-in-cell simulations reveal 'flow crossover' in 2D collisionless magnetic reconnection, where plasma from opposite inflow sides crosses paths and the midplane before forming outflow jets, with ions and electrons exhibiting different parallel driving mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand the detailed plasma flow patterns and driving mechanisms during collisionless magnetic reconnection, particularly how ions and electrons from different inflow regions interact and form outflows.

Method: Used particle-in-cell simulations with labeled ions and electrons according to their initial inflow regions to track plasma motion during 2D collisionless magnetic reconnection.

Result: Discovered 'flow crossover' phenomenon where plasma from each inflow side crosses paths with the other side and crosses the midplane before forming outflow jets. Ions and electrons have different parallel driving mechanisms - ions generate parallel flow locally within ion diffusion region, while electrons generate it mostly outside electron diffusion region. Reconnection outflows are more parallel than perpendicular, especially for electrons.

Conclusion: Flow crossover and parallel outflow patterns are general features of collisionless magnetic reconnection, occurring in both symmetric and asymmetric guide-field scenarios. This predicts that in situ observations across asymmetric reconnection outflow regions could reveal locally reversed gradients in plasma properties.

Abstract: Using particle-in-cell simulations that label ions and electrons according to
their initial inflow region, we find that during 2D collisionless magnetic
reconnection, the bulk flow of the plasma from each inflow side crosses paths
with plasma from the other inflow side and crosses the midplane before being
redirected into an outflow jet. This feature, which we term ``flow crossover,''
implies mechanisms to generate bulk motion in a direction parallel to the
magnetic field. We find that ions and electrons undergo different parallel
driving mechanisms, leading to different flow crossover patterns. The parallel
bulk flow for ions is generated more locally within the ion diffusion region,
whereas the parallel bulk flow for electrons is mostly generated outside the
electron diffusion region. Consequently, the reconnection outflows are more of
a parallel flow than a perpendicular flow, especially for the electron outflow.
The flow crossover and the parallel outflow patterns occur not only in
symmetric reconnection but also in the more complex scenario of a guide-field
asymmetric reconnection, suggesting that it is a general feature of
collisionless magnetic reconnection. Because the plasma on one side of the
outflow mostly originates from the inflow plasma on the other side, we predict
that near an asymmetric reconnection site in a collisionless space plasma, in
situ observations across the outflow region could reveal locally reversed
gradients in plasma properties.

</details>


### [79] [Is Turbulence able to Generate Magnetic Islands in Tokamaks? Gyrokinetic Simulations of Turbulence-Driven Magnetic Islands in Toroidal Geometry](https://arxiv.org/abs/2509.24750)
*Fabien Widmer,Emanuele Poli,Alexey Mishchenko,Akihiro Ishizawa,Thomas Hayward-Schneider,Alberto Bottino*

Main category: physics.plasm-ph

TL;DR: Turbulence-driven magnetic islands form through micro-instability-generated E×B flows that cause magnetic reconnection, creating small islands that coalesce into larger ones capable of flattening equilibrium profiles.


<details>
  <summary>Details</summary>
Motivation: To understand how magnetic islands can form in fusion plasmas through turbulence-driven mechanisms, particularly in linearly stable tearing modes under collisionless conditions.

Method: Used gyrokinetic simulations of linearly stable tearing modes in large-aspect-ratio toroidal geometry under collisionless conditions to study micro-instability effects.

Result: Micro-instabilities generate E×B flows that drive magnetic reconnection, forming multiple small-scale islands that coalesce into large-scale magnetic islands capable of flattening equilibrium profiles.

Conclusion: A universal mechanism exists for turbulence-driven magnetic island formation that operates independently of micro-instability parity and can seed further neoclassical growth.

Abstract: We report a universal mechanism for turbulence-driven magnetic islands in
fusion plasmas. Using gyrokinetic simulations of a linearly stable tearing mode
in a large-aspect-ratio toroidal geometry under collisionless conditions, we
demonstrate that micro-instabilities generate an
$\boldsymbol{E}\times\boldsymbol{B}$ flow that drives magnetic field line
reconnection. This process forms multiple small-scale islands along the
resonant surface, which interact nonlinearly and eventually coalesce into
large-scale magnetic islands. These islands are capable of significantly
flattening the equilibrium profile across the island O-points and thus acting
as a seed for their further neoclassical growth. Notably, the mechanism
operates independently of the parity of the destabilizing micro-instability.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [80] [Geometric structure of stationary problem for spatial 1D self-diffusion equation with logistic growth](https://arxiv.org/abs/2509.24752)
*Yu ICHIDA*

Main category: math.DS

TL;DR: Analysis of non-constant stationary states in 1D parabolic equations with nonlinear self-diffusion and logistic growth using Poincaré-Lyapunov compactification to classify dynamics including infinity.


<details>
  <summary>Details</summary>
Motivation: To understand how self-diffusion coefficients qualitatively change the structure and properties of non-constant stationary states in reaction-diffusion systems.

Method: Derived 2D ODE from stationary problem, used Poincaré-Lyapunov compactification to analyze dynamics including infinity, examined symmetries and conserved quantities.

Result: Self-diffusion coefficient acts as bifurcation parameter, changing dynamical system from saddle homoclinic orbit to heteroclinic orbit connecting saddle equilibria, significantly altering stationary state shapes.

Conclusion: Explicit relationship between linear and self-diffusion established, providing characterization of non-trivial stationary states and deep insight into nonlinear diffusion effects.

Abstract: This paper considers the solution structure of non-trivial, non-constant
stationary states of 1D spatial parabolic equations with nonlinear
self-diffusion and logistic growth terms. A two-dimensional ordinary
differential equation satisfying the stationary problem is derived and all its
dynamics, including to infinity, is revealed by the Poincar\'e-Lyapunov
compactification, one of the compactifications of phase space. The advantage of
this method is that it can be used to classify all dynamical systems
(especially connecting orbits) of a two-dimensional system including infinity.
Therefore, the classification results for the dynamical system including to
infinity give the classification results for the non-constant stationary states
obtained only from the structure of the original equations. This argument
allows us to observe a change in the classification of the non-constant
stationary states by an explicit relation between the linear diffusion
coefficient and the self-diffusion coefficient, combined with arguments about
the symmetries and conserved quantities of the ODEs. This means that changing
the self-diffusion coefficient as a bifurcation parameter not only
qualitatively changes the dynamical system from a big saddle homoclinic orbit
of the ODEs to a heteroclinic orbit that connects the saddle equilibria, but
also significantly changes the shape and the properties of the stationary
states. It explicitly shows the relationship between linear and self-diffusion,
gives a characterization of non-trivial stationary states in terms of dynamical
systems, and gives a deep insight into the influence of self-diffusion, one of
the nonlinear diffusions.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [81] [Neural Canonical Transformation for the Spectra of Fluxional Molecule CH5+](https://arxiv.org/abs/2509.23731)
*Ruisi Wang,Qi Zhang,Lei Wang*

Main category: physics.chem-ph

TL;DR: The paper applies neural canonical transformation (NCT) to calculate vibrational spectra of highly fluxional CH5+ molecule, successfully treating nuclear quantum effects and anharmonicities to reveal hydrogen scrambling dynamics.


<details>
  <summary>Details</summary>
Motivation: CH5+ is a highly fluxional molecule with significant hydrogen atom motions and anharmonic effects that complicate its excitation spectrum, requiring advanced methods to accurately model its quantum dynamics.

Method: Used neural canonical transformation (NCT) approach with wavefunctions in atomic coordinates (rather than normal coordinates) to solve vibrational spectra, treating nuclear quantum effects and anharmonicities effectively.

Result: Successfully calculated ground and excited states of CH5+, showing wavefunctions prefer three stationary points on potential energy surface, with all states characterized by continuous hydrogen swapping dynamics.

Conclusion: The work extends NCT approach's applicability for calculating excited states to fluxional molecules without fixed geometry, demonstrating its effectiveness for complex quantum systems.

Abstract: Protonated methane, CH5+, is a highly fluxional molecule with large spatial
motions of the hydrogen atoms. The molecule's anharmonic effects and the
scrambling of the hydrogen atoms significantly affect the excitation spectrum
of the molecule. The neural canonical transformation (NCT) approach, which we
previously developed to solve the vibrational spectra of molecules and solids,
is a powerful method that effectively treats nuclear quantum effects and
anharmonicities. Using NCT with wavefunctions in atomic coordinates rather than
normal coordinates, we successfully calculate the ground and excited states of
CH5+. We found that the wavefunctions for the ground state, as well as for low-
and high-energy excited states, show preferences for the three stationary
points on the potential energy surface. Furthermore, the dynamics of all these
states are characterized by continuous hydrogen swapping. This work extends the
applicability of the NCT approach for calculating excited states to fluxional
molecules without fixed geometry.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [82] [Anti-hyperuniform Critical States of Active Topological Defects](https://arxiv.org/abs/2509.22911)
*Simon Guldager Andersen,Tianxiang Ma,Makito F. Katsume,Kexin Li,Xiao Liu,Martin Cramer Pedersen,Amin Doostmohammadi*

Main category: cond-mat.soft

TL;DR: Critical states in active nematics exhibit slowed defect density relaxation, amplified fluctuations, and heightened sensitivity to activity, leading to anti-hyperuniform defect clustering with giant number fluctuations.


<details>
  <summary>Details</summary>
Motivation: To understand critical states in active nematics and their role in collective dynamics, spontaneous flows, and emergent pattern formation in non-equilibrium systems.

Method: Experimental measurements on large-field-of-view endothelial monolayers and analysis of defect interactions, scaling behavior, and fluctuations across different scales and conditions.

Result: Near criticality, defect interactions become long-ranged, scaling with system size, and the system enters an anti-hyperuniform regime with giant number fluctuations and defect clustering that is robust to varying parameters.

Conclusion: The observed anti-hyperuniformity originates from defect clustering rather than defect-unbinding or phase separation, with implications for biological processes like morphogenesis and collective cellular self-organization.

Abstract: Topological defects are fundamental to the collective dynamics of
non-equilibrium systems and in active matter, mediating spontaneous flows,
dynamic self-organization, and emergent pattern formation. Here, we reveal
critical states in active nematics, marked by slowed defect density relaxation,
amplified fluctuations, and heightened sensitivity to activity. Near
criticality, defect interactions become long-ranged, scaling with system size,
and the system enters an anti-hyperuniform regime with giant number
fluctuations of topological defects and defect clustering. This transition
reflects a dual scaling behavior: fluctuations are uniform at small scales but
become anti-hyperuniform at larger scales, \tm{as supported by experimental
measurements on large-field-of-view endothelial monolayers. We find that these
anti-hyperuniform states with multiscale defect density fluctuations are robust
to varying parameters, introducing frictional damping, and changing boundary
conditions.} Finally, we show that the observed anti-hyperuniformity originates
from defect clustering, distinguishing this transition from defect-unbinding or
phase separation processes. Beyond fundamental implications for non-equilibrium
systems, these results may inform biological contexts where topological defects
are integral to processes such as morphogenesis and collective cellular
self-organization.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [83] [Attosecond electron bunch generation by an intense high-order harmonic pulse interacting with a thin target](https://arxiv.org/abs/2509.24775)
*Yang He,Mamat Ali Bake,Cheng-Qi Zhang,Bai-Song Xie*

Main category: physics.acc-ph

TL;DR: Proposes using high-harmonics from laser-plasma interaction to generate attosecond electron bunches with excellent collimation and ultra-short duration (~100 as).


<details>
  <summary>Details</summary>
Motivation: Laser-accelerated electron bunches offer unique temporal resolution for probing ultrafast processes due to ultrashort pulse duration and high peak currents, enabling various practical applications.

Method: Utilizes high-harmonics generated through laser-plasma interaction as driving pulse, which interacts with thin target to produce attosecond electron bunches.

Result: Successfully generated electron bunch with 100 attoseconds duration, 0.38 nC charge, emittance of 4.5×10⁻³ mm·mrad, and divergence angle of ~10°. Optimized parameters through systematic analysis of laser intensity and target positioning.

Conclusion: Establishes robust foundation for ultrashort electron bunch generation and opens new prospects for applications in advanced high-energy and attosecond physics experiments.

Abstract: Laser-accelerated electron bunches and the secondary radiation sources they
produce exhibit unique temporal resolution for probing ultrafast physical
processes due to their ultrashort pulse duration. The inherently short temporal
profile of these pulses leads to extremely high peak bunch currents, thereby
enabling a wide range of practical applications. In this study, we propose an
innovative method for generating such bunch by utilizing high-harmonics
generated through laser-plasma interaction as the driving pulse, which
subsequently interacts with a thin target to produce an attosecond electron
bunch. Using this method, we successfully generated an electron bunch
characterized by excellent collimation and an ultra-short duration of
approximately 100 attoseconds, representing a substantial reduction in bunch
duration. The total bunch charge achieved was 0.38 nC, with an emittance of
$4.5 \times 10^{-3} \, \text{mm} \cdot \text{mrad}$ and a divergence angle of
approximately $10^\circ$. Moreover, by systematically analyzing the effects of
laser intensity and target positioning, we determined an optimized set of
simulation parameters. This research establishes a robust foundation for the
generation of ultrashort electron bunches and opens new prospects for their
application in advanced high-energy and attosecond physics experiments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [84] [Equation-Free Coarse Control of Distributed Parameter Systems via Local Neural Operators](https://arxiv.org/abs/2509.23975)
*Gianluca Fabiani,Constantinos Siettos,Ioannis G. Kevrekidis*

Main category: eess.SY

TL;DR: A data-driven method using local neural operators trained on spatiotemporal data to control high-dimensional distributed parameter systems without explicit equations, enabling efficient computation of steady states, linearization, and control design.


<details>
  <summary>Details</summary>
Motivation: Traditional equation-free approaches for controlling distributed parameter systems are computationally expensive when fine-scale simulators are unavailable or too costly, creating a need for purely data-driven alternatives.

Method: Train local neural operators on spatiotemporal data to create efficient short-time solution operators, then use Krylov subspace methods for steady-state computation, Jacobian-free linearization, and reduced-order modeling via Krylov-Arnoldi iterations.

Result: The approach enables computation of coarse steady/unsteady states and dominant eigenspectrum without explicit Jacobian assembly, supporting both discrete-time LQR and pole-placement controllers that can be lifted back to full nonlinear dynamics.

Conclusion: This data-driven framework provides an effective alternative to classical equation-free methods for controlling distributed parameter systems when only data is available, enabling feedback control without requiring explicit coarse-grained equations or expensive simulations.

Abstract: The control of high-dimensional distributed parameter systems (DPS) remains a
challenge when explicit coarse-grained equations are unavailable. Classical
equation-free (EF) approaches rely on fine-scale simulators treated as
black-box timesteppers. However, repeated simulations for steady-state
computation, linearization, and control design are often computationally
prohibitive, or the microscopic timestepper may not even be available, leaving
us with data as the only resource. We propose a data-driven alternative that
uses local neural operators, trained on spatiotemporal microscopic/mesoscopic
data, to obtain efficient short-time solution operators. These surrogates are
employed within Krylov subspace methods to compute coarse steady and
unsteady-states, while also providing Jacobian information in a matrix-free
manner. Krylov-Arnoldi iterations then approximate the dominant eigenspectrum,
yielding reduced models that capture the open-loop slow dynamics without
explicit Jacobian assembly. Both discrete-time Linear Quadratic Regulator
(dLQR) and pole-placement (PP) controllers are based on this reduced system and
lifted back to the full nonlinear dynamics, thereby closing the feedback loop.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [85] [Universal Numerical Simulation Model for Laser Material Processing](https://arxiv.org/abs/2509.22666)
*Andreas Otto,Michele Buttazzoni,Carlos Durán,Tobias Florian,Constantin Zenz*

Main category: physics.optics

TL;DR: A universal continuum mechanical model using Finite Volume Method that simulates various laser-material interaction processes including welding, additive manufacturing, cutting, ablation, drilling, and surface structuring across different time/length scales and materials.


<details>
  <summary>Details</summary>
Motivation: The complexity and diversity of coupled nonlinear physical phenomena in laser-material interactions make modeling challenging, yet simulation models are crucial for process understanding, explaining experimental observations, and optimizing industrial processes.

Method: Developed a continuum mechanical framework based on fundamental conservation principles (mass, momentum, energy) that captures multiphysical effects. Used Finite Volume Method for numerical solution and validated against benchmark problems with experimental data.

Result: Successfully simulated macroscopic processes like keyhole drilling, melt pool dynamics in copper welding, and microscopic copper ablation with femtosecond pulses. Applied to real-world industrial scenarios including deep penetration welding, powder bed fusion additive manufacturing, and ultrashort pulsed drilling of micro vias.

Conclusion: The universal model effectively bridges multiple laser processes and scales, providing valuable insights for explaining and optimizing industrial phenomena and defects across various laser manufacturing applications.

Abstract: High power lasers are used for a variety of manufacturing processes on time
and length scales that cover many orders of magnitude and on different
materials. The variety of processes achievable through laser-material
interaction results from numerous coupled, nonlinear physical phenomena.
Simulation models can be used to gain process understanding, explain
experimentally observed phenomena, and optimize or design processes. However,
the inherent complexity and diversity of the phenomena make modeling a
challenging task.
  Within this chapter, a universal model is presented that accurately simulates
a broad spectrum of processes, including welding, additive manufacturing,
cutting, ablation, drilling, and surface structuring, encompassing both
continuous wave and ultrashort pulsed lasers and their interaction with various
materials. Starting from the fundamental principles of conservation of mass,
momentum, and energy, a continuum mechanical framework is introduced that
captures the main multiphysical effects. A numerical solution using the Finite
Volume Method is presented and validated against benchmark problems where
detailed experimental data are available. Macroscopic examples with continuous
wave lasers include keyhole drilling and collapse under stationary illumination
as well as melt pool dynamics and pore formation in copper welding. At the
microscopic level, copper ablation with femtosecond pulses is simulated.
  Finally, a set of real-world applications is shown, where the model helps to
explain and optimize industrial phenomena and defects. Examples include deep
penetration welding of steel with dynamic beam shaping, multilayer additive
manufacturing via powder bed fusion, and ultrashort pulsed drilling of micro
vias in dielectric materials.

</details>


### [86] [A Review of Light-Field Imaging in Biomedical Sciences](https://arxiv.org/abs/2509.24191)
*Ruixuan Zhao,Xuanwen Hua,Woongjae Baek,Zhaoqiang Wang,Shu Jia,Liang Gao*

Main category: physics.optics

TL;DR: Light-field imaging enables snapshot volumetric imaging by capturing both spatial and angular light information, offering high-speed 3D acquisition without scanning for biomedical applications.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of conventional 3D imaging that require mechanical/optical scanning, enabling high-speed capture of rapid biological dynamics in a single snapshot.

Method: Theoretical foundations and implementations across microscopy, mesoscopy, and endoscopy, addressing trade-offs through compressive sensing, deep learning, and meta-optics.

Result: Light-field imaging provides a scalable and versatile tool for volumetric imaging with unique speed advantages compared to scanning-based methods.

Conclusion: Light-field imaging shows strong potential as a high-speed volumetric imaging tool for biological discovery and clinical applications, though challenges in resolution and depth of field remain.

Abstract: Light-field imaging is an emerging paradigm in biomedical optics, offering
the unique ability to capture volumetric information in a single snapshot by
encoding both the spatial and angular components of light. Unlike conventional
three-dimensional (3D) imaging modalities that rely on mechanical or optical
scanning, light-field imaging enables high-speed volumetric acquisition, making
it particularly well-suited for capturing rapid biological dynamics. This
review outlines the theoretical foundations of light-field imaging and surveys
its core implementations across microscopy, mesoscopy, and endoscopy. Special
attention is given to the fundamental trade-offs between imaging speed, spatial
resolution, and depth of field, as well as recent advances that address these
limitations through compressive sensing, deep learning, and meta-optics. By
positioning light-field imaging within the broader landscape of biomedical
imaging technologies, we highlight its unique strengths, existing challenges,
and future potential as a scalable and versatile tool for biological discovery
and clinical applications.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [87] [Effective medium theory for embedded sound-soft obstacles in an anisotropic inhomogeneous medium with applications](https://arxiv.org/abs/2509.23163)
*Huaian Diao,Qingle Meng,Zhiying Sun*

Main category: math-ph

TL;DR: This paper develops an effective medium theory for time-harmonic acoustic scattering in anisotropic media with multiple disjoint sound-soft obstacles, showing how obstacles can be approximated by isotropic lossy media to locate them in inverse scattering problems.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of simultaneously recovering embedded obstacles and the surrounding medium in inverse scattering problems, particularly for anisotropic media with complex topological structures containing multiple disjoint sound-soft obstacles.

Method: The authors propose a novel theoretical framework that approximates embedded obstacles using an isotropic and lossy medium with specified physical parameters. They establish rigorous mathematical estimates to validate this approximation and provide a concrete example to illustrate their theoretical results.

Result: The framework demonstrates that embedded obstacles can be effectively approximated by isotropic lossy media, where the total wave field exhibits decay properties related to material parameters at obstacle boundaries. This characterization enables obstacle localization in inverse scattering problems.

Conclusion: The proposed effective medium theory provides substantial applications for inverse scattering problems, offering a mathematical foundation for locating embedded obstacles in complex anisotropic media through effective medium approximations.

Abstract: This paper investigates the problem of time-harmonic acoustic scattering in
an inhomogeneous medium with a complex topological structure. Specifically, the
medium is anisotropic and contains several disjoint sound-soft obstacles. This
model commonly arises in the inverse scattering problem of simultaneously
recovering the embedded obstacles and the surrounding medium. We propose a
novel theoretical framework that demonstrates how embedded obstacles can be
effectively approximated by an isotropic and lossy medium with specified
physical parameters, wherein the total wave field exhibits decay properties
related to these specified material parameters at the boundaries of the
obstacles. This mathematical characterization of the wave in an effective
medium model can be used to locate the underlying obstacles. Furthermore, we
establish rigorous estimates to validate this approximation and provide a
concrete example illustrating our theoretical results. Our proposed effective
medium theory offers substantial applications within the context of the
aforementioned inverse problem.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [88] [New Insights and Algorithms for Optimal Diagonal Preconditioning](https://arxiv.org/abs/2509.23439)
*Saeed Ghadimi,Woosuk L. Jung,Arnesh Sujanani,David Torregrosa-Belén,Henry Wolkowicz*

Main category: math.OC

TL;DR: This paper presents efficient methods for finding optimal diagonal preconditioners by minimizing two types of condition numbers, with competitive subgradient approaches that outperform existing SDP methods.


<details>
  <summary>Details</summary>
Motivation: Preconditioning is essential in optimization and mathematics, but existing approaches like SDP-based methods are computationally expensive and don't scale well for large problems.

Method: The authors provide affine-based pseudoconvex reformulations for minimizing both κ-condition number and ω-condition number. They develop a competitive subgradient method with convergence guarantees for κ-optimal diagonal preconditioning.

Result: The proposed subgradient method scales better and is more efficient than SDP-based approaches. The preconditioners lead to better PCG performance for solving linear systems, and applying ω-optimal preconditioner to κ-optimally preconditioned matrices shows significantly improved convergence.

Conclusion: The paper presents efficient diagonal preconditioning methods that outperform existing approaches, with the interesting finding that combining both preconditioning strategies yields further performance improvements for PCG methods.

Abstract: Preconditioning (scaling) is essential in many areas of mathematics, and in
particular in optimization. In this work, we study the problem of finding an
optimal diagonal preconditioner. We focus on minimizing two different notions
of condition number: the classical, worst-case type, $\kappa$-condition number,
and the more averaging motivated $\omega$-condition number. We provide affine
based pseudoconvex reformulations of both optimization problems. The advantage
of our formulations is that the gradient of the objective is inexpensive to
compute and the optimization variable is just an $n\times 1$ vector. We also
provide elegant characterizations of the optimality conditions of both
problems.
  We develop a competitive subgradient method, with convergence guarantees, for
$\kappa$-optimal diagonal preconditioning that scales much better and is more
efficient than existing SDP-based approaches. We also show that the
preconditioners found by our subgradient method leads to better PCG performance
for solving linear systems than other approaches. Finally, we show the
interesting phenomenon that we can apply the $\omega$-optimal preconditioner to
the exact $\kappa$-optimally diagonally preconditioned matrix $A$ and get
consistent, significantly improved convergence results for PCG methods.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [89] [Algorithms and data structures for automatic precision estimation of neural networks](https://arxiv.org/abs/2509.24607)
*Igor V. Netay*

Main category: cs.DS

TL;DR: This paper introduces algorithms for automatic precision estimation in neural network computations, showing that computational inaccuracies accumulate in almost all neural networks, affecting reliability and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the problem of computational inaccuracies that accumulate in neural networks during training and inference, which causes deviations from mathematically predicted behavior and affects reliability.

Method: Developed algorithms and data structures to extend neural network libraries with automatic precision estimation for floating point computations, while maintaining high computation performance.

Result: Numerical experiments demonstrated significant precision loss in inference, gradients, and deviations from expected mathematical behavior, showing that almost all neural networks accumulate computational inaccuracies.

Conclusion: Tracking computational inaccuracies is crucial for ensuring reliability of neural network inference, training, and interpretability of results, as these inaccuracies cause behavior to deviate from mathematical predictions.

Abstract: We describe algorithms and data structures to extend a neural network library
with automatic precision estimation for floating point computations. We also
discuss conditions to make estimations exact and preserve high computation
performance of neural networks training and inference. Numerical experiments
show the consequences of significant precision loss for particular values such
as inference, gradients and deviations from mathematically predicted behavior.
  It turns out that almost any neural network accumulates computational
inaccuracies. As a result, its behavior does not coincide with predicted by the
mathematical model of neural network. This shows that tracking of computational
inaccuracies is important for reliability of inference, training and
interpretability of results.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [90] [IsingFormer: Augmenting Parallel Tempering With Learned Proposals](https://arxiv.org/abs/2509.23043)
*Saleh Bunaiyan,Corentin Delacour,Shuvro Chowdhury,Kyle Lee,Kerem Y. Camsari*

Main category: cs.LG

TL;DR: IsingFormer uses a Transformer trained on equilibrium samples to generate uncorrelated spin configurations as global proposals in Parallel Tempering, accelerating MCMC sampling and optimization in Ising models and spin glasses.


<details>
  <summary>Details</summary>
Motivation: MCMC methods like Parallel Tempering mix slowly near critical points and in rough landscapes due to reliance on local updates. The goal is to improve mixing by incorporating global moves that can generate entire configurations resembling the target distribution.

Method: Train a Transformer on equilibrium samples to generate uncorrelated spin configurations. Use these as proposals for global moves within Metropolis steps in Parallel Tempering, complementing single-spin flips. Apply to 2D Ising models for sampling, 3D spin glasses for optimization, and integer factorization encoded as Ising problems.

Result: On 2D Ising models, reproduces magnetization and free-energy curves, generalizes to unseen temperatures including critical region, and sharply reduces equilibration time. On 3D spin glasses, finds substantially lower-energy states. On factorization problems, successfully transfers to unseen semiprimes and boosts success rates beyond training distribution.

Conclusion: Neural proposals that capture global structure can systematically accelerate Monte Carlo methods, yielding faster sampling and stronger performance in combinatorial optimization, with demonstrated ability to generalize across problem instances.

Abstract: Markov Chain Monte Carlo (MCMC) underlies both statistical physics and
combinatorial optimization, but mixes slowly near critical points and in rough
landscapes. Parallel Tempering (PT) improves mixing by swapping replicas across
temperatures, yet each replica still relies on slow local updates to change its
configuration. We introduce IsingFormer, a Transformer trained on equilibrium
samples that can generate entire spin configurations resembling those from the
target distribution. These uncorrelated samples are used as proposals for
global moves within a Metropolis step in PT, complementing the usual
single-spin flips. On 2D Ising models (sampling), IsingFormer reproduces
magnetization and free-energy curves and generalizes to unseen temperatures,
including the critical region. Injecting even a single proposal sharply reduces
equilibration time, replacing thousands of local updates. On 3D spin glasses
(optimization), PT enhanced with IsingFormer finds substantially lower-energy
states, demonstrating how global moves accelerate search in rugged landscapes.
Finally, applied to integer factorization encoded as Ising problems,
IsingFormer trained on a limited set of semiprimes transfers successfully to
unseen semiprimes, boosting success rates beyond the training distribution.
Since factorization is a canonical hard benchmark, this ability to generalize
across instances highlights the potential of learning proposals that move
beyond single problems to entire families of instances. The IsingFormer
demonstrates that Monte Carlo methods can be systematically accelerated by
neural proposals that capture global structure, yielding faster sampling and
stronger performance in combinatorial optimization.

</details>


### [91] [PHASE: Physics-Integrated, Heterogeneity-Aware Surrogates for Scientific Simulations](https://arxiv.org/abs/2509.23453)
*Dawei Gao,Dali Wang,Zhuowei Gu,Qinglei Cao,Xiao Wang,Peter Thornton,Dan Ricciuto,Yunhe Feng*

Main category: cs.LG

TL;DR: PHASE is a physics-integrated AI surrogate framework that accelerates scientific simulations by 60x, achieving near-equilibrium states in 20 years instead of 1,200+ years for land modeling.


<details>
  <summary>Details</summary>
Motivation: Large-scale simulations are computationally expensive, and existing AI surrogates lack physical plausibility and trustworthiness needed for mission-critical scientific applications.

Method: PHASE combines data-type-aware encoders for heterogeneous inputs with multi-level physics-based constraints that ensure consistency from local dynamics to global system behavior.

Result: PHASE reduced required integration time from 1,200+ years to just 20 years (60x acceleration) for biogeochemical spin-up in Earth system modeling, with strong generalization to higher resolutions.

Conclusion: PHASE captures governing physical regularities rather than surface correlations, enabling practical, physically consistent acceleration of complex scientific workflows.

Abstract: Large-scale numerical simulations underpin modern scientific discovery but
remain constrained by prohibitive computational costs. AI surrogates offer
acceleration, yet adoption in mission-critical settings is limited by concerns
over physical plausibility, trustworthiness, and the fusion of heterogeneous
data. We introduce PHASE, a modular deep-learning framework for
physics-integrated, heterogeneity-aware surrogates in scientific simulations.
PHASE combines data-type-aware encoders for heterogeneous inputs with
multi-level physics-based constraints that promote consistency from local
dynamics to global system behavior. We validate PHASE on the biogeochemical
(BGC) spin-up workflow of the U.S. Department of Energy's Energy Exascale Earth
System Model (E3SM) Land Model (ELM), presenting-to our knowledge-the first
scientifically validated AI-accelerated solution for this task. Using only the
first 20 simulation years, PHASE infers a near-equilibrium state that otherwise
requires more than 1,200 years of integration, yielding an effective reduction
in required integration length by at least 60x. The framework is enabled by a
pipeline for fusing heterogeneous scientific data and demonstrates strong
generalization to higher spatial resolutions with minimal fine-tuning. These
results indicate that PHASE captures governing physical regularities rather
than surface correlations, enabling practical, physically consistent
acceleration of land-surface modeling and other complex scientific workflows.

</details>


### [92] [GeoFunFlow: Geometric Function Flow Matching for Inverse Operator Learning over Complex Geometries](https://arxiv.org/abs/2509.24117)
*Sifan Wang,Zhikai Wu,David van Dijk,Lu Lu*

Main category: cs.LG

TL;DR: GeoFunFlow is a geometric diffusion model framework that solves inverse PDE problems on complex geometries using a geometric function autoencoder and latent diffusion model, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Inverse PDE problems are challenging due to ill-posedness, data sparsity, and complex geometries. Classical methods are computationally expensive, while existing learning approaches are limited to regular domains or forward modeling.

Method: Combines a geometric function autoencoder (GeoFAE) with Perceiver module for processing unstructured meshes, and a latent diffusion model trained via rectified flow for posterior sampling from sparse noisy data.

Result: Achieves state-of-the-art reconstruction accuracy on complex geometries across five benchmarks, provides calibrated uncertainty quantification, and offers efficient inference compared to baselines.

Conclusion: GeoFunFlow provides an effective framework for solving inverse PDE problems on complex geometries with superior accuracy, uncertainty quantification, and computational efficiency.

Abstract: Inverse problems governed by partial differential equations (PDEs) are
crucial in science and engineering. They are particularly challenging due to
ill-posedness, data sparsity, and the added complexity of irregular geometries.
Classical PDE-constrained optimization methods are computationally expensive,
especially when repeated posterior sampling is required. Learning-based
approaches improve efficiency and scalability, yet most are designed for
regular domains or focus on forward modeling. Here, we introduce {\em
GeoFunFlow}, a geometric diffusion model framework for inverse problems on
complex geometries. GeoFunFlow combines a novel geometric function autoencoder
(GeoFAE) and a latent diffusion model trained via rectified flow. GeoFAE
employs a Perceiver module to process unstructured meshes of varying sizes and
produces continuous reconstructions of physical fields, while the diffusion
model enables posterior sampling from sparse and noisy data. Across five
benchmarks, GeoFunFlow achieves state-of-the-art reconstruction accuracy over
complex geometries, provides calibrated uncertainty quantification, and
delivers efficient inference compared to operator-learning and diffusion model
baselines.

</details>


### [93] [DRIFT-Net: A Spectral--Coupled Neural Operator for PDEs Learning](https://arxiv.org/abs/2509.24868)
*Jiayi Li,Flora D. Salim*

Main category: cs.LG

TL;DR: DRIFT-Net is a dual-branch neural PDE solver that combines spectral and image branches to capture global low-frequency information and local details, reducing error accumulation and improving performance over attention-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models for PDEs using multi-scale windowed self-attention suffer from weak global coupling due to locality, leading to error accumulation and drift during closed-loop rollouts.

Method: Proposes DRIFT-Net with dual-branch design: spectral branch for global low-frequency information and image branch for local details. Uses controlled lightweight mixing in low-frequency range and bandwise weighting fusion to avoid width inflation and instability.

Result: Achieves 7%-54% lower relative L1 error, 15% fewer parameters, and higher throughput than scOT baseline on Navier-Stokes benchmarks under identical training settings.

Conclusion: DRIFT-Net effectively preserves both global structure and high-frequency details across scales, demonstrating improved stability and effectiveness for PDE learning compared to attention-based methods.

Abstract: Learning PDE dynamics with neural solvers can significantly improve
wall-clock efficiency and accuracy compared with classical numerical solvers.
In recent years, foundation models for PDEs have largely adopted multi-scale
windowed self-attention, with the scOT backbone in \textsc{Poseidon} serving as
a representative example.
  However, because of their locality, truly globally consistent spectral
coupling can only be propagated gradually through deep stacking and window
shifting. This weakens global coupling and leads to error accumulation and
drift during closed-loop rollouts. To address this, we propose
\textbf{DRIFT-Net}. It employs a dual-branch design comprising a spectral
branch and an image branch. The spectral branch is responsible for capturing
global, large-scale low-frequency information, whereas the image branch focuses
on local details and nonstationary structures. Specifically, we first perform
controlled, lightweight mixing within the low-frequency range. Then we fuse the
spectral and image paths at each layer via bandwise weighting, which avoids the
width inflation and training instability caused by naive concatenation. The
fused result is transformed back into the spatial domain and added to the image
branch, thereby preserving both global structure and high-frequency details
across scales. Compared with strong attention-based baselines, DRIFT-Net
achieves lower error and higher throughput with fewer parameters under
identical training settings and budget. On Navier--Stokes benchmarks, the
relative $L_{1}$ error is reduced by 7\%--54\%, the parameter count decreases
by about 15\%, and the throughput remains higher than scOT. Ablation studies
and theoretical analyses further demonstrate the stability and effectiveness of
this design. The code is available at
https://github.com/cruiseresearchgroup/DRIFT-Net.

</details>


### [94] [Deep Learning for Subspace Regression](https://arxiv.org/abs/2509.23249)
*Vladimir Fanaskov,Vladislav Trifonov,Alexander Rudikov,Ekaterina Muravleva,Ivan Oseledets*

Main category: cs.LG

TL;DR: Proposes neural network-based subspace regression for parametric model reduction, using redundant subspace prediction to improve accuracy and handle high-dimensional parameter spaces.


<details>
  <summary>Details</summary>
Motivation: Traditional interpolation methods for parametric model reduction become infeasible in high-dimensional parameter spaces, requiring more robust approaches.

Method: Relax interpolation to regression, use neural networks with specialized loss functions for subspace data, and predict redundant larger subspaces to simplify learning.

Result: Redundant subspace prediction decreases mapping complexity and improves accuracy; method works for eigenproblems, PDEs, optimal control, and deflation techniques.

Conclusion: Subspace regression with neural networks and redundant prediction is effective for parametric model reduction in high-dimensional parameter spaces.

Abstract: It is often possible to perform reduced order modelling by specifying linear
subspace which accurately captures the dynamics of the system. This approach
becomes especially appealing when linear subspace explicitly depends on
parameters of the problem. A practical way to apply such a scheme is to compute
subspaces for a selected set of parameters in the computationally demanding
offline stage and in the online stage approximate subspace for unknown
parameters by interpolation. For realistic problems the space of parameters is
high dimensional, which renders classical interpolation strategies infeasible
or unreliable. We propose to relax the interpolation problem to regression,
introduce several loss functions suitable for subspace data, and use a neural
network as an approximation to high-dimensional target function. To further
simplify a learning problem we introduce redundancy: in place of predicting
subspace of a given dimension we predict larger subspace. We show theoretically
that this strategy decreases the complexity of the mapping for elliptic
eigenproblems with constant coefficients and makes the mapping smoother for
general smooth function on the Grassmann manifold. Empirical results also show
that accuracy significantly improves when larger-than-needed subspaces are
predicted. With the set of numerical illustrations we demonstrate that subspace
regression can be useful for a range of tasks including parametric
eigenproblems, deflation techniques, relaxation methods, optimal control and
solution of parametric partial differential equations.

</details>


### [95] [Sketching Low-Rank Plus Diagonal Matrices](https://arxiv.org/abs/2509.23587)
*Andres Fernandez,Felix Dangel,Philipp Hennig,Frank Schneider*

Main category: cs.LG

TL;DR: SKETCHLORD is a method that simultaneously estimates both low-rank and diagonal components of linear operators, outperforming sequential approaches and providing high-fidelity approximations for large-scale operators.


<details>
  <summary>Details</summary>
Motivation: Many machine learning and scientific computing tasks involve high-dimensional linear operators that are costly to evaluate via matrix-vector products. While sketched methods can construct low-rank or diagonal approximations, they introduce errors due to assuming simpler structures.

Method: SKETCHLORD jointly estimates low-rank and diagonal components through a convex optimization formulation, enabling scalable algorithm implementation for Low-Rank plus Diagonal (LoRD) operators.

Result: Theoretical and empirical analysis shows SKETCHLORD's joint estimation is superior to sequential variants (diagonal-then-low-rank or low-rank-then-diagonal), with comprehensive experiments on synthetic LoRD matrices confirming accurate structure recovery.

Conclusion: SKETCHLORD provides a valuable addition to structured approximation tools, particularly for high-fidelity approximations of large-scale operators like deep learning Hessians.

Abstract: Many relevant machine learning and scientific computing tasks involve
high-dimensional linear operators accessible only via costly matrix-vector
products. In this context, recent advances in sketched methods have enabled the
construction of *either* low-rank *or* diagonal approximations from few
matrix-vector products. This provides great speedup and scalability, but
approximation errors arise due to the assumed simpler structure. This work
introduces SKETCHLORD, a method that simultaneously estimates both low-rank
*and* diagonal components, targeting the broader class of Low-Rank *plus*
Diagonal (LoRD) linear operators. We demonstrate theoretically and empirically
that this joint estimation is superior also to any sequential variant
(diagonal-then-low-rank or low-rank-then-diagonal). Then, we cast SKETCHLORD as
a convex optimization problem, leading to a scalable algorithm. Comprehensive
experiments on synthetic (approximate) LoRD matrices confirm SKETCHLORD's
performance in accurately recovering these structures. This positions it as a
valuable addition to the structured approximation toolkit, particularly when
high-fidelity approximations are desired for large-scale operators, such as the
deep learning Hessian.

</details>


### [96] [MDD-Thinker: Towards Large Reasoning Models for Major Depressive Disorder Diagnosis](https://arxiv.org/abs/2509.24217)
*Yuyang Sha,Hongxin Pan,Gang Luo,Caijuan Shi,Jing Wang,Kefeng Li*

Main category: cs.LG

TL;DR: MDD-Thinker is an LLM-based diagnostic framework that integrates supervised fine-tuning with reinforcement learning to enhance depression diagnosis accuracy and interpretability, achieving superior performance over traditional methods and general-purpose LLMs.


<details>
  <summary>Details</summary>
Motivation: Current MDD diagnostic approaches rely on subjective assessments and lack multimodal integration. LLMs offer potential for improved accuracy but face challenges with interpretability, hallucination, and synthetic data reliance.

Method: Developed MDD-Thinker using supervised fine-tuning and reinforcement learning on 40,000 reasoning samples from UK Biobank plus 10,000 from public mental health datasets. Evaluated against machine learning, deep learning, and state-of-the-art LLM baselines.

Result: Achieved accuracy of 0.8268 and F1-score of 0.8081, significantly outperforming traditional methods and general-purpose LLMs. Combined SFT+RL yielded 29.0% accuracy improvement, 38.1% F1-score gain, and 34.8% AUC increase. Demonstrated comparable reasoning to larger LLMs with better efficiency.

Conclusion: MDD-Thinker represents the first reasoning-enhanced LLM framework for MDD diagnosis using real-world clinical data, balancing accuracy, interpretability, and efficiency for scalable psychiatric diagnostics.

Abstract: Background Major depressive disorder (MDD) is a leading cause of global
disability, yet current diagnostic approaches often rely on subjective
assessments and lack the ability to integrate multimodal clinical information.
Large language models (LLMs) hold promise for enhancing diagnostic accuracy
through advanced reasoning but face challenges in interpretability,
hallucination, and reliance on synthetic data.
  Methods We developed MDD-Thinker, an LLM-based diagnostic framework that
integrates supervised fine-tuning (SFT) with reinforcement learning (RL) to
strengthen reasoning ability and interpretability. Using the UK Biobank
dataset, we generated 40,000 reasoning samples, supplemented with 10,000
samples from publicly available mental health datasets. The model was
fine-tuned on these reasoning corpora, and its diagnostic and reasoning
performance was evaluated against machine learning, deep learning, and
state-of-the-art LLM baselines.
  Findings MDD-Thinker achieved an accuracy of 0.8268 and F1-score of 0.8081,
significantly outperforming traditional baselines such as SVM and MLP, as well
as general-purpose LLMs. Incorporating both SFT and RL yielded the greatest
improvements, with relative gains of 29.0% in accuracy, 38.1% in F1-score, and
34.8% in AUC. Moreover, the model demonstrated comparable reasoning performance
compared to much larger LLMs, while maintaining computational efficiency.
  Interpretation This study presents the first reasoning-enhanced LLM framework
for MDD diagnosis trained on large-scale real-world clinical data. By
integrating SFT and RL, MDD-Thinker balances accuracy, interpretability, and
efficiency, offering a scalable approach for intelligent psychiatric
diagnostics. These findings suggest that reasoning-oriented LLMs can provide
clinically reliable support for MDD detection and may inform broader
applications in mental health care.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [97] [A critical stochastic heat equation with long-range noise](https://arxiv.org/abs/2509.23790)
*Alexander Dunlap,Martin Hairer,Xue-Mei Li*

Main category: math.PR

TL;DR: Semilinear stochastic heat equation in high dimensions with long-range correlated noise, analyzed through diffusive scaling with logarithmic noise attenuation, showing pointwise statistics approximated by FBSDE solution.


<details>
  <summary>Details</summary>
Motivation: To understand the statistical behavior of semilinear stochastic heat equations with long-range correlated noise in high spatial dimensions, extending previous work on 2D case with space-time white noise.

Method: Apply diffusive scaling limit with logarithmic attenuation of noise, then approximate pointwise statistics using forward-backward stochastic differential equations (FBSDE).

Result: Pointwise statistics of the solution can be approximated by the solution to a specific FBSDE, different from previous results due to long-range noise correlations.

Conclusion: The scaling approach successfully captures statistical behavior of stochastic heat equations with long-range correlated noise, yielding different FBSDE structure compared to space-time white noise case.

Abstract: We consider a semilinear stochastic heat equation in spatial dimension at
least $3$, forced by a noise that is white in time with a covariance kernel
that decays like $\lvert x\rvert^{-2}$ as $\lvert x\rvert\to\infty$. We show
that in an appropriate diffusive scaling limit with a logarithmic attenuation
of the noise, the pointwise statistics of the solution can be approximated by
the solution to a forward-backward stochastic differential equation (FBSDE).
The scaling and structure of the problem is similar to that of the
two-dimensional stochastic heat equation forced by an approximation of
space-time white noise considered by the first author and Gu (Ann. Probab.,
2022). However the resulting FBSDE is different due to the long-range
correlations of the noise.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [98] [Strong Basin of Attraction for Unmixing Kernels With the Variable Projection Method](https://arxiv.org/abs/2509.24428)
*Santos Michelena,Maxime Ferreira Da Costa,José Picheral*

Main category: eess.SP

TL;DR: The paper studies PSF unmixing for spike signals on parametric manifolds with known spike locations, proposing a projected non-linear least squares estimator with theoretical guarantees and practical validation on LIBS data.


<details>
  <summary>Details</summary>
Motivation: To address the problem of recovering mixed spike signals convolved with distinct PSFs on parametric manifolds, eliminating the need for manual calibration in applications like laser-induced breakdown spectroscopy.

Method: Formulates PSF unmixing as a projected non-linear least squares estimator, establishes theoretical bounds on strong convexity region radius based on manifold coherence and Lipschitz properties.

Result: Numerical experiments show rapid decay of PSF class in problem conditioning, confirming theoretical analysis. The method successfully applied to real-world LIBS data, demonstrating practical relevance.

Conclusion: The proposed estimator provides guaranteed convergence and stability for PSF unmixing, enabling automated calibration in spectroscopic applications with validated theoretical foundations and practical performance.

Abstract: The problem of recovering a mixture of spike signals convolved with distinct
point spread functions (PSFs) lying on a parametric manifold, under the
assumption that the spike locations are known, is studied. The PSF unmixing
problem is formulated as a projected non-linear least squares estimator. A
lower bound on the radius of the region of strong convexity is established in
the presence of noise as a function of the manifold coherence and Lipschitz
properties, guaranteeing convergence and stability of the optimization program.
Numerical experiments highlight the speed of decay of the PSF class in the
problem's conditioning and confirm theoretical findings. Finally, the proposed
estimator is deployed on real-world spectroscopic data from laser-induced
breakdown spectroscopy (LIBS), removing the need for manual calibration and
validating the method's practical relevance.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [99] [Domains with globally exponentially integrable parabolic forward-in-time BMO](https://arxiv.org/abs/2509.24486)
*Kim Myyryläinen,Tuomas Oikari,Olli Saari*

Main category: math.CA

TL;DR: The paper characterizes open spacetime sets where parabolic forward-in-time BMO functions belong to a forward-in-time exponential integrability class, using connectivity assumptions and a growth bound on forward quasihyperbolic distance.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between parabolic forward-in-time BMO functions and exponential integrability in spacetime domains, providing conditions under which these function classes coincide.

Method: Uses qualitative connectivity assumptions on the domain and formulates characterization through quantitative growth bounds on a forward-in-time version of the classical quasihyperbolic distance.

Result: Establishes a characterization theorem showing when parabolic forward-in-time BMO functions are in forward-in-time exponential integrability class based on domain connectivity and quasihyperbolic distance growth.

Conclusion: The characterization provides necessary and sufficient conditions for the inclusion relationship between these function spaces in spacetime domains with appropriate connectivity properties.

Abstract: We characterize those open sets of the space time in which parabolic
forward-in-time BMO functions are in a certain forward-in-time exponential
integrability class. The characterization holds under qualitative connectivity
assumptions on the domain and is formulated in terms of a quantitative growth
bound on a forward-in-time version of the classical quasihyperbolic distance.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [100] [Graph-Based Learning of Free Surface Dynamics in Generalized Newtonian Fluids using Smoothed Particle Hydrodynamics](https://arxiv.org/abs/2509.24264)
*Hyo-Jin Kim,Jaekwang Kim,Hyung-Jun Park*

Main category: physics.flu-dyn

TL;DR: A graph neural network (GNN) model is proposed for efficient prediction of non-Newtonian power-law fluid flow with free surface dynamics, trained on SPH simulation data to accelerate computations while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional algorithms for Newtonian fluids struggle with non-Newtonian cases where viscosity varies dynamically. Power-law fluids with exponentially decreasing viscosity are particularly challenging, especially in free surface flow scenarios where computational complexity increases.

Method: A novel GNN-based numerical model trained on SPH simulation data, learning particle acceleration effects from SPH interactions based on fluid's power-law parameters. Uses particle-based SPH method which has advantages over grid-based techniques like FEM for free surface flows.

Result: The GNN significantly accelerates computations while maintaining reliable accuracy in benchmark tests including dam-break and droplet impact simulations.

Conclusion: The results demonstrate the potential of GNN-based simulation frameworks for efficiently modeling non-Newtonian fluid behavior, paving the way for future advancements in data-driven fluid simulations.

Abstract: In this study, we propose a graph neural network (GNN) model for efficiently
predicting the flow behavior of non-Newtonian fluids with free surface
dynamics. The numerical analysis of non-Newtonian fluids presents significant
challenges, as traditional algorithms designed for Newtonian fluids with
constant viscosity often struggle to converge when applied to non-Newtonian
cases, where rheological properties vary dynamically with flow conditions.
Among these, power-law fluids exhibit viscosity that decreases exponentially as
the shear rate increases, making numerical simulations particularly difficult.
The complexity further escalates in free surface flow scenarios, where
computational challenges intensify. In such cases, particle-based methods like
smoothed particle hydrodynamics (SPH) provide advantages over traditional
grid-based techniques, such as the finite element method (FEM). Building on
this approach, we introduce a novel GNN-based numerical model to enhance the
computational efficiency of non-Newtonian power-law fluid flow simulations. Our
model is trained on SPH simulation data, learning the effects of particle
accelerations in the presence of SPH interactions based on the fluid's
power-law parameters. The GNN significantly accelerates computations while
maintaining reliable accuracy in benchmark tests, including dam-break and
droplet impact simulations. The results underscore the potential of GNN-based
simulation frameworks for efficiently modeling non-Newtonian fluid behavior,
paving the way for future advancements in data-driven fluid simulations.

</details>


### [101] [Direct numerical simulation of two-phase flows with surfactant-induced surface viscous effects](https://arxiv.org/abs/2509.24722)
*Debashis Panda,Seungwon Shin,Abdullah M. Abdal,Lyes Kahouadji,Jalel Chergui,Damir Juric,Omar K. Matar*

Main category: physics.flu-dyn

TL;DR: This paper presents a numerical simulation framework using the Level Contour Reconstruction Method (LCRM) to study interfacial flows with surfactant-induced surface viscous stresses, including shear and dilatational viscosities.


<details>
  <summary>Details</summary>
Motivation: To develop a rigorous approach for modeling interfacial dynamics with surfactant-induced complexities, particularly surface viscous stresses that resist deformation and interfacial compressibility effects.

Method: Uses the Level Contour Reconstruction Method (LCRM), a hybrid front-tracking/level-set approach, combined with the Boussinesq-Scriven constitutive model to capture interfacial mechanics including surface diffusion, Marangoni stresses, and surface viscous stresses.

Result: Validated numerical predictions against benchmark cases involving deforming drops in flow fields and buoyancy-driven rising. Demonstrated significant effects of surface viscous stresses on interfacial deformation in unsteady parametric surface waves and atomisation events.

Conclusion: The study highlights the importance of adopting rigorous approaches in modeling interfacial dynamics and shows that surface viscous stresses play crucial roles in interfacial deformation phenomena.

Abstract: Direct numerical simulations of interfacial flows with surfactant-induced
complexities involving surface viscous stresses are performed within the
framework of the Level Contour Reconstruction Method (LCRM); this hybrid
front-tracking/level-set approach leverages the advantages of both methods. In
addition to interface-confined surfactant transport that results in surface
diffusion and Marangoni stresses, the interface, endowed with shear and
dilatational viscosities; these act to resist deformation arising from velocity
gradients in the plane of the two-dimensional manifold of the interface, and
interfacial compressibility effects, respectively. By adopting the
Boussinesq-Scriven constitutive model, We provide a mathematical formulation of
these effects that accurately captures the interfacial mechanics, which is then
implemented within the LCRM-based code by exploiting the benefits inherent to
the underlying front-tracking/level-sets hybrid approach. We validate our
numerical predictions against a number of benchmark cases that involve drops
undergoing deformation when subjected to a flow field or when rising under the
action of buoyancy. The results of these validation studies highlight the
importance of adopting a rigorous approach in modelling the interfacial
dynamics. We also present results that demonstrate the effects of surface
viscous stresses on interfacial deformation in unsteady parametric surface
waves and atomisation events.

</details>


### [102] [Advancing three dimensional modeling of adsorption physics in porous media for gas separation towards a new design of fixed bed adsorbers](https://arxiv.org/abs/2509.25062)
*Mohamad Najib Nadamani,Mostafa Safdari Shadloo,Talib Dbouk*

Main category: physics.flu-dyn

TL;DR: A new 3D multiphase CFD model for adsorption in porous media is developed with novel volumetric source terms that account for pore adsorption occupation rate. The model accurately predicts breakthrough curves and thermal fronts in CO2 adsorption on Zeolite-13X, and shows improved performance in new 3D fixed-bed designs.


<details>
  <summary>Details</summary>
Motivation: To develop a more accurate computational model for adsorption physics that can account for the impact of gas loading (pore adsorption occupation rate) in porous media, which previous models didn't properly address.

Method: Developed a 3D multiphase CFD model with new volumetric source terms integrated into multi-species gas transport and energy conservation equations. Validated through transient 3D simulations at atmospheric pressure with different CO2-He gas mixtures, comparing with experimental data from Zeolite-13X fixed-bed adsorption.

Result: The model accurately predicts breakthrough curves and thermal front propagation inside the bed. When applied to a new 3D fixed-bed design, it outperforms traditional cylindrical designs by shortening adsorption periods in pressure/temperature swing processes, increasing overall gas separation productivity.

Conclusion: The new CFD model successfully captures adsorption physics with improved accuracy and demonstrates practical benefits in gas separation process design, particularly through optimized 3D geometries that enhance surface area and process efficiency.

Abstract: A new three-dimensional (3D) multiphase computational fluid dynamics (CFD)
model for adsorption physics in porous media is developed and validated. The
model is constituted at a macroscopic scale that integrates new volumetric
source terms in the multi-species gas transport and energy conservation
equations. These new terms, for the first time, take into account the impact of
pores adsorption occupation rate (PAOR), or gas loading. Transient 3D
simulations are performed at an atmospheric pressure of about 1.02 bar for
different CO2-He gas mixture feed-in compositions (100%, 50%, and 15% CO2). The
3D model validation is conducted through quantitative comparisons with
experimental data from the literature for CO2 adsorption on porous Zeolite-13X
beads in a cylindrical fixed-bed. Results demonstrate the new model's ability
to accurately predict the breakthrough curves and the thermal front propagation
inside the bed. Finally, the new CFD model is applied to investigate CO2
capture in a new 3D design of fixed-bed adsorbers of equivalent adsorbent
material volume. The new design outperformed the reference cylindrical design
thanks to its new geometry with higher surface area. This allows to shorten the
adsorption periods in pressure and temperature swing adsorption processes and
thus increase the overall gas separation process productivity.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [103] [MAD: Manifold Attracted Diffusion](https://arxiv.org/abs/2509.24710)
*Dennis Elbrächter,Giovanni S. Alberti,Matteo Santacesaria*

Main category: stat.ML

TL;DR: The paper proposes a modified inference procedure for score-based diffusion models to generate noiseless samples when training data contains noise, leveraging the manifold hypothesis to distinguish between noise (off-manifold variations) and meaningful data (on-manifold variations).


<details>
  <summary>Details</summary>
Motivation: Training data often comes from noisy versions of target distributions, and standard diffusion models may generate samples that retain this noise. The manifold hypothesis suggests that meaningful data lies on low-dimensional manifolds while noise appears as small off-manifold variations.

Method: Introduces an extended score concept that can reduce small variations (noise) to zero while preserving large variations (meaningful data). Shows how to efficiently compute this extended score approximation from standard score approximations.

Result: Demonstrates efficacy on toy problems, synthetic data, and real data, showing the method can effectively generate noiseless samples from noisy training distributions.

Conclusion: The proposed extended score modification provides an efficient way to generate clean samples from noisy training data by leveraging the manifold structure to distinguish between noise and meaningful signal.

Abstract: Score-based diffusion models are a highly effective method for generating
samples from a distribution of images. We consider scenarios where the training
data comes from a noisy version of the target distribution, and present an
efficiently implementable modification of the inference procedure to generate
noiseless samples. Our approach is motivated by the manifold hypothesis,
according to which meaningful data is concentrated around some low-dimensional
manifold of a high-dimensional ambient space. The central idea is that noise
manifests as low magnitude variation in off-manifold directions in contrast to
the relevant variation of the desired distribution which is mostly confined to
on-manifold directions. We introduce the notion of an extended score and show
that, in a simplified setting, it can be used to reduce small variations to
zero, while leaving large variations mostly unchanged. We describe how its
approximation can be computed efficiently from an approximation to the standard
score and demonstrate its efficacy on toy problems, synthetic data, and real
data.

</details>


### [104] [On Spectral Learning for Odeco Tensors: Perturbation, Initialization, and Algorithms](https://arxiv.org/abs/2509.25126)
*Arnab Auddy,Ming Yuan*

Main category: stat.ML

TL;DR: Spectral learning for orthogonally decomposable tensors, focusing on statistical limits, optimization geometry, and initialization challenges.


<details>
  <summary>Details</summary>
Motivation: To understand the interplay between statistical limits, optimization geometry, and initialization in spectral learning for orthogonally decomposable tensors, addressing robustness under noise and computational bottlenecks.

Method: Study perturbation bounds, non-convex optimization analysis, and initialization strategies for odeco tensors, comparing with iterative methods like tensor power iterations.

Result: Recovery for odeco tensors does not depend on eigengaps, providing improved robustness under noise. Initialization emerges as the main computational bottleneck despite statistical efficiency of iterative methods.

Conclusion: Clarifies conditions when efficient algorithms achieve statistical limits versus when fundamental barriers persist in odeco tensor recovery.

Abstract: We study spectral learning for orthogonally decomposable (odeco) tensors,
emphasizing the interplay between statistical limits, optimization geometry,
and initialization. Unlike matrices, recovery for odeco tensors does not hinge
on eigengaps, yielding improved robustness under noise. While iterative methods
such as tensor power iterations can be statistically efficient, initialization
emerges as the main computational bottleneck. We investigate perturbation
bounds, non-convex optimization analysis, and initialization strategies,
clarifying when efficient algorithms attain statistical limits and when
fundamental barriers remain.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [105] [Unlicensed Band Allocation for Heterogeneous Networks](https://arxiv.org/abs/2509.23216)
*Po-Heng Chou*

Main category: cs.NI

TL;DR: This paper analyzes unlicensed band allocation schemes for LAA-Wi-Fi coexistence networks, evaluating four allocation methods (UFA, UTA, UFAB, UTAB) to optimize QoS and provide design guidelines.


<details>
  <summary>Details</summary>
Motivation: LAA and Wi-Fi systems interfere when sharing the same unlicensed channels in heterogeneous networks, making unlicensed band allocation crucial for maintaining quality of service for both systems.

Method: The authors propose an analytical model and conduct simulation experiments to study four unlicensed band allocation schemes: UFA, UTA, UFAB, and UTAB for LAA data packets.

Result: The study evaluates performance in terms of acceptance rate of both LAA and Wi-Fi packet data in the LAA buffer queue, providing quantitative analysis of different allocation strategies.

Conclusion: The research provides guidelines for designing the channel occupation phase and buffer size of LAA small cells to optimize coexistence with Wi-Fi systems in unlicensed bands.

Abstract: Based on the License-Assisted Access (LAA) small cell architecture, the LAA
coexisting with Wi-Fi heterogeneous networks provides LTE mobile users with
high bandwidth efficiency as the unlicensed channels are shared among LAA and
Wi-Fi. However, LAA and Wi-Fi interfere with each other when both systems use
the same unlicensed channel in heterogeneous networks. In such a network,
unlicensed band allocation for LAA and Wi-Fi is an important issue that may
affect the quality of service (QoS) of both systems significantly. In this
paper, we propose an analytical model and conduct simulation experiments to
study four allocations for the unlicensed band: unlicensed full allocation
(UFA), unlicensed time-division allocation (UTA), and UFA/UTA with buffering
mechanism (UFAB and UTAB) for the LAA data packets. We evaluate the performance
of these unlicensed band allocation schemes in terms of the acceptance rate of
both LAA and Wi-Fi packet data in the LAA buffer queue. Our study provides
guidelines for designing the channel occupation phase and the buffer size of
the LAA small cell.

</details>


### [106] [Modeling the Unlicensed Band Allocation for LAA With Buffering Mechanism](https://arxiv.org/abs/2509.23217)
*Po-Heng Chou*

Main category: cs.NI

TL;DR: Analytical model and simulations for LAA-WiFi coexistence in unlicensed bands with buffering mechanism, focusing on acceptance rates and design guidelines.


<details>
  <summary>Details</summary>
Motivation: Unlicensed band allocation between LAA and WiFi affects QoS for both systems, requiring performance evaluation and design guidelines.

Method: Developed analytical model and conducted simulation experiments for listen-before-talk based unlicensed band allocation with buffering mechanism.

Result: Evaluated performance in terms of acceptance rates for both LAA and WiFi packets.

Conclusion: Provides design guidelines for channel occupation phase and buffer threshold in LAA systems.

Abstract: In this letter, we propose an analytical model and conduct simulation
experiments to study listen-before-talk-based unlicensed band allocation with
the buffering mechanism for the License-Assisted Access (LAA) packets in the
heterogeneous networks. In such a network, unlicensed band allocation for LAA
and Wi-Fi is an important issue, which may affect the quality of service for
both systems significantly. We evaluate the performance of these unlicensed
band allocations in terms of the acceptance rate of both LAA and Wi-Fi packets.
This letter provides the guidelines for designing the channel occupation phase
and buffer threshold of the LAA systems.

</details>


### [107] [Markov Modeling for Licensed and Unlicensed Band Allocation in Underlay and Overlay D2D](https://arxiv.org/abs/2509.23218)
*Po-Heng Chou,Yen-Ting Liu,Wei-Chang Chen,Walid Saad*

Main category: cs.NI

TL;DR: A novel analytical model for resource allocation in D2D-assisted cellular networks that handles both underlay/overlay D2D systems and unlicensed band sharing with Wi-Fi, using threshold-based flow control to guarantee Wi-Fi QoS.


<details>
  <summary>Details</summary>
Motivation: To address resource allocation challenges in D2D-assisted cellular networks, including licensed band sharing for cellular traffic offloading and unlicensed band sharing with Wi-Fi systems while maintaining quality of service.

Method: Developed an analytical model with global system state reflecting interactions among D2D, cellular, and Wi-Fi packets, using threshold-based flow control under standard traffic model assumptions.

Result: Simulation shows the scheme slightly sacrifices conventional cellular performance to significantly improve overlay D2D performance while maintaining Wi-Fi user performance, with more flexible D2D-Wi-Fi adjustments than underlay schemes.

Conclusion: The proposed model effectively balances performance trade-offs between D2D, cellular, and Wi-Fi systems, providing flexible resource allocation while maintaining QoS guarantees.

Abstract: In this paper, a novel analytical model for resource allocation is proposed
for a device-to-device (D2D) assisted cellular network. The proposed model can
be applied to underlay and overlay D2D systems for sharing licensed bands and
offloading cellular traffic. The developed model also takes into account the
problem of unlicensed band sharing with Wi-Fi systems. In the proposed model, a
global system state reflects the interaction among D2D, conventional cellular,
and Wi-Fi packets. Under the standard traffic model assumptions, a
threshold-based flow control is proposed for guaranteeing the
quality-of-service (QoS) of Wi-Fi. The packet blockage probability is then
derived. Simulation results show the proposed scheme sacrifices conventional
cellular performance slightly to improve overlay D2D performance significantly
while maintaining the performance for Wi-Fi users. Meanwhile, the proposed
scheme has more flexible adjustments between D2D and Wi-Fi than the underlay
scheme.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [108] [Avalanches in Magnetohydrodynamical simulations](https://arxiv.org/abs/2509.25066)
*Henri Lamarre,Paul Charbonneau,Quentin Noraz,Antoine Strugarek,Alexis Blaise,Allan Sacha Brun,Mats Carlsson,Boris Vilhelm Gudiksen*

Main category: astro-ph.SR

TL;DR: Study demonstrates scale invariance in magnetic energy dissipation across MHD simulations, showing universal power-law distributions for energy dissipation and lifetimes in current sheets.


<details>
  <summary>Details</summary>
Motivation: To understand scale invariance in natural systems like solar flares and leverage computational advances to study magnetic energy dissipation processes across multiple scales.

Method: Analyzed current sheets in two distinct MHD simulations with different numerical and physical setups, comparing dissipative events over time.

Result: Dissipative events exhibit scale invariance with power-law distributions for energy dissipation and lifetimes, consistent across both simulations. Low plasma-beta regions show avalanche-like dynamics suggesting self-organized criticality.

Conclusion: Magnetic energy dissipation processes demonstrate universality across different physical conditions, with low plasma-beta regions exhibiting self-organized criticality behavior similar to avalanche models.

Abstract: Scale invariance is a hallmark of many natural systems, including solar
flares, where energy release spans a vast range of scales. Recent computational
advances, at the level of both algorithmics and hardware, have enabled
high-resolution magnetohydrodynamical (MHD) simulations to span multiple
scales, offering new insights into magnetic energy dissipation processes. Here,
we study scale invariance of magnetic energy dissipation in two distinct MHD
simulations. Current sheets are identified and analyzed over time. Results
demonstrate that dissipative events exhibit scale invariance, with power-law
distributions characterizing their energy dissipation and lifetimes.
Remarkably, these distributions are consistent across the two simulations,
despite differing numerical and physical setups, suggesting universality in the
process of magnetic energy dissipation. Comparisons between the evolution of
dissipation regions reveals distinct growth behaviors in high plasma-beta
regions (convective zone) and low plasma-beta regions (atmosphere). The latter
display spatiotemporal dynamics similar to those of avalanche models,
suggesting self-organized criticality and a common universality class.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [109] [Tensor Network Markov Chain Monte Carlo: Efficient Sampling of Three-Dimensional Spin Glasses and Beyond](https://arxiv.org/abs/2509.23945)
*Tao Chen,Jing Liu,Youjin Deng,Pan Zhang*

Main category: cond-mat.stat-mech

TL;DR: TNMCMC uses tensor networks on 2D slices to generate large-scale collective updates in MCMC, dramatically improving sampling efficiency for 3D spin glasses and overcoming exponential barriers in first-order transitions.


<details>
  <summary>Details</summary>
Motivation: Standard Monte Carlo methods are severely inefficient for 3D spin glasses due to rugged energy landscapes, critical slowing down, and ergodicity breaking, particularly for large systems at low temperatures.

Method: Tensor Network Markov Chain Monte Carlo (TNMCMC) generates large-scale collective updates using tensor networks on 2D slices of the 3D lattice, improving autocorrelation time and sampling efficiency.

Result: TNMCMC dramatically suppresses critical slowing down in 3D spin glasses up to size 64×64×64 using a single CPU, and efficiently traverses exponential barriers in 3-state Potts model first-order transitions where conventional MCMC fails.

Conclusion: TNMCMC opens a promising path for tackling formidable 3D problems in statistical physics, offering orders-of-magnitude speed-ups over conventional MCMC methods.

Abstract: Sampling the three-dimensional (3D) spin glass -- i.e., generating
equilibrium configurations of a 3D lattice with quenched random couplings -- is
widely regarded as one of the central and long-standing open problems in
statistical physics. The rugged energy landscape, pronounced critical slowing
down, and intrinsic ergodicity breaking render standard Monte Carlo methods
severely inefficient, particularly for large systems at low temperatures. In
this work, we introduce the Tensor Network Markov Chain Monte Carlo (TNMCMC)
approach to address the issue. It generates large-scale collective updates in
MCMC using tensor networks on the 2D slices of the 3D lattice, greatly
improving the autocorrelation time and offering orders-of-magnitude speed-ups
over conventional MCMC in generating unbiased samples of the Boltzmann
distribution. We conduct numerical experiments on 3D spin glasses up to system
size $64\times 64\times 64$ using a single CPU, and show that TNMCMC
dramatically suppresses critical slowing down in large disordered systems,
which usually require a supercomputer to perform MCMC simulations. Furthermore,
we apply our approach to the 3-state Potts model up to system size $64\times
64\times 64$ using a single CPU, and show that the TNMCMC approach efficiently
traverses the exponential barriers of the strong first-order transition,
whereas conventional MCMC fails. Our results reveal that TNMCMC opens a
promising path toward tackling long-standing, formidable three-dimensional
problems in statistical physics.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [110] [SymBoltz.jl: a symbolic-numeric, approximation-free and differentiable linear Einstein-Boltzmann solver](https://arxiv.org/abs/2509.24740)
*Herman Sletmoen*

Main category: astro-ph.CO

TL;DR: SymBoltz is a Julia package that symbolically solves Einstein-Boltzmann equations, enabling 100x shorter model definitions than CLASS while providing automatic differentiation and modern numerical solvers.


<details>
  <summary>Details</summary>
Motivation: To reduce friction in cosmological modeling by eliminating approximation switching schemes and enabling symbolic-numeric equation specification with automatic differentiation capabilities.

Method: Uses symbolic-numeric interface for equation specification, modern implicit solvers for stiff equations, automatic differentiation for exact derivatives, and component-based model building that scales well.

Result: Achieves up to 100x shorter model definitions compared to CLASS, agrees with established codes like CLASS and CAMB, and provides differentiable output for gradient-based methods.

Conclusion: SymBoltz can grow into an integrated symbolic-numeric cosmological modeling environment with a large library of models that delivers differentiable output as fast as other codes.

Abstract: SymBoltz is a new Julia package that solves the linear Einstein-Boltzmann
equations. It features a symbolic-numeric interface for specifying equations,
is free of approximation switching schemes and is compatible with automatic
differentiation. Cosmological models are built from replaceable physical
components in a way that scales well in model space. The modeler should simply
write down their equations, and SymBoltz solves them and eliminates much of the
friction in the process. SymBoltz enables up to 100x shorter model definitions
compared to browsing equivalent files in CLASS. Symbolic knowledge enables
powerful automation of tasks, such as separating computational stages like the
background and perturbations, generating the Jacobian matrix and its sparsity
pattern, and interpolating arbitrary expressions from the solution. Modern
implicit solvers integrate the full stiff equations at all times, reducing
slowdowns by taking long time steps, reusing the Jacobian and LU-factorizing it
over several time steps, and using fast linear system solvers. Automatic
differentiation gives exact derivatives of any output with respect to any
input, which is important for gradient-based Markov chain Monte Carlo methods
in large parameter spaces, training of emulators, Fisher forecasting and
sensitivity analysis. These features are useful in their own rights, but also
reinforce each other in a synergy. Results agree with established codes like
CLASS and CAMB. With more work, SymBoltz can grow into an integrated
symbolic-numeric cosmological modeling environment with a large library of
models that delivers differentiable output as fast as other codes. SymBoltz is
available at https://github.com/hersle/SymBoltz.jl with single-command
installation and extensive documentation, and welcomes questions, suggestions
and contributions.

</details>


### [111] [gCAMB: A GPU-accelerated Boltzmann solver for next-generation cosmological surveys](https://arxiv.org/abs/2509.25110)
*L. Storchi,P. Campeti,M. Lattanzi,N. Antonini,E. Calore,P. Lubrano*

Main category: astro-ph.CO

TL;DR: gCAMB is a GPU-accelerated version of the CAMB Boltzmann solver that speeds up cosmological power spectrum calculations while maintaining all original features, enabling faster generation of training datasets for neural network emulators.


<details>
  <summary>Details</summary>
Motivation: Traditional CMB analysis using Boltzmann solvers like CAMB is computationally expensive, creating bottlenecks for non-standard models and future surveys. Neural network emulators require large training datasets that are costly to generate.

Method: Porting the CAMB code to GPUs by offloading the most computationally intensive modules to GPU hardware while preserving all original features and functionality.

Result: gCAMB significantly accelerates power spectrum generation, saves massive computational time, halves power consumption in high-accuracy settings, and facilitates creation of extensive training datasets for cosmological analyses.

Conclusion: gCAMB provides an efficient GPU-accelerated solution for cosmological calculations that bridges the gap between traditional Boltzmann solvers and neural network emulators, with the software made publicly available to the community.

Abstract: Inferring cosmological parameters from Cosmic Microwave Background (CMB) data
requires repeated and computationally expensive calculations of theoretical
angular power spectra using Boltzmann solvers like CAMB. This creates a
significant bottleneck, particularly for non-standard cosmological models and
the high-accuracy demands of future surveys. While emulators based on deep
neural networks can accelerate this process by several orders of magnitude,
they first require large, pre-computed training datasets, which are costly to
generate and model-specific. To address this challenge, we introduce gCAMB, a
version of the CAMB code ported to GPUs, which preserves all the features of
the original CPU-only code. By offloading the most computationally intensive
modules to the GPU, gCAMB significantly accelerates the generation of power
spectra, saving massive computational time, halving the power consumption in
high-accuracy settings and, among other purposes, facilitating the creation of
extensive training sets needed for robust cosmological analyses. We make the
gCAMB software available to the community at
https://github.com/lstorchi/CAMB/tree/gpuport.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [112] [Deficit functions and the log Sobolev inequality](https://arxiv.org/abs/2509.22941)
*Tobias Holck Colding,William P. Minicozzi II*

Main category: math.DG

TL;DR: The paper establishes a connection between elliptic and parabolic monotonicity formulas, showing that parabolic deficits are pointwise limits of elliptic ones, and uses this to prove the log Sobolev inequality and discover new concentration of measure phenomena.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between the long-standing parabolic monotonicity formulas from various fields and the more recent elliptic theory, and to explore their connections and applications.

Method: Introduces two deficit functions (elliptic and parabolic), shows the parabolic deficit is the pointwise limit of the elliptic one, and demonstrates that the elliptic equation converges to the parabolic equation.

Result: The pointwise quantities and their equations recover the monotonicities, leading to an elliptic proof of the log Sobolev inequality and new concentration of measure phenomena.

Conclusion: The paper successfully connects elliptic and parabolic theories through deficit functions, providing new proofs and discoveries in functional inequalities and measure concentration.

Abstract: There is a long history of parabolic monotonicity formulas that developed
independently from several different fields and a much more recent elliptic
theory. The elliptic theory can be localized and there are additional monotone
quantities. There is also a surprising link: Taking a high-dimensional limit of
the right elliptic monotonicity can give a parabolic one as a limit. Poincar\'e
was the first to observe such a connection. We introduce two deficit functions,
one elliptic and one parabolic, then show that the parabolic deficit is
pointwise the limit of the elliptic and, that the elliptic satisfies an
equation that converges to the equation for the parabolic. These pointwise
quantities and their equations recover the monotonicities and leads to an
elliptic proof of the log Sobolev inequality as well as new concentration of
measure phenomena.

</details>


### [113] [Spectral comparison and splitting theorems for the infinity-Bakry-Emery Ricci curvature](https://arxiv.org/abs/2509.23182)
*Jia-Yong Wu*

Main category: math.DG

TL;DR: The paper extends spectral comparison theorems to weighted manifolds, proving diameter comparison, weighted volume comparison, and splitting theorems under lower bounds on infinity-Bakry-Emery Ricci curvature in the spectral sense.


<details>
  <summary>Details</summary>
Motivation: To generalize existing spectral comparison theorems (Bonnet-Myers, Bishop-Gromov, splitting) from Riemannian manifolds to weighted manifolds, and to supplement recent spectral results in weighted geometry.

Method: Using spectral analysis approach with infinity-Bakry-Emery Ricci curvature bounds in the spectrum sense for weighted manifolds.

Result: Successfully proved diameter comparison, global weighted volume comparison, and splitting theorems in weighted manifolds under spectral curvature conditions.

Conclusion: The results extend and supplement previous spectral comparison theorems, providing a comprehensive spectral framework for weighted manifold geometry with curvature bounds.

Abstract: In this paper, we prove the diameter comparison, the global weighted volume
comparison and the splitting theorem in weighted manifolds when the
infinity-Bakry-Emery Ricci curvature has a lower bound in the spectrum sense.
Our results extend Antonelli-Xu's spectral Bonnet-Myers and Bishop-Gromov
theorems, and Antonelli-Pozzetta-Xu's spectral splitting theorem to weighted
manifolds. Our results are also some supplements of Chu-Hao's spectral diameter
and global volume comparisons, and Yeung's spectral splitting theorem in
weighted manifolds.

</details>


### [114] [An ancient Ricci flow emerging from Taub-Bolt](https://arxiv.org/abs/2509.23276)
*John Hughes*

Main category: math.DG

TL;DR: Existence of non-trivial ancient Ricci flow solution from Taub-Bolt metric


<details>
  <summary>Details</summary>
Motivation: To establish the existence of ancient solutions to the Ricci flow that emerge from specific geometric structures, particularly the Taub-Bolt metric

Method: Mathematical proof demonstrating the existence of a non-trivial ancient solution

Result: Proved existence of non-trivial ancient Ricci flow solution emerging from Taub-Bolt metric

Conclusion: The Taub-Bolt metric gives rise to ancient solutions in Ricci flow theory

Abstract: This paper proves that there exists a non-trivial ancient solution to the
Ricci flow emerging from the Taub-Bolt metric.

</details>


### [115] [Sectional curvature and matrix displacement convexity](https://arxiv.org/abs/2509.23399)
*Gautam Aishwarya,Liran Rotem,Yair Shenfeld*

Main category: math.DG

TL;DR: The paper establishes an equivalence between nonnegative sectional curvature in Riemannian manifolds and matrix displacement convexity of the entropy functional, leading to improved evolution variational inequalities and Wasserstein contraction results.


<details>
  <summary>Details</summary>
Motivation: To characterize geometric properties of Riemannian manifolds through convexity properties of entropy functionals and derive improved functional inequalities for heat flows.

Method: Proving the equivalence between nonnegative sectional curvature and matrix displacement convexity of entropy, then applying this to derive intrinsic dimensional evolution variational inequalities.

Result: Obtained intrinsic dimensional evolution variational inequalities and corresponding Wasserstein contraction along heat flows that improve upon dimensional counterparts.

Conclusion: The equivalence between nonnegative sectional curvature and matrix displacement convexity provides a powerful tool for deriving improved functional inequalities in Riemannian geometry.

Abstract: We show that the sectional curvature of a Riemannian manifold is nonnegative
if, and only if, the entropy functional is matrix displacement convex. As an
application we obtain intrinsic dimensional evolution variational inequalities,
and the corresponding Wasserstein contraction along heat flows, which improve
on their dimensional counterparts.

</details>


### [116] [Steady Gradient Ricci Solitons with $O(p)\times O(q)$ Symmetry](https://arxiv.org/abs/2509.25081)
*Lucas Lavoyer,Luke T. Peachey*

Main category: math.DG

TL;DR: New steady gradient Ricci solitons with positive curvature operator constructed in dimensions 4+ using O(p)×O(q) symmetry


<details>
  <summary>Details</summary>
Motivation: To find new examples of steady gradient Ricci solitons with positive curvature operator in higher dimensions

Method: Using Lai's procedure to construct examples with O(p)×O(q) symmetry in dimension p+q for p,q≥2

Result: Successfully constructed new steady gradient Ricci solitons with positive curvature operator

Conclusion: The method provides a systematic way to construct steady gradient Ricci solitons with specific symmetry groups in various dimensions

Abstract: We find new examples of steady gradient Ricci solitons with positive
curvature operator in dimensions four and above. Utilising a procedure first
introduced by Lai, we construct examples with $O(p) \times O(q)$ symmetry in
dimension $p+q$ for any pair of integers $p,q \geq 2$ and discuss their
asymptotic geometry.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [117] [Dimension-dependent bounds for the SDIEP via phase optimisation and Paley-type constructions](https://arxiv.org/abs/2509.24079)
*Tomasz Kania*

Main category: math.SP

TL;DR: The paper refines cycle-walk templates to determine when δ-Suleïmanova spectra can be realized by symmetric doubly stochastic matrices, computing exact size-dependent thresholds and introducing an optimally phase-aligned cycle basis that improves bounds.


<details>
  <summary>Details</summary>
Motivation: To quantify when δ-Suleïmanova spectra (with negative eigenvalues) can be realized by symmetric doubly stochastic matrices, improving existing bounds and addressing artifacts in the analysis.

Method: Refined cycle-walk (Fourier) templates, computed exact size-dependent thresholds using canonical cycle basis, introduced optimally phase-aligned cycle basis to remove artifacts, and analyzed Walsh-Hadamard basis on abelian 2-groups.

Result: Derived exact threshold δ_n that improves 1/2 when 8∤n, proved sharpness. Introduced δ_n^(ph) that gives better bounds for all n≥3, with δ_n^(ph) = δ_n unless 8|n. Showed Walsh-Hadamard basis suffices for all Suleïmanova lists on abelian 2-groups.

Conclusion: The optimally phase-aligned cycle basis provides improved bounds for realizing Suleïmanova spectra, and Walsh-Hadamard basis offers complete solutions on abelian 2-groups and Hadamard orders.

Abstract: We refine the cycle-walk (Fourier) template of Gnacik and the author to
quantify when a~$\delta$-Sule\u{\i}manova spectrum
$(1,\lambda_2,\dots,\lambda_n)$ (with $\lambda_j\le 0$) is realised by a
symmetric doubly stochastic matrix. For the canonical cycle basis we compute
the \emph{exact} size-dependent threshold \[ \delta_n \;=\;
1-\frac{1}{2\cos^2\!\Big(\frac{\pi}{4n}\rho(n)\Big)}, \quad
\rho(n)\in\{0,1,2,4\}\ \text{determined by } n\bmod 8, \] which improves $1/2$
if and only if $8\nmid n$; we also prove sharpness for that template. We then
introduce an \emph{optimally phase-aligned} cycle basis which removes the
`$8\mid n$' artefact and yields better sufficient bound \[ \delta_n^{\rm (ph)}
\;=\; \begin{cases} \displaystyle 1-\dfrac{1}{2\cos^2(\pi/n)}, & n\equiv
0\pmod{4},\\[2mm] \displaystyle 1-\dfrac{1}{2\cos^2(\pi/2n)}, & n\equiv
2\pmod{4},\\[2mm] \displaystyle 1-\dfrac{1}{2\cos^2(\pi/4n)}, & n\ \text{odd},
\end{cases} \] so that $\delta_n^{\rm (ph)}<\tfrac12$ for \emph{every} $n\ge3$
and $\delta_n^{\rm (ph)}=\delta_n$ unless $8\mid n$. Next, on abelian
$2$-groups, the Walsh--Hadamard basis has coherence $M=1$ and hence suffices
for \emph{all} Sule\u{\i}manova lists ($\delta=0$); the same conclusion holds
in every Hadamard order (\emph{e.g.}, Paley families).

</details>
