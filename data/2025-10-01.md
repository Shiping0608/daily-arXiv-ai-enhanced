<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 16]
- [math.AP](#math.AP) [Total: 23]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [hep-th](#hep-th) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [math.OC](#math.OC) [Total: 2]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [eess.SP](#eess.SP) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [AW-EL-PINNs: A Multi-Task Learning Physics-Informed Neural Network for Euler-Lagrange Systems in Optimal Control Problems](https://arxiv.org/abs/2509.25262)
*Chuandong Li,Runtian Zeng*

Main category: math.NA

TL;DR: AW-EL-PINNs combine Euler-Lagrange theorem with physics-informed neural networks to solve optimal control problems, using adaptive loss weighting to improve accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: To address the tedious manual tuning of loss function weights in conventional PINNs and improve solution accuracy for Euler-Lagrange systems in optimal control problems.

Method: Systematically converts optimal control problems into two-point boundary value problems, integrates Euler-Lagrange theorem with deep learning architecture, and uses adaptive loss weighting mechanism to dynamically balance loss components during training.

Result: Achieves enhanced solution accuracy compared to baseline methods while maintaining stability throughout optimization, as demonstrated in six numerical examples.

Conclusion: The framework improves precision and ensures stability in solving Euler-Lagrange systems, offering potential strategies for physical applications in optimal control problems.

Abstract: This paper presents adaptive weighted Euler-Lagrange theorem combined
physics-informed neural networks (AW-EL-PINNs) for solving Euler-Lagrange
systems in optimal control problems. The framework systematically converts
optimal control frameworks into two-point boundary value problems (TPBVPs)
while establishing a multi-task learning paradigm through innovative
integration of the Euler-Lagrange theorem with deep learning architecture. An
adaptive loss weighting mechanism dynamically balances loss function components
during training, decreasing tedious manual tuning of weighting the loss
functions compared to the conventional physics-informed neural networks
(PINNs). Based on six numerical examples, it's clear that AW-EL-PINNs achieve
enhanced solution accuracy compared to baseline methods while maintaining
stability throughout the optimization process. These results highlight the
framework's capability to improve precision and ensure stability in solving
Euler-Lagrange systems in optimal control problems, offering potential
strategies for problems under physical applications.

</details>


### [2] [The Asad Correctional Power Series Method: A Novel Approach to Solving Fractional Differential Equations](https://arxiv.org/abs/2509.25354)
*Asad Freihet,Mohammed Alabedalhadi*

Main category: math.NA

TL;DR: The paper introduces ACPS method for solving fractional differential equations, showing it's more accurate and efficient than existing methods like Runge-Kutta, with applications to epidemic modeling.


<details>
  <summary>Details</summary>
Motivation: To develop a more accurate and computationally efficient method for solving fractional differential equations, particularly for capturing memory effects in real-world applications like epidemic modeling.

Method: ACPS combines algebraic manipulation with iterative refinement, incorporating principles from fractional calculus and functional analysis to handle linear to highly nonlinear fractional equations.

Result: ACPS demonstrated superior performance compared to classical Runge-Kutta method, validated through numerical and graphical analysis. Successfully applied to fractional SIR epidemic model, capturing memory effects better than integer-order models.

Conclusion: ACPS is a useful tool for solving fractional differential equations in real-world applications, offering improved accuracy and efficiency while better capturing hereditary effects in systems like disease transmission.

Abstract: This paper introduces the Asad Correctional Power Series Method (ACPS), a
novel and groundbreaking approach designed to simplify and optimize the
solution of fractional differential equations. The ACPS combines algebraic
manipulation with iterative refinement to achieve greater accuracy and
computational efficiency than mainstream methods. By incorporating principles
from both fractional calculus and functional analysis, the method offers a
flexible framework capable of addressing a wide range of fractional equations,
from linear to highly nonlinear cases. Additionally, a representative
counterexample is provided to indicate that the conformable fractional
derivative does not fulfill the mathematical criteria for a valid definition of
fractional differentiation. The Asad Correctional Power Series (ACPS) method is
employed to construct an analytic solution of the fractional SIR model in the
form of a rapidly convergent power series. Its performance is validated through
comparisons with the classical fourth-order Runge Kutta method, where both
numerical and graphical analyses corroborate the method's precision and
efficiency. The application of ACPS to the fractional epidemic model highlights
its ability to capture memory and hereditary effects, offering more realistic
insights into disease transmission dynamics than integer-order models. These
findings demonstrate that ACPS can serve as a useful tool for solving
fractional differential equations arising in real world applications

</details>


### [3] [Quasi-Monte Carlo methods for uncertainty quantification of tumor growth modeled by a parametric semi-linear parabolic reaction-diffusion equation](https://arxiv.org/abs/2509.25753)
*Alexander D. Gilbert,Frances Y. Kuo,Dirk Nuyens,Graham Pash,Ian H. Sloan,Karen E. Willcox*

Main category: math.NA

TL;DR: Application of quasi-Monte Carlo methods to efficiently propagate parameter uncertainties in tumor growth models for computing clinically relevant quantities of interest.


<details>
  <summary>Details</summary>
Motivation: Tumor growth models have significant parameter uncertainties due to inter-patient variability, disease heterogeneity, sparse data, and model inadequacy. These uncertainties need efficient propagation through models to inform clinical decisions.

Method: Use quasi-Monte Carlo (QMC) methods for uncertainty propagation in semi-linear parabolic reaction-diffusion PDEs modeling tumor growth. Theoretical analysis includes well-posedness results and error bounds for uniform random fields.

Result: QMC methods successfully compute expectations of meaningful quantities of interest. Theoretical linear error rate superior to standard Monte Carlo is verified numerically. Promising results for lognormal random fields suggest further theoretical development.

Conclusion: QMC methods are effective for uncertainty quantification in tumor growth models, providing superior error rates compared to standard Monte Carlo methods, with potential for clinical decision support.

Abstract: We study the application of a quasi-Monte Carlo (QMC) method to a class of
semi-linear parabolic reaction-diffusion partial differential equations used to
model tumor growth. Mathematical models of tumor growth are largely
phenomenological in nature, capturing infiltration of the tumor into
surrounding healthy tissue, proliferation of the existing tumor, and patient
response to therapies, such as chemotherapy and radiotherapy. Considerable
inter-patient variability, inherent heterogeneity of the disease, sparse and
noisy data collection, and model inadequacy all contribute to significant
uncertainty in the model parameters. It is crucial that these uncertainties can
be efficiently propagated through the model to compute quantities of interest
(QoIs), which in turn may be used to inform clinical decisions. We show that
QMC methods can be successful in computing expectations of meaningful QoIs.
Well-posedness results are developed for the model and used to show a
theoretical error bound for the case of uniform random fields. The theoretical
linear error rate, which is superior to that of standard Monte Carlo, is
verified numerically. Encouraging computational results are also provided for
lognormal random fields, prompting further theoretical development.

</details>


### [4] [Numerical approximations to invariant measures of hybrid stochastic differential equations with superlinear coefficients via the backward Euler-Maruyama method](https://arxiv.org/abs/2509.25799)
*Wei Liu,Jie Xu*

Main category: math.NA

TL;DR: The paper proposes a backward Euler-Maruyama method for approximating invariant measures in stochastic differential equations with Markovian switching, relaxing the global Lipschitz condition requirement.


<details>
  <summary>Details</summary>
Motivation: To address stochastic differential equations with Markovian switching that have superlinear drift and diffusion coefficients, where existing methods require global Lipschitz conditions on the diffusion coefficient.

Method: Backward Euler-Maruyama (BEM) method is proposed for numerical approximation of invariant measures, with proofs of existence and uniqueness of numerical invariant measures.

Result: The method successfully approximates invariant measures and converges to the underlying counterpart, while relaxing the global Lipschitz condition requirement from previous work.

Conclusion: The BEM method provides an effective numerical approach for invariant measure approximation in stochastic systems with Markovian switching and superlinear coefficients, with theoretical convergence guarantees.

Abstract: For stochastic stochastic differential equations with Markovian switching,
whose drift and diffusion coefficients are allowed to contain superlinear
terms, the backward Euler-Maruyama (BEM) method is proposed to approximate the
invariant measure. The existence and uniqueness of the invariant measure of the
numerical solution generated by the BEM method is proved. Then the convergence
of the numerical invariant measure to its underlying counterpart is shown. The
results obtained in this work release the requirement of the global Lipschitz
condition on the diffusion coefficient in [X. Li et al. SIAM J. Numer. Anal.
56(3)(2018), pp. 1435-1455]. Numerical simulations are provided to demonstrate
those theoretical results.

</details>


### [5] [A Reduced Basis Method for the Stochastic Landau-Lifshitz-Gilbert Equation](https://arxiv.org/abs/2509.25909)
*Andrea Scaglioni,Michael Feischl,Fernando Henríquez*

Main category: math.NA

TL;DR: This paper develops efficient surrogates for the Stochastic Landau-Lifshitz-Gilbert equation using Reduced Basis method with Proper Orthogonal Decomposition, employing a tangent plane scheme and stabilization techniques.


<details>
  <summary>Details</summary>
Motivation: The Stochastic LLG equation models magnetization evolution in ferromagnetic bodies under random heat perturbations, but solving it directly is computationally expensive due to its nonlinear parabolic nature and high-dimensional parameter space.

Method: Uses Reduced Basis method with Proper Orthogonal Decomposition on high-fidelity samples in offline phase, and tangent plane scheme in reduced space during online phase. Includes stabilization for saddle-point structure and compares with sparse grid interpolation approaches.

Result: Numerical experiments show clear advantage over earlier sparse grid interpolation approaches. A complementary data-driven sparse grid approximation exhibits weaknesses but benefits from increased stability.

Conclusion: The proposed RB method with POD and tangent plane scheme provides an efficient surrogate for SLLG equation, outperforming previous sparse grid methods while maintaining computational efficiency through proper space reduction and stabilization.

Abstract: In this work, we consider the construction of efficient surrogates for the
stochastic version of the Landau-Lifshitz-Gilbert (LLG) equation using model
order reduction techniques, in particular, the Reduced Basis (RB) method. The
Stochastic LLG (SLLG) equation is a widely used phenomenological model for the
time evolution of the magnetization field confined to a ferromagnetic body
while taking into account the effect of random heat perturbations. This
phenomenon is mathematically formulated as a nonlinear parabolic problem, where
the stochastic component is represented as a parameter-dependent datum
depending on a non-compact and high-dimensional parameter. In an
$\textit{offline}$ phase, we use Proper Orthogonal Decomposition (POD) on
high-fidelity samples of the unbounded parameter space. To that end, we use the
so-called $\textit{tangent plane scheme}$. For the $\textit{online}$ phase of
the RB method, we again employ the tangent plane scheme in the RB space. This
is possible due to our particular construction that reduces both spaces of the
magnetization and of its time derivative. Due to the saddle-point nature of
this scheme, a stabilization that appropriately enriches the RB space is
required. Numerical experiments show a clear advantage over earlier approaches
using sparse grid interpolation. In a complementary approach, we test a sparse
grid approximation of the reduced coefficients in a purely data-driven method,
exhibiting the weaknesses of earlier sparse grid approaches, but benefiting
from increased stability.

</details>


### [6] [Flexible fixed-point iteration and its applications for nonsymmetric algebraic Riccati equations](https://arxiv.org/abs/2509.25942)
*Zhen-Chen Guo,Xin Liang*

Main category: math.NA

TL;DR: The paper reveals the Toeplitz structure in nonsymmetric algebraic Riccati equations and proposes a RADI-type method with Leja shifts for efficient large-scale computation.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of solving large-scale nonsymmetric algebraic Riccati equations with sparse and low-rank structure by leveraging their intrinsic mathematical properties.

Method: Uses shift-involved fixed-point iteration and proposes a RADI-type method incorporating flexible Leja shifts based on rational approximation theory, with implementations for low-rank factorization, implicit matrix updates, and complex shift handling.

Result: Numerical experiments demonstrate the efficiency of both the proposed RADI-type method and the Leja shift-selection strategy.

Conclusion: The method successfully computes stabilizing solutions for large-scale nonsymmetric algebraic Riccati equations by exploiting their Toeplitz structure and using advanced shift strategies.

Abstract: In this paper, we reveal the intrinsic Toeplitz structure in the unique
stabilizing solution for nonsymmetric algebraic Riccati equations by employing
a shift-involved fixed-point iteration, and propose an RADI-type method for
computing this solution for large-scale equations of this type with sparse and
low-rank structure by incorporating flexible shifts into the fixed-point
iteration. We present a shift-selection strategy, termed Leja shifts, based on
rational approximation theory, which is incorporated into the RADI-type method.
We further discuss important implementation aspects for the method, such as
low-rank factorization of residuals, implicit update of large-scale sparse
matrices, real arithmetics with complex shifts, and related equations of other
type.
  Numerical experiments demonstrate the efficiency of both the proposed method
and the introduced shift-selection strategy.

</details>


### [7] [Embedding General Conservation Constraints in Discretizations of Hyperbolic Systems on Arbitrary Meshes: A Multidimensional Framework](https://arxiv.org/abs/2509.25967)
*Rémi Abgrall,Pierre-Henri Maire,Mario Ricchiuto*

Main category: math.NA

TL;DR: This paper reviews conservation formulations in hyperbolic systems at discrete levels, focusing on different solution representations and mesh types. It introduces a unified framework using graph theory to define local conservation through numerical fluxes or residuals.


<details>
  <summary>Details</summary>
Motivation: To establish a systematic framework for defining conservation in discrete hyperbolic systems, addressing the challenge of formulating conservation principles across different solution representations and mesh configurations.

Method: The authors propose a unified approach where conserved variable updates are expressed as u^{n+1}=u^n-Δt δu, with δu depending on the scheme's stencil. They define conservation using graph theory - either through mesh faces (numerical fluxes) or mesh elements (residuals).

Result: The paper demonstrates that both numerical flux and residual-based approaches are equivalent, but the residual method offers more flexibility, especially for satisfying additional algebraic constraints. The framework provides clear guidance for scheme design in Lagrangian formulations.

Conclusion: The residual-based conservation definition provides superior flexibility compared to flux-based approaches, particularly for complex constraints. The framework offers a unified perspective for conservation in discrete hyperbolic systems, with ongoing research questions identified.

Abstract: The purpose of this review is to discuss the notion of conservation in
hyperbolic systems and how one can formulate it at the discrete level depending
on the solution representation of the solution. A general theory is difficult.
We discuss several possibilities: if the solution is represented by average in
volumes; if the mesh is staggerred; if the solution is solely represented by
point values and an example where all the previous options are mixed.
  We show how each configuration can provide, or not, enough flexibility. The
discussion could be adapted to any hyperbolic system endowed with an entropy,
but we focus on compressible fluid mechanics, in its Eulerian and Lagrangian
formulations. The unifying element is that we systematically express the update
of conserved variables as $u^{n+1}=u^n- \Delta t\; \delta u$, where the
functional $\delta u$ depends on the value of $u$ in the stencil of the scheme.
Then, one can naturally define a graph connecting the states defining $\delta
u$. The notion of local conservation can be defined from this graph. We are
aware of only two possible situations: either the graph is constructed from the
faces of the mesh elements (or the dual mesh), or it is defined from the mesh
itself. Two notions of local conservation then emerge: either we define a
numerical flux, or we define a "residual" attached to elements and the degrees
of freedom within the element. We show that this two notions are in a way
equivalent, but the one with residual allows much more flexibility, especially
if additional algebraic constraints must be satisfied. Examples of specific
additional conservation constraints are provided to illustrate this. We also
show that this notion of conservation gives a very clear framework for the
design of scheme in the Lagrangian framework. We end by providing a number of
ongoing research questions, and highlight some open questions.

</details>


### [8] [Trustworthy AI in numerics: On verification algorithms for neural network-based PDE solvers](https://arxiv.org/abs/2509.26122)
*Emil Haugen,Alexei Stepanenko,Anders C. Hansen*

Main category: math.NA

TL;DR: New algorithms for a posteriori verification of neural networks approximating PDE solutions, providing accuracy guarantees and rejection of inaccurate solutions.


<details>
  <summary>Details</summary>
Motivation: To enable trustworthy NN-based PDE solvers by providing verification that can guarantee solution accuracy, addressing limitations of a priori error bounds due to undecidability in NN training optimization.

Method: Algorithms that compute accurate estimates of L^p norms of NNs and their derivatives, combined with residual bounds for specific PDEs to verify solution accuracy.

Result: The framework provides guarantees of ε-accuracy with respect to the true PDE solution for arbitrary ε>0, with algorithms that detect and reject inaccurate NNs while certifying accurate ones.

Conclusion: A posteriori verification is essential for trustworthy NN-based PDE solvers as it overcomes the limitations of a priori error bounds and ensures solution reliability regardless of how the NN was initially computed.

Abstract: We present new algorithms for a posteriori verification of neural networks
(NNs) approximating solutions to PDEs. These verification algorithms compute
accurate estimates of $L^p$ norms of NNs and their derivatives. When combined
with residual bounds for specific PDEs, the algorithms provide guarantees of
$\eps$-accuracy (in a suitable norm) with respect to the true, but unknown,
solution of the PDE -- for arbitrary $\eps >0$. In particular, if the NN fails
to meet the desired accuracy, our algorithms will detect that and reject it,
whereas any NN that passes the verification algorithms is certified to be
$\eps$-accurate. This framework enables trustworthy algorithms for NN-based PDE
solvers, regardless of how the NN is initially computed. Such a posteriori
verification is essential, since a priori error bounds in general cannot
guarantee the accuracy of computed solutions, due to algorithmic undecidability
of the optimization problems used to train NNs.

</details>


### [9] [Zeta expansion for long-range interactions under periodic boundary conditions with applications to micromagnetics](https://arxiv.org/abs/2509.26274)
*Andreas A. Buchheit,Jonathan K. Busse,Torsten Keßler,Filipp N. Rybakov*

Main category: math.NA

TL;DR: Efficient computation of power-law-based interaction potentials for periodic systems using generalized zeta functions instead of truncated lattice sums, achieving machine precision with exponential convergence.


<details>
  <summary>Details</summary>
Motivation: Current practice of truncating infinite lattice sums in micromagnetics with periodic boundary conditions introduces uncontrolled errors, requiring an exact method for computing these interactions.

Method: Complement a small direct sum with correction terms involving efficiently computable derivatives of generalized zeta functions, using a superexponentially convergent algorithm for evaluation.

Result: Achieves machine precision at computational cost comparable to truncated schemes, with exponential convergence in derivative order. Validated against known formulas and direct summation.

Conclusion: The method provides exact infinite sums for periodic interactions, enabling high-precision computations in micromagnetics and other fields like molecular dynamics, with broad applicability.

Abstract: We address the efficient computation of power-law-based interaction
potentials of homogeneous $d$-dimensional bodies with an infinite
$n$-dimensional array of copies, including their higher-order derivatives. This
problem forms a serious challenge in micromagnetics with periodic boundary
conditions and related fields. Nowadays, it is common practice to truncate the
associated infinite lattice sum to a finite number of images, introducing
uncontrolled errors. We show that, for general interacting geometries, the
exact infinite sum for both dipolar interactions and generalized Riesz
power-law potentials can be obtained by complementing a small direct sum by a
correction term that involves efficiently computable derivatives of generalized
zeta functions. We show that the resulting representation converges
exponentially in the derivative order, reaching machine precision at a
computational cost no greater than that of truncated summation schemes. In
order to compute the generalized zeta functions efficiently, we provide a
superexponentially convergent algorithm for their evaluation, as well as for
all required special functions, such as incomplete Bessel functions. Magnetic
fields can thus be evaluated to machine precision in arbitrary cuboidal domains
periodically extended along one or two dimensions. We benchmark our method
against known formulas for magnetic interactions and against direct summation
for Riesz potentials with large exponents, consistently achieving full
precision. In addition, we identify new corrections to the asymptotic limit of
the demagnetization field and tabulate high-precision benchmark values that can
be used as a reliable reference for micromagnetic solvers. The techniques
developed are broadly applicable, with direct impact in other areas such as
molecular dynamics.

</details>


### [10] [A robust computational framework for the mixture-energy-consistent six-equation two-phase model with instantaneous mechanical relaxation terms](https://arxiv.org/abs/2509.26284)
*Giuseppe Orlando,Ward Haegeman,Marica Pelanti,Marc Massot*

Main category: math.NA

TL;DR: A computational framework for solving hyperbolic 6-equation two-phase models, analyzing different discretizations of non-conservative terms and their impact on 5-equation model solutions, particularly for shocks.


<details>
  <summary>Details</summary>
Motivation: To understand how different discretizations of non-conservative terms in 6-equation two-phase models affect numerical solutions of the 5-equation Kapila model, especially for shock problems where jump conditions are incomplete.

Method: Uses phasic total energies as prognostic variables for discrete conservation, compares HLLC approximate Riemann solver with Rusanov and HLLC solvers for conservative parts, and analyzes path-conservative schemes for non-conservative terms.

Result: Shows that some discretization approaches fit within path-conservative schemes framework, and demonstrates the impact of theoretical model shortcomings and the importance of robust numerical strategies through test cases.

Conclusion: The choice of discretization for non-conservative terms significantly impacts solution accuracy and robustness, with some methods fitting path-conservative frameworks, highlighting the need for careful numerical strategy selection.

Abstract: We present a robust computational framework for the numerical solution of a
hyperbolic 6-equation single-velocity two-phase model. The model's main
interest is that, when combined with instantaneous mechanical relaxation, it
recovers the solution of the 5-equation model of Kapila. Several numerical
methods based on this strategy have been developed over the years. However,
neither the 5- nor 6-equation model admits a complete set of jump conditions
because they involve non-conservative products. Different discretizations of
these terms in the 6-equation model exist. The precise impact of these
discretizations on the numerical solutions of the 5-equation model, in
particular for shocks, is still an open question to which this work provides
new insights. We consider the phasic total energies as prognostic variables to
naturally enforce discrete conservation of total energy and compare the
accuracy and robustness of different discretizations for the hyperbolic
operator. Namely, we discuss the construction of an HLLC approximate Riemann
solver in relation to jump conditions. We then compare an HLLC wave-propagation
scheme which includes the non-conservative terms, with Rusanov and HLLC solvers
for the conservative part in combination with suitable approaches for the
non-conservative terms. We show that some approaches for the discretization of
non-conservative terms fit within the framework of path-conservative schemes
for hyperbolic problems. We then analyze the use of various numerical
strategies on several relevant test cases, showing both the impact of the
theoretical shortcomings of the models as well as the importance of the choice
of a robust framework for the global numerical strategy.

</details>


### [11] [Nearest matrix with multiple eigenvalues by Riemannian optimization](https://arxiv.org/abs/2509.26344)
*Vanni Noferini,Lauri Nyman,Federico Poloni*

Main category: math.NA

TL;DR: This paper extends a Riemannian optimization framework to find the nearest defective matrix by tracking both left and right eigenvectors simultaneously, with support for complex-linear constraints.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of finding the nearest matrix with multiple eigenvalues (defective matrix) from a given square complex matrix with distinct eigenvalues.

Method: Extends a Riemannian optimization framework using variable projection to simultaneously track left and right eigenvectors, allowing arbitrary complex-linear constraints on perturbations or perturbed matrices.

Result: The method enables computation of nearest defective matrices and can be used to study structured eigenvalue condition numbers.

Conclusion: The extended framework provides an effective approach for finding nearest defective matrices and analyzing structured eigenvalue problems, with numerical experiments showing comparisons to existing algorithms.

Abstract: Given a square complex matrix $A$, we tackle the problem of finding the
nearest matrix with multiple eigenvalues or, equivalently when $A$ had distinct
eigenvalues, the nearest defective matrix. To this goal, we extend the general
framework described in [M. Gnazzo, V. Noferini, L. Nyman, F. Poloni,
\emph{Riemann-Oracle: A general-purpose Riemannian optimizer to solve nearness
problems in matrix theory}, Found. Comput. Math., To appear] and based on
variable projection and Riemannian optimization, allowing the ambient manifold
to simultaneously track left and right eigenvectors. Our method also allows us
to impose arbitrary complex-linear constraints on either the perturbation or
the perturbed matrix; this can be useful to study structured eigenvalue
condition numbers. We present numerical experiments, comparing with preexisting
algorithms.

</details>


### [12] [HANN: Homotopy auxiliary neural network for solving nonlinear algebraic equations](https://arxiv.org/abs/2509.26358)
*Ling-Zhe Zai,Lei-Lei Guo,Zhi-Yong Zhang*

Main category: math.NA

TL;DR: The paper proposes a Homotopy Auxiliary Neural Network (HANN) that combines classical homotopy continuation with physics-informed neural networks to solve nonlinear algebraic equations more effectively than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional iterative methods and optimization algorithms have limitations like initial value sensitivity, limited accuracy, slow convergence, and inflexible neural network inputs for solving nonlinear algebraic equations.

Method: The HANN method integrates classical homotopy continuation with physics-informed neural networks, featuring two versions: HANN-1 for rapid acceptable solutions and HANN-2 for improved accuracy.

Result: Numerical results show HANN effectively solves various problems including single equation solutions, transcendental systems with absolute value/trigonometric functions, ill-conditioned high-dimensional systems, and time-varying nonlinear problems, outperforming Python's Fsolve function.

Conclusion: The HANN method demonstrates strong learning ability and effectiveness in solving challenging nonlinear algebraic equations where traditional methods like Fsolve exhibit significant limitations or fail completely.

Abstract: Solving nonlinear algebraic equations is a fundamental but challenging
problem in scientific computations and also has many applications in system
engineering. Though traditional iterative methods and modern optimization
algorithms have exerted effective roles in addressing certain specific
problems, there still exist certain weaknesses such as the initial value
sensitivity, limited accuracy and slow convergence rate, particulary without
flexible input for the neural network methods. In this paper, we propose a
homotopy auxiliary neural network (HANN) for solving nonlinear algebraic
equations which integrates the classical homotopy continuation method and
popular physics-informed neural network. Consequently, the HANN-1 has strong
learning ability and can rapidly give an acceptable solution for the problem
which outperforms some known methods, while the HANN-2 can further improve its
accuracy. Numerical results on the benchmark problems confirm that the HANN
method can effectively solve the problems of determining the total number of
solutions of a single equation, finding solutions of transcendental systems
involving the absolute value function or trigonometric function,
ill-conditioned and normal high-dimensional nonlinear systems and time-varying
nonlinear problems, for which the Python's built-in Fsolve function exhibits
significant limitations, even fails to work.

</details>


### [13] [Finite element discretizations of bending plates with prestrained microstructure](https://arxiv.org/abs/2509.26438)
*Klaus Böhnlein,Stefan Neukamm,Oliver Sander*

Main category: math.NA

TL;DR: Finite element discretization of an elastic bending-plate model with prestrain using Discrete Kirchhoff Triangle elements for macroscopic problem and Lagrange elements for microscopic corrector problem, showing Γ-convergence to continuous model.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze a numerical discretization scheme for a homogenized elastic bending-plate model with prestrain that was derived from three-dimensional microstructured elasticity.

Method: Uses Discrete Kirchhoff Triangle elements for macroscopic bending-plate problem (mesh scale H) and first-order Lagrange elements for microscopic corrector problem (mesh scale h), with Γ-convergence analysis.

Result: The discretized model Γ-converges to the continuous model as (h,H)→0, provided Lipschitz continuity of elasticity tensor on microstructure mesh elements. Convergence holds for consecutive limits h→0 and H→0, and these limits commute.

Conclusion: The discretization scheme is mathematically sound and extends previous results to prestrained composites, providing a rigorous foundation for numerical simulations of such materials.

Abstract: We investigate a finite element discretization of an elastic bending-plate
model with an effective prestrain. The model has been obtained via
homogenization and dimension reduction by B\"onlein at al. (2023). Its energy
functional is the $\Gamma$-limit of a three-dimensional nonlinear
microstructured elasticity functional. In the derived effective model, the
microstructure is incorporated as a local corrector problem, a system of linear
elliptic partial differential equations posed on a three-dimensional
representative volume element. The discretization uses Discrete Kirchhoff
Triangle elements for the macroscopic bending-plate problem on a mesh of scale
$H$, and first-order Lagrange elements for the microscopic corrector problem on
an axis-aligned mesh of scale $h$. We show that the discretized model
$\Gamma$-converges to the continuous one as $(h,H)\to 0$,provided that there
exists a microstructure mesh such that the elasticity tensor is Lipschitz
continuous on each mesh element. This extends earlier results by Rumpf et al.
(2024) to prestrained composites. Our argument does not require any rate of
convergence for the microscopic discretization error. As a corollary, we also
obtain convergence when $h \to 0$ and $H \to 0$ consecutively, and we prove
that these limit processes commute.

</details>


### [14] [Computing Linear Combinations of $\varphi$-Function Actions for Exponential Integrators](https://arxiv.org/abs/2509.26475)
*Awad H. Al-Mohy*

Main category: math.NA

TL;DR: A matrix-free algorithm for evaluating linear combinations of φ-function actions in exponential integrators, combining scaling/recovering with truncated Taylor series and spectral shifting for improved accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: To develop a more reliable and accurate method for evaluating φ-function actions in exponential integrators, especially for stiff and nonnormal matrices where existing Krylov-based methods deteriorate.

Method: Combines scaling and recovering method with truncated Taylor series, uses spectral shift and scaling parameter optimization, decouples stage abscissae from polynomial weights, and includes block variant for simultaneous evaluation.

Result: Achieves near-machine accuracy (IEEE double precision) for small step sizes and maintains reliable accuracy for larger steps where other Krylov-based algorithms fail, providing good balance of reliability and computational cost.

Conclusion: The proposed algorithm offers improved reliability and accuracy for φ-function evaluations in exponential integrators across various matrix types, outperforming existing Krylov-based methods especially for larger step sizes.

Abstract: We propose a matrix-free algorithm for evaluating linear combinations of
$\varphi$-function actions, $w_i := \sum_{j=0}^{p}
\alpha_i^{\,j}\,\varphi_j(t_i A)v_j$ for $i=1\colon r$, arising in exponential
integrators. The method combines the scaling and recovering method with a
truncated Taylor series, choosing a spectral shift and a scaling parameter by
minimizing a power-based objective of the shifted operator. Accuracy is
user-controlled and ultimately limited by the working precision. The algorithm
decouples the stage abscissae $t_i$ from the polynomial weights $\alpha_i^j$,
and a block variant enables simultaneous evaluation of $\{w_i\}_{i=1}^r$.
Across standard benchmarks, including stiff and highly nonnormal matrices, the
algorithm attains near-machine accuracy (IEEE double precision in our tests)
for small step sizes and maintains reliable accuracy for larger steps where
several existing Krylov-based algorithms deteriorate, providing a favorable
balance of reliability and computational cost.

</details>


### [15] [Structure-preserving numerical calculation of wave equation for a vector field](https://arxiv.org/abs/2509.26504)
*Takuya Tsuchiya*

Main category: math.NA

TL;DR: Derived canonical formulation with constraints for Proca equation from Stueckelberg action, developed structure-preserving discrete scheme to conserve constraints, and showed it provides more accurate and stable numerical solutions compared to standard schemes.


<details>
  <summary>Details</summary>
Motivation: To develop a structure-preserving numerical scheme for the Proca equation that can conserve constraints at the discrete level, addressing the need for more accurate and stable numerical solutions.

Method: Derived canonical formulation including constraints from Stueckelberg action, proposed discrete equations with structure-preserving scheme to conserve constraints, performed numerical simulations comparing with standard scheme.

Result: The structure-preserving scheme provided more accurate and stable numerical solutions compared to standard discrete equations.

Conclusion: Structure-preserving schemes are effective for maintaining constraints in discrete formulations of the Proca equation, leading to improved numerical accuracy and stability.

Abstract: For the Proca equation, which is a wave equation for a vector field, we
derive the canonical formulation including constraints from the Stueckelberg
action and propose discrete equations with a structure-preserving scheme for
conserving the constraints at the discrete level. Numerical simulations are
performed using these discrete equations and other discrete equations with a
standard scheme. We show the results obtained using the structure-preserving
scheme and provide more accurate and stable numerical solutions.

</details>


### [16] [Second order interlaced polynomial lattice rules for integration over $\mathbb{R}^s$](https://arxiv.org/abs/2509.26624)
*Tiangang Cui,Josef Dick,Friedrich Pillichshammer*

Main category: math.NA

TL;DR: The paper introduces order-2 localized Walsh functions for numerical integration of functions over probability measures, achieving improved error decay rates for quasi-Monte Carlo methods while handling boundary singularities.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of integrating functions with respect to probability measures, particularly when transformed integrands become unbounded at boundaries, requiring localized basis functions that combine approximation power with spatial localization.

Method: Transforms integration problems using inverse cumulative distribution functions to unit cubes, then introduces order-2 localized Walsh functions that combine classical Walsh function approximation power with Haar wavelet spatial localization properties.

Result: Shows that worst-case quasi-Monte Carlo integration error decays as O(N^{-1/λ}) for every λ ∈ (1/2,1], and applies the method to elliptic PDEs with log-normal random coefficients using stochastic Galerkin discretizations with importance sampling.

Conclusion: The new localized Walsh function system provides effective numerical integration for probability measures, handling boundary singularities while maintaining good convergence rates, with successful application to stochastic PDE problems.

Abstract: We study numerical integration of functions $f: \mathbb{R}^{s} \to
\mathbb{R}$ with respect to a probability measure. By applying the
corresponding inverse cumulative distribution function, the problem is
transformed into integrating an induced function over the unit cube
$(0,1)^{s}$. We introduce a new orthonormal system: \emph{order~2 localized
Walsh functions}. These basis functions retain the approximation power of
classical Walsh functions for twice-differentiable integrands while inheriting
the spatial localization of Haar wavelets. Localization is crucial because the
transformed integrand is typically unbounded at the boundary. We show that the
worst-case quasi-Monte Carlo integration error decays like
$\mathcal{O}(N^{-1/\lambda})$ for every $\lambda \in (1/2,1]$. As an
application, we consider elliptic partial differential equations with a finite
number of log-normal random coefficients and show that our error estimates
remain valid for their stochastic Galerkin discretizations by applying a
suitable importance sampling density.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [17] [Analysis of a Cahn--Hilliard model for viscoelastoplastic two-phase flows](https://arxiv.org/abs/2509.25508)
*Fan Cheng,Robert Lasarzik,Marita Thomas*

Main category: math.AP

TL;DR: This paper analyzes a Cahn-Hilliard two-phase model for viscoelastoplastic fluid flow in geodynamics, proving existence of weak solutions via stress-diffusion regularization and introducing dissipative solutions for the non-regularized case.


<details>
  <summary>Details</summary>
Motivation: To study the well-posedness of a two-phase flow model for viscoelastoplastic fluids in geodynamics, addressing the mathematical challenges posed by the nonlinear Zaremba-Jaumann derivative and non-smooth plastic potential.

Method: Two approaches: 1) Regularization via stress-diffusion to obtain Leray-Hopf-type weak solutions; 2) Introduction of dissipative solutions based on relative energy estimates, with limit passage for vanishing stress-diffusion.

Result: Existence of Leray-Hopf-type weak solutions with stress-diffusion regularization, and existence of dissipative solutions for the non-regularized model via relative energy inequality limit passage.

Conclusion: The paper establishes mathematical foundations for viscoelastoplastic two-phase flow models, providing existence results through both regularization and dissipative solution concepts.

Abstract: We study a Cahn--Hilliard two-phase model describing the flow of two
viscoelastoplastic fluids, which arises in geodynamics. A phase-field variable
indicates the proportional distribution of the two fluids in the mixture. The
motion of the incompressible mixture is described in terms of the
volume-averaged velocity. Besides a volume-averaged Stokes-like viscous
contribution, the Cauchy stress tensor in the momentum balance contains an
additional volume-averaged internal stress tensor to model the elastoplastic
behavior. This internal stress has its own evolution law featuring the
nonlinear Zaremba-Jaumann time-derivative and the subdifferential of a
non-smooth plastic potential. The well-posedness of this system is studied in
two cases: Based on a regularization by stress-diffusion we obtain the
existence of Leray-Hopf-type weak solutions. In order to deduce existence
results also in the absence of the regularization, we introduce the concept of
dissipative solutions, which is based on an estimate for the relative energy.
  We discuss general properties of dissipative solutions and show their
existence for the viscoelastoplastic two-phase model in the setting of
stress-diffusion. By a limit passage in the relative energy inequality for
vanishing stress-diffusion, we conclude an existence result for the
non-regularized model.

</details>


### [18] [Hyperbolic Monge-Ampère Equation on a Cylinder: Well-Posedness and Stability](https://arxiv.org/abs/2509.25553)
*Maria Deliyianni,Shankar C. Venkataramani*

Main category: math.AP

TL;DR: Develops analytic framework for hyperbolic Monge-Ampère equation on strip domains modeling elastic wrinkles, using hodograph transformation to convert nonlinear PDE to linear wave equation, resolving corner singularities via parametrix-corrector decomposition.


<details>
  <summary>Details</summary>
Motivation: Address the rigidity side of rigidity-flexibility dichotomy in thin elastic sheets by defining rigidity through partial convexity rather than high smoothness, providing fundamental understanding of wrinkled patterns.

Method: Use hodograph transformation to convert nonlinear Monge-Ampère equation to linear damped wave equation, formulate Cauchy-Goursat problem, develop parametrix-corrector decomposition to handle corner singularities, and recast as singular Volterra integral equation.

Result: Prove existence and uniqueness of new class of hodograph weak solutions, establish well-posedness of Cauchy-Goursat problem, and derive energy estimates showing quantitative stability under curvature perturbations.

Conclusion: Provides rigorous analytic framework for hyperbolic Monge-Ampère equation with corner singularities, establishing new solution class with proven stability properties, advancing understanding of rigid patterns in elastic sheets.

Abstract: This paper develops a rigorous analytic framework for the hyperbolic
Monge-Amp\`ere equation on strip-like domains, which model wrinkled patterns in
thin elastic sheets. Our work addresses the rigid side of the classical
rigidity-flexibility dichotomy by defining this regime not by high smoothness,
but by the more fundamental property of partial convexity. The hodograph
transformation is the natural tool for this setting, as its validity is
predicated on partial convexity. It converts the nonlinear Monge-Amp\`ere
equation into a linear damped wave equation, allowing us to formulate a
well-posed Cauchy-Goursat problem. A key challenge is the corner singularity
that arises where characteristic and non-characteristic boundary data meet. To
resolve this, we develop a parametrix-corrector decomposition that captures the
solution's inherent singular behavior. This method recasts the problem as a
singular Volterra integral equation, for which we prove the existence and
uniqueness of a new class of hodograph weak solutions. Finally, we derive
energy estimates to establish the quantitative stability of these rigid
solutions under perturbations of the underlying curvature function.

</details>


### [19] [Nonlocal critical problems with mixed boundary conditions and nearly resonant perturbations](https://arxiv.org/abs/2509.25581)
*Eduardo Colorado,Giovanni Monica Bisci,Alejandro Ortega,Luca Vilasi*

Main category: math.AP

TL;DR: Existence of multiple solutions for a nonlocal critical problem with mixed Dirichlet-Neumann boundary conditions using spectral fractional Laplacian operator and variational methods.


<details>
  <summary>Details</summary>
Motivation: To study the existence of multiple solutions for critical fractional Sobolev problems with mixed boundary conditions, which combines nonlocal operators with complex boundary behavior.

Method: Employed a ∇-theorem combined with a linking-type theorem to prove existence of multiple solutions when parameter λ is near eigenvalues of the spectral fractional Laplacian.

Result: Proved the existence of multiple solutions for the critical nonlocal problem with mixed Dirichlet-Neumann boundary conditions when λ is in a left neighborhood of eigenvalues.

Conclusion: The combination of ∇-theorem and linking-type methods successfully establishes multiplicity results for critical fractional problems with mixed boundary conditions.

Abstract: We consider the following nonlocal critical problem with mixed
Dirichlet-Neumann boundary conditions, \begin{equation} \left\{
  \begin{array}{ll}
  (-\Delta)^su=\lambda u+|u|^{2_s^*-2}u &\text{in}\ \Omega,\\ \mkern+38.5mu
u=0& \text{on}\ \Sigma_{\mathcal{D}},\\ \mkern+24mu \displaystyle
\frac{\partial u}{\partial \nu}=0 &\text{on}\ \Sigma_{\mathcal{N}},
  \end{array}
  \right. \end{equation} where $(-\Delta)^s$, $s\in (1/2,1)$, is the spectral
fractional Laplacian operator, $\Omega\subset\mathbb{R}^N$, $N>2s$, is a smooth
bounded domain, $2_s^*=\frac{2N}{N-2s}$ denotes the critical fractional Sobolev
exponent, $\lambda>0$ is a real parameter, $\nu$ is the outwards normal to
$\partial\Omega$, $\Sigma_{\mathcal{D}}$, $\Sigma_{\mathcal{N}}$ are smooth
$(N-1)$--dimensional submanifolds of $\partial\Omega$ such that
$\Sigma_{\mathcal{D}}\cup\Sigma_{\mathcal{N}}=\partial\Omega$,
$\Sigma_{\mathcal{D}}\cap\Sigma_{\mathcal{N}}=\emptyset$ and
$\Sigma_{\mathcal{D}}\cap\overline{\Sigma}_{\mathcal{N}}=\Gamma$ is a smooth
$(N-2)$--dimensional submanifold of $\partial\Omega$. By employing both a
$\nabla$-theorem combined with a linking-type theorem, we prove the existence
of multiple solutions when the parameter $\lambda$ is in a left neighborhood of
a given eigenvalue of $(-\Delta)^s$.

</details>


### [20] [Discrete Nonlinear Schrödinger versus Ablowitz-Ladik: Existence and dynamics of generalized NLS-type lattices over a nonzero background](https://arxiv.org/abs/2509.25650)
*Dirk Hennig,Nikos I. Karachalios,Dionyssios Mantzavinos,Dimitrios Mitsotakis*

Main category: math.AP

TL;DR: The paper establishes local well-posedness for generalized Ablowitz-Ladik and Discrete Nonlinear Schrödinger equations with nonzero boundary conditions, derives lifespan bounds, and shows potential for finite-time blow-up through theoretical analysis and numerical verification.


<details>
  <summary>Details</summary>
Motivation: To address the poorly understood well-posedness of generalized Ablowitz-Ladik and Discrete Nonlinear Schrödinger equations with nonzero boundary conditions on infinite lattices, which remains largely unexplored compared to vanishing boundary conditions.

Method: Theoretical analysis establishing local well-posedness, deriving analytical upper bounds for minimal guaranteed lifespan, proving distance estimates between solutions of different models, and conducting highly accurate numerical studies to verify theoretical predictions.

Result: Proved local well-posedness for both systems with nonzero boundary conditions, derived explicit lifespan bounds suggesting finite-time collapse, and numerically confirmed blow-up in Ablowitz-Ladik equation with excellent agreement between observed and predicted blow-up times.

Conclusion: The analysis reveals fundamental differences between the two models: Ablowitz-Ladik equation exhibits finite-time blow-up while Discrete Nonlinear Schrödinger equation shows global existence on finite lattices with quasi-collapse behavior manifested as persistent narrow oscillatory spikes.

Abstract: The question of well-posedness of the generalized Ablowitz-Ladik and Discrete
Nonlinear Schr\"{o}dinger equations with \textit{nonzero} boundary conditions
on the infinite lattice is far less understood than in the case where the
models are supplemented with vanishing boundary conditions. This question
remains largely unexplored even in the standard case of cubic nonlinearities in
which, in particular, the Ablowitz-Ladik equation is completely integrable
while the Discrete Nonlinear Schr\"{o}dinger equation is not (in contrast with
its continuous counterpart). We establish local well-posedness for both of
these generalized nonlinear systems supplemented with a broad class of nonzero
boundary conditions and, in addition, derive analytical upper bounds for the
minimal guaranteed lifespan of their solutions. These bounds depend explicitly
on the norm of the initial data, the background, and the nonlinearity
exponents. In particular, they suggest the possibility of finite-time collapse
(blow-up) of solutions. Furthermore, by comparing models with different
nonlinearity exponents, we prove estimates for the distance between their
respective solutions (measured in suitable metrics), valid up to their common
minimal guaranteed lifespan. Highly accurate numerical studies illustrate that
solutions of the generalized Ablowitz-Ladik equation may collapse in finite
time. Importantly, the numerically observed blow-up time is in excellent
agreement with the theoretically predicted order of the minimal guaranteed
lifespan. Furthermore, in the case of the Discrete Nonlinear Schr\"odinger
equation on a finite lattice we prove global existence of solutions; this is
consistent with our numerical observations of the phenomenon of
\textit{quasi-collapse}, manifested by narrow oscillatory spikes that
nevertheless persist throughout time -- continued in pdf ...

</details>


### [21] [Non-degeneracy and uniqueness of ground states to nonlinear elliptic equations with mixed local and nonlocal operators](https://arxiv.org/abs/2509.25677)
*Tianxiang Gou*

Main category: math.AP

TL;DR: This paper proves the non-degeneracy and uniqueness of ground states for a nonlinear elliptic equation with mixed local and nonlocal operators in the unit ball.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical properties of ground states for equations combining classical Laplacian and fractional Laplacian operators, which have applications in various physical and biological models.

Method: Proving the second eigenvalue of the linearized operator is simple and its eigenfunction changes sign once; applying a new Hopf type lemma; establishing a Picone type identity for antisymmetric functions; using blow-up and continuation arguments.

Result: The ground state is non-degenerate in both radially symmetric and non-radially symmetric function spaces, and uniqueness of ground states is established.

Conclusion: The paper successfully proves non-degeneracy and uniqueness of ground states for the mixed local-nonlocal elliptic equation in the unit ball under the given parameter conditions.

Abstract: This paper concerns the non-degeneracy and uniqueness of ground states to the
following nonlinear elliptic equation with mixed local and nonlocal operators,
$$ -\Delta u +(-\Delta)^s u + \lambda u=|u|^{p-2}u \quad \mbox{in} \,\,\, B,
\quad u=0 \quad \mbox{in} \,\,\, \R^N \backslash {B}, $$ where $N \geq 2$,
$0<s<1$, $2<p<2^*:=\frac{2N}{(N-2)^+}$, $\lambda > -\lambda_1$, $(-\Delta)^s$
denotes the fractional Laplacian, $\lambda_1>0$ denotes the first Dirichlet
eigenvalue of the operator $-\Delta +(-\Delta)^s$ in $B$ and $B$ denotes the
unit ball in $\R^N$. We prove that the second eigenvalue to the linearized
operator $-\Delta +(-\Delta)^s -(p-1)u^{p-2}$ in the space of radially
symmetric functions is simple, the corresponding eigenfunction changes sign
precisely once in the radial direction, where $u$ is a ground state. By
applying a new Hopf type lemma, we then get that $-\lambda$ cannot be an
eigenvalue of the linearized operator, which in turns leads to the
non-degeneracy of the ground state. Moreover, by establishing a Picone type
identity with respect to antisymmetric functions, we then derive the
non-degeneracy of the ground state in the space of non-radially symmetric
functions. Relying on the non-degeneracy of ground states and adapting a
blow-up argument together with a continuation argument, we then obtain the
uniqueness of ground states.

</details>


### [22] [The Mimura Integral: A Unified Framework for Riemann and Lebesgue Integration](https://arxiv.org/abs/2509.25875)
*Yoshifumi Mimura*

Main category: math.AP

TL;DR: A new integral equivalent to Lebesgue integral is constructed using extended Riemann sums, unifying measure theory and integration into a single framework.


<details>
  <summary>Details</summary>
Motivation: To create a more accessible and concrete alternative to the Lebesgue integral that avoids the traditional two-layer framework of measure and integration, while recovering the full measure-theoretic structure.

Method: Extending the notion of Riemann sums to construct an integral equivalent to the Lebesgue integral, unifying measure theory and integration into a single framework.

Result: Successfully constructed an integral equivalent to the Lebesgue integral that recovers outer measure, inner measure, and measurable sets, avoiding duplication in the classical approach.

Conclusion: This method provides a concrete and accessible alternative to the Riemann integral and serves as a natural bridge to abstract Lebesgue theory, unifying measure and integration layers.

Abstract: An integral on Euclidean space, equivalent to the Lebesgue integral, is
constructed by extending the notion of Riemann sums. In contrast to the
Henstock--Kurzweil and McShane integrals, the construction recovers the full
measure-theoretic structure -- outer measure, inner measure, and measurable
sets -- rather than merely reproducing integration with respect to the Lebesgue
measure. Whereas the classical approach to Lebesgue theory proceeds through a
two-layer framework of measure and integration, these layers are unified here
into a single framework, thereby avoiding duplication. Compared with the
Daniell integral, the method is more concrete and accessible, serving both as
an alternative to the Riemann integral and as a natural bridge to abstract
Lebesgue theory.

</details>


### [23] [On a fractional Alt-Caffarelli-Friedman-type monotonicity formula](https://arxiv.org/abs/2509.25891)
*Fausto Ferrari,Davide Giovagnoli,Enzo Maria Merlino*

Main category: math.AP

TL;DR: The paper introduces monotonicity formulas for nonlocal functionals using mean value properties of s-harmonic functions, without relying on extension techniques.


<details>
  <summary>Details</summary>
Motivation: To develop intrinsically nonlocal methods that mimic classical monotonicity formulas from Alt-Caffarelli-Friedman work, avoiding dependence on extension techniques.

Method: Exploits mean value properties of s-harmonic functions to derive monotonicity formulas in nonlocal settings, using purely nonlocal approaches.

Result: Established monotonicity formulas for nonlocal functionals, interior nonlocal gradient estimates, and a nonlocal analogue of the Bochner identity.

Conclusion: The paper successfully develops a framework for monotonicity formulas in nonlocal analysis that is independent of extension methods and provides important tools like nonlocal gradient estimates and Bochner identity analogues.

Abstract: In this note, by exploiting mean value properties of $s$-harmonic functions,
we introduce some monotonicity formulas in the nonlocal setting. We take into
account intrinsically nonlocal functionals mimicking those introduced by Alt,
Caffarelli and Friedman in the seminal work [Alt-Caffarelli-Friedman, Trans.
Amer. Math. Soc. (1984)]. Our approach is purely nonlocal and does not rely on
the extension technique. As a byproduct we also established interior nonlocal
gradient estimates and a nonlocal analogue of the Bochner identity.

</details>


### [24] [Inverse problem for connections in semi-linear wave equations on Lorentzian manifolds](https://arxiv.org/abs/2509.25971)
*Lauri Oksanen,Ruochong Zhang*

Main category: math.AP

TL;DR: Recovery of Hermitian connections for semi-linear wave equations with cubic nonlinearity on arbitrary globally hyperbolic Lorentzian manifolds using microlocal analysis.


<details>
  <summary>Details</summary>
Motivation: To extend the recovery of Hermitian connections to more general geometric settings, specifically arbitrary globally hyperbolic Lorentzian manifolds, beyond previous limited cases.

Method: Uses microlocal analysis of nonlinear wave interactions to recover a non-abelian broken light-ray transform, and inverts broken light-ray transforms on globally hyperbolic Lorentzian manifolds.

Result: Successfully recovers Hermitian connections for semi-linear wave equations with cubic nonlinearity in the general geometric setting of globally hyperbolic Lorentzian manifolds.

Conclusion: The approach demonstrates that microlocal analysis and inversion of broken light-ray transforms provide an effective method for recovering Hermitian connections in general globally hyperbolic Lorentzian settings.

Abstract: This paper recovers Hermitian connections of semi-linear wave equations with
cubic nonlinearity. The main novelty is in the geometric generality: we treat
the case of an arbitrary globally hyperbolic Lorentzian manifold. Our approach
is based on microlocal analysis of nonlinear wave interactions, which recovers
a non-abelian broken light-ray transform, and the inversion of broken light-ray
transforms on globally hyperbolic Lorentzian manifolds.

</details>


### [25] [Weak-strong uniqueness for general cross-diffusion systems with volume filling](https://arxiv.org/abs/2509.25978)
*Maria Heitzinger,Ansgar Jüngel*

Main category: math.AP

TL;DR: Establishes weak-strong uniqueness for cross-diffusion systems with volume filling using relative entropy method, overcoming non-symmetric/non-positive definite diffusion matrices through Boltzmann-type entropy structure.


<details>
  <summary>Details</summary>
Motivation: To prove weak-strong uniqueness for cross-diffusion systems where diffusion matrices are generally neither symmetric nor positive definite, which presents mathematical challenges.

Method: Uses relative entropy method with Boltzmann-type entropy structure; analyzes augmented mobility matrix that is positive definite only on specific subspaces; identifies general conditions on mobility matrix.

Result: Successfully establishes weak-strong uniqueness property for broad class of cross-diffusion systems with volume filling; provides several examples meeting required assumptions.

Conclusion: The approach effectively overcomes challenges posed by non-symmetric/non-positive definite diffusion matrices through entropy structure and augmented mobility analysis, with potential for extensions.

Abstract: The weak-strong uniqueness of solutions to a broad class of cross-diffusion
systems with volume filling is established. In general, the diffusion matrices
are neither symmetric nor positive definite. This issue is overcome by
supposing that the equations possess a Boltzmann-type entropy structure, which
ensures the existence of bounded weak solutions. In this framework, general
conditions on the mobility matrix are identified that allow for the proof of
the weak-strong uniqueness property by means of the relative entropy method.
The core idea consists in analyzing an augmented mobility matrix that is
positive definite only on a specific subspace. Several examples that meet the
required assumptions are provided, together with a discussion on possible
extensions.

</details>


### [26] [Sharp local well-posedness of $C^1$ vortex patches](https://arxiv.org/abs/2509.26046)
*Seungjae Lee*

Main category: math.AP

TL;DR: Establishes local well-posedness for vortex patches in C^{1,φ} spaces with modulus φ(r) = (-log r)^{-s} for s>3, which are rougher than Hölder spaces.


<details>
  <summary>Details</summary>
Motivation: The well-posedness of vortex patch boundary dynamics in C^1 remains an open problem, while C^{1,α} for 0<α<1 is known to be globally well-posed.

Method: Study the system of equations for curve parametrization γ ∈ C^{1,φ} and its Hilbert transform, deriving properties of Hilbert transform variants in critical spaces.

Result: Proved local well-posedness for vortex patches in C^{1,φ} spaces with specific modulus functions that are strictly rougher than Hölder-continuous curves.

Conclusion: The approach successfully handles vortex patches with boundary curves that are more singular than Hölder-continuous ones, advancing beyond previous results.

Abstract: It is well known that the boundary dynamics of vortex patches is globally
well-posed in the H\"older space $C^{1,\alpha}$ for $0<\alpha<1$, whereas the
well-posedness in $C^1$ remains an open problem, even locally. In this paper,
we establish the local well-posedness for vortex patches in the space
$C^{1,\varphi}$ defined via a modulus of continuity $\varphi$ that satisfies
certain structural assumptions. Our class includes curves that are strictly
rougher than the H\"older-continuous ones, with prototypical examples being
$\varphi(r) = (-\log r)^{-s}$ for $s>3$. Motivated by the fact that the
velocity operator in the contour dynamics equation is a nonlinear variant of
the Hilbert transform, we study the system of equations satisfied by the curve
parametrization $\gamma \in C^{1,\varphi}$ and its Hilbert transform. In doing
so, we derive several properties of the Hilbert transform and its variants in
critical spaces, which are essential for controlling the velocity operator and
its Hilbert transform.

</details>


### [27] [Initial traces and solvability of the fast diffusion equation with power-type nonlinearity](https://arxiv.org/abs/2509.26054)
*Kazuhiro Ishige,Nobuhito Miyake*

Main category: math.AP

TL;DR: Analysis of initial traces for nonnegative solutions to fast diffusion equation with power-type nonlinearity, identifying necessary conditions and establishing sharp sufficient conditions for solution existence using Morrey spaces.


<details>
  <summary>Details</summary>
Motivation: To understand the qualitative behavior of initial traces for nonnegative solutions to the fast diffusion equation with power-type nonlinearity and determine precise conditions for solution existence.

Method: Employ uniform local Morrey spaces and their generalizations to establish sharp sufficient conditions for solution existence.

Result: Identified necessary conditions for solution existence and established sharp sufficient conditions using Morrey space framework.

Conclusion: The paper provides comprehensive analysis of initial trace behavior and establishes precise conditions for existence of solutions to the fast diffusion equation with power-type nonlinearity.

Abstract: We investigate the qualitative behavior of the initial traces of nonnegative
solutions to the fast diffusion equation with power-type nonlinearity.
Necessary conditions for the existence of solutions to the corresponding Cauchy
problem are identified. Moreover, sharp sufficient conditions for the existence
of solutions are established by employing uniform local Morrey spaces and their
generalizations.

</details>


### [28] [Global mild solutions in a critical setting for a forced fractional Boussinesq system](https://arxiv.org/abs/2509.26098)
*Diego Chamorro,Maxence Mansais*

Main category: math.AP

TL;DR: Study of mild solutions for forced incompressible fractional Boussinesq system using fixed point arguments in critical functional frameworks, particularly parabolic Morrey spaces.


<details>
  <summary>Details</summary>
Motivation: To establish existence of global solutions for the fractional Boussinesq system by working in critical functional settings that provide the largest available frameworks for constructing mild solutions.

Method: Using fixed point arguments in an adapted functional framework with suitable estimates for the involved terms, working specifically with parabolic Morrey spaces as the critical functional framework.

Result: Obtained mild solutions for the forced incompressible fractional Boussinesq system through the fixed point approach in critical settings.

Conclusion: Parabolic Morrey spaces provide one of the largest critical functional frameworks for constructing mild solutions for fractional Boussinesq equations, enabling the existence of global solutions.

Abstract: We study here mild solutions for the forced, incompressible fractional
Boussinesq system. Under suitable estimates for the terms involved (in an
adapted functional framework) we can invoque a fixed point argument in order to
obtain mild solutions. Although many functional spaces can be considered, we
are interested here in a critical setting which ensures the existence of global
solutions and we will work in particular with parabolic Morrey spaces which
provide one of the largest critical functional frameworks available for
constructing mild solutions for the fractional Boussinesq equations.

</details>


### [29] [Enhancing PINN Performance Through Lie Symmetry Group](https://arxiv.org/abs/2509.26113)
*Ali Haider Shah,Naveed R. Butt,Asif Ahmad,Muhammad Omer Bin Saeed*

Main category: math.AP

TL;DR: Integrating Lie symmetry groups with Physics-Informed Neural Networks (PINNs) significantly improves PDE solving accuracy and efficiency through infinitesimal generators and adaptive techniques.


<details>
  <summary>Details</summary>
Motivation: To enhance PINN performance for solving partial differential equations by incorporating Lie symmetry group theory, which can provide exact solutions for symmetric PDEs.

Method: Combines PINNs with Lie symmetry group concepts using infinitesimal generators and adaptive techniques, tested on three distinct cases with progressive modifications.

Result: Numerical experiments show substantial improvements in PINN performance when Lie symmetry is incorporated, outperforming state-of-the-art numerical methods.

Conclusion: Integrating abstract mathematical concepts like Lie symmetry into deep learning frameworks is crucial for effectively solving complex scientific problems with PINNs.

Abstract: This paper presents intersection of Physics informed neural networks (PINNs)
and Lie symmetry group to enhance the accuracy and efficiency of solving
partial differential equation (PDEs). Various methods have been developed to
solve these equations. A Lie group is an efficient method that can lead to
exact solutions for the PDEs that possessing Lie Symmetry. Leveraging the
concept of infinitesimal generators from Lie symmetry group in a novel manner
within PINN leads to significant improvements in solution of PDEs. In this
study three distinct cases are discussed, each showing progressive improvements
achieved through Lie symmetry modifications and adaptive techniques.
State-of-the-art numerical methods are adopted for comparing the progressive
PINN models. Numerical experiments demonstrate the key role of Lie symmetry in
enhancing PINNs performance, emphasizing the importance of integrating abstract
mathematical concepts into deep learning for addressing complex scientific
problems adequately.

</details>


### [30] [On the propagation of mountain waves: linear theory](https://arxiv.org/abs/2509.26125)
*Adrian Constantin,Jörg Weber*

Main category: math.AP

TL;DR: A rigorous solution concept for linear mountain waves in 2D using Helmholtz equation with altitude-dependent potential and non-classical radiation condition.


<details>
  <summary>Details</summary>
Motivation: To establish a physically correct solution for mountain wave problems that differs from classical wave radiation conditions due to different physical situations.

Method: Linearized governing equations with variable change, Dirichlet boundary value problem for Helmholtz equation, transform method construction following Lyra's monotonicity criterion.

Result: Successfully derived and constructed the physically correct solution that distinguishes between vertically propagating waves and trapped lee waves.

Conclusion: First rigorous work on Helmholtz-like equations in upper half-plane with non-classical radiation condition, providing clear recognition of two mountain wave types.

Abstract: We derive and establish a solution concept for the linear mountain wave
problem in two dimensions. After linearizing the governing equations and a
change of variables, the problem can be stated as a Dirichlet boundary value
problem for a Helmholtz equation in terms of the vertical wind profile in the
upper half-plane, with altitude-dependent potential (the Scorer parameter). To
single out the correct solution, we have to make use of a radiation condition
which is, due to the different physical situation, different from the classical
Sommerfeld radiation condition for electromagnetic or acoustic waves. We
rigorously develop a transform method and construct the physically correct
solution, following Lyra's monotonicity criterion for mountain waves. In this
procedure, we clearly recognize the two typical types of mountain waves:
vertically propagating waves and trapped lee waves. This paper is the first
rigorous work on Helmholtz-like equations in the upper half-plane subject to
such a non-classical radiation condition.

</details>


### [31] [An all-topology two-fluid model for two-phase flows derived through Hamilton's Stationary Action Principle](https://arxiv.org/abs/2509.26298)
*Ward Haegeman,Giuseppe Orlando,Samuel Kokh,Marc Massot*

Main category: math.AP

TL;DR: A novel multi-fluid model for compressible two-phase flows derived using Stationary Action Principle, featuring full closure, hyperbolicity, and proper treatment of weak solutions.


<details>
  <summary>Details</summary>
Motivation: To develop a physically sound and mathematically rigorous model for compressible two-phase flows that is fully closed and handles all flow topologies properly.

Method: Derived through a newly developed Stationary Action Principle framework, introducing interfacial work as a new quantity and providing closures for interfacial quantities via the variational principle.

Result: The model is hyperbolic, symmetrizable, admits an entropy conservation law, has uniquely defined jump conditions for non-conservative products, includes lift forces in multi-dimensional settings, and provides proper treatment of weak solutions.

Conclusion: The model constitutes a sound basis for future numerical simulations of compressible two-phase flows.

Abstract: We present a novel multi-fluid model for compressible two-phase flows. The
model is derived through a newly developed Stationary Action Principle
framework. It is fully closed and introduces a new interfacial quantity, the
interfacial work. The closures for the interfacial quantities are provided by
the variational principle. They are physically sound and well-defined for all
type of flow topologies. The model is shown to be hyperbolic, symmetrizable,
and admits an entropy conservation law. Its non-conservative products yield
uniquely defined jump conditions which are provided. As such, it allows for the
proper treatment of weak solutions. In the multi-dimensional setting, the model
presents lift forces which are discussed. The model constitutes a sound basis
for future numerical simulations.

</details>


### [32] [Competition of small targets in planar domains: from Dirichlet to Robin and Steklov boundary condition](https://arxiv.org/abs/2509.26367)
*Denis S. Grebenkov,Michael J. Ward*

Main category: math.AP

TL;DR: Asymptotic analysis of diffusion to small targets with various boundary conditions, including partial reactivity effects and extensions to interior/exterior domains.


<details>
  <summary>Details</summary>
Motivation: To understand competition between multiple small targets for diffusing particles and the role of surface reactions in diffusion-controlled processes.

Method: Method of matched asymptotic expansions applied to diffusion problems with small targets, analyzing Dirichlet, Robin, and mixed Steklov-Neumann boundary conditions.

Result: Derived asymptotic behavior of splitting probabilities, showed how partial reactivity reduces effective target size, and obtained first asymptotic results for eigenvalues/eigenfunctions in small-target limit.

Conclusion: The asymptotic approach successfully handles various boundary conditions and can be extended to interior targets and exterior domains, with direct applications to diffusion-controlled reactions.

Abstract: We consider steady-state diffusion in a bounded planar domain with multiple
small targets on a smooth boundary. Using the method of matched asymptotic
expansions, we investigate the competition of these targets for a diffusing
particle and the crucial role of surface reactions on the targets. We start
from the classical problem of splitting probabilities for perfectly reactive
targets with Dirichlet boundary condition and improve some earlier results. We
discuss how this approach can be generalized to partially reactive targets
characterized by a Robin boundary condition. In particular, we show how partial
reactivity reduces the effective size of the target. In addition, we consider
more intricate surface reactions modeled by mixed Steklov-Neumann or
Steklov-Neumann-Dirichlet problems. We provide the first derivation of the
asymptotic behavior of the eigenvalues and eigenfunctions for these spectral
problems in the small-target limit. Finally, we show how our asymptotic
approach can be extended to interior targets in the bulk and to exterior
problems where diffusion occurs in an unbounded planar domain outside a compact
set. Direct applications of these results to diffusion-controlled reactions are
discussed.

</details>


### [33] [The Effective Reactivity for Capturing Brownian Motion by Partially Reactive Patches on a Spherical Surface](https://arxiv.org/abs/2509.26381)
*Denis S. Grebenkov,Michael J. Ward*

Main category: math.AP

TL;DR: The paper analyzes ligand trapping by a sphere with multiple small reactive patches using matched asymptotic expansions, deriving explicit formulas for target capacitance and effective reactivity that work for any patch reactivity.


<details>
  <summary>Details</summary>
Motivation: To understand how diffusing ligands are trapped by structured targets with multiple small reactive patches on an otherwise reflecting boundary, which has applications in chemical and biological systems.

Method: Used method of matched asymptotic expansions to derive target capacitance formulas, spectral expansion over Steklov eigenfunctions for local reactive capacitance, and homogenization limit for many patches.

Result: Derived explicit results for target capacitance valid for any patch reactivity, developed accurate sigmoidal approximation for local reactive capacitance, and established scaling law for effective capacitance and reactivity in homogenization limit.

Conclusion: The derived scaling law provides highly accurate approximation for effective capacitance and reactivity over full range of patch reactivities, even with moderately large number of reactive patches.

Abstract: We analyze the trapping of diffusing ligands, modeled as Brownian particles,
by a sphere that has $N$ partially reactive boundary patches, each of small
area, on an otherwise reflecting boundary. For such a structured target, the
partial reactivity of each boundary patch is characterized by a Robin boundary
condition, with a local boundary reactivity $\kappa_i$ for $i=1,\ldots,N$. For
any spatial arrangement of well-separated patches on the surface of the sphere,
the method of matched asymptotic expansions is used to derive explicit results
for the capacitance $C_{\rm T}$ of the structured target, which is valid for
any $\kappa_i>0$. This target capacitance $C_{\rm T}$ is defined in terms of a
Green's matrix, which depends on the spatial configuration of patches, the
local reactive capacitance $C_i(\kappa_i)$ of each patch and another
coefficient that depends on the local geometry near a patch. The analytical
dependence of $C_{i}(\kappa_i)$ on $\kappa_i$ is uncovered via a spectral
expansion over Steklov eigenfunctions. For circular patches, the latter are
readily computed numerically and provide an accurate fully explicit sigmoidal
approximation for $C_{i}(\kappa_i)$. In the homogenization limit of $N\gg 1$
identical uniformly-spaced patches with $\kappa_i=\kappa$, we derive an
explicit scaling law for the effective capacitance and the effective reactivity
of the structured target that is valid in the limit of small patch area
fraction. From a comparison with numerical simulations, we show that this
scaling law provides a highly accurate approximation over the full range
$\kappa>0$, even when there is only a moderately large number of reactive
patches.

</details>


### [34] [On the dependence of the nonlinear Schrodinger flow upon the power of the nonlinearity](https://arxiv.org/abs/2509.26414)
*Rémi Carles,Quentin Chauleur,Guillaume Ferriere*

Main category: math.AP

TL;DR: The paper analyzes continuity properties of the flow map for defocusing energy-subcritical nonlinear Schrödinger equations with varying power, including local/global time continuity and convergence to logarithmic Schrödinger equation when power approaches zero.


<details>
  <summary>Details</summary>
Motivation: To understand how the flow map continuity behaves as the nonlinear power parameter varies in Schrödinger equations, particularly addressing the challenging case of convergence to the logarithmic Schrödinger equation when power goes to zero.

Method: The proof uses estimates for perturbed porous medium equations involving the harmonic Fokker-Planck operator, and analyzes time-dependent rescalings with uniform continuity in Kantorovich distance.

Result: Shows local time continuity in energy space for any power, global time continuity for large powers, and uniform convergence of renormalized solutions to logarithmic Schrödinger equation when power approaches zero.

Conclusion: The study establishes comprehensive continuity properties for nonlinear Schrödinger equations with varying power parameters, including the delicate limiting case of convergence to the logarithmic equation.

Abstract: We prove continuity properties for the flow map associated to the defocusing
energy-subcritical power-like nonlinear Schr{\"o}dinger equation, when the
power varies. We show local in time continuity in the energy space for any
power, and global in time continuity for sufficiently large powers. When the
linear dispersive rate is counterbalanced by a time-dependent rescaling, we
show a uniform in time continuity of the squared modulus of this rescaled
function, in Kantorovich distance, for any power, including long range cases in
terms of scattering. The most difficult result addresses the convergence of
suitably renormalized solutions to the solution of the logarithmic
Schr{\"o}dinger equation, when the power goes to zero, uniformly in time, in
Kantorovich distance. The proof relies on estimates for perturbed porous medium
equations, involving the harmonic Fokker-Planck operator.

</details>


### [35] [Curl Measure Fields, the Generalized Stokes Theorem and Vorticity Fluxes](https://arxiv.org/abs/2509.26465)
*Gui-Qiang G. Chen,Franz Gmeineder,Monica Torres*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce and analyze the class $\mathscr{CM}^{p}$ of curl-measure fields
that are $p$-integrable vector fields whose distributional curl is a
vector-valued finite Radon measure. These spaces provide a unifying framework
for problems involving vorticity. A central focus of this paper is the
development of Stokes-type theorems in low-regularity regimes, made possible by
new trace theorems for curl-measure fields. To this end, we introduce Stokes
functionals on so-called good manifolds, defined by the finiteness of
manifold-adapted maximal operators. Using novel techniques that may be of
independent interest, we establish results that are new even in classical
settings, such as Sobolev spaces or their curl-variants
$\mathrm{H}^{\mathrm{curl}}(\mathbb{R}^{3})$, which arise, for example, in the
study of Maxwell's equations. The sharpness of our theorems is illustrated
through several fundamental examples.

</details>


### [36] [On uniqueness of radial potentials for given Dirichlet spectra with distinct angular momenta](https://arxiv.org/abs/2509.26485)
*Damien Gobin,Benoît Grébert,Bernard Helffer,François Nicoleau*

Main category: math.AP

TL;DR: The paper solves an inverse spectral problem for radial Schrödinger operators with singular potentials, showing that Dirichlet spectra for infinitely many angular momenta uniquely determine the potential, and proving uniqueness with just two spectra near zero potential.


<details>
  <summary>Details</summary>
Motivation: To address the inverse spectral problem for radial Schrödinger operators with singular potentials and verify Rundell and Sacks' conjecture about potential determination from spectral data.

Method: Explicit analysis of singular differential equations combined with the classical Kneser-Sommerfeld formula, using Dirichlet spectra for different angular momenta.

Result: Proved that: (1) infinitely many angular momenta spectra uniquely determine the potential; (2) near zero potential, two spectra (for angular momenta pairs (0,1) and (0,2)) suffice for unique determination.

Conclusion: The results confirm Rundell and Sacks' conjecture in linearized settings for specific angular momentum configurations, establishing rigorous uniqueness results for inverse spectral problems.

Abstract: We consider an inverse spectral problem for radial Schr\" odinger operators
with singular potentials. First, we show that the knowledge of the Dirichlet
spectra for infinitely many angular momenta~$\ell$ satisfying a M\"untz-type
condition uniquely determines the potential. Then, in a neighborhood of the
zero potential, we prove that the potential is uniquely determined by two
Dirichlet spectra associated with distinct angular momenta in the cases
\((\ell_1,\ell_2) = (0,1)\) and \((0,2)\). Our approach relies on an explicit
analysis of the corresponding singular differential equation, combined with the
classical Kneser--Sommerfeld formula. These results confirm, in the linearized
setting and in these configurations, a conjecture originally formulated by
Rundell and Sacks (2001).

</details>


### [37] [On Korn inequalities with lower order trace terms](https://arxiv.org/abs/2509.26526)
*Franz Gmeineder,Endre Süli,Tabea Tscherpel*

Main category: math.AP

TL;DR: Elementary estimate generalizing various Korn inequalities from literature, including inequalities with normal/tangential trace components and lower dimensional trace integrals.


<details>
  <summary>Details</summary>
Motivation: To provide a unified framework that encompasses and extends numerous scattered Korn inequalities found in the literature.

Method: Developed an elementary estimate that serves as a generalization tool for various Korn-type inequalities.

Result: Successfully derived a general estimate that yields Korn-type inequalities involving normal/tangential trace components and lower dimensional trace integrals as special cases.

Conclusion: The elementary estimate provides a comprehensive framework that unifies and extends existing Korn inequalities, offering broader applicability in mathematical analysis.

Abstract: We give an elementary estimate that entails and generalises numerous Korn
inequalities scattered in the literature. As special instances, we obtain
general Korn-type inequalities involving normal or tangential trace components,
or lower dimensional trace integrals.

</details>


### [38] [$C^{1+α}$ regularity for fractional $p$-harmonic functions](https://arxiv.org/abs/2509.26565)
*Davide Giovagnoli,David Jesus,Luis Silvestre*

Main category: math.AP

TL;DR: Interior C^{1,α} regularity estimates for solutions of fractional p-Laplace equations when p ∈ [2,2/(1-s))


<details>
  <summary>Details</summary>
Motivation: To establish higher regularity estimates for solutions of fractional p-Laplace equations, extending classical regularity theory to the nonlocal setting

Method: Mathematical analysis of the fractional p-Laplace operator (-Δ_p)^s u = 0, focusing on the parameter range p ∈ [2,2/(1-s))

Result: Proved interior C^{1,α} regularity for some α > 0 for solutions in the specified parameter range

Conclusion: Solutions of fractional p-Laplace equations exhibit C^{1,α} interior regularity when p is in the range [2,2/(1-s)), providing important regularity results for this class of nonlocal equations

Abstract: We establish interior $C^{1,\alpha}$ regularity estimates for some $\alpha >
0$, for solutions of the fractional $p$-Laplace equation $(-\Delta_p)^s u = 0$
when $p$ is in the range $p \in [2,2/(1-s))$.

</details>


### [39] [The distorted Fourier transform for the linearized Gross-Pitaevskii equation in the Hyperbolic plane](https://arxiv.org/abs/2509.26590)
*Oussama Landoulsi,Sohrab Shahshahani*

Main category: math.AP

TL;DR: Developed distorted Fourier transform for radial non-self-adjoint matrix Schrödinger operators on hyperbolic plane, motivated by Ginzburg-Landau vortex stability problem.


<details>
  <summary>Details</summary>
Motivation: Study stability problem for Ginzburg-Landau vortices on hyperbolic plane, particularly linearizing equivariant Ginzburg-Landau equation around degree one vortex.

Method: Systematically construct distorted Fourier transform using Stone formula for complex energies, taking limit as energy approaches real spectrum. Carefully analyze resolvent for complex energies near real line.

Result: Successfully developed distorted Fourier transform framework for general class of radial non-self-adjoint matrix Schrödinger operators on hyperbolic plane.

Conclusion: This analysis provides foundation for studying stability of Ginzburg-Landau vortex under equivariant perturbations, extending approaches from previous works to non-self-adjoint limiting operators.

Abstract: Motivated by the stability problem for Ginzburg-Landau vortices on the
hyperbolic plane, we develop the distorted Fourier transform for a general
class of radial non-self-adjoint matrix Schr\"odinger operators on the
hyperbolic plane. This applies in particular to the operator obtained by
linearizing the equivariant Ginzburg-Landau equation on the hyperbolic plane
around the degree one vortex. We systematically
  construct the distorted Fourier transform by writing the Stone formula for
complex energies and taking the limit as the energy tends to the spectrum of
the operator on the real line. This approach entails a careful analysis of the
resolvent for complex energies in a neighborhood of the real line. It is the
analogue of the approaches used in \cite{KS,ES2, LSS25}, where the limiting
operator as $r\to\infty$ is not self-adjoint and which we carry out for all
energies. Our analysis serves as the starting point for the study of the
stability of the Ginzburg-Landau vortex under equivariant perturbations.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [40] [A general optimization framework for mapping local transition-state networks](https://arxiv.org/abs/2509.26269)
*Qichen Xu,Anna Delin*

Main category: physics.comp-ph

TL;DR: A general optimization framework for systematically mapping energy landscapes and transition-state networks using multi-objective exploration with bilayer minimum-mode kernel, enabling efficient discovery of transition pathways in complex systems.


<details>
  <summary>Details</summary>
Motivation: Current methods for understanding state transitions in complex systems either prescribe endpoints or randomly sample limited saddles, lacking systematic coverage of energy landscapes needed for mechanism-based prediction.

Method: Uses a multi-objective explorer coupled with bilayer minimum-mode kernel: inner layer recovers lowest-curvature subspace via Hessian-vector products, outer layer optimizes on reflected force to reach index-1 saddles, with two-sided descent for connectivity certification. GPU-based pipeline works across autodiff backends.

Result: On atomistic-spin tests, matches explicit-Hessian accuracy while reducing peak memory and wall time by orders of magnitude. Applied to skyrmionic model, recovers known routes and reveals new mechanisms including meron-antimeron-mediated processes. Same core transfers to Cartesian atoms, mapping Ni(111) heptamer rearrangements.

Conclusion: The framework provides general, efficient approach for systematically expanding local coverage of energy landscapes, enabling discovery of previously unreported transition mechanisms across diverse physical systems.

Abstract: Understanding how complex systems transition between states requires mapping
the energy landscape that governs these changes. Local transition-state
networks reveal the barrier architecture that explains observed behaviour and
enables mechanism-based prediction across computational chemistry, biology, and
physics, yet current practice either prescribes endpoints or randomly samples
only a few saddles around an initial guess. We present a general optimization
framework that systematically expands local coverage by coupling a
multi-objective explorer with a bilayer minimum-mode kernel. The inner layer
uses Hessian-vector products to recover the lowest-curvature subspace (smallest
k eigenpairs), the outer layer optimizes on a reflected force to reach index-1
saddles, then a two-sided descent certifies connectivity. The GPU-based
pipeline is portable across autodiff backends and eigensolvers and, on large
atomistic-spin tests, matches explicit-Hessian accuracy while cutting peak
memory and wall time by orders of magnitude. Applied to a DFT-parameterized
N\'eel-type skyrmionic model, it recovers known routes and reveals previously
unreported mechanisms, including meron-antimeron-mediated N\'eel-type
skyrmionic duplication, annihilation, and chiral-droplet formation, enabling up
to 32 pathways between biskyrmion (Q=2) and biantiskyrmion (Q=-2). The same
core transfers to Cartesian atoms, automatically mapping canonical
rearrangements of a Ni(111) heptamer, underscoring the framework's generality.

</details>


### [41] [The uniqueness of inverse scattering problems, reciprocity principles, and nonradiating sources related to low-signature structures](https://arxiv.org/abs/2509.26304)
*Johan Helsing,Anders Karlsson*

Main category: physics.comp-ph

TL;DR: Perfectly conducting structures with cavities that produce negligible scattered power when exposed to electromagnetic waves, enabling object concealment.


<details>
  <summary>Details</summary>
Motivation: To develop structures that can conceal objects by producing minimal scattered electromagnetic power, with applications in low-observable technology.

Method: Theoretical investigation combined with accurate numerical computations of perfectly electrically conducting structures with cavities.

Result: Three key findings: uniqueness of inverse scattering solution, reciprocity relation for far-field scattering amplitude, and existence of non-radiating sources generating strong near-field electromagnetic fields.

Conclusion: The structures effectively conceal objects with minimal scattering, with important implications for low-observable applications.

Abstract: This paper is about perfectly electrically conducting structures designed to
produce negligible scattered power when exposed to a time-harmonic plane
electromagnetic wave. The structures feature cavities capable of concealing
objects. Theoretical investigations of the properties of the structures
combined with accurate numerical computations lead to three key findings: the
first concerns the uniqueness of the solution to an inverse scattering problem,
the second establishes a reciprocity relation for the far-field scattering
amplitude, and the third reveals the existence of non-radiating sources that
generate substantial electromagnetic fields near the source region. The results
have applications in low-observable technology.

</details>


### [42] [TBPLaS 2.0: a Tight-Binding Package for Large-scale Simulation](https://arxiv.org/abs/2509.26309)
*Yunhai Li,Zewen Wu,Miao Zhang,Junyi Wang,Shengjun Yuan*

Main category: physics.comp-ph

TL;DR: TBPLaS 2.0 introduces major upgrades including C++ implementation, GPU support, new physics features, and significant performance improvements over version 1.3.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency and capabilities of large-scale tight-binding simulations by providing optimized implementations and new computational features.

Method: Rewrote solvers in C++ from scratch, added C++ modeling tools, implemented GPU computing support, and introduced new physics features like spin texture and Berry curvature calculations.

Result: Achieved efficiency improvements of several orders for modeling tools and several times to an order for solvers, with unified workflow and comprehensive new features.

Conclusion: TBPLaS 2.0 represents a significant advancement in large-scale tight-binding simulations with enhanced performance, new capabilities, and improved user experience.

Abstract: We introduce version 2.0 of TBPLaS, a package for large-scale simulation
based on the tight-binding propagation method (TBPM). This new version brings
significant improvements with many new features. Existing Python/Cython
modeling tools have been thoroughly optimized, and a compatible C++
implementation of the modeling tools is now available, offering efficiency
enhancement of several orders. The solvers have been rewritten in C++ from
scratch, with the efficiency enhanced by several times or even by an order. The
workflow of utilizing solvers has also been unified into a more comprehensive
and consistent manner. New features include spin texture, Berry curvature and
Chern number calculation, search of eigenvalues within a specific energy range,
analytical Hamiltonian, and GPU computing support. The documentation and
tutorials have also been updated to the new version. In this paper, we discuss
the revisions with respect to version 1.3 and demonstrate the new features.
Benchmarks on modeling tools and solvers are also provided.

</details>


### [43] [Method for backtracking the layer thermal conductivities of multilayer thin film structure using coupled Newton Raphson approach and 3-omega approach](https://arxiv.org/abs/2509.26408)
*Aigbe Awenlimobor,Jiajun Xu*

Main category: physics.comp-ph

TL;DR: A novel method combining 3-omega experiments with Newton-Raphson numerical approach to determine individual layer thermal conductivities in multilayer thin films.


<details>
  <summary>Details</summary>
Motivation: Standard 3-omega method only provides bulk thermal conductivity for multilayer systems, making it impossible to determine individual layer properties without additional approaches.

Method: Coupled 3-omega experimental setup with Newton-Raphson numerical optimization to backtrack individual layer thermal conductivities in multilayer thin films.

Result: The method successfully determines individual layer thermal conductivities and is validated using high-fidelity literature data.

Conclusion: The combined experimental-numerical approach provides an effective solution for obtaining individual layer thermal properties in multilayer systems where standard methods only yield bulk values.

Abstract: The thermal conductivity of thin films is commonly estimated using the
3-omega experimental method. When calibrating the test setup, it is customary
to use a specimen with a known thermal conductivity for validation. However,
when determining the thermal conductivity of samples with unknown values,
numerical approximations can provide a means to validate experimental results
and ensure the integrity of the setup. A simple analytical or finite element
analysis (FEA) method can be used to achieve this. For multilayer systems of
unknown layer thermal conductivities, the 3-omega experimental setup only
provides information about the overall bulk thermal conductivity of the system.
To obtain the individual layer thermal conductivities, a combined experimental
and numerical approach can be used. This article presents a novel method for
backtracking the layer thermal conductivities of a multilayer thin film
structure using a coupled 3-omega experimental and Newton-Raphson numerical
approach. The method is validated using high-fidelity data obtained from
literature.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [44] [Persistence of Deuterium and Tritium Nuclear Spin-Polarization in Presence of High-Frequency Plasma Waves](https://arxiv.org/abs/2509.25506)
*J. W. S. Cook,H. Ali,J. F. Parisi,A. Diallo,N. Faatz*

Main category: physics.plasm-ph

TL;DR: First-principles calculations show wave-driven depolarization of spin-polarized deuterium and tritium nuclei is surprisingly weak in tokamak plasmas, contrary to prior expectations, enabling polarization maintenance for enhanced fusion reactivity.


<details>
  <summary>Details</summary>
Motivation: To investigate whether spin-polarized nuclear fuel can maintain its polarization in realistic tokamak plasmas long enough to boost fusion reactivity, addressing concerns about wave-driven depolarization that could undermine this performance enhancement approach.

Method: Combined first-principles numerical calculations with linear and nonlinear simulations, using full-orbit particle tracing integrated with a dedicated depolarization solver to analyze resonant interactions between spin-polarized nuclei and plasma waves in realistic tokamak conditions.

Result: Alpha particle-driven Alfvénic modes cause negligible depolarization, contrary to prior literature expectations. Other Alfvénic instabilities can degrade polarization but only under conditions unlikely to occur on transport timescales. Wave-driven depolarization is surprisingly weak in SPARC and ITER-scale devices.

Conclusion: Spin-polarized fuel can maintain polarization long enough to significantly boost fusion reactivity, providing a viable path for enhanced performance in magnetic confinement fusion power plants.

Abstract: We present first-principles numerical calculations of the depolarization rate
of spin-polarized deuterium and tritium nuclei in realistic tokamak plasmas,
driven by resonant interactions with plasma waves. Backed up by first-of-a-kind
linear and nonlinear simulations, we find that alpha particle-driven Alfv\'enic
modes cause only negligible depolarization, which is contrary to expectations
in prior literature. Other Alfv\'enic instabilities can in principle degrade
polarization, but only under conditions unlikely to be realized on transport
timescales. By combining full-orbit particle tracing with a dedicated
depolarization solver, we demonstrate that wave-driven depolarization is
surprisingly weak in SPARC and ITER-scale devices. These results provide strong
evidence that spin-polarized fuel can maintain its polarization long enough to
boost fusion reactivity, opening a viable path toward substantially enhanced
performance in magnetic confinement fusion power plants.

</details>


### [45] [Validation of ERMES 20.0 finite element code for MAST Upgrade O-X mode conversion](https://arxiv.org/abs/2509.26357)
*Ruben Otin,Ying Hao Matthew Liang,Thomas Wilson,Simon Freethy,Valerian Hall-Chen*

Main category: physics.plasm-ph

TL;DR: Validation of ERMES 20.0 finite element code against FDTD solvers for O-X mode conversion in EBW regime shows excellent agreement.


<details>
  <summary>Details</summary>
Motivation: To validate the accuracy and robustness of the frequency-domain finite element code ERMES 20.0 for modeling cold plasma wave interactions.

Method: Benchmarked ERMES 20.0 against Finite Difference Time Domain (FDTD) solvers, focusing on Ordinary-Extraordinary (O-X) mode conversion in Electron Bernstein Wave regime. Tested several finite element formulations.

Result: Simulations demonstrated excellent agreement between ERMES 20.0 and FDTD approaches in terms of mode conversion efficiency and wave propagation characteristics.

Conclusion: ERMES 20.0 is confirmed as accurate and robust for modeling cold plasma wave interactions, validated through comparison with established FDTD methods.

Abstract: This study presents the validation of the frequency-domain finite element
code ERMES 20.0, benchmarked against Finite Difference Time Domain (FDTD)
solvers. The simulations focus on Ordinary-Extraordinary (O-X) mode conversion
in the Electron Bernstein Wave (EBW) regime of the MAST Upgrade experiment.
Validation is performed in terms of mode conversion efficiency and wave
propagation characteristics. Several finite element formulations are tested and
compared with the FDTD results. The simulations demonstrate excellent agreement
between the different approaches, confirming the accuracy and robustness of
ERMES 20.0 for modeling cold plasma wave interactions.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [46] [Axial resolution post-processing engineering in Fresnel incoherent correlation holography](https://arxiv.org/abs/2509.26179)
*Shivasubramanian Gopinath,Joseph Rosen,Vijayakumar Anand*

Main category: physics.optics

TL;DR: PEAR-FINCH enables post-recording depth of focus engineering in Fresnel incoherent correlation holography by combining holograms from a library with different axial characteristics.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of conventional FINCH by enabling post-recording engineering of axial resolution and depth of focus, which was not previously possible.

Method: Record a library of FINCH holograms with different focal distances, then combine selected holograms and use two-step reconstruction (numerical back-propagation and deconvolution with point spread hologram).

Result: PEAR-FINCH achieves substantially extended depth of focus compared to both direct imaging and conventional FINCH, as confirmed by experiments with multiplane objects.

Conclusion: PEAR-FINCH is a promising method for biomedical imaging, holography, and fluorescence microscopy applications due to its ability to engineer axial characteristics post-recording.

Abstract: Fresnel incoherent correlation holography (FINCH) is a
self-interference-based incoherent digital holography method. In FINCH, light
from an object point is split into two beams, modulated differently using two
lenses with different focal distances, and creates a self-interference
hologram. At least three phase-shifted holograms are recorded and synthesized
into a complex hologram, which reconstructs the object image without twin image
and bias noises. Compared with conventional imaging, FINCH exhibits a longer
depth of focus (DOF) and higher lateral resolution. In this study, we propose
and demonstrate a new method termed post-engineering of axial resolution in
FINCH (PEAR-FINCH), which enables post-recording DOF engineering for the first
time. In PEAR-FINCH, a library of FINCH holograms catalogued with unique axial
characteristics, DOF, and focus location is recorded by changing the focal
distance of one of the diffractive lenses. Selected holograms from this library
are combined to engineer new axial characteristics not achievable in FINCH. A
two-step reconstruction, involving numerical back-propagation and deconvolution
with a point spread hologram, is implemented. Experiments with multiplane
objects having large axial separations confirm that PEAR-FINCH achieves a
substantially extended DOF compared with direct imaging and FINCH. PEAR-FINCH
will be promising for applications in biomedical imaging, holography, and
fluorescence microscopy.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [47] [Multi-patch isogeometric neural solver for partial differential equations on computer-aided design domains](https://arxiv.org/abs/2509.25450)
*Moritz von Tresckow,Ion Gabriel Ion,Dimitrios Loukrezis*

Main category: cs.CE

TL;DR: A computational framework combining physics-informed neural networks with multi-patch isogeometric analysis to solve PDEs on complex CAD geometries, demonstrating effectiveness on 2D magnetostatics and 3D nonlinear mechanics problems.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient neural solver for partial differential equations on complex computer-aided design geometries, bridging the gap between CAD models and computational analysis.

Method: Uses patch-local neural networks on isogeometric analysis reference domains with custom output layers for Dirichlet boundary conditions, interface networks for patch conformity, and variational training by minimizing energy functionals.

Result: Excellent agreement with high-fidelity finite element reference solutions for both 2D magnetostatics (quadrupole magnet) and 3D nonlinear solid/contact mechanics (mechanical holder) test cases.

Conclusion: The method shows strong potential for tackling complex engineering problems directly from CAD models, providing accurate solutions comparable to traditional high-fidelity finite element solvers.

Abstract: This work develops a computational framework that combines physics-informed
neural networks with multi-patch isogeometric analysis to solve partial
differential equations on complex computer-aided design geometries. The method
utilizes patch-local neural networks that operate on the reference domain of
isogeometric analysis. A custom output layer enables the strong imposition of
Dirichlet boundary conditions. Solution conformity across interfaces between
non-uniform rational B-spline patches is enforced using dedicated interface
neural networks. Training is performed using the variational framework by
minimizing the energy functional derived after the weak form of the partial
differential equation. The effectiveness of the suggested method is
demonstrated on two highly non-trivial and practically relevant use-cases,
namely, a 2D magnetostatics model of a quadrupole magnet and a 3D nonlinear
solid and contact mechanics model of a mechanical holder. The results show
excellent agreement to reference solutions obtained with high-fidelity finite
element solvers, thus highlighting the potential of the suggested neural solver
to tackle complex engineering problems given the corresponding computer-aided
design models.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [48] [Experimental demonstration of boson sampling as a hardware accelerator for monte carlo integration](https://arxiv.org/abs/2509.25404)
*Malaquias Correa Anguita,Teun Roelink,Sara Marzban,Wim Briels,Claudia Filippi,Jelmer Renema*

Main category: quant-ph

TL;DR: Experimental demonstration of boson sampling as hardware accelerator for Monte Carlo integration using hybrid quantum-classical approach with potential quantum advantage.


<details>
  <summary>Details</summary>
Motivation: To establish concrete use cases for near-term photonic quantum devices and demonstrate practical quantum advantage in scientific computing through efficient sampling from hard-to-simulate probability distributions.

Method: Hybrid quantum-classical computation using importance sampling to factorize integrands into quantum-sampled distributions and classically-evaluated functions, implemented on programmable photonic platform.

Result: Experimental results consistent with theoretical predictions and numerical simulations for computing first-order energy correction of three-boson system, with deviations explained by photon distinguishability, discretization, and unitary imperfections.

Conclusion: This work establishes a viable path toward practical quantum advantage in scientific computing and provides a concrete use case for near-term photonic quantum devices.

Abstract: We present an experimental demonstration of boson sampling as a hardware
accelerator for Monte Carlo integration. Our approach leverages importance
sampling to factorize an integrand into a distribution that can be sampled
using quantum hardware and a function that can be evaluated classically,
enabling hybrid quantum-classical computation. We argue that for certain
classes of integrals, this method offers a quantum advantage by efficiently
sampling from probability distributions that are hard to simulate classically.
We also identify structural criteria that must be satisfied to preserve
computational hardness, notably the sensitivity of the classical
post-processing function to high-order quantum correlations. To validate our
protocol, we implement a proof-of-principle experiment on a programmable
photonic platform to compute the first-order energy correction of a three-boson
system in a harmonic trap under an Efimov-inspired three-body perturbation. The
experimental results are consistent with theoretical predictions and numerical
simulations, with deviations explained by photon distinguishability,
discretization, and unitary imperfections. Additionally, we provide an error
budget quantifying the impact of these same sources of noise. Our work
establishes a concrete use case for near-term photonic quantum devices and
highlights a viable path toward practical quantum advantage in scientific
computing.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [49] [Aspects of holographic entanglement using physics-informed-neural-networks](https://arxiv.org/abs/2509.25311)
*Anirudh Deb,Yaman Sanghavi*

Main category: hep-th

TL;DR: PINNs are used to compute holographic entanglement entropy and entanglement wedge cross section for arbitrary subregion shapes in asymptotically AdS metrics.


<details>
  <summary>Details</summary>
Motivation: To develop a method that can compute holographic entanglement quantities for arbitrary subregion shapes where traditional approaches face difficulties.

Method: Implementation of Physics-Informed Neural Networks (PINNs) to solve the holographic entanglement entropy and entanglement wedge cross section problems.

Result: Successfully tested against known results and demonstrated utility in cases where conventional computations are not straightforward.

Conclusion: PINNs provide an effective computational approach for holographic entanglement analysis in complex scenarios with arbitrary subregion geometries.

Abstract: We implement physics-informed-neural-networks (PINNs) to compute holographic
entanglement entropy and entanglement wedge cross section. This technique
allows us to compute these quantities for arbitrary shapes of the subregions in
any asymptotically AdS metric. We test our computations against some known
results and further demonstrate the utility of PINNs in examples, where it is
not straightforward to perform such computations.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [50] [Supergranulation and poleward migration of the magnetic field at high latitudes of the Sun](https://arxiv.org/abs/2509.26586)
*L. P. Chitta,D. Calchetti,J. Hirzberger,G. Valori,E. R. Priest,S. K. Solanki,D. Berghmans,C. Verbeeck,E. Kraaikamp,K. Albert,T. Appourchaux,F. J. Bailén,L. R. Bellot Rubio,J. Blanco Rodríguez,A. Feller,A. Gandorfer,L. Gizon,A. Lagg,A. Moreno Vacas,D. Orozco Suárez,J. Schou,U. Schühle,J. Sinjan,H. Strecker,R. Volkmer,J. Woch,X. Li,T. Oba,A. Ulyanov*

Main category: astro-ph.SR

TL;DR: First out-of-ecliptic observations of solar south pole reveal supergranular cells (20-40 Mm) and poleward magnetic network migration at 10-20 m/s above 60° latitude.


<details>
  <summary>Details</summary>
Motivation: Magnetoconvection at solar surface governs upper atmosphere dynamics but is poorly understood near solar poles, crucial for understanding solar cycle and heliospheric magnetic field.

Method: Remote-sensing observations from Solar Orbiter spacecraft high-latitude campaign, analyzing spatial and temporal evolution of supergranular convective cells over 8 days starting March 16, 2025.

Result: Supergranular cells have 20-40 Mm spatial scales; magnetic network migrates poleward at 10-20 m/s above 60° latitude depending on tracked structures.

Conclusion: Results provide insights into polar magnetic field buildup, which is central to understanding solar cycle dynamics and heliospheric magnetic field.

Abstract: Magnetoconvection at the solar surface governs the dynamics in the upper
solar atmosphere and sustains the heliosphere. Properties of this fundamental
process are poorly described near the solar poles. Here we report the first
out-of-ecliptic remote-sensing observations of the south pole of the Sun from a
high-latitude campaign of the Solar Orbiter spacecraft which reveal spatial and
temporal evolution of supergranular convective cells. The supergranular cells
have spatial scales of 20--40\,Mm. From eight days of observations starting on
2025 March 16, our analysis shows that the magnetic network migrates poleward,
on average, at high latitudes (above 60\textdegree), with speeds in the range
of 10--20\,m\,s$^{-1}$, depending on the structures being tracked. These
results shed light on the buildup of the polar magnetic field that is central
to our understanding of the solar cycle and the heliospheric magnetic field.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [51] [Spatial correlations in SIS processes on regular random graphs](https://arxiv.org/abs/2509.25386)
*Alexander Leibenzon,Samuel W. S. Johnson,Ruth E. Baker,Michael Assaf*

Main category: cond-mat.stat-mech

TL;DR: The paper develops a mathematical framework that accounts for spatial correlations in SIS disease transmission on networks, improving upon mean-field approximations by incorporating spatial dependencies between connected individuals.


<details>
  <summary>Details</summary>
Motivation: Mean-field approximations fail to capture spatial correlations between neighboring nodes in network-based SIS models, leading to inaccurate infection predictions. There's a need for frameworks that balance accuracy with analytic tractability.

Method: Used existing corrections to mean-field theory on regular lattices to construct a general framework for regular random graphs. Derived and simulated ODEs for spatial correlation functions at different geodesic distances.

Result: The framework accurately predicts global infection density over time, showing good agreement with numerical simulations. It characterizes how network structural randomness affects disease dynamics.

Conclusion: This work significantly advances mean-field corrections for SIS processes and provides detailed understanding of how network structure randomness influences infectious disease trajectories.

Abstract: In network-based SIS models of infectious disease transmission, infection can
only occur between directly connected individuals. This constraint naturally
gives rise to spatial correlations between the states of neighboring nodes, as
the infection status of connected individuals becomes interdependent. Although
mean-field approximations are commonly invoked to simplify disease forecasting
on networks, they fail to account for these correlations by assuming that
infectious individuals are well-mixed within a population, leading to
inaccurate predictions of infection numbers over time. As such, the development
of mathematical frameworks that account for spatially correlated infections is
of great interest, as they offer a compromise between accurate disease
forecasting and analytic tractability. Here, we use existing corrections to
mean-field theory on the regular lattice to construct a more general framework
for equivalent corrections on regular random graph topologies. We derive and
simulate a system of ordinary differential equations for the time evolution of
the spatial correlation function at various geodesic distances on random
networks, and use solutions to this hierarchy of ordinary differential
equations to predict the global infection density as a function of time,
finding good agreement with corresponding numerical simulations. Our results
constitute a substantial development on existing corrections to mean-field
theory for infectious individuals in SIS processes and provide an in-depth
characterization of how structural randomness in networks affects the dynamical
trajectories of infectious diseases on networks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [52] [When Langevin Monte Carlo Meets Randomization: Non-asymptotic Error Bounds beyond Log-Concavity and Gradient Lipschitzness](https://arxiv.org/abs/2509.25630)
*Xiaojie Wang,Bin Yang*

Main category: stat.ML

TL;DR: The paper analyzes randomized Langevin Monte Carlo (RLMC) for sampling from high-dimensional distributions without log-concavity, proving uniform-in-time error bounds and proposing modified algorithms for non-globally Lipschitz gradients.


<details>
  <summary>Details</summary>
Motivation: Efficient sampling from complex high-dimensional distributions is fundamental in scientific computing, statistics, and machine learning, but existing methods often assume log-concavity or globally Lipschitz gradients.

Method: Revisits randomized Langevin Monte Carlo (RLMC) and proposes modified RLMC algorithms for cases where gradient of potential U is non-globally Lipschitz with superlinear growth.

Result: Proves uniform-in-time error bound of O(√d h) in W₂-distance under gradient Lipschitz condition and log-Sobolev inequality, matching best known bounds under log-concavity. For non-globally Lipschitz cases, establishes non-asymptotic error bounds for modified RLMC algorithms.

Conclusion: The paper provides improved theoretical guarantees for RLMC sampling in non-log-concave settings and introduces new modified algorithms with error bounds for non-globally Lipschitz gradients, advancing the state-of-the-art in high-dimensional sampling.

Abstract: Efficient sampling from complex and high dimensional target distributions
turns out to be a fundamental task in diverse disciplines such as scientific
computing, statistics and machine learning. In this paper, we revisit the
randomized Langevin Monte Carlo (RLMC) for sampling from high dimensional
distributions without log-concavity. Under the gradient Lipschitz condition and
the log-Sobolev inequality, we prove a uniform-in-time error bound in
$\mathcal{W}_2$-distance of order $O(\sqrt{d}h)$ for the RLMC sampling
algorithm, which matches the best one in the literature under the log-concavity
condition. Moreover, when the gradient of the potential $U$ is non-globally
Lipschitz with superlinear growth, modified RLMC algorithms are proposed and
analyzed, with non-asymptotic error bounds established. To the best of our
knowledge, the modified RLMC algorithms and their non-asymptotic error bounds
are new in the non-globally Lipschitz setting.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [53] [Position-Blind Ptychography: Viability of image reconstruction via data-driven variational inference](https://arxiv.org/abs/2509.25269)
*Simon Welker,Lorenz Kuger,Tim Roith,Berthy Feng,Martin Burger,Timo Gerkmann,Henry Chapman*

Main category: eess.IV

TL;DR: The paper introduces position-blind ptychography, a novel blind inverse problem where both the image and unknown scan positions must be jointly recovered from diffraction patterns without position knowledge.


<details>
  <summary>Details</summary>
Motivation: The problem is motivated by single-particle diffractive X-ray imaging, where particles in random orientations are illuminated and diffraction patterns are collected. With focused X-ray beams, measurements become ptychographic but positions relative to particles remain unknown.

Method: The authors use variational inference with modern data-driven image priors in the form of score-based diffusion models to investigate image reconstruction in a simulated 2-D variant of this problem.

Result: With appropriate illumination structure and strong priors, reliable and successful image reconstructions can be achieved even under measurement noise, except in the most difficult imaging scenarios.

Conclusion: Position-blind ptychography is viable for image reconstruction when using strong data-driven priors and proper illumination, demonstrating potential for applications in single-particle X-ray imaging despite the challenging nature of the problem.

Abstract: In this work, we present and investigate the novel blind inverse problem of
position-blind ptychography, i.e., ptychographic phase retrieval without any
knowledge of scan positions, which then must be recovered jointly with the
image. The motivation for this problem comes from single-particle diffractive
X-ray imaging, where particles in random orientations are illuminated and a set
of diffraction patterns is collected. If one uses a highly focused X-ray beam,
the measurements would also become sensitive to the beam positions relative to
each particle and therefore ptychographic, but these positions are also
unknown. We investigate the viability of image reconstruction in a simulated,
simplified 2-D variant of this difficult problem, using variational inference
with modern data-driven image priors in the form of score-based diffusion
models. We find that, with the right illumination structure and a strong prior,
one can achieve reliable and successful image reconstructions even under
measurement noise, in all except the most difficult evaluated imaging scenario.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [54] [The Hadamard parametrix on half-Minkowski with Robin boundary conditions: Fundamental solutions and Hadamard states](https://arxiv.org/abs/2509.26035)
*B. Costeri,C. Dappiaggi,B. A. Juárez-Aubry,R. D. Singh*

Main category: math-ph

TL;DR: Construction of fundamental solutions and Hadamard states for Klein-Gordon field in half-Minkowski spacetime with Robin boundary conditions in d≥2 dimensions, including Green operators representation and local/global Hadamard condition equivalence.


<details>
  <summary>Details</summary>
Motivation: To address the problem of constructing fundamental solutions and defining Hadamard states for Klein-Gordon fields in half-Minkowski spacetime with Robin boundary conditions, which is important for quantum field theory in bounded domains.

Method: Used generalization of Robin-to-Dirichlet map to obtain Green operator representations via convolution with inverse map kernel, developed local Hadamard parametrix capturing boundary reflections, and proved equivalence between local and global Hadamard conditions.

Result: Successfully constructed fundamental solutions with proven uniqueness and support properties, obtained local Hadamard parametrix representation that captures boundary reflections, and established equivalence between local and global Hadamard conditions (Radzikowski-like theorem).

Conclusion: The paper provides a complete framework for fundamental solutions and Hadamard states in half-Minkowski spacetime with Robin boundary conditions, with rigorous mathematical foundations for both local and global properties.

Abstract: We address the problem of constructing fundamental solutions and Hadamard
states for a Klein-Gordon field in half-Minkowski spacetime with Robin boundary
conditions in $d \geq 2$ spacetime dimensions. First, using a generalisation of
the Robin-to-Dirichlet map exploited by Bondurant and Fulling [J. Phys. A:
Math. Theor. {\bf 38} 7 (2005)] in dimension $2$, we obtain a representation
for the advanced and retarded Green operators in terms of a convolution with
the kernel of the inverse Robin-to-Dirichlet map. This allows us to prove the
uniqueness and support properties of the Green operators. Second, we obtain a
local representation for the Hadamard parametrix that provides the correct
local definition of Hadamard states in $d \geq 2$ dimensions, capturing
`reflected' singularities from the spacetime boundary. We show that our
fundamental solutions abide by this local parametrix representation. Finally,
we prove the equivalence of our local Hadamard condition and the global
Hadamard condition with a wave-front set described in terms of generalized
broken bi-characteristics, obtaining a Radzikowski-like theorem in
half-Minkowski spacetime.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [55] [Automated Code Development for PDE Solvers Using Large Language Models](https://arxiv.org/abs/2509.25194)
*Haoyang Wu,Xinxin Zhang,Lailai Zhu*

Main category: cs.SE

TL;DR: LLM-PDEveloper is a zero-shot multi-agent LLM framework that automates code development for PDE libraries by translating mathematical descriptions into source code, enabling continuous library expansion.


<details>
  <summary>Details</summary>
Motivation: To leverage LLMs' cross-domain knowledge and reasoning abilities for automating software development in numerical PDE libraries, specifically targeting secondary developers rather than just end users.

Method: A zero-shot, multi-agent LLM framework that translates mathematical and algorithmic descriptions directly into source code, generating new solvers/modules and adapting existing ones through an end-to-end math-to-code approach.

Result: Demonstrated on three tasks: building solvers for new PDEs, implementing new boundary conditions, and modifying existing solvers with additional terms, achieving moderate success rates. Identified and fixed syntactic errors, and analyzed mechanisms behind semantic errors.

Conclusion: LLM-PDEveloper enables a self-augmenting pipeline for continuous library expansion and provides insights for future research on semantic error mechanisms in automated code generation.

Abstract: Foundation models -- large language models (LLMs) in particular -- have
become ubiquitous, shaping daily life and driving breakthroughs across science,
engineering, and technology. Harnessing their broad cross-domain knowledge,
text-processing, and reasoning abilities for software development, e.g.,
numerical libraries for solving partial differential equations (PDEs), is
therefore attracting growing interest. Yet existing studies mainly automate
case setup and execution for end users. We introduce LLM-PDEveloper, a
zero-shot, multi-agent LLM framework that automates code development for PDE
libraries, specifically targeting secondary developers. By translating
mathematical and algorithmic descriptions directly into source code,
LLM-PDEveloper generates new solvers/modules and adapts existing ones. This
end-to-end math-to-code approach enables a self-augmenting pipeline that
continuously expands the codebase of a library, extends its capacities, and
broadens its scope. We demonstrate LLM-PDEveloper on three tasks: 1) build a
solver for a new PDE, 2) implement new BCs for a given PDE, and 3) modify an
existing solver to incorporate additional terms, achieving moderate success
rates. Failures due to syntactic errors made by LLMs are analyzed and we
propose effective fixes. We also identify the mechanisms underlying certain
semantic errors, guiding future research.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [56] [A Fourier/Modal-Spectral-Element Method for the Simulation of High-Reynolds Number Incompressible Stratified Flows in Domains with a Single Non-Periodic Direction](https://arxiv.org/abs/2509.20833)
*Nidia Reyes-Gil,Greg Thomsen,Kristopher Rowe,Peter Diamessis*

Main category: physics.flu-dyn

TL;DR: A high-order accurate Navier-Stokes solver for simulating high-Reynolds-number stratified flows, using Fourier pseudo-spectral method horizontally and modal spectral element discretization vertically with implicit-explicit time stepping.


<details>
  <summary>Details</summary>
Motivation: To address numerical and computational challenges in simulating high-Reynolds-number stratified flows, particularly for reproducing stratified turbulent dynamics observed in oceanic and atmospheric flows like thin high-shear regions, layered turbulence, and internal wave radiation.

Method: Uses Fourier pseudo-spectral method in horizontal direction and modal spectral element discretization in vertical direction. Implements implicit-explicit time discretization solving one-dimensional Helmholtz problems with static condensation and modal boundary-adapted basis functions, resulting in efficient tridiagonal system solutions.

Result: The solver demonstrates robustness through benchmark studies including 2D and 3D problems, successfully simulating turbulent stratified wake generated by a sphere in linear stratification.

Conclusion: The proposed numerical model effectively handles high-Reynolds-number stratified flow simulations, providing an efficient computational approach for studying complex stratified turbulent phenomena relevant to oceanic and atmospheric applications.

Abstract: We present the components of a high-order accurate Navier-Stokes solver
designed to simulate high-Reynolds-number stratified flows. The proposed
numerical model addresses some of the numerical and computational challenges
that high-Reynolds-number simulations pose, facilitating the reproduction of
stratified turbulent fluid dynamics typically observed in oceanic and
atmospheric flows, namely the development of thin regions of high vertical
shear, strongly layered turbulence at high Reynolds numbers and internal wave
radiation. This Navier-Stokes solver utilizes a Fourier pseudo-spectral method
in the horizontal direction and a modal spectral element discretization in the
vertical. We adopt an implicit-explicit time discretization scheme that
involves solving several one-dimensional Helmholtz problems at each time step.
Static condensation and modal boundary-adapted basis functions result in an
inexpensive algorithm based on solving many small tridiagonal systems. A series
of benchmark studies is presented to demonstrate the robustness of the flow
solver. These include two-dimensional and three-dimensional problems,
concluding with a turbulent stratified wake generated by a sphere in linear
stratification.

</details>


### [57] [Cross-Model Verification of Wall-Bounded Flows using Finite-JAX](https://arxiv.org/abs/2509.25569)
*Arturo Rodriguez,Avinash Potluri,Aryan Singh,Vyom Kumar,Kate Reza,Francisco O. Aguirre Ortega,Vineeth Vijaya Kumar,Noah L. Estrada*

Main category: physics.flu-dyn

TL;DR: Numerical simulation of channel flow using a differentiable finite-difference solver (Finite-JAX) and analytical Hagen-Poiseuille solution, with cross-model verification showing high accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of wall-bounded flows is crucial for advancing theoretical understanding and computational methods in fluid mechanics.

Method: Used a high-performance differentiable finite-difference solver (Finite-JAX) applied to incompressible Navier-Stokes equations with boundary conditions, and compared results with analytical Hagen-Poiseuille solution through systematic cross-model verification.

Result: Cross-model verification demonstrated strong agreement between numerical and analytical solutions, quantifying verification accuracy errors and confirming simulation fidelity.

Conclusion: Cross-model verification strengthens confidence in simulation results and provides a pathway for integrating differentiable solvers with established CFD platforms for future fluid flow research.

Abstract: Accurate prediction of wall-bounded flows remains central to advancing both
theoretical understanding and computational methods in fluid mechanics. In this
study, we perform a numerical simulation of channel flow using a complementary
approach: a high-performance, differentiable finite-difference solver developed
in JAX (Finite-JAX) and an analytical solution derived from the Navier-Stokes
Equations, also referred to as the Hagen-Poiseuille equation. The solver is
applied to the incompressible Navier-Stokes equations, along with appropriate
boundary conditions, to capture canonical flow features such as velocity
profiles and pressure gradients. Cross-model verification is conducted by
systematically comparing numerical results between Finite-JAX and the
analytical solution, with a focus on velocity distributions. In addition,
numerical results are benchmarked against analytical solutions for laminar
regimes, allowing for the direct quantification of verification accuracy
errors. Our findings demonstrate that cross-model verification not only
strengthens confidence in simulation fidelity but also provides a pathway for
integrating differentiable solvers with established computational fluid
dynamics platforms, paving the way for future fluid flow research.

</details>


### [58] [Reservoir computing based predictive reduced order model for steel grade intermixing in an industrial continuous casting tundish](https://arxiv.org/abs/2509.26293)
*Harshith Gowrachari,Mattia Giuseppe Barra,Giovanni Stabile,Gianluca Bazzaro,Gianluigi Rozza*

Main category: physics.flu-dyn

TL;DR: A reduced order model combining proper orthogonal decomposition and reservoir computing is developed to efficiently predict grade intermixing time during ladle changeover in continuous casting, enabling real-time monitoring with high accuracy and low computational cost.


<details>
  <summary>Details</summary>
Motivation: High-fidelity simulations of grade intermixing during ladle changeover are computationally expensive and impractical for real-time monitoring in industrial continuous casting operations.

Method: Three-dimensional transient two-phase turbulent flow simulations combined with a reduced order modeling approach using proper orthogonal decomposition (POD) and reservoir computing (RC).

Result: The ROM demonstrates excellent predictive accuracy with limited training data while requiring significantly less computational resources and training time compared to full-order simulations.

Conclusion: The proposed methodology shows potential as a fast, reliable tool for real-time process monitoring and optimization in industrial continuous casting operations.

Abstract: Continuous casting is a widely adopted process in the steel industry, where
maintaining high steel quality is paramount. Efficient prediction of grade
intermixing during ladle changeover operations is critical for maintaining
steel quality and minimizing material losses in the continuous casting process.
Among various factors influencing grade intermixing, operating parameters play
a significant role, in addition to tundish geometry and flow control devices.
In this study, three-dimensional, transient, two-phase turbulent flow
simulations are conducted to investigate the ladle changeover operation. During
this process, the molten steel level in the tundish typically varies over time,
significantly affecting the grade intermixing phenomena. The influence of ladle
change time on intermixing time has been presented. However, high-fidelity
full-order simulations of such complex transient phenomena are computationally
expensive and are impractical for real-time monitoring or design-space
exploration in industrial-scale applications. To address this issue, a reduced
order modelling approach based on proper orthogonal decomposition (POD) and
reservoir computing (RC) is employed to efficiently predict intermixing time.
The proposed reduced order model (ROM) demonstrates excellent predictive
accuracy using limited training data while requiring significantly less
computational resources and training time. The results demonstrate the
potential of the proposed methodology as a fast, reliable tool for real-time
process monitoring and optimization in industrial continuous casting
operations.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [59] [Error bounds for perspective cones of a class of nonnegative Legendre functions](https://arxiv.org/abs/2509.26289)
*Xiaozhou Wang,Bruno F. Lourenço,Ting Kei Pong*

Main category: math.OC

TL;DR: This paper explains why error bounds in conic optimization often have Hölder exponent 1/2 by analyzing perspective cones from Legendre functions, showing this property is generic.


<details>
  <summary>Details</summary>
Motivation: To explain the prevalence of the 1/2 exponent in Hölderian error bounds for conic optimization problems, which is crucial for analyzing algorithm convergence rates.

Method: Uses facial reduction technique and computation of one-step facial residual functions (1-FRFs) for perspective cones constructed from nonnegative Legendre functions on ℝ.

Result: Under appropriate assumptions, 1-FRFs are Hölderian with exponent 1/2 almost everywhere with respect to 2D Hausdorff measure, establishing uniform Hölderian error bounds with exponent 1/2 as a generic property.

Conclusion: The 1/2 exponent in error bounds is a generic phenomenon for feasibility problems involving perspective cones from Legendre functions, explaining its frequent appearance in conic optimization.

Abstract: Error bounds play a central role in the study of conic optimization problems,
including the analysis of convergence rates for numerous algorithms. Curiously,
those error bounds are often H\"olderian with exponent 1/2. In this paper, we
try to explain the prevalence of the 1/2 exponent by investigating generic
properties of error bounds for conic feasibility problems where the underlying
cone is a perspective cone constructed from a nonnegative Legendre function on
$\mathbb{R}$. Our analysis relies on the facial reduction technique and the
computation of one-step facial residual functions (1-FRFs). Specifically, under
appropriate assumptions on the Legendre function, we show that 1-FRFs can be
taken to be H\"olderian of exponent 1/2 almost everywhere with respect to the
two-dimensional Hausdorff measure. This enables us to further establish that
having a uniform H\"olderian error bound with exponent 1/2 is a generic
property for a class of feasibility problems involving these cones.

</details>


### [60] [Hamilton--Jacobi--Bellman equation for optimal control of stochastic Wasserstein--Hamiltonian system on graphs](https://arxiv.org/abs/2509.25965)
*Jianbo Cui,Tonghe Dang*

Main category: math.OC

TL;DR: Establishes existence and uniqueness of viscosity solutions for Hamilton-Jacobi-Bellman equations in optimal control of stochastic Wasserstein-Hamiltonian systems on graphs, combining Wasserstein and Euclidean geometries.


<details>
  <summary>Details</summary>
Motivation: Address optimal control problems for Hamiltonian dynamics on graphs with applications in mechanics and quantum field theory, particularly for systems with graph-based structures.

Method: Introduces energy-truncation technique within doubling of variables framework to handle interaction between Wasserstein space on graphs and unbounded Euclidean space, dealing with nonlinear geometric structure and logarithmic potential.

Result: Demonstrates well-posedness of HJB equations for optimal control of both stochastic Schrödinger equation with polynomial nonlinearity and stochastic logarithmic Schrödinger equation on graphs.

Conclusion: First development of HJB equations for optimal control of stochastic Wasserstein-Hamiltonian systems on graphs, providing theoretical foundation for controlling such complex systems.

Abstract: Stochastic optimal control problems for Hamiltonian dynamics on graphs have
wide-ranging applications in mechanics and quantum field theory, particularly
in systems with graph-based structures. In this paper, we establish the
existence and uniqueness of viscosity solutions for a new class of
Hamilton--Jacobi--Bellman (HJB) equations arising from the optimal control of
stochastic Wasserstein--Hamiltonian systems (SWHSs) on graphs. One distinctive
feature of these HJB equations is the simultaneous involvement of the
Wasserstein geometry on the Wasserstein space over graphs and the Euclidean
geometry in physical space. The nonlinear geometric structure, along with the
logarithmic potential induced by the graph-based state equation, adds further
complexity to the analysis. To address these challenges, we introduce an
energy-truncation technique within the doubling of variables framework,
specifically designed to handle the interaction between the interiorly defined
Wasserstein space on graphs and the unbounded Euclidean space. In particular,
our findings demonstrate the well-posedness of HJB equations related to optimal
control problems for both stochastic Schr\"odinger equation with polynomial
nonlinearity and stochastic logarithmic Schr\"odinger equation on graphs. To
the best of our knowledge, this work is the first to develop HJB equations for
the optimal control of SWHSs on graphs.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [61] [BABY 1L: First Tritium Breeding Campaign Results](https://arxiv.org/abs/2509.26174)
*Rémi Delaporte-Mathurin,Nikola Goles,Collin Dunn,Emily Edwards,Sara Ferry,Ross MacDonald,Ethan Peterson,Davide Pettinari,Stefano Segantin,Weiyue Zhou,Kevin B. Woller*

Main category: physics.ins-det

TL;DR: BABY 1L experiment demonstrates improved tritium breeding in molten salt systems with 10x larger volume, achieving six-fold TBR improvement over previous experiments and identifying diffusion-limited transport as dominant tritium release mechanism.


<details>
  <summary>Details</summary>
Motivation: Achieving tritium self-sufficiency is critical for future fusion power plants, requiring experimental validation of tritium breeding and release in molten salt breeder systems.

Method: Used sealed-tube DT neutron generators with 10x larger breeder volume, improved thermal/gas systems, enhanced neutron diagnostics including proton recoil telescope. Tritium collected via water bubblers and measured by liquid scintillation counting. Compared experimental TBRs with OpenMC neutronics simulations.

Result: Experimental TBRs showed very good agreement with simulations, achieving six-fold improvement over previous 100mL experiments. Identified diffusion-limited transport as dominant tritium release mechanism at 630-750°C. Hydrogen in carrier gas significantly accelerated tritium release via isotopic exchange.

Conclusion: Results provide critical insights for future liquid breeder system design and demonstrate BABY platform maturity as a testbed for tritium breeding studies, with all analysis conducted through open-source libra-toolbox.

Abstract: Achieving tritium self-sufficiency is a critical challenge for future fusion
power plants. The BABY 1L experiment, part of the LIBRA project at MIT, aims to
benchmark tritium breeding and release in molten salt breeder systems under
deuterium-tritium (DT) neutron irradiation. Building on the initial
\SI{100}{mL} campaign, BABY 1L introduces a tenfold increase in breeder volume,
improved thermal and gas handling systems, and enhanced neutron diagnostics,
including a proton recoil telescope. We report on results from four irradiation
experiments using sealed-tube DT neutron generators, with tritium collected by
water bubblers measured via liquid scintillation counting. Experimentally
determined Tritium Breeding Ratios (TBRs) were compared to OpenMC neutronics
simulations, showing very good agreement. The measured TBR values demonstrate a
six-fold improvement over the \SI{100}{mL} experiments, largely attributed to
the increased solid angle and improved measurement fidelity. We also
investigate tritium release dynamics and identify diffusion-limited transport
as the dominant regime in the salt volume in the temperature range 630-750
\si{\celsius}. Additionally, we observe that the introduction of hydrogen in
the helium carrier gas significantly accelerates tritium release, consistent
with an isotopic exchange mechanism. All analysis is conducted through the
open-source \texttt{libra-toolbox} \cite{libra-toolbox}, which streamlines
simulation, data processing, and validation across experimental campaigns.
These results provide critical insights into the design and operation of future
liquid breeder systems and demonstrate the maturity of the BABY platform as a
testbed for tritium breeding studies.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [62] [Weak Martingale Solutions of the Stochastic Schrödinger-Poisson-Landau-Lifshitz-Gilbert System](https://arxiv.org/abs/2509.25993)
*Yurong Wei,Huaqiao Wang*

Main category: math.PR

TL;DR: Existence of weak martingale solutions for 3D stochastic SPLLG system with continuous and jump noise, extending previous deterministic results to stochastic coupling case.


<details>
  <summary>Details</summary>
Motivation: Study spin transfer torque mechanism through spin-magnetization coupling in stochastic SPLLG system with multiplicative noise containing both continuous and jump components.

Method: Penalized functional technique, Faedo-Galerkin approximation, stochastic compactness method, three-layer approximation, new energy estimates using equation structure and martingale properties.

Result: Established existence of weak martingale solutions for the strongly coupled and nonlinear stochastic SPLLG system despite difficulties with energy estimates and test function non-negativity.

Conclusion: Successfully extended deterministic results by Brzeźniak & Manna and Chai et al. to both stochastic and coupling cases, overcoming challenges from strong nonlinearity and coupling effects.

Abstract: The Schr\"{o}dinger-Poisson-Landau-Lifshitz-Gilbert (SPLLG) system can
characterize the spin transfer torque mechanism transferring the spin angular
momentum to the magnetization dynamics through spin-magnetization coupling. We
study the three-dimensional stochastic SPLLG system driven by a multiplicative
stochastic force containing a continuous noise and a small jump noise. We
establish the existence of weak martingale solutions based on the penalized
functional technique, the Faedo-Galerkin approximation, stochastic compactness
method, and a careful identification of the limit. Due to the strong coupling
and strong nonlinearity caused by the SPLLG system and stochastic effects, some
crucial difficulties have been encountered in obtaining energy estimates and
avoiding non-negativity of the test function. We mainly utilize the structure
of equations and the property of martingales developing the new energy
estimates, and apply the three-layer approximation to overcome these
difficulties. In particular, we extend the results by Z. Brze\'{z}niak and U.
Manna (Comm. Math. Phys., 2019) and by L.H. Chai, C.J. Garc\'{\i}a-Cervera and
X. Yang (Arch. Ration. Mech. Anal., 2018) to both the stochastic case and the
coupling case.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [63] [All-Optical Generation of Dense, Multi-GeV, Longitudinally-Polarized Positron Beams](https://arxiv.org/abs/2509.25870)
*Rui-Qi Qin,Peng-Pei Xie,Yan-Fei Li,Xian-Zhang Wu,Zheng-Yang Zuo,Bing-Jun Li,Jun Liu,Liang-Liang Ji,Yu-Tong Li*

Main category: physics.acc-ph

TL;DR: An all-optical scheme for producing high-yield, longitudinally polarized positron beams using laser-driven QED cascades that simultaneously generates, accelerates, and spin-polarizes positrons to multi-GeV energies.


<details>
  <summary>Details</summary>
Motivation: Current laser-driven positron beam schemes typically yield only transverse polarization, require pre-polarized electron beams, and struggle to efficiently accelerate positrons to high energies, creating a need for more compact and efficient solutions.

Method: Head-on collision between an ultraintense circularly polarized laser pulse and counterpropagating unpolarized electron beam drives a QED cascade where positrons are generated via nonlinear Breit-Wheeler process, then instantly captured and accelerated via direct laser acceleration while their spins are rotated to longitudinal alignment by field dynamics.

Result: Monte-Carlo simulations confirm simultaneous achievement of high positron yield (~20 e+/e-), high average longitudinal polarization (~50%), and GeV-scale energies (up to ~9 GeV).

Conclusion: This all-optical source provides a compact and efficient solution for collider physics and fundamental high-energy experiments, feasible at upcoming ultraintense laser facilities.

Abstract: The production of high-yield, longitudinally polarized positron beams
represents an outstanding challenge in advanced accelerator science.
Laser-driven schemes offer a compact alternative but typically yield only
transverse polarization, or require pre-polarized electron beams, and struggle
to efficiently accelerate positrons to high energies. Here, we introduce an
all-optical scheme that overcomes these limitations by integrating positron
generation, acceleration, and spin manipulation in a unified framework. Through
a head-on collision between an ultraintense, circularly polarized laser pulse
and a counterpropagating unpolarized electron beam, we drive a robust QED
cascade. The nonlinear Breit-Wheeler process within the cascade produces
positrons that are born directly within the strong laser field. Crucially,
these positrons are instantaneously captured and accelerated to multi-GeV
energies (up to $\sim$9 GeV) via a direct laser acceleration mechanism, while
their spins are simultaneously rotated to longitudinal alignment by the field
dynamics. Our Monte-Carlo simulations confirm the simultaneous achievement of a
high positron yield ($\sim$20 $e^+/e^-$), a high average longitudinal
polarization ($\sim$50\%), and GeV-scale energies. This all-optical source,
feasible at upcoming ultraintense laser facilities, presents a compact and
efficient solution for applications in collider physics and fundamental
high-energy experiments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [64] [A Physics-Guided Probabilistic Surrogate Modeling Framework for Digital Twins of Underwater Radiated Noise](https://arxiv.org/abs/2509.25730)
*Indu Kant Deo,Akash Venkateshwaran,Rajeev K. Jaiman*

Main category: cs.LG

TL;DR: A physics-guided probabilistic framework for predicting 3D underwater transmission loss using machine learning, applied to ship noise mitigation in the Salish Sea.


<details>
  <summary>Details</summary>
Motivation: Ship traffic is increasing underwater radiated noise in coastal waters, creating need for real-time digital twins of ocean acoustics for operational noise mitigation.

Method: Combines physics-based mean functions with deep learning: learnable physics-informed mean, convolutional encoder for bathymetry, neural encoder for coordinates, and residual SVGP layer for uncertainty.

Result: Developed probabilistic digital twin that facilitates sound-exposure bounds and worst-case scenarios, demonstrated application to ship speed optimization for minimizing acoustic impacts on marine mammals.

Conclusion: The framework advances uncertainty-aware digital twins for ocean acoustics and shows how physics-guided machine learning can support sustainable maritime operations.

Abstract: Ship traffic is an increasing source of underwater radiated noise in coastal
waters, motivating real-time digital twins of ocean acoustics for operational
noise mitigation. We present a physics-guided probabilistic framework to
predict three-dimensional transmission loss in realistic ocean environments. As
a case study, we consider the Salish Sea along shipping routes from the Pacific
Ocean to the Port of Vancouver. A dataset of over 30 million source-receiver
pairs was generated with a Gaussian beam solver across seasonal sound speed
profiles and one-third-octave frequency bands spanning 12.5 Hz to 8 kHz. We
first assess sparse variational Gaussian processes (SVGP) and then incorporate
physics-based mean functions combining spherical spreading with
frequency-dependent absorption. To capture nonlinear effects, we examine deep
sigma-point processes and stochastic variational deep kernel learning. The
final framework integrates four components: (i) a learnable physics-informed
mean that represents dominant propagation trends, (ii) a convolutional encoder
for bathymetry along the source-receiver track, (iii) a neural encoder for
source, receiver, and frequency coordinates, and (iv) a residual SVGP layer
that provides calibrated predictive uncertainty. This probabilistic digital
twin facilitates the construction of sound-exposure bounds and worst-case
scenarios for received levels. We further demonstrate the application of the
framework to ship speed optimization, where predicted transmission loss
combined with near-field source models provides sound exposure level estimates
for minimizing acoustic impacts on marine mammals. The proposed framework
advances uncertainty-aware digital twins for ocean acoustics and illustrates
how physics-guided machine learning can support sustainable maritime
operations.

</details>


### [65] [Leveraging AI modelling for FDS with Simvue: monitor and optimise for more sustainable simulations](https://arxiv.org/abs/2509.26139)
*James Panayis,Matt Field,Vignesh Gopakumar,Andrew Lahiff,Kristian Zarebski,Aby Abraham,Jonathan L. Hodges*

Main category: cs.LG

TL;DR: A multi-pronged approach using ML surrogate models and guided optimization to dramatically improve fire simulation efficiency, with a 10x reduction in simulations needed and orders of magnitude faster predictions.


<details>
  <summary>Details</summary>
Motivation: High demand for fire simulations in both scale and quantity requires improvements in time and energy efficiency for meeting these computational demands.

Method: Uses custom machine learning surrogate model for fast heat propagation prediction, guided optimization with lightweight models to select simulations, and Simvue framework for simulation management and data reuse.

Result: ML surrogate predicts heat dynamics orders of magnitude faster than CFD software; guided optimization reduces required simulations by 10x for locating most dangerous fire locations based on smoke impact on visibility.

Conclusion: The presented multi-pronged approach and Simvue framework enable significant efficiency gains in fire simulations through ML acceleration, intelligent simulation selection, and better data management.

Abstract: There is high demand on fire simulations, in both scale and quantity. We
present a multi-pronged approach to improving the time and energy required to
meet these demands. We show the ability of a custom machine learning surrogate
model to predict the dynamics of heat propagation orders of magnitude faster
than state-of-the-art CFD software for this application. We also demonstrate
how a guided optimisation procedure can decrease the number of simulations
required to meet an objective; using lightweight models to decide which
simulations to run, we see a tenfold reduction when locating the most dangerous
location for a fire to occur within a building based on the impact of smoke on
visibility. Finally we present a framework and product, Simvue, through which
we access these tools along with a host of automatic organisational and
tracking features which enables future reuse of data and more savings through
better management of simulations and combating redundancy.

</details>


### [66] [Deep set based operator learning with uncertainty quantification](https://arxiv.org/abs/2509.25646)
*Lei Ma,Ling Guo,Hao Wu,Tao Zhou*

Main category: cs.LG

TL;DR: UQ-SONet is a permutation-invariant operator learning framework with built-in uncertainty quantification that handles sparse and variable sensor locations using set transformer embeddings and conditional variational autoencoders.


<details>
  <summary>Details</summary>
Motivation: Existing operator learning methods like DeepONets have limitations: they require fixed sensor configurations, lack uncertainty quantification mechanisms, and cannot handle sparse measurements or operators with inherent randomness.

Method: Integrates set transformer embedding for handling variable sensor locations and uses conditional variational autoencoder (cVAE) to approximate the conditional distribution of solution operators, minimizing negative ELBO for principled uncertainty estimation.

Result: Numerical experiments on deterministic and stochastic PDEs, including Navier-Stokes equation, demonstrate robustness and effectiveness of the framework in providing uncertainty quantification while maintaining predictive accuracy.

Conclusion: UQ-SONet successfully addresses key limitations of existing operator learning methods by providing built-in uncertainty quantification and handling sparse, variable sensor configurations through a principled probabilistic framework.

Abstract: Learning operators from data is central to scientific machine learning. While
DeepONets are widely used for their ability to handle complex domains, they
require fixed sensor numbers and locations, lack mechanisms for uncertainty
quantification (UQ), and are thus limited in practical applicability. Recent
permutationinvariant extensions, such as the Variable-Input Deep Operator
Network (VIDON), relax these sensor constraints but still rely on sufficiently
dense observations and cannot capture uncertainties arising from incomplete
measurements or from operators with inherent randomness. To address these
challenges, we propose UQ-SONet, a permutation-invariant operator learning
framework with built-in UQ. Our model integrates a set transformer embedding to
handle sparse and variable sensor locations, and employs a conditional
variational autoencoder (cVAE) to approximate the conditional distribution of
the solution operator. By minimizing the negative ELBO, UQ-SONet provides
principled uncertainty estimation while maintaining predictive accuracy.
Numerical experiments on deterministic and stochastic PDEs, including the
Navier-Stokes equation, demonstrate the robustness and effectiveness of the
proposed framework.

</details>


### [67] [PDE Solvers Should Be Local: Fast, Stable Rollouts with Learned Local Stencils](https://arxiv.org/abs/2509.26186)
*Chun-Wun Cheng,Bin Dong,Carola-Bibiane Schönlieb,Angelica I Aviles-Rivero*

Main category: cs.LG

TL;DR: FINO is a finite-difference-inspired neural architecture for solving PDEs that enforces strict locality while maintaining multiscale representational power, achieving better accuracy and computational efficiency than global mixing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing neural operator models for PDEs rely on global mixing mechanisms that oversmooth sharp local dynamics and introduce high computational costs.

Method: FINO replaces fixed finite-difference stencil coefficients with learnable convolutional kernels and uses an explicit, learnable time-stepping scheme. It features a Local Operator Block with differential stencil layer, gating mask, and linear fuse step to construct adaptive derivative-like local features.

Result: FINO achieves up to 44% lower error and around 2x speedups over state-of-the-art operator-learning baselines across six benchmarks and a climate modeling task.

Conclusion: Strict locality with learnable time-stepping provides an accurate and scalable foundation for neural PDE solvers, demonstrating that local approaches can outperform global mixing mechanisms.

Abstract: Neural operator models for solving partial differential equations (PDEs)
often rely on global mixing mechanisms-such as spectral convolutions or
attention-which tend to oversmooth sharp local dynamics and introduce high
computational cost. We present FINO, a finite-difference-inspired neural
architecture that enforces strict locality while retaining multiscale
representational power. FINO replaces fixed finite-difference stencil
coefficients with learnable convolutional kernels and evolves states via an
explicit, learnable time-stepping scheme. A central Local Operator Block
leverage a differential stencil layer, a gating mask, and a linear fuse step to
construct adaptive derivative-like local features that propagate forward in
time. Embedded in an encoder-decoder with a bottleneck, FINO captures
fine-grained local structures while preserving interpretability. We establish
(i) a composition error bound linking one-step approximation error to stable
long-horizon rollouts under a Lipschitz condition, and (ii) a universal
approximation theorem for discrete time-stepped PDE dynamics. (iii) Across six
benchmarks and a climate modelling task, FINO achieves up to 44\% lower error
and up to around 2\times speedups over state-of-the-art operator-learning
baselines, demonstrating that strict locality with learnable time-stepping
yields an accurate and scalable foundation for neural PDE solvers.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [68] [A solution to the mystery of the sub-harmonic series and to the combination tone via a linear mathematical model of the cochlea](https://arxiv.org/abs/2509.26395)
*Ugo Boscain,Xiangyu Ma,Dario Prandi,Giuseppina Turco*

Main category: eess.SP

TL;DR: A linear cochlea model using vibrating strings shows emergence of sub-harmonic series explaining minor chord consonance and combination tones.


<details>
  <summary>Details</summary>
Motivation: To understand how the cochlea processes sound information and explain historical musical phenomena like minor chord consonance and Tartini's third sound.

Method: Modeling the cochlea as a set of vibrating strings, analyzing energy stored in strings across all oscillation modes using a linear model approach.

Result: Demonstrated emergence of sub-harmonic series that explains minor chord consonance, and showed how nonlinear energy characteristics can explain combination tones (Tartini's third sound).

Conclusion: The simple linear cochlea model successfully explains long-debated musical phenomena, providing new insights into auditory processing and historical music theory concepts.

Abstract: In this paper, we study a simple linear model of the cochlea as a set of
vibrating strings. We make hypothesis that the information sent to the auditory
cortex is the energy stored in the strings and consider all oscillation modes
of the strings. We show the emergence of the sub-harmonic series whose
existence was hypothesized in the XVI century to explain the consonance of the
minor chord. We additionally show how the nonlinearity of the energy can be
used to study the emergence of the combination tone (Tartini's third sound)
shedding new light on this long debated subject.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [69] [Are neural scaling laws leading quantum chemistry astray?](https://arxiv.org/abs/2509.26397)
*Siwoo Lee,Adji Bousso Dieng*

Main category: physics.chem-ph

TL;DR: Neural scaling laws fail to produce reliable quantum chemical models even with large datasets and model capacity. Models trained only on stable structures cannot reproduce H2 energy curves, and foundation models fail to learn basic Coulomb's law.


<details>
  <summary>Details</summary>
Motivation: To test whether neural scaling laws can produce accurate and transferable quantum chemical models for predicting molecular properties like bond dissociation energy.

Method: Scaling model capacity and training data from quantum chemical calculations, evaluating models' predictions of H2 bond dissociation energy curves, and testing on various molecular geometries including compressed and stretched structures.

Result: Models trained only on stable structures fail dramatically to reproduce H2 energy curves. Only with explicit inclusion of compressed/stretched geometries do predictions resemble correct shape. Largest foundation models fail on simple diatomic molecules and cannot reproduce repulsive energy curve of two protons.

Conclusion: Scaling alone is insufficient for building reliable quantum chemical models. Models fail to learn basic electronic structure theory principles like Coulomb's law, highlighting limitations of current neural scaling approaches in quantum chemistry.

Abstract: Neural scaling laws are driving the machine learning community toward
training ever-larger foundation models across domains, assuring high accuracy
and transferable representations for extrapolative tasks. We test this promise
in quantum chemistry by scaling model capacity and training data from quantum
chemical calculations. As a generalization task, we evaluate the resulting
models' predictions of the bond dissociation energy of neutral H$_2$, the
simplest possible molecule. We find that, regardless of dataset size or model
capacity, models trained only on stable structures fail dramatically to even
qualitatively reproduce the H$_2$ energy curve. Only when compressed and
stretched geometries are explicitly included in training do the predictions
roughly resemble the correct shape. Nonetheless, the largest foundation models
trained on the largest and most diverse datasets containing dissociating
diatomics exhibit serious failures on simple diatomic molecules. Most
strikingly, they cannot reproduce the trivial repulsive energy curve of two
bare protons, revealing their failure to learn the basic Coulomb's law involved
in electronic structure theory. These results suggest that scaling alone is
insufficient for building reliable quantum chemical models.

</details>
