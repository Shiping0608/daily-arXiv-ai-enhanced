<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 9]
- [math.AP](#math.AP) [Total: 22]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [math.OC](#math.OC) [Total: 5]
- [math.PR](#math.PR) [Total: 1]
- [nucl-th](#nucl-th) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [math.SP](#math.SP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [math.DG](#math.DG) [Total: 2]
- [stat.CO](#stat.CO) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 5]
- [cs.LG](#cs.LG) [Total: 3]
- [math.CA](#math.CA) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Asymptotically compatible entropy-consistent discretization for a class of nonlocal conservation laws](https://arxiv.org/abs/2510.00221)
*Nicola De Nitti,Kuang Huang*

Main category: math.NA

TL;DR: The paper analyzes convergence of numerical approximations for nonlocal traffic flow conservation laws to local conservation laws, with explicit convergence rates.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous convergence results for numerical discretizations of nonlocal conservation laws modeling traffic flows, showing they converge to solutions of local conservation laws as nonlocal parameters and mesh sizes vanish.

Method: Uses Godunov-type numerical discretization for nonlocal conservation laws with convex kernels, proving uniform L∞- and TV-estimates for compactness and discrete entropy inequalities for entropy admissibility.

Result: Proves discrete approximations converge to entropy solutions of local conservation laws with explicit convergence rate of order ε+h+√(εt)+√(ht), and establishes L¹-contraction for Wε with exponential kernels.

Conclusion: The numerical scheme provides a reliable approximation framework for nonlocal traffic flow models, with proven convergence to local conservation law solutions under appropriate parameter limits.

Abstract: We consider a class of nonlocal conservation laws modeling traffic flows,
given by $ \partial_t \rho_\varepsilon + \partial_x(V(\rho_\varepsilon \ast
\gamma_\varepsilon) \rho_\varepsilon) = 0 $ with a suitable convex kernel $
\gamma_\varepsilon $, and its Godunov-type numerical discretization. We prove
that, as the nonlocal parameter $ \varepsilon $ and mesh size $ h $ tend to
zero simultaneously, the discrete approximation $ W_{\varepsilon,h} $ of $
W_\varepsilon := \rho_\varepsilon \ast \gamma_\varepsilon $ converges to the
entropy solution of the (local) scalar conservation law $ \partial_t \rho +
\partial_x(V(\rho) \rho) = 0 $, with an explicit convergence rate estimate of
order $ \varepsilon+h+\sqrt{\varepsilon\, t}+\sqrt{h\,t} $. In particular, with
an exponential kernel, we establish the same convergence result for the
discrete approximation $ \rho_{\varepsilon,h} $ of $ \rho_\varepsilon $, along
with an $ \mathrm{L}^1 $-contraction property for $ W_\varepsilon $. The key
ingredients in proving these results are uniform $ \mathrm{L}^\infty $- and
$\mathrm{TV}$-estimates that ensure compactness of approximate solutions, and
discrete entropy inequalities that ensure the entropy admissibility of the
limit solution.

</details>


### [2] [A posteriori error estimation for weak Galerkin method of the fourth-order singularly perturbed problem](https://arxiv.org/abs/2510.00354)
*Shicheng Liu,Qilong Zhai*

Main category: math.NA

TL;DR: Posteriori error estimation for weak Galerkin method applied to fourth order singularly perturbed problems


<details>
  <summary>Details</summary>
Motivation: To develop reliable error estimation for weak Galerkin methods in solving fourth order singularly perturbed problems

Method: Constructed a fully computable residual type error estimator for weak Galerkin discretization

Result: Proved both reliability and efficiency of the proposed error estimator

Conclusion: Numerical experiments validated the theoretical findings for the error estimator

Abstract: In this paper, we present a posteriori error estimation for weak Galerkin
method applied to fourth order singularly perturbed problem. The weak Galerkin
discretization space and numerical scheme are first described. A fully
computable residual type error estimator is then constructed. Both the
reliability and efficiency of the proposed estimator are rigorously
demonstrated. Numerical experiments are provided to validate the theoretical
findings.

</details>


### [3] [Numerical analysis of 2D Navier--Stokes equations with nonsmooth initial value in the critical space](https://arxiv.org/abs/2510.00393)
*Buyang Li,Qiqi Rao,Hui Zhang,Zhi Zhou*

Main category: math.NA

TL;DR: This paper presents a fully discrete numerical scheme for 2D Navier-Stokes equations with nonsmooth L² initial data, achieving second-order convergence in both space and time through graded time steps and discrete semigroup techniques.


<details>
  <summary>Details</summary>
Motivation: There is a gap between numerical analysis and computation for NS equations with L² initial data - while numerical results show second-order convergence, existing analysis only proves first-order accuracy. Solutions exhibit singularities at t=0 due to rough initial conditions.

Method: Uses Taylor-Hood or Stokes-MINI finite elements for spatial discretization, implicit-explicit Runge-Kutta time-stepping with graded stepsizes, discrete semigroup techniques, sharp regularity estimates, negative norm estimates, and L² projection onto divergence-free Raviart-Thomas element space.

Result: The proposed scheme achieves second-order convergence in both space and time, which is verified by numerical examples. The convergence in space is at most second order even with higher-order finite elements, showing the sharpness of the proved convergence order.

Conclusion: The work successfully bridges the gap between numerical analysis and computation for NS equations with L² initial data by developing a method that theoretically proves second-order convergence, matching what was previously only observed numerically.

Abstract: This paper addresses the numerical solution of the two-dimensional
Navier--Stokes (NS) equations with nonsmooth initial data in the $L^2$ space,
which is the critical space for the two-dimensional NS equations to be
well-posed. In this case, the solutions of the NS equations exhibit certain
singularities at $t=0$, e.g., the $H^s$ norm of the solution blows up as
$t\rightarrow 0$ when $s>0$. To date, the best convergence result proved in the
literature are first-order accuracy in both time and space for the
semi-implicit Euler time-stepping scheme and divergence-free finite elements
(even high-order finite elements are used), while numerical results demonstrate
that second-order convergence in time and space may be achieved. Therefore,
there is still a gap between numerical analysis and numerical computation for
the NS equations with $L^2$ initial data. The primary challenge to realizing
high-order convergence is the insufficient regularity in the solutions due to
the rough initial condition and the nonlinearity of the equations. In this
work, we propose a fully discrete numerical scheme that utilizes the
Taylor--Hood or Stokes-MINI finite element method for spatial discretization
and an implicit-explicit Runge--Kutta time-stepping method in conjunction with
graded stepsizes. By employing discrete semigroup techniques, sharp regularity
estimates, negative norm estimates and the $L^2$ projection onto the
divergence-free Raviart--Thomas element space, we prove that the proposed
scheme attains second-order convergence in both space and time. Numerical
examples are presented to support the theoretical analysis. In particular, the
convergence in space is at most second order even higher-order finite elements
are used. This shows the sharpness of the convergence order proved in this
article.

</details>


### [4] [A multi-resolution limiter for the Runge-Kutta discontinuous Galerkin method](https://arxiv.org/abs/2510.00511)
*Hua Shen,Bangwei She*

Main category: math.NA

TL;DR: A multi-resolution limiter for RKDG methods that dynamically adjusts polynomial order based on local solution smoothness by comparing derivatives against neighbor-based baselines.


<details>
  <summary>Details</summary>
Motivation: Classical limiters only detect solution discontinuities and dichotomize cells, lacking sensitivity to derivative discontinuities and requiring parameter tuning.

Method: Successively compares DG polynomial derivatives from high to low order against neighbor-based baselines. Reduces polynomial order when derivatives exceed baseline, otherwise keeps current order. Uses TVD limiter only when all derivatives exceed baseline.

Result: The limiter achieves optimal polynomial selection for local smoothness without parameter tuning, exhibits scale-invariance, and demonstrates accuracy and robustness in numerical tests.

Conclusion: The proposed MR limiter provides an effective, parameter-free approach for handling solution and derivative discontinuities in RKDG methods while maintaining accuracy and robustness.

Abstract: We propose a novel multi-resolution (MR) limiter for the Runge-Kutta
discontinuous Galerkin (RKDG) method for solving hyperbolic conservation laws
on a general unstructured mesh. Unlike classical limiters, which detects only
solution discontinuities to dichotomize cells into good or troubled, the
proposed MR limiter also takes into account the derivative discontinuities to
divide cells into several groups. The method operates by performing a
successive comparison of the local DG polynomial's derivatives, from high-order
to low-order, against a baseline constructed from neighboring cell averages. If
a $k$th-order derivative of the DG polynomial is larger than the baseline,
  then we reduce the order to $k-1$ and set the corresponding $k$th-order terms
to be 0; Otherwise, the remaining $k$th-order DG polynomial is used to
represent the final solution. Only if all the derivatives are larger than the
baseline, a TVD slope limiter is used to reconstruct the solution. In this
manner, the limiter dynamically selects an optimal polynomial suited to the
local solution smoothness without problem-dependent parameter to tune. Notably,
it also possesses a scale-invariance property that is absent in most classical
limiters. A series of numerical examples demonstrate the accuracy and
robustness of the proposed MR limiter.

</details>


### [5] [Time-marching multi-level variational multiscale tensor decomposition algorithm for heat conduction with moving heat source](https://arxiv.org/abs/2510.00516)
*Xinyi Guan,Jiayi Hu,Lei Zhang,Shaoqiang Tang,Wing Kam Liu*

Main category: math.NA

TL;DR: A multi-level VMS-TD algorithm for efficient simulation of heat equation with moving heat source in additive manufacturing, combining variational multiscale formulation with tensor decomposition.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient computational method for simulating heat transfer in additive manufacturing processes with moving heat sources, overcoming computational limitations of traditional methods.

Method: Time-marching approach with second-order centered difference time discretization, multi-level space resolution using tensor decomposition, VMS formulation for elliptic problems, and special inter-scale data transfers at scale interfaces.

Result: Multi-level VMS-TD algorithm is significantly more efficient than fully resolved TD algorithms and traditional methods, requiring much smaller degrees of freedom while maintaining accuracy compared to multi-grid methods and GO-MELT framework.

Conclusion: The proposed VMS-TD algorithm provides an efficient and accurate computational framework for additive manufacturing simulations, with potential for multi-time-scale extensions.

Abstract: In this paper, we propose a time-marching multi-level Variational
Multiscale-Tensor Decomposition (VMS-TD) algorithm to solve the heat equation
with a moving heat source model that arises from additive manufacturing. First,
we take a second-order centered difference for time semi-discretization. The
temperature field is decomposed according to multiple space resolution levels,
each represented by the TD method. Then we adopt the VMS formulation [T.J.R.
Hughes, G.R. Feijoo, L. Mazzei, J.B. Quincy. Comput. Methods Appl. Mech. Engrg.
166:3-24 (1998)] for the resulting elliptic problem to obtain a Galerkin weak
form, and design VMS-TD algorithm to solve it. Furthermore, to comply with the
TD solution scheme, special inter-scale data transfers are made at the scale
interface and moving fine-scale subdomains. Numerical results demonstrate that
the multi-level VMS-TD algorithm is much more efficient than the fully resolved
TD algorithm, let alone traditional direct numerical simulation methods such as
finite difference or finite element analysis. Compared with the well-known
multi-grid methods or more recent GO-MELT framework [J.P. Leonor, G.J. Wagner.
Comput. Methods Appl. Mech. Engrg, 426:116977 (2024)], the three-level VMS-TD
uses much smaller degrees of freedom to reach accurate results. A
multi-time-scale extension of VMS-TD algorithm is also proposed.

</details>


### [6] [A Computationally Efficient Finite Element Method for Shape Reconstruction of Inverse Conductivity Problems](https://arxiv.org/abs/2510.00597)
*Lefu Cai,Zhixin Liu,Minghui Song,Xianchao Wang*

Main category: math.NA

TL;DR: A non-iterative finite element method for solving the inverse conductivity problem that directly reconstructs object shapes from boundary measurements without requiring repeated forward problem solutions.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches for the inverse conductivity problem require repeated iterations and solving forward problems at each step, leading to high computational costs. This work aims to develop a more efficient method.

Method: Reformulate the inverse conductivity problem as a minimization problem with regularized residual functional, then transform it into a variational problem. Use finite element method for direct shape reconstruction without iterative procedures.

Result: The proposed approach successfully reconstructs object shapes from finite measurements without iterative procedures. A priori error estimates are established, and a criterion for regularization parameter selection is provided.

Conclusion: The method provides a computationally efficient alternative to traditional iterative approaches for inverse conductivity problems, with theoretical soundness demonstrated through error estimates and numerical verification.

Abstract: The inverse conductivity problem aims at determining the unknown conductivity
inside a bounded domain from boundary measurements. In practical applications,
algorithms based on minimizing a regularized residual functional subject to PDE
constraints have been widely used to deal with this problem. However, such
approaches typically require repeated iterations and solving the forward
problem at each iteration, which leads to a heavy computational cost. To
address this issue, we first reformulate the inverse conductivity problem as a
minimization problem involving a regularized residual functional. We then
transform this minimization problem into a variational problem and establish
the equivalence between them. This reformulation enables the employment of the
finite element method to reconstruct the shape of the object from finitely many
measurements. Notably, the proposed approach allows us to identify the object
directly without requiring any iterative procedure. {\it A prior} error
estimates are rigorously established to demonstrate the theoretical soundness
of the finite element method. Based on these estimates, we provide a criterion
for selecting the regularization parameter. Additionally, several numerical
examples are presented to verify the feasibility of the proposed approach in
shape reconstruction.

</details>


### [7] [Symbolic and High-Accuracy Solutions to Differential and Integral Problems via a Novel Recursive Inverse Laplace Method](https://arxiv.org/abs/2510.00719)
*Mohamed Mostafa*

Main category: math.NA

TL;DR: A novel semi-analytical method using inverse Laplace transform to solve differential, integro-differential, and delay equations with fractional/variable-order derivatives, providing symbolic solutions as generalized logarithmic-power series.


<details>
  <summary>Details</summary>
Motivation: To develop a more versatile and efficient approach for solving complex initial value problems that traditional Laplace-based methods struggle with, particularly for fractional, singular, and chaotic systems.

Method: The method starts with inverse Laplace transform (unlike traditional forward Laplace), assumes the unknown solution is the Laplace transform of an auxiliary function, reformulates the problem in time domain, recursively solves for the auxiliary function using symbolic operations, then applies Laplace transform to obtain final solution.

Result: The method constructs symbolic solutions as generalized logarithmic-power series with arbitrary accuracy, handles complex terms naturally, and demonstrates superior speed and precision across linear/nonlinear problems including singular, fractional, and chaotic systems compared to classical numerical methods.

Conclusion: The proposed method provides a powerful and flexible framework for symbolic computation of initial value problems, offering high versatility and validated reliability through benchmark examples.

Abstract: In this paper, we introduce a novel semi-analytical method for solving a
broad class of initial value problems involving differential,
integro-differential, and delay equations, including those with fractional and
variable-order derivatives. The proposed approach is based on the inverse
Laplace transform, applied initially - unlike traditional Laplace-based
techniques which begin with a forward transformation. By assuming the unknown
solution is the Laplace transform of an auxiliary function, the method
reformulates the problem in the time domain and recursively solves for this
function using symbolic operations. The final solution is then obtained by
applying the Laplace transform to the result. This strategy enables the
construction of symbolic solutions as generalized logarithmic-power series with
arbitrary accuracy, and naturally accommodates complex terms. The method is
highly versatile and demonstrates superior speed and precision across a wide
range of linear and nonlinear problems, including singular, fractional, and
chaotic systems. Several benchmark examples are provided to validate the
reliability and efficiency of the proposed technique compared to classical
numerical methods. The results confirm that the new method offers a powerful
and flexible framework for symbolic computation of initial value problems.

</details>


### [8] [Carleman Linearization of Parabolic PDEs: Well-posedness, convergence, and efficient numerical methods](https://arxiv.org/abs/2510.00722)
*Bernhard Heinzelreiter,John W. Pearson*

Main category: math.NA

TL;DR: Extension of Carleman linearization to infinite-dimensional Hilbert spaces with quadratic nonlinearities, showing well-posedness and convergence for parabolic PDEs like Navier-Stokes equations.


<details>
  <summary>Details</summary>
Motivation: To extend Carleman linearization analysis to infinite-dimensional systems with quadratic nonlinearities, particularly for common parabolic PDEs where traditional methods face challenges.

Method: Analyze truncated Carleman linearization under suitable assumptions, demonstrate decomposition of total approximation error into discretization and linearization components, and use structure-exploiting numerical methods like sparse grids.

Result: Established well-posedness and convergence of truncated Carleman linearization, with convergence radius and rate independent of discretization, verified through numerical experiments.

Conclusion: The approach justifies Carleman linearization for parabolic PDE problems and motivates structure-exploiting methods to handle the curse of dimensionality.

Abstract: We explore how the analysis of the Carleman linearization can be extended to
dynamical systems on infinite-dimensional Hilbert spaces with quadratic
nonlinearities. We demonstrate the well-posedness and convergence of the
truncated Carleman linearization under suitable assumptions on the dynamical
system, which encompass common parabolic semi-linear partial differential
equations such as the Navier-Stokes equations and nonlinear
diffusion-advection-reaction equations. Upon discretization, we show that the
total approximation error of the linearization decomposes into two independent
components: the discretization error and the linearization error. This
decomposition yields a convergence radius and convergence rate for the
discretized linearization that are independent of the discretization. We thus
justify the application of the linearization to parabolic PDE problems.
Furthermore, it motivates the use of non-standard structure-exploiting
numerical methods, such as sparse grids, taming the curse of dimensionality
associated with the Carleman linearization. Finally, we verify the results with
numerical experiments.

</details>


### [9] [Implementation techniques for multigrid solvers for high-order Discontinuous Galerkin methods](https://arxiv.org/abs/2510.00998)
*Sean Baccas,Alexander A. Belozerov,Eike H. Müller,Tobias Weinzierl*

Main category: math.NA

TL;DR: The paper presents matrix-free geometric multigrid solvers for elliptic PDEs using Higher-order Discontinuous Galerkin methods, focusing on execution strategies to overcome performance limitations and improve efficiency on modern architectures.


<details>
  <summary>Details</summary>
Motivation: Higher-order DG methods offer exponential convergence and suit modern computer architectures, but face performance issues due to non-local memory access, varying compute costs, and frequent synchronization requirements.

Method: Developed efficient execution strategies for hp-multigrid by: 1) separating cell- and facet-operations using auxiliary facet variables, 2) localizing data access and reducing synchronization, 3) enabling computation-communication overlap, 4) implementing loop fusion for single-touch scheme, and 5) interpreting strategies in task formalism to expose additional concurrency.

Result: The proposed strategies improve performance by localizing data access, reducing synchronization needs, enabling computation-communication overlap, and creating a single-touch scheme that reads cell data only once per smoothing step.

Conclusion: The paper makes advanced multigrid solver algorithms more accessible to the wider scientific computing community by providing practical implementation techniques that address performance bottlenecks in DG methods.

Abstract: Matrix-free geometric multigrid solvers for elliptic PDEs that have been
discretised with Higher-order Discontinuous Galerkin (DG) methods are ideally
suited to exploit state-of-the-art computer architectures. Higher polynomial
degrees offer exponential convergence, while the workload fits to vector units,
is straightforward to parallelise, and exhibits high arithmetic intensity. Yet,
DG methods such as the interior penalty DG discreisation do not magically
guarantee high performance: they require non-local memory access due to
coupling between neighbouring cells and break down into compute steps of widely
varying costs and compute character. We address these limitations by developing
efficient execution strategies for $hp$-multigrid. Separating cell- and
facet-operations by introducing auxiliary facet variables localizes data
access, reduces the need for frequent synchronization, and enables overlap of
computation and communication. Loop fusion results in a single-touch scheme
which reads (cell) data only once per smoothing step. We interpret the
resulting execution strategies in the context of a task formalism, which
exposes additional concurreny. The target audience of this paper are
practitioners in Scientific Computing who are not necessarily experts on
multigrid or familiar with sophisticated discretisation techniques. By
discussing implementation techniques for a powerful solver algorithm we aim to
make it accessible to the wider community.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [10] [Classification of solutions of an elliptic Hamilton-Jacobi equation](https://arxiv.org/abs/2510.00146)
*Alessio Porretta,Philippe Souplet*

Main category: math.AP

TL;DR: The paper proves that all classical solutions of the diffusive Hamilton-Jacobi equation in a half-space with zero boundary conditions are one-dimensional for 1<p≤2, removing the previous boundedness assumption.


<details>
  <summary>Details</summary>
Motivation: To complete the classification of Dirichlet problem solutions for the diffusive Hamilton-Jacobi equation in half-spaces by extending the result to the range 1<p≤2 without requiring boundedness assumptions.

Method: Mathematical analysis of the diffusive Hamilton-Jacobi equation -Δu = |∇u|^p in half-spaces with zero boundary conditions, building on existing techniques but removing the boundedness constraint.

Result: Showed that any classical solution for 1<p≤2 is necessarily one-dimensional, improving previous results that required boundedness from above.

Conclusion: This result, combined with existing results for p>2, completes the full classification of Dirichlet problem solutions for the diffusive Hamilton-Jacobi equation in half-spaces.

Abstract: We show that any classical solution of the diffusive Hamilton-Jacobi (DHJ)
equation $-\Delta u= |\nabla u|^p$ in a half-space with zero boundary
conditions for $1<p\le 2$ is necessarily one-dimensional. This improves the
previously known result, which required an extra assumption of boundedness from
above. Combined with the existing analogous result for $p>2$, our result
completes the full classification picture of the Dirichlet problem for equation
(DHJ) in a half-space.

</details>


### [11] [Stackelberg-Nash strategy for the null controllability of semilinear degenerate equations in non-cylindrical domains](https://arxiv.org/abs/2510.00173)
*Alfredo S. Gamboa,Juan Limaco,Luis P. Yapu*

Main category: math.AP

TL;DR: Local null controllability of semilinear parabolic equations in degenerated non-cylindrical domains using Stackelberg-Nash strategy


<details>
  <summary>Details</summary>
Motivation: To study controllability of semilinear parabolic equations where diffusion coefficient degenerates at boundary points in non-cylindrical domains

Method: Stackelberg-Nash strategy, Carleman inequality for degenerated non-autonomous systems, and Liusternik's inverse function theorem

Result: Proved local null controllability for the semilinear degenerated system

Conclusion: Successfully established local controllability using combined analytical techniques for degenerated parabolic systems

Abstract: In this paper we use a Stackelberg-Nash strategy to show the local null
controllability of a semilinear parabolic equation in one-dimension defined in
a non-cylindrical domain where the diffusion coefficient degenerates at one
point of the boundary. The linearized degenerated system is treated using a
Carleman inequality for degenerated non-autonomous systems proved by the autors
in [19] and the local controllability of the semilinear system is obtained
using Liusterniks inverse function theorem.

</details>


### [12] [Integrated Local Energy Decay for Waves with Time-Dependent Damping](https://arxiv.org/abs/2510.00179)
*Perry Kleinhenz,Michael McNulty*

Main category: math.AP

TL;DR: Proves integrated local energy decay for damped wave equations on asymptotically flat stationary space-times with time-dependent damping satisfying a generalized geometric control condition.


<details>
  <summary>Details</summary>
Motivation: To establish energy decay properties for wave equations with damping in curved space-time backgrounds, extending previous results to time-dependent damping scenarios.

Method: Uses a positive commutator estimate with an escape function constructed around space-time trajectories, combining high frequency estimates with existing low/medium frequency results for undamped problems.

Result: Successfully obtained local energy decay by treating the damping term as a perturbation after establishing the high frequency estimate.

Conclusion: The paper demonstrates that integrated local energy decay holds for damped wave equations with time-dependent damping under appropriate geometric control conditions on asymptotically flat stationary space-times.

Abstract: We prove integrated local energy decay for solutions of the damped wave
equation with time-dependent damping satisfying an appropriate generalization
of the geometric control condition on asymptotically flat, stationary
space-times. We first obtain a high frequency estimate, which we prove via a
positive commutator estimate using an escape function explicitly constructed in
terms of the damping around individual space-time trajectories. We combine the
high frequency estimate with low and medium frequency results for the undamped
problem, then we handle the damping term as a perturbation to obtain local
energy decay.

</details>


### [13] [Very Weak Solutions and Asymptotic Behavior of Leray Solutions to the Stationary Navier-Stokes Equations](https://arxiv.org/abs/2510.00265)
*Giovanni Paolo Galdi*

Main category: math.AP

TL;DR: The paper shows that if the L^s-norm of a Leray solution to the Navier-Stokes problem in an exterior domain is sufficiently small on a spherical surface, then the solution decays as |x|^{-1} at infinity.


<details>
  <summary>Details</summary>
Motivation: To establish decay conditions for Navier-Stokes solutions in exterior domains, particularly understanding when solutions exhibit specific decay rates at infinity.

Method: Uses an approach based on a new theory of very weak solutions in exterior domains, analyzing the L^s-norm of solutions on spherical surfaces.

Result: Proves that under certain conditions on the L^s-norm of the solution on a spherical surface, the solution must decay as |x|^{-1} for large |x|.

Conclusion: The paper provides a sufficient condition for |x|^{-1} decay of Navier-Stokes solutions in exterior domains, introducing a new approach with very weak solutions that has independent mathematical interest.

Abstract: Let $\bfu$ be a Leray solution to the Navier-Stokes boundary-value problem in
an exterior domain, vanishing at infinity and satisfying the generalized energy
inequality. We show that if there exist $R>0$ and ${\sf s}\ge \frac23 q$,
$q>6$, such that the $L^{\sf s}-$norm of $\bfu$ on the spherical surface of
radius $R$ divided by $R$ is less than a constant depending only on {\sf s} and
$q$, then $\bfu(x)$ must decay as $|x|^{-1}$ for $|x|\to\infty$. This result is
proved with an approach based on a new theory of very weak solutions in
exterior domains which, as such, is of independent interest.

</details>


### [14] [Blow-up of solutions for discrete semilinear wave equation with the scale-invariant damping](https://arxiv.org/abs/2510.00439)
*Koji Wada,Kyouhei Wakasa*

Main category: math.AP

TL;DR: Analysis of blow-up behavior in discretized scale-invariant nonlinear dissipative wave equations, comparing critical exponents with continuous equations.


<details>
  <summary>Details</summary>
Motivation: To extend blow-up results from continuous scale-invariant nonlinear dissipative wave equations to their discretized counterparts, and compare critical exponents between discrete and continuous cases.

Method: Based on Matsuya's approach for discrete semilinear wave equations without dissipative terms, adapted for dissipative cases. Focuses on critical exponents (Fujita and Strauss exponents) in relation to space dimensions.

Result: Found that the blow-up results for discretized equations are sharp (match exactly) with continuous equations in one and two space dimensions.

Conclusion: The discretized scale-invariant nonlinear dissipative wave equations exhibit blow-up behavior that corresponds precisely to their continuous counterparts in lower dimensions (1D and 2D), validating the discretization approach.

Abstract: We consider the blow-up problem for discretized scale-invariant nonlinear
dissipative wave equations. It is known that the critical exponents for
undiscretized equations (continuous equations) are given by Fujita and Strauss
exponents depending on the space dimensions. Our purpose is to obtain results
for the discretized equations that correspond to those shown for the continuous
one. The proof is based on Matsuya [6], who showed the blow-up problem for
discrete semilinear wave equations without dissipative terms, and we found that
the result is sharp in the case of one and two space dimensions compared to the
continuous equations.

</details>


### [15] [On Sharp Heisenberg Uncertainty Principle and the stability](https://arxiv.org/abs/2510.00453)
*Xia Huang,Dong Ye*

Main category: math.AP

TL;DR: The paper applies linearization methods to study Heisenberg Uncertainty Principles and stability problems, proving Cazacu-Flynn-Lam's conjecture in dimension 4 and improving recent estimates in R^2 and R^3, while also identifying best constants for stability estimates.


<details>
  <summary>Details</summary>
Motivation: To extend linearization methods from studying Heisenberg Uncertainty Principles to stability problems, and to resolve conjectures and improve existing estimates in uncertainty principle inequalities.

Method: Uses linearization method combined with spherical harmonic decomposition and Hardy inequalities to analyze uncertainty principles and stability problems.

Result: Proved Cazacu-Flynn-Lam's conjecture for sharp Hydrogen Uncertainty Principle in dimension 4, improved Chen-Tang's estimates in R^2 and R^3, and identified best constants for stability estimates studied by Duong-Nguyen and Do-Lam-Lu-Zhang.

Conclusion: Linearization methods are effective for both uncertainty principle analysis and stability problems, enabling resolution of conjectures and improvement of existing bounds across multiple dimensions.

Abstract: In this work, we summarize the linearization method to study the Heisenberg
Uncertainty Principles, and explain that the same approach can be used to
handle the stability problem. As examples of application, combining with
spherical harmonic decomposition and the Hardy inequalities, we revise two
families of inequalities. We give firstly an affirmative answer in dimension
four to Cazacu-Flynn-Lam's conjecture [JFA, 2022] for the sharp Hydrogen
Uncertainty Principle, and improve the recent estimates of Chen-Tang
[arXiv:2508.15221v1] in $\mathbb{R}^2$ and $\mathbb{R}^3$. On the other hand,
we identify the best constants and extremal functions for two stability
estimates associated to $\|\Delta u\|_2 \|r\nabla u\|_2 - \frac{N+2}{2}\|\nabla
u\|^2_2$ in $\mathbb{R}^N$ ($N \geq 2$), studied recently by Duong-Nguyen
[CVPDE, 2025] and Do-Lam-Lu-Zhang [arXiv:2505.02758v1].

</details>


### [16] [Stability of Lamb dipoles for odd-symmetric and non-negative initial disturbances without the finite mass condition](https://arxiv.org/abs/2510.00539)
*Ken Abe,Kyudong Choi,In-Jee Jeong*

Main category: math.AP

TL;DR: Stability analysis of Lamb dipole solution in 2D Euler equations under odd symmetry and non-negativity conditions for initial vorticity disturbances, without requiring finite mass condition.


<details>
  <summary>Details</summary>
Motivation: Motivated by experimental observations of large vortex dipole formation in two-dimensional turbulence, to understand under what initial disturbance conditions the Lamb dipole remains stable.

Method: Uses a new variational characterization of the Lamb dipole based on an improved energy inequality, assuming odd symmetry for x₂-variable and non-negativity in the upper half plane for initial vorticity disturbance.

Result: Established stability theorem for the Lamb dipole without assuming finite mass condition for the initial disturbance.

Conclusion: The Lamb dipole solution is stable under specified symmetry and non-negativity conditions, providing theoretical foundation for observed vortex dipole stability in 2D turbulence experiments.

Abstract: In this paper, we consider the stability of the Lamb dipole solution of the
two-dimensional Euler equations in $\mathbb{R}^{2}$ and question under which
initial disturbance the Lamb dipole is stable, motivated by experimental work
on the formation of a large vortex dipole in two-dimensional turbulence. We
assume (O) odd symmetry for the $x_2$-variable and (N) non-negativity in the
upper half plane for the initial disturbance of vorticity, and establish the
stability theorem of the Lamb dipole without assuming (F) finite mass
condition. The proof is based on a new variational characterization of the Lamb
dipole using an improved energy inequality.

</details>


### [17] [Revisiting the Cauchy problem for the Zakharov-Rubenchik/Benney-Roskes system](https://arxiv.org/abs/2510.00692)
*Hung Luong*

Main category: math.AP

TL;DR: Local well-posedness of Zakharov-Rubenchik/Benney-Roskes system in energy space H^1 for dimensions 2 and 3 using dispersive estimates and Bourgain spaces.


<details>
  <summary>Details</summary>
Motivation: To establish local well-posedness for the Zakharov-Rubenchik/Benney-Roskes system in the energy space, and potentially find exact existence time scales for water wave models.

Method: Based on dispersive estimates and suitable Bourgain's spaces to analyze the Cauchy problem.

Result: Obtained local well-posedness with main component ψ belonging to H^1(R^d) for d=2,3, which is the energy space for this component.

Conclusion: The approach provides local well-posedness results and suggests a potential method for determining exact existence time scales in water wave contexts.

Abstract: In this paper, we revisit the Cauchy problem for the
Zakharov-Rubenchik/Benney-Roskes system. Our method is based on the dispersive
estimates and the suitable Bourgain's spaces. We then, obtain the local
well-posedness of the solution with the main component $\psi$ belongs to
$H^1(\mathbb{R}^d)$ ($d=2, 3$) which is actually the energy space corresponding
to this component. Our result also suggests a potential approach to the problem
of finding exact existence time scale for the solution of Benney-Roskes model
in the context of water waves.

</details>


### [18] [Strichartz estimates and its application to the well-posedness of the nonlinear Schrödinger equations on H-type groups](https://arxiv.org/abs/2510.00709)
*Hiroyuki Hirayama,Yasuyuki Oka*

Main category: math.AP

TL;DR: Well-posedness results for nonlinear Schrödinger equations with power nonlinearities on H-type groups, with corrected dispersive and Strichartz estimates.


<details>
  <summary>Details</summary>
Motivation: To establish well-posedness for the Cauchy problem of nonlinear Schrödinger equations on H-type groups, addressing gaps in previous proofs by Hierro (2005).

Method: Prove corrected versions of dispersive and Strichartz estimates, then apply these to the nonlinear problem.

Result: Improved well-posedness results that surpass previous work by Bruno et al.

Conclusion: The paper successfully provides corrected proofs for key estimates and achieves improved well-posedness for nonlinear Schrödinger equations on H-type groups.

Abstract: The aim of this article is to give the well-posedness results for the Cauchy
problem of the nonlinear Schr\"odinger equation with power type nonlinearities
on H-type groups. To do this, we prove the dispersive estimate and Strichartz
estimate. Although these estimates are given by Hierro (2005), its complete
proofs cannot be find. We correct the statement of these estimates, give the
proofs, and apply to the nonlinear problem. Our well-posedness results are an
improvement of the previous result by Bruno et al.

</details>


### [19] [Lecture notes: Biological propagation via reaction-diffusion equations with nonlocal diffusion and free boundary](https://arxiv.org/abs/2510.00710)
*Yihong Du*

Main category: math.AP

TL;DR: Lecture notes on nonlocal reaction-diffusion models with free boundaries in 1D, covering existence, uniqueness, comparison principles, and extending results to weakly non-symmetric kernels.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive theoretical foundation for nonlocal reaction-diffusion models with free boundaries, particularly addressing the gap in understanding asymmetric kernel cases.

Method: Detailed mathematical proofs and theoretical analysis of propagation governed by nonlocal reaction-diffusion models, extending existing symmetric kernel results to weakly non-symmetric cases.

Result: Established existence and uniqueness of solutions, comparison principles for general cases, and demonstrated that existing techniques can be extended to handle weakly non-symmetric kernel functions.

Conclusion: The theory for nonlocal reaction-diffusion models with free boundaries can be successfully extended from symmetric to weakly non-symmetric kernels, though significant dynamical differences may arise in strongly asymmetric cases.

Abstract: These notes are based on the lectures given in a mini-course at VIASM
(Vietnam Institute for Advanced Study in Mathematics) 2025 Summer School. They
give a brief account of the theory (with detailed proofs) for propagation
governed by a nonlocal reaction-diffusion model with free boundaries in one
space dimension. The main part is concerned with a KPP reaction term, though
the basic results on the existence and uniqueness of solutions as well as on
the comparison principles are for more general situations. The contents are
mostly taken from published recent works of the author with several
collaborators, where the kernel function was assumed to be symmetric:
J(x)=J(-x). When J(x) is not symmetric, significant differences may arise in
the dynamics of the model, as shown in several preprints quoted in the
references at the end of these notes, but many of the existing techniques can
be easily extended to cover the "weakly non-symmetric case", and this is done
here with all the necessary details.

</details>


### [20] [Convergence to a receding wave in a monostable free boundary problem](https://arxiv.org/abs/2510.00715)
*Hongkai Cao,Yihong Du,Wenjie Ni*

Main category: math.AP

TL;DR: The paper studies a monostable reaction-diffusion equation with a free boundary governed by a "preferred population density" principle, showing that in the high-density regime, the habitat edge retreats with constant asymptotic speed and the population density converges to a semi-wave profile.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of population fronts in unbounded domains with free boundaries, particularly in retreating scenarios where existing techniques for advancing fronts fail.

Method: A "touching method" using lower and upper solutions constructed from semi-waves of auxiliary problems to apply comparison principles at the moving boundary.

Result: The free boundary retreats to infinity with constant speed c(δ)>0, and the population density converges uniformly to the semi-wave profile q_{c(δ)}(x) when shifted by the boundary position.

Conclusion: The developed touching method successfully overcomes limitations of previous techniques and provides a framework for analyzing retreating fronts in free boundary problems.

Abstract: We study a monostable reaction-diffusion equation of the form
$u_t=du_{xx}+f(u)$ over a semi-infinite spatial domain $[g(t),\infty)$, with
$x=g(t)$ the free boundary whose evolution is governed by equations derived
from a ``preferred population density'' principle, which postulates that the
species with population density $u(t,x)$ and population range $[g(t),\infty)$
maintains a certain density $\delta$ at the habitat edge $x=g(t)$. In the
``high-density'' regime, where $\delta$ exceeds the carrying capacity of the
favourable environment represented by a monostable function $f(u)$, it is known
(see \cite{DLNS} for the case of a bounded population range $[g(t), h(t)]$)
that for large time, the front retreats as time advances. In this work, the
unboundedness of the population range $[g(t),\infty)$ allows us to prove that,
as time $t$ converges to infinity, the free boundary $x=g(t)$ converges to
$\infty$ with a constant asymptotic speed $c(\delta)>0$ determined by an
associated semi-wave problem, and the population density $u(t,x)$ has the
property that $u(t,x+g(t))$ converges uniformly to $q_{c(\delta)}(x)$, the
semi-wave profile function associated with the speed $c(\delta)$. It turns out
that in the retreating situation considered here, some key techniques developed
for advancing fronts in related free boundary models do not work anymore. This
difficulty is overcome here by a ``touching method", which uses a family of
lower and upper solutions constructed from semi-waves of some carefully
designed auxiliary problems to touch the solution $u(t,x)$ at the moving
boundary $x=g(t)$, thereby generating a setting where the comparison principle
can be used to obtain the desired estimates for $g'(t)$ and $u(t,x)$. We
believe this method will find applications elsewhere.

</details>


### [21] [High-order Regularity Theory for High-contrast Elliptic Homogenization](https://arxiv.org/abs/2510.00737)
*Heikki Lohi*

Main category: math.AP

TL;DR: Global high-order regularity result in elliptic homogenization with high-contrast framework, including a high-contrast Caccioppoli inequality.


<details>
  <summary>Details</summary>
Motivation: To establish global high-order regularity results within the high-contrast framework of elliptic homogenization, which requires developing specialized mathematical tools.

Method: Formulate and prove a global high-order regularity result, and present a version of the high-contrast Caccioppoli inequality to support the analysis.

Result: Successfully formulated and proved the global high-order regularity result within the high-contrast elliptic homogenization framework.

Conclusion: The paper provides important mathematical foundations for high-contrast elliptic homogenization through the development of global regularity results and specialized inequalities.

Abstract: The purpose of this article is to formulate and prove a global high-order
regularity result within the high-contrast framework of elliptic
homogenization. In order to achieve this, we also present a version of the
high-contrast Caccioppoli inequality.

</details>


### [22] [Removable singularities and Harnack inequality for nonlinear Hörmander degenerate subelliptic equations](https://arxiv.org/abs/2510.00755)
*Jiayi Qiang,Yawei Wei,Mengnan Zhang*

Main category: math.AP

TL;DR: This paper studies quasilinear subelliptic equations from Hörmander vector fields, proving removable singularities, Harnack inequality via sharp Sobolev inequalities, and Hölder continuity in equiregular domains.


<details>
  <summary>Details</summary>
Motivation: To extend previous work by Serrin, Meier, Capogna, Danielli, and Garofalo on subelliptic equations by establishing key results under weaker integrability conditions for coefficients.

Method: Uses sharp Sobolev inequalities and analysis of quasilinear subelliptic equations derived from Hörmander vector fields, with weaker coefficient integrability requirements.

Result: Proves removable singularities, Harnack inequality, and obtains Hölder continuity when the domain is equiregular.

Conclusion: The paper successfully establishes fundamental regularity results for quasilinear subelliptic equations under more general conditions than previous work.

Abstract: This paper concerns the quasilinear subelliptic function derived from
H\"ormander vector fields. Based on the significant work of J. Serrin in
\cite{SER}, M. Meier in \cite{MM1}, and L. Capogna, D. Danielli and N. Garofalo
in \cite{LC1,LDN}, we obtain the removable singularities and Harnack inequality
by a sharp Sobolev inequalities under weaker integrability of coefficients in
structure conditions. Furthermore, we get the H\"older continuity when domain
$\Omega$ is equiregular.

</details>


### [23] [Splitting and Merging of Stagnation Points of Solutions to the 2D Navier-Stokes Equations](https://arxiv.org/abs/2510.00784)
*Isidro Benaroya,Alberto Enciso,Daniel Peralta-Salas*

Main category: math.AP

TL;DR: Construction of Navier-Stokes solutions on R² with multiple stagnation points that merge and split along prescribed trajectories.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the existence of Navier-Stokes solutions with complex stagnation point dynamics that can be controlled and prescribed.

Method: Constructing specific solutions to the Navier-Stokes equations that allow for arbitrary numbers of stagnation points with prescribed merging and splitting trajectories.

Result: Successfully constructed solutions where stagnation points can merge and split along trajectories that can be freely prescribed up to small deformations.

Conclusion: The paper shows that complex stagnation point dynamics in 2D Navier-Stokes flows can be engineered and controlled through appropriate solution construction.

Abstract: We construct solutions to the Navier-Stokes equations on $\mathbf{R}^2$ with
an arbitrary number of stagnation points which merge and split along
trajectories that can be prescribed freely, up to a small deformation.

</details>


### [24] [Global weak solutions and incompressible limit of two-dimensional isentropic compressible magnetohydrodynamic equations with ripped density and large initial data](https://arxiv.org/abs/2510.00812)
*Shuai Wang,Guochun Wu,Xin Zhong*

Main category: math.AP

TL;DR: Global existence of weak solutions for isentropic compressible magnetohydrodynamic equations with large bulk viscosity, and their convergence to incompressible equations when viscosity tends to infinity.


<details>
  <summary>Details</summary>
Motivation: To establish existence results for compressible magnetohydrodynamic equations with large initial data and vacuum states, and study their incompressible limit behavior.

Method: Uses effective viscous flux and Desjardins-type logarithmic interpolation inequality to prove global existence and convergence.

Result: Proves global existence of weak solutions for large bulk viscosity, and shows convergence to inhomogeneous incompressible magnetohydrodynamic equations as viscosity approaches infinity.

Conclusion: First result on incompressible limit of isentropic compressible magnetohydrodynamic equations for large bulk viscosity, allowing arbitrarily large initial data and interior vacuum states.

Abstract: We establish the global existence of weak solutions to isentropic
compressible magnetohydrodynamic equations with ripped density in the whole
plane provided the bulk viscosity coefficient is properly large. Moreover, we
show that such solutions converge globally in time to a weak solution of the
inhomogeneous incompressible magnetohydrodynamic equations when the bulk
viscosity coefficient tends to infinity. In particular, the initial data can be
arbitrarily large and vacuum states are allowed in interior regions. Our method
relies on the effective viscous flux and a Desjardins-type logarithmic
interpolation inequality. To the best of our knowledge, this is the first
result concerning incompressible limit of isentropic compressible
magnetohydrodynamic equations for the large bulk viscosity.

</details>


### [25] [Compactness of conformal metrics with constant $Q$-curvature of higher order](https://arxiv.org/abs/2510.00888)
*Saikat Mazumdar,Bruno Premoselli*

Main category: math.AP

TL;DR: The paper establishes compactness results for conformal metrics with prescribed constant positive Q-curvature of order 2k on closed Riemannian manifolds, overcoming the challenge of non-explicit GJMS operators using Juhl's recursive formulae.


<details>
  <summary>Details</summary>
Motivation: To investigate the compactness of conformal metrics with prescribed constant positive Q-curvature, particularly addressing the challenge that GJMS operators P_g are not explicit for arbitrary k, which has been an obstacle to proving compactness results.

Method: Uses Juhl's celebrated recursive formulae for the GJMS operator P_g to perform refined blow-up analysis for solutions of the Q-curvature equation and prove a Weyl vanishing result for P_g.

Result: Proves compactness under three scenarios: (1) locally conformally flat manifolds with positive mass, (2) dimensions 2k+1 ≤ n ≤ 2k+5 with positive mass, or (3) dimensions n ≥ 2k+4 with non-zero Weyl tensor. This is the first compactness result for arbitrary 1 ≤ k < n/2.

Conclusion: The paper successfully overcomes the non-explicit nature of GJMS operators using Juhl's formulae, establishing compactness and suggesting that the threshold dimension for compactness diverges as k → ∞.

Abstract: Let $k\ge1$ be a positive integer and let $P_g$ be the GJMS operator $P_{g}$
of order $2k$ on a closed Riemannian manifold $(M,g)$ of dimension $n>2k$. We
investigate the compactness of the set of conformal metrics to $g$ with
prescribed constant positive $Q$-curvature of order $2k$- or, equivalently, of
the set of positive solutions for the $2k$-th order $Q$-curvature equation.
Under a natural positivity-preserving condition on $P_{g}$ we establish
compactness, for an arbitrary $1 \le k < \frac{n}{2}$, under the following
assumptions: $(M,g)$ is locally conformally flat and $P_g$ has positive mass in
$M$, or $2k+1 \le n \le 2k+5$ and $P_g$ has positive mass in $M$, or $n \ge
2k+4$ and $|\text{W}_g|_g >0$ in $M$.
  For an arbitrary $1 \le k < \frac{n}{2}$, the expression of $P_g$ is not
explicit, which is an obstacle to proving compactness. We overcome this by
relying on Juhl's celebrated recursive formulae for $P_g$ to perform a refined
blow-up analysis for solutions of the $Q$-curvature equation and to prove a
Weyl vanishing result for $P_g$. This is the first compactness result for an
arbitrary $1 \le k < \frac{n}{2}$ and the first successful instance where
Juhl's formulae are used to yield compactness. Our result also hints that the
threshold dimension for compactness for the $2k$-th order $Q$-curvature
equation diverges as $k \to + \infty$.

</details>


### [26] [Existence of multiple normalized solutions to a critical growth Choquard equation involving mixed operator](https://arxiv.org/abs/2510.00893)
*Nidhi Nidhi,K. Sreenadh*

Main category: math.AP

TL;DR: Existence of normalized solutions for critical growth Choquard equation with mixed local and non-local operators, showing at least two distinct solutions under mass subcritical perturbation.


<details>
  <summary>Details</summary>
Motivation: To study normalized solutions of critical Choquard equations combining local Laplacian and fractional Laplacian operators, addressing the challenge of critical exponents in the presence of mixed operators.

Method: Analysis of the critical Choquard equation with mixed operators using variational methods, focusing on the mass constraint and Lagrange multiplier approach.

Result: Proved existence of at least two distinct normalized solutions when the mass subcritical perturbation parameter satisfies 2 < p < 2 + 4s/N, under certain assumptions on τ.

Conclusion: The paper successfully establishes multiplicity results for normalized solutions in critical Choquard equations with mixed local and non-local operators, extending previous work to more complex operator combinations.

Abstract: In this paper we study the normalized solutions of the following critical
growth Choquard equation with mixed local and non-local operators:
  \begin{equation*}
  \begin{array}{rcl}
  -\Delta u +(-\Delta)^s u & = & \lambda u +\mu |u|^{p-2}u
+(I_{\alpha}*|u|^{2^*_{\alpha}})|u|^{2^*_{\alpha}-2}u \text{ in }
\mathbb{R}^N;\;\;
  \left\| u \right\|_2 & = & \tau,
  \end{array}
  \end{equation*}
  here $N\geq 3$, $\tau>0$, $I_{\alpha}$ is the Riesz potential of order
$\alpha\in (0,N)$, $2^*_{\alpha}=\frac{N+\alpha}{N-2}$ is the critical exponent
corresponding to the Hardy Littlewood Sobolev inequality, $(-\Delta)^s$ is the
non-local fractional Laplacian operator with $s\in (0,1)$, $\mu>0$ is a
parameter and $\lambda$ appears as Lagrange multiplier. We have shown the
existence of atleast two distinct solutions in the presence of mass subcritical
perturbation, $\mu |u|^{p-2}u$ with $2<p<2+\frac{4s}{N}$ under some assumptions
on $\tau$.

</details>


### [27] [Exponential Dichotomies in Higher-Dimensional Spatial Dynamics for Elliptic Partial Differential Equations](https://arxiv.org/abs/2510.00917)
*Margaret Beck,Ryan Goh,Alanna Haslam-Hyde*

Main category: math.AP

TL;DR: The paper extends the spatial dynamics framework to prove that exponential dichotomies exist for elliptic PDEs on general multi-dimensional domains, enabling future analysis of coherent structures.


<details>
  <summary>Details</summary>
Motivation: To leverage exponential dichotomies for studying bounded solutions in ill-posed evolutionary equations and extend spatial dynamics techniques to more general domains beyond simple unbounded directions.

Method: Using spatial dynamics approach where spatial variables are treated as time-like evolutionary variables, applying classical dynamical systems techniques to elliptic PDEs on multi-dimensional domains.

Result: Demonstrated that exponential dichotomies exist for elliptic PDEs posed on general multi-dimensional spatial domains, extending previous work limited to domains with distinguished unbounded directions.

Conclusion: The existence of exponential dichotomies in this broader context enables future analysis of coherent structures like spatial patterns in reaction-diffusion equations on more general domains.

Abstract: Exponential dichotomies, when they exist, provide powerful information about
the structure of bounded solutions even in the case of an ill-posed
evolutionary equation. The method of spatial dynamics, in which one views a
spatial variable as a time-like evolutionary variable, allows for the use of
classical dynamical systems techniques, such as exponential dichotomies, in
broader contexts. This has been utilized to study stationary, traveling wave,
time-periodic, and spiral wave solutions of PDEs on spatial domains with a
distinguished unbounded direction (e.g. the real line or a channel of the form
$\mathbb{R}\times\Omega$). Recent work has shown how to extend the spatial
dynamics framework to elliptic PDEs posed on general multi-dimensional spatial
domains. In this paper, we show that, in the same context, exponential
dichotomies exist, thus allowing for their use in future analyses of coherent
structures, such as spatial patterns in reaction-diffusion equations on more
general domains.

</details>


### [28] [A Unified Hölder Lebesgue Framework for Caffarelli Kohn Nirenberg Inequalities](https://arxiv.org/abs/2510.00949)
*Mengxia Dong*

Main category: math.AP

TL;DR: The paper develops a unified Hölder-Lebesgue scale framework to extend the Caffarelli-Kohn-Nirenberg inequality beyond classical Lebesgue spaces, proving interpolation theorems and establishing generalized CKN inequalities on bounded punctured domains with precise characterization of constants.


<details>
  <summary>Details</summary>
Motivation: To extend the classical Caffarelli-Kohn-Nirenberg (CKN) inequality beyond the standard Lebesgue regime and develop a unified framework that bridges integrability and regularity across the Lebesgue-Hölder spectrum.

Method: Developed a unified Hölder Lebesgue scale X^p and its weighted, higher order variants X^{k,p,a}. Proved a two-parameter interpolation theorem continuous in (k,1/p,a). Used Trudinger-Moser and localized weighted Hardy lemma for critical endpoint analysis.

Result: Established generalized CKN inequality on bounded punctured domains with precise characterization of constants depending on weight integrability at origin. At critical endpoint p=n, obtained CKN inequality with logarithmic loss. Derived several corollaries including unified Hardy-Sobolev inequality.

Conclusion: The framework successfully extends CKN inequality beyond classical Lebesgue spaces, providing interpolation mechanisms that bridge integrability and regularity. Constants depend only on structural parameters and coarse geometry of domains, with applications yielding unified inequalities.

Abstract: We develop a unified H\"older Lebesgue scale \(X^p\) and its weighted, higher
order variants \(X^{k,p,a}\) to extend the Caffarelli Kohn Nirenberg (CKN)
inequality beyond the classical Lebesgue regime. Within this framework we prove
a two parameter interpolation theorem that is continuous in the triplet
\((k,1/p,a)\) and bridges integrability and regularity across the Lebesgue
H\"older spectrum. As a consequence we obtain a generalized CKN inequality on
bounded punctured domains \(\Omega\subset\mathbb{R}^n\setminus\{0\}\); the
dependence of the constant on \(\Omega\) is characterized precisely by the
(non)integrability of the weights at the origin. At the critical endpoint
\(p=n\) we establish a localized, weighted Brezis Wainger type bound via
Trudinger Moser together with a localized weighted Hardy lemma, yielding an
endpoint CKN inequality with a logarithmic loss. Sharp constants are not
pursued; rather, we prove existence of constants depending only on the
structural parameters and coarse geometry of \(\Omega\). Several corollaries,
including a unified Hardy--Sobolev inequality, follow from the same
interpolation mechanism.

</details>


### [29] [The inhomogeneous fractional Dirichlet problem](https://arxiv.org/abs/2510.01055)
*Florian Grube*

Main category: math.AP

TL;DR: The paper studies boundary regularity for inhomogeneous Dirichlet problems involving 2s-stable operators in generalized Hölder spaces, providing sharp results with explicit counterexamples.


<details>
  <summary>Details</summary>
Motivation: To address boundary regularity for inhomogeneous Dirichlet problems directly, without subtracting extensions of exterior data, which is a new approach even for the fractional Laplacian.

Method: The authors study 2s-stable operators in generalized Hölder spaces and develop a direct approach to the inhomogeneous Dirichlet problem, creating explicit counterexamples to demonstrate sharpness.

Result: The paper establishes new boundary regularity results for inhomogeneous Dirichlet problems, with the findings being novel even for the fractional Laplacian case.

Conclusion: The approach provides sharp boundary regularity results for 2s-stable operators in generalized Hölder spaces, with explicit counterexamples confirming optimality, offering a new perspective on the inhomogeneous Dirichlet problem.

Abstract: We study boundary regularity for the inhomogeneous Dirichlet problem for
$2s$-stable operators in generalized H\"older spaces. Moreover, we provide
explicit counterexamples that showcase the sharpness of our results. Our
approach directly addresses the inhomogeneous Dirichlet problem, rather than
subtracting an appropriate extension of the exterior data. Even for the
fractional Laplacian, our result is new.

</details>


### [30] [The $L_p$ chord Minkowski problem for super-critical exponent](https://arxiv.org/abs/2510.01084)
*Shibing Chen,Qi-Rui Li,Yuanyuan Li*

Main category: math.AP

TL;DR: The paper solves the $L_p$ chord Minkowski problem for super-critical exponents using a nonlocal Gauss curvature flow and topological methods.


<details>
  <summary>Details</summary>
Motivation: To determine necessary and sufficient conditions for a finite Borel measure to be the $L_p$ chord measure of a convex body, as recently introduced by Lutwak, Xi, Yang and Zhang.

Method: Combines a nonlocal Gauss curvature flow from previous work with a topological argument, providing a simplified approach for the topological part.

Result: Successfully solves the $L_p$ chord Minkowski problem for super-critical exponents.

Conclusion: The combination of nonlocal flow and topological methods provides an effective solution to this geometric problem, with simplified topological reasoning.

Abstract: The $L_p$ chord Minkowski problem was recently introduced by Lutwak, Xi, Yang
and Zhang, which seeks to determine the necessary and sufficient conditions for
a given finite Borel measure such that it is the $L_p$ chord measure of a
convex body. In this paper, we solve the $L_p$ chord Minkowski problem for the
super-critical exponents by combining a nonlocal Gauss curvature flow
introduced in \cite{HHLW exi} and a topological argument developed in
\cite{GLW2022}. Notably, we provide a simplified argument for the topological
part.

</details>


### [31] [On the vanishing viscosity limit for incompressible flows with inflow/outflow boundary conditions](https://arxiv.org/abs/2510.01101)
*Anna L. Mazzucato,Dehua Wang,Wei Wei*

Main category: math.AP

TL;DR: The paper studies the vanishing viscosity limit for incompressible Navier-Stokes equations in bounded domains with inflow-outflow boundary conditions, proving convergence to Euler equations in energy norm.


<details>
  <summary>Details</summary>
Motivation: To extend previous work by allowing general injection and suction angles (bounded away from zero) and rigorously establish convergence from Navier-Stokes to Euler equations as viscosity vanishes.

Method: Uses construction of boundary layer correctors via Prandtl-type equations and a higher-order asymptotic expansion to improve convergence rates.

Result: Proves interior convergence in both L² and Sobolev H¹ norms at same rates as normal boundary case, establishing rigorous convergence of NSE solutions to Euler equations.

Conclusion: The vanishing viscosity limit holds for general inflow-outflow boundary conditions with non-zero injection/suction angles, with boundary layer analysis providing the key convergence mechanism.

Abstract: We study the vanishing viscosity limit for the incompressible Navier-Stokes
equations (NSE) in a general bounded domain with inflow-outflow boundary
conditions. Extending the work of Gie, Hamouda, and Temam ( Netw. Heterog.
Media 7, 2012) and also of Lombardo and Sammartino (SIAM J. Math. Anal. 33,
2001), we allow for a general injection and suction angle, as long as it is
bounded away from zero. We rigorously establish the convergence of NSE
solutions to those of the Euler equations (EE) as viscosity vanishes in the
energy norm. We prove interior convergence in both the $L^2$ and the Sobolev
$H^1$ norms at the same rates as in the case of injection/suction normal to the
boundary. The proof relies on the construction of boundary layer correctors via
Prandtl-type equations and a higher-order asymptotic expansion that improves
the convergence rate.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [32] [Hybrid Delta Tracking Schemes Using a Track-Length Estimator](https://arxiv.org/abs/2510.00152)
*Joanna Piper Morgan,Ilham Variansyah,Kayla B. Clements,Todd S. Palmer,Kyle E. Niemeyer*

Main category: physics.comp-ph

TL;DR: This paper introduces a delta-tracking algorithm with track-length estimator for Monte Carlo radiation transport, enabling hybrid surface-delta tracking methods and showing performance improvements in various reactor benchmarks.


<details>
  <summary>Details</summary>
Motivation: To develop improved delta-tracking algorithms for Monte Carlo radiation transport that can tally fluxes to structured meshes using track-length estimators, enabling hybrid methods and better performance.

Method: Developed delta-tracking algorithm with track-length estimator for structured rectilinear mesh, implemented hybrid-in-energy and hybrid-in-material methods, and tested with continuously moving surfaces on CPU/GPU systems.

Result: Delta tracking with track-length tally modestly improved figures of merit (1.5X-2.5X) for void regions. Standard delta tracking with collision estimator performed best for PWR benchmarks. Hybrid-in-energy methods showed significant improvements (7X-11X) for continuous energy reactor benchmark.

Conclusion: The new delta-tracking algorithms with track-length estimators enable effective hybrid methods and show performance improvements, particularly hybrid-in-energy methods for continuous energy reactor problems.

Abstract: In Monte Carlo radiation transport calculations, Woodcock-delta tracking is a
common alternative to the more popular surface tracking technique. In this work
we introduce a delta-tracking algorithm that tallies fluxes to a structured
rectilinear mesh using the track-length estimator. This development also
enables hybrid surface-delta tracking algorithms, because the track-length
tally can be used everywhere for scalar flux estimation regardless of which
tracking algorithm is employed. We use this tallying technique to develop a
novel hybrid-in-energy method. We also implement a hybrid-in-material method,
like what is implemented in Serpent2. We demonstrate that these delta tracking
algorithms can be used in conjunction with continuously moving surfaces. We
compare these methods showing figures of merit on four time-dependent problems
(multi-group and continuous energy) solved with CPU- and GPU-based computers.
Our implementation of delta tracking with a track length tally modestly
improves figures of merit compared to standard delta tracking with a collision
estimator and surface tracking with a track length estimator (1.5X 2.5X) for a
problem with significant void regions. For both multi-group and continuous
energy pressurized water reactor benchmarks, standard delta tracking with a
collision estimator performs best. Hybrid-in-energy methods show significant
improvements (7X-11X) for a continuous energy reactor benchmark problem.

</details>


### [33] [Instabilities and Phase Transformations in Architected Metamaterials: a Gradient-Enhanced Continuum Approach](https://arxiv.org/abs/2510.00227)
*Sarvesh Joshi,S. Mohammad Mousavi,Craig M. Hamel,Stavros Gaitanaros,Prashant K. Purohit,Ryan Alberdi,Nikolaos Bouklas*

Main category: physics.comp-ph

TL;DR: A nonlocal continuum framework for modeling elastic architected metamaterials that captures microstructural instabilities and phase transformations, enabling simulation of complex behaviors like densification fronts, hysteresis, and auxetic modes.


<details>
  <summary>Details</summary>
Motivation: Conventional continuum models struggle to capture macroscopic responses of architected metamaterials governed by microstructural instabilities, limiting scalability and predictive capability for phenomena like energy dissipation, large deformations, and auxetic behavior.

Method: Extends anisotropic hyperelasticity with nonlocal variables and internal length scales reflective of microstructure. Augments local polyconvex free-energy models with non-(poly)convex energies to enable metastable and bistable responses. Implemented in finite element framework using hybrid monolithic-staggered solution strategy.

Result: Successfully captures densification fronts, forward/reverse transformations, hysteresis loops, imperfection sensitivity, and globally coordinated auxetic modes in simulations.

Conclusion: Provides robust foundation for accelerated modeling of instability-driven phenomena in architected materials, with potential extensions to anisotropic, dissipative, active systems and integration with data-driven/machine learning approaches.

Abstract: Architected metamaterials such as foams and lattices exhibit a wide range of
properties governed by microstructural instabilities and emerging phase
transformations. Their macroscopic response--including energy dissipation
during impact, large recoverable deformations, morphing between configurations,
and auxetic behavior--remains difficult to capture with conventional continuum
models, which often rely on discrete approaches that limit scalability. We
propose a nonlocal continuum formulation that captures both stable and unstable
responses of elastic architected metamaterials. The framework extends
anisotropic hyperelasticity by introducing nonlocal variables and internal
length scales reflective of microstructural features. Local polyconvex
free-energy models are systematically augmented with two families of
non-(poly)convex energies, enabling both metastable and bistable responses.
Implementation in a finite element framework enables solution using a hybrid
monolithic--staggered strategy. Simulations capture densification fronts,
forward and reverse transformations, hysteresis loops, imperfection
sensitivity, and globally coordinated auxetic modes. Overall, this framework
provides a robust foundation for accelerated modeling of instability-driven
phenomena in architected materials, while enabling extensions to anisotropic,
dissipative, and active systems as well as integration with data-driven and
machine learning approaches.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [34] [Electron neural closure for turbulent magnetosheath simulations: energy channels](https://arxiv.org/abs/2510.00282)
*George Miloshevich,Luka Vranckx,Felipe Nathan de Oliveira Lopes,Pietro Dazzi,Giuseppe Arrò,Giovanni Lapenta*

Main category: physics.plasm-ph

TL;DR: A non-local five-moment electron pressure tensor closure using Fully Convolutional Neural Network (FCNN) is developed for magnetosheath turbulence simulations, outperforming local closures and showing good reconstruction of pressure-strain interactions.


<details>
  <summary>Details</summary>
Motivation: Electron pressure plays a crucial role in generalized Ohm's law and competes with electron inertia, making accurate pressure tensor closures essential for understanding energy channels in turbulent plasmas.

Method: Trained FCNN on representative simulations with fewer particles per cell and validated generalization to simulations with more particles per cell, focusing on pressure-strain interaction for energy channel analysis.

Result: The FCNN-learned equation of state significantly outperforms local closures (MLP and double adiabatic expressions), reconstructs overall spatial distribution of pressure-strain well, though misses some small-scale off-diagonal pressure tensor features.

Conclusion: The method shows favorable scaling with more training data, indicating potential for improvement in capturing small-scale features, which will be addressed in future work.

Abstract: In this work, we introduce a non-local five-moment electron pressure tensor
closure parametrized by a Fully Convolutional Neural Network (FCNN). Electron
pressure plays an important role in generalized Ohm's law, competing with
electron inertia. This model is used in the development of a surrogate model
for a fully kinetic energy-conserving semi-implicit Particle-in-Cell simulation
of decaying magnetosheath turbulence. We achieve this by training FCNN on a
representative set of simulations with a smaller number of particles per cell
and showing that our results generalise to a simulation with a large number of
particles per cell. We evaluate the statistical properties of the learned
equation of state, with a focus on pressure-strain interaction, which is
crucial for understanding energy channels in turbulent plasmas. The resulting
equation of state learned via FCNN significantly outperforms local closures,
such as those learned by Multi-Layer Perceptron (MLP) or double adiabatic
expressions. We report that the overall spatial distribution of pressure-strain
and its conditional averages are reconstructed well. However, some small-scale
features are missed, especially for the off-diagonal components of the pressure
tensor. Nevertheless, the results are substantially improved with more training
data, indicating favorable scaling and potential for improvement, which will be
addressed in future work.

</details>


### [35] [A study of the electronic and ionic structure, for co-existing states of fully and partially ionized hydrogen, using the neutral pseudo-atom method as well as a classical map for the electron subsystem](https://arxiv.org/abs/2510.00388)
*M. W. C. Dharma-wardana*

Main category: physics.plasm-ph

TL;DR: The paper reviews theoretical methods for studying warm-dense matter (WDM) hydrogen, showing that both fully and partially ionized states can coexist at the same density and temperature. It presents efficient computational approaches using classical-map schemes and one-atom DFT methods.


<details>
  <summary>Details</summary>
Motivation: To address the theoretical challenges in modeling electron-proton systems at temperatures comparable to the Fermi energy, particularly for warm-dense matter hydrogen, which is important for astrophysics and high-energy density physics.

Method: Uses the neutral-pseudo atom (NPA) method and classical map for quantum electrons to study fully and partially ionized hydrogen. Also employs path-integral Monte Carlo methods and N-atom Density Functional Theory (DFT) simulations.

Result: Shows that both fully and partially ionized states can independently exist at the same density and temperature in many cases. Demonstrates that structure data can be rapidly and inexpensively evaluated using classical-map schemes for fully ionized plasmas and one-atom DFT methods for partially ionized systems.

Conclusion: Efficient computational methods using classical-map schemes and one-atom DFT approaches provide sufficient accuracy for studying warm-dense matter hydrogen systems, enabling rapid evaluation of essential structure data needed for experimental diagnostics.

Abstract: Prof. Michael Bonitz and his collaborators have made seminal contributions to
the study of the uniform electron fluid and the electron-proton fluid, viz.,
hydrogen, in using {\it ab initio} simulations. These studies provide essential
inputs to astrophysics as well as high-energy density physics. This is
reflected in the contributions to this festshrift in his honour. The
electron-proton system becomes particularly difficult for theoretical modeling
when the temperature becomes comparable to the Fermi energy $E_F$, when the
warm-dense matter (WDM) state of hydrogen is reached. In this study we briefly
review the theoretical methods available for the study of WDM systems, and use
the neutral-pseudo atom (NPA) method, and a classical map for quantum electrons
to study fully and partially ionized hydrogen. It is shown that {\it both}
fully and partially ionized states can independently exist at the {\it same
density and temperature} in many cases. Recent studies using path-integral
Monte Carlo methods, and $N$-atom Density Functional Theory (DFT) simulations
have provided essential structure data including the electron-electron
structure factor $S_{ee}(k)$ that enters into interpretation of X-ray Thomson
scattering and other diagnostics. We show that these structure data can be
rapidly and inexpensively evaluated, with sufficient accuracy, using
classical-map schemes for fully ionized plasmas, and more generally, using
one-atom (average-atom) DFT methods for partially ionized systems.

</details>


### [36] [Correlation function metrology for warm dense matter: Recent developments and practical guidelines](https://arxiv.org/abs/2510.00493)
*Maximilian Peter Böhme,Willow Martin,Hannah Bellenbaum,Magaret Berrens,Jan Vorberger,Sebastian Schwalbe,Zhandos Moldabekov,Thomas Gawne,Sebastien Hamel,Brianna Aguilar-Solis,Abhiraj Sharma,Frank Graziani,Tilo Döppner,Siegfried Glenzer,Tobias Dornheim,David Bishel*

Main category: physics.plasm-ph

TL;DR: This paper provides a systematic overview of the imaginary-time formalism for X-ray Thomson scattering (XRTS) analysis, demonstrating practical applications for temperature inference and presenting new methods to determine absolute normalization, Rayleigh weight, and density without model assumptions.


<details>
  <summary>Details</summary>
Motivation: XRTS is valuable for diagnosing matter under extreme conditions, but the imaginary-time formalism is difficult to understand. The authors aim to make this powerful method more accessible and demonstrate its practical applications.

Method: The paper uses the imaginary-time formalism for XRTS analysis, providing systematic theoretical foundations and practical workflows. It includes methods for temperature inference, absolute normalization determination, Rayleigh weight extraction, and density estimation without relying on uncontrolled model assumptions.

Result: The authors demonstrate how to extract key observables from XRTS measurements using the imaginary-time formalism, including temperature, absolute normalization, Rayleigh weight, and density, all without requiring model-dependent assumptions.

Conclusion: The paper presents a unified workflow for extracting multiple key observables from XRTS measurements using the imaginary-time formalism, making this powerful diagnostic technique more practical and accessible for experimental applications.

Abstract: X-ray Thomson scattering (XRTS) has emerged as a valuable diagnostic for
matter under extreme conditions, as it captures the intricate many-body physics
of the probed sample. Recent advances, such as the model-free temperature
diagnostic of Dornheim et al. [Nat.Commun. 13, 7911 (2022)], have demonstrated
how much information can be extracted directly within the imaginary-time
formalism. However, since the imaginary-time formalism is a concept often
difficult to grasp, we provide here a systematic overview of its theoretical
foundations and explicitly demonstrate its practical applications to
temperature inference, including relevant subtleties. Furthermore, we present
recent developments that enable the determination of the absolute
normalization, Rayleigh weight, and density from XRTS measurements without
reliance on uncontrolled model assumptions. Finally, we outline a unified
workflow that guides the extraction of these key observables, offering a
practical framework for applying the method to interpret experimental
measurements.

</details>


### [37] [Do plasmoids induce fast magnetic reconnection in well-resolved current sheets in 2D MHD simulations?](https://arxiv.org/abs/2510.01060)
*G. H. Vicentin,G. Kowal,E. M. de Gouveia Dal Pino,A. Lazarian*

Main category: physics.plasm-ph

TL;DR: High-resolution 2D MHD simulations challenge the conventional belief that tearing-mode instability triggers plasmoid cascades enabling fast reconnection. Results show Sweet-Parker scaling persists up to S~10^4, with only slight enhancement to S^{-1/3} at higher S, indicating reconnection remains resistivity-dependent.


<details>
  <summary>Details</summary>
Motivation: To investigate whether tearing-mode instability truly enables fast reconnection independent of Lundquist number, challenging the conventional plasmoid cascade theory.

Method: Used highest resolution 2D magnetohydrodynamic simulations of reconnecting current sheets on uniform grid for Lundquist numbers 10^3 ≤ S ≤ 2×10^5.

Result: Sweet-Parker scaling (V_rec ~ S^{-1/2}) persists up to S~10^4. At higher S, plasmoid formation provides only slight enhancement (V_rec ~ S^{-1/3}), not resistivity independence. Plasmoids are rapidly advected out without forming cascade mergers.

Conclusion: Plasmoid formation's role in 2D high-S reconnection needs revision. Even if future studies show resistivity independence in 2D, extending to 3D astrophysical environments is not justified due to turbulence's dominant role at high Reynolds numbers.

Abstract: We investigate the development of tearing-mode instability using the highest
resolution two-dimensional magnetohydrodynamic simulations of reconnecting
current sheets on a uniform grid, for Lundquist numbers $10^3 \le S \le 2
\times 10^5$. Although the tearing-mode instability is commonly thought to
trigger a plasmoid cascade that enables fast reconnection - i.e., independent
of $S$ - our results, in broad agreement with the recent findings of Morillo \&
Alexakis (2025), challenge this belief. We demonstrate a Sweet-Parker scaling
of the reconnection rate $V_{\text{rec}} \sim S^{-1/2}$ up to Lundquist numbers
$S \sim 10^4$. For larger values, plasmoid formation sets in leading to a
slight enhancement of the reconnection rate, $V_{\text{rec}} \sim S^{-1/3}$,
consistent with the prediction from linear tearing mode induced reconnection,
indicating that reconnection remains resistivity dependent, and therefore slow.
In our simulations, the plasmoids do not form a cascade of mergers, as they are
rapidly advected out of the reconnection layer. Our findings call for the
revision of the role of plasmoid formation in 2D high Lundquist number magnetic
reconnection. Even if future studies demonstrate that 2D plasmoid-reconnection
becomes resistivity-independent at sufficiently large $S$, directly extending
those results to 3D astrophysical environments is not justified, as in
realistic circumstances, the increase of $S$ also raises the Reynolds number of
the outflows, making it essential to account for the dominant role of
turbulence.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [38] [A semi-Lagrangian method for solving state constraint Mean Field Games in Macroeconomics](https://arxiv.org/abs/2510.00768)
*Fabio Camilli,Qing Tang,Yong-shen Zhou*

Main category: math.OC

TL;DR: The paper analyzes continuous-time heterogeneous agent models in the Aiyagari-Bewley-Huggett framework using Mean Field Games, coupling HJB and FPK equations with numerical solution methods.


<details>
  <summary>Details</summary>
Motivation: To study continuous-time heterogeneous agent models in the Aiyagari-Bewley-Huggett framework using Mean Field Games approach, addressing the coupling between individual optimization and wealth distribution dynamics.

Method: Establishes comparison principle for constrained viscosity solutions of HJB equation, proposes semi-Lagrangian scheme for numerical solution with convergence proof via Barles-Souganidis method, uses policy iteration for state constraints, and dual SL scheme for FPK equation.

Result: Develops fully discrete, implementable numerical methods for solving the coupled HJB-FPK system in continuous-time heterogeneous agent models.

Conclusion: The paper provides rigorous mathematical foundations and practical numerical methods for analyzing continuous-time heterogeneous agent models within the Mean Field Games framework, with proven convergence properties.

Abstract: We study continuous-time heterogeneous agent models cast as Mean Field Games,
in the Aiyagari-Bewley-Huggett framework. The model couples a
Hamilton-Jacobi-Bellman equation for individual optimization with a
Fokker-Planck-Kolmogorov equation for the wealth distribution. We establish a
comparison principle for constrained viscosity solutions of the HJB equation
and propose a semi-Lagrangian (SL) scheme for its numerical solution, proving
convergence via the Barles-Souganidis method. A policy iteration algorithm
handles state constraints, and a dual SL scheme is used for the FPK equation.
Numerical methods are presented in a fully discrete, implementable form.

</details>


### [39] [Digital Twins: McKean-Pontryagin Control for Partially Observed Physical Twins](https://arxiv.org/abs/2510.00937)
*Manfred Opper,Sebastian Reich*

Main category: math.OC

TL;DR: This paper proposes a novel approach for optimal control of partially observed diffusion processes by combining ensemble Kalman filter data assimilation with the McKean-Pontryagin stochastic optimal control method, enabling real-time digital twin applications.


<details>
  <summary>Details</summary>
Motivation: While optimal control for fully observed diffusion processes is well-established, there are fewer algorithms available for partially observed processes, which is crucial for digital twin paradigms where physical systems are only partially observed and control is derived from digital models.

Method: The methodology combines data assimilation using ensemble Kalman filter with the McKean-Pontryagin approach to stochastic optimal control, deriving forward-evolving mean-field evolution equations for states and co-states that enable simultaneous online data assimilation and control law computation.

Result: The proposed method is demonstrated through numerical experiments on a controlled Lorenz-63 system and an inverted pendulum, showing its effectiveness for real-time applications.

Conclusion: The combined approach of ensemble Kalman filter data assimilation with McKean-Pontryagin optimal control provides a practical solution for real-time digital twin applications where partial observations and online control computation are required.

Abstract: Optimal control for fully observed diffusion processes is well established
and has led to numerous numerical implementations based on, for example,
Bellman's principle, model free reinforcement learning, Pontryagin's maximum
principle, and model predictive control. On the contrary, much fewer algorithms
are available for optimal control of partially observed processes. However,
this scenario is central to the digital twin paradigm where a physical twin is
partially observed and control laws are derived based on a digital twin. In
this paper, we contribute to this challenge by combining data assimilation in
the form of the ensemble Kalman filter with the recently proposed
McKean-Pontryagin approach to stochastic optimal control. We derive forward
evolving mean-field evolution equations for states and co-states which
simultaneously allow for an online assimilation of data as well as an online
computation of control laws. The proposed methodology is therefore perfectly
suited for real time applications of digital twins. We present numerical
results for a controlled Lorenz-63 system and an inverted pendulum.

</details>


### [40] [A first-order method for constrained nonconvex--nonconcave minimax problems under a local Kurdyka-Łojasiewicz condition](https://arxiv.org/abs/2510.01168)
*Zhaosong Lu,Xiangyuan Wang*

Main category: math.OC

TL;DR: The paper proposes an inexact proximal gradient method for constrained nonconvex-nonconcave minimax problems, using sequential convex programming to compute gradients and establishing convergence under local KL conditions.


<details>
  <summary>Details</summary>
Motivation: To address constrained nonconvex-nonconcave minimax problems where the inner maximization involves complex constraints, which are challenging due to their non-smooth nature and lack of standard convexity assumptions.

Method: Developed an inexact proximal gradient method where the gradient of the maximal function is computed via sequential convex programming applied to locally KL-structured subproblems.

Result: Established that the maximal function enjoys local Hölder smoothness under local KL conditions, and provided complexity guarantees for computing approximate stationary points.

Conclusion: The proposed method effectively handles constrained minimax problems with complex inner constraints by leveraging local KL structure and sequential convex programming, with proven convergence rates.

Abstract: We study a class of constrained nonconvex--nonconcave minimax problems in
which the inner maximization involves potentially complex constraints. Under
the assumption that the inner problem of a novel lifted minimax problem
satisfies a local Kurdyka-{\L}ojasiewicz (KL) condition, we show that the
maximal function of the original problem enjoys a local H\"older smoothness
property. We also propose a sequential convex programming (SCP) method for
solving constrained optimization problems and establish its convergence rate
under a local KL condition. Leveraging these results, we develop an inexact
proximal gradient method for the original minimax problem, where the inexact
gradient of the maximal function is computed via the SCP method applied to a
locally KL-structured subproblem. Finally, we establish complexity guarantees
for the proposed method in computing an approximate stationary point of the
original minimax problem.

</details>


### [41] [Control of Conservation Laws in the Nonlocal-to-Local Limit](https://arxiv.org/abs/2510.00677)
*Jan Friedrich,Michael Herty,Claudia Nocita*

Main category: math.OC

TL;DR: Analysis of control problems where initial data serves as control and state is given by entropy solutions of conservation laws via nonlocal-to-local limiting approach.


<details>
  <summary>Details</summary>
Motivation: To study control problems involving conservation laws where initial conditions act as controls, and to understand the relationship between nonlocal and local control formulations.

Method: Nonlocal-to-local limiting strategy and Eulerian-Lagrangian scheme for discrete analysis.

Result: Characterization of limit minimizers from nonlocal to local control problems, with analogous discrete results.

Conclusion: Minimizers of nonlocal control problems converge to minimizers of corresponding local ones, with consistent behavior at discrete level.

Abstract: We analyze a class of control problems where the initial datum acts as a
control and the state is given by the entropy solution of (local) conservation
laws by a nonlocal-to-local limiting strategy. In particular we characterize
the limit up to subsequence of minimizers to nonlocal control problems as
minimizer of the corresponding local ones. Moreover, we also prove an analogous
result at a discrete level by means of a Eulerian-Lagrangian scheme.

</details>


### [42] [Funnel control for passive infinite-dimensional systems](https://arxiv.org/abs/2510.01027)
*Thavamani Govindaraj,Anthony Hastir,Lassi Paunonen,Timo Reis*

Main category: math.OC

TL;DR: Funnel control for linear infinite-dimensional impedance passive systems using system node approach, with applications to Euler-Bernoulli beam in boundary and distributed control scenarios.


<details>
  <summary>Details</summary>
Motivation: To extend funnel control methodology to linear infinite-dimensional systems that are impedance passive, which satisfy energy balance conditions with stored energy as squared state norm and supplied power as input-output inner product.

Method: Employ the system node approach as a unified framework for infinite-dimensional systems with boundary and distributed control and observation. Analyze closed-loop dynamics governed by nonlinear evolution equations and establish solvability.

Result: Successfully established solvability of the nonlinear evolution equation governing closed-loop dynamics, demonstrating applicability of funnel control to impedance passive infinite-dimensional systems.

Conclusion: Funnel control is applicable to linear infinite-dimensional impedance passive systems, as validated through analysis of Euler-Bernoulli beam examples with both boundary and distributed control configurations.

Abstract: We consider funnel control for linear infinite-dimensional systems that are
impedance passive, meaning that they satisfy an energy balance in which the
stored energy equals the squared norm of the state and the supplied power is
the inner product of input and output. For the analysis we employ the system
node approach, which offers a unified framework for infinite-dimensional
systems with boundary and distributed control and observation. The resulting
closed-loop dynamics are governed by a nonlinear evolution equation; we
establish its solvability and hence the applicability of funnel control to this
class. The applicability is illustrated by an Euler-Bernoulli beam, which is
studied in two distinct scenarios: once with boundary control and once with
distributed control.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [43] [A viscosity solution approach to the large deviation principle for stochastic convective Brinkman-Forchheimer equations](https://arxiv.org/abs/2510.01166)
*Sagar Gautam,Manil T. Mohan*

Main category: math.PR

TL;DR: Develops viscosity solution approach for large deviation principle of 2D/3D stochastic convective Brinkman-Forchheimer equations with small noise, establishing Laplace principle without Bryc's theorem and deriving exponential moment bounds.


<details>
  <summary>Details</summary>
Motivation: To establish large deviation principles for stochastic convective Brinkman-Forchheimer equations using viscosity solution methods, avoiding reliance on additional sufficient conditions like Bryc's theorem.

Method: Uses Varadhan and Bryc framework with Feng's techniques, employs comparison principle to identify Laplace limit as convergence of viscosity solution of associated Hamilton-Jacobi-Bellman equation.

Result: Establishes large deviation principle in Skorohod space and continuous space via C-exponential tightness; derives exponential moment bounds for r>3 and r=3 with 2βμ≥1 without orthogonality condition.

Conclusion: Viscosity solution approach successfully establishes large deviation principle for stochastic convective Brinkman-Forchheimer equations, providing advantages over traditional methods that require additional sufficient conditions.

Abstract: This article develops the viscosity solution approach to the large deviation
principle for the following two- and three-dimensional stochastic convective
Brinkman-Forchheimer equations on the torus $\mathbb{T}^d,\ d\in\{2,3\}$ with
small noise intensity:
  \begin{align*}
  \mathrm{d}\boldsymbol{u}_n+[-\mu\Delta\boldsymbol{u}_n+
(\boldsymbol{u}_n\cdot\nabla)\boldsymbol{u}_n
+\alpha\boldsymbol{u}_n+\beta|\boldsymbol{u}_n|^{r-1}\boldsymbol{u}_n+\nabla
p_n]\mathrm{d} t=\boldsymbol{f}\mathrm{d}
t+\frac{1}{\sqrt{n}}\mathrm{Q}^{\frac12}\mathrm{d}\mathrm{W}, \
\nabla\cdot\boldsymbol{u}_n=0,
  \end{align*} where $\mu,\alpha,\beta>0$, $r\in[1,\infty)$, $\mathrm{Q}$ is a
trace class operator and $\mathrm{W}$ is Hilbert-valued calendrical Wiener
process. We build our analysis on the framework of Varadhan and Bryc, together
with the techniques of [J. Feng et.al., Large Deviations for Stochastic
Processes, American Mathematical Society (2006) vol. \textbf{131}]. By
employing the techniques from the comparison principle, we identify the Laplace
limit as the convergence of the viscosity solution of the associated
second-order singularly perturbed Hamilton-Jacobi-Bellman equation. A key
advantage of this method is that it establishes a Laplace principle without
relying on additional sufficient conditions such as Bryc's theorem, which the
literature commonly requires. For $r>3$ and $r=3$ with $2\beta\mu\geq1$, we
also derive the exponential moment bounds without imposing the classical
orthogonality condition
$((\boldsymbol{u}_n\cdot\nabla)\boldsymbol{u}_n,\mathrm{A}\boldsymbol{u}_n)=0$,
where $\mathrm{A}=-\Delta$, in both two-and three-dimensions. We first
establish the large deviation principle in the Skorohod space. Then, by using
the $\mathrm{C}-$exponential tightness, we finally establish the large
deviation principle in the continuous space.

</details>


<div id='nucl-th'></div>

# nucl-th [[Back]](#toc)

### [44] [Bulk and spectroscopic nuclear properties within an ab initio renormalized random-phase approximation framework](https://arxiv.org/abs/2510.00878)
*Radek Folprecht,František Knapp,Giovanni De Gregorio,Riccardo Mancino,Petr Veselý,Nicola Lo Iudice*

Main category: nucl-th

TL;DR: A chiral potential with three-body force is used in a particle-hole renormalized RPA scheme to study closed-shell nuclei properties, removing QBA instabilities and improving experimental agreement.


<details>
  <summary>Details</summary>
Motivation: To address instabilities in traditional RPA methods and improve consistency with experimental data for nuclear properties.

Method: Particle-hole renormalized random-phase approximation (RRPA) using Hartree-Fock single-particle basis with modern chiral potential including three-body force.

Result: All quasiboson approximation instabilities are removed and better consistency with experiments achieved for all observables of investigated closed-shell nuclei.

Conclusion: The approach successfully addresses RPA limitations but residual discrepancies indicate need to go beyond particle-hole space.

Abstract: A modern chiral potential incorporating the three-body force is adopted to
investigate bulk properties, spectra, and nuclear responses of
closed-(sub)shell nuclei throughout the nuclear chart within a particle-hole
(p-h) renormalized random-phase approximation (RRPA) scheme using a Hartree-
Fock (HF) single-particle basis. Our analysis shows that all instabilities
induced by the quasiboson approximation (QBA) underlying RPA are removed and an
overall better consistency with the experiments is achieved for all observables
of the investigated nuclei. The residual discrepancies point out the need of
going beyond the p-h space.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [45] [Mathematical and numerical analysis of quantum signal processing](https://arxiv.org/abs/2510.00443)
*Lin Lin*

Main category: quant-ph

TL;DR: This paper surveys recent advances in quantum signal processing (QSP), the mathematical foundation of quantum singular value transformation (QSVT), focusing on generalizations beyond polynomials, computational complexity of phase factor evaluation algorithms, and numerical stability.


<details>
  <summary>Details</summary>
Motivation: QSP is fundamental to QSVT, one of the most important quantum algorithms with broad applications in scientific computing including Hamiltonian simulation, linear systems, and eigenvalue problems.

Method: The analysis involves mathematical and numerical examination of QSP, exploring connections with nonlinear Fourier analysis on SU(2), fast polynomial multiplications, and Gaussian elimination for matrices with displacement structure.

Result: The survey reveals unexpected interconnections between QSP and various mathematical techniques, providing insights into computational complexity and numerical stability of phase factor evaluation algorithms.

Conclusion: Recent advances in QSP analysis demonstrate its deep mathematical connections and provide improved understanding of computational aspects crucial for practical quantum algorithm implementation.

Abstract: Quantum signal processing (QSP) provides a representation of scalar
polynomials of degree $d$ as products of matrices in $\mathrm{SU}(2)$,
parameterized by $(d+1)$ real numbers known as phase factors. QSP is the
mathematical foundation of quantum singular value transformation (QSVT), which
is often regarded as one of the most important quantum algorithms of the past
decade, with a wide range of applications in scientific computing, from
Hamiltonian simulation to solving linear systems of equations and eigenvalue
problems. In this article we survey recent advances in the mathematical and
numerical analysis of QSP. In particular, we focus on its generalization beyond
polynomials, the computational complexity of algorithms for phase factor
evaluation, and the numerical stability of such algorithms. The resolution to
some of these problems relies on an unexpected interplay between QSP, nonlinear
Fourier analysis on $\mathrm{SU}(2)$, fast polynomial multiplications, and
Gaussian elimination for matrices with displacement structure.

</details>


### [46] [Combining Error Detection and Mitigation: A Hybrid Protocol for Near-Term Quantum Simulation](https://arxiv.org/abs/2510.01181)
*Dawei Zhong,William Munizzi,Huo Chen,Wibe Albert de Jong*

Main category: quant-ph

TL;DR: A hybrid error suppression protocol combining Pauli twirling, probabilistic error cancellation, and quantum error detecting codes to improve performance of noisy quantum circuits without full fault tolerance.


<details>
  <summary>Details</summary>
Motivation: Quantum error correction requires fault-tolerant hardware, while error mitigation offers practical improvements for near-term quantum devices without full fault tolerance requirements.

Method: Developed a hybrid protocol integrating Pauli twirling, probabilistic error cancellation, and the [[n, n-2, 2]] quantum error detecting code. Modified Pauli twirling to reduce overhead by lowering the number of Pauli operators, and applied probabilistic error cancellation at the end of encoded circuits to remove undetectable errors.

Result: Demonstrated the protocol on a non-Clifford variational quantum eigensolver circuit estimating the ground state energy of H₂ using both qiskit AerSimulator and IBM quantum processor ibm_brussels.

Conclusion: The hybrid error suppression protocol effectively improves quantum circuit performance on near-term hardware by combining multiple error mitigation techniques with reduced overhead.

Abstract: Practical implementation of quantum error correction is currently limited by
near-term quantum hardware. In contrast, quantum error mitigation has
demonstrated strong promise for improving the performance of noisy quantum
circuits without the requirement of full fault tolerance. In this work, we
develop a hybrid error suppression protocol that integrates Pauli twirling,
probabilistic error cancellation, and the $[[n, n-2, 2]]$ quantum error
detecting code. In addition, to reduce overhead from error mitigation
components of our method, we modify Pauli twirling by lowering the number of
Pauli operators in the twirling set, and apply probabilistic error cancellation
at the end of the encoded circuit to remove undetectable errors. Finally, we
demonstrate our protocol on a non-Clifford variational quantum eigensolver
circuit that estimates the ground state energy of $\rm H_2$ using both
\texttt{qiskit} AerSimulator and the IBM quantum processor
\texttt{ibm\_brussels}.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [47] [Spectral minimal partitions of unbounded domains](https://arxiv.org/abs/2510.00811)
*Matthias Hofmann,James B. Kennedy,Hugo Tavares*

Main category: math.SP

TL;DR: This paper studies k-spectral minimal partitions for domains (including unbounded ones) using p-norms of Schrödinger operator eigenvalues, proving existence results and threshold behaviors.


<details>
  <summary>Details</summary>
Motivation: To extend spectral partition theory to unbounded domains and infinite volume cases, addressing new phenomena that arise when domains are not compact.

Method: Uses concentration-compactness arguments to prove existence of optimal partitions below a threshold energy level, and analyzes different behaviors for p<∞ and p=∞ cases.

Result: Proves sharp upper bounds for k-partition energies, shows existence of minimizing partitions below threshold, and demonstrates varied behaviors at threshold level depending on p-norm.

Conclusion: Spectral minimal partitions exhibit complex behaviors in unbounded domains, with different threshold phenomena for finite vs infinite p-norms, and equipartition properties depend on energy level.

Abstract: We study the problem of constructing $k$-spectral minimal partitions of
domains in $d$ dimensions, where the energy functional to be minimized is a
$p$-norm ($1 \le p \le \infty$) of the infimum of the spectrum of a suitable
Schr\"odinger operator $-\Delta +V$, with Dirichlet conditions on the boundary
of the partition elements (cells). The main novelty of this paper is that the
domains may be unbounded, including of infinite volume.
  First, we prove a sharp upper bound for the infimal energy among all
$k$-partitions by a threshold value which involves the infimum $\Sigma$ of the
essential spectrum of the Schr\"odinger operator on the whole domain as well as
the infimal energy among all $k-1$-partitions. Strictly below such threshold,
we develop a concentration-compactness-type argument showing optimal partitions
exist, and each cell admits ground states (i.e., the infimum of the spectrum on
each cell is a simple isolated eigenvalue).
  Second, for $p<\infty$, when the energy and the threshold level coincide, we
show there may or may not be minimizing partitions. Moreover, even when these
exist, they may not have ground states.
  Third, for $p=\infty$, minimal partitions always exist, even at the threshold
level, but these may or may not admit ground states. Moreover, below the
threshold, we can always construct a minimizer, which is an equipartition. At
the threshold value we show that spectral minimal partitions may not need to be
equipartitions.
  We give a variety of examples of both domains and potentials to illustrate
the new phenomena that occur in this setting.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [48] [Approximation of differential entropy in Bayesian optimal experimental design](https://arxiv.org/abs/2510.00734)
*Chuntao Chen,Tapio Helin,Nuutti Hyvönen,Yuya Suzuki*

Main category: stat.ML

TL;DR: A computational approach for estimating expected information gain in Bayesian optimal experimental design that separates likelihood entropy evaluation from evidence approximation, achieving efficient convergence rates.


<details>
  <summary>Details</summary>
Motivation: Address computational challenges in large-scale inference problems like inverse problems where likelihood evaluations are expensive, by reducing the problem to maximum entropy estimation.

Method: Approximate evidence density using Monte Carlo or quasi-Monte Carlo surrogates while evaluating differential entropy using standard methods without additional likelihood evaluations.

Result: Achieves convergence rates comparable to or better than state-of-the-art methods for full expected information gain estimation, particularly when entropy evaluation cost is negligible.

Conclusion: The proposed approach provides an efficient computational strategy for Bayesian optimal experimental design with mild smoothness requirements and avoids stronger technical assumptions of earlier work.

Abstract: Bayesian optimal experimental design provides a principled framework for
selecting experimental settings that maximize obtained information. In this
work, we focus on estimating the expected information gain in the setting where
the differential entropy of the likelihood is either independent of the design
or can be evaluated explicitly. This reduces the problem to maximum entropy
estimation, alleviating several challenges inherent in expected information
gain computation.
  Our study is motivated by large-scale inference problems, such as inverse
problems, where the computational cost is dominated by expensive likelihood
evaluations. We propose a computational approach in which the evidence density
is approximated by a Monte Carlo or quasi-Monte Carlo surrogate, while the
differential entropy is evaluated using standard methods without additional
likelihood evaluations. We prove that this strategy achieves convergence rates
that are comparable to, or better than, state-of-the-art methods for full
expected information gain estimation, particularly when the cost of entropy
evaluation is negligible. Moreover, our approach relies only on mild smoothness
of the forward map and avoids stronger technical assumptions required in
earlier work. We also present numerical experiments, which confirm our
theoretical findings.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [49] [Frequent, disjoint hypercyclicity and strong topological transitivity of generalized weighted shift operators on Hilbert C-modules](https://arxiv.org/abs/2510.00445)
*Song-Ung Ri,Hyon-Hui Ju,Jin-Myong Kim*

Main category: math.FA

TL;DR: Study of dynamical properties for generalized bilateral weighted shift operators on Hilbert C*-modules over compact operators


<details>
  <summary>Details</summary>
Motivation: To investigate various dynamical properties including hypercyclicity, chaos, and transitivity for shift operators in the context of Hilbert C*-modules

Method: Analysis via Furstenberg family F for generalized bilateral weighted shift operators on standard Hilbert C*-module over C*-algebra of compact operators

Result: Not specified in abstract

Conclusion: Not specified in abstract

Abstract: In this paper we study some dynamical properties such as Frequent
Hypercyclicity Criterion, chaos, disjoint hypercyclicity and F-transitivity via
Furstenberg family F for generalized bilateral weighted shift operator on the
standard Hilbert C-module over C-algebra of compact operators on a separable
Hilbert space.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [50] [Kinetic closure of turbulence](https://arxiv.org/abs/2510.00779)
*Francesco Marson,Orestis Malaspinas*

Main category: physics.flu-dyn

TL;DR: A kinetic closure for the filtered Boltzmann-BGK equation that provides an alternative turbulence description without explicit subfilter stress modeling, improving stability and reducing dissipation compared to Smagorinsky model.


<details>
  <summary>Details</summary>
Motivation: To develop an alternative turbulence description that naturally incorporates turbulent subfilter stress without explicit modeling, avoiding scale separation and Smagorinsky-type assumptions.

Method: Generalizes the BGK collision operator to account for subfilter turbulent diffusion in nonconserved moments, using Chapman-Enskog analysis and lattice Boltzmann simulations for validation.

Result: The model converges exactly to filtered Navier-Stokes equations in hydrodynamic limit, with velocity gradients isolating subfilter contributions. Validations show improved stability and reduced dissipation.

Conclusion: The kinetic closure provides a viable alternative to traditional turbulence models, naturally handling subfilter stresses without explicit modeling and demonstrating superior performance in benchmark tests.

Abstract: This letter presents a kinetic closure of the filtered Boltzmann-BGK
equation, paving the way towards an alternative description of turbulence. The
closure naturally incorporates the turbulent subfilter stress tensor without
the need for explicit modeling, unlike in Navier-Stokes models. In contrast, it
accounts for the subfilter turbulent diffusion in the nonconserved moments by
generalizing the BGK collision operator. The model requires neither scale
separation nor a Smagorinsky-type ansatz for the subfilter stress tensor. The
Chapman-Enskog analysis shows that its hydrodynamic limit converges exactly to
the filtered Navier-Stokes equations, with velocity gradients isolating
subfilter contributions. Validations through lattice Boltzmann simulations of
the Taylor-Green vortex and the turbulent mixing layer demonstrate improved
stability and reduced dissipation, benchmarked against the Smagorinsky model.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [51] [A Bayesian Characterization of Ensemble Kalman Updates](https://arxiv.org/abs/2510.00158)
*Frederic J. N. Jorgensen,Youssef M. Marzouk*

Main category: math.ST

TL;DR: The paper analyzes the Ensemble Kalman Update (EnKU) and characterizes it as the unique affine conditioning map that achieves exact Bayesian inference for a broad class of distributions beyond Gaussians.


<details>
  <summary>Details</summary>
Motivation: To understand why the Ensemble Kalman Update is preferred among infinitely many possible affine conditioning maps for Bayesian inference, and to characterize its exactness properties beyond Gaussian distributions.

Method: Mathematical analysis of affine conditioning maps, characterization of the exactness set E^EnKU where EnKU yields exact conditioning, and comparison with the maximal possible exactness set F for affine transports.

Result: EnKU's exactness set is larger than Gaussian family, and EnKU is the unique exact affine conditioning map for most distributions in E^EnKU. The EnKU's exactness set is almost maximal, differing from the maximal set F only by a small symmetry class S_nl-dec.

Conclusion: The Ensemble Kalman Update is near-optimal among affine transports for exact conditioning beyond Gaussians and is uniquely exact for most measures in the maximal possible exactness set.

Abstract: The update in the Ensemble Kalman Filter (EnKF), called the Ensemble Kalman
Update (EnKU), is widely used for Bayesian inference in inverse problems and
data assimilation. At each filtering step, it approximates the solution to a
likelihood-free Bayesian inversion from an ensemble of particles
$(X_i,Y_i)\sim\pi$ sampled from a joint measure $\pi$ and an observation
$y_\ast\in\mathbb{R}^m$. The posterior ${\pi}_{X|Y=y_\ast}$ is approximated by
transporting $(X_i,Y_i)$ through an affine map
$L^{\mathrm{EnKU}}_{y_\ast}(x,y)$ determined by the Kalman gain. While the EnKU
is exact for Gaussian joints $\pi$ in the mean-field limit, exactness alone
does not fix the update: infinitely many affine maps $L_{y_\ast}$ push a
Gaussian $\pi$ to $\pi_{X|Y=y_\ast}$. This raises a question: which affine map
should estimate the posterior? We provide a characterization of the EnKU among
all such maps. First, we describe the set $\mathrm{E}^{\mathrm{EnKU}}$ of laws
where the EnKU yields exact conditioning, showing it is larger than the
Gaussian family. Next, we prove that, except for a small class of highly
symmetric distributions in $\mathrm{E}^{\mathrm{EnKU}}$ (including Gaussians),
the EnKU is the unique exact affine conditioning map. Finally, we ask for the
largest possible set $\mathrm{F}$ where any measure-dependent affine transport
could be exact; after characterizing $\mathrm{F}$, we show the EnKU's exactness
set is almost maximal:
$\mathrm{F}=\mathrm{E}^{\mathrm{EnKU}}\cup\mathrm{S}_{\mathrm{nl-dec}}$, where
$\mathrm{S}_{\mathrm{nl-dec}}$ is a small symmetry class. Thus, among affine
transports, the EnKU is near-optimal for exact conditioning beyond Gaussians
and is the unique affine update achieving exactness for any measure in
$\mathrm{F}$ except a subclass of strongly symmetric laws.

</details>


### [52] [Zero variance self-normalized importance sampling via estimating equations](https://arxiv.org/abs/2510.00389)
*Art B. Owen*

Main category: math.ST

TL;DR: This paper develops a zero variance solution for self-normalized importance sampling using Fieller's technique and positivisation strategy applied to estimating equations.


<details>
  <summary>Details</summary>
Motivation: Self-normalized importance sampling uses ratio estimates where the optimal sampler cannot achieve zero variance, unlike ordinary importance sampling. The authors aim to develop a method that can approach zero variance.

Method: The method writes the desired expectation as the zero of an estimating equation using Fieller's technique, then applies the positivisation strategy to the estimating equation.

Result: The paper provides conditions for existence and uniqueness of the sample solution, conditions for consistency and asymptotic normality, and an expression for asymptotic variance. The sample size multiplied by variance becomes arbitrarily close to zero for certain sampling strategies.

Conclusion: The proposed method achieves a zero variance solution for self-normalized importance sampling, overcoming the limitation that traditional approaches cannot approach zero variance.

Abstract: In ordinary importance sampling with a nonnegative integrand there exists an
importance sampling strategy with zero variance. Practical sampling strategies
are often based on approximating that optimal solution, potentially approaching
zero variance. There is a positivisation extension of that method to handle
integrands that take both positive and negative values. Self-normalized
importance sampling uses a ratio estimate, for which the optimal sampler does
not have zero variance and so zero variance cannot even be approached in
practice. Strategies that separately estimate the numerator and denominator of
that ratio can approach zero variance. This paper develops another zero
variance solution for self-normalized importance sampling. The first step is to
write the desired expectation as the zero of an estimating equation using
Fieller's technique. Then we apply the positivisation strategy to the
estimating equation. This paper give conditions for existence and uniqueness of
the sample solution to the estimating equation. Then it give conditions for
consistency and asymptotic normality and an expression for the asymptotic
variance. The sample size multiplied by the variance of the asymptotic formula
becomes arbitrarily close to zero for certain sampling strategies.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [53] [The exterior derivative and the mean value equality in $\mathbb{R}^n$](https://arxiv.org/abs/2510.00999)
*Daniel Fadel,Henrique N. Sá Earp,Tomás S. R. Silva*

Main category: math.DG

TL;DR: A survey that reinterprets the exterior derivative as infinitesimal flux, leading to a higher-dimensional Mean Value Theorem and a simplified Stokes' theorem formulation. Also proposes a mesh-free numerical algorithm for exterior differentiation.


<details>
  <summary>Details</summary>
Motivation: To provide a fresh perspective on classical vector calculus by generalizing the concept of exterior derivative as a measure of infinitesimal flux, and to develop practical computational tools that don't require mesh discretization.

Method: Reinterprets exterior derivative through the lens of infinitesimal flux, establishes a higher-dimensional analogue of the Mean Value Theorem for differential k-forms, and develops a numerical algorithm for exterior differentiation using only black-box access to differential forms.

Result: Achieves a natural formulation of Stokes' theorem that mirrors the Fundamental Theorem of Calculus without requiring full C^1 smoothness, and creates a practical computational algorithm that operates without mesh discretization or symbolic expressions.

Conclusion: The infinitesimal flux perspective provides a unified framework for classical vector calculus results and enables practical numerical computation of exterior derivatives in R^n through black-box access to differential forms.

Abstract: This survey revisits classical results in vector calculus and analysis by
exploring a generalised perspective on the exterior derivative, interpreting it
as a measure of "infinitesimal flux". This viewpoint leads to a
higher-dimensional analogue of the Mean Value Theorem, valid for differential
$k$-forms, and provides a natural formulation of Stokes' theorem that mirrors
the exact hypotheses of the Fundamental Theorem of Calculus - without requiring
full $C^1$ smoothness of the differential form.
  As a numerical application, we propose an algorithm for exterior
differentiation in $\mathbb{R}^n$ that relies solely on black-box access to the
differential form, offering a practical tool for computation without the need
for mesh discretization or explicit symbolic expressions.

</details>


### [54] [Uniqueness of the asymptotic limits for Ricci-flat manifolds with linear volume growth](https://arxiv.org/abs/2510.00420)
*Zetian Yan,Xingyu Zhu*

Main category: math.DG

TL;DR: The paper establishes uniqueness of asymptotic limits and exponential convergence for complete noncollapsed Ricci-flat manifolds with linear volume growth, which only admit cylindrical asymptotic limits. In dimension 4, these results are unconditional.


<details>
  <summary>Details</summary>
Motivation: To prove uniqueness and exponential convergence for asymptotically cylindrical Calabi-Yau manifolds, answering a question by Haskins-Hein-Nordström, and strengthening previous results in dimension 4.

Method: Using natural assumptions on curvature and cross section to establish uniqueness of asymptotic limits and exponential convergence rate for complete noncollapsed Ricci-flat manifolds with linear volume growth.

Result: Proved uniqueness of asymptotic limits and exponential convergence rate for such manifolds. In dimension 4, these results hold automatically without additional assumptions.

Conclusion: All asymptotically cylindrical Calabi-Yau manifolds converge exponentially to their asymptotic limits, confirming Haskins-Hein-Nordström's question and strengthening Chen-Chen's results in dimension 4.

Abstract: Under natural assumptions on curvature and cross section, we establish the
uniqueness of asymptotic limits and the exponential convergence rate for
complete noncollapsed Ricci-flat manifolds with linear volume growth, which are
known to only admit cylindrical asymptotic limits. In dimension four, these
assumptions hold automatically, yielding unconditional uniqueness and
convergence. In particular, our results show that all asymptotically
cylindrical Calabi--Yau manifolds converge exponentially to their asymptotic
limits, thereby answering affirmatively a question by
Haskins--Hein--Nordstr\"om. In dimension four our result strengthens those of
Chen--Chen, who proved exponential convergence to its asymptotic limit space
for any ALH instanton.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [55] [Bayesian power spectral density estimation for LISA noise based on P-splines with a parametric boost](https://arxiv.org/abs/2510.00533)
*Nazeela Aimen,Patricio Maturana-Russel,Avi Vajpeyi,Nelson Christensen,Renate Meyer*

Main category: stat.CO

TL;DR: A fast Bayesian method for estimating power spectral density (PSD) of stationary time series, combining parametric and nonparametric components using penalized B-splines for LISA gravitational wave data analysis.


<details>
  <summary>Details</summary>
Motivation: Flexible and efficient noise characterization is crucial for precise gravitational wave parameter estimation, especially for long datasets like those from LISA mission.

Method: Models PSD as geometric mean of parametric and nonparametric components. Uses mixture of penalized B-splines with adaptive knot placement and hierarchical roughness penalty priors to prevent overfitting.

Result: Achieves relative integrated absolute errors of O(10^-2) with computation times under 3 minutes for simulated LISA data. Well-matched parametric components reduce error and require fewer spline knots.

Conclusion: The method provides stable, flexible PSD estimates suitable for iterative analysis pipelines and multi-year mission datasets, significantly reducing runtime from hours to minutes.

Abstract: Flexible and efficient noise characterization is crucial for the precise
estimation of gravitational wave parameters. We introduce a fast and accurate
Bayesian method for estimating the power spectral density (PSD) of long,
stationary time series tailored specifically for LISA data analysis. Our
approach models the PSD as a geometric mean of a parametric and a nonparametric
component, combining the computational efficiency of parametric models with the
flexibility to capture deviations from theoretical expectations. The
nonparametric component is expressed by a mixture of penalized B-splines.
Adaptive, data-driven knot placement performed once during initialization
eliminates computationally expensive reversible-jump Markov Chain Monte Carlo,
while hierarchical roughness penalty priors prevent overfitting. This design
yields stable, flexible PSD estimates with runtimes of minutes instead of
hours. Validation on simulated autoregressive AR(4) data demonstrates estimator
consistency. It shows that well-matched parametric components reduce the
integrated absolute error compared to an uninformative baseline, requiring
fewer spline knots to achieve comparable accuracy. Applied to a year of
simulated LISA $X$-channel noise, our method achieves relative integrated
absolute errors of $\mathcal{O}(10^{-2})$ with computation times less than
three minutes, which makes it suitable for iterative analysis pipelines and
multi-year mission datasets.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [56] [FourPhonon_GPU: A GPU-accelerated framework for calculating phonon scattering rates and thermal conductivity](https://arxiv.org/abs/2510.00518)
*Ziqi Guo,Xiulin Ruan,Guang Lin*

Main category: cond-mat.mtrl-sci

TL;DR: FourPhonon_GPU is a GPU-accelerated framework that speeds up three-phonon and four-phonon scattering rate calculations by over 25x for computation and 10x overall runtime using OpenACC and heterogeneous CPU-GPU computing.


<details>
  <summary>Details</summary>
Motivation: The computational cost of phonon scattering calculations, especially four-phonon scattering, becomes prohibitive when dealing with large numbers of phonon branches and scattering processes, limiting efficient thermal transport modeling.

Method: Developed FourPhonon_GPU framework using OpenACC and heterogeneous CPU-GPU computing strategy, offloading parallelizable tasks to GPU while using CPU for process enumeration and control-heavy operations.

Result: Achieved over 25x acceleration for scattering rate computation and over 10x total runtime speedup without accuracy loss. Benchmarks confirmed scalability across GPU architectures.

Conclusion: The framework provides an efficient and accurate computational tool for phonon transport modeling and enables accelerated materials discovery through significant performance improvements.

Abstract: Accurately predicting phonon scattering is crucial for understanding thermal
transport properties. However, the computational cost of such calculations,
especially for four-phonon scattering, can often be more prohibitive when large
number of phonon branches and scattering processes are involved. In this work,
we present FourPhonon_GPU, a GPU-accelerated framework for three-phonon and
four-phonon scattering rate calculations based on the FourPhonon package. By
leveraging OpenACC and adopting a heterogeneous CPU-GPU computing strategy, we
efficiently offload massive, parallelizable tasks to the GPU while using the
CPU for process enumeration and control-heavy operations. Our approach achieves
over 25x acceleration for the scattering rate computation step and over 10x
total runtime speedup without sacrificing accuracy. Benchmarking on various GPU
architectures confirms the method's scalability and highlights the importance
of aligning parallelization strategies with hardware capabilities. This work
provides an efficient and accurate computational tool for phonon transport
modeling and opens pathways for accelerated materials discovery.

</details>


### [57] [Orbital-Engineered Spin Asymmetry and Multifunctionality in Eu-Activated CaAlSiN$_3$: A First-Principles Roadmap to Optical-Thermoelectric Fusion](https://arxiv.org/abs/2510.00863)
*Muhammad Tayyab,Faiq Umar,Sikander Azam,Qaiser Rafiq,Rajwali Khan,Muhammad Tahir Khan,Vineet Tirth,Ali Algahtani*

Main category: cond-mat.mtrl-sci

TL;DR: DFT study shows Eu3+-doped CaAlSiN3 is a stable red-emitting phosphor with improved optical and thermoelectric properties for white LED applications.


<details>
  <summary>Details</summary>
Motivation: To understand the electronic, optical, and thermoelectric properties of Eu3+-doped CaAlSiN3 for solid-state lighting applications, particularly white LEDs.

Method: First-principles DFT calculations using GGA+U approach, analyzing electronic structure, optical spectra, formation energy, elastic constants, and thermoelectric transport properties.

Result: Eu doping introduces localized 4f states, narrows band gap, enables red photoluminescence, improves thermoelectric power factor, and maintains mechanical robustness.

Conclusion: Eu-doped CaAlSiN3 is a promising stable red phosphor for WLEDs with enhanced optical and thermoelectric performance.

Abstract: Rare-earth-doped nitride phosphors are promising materials for solid-state
lighting and photonic applications due to their thermal stability, sharp
emission lines, and strong UV-blue absorption. In this work, we present a
first-principles density functional theory (DFT) study, using the GGA+U
approach, of pristine and Eu3+-doped CaAlSiN3 at doping levels of 8.5% and 17%.
Electronic structure calculations show that Eu incorporation introduces
localized 4f states within the band gap, leading to band-gap narrowing and
enabling red photoluminescence through the 5D0 -> 7F2 transition.
Spin-polarized density of states and spin density mapping confirm the magnetic
nature of Eu3+, while charge density, Bader analysis, and electron localization
function (ELF) indicate mixed ionic-covalent bonding and charge transfer from
Eu to neighboring N and Al atoms, stabilizing the doped lattice. Optical
spectra, including dielectric function, absorption, refractive index, and
reflectivity, reveal red-shifted absorption edges and enhanced visible-range
light-matter interactions, consistent with experimental red to near-infrared
emission. Formation energy analysis confirms the thermodynamic feasibility of
Eu substitution, while elastic constants and Pugh's ratio indicate mechanical
robustness and ductility. Thermoelectric transport properties, obtained using
WIEN2k and BoltzTraP, suggest that moderate Eu3+ doping improves the power
factor and reduces lattice thermal conductivity through disorder scattering.
These results establish Eu-doped CaAlSiN3 as a stable and efficient
red-emitting phosphor for white light-emitting diodes (WLEDs) and provide
theoretical insights for crystal site engineering in advanced optoelectronic
materials.

</details>


### [58] [Exploring Chalcogen Influence on Sc2BeX4 (X = S, Se) for Green Energy Applications Using DFT](https://arxiv.org/abs/2510.00955)
*Ahmad Ali,Haris Haider,Sikander Azam,Muhammad Talha,Muhammad Jawad,Imran Shakir*

Main category: cond-mat.mtrl-sci

TL;DR: First-principles DFT study shows Sc2BeX4 (X=S,Se) chalcogenides are stable materials with direct band gaps (1.8 eV and 1.2 eV), strong visible-light absorption, and excellent thermoelectric properties, making them promising for photovoltaic and thermoelectric applications.


<details>
  <summary>Details</summary>
Motivation: To investigate Sc2BeX4 chalcogenides for energy applications, specifically their potential in photovoltaic and thermoelectric devices, given the need for efficient energy conversion materials.

Method: First-principles density functional theory calculations were used to study structural, electronic, optical, and thermoelectric properties, including TB-mBJ approximation for band gaps and various transport property calculations.

Result: Both compounds are dynamically and thermodynamically stable with negative formation energies. They exhibit direct band gaps suitable for visible-light absorption, high dielectric constants, and promising thermoelectric performance with maximum ZT of 0.80 at 800 K.

Conclusion: Sc2BeX4 chalcogenides are promising materials for photovoltaic and thermoelectric applications due to their stability, suitable band gaps, strong optical absorption, and excellent thermoelectric properties.

Abstract: We present a first-principles density functional theory study of the
structural, electronic, optical, and thermoelectric properties of Sc2BeX4 (X =
S, Se) chalcogenides for energy applications. Both compounds are dynamically
and thermodynamically stable, exhibiting negative formation energies of -2.6 eV
(Sc2BeS4) and -2.2 eV (Sc2BeSe4). They feature direct band gaps of 1.8 eV and
1.2 eV, respectively, within the TB-mBJ approximation, indicating strong
visible-light absorption. Optical analysis reveals high static dielectric
constants (9.0 for S and 16.5 for Se), absorption peaks near 13.5 eV, and
reflectivity below 30 percent. Thermoelectric calculations predict p-type
conduction with Seebeck coefficients reaching 2.5e-4 V/K and electrical
conductivities of 2.45e18 and 1.91e18 (Ohm m s)^-1 at 300 K. Power factors
approach 1.25e11 W/K^2 m s, with a maximum dimensionless figure of merit (ZT)
of 0.80 at 800 K. Calculated Debye temperatures (420 K for Sc2BeS4 and 360 K
for Sc2BeSe4) imply low lattice thermal conductivity. These findings establish
Sc2BeX4 chalcogenides as promising materials for photovoltaic and
thermoelectric applications.

</details>


### [59] [High-Pressure DFT Study of BeX (X = S, Se, Te): Phonon Spectra, Optical Properties, and Thermodynamic Stability for Advanced Optoelectronic Applications](https://arxiv.org/abs/2510.01056)
*Muhammad Shahzad,Sikander Azam,Syed Awais Ahmad,Ming Li*

Main category: cond-mat.mtrl-sci

TL;DR: First-principles DFT study of BeX compounds (X=S, Se, Te) under 0-10 GPa pressure, showing structural stability, pressure-dependent electronic/optical properties, and thermodynamic behavior suitable for optoelectronic and thermoelectric applications.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate the pressure-dependent structural, electronic, optical, and thermodynamic properties of BeX compounds for potential applications in pressure-sensitive optoelectronic and thermoelectric devices.

Method: Density functional theory (DFT) calculations using GGA-PBE functional implemented in CASTEP code, with phonon dispersion analysis and property calculations across 0-10 GPa pressure range.

Result: All BeX compounds remain dynamically stable under pressure; BeS maintains widest bandgap; strong UV optical absorption observed; pressure reduces atomic vibrations and heat capacity while Gibbs free energy shows consistent temperature dependence.

Conclusion: BeX compounds are suitable for pressure-sensitive optoelectronic and thermoelectric devices, with strong UV absorption making them promising for UV optoelectronic applications and thermal barrier coatings.

Abstract: We present a comprehensive first-principles investigation of the structural,
electronic, optical, and thermodynamic properties of BeX compounds (X = S, Se,
Te) under hydrostatic pressures ranging from 0 to 10 GPa. Calculations were
performed using density functional theory (DFT) within the Generalized Gradient
Approximation (GGA) using the Perdew-Burke-Ernzerhof (PBE) functional, as
implemented in the CASTEP code. Phonon dispersion analyses confirm the
dynamical stability of all compounds across the studied pressure range, as
indicated by the absence of imaginary frequencies throughout the Brillouin
zone. The electronic band structure reveals pressure-induced band
modifications, with BeS retaining the widest bandgap. Optical properties,
including the dielectric function, absorption coefficient, reflectivity, and
energy loss spectra, were computed for photon energies up to 30 eV. The
materials exhibit strong optical absorption in the ultraviolet region,
suggesting potential for UV optoelectronic applications. Thermodynamic
parameters such as Debye temperature, heat capacity, and entropy were
evaluated, showing pressure-dependent trends. Notably, increasing pressure
leads to reduced atomic vibrations and heat capacity, while the Gibbs free
energy exhibits a consistent slope with temperature, reflecting entropy
variation. These results highlight the suitability of BeX compounds for
pressure-sensitive optoelectronic and thermoelectric devices, as well as
thermal barrier applications.

</details>


### [60] [exa-AMD: An Exascale-Ready Framework for Accelerating the Discovery and Design of Functional Materials](https://arxiv.org/abs/2510.01170)
*Weiyi Xiaa,Maxim Moraru,Ying Wai Li,Cai-Zhuang Wang*

Main category: cond-mat.mtrl-sci

TL;DR: exa-AMD is an open-source high-performance simulation code for accelerated materials discovery on exascale supercomputers, featuring task-based parallelization, modular design, and automated workflows for discovering novel materials.


<details>
  <summary>Details</summary>
Motivation: To address computational challenges in large-scale materials discovery and leverage exascale computing for unprecedented simulations in materials science.

Method: Employs task-based parallelization strategies, optimized data management, modular design supporting distributed and on-node parallelism, and automated workflows on CPU/GPU clusters.

Result: Demonstrates strong scaling across multiple HPC platforms, successfully applied to design Fe-Co-Zr and Na-B-C compounds, automating structure discovery and phase diagram updates.

Conclusion: exa-AMD provides a robust, efficient, and extensible tool for materials science research on exascale platforms, advancing accelerated materials discovery through community-accessible open-source software.

Abstract: Exascale computing is transforming the field of materials science by enabling
simulations of unprecedented scale and complexity. We present exa-AMD, an
open-source, high-performance simulation code specifically designed for
accelerated materials discovery on modern supercomputers. exa-AMD addresses the
computational challenges inherent in large-scale materials discovery by
employing task-based parallelization strategies and optimized data management
tailored for high performance computers. The code features a modular design,
supports both distributed and on-node parallelism, and is designed for
flexibility and extensibility to accommodate a wide range of materials science
applications. We detail the underlying algorithms and implementation, and
provide comprehensive benchmark results demonstrating strong scaling across
multiple high performance computing platforms. We provide two example
applications, the design of Fe-Co-Zr and Na-B-C compounds, to illustrate the
code's effectiveness in accelerating the discovery and characterization of
novel materials. With only a set of elements as input, exa-AMD automates the
workflow on CPU or GPU-enabled clusters, outputs the structures and energies of
promising candidates, and updates the phase diagram. exa-AMD is publicly
available on GitHub, with detailed documentation and reproducible test cases to
support community engagement and collaborative research. This work aims to
advance materials science by providing a robust, efficient, and extensible tool
ready for exascale platforms.ady for exascale platforms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [61] [BigBang-Proton Technical Report: Next-Word-Prediction is Scientific Multitask Learner](https://arxiv.org/abs/2510.00129)
*Hengkui Wu,Liujiang Liu,Jihua He,Qihao Wang,Keke Zhao,Shuyang Hu,Renle Fu,Dahao Liang,Lingyu Zeng,Bruce Liu,Yuan Liu,Jin Zhan,Jiaqiang Niu,Xinglong Jia,Yaqin Hu,Wenjun Ji,Panpan Chi,Ken Chen,Hengyuan Wu,Yingsi Xin,Yongfeng Zhu,Yuexin Wang,Manqi Ruan,Ningtao Bian,Xiaohua Wu,Weipeng Xu*

Main category: cs.LG

TL;DR: BigBang-Proton is a unified sequence-based architecture for auto-regressive language modeling pretrained on cross-scale scientific tasks, featuring Theory-Experiment Learning, Binary Patch Encoding, and Monte Carlo Attention innovations.


<details>
  <summary>Details</summary>
Motivation: To create a scientific multi-task learner that can handle cross-discipline scientific tasks while maintaining multitask learning capabilities, moving toward developing a material world foundational model.

Method: Uses next-word-prediction pretraining on cross-discipline scientific datasets mixed with general corpus, featuring Theory-Experiment Learning paradigm, Binary Patch Encoding instead of BPE tokenization, and Monte Carlo Attention replacing traditional transformers.

Result: Achieves 100% accuracy in 50-digit arithmetic, par performance with specialized models in particle physics jet tagging, matches MAE of specialized models in inter-atomic potential simulation, comparable performance to traditional spatiotemporal models in water quality prediction, and benchmark-exceeding performance in genome modeling.

Conclusion: Language-guided scientific computing can match or exceed task-specific scientific models while maintaining multitask learning capabilities, with plans to scale pretraining to universe scale for material world foundational model development.

Abstract: We introduce BigBang-Proton, a unified sequence-based architecture for
auto-regressive language modeling pretrained on cross-scale, cross-structure,
cross-discipline real-world scientific tasks to construct a scientific
multi-task learner. BigBang-Proton incorporates three fundamental innovations
compared to mainstream general-purpose LLMs: Theory-Experiment Learning
paradigm aligns large-scale numerical experimental data with theoretical text
corpora; Binary Patch Encoding replaces byte pair encoding(BPE) tokenization;
Monte Carlo Attention substitutes traditional transformer architectures.
Through next-word-prediction pretraining on cross-discipline scientific
datasets of real-world problems mixed with general textual corpus, followed by
fine-tuning and inference on downstream tasks, BigBang-Proton demonstrates
100\% accuracy in up to 50-digit arithmetic addition operations, performance on
par with leading specialized models in particle physics jet tagging, matching
MAE of specialized models in inter-atomic potential simulation, performance
comparable to traditional spatiotemporal models in water quality prediction,
and benchmark-exceeding performance in genome modeling. These results prove
that language-guided scientific computing can match or exceed the performance
of task-specific scientific models while maintaining multitask learning
capabilities. We further hypothesize to scale the pretraining to the universe
scale as a fundamental step toward developing material world foundational
model.

</details>


### [62] [Physics-Informed Extreme Learning Machine (PIELM) for Tunnelling-Induced Soil-Pile Interactions](https://arxiv.org/abs/2510.00698)
*Fu-Chen Guo,Pei-Zhi Zhuang,Fei Ren,Hong-Ya Yue,He Yang*

Main category: cs.LG

TL;DR: Proposes a physics-informed extreme learning machine (PIELM) framework for analyzing tunneling-induced soil-pile interactions, combining Euler-Bernoulli beam theory with Pasternak foundation modeling through a fourth-order ODE.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient physics-informed machine learning approach for real-time monitoring and safety assessment of pile foundations during tunneling, addressing the need for intelligent early-warning systems in geotechnical engineering.

Method: Combines physics (fourth-order ODE from Euler-Bernoulli beam and Pasternak foundation) with data-driven components in an extreme learning machine (ELM) network, trained within 1 second using least squares method. Validated with boundary element method and finite difference method.

Result: PIELM achieves fast training (1 second) and shows that monitored data should be placed at positions with significant pile deflection gradients (pile tip/top and near tunneling zones). The approach effectively combines physics and data for soil-pile interaction analysis.

Conclusion: The proposed PIELM framework demonstrates great potential for real-time monitoring and safety assessment of pile foundations, benefiting intelligent early-warning systems in geotechnical engineering by efficiently integrating physics and data-driven approaches.

Abstract: Physics-informed machine learning has been a promising data-driven and
physics-informed approach in geotechnical engineering. This study proposes a
physics-informed extreme learning machine (PIELM) framework for analyzing
tunneling-induced soil-pile interactions. The pile foundation is modeled as an
Euler-Bernoulli beam, and the surrounding soil is modeled as a Pasternak
foundation. The soil-pile interaction is formulated into a fourth-order
ordinary differential equation (ODE) that constitutes the physics-informed
component, while measured data are incorporated into PIELM as the data-driven
component. Combining physics and data yields a loss vector of the extreme
learning machine (ELM) network, which is trained within 1 second by the least
squares method. After validating the PIELM approach by the boundary element
method (BEM) and finite difference method (FDM), parametric studies are carried
out to examine the effects of ELM network architecture, data monitoring
locations and numbers on the performance of PIELM. The results indicate that
monitored data should be placed at positions where the gradients of pile
deflections are significant, such as at the pile tip/top and near tunneling
zones. Two application examples highlight the critical role of physics-informed
and data-driven approach for tunnelling-induced soil-pile interactions. The
proposed approach shows great potential for real-time monitoring and safety
assessment of pile foundations, and benefits for intelligent early-warning
systems in geotechnical engineering.

</details>


### [63] [Neural Hamilton--Jacobi Characteristic Flows for Optimal Transport](https://arxiv.org/abs/2510.01153)
*Yesom Park,Shu Liu,Mo Zhou,Stanley Osher*

Main category: cs.LG

TL;DR: A novel Hamilton-Jacobi equation-based framework for optimal transport that uses method of characteristics to derive closed-form bidirectional transport maps, eliminating numerical integration and adversarial training.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient and principled approach to optimal transport problems that avoids computational complexity of numerical integration and adversarial training while guaranteeing convergence to optimal maps.

Method: Uses Hamilton-Jacobi equation whose viscosity solution characterizes OT map, applies method of characteristics to derive closed-form bidirectional transport maps, trains single neural network with loss from HJ characteristics method.

Result: Achieves accurate, scalable, and efficient optimal transport across diverse datasets, supports various cost functions and class-conditional transport, with provable optimality.

Conclusion: The framework provides a principled and versatile tool for OT applications that eliminates adversarial training, reduces computational complexity, and guarantees convergence to optimal maps.

Abstract: We present a novel framework for solving optimal transport (OT) problems
based on the Hamilton--Jacobi (HJ) equation, whose viscosity solution uniquely
characterizes the OT map. By leveraging the method of characteristics, we
derive closed-form, bidirectional transport maps, thereby eliminating the need
for numerical integration. The proposed method adopts a pure minimization
framework: a single neural network is trained with a loss function derived from
the method of characteristics of the HJ equation. This design guarantees
convergence to the optimal map while eliminating adversarial training stages,
thereby substantially reducing computational complexity. Furthermore, the
framework naturally extends to a wide class of cost functions and supports
class-conditional transport. Extensive experiments on diverse datasets
demonstrate the accuracy, scalability, and efficiency of the proposed method,
establishing it as a principled and versatile tool for OT applications with
provable optimality.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [64] [Bifurcation Curve Diagrams for a Diffusive Generalized Logistic Problem with Minkowski Curvature Operator and Constant-Yield Harvesting](https://arxiv.org/abs/2510.00362)
*Shao-Yuan Huang*

Main category: math.CA

TL;DR: Analysis of bifurcation diagrams for positive solutions in a 1D diffusive generalized logistic problem with Minkowski curvature operator and constant yield harvesting, showing C-shaped bifurcation curves and exact multiplicity of solutions.


<details>
  <summary>Details</summary>
Motivation: To understand the structure of positive solutions in boundary-value problems involving Minkowski curvature operators and harvesting effects, which have applications in mathematical biology and physics.

Method: Using bifurcation theory to analyze the diffusive generalized logistic boundary-value problem with Minkowski curvature operator and constant yield harvesting, characterizing bifurcation curves and sets.

Result: Proved that bifurcation curves on (lambda, sup-norm of u)-plane and (mu, sup-norm of u)-plane are C-shaped, and determined exact multiplicity of positive solutions by characterizing the bifurcation set on (mu, lambda)-plane.

Conclusion: The study provides complete characterization of solution multiplicity and bifurcation structure for this class of problems, revealing C-shaped bifurcation patterns and exact solution counts.

Abstract: This paper investigates the bifurcation diagrams of positive solutions for a
one-dimensional diffusive generalized logistic boundary-value problem with the
Minkowski curvature operator and constant yield harvesting. We prove that the
corresponding bifurcation curves on both the (lambda, sup-norm of u)-plane and
the (mu, sup-norm of u)-plane are C-shaped. Furthermore, by characterizing the
bifurcation set on the (mu, lambda)-plane, we determine the exact multiplicity
of positive solutions.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [65] [Chase Orbits, not Time: A Scalable Paradigm for Long-Duration Eccentric Gravitational-Wave Surrogates](https://arxiv.org/abs/2510.00116)
*Akash Maurya,Prayush Kumar,Scott E. Field,Chandra Kant Mishra,Peter James Nee,Kaushik Paul,Harald P. Pfeiffer,Adhrit Ravichandran,Vijay Varma*

Main category: gr-qc

TL;DR: A novel surrogate modeling technique for eccentric binary black hole waveforms using mean anomaly parameterization instead of time, achieving order-of-magnitude efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: Eccentric binary black hole waveforms are challenging to model due to complex morphology from orbital timescale variations, especially for long-duration waveforms needed for current gravitational wave detectors.

Method: Parameterizes waveforms in terms of mean anomaly (angular orbital element) instead of time, simplifying harmonic content and making parameter-space variations more regular.

Result: Achieves order of magnitude fewer surrogate basis functions compared to time-based parameterization, with more regular parameter-space fitting.

Conclusion: Makes long-duration eccentric surrogate modeling feasible for current and future third-generation gravitational wave detectors.

Abstract: Surrogate modeling of eccentric binary black hole waveforms has remained
challenging. The complicated morphology of these waveforms due to the eccentric
orbital timescale variations makes it difficult to construct accurate and
efficient surrogate models, especially for waveforms long enough to cover the
sensitivity band of the current ground-based gravitational wave detectors. We
present a novel and scalable surrogate building technique which makes surrogate
modeling of long-duration eccentric binary black hole waveforms both feasible
and highly efficient. The technique aims to simplify the harmonic content of
the intermediate eccentric surrogate data pieces by modeling them in terms of
an angular orbital element called the mean anomaly, instead of time. We show
that this novel parameterization yields an order of magnitude fewer surrogate
basis functions than using the contemporary parameterization in terms of time.
We show that variations in surrogate data-pieces across parameter space become
much more regular when expressed in terms of the instantaneous waveform
eccentricity and mean anomaly, greatly easing their parameter-space fitting.
The methods presented in this work make it feasible to build long-duration
eccentric surrogates for the current as well as future third-generation
gravitational wave detectors.

</details>
