<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 9]
- [math.AP](#math.AP) [Total: 16]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [nlin.SI](#nlin.SI) [Total: 2]
- [math.SP](#math.SP) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [math.DS](#math.DS) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [quant-ph](#quant-ph) [Total: 5]
- [cs.MS](#cs.MS) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.SC](#cs.SC) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Deep Learning Accelerated Algebraic Multigrid Methods for Polytopal Discretizations of Second-Order Differential Problems](https://arxiv.org/abs/2510.01442)
*Paola F. Antonietti,Matteo Caldana,Lorenzo Gentile,Marco Verani*

Main category: math.NA

TL;DR: A deep learning approach to automatically tune AMG parameters (strong threshold and smoother choice) for polytopal discretizations, treating sparse matrices as images and using pooling to extract features, achieving up to 27% solver time reduction.


<details>
  <summary>Details</summary>
Motivation: AMG methods are sensitive to parameter choices, especially for polytopal discretizations (PolyDG/VEM) where mesh variability creates diverse sparsity patterns, making manual parameter tuning difficult and critical for performance.

Method: Interpret sparse matrices from polytopal discretizations as grayscale images, apply pooling to extract compact features, and use neural networks to automatically select optimal AMG parameters (strong threshold and smoother choice).

Result: The approach reduces AMG solver time by up to 27% across various 2D/3D differential problems with heterogeneous coefficients and polygonal/polyhedral meshes, demonstrating good generalization.

Conclusion: Deep learning can effectively automate AMG parameter tuning for polytopal discretizations, significantly improving solver performance with minimal code modifications to existing PolyDG and VEM implementations.

Abstract: Algebraic Multigrid (AMG) methods are state-of-the-art algebraic solvers for
partial differential equations. Still, their efficiency depends heavily on the
choice of suitable parameters and/or ingredients. Paradigmatic examples include
the so-called strong threshold parameter $\theta$, which controls the algebraic
coarse-grid hierarchy, as well as the smoother, i.e., the relaxation methods
used on the fine grid to damp out high-frequency errors. In AMG, since the
coarse grids are constructed algebraically (without geometric intuition), the
smoother's performance is even more critical. For the linear systems stemming
from polytopal discretizations, such as Polytopal Discontinuous Galerkin
(PolyDG) and Virtual Element Methods (VEM), AMG sensitivity to such choices is
even more critical due to the significant variability of the underlying meshes,
which results in algebraic systems with different sparsity patterns. We propose
a novel deep learning approach that automatically tunes the strong threshold
parameter, as well as the smoother choice in AMG solvers, for linear systems of
equations arising from polytopal discretizations, thereby maximizing AMG
performance. We interpret the sparse matrix resulting from polytopal
discretization as a grayscale image, and by applying pooling, our neural
network extracts compact features that preserve the necessary information at a
low computational cost. We test various differential problems in both two- and
three-dimensional settings, with heterogeneous coefficients and
polygonal/polyhedral meshes, and demonstrate that the proposed approach
generalizes well. In practice, we demonstrate that we can reduce AMG solver
time by up to $27\%$ with minimal changes to existing PolyDG and VEM codes.

</details>


### [2] [Data selection: at the interface of PDE-based inverse problem and randomized linear algebra](https://arxiv.org/abs/2510.01567)
*Kathrin Hellmuth,Ruhui Jin,Qin Li,Stephen J. Wright*

Main category: math.NA

TL;DR: This review explores data selection in PDE-based inverse problems, focusing on infinite-dimensional challenges and the application of randomized numerical linear algebra methods for efficient data sampling with probabilistic guarantees.


<details>
  <summary>Details</summary>
Motivation: Inverse problems rely on data to recover unknown parameters, but not all data are equally informative. The central challenge is selecting the most informative data, particularly difficult in PDE-based inverse problems due to their inherently infinite-dimensional nature in both parameter and design spaces.

Method: The paper surveys the adaptation and application of randomized numerical linear algebra (RNLA) strategies to address data selection in PDE-based inverse problems. These probabilistic methods use randomly selected, weighted samples to preserve information with mathematical guarantees.

Result: RNLA methods provide powerful tools for data selection in infinite-dimensional settings, offering guarantees that information is preserved with probability at least 1-p when using N randomly selected samples, with different mathematical formulations of information depending on the specific problem context.

Conclusion: Randomized numerical linear algebra, originally developed for different contexts, has proven to be an effective approach for addressing the unique challenges of data selection in PDE-based inverse problems, particularly their infinite-dimensional nature.

Abstract: All inverse problems rely on data to recover unknown parameters, yet not all
data are equally informative. This raises the central question of data
selection. A distinctive challenge in PDE-based inverse problems is their
inherently infinite-dimensional nature: both the parameter space and the design
space are infinite, which greatly complicates the selection process. Somewhat
unexpectedly, randomized numerical linear algebra (RNLA), originally developed
in very different contexts, has provided powerful tools for addressing this
challenge. These methods are inherently probabilistic, with guarantees
typically stating that information is preserved with probability at least 1-p
when using N randomly selected, weighted samples. Here, the notion of
information can take different mathematical forms depending on the setting. In
this review, we survey the problem of data selection in PDE-based inverse
problems, emphasize its unique infinite-dimensional aspects, and highlight how
RNLA strategies have been adapted and applied in this context.

</details>


### [3] [Instability of the Sherman-Morrison formula and stabilization by iterative refinement](https://arxiv.org/abs/2510.01696)
*Behnam Hashemi,Yuji Nakatsukasa*

Main category: math.NA

TL;DR: The Sherman-Morrison formula's numerical stability has been an open question. This paper analyzes its backward stability, demonstrates instability in common scenarios, and incorporates iterative refinement to achieve backward stability while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: The Sherman-Morrison formula is widely used for solving rank-one perturbed linear systems due to its simplicity and efficiency, but its numerical stability properties have remained an open research question for decades.

Method: Analyzed backward stability of SM formula, addressed Higham's open question, and incorporated fixed-precision iterative refinement into SM framework while reusing previously computed decompositions.

Result: Demonstrated SM formula's instability in common scenarios, proved that with iterative refinement it achieves backward stability under reasonable assumptions, and empirically observed backward stable solutions in all experiments.

Conclusion: With iterative refinement, the SM formula yields backward stable solutions when the condition numbers of both A and A+uv^T are safely bounded away from the inverse of unit roundoff.

Abstract: Owing to its simplicity and efficiency, the Sherman-Morrison (SM) formula has
seen widespread use across various scientific and engineering applications for
solving rank-one perturbed linear systems of the form $(A+uv^T)x = b$. Although
the formula dates back at least to 1944, its numerical stability properties
have remained an open question and continue to be a topic of current research.
We analyze the backward stability of the SM, demonstrate its instability in a
scenario increasingly common in scientific computing and address an open
question posed by Nick Higham on the proportionality of the backward error
bound to the condition number of $A$. We then incorporate fixed-precision
iterative refinement into the SM framework reusing the previously computed
decompositions and prove that, under reasonable assumptions, it achieves
backward stability without sacrificing the efficiency of the SM formula. While
our theory does not prove the SM formula with iterative refinement always
outputs a backward stable solution, empirically it is observed to eventually
produce a backward stable solution in all our numerical experiments. We
conjecture that with iterative refinement, the SM formula yields a backward
stable solution provided that $\kappa_2(A), \kappa_2(A+uv^T)$ are both bounded
safely away from $\epsilon_M^{-1}$, where $\epsilon_M$ is the unit roundoff.

</details>


### [4] [Efficient manifold evolution algorithm using adaptive B-Spline interpolation](https://arxiv.org/abs/2510.01790)
*Muhammad Ammad,Leevan Ling*

Main category: math.NA

TL;DR: Efficient Lagrangian approach using B-Splines for evolving point clouds on manifolds, providing geometric coefficient manipulation and seamless point updates as an alternative to RBF methods.


<details>
  <summary>Details</summary>
Motivation: To develop an alternative to conventional radial basis function (RBF) approaches for evolving point cloud data on smooth manifolds, particularly for higher-dimensional applications.

Method: Uses B-Spline basis functions for local interpolations, enabling geometric feature approximation and coefficient manipulation that facilitates rapid updates without frequent re-interpolation.

Result: Numerical results show convergence of geometric quantities and effective handling of regions with significant point density fluctuations, with successful simulations of curvature flows coupled with reaction-diffusion systems.

Conclusion: The B-Spline approach provides an efficient alternative to RBF methods with geometric coefficient manipulation, seamless point updates, and successful application to complex pattern formation problems.

Abstract: This paper explores an efficient Lagrangian approach for evolving point cloud
data on smooth manifolds. In this preliminary study, we focus on analyzing
plane curves, and our ultimate goal is to provide an alternative to the
conventional radial basis function (RBF) approach for manifolds in higher
dimensions. In particular, we use the B-Spline as the basis function for all
local interpolations. Just like RBF and other smooth basis functions, B-Splines
enable the approximation of geometric features such as normal vectors and
curvature. Once properly set up, the advantage of using B-Splines is that their
coefficients carry geometric meanings. This allows the coefficients to be
manipulated like points, facilitates rapid updates of the interpolant, and
eliminates the need for frequent re-interpolation. Consequently, the removal
and insertion of point cloud data become seamless processes, particularly
advantageous in regions experiencing significant fluctuations in point density.
The numerical results demonstrate the convergence of geometric quantities and
the effectiveness of our approach. Finally, we show simulations of curvature
flows whose speeds depend on the solutions of coupled reaction--diffusion
systems for pattern formation.

</details>


### [5] [Asymptotic preserving schemes for hyperbolic systems with relaxation](https://arxiv.org/abs/2510.01828)
*C Mahmoud,H Mathis*

Main category: math.NA

TL;DR: Construction of two asymptotic preserving numerical schemes for hyperbolic systems with relaxation source terms that handle convection and source terms together without separation.


<details>
  <summary>Details</summary>
Motivation: To develop numerical methods that can handle hyperbolic systems with relaxation source terms as a unified system rather than separating convective and source term resolution, while maintaining asymptotic preservation properties.

Method: First scheme combines centered FORCE approach with unsplit strategy; second scheme uses an approximate Riemann solver that carefully handles source term approximation. Both schemes are designed to be asymptotic preserving without CFL restrictions.

Result: Two numerical schemes were successfully constructed that are asymptotic preserving - their limit schemes remain consistent with the equilibrium model as relaxation parameter approaches zero, without CFL restrictions. For specific models, they preserve invariant domains and admit discrete entropy inequalities.

Conclusion: The proposed schemes provide effective numerical methods for hyperbolic relaxation systems that maintain important mathematical properties while handling convection and source terms in a unified manner.

Abstract: This paper presents the construction of two numerical schemes for the
solution of hyperbolic systems with relaxation source terms. The methods are
built by considering the relaxation system as a whole, without separating the
resolution of the convective part from that of the source term. The first
scheme combines the centered FORCE approach of Toro and co-authors with the
unsplit strategy proposed by B{\'e}reux and Sainsaulieu. The second scheme
consists of an approximate Riemann solver which carefully handles the source
term approximation. The two schemes are built to be asymptotic preserving, in
the sense that their limit schemes are consistent with the equilibrium model as
the relaxation parameter tends to zero, without any CFL restriction. For
specific models, it is possible to prove that they preserve invariant domains
and admit a discrete entropy inequality.

</details>


### [6] [A nodally bound-preserving composite discontinuous Galerkin method on polytopic meshes](https://arxiv.org/abs/2510.02094)
*Abdolreza Amiri,Gabriel R. Barrenechea,Emmanuil H. Georgoulis,Tristan Pryer*

Main category: math.NA

TL;DR: A nodally bound-preserving Galerkin method for elliptic problems on polytopic meshes that enforces solution bounds at user-defined points using simplicial submeshes without adding global degrees of freedom.


<details>
  <summary>Details</summary>
Motivation: To develop a method that combines geometric flexibility of polytopic meshes with bound preservation capabilities for numerical solutions, particularly important for problems with sharp gradients like boundary layers.

Method: Uses interior penalty discontinuous Galerkin formulation on polytopic meshes with simplicial submeshes to enforce bound preservation at submesh nodes via nonlinear iteration, preserving accuracy without additional global degrees of freedom.

Result: The method maintains optimal convergence for smooth problems, demonstrates robustness with sharp gradients, and automatically reverts to standard DG when no bounds are violated. Existence and uniqueness of solutions are proven.

Conclusion: The composite method successfully combines polytopic mesh flexibility with DG accuracy and stability while guaranteeing bound preservation, with proven mathematical properties and demonstrated numerical performance.

Abstract: We introduce a nodally bound-preserving Galerkin method for second-order
elliptic problems on general polygonal/polyhedral, henceforth collectively
termed as \emph{polytopic}, meshes. Starting from an interior penalty
discontinuous Galerkin (DG) formulation posed on a polytopic mesh, the method
enforces preservation of \emph{a priori} prescribed upper and lower bounds for
the numerical solution at an arbitrary number of user-defined points
\emph{within} each polytopic element. This is achieved by employing a
simplicial submesh and enforcing bound preservation at the submesh nodes via a
nonlinear iteration. By construction, the submeshing procedure preserves the
order of accuracy of the DG method, \emph{without} introducing any additional
global numerical degrees of freedom compared to the baseline DG method,
thereby, falling into the category of composite finite element approaches. A
salient feature of the proposed method is that it automatically reverts to the
standard DG method on polytopic meshes when no prescribed bound violation
occurs. In particular, the choice of the discontinuity-penalisation parameter
is independent of the submesh granularity. The resulting composite method
combines the geometric flexibility of polytopic meshes with the accuracy and
stability of discontinuous Galerkin discretisations, while rigorously
guaranteeing bound preservation. The existence and uniqueness of the numerical
solution is proven. A priori error bounds, assuming sufficient regularity of
the exact solution are shown, employing a non-standard construction of discrete
nodally bound-preserving interpolant. Numerical experiments confirm optimal
convergence for smooth problems and demonstrate robustness in the presence of
sharp gradients, such as boundary and interior layers.

</details>


### [7] [Coarse scrambling for Sobol' and Niederreiter sequences](https://arxiv.org/abs/2510.02111)
*Kosuke Suzuki*

Main category: math.NA

TL;DR: Coarse scrambling is a new randomization method for digital sequences that permutes blocks of digits while preserving (0,e,d)-sequence properties, achieving O(n^{-3+ε}) variance decay with logarithmic dimension dependence O(log d).


<details>
  <summary>Details</summary>
Motivation: To address the curse of dimensionality affecting scrambled Sobol' sequences by developing a randomization method with better dimension scaling.

Method: Coarse scrambling permutes blocks of digits in mixed-radix representations while preserving the (0,e,d)-sequence property of the underlying points.

Result: Achieves canonical O(n^{-3+ε}) variance decay rate and shows maximal gain coefficient grows only logarithmically with dimension O(log d), providing robustness against curse of dimensionality.

Conclusion: Coarse scrambling is competitive for functions with low effective truncation dimension, while Owen's scrambling remains superior for integrands sensitive to low-dimensional projections.

Abstract: We introduce \emph{coarse scrambling}, a novel randomization for digital
sequences that permutes blocks of digits in a mixed-radix representation. This
construction is designed to preserve the powerful
$(0,\boldsymbol{e},d)$-sequence property of the underlying points. For
sufficiently smooth integrands, we prove that this method achieves the
canonical $O(n^{-3+\epsilon})$ variance decay rate, matching that of standard
Owen's scrambling. Crucially, we show that its maximal gain coefficient grows
only logarithmically with dimension, $O(\log d)$, thus providing theoretical
robustness against the curse of dimensionality affecting scrambled Sobol'
sequences. Numerical experiments validate these findings and illustrate a
practical trade-off: while Owen's scrambling is superior for integrands
sensitive to low-dimensional projections, coarse scrambling is competitive for
functions with low effective truncation dimension.

</details>


### [8] [Mixed-precision iterative refinement for low-rank Lyapunov equations](https://arxiv.org/abs/2510.02126)
*Peter Benner,Xiaobo Liu*

Main category: math.NA

TL;DR: Mixed-precision iterative refinement framework for solving low-rank Lyapunov matrix equations using reduced precisions like half precision to accelerate computation while maintaining solution quality.


<details>
  <summary>Details</summary>
Motivation: To accelerate the solution of Lyapunov matrix equations by using reduced precision computations without compromising solution quality, especially for equations with condition numbers up to the inverse of the solver precision's unit roundoff.

Method: Developed a mixed-precision iterative refinement framework using the sign function Newton iteration as the solver. Performed rounding error analysis to derive sufficient conditions for attainable normwise residuals and determine optimal algorithmic parameters.

Result: Showed that reduced precisions (such as half precision with unit roundoff u_s) can be used to accelerate Lyapunov equation solutions for condition numbers up to 1/u_s while maintaining solution quality.

Conclusion: Mixed-precision iterative refinement with reduced precision solvers enables significant acceleration of Lyapunov equation solutions for a wide range of condition numbers without sacrificing accuracy.

Abstract: We develop a mixed-precision iterative refinement framework for solving
low-rank Lyapunov matrix equations $AX + XA^T + W =0$, where $W=LL^T$ or
$W=LSL^T$. Via rounding error analysis of the algorithms we derive sufficient
conditions for the attainable normwise residuals in different precision
settings and show how the algorithmic parameters should be chosen. Using the
sign function Newton iteration as the solver, we show that reduced precisions,
such as the half precision, can be used as the solver precision (with unit
roundoff $u_s$) to accelerate the solution of Lyapunov equations of condition
number up to $1/u_s$ without compromising its quality.

</details>


### [9] [A Fast solver for high condition linear systems using randomized stable solutions of its blocks](https://arxiv.org/abs/2510.02156)
*Suvendu Kar,Murugesan Venkatapathi*

Main category: math.NA

TL;DR: Enhanced randomized block-Kaczmarz method with regularization and dynamic proposal distribution for solving linear systems, especially effective for high-condition number problems.


<details>
  <summary>Details</summary>
Motivation: To improve solving high-condition number linear systems that are sparse or dense least-squares problems with poor preconditioner generalizability.

Method: Uses regularization during block updates and dynamic proposal distribution based on current residue and effective orthogonality between blocks.

Result: Provides significant gains in solving high-condition number linear systems, particularly for sparse systems or dense least-squares problems that are significantly over/under determined.

Conclusion: The improved method can serve as a pre-solver for other iterative numerical methods and as an inner iteration in certain GMRES solvers for linear systems.

Abstract: We present an enhanced version of the row-based randomized block-Kaczmarz
method to solve a linear system of equations. This improvement makes use of a
regularization during block updates in the solution, and a dynamic proposal
distribution based on the current residue and effective orthogonality between
blocks. This improved method provides significant gains in solving
high-condition number linear systems that are either sparse, or dense
least-squares problems that are significantly over/under determined.
Considering the poor generalizability of preconditioners for such problems, it
can also serve as a pre-solver for other iterative numerical methods when
required, and as an inner iteration in certain types of GMRES solvers for
linear systems.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [10] [Localized Pattern Formation and Oscillatory Instabilities in a Three-component Gierer Meinhardt Model](https://arxiv.org/abs/2510.01401)
*Chunyi Gai,Fahad Al Saadi*

Main category: math.AP

TL;DR: A three-component Gierer-Meinhardt model with large diffusivity ratio exhibits Hopf bifurcations in both amplitude and position of interior spikes, revealing richer oscillatory dynamics than classical two-component systems.


<details>
  <summary>Details</summary>
Motivation: To explore richer spike behavior and oscillatory dynamics in three-component reaction-diffusion systems beyond classical two-component models, particularly focusing on dual Hopf bifurcations.

Method: Asymptotic analysis and numerical path-following to construct localized spike equilibria, analyze spike nucleation through saddle-node bifurcations, and study stability using time-scaling parameters.

Result: Identified two distinct instability mechanisms: amplitude oscillations from large-eigenvalue instabilities and oscillatory spike motion from small eigenvalues, with numerical simulations confirming these dynamics and transition regimes.

Conclusion: Three-component systems exhibit richer spike behavior through dual instability mechanisms, suggesting several open problems for future study in reaction-diffusion pattern formation.

Abstract: In this paper, we introduce a three-component Gierer-Meinhardt model in the
semi-strong interaction regime, characterized by an asymptotically large
diffusivity ratio. A key feature of this model is that the interior spike can
undergo Hopf bifurcations in both amplitude and position, leading to rich
oscillatory dynamics not present in classical two-component systems. Using
asymptotic analysis and numerical path-following, we construct localized spike
equilibria and analyze spike nucleation that occurs through slow passage beyond
a saddle-node bifurcation. Moreover, stability of spike equilibrium is analyzed
by introducing time-scaling parameters, which reveal two distinct mechanisms:
amplitude oscillations triggered by large-eigenvalue instabilities and
oscillatory spike motion associated with small eigenvalues. Numerical
simulations illustrate these dynamics and their transition regimes. This dual
mechanism highlights richer spike behavior in three-component systems and
suggests several open problems for future study.

</details>


### [11] [Symmetry analysis and new partially invariant solutions for the gas dynamics system with a special equation of state](https://arxiv.org/abs/2510.01415)
*Dilara Siraeva,Irina A. Kogan*

Main category: math.AP

TL;DR: Symmetry analysis of gas dynamics with special state equation, computing invariants for 4D subalgebras and constructing explicit solutions.


<details>
  <summary>Details</summary>
Motivation: Advance the symmetry analysis of gas dynamics systems following Ovsyannikov's submodels program, continuing previous work on 4D subalgebras.

Method: Compute complete sets of generating invariants for non-similar 4D subalgebras, construct partially symmetry-reduced systems, and explicitly solve them.

Result: Obtained new families of explicit solutions for the original gas dynamics system and analyzed their trajectories.

Conclusion: Successfully extended symmetry analysis, matched subalgebras with isomorphism classes, and laid groundwork for future study of reduced systems hierarchy.

Abstract: This paper is a contribution to the symmetry analysis of the gas dynamics
system in the vein of the ''podmodeli'' (submodels) program outlined by
Ovsyannikov (1994). We consider the case of the special state equation,
prescribing pressure to be the sum of entropy and an arbitrary function of
density. Such a system has a 12-dimensional symmetry Lie algebra. This work
advances the study of its four-dimensional subalgebras, continuing the work
started in Siraeva (2024). For a large subset of not previously considered,
non-similar four-dimensional subalgebras from an optimal list in Siraeva
(2014), we compute a complete set of generating invariants. For one of the
subalgebras, we construct a partially symmetry-reduced system. We explicitly
solve this reduced system (submodel). This leads to new families of explicit
solutions of the original system. We analyze the trajectories of these
solutions. Additionally, we match each of the subalgebras considered in this
paper with its isomorphism class, planting a seed for future study of the
hierarchy of the reduced systems.

</details>


### [12] [Correlation estimates for Brownian particles with singular interactions](https://arxiv.org/abs/2510.01507)
*Mitia Duerinckx,Pierre-Emmanuel Jabin*

Main category: math.AP

TL;DR: This paper develops a new framework using linearized correlation functions to analyze particle systems with singular pairwise interactions and non-vanishing diffusion, providing the first systematic control of correlations for square-integrable interaction kernels.


<details>
  <summary>Details</summary>
Motivation: Previous approaches to describing corrections to mean-field behavior through correlation functions were limited to bounded interactions, leaving singular interactions out of reach. The goal is to extend correlation analysis to more singular interaction regimes.

Method: The authors develop a new framework based on linearized correlation functions and use the BBGKY hierarchy to derive robust bounds for systems with square-integrable interaction kernels.

Result: The method provides systematic control of correlations in singular settings, recovers optimal estimates for bounded interactions, and establishes validity of Bogolyubov correction to mean field and central limit theorem for empirical measure beyond bounded interaction regime.

Conclusion: The proposed framework successfully extends correlation analysis to singular interactions, enabling rigorous treatment of mean-field corrections and statistical properties in previously inaccessible regimes.

Abstract: We study particle systems with singular pairwise interactions and
non-vanishing diffusion in the mean-field scaling. A classical approach to
describing corrections to mean-field behavior is through the analysis of
correlation functions. For bounded interactions, the optimal estimates on
correlations are well known: the $m$-particle correlation function is
$G_{N,m}=O(N^{1-m})$ for all $m$. Such estimates, however, have remained out of
reach for more singular interactions. In this work, we develop a new framework
based on linearized correlation functions, which allows us to derive robust
bounds for systems with merely square-integrable interaction kernels, providing
the first systematic control of correlations in the singular setting. Although
at first not optimal, our estimates can be partially refined a posteriori using
the BBGKY hierarchy: in the case of bounded interactions, our method recovers
the known optimal estimates with a simplified argument. As key applications, we
establish the validity of the Bogolyubov correction to mean field and prove a
central limit theorem for the empirical measure, extending these results beyond
the bounded interaction regime for the first time.

</details>


### [13] [On the attainment of boundary data in variational problems with linear growth](https://arxiv.org/abs/2510.01515)
*David Meyer*

Main category: math.AP

TL;DR: The paper studies convex variational problems with linear growth and Dirichlet boundary conditions, showing that under mean-convexity conditions and for boundary data in BV or W^α,p with αp≥2, minimizers attain boundary data in trace sense without continuity assumptions. The methods also handle systems under quasi-isotropy assumptions.


<details>
  <summary>Details</summary>
Motivation: To address the well-known issue that convex variational problems with linear growth and Dirichlet boundary conditions might not have minimizers if boundary conditions are not properly relaxed, and to extend results to systems.

Method: The authors use relaxed variational formulations and trace theory, working under mean-convexity conditions on the boundary. They develop methods that can treat systems under quasi-isotropy assumptions on the integrand.

Result: For a wide range of integrands including least gradient and Plateau problems, minimizers attain boundary data in trace sense for BV or W^α,p data with αp≥2. Without quasi-isotropy, smooth counterexamples exist on uniformly convex domains.

Conclusion: The approach successfully handles boundary value problems for convex variational problems with linear growth, extends to systems under quasi-isotropy, and provides applications to uniqueness, ROF functional with Dirichlet conditions, and trace spaces of least gradient functions.

Abstract: It is well-known that convex variational problems with linear growth and
Dirichlet boundary conditions might not have minimizers if the boundary
condition is not suitably relaxed.
  We show that for a wide range of integrands, including the least gradient
problem and the non-parametric Plateau problem, and under suitable
mean-convexity conditions of the boundary, minimizers of the relaxed problem
attain the boundary data in the trace sense if it lies in $BV$ or
$W^{\alpha,p}$ with $\alpha p\geq 2$ without any kind of continuity assumption.
Unlike previous works, our methods are also able to treat systems under a
certain quasi-isotropy assumption on the integrand. We further show that
without this quasi-isotropy assumption, smooth counterexamples on uniformly
convex domains exist.
  Further applications to the uniqueness of minimizers and to open problems
about the ROF functional with Dirichlet boundary conditions, and to the trace
space of functions of least gradient are given.

</details>


### [14] [Inertial instability of Couette flow with Coriolis force](https://arxiv.org/abs/2510.01602)
*Yanlong Fan,Daozhi Han,Quan Wang*

Main category: math.AP

TL;DR: The paper analyzes nonlinear inertial instability of Couette flow under Coriolis forcing in 3D, showing velocity instability for specific Coriolis coefficient ranges despite no exponential eigenfunction growth.


<details>
  <summary>Details</summary>
Motivation: To understand the stability properties of Couette flow under Coriolis forcing, particularly when the linearized system lacks exponentially growing eigenfunctions but still exhibits instability through non-ideal spectral behavior.

Method: Analysis of the non-normal operator's spectrum, construction of pseudo-eigenfunctions, and bootstrap arguments to handle non-ideal spectral properties and establish Hadamard instability.

Result: Velocity instability of Couette flow is established in the Hadamard sense for Coriolis coefficients f in the interval (2/17(5-2√2), 2/17(5+2√2)), despite the linearized system having only continuous spectrum and no exponentially growing eigenfunctions.

Conclusion: Couette flow under Coriolis forcing exhibits nonlinear inertial instability through pseudo-eigenfunctions even when traditional linear stability analysis suggests stability, demonstrating the importance of non-normal operators and non-ideal spectral behavior in fluid stability problems.

Abstract: We analyze the nonlinear inertial instability of Couette flow under Coriolis
forcing in \(\mathbb{R}^{3}\). For the Coriolis coefficient \(f \in (0,1)\), we
show that the non-normal operator associated with the linearized system admits
only continuous spectrum. Hence, there are no exponentially growing
eigenfunctions for the linearized system. Instead, we construct unstable
solutions in the form of pseudo-eigenfunctions that exhibit non-ideal spectral
properties. Then through a bootstrap argument and resolving the challenges
posed by the non-ideal spectral behavior of pseudo-eigenfunctions, we establish
the velocity instability of Couette flow in the Hadamard sense for $ f \in
\Big(\frac{2}{17} \left(5-2 \sqrt{2}\right), \frac{2}{17} \left(5 + 2
\sqrt{2}\right) \Big)$.

</details>


### [15] [The weighted isoperimetric inequality and Sobolev inequality outside convex sets](https://arxiv.org/abs/2510.01647)
*Lu Chen,Jiali Lan*

Main category: math.AP

TL;DR: Extension of weighted Sobolev inequalities to capillary settings outside convex sets using weighted isoperimetric inequalities and capillary Schwarz symmetrization.


<details>
  <summary>Details</summary>
Motivation: To generalize weighted Sobolev inequalities from half-spaces to capillary settings outside convex sets, building on previous work by Ciraolo-Figalli-Roncoroni.

Method: Uses weighted capillary isoperimetric inequality with λw-ABP method, capillary Schwarz symmetrization outside convex sets, and establishes weighted Pólya-Szegő principle.

Result: Proves sharp weighted capillary Sobolev inequality outside convex domains, extending previous results from half-spaces to more general capillary settings.

Conclusion: Successfully extends weighted Sobolev inequalities to capillary contexts outside convex sets, providing a broader framework for such inequalities.

Abstract: In this paper, we establish a weighted capillary isoperimetric inequality
outside convex sets using the $\lambda_w$-ABP method. The weight function $w$
is assumed to be positive, even, and homogeneous of degree $\alpha$, such that
$w^{1/\alpha}$ is concave on $\R^n$.
  Based on the weighted isoperimetric inequality, we develop a technique of
capillary Schwarz symmetrization outside convex sets, and establish a weighted
P\'{o}lya-Szeg\"{o} principle and a sharp weighted capillary Sobolev inequality
outside convex domain. Our result can be seen as an extension of the weighted
Sobolev inequality in the half-space established by Ciraolo-Figalli-Roncoroni
in \cite{CFR}.

</details>


### [16] [On dispersive decay for the generalized Korteweg--de Vries equation](https://arxiv.org/abs/2510.01728)
*Matthew Kowalski,Minjie Shan*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We prove pointwise-in-time dispersive estimates for solutions to the
generalized Korteweg--de Vries (gKdV) equation. In particular, for solutions to
the mass-critical model, we assume only that initial data lie in
$\dot{H}^{\frac{1}{4}} \cap \dot{H}^{-\frac{1}{12}}$ and show that solutions
decay in $L^\infty$ like $|t|^{-\frac{1}{3}}$. To accomplish this, we develop a
persistence of negative regularity for solutions to gKdV and extend
Lorentz--Strichartz estimates to the mixed norm case.

</details>


### [17] [Nonlinear Forward-Backward Problems](https://arxiv.org/abs/2510.01732)
*Anne-Laure Dalibard,Frédéric Marbach,Jean Rax*

Main category: math.AP

TL;DR: Existence and uniqueness of strong solutions to a quasilinear forward-backward parabolic equation near linear shear flow, with orthogonality conditions for regularity.


<details>
  <summary>Details</summary>
Motivation: Study a quasilinear forward-backward parabolic problem that changes type across a critical curved line, with solutions having opposite signs in different domain regions, requiring careful treatment of boundary conditions and singular solutions.

Method: Developed an iterative approximation procedure with careful handling of finite singular solutions in linearized equations, proving stability of orthogonality conditions during nonlinear fixed-point scheme.

Result: Proved existence and uniqueness of strong solutions for suitable data satisfying orthogonality conditions, with solutions regular only when source terms meet these conditions.

Conclusion: Developed a natural and adaptable methodology for proving existence of regular solutions to nonlinear problems despite singular solutions at linear level, applicable to similar situations.

Abstract: We prove the existence and uniqueness of strong solutions to the equation $u
u_x - u_{yy} = f$ in the vicinity of the linear shear flow, subject to
perturbations of the source term and lateral boundary conditions. Since the
solutions we consider have opposite signs in the lower and upper half of the
domain, this is a quasilinear forward-backward parabolic problem, which changes
type across a critical curved line within the domain. In particular, lateral
boundary conditions can be imposed only where the characteristics are inwards.
There are several difficulties associated with this problem. First, the
forward-backward geometry depends on the solution itself. This requires to be
quite careful with the approximation procedure used to construct solutions.
Second, and more importantly, the linearized equations solved at each step of
the iterative scheme admit a finite number of singular solutions, of which we
provide an explicit construction. This is similar to well-known phenomena in
elliptic problems in nonsmooth domains. Hence, the solutions to the equation
are regular if and only if the source terms satisfy a finite number of
orthogonality conditions. A key difficulty of this work is to cope with these
orthogonality conditions during the nonlinear fixed-point scheme. In
particular, we are led to prove their stability with respect to the underlying
base flow. To tackle this deceivingly simple problem, we develop a methodology
which we believe to be both quite natural and adaptable to other situations in
which one wishes to prove the existence of regular solutions to a nonlinear
problem for suitable data despite the existence of singular solutions at the
linear level. This paper is a shorter version of [3].

</details>


### [18] [Notes on Schauder estimates by scaling for elliptic PDEs in divergence form](https://arxiv.org/abs/2510.01765)
*Stefano Vita*

Main category: math.AP

TL;DR: PhD course notes on classical Schauder estimates for elliptic PDEs using geometric techniques from minimal surfaces and free boundary problems.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive treatment of interior and local Schauder estimates for second-order linear elliptic PDEs in divergence form, connecting geometric methods with classical PDE theory.

Method: Uses scaling techniques, compactness and blow-up arguments combined with rigidity results (Liouville theorems), adopting geometric approaches from minimal surfaces and free boundary problems.

Result: Presents a self-contained framework for deriving Schauder estimates through geometric methods that bridge PDE theory and geometric analysis.

Conclusion: The approach demonstrates how geometric techniques developed for minimal surfaces and free boundary problems can be effectively applied to classical elliptic PDE regularity theory.

Abstract: These are the notes of a part of the PhD course Regularity for free boundary
problems and for elliptic PDEs, held in Pavia in the spring of 2025. The aim is
to provide a comprehensive and self-contained treatment of classical interior
and local Schauder estimates for second-order linear elliptic PDEs in
divergence form via scaling in the spirit of Simon's work. The main techniques
presented here are geometric in nature and were primarily developed in the
study of geometric problems such as minimal surfaces. The adopted approach
relies on compactness and blow-up arguments, combined with rigidity results
(Liouville theorems), and shares many features with the one used in the study
of free boundary problems, which was the main topic of the other part of the
PhD course.

</details>


### [19] [Strichartz and dispersive estimates for quantum bouncing ball model: exponential sums and Van der Corput methods in 1d semi-classical Schrödinger equations](https://arxiv.org/abs/2510.01779)
*Oana Ivanovici*

Main category: math.AP

TL;DR: Improved dispersive and Strichartz estimates for 1D semi-classical Schrödinger equation with linear potential on half-line, reducing losses from 1/4 to potentially 1/6+ε.


<details>
  <summary>Details</summary>
Motivation: To establish better space-time behavior understanding of solutions to the semi-classical Schrödinger equation with linear potential and Dirichlet boundary conditions, improving upon previous suboptimal estimates.

Method: Using Van der Corput-type derivative tests to prove refined Strichartz bounds, and analyzing exponential sums to potentially achieve sharper estimates.

Result: Proved improved Strichartz estimates that beat previous 1/4 losses, with potential to reduce losses to 1/6+ε for all ε>0 under sharp exponential sum bounds.

Conclusion: The refined Strichartz bounds represent significant improvement, and analogous results are expected to hold in higher dimensions within the Friedlander model domain.

Abstract: We analyze the one-dimensional semi-classical Schr\"odinger equation on the
half-line with a linear potential and Dirichlet boundary conditions. Our main
focus is on establishing improved dispersive and Strichartz estimates for this
model, which govern the space-time behavior of solutions. We prove refined
Strichartz bounds using Van der Corput-type derivative tests, beating previous
known results where Strichartz estimates incur 1/4 losses. Moreover, assuming
sharp bounds for certain exponential sums, our results indicate the possibility
to reduce these losses further to $1/6 + \epsilon$ for all $\epsilon>0$, which
would be sharp. We further expect that analogous Strichartz bounds should hold
within the Friedlander model domain in higher dimensions.

</details>


### [20] [Monotonicity and Liouville-type theorems for semilinear elliptic problems in the half space](https://arxiv.org/abs/2510.01865)
*Berardino Sciunzi,Domenico Vuono*

Main category: math.AP

TL;DR: Positive solutions to -Δu = f(u) in half-spaces with Dirichlet boundary conditions are strictly monotone increasing in the direction orthogonal to the boundary when directionally bounded on finite strips.


<details>
  <summary>Details</summary>
Motivation: To establish monotonicity properties of solutions to semilinear elliptic equations in half-spaces and derive new Liouville-type theorems.

Method: Analysis of classical solutions to -Δu = f(u) in half-spaces under homogeneous Dirichlet boundary conditions, with the key assumption of directional boundedness on finite strips.

Result: Proved that any positive solution is strictly monotone increasing in the direction orthogonal to the boundary. Derived a new Liouville-type theorem for the Lane-Emden equation as a corollary.

Conclusion: The paper establishes strong monotonicity results for semilinear elliptic equations in half-spaces and provides new Liouville-type theorems, advancing the understanding of solution behavior in unbounded domains.

Abstract: We consider classical solutions to $-\Delta u = f(u)$ in half-spaces, under
homogeneous Dirichlet boundary conditions. We prove that any positive solution
is strictly monotone increasing in the direction orthogonal to the boundary,
provided that it is directionally bounded on finite strips. As a corollary, we
deduce a new Liouville-type theorem for the Lane-Emden equation.

</details>


### [21] [On sharp Strichartz estimate for hyperbolic Schrödinger equation on $\mathbb{T}^3$](https://arxiv.org/abs/2510.01886)
*Baoping Liu,Xu Zheng*

Main category: math.AP

TL;DR: Sharp Strichartz estimate for hyperbolic Schrödinger equation on 3D torus via incidence geometry, leading to optimal local well-posedness for nonlinear versions.


<details>
  <summary>Details</summary>
Motivation: To establish precise mathematical bounds for hyperbolic Schrödinger equations on three-dimensional torus domains, which are fundamental in understanding wave propagation and quantum mechanics in periodic settings.

Method: Uses incidence geometry approach to prove sharp Strichartz estimates for hyperbolic Schrödinger equation on 𝕋³.

Result: Successfully proved the sharp Strichartz estimate for hyperbolic Schrödinger equation on three-dimensional torus.

Conclusion: The sharp estimates enable optimal local well-posedness results for nonlinear hyperbolic Schrödinger equations, advancing the mathematical theory of these important PDEs.

Abstract: We prove the sharp Strichartz estimate for hyperbolic Schr\"{o}dinger
equation on $\mathbb{T}^3 $ via an incidence geometry approach. As application,
we obtain optimal local well-posedness of nonlinear hyperbolic Schr\"{o}dinger
equations.

</details>


### [22] [A note on the recovery sequence in the double gradient model for phase transitions](https://arxiv.org/abs/2510.01893)
*Jakob Deutsch*

Main category: math.AP

TL;DR: Analysis of the limsup inequality in the double gradient model for phase transitions using Modica-Mortola functionals with double-well potentials in 2D.


<details>
  <summary>Details</summary>
Motivation: To characterize the limiting interfacial energy in phase transition models governed by double-well potentials and understand the behavior as the regularization parameter approaches zero.

Method: Using energy functionals with double-well potentials and second-order gradients, analyzing recovery sequences and cell problems on unit cubes with bounds on optimal profile constants.

Result: Successfully characterized the limiting interfacial energy through periodic recovery sequences as the regularization parameter ε approaches zero.

Conclusion: The study provides a complete characterization of the interfacial energy limit in the double gradient phase transition model under the given assumptions.

Abstract: We investigate the $\limsup$ inequality in the double gradient model for
phase transitions governed by a Modica--Mortola functional with a double-well
potential in two dimensions. Specifically, we consider energy functionals of
the form \[ E_\varepsilon(u, \Omega) = \int_\Omega \left( \frac{1}{\varepsilon}
W(\nabla u) + \varepsilon |\nabla^2 u|^2 \right) dx \] for maps $ u \in
H^2(\Omega; \mathbb{R}^2) $, where $ W $ vanishes only at two wells. Assuming a
bound on the optimal profile constant -- namely the cell problem on the unit
cube -- in terms of the geodesic distance between the two wells, we
characterise the limiting interfacial energy via periodic recovery sequences as
$\varepsilon \to 0^+$.

</details>


### [23] [Subwavelength resonances in two-dimensional elastic media with high contrast](https://arxiv.org/abs/2510.01911)
*Yuanchun Ren,Yixian Gao*

Main category: math.AP

TL;DR: This paper uses layer potential techniques to study wave scattering in 2D elastic media with high parameter contrasts, characterizing resonant frequencies, analyzing scattered fields, and examining subwavelength bandgaps in phononic crystals.


<details>
  <summary>Details</summary>
Motivation: To investigate wave scattering phenomena in two-dimensional elastic media that exhibit significant contrasts in both Lamé parameters and density, which is relevant for understanding wave propagation in heterogeneous materials.

Method: Employed layer potential techniques and boundary integral operators, constructed an invertible operator based on kernel spaces, used asymptotic analysis to derive resonant frequency equations, and analyzed scattered fields across different frequency regimes.

Result: Developed characterization of resonant frequencies through orthogonality conditions, derived equations for leading-order resonant frequencies, analyzed interior scattered fields and exterior far-field patterns, and examined subwavelength bandgaps in dilute phononic crystals.

Conclusion: The study provides comprehensive analytical tools for understanding wave scattering in high-contrast elastic media, with applications to phononic crystals and metamaterials design.

Abstract: This paper employs layer potential techniques to investigate wave scattering
in two-dimensional elastic media exhibiting high contrasts in both Lam\'{e}
parameters and density. Our contributions are fourfold. First, we construct an
invertible operator based on the kernel spaces of boundary integral operators,
which enables the characterization of resonant frequencies through an
orthogonality condition. Second, we use asymptotic analysis to derive the
equation governing the leading-order terms of these resonant frequencies.
Third, we analyze the scattered field in the interior domain for incident
frequencies across different regimes and characterize the longitudinal and
transverse far-field patterns in the exterior domain. Finally, we examine the
subwavelength bandgap in the phononic crystal with a dilute structure.

</details>


### [24] [Low regularity Sobolev well-posedness for Vlasov--Poisson](https://arxiv.org/abs/2510.02112)
*In-Jee Jeong,Sangwook Tae*

Main category: math.AP

TL;DR: Local well-posedness of Vlasov-Poisson equation in H^s spaces with s > n/2 - 1/4 for n ≥ 3, allowing data not in L^p for large p.


<details>
  <summary>Details</summary>
Motivation: To establish local well-posedness for the Vlasov-Poisson equation in higher-dimensional settings with lower regularity requirements than previously known, particularly allowing initial distributions that don't belong to L^p spaces for large p.

Method: Analysis of the Vlasov-Poisson equation on R^n × R^n using Sobolev space techniques, specifically working in H^s spaces with compact support in velocity variable v.

Result: Proved local well-posedness in H^s(R^n × R^n) with s > n/2 - 1/4 for n ≥ 3, for initial distributions f_0 ∈ H^s with compact support in v.

Conclusion: The Vlasov-Poisson equation is locally well-posed in lower regularity Sobolev spaces than previously established, expanding the class of admissible initial data.

Abstract: We consider the Vlasov--Poisson equation on $\mathbb{R}^n \times
\mathbb{R}^n$ with $n \ge 3$. We prove local well-posedness in
$H^{s}(\mathbb{R}^n \times \mathbb{R}^n)$ with $s> n/2-1/4$, for initial
distribution $f_{0} \in H^{s}(\mathbb{R}^n \times \mathbb{R}^n)$ having compact
support in $v$. In particular, data not belonging to $L^p(\mathbb{R}^n \times
\mathbb{R}^n)$ for large $p$ are allowed.

</details>


### [25] [Transfer of Stability from the Classical to the Fractional Anisotropic Calderón Problem](https://arxiv.org/abs/2510.02242)
*Hendrik Baers,Angkana Rüland*

Main category: math.AP

TL;DR: This paper analyzes spectral fractional anisotropic Calderón problems with source-to-solution measurements and establishes quantitative relationships between local and nonlocal Calderón problems, providing stability estimates and uniqueness transfers.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between local and nonlocal Calderón problems by quantifying their relationships and establishing stability results for fractional Calderón type problems where no Liouville-type reduction is known.

Method: Uses quantitative unique continuation arguments to relate local and nonlocal Calderón problems, and proves quantitative relations between source-to-solution and Dirichlet-to-Neumann measurements for classical Calderón problems.

Result: Established that any stability result for the local problem with source-to-solution data has a direct nonlocal analogue (with logarithmic loss), and obtained first stability results for the principal part of fractional Calderón problems without known Liouville-type reductions.

Conclusion: The paper successfully quantifies the relationship between local and nonlocal Calderón problems, provides stability estimates, and demonstrates that uniqueness properties can be transferred from local to nonlocal settings using quantitative unique continuation techniques.

Abstract: We discuss two spectral fractional anisotropic Calder\'on problems with
source-to-solution measurements and their quantitative relation to the
classical Calder\'on problem. Firstly, we consider the anistropic fractional
Calder\'on problem from [FGKU25]. In this setting, we quantify the relation
between the local and nonlocal Calder\'on problems which had been deduced in
[R25] and provide an associated stability estimate. As a consequence, any
stability result which holds on the level of the local problem with
source-to-solution data has a direct nonlocal analogue (up to a logarithmic
loss). Secondly, we introduce and discuss the fractional Calder\'on problem
with source-to-solution measurements for the spectral fractional Dirichlet
Laplacian on open, bounded, connected, Lipschitz sets on $\mathbb{R}^n$. Also
in this context, we provide a qualitative and quantitative transfer of
uniqueness from the local to the nonlocal setting. As a consequence, we infer
the first stability results for the principal part for a fractional Calder\'on
type problem for which no reduction of Liouville type is known. Our arguments
rely on quantitative unique continuation arguments. As a result of independent
interest, we also prove a quantitative relation between source-to-solution and
Dirichlet-to-Neumann measurements for the classical Calder\'on problem.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [26] [Integrated Software/Hardware Execution Models for High-Accuracy Methods in Chemistry](https://arxiv.org/abs/2510.01205)
*Nicholas Bauman,Ajay Panyala,Libor Veis,Jiri Brabec,Paul Rigor,Randy Meyer,Skyler Windh,Craig Warner,Tony Brewer,Karol Kowalski*

Main category: physics.comp-ph

TL;DR: The paper presents a hybrid approach combining Micron CXL memory technology and Azure Quantum Element cloud computing for efficient DMRG-DUCC quantum chemistry simulations, optimizing resource usage based on workflow component requirements.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimally utilizing diverse computational resources for quantum chemistry simulations, particularly for memory-intensive coupled-cluster calculations and DMRG simulations.

Method: Developed a hybrid workflow using Micron CXL hardware for memory-intensive CC downfolding phase and Azure Quantum Element cloud computing for less resource-intensive DMRG simulations, leveraging the DMRG-DUCC approach with double unitary coupled cluster Ansatz.

Result: The approach enables efficient resource allocation by matching hardware capabilities to specific workflow components, with performance analysis conducted using the scalable ExaChem suite on Micron prototype systems.

Conclusion: The synergistic combination of specialized hardware technologies provides an effective strategy for optimizing quantum chemistry simulations by properly addressing the distinct computational requirements of different workflow components.

Abstract: The effective deployment and application of advanced methodologies for
quantum chemistry is inherently linked to the optimal usage of emerging and
highly diversified computational resources. This paper examines the synergistic
utilization of Micron memory technologies and Azure Quantum Element cloud
computing in Density Matrix Renormalization Group (DMRG) simulations leveraging
coupled-cluster (CC) downfolded/effective Hamiltonians based on the double
unitary coupled cluster (DUCC) Ansatz. We analyze the performance of the
DMRG-DUCC workflow, emphasizing the proper choice of hardware that reflects the
numerical overheads associated with specific components of the workflow. We
report a hybrid approach that takes advantage of Micron CXL hardware for the
memory capacity intensive CC downfolding phase while employing AQE cloud
computing for the less resource-intensive DMRG simulations. Furthermore, we
analyze the performance of the scalable ExaChem suite of electronic simulations
conducted on Micron prototype systems.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [Detailed Derivation of the Scalar Explicit Expressions Governing the Electric Field, Current Density, and Volumetric Power Density in the Four Types of Linear Divergent MHD Channels Under a Unidirectional Applied Magnetic Field](https://arxiv.org/abs/2510.01289)
*Osama A. Marzouk*

Main category: physics.plasm-ph

TL;DR: Analytical derivation of electric field, current density, and power density for four MHD channel types in Open-Cycle Magnetohydrodynamic generators, with performance comparisons under typical operational conditions.


<details>
  <summary>Details</summary>
Motivation: To provide analytical expressions for electric field, current density, and power density in MHD channels to aid in selecting the most suitable channel type for specific applications in plasma physics and electric power generation.

Method: Mathematical analysis of algebraic equations governing electric field and current density vectors in MHD linear two-dimensional divergent supersonic channels, deriving analytical expressions for four common channel types.

Result: Under typical conditions (5 S/m conductivity, 5 T magnetic field, 2000 m/s plasma speed, 0.5 load factor): Continuous-electrode Faraday channel achieves 5 kV/m electric field, 12.5 kA/m² current density, 62.5 MW/m³ power density, 50% efficiency; Hall-linear channel achieves 25 kV/m, 4.808 kA/m², 120.19 MW/m³, 46.30% efficiency.

Conclusion: The analytical expressions and performance comparisons help determine the optimal MHD channel type for specific applications, with Hall-linear channels showing higher power density but slightly lower efficiency compared to continuous-electrode Faraday channels.

Abstract: The current study belongs to the field of applied mathematics in plasma
physics and electric power, where mathematical analysis of the algebraic
equations governing the electric field vector, and the electric-current density
field vector within a Magnetohydrodynamic (MHD) linear two-dimensional
divergent supersonic channel is utilized to derive analytical expressions for
these important fields, as well as closed-form equations for the volumetric
power density (output electric power per unit volume of the plasma channel).
The expressions presented here describe analytically the operation of the MHD
channel as an electric power source within an Open-Cycle Magnetohydrodynamic
(OCMHD) generator. The four common types of the MHD linear channels are covered
here: namely, (1) continuous-electrode Faraday channel, (2) linear Hall
channel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode
channel. The mathematical results, their detailed derivation, and the companion
graphical illustrations aid in making a proper decision regarding which channel
type is the most suitable for a given application.Under typical operational
conditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000
m/s plasma speed, as well as an optimized load factor of 0.5, we estimate the
following numerical values (unsigned magnitudes) for the continuous-electrode
Faraday channel (with a Hall parameter of 1): useful electric field (across the
external electric load): 5 kV/m, useful electric current-density (between the
terminal electrodes within the channel): 12.5 kA/m2 , volumetric power density
(dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric
efficiency (for the electric field or voltage): 50%. For the Halllinear channel
(with a Hall parameter of 5), these quantitative performance values become25
kV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%.

</details>


### [28] [Suppression of inverse magnetic energy transfer in collisionless marginally magnetized plasmas](https://arxiv.org/abs/2510.01573)
*Zhuo Liu,Muni Zhou,Nuno F. G. Loureiro*

Main category: physics.plasm-ph

TL;DR: Inverse magnetic energy cascade in collisionless plasmas is suppressed by pressure-anisotropy-driven instabilities, particularly firehose instability, which nullifies magnetic tension and prevents reconnection-driven coalescence of magnetic structures.


<details>
  <summary>Details</summary>
Motivation: To understand how magnetic energy transfers to larger scales in collisionless plasmas, particularly in astrophysical contexts where Weibel-generated seed fields may play a role in cosmic magnetogenesis.

Method: First-principles numerical simulations and analytical theory investigating decaying collisionless plasmas with moderate to high-β values.

Result: Firehose instability suppresses inverse energy transfer by nullifying magnetic tension, leaving magnetic structures elongated and confined to Larmor-radius scales. Guide fields or larger scale separation can restore inverse transfer capability.

Conclusion: Inverse energy transfer in collisionless plasmas is not guaranteed and sensitively depends on magnetization, identifying a kinetic mechanism that may limit Weibel-generated seed fields' role in cosmic magnetogenesis.

Abstract: We investigate the inverse cascade of magnetic energy in decaying,
collisionless plasmas with moderate to high-$\beta$ values via first-principles
numerical simulations and analytical theory. We find that
pressure-anisotropy-driven instabilities, in particular the firehose
instability, suppress reconnection-driven coalescence of magnetic structures
(i.e., inverse transfer) by nullifying magnetic tension. This suppression
leaves such structures elongated and confined to scales comparable to the
Larmor radius of the particles. The presence of a magnetic guide field of
sufficient strength, or a greater scale separation between the initial size of
the magnetic structures and the Larmor radius, restores the system's ability to
inverse transfer magnetic energy. These results reveal that inverse energy
transfer in collisionless plasmas is not guaranteed, but instead sensitively
depends on magnetization. In the astrophysical context, this identifies a
kinetic mechanism by which Weibel-generated seed fields may fail to merge
consistently, potentially limiting their role in cosmic magnetogenesis.

</details>


### [29] [Accelerating kinetic plasma simulations with machine learning generated initial conditions](https://arxiv.org/abs/2510.01977)
*Andrew T. Powis,Domenica Corona Rivera,Alexander Khrabry,Igor D. Kaganovich*

Main category: physics.plasm-ph

TL;DR: Machine learning accelerates kinetic plasma simulations by generating initial conditions, achieving up to 17.1x speedup in convergence time for capacitively coupled plasma discharge modeling.


<details>
  <summary>Details</summary>
Motivation: Multi-time-scale plasma systems require many time steps to reach quasi-steady state, making computer-aided engineering challenging. Machine learning combined with traditional simulations offers pathways to resolve this computational challenge.

Method: Three machine learning models (multi-layer perceptron, principal component analysis, convolutional neural networks) are trained on simulations across device driving frequency and pressure parameters to predict final time-averaged profiles of ion-density and velocity distribution functions.

Result: The data-driven initial condition generators provide mean speedup of 17.1x with offline procedure or 4.4x with online procedure. Convolutional neural networks performed best among the three model types.

Conclusion: The paper presents a workflow for continuous data-driven model improvement and simulation speedup, aiming to generate sufficient data for full device digital twins in plasma systems.

Abstract: Computer aided engineering of multi-time-scale plasma systems which exhibit a
quasi-steady state solution are challenging due to the large number of time
steps required to reach convergence. Machine learning techniques combined with
traditional first-principles simulations and high-performance computing offer
many interesting pathways towards resolving this challenge. We consider
acceleration of kinetic plasma simulations via machine learning generated
initial conditions. The approach is demonstrated through modeling of
capacitively coupled plasma discharges relevant to the microelectronics
industry. Three models are trained on simulations across a parameter space of
device driving frequency and operating pressure. The models incorporate
elements of a multi-layer perceptron, principal component analysis, and
convolutional neural networks to predict the final time-averaged profiles of
ion-density and velocity distribution functions. These data-driven initial
condition generators (ICGs) provide a mean speedup of 17.1x in convergence
time, when measured using an offline procedure, or a 4.4x speedup with an
online procedure, with convolutional neural networks leading to the best
performance. The paper also outlines a workflow for continuous data-driven
model improvement and simulation speedup, with the aim of generating sufficient
data for full device digital twins.

</details>


### [30] [A neural network approach to kinetic Mie polarimetry for particle size diagnostics in nanodusty plasmas](https://arxiv.org/abs/2510.02088)
*Alexander Schmitz,Andreas Petersen,Franko Greiner*

Main category: physics.plasm-ph

TL;DR: A neural network is introduced to analyze nanoparticle sizes in plasma technology, replacing traditional back-calculation methods that require user expertise.


<details>
  <summary>Details</summary>
Motivation: Standard light scattering techniques for nanoparticle size analysis require expertise and are not automated. The paper aims to develop a more accessible and efficient method.

Method: A neural network is trained to analyze the size of plasma-grown amorphous carbon nanoparticles using light scattering data based on Mie theory.

Result: The neural network achieves results comparable to prior fitting algorithms with higher stability, faster computation, and better automation.

Conclusion: The neural network approach offers a superior alternative to traditional methods, providing methodical stability, speed, and automation for nanoparticle size analysis.

Abstract: The analysis of the size of nanoparticles is an essential task in plasma
technology and dusty plasmas. Light scattering techniques, based on Mie theory,
can be used as a non-invasive and in-situ diagnostic tool for this purpose.
However, the standard back-calculation methods require expertise from the user.
To address this, we introduce a neural network that performs the same task. We
discuss how we set up and trained the network to analyze the size of
plasma-grown amorphous carbon nanoparticles (a:C-H) with a refractive index n
in the range of real(n) = 1.4-2.2 and imag(n) = 0.04i-0.1i and a radius of up
to several hundred nanometers, depending on the used wavelength. The diagnostic
approach is kinetic, which means that the particles need to change in size due
to growth or etching. An uncertainty analysis as well as a test with
experimental data are presented. Our neural network achieves results that agree
with those of prior fitting algorithms while offering higher methodical
stability. The model also holds a major advantage in terms of computing speed
and automation.

</details>


<div id='nlin.SI'></div>

# nlin.SI [[Back]](#toc)

### [31] [Non-commutative multiple bi-orthogonal polynomials: formal approach and integrability](https://arxiv.org/abs/2510.02207)
*Adam Doliwa*

Main category: nlin.SI

TL;DR: The paper introduces non-commutative multiple bi-orthogonal polynomial systems, which generalize multiple orthogonality, matrix orthogonal polynomials, and bi-orthogonality, and shows their connections to integrable systems.


<details>
  <summary>Details</summary>
Motivation: To generalize concepts of multiple orthogonality, matrix orthogonal polynomials, and bi-orthogonality into a unified non-commutative framework and explore their connections to integrable systems theory.

Method: Defines non-commutative multiple bi-orthogonal polynomial systems, presents quasideterminantal expressions using formal bi-moments, and studies normalization functions satisfying non-commutative Hirota equations.

Result: The polynomial systems satisfy non-commutative Hirota equations and provide solutions to corresponding linear systems, establishing their role in integrable systems theory. Specialization to non-commutative multiple orthogonal polynomials yields Hankel-type quasideterminantal expressions.

Conclusion: The introduced polynomial systems form part of integrable systems theory, with specialization leading to non-commutative versions of multidimensional discrete-time Toda equations.

Abstract: We define the non-commutative multiple bi-orthogonal polynomial systems,
which simultaneously generalize the concepts of multiple orthogonality, matrix
orthogonal polynomials and of the bi-orthogonality. We present
quasideterminantal expressions for such polynomial systems in terms of formal
bi-moments. The normalization functions for such monic polynomials satisfy the
non-commutative Hirota equations, while the polynomials provide solution of the
corresponding linear system. This shows, in particular, that our polynomial
systems form a part of the theory of integrable systems. We study also a
specialization of the problem to non-commutative multiple orthogonal
polynomials, what results in the corresponding Hankel-type quasideterminantal
expressions in terms of the moments. Moreover, such a reduction allows to
introduce in a standard way the discrete-time variable and gives rise to an
integrable system which is non-commutative version of the multidimensional
discrete-time Toda equations.

</details>


### [32] [The noncommutative KP hierarchy and its solution via descent algebra](https://arxiv.org/abs/2510.01352)
*Gordon Blower,Simon J. A. Malham*

Main category: nlin.SI

TL;DR: The paper provides a complete solution to the noncommutative KP hierarchy using direct linearisation via the GLM equation, with two approaches: Sato-Wilson dressing transformation and Poppe's semi-additive scattering data method.


<details>
  <summary>Details</summary>
Motivation: To solve the complete noncommutative Kadomtsev-Petviashvili (KP) hierarchy, which is an important integrable system in mathematical physics.

Method: Uses direct linearisation through the Gelfand-Levitan-Marchenko (GLM) equation with two approaches: (1) standard Sato-Wilson dressing transformation, (2) Poppe's approach using semi-additive scattering data and augmented pre-Poppe algebra (nonassociative descent algebra).

Result: Shows that the solution to the GLM equation coincides with the solution to the noncommutative KP hierarchy, establishing the complete solution to the hierarchy.

Conclusion: The second approach using Poppe's method is constructive, explicit, reveals combinatorial structures, and shows the solution mechanism, with final results residing in the natural associative subalgebra.

Abstract: We give the solution to the complete noncommutative Kadomtsev--Petviashvili
(KP) hierarchy. We achieve this via direct linearisation which involves the
Gelfand--Levitan--Marchenko (GLM) equation. This is a linear integral equation
in which the scattering data satisfies the linearised KP hierarchy. The
solution to the GLM equation is then shown to coincide with the solution to the
noncommutative KP hierarchy. We achieve this using two approaches. In the first
approach we use the standard Sato-Wilson dressing transformation. In the second
approach, which was pioneered by Poppe, we assume the scattering data is
semi-additive and by direct substitution, we show that the solution to the GLM
equation satisfies the infinite set of field equations representing the
noncommutative KP hierarchy. This approach relies on the augmented pre-Poppe
algebra. This is a representative algebra that underlies the field equations
representing the hierarchy. It is nonassociative and isomorphic to a descent
algebra equipped with a grafting product. While we perform computations in the
nonassociative descent algebra, the final result which establishes the solution
to the complete hierarchy, resides in the natural associative subalgebra. The
advantages of this second approach are that it is constructive, explicit,
highlights the underlying combinatorial structures within the hierarchy, and
reveals the mechanisms underlying the solution procedure.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [33] [On Lieb-Thirring inequalities for multidimensional Schrödinger operators with complex potentials](https://arxiv.org/abs/2510.02192)
*Sabine Bögli,Sukrid Petpradittha,František Štampach*

Main category: math.SP

TL;DR: Counter-example construction disproves generalization of Lieb-Thirring inequality for complex-valued potentials in higher dimensions.


<details>
  <summary>Details</summary>
Motivation: To solve the open problem by Demuth, Hansmann, and Katriel regarding generalization of Lieb-Thirring inequality to complex-valued potentials.

Method: Counter-example construction, generalizing the one-dimensional counter-example to higher dimensions.

Result: Successfully constructed counter-example showing the inequality does not hold for complex-valued potentials in higher dimensions.

Conclusion: The Lieb-Thirring inequality cannot be generalized to complex-valued potentials in higher dimensions.

Abstract: We solve the open problem by Demuth, Hansmann, and Katriel announced in
[Integr. Equ. Oper. Theory 75 (2013), 1-5] by a counter-example construction.
The problem concerns a possible generalisation of the Lieb-Thirring inequality
for Schr\"odinger operators in to the case of complex-valued potentials. A
counter-example has already been found for the one-dimensional case by the
first and third authors in [J. Spectr. Theory 11 (2021), 1391-1413]. Here we
generalise the counter-example to higher dimensions.

</details>


### [34] [Optimal Lieb-Thirring type inequalities for Schrödinger and Jacobi operators with complex potentials](https://arxiv.org/abs/2510.02288)
*Sabine Bögli,Sukrid Petpradittha*

Main category: math.SP

TL;DR: Optimal Lieb-Thirring inequalities for Schrödinger and Jacobi operators with complex potentials, bounding eigenvalue power sums by L^p norms with essential spectrum-dependent weights.


<details>
  <summary>Details</summary>
Motivation: To extend Lieb-Thirring inequalities from self-adjoint to complex potential operators, addressing the need for different weighting due to complex eigenvalues and their relationship to the essential spectrum.

Method: Prove optimal bounds for eigenvalue power sums weighted by functions of the distance to essential spectrum endpoints, and establish divergence estimates for non-integrable weights to demonstrate optimality.

Result: Achieved optimal Lieb-Thirring type inequalities that hold only for integrable weight functions, with divergence rates showing logarithmic/polynomial improvements over semiclassical methods for real potentials.

Conclusion: Complex potentials require essential spectrum-dependent weighting in Lieb-Thirring inequalities, and the established bounds are optimal with improved divergence behavior compared to real potential cases.

Abstract: We prove optimal Lieb-Thirring type inequalities for Schr\"odinger and Jacobi
operators with complex potentials. Our results bound eigenvalue power sums
(Riesz means) by the $L^p$ norm of the potential, where in contrast to the
self-adjoint case, each term needs to be weighted by a function of the ratio of
the distance of the eigenvalue to the essential spectrum and the distance to
the endpoint(s) thereof. Our Lieb-Thirring type bounds only hold for integrable
weight functions. To prove optimality, we establish divergence estimates for
non-integrable weight functions. The divergence rates exhibit a logarithmic or
even polynomial gain compared to semiclassical methods (Weyl asymptotics) for
real potentials.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [35] [Multiscale analysis of large twist ferroelectricity and swirling dislocations in bilayer hexagonal boron nitride](https://arxiv.org/abs/2510.01419)
*Md Tusher Ahmed,Chenhaoyue Wang,Amartya S. Banerjee,Nikhil Chandra Admal*

Main category: cond-mat.mtrl-sci

TL;DR: This paper demonstrates that ferroelectricity persists in bilayer hBN under large heterodeformations, not just small ones. It establishes the crystallographic origin using Smith normal form bicrystallography and develops a multiscale model to predict ferroelectricity in large-unit-cell heterostructures.


<details>
  <summary>Details</summary>
Motivation: Previous studies focused only on small heterodeformations in bilayer hBN, leaving the persistence of ferroelectricity under large heterodeformations unexplored. This work aims to fill this gap and understand ferroelectric behavior across different stacking configurations.

Method: Used Smith normal form bicrystallography to establish crystallographic origin, performed atomistic simulations for AA-vicinal systems, and developed a density-functional-theory-informed continuum framework (BFIM model) for Σ7-vicinal systems where atomistic potentials are unreliable.

Result: Demonstrated out-of-plane ferroelectricity in bilayer hBN across configurations vicinal to both AA and Σ7 stacking. Found that AA-vicinal systems support ferroelectricity under small twist and strain, with different polarization switching mechanisms. Interface dislocations in large heterodeformations have smaller Burgers vectors.

Conclusion: Ferroelectricity persists in bilayer hBN under large heterodeformations. The BFIM model provides an efficient computational framework for predicting ferroelectricity in large-unit-cell heterostructures where atomistic simulations are too expensive.

Abstract: With its atomically thin structure and intrinsic ferroelectric properties,
heterodeformed bilayer hexagonal boron nitride (hBN) has gained prominence in
next-generation non-volatile memory applications. However, studies to date have
focused almost exclusively on small heterodeformations, leaving the question of
whether ferroelectricity can persist under large heterodeformation entirely
unexplored. In this work, we establish the crystallographic origin of
ferroelectricity in bilayer hBN configurations heterodeformed relative to
high-symmetry configurations such as the AA-stacking and the 21.786789 $\circ$
twisted configuration, using Smith normal form bicrystallography. We then
demonstrate out-of-plane ferroelectricity in bilayer hBN across configurations
vicinal to both the AA and $\Sigma 7$ stacking. Atomistic simulations reveal
that AA-vicinal systems support ferroelectricity under both small twist and
small strain, with polarization switching in the latter governed by the
deformation of swirling dislocations rather than the straight interface
dislocations seen in the former. For $\Sigma 7$-vicinal systems, where reliable
interatomic potentials are lacking, we develop a
density-functional-theory-informed continuum framework--the
bicrystallography-informed frame-invariant multiscale (BFIM) model, which
captures out-of-plane ferroelectricity in heterodeformed configurations vicinal
to the $\Sigma 7$ stacking. Interface dislocations in these large
heterodeformed bilayer configurations exhibit markedly smaller Burgers vectors
compared to the interface dislocations in small-twist and small-strain bilayer
hBN. The BFIM model reproduces atomistic simulation results and provides a
powerful, computationally efficient framework for predicting ferroelectricity
in large-unit-cell heterostructures where atomistic simulations are
prohibitively expensive.

</details>


### [36] [Enhancing the Efficiency of Time-Dependent Density Functional Theory Calculations of Dynamic Response Properties](https://arxiv.org/abs/2510.01875)
*Zhandos A. Moldabekov,Sebastian Schwalbe,Uwe Hernandez Acosta,Thomas Gawne,Jan Vorberger,Michele Pavanello,Tobias Dornheim*

Main category: cond-mat.mtrl-sci

TL;DR: A method to speed up TDDFT calculations for X-ray Thomson scattering by using imaginary time mapping and noise attenuation, achieving up to 10x speed-up.


<details>
  <summary>Details</summary>
Motivation: TDDFT is accurate for modeling XRTS spectra under extreme conditions, but computational costs are high due to thermal excitations, temperature/density variations, and detector size effects.

Method: Uses mapping between dynamic structure factor and imaginary time density-density correlation function from Feynman's path integral theory, combined with convergence tests and constraints-based noise attenuation.

Result: Achieves up to an order of magnitude speed-up in TDDFT calculations, potentially saving millions of CPU hours for single XRTS measurements.

Conclusion: The method provides efficient TDDFT modeling for extreme conditions without significant bias, enabling more practical XRTS analysis.

Abstract: X-ray Thomson scattering (XRTS) constitutes an essential technique for
diagnosing material properties under extreme conditions, such as high pressures
and intense laser heating. Time-dependent density functional theory (TDDFT) is
one of the most accurate available ab initio methods for modeling XRTS spectra,
as well as a host of other dynamic material properties. However, strong thermal
excitations, along with the need to account for variations in temperature and
density as well as the finite size of the detector significantly increase the
computational cost of TDDFT simulations compared to ambient conditions. In this
work, we present a broadly applicable method for optimizing and enhancing the
efficiency of TDDFT calculations. Our approach is based on a one-to-one mapping
between the dynamic structure factor and the imaginary time density--density
correlation function, which naturally emerges in Feynman's path integral
formulation of quantum many-body theory. Specifically, we combine rigorous
convergence tests in the imaginary time domain with a constraints-based noise
attenuation technique to improve the efficiency of TDDFT modeling without the
introduction of any significant bias. As a result, we can report a speed-up by
up to an order of magnitude, thus potentially saving millions of CPU hours for
modeling a single XRTS measurement of matter under extreme conditions.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [37] [Numerical tests of formulae for volume enclosed by flux surfaces of integrable magnetic fields](https://arxiv.org/abs/2510.01957)
*David Martinez-del-Rio,Robert S. MacKay*

Main category: math.DS

TL;DR: Numerical tests of volume formulae for integrable 3D vector fields with various symmetries, including a new proposed case.


<details>
  <summary>Details</summary>
Motivation: To efficiently compute volumes enclosed between flux surfaces for integrable 3D vector fields with different symmetry levels.

Method: Present numerical tests of volume formulae and propose a new case for testing.

Result: Successful numerical testing of volume formulae for various symmetry cases.

Conclusion: The volume formulae are effective for computing volumes in integrable 3D vector fields with different symmetry properties.

Abstract: Numerical tests of volume formulae are presented to compute efficiently the
volume enclosed between flux surfaces for integrable 3D vector fields with
various degrees of symmetry. In the process, a new case is proposed and tested.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [38] [The centered maximal operator removes the non-concave Cantor part from the gradient](https://arxiv.org/abs/2510.01936)
*Panu Lahti,Julian Weigt*

Main category: math.CA

TL;DR: The paper studies the regularity of the centered Hardy-Littlewood maximal function for functions of bounded variation in R^d, showing conditions under which BV regularity can be upgraded to Sobolev regularity.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of the Hardy-Littlewood maximal function for functions of bounded variation, particularly when BV regularity can be improved to Sobolev regularity.

Method: Analyze the behavior of the maximal function at points where the function has non-concave blow-ups, and study conditions on the variation measure.

Result: At |D^c f|-a.e. point where f has a non-concave blow-up, M f(x) > f*(x). If the variation measure has no jump part and its Cantor part has non-concave blow-ups, then BV regularity of M f implies Sobolev regularity.

Conclusion: The presence of non-concave blow-ups in the Cantor part of the variation measure (without jump part) enables the upgrade from BV to Sobolev regularity for the Hardy-Littlewood maximal function.

Abstract: We study regularity of the centered Hardy--Littlewood maximal function $M f$
of a function $f$ of bounded variation in $\mathbb R^d$, $d\in \mathbb N$. In
particular, we show that at $|D^c f|$-a.e. point $x$ where $f$ has a
non-concave blow-up, it holds that $M f(x)>f^*(x)$. We further deduce from this
that if the variation measure of $f$ has no jump part and its Cantor part has
non-concave blow-ups, then BV regularity of $M f$ can be upgraded to Sobolev
regularity.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [39] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: SPUS is a compact foundation model using lightweight residual U-Net architecture for solving diverse PDEs, achieving SOTA generalization with fewer parameters and minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing PDE foundation models rely on large transformer architectures with high computational overhead, creating a need for more parameter-efficient alternatives.

Method: Uses lightweight residual U-Net architecture with auto-regressive pretraining strategy that replicates numerical solver behavior, pretrained on diverse fluid dynamics PDEs.

Result: Achieves state-of-the-art generalization on 6 challenging unseen downstream PDEs while requiring significantly fewer parameters and minimal fine-tuning data.

Conclusion: SPUS demonstrates potential as a highly parameter-efficient foundation model for solving diverse PDE systems, making U-Net a viable alternative to transformer-based architectures.

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems](https://arxiv.org/abs/2510.01396)
*Wasut Pornpatcharapong*

Main category: cs.LG

TL;DR: A neural network surrogate framework that learns collective variables from Cartesian coordinates and provides Jacobians via automatic differentiation, enabling gradient-based free energy methods to use complex CVs without analytical forms.


<details>
  <summary>Details</summary>
Motivation: Traditional free energy reconstruction methods require analytical Jacobians of collective variables, which restricts the use of complex or machine-learned CVs in simulations.

Method: Neural network surrogate framework that learns CVs directly from Cartesian coordinates and uses automatic differentiation to compute Jacobians, bypassing the need for analytical forms.

Result: Achieved high accuracy for both simple distance CV and complex coordination-number CV on MgCl2 ion-pairing system. Jacobian errors followed near-Gaussian distribution suitable for GPR pipelines.

Conclusion: This framework enables gradient-based free energy methods to incorporate complex and machine-learned CVs, broadening applications in biochemistry and materials simulations.

Abstract: Free energy reconstruction methods such as Gaussian Process Regression (GPR)
require Jacobians of the collective variables (CVs), a bottleneck that
restricts the use of complex or machine-learned CVs. We introduce a neural
network surrogate framework that learns CVs directly from Cartesian coordinates
and uses automatic differentiation to provide Jacobians, bypassing analytical
forms. On an MgCl2 ion-pairing system, our method achieved high accuracy for
both a simple distance CV and a complex coordination-number CV. Moreover,
Jacobian errors also followed a near-Gaussian distribution, making them
suitable for GPR pipelines. This framework enables gradient-based free energy
methods to incorporate complex and machine-learned CVs, broadening the scope of
biochemistry and materials simulations.

</details>


### [41] [Learning Regularization Functionals for Inverse Problems: A Comparative Study](https://arxiv.org/abs/2510.01755)
*Johannes Hertrich,Hok Shing Wong,Alexander Denker,Stanislas Ducotterd,Zhenghan Fang,Markus Haltmeier,Željko Kereta,Erich Kobler,Oscar Leong,Mohammad Sadegh Salehi,Carola-Bibiane Schönlieb,Johannes Schwab,Zakhar Shumaylov,Jeremias Sulam,German Shâma Wache,Martin Zach,Yasi Zhang,Matthias J. Ehrhardt,Sebastian Neumayer*

Main category: cs.LG

TL;DR: The paper presents a unified framework for comparing learned regularization methods in imaging inverse problems, addressing implementation disparities and providing systematic analysis.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in comparing different learned regularization methods due to their varied architectural designs and non-modular implementations.

Method: Collecting and unifying available code into a common framework to enable systematic comparison of different approaches.

Result: The unified framework allows systematic comparison of methods, highlighting their strengths and limitations, and provides practical guidelines.

Conclusion: The unified approach offers valuable insights into the future potential of learned regularization methods for imaging inverse problems.

Abstract: In recent years, a variety of learned regularization frameworks for solving
inverse problems in imaging have emerged. These offer flexible modeling
together with mathematical insights. The proposed methods differ in their
architectural design and training strategies, making direct comparison
challenging due to non-modular implementations. We address this gap by
collecting and unifying the available code into a common framework. This
unified view allows us to systematically compare the approaches and highlight
their strengths and limitations, providing valuable insights into their future
potential. We also provide concise descriptions of each method, complemented by
practical guidelines.

</details>


### [42] [Neural non-canonical Hamiltonian dynamics for long-time simulations](https://arxiv.org/abs/2510.01788)
*Clémentine Courtès,Emmanuel Franck,Michael Kraus,Laurent Navoret,Léopold Trémant*

Main category: cs.LG

TL;DR: This paper addresses numerical instability in learning non-canonical Hamiltonian dynamics by proposing two training strategies that overcome gauge dependency issues in combined potential-based architectures and degenerate variational integrators.


<details>
  <summary>Details</summary>
Motivation: Previous research on learning non-canonical Hamiltonian dynamics focused separately on potential-based architectures and degenerate variational integrators, but combining both creates numerical instability due to gauge dependency, making long-term simulations impossible.

Method: Two training strategies: 1) directly learning the vector field, or 2) learning time-discrete dynamics through the numerical scheme itself.

Result: The methods successfully learn complex physical dynamics, including the guiding center from gyrokinetic plasma physics, as demonstrated through several numerical test cases.

Conclusion: The proposed training strategies effectively address the numerical instability problem caused by gauge dependency, enabling stable long-term simulations of learned non-canonical Hamiltonian dynamics.

Abstract: This work focuses on learning non-canonical Hamiltonian dynamics from data,
where long-term predictions require the preservation of structure both in the
learned model and in numerical schemes. Previous research focused on either
facet, respectively with a potential-based architecture and with degenerate
variational integrators, but new issues arise when combining both. In
experiments, the learnt model is sometimes numerically unstable due to the
gauge dependency of the scheme, rendering long-time simulations impossible. In
this paper, we identify this problem and propose two different training
strategies to address it, either by directly learning the vector field or by
learning a time-discrete dynamics through the scheme. Several numerical test
cases assess the ability of the methods to learn complex physical dynamics,
like the guiding center from gyrokinetic plasma physics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [43] [Variational approach to open quantum systems with long-range competing interactions](https://arxiv.org/abs/2510.01543)
*Dawid A. Hryniuk,Marzena H. Szymańska*

Main category: quant-ph

TL;DR: Efficient computational method for simulating open quantum many-body systems with long-range interactions using matrix product operators and variational Monte Carlo.


<details>
  <summary>Details</summary>
Motivation: Need for accurate computational methods to study quantum systems with competing short- and long-range interactions, which are experimentally accessible but computationally challenging.

Method: Combines matrix product operators and time-dependent variational Monte Carlo to simulate dissipative quantum lattices in 1D and 2D.

Result: Successfully simulated non-equilibrium dynamics and steady states of spin-1/2 lattices with algebraically-decaying interactions for up to 200 sites, revealing spatially-modulated magnetic order.

Conclusion: This scalable approach enables study of complex non-equilibrium phenomena in quantum systems with long-range interactions, applicable to Rydberg atoms, dipolar molecules, and trapped ions.

Abstract: Competition between short- and long-range interactions underpins many
emergent phenomena in nature. Despite rapid progress in their experimental
control, computational methods capable of accurately simulating open quantum
many-body systems with complex long-ranged interactions at scale remain scarce.
Here, we address this limitation by introducing an efficient and scalable
approach to dissipative quantum lattices in one and two dimensions, combining
matrix product operators and time-dependent variational Monte Carlo. We
showcase the versatility, effectiveness, and unique methodological advantages
of our algorithm by simulating the non-equilibrium dynamics and steady states
of spin-$\frac{1}{2}$ lattices with competing algebraically-decaying
interactions for as many as $N=200$ sites, revealing the emergence of
spatially-modulated magnetic order far from equilibrium. This approach offers
promising prospects for advancing our understanding of the complex
non-equilibrium properties of a diverse variety of experimentally-realizable
quantum systems with long-ranged interactions, including Rydberg atoms,
ultracold dipolar molecules, and trapped ions.

</details>


### [44] [Hybrid Quantum-Classical Walks for Graph Representation Learning in Community Detection](https://arxiv.org/abs/2510.01918)
*Adrián Marın,Mauricio Soto-Gomez,Giorgio Valentini,Elena Casiraghi,Carlos Cano,Daniel Manzano*

Main category: quant-ph

TL;DR: A quantum-inspired algorithm using hybrid Quantum-Classical Walks for Graph Representation Learning that adapts to complex graph topologies by exploring both local and global connections simultaneously.


<details>
  <summary>Details</summary>
Motivation: Traditional GRL methods struggle to capture intricate relationships in complex graphs with non-trivial structural properties like power-law distributions or hierarchical structures.

Method: Hybrid Quantum-Classical Walks that combine quantum and classical dynamics to allow simultaneous exploration of both local and far-reaching graph connections.

Result: Preliminary results for network community detection show the hybrid dynamic enables effective adaptation to complex graph topologies.

Conclusion: The approach offers a robust and versatile solution for GRL tasks by overcoming limitations of traditional methods through quantum-inspired dynamics.

Abstract: Graph Representation Learning (GRL) has emerged as a cornerstone technique
for analysing complex, networked data across diverse domains, including
biological systems, social networks, and data analysis. Traditional GRL methods
often struggle to capture intricate relationships within complex graphs,
particularly those exhibiting non-trivial structural properties such as
power-law distributions or hierarchical structures. This paper introduces a
novel quantum-inspired algorithm for GRL, utilizing hybrid Quantum-Classical
Walks to overcome these limitations. Our approach combines the benefits of both
quantum and classical dynamics, allowing the walker to simultaneously explore
both highly local and far-reaching connections within the graph. Preliminary
results for a case study in network community detection shows that this hybrid
dynamic enables the algorithm to adapt effectively to complex graph topologies,
offering a robust and versatile solution for GRL tasks.

</details>


### [45] [Quantum advantages in ground state preparation, combinatorial optimization, and quantum state preparation](https://arxiv.org/abs/2510.01563)
*Taehee Ko,Sungbin Lim*

Main category: quant-ph

TL;DR: Ground states of quantum Hamiltonians with inverse-polynomial gaps can be prepared with polynomial-depth circuits using Pauli rotations, and any quantum state can be approximated with constant Pauli rotations for large systems.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that quantum ground state preparation and general state preparation can be achieved efficiently with simple circuits for large system sizes, revealing exponential quantum advantages.

Method: Using polynomial-depth circuits composed of Pauli rotations without ancilla qubits to prepare ground states of Hamiltonians with inverse-polynomial gaps, and extending to approximate any quantum state with constant Pauli rotations.

Result: Ground states can be prepared to inverse-polynomial precision with polynomial circuit depth, and any quantum state can be approximated with constant Pauli rotations to constant precision for sufficiently large qubit numbers.

Conclusion: The findings reveal exponential quantum advantages in ground state preparation, combinatorial optimization, and quantum state preparation, showing efficient preparation methods for large quantum systems.

Abstract: We show that for any quantum Hamiltonian with an inverse-polynomial gap, the
ground state can be prepared in a polynomial circuit depth to
inverse-polynomial precision, if the system size is sufficiently large. The
resulting circuit is composed of a polynomial number of Pauli rotations without
ancilla qubit. Extending this result, we prove that for sufficiently large
qubit number, any quantum state can be approximately prepared with a constant
(polynomial) number of Pauli rotations to constant (inverse-polynomial)
precision. Our theoretical findings reveal exponential quantum advantages in
the prominent applications: ground state preparation, combinatorial
optimization, and quantum state preparation.

</details>


### [46] [Improving neural network performance for solving quantum sign structure](https://arxiv.org/abs/2510.02051)
*Xiaowei Ou,Tianshu Huang,Vidvuds Ozolins*

Main category: quant-ph

TL;DR: A modified stochastic reconfiguration method that uses different imaginary time steps for amplitude and phase optimization, enabling simultaneous training of phase and amplitude neural networks for non-stoquastic Hamiltonians.


<details>
  <summary>Details</summary>
Motivation: Existing neural quantum state approaches for non-stoquastic Hamiltonians often require a priori knowledge of sign structure or separate pre-training of phase networks, which limits their efficiency and applicability.

Method: Modified stochastic reconfiguration with differing imaginary time steps - larger time step for phase optimization and smaller for amplitude - allowing simultaneous training of phase and amplitude neural networks.

Result: The method successfully demonstrates efficacy on the Heisenberg J_1-J_2 model, showing improved training efficiency for neural quantum states.

Conclusion: The proposed approach provides an efficient alternative to existing methods by enabling simultaneous optimization of phase and amplitude networks without requiring pre-trained phase networks or prior sign structure knowledge.

Abstract: Neural quantum states have emerged as a widely used approach to the numerical
study of the ground states of non-stoquastic Hamiltonians. However, existing
approaches often rely on a priori knowledge of the sign structure or require a
separately pre-trained phase network. We introduce a modified stochastic
reconfiguration method that effectively uses differing imaginary time steps to
evolve the amplitude and phase. Using a larger time step for phase
optimization, this method enables a simultaneous and efficient training of
phase and amplitude neural networks. The efficacy of our method is demonstrated
on the Heisenberg J_1-J_2 model.

</details>


### [47] [A quantum analogue of convex optimization](https://arxiv.org/abs/2510.02151)
*Eunou Lee*

Main category: quant-ph

TL;DR: A quantum algorithm called Fundamental Gap Algorithm (FGA) efficiently computes the minimum eigenvalue of Schrödinger operators with convex potentials, achieving polynomial time complexity in dimension and accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop quantum analogues of classical convex optimization problems, specifically computing minimum eigenvalues of Schrödinger operators with convex potentials that grow at infinity.

Method: Uses adiabatic evolution of the ground state as a key subroutine, with novel techniques to focus on low-energy space. The FGA computes minimum eigenvalues up to error ε in polynomial time.

Result: The algorithm achieves polynomial time complexity in dimension n, accuracy 1/ε, and parameters depending on the potential V. Successfully applied to find lowest frequencies of n-dimensional convex drums.

Conclusion: FGA provides the first polynomial-time quantum algorithm for computing minimum eigenvalues of Dirichlet Laplacians on convex regions defined by linear constraints, with applications in quantum optimization.

Abstract: Convex optimization is the powerhouse behind the theory and practice of
optimization. We introduce a quantum analogue of unconstrained convex
optimization: computing the minimum eigenvalue of a Schr\"odinger operator $h =
-\Delta + V $ with convex potential $V:\mathbb R^n \rightarrow \mathbb R_{\ge
0}$ such that $V(x)\rightarrow\infty $ as $\|x\|\rightarrow\infty$. For this
problem, we present an efficient quantum algorithm, called the Fundamental Gap
Algorithm (FGA), that computes the minimum eigenvalue of $h$ up to error
$\epsilon$ in polynomial time in $n$, $1/\epsilon$, and parameters that depend
on $V$. Adiabatic evolution of the ground state is used as a key subroutine,
which we analyze with novel techniques that allow us to focus on the low-energy
space. We apply the FGA to give the first known polynomial-time algorithm for
finding the lowest frequency of an $n$-dimensional convex drum, or
mathematically, the minimum eigenvalue of the Dirichlet Laplacian on an
$n$-dimensional region that is defined by $m$ linear constraints in polynomial
time in $n$, $m$, $1/\epsilon$ and the radius $R$ of a ball encompassing the
region.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [48] [Improving Runtime Performance of Tensor Computations using Rust From Python](https://arxiv.org/abs/2510.01495)
*Kimmie Harding,Daniel M. Dunlavy*

Main category: cs.MS

TL;DR: This paper investigates improving Python Tensor Toolbox performance by implementing key computational kernels in Rust via Python extension modules, showing consistent runtime improvements over Python alone, Numba JIT compilation, and NumPy implementations.


<details>
  <summary>Details</summary>
Motivation: To enhance runtime performance of tensor computational kernels in the Python Tensor Toolbox (pyttb) for better tensor data analysis across various applications, leveraging Rust's compiled language advantages through Python extension modules.

Method: Implemented key tensor kernels of increasing complexity in Rust using Python C API extension modules, ranging from simple sum-of-products operations to advanced tensor multiplication kernels used in low-rank tensor decomposition and tensor regression algorithms.

Result: Numerical experiments with synthetic tensor data showed consistent runtime performance improvements when using Rust from Python compared to: 1) pure Python, 2) Python with Numba JIT compiler, and 3) NumPy implementations of pyttb kernels.

Conclusion: Using Rust via Python extension modules provides significant runtime performance benefits for tensor computational kernels, making it a viable approach for optimizing Python-based tensor analysis packages.

Abstract: In this work, we investigate improving the runtime performance of key
computational kernels in the Python Tensor Toolbox (pyttb), a package for
analyzing tensor data across a wide variety of applications. Recent runtime
performance improvements have been demonstrated using Rust, a compiled
language, from Python via extension modules leveraging the Python C API --
e.g., web applications, data parsing, data validation, etc. Using this same
approach, we study the runtime performance of key tensor kernels of increasing
complexity, from simple kernels involving sums of products over data accessed
through single and nested loops to more advanced tensor multiplication kernels
that are key in low-rank tensor decomposition and tensor regression algorithms.
In numerical experiments involving synthetically generated tensor data of
various sizes and these tensor kernels, we demonstrate consistent improvements
in runtime performance when using Rust from Python over 1) using Python alone,
2) using Python and the Numba just-in-time Python compiler (for loop-based
kernels), and 3) using the NumPy Python package for scientific computing (for
pyttb kernels).

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [49] [Perturbations of Minkowski spacetime with regular conformal compactification](https://arxiv.org/abs/2510.01964)
*Andrea Nützi*

Main category: gr-qc

TL;DR: The paper constructs perturbations of Minkowski spacetime with initial data decaying to Kerr initial data, showing they admit regular conformal compactification at null and timelike infinity with regularity increasing with decay rate.


<details>
  <summary>Details</summary>
Motivation: To generalize Friedrich's results on smooth conformal compactification by allowing initial data that decays to Kerr initial data rather than being identical to Kerr outside compact sets.

Method: Uses a novel formulation of Einstein equations about Minkowski spacetime as quasilinear symmetric hyperbolic PDE regular at null infinity, with tailored energy estimates near spacelike infinity organized by scaling homogeneity.

Result: Perturbations admit regular conformal compactification where regularity increases linearly with initial data decay rate to Kerr, with smooth compactification for rapidly decaying data.

Conclusion: Successfully generalized Friedrich's results using a new formulation of Einstein equations and scaling-based analysis near spacelike infinity, handling Kerr asymptotics as lower-order contributions.

Abstract: We construct perturbations of Minkowski spacetime in general relativity, when
given initial data that decays inverse polynomially to initial data of a Kerr
spacetime towards spacelike infinity. We show that the perturbations admit a
regular conformal compactification at null and timelike infinity, where the
degree of regularity increases linearly with the rate of decay of the initial
data to Kerr initial data. In particular, the compactification is smooth if the
initial data decays rapidly to Kerr initial data. This generalizes results of
Friedrich, who constructed spacetimes with a smooth conformal compactification
in the case when the initial data is identical to Kerr initial data on the
complement of a compact set. Our results rely on a novel formulation of the
Einstein equations about Minkowski spacetime introduced by the author, that
allows one to formulate the dynamic problem as a quasilinear, symmetric
hyperbolic PDE that is regular at null infinity and with null infinity being at
a fixed locus. It is not regular at spacelike infinity, due to the asymptotics
of Kerr. Thus the main technical task is the construction of solutions near
spacelike infinity, using tailored energy estimates. To accomplish this, we
organize the equations according to homogeneity with respect to scaling about
spacelike infinity, which identifies terms that are leading, respectively lower
order, near spacelike infinity, with contributions from Kerr being lower order.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [50] [A Novel Algorithm for Representing Positive Semi-Definite Polynomials as Sums of Squares with Rational Coefficients](https://arxiv.org/abs/2510.01568)
*Zhenbing Zeng,Yong Huang,Lu Yang,Yongsheng Rao*

Main category: cs.SC

TL;DR: Novel algorithm for constructing sum-of-squares decomposition with rational coefficients for positive semi-definite polynomials, ensuring exact arithmetic for formal verification.


<details>
  <summary>Details</summary>
Motivation: Existing methods produce SOS decompositions with floating-point coefficients, which are problematic for formal verification and symbolic computation where exact arithmetic is required.

Method: Stepwise reduction technique that transforms polynomials into sum of ladder-like squares while preserving rationality of coefficients.

Result: Experimental results show effectiveness compared to existing numerical approaches, providing exact rational coefficient decompositions.

Conclusion: The method successfully generates SOS decompositions with rational coefficients, addressing limitations of numerical approaches in exact computation contexts.

Abstract: This paper presents a novel algorithm for constructing a sum-of-squares (SOS)
decomposition for positive semi-definite polynomials with rational
coefficients. Unlike previous methods that typically yield SOS decompositions
with floating-point coefficients, our approach ensures that all coefficients in
the decomposition remain rational. This is particularly useful in formal
verification and symbolic computation, where exact arithmetic is required. We
introduce a stepwise reduction technique that transforms a given polynomial
into a sum of ladder-like squares while preserving rationality. Experimental
results demonstrate the effectiveness of our method compared to existing
numerical approaches. This artical is an extension of the following Chinnese
paper: HUANG Yong , ZENG Zhenbing , YANG Lu , RAO Yongsheng. An Algorithm to
Represent Positive Semi-Definite Polynomials to Sum of Lader-Like Squares of
Polynomials with Rational Coefficients (in Chinese). Journal of Systems Science
and Mathematical Sciences, 2024, 44(5): 1241-1271
https://doi.org/10.12341/jssms23584CM

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [51] [Exponential convergence of a distributed divide-and-conquer algorithm for constrained convex optimization on networks](https://arxiv.org/abs/2510.01511)
*Nazar Emirov,Guohui Song,Qiyu Sun*

Main category: math.OC

TL;DR: A distributed divide-and-conquer algorithm for constrained convex optimization over networks with exponential convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: To solve large-scale constrained convex optimization problems where the global objective is distributed across multiple agents in a network, requiring fully distributed computation with minimal coordination.

Method: Divide-and-conquer approach where each iteration solves local subproblems around selected fusion centers and coordinates only with neighboring fusion centers. Works under smoothness, strong convexity, and locality assumptions.

Result: Established exponential convergence of DAC iterations with explicit bounds for both exact and inexact local solvers. Numerical experiments on L2 distance, quadratic, and entropy losses confirm theory and demonstrate scalability.

Conclusion: The proposed DAC algorithm provides an effective and scalable distributed solution for constrained convex optimization over networks with provable exponential convergence rates.

Abstract: We propose a divide-and-conquer (DAC) algorithm for constrained convex
optimization over networks, where the global objective is the sum of local
objectives attached to individual agents. The algorithm is fully distributed:
each iteration solves local subproblems around selected fusion centers and
coordinates only with neighboring fusion centers. Under standard assumptions of
smoothness, strong convexity, and locality on the objective function, together
with polynomial growth conditions on the underlying graph, we establish
exponential convergence of the DAC iterations and derive explicit bounds for
both exact and inexact local solvers. Numerical experiments on three
representative losses ($L_2$ distance, quadratic, and entropy) confirm the
theory and demonstrate scalability and effectiveness.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [52] [Mean curvature flow through singularities](https://arxiv.org/abs/2510.01355)
*Robert Haslhofer*

Main category: math.DG

TL;DR: This paper provides an overview of mean curvature flow, recent advances in singularity theory in R³, classification of noncollapsed singularities in R⁴, and discusses open problems.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive theory for mean curvature flow through singularities, extending beyond R³ to higher dimensions like R⁴.

Method: General introduction to mean curvature flow, analysis of fundamental results from the last decade in R³, classification of noncollapsed singularities in R⁴, and discussion of open problems.

Result: Established a precise theory for mean curvature flow through singularities in R³ and provided classification of all noncollapsed singularities in R⁴.

Conclusion: The paper advances the understanding of mean curvature flow singularities and identifies key open problems for future research in higher dimensions.

Abstract: We first give a general introduction to the mean curvature flow, and then
discuss fundamental results established over the last 10 years that yield a
precise theory for the flow through singularities in $\mathbb{R}^3$. With the
aim of developing a satisfying theory in higher dimensions, we then describe
our recent classification of all noncollapsed singularities in $\mathbb{R}^4$.
Finally, we provide a detailed discussion of open problems and conjectures.

</details>


### [53] [Uniqueness in the Plateau problem for calibrated currents](https://arxiv.org/abs/2510.02299)
*Bryan Dimler,Chen-Kuan Lee*

Main category: math.DG

TL;DR: Compactly supported calibrated integral currents with connected C³,α boundary are unique solutions to the oriented Plateau problem for their boundary data.


<details>
  <summary>Details</summary>
Motivation: To establish uniqueness of solutions to the oriented Plateau problem for certain classes of calibrated integral currents with regular boundaries.

Method: Combines boundary regularity theory for area-minimizing currents with classical unique continuation principles adapted to the minimal surface system.

Result: Proves that such currents are unique solutions to the oriented Plateau problem for their boundary data.

Conclusion: The paper establishes a uniqueness result for the oriented Plateau problem under specific regularity conditions on the boundary.

Abstract: We show that every compactly supported calibrated integral current with
connected $C^{3,\alpha}$ boundary is the unique solution to the oriented
Plateau problem for its boundary data. This is proved as a consequence of the
boundary regularity theory for area-minimizing currents and classical unique
continuation principles adapted to the minimal surface system.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [54] [Relativistic Jets and Winds in Radio-Identified Supermassive Black Hole Binary Candidates](https://arxiv.org/abs/2510.02301)
*Andrew G. Sullivan,Roger D. Blandford,Anna Synani,Philipe V. de la Parra,Noémie Globus,Mitchell C. Begelman,Anthony C. S. Readhead*

Main category: astro-ph.HE

TL;DR: Proposes a jet model to explain periodic flux variations in two SMBHB candidates, suggesting helical jet channels created by mildly relativistic winds cause observable aberrations and delays.


<details>
  <summary>Details</summary>
Motivation: To explain the periodic flux density variations observed in potential supermassive black hole binary candidates PKS 2131-021 and PKS J0805-0111, which show clock-like behavior but lack confirmed individual nHz gravitational wave sources.

Method: Developed a generalizable jet model where a mildly relativistic wind creates an outward-moving helical channel for ultra-relativistic jet propagation, with flux variations mainly due to aberration effects and delayed lower-frequency emission from larger radii.

Result: The model successfully reproduces the main observable features of both SMBHB candidates, including periodic flux variations and frequency-dependent delays, and makes testable predictions for radio polarization, direct imaging, and emission line variations.

Conclusion: The proposed model provides a viable explanation for SMBHB candidate observations, motivates future numerical simulations of jetted systems, and has implications for understanding blazar jet fueling, structure, and evolution.

Abstract: Supermassive black hole binary systems (SMBHBs) are thought to emit the
recently discovered nHz gravitational wave background; however, not a single
individual nHz source has been confirmed to date. Long-term radio-monitoring at
the Owens Valley Radio Observatory has revealed two potential SMBHB candidates:
blazars PKS 2131-021 and PKS J0805-0111. These sources show periodic flux
density variations across the electromagnetic spectrum, signaling the presence
of a good clock. To explain the emission, we propose a generalizable jet model,
where a mildly relativistic wind creates an outward-moving helical channel,
along which the ultra-relativistic jet propagates. The observed flux variation
from the jet is mostly due to aberration. The emission at lower frequency
arises at larger radius and its variation is consequently delayed, as observed.
Our model reproduces the main observable features of both sources and can be
applied to other sources as they are discovered. We make predictions for radio
polarization, direct imaging, and emission line variation, which can be tested
with forthcoming observations. Our results motivate future numerical
simulations of jetted SMBHB systems and have implications for the fueling,
structure, and evolution of blazar jets.

</details>
