<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 9]
- [math.AP](#math.AP) [Total: 16]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [gr-qc](#gr-qc) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [math.DS](#math.DS) [Total: 1]
- [cs.SC](#cs.SC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 5]
- [cs.MS](#cs.MS) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [nlin.SI](#nlin.SI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 3]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [math.SP](#math.SP) [Total: 2]
- [cs.CV](#cs.CV) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Deep Learning Accelerated Algebraic Multigrid Methods for Polytopal Discretizations of Second-Order Differential Problems](https://arxiv.org/abs/2510.01442)
*Paola F. Antonietti,Matteo Caldana,Lorenzo Gentile,Marco Verani*

Main category: math.NA

TL;DR: A deep learning approach to automatically tune AMG parameters (strong threshold and smoother choice) for polytopal discretizations, treating sparse matrices as images and using pooling to extract features, achieving up to 27% solver time reduction.


<details>
  <summary>Details</summary>
Motivation: AMG efficiency depends heavily on parameter choices like strong threshold and smoother selection, which is especially critical for polytopal discretizations (PolyDG/VEM) due to variable mesh structures and sparsity patterns.

Method: Interpret sparse matrices from polytopal discretizations as grayscale images, apply pooling in neural networks to extract compact features, and use deep learning to automatically tune AMG parameters (strong threshold and smoother choice).

Result: The approach reduces AMG solver time by up to 27% while maintaining good generalization across various 2D/3D differential problems with heterogeneous coefficients and polygonal/polyhedral meshes.

Conclusion: Deep learning can effectively automate AMG parameter tuning for polytopal discretizations, significantly improving solver performance with minimal code modifications to existing PolyDG and VEM implementations.

Abstract: Algebraic Multigrid (AMG) methods are state-of-the-art algebraic solvers for
partial differential equations. Still, their efficiency depends heavily on the
choice of suitable parameters and/or ingredients. Paradigmatic examples include
the so-called strong threshold parameter $\theta$, which controls the algebraic
coarse-grid hierarchy, as well as the smoother, i.e., the relaxation methods
used on the fine grid to damp out high-frequency errors. In AMG, since the
coarse grids are constructed algebraically (without geometric intuition), the
smoother's performance is even more critical. For the linear systems stemming
from polytopal discretizations, such as Polytopal Discontinuous Galerkin
(PolyDG) and Virtual Element Methods (VEM), AMG sensitivity to such choices is
even more critical due to the significant variability of the underlying meshes,
which results in algebraic systems with different sparsity patterns. We propose
a novel deep learning approach that automatically tunes the strong threshold
parameter, as well as the smoother choice in AMG solvers, for linear systems of
equations arising from polytopal discretizations, thereby maximizing AMG
performance. We interpret the sparse matrix resulting from polytopal
discretization as a grayscale image, and by applying pooling, our neural
network extracts compact features that preserve the necessary information at a
low computational cost. We test various differential problems in both two- and
three-dimensional settings, with heterogeneous coefficients and
polygonal/polyhedral meshes, and demonstrate that the proposed approach
generalizes well. In practice, we demonstrate that we can reduce AMG solver
time by up to $27\%$ with minimal changes to existing PolyDG and VEM codes.

</details>


### [2] [Data selection: at the interface of PDE-based inverse problem and randomized linear algebra](https://arxiv.org/abs/2510.01567)
*Kathrin Hellmuth,Ruhui Jin,Qin Li,Stephen J. Wright*

Main category: math.NA

TL;DR: This review explores data selection in PDE-based inverse problems, emphasizing the challenge of infinite-dimensional parameter and design spaces, and how randomized numerical linear algebra provides probabilistic tools for efficient data selection.


<details>
  <summary>Details</summary>
Motivation: Inverse problems rely on data to recover unknown parameters, but not all data are equally informative. The central challenge is selecting the most informative data, which is particularly difficult in PDE-based inverse problems due to their inherently infinite-dimensional nature.

Method: The paper surveys how randomized numerical linear algebra (RNLA) methods, originally developed in different contexts, have been adapted to address data selection in PDE-based inverse problems. These methods use random sampling with probabilistic guarantees.

Result: RNLA provides powerful tools for data selection with probabilistic guarantees that information is preserved with high probability (1-p) when using N randomly selected, weighted samples. The mathematical form of 'information' varies depending on the specific setting.

Conclusion: Randomized numerical linear algebra offers effective strategies for addressing the unique challenges of data selection in infinite-dimensional PDE-based inverse problems, providing probabilistic guarantees for information preservation through carefully designed random sampling approaches.

Abstract: All inverse problems rely on data to recover unknown parameters, yet not all
data are equally informative. This raises the central question of data
selection. A distinctive challenge in PDE-based inverse problems is their
inherently infinite-dimensional nature: both the parameter space and the design
space are infinite, which greatly complicates the selection process. Somewhat
unexpectedly, randomized numerical linear algebra (RNLA), originally developed
in very different contexts, has provided powerful tools for addressing this
challenge. These methods are inherently probabilistic, with guarantees
typically stating that information is preserved with probability at least 1-p
when using N randomly selected, weighted samples. Here, the notion of
information can take different mathematical forms depending on the setting. In
this review, we survey the problem of data selection in PDE-based inverse
problems, emphasize its unique infinite-dimensional aspects, and highlight how
RNLA strategies have been adapted and applied in this context.

</details>


### [3] [Instability of the Sherman-Morrison formula and stabilization by iterative refinement](https://arxiv.org/abs/2510.01696)
*Behnam Hashemi,Yuji Nakatsukasa*

Main category: math.NA

TL;DR: The Sherman-Morrison formula's numerical stability remains an open question. This paper analyzes its backward stability, demonstrates instability in common scenarios, and incorporates iterative refinement to achieve backward stability while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: The Sherman-Morrison formula is widely used for solving rank-one perturbed linear systems due to its simplicity and efficiency, but its numerical stability properties have remained an open question since 1944.

Method: Analyzed backward stability of SM formula, demonstrated instability scenarios, and incorporated fixed-precision iterative refinement into SM framework while reusing previously computed decompositions.

Result: The paper proves that with iterative refinement under reasonable assumptions, backward stability is achieved without sacrificing SM formula efficiency. Empirically observed to produce backward stable solutions in all numerical experiments.

Conclusion: With iterative refinement, the SM formula yields backward stable solutions when both κ₂(A) and κ₂(A+uvᵀ) are bounded safely away from εᴹ⁻¹, where εᴹ is the unit roundoff.

Abstract: Owing to its simplicity and efficiency, the Sherman-Morrison (SM) formula has
seen widespread use across various scientific and engineering applications for
solving rank-one perturbed linear systems of the form $(A+uv^T)x = b$. Although
the formula dates back at least to 1944, its numerical stability properties
have remained an open question and continue to be a topic of current research.
We analyze the backward stability of the SM, demonstrate its instability in a
scenario increasingly common in scientific computing and address an open
question posed by Nick Higham on the proportionality of the backward error
bound to the condition number of $A$. We then incorporate fixed-precision
iterative refinement into the SM framework reusing the previously computed
decompositions and prove that, under reasonable assumptions, it achieves
backward stability without sacrificing the efficiency of the SM formula. While
our theory does not prove the SM formula with iterative refinement always
outputs a backward stable solution, empirically it is observed to eventually
produce a backward stable solution in all our numerical experiments. We
conjecture that with iterative refinement, the SM formula yields a backward
stable solution provided that $\kappa_2(A), \kappa_2(A+uv^T)$ are both bounded
safely away from $\epsilon_M^{-1}$, where $\epsilon_M$ is the unit roundoff.

</details>


### [4] [Efficient manifold evolution algorithm using adaptive B-Spline interpolation](https://arxiv.org/abs/2510.01790)
*Muhammad Ammad,Leevan Ling*

Main category: math.NA

TL;DR: An efficient Lagrangian approach using B-Splines for evolving point cloud data on manifolds, providing an alternative to RBF methods with geometric coefficient manipulation.


<details>
  <summary>Details</summary>
Motivation: To provide an efficient alternative to conventional radial basis function (RBF) approaches for evolving point cloud data on smooth manifolds in higher dimensions.

Method: Uses B-Spline basis functions for local interpolations, where coefficients carry geometric meanings and can be manipulated like points, enabling rapid updates without frequent re-interpolation.

Result: Numerical results demonstrate convergence of geometric quantities and effectiveness of the approach, with successful simulations of curvature flows coupled with reaction-diffusion systems.

Conclusion: The B-Spline approach offers advantages over RBF methods by enabling seamless point cloud manipulation, particularly in regions with fluctuating point density, and effectively handles complex geometric evolution problems.

Abstract: This paper explores an efficient Lagrangian approach for evolving point cloud
data on smooth manifolds. In this preliminary study, we focus on analyzing
plane curves, and our ultimate goal is to provide an alternative to the
conventional radial basis function (RBF) approach for manifolds in higher
dimensions. In particular, we use the B-Spline as the basis function for all
local interpolations. Just like RBF and other smooth basis functions, B-Splines
enable the approximation of geometric features such as normal vectors and
curvature. Once properly set up, the advantage of using B-Splines is that their
coefficients carry geometric meanings. This allows the coefficients to be
manipulated like points, facilitates rapid updates of the interpolant, and
eliminates the need for frequent re-interpolation. Consequently, the removal
and insertion of point cloud data become seamless processes, particularly
advantageous in regions experiencing significant fluctuations in point density.
The numerical results demonstrate the convergence of geometric quantities and
the effectiveness of our approach. Finally, we show simulations of curvature
flows whose speeds depend on the solutions of coupled reaction--diffusion
systems for pattern formation.

</details>


### [5] [Asymptotic preserving schemes for hyperbolic systems with relaxation](https://arxiv.org/abs/2510.01828)
*C Mahmoud,H Mathis*

Main category: math.NA

TL;DR: Two numerical schemes for hyperbolic systems with relaxation source terms that treat the system as a whole without separating convection from source terms, with asymptotic preserving properties.


<details>
  <summary>Details</summary>
Motivation: To develop numerical methods that handle hyperbolic systems with relaxation source terms in a unified manner, avoiding the separation of convective and source term resolution.

Method: First scheme combines FORCE approach with unsplit strategy; second scheme uses an approximate Riemann solver with careful source term approximation. Both are designed to be asymptotic preserving.

Result: The schemes maintain consistency with equilibrium models as relaxation parameter approaches zero without CFL restrictions, and preserve invariant domains with discrete entropy inequalities for specific models.

Conclusion: Two effective numerical schemes were developed that successfully handle hyperbolic relaxation systems as unified entities with desirable mathematical properties.

Abstract: This paper presents the construction of two numerical schemes for the
solution of hyperbolic systems with relaxation source terms. The methods are
built by considering the relaxation system as a whole, without separating the
resolution of the convective part from that of the source term. The first
scheme combines the centered FORCE approach of Toro and co-authors with the
unsplit strategy proposed by B{\'e}reux and Sainsaulieu. The second scheme
consists of an approximate Riemann solver which carefully handles the source
term approximation. The two schemes are built to be asymptotic preserving, in
the sense that their limit schemes are consistent with the equilibrium model as
the relaxation parameter tends to zero, without any CFL restriction. For
specific models, it is possible to prove that they preserve invariant domains
and admit a discrete entropy inequality.

</details>


### [6] [A nodally bound-preserving composite discontinuous Galerkin method on polytopic meshes](https://arxiv.org/abs/2510.02094)
*Abdolreza Amiri,Gabriel R. Barrenechea,Emmanuil H. Georgoulis,Tristan Pryer*

Main category: math.NA

TL;DR: A nodally bound-preserving Galerkin method for elliptic problems on polytopic meshes that enforces solution bounds at user-defined points within elements using a simplicial submesh, without adding extra degrees of freedom.


<details>
  <summary>Details</summary>
Motivation: To develop a method that combines geometric flexibility of polytopic meshes with bound preservation capabilities for numerical solutions, addressing issues with sharp gradients and boundary layers.

Method: Uses interior penalty discontinuous Galerkin formulation on polytopic meshes with simplicial submesh to enforce bounds at submesh nodes via nonlinear iteration, preserving accuracy without additional degrees of freedom.

Result: Proven existence and uniqueness of numerical solution, optimal convergence for smooth problems, robustness with sharp gradients, and automatic reversion to standard DG when no bounds are violated.

Conclusion: The composite method successfully combines polytopic mesh flexibility with DG accuracy and stability while guaranteeing bound preservation, with proven mathematical properties and demonstrated numerical performance.

Abstract: We introduce a nodally bound-preserving Galerkin method for second-order
elliptic problems on general polygonal/polyhedral, henceforth collectively
termed as \emph{polytopic}, meshes. Starting from an interior penalty
discontinuous Galerkin (DG) formulation posed on a polytopic mesh, the method
enforces preservation of \emph{a priori} prescribed upper and lower bounds for
the numerical solution at an arbitrary number of user-defined points
\emph{within} each polytopic element. This is achieved by employing a
simplicial submesh and enforcing bound preservation at the submesh nodes via a
nonlinear iteration. By construction, the submeshing procedure preserves the
order of accuracy of the DG method, \emph{without} introducing any additional
global numerical degrees of freedom compared to the baseline DG method,
thereby, falling into the category of composite finite element approaches. A
salient feature of the proposed method is that it automatically reverts to the
standard DG method on polytopic meshes when no prescribed bound violation
occurs. In particular, the choice of the discontinuity-penalisation parameter
is independent of the submesh granularity. The resulting composite method
combines the geometric flexibility of polytopic meshes with the accuracy and
stability of discontinuous Galerkin discretisations, while rigorously
guaranteeing bound preservation. The existence and uniqueness of the numerical
solution is proven. A priori error bounds, assuming sufficient regularity of
the exact solution are shown, employing a non-standard construction of discrete
nodally bound-preserving interpolant. Numerical experiments confirm optimal
convergence for smooth problems and demonstrate robustness in the presence of
sharp gradients, such as boundary and interior layers.

</details>


### [7] [Coarse scrambling for Sobol' and Niederreiter sequences](https://arxiv.org/abs/2510.02111)
*Kosuke Suzuki*

Main category: math.NA

TL;DR: Coarse scrambling is a new randomization method for digital sequences that permutes blocks of digits while preserving (0,e,d)-sequence properties, achieving O(n^{-3+ε}) variance decay with only logarithmic dimension dependence O(log d).


<details>
  <summary>Details</summary>
Motivation: To address the curse of dimensionality affecting scrambled Sobol' sequences by developing a randomization method with better dimensional robustness.

Method: Permutes blocks of digits in mixed-radix representation while preserving the (0,e,d)-sequence property of underlying points.

Result: Achieves canonical O(n^{-3+ε}) variance decay rate for smooth integrands, with maximal gain coefficient growing only logarithmically with dimension O(log d).

Conclusion: Coarse scrambling provides theoretical robustness against dimensionality curse, being competitive with Owen's scrambling for functions with low effective truncation dimension.

Abstract: We introduce \emph{coarse scrambling}, a novel randomization for digital
sequences that permutes blocks of digits in a mixed-radix representation. This
construction is designed to preserve the powerful
$(0,\boldsymbol{e},d)$-sequence property of the underlying points. For
sufficiently smooth integrands, we prove that this method achieves the
canonical $O(n^{-3+\epsilon})$ variance decay rate, matching that of standard
Owen's scrambling. Crucially, we show that its maximal gain coefficient grows
only logarithmically with dimension, $O(\log d)$, thus providing theoretical
robustness against the curse of dimensionality affecting scrambled Sobol'
sequences. Numerical experiments validate these findings and illustrate a
practical trade-off: while Owen's scrambling is superior for integrands
sensitive to low-dimensional projections, coarse scrambling is competitive for
functions with low effective truncation dimension.

</details>


### [8] [Mixed-precision iterative refinement for low-rank Lyapunov equations](https://arxiv.org/abs/2510.02126)
*Peter Benner,Xiaobo Liu*

Main category: math.NA

TL;DR: Mixed-precision iterative refinement framework for solving low-rank Lyapunov equations using reduced precisions like half precision to accelerate solution without compromising quality.


<details>
  <summary>Details</summary>
Motivation: To accelerate the solution of low-rank Lyapunov matrix equations by using reduced precision arithmetic while maintaining solution quality for problems with condition numbers up to the inverse of the solver precision's unit roundoff.

Method: Developed a mixed-precision iterative refinement framework using the sign function Newton iteration as solver, with rounding error analysis to derive conditions for attainable normwise residuals and guide parameter selection.

Result: Shows that reduced precisions (e.g., half precision) can be used as solver precision to accelerate Lyapunov equation solutions for condition numbers up to 1/u_s (inverse of unit roundoff) without quality compromise.

Conclusion: Mixed-precision iterative refinement enables efficient solution of low-rank Lyapunov equations using reduced precision arithmetic while maintaining accuracy through proper parameter selection guided by error analysis.

Abstract: We develop a mixed-precision iterative refinement framework for solving
low-rank Lyapunov matrix equations $AX + XA^T + W =0$, where $W=LL^T$ or
$W=LSL^T$. Via rounding error analysis of the algorithms we derive sufficient
conditions for the attainable normwise residuals in different precision
settings and show how the algorithmic parameters should be chosen. Using the
sign function Newton iteration as the solver, we show that reduced precisions,
such as the half precision, can be used as the solver precision (with unit
roundoff $u_s$) to accelerate the solution of Lyapunov equations of condition
number up to $1/u_s$ without compromising its quality.

</details>


### [9] [A Fast solver for high condition linear systems using randomized stable solutions of its blocks](https://arxiv.org/abs/2510.02156)
*Suvendu Kar,Murugesan Venkatapathi*

Main category: math.NA

TL;DR: Enhanced randomized block-Kaczmarz method with regularization and dynamic proposal distribution for solving high-condition number linear systems.


<details>
  <summary>Details</summary>
Motivation: To improve solving high-condition number linear systems that are sparse or dense least-squares problems that are significantly over/under determined, addressing poor generalizability of preconditioners.

Method: Uses regularization during block updates and dynamic proposal distribution based on current residue and effective orthogonality between blocks.

Result: Provides significant gains in solving high-condition number linear systems.

Conclusion: Can serve as a pre-solver for other iterative numerical methods and as an inner iteration in certain types of GMRES solvers for linear systems.

Abstract: We present an enhanced version of the row-based randomized block-Kaczmarz
method to solve a linear system of equations. This improvement makes use of a
regularization during block updates in the solution, and a dynamic proposal
distribution based on the current residue and effective orthogonality between
blocks. This improved method provides significant gains in solving
high-condition number linear systems that are either sparse, or dense
least-squares problems that are significantly over/under determined.
Considering the poor generalizability of preconditioners for such problems, it
can also serve as a pre-solver for other iterative numerical methods when
required, and as an inner iteration in certain types of GMRES solvers for
linear systems.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [10] [Localized Pattern Formation and Oscillatory Instabilities in a Three-component Gierer Meinhardt Model](https://arxiv.org/abs/2510.01401)
*Chunyi Gai,Fahad Al Saadi*

Main category: math.AP

TL;DR: Three-component Gierer-Meinhardt model with large diffusivity ratio exhibits Hopf bifurcations in both amplitude and position of interior spikes, revealing richer oscillatory dynamics than two-component systems.


<details>
  <summary>Details</summary>
Motivation: To explore richer spike behavior and oscillatory dynamics in three-component reaction-diffusion systems compared to classical two-component models, particularly focusing on dual Hopf bifurcations.

Method: Asymptotic analysis and numerical path-following to construct localized spike equilibria, analyze spike nucleation through saddle-node bifurcation, and study stability using time-scaling parameters.

Result: Identified two distinct instability mechanisms: amplitude oscillations from large-eigenvalue instabilities and oscillatory spike motion from small eigenvalues, with numerical simulations showing transition regimes.

Conclusion: Three-component systems exhibit richer spike dynamics with dual oscillation mechanisms, suggesting several open problems for future investigation in complex pattern formation.

Abstract: In this paper, we introduce a three-component Gierer-Meinhardt model in the
semi-strong interaction regime, characterized by an asymptotically large
diffusivity ratio. A key feature of this model is that the interior spike can
undergo Hopf bifurcations in both amplitude and position, leading to rich
oscillatory dynamics not present in classical two-component systems. Using
asymptotic analysis and numerical path-following, we construct localized spike
equilibria and analyze spike nucleation that occurs through slow passage beyond
a saddle-node bifurcation. Moreover, stability of spike equilibrium is analyzed
by introducing time-scaling parameters, which reveal two distinct mechanisms:
amplitude oscillations triggered by large-eigenvalue instabilities and
oscillatory spike motion associated with small eigenvalues. Numerical
simulations illustrate these dynamics and their transition regimes. This dual
mechanism highlights richer spike behavior in three-component systems and
suggests several open problems for future study.

</details>


### [11] [Symmetry analysis and new partially invariant solutions for the gas dynamics system with a special equation of state](https://arxiv.org/abs/2510.01415)
*Dilara Siraeva,Irina A. Kogan*

Main category: math.AP

TL;DR: Symmetry analysis of gas dynamics with special state equation (pressure = entropy + function of density). Computes invariants for 4D subalgebras and constructs explicit solutions.


<details>
  <summary>Details</summary>
Motivation: Continue the symmetry analysis program for gas dynamics systems, specifically studying 4D subalgebras not previously considered in earlier work.

Method: Compute complete sets of generating invariants for non-similar 4D subalgebras from optimal list, construct partially symmetry-reduced system, and explicitly solve the reduced system.

Result: Obtained new families of explicit solutions to the original gas dynamics system and analyzed their trajectories. Matched subalgebras with isomorphism classes.

Conclusion: Advanced the symmetry analysis of gas dynamics systems, providing explicit solutions and establishing foundation for future study of reduced system hierarchy.

Abstract: This paper is a contribution to the symmetry analysis of the gas dynamics
system in the vein of the ''podmodeli'' (submodels) program outlined by
Ovsyannikov (1994). We consider the case of the special state equation,
prescribing pressure to be the sum of entropy and an arbitrary function of
density. Such a system has a 12-dimensional symmetry Lie algebra. This work
advances the study of its four-dimensional subalgebras, continuing the work
started in Siraeva (2024). For a large subset of not previously considered,
non-similar four-dimensional subalgebras from an optimal list in Siraeva
(2014), we compute a complete set of generating invariants. For one of the
subalgebras, we construct a partially symmetry-reduced system. We explicitly
solve this reduced system (submodel). This leads to new families of explicit
solutions of the original system. We analyze the trajectories of these
solutions. Additionally, we match each of the subalgebras considered in this
paper with its isomorphism class, planting a seed for future study of the
hierarchy of the reduced systems.

</details>


### [12] [Correlation estimates for Brownian particles with singular interactions](https://arxiv.org/abs/2510.01507)
*Mitia Duerinckx,Pierre-Emmanuel Jabin*

Main category: math.AP

TL;DR: This paper develops a new framework using linearized correlation functions to analyze particle systems with singular pairwise interactions and non-vanishing diffusion, providing the first systematic control of correlations for square-integrable interaction kernels.


<details>
  <summary>Details</summary>
Motivation: Previous approaches to describing corrections to mean-field behavior through correlation functions were limited to bounded interactions, with optimal estimates G_{N,m}=O(N^{1-m}) known only for bounded cases. More singular interactions remained out of reach.

Method: The authors develop a new framework based on linearized correlation functions that allows derivation of robust bounds for systems with square-integrable interaction kernels. The estimates can be partially refined using the BBGKY hierarchy.

Result: The method provides the first systematic control of correlations in the singular setting. For bounded interactions, it recovers known optimal estimates with simplified arguments. Key applications include establishing validity of Bogolyubov correction to mean field and proving a central limit theorem for the empirical measure.

Conclusion: The framework extends correlation analysis beyond the bounded interaction regime, enabling rigorous treatment of singular interactions in particle systems with non-vanishing diffusion.

Abstract: We study particle systems with singular pairwise interactions and
non-vanishing diffusion in the mean-field scaling. A classical approach to
describing corrections to mean-field behavior is through the analysis of
correlation functions. For bounded interactions, the optimal estimates on
correlations are well known: the $m$-particle correlation function is
$G_{N,m}=O(N^{1-m})$ for all $m$. Such estimates, however, have remained out of
reach for more singular interactions. In this work, we develop a new framework
based on linearized correlation functions, which allows us to derive robust
bounds for systems with merely square-integrable interaction kernels, providing
the first systematic control of correlations in the singular setting. Although
at first not optimal, our estimates can be partially refined a posteriori using
the BBGKY hierarchy: in the case of bounded interactions, our method recovers
the known optimal estimates with a simplified argument. As key applications, we
establish the validity of the Bogolyubov correction to mean field and prove a
central limit theorem for the empirical measure, extending these results beyond
the bounded interaction regime for the first time.

</details>


### [13] [On the attainment of boundary data in variational problems with linear growth](https://arxiv.org/abs/2510.01515)
*David Meyer*

Main category: math.AP

TL;DR: The paper shows that for convex variational problems with linear growth and Dirichlet boundary conditions, minimizers attain boundary data in trace sense under mean-convexity conditions, without requiring continuity assumptions. The methods also work for systems under quasi-isotropy assumptions.


<details>
  <summary>Details</summary>
Motivation: Convex variational problems with linear growth and Dirichlet boundary conditions may lack minimizers if boundary conditions aren't properly relaxed. The paper aims to establish conditions under which minimizers actually attain boundary data.

Method: The authors use relaxed variational formulations and prove that under mean-convexity boundary conditions and for boundary data in BV or W^{α,p} with αp≥2, minimizers attain boundary data in trace sense. They also treat systems under quasi-isotropy assumptions.

Result: Minimizers of relaxed problems attain boundary data in trace sense for mean-convex boundaries when boundary data lies in BV or W^{α,p} with αp≥2, without continuity requirements. Without quasi-isotropy, counterexamples exist on uniformly convex domains.

Conclusion: The paper establishes sufficient conditions for boundary data attainment in variational problems, extends results to systems under quasi-isotropy, and provides applications to uniqueness problems and trace spaces of least gradient functions.

Abstract: It is well-known that convex variational problems with linear growth and
Dirichlet boundary conditions might not have minimizers if the boundary
condition is not suitably relaxed.
  We show that for a wide range of integrands, including the least gradient
problem and the non-parametric Plateau problem, and under suitable
mean-convexity conditions of the boundary, minimizers of the relaxed problem
attain the boundary data in the trace sense if it lies in $BV$ or
$W^{\alpha,p}$ with $\alpha p\geq 2$ without any kind of continuity assumption.
Unlike previous works, our methods are also able to treat systems under a
certain quasi-isotropy assumption on the integrand. We further show that
without this quasi-isotropy assumption, smooth counterexamples on uniformly
convex domains exist.
  Further applications to the uniqueness of minimizers and to open problems
about the ROF functional with Dirichlet boundary conditions, and to the trace
space of functions of least gradient are given.

</details>


### [14] [Inertial instability of Couette flow with Coriolis force](https://arxiv.org/abs/2510.01602)
*Yanlong Fan,Daozhi Han,Quan Wang*

Main category: math.AP

TL;DR: Analysis of nonlinear inertial instability in Couette flow with Coriolis forcing, showing velocity instability through pseudo-eigenfunctions despite no exponential eigenfunction growth.


<details>
  <summary>Details</summary>
Motivation: To understand the stability properties of Couette flow under Coriolis forcing, particularly investigating instability mechanisms when traditional linear analysis shows no exponentially growing eigenfunctions.

Method: Used spectral analysis of the linearized system, constructed pseudo-eigenfunctions to demonstrate non-ideal spectral properties, and applied bootstrap arguments to establish instability.

Result: Proved velocity instability in the Hadamard sense for Coriolis coefficient f in the interval (2/17(5-2√2), 2/17(5+2√2)), showing instability occurs through pseudo-eigenfunctions rather than traditional eigenfunction growth.

Conclusion: Couette flow under Coriolis forcing exhibits nonlinear inertial instability through pseudo-eigenfunctions, demonstrating that instability can occur even when the linearized system lacks exponentially growing eigenfunctions.

Abstract: We analyze the nonlinear inertial instability of Couette flow under Coriolis
forcing in \(\mathbb{R}^{3}\). For the Coriolis coefficient \(f \in (0,1)\), we
show that the non-normal operator associated with the linearized system admits
only continuous spectrum. Hence, there are no exponentially growing
eigenfunctions for the linearized system. Instead, we construct unstable
solutions in the form of pseudo-eigenfunctions that exhibit non-ideal spectral
properties. Then through a bootstrap argument and resolving the challenges
posed by the non-ideal spectral behavior of pseudo-eigenfunctions, we establish
the velocity instability of Couette flow in the Hadamard sense for $ f \in
\Big(\frac{2}{17} \left(5-2 \sqrt{2}\right), \frac{2}{17} \left(5 + 2
\sqrt{2}\right) \Big)$.

</details>


### [15] [The weighted isoperimetric inequality and Sobolev inequality outside convex sets](https://arxiv.org/abs/2510.01647)
*Lu Chen,Jiali Lan*

Main category: math.AP

TL;DR: Extension of weighted Sobolev inequalities to capillary settings outside convex sets using weighted isoperimetric inequalities and capillary Schwarz symmetrization.


<details>
  <summary>Details</summary>
Motivation: To extend weighted Sobolev inequalities from half-spaces to more general capillary settings outside convex sets, building on previous work by Ciraolo-Figalli-Roncoroni.

Method: Uses weighted capillary isoperimetric inequality via λw-ABP method, develops capillary Schwarz symmetrization outside convex sets, and establishes weighted Pólya-Szegő principle.

Result: Proves sharp weighted capillary Sobolev inequality outside convex domains, extending previous results from half-spaces to more general settings.

Conclusion: Successfully extends weighted Sobolev inequalities to capillary contexts outside convex sets, providing new tools for analysis in these geometric settings.

Abstract: In this paper, we establish a weighted capillary isoperimetric inequality
outside convex sets using the $\lambda_w$-ABP method. The weight function $w$
is assumed to be positive, even, and homogeneous of degree $\alpha$, such that
$w^{1/\alpha}$ is concave on $\R^n$.
  Based on the weighted isoperimetric inequality, we develop a technique of
capillary Schwarz symmetrization outside convex sets, and establish a weighted
P\'{o}lya-Szeg\"{o} principle and a sharp weighted capillary Sobolev inequality
outside convex domain. Our result can be seen as an extension of the weighted
Sobolev inequality in the half-space established by Ciraolo-Figalli-Roncoroni
in \cite{CFR}.

</details>


### [16] [On dispersive decay for the generalized Korteweg--de Vries equation](https://arxiv.org/abs/2510.01728)
*Matthew Kowalski,Minjie Shan*

Main category: math.AP

TL;DR: Pointwise-in-time dispersive estimates for gKdV equation solutions, showing L^∞ decay like |t|^{-1/3} for mass-critical model with initial data in specific Sobolev spaces.


<details>
  <summary>Details</summary>
Motivation: To establish dispersive estimates for generalized Korteweg-de Vries equation solutions, particularly for the mass-critical case, extending understanding of solution behavior over time.

Method: Developed persistence of negative regularity for gKdV solutions and extended Lorentz-Strichartz estimates to mixed norm cases.

Result: Proved that solutions to mass-critical gKdV decay in L^∞ like |t|^{-1/3} when initial data lie in H^{1/4} ∩ H^{-1/12}.

Conclusion: Successfully established pointwise-in-time dispersive estimates for gKdV equation, demonstrating specific decay rates for solutions with appropriate initial conditions.

Abstract: We prove pointwise-in-time dispersive estimates for solutions to the
generalized Korteweg--de Vries (gKdV) equation. In particular, for solutions to
the mass-critical model, we assume only that initial data lie in
$\dot{H}^{\frac{1}{4}} \cap \dot{H}^{-\frac{1}{12}}$ and show that solutions
decay in $L^\infty$ like $|t|^{-\frac{1}{3}}$. To accomplish this, we develop a
persistence of negative regularity for solutions to gKdV and extend
Lorentz--Strichartz estimates to the mixed norm case.

</details>


### [17] [Nonlinear Forward-Backward Problems](https://arxiv.org/abs/2510.01732)
*Anne-Laure Dalibard,Frédéric Marbach,Jean Rax*

Main category: math.AP

TL;DR: Existence and uniqueness of strong solutions for a quasilinear forward-backward parabolic equation near linear shear flow, with solutions changing sign across a critical line and requiring careful handling of orthogonality conditions.


<details>
  <summary>Details</summary>
Motivation: To study quasilinear forward-backward parabolic problems that change type across critical curved lines, where lateral boundary conditions can only be imposed where characteristics are inwards, addressing difficulties from solution-dependent geometry and singular solutions in linearized equations.

Method: Developed an iterative approximation procedure with careful handling of orthogonality conditions for singular solutions in linearized equations, proving stability of these conditions with respect to base flow, using a nonlinear fixed-point scheme.

Result: Proved existence and uniqueness of strong solutions when source terms satisfy finite orthogonality conditions, with solutions regular if and only if these conditions are met.

Conclusion: Developed a natural and adaptable methodology for proving existence of regular solutions to nonlinear problems despite singular solutions at linear level, applicable to other similar situations.

Abstract: We prove the existence and uniqueness of strong solutions to the equation $u
u_x - u_{yy} = f$ in the vicinity of the linear shear flow, subject to
perturbations of the source term and lateral boundary conditions. Since the
solutions we consider have opposite signs in the lower and upper half of the
domain, this is a quasilinear forward-backward parabolic problem, which changes
type across a critical curved line within the domain. In particular, lateral
boundary conditions can be imposed only where the characteristics are inwards.
There are several difficulties associated with this problem. First, the
forward-backward geometry depends on the solution itself. This requires to be
quite careful with the approximation procedure used to construct solutions.
Second, and more importantly, the linearized equations solved at each step of
the iterative scheme admit a finite number of singular solutions, of which we
provide an explicit construction. This is similar to well-known phenomena in
elliptic problems in nonsmooth domains. Hence, the solutions to the equation
are regular if and only if the source terms satisfy a finite number of
orthogonality conditions. A key difficulty of this work is to cope with these
orthogonality conditions during the nonlinear fixed-point scheme. In
particular, we are led to prove their stability with respect to the underlying
base flow. To tackle this deceivingly simple problem, we develop a methodology
which we believe to be both quite natural and adaptable to other situations in
which one wishes to prove the existence of regular solutions to a nonlinear
problem for suitable data despite the existence of singular solutions at the
linear level. This paper is a shorter version of [3].

</details>


### [18] [Notes on Schauder estimates by scaling for elliptic PDEs in divergence form](https://arxiv.org/abs/2510.01765)
*Stefano Vita*

Main category: math.AP

TL;DR: These notes cover classical interior and local Schauder estimates for second-order linear elliptic PDEs in divergence form using scaling techniques inspired by Simon's work, with geometric methods from minimal surfaces.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive and self-contained treatment of Schauder estimates for elliptic PDEs using geometric techniques developed in the study of minimal surfaces and free boundary problems.

Method: Uses scaling, compactness and blow-up arguments combined with rigidity results (Liouville theorems), adopting geometric approaches from minimal surfaces theory.

Result: Presents classical interior and local Schauder estimates for second-order linear elliptic PDEs in divergence form.

Conclusion: The geometric approach using scaling, compactness, and blow-up arguments provides an effective framework for deriving Schauder estimates, sharing features with methods used in free boundary problems.

Abstract: These are the notes of a part of the PhD course Regularity for free boundary
problems and for elliptic PDEs, held in Pavia in the spring of 2025. The aim is
to provide a comprehensive and self-contained treatment of classical interior
and local Schauder estimates for second-order linear elliptic PDEs in
divergence form via scaling in the spirit of Simon's work. The main techniques
presented here are geometric in nature and were primarily developed in the
study of geometric problems such as minimal surfaces. The adopted approach
relies on compactness and blow-up arguments, combined with rigidity results
(Liouville theorems), and shares many features with the one used in the study
of free boundary problems, which was the main topic of the other part of the
PhD course.

</details>


### [19] [Strichartz and dispersive estimates for quantum bouncing ball model: exponential sums and Van der Corput methods in 1d semi-classical Schrödinger equations](https://arxiv.org/abs/2510.01779)
*Oana Ivanovici*

Main category: math.AP

TL;DR: Improved dispersive and Strichartz estimates for 1D semi-classical Schrödinger equation with linear potential on half-line, reducing losses from 1/4 to potentially 1/6+ε.


<details>
  <summary>Details</summary>
Motivation: To establish better space-time behavior understanding of solutions to the semi-classical Schrödinger equation with linear potential and Dirichlet boundary conditions.

Method: Using Van der Corput-type derivative tests to prove refined Strichartz bounds, and analyzing exponential sums to potentially further reduce losses.

Result: Proved improved Strichartz estimates that beat previous 1/4 losses, with potential for further reduction to 1/6+ε losses under sharp exponential sum bounds.

Conclusion: The results suggest optimal Strichartz bounds are achievable and analogous estimates should extend to higher-dimensional Friedlander model domains.

Abstract: We analyze the one-dimensional semi-classical Schr\"odinger equation on the
half-line with a linear potential and Dirichlet boundary conditions. Our main
focus is on establishing improved dispersive and Strichartz estimates for this
model, which govern the space-time behavior of solutions. We prove refined
Strichartz bounds using Van der Corput-type derivative tests, beating previous
known results where Strichartz estimates incur 1/4 losses. Moreover, assuming
sharp bounds for certain exponential sums, our results indicate the possibility
to reduce these losses further to $1/6 + \epsilon$ for all $\epsilon>0$, which
would be sharp. We further expect that analogous Strichartz bounds should hold
within the Friedlander model domain in higher dimensions.

</details>


### [20] [Monotonicity and Liouville-type theorems for semilinear elliptic problems in the half space](https://arxiv.org/abs/2510.01865)
*Berardino Sciunzi,Domenico Vuono*

Main category: math.AP

TL;DR: Positive solutions to -Δu = f(u) in half-spaces with Dirichlet boundary conditions are strictly monotone increasing in the direction orthogonal to the boundary, given directional boundedness on finite strips. This leads to a new Liouville-type theorem for the Lane-Emden equation.


<details>
  <summary>Details</summary>
Motivation: To establish monotonicity properties of solutions to semilinear elliptic equations in half-spaces and derive new Liouville-type results, particularly for the Lane-Emden equation.

Method: Analysis of classical solutions to -Δu = f(u) in half-spaces with homogeneous Dirichlet boundary conditions, using directional boundedness assumptions on finite strips to prove strict monotonicity.

Result: Proved that any positive solution is strictly monotone increasing in the direction orthogonal to the boundary under the given boundedness condition.

Conclusion: The monotonicity result provides a new Liouville-type theorem for the Lane-Emden equation, extending the understanding of solution behavior in half-spaces.

Abstract: We consider classical solutions to $-\Delta u = f(u)$ in half-spaces, under
homogeneous Dirichlet boundary conditions. We prove that any positive solution
is strictly monotone increasing in the direction orthogonal to the boundary,
provided that it is directionally bounded on finite strips. As a corollary, we
deduce a new Liouville-type theorem for the Lane-Emden equation.

</details>


### [21] [On sharp Strichartz estimate for hyperbolic Schrödinger equation on $\mathbb{T}^3$](https://arxiv.org/abs/2510.01886)
*Baoping Liu,Xu Zheng*

Main category: math.AP

TL;DR: Sharp Strichartz estimate for hyperbolic Schrödinger equation on 3D torus via incidence geometry, with applications to optimal local well-posedness of nonlinear equations.


<details>
  <summary>Details</summary>
Motivation: To establish precise estimates for hyperbolic Schrödinger equations, which are important for understanding wave propagation and nonlinear dynamics in periodic domains.

Method: Used an incidence geometry approach to prove the sharp Strichartz estimate on the 3D torus.

Result: Proved the sharp Strichartz estimate for hyperbolic Schrödinger equation on T^3.

Conclusion: The sharp estimate enables optimal local well-posedness results for nonlinear hyperbolic Schrödinger equations.

Abstract: We prove the sharp Strichartz estimate for hyperbolic Schr\"{o}dinger
equation on $\mathbb{T}^3 $ via an incidence geometry approach. As application,
we obtain optimal local well-posedness of nonlinear hyperbolic Schr\"{o}dinger
equations.

</details>


### [22] [A note on the recovery sequence in the double gradient model for phase transitions](https://arxiv.org/abs/2510.01893)
*Jakob Deutsch*

Main category: math.AP

TL;DR: Analysis of the limsup inequality in double gradient models for phase transitions using Modica-Mortola functionals with double-well potentials in 2D.


<details>
  <summary>Details</summary>
Motivation: To understand the limiting behavior of interfacial energies in phase transition models governed by double gradient functionals as the regularization parameter approaches zero.

Method: Using energy functionals with double-well potentials and double gradient terms, analyzing the limsup inequality through periodic recovery sequences and optimal profile constants.

Result: Characterization of the limiting interfacial energy under assumptions about the optimal profile constant in relation to geodesic distance between wells.

Conclusion: The study provides insights into the asymptotic behavior of phase transition models with double gradient structure and establishes connections between optimal profile constants and interfacial energies.

Abstract: We investigate the $\limsup$ inequality in the double gradient model for
phase transitions governed by a Modica--Mortola functional with a double-well
potential in two dimensions. Specifically, we consider energy functionals of
the form \[ E_\varepsilon(u, \Omega) = \int_\Omega \left( \frac{1}{\varepsilon}
W(\nabla u) + \varepsilon |\nabla^2 u|^2 \right) dx \] for maps $ u \in
H^2(\Omega; \mathbb{R}^2) $, where $ W $ vanishes only at two wells. Assuming a
bound on the optimal profile constant -- namely the cell problem on the unit
cube -- in terms of the geodesic distance between the two wells, we
characterise the limiting interfacial energy via periodic recovery sequences as
$\varepsilon \to 0^+$.

</details>


### [23] [Subwavelength resonances in two-dimensional elastic media with high contrast](https://arxiv.org/abs/2510.01911)
*Yuanchun Ren,Yixian Gao*

Main category: math.AP

TL;DR: This paper uses layer potential techniques to study wave scattering in 2D elastic media with high parameter contrasts, characterizing resonant frequencies, analyzing scattered fields, and examining subwavelength bandgaps in phononic crystals.


<details>
  <summary>Details</summary>
Motivation: To investigate wave scattering phenomena in two-dimensional elastic media that exhibit significant contrasts in Lamé parameters and density, which is relevant for understanding wave propagation in heterogeneous materials and designing phononic crystals.

Method: Employed layer potential techniques and boundary integral operators, constructed an invertible operator using kernel spaces, performed asymptotic analysis to derive resonant frequency equations, and analyzed scattered fields across different frequency regimes.

Result: Developed a framework to characterize resonant frequencies through orthogonality conditions, derived leading-order equations for resonant frequencies, characterized longitudinal and transverse far-field patterns, and examined subwavelength bandgaps in dilute phononic crystal structures.

Conclusion: The study provides comprehensive analytical tools for understanding wave scattering in high-contrast elastic media, with applications to phononic crystal design and wave manipulation in heterogeneous materials.

Abstract: This paper employs layer potential techniques to investigate wave scattering
in two-dimensional elastic media exhibiting high contrasts in both Lam\'{e}
parameters and density. Our contributions are fourfold. First, we construct an
invertible operator based on the kernel spaces of boundary integral operators,
which enables the characterization of resonant frequencies through an
orthogonality condition. Second, we use asymptotic analysis to derive the
equation governing the leading-order terms of these resonant frequencies.
Third, we analyze the scattered field in the interior domain for incident
frequencies across different regimes and characterize the longitudinal and
transverse far-field patterns in the exterior domain. Finally, we examine the
subwavelength bandgap in the phononic crystal with a dilute structure.

</details>


### [24] [Low regularity Sobolev well-posedness for Vlasov--Poisson](https://arxiv.org/abs/2510.02112)
*In-Jee Jeong,Sangwook Tae*

Main category: math.AP

TL;DR: Local well-posedness of Vlasov-Poisson equation in H^s spaces with s > n/2 - 1/4 for n ≥ 3, allowing non-L^p data.


<details>
  <summary>Details</summary>
Motivation: To establish local well-posedness for the Vlasov-Poisson equation in Sobolev spaces with lower regularity requirements, particularly allowing initial data that may not belong to L^p spaces for large p.

Method: Analysis of the Vlasov-Poisson equation on R^n × R^n using Sobolev space techniques, focusing on initial distributions with compact velocity support.

Result: Proved local well-posedness in H^s(R^n × R^n) with s > n/2 - 1/4 for n ≥ 3, for initial data f_0 ∈ H^s with compact support in velocity variable.

Conclusion: The Vlasov-Poisson equation is locally well-posed in Sobolev spaces with regularity index above n/2 - 1/4, extending the class of admissible initial data beyond traditional L^p spaces.

Abstract: We consider the Vlasov--Poisson equation on $\mathbb{R}^n \times
\mathbb{R}^n$ with $n \ge 3$. We prove local well-posedness in
$H^{s}(\mathbb{R}^n \times \mathbb{R}^n)$ with $s> n/2-1/4$, for initial
distribution $f_{0} \in H^{s}(\mathbb{R}^n \times \mathbb{R}^n)$ having compact
support in $v$. In particular, data not belonging to $L^p(\mathbb{R}^n \times
\mathbb{R}^n)$ for large $p$ are allowed.

</details>


### [25] [Transfer of Stability from the Classical to the Fractional Anisotropic Calderón Problem](https://arxiv.org/abs/2510.02242)
*Hendrik Baers,Angkana Rüland*

Main category: math.AP

TL;DR: This paper analyzes spectral fractional anisotropic Calderón problems with source-to-solution measurements and establishes quantitative relationships between local and nonlocal Calderón problems, including stability estimates and uniqueness transfers.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between local and nonlocal Calderón problems by quantifying their relationships and providing stability estimates, particularly for cases where no Liouville-type reduction is known.

Method: Uses quantitative unique continuation arguments to relate source-to-solution measurements in fractional Calderón problems to classical Calderón problems, establishing stability estimates and uniqueness transfers between local and nonlocal settings.

Result: Demonstrates that stability results for local Calderón problems with source-to-solution data directly translate to nonlocal analogues (with logarithmic loss), and provides first stability results for principal parts in fractional Calderón problems without known Liouville-type reductions.

Conclusion: The paper successfully establishes quantitative connections between local and nonlocal Calderón problems, providing stability estimates and uniqueness transfers that extend classical results to fractional settings, with applications to anisotropic problems and spectral fractional Dirichlet Laplacians.

Abstract: We discuss two spectral fractional anisotropic Calder\'on problems with
source-to-solution measurements and their quantitative relation to the
classical Calder\'on problem. Firstly, we consider the anistropic fractional
Calder\'on problem from [FGKU25]. In this setting, we quantify the relation
between the local and nonlocal Calder\'on problems which had been deduced in
[R25] and provide an associated stability estimate. As a consequence, any
stability result which holds on the level of the local problem with
source-to-solution data has a direct nonlocal analogue (up to a logarithmic
loss). Secondly, we introduce and discuss the fractional Calder\'on problem
with source-to-solution measurements for the spectral fractional Dirichlet
Laplacian on open, bounded, connected, Lipschitz sets on $\mathbb{R}^n$. Also
in this context, we provide a qualitative and quantitative transfer of
uniqueness from the local to the nonlocal setting. As a consequence, we infer
the first stability results for the principal part for a fractional Calder\'on
type problem for which no reduction of Liouville type is known. Our arguments
rely on quantitative unique continuation arguments. As a result of independent
interest, we also prove a quantitative relation between source-to-solution and
Dirichlet-to-Neumann measurements for the classical Calder\'on problem.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [26] [Integrated Software/Hardware Execution Models for High-Accuracy Methods in Chemistry](https://arxiv.org/abs/2510.01205)
*Nicholas Bauman,Ajay Panyala,Libor Veis,Jiri Brabec,Paul Rigor,Randy Meyer,Skyler Windh,Craig Warner,Tony Brewer,Karol Kowalski*

Main category: physics.comp-ph

TL;DR: This paper presents a hybrid approach combining Micron CXL memory hardware for CC downfolding and Azure Quantum Element cloud computing for DMRG simulations in quantum chemistry workflows.


<details>
  <summary>Details</summary>
Motivation: To optimize quantum chemistry simulations by effectively utilizing emerging computational resources, particularly addressing the memory-intensive nature of coupled-cluster downfolding phases.

Method: Synergistic utilization of Micron memory technologies and Azure Quantum Element cloud computing in DMRG simulations using DUCC-based effective Hamiltonians, with a hybrid approach allocating different computational resources based on workflow component requirements.

Result: Developed a workflow that leverages Micron CXL hardware for memory-intensive CC downfolding and AQE cloud computing for less resource-intensive DMRG simulations, with performance analysis conducted on Micron prototype systems using the ExaChem suite.

Conclusion: The hybrid resource allocation strategy effectively addresses the varying computational demands of different workflow components in quantum chemistry simulations, enabling optimized performance through proper hardware selection.

Abstract: The effective deployment and application of advanced methodologies for
quantum chemistry is inherently linked to the optimal usage of emerging and
highly diversified computational resources. This paper examines the synergistic
utilization of Micron memory technologies and Azure Quantum Element cloud
computing in Density Matrix Renormalization Group (DMRG) simulations leveraging
coupled-cluster (CC) downfolded/effective Hamiltonians based on the double
unitary coupled cluster (DUCC) Ansatz. We analyze the performance of the
DMRG-DUCC workflow, emphasizing the proper choice of hardware that reflects the
numerical overheads associated with specific components of the workflow. We
report a hybrid approach that takes advantage of Micron CXL hardware for the
memory capacity intensive CC downfolding phase while employing AQE cloud
computing for the less resource-intensive DMRG simulations. Furthermore, we
analyze the performance of the scalable ExaChem suite of electronic simulations
conducted on Micron prototype systems.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [Detailed Derivation of the Scalar Explicit Expressions Governing the Electric Field, Current Density, and Volumetric Power Density in the Four Types of Linear Divergent MHD Channels Under a Unidirectional Applied Magnetic Field](https://arxiv.org/abs/2510.01289)
*Osama A. Marzouk*

Main category: physics.plasm-ph

TL;DR: Analytical derivation of electric field, current density, and power density expressions for four types of MHD linear channels in Open-Cycle Magnetohydrodynamic generators.


<details>
  <summary>Details</summary>
Motivation: To provide mathematical analysis and analytical expressions for electric field, current density, and power density in MHD channels to aid in selecting the most suitable channel type for specific applications.

Method: Mathematical analysis of algebraic equations governing electric field vector and electric-current density field vector in Magnetohydrodynamic linear two-dimensional divergent supersonic channels.

Result: Derived analytical expressions for four MHD channel types with numerical estimates showing Hall-linear channel achieves higher power density (120.19 MW/m³) compared to Faraday channel (62.5 MW/m³) under typical conditions.

Conclusion: The analytical framework and numerical results help determine optimal MHD channel type selection for electric power generation applications based on performance metrics.

Abstract: The current study belongs to the field of applied mathematics in plasma
physics and electric power, where mathematical analysis of the algebraic
equations governing the electric field vector, and the electric-current density
field vector within a Magnetohydrodynamic (MHD) linear two-dimensional
divergent supersonic channel is utilized to derive analytical expressions for
these important fields, as well as closed-form equations for the volumetric
power density (output electric power per unit volume of the plasma channel).
The expressions presented here describe analytically the operation of the MHD
channel as an electric power source within an Open-Cycle Magnetohydrodynamic
(OCMHD) generator. The four common types of the MHD linear channels are covered
here: namely, (1) continuous-electrode Faraday channel, (2) linear Hall
channel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode
channel. The mathematical results, their detailed derivation, and the companion
graphical illustrations aid in making a proper decision regarding which channel
type is the most suitable for a given application.Under typical operational
conditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000
m/s plasma speed, as well as an optimized load factor of 0.5, we estimate the
following numerical values (unsigned magnitudes) for the continuous-electrode
Faraday channel (with a Hall parameter of 1): useful electric field (across the
external electric load): 5 kV/m, useful electric current-density (between the
terminal electrodes within the channel): 12.5 kA/m2 , volumetric power density
(dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric
efficiency (for the electric field or voltage): 50%. For the Halllinear channel
(with a Hall parameter of 5), these quantitative performance values become25
kV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%.

</details>


### [28] [Suppression of inverse magnetic energy transfer in collisionless marginally magnetized plasmas](https://arxiv.org/abs/2510.01573)
*Zhuo Liu,Muni Zhou,Nuno F. G. Loureiro*

Main category: physics.plasm-ph

TL;DR: Inverse magnetic energy cascade in collisionless plasmas is suppressed by pressure-anisotropy-driven instabilities like firehose instability, which nullify magnetic tension and prevent reconnection-driven coalescence of magnetic structures.


<details>
  <summary>Details</summary>
Motivation: To understand the inverse transfer of magnetic energy in decaying collisionless plasmas and investigate why Weibel-generated seed fields may fail to merge consistently in astrophysical contexts.

Method: First-principles numerical simulations and analytical theory to study decaying collisionless plasmas with moderate to high-β values.

Result: Firehose instability suppresses inverse energy transfer by nullifying magnetic tension, leaving magnetic structures elongated at Larmor radius scales. A strong magnetic guide field or greater scale separation between initial structure size and Larmor radius restores inverse transfer capability.

Conclusion: Inverse energy transfer in collisionless plasmas is not guaranteed and sensitively depends on magnetization, revealing a kinetic mechanism that may limit the role of Weibel-generated fields in cosmic magnetogenesis.

Abstract: We investigate the inverse cascade of magnetic energy in decaying,
collisionless plasmas with moderate to high-$\beta$ values via first-principles
numerical simulations and analytical theory. We find that
pressure-anisotropy-driven instabilities, in particular the firehose
instability, suppress reconnection-driven coalescence of magnetic structures
(i.e., inverse transfer) by nullifying magnetic tension. This suppression
leaves such structures elongated and confined to scales comparable to the
Larmor radius of the particles. The presence of a magnetic guide field of
sufficient strength, or a greater scale separation between the initial size of
the magnetic structures and the Larmor radius, restores the system's ability to
inverse transfer magnetic energy. These results reveal that inverse energy
transfer in collisionless plasmas is not guaranteed, but instead sensitively
depends on magnetization. In the astrophysical context, this identifies a
kinetic mechanism by which Weibel-generated seed fields may fail to merge
consistently, potentially limiting their role in cosmic magnetogenesis.

</details>


### [29] [Accelerating kinetic plasma simulations with machine learning generated initial conditions](https://arxiv.org/abs/2510.01977)
*Andrew T. Powis,Domenica Corona Rivera,Alexander Khrabry,Igor D. Kaganovich*

Main category: physics.plasm-ph

TL;DR: Machine learning-generated initial conditions accelerate kinetic plasma simulations, achieving up to 17.1x speedup in convergence time for capacitively coupled plasma discharge modeling.


<details>
  <summary>Details</summary>
Motivation: Multi-time-scale plasma systems require many time steps to reach quasi-steady state convergence, making computer-aided engineering challenging. Machine learning combined with traditional simulations offers pathways to resolve this computational bottleneck.

Method: Three machine learning models (multi-layer perceptron, principal component analysis, and convolutional neural networks) are trained on simulations across device parameters to predict final time-averaged ion-density and velocity distribution profiles. A workflow for continuous model improvement is also outlined.

Result: Data-driven initial condition generators provide mean speedup of 17.1x with offline procedure and 4.4x with online procedure. Convolutional neural networks performed best among the three model types.

Conclusion: Machine learning-generated initial conditions effectively accelerate plasma simulations, with a proposed workflow for continuous improvement toward generating sufficient data for full device digital twins.

Abstract: Computer aided engineering of multi-time-scale plasma systems which exhibit a
quasi-steady state solution are challenging due to the large number of time
steps required to reach convergence. Machine learning techniques combined with
traditional first-principles simulations and high-performance computing offer
many interesting pathways towards resolving this challenge. We consider
acceleration of kinetic plasma simulations via machine learning generated
initial conditions. The approach is demonstrated through modeling of
capacitively coupled plasma discharges relevant to the microelectronics
industry. Three models are trained on simulations across a parameter space of
device driving frequency and operating pressure. The models incorporate
elements of a multi-layer perceptron, principal component analysis, and
convolutional neural networks to predict the final time-averaged profiles of
ion-density and velocity distribution functions. These data-driven initial
condition generators (ICGs) provide a mean speedup of 17.1x in convergence
time, when measured using an offline procedure, or a 4.4x speedup with an
online procedure, with convolutional neural networks leading to the best
performance. The paper also outlines a workflow for continuous data-driven
model improvement and simulation speedup, with the aim of generating sufficient
data for full device digital twins.

</details>


### [30] [A neural network approach to kinetic Mie polarimetry for particle size diagnostics in nanodusty plasmas](https://arxiv.org/abs/2510.02088)
*Alexander Schmitz,Andreas Petersen,Franko Greiner*

Main category: physics.plasm-ph

TL;DR: A neural network is introduced to analyze nanoparticle sizes in plasma technology, replacing traditional Mie theory back-calculation methods that require expertise.


<details>
  <summary>Details</summary>
Motivation: Standard light scattering techniques for nanoparticle size analysis require user expertise and are not automated. The goal is to create a more accessible and efficient diagnostic tool.

Method: A neural network is set up and trained to analyze plasma-grown amorphous carbon nanoparticles using kinetic diagnostic approach where particles change size due to growth or etching.

Result: The neural network achieves results comparable to prior fitting algorithms but with higher methodical stability, faster computing speed, and better automation capabilities.

Conclusion: The neural network approach provides a superior alternative to traditional methods for nanoparticle size analysis in plasma technology, offering improved stability, speed, and automation.

Abstract: The analysis of the size of nanoparticles is an essential task in plasma
technology and dusty plasmas. Light scattering techniques, based on Mie theory,
can be used as a non-invasive and in-situ diagnostic tool for this purpose.
However, the standard back-calculation methods require expertise from the user.
To address this, we introduce a neural network that performs the same task. We
discuss how we set up and trained the network to analyze the size of
plasma-grown amorphous carbon nanoparticles (a:C-H) with a refractive index n
in the range of real(n) = 1.4-2.2 and imag(n) = 0.04i-0.1i and a radius of up
to several hundred nanometers, depending on the used wavelength. The diagnostic
approach is kinetic, which means that the particles need to change in size due
to growth or etching. An uncertainty analysis as well as a test with
experimental data are presented. Our neural network achieves results that agree
with those of prior fitting algorithms while offering higher methodical
stability. The model also holds a major advantage in terms of computing speed
and automation.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [31] [Multiscale analysis of large twist ferroelectricity and swirling dislocations in bilayer hexagonal boron nitride](https://arxiv.org/abs/2510.01419)
*Md Tusher Ahmed,Chenhaoyue Wang,Amartya S. Banerjee,Nikhil Chandra Admal*

Main category: cond-mat.mtrl-sci

TL;DR: This paper demonstrates that ferroelectricity persists in bilayer hexagonal boron nitride (hBN) under large heterodeformations, not just small ones. The authors develop a bicrystallography-informed multiscale model to study ferroelectricity in configurations near both AA and Σ7 stacking, revealing different polarization switching mechanisms.


<details>
  <summary>Details</summary>
Motivation: Previous studies focused only on small heterodeformations in bilayer hBN, leaving the persistence of ferroelectricity under large heterodeformations unexplored. This work aims to address this gap and understand ferroelectric behavior across different deformation regimes.

Method: Used Smith normal form bicrystallography to establish crystallographic origin of ferroelectricity. Conducted atomistic simulations for AA-vicinal systems, and developed a density-functional-theory-informed continuum framework (BFIM model) for Σ7-vicinal systems where atomistic methods are unreliable.

Result: Found that AA-vicinal systems support ferroelectricity under both small twist and small strain, with different polarization switching mechanisms. For Σ7-vicinal systems, the BFIM model successfully captures out-of-plane ferroelectricity. Interface dislocations in large heterodeformations have smaller Burgers vectors compared to small-deformation cases.

Conclusion: Ferroelectricity persists in bilayer hBN under large heterodeformations. The developed BFIM model provides an efficient computational framework for predicting ferroelectricity in large-unit-cell heterostructures where atomistic simulations are too expensive.

Abstract: With its atomically thin structure and intrinsic ferroelectric properties,
heterodeformed bilayer hexagonal boron nitride (hBN) has gained prominence in
next-generation non-volatile memory applications. However, studies to date have
focused almost exclusively on small heterodeformations, leaving the question of
whether ferroelectricity can persist under large heterodeformation entirely
unexplored. In this work, we establish the crystallographic origin of
ferroelectricity in bilayer hBN configurations heterodeformed relative to
high-symmetry configurations such as the AA-stacking and the 21.786789 $\circ$
twisted configuration, using Smith normal form bicrystallography. We then
demonstrate out-of-plane ferroelectricity in bilayer hBN across configurations
vicinal to both the AA and $\Sigma 7$ stacking. Atomistic simulations reveal
that AA-vicinal systems support ferroelectricity under both small twist and
small strain, with polarization switching in the latter governed by the
deformation of swirling dislocations rather than the straight interface
dislocations seen in the former. For $\Sigma 7$-vicinal systems, where reliable
interatomic potentials are lacking, we develop a
density-functional-theory-informed continuum framework--the
bicrystallography-informed frame-invariant multiscale (BFIM) model, which
captures out-of-plane ferroelectricity in heterodeformed configurations vicinal
to the $\Sigma 7$ stacking. Interface dislocations in these large
heterodeformed bilayer configurations exhibit markedly smaller Burgers vectors
compared to the interface dislocations in small-twist and small-strain bilayer
hBN. The BFIM model reproduces atomistic simulation results and provides a
powerful, computationally efficient framework for predicting ferroelectricity
in large-unit-cell heterostructures where atomistic simulations are
prohibitively expensive.

</details>


### [32] [Enhancing the Efficiency of Time-Dependent Density Functional Theory Calculations of Dynamic Response Properties](https://arxiv.org/abs/2510.01875)
*Zhandos A. Moldabekov,Sebastian Schwalbe,Uwe Hernandez Acosta,Thomas Gawne,Jan Vorberger,Michele Pavanello,Tobias Dornheim*

Main category: cond-mat.mtrl-sci

TL;DR: A method to speed up TDDFT calculations for X-ray Thomson scattering by using imaginary time mapping and noise attenuation, achieving up to 10x speed-up.


<details>
  <summary>Details</summary>
Motivation: TDDFT is accurate for modeling XRTS spectra but computationally expensive under extreme conditions due to thermal excitations, temperature/density variations, and detector size.

Method: Use one-to-one mapping between dynamic structure factor and imaginary time density-density correlation function from Feynman's path integral, combined with convergence tests and constraints-based noise attenuation.

Result: Achieved speed-up by up to an order of magnitude, potentially saving millions of CPU hours for single XRTS measurements.

Conclusion: The method significantly improves TDDFT efficiency for XRTS modeling under extreme conditions without introducing significant bias.

Abstract: X-ray Thomson scattering (XRTS) constitutes an essential technique for
diagnosing material properties under extreme conditions, such as high pressures
and intense laser heating. Time-dependent density functional theory (TDDFT) is
one of the most accurate available ab initio methods for modeling XRTS spectra,
as well as a host of other dynamic material properties. However, strong thermal
excitations, along with the need to account for variations in temperature and
density as well as the finite size of the detector significantly increase the
computational cost of TDDFT simulations compared to ambient conditions. In this
work, we present a broadly applicable method for optimizing and enhancing the
efficiency of TDDFT calculations. Our approach is based on a one-to-one mapping
between the dynamic structure factor and the imaginary time density--density
correlation function, which naturally emerges in Feynman's path integral
formulation of quantum many-body theory. Specifically, we combine rigorous
convergence tests in the imaginary time domain with a constraints-based noise
attenuation technique to improve the efficiency of TDDFT modeling without the
introduction of any significant bias. As a result, we can report a speed-up by
up to an order of magnitude, thus potentially saving millions of CPU hours for
modeling a single XRTS measurement of matter under extreme conditions.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [33] [Perturbations of Minkowski spacetime with regular conformal compactification](https://arxiv.org/abs/2510.01964)
*Andrea Nützi*

Main category: gr-qc

TL;DR: The paper constructs perturbations of Minkowski spacetime in general relativity with initial data decaying polynomially to Kerr spacetime data, showing these perturbations admit regular conformal compactification at null and timelike infinity.


<details>
  <summary>Details</summary>
Motivation: To generalize Friedrich's results on spacetimes with smooth conformal compactification from compactly supported perturbations to more general initial data that decays polynomially to Kerr initial data.

Method: Uses a novel formulation of Einstein equations about Minkowski spacetime as a quasilinear symmetric hyperbolic PDE, regular at null infinity. Main technical work involves constructing solutions near spacelike infinity using tailored energy estimates and organizing equations by scaling homogeneity.

Result: Perturbations admit regular conformal compactification at null and timelike infinity, with regularity increasing linearly with decay rate of initial data to Kerr data. Smooth compactification achieved for rapidly decaying initial data.

Conclusion: Successfully extends Friedrich's results to more general initial data conditions, demonstrating that polynomial decay to Kerr initial data suffices for regular conformal compactification, with smoothness achieved for rapid decay.

Abstract: We construct perturbations of Minkowski spacetime in general relativity, when
given initial data that decays inverse polynomially to initial data of a Kerr
spacetime towards spacelike infinity. We show that the perturbations admit a
regular conformal compactification at null and timelike infinity, where the
degree of regularity increases linearly with the rate of decay of the initial
data to Kerr initial data. In particular, the compactification is smooth if the
initial data decays rapidly to Kerr initial data. This generalizes results of
Friedrich, who constructed spacetimes with a smooth conformal compactification
in the case when the initial data is identical to Kerr initial data on the
complement of a compact set. Our results rely on a novel formulation of the
Einstein equations about Minkowski spacetime introduced by the author, that
allows one to formulate the dynamic problem as a quasilinear, symmetric
hyperbolic PDE that is regular at null infinity and with null infinity being at
a fixed locus. It is not regular at spacelike infinity, due to the asymptotics
of Kerr. Thus the main technical task is the construction of solutions near
spacelike infinity, using tailored energy estimates. To accomplish this, we
organize the equations according to homogeneity with respect to scaling about
spacelike infinity, which identifies terms that are leading, respectively lower
order, near spacelike infinity, with contributions from Kerr being lower order.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [34] [Mean curvature flow through singularities](https://arxiv.org/abs/2510.01355)
*Robert Haslhofer*

Main category: math.DG

TL;DR: This paper provides an overview of mean curvature flow theory, recent advances in singularity analysis in R³, classification of noncollapsed singularities in R⁴, and discusses open problems.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive theory for mean curvature flow through singularities, particularly extending the precise theory from R³ to higher dimensions like R⁴.

Method: General introduction to mean curvature flow, analysis of fundamental results from the last decade, classification of noncollapsed singularities in R⁴, and discussion of open problems.

Result: Established a precise theory for mean curvature flow through singularities in R³ over the last 10 years, and achieved classification of all noncollapsed singularities in R⁴.

Conclusion: The paper presents significant progress in understanding mean curvature flow singularities, with successful classification in R⁴ and identification of remaining challenges for higher dimensions.

Abstract: We first give a general introduction to the mean curvature flow, and then
discuss fundamental results established over the last 10 years that yield a
precise theory for the flow through singularities in $\mathbb{R}^3$. With the
aim of developing a satisfying theory in higher dimensions, we then describe
our recent classification of all noncollapsed singularities in $\mathbb{R}^4$.
Finally, we provide a detailed discussion of open problems and conjectures.

</details>


### [35] [Uniqueness in the Plateau problem for calibrated currents](https://arxiv.org/abs/2510.02299)
*Bryan Dimler,Chen-Kuan Lee*

Main category: math.DG

TL;DR: Compactly supported calibrated integral currents with connected C³,α boundaries are unique solutions to the oriented Plateau problem for their boundary data.


<details>
  <summary>Details</summary>
Motivation: To establish uniqueness results for the oriented Plateau problem by leveraging boundary regularity and unique continuation principles.

Method: Uses boundary regularity theory for area-minimizing currents and adapts classical unique continuation principles to the minimal surface system.

Result: Proves that such currents are the unique solutions to the oriented Plateau problem for their given boundary data.

Conclusion: The combination of boundary regularity and unique continuation ensures uniqueness in the oriented Plateau problem for these specific currents.

Abstract: We show that every compactly supported calibrated integral current with
connected $C^{3,\alpha}$ boundary is the unique solution to the oriented
Plateau problem for its boundary data. This is proved as a consequence of the
boundary regularity theory for area-minimizing currents and classical unique
continuation principles adapted to the minimal surface system.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [36] [Numerical tests of formulae for volume enclosed by flux surfaces of integrable magnetic fields](https://arxiv.org/abs/2510.01957)
*David Martinez-del-Rio,Robert S. MacKay*

Main category: math.DS

TL;DR: Numerical tests of volume formulae for integrable 3D vector fields with various symmetries, including a new proposed case.


<details>
  <summary>Details</summary>
Motivation: To efficiently compute volumes enclosed between flux surfaces for integrable 3D vector fields with different symmetry properties.

Method: Numerical testing of volume formulae, with proposal and testing of a new case.

Result: Volume formulae were tested numerically for various symmetry cases.

Conclusion: The study presents numerical validation of volume computation methods for integrable 3D vector fields, including a newly proposed case.

Abstract: Numerical tests of volume formulae are presented to compute efficiently the
volume enclosed between flux surfaces for integrable 3D vector fields with
various degrees of symmetry. In the process, a new case is proposed and tested.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [37] [A Novel Algorithm for Representing Positive Semi-Definite Polynomials as Sums of Squares with Rational Coefficients](https://arxiv.org/abs/2510.01568)
*Zhenbing Zeng,Yong Huang,Lu Yang,Yongsheng Rao*

Main category: cs.SC

TL;DR: A novel algorithm for constructing sum-of-squares decompositions with rational coefficients for positive semi-definite polynomials, ensuring exact arithmetic in formal verification and symbolic computation.


<details>
  <summary>Details</summary>
Motivation: Existing methods typically yield SOS decompositions with floating-point coefficients, which are problematic in applications requiring exact arithmetic like formal verification and symbolic computation.

Method: A stepwise reduction technique that transforms polynomials into sum of ladder-like squares while preserving rationality of coefficients.

Result: Experimental results demonstrate the effectiveness of the method compared to existing numerical approaches.

Conclusion: The algorithm successfully constructs SOS decompositions with rational coefficients, providing exact solutions for applications requiring precise arithmetic.

Abstract: This paper presents a novel algorithm for constructing a sum-of-squares (SOS)
decomposition for positive semi-definite polynomials with rational
coefficients. Unlike previous methods that typically yield SOS decompositions
with floating-point coefficients, our approach ensures that all coefficients in
the decomposition remain rational. This is particularly useful in formal
verification and symbolic computation, where exact arithmetic is required. We
introduce a stepwise reduction technique that transforms a given polynomial
into a sum of ladder-like squares while preserving rationality. Experimental
results demonstrate the effectiveness of our method compared to existing
numerical approaches. This artical is an extension of the following Chinnese
paper: HUANG Yong , ZENG Zhenbing , YANG Lu , RAO Yongsheng. An Algorithm to
Represent Positive Semi-Definite Polynomials to Sum of Lader-Like Squares of
Polynomials with Rational Coefficients (in Chinese). Journal of Systems Science
and Mathematical Sciences, 2024, 44(5): 1241-1271
https://doi.org/10.12341/jssms23584CM

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [38] [Variational approach to open quantum systems with long-range competing interactions](https://arxiv.org/abs/2510.01543)
*Dawid A. Hryniuk,Marzena H. Szymańska*

Main category: quant-ph

TL;DR: The paper introduces an efficient computational method combining matrix product operators and time-dependent variational Monte Carlo to simulate dissipative quantum lattices with long-range interactions, enabling study of non-equilibrium dynamics in systems up to 200 sites.


<details>
  <summary>Details</summary>
Motivation: There is a need for accurate computational methods to simulate open quantum many-body systems with complex long-range interactions, which are experimentally controllable but computationally challenging.

Method: Combination of matrix product operators and time-dependent variational Monte Carlo for simulating dissipative quantum lattices in 1D and 2D with competing algebraically-decaying interactions.

Result: Successfully simulated non-equilibrium dynamics and steady states of spin-1/2 lattices with up to 200 sites, revealing emergence of spatially-modulated magnetic order far from equilibrium.

Conclusion: This approach provides promising prospects for understanding complex non-equilibrium properties of various experimentally-realizable quantum systems with long-range interactions, including Rydberg atoms, ultracold dipolar molecules, and trapped ions.

Abstract: Competition between short- and long-range interactions underpins many
emergent phenomena in nature. Despite rapid progress in their experimental
control, computational methods capable of accurately simulating open quantum
many-body systems with complex long-ranged interactions at scale remain scarce.
Here, we address this limitation by introducing an efficient and scalable
approach to dissipative quantum lattices in one and two dimensions, combining
matrix product operators and time-dependent variational Monte Carlo. We
showcase the versatility, effectiveness, and unique methodological advantages
of our algorithm by simulating the non-equilibrium dynamics and steady states
of spin-$\frac{1}{2}$ lattices with competing algebraically-decaying
interactions for as many as $N=200$ sites, revealing the emergence of
spatially-modulated magnetic order far from equilibrium. This approach offers
promising prospects for advancing our understanding of the complex
non-equilibrium properties of a diverse variety of experimentally-realizable
quantum systems with long-ranged interactions, including Rydberg atoms,
ultracold dipolar molecules, and trapped ions.

</details>


### [39] [Hybrid Quantum-Classical Walks for Graph Representation Learning in Community Detection](https://arxiv.org/abs/2510.01918)
*Adrián Marın,Mauricio Soto-Gomez,Giorgio Valentini,Elena Casiraghi,Carlos Cano,Daniel Manzano*

Main category: quant-ph

TL;DR: A quantum-inspired algorithm using hybrid Quantum-Classical Walks for Graph Representation Learning that effectively captures complex graph relationships and adapts to various topologies.


<details>
  <summary>Details</summary>
Motivation: Traditional GRL methods struggle with complex graphs exhibiting non-trivial structural properties like power-law distributions and hierarchical structures, requiring more sophisticated approaches to capture intricate relationships.

Method: Hybrid Quantum-Classical Walks that combine quantum and classical dynamics, allowing simultaneous exploration of both local and far-reaching connections within graphs.

Result: Preliminary results for network community detection show the hybrid dynamic enables effective adaptation to complex graph topologies.

Conclusion: The approach offers a robust and versatile solution for GRL tasks by overcoming limitations of traditional methods through quantum-inspired hybrid dynamics.

Abstract: Graph Representation Learning (GRL) has emerged as a cornerstone technique
for analysing complex, networked data across diverse domains, including
biological systems, social networks, and data analysis. Traditional GRL methods
often struggle to capture intricate relationships within complex graphs,
particularly those exhibiting non-trivial structural properties such as
power-law distributions or hierarchical structures. This paper introduces a
novel quantum-inspired algorithm for GRL, utilizing hybrid Quantum-Classical
Walks to overcome these limitations. Our approach combines the benefits of both
quantum and classical dynamics, allowing the walker to simultaneously explore
both highly local and far-reaching connections within the graph. Preliminary
results for a case study in network community detection shows that this hybrid
dynamic enables the algorithm to adapt effectively to complex graph topologies,
offering a robust and versatile solution for GRL tasks.

</details>


### [40] [Improving neural network performance for solving quantum sign structure](https://arxiv.org/abs/2510.02051)
*Xiaowei Ou,Tianshu Huang,Vidvuds Ozolins*

Main category: quant-ph

TL;DR: Modified stochastic reconfiguration method with different imaginary time steps for amplitude and phase enables simultaneous training of neural quantum states without prior sign structure knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing neural quantum state approaches for non-stoquastic Hamiltonians often require a priori knowledge of sign structure or separate pre-training of phase networks, which limits their applicability.

Method: Modified stochastic reconfiguration using different imaginary time steps - larger step for phase optimization and smaller step for amplitude optimization - to enable simultaneous training of phase and amplitude neural networks.

Result: The method successfully demonstrated its efficacy on the Heisenberg J_1-J_2 model, showing it can handle complex sign structures without prior knowledge.

Conclusion: The proposed approach provides an effective way to simultaneously train phase and amplitude networks for neural quantum states, eliminating the need for separate pre-training or prior sign structure knowledge.

Abstract: Neural quantum states have emerged as a widely used approach to the numerical
study of the ground states of non-stoquastic Hamiltonians. However, existing
approaches often rely on a priori knowledge of the sign structure or require a
separately pre-trained phase network. We introduce a modified stochastic
reconfiguration method that effectively uses differing imaginary time steps to
evolve the amplitude and phase. Using a larger time step for phase
optimization, this method enables a simultaneous and efficient training of
phase and amplitude neural networks. The efficacy of our method is demonstrated
on the Heisenberg J_1-J_2 model.

</details>


### [41] [Quantum advantages in ground state preparation, combinatorial optimization, and quantum state preparation](https://arxiv.org/abs/2510.01563)
*Taehee Ko,Sungbin Lim*

Main category: quant-ph

TL;DR: Ground states of quantum Hamiltonians with inverse-polynomial gaps can be prepared with polynomial-depth circuits using Pauli rotations, and any quantum state can be approximated with constant/polynomial Pauli rotations for large qubit systems.


<details>
  <summary>Details</summary>
Motivation: To demonstrate efficient quantum state preparation methods that reveal exponential quantum advantages in applications like ground state preparation, combinatorial optimization, and quantum state preparation.

Method: Using polynomial-depth circuits composed of polynomial number of Pauli rotations without ancilla qubits for ground state preparation, and extending to constant/polynomial Pauli rotations for approximating any quantum state in large systems.

Result: Ground states can be prepared to inverse-polynomial precision with polynomial circuit depth, and any quantum state can be approximated with constant/polynomial Pauli rotations to constant/inverse-polynomial precision for sufficiently large qubit numbers.

Conclusion: The findings reveal exponential quantum advantages in key applications including ground state preparation, combinatorial optimization, and quantum state preparation.

Abstract: We show that for any quantum Hamiltonian with an inverse-polynomial gap, the
ground state can be prepared in a polynomial circuit depth to
inverse-polynomial precision, if the system size is sufficiently large. The
resulting circuit is composed of a polynomial number of Pauli rotations without
ancilla qubit. Extending this result, we prove that for sufficiently large
qubit number, any quantum state can be approximately prepared with a constant
(polynomial) number of Pauli rotations to constant (inverse-polynomial)
precision. Our theoretical findings reveal exponential quantum advantages in
the prominent applications: ground state preparation, combinatorial
optimization, and quantum state preparation.

</details>


### [42] [A quantum analogue of convex optimization](https://arxiv.org/abs/2510.02151)
*Eunou Lee*

Main category: quant-ph

TL;DR: Quantum algorithm for convex optimization by computing minimum eigenvalues of Schrödinger operators with convex potentials, achieving polynomial-time efficiency.


<details>
  <summary>Details</summary>
Motivation: Extend convex optimization theory to quantum computing domain by solving minimum eigenvalue problems for Schrödinger operators with convex potentials.

Method: Fundamental Gap Algorithm (FGA) using adiabatic evolution of ground state with novel low-energy space analysis techniques.

Result: Efficient quantum algorithm computes minimum eigenvalue up to error ε in polynomial time in n, 1/ε, and potential parameters.

Conclusion: FGA provides first polynomial-time quantum algorithm for finding lowest frequency of n-dimensional convex drums via Dirichlet Laplacian eigenvalues.

Abstract: Convex optimization is the powerhouse behind the theory and practice of
optimization. We introduce a quantum analogue of unconstrained convex
optimization: computing the minimum eigenvalue of a Schr\"odinger operator $h =
-\Delta + V $ with convex potential $V:\mathbb R^n \rightarrow \mathbb R_{\ge
0}$ such that $V(x)\rightarrow\infty $ as $\|x\|\rightarrow\infty$. For this
problem, we present an efficient quantum algorithm, called the Fundamental Gap
Algorithm (FGA), that computes the minimum eigenvalue of $h$ up to error
$\epsilon$ in polynomial time in $n$, $1/\epsilon$, and parameters that depend
on $V$. Adiabatic evolution of the ground state is used as a key subroutine,
which we analyze with novel techniques that allow us to focus on the low-energy
space. We apply the FGA to give the first known polynomial-time algorithm for
finding the lowest frequency of an $n$-dimensional convex drum, or
mathematically, the minimum eigenvalue of the Dirichlet Laplacian on an
$n$-dimensional region that is defined by $m$ linear constraints in polynomial
time in $n$, $m$, $1/\epsilon$ and the radius $R$ of a ball encompassing the
region.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [43] [Improving Runtime Performance of Tensor Computations using Rust From Python](https://arxiv.org/abs/2510.01495)
*Kimmie Harding,Daniel M. Dunlavy*

Main category: cs.MS

TL;DR: This paper investigates improving runtime performance of tensor computational kernels in Python Tensor Toolbox by implementing them in Rust and calling from Python, showing consistent performance improvements over Python alone, Numba JIT compiler, and NumPy.


<details>
  <summary>Details</summary>
Motivation: To improve runtime performance of key computational kernels in the Python Tensor Toolbox (pyttb) for analyzing tensor data across various applications, leveraging Rust's compiled language advantages through Python extension modules.

Method: Implement key tensor kernels of increasing complexity in Rust and call them from Python via extension modules using the Python C API. Compare performance against Python alone, Numba JIT compiler for loop-based kernels, and NumPy implementations.

Result: Demonstrated consistent improvements in runtime performance when using Rust from Python compared to: 1) Python alone, 2) Python with Numba JIT compiler, and 3) NumPy implementations, across various tensor sizes and kernel complexities.

Conclusion: Using Rust from Python via extension modules provides significant runtime performance improvements for tensor computational kernels, making it a viable approach for optimizing performance-critical tensor operations in scientific computing applications.

Abstract: In this work, we investigate improving the runtime performance of key
computational kernels in the Python Tensor Toolbox (pyttb), a package for
analyzing tensor data across a wide variety of applications. Recent runtime
performance improvements have been demonstrated using Rust, a compiled
language, from Python via extension modules leveraging the Python C API --
e.g., web applications, data parsing, data validation, etc. Using this same
approach, we study the runtime performance of key tensor kernels of increasing
complexity, from simple kernels involving sums of products over data accessed
through single and nested loops to more advanced tensor multiplication kernels
that are key in low-rank tensor decomposition and tensor regression algorithms.
In numerical experiments involving synthetically generated tensor data of
various sizes and these tensor kernels, we demonstrate consistent improvements
in runtime performance when using Rust from Python over 1) using Python alone,
2) using Python and the Numba just-in-time Python compiler (for loop-based
kernels), and 3) using the NumPy Python package for scientific computing (for
pyttb kernels).

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [44] [The centered maximal operator removes the non-concave Cantor part from the gradient](https://arxiv.org/abs/2510.01936)
*Panu Lahti,Julian Weigt*

Main category: math.CA

TL;DR: The paper studies regularity properties of the centered Hardy-Littlewood maximal function for functions of bounded variation in R^d, showing improved regularity conditions when the variation measure has specific blow-up properties.


<details>
  <summary>Details</summary>
Motivation: To understand when the Hardy-Littlewood maximal function of a BV function exhibits improved regularity beyond BV, particularly upgrading to Sobolev regularity under certain conditions on the variation measure.

Method: Analyzing the behavior of the maximal function at points where the function has non-concave blow-ups, and studying the relationship between the variation measure's structure (jump part, Cantor part) and the regularity of the maximal function.

Result: Shows that at |D^c f|-a.e. point where f has a non-concave blow-up, M f(x) > f*(x). Furthermore, if the variation measure has no jump part and its Cantor part has non-concave blow-ups, then BV regularity of M f can be upgraded to Sobolev regularity.

Conclusion: The structure of the variation measure, particularly the absence of jump parts and presence of non-concave blow-ups in the Cantor part, plays a crucial role in determining when the Hardy-Littlewood maximal function exhibits Sobolev regularity rather than just BV regularity.

Abstract: We study regularity of the centered Hardy--Littlewood maximal function $M f$
of a function $f$ of bounded variation in $\mathbb R^d$, $d\in \mathbb N$. In
particular, we show that at $|D^c f|$-a.e. point $x$ where $f$ has a
non-concave blow-up, it holds that $M f(x)>f^*(x)$. We further deduce from this
that if the variation measure of $f$ has no jump part and its Cantor part has
non-concave blow-ups, then BV regularity of $M f$ can be upgraded to Sobolev
regularity.

</details>


<div id='nlin.SI'></div>

# nlin.SI [[Back]](#toc)

### [45] [The noncommutative KP hierarchy and its solution via descent algebra](https://arxiv.org/abs/2510.01352)
*Gordon Blower,Simon J. A. Malham*

Main category: nlin.SI

TL;DR: The paper presents a complete solution to the noncommutative KP hierarchy using direct linearisation via the GLM equation and two approaches: Sato-Wilson dressing transformation and Poppe's method with semi-additive scattering data.


<details>
  <summary>Details</summary>
Motivation: To provide a complete solution to the noncommutative Kadomtsev-Petviashvili (KP) hierarchy, which is an important integrable system in mathematical physics.

Method: Uses direct linearisation involving the Gelfand-Levitan-Marchenko (GLM) equation with two approaches: (1) standard Sato-Wilson dressing transformation, (2) Poppe's method assuming semi-additive scattering data and using augmented pre-Poppe algebra in nonassociative descent algebra.

Result: Shows that the solution to the GLM equation coincides with the solution to the noncommutative KP hierarchy, establishing the complete solution to the hierarchy.

Conclusion: The second approach (Poppe's method) is advantageous as it is constructive, explicit, reveals underlying combinatorial structures, and clarifies the solution mechanisms, with final results residing in the natural associative subalgebra.

Abstract: We give the solution to the complete noncommutative Kadomtsev--Petviashvili
(KP) hierarchy. We achieve this via direct linearisation which involves the
Gelfand--Levitan--Marchenko (GLM) equation. This is a linear integral equation
in which the scattering data satisfies the linearised KP hierarchy. The
solution to the GLM equation is then shown to coincide with the solution to the
noncommutative KP hierarchy. We achieve this using two approaches. In the first
approach we use the standard Sato-Wilson dressing transformation. In the second
approach, which was pioneered by Poppe, we assume the scattering data is
semi-additive and by direct substitution, we show that the solution to the GLM
equation satisfies the infinite set of field equations representing the
noncommutative KP hierarchy. This approach relies on the augmented pre-Poppe
algebra. This is a representative algebra that underlies the field equations
representing the hierarchy. It is nonassociative and isomorphic to a descent
algebra equipped with a grafting product. While we perform computations in the
nonassociative descent algebra, the final result which establishes the solution
to the complete hierarchy, resides in the natural associative subalgebra. The
advantages of this second approach are that it is constructive, explicit,
highlights the underlying combinatorial structures within the hierarchy, and
reveals the mechanisms underlying the solution procedure.

</details>


### [46] [Non-commutative multiple bi-orthogonal polynomials: formal approach and integrability](https://arxiv.org/abs/2510.02207)
*Adam Doliwa*

Main category: nlin.SI

TL;DR: The paper introduces non-commutative multiple bi-orthogonal polynomial systems that generalize multiple orthogonality, matrix orthogonal polynomials, and bi-orthogonality, showing connections to integrable systems and discrete-time Toda equations.


<details>
  <summary>Details</summary>
Motivation: To generalize concepts of multiple orthogonality, matrix orthogonal polynomials, and bi-orthogonality within a non-commutative framework, and explore their connections to integrable systems theory.

Method: Define non-commutative multiple bi-orthogonal polynomial systems, present quasideterminantal expressions using formal bi-moments, study normalization functions satisfying non-commutative Hirota equations, and specialize to non-commutative multiple orthogonal polynomials.

Result: The polynomial systems satisfy non-commutative Hirota equations and provide solutions to corresponding linear systems, establishing them as part of integrable systems theory. Specialization leads to non-commutative versions of multidimensional discrete-time Toda equations.

Conclusion: The introduced non-commutative multiple bi-orthogonal polynomial systems form a significant extension of classical orthogonal polynomial theories and provide new connections to integrable systems, particularly through non-commutative versions of discrete-time Toda equations.

Abstract: We define the non-commutative multiple bi-orthogonal polynomial systems,
which simultaneously generalize the concepts of multiple orthogonality, matrix
orthogonal polynomials and of the bi-orthogonality. We present
quasideterminantal expressions for such polynomial systems in terms of formal
bi-moments. The normalization functions for such monic polynomials satisfy the
non-commutative Hirota equations, while the polynomials provide solution of the
corresponding linear system. This shows, in particular, that our polynomial
systems form a part of the theory of integrable systems. We study also a
specialization of the problem to non-commutative multiple orthogonal
polynomials, what results in the corresponding Hankel-type quasideterminantal
expressions in terms of the moments. Moreover, such a reduction allows to
introduce in a standard way the discrete-time variable and gives rise to an
integrable system which is non-commutative version of the multidimensional
discrete-time Toda equations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems](https://arxiv.org/abs/2510.01396)
*Wasut Pornpatcharapong*

Main category: cs.LG

TL;DR: A neural network surrogate framework that learns collective variables (CVs) from Cartesian coordinates and uses automatic differentiation for Jacobians, eliminating the need for analytical forms and enabling complex CVs in free energy reconstruction.


<details>
  <summary>Details</summary>
Motivation: Traditional free energy reconstruction methods like Gaussian Process Regression require analytical Jacobians of CVs, which is a bottleneck for using complex or machine-learned CVs.

Method: Neural network surrogate framework that learns CVs directly from Cartesian coordinates and uses automatic differentiation to compute Jacobians, bypassing the need for analytical forms.

Result: Achieved high accuracy on MgCl2 ion-pairing system for both simple distance CV and complex coordination-number CV. Jacobian errors followed near-Gaussian distribution, making them suitable for GPR pipelines.

Conclusion: This framework enables gradient-based free energy methods to incorporate complex and machine-learned CVs, broadening applications in biochemistry and materials simulations.

Abstract: Free energy reconstruction methods such as Gaussian Process Regression (GPR)
require Jacobians of the collective variables (CVs), a bottleneck that
restricts the use of complex or machine-learned CVs. We introduce a neural
network surrogate framework that learns CVs directly from Cartesian coordinates
and uses automatic differentiation to provide Jacobians, bypassing analytical
forms. On an MgCl2 ion-pairing system, our method achieved high accuracy for
both a simple distance CV and a complex coordination-number CV. Moreover,
Jacobian errors also followed a near-Gaussian distribution, making them
suitable for GPR pipelines. This framework enables gradient-based free energy
methods to incorporate complex and machine-learned CVs, broadening the scope of
biochemistry and materials simulations.

</details>


### [48] [Learning Regularization Functionals for Inverse Problems: A Comparative Study](https://arxiv.org/abs/2510.01755)
*Johannes Hertrich,Hok Shing Wong,Alexander Denker,Stanislas Ducotterd,Zhenghan Fang,Markus Haltmeier,Željko Kereta,Erich Kobler,Oscar Leong,Mohammad Sadegh Salehi,Carola-Bibiane Schönlieb,Johannes Schwab,Zakhar Shumaylov,Jeremias Sulam,German Shâma Wache,Martin Zach,Yasi Zhang,Matthias J. Ehrhardt,Sebastian Neumayer*

Main category: cs.LG

TL;DR: The paper presents a unified framework for comparing learned regularization methods in imaging inverse problems, addressing implementation inconsistencies and providing systematic analysis.


<details>
  <summary>Details</summary>
Motivation: To enable fair comparison of different learned regularization methods that have non-modular implementations and different architectural designs/training strategies.

Method: Collect and unify available code into a common framework, then systematically compare approaches through this unified view.

Result: The unified framework allows systematic comparison of methods, highlighting their strengths and limitations, and provides practical guidelines.

Conclusion: The unified framework offers valuable insights into the future potential of learned regularization methods and facilitates better comparison across different approaches.

Abstract: In recent years, a variety of learned regularization frameworks for solving
inverse problems in imaging have emerged. These offer flexible modeling
together with mathematical insights. The proposed methods differ in their
architectural design and training strategies, making direct comparison
challenging due to non-modular implementations. We address this gap by
collecting and unifying the available code into a common framework. This
unified view allows us to systematically compare the approaches and highlight
their strengths and limitations, providing valuable insights into their future
potential. We also provide concise descriptions of each method, complemented by
practical guidelines.

</details>


### [49] [Neural non-canonical Hamiltonian dynamics for long-time simulations](https://arxiv.org/abs/2510.01788)
*Clémentine Courtès,Emmanuel Franck,Michael Kraus,Laurent Navoret,Léopold Trémant*

Main category: cs.LG

TL;DR: Learning non-canonical Hamiltonian dynamics from data while preserving structure in both model and numerical schemes, addressing numerical instability issues through new training strategies.


<details>
  <summary>Details</summary>
Motivation: Previous methods focused on either structure-preserving models or numerical schemes separately, but combining both caused numerical instability due to gauge dependency, making long-term simulations impossible.

Method: Proposed two training strategies: directly learning the vector field, or learning time-discrete dynamics through the numerical scheme.

Result: Methods were assessed on complex physical dynamics like guiding center from gyrokinetic plasma physics.

Conclusion: The proposed strategies address the numerical instability problem when combining structure-preserving models and numerical schemes for learning non-canonical Hamiltonian dynamics.

Abstract: This work focuses on learning non-canonical Hamiltonian dynamics from data,
where long-term predictions require the preservation of structure both in the
learned model and in numerical schemes. Previous research focused on either
facet, respectively with a potential-based architecture and with degenerate
variational integrators, but new issues arise when combining both. In
experiments, the learnt model is sometimes numerically unstable due to the
gauge dependency of the scheme, rendering long-time simulations impossible. In
this paper, we identify this problem and propose two different training
strategies to address it, either by directly learning the vector field or by
learning a time-discrete dynamics through the scheme. Several numerical test
cases assess the ability of the methods to learn complex physical dynamics,
like the guiding center from gyrokinetic plasma physics.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [50] [Relativistic Jets and Winds in Radio-Identified Supermassive Black Hole Binary Candidates](https://arxiv.org/abs/2510.02301)
*Andrew G. Sullivan,Roger D. Blandford,Anna Synani,Philipe V. de la Parra,Noémie Globus,Mitchell C. Begelman,Anthony C. S. Readhead*

Main category: astro-ph.HE

TL;DR: Proposes a jet model to explain periodic flux variations in two blazar candidates for supermassive black hole binaries, predicting observable features that can be tested with future observations.


<details>
  <summary>Details</summary>
Motivation: To explain the periodic flux density variations observed in two potential supermassive black hole binary candidates (PKS 2131-021 and PKS J0805-0111) and provide a generalizable model for such systems.

Method: Developed a jet model where a mildly relativistic wind creates an outward-moving helical channel for ultra-relativistic jet propagation, with observed flux variation primarily due to aberration effects.

Result: The model successfully reproduces the main observable features of both sources, including the delayed variation at lower frequencies, and can be applied to other similar sources.

Conclusion: The results motivate future numerical simulations of jetted SMBHB systems and have implications for understanding blazar jet fueling, structure, and evolution, with testable predictions for radio polarization, direct imaging, and emission line variation.

Abstract: Supermassive black hole binary systems (SMBHBs) are thought to emit the
recently discovered nHz gravitational wave background; however, not a single
individual nHz source has been confirmed to date. Long-term radio-monitoring at
the Owens Valley Radio Observatory has revealed two potential SMBHB candidates:
blazars PKS 2131-021 and PKS J0805-0111. These sources show periodic flux
density variations across the electromagnetic spectrum, signaling the presence
of a good clock. To explain the emission, we propose a generalizable jet model,
where a mildly relativistic wind creates an outward-moving helical channel,
along which the ultra-relativistic jet propagates. The observed flux variation
from the jet is mostly due to aberration. The emission at lower frequency
arises at larger radius and its variation is consequently delayed, as observed.
Our model reproduces the main observable features of both sources and can be
applied to other sources as they are discovered. We make predictions for radio
polarization, direct imaging, and emission line variation, which can be tested
with forthcoming observations. Our results motivate future numerical
simulations of jetted SMBHB systems and have implications for the fueling,
structure, and evolution of blazar jets.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [51] [Exponential convergence of a distributed divide-and-conquer algorithm for constrained convex optimization on networks](https://arxiv.org/abs/2510.01511)
*Nazar Emirov,Guohui Song,Qiyu Sun*

Main category: math.OC

TL;DR: A distributed divide-and-conquer algorithm for constrained convex optimization over networks with exponential convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: To solve large-scale constrained convex optimization problems over networks where the global objective is the sum of local objectives, enabling fully distributed computation with only local coordination.

Method: Divide-and-conquer approach where each iteration solves local subproblems around selected fusion centers and coordinates only with neighboring fusion centers. Works under smoothness, strong convexity, and locality assumptions.

Result: Established exponential convergence of DAC iterations with explicit bounds for both exact and inexact local solvers. Numerical experiments on L2 distance, quadratic, and entropy losses confirm theory and demonstrate scalability.

Conclusion: The DAC algorithm provides an effective and scalable distributed solution for network optimization problems with proven exponential convergence rates.

Abstract: We propose a divide-and-conquer (DAC) algorithm for constrained convex
optimization over networks, where the global objective is the sum of local
objectives attached to individual agents. The algorithm is fully distributed:
each iteration solves local subproblems around selected fusion centers and
coordinates only with neighboring fusion centers. Under standard assumptions of
smoothness, strong convexity, and locality on the objective function, together
with polynomial growth conditions on the underlying graph, we establish
exponential convergence of the DAC iterations and derive explicit bounds for
both exact and inexact local solvers. Numerical experiments on three
representative losses ($L_2$ distance, quadratic, and entropy) confirm the
theory and demonstrate scalability and effectiveness.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [52] [On Lieb-Thirring inequalities for multidimensional Schrödinger operators with complex potentials](https://arxiv.org/abs/2510.02192)
*Sabine Bögli,Sukrid Petpradittha,František Štampach*

Main category: math.SP

TL;DR: The paper provides a counter-example to a proposed generalization of the Lieb-Thirring inequality for complex-valued potentials in higher dimensions, solving an open problem.


<details>
  <summary>Details</summary>
Motivation: To address the open problem by Demuth, Hansmann, and Katriel regarding the generalization of Lieb-Thirring inequality to complex-valued potentials in higher dimensions, building on previous counter-example work in one dimension.

Method: Construction of a counter-example that generalizes the one-dimensional case to higher dimensions.

Result: Successfully constructed a counter-example showing that the proposed generalization of Lieb-Thirring inequality for complex-valued potentials does not hold in higher dimensions.

Conclusion: The counter-example definitively resolves the open problem, demonstrating that the Lieb-Thirring inequality cannot be generalized as proposed for complex-valued potentials in higher dimensions.

Abstract: We solve the open problem by Demuth, Hansmann, and Katriel announced in
[Integr. Equ. Oper. Theory 75 (2013), 1-5] by a counter-example construction.
The problem concerns a possible generalisation of the Lieb-Thirring inequality
for Schr\"odinger operators in to the case of complex-valued potentials. A
counter-example has already been found for the one-dimensional case by the
first and third authors in [J. Spectr. Theory 11 (2021), 1391-1413]. Here we
generalise the counter-example to higher dimensions.

</details>


### [53] [Optimal Lieb-Thirring type inequalities for Schrödinger and Jacobi operators with complex potentials](https://arxiv.org/abs/2510.02288)
*Sabine Bögli,Sukrid Petpradittha*

Main category: math.SP

TL;DR: Optimal Lieb-Thirring inequalities for Schrödinger and Jacobi operators with complex potentials, bounding eigenvalue power sums by L^p norms with specific weighting functions.


<details>
  <summary>Details</summary>
Motivation: To extend Lieb-Thirring inequalities from self-adjoint to complex potential cases, requiring new weighting schemes due to different eigenvalue behavior near essential spectrum boundaries.

Method: Prove optimal bounds for integrable weight functions and establish divergence estimates for non-integrable cases, comparing with semiclassical methods.

Result: Achieved optimal Lieb-Thirring type inequalities with weighted terms depending on eigenvalue distance to essential spectrum, showing improved divergence rates over real potential cases.

Conclusion: Complex potentials require fundamentally different weighting approaches in Lieb-Thirring inequalities, with non-integrable weights exhibiting enhanced divergence behavior compared to semiclassical predictions.

Abstract: We prove optimal Lieb-Thirring type inequalities for Schr\"odinger and Jacobi
operators with complex potentials. Our results bound eigenvalue power sums
(Riesz means) by the $L^p$ norm of the potential, where in contrast to the
self-adjoint case, each term needs to be weighted by a function of the ratio of
the distance of the eigenvalue to the essential spectrum and the distance to
the endpoint(s) thereof. Our Lieb-Thirring type bounds only hold for integrable
weight functions. To prove optimality, we establish divergence estimates for
non-integrable weight functions. The divergence rates exhibit a logarithmic or
even polynomial gain compared to semiclassical methods (Weyl asymptotics) for
real potentials.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [54] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: SPUS is a compact foundation model using residual U-Net architecture for solving diverse PDEs, achieving SOTA performance with fewer parameters and minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing PDE foundation models use large transformer architectures with high computational overhead, while U-Net architectures remain underexplored despite their potential efficiency.

Method: Lightweight residual U-Net architecture with auto-regressive pretraining strategy that mimics numerical solver behavior, pretrained on diverse fluid dynamics PDEs.

Result: Achieves state-of-the-art generalization on 6 challenging unseen PDEs while requiring significantly fewer parameters and minimal fine-tuning data.

Conclusion: SPUS demonstrates potential as a highly parameter-efficient foundation model for solving diverse PDE systems.

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>
