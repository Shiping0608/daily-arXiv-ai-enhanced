<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 9]
- [math.AP](#math.AP) [Total: 16]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [cs.CV](#cs.CV) [Total: 1]
- [math.SP](#math.SP) [Total: 2]
- [cs.SC](#cs.SC) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [quant-ph](#quant-ph) [Total: 5]
- [math.DS](#math.DS) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [nlin.SI](#nlin.SI) [Total: 2]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [math.CA](#math.CA) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Deep Learning Accelerated Algebraic Multigrid Methods for Polytopal Discretizations of Second-Order Differential Problems](https://arxiv.org/abs/2510.01442)
*Paola F. Antonietti,Matteo Caldana,Lorenzo Gentile,Marco Verani*

Main category: math.NA

TL;DR: A deep learning approach to automatically tune AMG solver parameters (strong threshold and smoother choice) for polytopal discretizations, treating sparse matrices as images and using pooling to extract features, achieving up to 27% speedup.


<details>
  <summary>Details</summary>
Motivation: AMG methods are sensitive to parameter choices like strong threshold and smoother selection, especially for polytopal discretizations (PolyDG/VEM) where mesh variability creates different sparsity patterns, making manual tuning difficult.

Method: Treat sparse matrices from polytopal discretizations as grayscale images, apply pooling to extract compact features, and use neural networks to automatically select optimal AMG parameters (strong threshold and smoother choice).

Result: The approach generalizes well across 2D/3D problems with heterogeneous coefficients and polygonal/polyhedral meshes, reducing AMG solver time by up to 27% with minimal code changes.

Conclusion: Deep learning can effectively automate AMG parameter tuning for polytopal discretizations, significantly improving solver performance while maintaining compatibility with existing PolyDG and VEM codes.

Abstract: Algebraic Multigrid (AMG) methods are state-of-the-art algebraic solvers for
partial differential equations. Still, their efficiency depends heavily on the
choice of suitable parameters and/or ingredients. Paradigmatic examples include
the so-called strong threshold parameter $\theta$, which controls the algebraic
coarse-grid hierarchy, as well as the smoother, i.e., the relaxation methods
used on the fine grid to damp out high-frequency errors. In AMG, since the
coarse grids are constructed algebraically (without geometric intuition), the
smoother's performance is even more critical. For the linear systems stemming
from polytopal discretizations, such as Polytopal Discontinuous Galerkin
(PolyDG) and Virtual Element Methods (VEM), AMG sensitivity to such choices is
even more critical due to the significant variability of the underlying meshes,
which results in algebraic systems with different sparsity patterns. We propose
a novel deep learning approach that automatically tunes the strong threshold
parameter, as well as the smoother choice in AMG solvers, for linear systems of
equations arising from polytopal discretizations, thereby maximizing AMG
performance. We interpret the sparse matrix resulting from polytopal
discretization as a grayscale image, and by applying pooling, our neural
network extracts compact features that preserve the necessary information at a
low computational cost. We test various differential problems in both two- and
three-dimensional settings, with heterogeneous coefficients and
polygonal/polyhedral meshes, and demonstrate that the proposed approach
generalizes well. In practice, we demonstrate that we can reduce AMG solver
time by up to $27\%$ with minimal changes to existing PolyDG and VEM codes.

</details>


### [2] [Data selection: at the interface of PDE-based inverse problem and randomized linear algebra](https://arxiv.org/abs/2510.01567)
*Kathrin Hellmuth,Ruhui Jin,Qin Li,Stephen J. Wright*

Main category: math.NA

TL;DR: This review explores how randomized numerical linear algebra (RNLA) methods address the challenge of data selection in PDE-based inverse problems, focusing on their infinite-dimensional nature and probabilistic guarantees.


<details>
  <summary>Details</summary>
Motivation: PDE-based inverse problems face the unique challenge of infinite-dimensional parameter and design spaces, making data selection particularly difficult. Not all data are equally informative, and traditional methods struggle with this dimensionality.

Method: The paper surveys the adaptation and application of randomized numerical linear algebra (RNLA) strategies, which use probabilistic sampling with weighted random samples to preserve information with high probability.

Result: RNLA methods provide powerful tools for data selection in infinite-dimensional settings, offering guarantees that information is preserved with probability at least 1-p using N randomly selected, weighted samples.

Conclusion: Randomized numerical linear algebra offers effective solutions for the challenging problem of data selection in PDE-based inverse problems, successfully addressing their infinite-dimensional nature through probabilistic sampling approaches.

Abstract: All inverse problems rely on data to recover unknown parameters, yet not all
data are equally informative. This raises the central question of data
selection. A distinctive challenge in PDE-based inverse problems is their
inherently infinite-dimensional nature: both the parameter space and the design
space are infinite, which greatly complicates the selection process. Somewhat
unexpectedly, randomized numerical linear algebra (RNLA), originally developed
in very different contexts, has provided powerful tools for addressing this
challenge. These methods are inherently probabilistic, with guarantees
typically stating that information is preserved with probability at least 1-p
when using N randomly selected, weighted samples. Here, the notion of
information can take different mathematical forms depending on the setting. In
this review, we survey the problem of data selection in PDE-based inverse
problems, emphasize its unique infinite-dimensional aspects, and highlight how
RNLA strategies have been adapted and applied in this context.

</details>


### [3] [Instability of the Sherman-Morrison formula and stabilization by iterative refinement](https://arxiv.org/abs/2510.01696)
*Behnam Hashemi,Yuji Nakatsukasa*

Main category: math.NA

TL;DR: The Sherman-Morrison formula's numerical stability has been an open question despite its widespread use. This paper analyzes its backward stability, addresses an open problem about error bounds, and proposes iterative refinement to achieve backward stability while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: The Sherman-Morrison formula is widely used for solving rank-one perturbed linear systems due to its simplicity and efficiency, but its numerical stability properties have remained an open research question since 1944, with current research still investigating this topic.

Method: The paper analyzes the backward stability of the Sherman-Morrison formula, demonstrates instability in common scenarios, and incorporates fixed-precision iterative refinement into the SM framework while reusing previously computed decompositions.

Result: The authors prove that with iterative refinement under reasonable assumptions, the method achieves backward stability without sacrificing the efficiency of the SM formula. Empirical results show it eventually produces backward stable solutions in all numerical experiments.

Conclusion: While theoretical guarantees don't prove the SM formula with iterative refinement always outputs a backward stable solution, the authors conjecture it yields backward stability when both κ₂(A) and κ₂(A+uv^T) are bounded safely away from the unit roundoff's inverse.

Abstract: Owing to its simplicity and efficiency, the Sherman-Morrison (SM) formula has
seen widespread use across various scientific and engineering applications for
solving rank-one perturbed linear systems of the form $(A+uv^T)x = b$. Although
the formula dates back at least to 1944, its numerical stability properties
have remained an open question and continue to be a topic of current research.
We analyze the backward stability of the SM, demonstrate its instability in a
scenario increasingly common in scientific computing and address an open
question posed by Nick Higham on the proportionality of the backward error
bound to the condition number of $A$. We then incorporate fixed-precision
iterative refinement into the SM framework reusing the previously computed
decompositions and prove that, under reasonable assumptions, it achieves
backward stability without sacrificing the efficiency of the SM formula. While
our theory does not prove the SM formula with iterative refinement always
outputs a backward stable solution, empirically it is observed to eventually
produce a backward stable solution in all our numerical experiments. We
conjecture that with iterative refinement, the SM formula yields a backward
stable solution provided that $\kappa_2(A), \kappa_2(A+uv^T)$ are both bounded
safely away from $\epsilon_M^{-1}$, where $\epsilon_M$ is the unit roundoff.

</details>


### [4] [Efficient manifold evolution algorithm using adaptive B-Spline interpolation](https://arxiv.org/abs/2510.01790)
*Muhammad Ammad,Leevan Ling*

Main category: math.NA

TL;DR: Efficient Lagrangian approach using B-Splines for evolving point clouds on manifolds, providing alternative to RBF methods with geometric coefficient manipulation.


<details>
  <summary>Details</summary>
Motivation: To provide an alternative to conventional radial basis function (RBF) approaches for manifolds in higher dimensions, particularly for handling regions with significant point density fluctuations.

Method: Uses B-Spline basis functions for local interpolations, enabling approximation of geometric features like normal vectors and curvature. B-Spline coefficients carry geometric meanings and can be manipulated like points.

Result: Numerical results demonstrate convergence of geometric quantities and effectiveness of the approach. Successfully simulates curvature flows with coupled reaction-diffusion systems for pattern formation.

Conclusion: B-Spline approach enables seamless point cloud updates, eliminates frequent re-interpolation needs, and provides efficient alternative to RBF methods for manifold evolution.

Abstract: This paper explores an efficient Lagrangian approach for evolving point cloud
data on smooth manifolds. In this preliminary study, we focus on analyzing
plane curves, and our ultimate goal is to provide an alternative to the
conventional radial basis function (RBF) approach for manifolds in higher
dimensions. In particular, we use the B-Spline as the basis function for all
local interpolations. Just like RBF and other smooth basis functions, B-Splines
enable the approximation of geometric features such as normal vectors and
curvature. Once properly set up, the advantage of using B-Splines is that their
coefficients carry geometric meanings. This allows the coefficients to be
manipulated like points, facilitates rapid updates of the interpolant, and
eliminates the need for frequent re-interpolation. Consequently, the removal
and insertion of point cloud data become seamless processes, particularly
advantageous in regions experiencing significant fluctuations in point density.
The numerical results demonstrate the convergence of geometric quantities and
the effectiveness of our approach. Finally, we show simulations of curvature
flows whose speeds depend on the solutions of coupled reaction--diffusion
systems for pattern formation.

</details>


### [5] [Asymptotic preserving schemes for hyperbolic systems with relaxation](https://arxiv.org/abs/2510.01828)
*C Mahmoud,H Mathis*

Main category: math.NA

TL;DR: Two numerical schemes for hyperbolic systems with relaxation: one combines FORCE approach with unsplit strategy, the other uses an approximate Riemann solver with careful source term handling. Both are asymptotic preserving.


<details>
  <summary>Details</summary>
Motivation: To develop numerical methods that treat hyperbolic relaxation systems as a whole without separating convective and source terms, ensuring proper asymptotic behavior.

Method: First scheme: combines centered FORCE approach with unsplit strategy. Second scheme: approximate Riemann solver with careful source term approximation. Both designed to be asymptotic preserving.

Result: Both schemes are asymptotic preserving - their limit schemes remain consistent with equilibrium model as relaxation parameter approaches zero, without CFL restrictions.

Conclusion: The methods preserve invariant domains and admit discrete entropy inequality for specific models, providing robust numerical solutions for hyperbolic relaxation systems.

Abstract: This paper presents the construction of two numerical schemes for the
solution of hyperbolic systems with relaxation source terms. The methods are
built by considering the relaxation system as a whole, without separating the
resolution of the convective part from that of the source term. The first
scheme combines the centered FORCE approach of Toro and co-authors with the
unsplit strategy proposed by B{\'e}reux and Sainsaulieu. The second scheme
consists of an approximate Riemann solver which carefully handles the source
term approximation. The two schemes are built to be asymptotic preserving, in
the sense that their limit schemes are consistent with the equilibrium model as
the relaxation parameter tends to zero, without any CFL restriction. For
specific models, it is possible to prove that they preserve invariant domains
and admit a discrete entropy inequality.

</details>


### [6] [A nodally bound-preserving composite discontinuous Galerkin method on polytopic meshes](https://arxiv.org/abs/2510.02094)
*Abdolreza Amiri,Gabriel R. Barrenechea,Emmanuil H. Georgoulis,Tristan Pryer*

Main category: math.NA

TL;DR: A nodally bound-preserving Galerkin method for elliptic problems on polytopic meshes that enforces solution bounds at user-defined points using simplicial submeshes without adding extra degrees of freedom.


<details>
  <summary>Details</summary>
Motivation: To develop a method that combines the geometric flexibility of polytopic meshes with the accuracy of discontinuous Galerkin discretizations while rigorously guaranteeing bound preservation for numerical solutions.

Method: Uses interior penalty discontinuous Galerkin formulation on polytopic meshes with simplicial submeshes to enforce bound preservation at submesh nodes via nonlinear iteration, preserving accuracy without additional global degrees of freedom.

Result: The method maintains optimal convergence for smooth problems, demonstrates robustness with sharp gradients, and automatically reverts to standard DG when no bound violations occur. Existence and uniqueness of solutions are proven.

Conclusion: The composite method successfully combines polytopic mesh flexibility with DG accuracy and stability while ensuring rigorous bound preservation, with proven mathematical properties and demonstrated numerical performance.

Abstract: We introduce a nodally bound-preserving Galerkin method for second-order
elliptic problems on general polygonal/polyhedral, henceforth collectively
termed as \emph{polytopic}, meshes. Starting from an interior penalty
discontinuous Galerkin (DG) formulation posed on a polytopic mesh, the method
enforces preservation of \emph{a priori} prescribed upper and lower bounds for
the numerical solution at an arbitrary number of user-defined points
\emph{within} each polytopic element. This is achieved by employing a
simplicial submesh and enforcing bound preservation at the submesh nodes via a
nonlinear iteration. By construction, the submeshing procedure preserves the
order of accuracy of the DG method, \emph{without} introducing any additional
global numerical degrees of freedom compared to the baseline DG method,
thereby, falling into the category of composite finite element approaches. A
salient feature of the proposed method is that it automatically reverts to the
standard DG method on polytopic meshes when no prescribed bound violation
occurs. In particular, the choice of the discontinuity-penalisation parameter
is independent of the submesh granularity. The resulting composite method
combines the geometric flexibility of polytopic meshes with the accuracy and
stability of discontinuous Galerkin discretisations, while rigorously
guaranteeing bound preservation. The existence and uniqueness of the numerical
solution is proven. A priori error bounds, assuming sufficient regularity of
the exact solution are shown, employing a non-standard construction of discrete
nodally bound-preserving interpolant. Numerical experiments confirm optimal
convergence for smooth problems and demonstrate robustness in the presence of
sharp gradients, such as boundary and interior layers.

</details>


### [7] [Coarse scrambling for Sobol' and Niederreiter sequences](https://arxiv.org/abs/2510.02111)
*Kosuke Suzuki*

Main category: math.NA

TL;DR: Coarse scrambling is a new randomization method for digital sequences that permutes blocks of digits while preserving (0,e,d)-sequence properties, achieving O(n^{-3+ε}) variance decay with logarithmic dimension dependence O(log d).


<details>
  <summary>Details</summary>
Motivation: To address the curse of dimensionality affecting scrambled Sobol' sequences by developing a randomization method with better theoretical robustness in high dimensions.

Method: Permutes blocks of digits in mixed-radix representation while preserving the (0,e,d)-sequence property of underlying points.

Result: Achieves O(n^{-3+ε}) variance decay rate for smooth integrands, with maximal gain coefficient growing only O(log d) with dimension, providing theoretical robustness against curse of dimensionality.

Conclusion: Coarse scrambling is competitive for functions with low effective truncation dimension, while Owen's scrambling remains superior for integrands sensitive to low-dimensional projections.

Abstract: We introduce \emph{coarse scrambling}, a novel randomization for digital
sequences that permutes blocks of digits in a mixed-radix representation. This
construction is designed to preserve the powerful
$(0,\boldsymbol{e},d)$-sequence property of the underlying points. For
sufficiently smooth integrands, we prove that this method achieves the
canonical $O(n^{-3+\epsilon})$ variance decay rate, matching that of standard
Owen's scrambling. Crucially, we show that its maximal gain coefficient grows
only logarithmically with dimension, $O(\log d)$, thus providing theoretical
robustness against the curse of dimensionality affecting scrambled Sobol'
sequences. Numerical experiments validate these findings and illustrate a
practical trade-off: while Owen's scrambling is superior for integrands
sensitive to low-dimensional projections, coarse scrambling is competitive for
functions with low effective truncation dimension.

</details>


### [8] [Mixed-precision iterative refinement for low-rank Lyapunov equations](https://arxiv.org/abs/2510.02126)
*Peter Benner,Xiaobo Liu*

Main category: math.NA

TL;DR: A mixed-precision iterative refinement framework for solving low-rank Lyapunov matrix equations using reduced precisions like half precision to accelerate computation while maintaining solution quality.


<details>
  <summary>Details</summary>
Motivation: To accelerate the solution of low-rank Lyapunov matrix equations by using reduced precision computations without compromising solution quality, particularly for problems with condition numbers up to the inverse of the solver precision's unit roundoff.

Method: Developed a mixed-precision iterative refinement framework using the sign function Newton iteration as the solver, with rounding error analysis to derive sufficient conditions for attainable normwise residuals and optimal algorithmic parameter selection.

Result: The framework enables using reduced precisions (e.g., half precision) as the solver precision to accelerate Lyapunov equation solutions for condition numbers up to 1/u_s (where u_s is the unit roundoff of solver precision) while maintaining solution quality.

Conclusion: Mixed-precision iterative refinement with reduced precision solvers can effectively accelerate low-rank Lyapunov equation solutions without sacrificing accuracy, making it suitable for problems with condition numbers up to the inverse of the solver's unit roundoff.

Abstract: We develop a mixed-precision iterative refinement framework for solving
low-rank Lyapunov matrix equations $AX + XA^T + W =0$, where $W=LL^T$ or
$W=LSL^T$. Via rounding error analysis of the algorithms we derive sufficient
conditions for the attainable normwise residuals in different precision
settings and show how the algorithmic parameters should be chosen. Using the
sign function Newton iteration as the solver, we show that reduced precisions,
such as the half precision, can be used as the solver precision (with unit
roundoff $u_s$) to accelerate the solution of Lyapunov equations of condition
number up to $1/u_s$ without compromising its quality.

</details>


### [9] [A Fast solver for high condition linear systems using randomized stable solutions of its blocks](https://arxiv.org/abs/2510.02156)
*Suvendu Kar,Murugesan Venkatapathi*

Main category: math.NA

TL;DR: Enhanced randomized block-Kaczmarz method with regularization and dynamic proposal distribution for solving high-condition number linear systems.


<details>
  <summary>Details</summary>
Motivation: To improve solving high-condition number linear systems that are sparse or dense least-squares problems, especially when preconditioners perform poorly.

Method: Uses regularization during block updates and a dynamic proposal distribution based on current residue and effective orthogonality between blocks.

Result: Provides significant gains in solving high-condition number linear systems, particularly for sparse systems and over/under determined least-squares problems.

Conclusion: The improved method can serve as a pre-solver for other iterative numerical methods and as an inner iteration in certain GMRES solvers.

Abstract: We present an enhanced version of the row-based randomized block-Kaczmarz
method to solve a linear system of equations. This improvement makes use of a
regularization during block updates in the solution, and a dynamic proposal
distribution based on the current residue and effective orthogonality between
blocks. This improved method provides significant gains in solving
high-condition number linear systems that are either sparse, or dense
least-squares problems that are significantly over/under determined.
Considering the poor generalizability of preconditioners for such problems, it
can also serve as a pre-solver for other iterative numerical methods when
required, and as an inner iteration in certain types of GMRES solvers for
linear systems.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [10] [Localized Pattern Formation and Oscillatory Instabilities in a Three-component Gierer Meinhardt Model](https://arxiv.org/abs/2510.01401)
*Chunyi Gai,Fahad Al Saadi*

Main category: math.AP

TL;DR: A three-component Gierer-Meinhardt model with large diffusivity ratio exhibits dual Hopf bifurcations in spike amplitude and position, revealing richer oscillatory dynamics than two-component systems.


<details>
  <summary>Details</summary>
Motivation: To explore richer spike dynamics in three-component reaction-diffusion systems beyond classical two-component models, particularly oscillatory behavior in both amplitude and position.

Method: Asymptotic analysis and numerical path-following to construct localized spike equilibria, analyze spike nucleation via saddle-node bifurcation, and study stability using time-scaling parameters.

Result: Identified two distinct instability mechanisms: amplitude oscillations from large-eigenvalue instabilities and oscillatory spike motion from small eigenvalues, with numerical simulations showing transition regimes.

Conclusion: Three-component systems exhibit richer spike behavior through dual oscillation mechanisms, suggesting several open problems for future investigation.

Abstract: In this paper, we introduce a three-component Gierer-Meinhardt model in the
semi-strong interaction regime, characterized by an asymptotically large
diffusivity ratio. A key feature of this model is that the interior spike can
undergo Hopf bifurcations in both amplitude and position, leading to rich
oscillatory dynamics not present in classical two-component systems. Using
asymptotic analysis and numerical path-following, we construct localized spike
equilibria and analyze spike nucleation that occurs through slow passage beyond
a saddle-node bifurcation. Moreover, stability of spike equilibrium is analyzed
by introducing time-scaling parameters, which reveal two distinct mechanisms:
amplitude oscillations triggered by large-eigenvalue instabilities and
oscillatory spike motion associated with small eigenvalues. Numerical
simulations illustrate these dynamics and their transition regimes. This dual
mechanism highlights richer spike behavior in three-component systems and
suggests several open problems for future study.

</details>


### [11] [Symmetry analysis and new partially invariant solutions for the gas dynamics system with a special equation of state](https://arxiv.org/abs/2510.01415)
*Dilara Siraeva,Irina A. Kogan*

Main category: math.AP

TL;DR: Symmetry analysis of gas dynamics with special state equation, computing invariants for 4D subalgebras and constructing explicit solutions.


<details>
  <summary>Details</summary>
Motivation: To advance the symmetry analysis of gas dynamics systems following Ovsyannikov's submodels program, specifically for systems with pressure as sum of entropy and function of density.

Method: Study 4-dimensional subalgebras from optimal list, compute complete sets of generating invariants, construct partially symmetry-reduced systems, and explicitly solve them.

Result: Obtained new families of explicit solutions for the original gas dynamics system and analyzed their trajectories.

Conclusion: Successfully advanced symmetry analysis by computing invariants for subalgebras and generating new explicit solutions, while establishing groundwork for future study of reduced systems hierarchy.

Abstract: This paper is a contribution to the symmetry analysis of the gas dynamics
system in the vein of the ''podmodeli'' (submodels) program outlined by
Ovsyannikov (1994). We consider the case of the special state equation,
prescribing pressure to be the sum of entropy and an arbitrary function of
density. Such a system has a 12-dimensional symmetry Lie algebra. This work
advances the study of its four-dimensional subalgebras, continuing the work
started in Siraeva (2024). For a large subset of not previously considered,
non-similar four-dimensional subalgebras from an optimal list in Siraeva
(2014), we compute a complete set of generating invariants. For one of the
subalgebras, we construct a partially symmetry-reduced system. We explicitly
solve this reduced system (submodel). This leads to new families of explicit
solutions of the original system. We analyze the trajectories of these
solutions. Additionally, we match each of the subalgebras considered in this
paper with its isomorphism class, planting a seed for future study of the
hierarchy of the reduced systems.

</details>


### [12] [Correlation estimates for Brownian particles with singular interactions](https://arxiv.org/abs/2510.01507)
*Mitia Duerinckx,Pierre-Emmanuel Jabin*

Main category: math.AP

TL;DR: New framework for analyzing particle systems with singular interactions using linearized correlation functions, providing first systematic control of correlations for square-integrable kernels and extending key results beyond bounded interactions.


<details>
  <summary>Details</summary>
Motivation: Previous correlation function analysis only worked for bounded interactions, leaving singular interactions out of reach. The optimal estimates G_{N,m}=O(N^{1-m}) were unattainable for more singular interactions.

Method: Developed a new framework based on linearized correlation functions that provides robust bounds for systems with square-integrable interaction kernels. The estimates can be refined using the BBGKY hierarchy.

Result: First systematic control of correlations in singular setting. Method recovers optimal estimates for bounded interactions with simplified argument. Established validity of Bogolyubov correction to mean field and proved central limit theorem for empirical measure.

Conclusion: The framework extends key results beyond bounded interaction regime, providing systematic approach to analyze particle systems with singular pairwise interactions and non-vanishing diffusion.

Abstract: We study particle systems with singular pairwise interactions and
non-vanishing diffusion in the mean-field scaling. A classical approach to
describing corrections to mean-field behavior is through the analysis of
correlation functions. For bounded interactions, the optimal estimates on
correlations are well known: the $m$-particle correlation function is
$G_{N,m}=O(N^{1-m})$ for all $m$. Such estimates, however, have remained out of
reach for more singular interactions. In this work, we develop a new framework
based on linearized correlation functions, which allows us to derive robust
bounds for systems with merely square-integrable interaction kernels, providing
the first systematic control of correlations in the singular setting. Although
at first not optimal, our estimates can be partially refined a posteriori using
the BBGKY hierarchy: in the case of bounded interactions, our method recovers
the known optimal estimates with a simplified argument. As key applications, we
establish the validity of the Bogolyubov correction to mean field and prove a
central limit theorem for the empirical measure, extending these results beyond
the bounded interaction regime for the first time.

</details>


### [13] [On the attainment of boundary data in variational problems with linear growth](https://arxiv.org/abs/2510.01515)
*David Meyer*

Main category: math.AP

TL;DR: Convex variational problems with linear growth and Dirichlet boundary conditions may lack minimizers unless boundary conditions are relaxed. The paper shows that for various integrands under mean-convex boundary conditions, relaxed minimizers attain boundary data in trace sense for BV or W^α,p data with αp≥2, without continuity assumptions. Methods also work for systems under quasi-isotropy assumption, and counterexamples exist without this assumption.


<details>
  <summary>Details</summary>
Motivation: To address the problem that convex variational problems with linear growth and Dirichlet boundary conditions might not have minimizers unless boundary conditions are suitably relaxed, and to extend results to systems and various function spaces.

Method: Relaxation of boundary conditions, analysis under mean-convexity conditions, treatment of systems under quasi-isotropy assumption on the integrand, construction of counterexamples on uniformly convex domains.

Result: Minimizers of relaxed problems attain boundary data in trace sense for BV or W^α,p data with αp≥2 without continuity assumptions. Systems work under quasi-isotropy assumption, but smooth counterexamples exist without this assumption on uniformly convex domains.

Conclusion: The approach successfully handles boundary data attainment for relaxed variational problems, extends to systems under quasi-isotropy, and provides insights into uniqueness, ROF functional with Dirichlet conditions, and trace spaces of least gradient functions.

Abstract: It is well-known that convex variational problems with linear growth and
Dirichlet boundary conditions might not have minimizers if the boundary
condition is not suitably relaxed.
  We show that for a wide range of integrands, including the least gradient
problem and the non-parametric Plateau problem, and under suitable
mean-convexity conditions of the boundary, minimizers of the relaxed problem
attain the boundary data in the trace sense if it lies in $BV$ or
$W^{\alpha,p}$ with $\alpha p\geq 2$ without any kind of continuity assumption.
Unlike previous works, our methods are also able to treat systems under a
certain quasi-isotropy assumption on the integrand. We further show that
without this quasi-isotropy assumption, smooth counterexamples on uniformly
convex domains exist.
  Further applications to the uniqueness of minimizers and to open problems
about the ROF functional with Dirichlet boundary conditions, and to the trace
space of functions of least gradient are given.

</details>


### [14] [Inertial instability of Couette flow with Coriolis force](https://arxiv.org/abs/2510.01602)
*Yanlong Fan,Daozhi Han,Quan Wang*

Main category: math.AP

TL;DR: Analysis of nonlinear inertial instability in Couette flow under Coriolis forcing, showing velocity instability through pseudo-eigenfunctions despite absence of exponential growth in linearized system.


<details>
  <summary>Details</summary>
Motivation: To understand the nonlinear inertial instability of Couette flow under Coriolis forcing, particularly when the linearized system lacks exponentially growing eigenfunctions due to continuous spectrum.

Method: Construction of unstable solutions as pseudo-eigenfunctions with non-ideal spectral properties, followed by bootstrap argument to address challenges from non-ideal spectral behavior.

Result: Established velocity instability of Couette flow in the Hadamard sense for Coriolis coefficient f in the interval (2/17(5-2√2), 2/17(5+2√2)).

Conclusion: Couette flow exhibits nonlinear inertial instability under Coriolis forcing through pseudo-eigenfunctions, even when linear analysis shows no exponential growth, with instability occurring for specific Coriolis parameter ranges.

Abstract: We analyze the nonlinear inertial instability of Couette flow under Coriolis
forcing in \(\mathbb{R}^{3}\). For the Coriolis coefficient \(f \in (0,1)\), we
show that the non-normal operator associated with the linearized system admits
only continuous spectrum. Hence, there are no exponentially growing
eigenfunctions for the linearized system. Instead, we construct unstable
solutions in the form of pseudo-eigenfunctions that exhibit non-ideal spectral
properties. Then through a bootstrap argument and resolving the challenges
posed by the non-ideal spectral behavior of pseudo-eigenfunctions, we establish
the velocity instability of Couette flow in the Hadamard sense for $ f \in
\Big(\frac{2}{17} \left(5-2 \sqrt{2}\right), \frac{2}{17} \left(5 + 2
\sqrt{2}\right) \Big)$.

</details>


### [15] [The weighted isoperimetric inequality and Sobolev inequality outside convex sets](https://arxiv.org/abs/2510.01647)
*Lu Chen,Jiali Lan*

Main category: math.AP

TL;DR: Extension of weighted Sobolev inequality to capillary settings outside convex sets using weighted capillary isoperimetric inequality and capillary Schwarz symmetrization.


<details>
  <summary>Details</summary>
Motivation: To extend weighted Sobolev inequalities from half-spaces to more general capillary settings outside convex sets, building on previous work by Ciraolo-Figalli-Roncoroni.

Method: Use weighted capillary isoperimetric inequality via λ_w-ABP method, develop capillary Schwarz symmetrization outside convex sets.

Result: Established weighted Pólya-Szegő principle and sharp weighted capillary Sobolev inequality outside convex domains.

Conclusion: Successfully extended weighted Sobolev inequality framework to capillary settings outside convex sets, providing new tools for analysis in such domains.

Abstract: In this paper, we establish a weighted capillary isoperimetric inequality
outside convex sets using the $\lambda_w$-ABP method. The weight function $w$
is assumed to be positive, even, and homogeneous of degree $\alpha$, such that
$w^{1/\alpha}$ is concave on $\R^n$.
  Based on the weighted isoperimetric inequality, we develop a technique of
capillary Schwarz symmetrization outside convex sets, and establish a weighted
P\'{o}lya-Szeg\"{o} principle and a sharp weighted capillary Sobolev inequality
outside convex domain. Our result can be seen as an extension of the weighted
Sobolev inequality in the half-space established by Ciraolo-Figalli-Roncoroni
in \cite{CFR}.

</details>


### [16] [On dispersive decay for the generalized Korteweg--de Vries equation](https://arxiv.org/abs/2510.01728)
*Matthew Kowalski,Minjie Shan*

Main category: math.AP

TL;DR: Pointwise-in-time dispersive estimates for generalized Korteweg-de Vries equation solutions, showing L∞ decay like |t|^{-1/3} for mass-critical model with minimal initial data assumptions.


<details>
  <summary>Details</summary>
Motivation: To establish dispersive estimates for gKdV equation solutions, particularly for the mass-critical case, with minimal regularity requirements on initial data.

Method: Developed persistence of negative regularity for gKdV solutions and extended Lorentz-Strichartz estimates to mixed norm cases.

Result: Proved that solutions to mass-critical gKdV decay in L∞ norm like |t|^{-1/3} when initial data lies in H^{1/4} ∩ H^{-1/12}.

Conclusion: Successfully established pointwise-in-time dispersive estimates for gKdV equation with minimal initial data regularity assumptions through novel analytical techniques.

Abstract: We prove pointwise-in-time dispersive estimates for solutions to the
generalized Korteweg--de Vries (gKdV) equation. In particular, for solutions to
the mass-critical model, we assume only that initial data lie in
$\dot{H}^{\frac{1}{4}} \cap \dot{H}^{-\frac{1}{12}}$ and show that solutions
decay in $L^\infty$ like $|t|^{-\frac{1}{3}}$. To accomplish this, we develop a
persistence of negative regularity for solutions to gKdV and extend
Lorentz--Strichartz estimates to the mixed norm case.

</details>


### [17] [Nonlinear Forward-Backward Problems](https://arxiv.org/abs/2510.01732)
*Anne-Laure Dalibard,Frédéric Marbach,Jean Rax*

Main category: math.AP

TL;DR: Existence and uniqueness of strong solutions to quasilinear forward-backward parabolic equation near linear shear flow, with orthogonality conditions for source terms.


<details>
  <summary>Details</summary>
Motivation: Study quasilinear forward-backward parabolic problems that change type across critical lines, where solutions have opposite signs in different domain regions and lateral boundary conditions can only be imposed where characteristics are inwards.

Method: Nonlinear fixed-point iterative scheme with careful approximation procedure, addressing singular solutions in linearized equations and proving stability of orthogonality conditions with respect to base flow.

Result: Solutions are regular if and only if source terms satisfy finite number of orthogonality conditions; developed methodology for proving existence of regular solutions despite singular solutions at linear level.

Conclusion: Developed adaptable methodology for nonlinear problems with singular linear solutions, requiring orthogonality conditions for source terms to ensure regular solutions.

Abstract: We prove the existence and uniqueness of strong solutions to the equation $u
u_x - u_{yy} = f$ in the vicinity of the linear shear flow, subject to
perturbations of the source term and lateral boundary conditions. Since the
solutions we consider have opposite signs in the lower and upper half of the
domain, this is a quasilinear forward-backward parabolic problem, which changes
type across a critical curved line within the domain. In particular, lateral
boundary conditions can be imposed only where the characteristics are inwards.
There are several difficulties associated with this problem. First, the
forward-backward geometry depends on the solution itself. This requires to be
quite careful with the approximation procedure used to construct solutions.
Second, and more importantly, the linearized equations solved at each step of
the iterative scheme admit a finite number of singular solutions, of which we
provide an explicit construction. This is similar to well-known phenomena in
elliptic problems in nonsmooth domains. Hence, the solutions to the equation
are regular if and only if the source terms satisfy a finite number of
orthogonality conditions. A key difficulty of this work is to cope with these
orthogonality conditions during the nonlinear fixed-point scheme. In
particular, we are led to prove their stability with respect to the underlying
base flow. To tackle this deceivingly simple problem, we develop a methodology
which we believe to be both quite natural and adaptable to other situations in
which one wishes to prove the existence of regular solutions to a nonlinear
problem for suitable data despite the existence of singular solutions at the
linear level. This paper is a shorter version of [3].

</details>


### [18] [Notes on Schauder estimates by scaling for elliptic PDEs in divergence form](https://arxiv.org/abs/2510.01765)
*Stefano Vita*

Main category: math.AP

TL;DR: These notes present classical Schauder estimates for elliptic PDEs using geometric methods from minimal surfaces theory, featuring compactness, blow-up arguments, and Liouville theorems.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive treatment of interior and local Schauder estimates for second-order linear elliptic PDEs in divergence form, connecting geometric techniques from minimal surfaces to PDE analysis.

Method: Uses scaling techniques inspired by Simon's work, combining compactness and blow-up arguments with rigidity results (Liouville theorems) in a geometric approach.

Result: Develops a self-contained framework for classical Schauder estimates through geometric methods that share features with free boundary problem analysis.

Conclusion: The geometric approach from minimal surfaces theory provides an effective methodology for deriving Schauder estimates for elliptic PDEs, demonstrating connections between geometric analysis and PDE theory.

Abstract: These are the notes of a part of the PhD course Regularity for free boundary
problems and for elliptic PDEs, held in Pavia in the spring of 2025. The aim is
to provide a comprehensive and self-contained treatment of classical interior
and local Schauder estimates for second-order linear elliptic PDEs in
divergence form via scaling in the spirit of Simon's work. The main techniques
presented here are geometric in nature and were primarily developed in the
study of geometric problems such as minimal surfaces. The adopted approach
relies on compactness and blow-up arguments, combined with rigidity results
(Liouville theorems), and shares many features with the one used in the study
of free boundary problems, which was the main topic of the other part of the
PhD course.

</details>


### [19] [Strichartz and dispersive estimates for quantum bouncing ball model: exponential sums and Van der Corput methods in 1d semi-classical Schrödinger equations](https://arxiv.org/abs/2510.01779)
*Oana Ivanovici*

Main category: math.AP

TL;DR: Improved dispersive and Strichartz estimates for 1D semi-classical Schrödinger equation with linear potential on half-line, reducing losses from 1/4 to potentially 1/6+ε.


<details>
  <summary>Details</summary>
Motivation: To establish better space-time behavior understanding of solutions to the semi-classical Schrödinger equation with linear potential and Dirichlet boundary conditions on half-line.

Method: Using Van der Corput-type derivative tests to prove refined Strichartz bounds, and analyzing exponential sums to potentially reduce losses further.

Result: Proved improved Strichartz estimates that beat previous 1/4 losses, with potential to reduce to 1/6+ε losses assuming sharp exponential sum bounds.

Conclusion: The results suggest sharp Strichartz bounds are achievable and analogous estimates should extend to higher-dimensional Friedlander model domains.

Abstract: We analyze the one-dimensional semi-classical Schr\"odinger equation on the
half-line with a linear potential and Dirichlet boundary conditions. Our main
focus is on establishing improved dispersive and Strichartz estimates for this
model, which govern the space-time behavior of solutions. We prove refined
Strichartz bounds using Van der Corput-type derivative tests, beating previous
known results where Strichartz estimates incur 1/4 losses. Moreover, assuming
sharp bounds for certain exponential sums, our results indicate the possibility
to reduce these losses further to $1/6 + \epsilon$ for all $\epsilon>0$, which
would be sharp. We further expect that analogous Strichartz bounds should hold
within the Friedlander model domain in higher dimensions.

</details>


### [20] [Monotonicity and Liouville-type theorems for semilinear elliptic problems in the half space](https://arxiv.org/abs/2510.01865)
*Berardino Sciunzi,Domenico Vuono*

Main category: math.AP

TL;DR: Positive solutions to -Δu = f(u) in half-spaces with Dirichlet boundary conditions are strictly monotone increasing in the direction orthogonal to the boundary, given directional boundedness on finite strips. This leads to a new Liouville-type theorem for the Lane-Emden equation.


<details>
  <summary>Details</summary>
Motivation: To establish monotonicity properties of solutions to semilinear elliptic equations in half-spaces and derive new nonexistence results.

Method: Analysis of classical solutions to -Δu = f(u) in half-spaces with homogeneous Dirichlet boundary conditions, proving strict monotonicity in the direction orthogonal to the boundary under directional boundedness assumptions.

Result: Proved that any positive solution is strictly monotone increasing in the direction orthogonal to the boundary. Derived a new Liouville-type theorem for the Lane-Emden equation as a corollary.

Conclusion: The monotonicity property provides a powerful tool for studying semilinear elliptic equations in half-spaces, leading to new nonexistence results through Liouville-type theorems.

Abstract: We consider classical solutions to $-\Delta u = f(u)$ in half-spaces, under
homogeneous Dirichlet boundary conditions. We prove that any positive solution
is strictly monotone increasing in the direction orthogonal to the boundary,
provided that it is directionally bounded on finite strips. As a corollary, we
deduce a new Liouville-type theorem for the Lane-Emden equation.

</details>


### [21] [On sharp Strichartz estimate for hyperbolic Schrödinger equation on $\mathbb{T}^3$](https://arxiv.org/abs/2510.01886)
*Baoping Liu,Xu Zheng*

Main category: math.AP

TL;DR: Sharp Strichartz estimate for hyperbolic Schrödinger equation on 3D torus via incidence geometry, leading to optimal local well-posedness for nonlinear versions.


<details>
  <summary>Details</summary>
Motivation: To establish optimal regularity results for hyperbolic Schrödinger equations through precise Strichartz estimates.

Method: Incidence geometry approach to prove sharp Strichartz estimate on 3D torus.

Result: Proved sharp Strichartz estimate for hyperbolic Schrödinger equation on T³.

Conclusion: Optimal local well-posedness achieved for nonlinear hyperbolic Schrödinger equations as application.

Abstract: We prove the sharp Strichartz estimate for hyperbolic Schr\"{o}dinger
equation on $\mathbb{T}^3 $ via an incidence geometry approach. As application,
we obtain optimal local well-posedness of nonlinear hyperbolic Schr\"{o}dinger
equations.

</details>


### [22] [A note on the recovery sequence in the double gradient model for phase transitions](https://arxiv.org/abs/2510.01893)
*Jakob Deutsch*

Main category: math.AP

TL;DR: Analysis of the limsup inequality in double gradient phase transition models using Modica-Mortola functionals with double-well potentials in 2D.


<details>
  <summary>Details</summary>
Motivation: To characterize the limiting interfacial energy in phase transitions governed by double gradient models with two-well potentials.

Method: Using energy functionals with double gradient terms and periodic recovery sequences as epsilon approaches zero, assuming bounds on the optimal profile constant.

Result: Successful characterization of the limiting interfacial energy through periodic recovery sequences in the double gradient phase transition model.

Conclusion: The approach provides a framework for understanding interfacial energy limits in double gradient phase transition models with two-well potentials.

Abstract: We investigate the $\limsup$ inequality in the double gradient model for
phase transitions governed by a Modica--Mortola functional with a double-well
potential in two dimensions. Specifically, we consider energy functionals of
the form \[ E_\varepsilon(u, \Omega) = \int_\Omega \left( \frac{1}{\varepsilon}
W(\nabla u) + \varepsilon |\nabla^2 u|^2 \right) dx \] for maps $ u \in
H^2(\Omega; \mathbb{R}^2) $, where $ W $ vanishes only at two wells. Assuming a
bound on the optimal profile constant -- namely the cell problem on the unit
cube -- in terms of the geodesic distance between the two wells, we
characterise the limiting interfacial energy via periodic recovery sequences as
$\varepsilon \to 0^+$.

</details>


### [23] [Subwavelength resonances in two-dimensional elastic media with high contrast](https://arxiv.org/abs/2510.01911)
*Yuanchun Ren,Yixian Gao*

Main category: math.AP

TL;DR: This paper uses layer potential techniques to study wave scattering in 2D elastic media with high parameter contrasts, focusing on resonant frequencies, scattered fields, and phononic crystal bandgaps.


<details>
  <summary>Details</summary>
Motivation: To investigate wave scattering phenomena in highly contrasting elastic media and understand resonant frequency behavior, scattered field patterns, and subwavelength bandgap properties in phononic crystals.

Method: Employed layer potential techniques and boundary integral operators to construct invertible operators for resonant frequency characterization, used asymptotic analysis for leading-order frequency equations, and analyzed scattered fields across different frequency regimes.

Result: Developed a framework for characterizing resonant frequencies through orthogonality conditions, derived equations for leading-order resonant frequencies, characterized longitudinal and transverse far-field patterns, and examined subwavelength bandgaps in dilute phononic crystals.

Conclusion: The study provides comprehensive analytical tools for understanding wave scattering in highly contrasting elastic media, with applications to resonant frequency analysis and phononic crystal design.

Abstract: This paper employs layer potential techniques to investigate wave scattering
in two-dimensional elastic media exhibiting high contrasts in both Lam\'{e}
parameters and density. Our contributions are fourfold. First, we construct an
invertible operator based on the kernel spaces of boundary integral operators,
which enables the characterization of resonant frequencies through an
orthogonality condition. Second, we use asymptotic analysis to derive the
equation governing the leading-order terms of these resonant frequencies.
Third, we analyze the scattered field in the interior domain for incident
frequencies across different regimes and characterize the longitudinal and
transverse far-field patterns in the exterior domain. Finally, we examine the
subwavelength bandgap in the phononic crystal with a dilute structure.

</details>


### [24] [Low regularity Sobolev well-posedness for Vlasov--Poisson](https://arxiv.org/abs/2510.02112)
*In-Jee Jeong,Sangwook Tae*

Main category: math.AP

TL;DR: Local well-posedness of Vlasov-Poisson equation in H^s spaces with s > n/2 - 1/4 for n≥3, allowing non-L^p data.


<details>
  <summary>Details</summary>
Motivation: To establish local well-posedness for Vlasov-Poisson equation with lower regularity requirements than previous results, particularly allowing initial data that may not belong to L^p spaces for large p.

Method: Analysis of Vlasov-Poisson equation on R^n × R^n using Sobolev spaces H^s, with compact support condition in velocity variable v.

Result: Proved local well-posedness in H^s(R^n × R^n) with s > n/2 - 1/4 for n≥3, for initial distributions with compact support in v.

Conclusion: The Vlasov-Poisson equation is locally well-posed in lower regularity Sobolev spaces than previously known, expanding the class of admissible initial data beyond L^p spaces.

Abstract: We consider the Vlasov--Poisson equation on $\mathbb{R}^n \times
\mathbb{R}^n$ with $n \ge 3$. We prove local well-posedness in
$H^{s}(\mathbb{R}^n \times \mathbb{R}^n)$ with $s> n/2-1/4$, for initial
distribution $f_{0} \in H^{s}(\mathbb{R}^n \times \mathbb{R}^n)$ having compact
support in $v$. In particular, data not belonging to $L^p(\mathbb{R}^n \times
\mathbb{R}^n)$ for large $p$ are allowed.

</details>


### [25] [Transfer of Stability from the Classical to the Fractional Anisotropic Calderón Problem](https://arxiv.org/abs/2510.02242)
*Hendrik Baers,Angkana Rüland*

Main category: math.AP

TL;DR: This paper analyzes spectral fractional anisotropic Calderón problems with source-to-solution measurements and establishes quantitative relationships between local and nonlocal Calderón problems, providing stability estimates.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between local and nonlocal Calderón problems by quantifying their relationships and establishing stability results for fractional Calderón type problems where no Liouville-type reduction is known.

Method: Uses quantitative unique continuation arguments to relate source-to-solution measurements in fractional Calderón problems to Dirichlet-to-Neumann measurements in classical Calderón problems.

Result: Established that any stability result for the local Calderón problem with source-to-solution data has a direct nonlocal analogue (with logarithmic loss), and proved the first stability results for the principal part in fractional Calderón problems without Liouville-type reduction.

Conclusion: The paper successfully quantifies the relationship between local and nonlocal Calderón problems, provides stability estimates, and demonstrates that uniqueness transfers from local to nonlocal settings, with applications to both anisotropic and spectral fractional Dirichlet Laplacian settings.

Abstract: We discuss two spectral fractional anisotropic Calder\'on problems with
source-to-solution measurements and their quantitative relation to the
classical Calder\'on problem. Firstly, we consider the anistropic fractional
Calder\'on problem from [FGKU25]. In this setting, we quantify the relation
between the local and nonlocal Calder\'on problems which had been deduced in
[R25] and provide an associated stability estimate. As a consequence, any
stability result which holds on the level of the local problem with
source-to-solution data has a direct nonlocal analogue (up to a logarithmic
loss). Secondly, we introduce and discuss the fractional Calder\'on problem
with source-to-solution measurements for the spectral fractional Dirichlet
Laplacian on open, bounded, connected, Lipschitz sets on $\mathbb{R}^n$. Also
in this context, we provide a qualitative and quantitative transfer of
uniqueness from the local to the nonlocal setting. As a consequence, we infer
the first stability results for the principal part for a fractional Calder\'on
type problem for which no reduction of Liouville type is known. Our arguments
rely on quantitative unique continuation arguments. As a result of independent
interest, we also prove a quantitative relation between source-to-solution and
Dirichlet-to-Neumann measurements for the classical Calder\'on problem.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [26] [Integrated Software/Hardware Execution Models for High-Accuracy Methods in Chemistry](https://arxiv.org/abs/2510.01205)
*Nicholas Bauman,Ajay Panyala,Libor Veis,Jiri Brabec,Paul Rigor,Randy Meyer,Skyler Windh,Craig Warner,Tony Brewer,Karol Kowalski*

Main category: physics.comp-ph

TL;DR: This paper presents a hybrid approach combining Micron CXL hardware for memory-intensive CC downfolding and Azure Quantum Element cloud computing for DMRG simulations in quantum chemistry workflows.


<details>
  <summary>Details</summary>
Motivation: To optimize quantum chemistry simulations by effectively utilizing emerging computational resources and addressing the memory capacity challenges in coupled-cluster downfolding phases.

Method: Synergistic utilization of Micron memory technologies and Azure Quantum Element cloud computing in DMRG-DUCC workflow, with hybrid approach using CXL hardware for CC downfolding and cloud computing for DMRG simulations.

Result: Analysis of DMRG-DUCC workflow performance with proper hardware selection, and evaluation of scalable ExaChem suite on Micron prototype systems.

Conclusion: Hybrid hardware approach enables efficient quantum chemistry simulations by matching computational resources to specific workflow components' resource requirements.

Abstract: The effective deployment and application of advanced methodologies for
quantum chemistry is inherently linked to the optimal usage of emerging and
highly diversified computational resources. This paper examines the synergistic
utilization of Micron memory technologies and Azure Quantum Element cloud
computing in Density Matrix Renormalization Group (DMRG) simulations leveraging
coupled-cluster (CC) downfolded/effective Hamiltonians based on the double
unitary coupled cluster (DUCC) Ansatz. We analyze the performance of the
DMRG-DUCC workflow, emphasizing the proper choice of hardware that reflects the
numerical overheads associated with specific components of the workflow. We
report a hybrid approach that takes advantage of Micron CXL hardware for the
memory capacity intensive CC downfolding phase while employing AQE cloud
computing for the less resource-intensive DMRG simulations. Furthermore, we
analyze the performance of the scalable ExaChem suite of electronic simulations
conducted on Micron prototype systems.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [Detailed Derivation of the Scalar Explicit Expressions Governing the Electric Field, Current Density, and Volumetric Power Density in the Four Types of Linear Divergent MHD Channels Under a Unidirectional Applied Magnetic Field](https://arxiv.org/abs/2510.01289)
*Osama A. Marzouk*

Main category: physics.plasm-ph

TL;DR: Analytical derivation of electric field, current density, and power density equations for four types of MHD linear channels in plasma physics applications.


<details>
  <summary>Details</summary>
Motivation: To provide analytical expressions for electric field, current density, and power density in MHD channels to aid in selecting the most suitable channel type for specific applications.

Method: Mathematical analysis of algebraic equations governing electric field and current density vectors in MHD linear two-dimensional divergent supersonic channels, covering four channel types: continuous-electrode Faraday, linear Hall, segmented-electrode Faraday, and diagonal-electrode channels.

Result: Derived analytical expressions for electric fields and power densities. Under typical conditions (5 S/m conductivity, 5 T magnetic field, 2000 m/s plasma speed, 0.5 load factor): Continuous-electrode Faraday channel yields 5 kV/m electric field, 12.5 kA/m² current density, 62.5 MW/m³ power density, 50% efficiency. Hall linear channel yields 25 kV/m, 4.808 kA/m², 120.19 MW/m³, 46.30% efficiency.

Conclusion: The analytical expressions and numerical results provide valuable guidance for selecting appropriate MHD channel types based on specific application requirements, with different channel types offering varying performance characteristics in terms of electric field, current density, power density, and efficiency.

Abstract: The current study belongs to the field of applied mathematics in plasma
physics and electric power, where mathematical analysis of the algebraic
equations governing the electric field vector, and the electric-current density
field vector within a Magnetohydrodynamic (MHD) linear two-dimensional
divergent supersonic channel is utilized to derive analytical expressions for
these important fields, as well as closed-form equations for the volumetric
power density (output electric power per unit volume of the plasma channel).
The expressions presented here describe analytically the operation of the MHD
channel as an electric power source within an Open-Cycle Magnetohydrodynamic
(OCMHD) generator. The four common types of the MHD linear channels are covered
here: namely, (1) continuous-electrode Faraday channel, (2) linear Hall
channel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode
channel. The mathematical results, their detailed derivation, and the companion
graphical illustrations aid in making a proper decision regarding which channel
type is the most suitable for a given application.Under typical operational
conditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000
m/s plasma speed, as well as an optimized load factor of 0.5, we estimate the
following numerical values (unsigned magnitudes) for the continuous-electrode
Faraday channel (with a Hall parameter of 1): useful electric field (across the
external electric load): 5 kV/m, useful electric current-density (between the
terminal electrodes within the channel): 12.5 kA/m2 , volumetric power density
(dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric
efficiency (for the electric field or voltage): 50%. For the Halllinear channel
(with a Hall parameter of 5), these quantitative performance values become25
kV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%.

</details>


### [28] [Suppression of inverse magnetic energy transfer in collisionless marginally magnetized plasmas](https://arxiv.org/abs/2510.01573)
*Zhuo Liu,Muni Zhou,Nuno F. G. Loureiro*

Main category: physics.plasm-ph

TL;DR: Inverse magnetic energy cascade in collisionless plasmas is suppressed by pressure-anisotropy-driven instabilities like firehose instability, preventing magnetic structure coalescence. Guide fields or larger scale separation can restore inverse transfer.


<details>
  <summary>Details</summary>
Motivation: To understand how magnetic energy transfers to larger scales in collisionless plasmas, particularly relevant for astrophysical contexts like cosmic magnetogenesis from Weibel-generated seed fields.

Method: First-principles numerical simulations and analytical theory investigating decaying collisionless plasmas with moderate to high-β values.

Result: Firehose instability suppresses reconnection-driven coalescence by nullifying magnetic tension, confining structures to Larmor radius scales. Guide fields or larger scale separation between initial structures and Larmor radius restore inverse transfer capability.

Conclusion: Inverse energy transfer in collisionless plasmas is not guaranteed and sensitively depends on magnetization, revealing a kinetic mechanism that may limit Weibel-generated seed fields' role in cosmic magnetogenesis.

Abstract: We investigate the inverse cascade of magnetic energy in decaying,
collisionless plasmas with moderate to high-$\beta$ values via first-principles
numerical simulations and analytical theory. We find that
pressure-anisotropy-driven instabilities, in particular the firehose
instability, suppress reconnection-driven coalescence of magnetic structures
(i.e., inverse transfer) by nullifying magnetic tension. This suppression
leaves such structures elongated and confined to scales comparable to the
Larmor radius of the particles. The presence of a magnetic guide field of
sufficient strength, or a greater scale separation between the initial size of
the magnetic structures and the Larmor radius, restores the system's ability to
inverse transfer magnetic energy. These results reveal that inverse energy
transfer in collisionless plasmas is not guaranteed, but instead sensitively
depends on magnetization. In the astrophysical context, this identifies a
kinetic mechanism by which Weibel-generated seed fields may fail to merge
consistently, potentially limiting their role in cosmic magnetogenesis.

</details>


### [29] [Accelerating kinetic plasma simulations with machine learning generated initial conditions](https://arxiv.org/abs/2510.01977)
*Andrew T. Powis,Domenica Corona Rivera,Alexander Khrabry,Igor D. Kaganovich*

Main category: physics.plasm-ph

TL;DR: Machine learning-generated initial conditions accelerate kinetic plasma simulations for capacitively coupled plasma discharges, achieving up to 17.1x speedup in convergence time.


<details>
  <summary>Details</summary>
Motivation: Multi-time-scale plasma systems require many time steps to reach quasi-steady state, making computer-aided engineering challenging. Machine learning combined with traditional simulations offers pathways to resolve this computational burden.

Method: Three ML models (multi-layer perceptron, PCA, and convolutional neural networks) trained on simulations across device parameters to predict final time-averaged ion-density and velocity profiles. Used data-driven initial condition generators (ICGs) with offline and online procedures.

Result: Achieved mean speedup of 17.1x with offline procedure and 4.4x with online procedure. Convolutional neural networks performed best. Developed workflow for continuous model improvement and simulation acceleration.

Conclusion: Machine learning-generated initial conditions effectively accelerate plasma simulations, with potential for generating data for full device digital twins through continuous improvement workflows.

Abstract: Computer aided engineering of multi-time-scale plasma systems which exhibit a
quasi-steady state solution are challenging due to the large number of time
steps required to reach convergence. Machine learning techniques combined with
traditional first-principles simulations and high-performance computing offer
many interesting pathways towards resolving this challenge. We consider
acceleration of kinetic plasma simulations via machine learning generated
initial conditions. The approach is demonstrated through modeling of
capacitively coupled plasma discharges relevant to the microelectronics
industry. Three models are trained on simulations across a parameter space of
device driving frequency and operating pressure. The models incorporate
elements of a multi-layer perceptron, principal component analysis, and
convolutional neural networks to predict the final time-averaged profiles of
ion-density and velocity distribution functions. These data-driven initial
condition generators (ICGs) provide a mean speedup of 17.1x in convergence
time, when measured using an offline procedure, or a 4.4x speedup with an
online procedure, with convolutional neural networks leading to the best
performance. The paper also outlines a workflow for continuous data-driven
model improvement and simulation speedup, with the aim of generating sufficient
data for full device digital twins.

</details>


### [30] [A neural network approach to kinetic Mie polarimetry for particle size diagnostics in nanodusty plasmas](https://arxiv.org/abs/2510.02088)
*Alexander Schmitz,Andreas Petersen,Franko Greiner*

Main category: physics.plasm-ph

TL;DR: A neural network is developed to analyze nanoparticle sizes in plasma technology using light scattering, replacing complex back-calculation methods with faster, more stable automated analysis.


<details>
  <summary>Details</summary>
Motivation: Standard light scattering methods for nanoparticle size analysis require user expertise and are complex, creating a need for automated, accessible alternatives.

Method: A neural network is trained to analyze plasma-grown amorphous carbon nanoparticles using Mie theory-based light scattering, handling particles with refractive index n = 1.4-2.2 + 0.04i-0.1i and radii up to several hundred nanometers.

Result: The neural network achieves results comparable to prior fitting algorithms but with higher stability, faster computation, and full automation.

Conclusion: The neural network approach provides a superior alternative to traditional methods, offering improved stability, speed, and automation for nanoparticle size analysis in plasma diagnostics.

Abstract: The analysis of the size of nanoparticles is an essential task in plasma
technology and dusty plasmas. Light scattering techniques, based on Mie theory,
can be used as a non-invasive and in-situ diagnostic tool for this purpose.
However, the standard back-calculation methods require expertise from the user.
To address this, we introduce a neural network that performs the same task. We
discuss how we set up and trained the network to analyze the size of
plasma-grown amorphous carbon nanoparticles (a:C-H) with a refractive index n
in the range of real(n) = 1.4-2.2 and imag(n) = 0.04i-0.1i and a radius of up
to several hundred nanometers, depending on the used wavelength. The diagnostic
approach is kinetic, which means that the particles need to change in size due
to growth or etching. An uncertainty analysis as well as a test with
experimental data are presented. Our neural network achieves results that agree
with those of prior fitting algorithms while offering higher methodical
stability. The model also holds a major advantage in terms of computing speed
and automation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [31] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: SPUS is a compact foundation model using lightweight residual U-Net architecture for solving diverse PDEs, achieving SOTA generalization with fewer parameters and minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing PDE foundation models use large transformer architectures with high computational overhead, while U-Net architectures remain underexplored as foundation models in this domain.

Method: Leverages lightweight residual U-Net architecture with auto-regressive pretraining strategy that replicates numerical solver behavior, pretrained on diverse fluid dynamics PDEs.

Result: Achieves state-of-the-art generalization across 6 challenging unseen PDEs while requiring significantly fewer parameters and minimal fine-tuning data.

Conclusion: SPUS demonstrates potential as a highly parameter-efficient foundation model for solving diverse PDE systems, offering compact and efficient alternative to large transformer-based models.

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [32] [On Lieb-Thirring inequalities for multidimensional Schrödinger operators with complex potentials](https://arxiv.org/abs/2510.02192)
*Sabine Bögli,Sukrid Petpradittha,František Štampach*

Main category: math.SP

TL;DR: Counter-example construction disproves generalization of Lieb-Thirring inequality for complex-valued potentials in higher dimensions.


<details>
  <summary>Details</summary>
Motivation: To solve the open problem by Demuth, Hansmann, and Katriel regarding generalization of Lieb-Thirring inequality to complex-valued potentials.

Method: Constructed a counter-example that generalizes the one-dimensional case to higher dimensions.

Result: Successfully provided counter-example showing the inequality does not hold for complex-valued potentials in higher dimensions.

Conclusion: The Lieb-Thirring inequality cannot be generalized to complex-valued potentials in higher dimensions.

Abstract: We solve the open problem by Demuth, Hansmann, and Katriel announced in
[Integr. Equ. Oper. Theory 75 (2013), 1-5] by a counter-example construction.
The problem concerns a possible generalisation of the Lieb-Thirring inequality
for Schr\"odinger operators in to the case of complex-valued potentials. A
counter-example has already been found for the one-dimensional case by the
first and third authors in [J. Spectr. Theory 11 (2021), 1391-1413]. Here we
generalise the counter-example to higher dimensions.

</details>


### [33] [Optimal Lieb-Thirring type inequalities for Schrödinger and Jacobi operators with complex potentials](https://arxiv.org/abs/2510.02288)
*Sabine Bögli,Sukrid Petpradittha*

Main category: math.SP

TL;DR: Optimal Lieb-Thirring inequalities for Schrödinger and Jacobi operators with complex potentials, bounding eigenvalue power sums by L^p norms of potentials with specific weighting functions.


<details>
  <summary>Details</summary>
Motivation: To extend Lieb-Thirring inequalities from self-adjoint to complex potential operators, addressing the need for weighted terms based on eigenvalue positions relative to essential spectrum.

Method: Proving optimal bounds for integrable weight functions and establishing divergence estimates for non-integrable weights, comparing with semiclassical methods.

Result: Achieved optimal Lieb-Thirring type inequalities with weighted eigenvalue terms, showing improved divergence rates (logarithmic/polynomial gain) over semiclassical approaches.

Conclusion: The paper establishes fundamental bounds for complex potential operators with weighted eigenvalue terms, demonstrating superiority over traditional semiclassical methods.

Abstract: We prove optimal Lieb-Thirring type inequalities for Schr\"odinger and Jacobi
operators with complex potentials. Our results bound eigenvalue power sums
(Riesz means) by the $L^p$ norm of the potential, where in contrast to the
self-adjoint case, each term needs to be weighted by a function of the ratio of
the distance of the eigenvalue to the essential spectrum and the distance to
the endpoint(s) thereof. Our Lieb-Thirring type bounds only hold for integrable
weight functions. To prove optimality, we establish divergence estimates for
non-integrable weight functions. The divergence rates exhibit a logarithmic or
even polynomial gain compared to semiclassical methods (Weyl asymptotics) for
real potentials.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [34] [A Novel Algorithm for Representing Positive Semi-Definite Polynomials as Sums of Squares with Rational Coefficients](https://arxiv.org/abs/2510.01568)
*Zhenbing Zeng,Yong Huang,Lu Yang,Yongsheng Rao*

Main category: cs.SC

TL;DR: A novel algorithm for constructing sum-of-squares decompositions with rational coefficients for positive semi-definite polynomials, ensuring exact arithmetic for formal verification.


<details>
  <summary>Details</summary>
Motivation: Previous methods yield SOS decompositions with floating-point coefficients, which are unsuitable for formal verification and symbolic computation where exact arithmetic is required.

Method: Stepwise reduction technique that transforms polynomials into sum of ladder-like squares while preserving rationality of coefficients.

Result: Experimental results demonstrate effectiveness compared to existing numerical approaches, with all coefficients remaining rational.

Conclusion: The method provides exact SOS decompositions with rational coefficients, making it particularly useful for formal verification and symbolic computation applications.

Abstract: This paper presents a novel algorithm for constructing a sum-of-squares (SOS)
decomposition for positive semi-definite polynomials with rational
coefficients. Unlike previous methods that typically yield SOS decompositions
with floating-point coefficients, our approach ensures that all coefficients in
the decomposition remain rational. This is particularly useful in formal
verification and symbolic computation, where exact arithmetic is required. We
introduce a stepwise reduction technique that transforms a given polynomial
into a sum of ladder-like squares while preserving rationality. Experimental
results demonstrate the effectiveness of our method compared to existing
numerical approaches. This artical is an extension of the following Chinnese
paper: HUANG Yong , ZENG Zhenbing , YANG Lu , RAO Yongsheng. An Algorithm to
Represent Positive Semi-Definite Polynomials to Sum of Lader-Like Squares of
Polynomials with Rational Coefficients (in Chinese). Journal of Systems Science
and Mathematical Sciences, 2024, 44(5): 1241-1271
https://doi.org/10.12341/jssms23584CM

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [35] [Mean curvature flow through singularities](https://arxiv.org/abs/2510.01355)
*Robert Haslhofer*

Main category: math.DG

TL;DR: This paper provides an overview of mean curvature flow, recent advances in singularity theory in 3D, new classification results for 4D singularities, and discusses open problems.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive theory for mean curvature flow through singularities, particularly extending precise understanding from 3D to higher dimensions like 4D.

Method: General introduction to mean curvature flow, review of fundamental results from the last decade in R³, classification of noncollapsed singularities in R⁴, and discussion of open problems.

Result: Established a precise theory for mean curvature flow through singularities in R³ and provided a classification of all noncollapsed singularities in R⁴.

Conclusion: The paper advances the understanding of mean curvature flow singularities and identifies key open problems for future research in higher dimensions.

Abstract: We first give a general introduction to the mean curvature flow, and then
discuss fundamental results established over the last 10 years that yield a
precise theory for the flow through singularities in $\mathbb{R}^3$. With the
aim of developing a satisfying theory in higher dimensions, we then describe
our recent classification of all noncollapsed singularities in $\mathbb{R}^4$.
Finally, we provide a detailed discussion of open problems and conjectures.

</details>


### [36] [Uniqueness in the Plateau problem for calibrated currents](https://arxiv.org/abs/2510.02299)
*Bryan Dimler,Chen-Kuan Lee*

Main category: math.DG

TL;DR: Compactly supported calibrated integral currents with connected C³,α boundary are unique solutions to the oriented Plateau problem for their boundary data.


<details>
  <summary>Details</summary>
Motivation: To establish uniqueness results for the oriented Plateau problem by leveraging boundary regularity and unique continuation principles.

Method: Using boundary regularity theory for area-minimizing currents and adapting classical unique continuation principles to the minimal surface system.

Result: Proved that such currents are unique solutions to the oriented Plateau problem for their boundary.

Conclusion: The combination of boundary regularity and unique continuation ensures uniqueness in the oriented Plateau problem for the specified class of currents.

Abstract: We show that every compactly supported calibrated integral current with
connected $C^{3,\alpha}$ boundary is the unique solution to the oriented
Plateau problem for its boundary data. This is proved as a consequence of the
boundary regularity theory for area-minimizing currents and classical unique
continuation principles adapted to the minimal surface system.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [37] [Multiscale analysis of large twist ferroelectricity and swirling dislocations in bilayer hexagonal boron nitride](https://arxiv.org/abs/2510.01419)
*Md Tusher Ahmed,Chenhaoyue Wang,Amartya S. Banerjee,Nikhil Chandra Admal*

Main category: cond-mat.mtrl-sci

TL;DR: This paper demonstrates that ferroelectricity persists in bilayer hBN under large heterodeformations, not just small ones. It establishes the crystallographic origin using Smith normal form bicrystallography and develops a multiscale model to predict ferroelectricity in large-unit-cell configurations.


<details>
  <summary>Details</summary>
Motivation: Previous studies focused only on small heterodeformations in bilayer hBN, leaving the persistence of ferroelectricity under large heterodeformations unexplored. This work addresses this gap by investigating ferroelectric behavior across various stacking configurations.

Method: Used Smith normal form bicrystallography to establish crystallographic origin, performed atomistic simulations for AA-vicinal systems, and developed a density-functional-theory-informed continuum framework (BFIM model) for Σ7-vicinal systems where atomistic potentials are unreliable.

Result: Demonstrated out-of-plane ferroelectricity in bilayer hBN across configurations near both AA and Σ7 stacking. Found that AA-vicinal systems support ferroelectricity under small twist and strain, with different polarization switching mechanisms. Interface dislocations in large heterodeformations have smaller Burgers vectors.

Conclusion: Ferroelectricity persists in bilayer hBN under large heterodeformations. The BFIM model provides an efficient computational framework for predicting ferroelectricity in large-unit-cell heterostructures where atomistic simulations are too expensive.

Abstract: With its atomically thin structure and intrinsic ferroelectric properties,
heterodeformed bilayer hexagonal boron nitride (hBN) has gained prominence in
next-generation non-volatile memory applications. However, studies to date have
focused almost exclusively on small heterodeformations, leaving the question of
whether ferroelectricity can persist under large heterodeformation entirely
unexplored. In this work, we establish the crystallographic origin of
ferroelectricity in bilayer hBN configurations heterodeformed relative to
high-symmetry configurations such as the AA-stacking and the 21.786789 $\circ$
twisted configuration, using Smith normal form bicrystallography. We then
demonstrate out-of-plane ferroelectricity in bilayer hBN across configurations
vicinal to both the AA and $\Sigma 7$ stacking. Atomistic simulations reveal
that AA-vicinal systems support ferroelectricity under both small twist and
small strain, with polarization switching in the latter governed by the
deformation of swirling dislocations rather than the straight interface
dislocations seen in the former. For $\Sigma 7$-vicinal systems, where reliable
interatomic potentials are lacking, we develop a
density-functional-theory-informed continuum framework--the
bicrystallography-informed frame-invariant multiscale (BFIM) model, which
captures out-of-plane ferroelectricity in heterodeformed configurations vicinal
to the $\Sigma 7$ stacking. Interface dislocations in these large
heterodeformed bilayer configurations exhibit markedly smaller Burgers vectors
compared to the interface dislocations in small-twist and small-strain bilayer
hBN. The BFIM model reproduces atomistic simulation results and provides a
powerful, computationally efficient framework for predicting ferroelectricity
in large-unit-cell heterostructures where atomistic simulations are
prohibitively expensive.

</details>


### [38] [Enhancing the Efficiency of Time-Dependent Density Functional Theory Calculations of Dynamic Response Properties](https://arxiv.org/abs/2510.01875)
*Zhandos A. Moldabekov,Sebastian Schwalbe,Uwe Hernandez Acosta,Thomas Gawne,Jan Vorberger,Michele Pavanello,Tobias Dornheim*

Main category: cond-mat.mtrl-sci

TL;DR: A method to optimize TDDFT calculations for X-ray Thomson scattering by mapping dynamic structure factor to imaginary time density-density correlation function, achieving up to 10x speed-up.


<details>
  <summary>Details</summary>
Motivation: TDDFT is accurate for modeling XRTS spectra but computationally expensive under extreme conditions due to thermal excitations, temperature/density variations, and finite detector size.

Method: Combines one-to-one mapping between dynamic structure factor and imaginary time density-density correlation function with rigorous convergence tests and constraints-based noise attenuation.

Result: Achieves up to an order of magnitude speed-up in TDDFT calculations, potentially saving millions of CPU hours per XRTS measurement.

Conclusion: The method significantly enhances TDDFT efficiency for XRTS modeling under extreme conditions without introducing significant bias.

Abstract: X-ray Thomson scattering (XRTS) constitutes an essential technique for
diagnosing material properties under extreme conditions, such as high pressures
and intense laser heating. Time-dependent density functional theory (TDDFT) is
one of the most accurate available ab initio methods for modeling XRTS spectra,
as well as a host of other dynamic material properties. However, strong thermal
excitations, along with the need to account for variations in temperature and
density as well as the finite size of the detector significantly increase the
computational cost of TDDFT simulations compared to ambient conditions. In this
work, we present a broadly applicable method for optimizing and enhancing the
efficiency of TDDFT calculations. Our approach is based on a one-to-one mapping
between the dynamic structure factor and the imaginary time density--density
correlation function, which naturally emerges in Feynman's path integral
formulation of quantum many-body theory. Specifically, we combine rigorous
convergence tests in the imaginary time domain with a constraints-based noise
attenuation technique to improve the efficiency of TDDFT modeling without the
introduction of any significant bias. As a result, we can report a speed-up by
up to an order of magnitude, thus potentially saving millions of CPU hours for
modeling a single XRTS measurement of matter under extreme conditions.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [39] [Variational approach to open quantum systems with long-range competing interactions](https://arxiv.org/abs/2510.01543)
*Dawid A. Hryniuk,Marzena H. Szymańska*

Main category: quant-ph

TL;DR: Efficient computational method for simulating open quantum many-body systems with long-range interactions using matrix product operators and time-dependent variational Monte Carlo.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of accurate computational methods for simulating open quantum systems with complex long-range interactions at scale, which is crucial for understanding emergent phenomena in nature.

Method: Combines matrix product operators and time-dependent variational Monte Carlo to simulate dissipative quantum lattices in 1D and 2D with competing algebraically-decaying interactions.

Result: Successfully simulated non-equilibrium dynamics and steady states of spin-1/2 lattices with up to 200 sites, revealing emergence of spatially-modulated magnetic order far from equilibrium.

Conclusion: This approach provides promising prospects for understanding complex non-equilibrium properties of various experimentally-realizable quantum systems with long-range interactions.

Abstract: Competition between short- and long-range interactions underpins many
emergent phenomena in nature. Despite rapid progress in their experimental
control, computational methods capable of accurately simulating open quantum
many-body systems with complex long-ranged interactions at scale remain scarce.
Here, we address this limitation by introducing an efficient and scalable
approach to dissipative quantum lattices in one and two dimensions, combining
matrix product operators and time-dependent variational Monte Carlo. We
showcase the versatility, effectiveness, and unique methodological advantages
of our algorithm by simulating the non-equilibrium dynamics and steady states
of spin-$\frac{1}{2}$ lattices with competing algebraically-decaying
interactions for as many as $N=200$ sites, revealing the emergence of
spatially-modulated magnetic order far from equilibrium. This approach offers
promising prospects for advancing our understanding of the complex
non-equilibrium properties of a diverse variety of experimentally-realizable
quantum systems with long-ranged interactions, including Rydberg atoms,
ultracold dipolar molecules, and trapped ions.

</details>


### [40] [Hybrid Quantum-Classical Walks for Graph Representation Learning in Community Detection](https://arxiv.org/abs/2510.01918)
*Adrián Marın,Mauricio Soto-Gomez,Giorgio Valentini,Elena Casiraghi,Carlos Cano,Daniel Manzano*

Main category: quant-ph

TL;DR: A quantum-inspired algorithm using hybrid Quantum-Classical Walks is proposed for Graph Representation Learning to better capture complex graph relationships.


<details>
  <summary>Details</summary>
Motivation: Traditional GRL methods struggle with complex graphs exhibiting power-law distributions and hierarchical structures, requiring more effective relationship capture.

Method: Hybrid Quantum-Classical Walks that combine quantum and classical dynamics to simultaneously explore both local and far-reaching graph connections.

Result: Preliminary results in network community detection show effective adaptation to complex graph topologies.

Conclusion: The hybrid approach offers a robust and versatile solution for Graph Representation Learning tasks.

Abstract: Graph Representation Learning (GRL) has emerged as a cornerstone technique
for analysing complex, networked data across diverse domains, including
biological systems, social networks, and data analysis. Traditional GRL methods
often struggle to capture intricate relationships within complex graphs,
particularly those exhibiting non-trivial structural properties such as
power-law distributions or hierarchical structures. This paper introduces a
novel quantum-inspired algorithm for GRL, utilizing hybrid Quantum-Classical
Walks to overcome these limitations. Our approach combines the benefits of both
quantum and classical dynamics, allowing the walker to simultaneously explore
both highly local and far-reaching connections within the graph. Preliminary
results for a case study in network community detection shows that this hybrid
dynamic enables the algorithm to adapt effectively to complex graph topologies,
offering a robust and versatile solution for GRL tasks.

</details>


### [41] [Quantum advantages in ground state preparation, combinatorial optimization, and quantum state preparation](https://arxiv.org/abs/2510.01563)
*Taehee Ko,Sungbin Lim*

Main category: quant-ph

TL;DR: Quantum ground states can be prepared with polynomial-depth circuits using Pauli rotations, enabling exponential quantum advantages in optimization and state preparation.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that quantum systems with inverse-polynomial gaps can be efficiently prepared using shallow circuits, revealing fundamental quantum advantages.

Method: Using polynomial-depth circuits composed of Pauli rotations without ancilla qubits to prepare ground states and arbitrary quantum states.

Result: Ground states can be prepared to inverse-polynomial precision with polynomial circuit depth, and any quantum state can be prepared with constant Pauli rotations to constant precision for large systems.

Conclusion: The findings reveal exponential quantum advantages in ground state preparation, combinatorial optimization, and quantum state preparation for sufficiently large quantum systems.

Abstract: We show that for any quantum Hamiltonian with an inverse-polynomial gap, the
ground state can be prepared in a polynomial circuit depth to
inverse-polynomial precision, if the system size is sufficiently large. The
resulting circuit is composed of a polynomial number of Pauli rotations without
ancilla qubit. Extending this result, we prove that for sufficiently large
qubit number, any quantum state can be approximately prepared with a constant
(polynomial) number of Pauli rotations to constant (inverse-polynomial)
precision. Our theoretical findings reveal exponential quantum advantages in
the prominent applications: ground state preparation, combinatorial
optimization, and quantum state preparation.

</details>


### [42] [Improving neural network performance for solving quantum sign structure](https://arxiv.org/abs/2510.02051)
*Xiaowei Ou,Tianshu Huang,Vidvuds Ozolins*

Main category: quant-ph

TL;DR: A modified stochastic reconfiguration method that uses different imaginary time steps for amplitude and phase optimization, enabling simultaneous training of phase and amplitude neural networks without requiring a priori sign structure knowledge or pre-trained phase networks.


<details>
  <summary>Details</summary>
Motivation: Existing neural quantum state approaches for non-stoquastic Hamiltonians often rely on a priori knowledge of the sign structure or require separately pre-trained phase networks, which limits their practical application.

Method: Modified stochastic reconfiguration with differing imaginary time steps - larger time step for phase optimization and smaller for amplitude - allowing simultaneous training of both networks.

Result: The method successfully demonstrated on the Heisenberg J_1-J_2 model, showing effective training of both phase and amplitude neural networks.

Conclusion: The proposed approach provides an efficient way to train neural quantum states for non-stoquastic Hamiltonians without requiring prior sign structure knowledge or separate phase pre-training.

Abstract: Neural quantum states have emerged as a widely used approach to the numerical
study of the ground states of non-stoquastic Hamiltonians. However, existing
approaches often rely on a priori knowledge of the sign structure or require a
separately pre-trained phase network. We introduce a modified stochastic
reconfiguration method that effectively uses differing imaginary time steps to
evolve the amplitude and phase. Using a larger time step for phase
optimization, this method enables a simultaneous and efficient training of
phase and amplitude neural networks. The efficacy of our method is demonstrated
on the Heisenberg J_1-J_2 model.

</details>


### [43] [A quantum analogue of convex optimization](https://arxiv.org/abs/2510.02151)
*Eunou Lee*

Main category: quant-ph

TL;DR: A quantum algorithm called Fundamental Gap Algorithm (FGA) efficiently computes the minimum eigenvalue of Schrödinger operators with convex potentials, achieving polynomial time complexity in dimension, precision, and potential parameters.


<details>
  <summary>Details</summary>
Motivation: Convex optimization is fundamental in optimization theory and practice, and this work aims to develop quantum analogues for unconstrained convex optimization problems, specifically computing minimum eigenvalues of Schrödinger operators.

Method: The Fundamental Gap Algorithm (FGA) uses adiabatic evolution of the ground state as a key subroutine, with novel analysis techniques focusing on the low-energy space to compute minimum eigenvalues efficiently.

Result: FGA computes the minimum eigenvalue of Schrödinger operators with convex potentials up to error ε in polynomial time in dimension n, 1/ε, and potential-dependent parameters. It also provides the first polynomial-time algorithm for finding the lowest frequency of n-dimensional convex drums.

Conclusion: The FGA represents an efficient quantum approach to convex optimization problems, enabling polynomial-time computation of minimum eigenvalues for Schrödinger operators and solving the convex drum eigenvalue problem efficiently.

Abstract: Convex optimization is the powerhouse behind the theory and practice of
optimization. We introduce a quantum analogue of unconstrained convex
optimization: computing the minimum eigenvalue of a Schr\"odinger operator $h =
-\Delta + V $ with convex potential $V:\mathbb R^n \rightarrow \mathbb R_{\ge
0}$ such that $V(x)\rightarrow\infty $ as $\|x\|\rightarrow\infty$. For this
problem, we present an efficient quantum algorithm, called the Fundamental Gap
Algorithm (FGA), that computes the minimum eigenvalue of $h$ up to error
$\epsilon$ in polynomial time in $n$, $1/\epsilon$, and parameters that depend
on $V$. Adiabatic evolution of the ground state is used as a key subroutine,
which we analyze with novel techniques that allow us to focus on the low-energy
space. We apply the FGA to give the first known polynomial-time algorithm for
finding the lowest frequency of an $n$-dimensional convex drum, or
mathematically, the minimum eigenvalue of the Dirichlet Laplacian on an
$n$-dimensional region that is defined by $m$ linear constraints in polynomial
time in $n$, $m$, $1/\epsilon$ and the radius $R$ of a ball encompassing the
region.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [44] [Numerical tests of formulae for volume enclosed by flux surfaces of integrable magnetic fields](https://arxiv.org/abs/2510.01957)
*David Martinez-del-Rio,Robert S. MacKay*

Main category: math.DS

TL;DR: Numerical tests of volume formulae for computing volumes between flux surfaces in integrable 3D vector fields with varying symmetry levels, including a new proposed case.


<details>
  <summary>Details</summary>
Motivation: To develop efficient computational methods for calculating volumes enclosed between flux surfaces in integrable 3D vector fields, particularly addressing fields with different symmetry properties.

Method: Presented numerical tests of volume formulae, proposed and tested a new case for volume computation in integrable 3D vector fields.

Result: Successfully tested volume formulae for various symmetry degrees in integrable 3D vector fields and validated a newly proposed computational case.

Conclusion: The study demonstrates effective volume computation methods for integrable 3D vector fields across different symmetry levels, with the new case showing promising results.

Abstract: Numerical tests of volume formulae are presented to compute efficiently the
volume enclosed between flux surfaces for integrable 3D vector fields with
various degrees of symmetry. In the process, a new case is proposed and tested.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [45] [Relativistic Jets and Winds in Radio-Identified Supermassive Black Hole Binary Candidates](https://arxiv.org/abs/2510.02301)
*Andrew G. Sullivan,Roger D. Blandford,Anna Synani,Philipe V. de la Parra,Noémie Globus,Mitchell C. Begelman,Anthony C. S. Readhead*

Main category: astro-ph.HE

TL;DR: Proposes a jet model to explain periodic flux variations in two SMBHB candidates, where a mildly relativistic wind creates a helical channel for the ultra-relativistic jet, with flux variations mainly due to aberration effects.


<details>
  <summary>Details</summary>
Motivation: To explain the periodic flux density variations observed in two supermassive black hole binary candidates (PKS 2131-021 and PKS J0805-0111) that show clock-like behavior but lack confirmed individual nHz gravitational wave sources.

Method: Developed a generalizable jet model where a mildly relativistic wind creates an outward-moving helical channel along which the ultra-relativistic jet propagates, with observed flux variations primarily due to aberration effects.

Result: The model successfully reproduces the main observable features of both sources, including delayed variations at lower frequencies due to emission arising at larger radii, and can be applied to other similar sources.

Conclusion: The model provides testable predictions for radio polarization, direct imaging, and emission line variations, motivates future numerical simulations of jetted SMBHB systems, and has implications for understanding blazar jet fueling, structure, and evolution.

Abstract: Supermassive black hole binary systems (SMBHBs) are thought to emit the
recently discovered nHz gravitational wave background; however, not a single
individual nHz source has been confirmed to date. Long-term radio-monitoring at
the Owens Valley Radio Observatory has revealed two potential SMBHB candidates:
blazars PKS 2131-021 and PKS J0805-0111. These sources show periodic flux
density variations across the electromagnetic spectrum, signaling the presence
of a good clock. To explain the emission, we propose a generalizable jet model,
where a mildly relativistic wind creates an outward-moving helical channel,
along which the ultra-relativistic jet propagates. The observed flux variation
from the jet is mostly due to aberration. The emission at lower frequency
arises at larger radius and its variation is consequently delayed, as observed.
Our model reproduces the main observable features of both sources and can be
applied to other sources as they are discovered. We make predictions for radio
polarization, direct imaging, and emission line variation, which can be tested
with forthcoming observations. Our results motivate future numerical
simulations of jetted SMBHB systems and have implications for the fueling,
structure, and evolution of blazar jets.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [46] [Exponential convergence of a distributed divide-and-conquer algorithm for constrained convex optimization on networks](https://arxiv.org/abs/2510.01511)
*Nazar Emirov,Guohui Song,Qiyu Sun*

Main category: math.OC

TL;DR: A distributed divide-and-conquer algorithm for constrained convex optimization over networks with exponential convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: To solve large-scale constrained convex optimization problems over networks where the global objective is distributed across multiple agents, requiring efficient distributed algorithms that can handle local constraints and coordinate effectively.

Method: Divide-and-conquer approach with fully distributed iterations: each iteration solves local subproblems around selected fusion centers and coordinates only with neighboring fusion centers. Uses smoothness, strong convexity, and locality assumptions on the objective function.

Result: Established exponential convergence of DAC iterations with explicit bounds for both exact and inexact local solvers. Numerical experiments on L2 distance, quadratic, and entropy losses confirm theoretical results and demonstrate scalability and effectiveness.

Conclusion: The proposed DAC algorithm provides an efficient distributed solution for constrained convex optimization over networks with proven exponential convergence and practical effectiveness across various loss functions.

Abstract: We propose a divide-and-conquer (DAC) algorithm for constrained convex
optimization over networks, where the global objective is the sum of local
objectives attached to individual agents. The algorithm is fully distributed:
each iteration solves local subproblems around selected fusion centers and
coordinates only with neighboring fusion centers. Under standard assumptions of
smoothness, strong convexity, and locality on the objective function, together
with polynomial growth conditions on the underlying graph, we establish
exponential convergence of the DAC iterations and derive explicit bounds for
both exact and inexact local solvers. Numerical experiments on three
representative losses ($L_2$ distance, quadratic, and entropy) confirm the
theory and demonstrate scalability and effectiveness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems](https://arxiv.org/abs/2510.01396)
*Wasut Pornpatcharapong*

Main category: cs.LG

TL;DR: A neural network surrogate framework that learns collective variables (CVs) from Cartesian coordinates and provides Jacobians via automatic differentiation, enabling gradient-based free energy methods to use complex CVs without analytical forms.


<details>
  <summary>Details</summary>
Motivation: Traditional free energy reconstruction methods like Gaussian Process Regression require Jacobians of CVs, which is a bottleneck for using complex or machine-learned CVs.

Method: Neural network surrogate framework that learns CVs directly from Cartesian coordinates and uses automatic differentiation to provide Jacobians, bypassing the need for analytical forms.

Result: Achieved high accuracy on MgCl2 ion-pairing system for both simple distance CV and complex coordination-number CV. Jacobian errors followed near-Gaussian distribution suitable for GPR pipelines.

Conclusion: This framework enables gradient-based free energy methods to incorporate complex and machine-learned CVs, broadening the scope of biochemistry and materials simulations.

Abstract: Free energy reconstruction methods such as Gaussian Process Regression (GPR)
require Jacobians of the collective variables (CVs), a bottleneck that
restricts the use of complex or machine-learned CVs. We introduce a neural
network surrogate framework that learns CVs directly from Cartesian coordinates
and uses automatic differentiation to provide Jacobians, bypassing analytical
forms. On an MgCl2 ion-pairing system, our method achieved high accuracy for
both a simple distance CV and a complex coordination-number CV. Moreover,
Jacobian errors also followed a near-Gaussian distribution, making them
suitable for GPR pipelines. This framework enables gradient-based free energy
methods to incorporate complex and machine-learned CVs, broadening the scope of
biochemistry and materials simulations.

</details>


### [48] [Learning Regularization Functionals for Inverse Problems: A Comparative Study](https://arxiv.org/abs/2510.01755)
*Johannes Hertrich,Hok Shing Wong,Alexander Denker,Stanislas Ducotterd,Zhenghan Fang,Markus Haltmeier,Željko Kereta,Erich Kobler,Oscar Leong,Mohammad Sadegh Salehi,Carola-Bibiane Schönlieb,Johannes Schwab,Zakhar Shumaylov,Jeremias Sulam,German Shâma Wache,Martin Zach,Yasi Zhang,Matthias J. Ehrhardt,Sebastian Neumayer*

Main category: cs.LG

TL;DR: The paper presents a unified framework for comparing learned regularization methods in imaging inverse problems, addressing implementation inconsistencies and providing systematic analysis.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of comparing different learned regularization methods due to non-modular implementations and varying architectural designs and training strategies.

Method: Collecting and unifying available code for learned regularization methods into a common framework to enable systematic comparison.

Result: The unified framework allows systematic comparison of approaches, highlighting their strengths and limitations, and provides practical guidelines.

Conclusion: The unified view offers valuable insights into the future potential of learned regularization methods for imaging inverse problems.

Abstract: In recent years, a variety of learned regularization frameworks for solving
inverse problems in imaging have emerged. These offer flexible modeling
together with mathematical insights. The proposed methods differ in their
architectural design and training strategies, making direct comparison
challenging due to non-modular implementations. We address this gap by
collecting and unifying the available code into a common framework. This
unified view allows us to systematically compare the approaches and highlight
their strengths and limitations, providing valuable insights into their future
potential. We also provide concise descriptions of each method, complemented by
practical guidelines.

</details>


### [49] [Neural non-canonical Hamiltonian dynamics for long-time simulations](https://arxiv.org/abs/2510.01788)
*Clémentine Courtès,Emmanuel Franck,Michael Kraus,Laurent Navoret,Léopold Trémant*

Main category: cs.LG

TL;DR: This paper addresses numerical instability in learning non-canonical Hamiltonian dynamics by proposing two training strategies to overcome gauge dependency issues in variational integrators.


<details>
  <summary>Details</summary>
Motivation: Previous methods for learning Hamiltonian dynamics focused separately on model architecture or numerical schemes, but combining both creates gauge dependency issues that cause numerical instability and prevent long-term simulations.

Method: Two training strategies: 1) directly learning the vector field, and 2) learning time-discrete dynamics through the numerical scheme itself.

Result: The methods are tested on complex physical dynamics including guiding center from gyrokinetic plasma physics, showing improved numerical stability.

Conclusion: The proposed training strategies successfully address gauge dependency issues in learning non-canonical Hamiltonian dynamics, enabling stable long-term simulations.

Abstract: This work focuses on learning non-canonical Hamiltonian dynamics from data,
where long-term predictions require the preservation of structure both in the
learned model and in numerical schemes. Previous research focused on either
facet, respectively with a potential-based architecture and with degenerate
variational integrators, but new issues arise when combining both. In
experiments, the learnt model is sometimes numerically unstable due to the
gauge dependency of the scheme, rendering long-time simulations impossible. In
this paper, we identify this problem and propose two different training
strategies to address it, either by directly learning the vector field or by
learning a time-discrete dynamics through the scheme. Several numerical test
cases assess the ability of the methods to learn complex physical dynamics,
like the guiding center from gyrokinetic plasma physics.

</details>


<div id='nlin.SI'></div>

# nlin.SI [[Back]](#toc)

### [50] [Non-commutative multiple bi-orthogonal polynomials: formal approach and integrability](https://arxiv.org/abs/2510.02207)
*Adam Doliwa*

Main category: nlin.SI

TL;DR: The paper introduces non-commutative multiple bi-orthogonal polynomial systems, which generalize multiple orthogonality, matrix orthogonal polynomials, and bi-orthogonality. It provides quasideterminantal expressions, connects them to integrable systems via Hirota equations, and shows they yield non-commutative multidimensional discrete-time Toda equations.


<details>
  <summary>Details</summary>
Motivation: To generalize and unify concepts of multiple orthogonality, matrix orthogonal polynomials, and bi-orthogonality within a non-commutative framework, and to explore their connections to integrable systems.

Method: Define non-commutative multiple bi-orthogonal polynomial systems, derive quasideterminantal expressions using formal bi-moments, study normalization functions satisfying non-commutative Hirota equations, and specialize to non-commutative multiple orthogonal polynomials with Hankel-type quasideterminants.

Result: The polynomial systems satisfy non-commutative Hirota equations and provide solutions to corresponding linear systems, establishing them as part of integrable systems theory. The specialization yields non-commutative versions of multidimensional discrete-time Toda equations.

Conclusion: The introduced polynomial systems successfully generalize multiple orthogonality concepts in non-commutative settings and are deeply connected to integrable systems, particularly through Hirota equations and Toda lattice equations.

Abstract: We define the non-commutative multiple bi-orthogonal polynomial systems,
which simultaneously generalize the concepts of multiple orthogonality, matrix
orthogonal polynomials and of the bi-orthogonality. We present
quasideterminantal expressions for such polynomial systems in terms of formal
bi-moments. The normalization functions for such monic polynomials satisfy the
non-commutative Hirota equations, while the polynomials provide solution of the
corresponding linear system. This shows, in particular, that our polynomial
systems form a part of the theory of integrable systems. We study also a
specialization of the problem to non-commutative multiple orthogonal
polynomials, what results in the corresponding Hankel-type quasideterminantal
expressions in terms of the moments. Moreover, such a reduction allows to
introduce in a standard way the discrete-time variable and gives rise to an
integrable system which is non-commutative version of the multidimensional
discrete-time Toda equations.

</details>


### [51] [The noncommutative KP hierarchy and its solution via descent algebra](https://arxiv.org/abs/2510.01352)
*Gordon Blower,Simon J. A. Malham*

Main category: nlin.SI

TL;DR: The paper presents a solution to the complete noncommutative KP hierarchy using direct linearisation via the GLM equation and two approaches: Sato-Wilson dressing transformation and Poppe's semi-additive scattering data method.


<details>
  <summary>Details</summary>
Motivation: To solve the complete noncommutative Kadomtsev-Petviashvili (KP) hierarchy, which is an important problem in mathematical physics and integrable systems theory.

Method: Two approaches: 1) Standard Sato-Wilson dressing transformation using GLM equation, 2) Poppe's approach with semi-additive scattering data and augmented pre-Poppe algebra (nonassociative descent algebra with grafting product).

Result: The solution to the GLM equation coincides with the solution to the noncommutative KP hierarchy, establishing the complete solution through both approaches.

Conclusion: The second Poppe approach is advantageous as it is constructive, explicit, reveals combinatorial structures, and shows solution mechanisms, while the final result resides in the natural associative subalgebra.

Abstract: We give the solution to the complete noncommutative Kadomtsev--Petviashvili
(KP) hierarchy. We achieve this via direct linearisation which involves the
Gelfand--Levitan--Marchenko (GLM) equation. This is a linear integral equation
in which the scattering data satisfies the linearised KP hierarchy. The
solution to the GLM equation is then shown to coincide with the solution to the
noncommutative KP hierarchy. We achieve this using two approaches. In the first
approach we use the standard Sato-Wilson dressing transformation. In the second
approach, which was pioneered by Poppe, we assume the scattering data is
semi-additive and by direct substitution, we show that the solution to the GLM
equation satisfies the infinite set of field equations representing the
noncommutative KP hierarchy. This approach relies on the augmented pre-Poppe
algebra. This is a representative algebra that underlies the field equations
representing the hierarchy. It is nonassociative and isomorphic to a descent
algebra equipped with a grafting product. While we perform computations in the
nonassociative descent algebra, the final result which establishes the solution
to the complete hierarchy, resides in the natural associative subalgebra. The
advantages of this second approach are that it is constructive, explicit,
highlights the underlying combinatorial structures within the hierarchy, and
reveals the mechanisms underlying the solution procedure.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [52] [Perturbations of Minkowski spacetime with regular conformal compactification](https://arxiv.org/abs/2510.01964)
*Andrea Nützi*

Main category: gr-qc

TL;DR: The paper constructs perturbations of Minkowski spacetime from initial data decaying to Kerr data at spacelike infinity, showing these perturbations admit regular conformal compactification at null and timelike infinity.


<details>
  <summary>Details</summary>
Motivation: To generalize Friedrich's results by allowing initial data that decays to Kerr data rather than being identical to it on compact sets, and to develop methods for handling spacelike infinity asymptotics.

Method: Uses a novel formulation of Einstein equations about Minkowski spacetime as quasilinear symmetric hyperbolic PDEs regular at null infinity, with tailored energy estimates and organization by scaling homogeneity near spacelike infinity.

Result: Shows perturbations admit regular conformal compactification where regularity increases linearly with decay rate of initial data to Kerr data, with smooth compactification for rapidly decaying data.

Conclusion: Successfully generalizes Friedrich's work by developing new analytical techniques to handle spacelike infinity asymptotics and proving regularity results for perturbations decaying to Kerr initial data.

Abstract: We construct perturbations of Minkowski spacetime in general relativity, when
given initial data that decays inverse polynomially to initial data of a Kerr
spacetime towards spacelike infinity. We show that the perturbations admit a
regular conformal compactification at null and timelike infinity, where the
degree of regularity increases linearly with the rate of decay of the initial
data to Kerr initial data. In particular, the compactification is smooth if the
initial data decays rapidly to Kerr initial data. This generalizes results of
Friedrich, who constructed spacetimes with a smooth conformal compactification
in the case when the initial data is identical to Kerr initial data on the
complement of a compact set. Our results rely on a novel formulation of the
Einstein equations about Minkowski spacetime introduced by the author, that
allows one to formulate the dynamic problem as a quasilinear, symmetric
hyperbolic PDE that is regular at null infinity and with null infinity being at
a fixed locus. It is not regular at spacelike infinity, due to the asymptotics
of Kerr. Thus the main technical task is the construction of solutions near
spacelike infinity, using tailored energy estimates. To accomplish this, we
organize the equations according to homogeneity with respect to scaling about
spacelike infinity, which identifies terms that are leading, respectively lower
order, near spacelike infinity, with contributions from Kerr being lower order.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [53] [Improving Runtime Performance of Tensor Computations using Rust From Python](https://arxiv.org/abs/2510.01495)
*Kimmie Harding,Daniel M. Dunlavy*

Main category: cs.MS

TL;DR: This paper investigates improving runtime performance of tensor computational kernels in Python Tensor Toolbox by implementing them in Rust and calling from Python via extension modules.


<details>
  <summary>Details</summary>
Motivation: To enhance the runtime performance of key computational kernels in pyttb tensor analysis package, leveraging Rust's compiled language advantages over Python implementations.

Method: Implemented tensor kernels of varying complexity in Rust and called them from Python via extension modules using Python C API, comparing against Python alone, Numba JIT compiler, and NumPy implementations.

Result: Demonstrated consistent runtime performance improvements using Rust from Python compared to Python alone, Numba JIT compiler for loop-based kernels, and NumPy implementations across various tensor sizes.

Conclusion: Using Rust via extension modules provides significant runtime performance benefits for tensor computational kernels over existing Python-based approaches, making it a viable optimization strategy for tensor analysis packages.

Abstract: In this work, we investigate improving the runtime performance of key
computational kernels in the Python Tensor Toolbox (pyttb), a package for
analyzing tensor data across a wide variety of applications. Recent runtime
performance improvements have been demonstrated using Rust, a compiled
language, from Python via extension modules leveraging the Python C API --
e.g., web applications, data parsing, data validation, etc. Using this same
approach, we study the runtime performance of key tensor kernels of increasing
complexity, from simple kernels involving sums of products over data accessed
through single and nested loops to more advanced tensor multiplication kernels
that are key in low-rank tensor decomposition and tensor regression algorithms.
In numerical experiments involving synthetically generated tensor data of
various sizes and these tensor kernels, we demonstrate consistent improvements
in runtime performance when using Rust from Python over 1) using Python alone,
2) using Python and the Numba just-in-time Python compiler (for loop-based
kernels), and 3) using the NumPy Python package for scientific computing (for
pyttb kernels).

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [54] [The centered maximal operator removes the non-concave Cantor part from the gradient](https://arxiv.org/abs/2510.01936)
*Panu Lahti,Julian Weigt*

Main category: math.CA

TL;DR: The paper studies regularity properties of the centered Hardy-Littlewood maximal function for functions of bounded variation in R^d, showing conditions under which maximal function values exceed function values and when BV regularity can be upgraded to Sobolev regularity.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of the Hardy-Littlewood maximal function for functions of bounded variation, particularly when the maximal function exceeds the function value and when BV regularity implies Sobolev regularity.

Method: Analysis of the centered Hardy-Littlewood maximal function Mf for functions f of bounded variation, focusing on points where f has non-concave blow-ups and studying the variation measure structure.

Result: At |D^c f|-a.e. point x where f has a non-concave blow-up, Mf(x) > f*(x). When the variation measure has no jump part and its Cantor part has non-concave blow-ups, BV regularity of Mf implies Sobolev regularity.

Conclusion: The paper establishes precise conditions under which the maximal function exceeds the function value and provides criteria for upgrading BV regularity to Sobolev regularity for the Hardy-Littlewood maximal function.

Abstract: We study regularity of the centered Hardy--Littlewood maximal function $M f$
of a function $f$ of bounded variation in $\mathbb R^d$, $d\in \mathbb N$. In
particular, we show that at $|D^c f|$-a.e. point $x$ where $f$ has a
non-concave blow-up, it holds that $M f(x)>f^*(x)$. We further deduce from this
that if the variation measure of $f$ has no jump part and its Cantor part has
non-concave blow-ups, then BV regularity of $M f$ can be upgraded to Sobolev
regularity.

</details>
