<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 21]
- [math.AP](#math.AP) [Total: 28]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 7]
- [cs.DC](#cs.DC) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [math-ph](#math-ph) [Total: 3]
- [cs.CE](#cs.CE) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [math.NT](#math.NT) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [stat.CO](#stat.CO) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [cs.LG](#cs.LG) [Total: 10]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [math.OC](#math.OC) [Total: 4]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [math.DG](#math.DG) [Total: 4]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A Generalized Second-Order Positivity-Preserving Numerical Method for Non-Autonomous Dynamical Systems with Applications](https://arxiv.org/abs/2510.03420)
*Manh Tuan Hoang,Matthias Ehrhardt*

Main category: math.NA

TL;DR: A second-order nonstandard finite difference method for non-autonomous systems that preserves positivity unconditionally and avoids restrictive conditions of existing methods.


<details>
  <summary>Details</summary>
Motivation: To develop a generalized NSFD method that achieves second-order convergence while unconditionally preserving solution positivity without the restrictive conditions of existing approaches.

Method: Combines NSFD framework with a new non-local approximation of the right-hand side function to create a second-order method that maintains positivity for all step sizes.

Result: The method achieves second-order convergence, unconditionally preserves positivity, is easy to implement, computationally efficient, and works well for SIR epidemic models, nonlinear PDEs, and boundary value problems.

Conclusion: The proposed NSFD method successfully provides a versatile, positivity-preserving second-order scheme applicable to various real-world modeling problems without restrictive conditions.

Abstract: In this work, we propose a generalized, second-order, nonstandard finite
difference (NSFD) method for non-autonomous dynamical systems. The proposed
method combines the NSFD framework with a new non-local approximation of the
right-hand side function. This method achieves second-order convergence and
unconditionally preserves the positivity of solutions for all step sizes.
Especially, it avoids the restrictive conditions required by many existing
positivity-preserving, second-order NSFD methods. The method is easy to
implement and computationally efficient. Numerical experiments, including an
improved NSFD scheme for an SIR epidemic model, confirm the theoretical
results. Additionally, we demonstrate the method's applicability to nonlinear
partial differential equations and boundary value problems with positive
solutions, showcasing its versatility in real-world modeling.

</details>


### [2] [Generalized rational Prony and Bernoulli methods](https://arxiv.org/abs/2510.03510)
*Tamás Dózsa,Matthias Voigt,Zoltán Szabó,József Bokor,Péter Kovács*

Main category: math.NA

TL;DR: This paper connects the generalized operator-based Prony method with Bernoulli's algorithm, showing that any Prony problem can be treated as a rational variant and addressing numerical issues.


<details>
  <summary>Details</summary>
Motivation: To establish the connection between Prony and Bernoulli methods and address numerical pitfalls in signal parameter recovery problems.

Method: Develops a rational variant of the generalized operator-based Prony method and shows its equivalence to Bernoulli methods for parameter recovery in finite-dimensional subspaces.

Result: Demonstrates that any Prony problem can be treated using the rational variant approach, successfully applying the methods to time-delayed linear systems and reproducing kernel Hilbert spaces.

Conclusion: The connection between Prony and Bernoulli methods provides a unified framework for parameter recovery with improved numerical stability, as validated through various applications.

Abstract: The generalized operator-based Prony method is an important tool for
describing signals which can be written as finite linear combinations of
eigenfunctions of certain linear operators. On the other hand, Bernoulli's
algorithm and its generalizations can be used to recover the parameters of
rational functions belonging to finite-dimensional subspaces of $H_2$
Hardy-Hilbert spaces. In this work, we discuss several results related to these
methods. We discuss a rational variant of the generalized operator-based Prony
method and show that in fact, any Prony problem can be treated this way. This
realization establishes the connection between Prony and Bernoulli methods and
allows us to address some well-known numerical pitfalls. Several numerical
experiments are provided to showcase the usefulness of the introduced methods.
These include problems related to the identification of time-delayed linear
systems and parameter recovery problems in reproducing kernel Hilbert spaces.

</details>


### [3] [General Order Virtual Element Approximation for the Smagorinsky turbulence model](https://arxiv.org/abs/2510.03563)
*Stefano Berrone,Karol L. Cascavita,Enrique Delgado Ávila,Samuele Rubino,Maria Strazzullo,Fabio Vicini*

Main category: math.NA

TL;DR: The paper presents a Smagorinsky model in a virtual element framework for simulating convection-dominated Navier-Stokes equations, with numerical validation and performance analysis on lid-driven cavity flows.


<details>
  <summary>Details</summary>
Motivation: To develop and validate a novel virtual element discretization that includes the Smagorinsky term for simulating convection-dominated flows, addressing the need for accurate and stable numerical methods for high Reynolds number flows.

Method: A two-dimensional numerical investigation using general order virtual element approximation with Smagorinsky model. The study examines convergence with meshsize and tests on lid-driven cavity with different Reynolds numbers (up to 10000) and mesh types (uniform, anisotropic, and isotropic with hanging nodes).

Result: The virtual element method with isotropic refinement using hanging nodes provides enhanced accuracy compared to anisotropic meshes, uses fewer degrees of freedom than uniform meshes, and demonstrates the most stable convergence behavior for the Newton solver.

Conclusion: The virtual elements method with isotropic refinement and hanging nodes offers significant advantages for simulating convection-dominated Navier-Stokes equations, providing improved accuracy, computational efficiency, and solver stability compared to traditional mesh approaches.

Abstract: In this paper, we investigate a Smagorinsky model in a virtual element
framework to simulate convection-dominated Navier-Stokes equations. We conduct
a two-dimensional numerical investigation to assess the performance of the
general order virtual element approximation in this context. First, we examine
numerically the convergence of the method with respect to the meshsize to
certify the novel virtual element numerical discretization, which includes, for
the first time, a discretization of the Smagorinsky term. Moreover, we present
a numerical study of a lid-driven cavity for different Reynolds numbers (up to
10000) and meshes (uniform, anisotropic, and isotropic with hanging nodes). The
results highlight the main advantage of using the virtual elements method in
this context: the isotropic refinement with hanging nodes enhances the accuracy
of the solution compared to the anisotropic mesh, uses fewer degrees of freedom
with respect to the uniform mesh, and yields the most stable behavior in terms
of convergence of the Newton solver.

</details>


### [4] [Fully discrete finite element methods for the stochastic Kuramoto-Sivashinsky equation with multiplicative noise](https://arxiv.org/abs/2510.03670)
*Liet Vo,Hung Dang Nguyen*

Main category: math.NA

TL;DR: Finite element analysis of stochastic Kuramoto-Sivashinsky equation with implicit Euler-Maruyama time discretization, establishing error estimates for bounded and general multiplicative noise regimes.


<details>
  <summary>Details</summary>
Motivation: To provide the first comprehensive error analysis for numerical approximations of the stochastic Kuramoto-Sivashinsky equation, addressing both bounded and general multiplicative noise cases.

Method: Combines standard finite element methods for spatial discretization with implicit Euler-Maruyama scheme for time discretization, using stochastic Gronwall inequality and exponential stability estimates for bounded noise, and localization technique for general noise.

Result: Optimal strong convergence rates in full expectation for bounded multiplicative noise, and sub-optimal convergence rates in probability for general multiplicative noise using localization on sample space subsets.

Conclusion: The fully discrete scheme achieves strong convergence under bounded noise and probabilistic convergence under general multiplicative noise, providing comprehensive error analysis for stochastic Kuramoto-Sivashinsky equation approximations.

Abstract: We investigate a fully discrete finite element approximation for the
stochastic Kuramoto-Sivashinsky equation, combining the standard finite element
methods in spatial discretization with the implicit Euler-Maruyama scheme in
time. Rigorous error estimates are established for two distinct noise regimes.
In the case of bounded multiplicative noise, we prove optimal strong
convergence rates in full expectation. The analysis relies crucially on a
stochastic Gronwall inequality and an exponential stability estimate for the
PDE solution, which together control the interplay between the nonlinear drift
and the multiplicative stochastic forcing. For general multiplicative noise,
where boundedness no longer holds, we derive sub-optimal convergence rates in
probability by introducing a localization technique based on carefully
constructed subsets of the sample space. This dual framework demonstrates that
the proposed fully discrete scheme achieves strong convergence under bounded
noise and probabilistic convergence under general multiplicative noise, thus
providing the first comprehensive error analysis for numerical approximations
of the stochastic Kuramoto-Sivashinsky equation.

</details>


### [5] [A Variational Method for Conformable Fractional Equations Using Rank-One Updates](https://arxiv.org/abs/2510.03778)
*Maatank Parashar,Tejas Dhulipalla*

Main category: math.NA

TL;DR: Complete variational analysis of rank-one Proper Generalised Decomposition for separable fractional PDEs with conformable derivatives, including greedy updates, energy decay proofs, and discretization methods.


<details>
  <summary>Details</summary>
Motivation: To develop a rigorous mathematical framework for Proper Generalised Decomposition methods applied to fractional partial differential equations with conformable derivatives, addressing both theoretical foundations and practical implementations.

Method: Variational treatment in Hilbert space using symmetric coercive bilinear forms; greedy rank-one updates via energy Rayleigh quotient maximization; alternating least squares approach; discretization with weighted finite elements and Grünwald schemes.

Result: Proved exact one-step energy decrease identity and geometric decay of energy error under weak greedy conditions; established well-posedness of alternating subproblems and monotonicity properties; implemented methods for fractional Poisson and space-time fractional diffusion problems.

Conclusion: The framework provides rigorous mathematical foundations for PGD methods in fractional PDEs, with proven convergence properties and practical discretization schemes that bridge continuous theory to matrix implementations.

Abstract: We make a complete variational treatment of rank-one Proper Generalised
Decomposition for separable fractional partial differential equations with
conformable derivatives. The setting is Hilbertian, the energy is induced by a
symmetric coercive bilinear form, and the residual is placed in the dual space.
A greedy rank-one update is obtained by maximizing an energy Rayleigh quotient
over the rank-one manifold, followed by an exact line search. An exact one step
energy decrease identity is proved, together with geometric decay of the energy
error under a weak greedy condition that measures how well the search captures
the Riesz representer of the residual. The alternating least squares
realization is analyzed at the level of operators, including well posedness of
the alternating subproblems, a characterization of stationary points, and
monotonicity of the Rayleigh quotient along the inner iteration.
Discretizations based on weighted finite elements and on Gr\"unwald type
schemes are described in detail, including assembly, boundary conditions,
complexity, and memory. Two model problems, a stationary fractional Poisson
problem and a space time fractional diffusion problem, are treated from the
continuous level down to matrices.

</details>


### [6] [Fourier-Galerkin method for scattering poles of sound soft obstacles](https://arxiv.org/abs/2510.03826)
*Yunyun Ma,Jiguang Sun*

Main category: math.NA

TL;DR: Novel method for computing scattering poles of sound-soft obstacles using boundary integral operators and Fourier-Galerkin discretization with error analysis.


<details>
  <summary>Details</summary>
Motivation: To develop an effective computational method for determining scattering poles, which are important in wave scattering problems and correspond to eigenvalues of boundary integral operators.

Method: Construct novel decompositions of boundary integral operators, prove they are Fredholm, use Fourier-Galerkin method for discretization, and apply abstract approximation theory for eigenvalue problems.

Result: Established regular convergence of discrete operators, derived error estimates, and validated the method through numerical examples showing its effectiveness.

Conclusion: The proposed method provides a rigorous and effective computational approach for computing scattering poles with proven convergence properties and error estimates.

Abstract: The computation of scattering poles for a sound-soft obstacle is
investigated. These poles correspond to the eigenvalues of two boundary
integral operators. We construct novel decompositions of these operators and
show that they are Fredholm. Then a Fourier-Galerkin method is proposed for
discretization. By establishing the regular convergence of the discrete
operators, an error estimate is established using the abstract approximation
theory for eigenvalue problems of holomorphic Fredholm operator functions. We
give details of the numerical implementation. Several examples are presented to
validate the theory and demonstrate the effectiveness of the proposed method.

</details>


### [7] [High-order, Compact, and Symmetric Finite Difference Methods for a $d$-Dimensional Hypercube](https://arxiv.org/abs/2510.03927)
*Qiwei Feng,Bin Han,Michelle Michelle,Jiwoon Sim*

Main category: math.NA

TL;DR: Compact symmetric high-order finite difference methods for variable Poisson equation on hypercubes, achieving up to 4th order in d≥2 dimensions with symmetric positive definite linear systems.


<details>
  <summary>Details</summary>
Motivation: To develop finite difference methods that produce symmetric linear systems for variable Poisson equations, enabling use of fast solvers while maintaining compact stencils and minimal storage requirements.

Method: Develop compact symmetric finite difference methods on uniform grids for d-dimensional hypercubes, with theoretical analysis of maximum achievable consistency orders under different conditions.

Result: Proved that 1D compact symmetric FDMs can achieve arbitrary order, while d≥2 dimensions have maximum order 4 (or 6 for d=2 under certain conditions). Provided explicit stencils for 4th-order methods in d≥3 as linear combinations of single-axis and two-axis derivative FDMs.

Conclusion: Compact symmetric high-order FDMs are feasible for variable Poisson equations with proven consistency order bounds, offering symmetric positive definite systems suitable for fast solvers with minimal storage.

Abstract: This paper presents compact, symmetric, and high-order finite difference
methods (FDMs) for the variable Poisson equation on a $d$-dimensional
hypercube. Our scheme produces a symmetric linear system: an important property
that does not immediately hold for a high-order FDM. Since the model problem is
coercive, the linear system is in fact symmetric positive definite, and
consequently many fast solvers are applicable. Furthermore, the symmetry
combined with the minimum support of the stencil keeps the storage requirement
minimal. Theoretically speaking, we prove that a compact, symmetric 1D FDM on a
uniform grid can achieve arbitrary consistency order. On the other hand, in the
$d$-dimensional setting, where $d \ge 2$, the maximum consistency order that a
compact, symmetric FDM on a uniform grid can achieve is 4. If $d=2$ and the
diffusion coefficient satisfies a certain derivative condition, the maximum
consistency order is 6. Moreover, the finite compact, symmetric, 4th-order FDMs
for $d\ge 3$, can be conveniently expressed as a linear combination of two
types of FDMs: one that depends on partial derivatives along one axis, and the
other along two axes. All finite difference stencils are explicitly provided
for ease of reproducibility.

</details>


### [8] [A discrete data assimilation algorithm for the reconstruction of Gray--Scott dynamics](https://arxiv.org/abs/2510.03972)
*Tsiry Avisoa Randrianasolo*

Main category: math.NA

TL;DR: The paper presents a nudging-based data assimilation algorithm for the Gray-Scott reaction-diffusion model to address sensitivity to initial conditions, proving synchronization between nudged and true solutions with exponential error decay.


<details>
  <summary>Details</summary>
Motivation: The Gray-Scott model is highly sensitive to poorly observed initial conditions, causing simulations to quickly deviate from true dynamics, which motivates the need for data assimilation methods.

Method: A nudging-based data assimilation algorithm using coarse, cell-averaged measurements injected through a feedback term implemented as a finite-volume interpolant, with both continuous and discrete formulations.

Result: Theoretical proofs show: (i) for continuous problem, nudged solution synchronizes with true dynamics with exponential L²-error decay; (ii) for discrete scheme, same synchronization holds with mild time-step restriction. Numerical tests confirm recovery of fine structure from sparse data.

Conclusion: The nudging method effectively addresses initial condition sensitivity in the Gray-Scott model, enabling accurate pattern recovery from sparse observations through proper parameter tuning of observation resolution, nudging gain, and update frequency.

Abstract: The Gray--Scott model governs the interaction of two chemical species via a
system of reaction-diffusion equations. Despite its simple form, it produces
extremely rich patterns such as spots, stripes, waves, and labyrinths. That
makes it ideal for studying emergent behavior, self-organization, and
instability-driven pattern formation. It is also known for its sensitivity to
poorly observed initial conditions. Using such initial conditions alone quickly
leads simulations to deviate from the true dynamics. The present paper
addresses this challenge with a nudging-based data assimilation algorithm:
coarse, cell-averaged measurements are injected into the model through a
feedback (nudging) term, implemented as a finite-volume interpolant. We prove
two main results. (i) For the continuous problem, the nudged solution
synchronizes with the true dynamics, and the $L^2$-error decays exponentially
under conditions that tie observation resolution, nudging gains, and diffusion.
(ii) For the fully discrete semi-implicit finite-volume scheme, the same
synchronization holds, up to a mild time-step restriction. Numerical tests on
labyrinthine patterns support the theory. They show recovery of fine structure
from sparse data and clarify how the observation resolution, the nudging gain,
and the frequency of updates affect the decay rate.

</details>


### [9] [Sharp Lower Bounds for Linearized ReLU^k Approximation on the Sphere](https://arxiv.org/abs/2510.04060)
*Tong Mao,Jinchao Xu*

Main category: math.NA

TL;DR: The paper establishes the exact saturation order for linearized shallow ReLU^k neural networks on the unit sphere, showing they cannot converge faster than n^{-(d+2k+1)/(2d)} for target functions with smoothness r > (d+2k+1)/2.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental limitations of ReLU^k neural network approximation and place linearized neural-network approximation within classical saturation theory, comparing their performance with finite element methods.

Method: Proving a saturation theorem for linearized shallow ReLU^k networks on the unit sphere using antipodally quasi-uniform sets of centers and analyzing approximation rates in L^2(S^d) norm.

Result: The best L^2(S^d) approximation cannot converge faster than order n^{-(d+2k+1)/(2d)} when the target function has smoothness r > (d+2k+1)/2, matching existing upper bounds and establishing the exact saturation order.

Conclusion: ReLU^k networks outperform finite elements under equal degrees k, but this advantage is intrinsically limited by the saturation order (d+2k+1)/(2d), placing linearized neural-network approximation firmly within classical saturation framework.

Abstract: We prove a saturation theorem for linearized shallow ReLU$^k$ neural networks
on the unit sphere $\mathbb S^d$. For any antipodally quasi-uniform set of
centers, if the target function has smoothness $r>\tfrac{d+2k+1}{2}$, then the
best $\mathcal{L}^2(\mathbb S^d)$ approximation cannot converge faster than
order $n^{-\frac{d+2k+1}{2d}}$. This lower bound matches existing upper bounds,
thereby establishing the exact saturation order $\tfrac{d+2k+1}{2d}$ for such
networks. Our results place linearized neural-network approximation firmly
within the classical saturation framework and show that, although ReLU$^k$
networks outperform finite elements under equal degrees $k$, this advantage is
intrinsically limited.

</details>


### [10] [High order well-balanced and total-energy-conserving local discontinuous Galerkin methods for compressible self-gravitating Euler equations](https://arxiv.org/abs/2510.04112)
*Liang Pan,Wei Chen,Jianxian Qiu,Tao Xiong*

Main category: math.NA

TL;DR: A high-order local discontinuous Galerkin scheme for compressible self-gravitating Euler equations that achieves well-balanced property for general polytropic equilibrium and total energy conservation in multiple dimensions.


<details>
  <summary>Details</summary>
Motivation: The compressible self-gravitating Euler equations pose challenges due to time-dependent gravitational potential, requiring schemes that maintain well-balanced properties and energy conservation, especially for high-order accuracy.

Method: Decompose gravitational potential into equilibrium and perturbation parts, use modified HLLC flux and source term discretization for well-balance, rewrite energy equation in conservative form with carefully designed energy flux using DG weak formulation.

Result: The scheme achieves well-balanced property for general polytropic equilibrium states and total energy conservation in multiple spatial dimensions without spherical symmetry assumption, with high-order accuracy in time.

Conclusion: Numerical examples in 2D and 3D verify the scheme's shock-capturing, high-order accuracy, well-balanced property, and total energy conservation capabilities.

Abstract: In this paper, we develop a high order structure-preserving local
discontinuous Galerkin (DG) scheme for the compressible self-gravitating Euler
equations, which pose great challenges due to the presence of time-dependent
gravitational potential. The designed scheme is well-balanced for general
polytropic equilibrium state and total energy conserving for multiple spatial
dimensions without an assumption of spherical symmetry. The well-balanced
property is achieved by decomposing the gravitational potential into
equilibrium and perturbation parts, employing a modified Harten-Lax-van
Leer-contact flux and a modification of the discretization for the source term.
Conservation of total energy is particularly challenging in the presence of
self-gravity, especially when aiming for high order accuracy. To address this,
we rewrite the energy equation into a conservative form, and carefully design
an energy flux with the aid of weak formulation from the DG method to maintain
conservation as well as high order accuracy. The resulting scheme can be
extended to high order in time discretizations. Numerical examples for two and
three dimensional problems are provided to verify the desired properties of our
proposed scheme, including shock-capturing, high order accuracy, well balance,
and total energy conservation.

</details>


### [11] [Bound-Preserving WENO Schemes for Temple-class systems](https://arxiv.org/abs/2510.04123)
*Wei Chen,Shumo Cui,Kailiang Wu,Tao Xiong,Baoyue Yu*

Main category: math.NA

TL;DR: This paper introduces a novel bound-preserving numerical scheme for Temple-class systems using a moving mesh approach and parameterized flux limiter to handle non-convex invariant domains, with applications to traffic flow, elasticity, and sedimentation models.


<details>
  <summary>Details</summary>
Motivation: To address challenges in preserving non-convex invariant domains in numerical simulations of Temple-class systems, which is crucial for avoiding non-physical solutions and ensuring robustness in applications like traffic flow, elasticity, and sedimentation.

Method: Developed local and global bound-preserving methods based on finite difference schemes with a moving mesh approach. Introduced a parameterized flux limiter to restrict high-order fluxes and maintain bound preservation for non-convex sets.

Result: Numerical experiments demonstrated the effectiveness and reliability of the proposed methods. The parameterized flux limiter approach is the first application to non-convex sets, offering significant improvements over traditional methods.

Conclusion: The paper provides a comprehensive method that maintains physical and mathematical constraints of Temple-class systems, with broad applicability to general Temple-class systems and specific models like ARZ traffic flow networks, representing a significant contribution to the field.

Abstract: This paper explores numerical schemes for Temple-class systems, which are
integral to various applications including one-dimensional two-phase flow,
elasticity, traffic flow, and sedimentation. Temple-class systems are
characterized by conservative equations, with different pressure function
expressions leading to specific models such as the Aw-Rascle-Zhang (ARZ)
traffic model and the sedimentation model. Our work extends existing studies by
introducing a moving mesh approach to address the challenges of preserving
non-convex invariant domains, a common issue in the numerical simulation of
such systems. Our study outlines a novel bound-preserving (BP) and conservative
numerical scheme, designed specifically for non-convex sets in Temple-class
systems, which is critical for avoiding non-physical solutions and ensuring
robustness in simulations. We develop both local and global BP methods based on
finite difference schemes, with numerical experiments demonstrating the
effectiveness and reliability of our methods. Furthermore, a parameterized flux
limiter is introduced to restrict high-order fluxes and maintain bound
preservation. This innovation marks the first time such a parameterized
approach has been applied to non-convex sets, offering significant improvements
over traditional methods. The findings presented extend beyond theoretical
implications, as they are applicable to general Temple-class systems and can be
tailored to ARZ traffic flow networks, highlighting the versatility and broad
applicability of our approach. The paper contributes significantly to the field
by providing a comprehensive method that maintains the physical and
mathematical constrains of Temple-class systems.

</details>


### [12] [Robust and efficient solvers for nonlinear partial differential equations based on random feature method](https://arxiv.org/abs/2510.04170)
*Longze Tan*

Main category: math.NA

TL;DR: Proposes two randomized Newton-type solvers (IPN and AMIPN) to efficiently solve large-scale nonlinear PDEs using the random feature method, overcoming computational challenges through preconditioning and adaptive strategies.


<details>
  <summary>Details</summary>
Motivation: The random feature method faces challenges with large 3D nonlinear PDEs, requiring many collocation points and features that lead to extremely large and ill-conditioned systems.

Method: IPN uses randomized Jacobian compression and QR factorization for preconditioning, solved by LSQR with line search. AMIPN reuses preconditioned Jacobians across iterations with adaptive stopping criteria.

Result: Extensive experiments on 3D steady-state and 2D time-dependent PDEs show substantial accuracy improvements and robust convergence compared to classical and recent ML-based approaches.

Conclusion: RFM combined with IPN/AMIPN establishes an efficient framework for large-scale nonlinear PDEs with complex geometries.

Abstract: The random feature method (RFM), a mesh-free machine learning-based
framework, has emerged as a promising alternative for solving PDEs on complex
domains. However, for large three-dimensional nonlinear problems, attaining
high accuracy typically requires domain partitioning with many collocation
points and random features per subdomain, which leads to extremely large and
ill-conditioned nonlinear least-squares systems. To overcome these challenges,
we propose two randomized Newton-type solvers. The first is an inexact Newton
method with right preconditioning (IPN), in which randomized Jacobian
compression and QR factorization are used to construct an efficient
preconditioner that substantially reduces the condition number. Each Newton
step is then approximately solved by LSQR, and a derivative-free line search is
incorporated to ensure residual reduction and stable convergence. Building upon
this framework, we further develop an adaptive multi-step inexact
preconditioned Newton method (AMIPN). In this approach, the preconditioned
Jacobian is reused across multiple inner iterations, while a prescribed maximum
number of inner iterations together with an adaptive early-stopping criterion
determines whether the current preconditioner can be retained in subsequent
outer iterations. These mechanisms effectively avoid redundant computations and
enhance robustness. Extensive numerical experiments on both three-dimensional
steady-state and two-dimensional time-dependent PDEs with complex geometries
confirm the remarkable effectiveness of the proposed solvers. Compared with
classical discretization techniques and recent machine-learning-based
approaches, the methods consistently deliver substantial accuracy improvements
and robust convergence, thereby establishing the RFM combined with IPN/AMIPN as
an efficient framework for large-scale nonlinear PDEs. .

</details>


### [13] [A note on spectral Monte-Carlo method for fractional Poisson equation on high-dimensional ball](https://arxiv.org/abs/2510.04427)
*Lisen Ding,Mingyi Wang,Dongling Wang*

Main category: math.NA

TL;DR: Extension of efficient spectral Monte-Carlo methods from 1D to high-dimensional balls for solving fractional Poisson equations with radial solutions, using new eigenfunctions and interpolation formulas.


<details>
  <summary>Details</summary>
Motivation: Previous methods were limited to one-dimensional formulations, despite achieving spectral accuracy. The goal is to extend these efficient methods to handle radial solutions in general high-dimensional settings.

Method: Employ different set of eigenfunctions for the fractional Laplacian and derive new interpolation formulas to extend the approach from 1D to high-dimensional balls.

Result: Numerical experiments confirm the efficacy of the proposed extension, showing successful application to radial solutions in high-dimensional settings.

Conclusion: The methodology successfully extends efficient spectral Monte-Carlo methods to high-dimensional balls for fractional Poisson equations with radial solutions, providing a comprehensive framework with validated numerical performance.

Abstract: Recently, a class of efficient spectral Monte-Carlo methods was developed in
\cite{Feng2025ExponentiallyAS} for solving fractional Poisson equations. These
methods fully consider the low regularity of the solution near boundaries and
leverage the efficiency of walk-on-spheres algorithms, achieving spectral
accuracy. However, the underlying formulation is essentially one-dimensional.
In this work, we extend this approach to radial solutions in general
high-dimensional balls. This is accomplished by employing a different set of
eigenfunctions for the fractional Laplacian and deriving new interpolation
formulas. We provide a comprehensive description of our methodology and a
detailed comparison with existing techniques. Numerical experiments confirm the
efficacy of the proposed extension.

</details>


### [14] [Some Remarks on Commuting Probability](https://arxiv.org/abs/2510.04458)
*Alexander Kushkuley*

Main category: math.NA

TL;DR: Introduces a weighted sum of irreducible character ratios as an estimator for commutator probabilities, which yields Frobenius formula when applied to regular representation.


<details>
  <summary>Details</summary>
Motivation: To develop an estimator for commutator probabilities using character theory methods.

Method: Uses a weighted sum of irreducible character ratios as the estimator approach.

Result: The estimator produces the Frobenius formula when applied to a regular representation.

Conclusion: The proposed weighted sum of irreducible character ratios serves as an effective estimator for commutator probabilities.

Abstract: We introduce a weighted sum of irreducible character ratios as an estimator
for commutator probabilities. The estimator yields Frobenius formula when
applied to a regular representation

</details>


### [15] [Convergence Analysis of the Random Ordinate Method for Mitigating the Ray Effect](https://arxiv.org/abs/2510.04540)
*Lei Li,Min Tang,Yuqi Yang*

Main category: math.NA

TL;DR: The paper introduces the Random Ordinates Method (ROM) to address the ray effect in Discrete Ordinates Method (DOM) for radiative transport simulations. ROM uses random velocity discretization and averaging of multiple samples to achieve better convergence.


<details>
  <summary>Details</summary>
Motivation: DOM suffers from ray effects when velocity discretization is insufficiently refined. This limitation motivates the development of ROM to mitigate ray effects without increasing computational costs.

Method: ROM partitions velocity space into n cells, selects random ordinates from each cell, and solves DOM systems with these ordinates. It uses averaging of multiple samples to achieve higher convergence order, particularly for solutions with low velocity regularity.

Result: ROM provides detailed convergence analysis focusing on bias and single-run errors. This helps determine optimal mesh size and number of samples needed for desired accuracy.

Conclusion: ROM effectively mitigates ray effects in radiative transport simulations while maintaining computational efficiency through random velocity discretization and sample averaging.

Abstract: The Discrete Ordinates Method (DOM) is widely used for velocity
discretization in radiative transport simulations. However, DOM tends to
exhibit the ray effect when the velocity discretization is not sufficiently
refined, a limitation that is well documented. To counter this, we have
developed the Random Ordinates Method (ROM) by integrating randomness into the
velocity discretization, which mitigates the ray effect without incurring
additional computational costs. ROM partitions the velocity space into n cells,
selects a random ordinate from each cell, and solves a DOM system with these
ordinates. It leverages the average of multiple samples to achieve a higher
convergence order, especially for solutions with low regularity in the velocity
variable. In this work, we provide a detailed convergence analysis for ROM,
focusing on bias and single-run errors. This analysis is crucial for
determining the necessary mesh size and the optimal number of samples required
to attain a specified level of accuracy.

</details>


### [16] [Computational identification of the source domain in an inverse problem of potential theory](https://arxiv.org/abs/2510.04693)
*P. N. Vabishchevich*

Main category: math.NA

TL;DR: The paper addresses the inverse potential problem by identifying the domain containing sources using regularizing algorithms with nonnegativity constraints, solved via Nonnegative Least Squares.


<details>
  <summary>Details</summary>
Motivation: The inverse potential problem is ill-posed due to non-uniqueness and instability with respect to measurement errors, requiring additional assumptions and regularization to obtain stable solutions.

Method: The computational algorithm approximates the volume potential by the single-layer potential on the boundary of the source domain, with residual minimization in the class of nonnegative solutions using Nonnegative Least Squares.

Result: Numerical experiments for a two-dimensional test problem with an analytically prescribed potential demonstrate the capabilities of the proposed approach.

Conclusion: The proposed method effectively solves the inverse potential problem under nonnegativity constraints, providing a stable and computationally feasible solution through regularized optimization.

Abstract: The inverse potential problem consists in determining the density of the
volume potential from measurements outside the sources. Its ill-posedness is
due both to the non-uniqueness of the solution and to the instability of the
solution with respect to measurement errors. The inverse problem is solved
under additional assumptions about the sources using regularizing algorithms.
In this work, an inverse problem is posed for identifying the domain that
contains the sources. The computational algorithm is based on approximating the
volume potential by the single-layer potential on the boundary of the domain
containing the sources. The inverse problem is considered in the class of a
priori constraints of nonnegativity of the potential density. Residual
minimization in the class of nonnegative solutions is performed using the
classical Nonnegative Least Squares algorithm. The capabilities of the proposed
approach are illustrated by numerical experiments for a two-dimensional test
problem with an analytically prescribed potential on the observation surface.

</details>


### [17] [Higher-Order Boundary Conditions for Atomistic Dislocation Simulations](https://arxiv.org/abs/2510.04751)
*Xinyi Wei,Julian Braun,Yangshuai Wang,Lei Zhang*

Main category: math.NA

TL;DR: A higher-order boundary condition method for atomistic dislocation simulations that improves convergence by combining continuum predictors with discrete moment corrections.


<details>
  <summary>Details</summary>
Motivation: To address the slow convergence of standard supercell methods in atomistic simulations of dislocations.

Method: Uses multipole expansion of equilibrium displacement, combining continuum predictor solutions (solved via Galerkin spectral method for singular elliptic PDEs) with discrete moment corrections (evaluated from force-moment identities).

Result: Numerical results for screw and edge dislocations confirm predicted convergence rates in geometry and energy norms, with reduced finite-size effects and moderate computational cost.

Conclusion: The method enables systematically improvable high-order boundary conditions through coupling accurate continuum predictors with moment evaluations.

Abstract: We present a higher-order boundary condition for atomistic simulations of
dislocations that address the slow convergence of standard supercell methods.
The method is based on a multipole expansion of the equilibrium displacement,
combining continuum predictor solutions with discrete moment corrections. The
continuum predictors are computed by solving a hierarchy of singular elliptic
PDEs via a Galerkin spectral method, while moment coefficients are evaluated
from force-moment identities with controlled approximation error. A key feature
is the coupling between accurate continuum predictors and moment evaluations,
enabling the construction of systematically improvable high-order boundary
conditions. We thus design novel algorithms, and numerical results for screw
and edge dislocations confirm the predicted convergence rates in geometry and
energy norms, with reduced finite-size effects and moderate computational cost.

</details>


### [18] [SubApSnap: Solving parameter-dependent linear systems with a snapshot and subsampling](https://arxiv.org/abs/2510.04825)
*Eleanor Jones,Yuji Nakatsukasa*

Main category: math.NA

TL;DR: SubApSnap is a sublinear-time algorithm for solving parameter-dependent linear systems A(p)x(p)=b(p) by combining model order reduction and randomized linear algebra, achieving dramatic speedups of over 20,000× for large problems.


<details>
  <summary>Details</summary>
Motivation: Many computational mathematics problems involve solving many related linear systems that depend smoothly on parameters, which can be computationally expensive when done naively for each parameter value.

Method: Combines model order reduction (snapshot matrix) with randomized linear algebra (subsampling-based dimension reduction) to solve tall-skinny least-squares problems efficiently without reading entire matrices.

Result: SubApSnap achieves speedups of many orders of magnitude (e.g., over 20,000× for 10^7×10^7 problems) while maintaining good accuracy, and is a strict generalization of the DEIM algorithm.

Conclusion: The algorithm provides an efficient solution for parameter-dependent linear systems, requiring only subsampled data to solve for new parameter values, making it particularly valuable for large-scale problems in PDEs, model reduction, and kernel ridge regression.

Abstract: A growing number of problems in computational mathematics can be reduced to
the solution of many linear systems that are related, often depending smoothly
or slowly on a parameter $p$, that is, $A(p)x(p)=b(p)$. We introduce an
efficient algorithm for solving such parameter-dependent linear systems for
many values of $p$. The algorithm, which we call SubApSnap (for
\emph{Sub}sampled $A(p)$ times \emph{Snap}shot), is based on combining ideas
from model order reduction and randomised linear algebra: namely, taking a
snapshot matrix, and solving the resulting tall-skinny least-squares problems
using a subsampling-based dimension-reduction approach. We show that SubApSnap
is a strict generalisation of the popular DEIM algorithm in nonlinear model
order reduction. SubApSnap is a sublinear-time algorithm, and once the snapshot
and subsampling are determined, it solves $A(p_*)x(p_*)=b(p_*)$ for a new value
of $p_*$ at a dramatically improved speed: it does not even need to read the
whole matrix $A(p_*)$ to solve the linear system for a new value of $p_*$. We
prove under natural assumptions that, given a good subsampling and snapshot,
SubApSnap yields solutions with small residual for all parameter values of
interest. We illustrate the efficiency and performance of the algorithm with
problems arising in PDEs, model reduction, and kernel ridge regression, where
SubApSnap achieves speedups of many orders of magnitude over a standard
solution; for example over $20,000\times$ for a $10^7\times 10^7$ problem,
while providing good accuracy.

</details>


### [19] [Efficient structure-preserving scheme for chemotaxis PDEs with singular sensitivity in crime and epidemic modeling](https://arxiv.org/abs/2510.04826)
*Rui Wang,Yunfeng Xiong,Zhengru Zhang*

Main category: math.NA

TL;DR: This paper develops a second-order accurate, positivity-preserving implicit-explicit numerical scheme for chemotaxis PDEs with singular sensitivity, originally used for crime hotspot modeling and extended to epidemic dynamics.


<details>
  <summary>Details</summary>
Motivation: There is an urgent need for stable and accurate numerical schemes to characterize phase transitions, pattern formation, and statistical properties in long-term dynamics of chemotaxis PDEs with singular sensitivity, which face challenges due to positivity constraints and absence of energy functionals.

Method: Constructed an efficient positivity-preserving implicit-explicit scheme with second-order accuracy, using Lagrange multiplier correction to handle singular sensitivity. Extended the framework to multi-agent epidemic models with degenerate diffusion while maintaining positivity and mass conservation.

Result: The proposed scheme successfully handles singular sensitivity through rigorous error estimation with Lagrange multiplier correction. Numerical examples validate theoretical results and demonstrate the necessity of the correction strategy for studying crime hotspots and epidemic dynamics.

Conclusion: The developed scheme enables study of crime hotspot nucleation, spread, and dissipation, and validates that population density clustering accelerates virus transmission in epidemics, potentially aggravating second infectious waves.

Abstract: The system of chemotaxis PDEs with singular sensitivity was originally
proposed by Short et al. [Math. Mod. Meth. Appl. Sci., 18:1249-1267, 2008] as
the continuum limit of a biased random walk model to account for the formation
of crime hotspots and environmental feedback successfully. Recently, this idea
has also been applied to epidemiology to model the impact of human social
behaviors on disease transmission. In order to characterize the phase
transition, pattern formation and statistical properties in the long-term
dynamics, a stable and accurate numerical scheme is urgently demanded, which
still remains challenging due to the positivity constraint on the singular
sensitivity and the absence of an energy functional. To address these numerical
challenges, this paper constructs an efficient positivity-preserving,
implicit-explicit scheme with second-order accuracy. A rigorous error
estimation is provided with the Lagrange multiplier correction to deal with the
singular sensitivity. The whole framework is extended to a multi-agent epidemic
model with degenerate diffusion, in which both positivity and mass conservation
are achieved. Typical numerical examples are conducted to validate our
theoretical results and to demonstrate the necessity of correction strategy.
The proposed scheme allows us to study the nucleation, spread, and dissipation
of crime hotspots, as well as validate that clustering of population density
may accelerate virus transmission in epidemic dynamics and potentially
aggravate the second infectious wave.

</details>


### [20] [The path of hyperinterpolation: A survey](https://arxiv.org/abs/2510.04904)
*Congpei An,Jiashu Ran,Hao-Ning Wu*

Main category: math.NA

TL;DR: Survey of hyperinterpolation methods, covering classical results, examples on domains, recent progress on relaxed quadrature exactness, methodological variants, and applications to differential/integral equations.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of hyperinterpolation as a quadrature-based approximation scheme and its developments.

Method: Survey approach covering classical theory, domain examples, recent advances in relaxed quadrature exactness, and methodological variants.

Result: Comprehensive review of hyperinterpolation methods, their theoretical foundations, and practical implementations across various domains.

Conclusion: Hyperinterpolation is a versatile quadrature-based approximation method with broad applications in differential and integral equations, with ongoing developments in methodology and theory.

Abstract: This paper surveys hyperinterpolation, a quadrature-based approximation
scheme. We cover classical results, provide examples on several domains, review
recent progress on relaxed quadrature exactness, introduce methodological
variants, and discuss applications to differential and integral equations.

</details>


### [21] [Data-driven linear solver selection and performance tuning for multiphysics simulations in porous media](https://arxiv.org/abs/2510.04920)
*Yury Zabegaev,Inga Berre,Eirik Keilegavlen*

Main category: math.NA

TL;DR: Automated selection and tuning of preconditioned linear solvers for multiphysics simulations using machine learning with adaptive refinement during runtime.


<details>
  <summary>Details</summary>
Motivation: Optimizing linear solver choices for multiphysics simulations is challenging due to vast algorithm combinations and changing simulation setups that make past performance data less representative.

Method: Proposes a solver selection algorithm that collects performance data during simulation runtime and continuously updates a machine learning model for adaptive solver selection.

Result: The algorithm selects efficient and robust solvers with negligible overhead, performing comparably to reference policies with full access to prior simulation data.

Conclusion: The approach effectively addresses solver selection challenges, providing value especially when expert knowledge on linear solver tuning is unavailable.

Abstract: Modeling multiphysics processes in porous media requires preconditioned
iterative linear solvers to enable efficient simulations at industry-relevant
scales. These solvers are typically composed of sub-algorithms that target
individual physical processes. Various options are available for each
algorithm, with the corresponding ranges of numerical parameters. The choices
of sub-algorithms and their parameters significantly affects simulation
performance and robustness. Optimizing these choices for each simulation is
challenging due to the vast number of possible combinations. Moreover,
optimization relies on performance data from past simulations, which becomes
less representative as the simulation setup changes. This paper addresses the
problem of automated selection and tuning of preconditioned linear solvers for
multiphysics simulations. The proposed solver selection algorithm collects
performance data during the run of the target simulation and continuously
updates a machine learning model responsible for solver selection, resulting in
an adaptively refined selection policy. The algorithm is evaluated on two
time-dependent nonlinear model problems: (i) coupled fluid flow and heat
transfer in porous media and (ii) thermo-poromechanics in porous media with
fractures, governed by frictional contact mechanics. These experiments
demonstrate that the algorithm selects efficient and robust solvers with
negligible overhead and performs comparably to a reference selection policy
that has full access to the performance data of prior simulations. Our results
indicate that the proposed approach effectively addresses the challenge of
solver selection and tuning, providing particular value to simulation engineers
and researchers, especially when expert knowledge on linear solver tuning is
not readily available.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [22] [Inverse photoacoustic tomography problem in media with fractional attenuation](https://arxiv.org/abs/2510.03408)
*Sebastian Acosta,Benjamin Palacios*

Main category: math.AP

TL;DR: The paper studies inverse source recovery for the wave equation with fractional attenuation, with applications to photoacoustic tomography. It establishes uniqueness under geometric conditions, proves stability via continuity inequalities, and develops a Neumann series-based reconstruction scheme.


<details>
  <summary>Details</summary>
Motivation: Motivated by photoacoustic tomography (PAT), the research aims to solve the inverse problem of recovering initial sources in wave equations with fractional attenuation, which models memory effects in wave propagation.

Method: The approach adapts Carleman estimates to the fractional setting, uses continuity inequalities for fractional time-derivatives of wave solutions to prove stability, and develops a reconstruction scheme based on Neumann series.

Result: Established uniqueness under geometric foliation conditions, proved stability through continuity inequalities, and derived a practical reconstruction scheme using Neumann series.

Conclusion: The results directly apply to PAT and the analytical methods developed are expected to be broadly relevant for PDEs with memory effects and inverse problems for attenuated wave models.

Abstract: We investigate the inverse problem of recovering an initial source for the
wave equation with fractional attenuation, motivated by photoacoustic
tomography (PAT). The attenuation is modeled by a Caputo fractional derivative
of order $\alpha\in(0,1)$. We establish uniqueness under a geometric foliation
condition via an adaptation of two types of Carleman estimates to the
fractional setting, prove stability through continuity inequalities for
fractional time-derivatives of wave solutions, and derive a reconstruction
scheme based on a Neumann series. While our results apply directly to PAT, we
expect that the analytic approach and tools employed might be of broader
relevance to the analysis of PDEs with memory effects and to inverse problems
for attenuated wave models.

</details>


### [23] [Local boundedness for weak solutions to strongly degenerate orthotropic parabolic equations](https://arxiv.org/abs/2510.03412)
*Pasquale Ambrosio,Simone Ciani*

Main category: math.AP

TL;DR: The paper proves local boundedness of weak solutions to a degenerate anisotropic parabolic equation combining orthotropic structure with strong degeneracy.


<details>
  <summary>Details</summary>
Motivation: To extend classical boundedness theorems from the parabolic p-Laplacian to a more complex anisotropic and degenerate setting where the equation has both orthotropic structure and strong degeneracy.

Method: The authors analyze local weak solutions to the given parabolic equation with orthotropic structure and degenerate behavior, using mathematical analysis techniques to prove boundedness properties.

Result: Successfully proved local boundedness of local weak solutions to the degenerate anisotropic parabolic equation, extending classical results to this more complex setting.

Conclusion: The main novelty is combining orthotropic structure with strong degeneracy, and the core result extends classical boundedness theorems to widely degenerate anisotropic settings, with the isotropic counterpart also covered as a byproduct.

Abstract: We prove the local boundedness of local weak solutions to the parabolic
equation \[ \partial_{t}u\,=\,\sum_{i=1}^{n}\partial_{x_{i}}\left[(\vert
u_{x_{i}}\vert-\delta_{i})_{+}^{p-1}\frac{u_{x_{i}}}{\vert
u_{x_{i}}\vert}\right]\,\,\,\,\,\,\,\,\,\,\mathrm{in}\,\,\,\Omega_{T}=\Omega\times(0,T]\,,
\] where $\Omega$ is a bounded domain in $\mathbb{R}^{n}$ with $n\geq2$,
$p\geq2$, $\delta_{1},\ldots,\delta_{n}$ are non-negative numbers and
$\left(\,\cdot\,\right)_{+}$ denotes the positive part. The main novelty here
is that the above equation combines an orthotropic structure with a strongly
degenerate behavior. The core result of this paper thus extends a classical
boundedness theorem, originally proved for the parabolic $p$-Laplacian, to a
widely degenerate anisotropic setting. As a byproduct, we also obtain the local
boundedness of local weak solutions to the isotropic counterpart of the above
equation.

</details>


### [24] [Forward and backward problems for abstract time-fractional Schrödinger equations](https://arxiv.org/abs/2510.03600)
*S. E. Chorfi,F. Et-tahri,L. Maniar,M. Yamamoto*

Main category: math.AP

TL;DR: Analysis of forward and backward problems for abstract time-fractional Schrödinger equations with Caputo derivatives, establishing well-posedness and stability results using eigenvector expansions and Mittag-Leffler function properties.


<details>
  <summary>Details</summary>
Motivation: To model quantum systems with memory effects and anomalous wave propagation using time-fractional Schrödinger equations, which incorporate Caputo derivatives to capture non-local time behavior.

Method: Employ solution's eigenvector expansion combined with properties of Mittag-Leffler functions, including zero distribution and asymptotic expansions, to analyze two cases: ν=1 with α∈(0,1) and ν=α with α∈(0,1)∪(1,2).

Result: Established well-posedness for forward problems in both scenarios and proved well-posedness and stability results for backward problems, with analysis depending on the two cases ν=1 and ν=α.

Conclusion: Successfully analyzed time-fractional Schrödinger equations with memory effects, providing rigorous mathematical foundations for both forward and backward problems, while identifying remaining open problems for future research.

Abstract: We investigate forward and backward problems associated with abstract
time-fractional Schr\"odinger equations $\mathrm{i}^\nu \partial_t^\alpha u(t)
+ A u(t)=0$, $\alpha \in (0,1)\cup (1,2)$ and $\nu\in\{1,\alpha\}$, where $A$
is a self-adjoint operator with compact resolvent on a Hilbert space $H$. This
kind of equation, which incorporates the Caputo time-fractional derivative of
order $\alpha$, models quantum systems with memory effects and anomalous wave
propagation. We first establish the well-posedness of the forward problems in
two scenarios: ($\nu=1,\,$ $\alpha \in (0,1)$) and ($\nu=\alpha,\,$ $\alpha \in
(0,1)\cup (1,2)$). Then, we prove well-posedness and stability results for the
backward problems depending on the two cases $\nu=1$ and $\nu=\alpha$. Our
approach employs the solution's eigenvector expansion along with the properties
of the Mittag-Leffler functions, including the distribution of zeros and
asymptotic expansions. Finally, we conclude with a discussion of some open
problems.

</details>


### [25] [Inverse spectral problems for higher-order coefficients](https://arxiv.org/abs/2510.03708)
*Mourad Choulli*

Main category: math.AP

TL;DR: Uniqueness and stability inequalities for determining higher-order coefficients of elliptic operators from boundary spectral data, with extensions to conductivity and potential determination.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for inverse problems involving elliptic operators, specifically determining coefficients from boundary spectral data.

Method: Analysis based on relationship between boundary spectral data and elliptic/hyperbolic Dirichlet-to-Neumann maps, adapting approach for different coefficient types.

Result: Proved uniqueness and stability inequalities for higher-order coefficients, conductivity, and potential determination from boundary spectral data.

Conclusion: The paper provides mathematical guarantees for coefficient determination in elliptic operators using boundary spectral data, with applications to conductivity and potential recovery.

Abstract: We establish uniqueness and stability inequalities for the problem of
determining the higher-order coefficients of an elliptic operator from the
corresponding boundary spectral data (BSD). Our analysis relies on the
relationship between boundary spectral data and elliptic and hyperbolic
Dirichlet to Neumann (DtN) maps. We also show how to adapt our analysis to
obtain uniqueness and stability inequalities for determining the conductivity
or the potential in an elliptic operator from the corresponding BSD.

</details>


### [26] [Rapid boundary stabilization of 1D nonlinear parabolic equations](https://arxiv.org/abs/2510.03740)
*Yu Xiao,Can Zhang*

Main category: math.AP

TL;DR: Rapid boundary stabilization of 1D nonlinear parabolic equations using modal decomposition method to modify unstable eigenvalues for spectral reduction.


<details>
  <summary>Details</summary>
Motivation: To achieve rapid stabilization of nonlinear parabolic equations through boundary control, addressing both local and global stabilization challenges.

Method: Modal decomposition method with feedback control that modifies unstable eigenvalues, combined with Lyapunov stability analysis and dissipative property utilization.

Result: Established locally rapid stabilization through nonlinearity estimation in Lyapunov analysis, and globally rapid stabilization for dissipative systems like Burgers and Allen-Cahn equations.

Conclusion: The modal decomposition approach successfully achieves both local and global rapid stabilization for 1D nonlinear parabolic equations with appropriate nonlinearity conditions.

Abstract: In this paper, we focus on the rapid boundary stabilization of 1D nonlinear
parabolic equations via the modal decomposition method. The nonlinear term is
assumed to satisfy certain local Lipschitz continuity and global growth
conditions. Through the modal decomposition, we construct a feedback control
that modifies only the unstable eigenvalues to achieve spectral reduction.
Under this control, we establish locally rapid stabilization by estimating the
nonlinearity in Lyapunov stability analysis. Furthermore, utilizing the
dissipative property, we derive a globally rapid stabilization result for
dissipative systems such as the Burgers equation and the Allen-Cahn equation.

</details>


### [27] [Gamma Convergence of Partially Segregated Elliptic Systems](https://arxiv.org/abs/2510.03794)
*Farid Bozorgnia,Avetik Arakelyan*

Main category: math.AP

TL;DR: Gamma convergence of penalized energy to constrained Dirichlet energy with strict segregation for partially segregated elliptic systems in 2D.


<details>
  <summary>Details</summary>
Motivation: Study multi-component ultracold gas mixtures and systems with multiple interacting fluid/gas species through Gross-Pitaevskii-type energies.

Method: Use penalized energy functionals and combine lower semicontinuity arguments with recovery sequence construction based on geometric decompositions near interfaces and triple junctions.

Result: Established Gamma convergence in planar domains (R^2) from penalized energy to constrained Dirichlet energy with strict segregation.

Conclusion: Provides rigorous variational link between penalized and constrained formulations for partially segregated elliptic systems.

Abstract: We study partially segregated elliptic systems through the use of penalized
energy functionals. These systems arise from the minimization of
Gross-Pitaevskii-type energies that capture the behavior of multi-component
ultracold gas mixtures and other systems involving multiple interacting fluid
or gas species. In the case when the domain is planar, i.e., in $\mathbb{R}^2$,
our main result is the Gamma convergence of penalized energy to the constrained
Dirichlet energy with strict segregation. The proof combines lower
semicontinuity arguments with a recovery sequence construction based on
geometric decompositions near interfaces and triple junctions. This establishes
a rigorous variational link between the penalized and constrained formulations.

</details>


### [28] [The non-cutoff Vlasov-Poisson-Boltzmann system with weak collisions](https://arxiv.org/abs/2510.03835)
*Yuanjie Lei,Shuangqian Liu,Qinghua Xiao,Huijiang Zhao*

Main category: math.AP

TL;DR: Global existence of smooth solutions near Maxwellians for non-cutoff Vlasov-Poisson-Boltzmann system in weakly collisional regime, establishing Landau damping for density and electric field.


<details>
  <summary>Details</summary>
Motivation: To address the weak dissipation of the non-cutoff linearized Boltzmann operator and establish the first global-in-time result for the non-cutoff Vlasov-Poisson-Boltzmann system.

Method: Developed a refined velocity-weighted energy framework combined with vector-field techniques to control transport term, nonlinear collisions, and self-consistent electric field.

Result: Proved global existence of smooth solutions near Maxwellians, obtained uniform-in-time bounds, captured enhanced dissipation, and established Landau damping for both density and electric field.

Conclusion: Successfully established the first global-in-time result for the non-cutoff Vlasov-Poisson-Boltzmann system with Landau damping effects.

Abstract: We prove global existence of smooth solutions near Maxwellians for the
non-cutoff Vlasov-Poisson-Boltzmann system in the weakly collisional regime. To
address the weak dissipation of the non-cutoff linearized Boltzmann operator,
we develop a refined velocity-weighted energy framework combined with
vector-field techniques to control the transport term, nonlinear collisions,
and the self-consistent electric field. This approach yields uniform-in-time
bounds, captures enhanced dissipation of the solution, and establishes Landau
damping for both the density and electric field, providing the first
global-in-time result of this type for the non-cutoff Vlasov-Poisson-Boltzmann
system. Our approach is inspired by the recent work of Chaturvedi-Luk-Nguyen
({\it J. Amer. Math. Soc.} {\bf 36} (2023), no. 4, 1103--1189.)

</details>


### [29] [On the existence of fibered three-dimensional perfect fluid equilibria without continuous Euclidean symmetry](https://arxiv.org/abs/2510.02955)
*Theodore D. Drivas,Tarek M. Elgindi,Daniel Ginsberg*

Main category: math.AP

TL;DR: Construction of 3D steady Euler flows with no continuous symmetry but discrete m-fold symmetry, contradicting Grad's conjecture about pressure surface fibering.


<details>
  <summary>Details</summary>
Motivation: To challenge Grad's conjecture that only solutions with continuous symmetry can be fibered by pressure surfaces, by constructing counterexamples.

Method: Following Lortz's approach but with discrete m-fold symmetry, constructing smooth steady states of ideal incompressible Euler equations in 3D with planar reflection symmetry.

Result: Successfully created a family of steady states with no continuous Euclidean symmetry but possessing discrete m-fold symmetry and invariant cylindrical level sets of non-degenerate Bernoulli pressure.

Conclusion: These examples narrow the validity scope of Grad's conjecture, showing that pressure surface fibering can occur without continuous symmetry.

Abstract: Following Lortz, we construct a family of smooth steady states of the ideal,
incompressible Euler equation in three dimensions that possess no continuous
Euclidean symmetry. As in Lortz, they do possess a planar reflection symmetry
and, as such, all the orbits of the velocity are closed. Different from Lortz,
our construction has a discrete m-fold symmetry and is foliated by invariant
cylindrical level sets of a non-degenerate Bernoulli pressure. Such examples
narrow the scope of validity of Grad's conjecture that the only solutions with
a continuous symmetry can be fibered by pressure surfaces.

</details>


### [30] [Partial regularity for parabolic systems of double phase type](https://arxiv.org/abs/2510.03849)
*Jihoon Ok,Giovanni Scilla,Bianca Stroffolini*

Main category: math.AP

TL;DR: The paper proves partial regularity for parabolic systems of double phase type, showing that under certain conditions on the exponents p and q, the spatial gradient of weak solutions is locally Hölder continuous except on a measure zero set.


<details>
  <summary>Details</summary>
Motivation: To establish regularity results for parabolic systems with double phase growth conditions, extending previous work on elliptic systems to the parabolic case and determining optimal conditions for Hölder continuity of gradients.

Method: The authors study weak solutions to nondegenerate parabolic systems with growth function H(z,s)=s^p+a(z)s^q, where a(z) is a nonnegative Hölder continuous function. They use techniques from partial regularity theory for parabolic systems.

Result: The main result shows that if q < min{p + αp/(n+2), p+1}, then the spatial gradient of any weak solution is locally Hölder continuous except on a set of measure zero.

Conclusion: The paper provides optimal conditions for partial regularity in parabolic systems of double phase type, establishing Hölder continuity of gradients under the specified range of exponents p and q.

Abstract: We study partial regularity for nondegenerate parabolic systems of double
phase type, where the growth function is given by $H(z,s)=s^p+a(z)s^q$,
$z=(x,t)\in\Omega_T$, with $\tfrac{2n}{n+2}<p\le q$ and $a(z)$ a nonnegative
$C^{0,\alpha,\frac{\alpha}{2}}$-continuous function for some $\alpha\in(0,1]$.
As the main result we prove that if $q< \min \{p+\tfrac{\alpha p }{n+2}, p+1
\}$ the spatial gradient of any weak solution is locally H\"older continuous,
except on a set of measure zero.

</details>


### [31] [Local Well-Posedness For Barotropic Compressible Fluid-Viscoelastic Shell Interactions](https://arxiv.org/abs/2510.03936)
*Pierre Marie Ngougoue Ngougoue*

Main category: math.AP

TL;DR: Local strong well-posedness for 3D compressible Navier-Stokes flow interacting with viscoelastic shell using Eulerian approach with Hanzawa transform, without vanishing initial displacement.


<details>
  <summary>Details</summary>
Motivation: To establish local strong well-posedness for compressible fluid-shell interaction, extending beyond beams and plates to bending shells, addressing wave-to-bending direction in fluid-structure interaction literature.

Method: Eulerian analysis using localized Hanzawa transform near shell patch, combining well-posedness theory for continuity equation with momentum-structure subproblem analysis via linearisation-energy estimate and fixed-point scheme on fixed reference domain.

Result: Proved local-in-time existence and uniqueness of strong solutions for compatible data, with viscosity-weighted energy yielding bounds that grow linearly in viscosity parameters and are inviscid-limit compatible.

Conclusion: Provides local strong well-posedness complementing global weak theory, extends strong boundary results to bending shells, and addresses wave-to-bending direction while remaining fully Eulerian.

Abstract: We study a three-dimensional barotropic compressible Navier-Stokes flow
interacting with a viscoelastic shell that occupies a portion of the fluid
boundary. The analysis is entirely Eulerian and the moving interface is
parametrised by a localised Hanzawa transform supported near the shell patch,
which preserves the transport structure of the continuity equation and avoids a
global Lagrangian map. We prove local-in-time existence and uniqueness of
strong solutions for compatible data and without imposing a vanishing initial
shell displacement. The proof combines a well-posedness theory for the
continuity equation, solved by the method of characteristics in the Hanzawa
frame, with an analysis of the momentum-structure subproblem carried out by the
classical linearisation-energy estimate -- fixed-point scheme on a fixed
reference domain. A Banach fixed point then couples the two steps and closes
the argument on a short time interval. We work with a viscosity-weighted energy
that makes the scaling in the shear and bulk viscosities explicit. This yields
bounds whose constants grow at most linearly in these parameters. In
particular, the estimates are inviscid-limit compatible. The result complements
the global finite-energy weak theory for compressible fluid-shell interaction
by providing a local strong well-posedness statement in the same geometric
configuration. It also extends strong boundary results beyond beams and plates
to bending shells, and addresses the wave-to-bending direction suggested in the
literature on compressible fluid-structure interaction at structural
boundaries, while remaining fully Eulerian.

</details>


### [32] [Abnormal boundary decay for the fractional Laplacian](https://arxiv.org/abs/2510.03961)
*Soobin Cho,Renming Song*

Main category: math.AP

TL;DR: The paper proves that C¹, Dini regularity is the optimal boundary condition for standard boundary decay properties of α-harmonic functions and fractional heat kernels.


<details>
  <summary>Details</summary>
Motivation: To determine the minimal boundary regularity required for standard boundary decay properties in fractional Laplacian problems, specifically identifying whether C¹, Dini regularity is necessary and sufficient.

Method: The authors prove three main results: (i) boundary decay rate for α-harmonic functions in C¹, Dini domains, (ii) boundary decay rate for fractional heat kernels in C¹, Dini domains, and (iii) counterexamples using non-Dini modulus of continuity functions to show the necessity of C¹, Dini condition.

Result: C¹, Dini regularity is both sufficient and necessary for the standard boundary decay properties - functions decay at rate dist(x, D^c)^{α/2} near the boundary. Without C¹, Dini regularity, these decay properties fail.

Conclusion: The C¹, Dini boundary regularity assumption is optimal for boundary decay properties of α-harmonic functions and fractional heat kernels, establishing a sharp threshold for these analytical properties.

Abstract: In this paper, we show that, for $\alpha \in (0,2)$, the $C^{1, \rm Dini}$
regularity assumption on an open set $D\subset \mathbb R^d$ is optimal for the
standard boundary decay property for nonnegative $\alpha$-harmonic functions in
$D$ and for the standard boundary decay property of the heat kernel
$p^D(t,x,y)$ of the Dirichlet fractional Laplacian $\Delta^{\alpha/2}|_D$ by
proving the following: (i) If $D$ is a $C^{1, \rm Dini}$ open set and $h$ is a
non-negative function which is $\alpha$-harmonic in $D$ and vanishes near a
portion of $\partial D$, then the rate at which $h(x)$ decays to 0 near that
portion of $\partial D$ is ${\rm dist} (x, D^c)^{\alpha/2}$. (ii) If $D$ is a
$C^{1, \rm Dini}$ open set, then, as $x\to \partial D$, the rate at which
$p^D(t,x,y)$ tends to 0 is ${\rm dist} (x, D^c)^{\alpha/2}$. (iii) For any
non-Dini modulus of continuity $\ell$, there exist non-$C^{1, \rm Dini}$ open
sets $D$, with $\partial D$ locally being the graph of a $C^{1, \ell}$
function, such that the standard boundary decay properties above do not hold
for $D$.

</details>


### [33] [Long time evolution of a pair of 2D viscous point vortices](https://arxiv.org/abs/2510.03991)
*Ping Zhang,Yibin Zhang*

Main category: math.AP

TL;DR: This paper analyzes the long-term evolution of two point vortices in 2D Navier-Stokes equations, deriving asymptotic expansions for vorticity over extended time scales and introducing pseudo-momenta as key innovations.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time behavior of two point vortices under 2D Navier-Stokes equations, particularly over time scales longer than advection but shorter than diffusion, building on previous research.

Method: Constructs approximate solutions Ω_a, employs Arnold's method to define nonlinear energy functional E_ε[ω], and introduces pseudo-momenta (ϱ^e_a, ϱ^o_a, ϱ^{te}_a, ϱ^{to}_a) derived from the Lie structure of 2D Euler equations.

Result: Develops asymptotic expansion for vorticity evolution and establishes that the linearized operator Λ^{E,⋆} around approximate solutions is nearly skew-adjoint with respect to the energy functional.

Conclusion: The introduction of pseudo-momenta provides a novel approach to analyzing vortex dynamics, leveraging the underlying Lie structure of 2D Euler equations for improved understanding of long-term vortex evolution.

Abstract: This paper studies the long-time evolution of two point vortices under the 2D
Navier-Stokes tokes equations. Starting from initial data given by a pair of
Dirac measures, we derive an asymptotic expansion for the vorticity over time
scales significantly longer than the advection time, yet shorter than the
diffusion time. Building on previous works \cite{GS24-1, DG24}, we construct
suitable approximate solutions $\Omega_a$ and employ Arnold's method to define
a nonlinear energy functional $E_\ve[\om]$, with respect to which the
linearized operator $\Lambda^{E,\star}$ around $\Omega_a$ is nearly
skew-adjoint. A key innovation in this work is the introduction of
``pseudo-momenta'': $\varrho^e_a, \varrho^o_a,\varrho^{te}_a, \varrho^{to}_a$,
which correspond to eigenfunctions or other nontrivial elements in invariant
subspaces of $\Lambda^E$, derived from the Lie structure of the 2D Euler
equations.

</details>


### [34] [Two alternative proofs of weak Harnack inequality for mixed local and nonlocal $p$-Laplace equations with a nonhomogeneity](https://arxiv.org/abs/2510.04065)
*Prashanta Garain*

Main category: math.AP

TL;DR: The paper studies mixed local and nonlocal p-Laplace equations and provides sufficient conditions on f to prove weak Harnack inequality for sign-changing supersolutions, presenting two novel proofs that avoid traditional methods.


<details>
  <summary>Details</summary>
Motivation: To establish Harnack inequalities for mixed local-nonlocal p-Laplace equations, which are important for understanding regularity properties of solutions to these partial differential equations.

Method: Two different proof approaches: one using the John-Nirenberg lemma and another using the Bombieri-Giusti lemma, both avoiding the Krylov-Safonov covering lemma and expansion of positivity. Also develops local boundedness results and tail estimates.

Result: Proves weak Harnack inequality with tail term for sign-changing supersolutions, establishes Harnack inequality for solutions, provides new proof of reverse Hölder inequality for supersolutions.

Conclusion: The paper presents novel approaches to proving Harnack inequalities for mixed local-nonlocal p-Laplace equations, offering alternative methods that work even for the classical case p=2 with f=0.

Abstract: We study a class of mixed local and nonlocal $p$-Laplace equations with
prototype \[ -\Delta_p u + (-\Delta_p)^s u = f \quad \text{in } \Omega, \]
where $\Omega \subset \mathbb{R}^n$ is bounded and open. We provide sufficient
condition on $f$ to ensure weak Harnack inequality with a tail term for
sign-changing supersolutions. Two different proofs are presented, avoiding the
Krylov--Safonov covering lemma and expansion of positivity: one via the
John--Nirenberg lemma, the other via the Bombieri--Giusti lemma. To our
knowledge, these approaches are new, even for $p = 2$ with $f \equiv 0$, and
include a new proof of the reverse H\"older inequality for supersolutions.
Further, we establish Harnack inequality for solutions by first deriving a
local boundedness result, together with a tail estimate and an initial weak
Harnack inequality.

</details>


### [35] [Negative Order Bochner-Riesz Operators for the Critical Magnetic Schrödinger Operator in $\mathbb{R}^2$](https://arxiv.org/abs/2510.04082)
*Huanqing Guo,Junyong Zhang,Jiqiang Zheng*

Main category: math.AP

TL;DR: This paper characterizes the sharp L^p-L^q boundedness of the Bochner-Riesz operator associated with a magnetic Schrödinger operator on R^2, identifying the boundedness region as a pentagonal subset in the (1/p, 1/q)-plane.


<details>
  <summary>Details</summary>
Motivation: To extend previous uniform resolvent results and determine precise conditions for the boundedness of Bochner-Riesz operators associated with magnetic Schrödinger operators.

Method: Studying the sharp L^p-L^q boundedness of the Bochner-Riesz operator S^δ_λ(L_A) for δ ∈ (-3/2, 0) associated with a scaling-critical magnetic Schrödinger operator on R^2.

Result: The boundedness region is characterized as a pentagonal subset Δ(δ) of the (1/p, 1/q)-plane, extending previous uniform resolvent results.

Conclusion: The paper successfully determines the sharp conditions on exponents p and q for the boundedness of the Bochner-Riesz operator associated with magnetic Schrödinger operators on R^2.

Abstract: This paper studies the sharp $L^p$-$L^q$ boundedness of the Bochner-Riesz
operator $S^{\delta}_{\lambda}(\mathcal{L}_{\mathbf{A}})$ associated with a
scaling-critical magnetic Schr\"odinger operator $\mathcal{L}_{\mathbf{A}}$ on
$\mathbb{R}^2$, where $\delta \in (-3/2, 0)$. We determine the conditions on
the exponents $p$ and $q$ under which the operator is bounded from
$L^p(\mathbb{R}^2)$ to $L^q(\mathbb{R}^2)$. Our main result characterizes the
boundedness region as a pentagonal subset $\Delta(\delta)$ of the $(1/p,
1/q)$-plane, extending previous uniform resolvent result in Fanelli, Zhang and
Zheng[Int. Math. Res. Not., 20(2023), 17656-17703].

</details>


### [36] [Entropy-energy solutions for Thermo-Visco-Elastic systems with Mróz-type inelastic behavior](https://arxiv.org/abs/2510.04152)
*Tomasz Cieślak,Sebastian Owczarek,Karolina Wielgos*

Main category: math.AP

TL;DR: Global existence of weak entropy-energy solutions for a thermodynamically consistent thermo-visco-elastic model with temperature-dependent elastic and inelastic constitutive relations.


<details>
  <summary>Details</summary>
Motivation: To study a thermodynamically consistent thermo-visco-elastic model that describes internal energy balance in heat-conducting inelastic bodies, where temperature dependence appears in both elastic and inelastic constitutive relations.

Method: Introduce the concept of weak entropy-energy solutions that satisfy the entropy equality instead of the internal energy equation, and prove existence results despite the model lacking mathematically favorable structural properties.

Result: Prove the global-in-time existence of weak entropy-energy solutions for large initial data, even without Kelvin-Voigt type effects or simplifications eliminating temperature from constitutive relations.

Conclusion: The proposed weak entropy-energy solution concept enables rigorous mathematical analysis of thermodynamically consistent thermo-visco-elastic models with temperature-dependent constitutive relations, establishing global existence results for challenging physical systems.

Abstract: In this article, we study a thermodynamically consistent thermo-visco-elastic
model describing the balance of internal energy in a heat-conducting inelastic
body. In the considered problem, the temperature dependence appears in both the
elastic and inelastic constitutive relations. For such a system, we introduce
the concept of a weak entropy-energy solution, which satisfies the entropy
equality instead of the internal energy equation. Although the model does not
possess any mathematically favorable structural properties, such as
Kelvin-Voigt type effects or simplifications eliminating temperature from the
constitutive relations, we prove the global-in-time existence of weak
entropy-energy solutions for large initial data.

</details>


### [37] [Mass concentration in a spatially inhomogeneous coagulation model with fast sedimentation](https://arxiv.org/abs/2510.04270)
*Iulia Cristian,Juan J. L. Velázquez*

Main category: math.AP

TL;DR: Study of a spatially inhomogeneous coagulation model with transport term modeling gravity-induced particle motion. Local existence proved for mass-conserving solutions, and global existence insights obtained by allowing fast sedimentation speed.


<details>
  <summary>Details</summary>
Motivation: To understand how to prove global existence of solutions for coagulation models with transport terms, particularly in cases where instantaneous gelation occurs in the homogeneous case.

Method: Analysis of a spatially inhomogeneous coagulation model with transport term. For very fast sedimentation speed, rigorous proof that solutions converge to a Dirac measure in space variable, and formal derivation of a one-dimensional coagulation equation with diagonal kernel.

Result: For very fast sedimentation speed, solutions converge to a Dirac measure in the spatial variable. In the limit, a one-dimensional coagulation equation with diagonal kernel emerges, where only particles of the same size interact.

Conclusion: The study provides physical intuition on how coagulation models with diagonal kernels emerge from spatially inhomogeneous models with fast sedimentation, offering insights for proving global existence of solutions.

Abstract: We study a spatially inhomogeneous coagulation model that contains a
transport term in the spatial variable. The transport term models the vertical
motion of particles due to gravity, thereby incorporating their fall into the
dynamics. Local existence of mass-conserving solutions for a class of
coagulation rates for which in the spatially homogeneous case instantaneous
gelation (i.e., instantaneous loss of mass) occurs has been proved in
[Cristian-Niethammer-Vel\'azquez, 2024]. In order to obtain some insight into
how to prove global existence of solutions, we allow a fast sedimentation
speed. For very fast sedimentation speed, we rigorously prove that solutions
converge to a Dirac measure in the space variable. We also formally obtain in
the limit a one-dimensional coagulation equation with diagonal kernel, i.e.,
only particles of the same size interact. This provides a physical intuition on
how coagulation models with a diagonal kernel emerge.

</details>


### [38] [Existence and qualitative behavior of solutions of abstract differential-algebraic equations](https://arxiv.org/abs/2510.04433)
*Maria Filipkovska*

Main category: math.AP

TL;DR: Study of semilinear abstract differential-algebraic equations (ADAEs) focusing on existence, uniqueness, and solution behavior including global solvability, boundedness, and blow-up.


<details>
  <summary>Details</summary>
Motivation: To establish comprehensive theoretical foundations for semilinear ADAEs with arbitrarily high index characteristic pencils, extending previous work on reduced systems.

Method: Reduce ADAEs to explicit differential and algebraic equations using projectors, with system size determined by the index of the characteristic pencil.

Result: Proved theorems on existence/uniqueness of solutions, maximal existence intervals, global solvability, boundedness, and blow-up behavior for high-index ADAEs.

Conclusion: Established complete theoretical framework for semilinear ADAEs with arbitrary index, providing fundamental results on solution properties and behavior.

Abstract: Abstract differential-algebraic equations (ADAEs) of a semilinear type are
studied. Theorems on the existence and uniqueness of solutions and the maximal
interval of existence, on the global solvability of the ADAEs, the boundedness
of solutions and the blow-up of solutions are presented. Previously, an ADAE is
reduced to a system of explicit differential equations and algebraic equations
by using projectors. The number of equations of the system depends on the index
of the characteristic pencil of the ADAE. We consider the pencil of an
arbitrarily high index.

</details>


### [39] [Non-Monotone Traveling Waves of the Weak Competition Lotka-Volterra System](https://arxiv.org/abs/2510.04501)
*Chiun-Chuan Chen,Ting-Yang Hsiao,Shun-Chieh Wang*

Main category: math.AP

TL;DR: Existence of traveling wave solutions in two-species Lotka-Volterra competition system under weak competition, including non-monotone waves and front-pulse waves.


<details>
  <summary>Details</summary>
Motivation: To establish existence of traveling waves in weak competition regime and provide explicit conditions for non-monotone waves, which were not addressed in previous studies.

Method: Construct refined upper and lower solutions combined with Schauder fixed point theorem.

Result: Traveling waves exist for all wave speeds s ≥ s* = max{2,2√(ad)}, with sufficient conditions for non-monotone waves. Front-pulse traveling waves exist in critical weak competition case.

Conclusion: Successfully established existence of traveling waves in weak competition regime, including non-monotone waves and front-pulse waves, filling gaps in previous research.

Abstract: We investigate traveling wave solutions in the two-species reaction-diffusion
Lotka-Volterra competition system under weak competition. For the strict weak
competition regime $(b<a<1/c,\,d>0)$, we construct refined upper and lower
solutions combined with the Schauder fixed point theorem to establish the
existence of traveling waves for all wave speeds $s\geq
s^*:=\max\{2,2\sqrt{ad}\}$, and provide verifiable sufficient conditions for
the emergence of non-monotone waves. Such conditions for non-monotonic waves
have not been explicitly addressed in previous studies. It is interesting to
point out that our result for non-monotone waves also hold for the critical
speed case $s=s^*$. In addition, in the critical weak competition case
$(b<a=1/c,\,d>0)$, we rigorously prove, for the first time, the existence of
front-pulse traveling waves.

</details>


### [40] [Inverse curvature flow of closed Legendre curves](https://arxiv.org/abs/2510.04566)
*Takashi Kagaya,Masatomo Takahashi*

Main category: math.AP

TL;DR: Analysis of inverse curvature flow for ℓ-convex Legendre curves, studying global existence, cusp behavior, and asymptotic convergence to self-similar solutions.


<details>
  <summary>Details</summary>
Motivation: To generalize classical inverse curvature flow from regular curves to Legendre curves, which are natural generalizations of regular curves, and understand their geometric evolution properties.

Method: Study the initial value problem for inverse curvature flow of ℓ-convex Legendre curves, analyzing global existence, monotonicity of singular cusps over time, and asymptotic behavior as t→∞.

Result: The flow exists globally, the number of singular cusps decreases monotonically with time, and the flow asymptotically converges to self-similar solutions after appropriate scaling, with convergence behavior fully categorized by initial curve properties.

Conclusion: Inverse curvature flow for Legendre curves exhibits well-behaved global dynamics with predictable cusp evolution and complete classification of asymptotic convergence to self-similar solutions based on initial conditions.

Abstract: In this paper, we deal with an inverse curvature flow of $\ell$-convex
Legendre curves. Since the Legendre curve is a natural generalization of
regular curve, the flow is a generalization of the classical inverse curvature
flow of regular curves. For the initial value problem, we study on the unique
existence of the flow in global time, the monotonicity of the number of the
singular cusps with respect to t > 0 and the asymptotic behavior of the flow as
$t \to \infty$. Regarding the asymptotics, the flow asymptotically converges to
one of the self similar solutions by scaling appropriately, and the convergence
is completely categorized depending on the initial curve.

</details>


### [41] [A New Quasi-Singularity Formation Mechanism for Second-order Hyperbolic Equations](https://arxiv.org/abs/2510.04614)
*Huaian Diao,Xieling Fan,Hongyu Liu*

Main category: math.AP

TL;DR: The paper studies quasi-singularity formation in hyperbolic wave equations, showing that over any finite time, there exist inputs causing arbitrarily large Hölder norms, while the set of such blowup points has vanishing measure.


<details>
  <summary>Details</summary>
Motivation: To understand intermediate states between classical singularity and regularity in wave equations, and investigate how quasi-singularities can arise from smooth coefficients and inputs.

Method: Analysis of linear and nonlinear hyperbolic wave equations in 2D and 3D, proving existence of inputs causing arbitrarily large Hölder norms over finite time intervals.

Result: Proved that quasi-singular states exist where wave field Hölder norms can exceed any bound, but the set of such blowup points has vanishing measure as the bound increases.

Conclusion: Quasi-singularity is an intrinsic phenomenon in hyperbolic wave equations that arises from equation structure and specific input characteristics, even with smooth coefficients and inputs.

Abstract: This paper investigates a novel mechanism for quasi-singularity formation in
both linear and nonlinear hyperbolic wave equations in two and three
dimensions. We prove that over any finite time interval, there exist inputs
such that the H\"older norm of the resulting wave field exceeds any prescribed
bound. Conversely, the set of such almost-blowup points has vanishing measure
when the aforementioned bound goes to infinity. This phenomenon thus defines a
quasi-singular state, intermediate between classical singularity and
regularity. Crucially, both the equation coefficients and the inputs can be
arbitrarily smooth; the quasi-singularity arises intrinsically from the
structure of the hyperbolic wave equation combined with specific input
characteristics.

</details>


### [42] [Relaxation of quasi-convex functionals with variable exponent growth](https://arxiv.org/abs/2510.04672)
*Giacomo Bertazzoni,Petteri Harjulehto,Peter Hästö,Elvira Zappale*

Main category: math.AP

TL;DR: Relaxation result for quasi-convex bulk integral functional with variable exponent growth in BV-type spaces, using energy decomposition into absolutely continuous and singular parts weighted by recession function.


<details>
  <summary>Details</summary>
Motivation: To establish relaxation results for integral functionals with variable exponent growth in spaces of bounded variation, which are important in calculus of variations and partial differential equations.

Method: Prove relaxation by decomposing the energy functional into absolutely continuous and singular parts weighted through a recession function under mild assumptions.

Result: Successfully proved the relaxation result for quasi-convex bulk integral functional with variable exponent growth in bounded variation type spaces.

Conclusion: The relaxation approach using energy decomposition with recession function weighting provides an effective framework for handling variable exponent growth functionals in BV-type spaces.

Abstract: We prove a relaxation result for a quasi-convex bulk integral functional with
variable exponent growth in a suitable space of bounded variation type. A key
tool is a decomposition under mild assumptions of the energy into absolutely
continuous and singular parts weighted via a recession function.

</details>


### [43] [Asymptotic behaviour and existence of positive solutions for mixed local nonlocal elliptic equations with Hardy potential](https://arxiv.org/abs/2510.04763)
*Shammi Malhotra,Sarika Goyal,K. Sreenadh*

Main category: math.AP

TL;DR: The paper studies positive solutions to a mixed Laplacian-fractional Laplacian equation with Hardy potential, establishing asymptotic estimates and proving existence/multiplicity results depending on the exponent p.


<details>
  <summary>Details</summary>
Motivation: To investigate the existence and multiplicity of positive solutions for a nonlinear elliptic problem combining classical and fractional Laplacians with Hardy potential, which arises in various physical applications.

Method: Uses uniform asymptotic estimates via suitable transformations and analyzes three distinct cases based on exponent p values (sublinear, critical, superlinear). Employs variational methods and perturbation techniques.

Result: Proves existence of positive solutions for all three p-regimes. In sublinear case (1 < p < 2), demonstrates multiple positive solutions for small perturbations of the fractional Laplacian.

Conclusion: The mixed Laplacian-fractional Laplacian operator with Hardy potential admits positive solutions, with multiplicity occurring in the sublinear regime under small fractional perturbations.

Abstract: We investigate the existence and multiplicity of positive solutions to the
following problem driven by the superposition of the Laplacian and the
fractional Laplacian with Hardy potential \begin{equation*} \left\{
\begin{aligned}
  -\Delta u + (-\Delta)^s u - \mu \frac{u}{|x|^2} &= \lambda |u|^{p-2} u +
|u|^{2^*-2} u \quad \text{in } \Omega \subset \mathbb{R}^N,
  u &= 0 \quad \text{in } \mathbb{R}^N \setminus \Omega, \end{aligned} \right.
\end{equation*} where $ \Omega \subset \mathbb{R}^N $ is a bounded domain with
smooth boundary, $ 0 < s < 1 $, $ 1 < p < 2^* $, with $ 2^* = \frac{2N}{N-2} $,
$ \lambda > 0 $, and $ \mu \in (0, \bar{\mu}) $ where $\bar \mu = \left(
\frac{N-2}{2} \right)^2$.
  The aim of this paper is twofold. First, we establish uniform asymptotic
estimates for solutions of the problem by means of a suitable transformation.
Then, according to the value of the exponent $p$, we analyze three distinct
cases and prove the existence of a positive solution. Moreover, in the
sublinear regime $1 < p < 2$, we demonstrate the existence of multiple positive
solutions for small perturbations of the fractional Laplacian.

</details>


### [44] [Thermal effects in fluid structure interactions](https://arxiv.org/abs/2510.04801)
*Sourav Mitra,Sebastian Schwarzacher*

Main category: math.AP

TL;DR: Existence of weak solutions for two heat-conducting fluids separated by a nonlinear elastic Koiter shell, with varying transmission laws from insulation to superconductivity.


<details>
  <summary>Details</summary>
Motivation: To model and analyze the interaction between two heat-conducting fluids separated by an elastic shell, where the shell's motion affects the fluid domains and heat transmission properties.

Method: Variational approach for fluid-structure interactions using a novel two-step minimization scheme to incorporate temperature effects.

Result: Demonstrated existence of weak solutions that are energetically closed and include strictly positive temperature.

Conclusion: Successfully established the existence of weak solutions for this complex fluid-structure interaction problem with heat conduction and varying transmission properties.

Abstract: In this article we consider two different heat conducting fluids each
modelled by the incompressible Navier-Stokes-Fourier system separated by a
non-linear elastic Koiter shell. The motion of the shell changes the domain of
definition of the two separated fluids. For this setting we show the existence
of a weak solution. The heat capacity of the shell is given energetically. It
allows to consider transmission laws ranging from insulation to
superconductivity.We follow a variational approach for fluid-structure
interactions. To include temperature a novel two step minimization scheme is
used to produce an approximation. The weak solutions are energetically closed
and include a strictly positive temperature.

</details>


### [45] [On the eigenvalues of the biharmonic operator on annuli](https://arxiv.org/abs/2510.04809)
*Davide Buoso,Riccardo Molinarolo*

Main category: math.AP

TL;DR: The paper analyzes the simplicity of the fundamental tone of the bilaplacian with Dirichlet or Navier boundary conditions on radially symmetric domains.


<details>
  <summary>Details</summary>
Motivation: To understand the spectral properties of the bilaplacian operator on symmetric domains, particularly whether the fundamental tone is simple or degenerate.

Method: Mathematical analysis of the bilaplacian operator with Dirichlet or Navier boundary conditions on radially symmetric domains, examining different dimensions.

Result: In dimensions N≥3, the fundamental tone is always simple. In dimension N=2, it is simple when the inner radius is sufficiently large.

Conclusion: The fundamental tone of the bilaplacian exhibits different behavior based on dimension and domain geometry, with guaranteed simplicity in higher dimensions and conditional simplicity in 2D.

Abstract: We show that the fundamental tone of the bilaplacian with Dirichlet or Navier
boundary conditions on radially symmetric domains is always simple in dimension
$N\ge3$. In dimension $N=2$ we show that it is simple if the inner radius is
big enough.

</details>


### [46] [Uniqueness Result For Semi-linear Wave Equations With Sources](https://arxiv.org/abs/2510.04810)
*Dong Qiu,Xiang Xu,Yeqiong Ye,Ting Zhou*

Main category: math.AP

TL;DR: This paper studies the inverse problem of recovering multiple parameters for semilinear wave equations from boundary measurements. It shows that unique recovery is not always possible, but characterizes the gauge equivalence classes and identifies which parameters can be uniquely determined versus those subject to gauge transformations.


<details>
  <summary>Details</summary>
Motivation: To understand the limitations and possibilities of parameter recovery in semilinear wave equations from boundary measurements, particularly addressing the inverse problem of determining nonlinear coefficients, source terms, and initial data from the Dirichlet-to-Neumann map.

Method: Combines higher-order linearization and construction of complex geometrical optics (CGO) solutions to analyze the inverse problem and characterize gauge equivalence classes.

Result: For polynomial nonlinearities of degree n, only the highest-order coefficient can be uniquely determined; lower-order coefficients and source terms can only be recovered up to specific gauge transformations. Provides sufficient conditions for unique determination of all parameters.

Conclusion: The nature of the nonlinearity critically influences whether unique recovery or gauge symmetry is obtained, with precise characterization of gauge equivalence classes for various nonlinearities including polynomial and non-polynomial types.

Abstract: This paper addresses the inverse problem of simultaneously recovering
multiple unknown parameters for semilinear wave equations from boundary
measurements. We consider an initial-boundary value problem for a wave equation
with a general semilinear term and an internal source. The inverse problem is
to determine the nonlinear coefficients (potentials), the source term, and the
initial data from the Dirichlet-to-Neumann (DtN) map. Our approach combines
higher-order linearization and the construction of complex geometrical optics
(CGO) solutions. The main results establish that while unique recovery is not
always possible, we can precisely characterize the gauge equivalence classes in
the solutions to this inverse problem. For a wave equation with a polynomial
nonlinearity of degree $n$, we prove that only the highest-order coefficient
can be uniquely determined from the DtN map; the lower-order coefficients and
the source can only be recovered up to a specific gauge transformation
involving a function $\psi$. Furthermore, we provide sufficient conditions
under which unique determination of all parameters is guaranteed. We also
extend these results to various specific non-polynomial nonlinearities,
demonstrating that the nature of the nonlinearity critically influences whether
unique recovery or a gauge symmetry is obtained.

</details>


### [47] [The parabolic Dirichlet problem with continuous and Hölder boundary data, and rough coefficients](https://arxiv.org/abs/2510.04833)
*Pablo Hidalgo-Palencia,Cody Hutcheson,Joseph Kasel*

Main category: math.AP

TL;DR: The paper establishes mild sufficient conditions for well-posedness of Dirichlet problems for parabolic operators with bounded coefficients in general space-time domains, solving an open problem about parabolic measure existence.


<details>
  <summary>Details</summary>
Motivation: To address the open problem posed by Genschaw and Hofmann (2020) regarding the existence of parabolic measure for unbounded domains, overcoming inherent difficulties due to the parabolic nature where solution behavior near boundaries depends strongly on operator coefficients.

Method: Introduces two sufficient conditions: (1) time-backwards capacity density condition (quantitative version of parabolic Wiener's criterion, adapted to the operator), and (2) time-backwards Hausdorff content condition (purely geometrical, operator-independent).

Result: Proves that both conditions ensure well-posedness of continuous and Hölder Dirichlet problems for any parabolic operator in divergence form with bounded coefficients, establishing existence of parabolic measure even for unbounded domains.

Conclusion: The paper successfully solves the open problem by providing practical, verifiable conditions that guarantee parabolic measure existence and well-posedness of Dirichlet problems in general space-time domains.

Abstract: We provide very mild sufficient conditions for space-time domains
(non-necessarily cylindrical) which ensure that the continuous Dirichlet
problem and the H\"older Dirichlet problem are well-posed, for any parabolic
operator in divergence form with merely bounded coefficients. Concretely, we
show that the parabolic measure exists, even for unbounded domains, hence
solving an open problem posed by Genschaw and Hofmann (2020).
  This problem has inherent difficulties because of its parabolic nature, as
the behavior of solutions near the boundary may depend strongly on the values
of the coefficients of the operator. One of our sufficient conditions, the
time-backwards capacity density condition, is a quantitative version of the
parabolic Wiener's criterion, and hence is adapted to the operator under
consideration. The other condition, the time-backwards Hausdorff content
condition, is (albeit slightly stronger) purely geometrical and independent of
the operator, hence much easier to check in practice.

</details>


### [48] [Riesz fractional gradient functionals defined on partitions: nonlocal-to-local variational limits](https://arxiv.org/abs/2510.04881)
*Stefano Almi,Maicol Caponi,Manuel Friedrich,Francesco Solombrino*

Main category: math.AP

TL;DR: Study of asymptotics for functionals with linear growth depending on Riesz s-fractional gradient on piecewise constant functions, characterizing local limiting functionals via Γ-convergence as s→1.


<details>
  <summary>Details</summary>
Motivation: To understand the limiting behavior of fractional gradient functionals with linear growth as the fractional parameter approaches 1, particularly for piecewise constant functions.

Method: Analysis of general class of varying energy densities depending on Riesz s-fractional gradient, using Γ-convergence techniques to characterize local limiting functionals as s→1.

Result: Characterization of local limiting functionals in the Γ-convergence sense for the asymptotics of these fractional gradient functionals.

Conclusion: The paper successfully establishes the asymptotic behavior and identifies the limiting functionals for fractional gradient problems with linear growth as the fractional parameter approaches 1.

Abstract: This paper addresses the asymptotics of functionals with linear growth
depending on the Riesz $s$-fractional gradient on piecewise constant functions.
We consider a general class of varying energy densities and, as $s\to 1$, we
characterize their local limiting functionals in the sense of
$\Gamma$-convergence.

</details>


### [49] [Fractional critical systems with mixed boundary conditions](https://arxiv.org/abs/2510.04975)
*R. Kumar,A. Ortega*

Main category: math.AP

TL;DR: Existence of weak solutions for fractional elliptic systems with critical nonlinearities and mixed Dirichlet-Neumann boundary conditions


<details>
  <summary>Details</summary>
Motivation: To analyze the existence of solutions for fractional elliptic systems coupled by critical nonlinearities with mixed boundary conditions

Method: Variational methods and an orthogonalization-like process in Sobolev spaces

Result: Established existence of at least one weak solution

Conclusion: Successfully proved solution existence using variational techniques and orthogonalization methods

Abstract: In this paper, we analyze the existence of solution for a fractional elliptic
system coupled by critical nonlinearities and endowed with mixed
Dirichlet-Neumann boundary conditions. By means of variational methods and an
orthogonalization-like process in the corresponding Sobolev space, we establish
the existence of at least one weak solution.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [50] [Uncertainty quantification of reacting fluids interacting with porous media using a hybrid physics-based and data-driven approach](https://arxiv.org/abs/2510.03649)
*Diba Behnoudfar,Kyle E. Niemeyer*

Main category: physics.comp-ph

TL;DR: Combines physics-based modeling with data-driven reduced-order modeling to efficiently quantify uncertainty in coupled physical processes like ablative heat shield design.


<details>
  <summary>Details</summary>
Motivation: Repeatedly evaluating detailed multi-physics models for uncertainty quantification is computationally prohibitive, especially for performance-critical applications like combustion systems and heat shield design.

Method: Uses operator inference method to combine single-domain physics-based modeling with data-driven reduced-order modeling, building parametric reduced-order models from physics-based simulation data.

Result: Physics-based simulations reproduce measured surface temperature within 5% accuracy. The reduced-order model successfully captures variations from parameter changes in permeability and heat transfer coefficient across different scenarios.

Conclusion: The hybrid approach enables efficient uncertainty quantification for coupled physical processes while maintaining accuracy, demonstrated through successful applications in solid fuel combustion and ablation scenarios.

Abstract: Accurately simulating coupled physical processes under uncertainty is
essential for reliable modeling and design in performance-critical applications
such as combustion systems. Ablative heat shield design, as a specific example
of this class, involves modeling multi-physics interactions between reacting
flows and a porous material. Repeatedly evaluating these models to quantify
parametric uncertainties would be prohibitively computationally expensive. In
this work, we combine physics-based modeling using a single-domain approach
with data-driven reduced-order modeling to quantify uncertainty via the
operator inference method. The detailed physics-based simulations reproduce the
measured surface temperature of an object exposed to high-enthalpy flow in a
plasma wind tunnel experiment within 5%. We further use the model to
demonstrate the effect of complex flow situations on the dynamic interactions
between the porous heat shield material and the surrounding gas. The parametric
reduced-order model, built on physics-based simulation data, successfully
captures variations in quantities of interest resulting from changes in the
permeability and heat transfer coefficient of the porous material in two
separate studies: solid fuel combustion and emission of buoyant reacting plumes
in quiescent air and ablation in a wind tunnel.

</details>


### [51] [Structure-Preserving MHD-Driftkinetic Discretization for Wave-Particle Interactions](https://arxiv.org/abs/2510.04385)
*Byung Kyu Na,Stefan Possanner,Xin Wang*

Main category: physics.comp-ph

TL;DR: Structure-preserving discretization of hybrid MHD-driftkinetic system for simulating low-frequency wave-particle interactions with energy-conserving properties.


<details>
  <summary>Details</summary>
Motivation: To develop high-fidelity simulations of hybrid magnetohydrodynamics-driftkinetic systems for studying wave-particle interactions, particularly energetic particle-induced excitations in plasma physics.

Method: Variational principle derivation, finite-element-exterior-calculus (FEEC) for MHD, particle-in-cell (PIC) for driftkinetic particles, discrete gradient time integrators for non-quadratic energy terms.

Result: Numerical experiments confirm energy conservation and demonstrate capability to simulate energetic particle-induced excitation of toroidal Alfvén eigenmodes without artificial dissipation.

Conclusion: Structure-preserving schemes show potential for high-fidelity simulations of hybrid systems, enabling accurate modeling of wave-particle interactions in plasma physics.

Abstract: We present a structure-preserving discretization of the hybrid
magnetohydrodynamics (MHD)-driftkinetic system for simulations of low-frequency
wave-particle interactions. The model equations are derived from a variational
principle, assuring energetically consistent couplings between MHD fluids and
driftkinetic particles. The spatial discretization is based on a
finite-element-exterior-calculus (FEEC) framework for the MHD and a
particle-in-cell (PIC) method for the driftkinetic. A key feature of the scheme
is the inclusion of the non-quadratic particle magnetic moment energy term in
the Hamiltonian, which is introduced by the guiding-center approximation. The
resulting discrete Hamiltonian structure naturally organizes the dynamics into
skew-symmetric subsystems, enabling balanced energy exchange. To handle the
non-quadratic energy term, we develop energy-preserving time integrators based
on discrete gradient methods. The algorithm is implemented in the open-source
Python package \texttt{STRUPHY}. Numerical experiments confirm the
energy-conserving property of the scheme and demonstrate the capability to
simulate energetic particles (EP) induced excitation of toroidal Alfv\'en
eigenmodes (TAE) without artificial dissipation or mode filtering. This
capability highlights the potential of structure-preserving schemes for
high-fidelity simulations of hybrid systems.

</details>


### [52] [The dawn of alchemical free-energy methods in biomolecular simulations](https://arxiv.org/abs/2510.04917)
*Daniele Macuglia,Giovanni Ciccotti,Benoît Roux*

Main category: physics.comp-ph

TL;DR: This paper traces the historical development of alchemical free-energy methods from 19th century statistical mechanics to operational tools for biomolecular simulation, particularly in drug discovery.


<details>
  <summary>Details</summary>
Motivation: To understand how statistical mechanical approaches like thermodynamic integration and free-energy perturbation became practical tools for studying complex biomolecular systems, examining the conditions that enabled their operationalization.

Method: Historical analysis using oral history interviews and primary literature to examine technical, institutional, theoretical, and infrastructural conditions that facilitated implementation of free-energy methods.

Result: Found that progress occurred through iterative troubleshooting and alignment of practical and theoretical considerations rather than unified convergence, involving lab-specific software, simulation protocols, and statistical mechanical clarifications.

Conclusion: Free-energy methods acquired practical reliability through a historically situated process of alignment between theoretical constructs and practical implementation requirements in biomolecular research contexts.

Abstract: From the onset of fundamental statistical mechanical constructs formulated in
the late 19th century, alchemical free-energy methods slowly emerged and
transitioned to become operational tools of biomolecular simulation applicable
to a wide range of problems including protein-ligand binding for drug discovery
research. This article reconstructs how statistical mechanical approaches such
as thermodynamic integration and free-energy perturbation were reconfigured in
the early 1980's to address the complexities of increasingly heterogeneous
biomolecular systems. Drawing on oral history interviews and primary
literature, the study examines the technical, institutional, theoretical, and
infrastructural conditions under which these methods were implemented, and
became progressively operational. These conditions encompassed the
consolidation of lab-specific software infrastructures, the formulation of
practical simulation protocols, as well as essential statistical mechanical
clarifications. From this perspective, the progress of free-energy methods
proceeded less from a unified convergence than from an iterative
troubleshooting process of alignment involving practical and theoretical
considerations. The aim of the present article is to offer a historically
grounded account of how free-energy techniques acquired practical and
functional reliability.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [53] [Spectroscopic measurements of graphite electrode erosion on the ZaP-HD sheared-flow-stabilized Z-pinch device](https://arxiv.org/abs/2510.03414)
*Amierul Aqil Khairi,Elyse Lian,Uri Shumlak*

Main category: physics.plasm-ph

TL;DR: The paper applies the S/XB method to measure graphite electrode erosion in Z-pinch devices, finding erosion fluxes of 10^30-10^31 atoms m^-2s^-1, primarily from sublimation rather than sputtering.


<details>
  <summary>Details</summary>
Motivation: Understanding electrode erosion in sheared-flow-stabilized Z-pinch devices is crucial for electrode durability, especially as energy density increases. Electrodes directly contact core plasma and face large particle and heat fluxes.

Method: Used S/XB method with C-III emission at 229.7 nm on ZaP-HD device. Determined S/XB coefficients from electron density and temperature profiles obtained via Digital Holographic Interferometry (DHI), with an approach to expand profiles to represent plasma contacting the electrode.

Result: Measured erosion fluxes were 10^30-10^31 atoms m^-2s^-1, significantly larger than expected physical sputtering but comparable to theoretical sublimation fluxes, suggesting sublimation is the primary erosion mechanism.

Conclusion: Carbon electrode erosion in Z-pinch devices is dominated by sublimation rather than sputtering, which implies differences in eroded neutral energies that may provide insight into redeposition and net erosion behavior.

Abstract: The ionizations per photon, or S/XB, method uses spectroscopic measurements
of radiating impurity ions to determine the influx from a solid surface. It is
highly useful as a non-perturbing, in-situ measure of the gross erosion flux of
plasma-facing components (PFCs). In sheared-flow-stabilized (SFS) Z-pinch
devices, the electrode supplies the plasma current and directly contacts the
core Z-pinch plasma. Electrode erosion due to the large particle and heat
fluxes affects electrode durability, which is an important factor in existing
and future devices. An improved understanding of these plasma-electrode
interactions is required, in particular as energy density increases.
Experiments on the ZaP-HD device investigate erosion of the graphite electrode
by applying the S/XB method for C-III emission at 229.7 nm. The S/XB
coefficients are determined from electron density and temperature profiles
obtained from Digital Holographic Interferometry (DHI) measurements. An
approach for expanding these profiles to represent plasma contacting the
electrode is described. In both cases, the measured erosion fluxes are on the
order of 10$^{30}$-10$^{31}$ atoms m$^{-2}$s$^{-1}$. These values are
significantly larger than the expected erosion flux due to physical sputtering
of H$^+$ ions on carbon, but are comparable to theoretical sublimation fluxes.
This suggests that the source of carbon erosion flux is primarily from
sublimation as opposed to sputtering. The dominance of sublimation over
sputtering processes implies a difference in energy of the eroded neutrals
which may provide insight on redeposition and net erosion behavior.

</details>


### [54] [An Explicit Energy-Conserving Particle Method for the Vlasov-Fokker-Planck Equation](https://arxiv.org/abs/2510.03960)
*Jiyoung Yoo,Jingwei Hu,Lee F. Ricketson*

Main category: physics.plasm-ph

TL;DR: An explicit particle method for Vlasov-Fokker-Planck equation that conserves energy at discrete level using deterministic particle discretization and second-order time integrator.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method that maintains energy conservation properties at the fully discrete level for the Vlasov-Fokker-Planck equation, which is important for accurate plasma simulations.

Method: Combines deterministic conservative particle discretization for nonlinear Fokker-Planck operator with second-order explicit time integrator featuring accuracy-justifiable correction for energy conservation.

Result: Method successfully validated on plasma benchmarks including collisional Landau damping and two-stream instability, demonstrating effectiveness.

Conclusion: The proposed explicit particle method achieves discrete energy conservation while maintaining computational efficiency, making it suitable for plasma physics simulations.

Abstract: We propose an explicit particle method for the Vlasov-Fokker-Planck equation
that conserves energy at the fully discrete level. The method features two key
components: a deterministic and conservative particle discretization for the
nonlinear Fokker-Planck operator (also known as the Lenard-Bernstein or
Dougherty operator), and a second-order explicit time integrator that ensures
energy conservation through an accuracy-justifiable correction. We validate the
method on several plasma benchmarks, including collisional Landau damping and
two-stream instability, demonstrating its effectiveness.

</details>


### [55] [A Particle-in-Cell Simulation Framework for Thomson Scattering Analysis in Inertial Confinement Fusion](https://arxiv.org/abs/2510.04298)
*Ziang Zhu,Yifan Liu,Jun Li,Han Wen,Shihui Cao,Yin Shi,Qing Jia,Chaoxin Chen,Yaoyuan Liu,Hang Zhao,Tao Gong,Zhichao Li,Dong Yang,Jian Zheng*

Main category: physics.plasm-ph

TL;DR: First-principles numerical approach for Thomson scattering analysis in ICF plasmas shows TS signals remain significant even with imperfect wave-vector matching, challenging conventional expectations.


<details>
  <summary>Details</summary>
Motivation: To develop a practical framework for interpreting Thomson scattering signals from driven ion modes in inertial confinement fusion plasmas, which are common but complex features.

Method: Used particle-in-cell simulations under typical ICF conditions to obtain scattered light signals of ion acoustic features with high resolution in angle and frequency space.

Result: Good agreement with existing theories for thermal collective TS; in super-thermal regime, alignment with theory when wave vectors match; TS signals remain significant even with imperfect wave-vector matching due to beating wave mechanism.

Conclusion: Provides a practical framework for interpreting TS signals from driven ion modes in ICF plasmas, challenging conventional understanding that TS spectrum strictly follows plasma density spectrum.

Abstract: In inertial confinement fusion (ICF), Thomson scattering (TS) is a widely
used diagnostic technique for probing plasma conditions. We present a
first-principles numerical approach to obtaining scattered light signals of ion
acoustic features with high resolution in angle and frequency space using
particle-in-cell simulations under typical ICF conditions. Our method
demonstrates good agreement with existing theories for thermal collective TS.
In the super-thermal collective regime, the results align with theory when the
driven plasma modes are well-matched in wave vectors to the probe and
collecting beams. Moreover, we also find that TS signals can remain significant
even under imperfect wave-vector matching-a result that contradicts the
conventional expectation that the TS spectrum strictly follows the plasma
density spectrum. We attribute this discrepancy to a beating wave mechanism
arising from the interaction between the probe beam and driven plasma density
modulations. Our work thus provides a practical framework for interpreting TS
signals from driven ion modes, a common yet complex feature in ICF plasmas.

</details>


### [56] [Non-local transport in Radiation-Hydrodynamics codes for ICF by efficient coupling to an external Vlasov-Fokker-Planck code](https://arxiv.org/abs/2510.04356)
*Abetharan Antony,Robert Kingham,Stefan Mijin,Marty Marinak*

Main category: physics.plasm-ph

TL;DR: A coupling methodology between Vlasov-Fokker-Planck electron kinetic code and radiation-hydrodynamics codes to improve non-local transport modeling without full VFP integration.


<details>
  <summary>Details</summary>
Motivation: Non-local transport has been difficult to incorporate accurately into radiation-hydrodynamics codes, affecting fusion modeling where it's known to be present.

Method: Coupling VFP electron kinetic code with rad-hydro codes, using VFP to adjust native electron transport without full VFP solver integration, requiring only occasional VFP invocation.

Result: Method is more accurate than other simplified methods in thermal decay systems relevant to ICF and can replicate standard theoretical results with high accuracy.

Conclusion: The approach enables improved transport modeling with reduced computational intensity compared to full VFP evolution on fluid time scales.

Abstract: Accurately incorporating non-local transport into radiation-hydrodynamics
codes, and indeed any fluid system, has long been elusive. To date, a
simplified and accurate theory that can be easily integrated has not been
available. This limitation affects modeling in inertial confinement fusion and
magnetic confinement fusion systems, among others, where non-local transport is
well-known to be present. Here, we present a coupling methodology between a
full Vlasov-Fokker-Planck (VFP) electron kinetic code and
radiation-hydrodynamics (rad-hydro) codes. The VFP code is used to adjust
native electron transport in the rad-hydro code, thus enabling improved
transport without the need to integrate a full electron VFP solver into the
rad-hydro code. This approach necessitates only occasional invocation of the
VFP code, reducing computational intensity compared to following the dynamic
evolution entirely with the VFP code on fluid time scales. We illustrate that
the methodology is more accurate than other simplified methods in thermal decay
systems relevant to inertial confinement fusion and can replicate standard
theoretical results with high accuracy.

</details>


### [57] [Numerical Demonstration of Kolmogorov Scaling in Magnetohydrodynamic Turbulence](https://arxiv.org/abs/2510.04658)
*Manthan Verma,Abhishek K. Jha,Shashwat Nirgudkar,Mahendra K. Verma*

Main category: physics.plasm-ph

TL;DR: This paper identifies the valid MHD turbulence model using high-resolution simulations, showing that Kolmogorov's k^{-5/3} scaling is supported over Iroshnikov-Kraichnan's k^{-3/2} scaling through energy spectra, structure functions, intermittency exponents, and energy flux analysis.


<details>
  <summary>Details</summary>
Motivation: To resolve the competing predictions between Kolmogorov's k^{-5/3} and Iroshnikov-Kraichnan's k^{-3/2} scalings in isotropic magnetohydrodynamic (MHD) turbulence using high-resolution numerical simulations.

Method: Used high-resolution forced MHD simulations on grids of 8192^2, 4096^2, 1024^3, and 512^3, analyzing energy spectra, third-order structure functions, intermittency exponents, and energy fluxes of imbalance MHD.

Result: Energy spectra support Kolmogorov's k^{-5/3} spectrum over Iroshnikov-Kraichnan's k^{-3/2}, though the difference in spectral exponents is small. Third-order structure functions and intermittency exponents consistently support Kolmogorov scaling in both 2D and 3D. Energy fluxes of imbalance MHD follow Kolmogorov scaling predictions.

Conclusion: Kolmogorov scaling is the valid model for MHD turbulence, which will help improve modeling of solar wind, solar corona, and dynamos.

Abstract: The two leading models of isotropic magnetohydrodynamic (MHD) turbulence have
competing predictions: $k^{-5/3}$ (Kolmogorov) and $k^{-3/2}$
(Iroshnikov-Kraichnan) scalings. This paper identifies the valid MHD turbulence
model using high-resolution numerical and diagnostics-structure functions,
intermittency exponents, and energy spectra and fluxes of imbalance MHD. The
energy spectra of our forced MHD simulations on $8192^2$, $4096^2$, $1024^3$,
and $512^3$ support Kolmogorov's k^{-5/3} spectrum over Iroshnikov-Kraichnan's
k^{-3/2} spectrum, but the difference in the spectral exponents is small.
However, the numerically computed third-order structure functions and
intermittency exponents support Kolmogorov scaling in both two and three
dimensions. Also, the energy fluxes of the imbalance MHD follow the predictions
of Kolmogorov scaling. These results would help in better modelling of solar
wind, solar corona, and dynamos.

</details>


### [58] [Pushing the Frontiers of Light: Magnetized Plasma Lenses and Chirp Tailoring for Extreme Intensities](https://arxiv.org/abs/2510.05058)
*Trishul Dhalia,Rohit Juneja,Amita Das*

Main category: physics.plasm-ph

TL;DR: A magnetized plasma lens scheme achieves refractive index >1 for RCP waves, enabling 100x laser intensity amplification through combined focusing and compression.


<details>
  <summary>Details</summary>
Motivation: To develop a method for achieving extreme laser intensities using magnetized plasmas as optical elements, leveraging recent advances in magnet technology and laser systems.

Method: Used 2D/3D PIC simulations with OSIRIS 4.0 framework, combining tailored plasma lens geometry, structured magnetic field, and chirped laser pulse for simultaneous focusing and compression.

Result: Achieved up to 100-fold increase in laser intensity through transverse focusing and pulse compression enabled by the magnetized plasma lens system.

Conclusion: This approach provides a promising experimental pathway to reach extreme laser intensities using current magnet, plasma target, and chirped laser technologies.

Abstract: In this work, an innovative scheme is proposed that exploits the response of
magnetized plasmas to realize a refractive index exceeding unity for right
circularly polarized (RCP) waves. Using two- and three-dimensional
Particle-in-Cell (PIC) simulations with the OSIRIS 4.0 framework, it is shown
that a shaped magnetized plasma lens (MPL) can act as a glass/solid-state-based
convex lens, amplifying laser intensity via transverse focusing. Moreover, by
integrating three key ingredients, a tailored plasma lens geometry, a spatially
structured strong magnetic field, and a suitably chirped laser pulse,
simultaneous focusing and compression of the pulse has been achieved. The
simulations reveal up to a 100-fold increase in laser intensity, enabled by the
combined action of the MPL and the chirped pulse profile. With recent advances
in high-field magnet technology, shaped plasma targets, and controlled chirped
laser systems, this approach offers a promising pathway toward experimentally
reaching extreme intensities.

</details>


### [59] [Electrospray Thruster Plume Impingement on CubeSat Solar Arrays: A Particle-Tracking Study](https://arxiv.org/abs/2510.05084)
*Ethan Kahn*

Main category: physics.plasm-ph

TL;DR: Study analyzes electrospray thruster placement on CubeSats to optimize thrust efficiency and minimize surface contamination using validated particle-tracking simulations.


<details>
  <summary>Details</summary>
Motivation: Electrospray thrusters are promising for CubeSats but their divergent ion plumes can contaminate spacecraft surfaces, particularly body-mounted solar arrays, reducing thrust efficiency.

Method: Validated particle-tracking simulation using cosine power distribution (k=1.8) with 46° half-angle, validated against experimental data with <7% errors, analyzing various thruster placements on 1U, 3U, and 6U CubeSats.

Result: Thrust efficiency ranges from 53.6% (rear-mounted on 3U) to 100% (side-mounted with deployable arrays). Deployable arrays reduce contamination by 77%, side-mounted thrusters eliminate impingement with only 1.6% efficiency loss. 3U platforms have 46.4% contamination vs 16.6% for 1U.

Conclusion: Quantitative design guidelines enable mission planners to optimize thruster integration based on power budget and propellant mass constraints, with statistical uncertainty below 0.15% across configurations.

Abstract: Electrospray thrusters are emerging as a leading propulsion technology for
CubeSats, offering high specific impulse ($I_{sp} > 1000$ s) and low power
requirements. However, the divergent ion plumes can impinge on spacecraft
surfaces, particularly body-mounted solar arrays, causing contamination and
thrust efficiency losses. This study presents a validated particle-tracking
simulation to quantify the effects of thruster placement on thrust efficiency
and surface contamination for 1U, 3U, and 6U CubeSats. The plume model employs
a cosine power distribution ($k=1.8$) with half-angle $46^\circ$, validated
against experimental data with errors below 7%. Results show that thrust
efficiency ranges from 53.6% for rear-mounted thrusters on 3U body-mounted
configurations to 100% for side-mounted configurations with deployable arrays.
CubeSat size significantly affects impingement: 3U platforms experience 46.4%
contamination with rear-mounted thrusters compared to 16.6% for 1U. Deployable
solar arrays reduce contamination by 77% compared to body-mounted arrays, while
side-mounted thrusters eliminate impingement entirely at the cost of only 1.6%
efficiency loss. Corner-mounted configurations at $30^\circ$ cant provide
intermediate performance with 88.9% efficiency and 11.1% contamination. These
quantitative design guidelines enable mission planners to optimize thruster
integration based on power budget and propellant mass constraints, with
statistical uncertainty below 0.15% across all configurations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [60] [Cosmological Hydrodynamics at Exascale: A Trillion-Particle Leap in Capability](https://arxiv.org/abs/2510.03557)
*Nicholas Frontiere,J. D. Emberson,Michael Buehlmann,Esteban M. Rangel,Salman Habib,Katrin Heitmann,Patricia Larsen,Vitali Morozov,Adrian Pope,Claude-André Faucher-Giguère,Antigoni Georgiadou,Damien Lebrun-Grandié,Andrey Prokopenko*

Main category: cs.DC

TL;DR: CRK-HACC cosmological hydrodynamics code executed Frontier-E, a four trillion particle full-sky simulation, achieving 513.1 PFLOPs peak performance and processing 46.6 billion particles per second.


<details>
  <summary>Details</summary>
Motivation: To address fundamental cosmology questions and match the scale, fidelity, and complexity required by next-generation sky surveys through realistic simulations that incorporate detailed gas dynamics and astrophysical effects.

Method: Used CRK-HACC code with separation-of-scale techniques, GPU-resident tree solvers, in situ analysis pipelines, and multi-tiered I/O for extreme scalability.

Result: Successfully ran Frontier-E simulation with four trillion particles, achieving 513.1 PFLOPs peak performance, processing 46.6 billion particles per second, and writing over 100 PB of data in one week.

Conclusion: Exascale computing enables unprecedented cosmological simulations that incorporate key subgrid processes and span survey-scale volumes, advancing the realism needed for scientific partnership with next-generation sky surveys.

Abstract: Resolving the most fundamental questions in cosmology requires simulations
that match the scale, fidelity, and physical complexity demanded by
next-generation sky surveys. To achieve the realism needed for this critical
scientific partnership, detailed gas dynamics, along with a host of
astrophysical effects, must be treated self-consistently with gravity for
end-to-end modeling of structure formation. As an important step on this
roadmap, exascale computing enables simulations that span survey-scale volumes
while incorporating key subgrid processes that shape complex cosmic structures.
We present results from CRK-HACC, a cosmological hydrodynamics code built for
the extreme scalability requirements set by modern cosmological surveys. Using
separation-of-scale techniques, GPU-resident tree solvers, in situ analysis
pipelines, and multi-tiered I/O, CRK-HACC executed Frontier-E: a four trillion
particle full-sky simulation, over an order of magnitude larger than previous
efforts. The run achieved 513.1 PFLOPs peak performance, processing 46.6
billion particles per second and writing more than 100 PB of data in just over
one week of runtime.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [61] [Mean-field limits à la Tanaka and large deviations for particle systems with network interactions](https://arxiv.org/abs/2510.04894)
*Louis-Pierre Chaintron,Antoine Diez*

Main category: math.PR

TL;DR: A unified framework for non-exchangeable mean-field particle systems with general interactions, extending Tanaka's fixed-point formulation to non-exchangeable cases including adaptive networks.


<details>
  <summary>Details</summary>
Motivation: To extend mean-field analysis beyond exchangeable particle systems to include non-exchangeable cases with adaptive interaction networks, which are common in real-world applications.

Method: Uses Tanaka's fixed-point formulation of particle systems, introduces sufficient conditions on network structure, and develops abstract framework for proving mean-field limits and large deviations.

Result: Establishes mean-field limit and new large deviations principle for interaction measures in non-exchangeable particle systems with adaptive networks.

Conclusion: The framework successfully extends mean-field theory to non-exchangeable systems and provides conditions for deriving PDE characterizations of the limit in important models.

Abstract: This article proposes a unified framework to study non-exchangeable
mean-field particle systems with some general interaction mechanisms. The
starting point is a fixed-point formulation of particle systems originally due
to Tanaka that allows us to prove mean-field limit and large deviation results
in an abstract setting. While it has been recently shown that such formulation
encompasses a large class of exchangeable particle systems, we propose here a
setting for the non-exchangeable case, including the case of adaptive
interaction networks. We introduce sufficient conditions on the network
structure that imply the mean-field limit and a new large deviations principle
for the interaction measure. Finally, we formally highlight important models
for which it is possible to derive a closed PDE characterization of the limit.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [62] [Cluster Analysis for Globally Coupled Map using Optimal Transport Distance and Complexity of Attractor-ruin](https://arxiv.org/abs/2510.00538)
*Koji Wada,Takao Namiki*

Main category: math.DS

TL;DR: Analysis of attractor-ruin strength in globally coupled logistic maps using optimal transport distance, showing high strength in the partially ordered phase where chaotic itinerancy occurs.


<details>
  <summary>Details</summary>
Motivation: To understand the instability mechanisms and statistical properties of chaotic itinerancy - intermittent transitions between attractor-ruins observed in complex systems.

Method: Analyzed orbit instability in globally coupled maps using optimal transport distance for clustering, then numerically evaluated attractor-ruin strength.

Result: Found that various attractor-ruins have high strength specifically in the partially ordered phase parameter region where chaotic itinerancy occurs.

Conclusion: The strength of attractor-ruins is closely related to the occurrence of chaotic itinerancy in the partially ordered phase of globally coupled maps.

Abstract: In this paper, we show the results of the strength of attractorruins for a
globally coupled map. The globally coupled map (GCM) is a discrete dynamical
system, and here we consider a model in which the logistic map is globally
coupled. An attractor-ruin is a set in which the attractor is destabilized by a
change in parameters, which is characterized by a Milnor attractor.
Intermittent phenomena called chaotic itinerancy, in which orbits transition
between attractor-ruin, have been observed in various complex systems, and
their onset mechanisms and statistical properties have attracted attention. In
this study, the instability of orbits of GCM is analyzed from the perspective
of clustering using the optimal transport distance, and the strength of
attractor-ruins is numerically evaluated by applying this method. As a result,
it was found that the strength of various attractor-ruins is high in the
parameter region called the partially ordered phase, where chaotic itinerancy
occurs.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [63] [Investigating into mechanisms of high temperature strength of refractory high-entropy alloys](https://arxiv.org/abs/2510.04589)
*Sai Anandhi Seetharaman,Soumyadipta Maiti,Ambesh Gupta,Beena Rai*

Main category: cond-mat.mtrl-sci

TL;DR: The yield strength plateau in BCC refractory high entropy alloys is caused by dual mechanisms: Dynamic Strain Ageing (DSA) via atomic diffusion at high temperatures, and athermal stress from lattice distortions at intermediate temperatures.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying mechanisms behind the yield strength plateau observed in BCC refractory high entropy alloys (RHEAs) at intermediate temperatures, which has been experimentally observed but not fully explained.

Method: Used hybrid Monte Carlo and molecular dynamics (MC/MD) simulations to analyze atomic diffusivities, vacancy formation/migration energies, and critical atomic swaps around dislocation cores at different temperatures.

Result: Above 1400K, dislocation motion stress saturates due to DSA via cross core motion. At intermediate temperatures, similar plateau occurs from athermal stress due to lattice distortions from solid solution strengthening.

Conclusion: The yield strength plateau in RHEAs results from an interplay between DSA-driven diffusion processes and athermal stress from lattice distortions, providing a comprehensive dual-mechanism explanation for experimental observations.

Abstract: The yield strength plateau of two BCC refractory high entropy alloys (RHEAs)
- MoNbTaVW and MoNbTaW was examined through hybrid Monte Carlo and molecular
dynamics (MC/MD) simulations. By analyzing atomic diffusivities derived from
vacancy formation and migration energies around the edge dislocation cores, the
number of critical atomic swaps were calculated at different temperatures.
Using hybrid MC/MD simulations of these critical swaps, we demonstrate that
above 1400K, the stress required to move the dislocations gets saturated,
indicating the effect of Dynamic Strain Ageing (DSA) via cross core motion.
Further simulations on random solid solutions (0 MC swaps) revealed a similar
plateau effect at the intermediate temperatures. This was attributed to the
additional athermal stress arising from lattice distortions due to solid
solution strengthening. Our findings suggest that the yield strength plateau
results from an interplay between the DSA-driven diffusion process and athermal
stress. Specifically, the plateau emerges from DSA mechanisms in the presence
of atomic diffusion, whereas in the absence of diffusion, it is governed by
athermal statistical lattice distortions. This dual mechanism framework
provides a comprehensive explanation for the experimentally observed Yield
strength behavior in RHEAs at intermediate temperatures.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [64] [Environment-Aware Indoor LoRaWAN Path Loss: Parametric Regression Comparisons, Shadow Fading, and Calibrated Fade Margins](https://arxiv.org/abs/2510.04346)
*Nahshon Mokua Obiri,Kristof Van Laerhoven*

Main category: cs.NI

TL;DR: The paper presents an environment-aware path loss framework for indoor LoRaWAN that improves prediction accuracy and reliability by incorporating environmental factors and using polynomial models, reducing required fade margins for 99% packet delivery.


<details>
  <summary>Details</summary>
Motivation: Traditional log-distance models for indoor LoRaWAN propagation are challenged by structural and time-varying environmental factors, leading to inaccurate predictions and unreliable network planning.

Method: Used a 12-month measurement campaign in an office environment, augmented log-distance multi-wall mean with environmental covariates, compared multiple regression methods, analyzed predictor relevance with ANOVA, and modeled shadow fading with various distributions including Gaussian mixtures.

Result: The polynomial mean reduced cross-validated RMSE from 8.07 to 7.09 dB and increased R² from 0.81 to 0.86. For 99% packet delivery ratio, the environment-aware polynomial required only 25.7 dB fade margin compared to 27.7-27.9 dB for linear baselines.

Conclusion: The framework provides a deployment-ready, interpretable workflow with calibrated reliability control for indoor IoT planning, aligning with 6G targets by improving prediction accuracy and reducing required fade margins.

Abstract: Indoor LoRaWAN propagation is shaped by structural and time-varying context
factors, which challenge log-distance models and the assumption of log-normal
shadowing. We present an environment-aware, statistically disciplined path loss
framework evaluated using leakage-safe cross-validation on a 12-month campaign
in an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is
augmented with environmental covariates (relative humidity, temperature, carbon
dioxide, particulate matter, and barometric pressure), as well as the
signal-to-noise ratio. We compare multiple linear regression with regularized
variants, Bayesian linear regression, and a selective second-order polynomial
applied to continuous drivers. Predictor relevance is established using
heteroscedasticity-robust Type II and III analysis of variance and nested
partial F tests. Shadow fading is profiled with kernel density estimation and
non-parametric families, including Normal, Skew-Normal, Student's t, and
Gaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07
to 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are
non-Gaussian; a 3-component mixture captures a sharp core with a light, broad
tail. We convert accuracy into reliability by prescribing the fade margin as
the upper-tail quantile of cross-validated residuals, quantifying uncertainty
via a moving-block bootstrap, and validating on a held-out set. At 99% packet
delivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7
to 27.9 dB for linear baselines. This result presents a deployment-ready,
interpretable workflow with calibrated reliability control for indoor Internet
of Things planning, aligned with 6G targets.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [65] [A Direct Approach for Detection of Bottom Topography in Shallow Water](https://arxiv.org/abs/2510.03505)
*Lamsahel Noureddine,Carole Rosier*

Main category: math-ph

TL;DR: A fast, stable analytic method for detecting underwater channel topography from surface wave measurements using shallow water equations, requiring only surface data at a single time instant.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and reliable method for reconstructing underwater channel topography from easily measurable surface wave data, overcoming limitations of traditional methods.

Method: Restructured forward shallow water equations into an inverse model, discretized with second-order finite-difference scheme, requiring only free surface and its first two time derivatives at a single measurement time.

Result: The method achieves high approximation accuracy, stable reconstruction under noise, and satisfies Lipschitz stability independent of initial conditions. Sufficient conditions for positivity are met across all tested scenarios.

Conclusion: The proposed analytic approach provides an effective, stable, and direct method for underwater topography detection that works across various flow regimes and bathymetry types.

Abstract: We propose a fast, stable, and direct analytic method to detect underwater
channel topography from surface wave measurements, based on one-dimensional
shallow water equations. The technique requires knowledge of the free surface
and its first two time derivatives at a single instant $t^{\star}$ above the
fixed, bounded open segment of the domain. We first restructure the forward
shallow water equations to obtain an inverse model in which the bottom profile
is the only unknown, and then discretize this model using a second-order
finite-difference scheme to infer the floor topography. We demonstrate that the
approach satisfies a Lipschitz stability and is independent of the initial
conditions of the forward problem. The well-posedness of this inverse model
requires that, at the chosen measurement time $t^{\star}$, the discharge be
strictly positive across the fixed portion of the open channel, which is
automatically satisfied for steady and supercritical flows. For unsteady
subcritical and transcritical flows, we derive two empirically validated
sufficient conditions ensuring strict positivity after a sufficiently large
time. The proposed methodology is tested on a range of scenarios, including
classical benchmarks and different types of inlet discharges and bathymetries.
We find that this analytic approach yields high approximation accuracy and that
the bed profile reconstruction is stable under noise. In addition, the
sufficient conditions are met across all tests.

</details>


### [66] [Inverse scattering for $N$-body time-decaying harmonic oscillators](https://arxiv.org/abs/2510.04702)
*Atsuhide Ishida*

Main category: math-ph

TL;DR: Extension of uniqueness proof for potential functions from two-body to N-body quantum systems using high-velocity scattering operator limits.


<details>
  <summary>Details</summary>
Motivation: To generalize previous uniqueness results for short-range potential functions from two-body quantum systems to N-body systems with time-decaying harmonic oscillators.

Method: Uses Enss-Weder time-dependent method and approaches from Enss-Weder (1995), Weder (1996), and Valencia-Weder (2012) to analyze high-velocity limit of scattering operator.

Result: Proves that high-velocity limit of scattering operator uniquely determines all pairwise interaction potentials among N particles.

Conclusion: Successfully extends uniqueness theorem for potential functions to N-body quantum systems, with focus on determining each fixed pair of particles' interaction potentials.

Abstract: In the previous study (Ishida, 2025), the author proved the uniqueness of
short-range potential functions using the Enss-Weder time-dependent method
(Enss and Weder, 1995) for a two-body quantum system described by time-decaying
harmonic oscillators. In this study, we extend the result of Ishida (2025) to
the $N$-body case. We use the approaches developed in Enss and Weder (1995),
Weder (1996), and Valencia and Weder (2012) to prove that the high-velocity
limit of the scattering operator uniquely determines all the pairwise
interaction potentials among the $N$ particles, focusing respectively on each
fixed pair of particles.

</details>


### [67] [Validity condition of normal form transformation for the $β$-FPUT system](https://arxiv.org/abs/2510.04831)
*Boyang Wu,Miguel Onorato,Zaher Hani,Yulin Pan*

Main category: math-ph

TL;DR: The paper establishes a validity condition for normal form transformation in the β-FPUT system, showing it's valid with high probability when β ≪ 1/N^(1+ε), and demonstrates that βN governs the importance of non-resonant terms.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous validity conditions for normal form transformations in Hamiltonian systems, specifically addressing when non-resonant cubic terms can be removed in the β-FPUT system.

Method: Developed a bound for a summation in the transformation equation, proved rigorously, and designed numerical experiments to test the condition for both thermal-equilibrium and out-of-equilibrium spectra.

Result: The normal form transformation is valid with dominant probability when β ≪ 1/N^(1+ε), and numerical experiments confirm that βN governs the importance of non-resonant terms.

Conclusion: The methodology is applicable to other Hamiltonian systems requiring normal form transformations, and the parameter βN determines the significance of non-resonant terms in the evolution equation.

Abstract: In this work, we provide a validity condition for the normal form
transformation to remove the non-resonant cubic terms in the $\beta$-FPUT
system. We show that for a wave field with random phases, the normal form
transformation is valid by dominant probability if $\beta \ll
1/N^{1+\epsilon}$, with $N$ the number of masses and $\epsilon$ an arbitrarily
small constant. To obtain this condition, a bound is needed for a summation in
the transformation equation, which we prove rigorously in the paper. The
condition also suggests that the importance of the non-resonant terms in the
evolution equation is governed by the parameter $\beta N$. We design numerical
experiments to demonstrate that this is indeed the case for spectra at both
thermal-equilibrium and out-of-equilibrium conditions. The methodology
developed in this paper is applicable to other Hamiltonian systems where a
normal form transformation needs to be applied.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [68] [Towards Fast Option Pricing PDE Solvers Powered by PIELM](https://arxiv.org/abs/2510.04322)
*Akshay Govind Srinivasan,Anuj Jagannath Said,Sathwik Pentela,Vikas Dwivedi,Balaji Srinivasan*

Main category: cs.CE

TL;DR: PIELMs provide a faster alternative to PINNs for solving financial PDEs by replacing iterative optimization with single least-squares solves, achieving comparable accuracy with 30x speedup.


<details>
  <summary>Details</summary>
Motivation: PINNs are computationally expensive and scale poorly with model size due to iterative gradient descent optimization, limiting their practical use in real-time financial modeling.

Method: Physics-Informed Extreme Learning Machines (PIELMs) replace the iterative optimization of PINNs with a single least-squares solve for deterministic and efficient training.

Result: PIELMs achieve accuracy comparable to PINNs while being up to 30x faster on Black-Scholes and Heston-Hull-White models for both forward pricing and inverse calibration tasks.

Conclusion: PIELMs offer significant computational advantages over PINNs for financial PDE solving, making them suitable for real-time applications in quantitative finance.

Abstract: Partial differential equation (PDE) solvers underpin modern quantitative
finance, governing option pricing and risk evaluation. Physics-Informed Neural
Networks (PINNs) have emerged as a promising approach for solving the forward
and inverse problems of partial differential equations (PDEs) using deep
learning. However they remain computationally expensive due to their iterative
gradient descent based optimization and scale poorly with increasing model
size. This paper introduces Physics-Informed Extreme Learning Machines (PIELMs)
as fast alternative to PINNs for solving both forward and inverse problems in
financial PDEs. PIELMs replace iterative optimization with a single
least-squares solve, enabling deterministic and efficient training. We
benchmark PIELM on the Black-Scholes and Heston-Hull-White models for forward
pricing and demonstrate its capability in inverse model calibration to recover
volatility and interest rate parameters from noisy data. From experiments we
observe that PIELM achieve accuracy comparable to PINNs while being up to
$30\times$ faster, highlighting their potential for real-time financial
modeling.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [69] [A weighted formulation of refined decoupling and inequalities of Mizohata-Takeuchi-type for the moment curve](https://arxiv.org/abs/2510.04345)
*Anthony Carbery,Zane Kun Li,Yixuan Pang,Po-Lam Yung*

Main category: math.CA

TL;DR: The paper proves improved Fourier extension estimates for curves in dimensions n≥3, making partial progress on the Mizohata-Takeuchi conjecture by establishing a weighted inequality with exponent a > (n-3)/2 + 2/n - 2/(n²(n+1)), which improves upon the previous bound a=n-1 from the Agmon-Hörmander trace inequality.


<details>
  <summary>Details</summary>
Motivation: To make progress on the Mizohata-Takeuchi conjecture for curves in higher dimensions (n≥3), which concerns Fourier extension estimates and has implications for understanding restriction phenomena in harmonic analysis.

Method: Uses weighted formulation of refined decoupling for well-curved curves as the main tool, building on axiomatic decoupling theory for curves.

Result: Established improved weighted Fourier extension inequality with exponent a > (n-3)/2 + 2/n - 2/(n²(n+1)) for curves in Rⁿ, representing partial progress on the Mizohata-Takeuchi conjecture.

Conclusion: The paper provides improved bounds for Fourier extension on curves in dimensions n≥3, discusses sharpness of exponents, and explores connections with axiomatic decoupling theory, advancing our understanding of restriction phenomena.

Abstract: Let $\Gamma$ be a compact patch of a well-curved $C^{n+1}$ curve in
$\mathbb{R}^n$ with induced Lebesgue measure ${\rm d} \lambda$, and let $g
\mapsto \widehat{g \,{\rm d}\lambda}$ be the Fourier extension operator for
$\Gamma$. Then we have, for arbitrary non-negative weights $w$,
\begin{equation*}
  \int_{B_R} |\widehat{g \,{\rm d}\lambda}|^2w \leq C_{n,a} R^{a} \sup_S
\left(\int_S w\right)\int_\Gamma |g|^2 \, {\rm d} \lambda
  \end{equation*} for any $a> \frac{n-3}{2} + \frac{2}{n} -
\frac{2}{n^2(n+1)}$, where the $\sup$ is over all $1$-neighbourhoods $S$ of
hyperplanes whose normals are parallel to the tangent at some point of
$\Gamma$. This represents partial progress on the Mizohata-Takeuchi conjecture
for curves in dimensions $n \geq 3$, improving upon the exponent $a=n-1$ which
can be obtained as a consequence of the Agmon-H\"ormander trace inequality. Our
main tool in establishing this inequality will be a weighted formulation of
refined decoupling for well-curved curves. We also discuss the sharpness of the
exponents we obtain in this and in auxiliary results, and further explore this
in the context of axiomatic decoupling for curves.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [70] [Reducibility and rational torsion in modular abelian varieties](https://arxiv.org/abs/2510.04323)
*Amod Agashe,Matthew Winters*

Main category: math.NT

TL;DR: This paper shows that for square-free N and weight 2 newforms f, if certain Galois representations are reducible, then the prime r divides the cuspidal subgroup order and there exists a nontrivial rational point in A[m].


<details>
  <summary>Details</summary>
Motivation: To understand connections between reducibility of Galois representations, cuspidal subgroups, and rational points on abelian varieties, with applications to the Birch and Swinnerton-Dyer conjecture.

Method: Using properties of newforms, abelian varieties associated to modular forms, Hecke algebras, and Galois representations to establish divisibility conditions and existence of rational points.

Result: When either A[m] or the canonical representation ρ_m is reducible, then r divides the order of the cuspidal subgroup of J₀(N) and A[m] has a nontrivial rational point.

Conclusion: The reducibility of certain Galois representations implies arithmetic consequences for cuspidal subgroups and rational points, with implications for the Birch and Swinnerton-Dyer conjecture.

Abstract: Let N be a square-free positive integer and let f be a newform of weight 2 on
\Gamma_0(N). Let A denote the abelian subvariety of J_0(N) associated to f and
let m be a maximal ideal of the Hecke algebra T that contains Ann_T(f) and has
residue characteristic r such that r does not divide 6N. We show that if either
A[m] or the canonical representation \rho_m over T/m associated to m is
reducible, then r divides the order of the cuspidal subgroup of J_0(N) and A[m]
has a nontrivial rational point. We mention some applications of this result,
including an application to the second part of the Birch and Swinnerton-Dyer
conjecture for A.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [71] [Optimal frames for Phase Retrieval from Edge Vectors of Optimal Polygons](https://arxiv.org/abs/2510.04099)
*Zhiqiang Xu,Zili Xu,Xinyue Zhang*

Main category: cs.IT

TL;DR: This paper characterizes optimal frames for phase retrieval in 2D real space by connecting them to the perimeter-maximizing isodiametric problem, showing that optimal solutions to this geometric problem yield optimal frames.


<details>
  <summary>Details</summary>
Motivation: To characterize optimal frames for phase retrieval and establish connections between phase retrieval and discrete geometry problems, specifically the perimeter-maximizing isodiametric problem.

Method: By revealing the connection between optimal frames for phase retrieval and the perimeter-maximizing isodiametric problem, and recasting the optimal polygons problem as one concerning the discrepancy of roots of unity.

Result: Characterized all optimal frames with m vectors in R² for phase retrieval when m ≥ 3 has an odd factor, and proved that harmonic frame E_m is not optimal for any even integer m ≥ 4.

Conclusion: The work establishes fundamental connections between phase retrieval and discrete geometry, disproves a previous conjecture about harmonic frames, and provides complete characterization of optimal frames under certain conditions.

Abstract: This paper aims to characterize the optimal frame for phase retrieval,
defined as the frame whose condition number for phase retrieval attains its
minimal value. In the context of the two-dimensional real case, we reveal the
connection between optimal frames for phase retrieval and the
perimeter-maximizing isodiametric problem, originally proposed by Reinhardt in
1922. Our work establishes that every optimal solution to the
perimeter-maximizing isodiametric problem inherently leads to an optimal frame
in ${\mathbb R}^2$. By recasting the optimal polygons problem as one concerning
the discrepancy of roots of unity, we characterize all optimal polygons.
Building upon this connection, we then characterize all optimal frames with $m$
vectors in ${\mathbb R}^2$ for phase retrieval when $m \geq 3$ has an odd
factor. As a key corollary, we show that the harmonic frame $E_m$ is {\em not}
optimal for any even integer $m \geq 4$. This finding disproves a conjecture
proposed by Xia, Xu, and Xu (Math. Comp., 90(356): 2931-2960). Previous work
has established that the harmonic frame $E_m \subset {\mathbb R}^2$ is indeed
optimal when $m$ is an odd integer.
  Exploring the connection between phase retrieval and discrete geometry, this
paper aims to illuminate advancements in phase retrieval and offer new
perspectives on the perimeter-maximizing isodiametric problem.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [72] [Filtered Quantum Phase Estimation](https://arxiv.org/abs/2510.04294)
*Gwonhak Lee,Minhyeok Kang,Jungsoo Hong,Stepan Fomichev,Joonsuk Huh*

Main category: quant-ph

TL;DR: A unified framework for filtered-state preparation that enhances eigenstate overlap through spectral filtering, including polynomial and trigonometric filters, with applications to quantum phase estimation that significantly reduces runtime.


<details>
  <summary>Details</summary>
Motivation: Accurate state preparation is a critical bottleneck in quantum algorithms, particularly for ground state energy estimation, where achieving sufficient overlap with desired eigenstates remains challenging even in fault-tolerant quantum computing.

Method: Developed a unified framework for filtered-state preparation using spectral filtering (polynomial and trigonometric realizations), introduced signal-processing-inspired filters like Gaussian filters and Krylov subspace-based filters, and created a filtered variant of QPE (FQPE) that mitigates dependence on initial overlap.

Result: Numerical experiments on Fermi-Hubbard models show FQPE reduces total runtime by more than two orders of magnitude in high-precision regime, with overlap amplification exceeding a factor of one hundred.

Conclusion: The filtered-state preparation framework provides an effective approach to overcome state preparation bottlenecks in quantum algorithms, particularly through FQPE which dramatically improves efficiency for ground state energy estimation.

Abstract: Accurate state preparation is a critical bottleneck in many quantum
algorithms, particularly those for ground state energy estimation. Even in
fault-tolerant quantum computing, preparing a quantum state with sufficient
overlap to the desired eigenstate remains a major challenge. To address this,
we develop a unified framework for filtered-state preparation that enhances the
overlap of a given input state through spectral filtering. This framework
encompasses the polynomial and trigonometric realizations of filters, allowing
a transparent analysis of the trade-offs between overlap amplification and
preparation cost. As examples, we introduce signal-processing-inspired filters,
such as Gaussian filters and Krylov subspace-based filters, that adaptively
suppress excited-state contributions using low-rank projections. Within this
framework, we further develop a filtered variant of QPE (FQPE) that mitigates
the unfavorable dependence on the initial overlap present in standard QPE.
Numerical experiments on Fermi-Hubbard models show that FQPE reduces the total
runtime by more than two orders of magnitude in the high-precision regime, with
overlap amplification exceeding a factor of one hundred.

</details>


### [73] [FewBodyToolkit.jl: a Julia package for solving quantum few-body problems](https://arxiv.org/abs/2510.04447)
*Lucas Happ*

Main category: quant-ph

TL;DR: FewBodyToolkit.jl is a Julia package for quantum few-body simulations using Gaussian expansion method, supporting 2-3 body systems with arbitrary interactions in various dimensions.


<details>
  <summary>Details</summary>
Motivation: To provide an accessible computational tool for studying quantum few-body systems that bridge single-particle and many-body physics regimes.

Method: Implementation based on the Gaussian expansion method, supporting general two- and three-body systems with arbitrary pair-interactions in various spatial dimensions.

Result: The package successfully calculates bound and resonant states, demonstrated through benchmarks and research examples with comprehensive documentation.

Conclusion: FewBodyToolkit.jl is a useful tool for research, teaching, benchmarking, and method development in quantum few-body physics.

Abstract: Few-body physics explores quantum systems of a small number of particles,
bridging the gap between single-particle and many-body regimes. To provide an
accessible tool for such studies, we present FewBodyToolkit.jl, a Julia package
for quantum few-body simulations. The package supports general two- and
three-body systems in various spatial dimensions with arbitrary
pair-interactions, and allows to calculate bound and resonant states. The
implementation is based on the well-established Gaussian expansion method and
we illustrate the package's capabilities through benchmarks and research
examples. The package comes with documentation and examples, making it useful
for research, teaching, benchmarking, and method development.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [74] [Analysis of kinetic Langevin Monte Carlo under the stochastic exponential Euler discretization from underdamped all the way to overdamped](https://arxiv.org/abs/2510.03949)
*Kyurae Kim,Samuel Gruffaz,Ji Won Park,Alain Oliviero Durmus*

Main category: stat.CO

TL;DR: This paper provides a refined analysis of kinetic Langevin Monte Carlo (KLMC) using exponential integrator discretization, showing it works in the overdamped regime with proper time acceleration, overcoming limitations of previous analyses.


<details>
  <summary>Details</summary>
Motivation: Previous analyses of KLMC with exponential integrator had restrictive parameter limitations and failed to explain behavior in the overdamped regime, suggesting the method degenerates in that limit.

Method: The authors revisit the synchronous Wasserstein coupling analysis of KLMC with exponential integrator, using refined analytical techniques to derive Wasserstein contractions and bounds on asymptotic bias.

Result: The refined analysis shows that exponential integrator can stably simulate kinetic Langevin dynamics in the overdamped regime when proper time acceleration is applied, with weaker parameter restrictions than previous work.

Conclusion: The exponential integrator for KLMC is capable of stable simulation in the overdamped regime, contrary to previous understanding, as long as appropriate time acceleration is used.

Abstract: Simulating the kinetic Langevin dynamics is a popular approach for sampling
from distributions, where only their unnormalized densities are available.
Various discretizations of the kinetic Langevin dynamics have been considered,
where the resulting algorithm is collectively referred to as the kinetic
Langevin Monte Carlo (KLMC) or underdamped Langevin Monte Carlo. Specifically,
the stochastic exponential Euler discretization, or exponential integrator for
short, has previously been studied under strongly log-concave and log-Lipschitz
smooth potentials via the synchronous Wasserstein coupling strategy. Existing
analyses, however, impose restrictions on the parameters that do not explain
the behavior of KLMC under various choices of parameters. In particular, all
known results fail to hold in the overdamped regime, suggesting that the
exponential integrator degenerates in the overdamped limit. In this work, we
revisit the synchronous Wasserstein coupling analysis of KLMC with the
exponential integrator. Our refined analysis results in Wasserstein
contractions and bounds on the asymptotic bias that hold under weaker
restrictions on the parameters, which assert that the exponential integrator is
capable of stably simulating the kinetic Langevin dynamics in the overdamped
regime, as long as proper time acceleration is applied.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [75] [Mixed Stochastic-Deterministic Density Functional Theoretic Decomposition of Kubo-Greenwood Conductivities in the Projector Augmented Wave Formalism](https://arxiv.org/abs/2510.03388)
*Vidushi Sharma,Lee A. Collins,Alexander J. White*

Main category: physics.chem-ph

TL;DR: Mixed stochastic-deterministic DFT (mDFT) combines accuracy and efficiency to study dynamic charge-transport in warm dense matter, using Kubo-Greenwood and real-time TD-mDFT approaches.


<details>
  <summary>Details</summary>
Motivation: To investigate dynamic charge-transport properties in warm dense matter systems with favorable computational scaling using a mixed stochastic-deterministic approach.

Method: Employed mDFT formalism with two complementary approaches: Kubo-Greenwood in mDFT picture and real-time Time-Dependent mDFT. Also developed decomposition of Onsager coefficients to analyze electronic state transitions.

Result: Computed optical conductivity spectra for single- and multi-component mixtures of carbon, hydrogen, and beryllium in warm dense matter regime.

Conclusion: mDFT provides an efficient framework for studying charge-transport in warm dense matter with decomposition revealing contributions from different electronic state transitions.

Abstract: Pairing the accuracy of Kohn-Sham density-functional framework with the
efficiency of a stochastic algorithmic approach, mixed stochastic-deterministic
Density Functional Theory (mDFT) achieves a favorable computational scaling
with system sizes and electronic temperatures. We employ the recently developed
mDFT formalism to investigate the dynamic charge-transport properties of
systems in the warm dense matter regime. The optical conductivity spectra are
computed for single- and multi- component mixtures of carbon, hydrogen, and
beryllium using two complementary approaches: Kubo-Greenwood in the mDFT
picture and real-time Time-Dependent mDFT. We further devise a decomposition of
the Onsager coefficients leading up to the Kubo-Greenwood spectra to exhibit
contributions from the deterministic, stochastic, and mixed electronic state
transitions at different incident photon energies.

</details>


### [76] [Mixed-precision ab initio tensor network state methods adapted for NVIDIA Blackwell technology via emulated FP64 arithmetic](https://arxiv.org/abs/2510.04795)
*Cole Brower,Samuel Rodriguez Bernabeu,Jeff Hammond,John Gunnels,Sotiris S. Xanthea,Martin Ganahl,Andor Menczer,Örs Legeza*

Main category: physics.chem-ph

TL;DR: Mixed-precision DMRG calculations using FP64 emulation via fixed-point arithmetic achieve chemical accuracy for large active spaces (up to CAS(113,76)) while providing a benchmark for new hardware.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that chemical accuracy can be achieved in large-scale electronic structure calculations using mixed-precision arithmetic with FP64 emulation, enabling the use of modern fixed-point hardware.

Method: Mixed-precision spin adapted ab initio DMRG using Ozaki scheme for FP64 emulation through fixed-point compute resources, approximating matrix/tensor algebra with fixed-point "slices", with systematic interpolation between double- and pseudo-half-precision.

Result: Chemical accuracy achieved for FeMoco (CAS(113,76)) and cytochrome P450 (CAS(63,58)) enzymes, with detailed numerical error analysis showing viability of mixed-precision approach.

Conclusion: DMRG provides ideal benchmarking for new hardware and numerical libraries, and this work enables utilization of Blackwell technology in tensor network state calculations, opening new research directions.

Abstract: We report cutting-edge performance results via mixed-precision spin adapted
ab initio Density Matrix Renormalization Group (DMRG) electronic structure
calculations utilizing the Ozaki scheme for emulating FP64 arithmetic through
the use of fixed-point compute resources. By approximating the underlying
matrix and tensor algebra with operations on a modest number of fixed-point
representatives (``slices''), we demonstrate on smaller benchmark systems and
for the active compounds of the FeMoco and cytochrome P450 (CYP) enzymes with
complete active space (CAS) sizes of up to 113 electrons in 76 orbitals
[CAS(113, 76)] and 63 electrons in 58 orbitals [CAS(63, 58)], respectively,
that the chemical accuracy can be reached with mixed-precision arithmetic. We
also show that, due to its variational nature, DMRG provides an ideal tool to
benchmark accuracy domains, as well as the performance of new hardware
developments and related numerical libraries. Detailed numerical error analysis
and performance assessment are also presented for subcomponents of the DMRG
algebra by systematically interpolating between double- and
pseudo-half-precision. Our analyis represents the first quantum chemistry
evaluation of FP64 emulation for correlated calculations capable of achieving
chemical accuracy and emulation based on fixed-point arithmetic, and it paves
the way for the utilization of state-of-the-art Blackwell technology in
tree-like tensor network state electronic structure calculations, opening new
research directions in materials sciences and beyond.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [Fast frequency reconstruction using Deep Learning for event recognition in ring laser data](https://arxiv.org/abs/2510.03325)
*Giuseppe Di Somma,Giorgio Carelli,Angela D. V. Di Virgilio,Francesco Fuso,Enrico Maccioni,Paolo Marsili*

Main category: cs.LG

TL;DR: Neural network approach for fast frequency reconstruction from sinusoidal signals, achieving ~10ms processing time and 2x better precision than Fourier methods, with automated classification of physical disturbances.


<details>
  <summary>Details</summary>
Motivation: Need for rapid frequency reconstruction from sinusoidal signals in applications like Ring Laser Gyroscopes, where conventional methods require several seconds of data.

Method: Neural network approach for frequency estimation and automated classification framework to identify physical disturbances like laser instabilities and seismic events.

Result: Frequency reconstruction within ~10ms (vs seconds for conventional methods), 2x improvement in frequency estimation precision, and 99-100% accuracy for seismic event classification on test datasets.

Conclusion: Successful integration of AI into signal analysis for geophysical applications, enabling rapid trigger generation and improved disturbance identification.

Abstract: The reconstruction of a frequency with minimal delay from a sinusoidal signal
is a common task in several fields; for example Ring Laser Gyroscopes, since
their output signal is a beat frequency. While conventional methods require
several seconds of data, we present a neural network approach capable of
reconstructing frequencies of several hundred Hertz within approximately 10
milliseconds. This enables rapid trigger generation. The method outperforms
standard Fourier-based techniques, improving frequency estimation precision by
a factor of 2 in the operational range of GINGERINO, our Ring Laser
Gyroscope.\\ In addition to fast frequency estimation, we introduce an
automated classification framework to identify physical disturbances in the
signal, such as laser instabilities and seismic events, achieving accuracy
rates between 99\% and 100\% on independent test datasets for the seismic
class. These results mark a step forward in integrating artificial intelligence
into signal analysis for geophysical applications.

</details>


### [78] [Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation](https://arxiv.org/abs/2510.03426)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: GOOMs enable stable computation over large dynamic ranges of real numbers, outperforming traditional floating-point approaches in compounding operations, Lyapunov exponent estimation, and deep recurrent neural networks.


<details>
  <summary>Details</summary>
Motivation: Many domains require compounding real numbers over long sequences, leading to catastrophic numerical underflow or overflow with traditional floating-point numbers.

Method: Introduce generalized orders of magnitude (GOOMs) with efficient custom parallel prefix scan implementation for native execution on parallel hardware like GPUs.

Result: GOOMs enable previously impractical/impossible experiments: compounding real matrix products beyond floating-point limits, faster Lyapunov exponent estimation with selective-resetting, and stable long-range dependencies in deep RNNs without stabilization.

Conclusion: GOOMs combined with efficient parallel scanning offer a scalable and numerically robust alternative to conventional floating-point numbers for high-dynamic-range applications.

Abstract: Many domains, from deep learning to finance, require compounding real numbers
over long sequences, often leading to catastrophic numerical underflow or
overflow. We introduce generalized orders of magnitude (GOOMs), a principled
extension of traditional orders of magnitude that incorporates floating-point
numbers as a special case, and which in practice enables stable computation
over significantly larger dynamic ranges of real numbers than previously
possible. We implement GOOMs, along with an efficient custom parallel prefix
scan, to support native execution on parallel hardware such as GPUs. We
demonstrate that our implementation of GOOMs outperforms traditional approaches
with three representative experiments, all of which were previously considered
impractical or impossible, and now become possible and practical: (1)
compounding real matrix products far beyond standard floating-point limits; (2)
estimating spectra of Lyapunov exponents in parallel, orders of magnitude
faster than with previous methods, applying a novel selective-resetting method
to prevent state colinearity; and (3) capturing long-range dependencies in deep
recurrent neural networks with non-diagonal recurrent states, computed in
parallel via a prefix scan, without requiring any form of stabilization. Our
results show that our implementation of GOOMs, combined with efficient parallel
scanning, offers a scalable and numerically robust alternative to conventional
floating-point numbers for high-dynamic-range applications.

</details>


### [79] [Sequential decoder training for improved latent space dynamics identification](https://arxiv.org/abs/2510.03535)
*William Anderson,Seung Whan Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: Multi-stage LaSDI (mLaSDI) improves reduced-order modeling by sequentially learning additional decoders to correct residual errors, achieving better accuracy and faster training than standard LaSDI for the Vlasov equation.


<details>
  <summary>Details</summary>
Motivation: Standard LaSDI framework can compromise reconstruction accuracy when enforcing latent dynamics during training, creating a need for improved methods that maintain both interpretable dynamics and high accuracy.

Method: Multi-stage LaSDI sequentially learns additional decoders to correct residual errors from previous stages, building upon the autoencoder and equation discovery framework of standard LaSDI.

Result: mLaSDI consistently outperforms standard LaSDI on the 1D-1V Vlasov equation, achieving lower prediction errors and reduced training time across various architectures.

Conclusion: The multi-stage approach effectively improves both reconstruction and prediction accuracy while maintaining the interpretable latent dynamics framework, making it a superior alternative to standard LaSDI.

Abstract: Accurate numerical solutions of partial differential equations are essential
in many scientific fields but often require computationally expensive solvers,
motivating reduced-order models (ROMs). Latent Space Dynamics Identification
(LaSDI) is a data-driven ROM framework that combines autoencoders with equation
discovery to learn interpretable latent dynamics. However, enforcing latent
dynamics during training can compromise reconstruction accuracy of the model
for simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that
improves reconstruction and prediction accuracy by sequentially learning
additional decoders to correct residual errors from previous stages. Applied to
the 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI,
achieving lower prediction errors and reduced training time across a wide range
of architectures.

</details>


### [80] [LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design](https://arxiv.org/abs/2510.03650)
*Amir Sadikov*

Main category: cs.LG

TL;DR: LLM-guided evolutionary program synthesis automates the discovery of high-quality quasi-Monte Carlo constructions, improving finite point sets and Sobol' sequences for numerical integration.


<details>
  <summary>Details</summary>
Motivation: To solve long-standing QMC design problems by automating the construction of low-discrepancy point sets and optimal Sobol' direction numbers, which are traditionally challenging to design manually.

Method: Two-phase procedure combining constructive code proposals with iterative numerical refinement, using an LLM-guided evolutionary loop that mutates and selects code under task-specific fitness functions.

Result: Achieved new best-known 2D benchmarks for N >= 40, matched most known 3D optima up to N <= 8, and improved 3D benchmarks beyond. For Sobol' sequences, consistently reduced rQMC error in 32-dimensional option pricing compared to Joe-Kuo parameters.

Conclusion: LLM-driven evolutionary program synthesis can automate QMC construction discovery, recovering classical designs where optimal and improving them where finite-N structure matters, demonstrating practical automation of numerical design problems.

Abstract: Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo
(QMC) methods for high-dimensional integration. We cast two long-standing QMC
design problems as program synthesis and solve them with an LLM-guided
evolutionary loop that mutates and selects code under task-specific fitness:
(i) constructing finite 2D/3D point sets with low star discrepancy, and (ii)
choosing Sobol' direction numbers that minimize randomized QMC error on
downstream integrands. Our two-phase procedure combines constructive code
proposals with iterative numerical refinement. On finite sets, we rediscover
known optima in small 2D cases and set new best-known 2D benchmarks for N >=
40, while matching most known 3D optima up to the proven frontier (N <= 8) and
reporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol'
parameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC)
mean-squared error for several 32-dimensional option-pricing tasks relative to
widely used Joe--Kuo parameters, while preserving extensibility to any sample
size and compatibility with standard randomizations. Taken together, the
results demonstrate that LLM-driven evolutionary program synthesis can automate
the discovery of high-quality QMC constructions, recovering classical designs
where they are optimal and improving them where finite-N structure matters.
Data and code are available at
https://github.com/hockeyguy123/openevolve-star-discrepancy.git.

</details>


### [81] [Neural Low-Discrepancy Sequences](https://arxiv.org/abs/2510.03745)
*Michael Etienne Van Huffel,Nathan Kirk,Makram Chahine,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: NeuroLDS is the first machine learning framework for generating low-discrepancy sequences (LDS) that outperform classical constructions, enabling applications in numerical integration, robotics, and scientific ML.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning approaches like MPMC can only generate point sets but not sequences where every prefix has low discrepancy, which is essential for many applications. Classical LDS constructions rely on abstract algebra and number theory, limiting their performance.

Method: Train a neural network to map indices to points using a two-stage process: supervised approximation of classical constructions followed by unsupervised fine-tuning to minimize prefix discrepancies across all sequence lengths.

Result: NeuroLDS significantly outperforms all previous LDS constructions in discrepancy measures and demonstrates effectiveness in numerical integration, robot motion planning, and scientific machine learning applications.

Conclusion: The framework shows promise for broad applications and represents a significant advancement over traditional methods for generating low-discrepancy sequences.

Abstract: Low-discrepancy points are designed to efficiently fill the space in a
uniform manner. This uniformity is highly advantageous in many problems in
science and engineering, including in numerical integration, computer vision,
machine perception, computer graphics, machine learning, and simulation.
Whereas most previous low-discrepancy constructions rely on abstract algebra
and number theory, Message-Passing Monte Carlo (MPMC) was recently introduced
to exploit machine learning methods for generating point sets with lower
discrepancy than previously possible. However, MPMC is limited to generating
point sets and cannot be extended to low-discrepancy sequences (LDS), i.e.,
sequences of points in which every prefix has low discrepancy, a property
essential for many applications. To address this limitation, we introduce
Neural Low-Discrepancy Sequences ($NeuroLDS$), the first machine learning-based
framework for generating LDS. Drawing inspiration from classical LDS, we train
a neural network to map indices to points such that the resulting sequences
exhibit minimal discrepancy across all prefixes. To this end, we deploy a
two-stage learning process: supervised approximation of classical constructions
followed by unsupervised fine-tuning to minimize prefix discrepancies. We
demonstrate that $NeuroLDS$ outperforms all previous LDS constructions by a
significant margin with respect to discrepancy measures. Moreover, we
demonstrate the effectiveness of $NeuroLDS$ across diverse applications,
including numerical integration, robot motion planning, and scientific machine
learning. These results highlight the promise and broad significance of Neural
Low-Discrepancy Sequences. Our code can be found at
https://github.com/camail-official/neuro-lds.

</details>


### [82] [A Mathematical Explanation of Transformers for Large Language Models and GPTs](https://arxiv.org/abs/2510.03989)
*Xue-Cheng Tai,Hao Liu,Lingfeng Li,Raymond H. Chan*

Main category: cs.LG

TL;DR: The paper proposes a continuous mathematical framework that interprets the Transformer architecture as a discretization of a structured integro-differential equation, providing a unified theoretical foundation.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive mathematical theory that explains the Transformer architecture's structure and operations, which currently lacks rigorous theoretical understanding despite its revolutionary impact on sequence modeling and LLMs.

Method: The authors formulate a continuous framework where the Transformer is interpreted as a discretization of a structured integro-differential equation. Self-attention emerges as a non-local integral operator, and layer normalization is characterized as projection to time-dependent constraints.

Result: The framework provides a unified and interpretable foundation for understanding Transformer components (attention, feedforward layers, normalization) by embedding the entire operation in continuous domains for both token indices and feature dimensions.

Conclusion: This work bridges the gap between deep learning architectures and continuous mathematical modeling, offering new directions for architecture design, analysis, and control-based interpretations while contributing to theoretically grounded neural network models.

Abstract: The Transformer architecture has revolutionized the field of sequence
modeling and underpins the recent breakthroughs in large language models
(LLMs). However, a comprehensive mathematical theory that explains its
structure and operations remains elusive. In this work, we propose a novel
continuous framework that rigorously interprets the Transformer as a
discretization of a structured integro-differential equation. Within this
formulation, the self-attention mechanism emerges naturally as a non-local
integral operator, and layer normalization is characterized as a projection to
a time-dependent constraint. This operator-theoretic and variational
perspective offers a unified and interpretable foundation for understanding the
architecture's core components, including attention, feedforward layers, and
normalization. Our approach extends beyond previous theoretical analyses by
embedding the entire Transformer operation in continuous domains for both token
indices and feature dimensions. This leads to a principled and flexible
framework that not only deepens theoretical insight but also offers new
directions for architecture design, analysis, and control-based
interpretations. This new interpretation provides a step toward bridging the
gap between deep learning architectures and continuous mathematical modeling,
and contributes a foundational perspective to the ongoing development of
interpretable and theoretically grounded neural network models.

</details>


### [83] [Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws](https://arxiv.org/abs/2510.04102)
*Ramzi Dakhmouche,Hossein Gorji*

Main category: cs.LG

TL;DR: The paper identifies a fundamental property that explains why deep learning models struggle with extrapolation in time series forecasting, contrasting with physical laws that have strong extrapolation capabilities.


<details>
  <summary>Details</summary>
Motivation: Foundation Models (FMs) have succeeded in short-range time series forecasting but fail at long-range extrapolation, performing worse than simple baselines. This contrasts with physical laws' strong extrapolation properties, raising questions about fundamental differences between neural networks and physical laws.

Method: The authors identify and formalize a fundamental property characterizing statistical learning models' ability to predict accurately outside their training domain. They provide theoretical analysis and empirical results demonstrating this property's implications on current deep learning architectures.

Result: The research clarifies the root causes of the extrapolation gap in deep learning models and shows how their performance deteriorates in extrapolation settings compared to their training domain performance.

Conclusion: The findings not only explain the extrapolation gap but also suggest directions for designing next-generation forecasting models capable of mastering extrapolation, potentially bridging the gap between neural networks and physical laws.

Abstract: Motivated by the remarkable success of Foundation Models (FMs) in language
modeling, there has been growing interest in developing FMs for time series
prediction, given the transformative power such models hold for science and
engineering. This culminated in significant success of FMs in short-range
forecasting settings. However, extrapolation or long-range forecasting remains
elusive for FMs, which struggle to outperform even simple baselines. This
contrasts with physical laws which have strong extrapolation properties, and
raises the question of the fundamental difference between the structure of
neural networks and physical laws. In this work, we identify and formalize a
fundamental property characterizing the ability of statistical learning models
to predict more accurately outside of their training domain, hence explaining
performance deterioration for deep learning models in extrapolation settings.
In addition to a theoretical analysis, we present empirical results showcasing
the implications of this property on current deep learning architectures. Our
results not only clarify the root causes of the extrapolation gap but also
suggest directions for designing next-generation forecasting models capable of
mastering extrapolation.

</details>


### [84] [Can Linear Probes Measure LLM Uncertainty?](https://arxiv.org/abs/2510.04108)
*Ramzi Dakhmouche,Adrien Letellier,Hossein Gorji*

Main category: cs.LG

TL;DR: The paper proposes a Bayesian approach using linear regression models to improve uncertainty quantification in LLMs for multiple-choice tasks, outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: Current uncertainty quantification for LLMs in multiple-choice scenarios is dominated by naive maximum softmax score baselines, which is insufficient for reliable deployment in automated decision-making.

Method: Train multiple Bayesian linear models that predict each layer's output from the previous layer, then use layer-level posterior distributions to infer global uncertainty through sparse combination of distributional features.

Result: Numerical experiments on various LLMs show consistent improvement over state-of-the-art baselines in uncertainty quantification.

Conclusion: A principled Bayesian approach with simple linear models can effectively improve uncertainty quantification in LLMs for multiple-choice generation tasks.

Abstract: Effective Uncertainty Quantification (UQ) represents a key aspect for
reliable deployment of Large Language Models (LLMs) in automated
decision-making and beyond. Yet, for LLM generation with multiple choice
structure, the state-of-the-art in UQ is still dominated by the naive baseline
given by the maximum softmax score. To address this shortcoming, we demonstrate
that taking a principled approach via Bayesian statistics leads to improved
performance despite leveraging the simplest possible model, namely linear
regression. More precisely, we propose to train multiple Bayesian linear
models, each predicting the output of a layer given the output of the previous
one. Based on the obtained layer-level posterior distributions, we infer the
global uncertainty level of the LLM by identifying a sparse combination of
distributional features, leading to an efficient UQ scheme. Numerical
experiments on various LLMs show consistent improvement over state-of-the-art
baselines.

</details>


### [85] [PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression](https://arxiv.org/abs/2510.04205)
*Di Zhang*

Main category: cs.LG

TL;DR: PolyKAN provides a theoretical framework for compressing Kolmogorov-Arnold Networks (KANs) with formal guarantees on model size reduction and approximation error through optimal polyhedral region merging.


<details>
  <summary>Details</summary>
Motivation: KANs offer enhanced interpretability over MLPs but suffer from parameter inefficiency, limiting practical deployment. There is a need for compression methods with mathematical guarantees.

Method: Leverages the piecewise polynomial structure of KANs, formulates compression as optimal polyhedral region merging, establishes polyhedral characterization, develops ε-equivalent compression theory, and designs an optimal dynamic programming algorithm.

Result: PolyKAN achieves provably minimal compression with strict error control and polynomial-time complexity, providing the first formal foundation for KAN compression with mathematical guarantees.

Conclusion: The framework enables efficient deployment of interpretable neural architectures and opens new directions for KAN compression with rigorous mathematical foundations.

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
traditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability
and a strong mathematical foundation. However, their parameter efficiency
remains a significant challenge for practical deployment. This paper introduces
PolyKAN, a novel theoretical framework for KAN compression that provides formal
guarantees on both model size reduction and approximation error. By leveraging
the inherent piecewise polynomial structure of KANs, we formulate the
compression problem as one of optimal polyhedral region merging. We establish a
rigorous polyhedral characterization of KANs, develop a complete theory of
$\epsilon$-equivalent compression, and design an optimal dynamic programming
algorithm that guarantees minimal compression under specified error bounds. Our
theoretical analysis demonstrates that PolyKAN achieves provably minimal
compression while maintaining strict error control, with polynomial-time
complexity in all network parameters. The framework provides the first formal
foundation for KAN compression with mathematical guarantees, opening new
directions for efficient deployment of interpretable neural architectures.

</details>


### [86] [Power Transform Revisited: Numerically Stable, and Federated](https://arxiv.org/abs/2510.04995)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: Power transforms suffer from numerical instabilities in direct implementations. This paper analyzes the instability sources, proposes remedies, and extends power transforms to federated learning.


<details>
  <summary>Details</summary>
Motivation: Power transforms are widely used for making data Gaussian-like in preprocessing, but direct implementations have severe numerical instabilities that can cause incorrect results or crashes.

Method: Comprehensive analysis of instability sources and development of effective remedies. Extension of power transforms to federated learning setting, addressing both numerical and distributional challenges.

Result: Experiments on real-world datasets show the methods are effective and robust, with substantial stability improvements compared to existing approaches.

Conclusion: The proposed methods successfully address numerical instabilities in power transforms and enable their reliable use in federated learning contexts.

Abstract: Power transforms are popular parametric techniques for making data more
Gaussian-like, and are widely used as preprocessing steps in statistical
analysis and machine learning. However, we find that direct implementations of
power transforms suffer from severe numerical instabilities, which can lead to
incorrect results or even crashes. In this paper, we provide a comprehensive
analysis of the sources of these instabilities and propose effective remedies.
We further extend power transforms to the federated learning setting,
addressing both numerical and distributional challenges that arise in this
context. Experiments on real-world datasets demonstrate that our methods are
both effective and robust, substantially improving stability compared to
existing approaches.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [87] [Assessment of Hybrid RANS-LES and WALE Formulations for Wake and Resistance Prediction of the BB2 Submarine](https://arxiv.org/abs/2510.03383)
*Noh Zainal Abidin,Frederic Grondin,Pol Muller,Jean-François Sigrist*

Main category: physics.flu-dyn

TL;DR: This paper analyzes submarine hydrodynamics using hybrid RANS-LES models to better capture flow separation, wake structures, and turbulence at high Reynolds numbers, addressing limitations of traditional RANS approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional RANS methods fail to resolve unsteady flow structures and turbulence in submarine hydrodynamics, particularly for predicting flow separation, wake structure, and resistance at high Reynolds numbers. Hybrid models are needed to improve accuracy for hydroacoustic analysis and stealth characteristics.

Method: Used hybrid turbulence models (DES, LES, IDDES) with wall-resolved and wall-modelled meshes (56-76 million cells) at different y+ values. Parameterized meshes based on Taylor microscale refinement and employed WALE model to resolve turbulence in wake regions at Re=3.6×10^6.

Result: Hybrid models successfully captured asymmetric vortex shedding and detailed turbulent structures that RANS missed. Wall-resolved mesh (y+<5) and wall-modelled mesh (y+>30) both produced more detailed vortices and turbulence fluctuations than steady-state RANS while maintaining accuracy for global resistance prediction.

Conclusion: Hybrid RANS-LES models like DES and IDDES are superior to traditional RANS for submarine hydrodynamics, effectively resolving complex 3D vortex structures and unsteady flow phenomena essential for hydroacoustic analysis and stealth optimization.

Abstract: Submarine hydrodynamics presents unique challenges in accurately predicting
flow separation, wake structure, and resistance due to complex geometry and
turbulent behaviour at high Reynolds (Re) numbers. Traditional
Reynolds-Averaged Navier-Stokes (RANS) approaches are often limited in
resolving unsteady flow structures and turbulence in the near and far regions.
To address these limitations, hybrid RANS-LES models such as Detached Eddy
Simulation (DES) and Large Eddy Simulation (LES) offer improved performance in
capturing near-wall vortical structures. The capturing of turbulent vortices
and wake structures significantly contributes to conduct hydrodynamic noise
analysis. Detailed resolution and understanding of these coherent structures
help minimize hydroacoustic signatures, essential for submarines' stealth
characteristics. Based on prior studies, Breuer et al. (2003) reported that
RANS failed to capture unsteady vortex shedding, producing only steady results
even in 3D simulations. In contrast, DES and LES successfully resolved
asymmetric shedding across different grid resolutions. Spalart (2009) reported
that DES is more effective than RANS or LES for high Re flows, although it
suffers from challenges related to ambiguous grids and nonmonotonic grid
refinement behaviour. Whereas Liang \& Xue (2014) found that DES predicts tip
vortex flow characteristics more accurately than RANS-SA and can capture
complex 3D vortex structures. In addition, Guilmineau et al. (2018)
demonstrated that the IDDES model accurately predicts recirculation bubbles and
aligns more closely with experimental data for flow prediction. Long et al.
(2021) confirmed the capability of DDES in simulating cavitating flows around
hydrofoils and marine propellers. Lungu (2022) highlighted the efficiency and
accuracy of the hybrid IDDES-SST model in DARPA submarine simulations. Zhang et
al. (2023) also noted that URANS struggles with resolving small-scale
turbulence structures, whereas IDDES is better suited for predicting complex
phenomena such as ship air wake asymmetry. Nevertheless, capturing the
unsteadiness and turbulence fluctuations scales is extremely challenging
because the cell size requirements should suit each turbulence model employed.
Thus, as a continuation of the prior work of Abidin et al. (2024), the unsteady
simulation with high mesh resolution at $U_m=1.8235$ m/s and Re of $3.6\times
10^6$ to generate the asymmetrical wake dynamic and vortical structure. The
current research expands the methodology by parameterizing the meshes and
numerical scheme based on Taylor microscale refinement with respect to the
characteristic length of (L, B and D) of submarine, particularly focusing on
hybrid turbulence model and WALE to observe the ability to resolve turbulence
in the wake region. The transient simulations were performed initially using
wall-resolved mesh (76x10^6 cells) at y^+ < 5 and then wall-modelled mesh
(56x10^6 to 74x10^6 cells) at y^+ > 30, which produced notably different and
more detailed results (vortices and turbulence fluctuation) than previous
steady-state RANS simulations without risking the accuracy of quantity of
interest (global resistance).

</details>


### [88] [Consistent kinetic modeling of compressible flows with variable Prandtl numbers: Double-distribution quasi-equilibrium approach](https://arxiv.org/abs/2510.04197)
*R. M. Strässle,S. A. Hosseini,I. V. Karlin*

Main category: physics.flu-dyn

TL;DR: Developed a consistent kinetic modeling and discretization strategy for compressible flows across all Prandtl numbers and specific heat ratios using quasi-equilibrium approach in double-distribution frameworks.


<details>
  <summary>Details</summary>
Motivation: To create an accurate and efficient framework for kinetic simulations of compressible flows that works across all Prandtl numbers and specific heat ratios, enabling study of complex fluid dynamics problems.

Method: Used quasi-equilibrium approach within double-distribution frameworks with detailed hydrodynamic limit analysis, careful construction of equilibrium and quasi-equilibrium attractors, and high-order velocity lattices with static reference frame in discrete velocity Boltzmann context.

Result: Models demonstrate high accuracy, numerical stability, and Galilean invariance across wide Mach numbers and temperature ratios. Simulations successfully reproduce viscous Navier-Stokes-Fourier-level physics in sensitive shock-vortex interactions.

Conclusion: The proposed models establish an accurate, efficient and scalable framework for kinetic simulations of compressible flows with moderate supersonic speeds and discontinuities, offering a valuable tool for fluid dynamics and paving way for future extensions.

Abstract: A consistent kinetic modeling and discretization strategy for compressible
flows across all Prandtl numbers and specific heat ratios is developed using
the quasi-equilibrium approach within two of the most widely used
double-distribution frameworks. The methodology ensures accurate recovery of
the Navier-Stokes-Fourier equations, including all macroscopic moments and
dissipation rates, through detailed hydrodynamic limit analysis and careful
construction of equilibrium and quasi-equilibrium attractors. Discretization is
performed using high-order velocity lattices with a static reference frame in a
discrete velocity Boltzmann context to isolate key modeling aspects such as the
necessary requirements on expansion and quadrature orders. The proposed models
demonstrate high accuracy, numerical stability and Galilean invariance across a
wide range of Mach numbers and temperature ratios. Separate tests for strict
conservation and measurements of all dissipation rates confirm these insights
for all Prandtl numbers and specific heat ratios. Simulations on a sensitive
two-dimensional shock-vortex interaction excellently reproduce viscous
Navier-Stokes-Fourier-level physics. The proposed models establish an accurate,
efficient and scalable framework for kinetic simulations of compressible flows
with moderate supersonic speeds and discontinuities at arbitrary Prandtl
numbers and specific heat ratios, offering a valuable tool for studying complex
problems in fluid dynamics and paving the way for future extensions to the
lattice Boltzmann context, by application of correction terms, as well as
high-Mach and hypersonic regimes, employing target-designed reference frames.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [89] [Machine Learning and Control: Foundations, Advances, and Perspectives](https://arxiv.org/abs/2510.03303)
*Enrique Zuazua*

Main category: math.OC

TL;DR: Control theory provides insights into deep neural networks' properties, explores depth-width tradeoffs, introduces hybrid modeling methods, and explains generative AI success through diffusion processes.


<details>
  <summary>Details</summary>
Motivation: To apply control theory concepts to understand deep neural networks and machine learning architectures, bridging control theory with machine learning, numerical analysis, and PDEs.

Method: Uses simultaneous/ensemble controllability for network analysis, explores depth-width relationships via turnpike concept, develops HYCO hybrid modeling combining mechanics and data, and applies diffusion process theory.

Result: Establishes connections between control theory and neural networks, provides theoretical foundations for network properties, creates hybrid modeling framework, and explains generative AI mechanisms.

Conclusion: Control theory, machine learning, numerical analysis, and PDEs form a fertile interdisciplinary research area with significant potential for advancing neural network understanding and applications.

Abstract: Control theory of dynamical systems offers a powerful framework for tackling
challenges in deep neural networks and other machine learning architectures. We
show that concepts such as simultaneous and ensemble controllability offer new
insights into the classification and representation properties of deep neural
networks while the control and optimization of static systems can be employed
to better understand the performance of shallow networks. Inspired by the
classical concept of turnpike, we also explore the relationship between dynamic
and static neural networks, where depth is traded for width, and the role of
transformers as mechanisms for accelerating classical neural network tasks. We
also exploit the expressive power of neural networks (exemplified, for
instance, by the Universal Approximation Theorem) to develop a novel hybrid
modeling methodology, the Hybrid-Cooperative Learning (HYCO), combining
mechanics and data-driven methods in a game-theoretic setting. Finally, we
describe how classical properties of diffusion processes, long established in
the context of partial differential equations, contribute to explaining the
success of modern generative artificial intelligence (AI). We present an
overview of our recent results in these areas, illustrating how control,
machine learning, numerical analysis, and partial differential equations come
together to motivate a fertile ground for future research.

</details>


### [90] [Well-Posedness and Efficient Algorithms for Inverse Optimal Transport with Bregman Regularization](https://arxiv.org/abs/2510.03803)
*Chenglong Bao,Zanyu Li,Yunan Yang*

Main category: math.OC

TL;DR: Analysis of inverse optimal transport under Bregman regularization, establishing well-posedness and proposing an efficient inexact block coordinate descent method with linear convergence.


<details>
  <summary>Details</summary>
Motivation: To address the inverse optimal transport problem with regularization, establishing theoretical foundations and developing efficient computational methods for practical applications.

Method: Established well-posedness results (existence, uniqueness, stability) under structural assumptions on cost matrix. Proposed inexact block coordinate descent method with strongly convex penalty, using element-wise Newton updates for quadratic penalties.

Result: Proved existence and uniqueness of solutions under sufficient conditions. Developed efficient algorithm with linear convergence rate. Demonstrated practical performance through numerical experiments including stability validation and marriage matching applications.

Conclusion: The proposed framework provides rigorous theoretical guarantees and efficient computational methods for inverse optimal transport with Bregman regularization, enabling practical applications across various domains.

Abstract: This work analyzes the inverse optimal transport (IOT) problem under Bregman
regularization. We establish well-posedness results, including existence,
uniqueness (up to equivalence classes of solutions), and stability, under
several structural assumptions on the cost matrix. On the computational side,
we investigate the existence of solutions to the optimization problem with
general constraints on the cost matrix and provide a sufficient condition
guaranteeing existence. In addition, we propose an inexact block coordinate
descent (BCD) method for the problem with a strongly convex penalty term. In
particular, when the penalty is quadratic, the subproblems admit a diagonal
Hessian structure, which enables highly efficient element-wise Newton updates.
We establish a linear convergence rate for the algorithm and demonstrate its
practical performance through numerical experiments, including the validation
of stability bounds, the investigation of regularization effects, and the
application to a marriage matching dataset.

</details>


### [91] [Rapid stabilization for a wave equation with boundary disturbance](https://arxiv.org/abs/2510.04893)
*Patricio Guzmán,Agustín Huerta,Hugo Parada*

Main category: math.OC

TL;DR: Rapid stabilization of unstable wave equations with unknown boundary disturbances using backstepping and Lyapunov methods.


<details>
  <summary>Details</summary>
Motivation: Address stabilization of unstable wave equations when unknown disturbances affect boundary conditions, requiring robust control that suppresses disturbance effects.

Method: Used backstepping method, Lyapunov techniques, and sign multivalued operator for feedback design; employed maximal monotone operator theory for well-posedness analysis.

Result: Designed feedback laws that achieve exponential decay of energy at any desired rate while suppressing unknown boundary disturbances for both Dirichlet-Dirichlet and Dirichlet-Neumann cases.

Conclusion: Successfully developed stabilization methods for wave equations with unknown boundary disturbances, proving well-posedness of the resulting differential inclusion system.

Abstract: In this paper, we study the rapid stabilization of an unstable wave equation,
in which an unknown disturbance is located at the boundary condition. We
address two different boundary conditions: Dirichlet- Dirichlet and
Dirichlet-Neumann. In both cases, we design a feedback law, located at the same
place as the unknown disturbance, that forces the exponential decay of the
energy for any desired decay rate while suppressing the effects of the unknown
disturbance. For the feedback design, we employ the backstepping method,
Lyapunov techniques and the sign multivalued operator. The well-posedness of
the closed-loop system, which is a differential inclusion, is shown with the
maximal monotone operator theory.

</details>


### [92] [Overlapping Schwarz Scheme for Linear-Quadratic Programs in Continuous Time](https://arxiv.org/abs/2510.04478)
*Hongli Zhao,Mihai Anitescu,Sen Na*

Main category: math.OC

TL;DR: An optimize-then-discretize framework for linear-quadratic optimal control problems using overlapping Schwarz decomposition based on Pontryagin Minimum Principle, with convergence ensured by boundary condition updates and exponential decay of sensitivity.


<details>
  <summary>Details</summary>
Motivation: To develop a flexible framework for solving linear-quadratic optimal control problems governed by time-inhomogeneous ODEs that can incorporate different numerical integration methods, unlike the discretize-then-optimize approach.

Method: Modified overlapping Schwarz decomposition based on Pontryagin Minimum Principle, partitioning temporal domain into overlapping intervals and independently solving Hamiltonian systems in continuous time with boundary condition updates.

Result: The method ensures convergence by appropriately updating boundary conditions and proves that exponential decay of sensitivity from discrete-time OCPs carries over to continuous-time setting.

Conclusion: The framework provides a practical approach for linear-quadratic OCPs that can flexibly use different numerical integration methods, including adaptive-time integrators, as demonstrated in numerical experiments.

Abstract: We present an optimize-then-discretize framework for solving linear-quadratic
optimal control problems (OCP) governed by time-inhomogeneous ordinary
differential equations (ODEs). Our method employs a modified overlapping
Schwarz decomposition based on the Pontryagin Minimum Principle, partitioning
the temporal domain into overlapping intervals and independently solving
Hamiltonian systems in continuous time. We demonstrate that the convergence is
ensured by appropriately updating the boundary conditions of the individual
Hamiltonian dynamics. The cornerstone of our analysis is to prove that the
exponential decay of sensitivity (EDS) exhibited in discrete-time OCPs carries
over to the continuous-time setting. Unlike the discretize-then-optimize
approach, our method can flexibly incorporate different numerical integration
methods for solving the resulting Hamiltonian two-point boundary-value
subproblems, including adaptive-time integrators. A numerical experiment on a
linear-quadratic OCP illustrates the practicality of our approach in broad
scientific applications.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [93] [Mixing of a binary passive particle system using smart active particles](https://arxiv.org/abs/2510.04305)
*Thomas Jacob,Siddhant Mohapatra,Rajalingam A,Sam Mathew,Pallab Sinha Mahapatra*

Main category: cond-mat.soft

TL;DR: Smart active particles guided by AI achieve superior mixing of passive particles through eccentric rotational motion, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: To develop intelligent active matter systems for applications like drug delivery and microfluidic transport by optimizing mixing of passive particles using adaptive AI agents.

Method: Used 2% active particles controlled by a trained Artificial Neural Network agent, creating eccentric zones of activity that induce global rotational motion in passive particles.

Result: AI-guided active particles achieved faster and more efficient mixing than conventional run-and-tumble particles, with optimal strategy involving eccentric motion patterns creating rotational flow.

Conclusion: Integrating machine learning with active matter enables superior control and emergent behaviors, demonstrating the potential of adaptive learning frameworks in active matter systems.

Abstract: Controlled activity of active entities interacting with a passive environment
can generate emergent system-level phenomena, positioning such systems as
promising platforms for potential downstream applications in targeted drug
delivery, adaptive and reconfigurable materials, microfluidic transport and
related fields. The present work aims to realise an optimal mixing of two
segregated species of passive particles by introducing a small fraction of
active particles (2% by composition) with adaptive and intelligent behaviour,
directed by a trained Artificial Neural Network-based agent. While conventional
run-and-tumble particles can induce mixing in the system, the smart active
particles demonstrate superior performance, achieving faster and more efficient
mixing. Interestingly, an optimal mixing strategy doesn't involve a uniform
dispersion of active particles in the domain, but rather limiting their motion
to an eccentrically placed zone of activity, inducing a global rotational
motion of the passive particles about the system centre. A transition in the
directionality of the passive particles' motion is observed along the radius
towards the centre, likening the active particles' motion to an ellipse-shaped
void with a defined surface speed. Situated at the intersection of active
matter and machine learning, this work highlights the potential of integrating
adaptive learning frameworks into traditional active matter models.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [94] [Estimates of the first Dirichlet eigenvalue of graphs](https://arxiv.org/abs/2510.04557)
*Huiqiu Lin,Lianping Liu,Zhe You,Da Zhao*

Main category: math.CO

TL;DR: The paper establishes lower bounds for the first Dirichlet eigenvalue of graphs in terms of diameter and inscribed radius, proving sharp inequalities that generalize classical Li-Yau estimates to discrete settings.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by extending classical Li-Yau eigenvalue-diameter estimates from Riemannian geometry to discrete graphs, seeking analogous bounds for Dirichlet eigenvalues in terms of geometric parameters like inscribed radius and diameter.

Method: The authors use graph-theoretic and combinatorial methods to derive eigenvalue bounds, working with graphs having boundary vertices and connected interiors. They employ inscribed radius, maximum degree, and vertex counts as key parameters in their analysis.

Result: Several sharp lower bounds are proven: λ₁(G,B) ≥ (d-1)/(r d^r) where d is maximum degree and r is inscribed radius; λ₁(G,B) ≥ 1/(r|Ω|); and for trees with ≥3 vertices, λ₁(T) ≥ 4 sin²(π/(4r+6)) ≥ 1/(r+1)². All bounds are sharp up to constant factors.

Conclusion: The paper successfully establishes discrete analogues of classical Li-Yau bounds for Dirichlet eigenvalues, providing sharp geometric estimates that connect spectral properties with graph diameter and inscribed radius, with particularly strong results for trees.

Abstract: Inspired by the Li--Yau eigenvalue-diameter estimates, we investigate lower
bounds for the first Dirichlet eigenvalue in terms of the diameter (or
inscribed radius) of a graph. Let $G = (V, E)$ be a graph with boundary $B$.
Assume that the interior $\Omega = V \setminus B$ is connected. Let $r$ be the
inscribed radius of $(G, B)$ and $d$ be the maximum degree of $G$. We prove
that $$\lambda_1(G, B) \geq \frac{d - 1}{r d^r},$$ which can be viewed as an
analogue of the Lin--Yau bound and the Meng--Lin bound for normalized
Dirichlet/Laplacian eigenvalues. We also derive the inequality $$\lambda_1(G,
B) \geq \frac{1}{r |\Omega|}.$$ In particular, for a tree $T$ with at least $3$
vertices, we show that $$\lambda_1(T) \geq 4 \sin^2 \frac{\pi}{4r + 6} \geq
\frac{1}{(r + 1)^2}.$$ Notably, both of the two preceding bounds are sharp up
to a constant factor. We additionally examine upper bounds on the first
Dirichlet eigenvalue under constraints on the numbers of interior and boundary
vertices.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [95] [Tensor tomography on asymptotically hyperbolic surfaces](https://arxiv.org/abs/2510.04144)
*Nikolas Eptaminitakis,François Monard,Yuzhou Joey Zou*

Main category: math.DG

TL;DR: Study of geodesic X-ray transform inversion for symmetric m-tensor fields on asymptotically hyperbolic surfaces, introducing an "iterated-tt" gauge representative distinct from solenoidal gauge.


<details>
  <summary>Details</summary>
Motivation: The geodesic X-ray transform has a non-trivial kernel for m≥1, requiring gauge representatives to reconstruct tensor fields from X-ray data. Previous approaches used solenoidal representatives, but this work proposes an alternative "iterated-tt" representative.

Method: Proves a "tt-potential-conformal" decomposition theorem using elliptic decompositions of Guillemin-Kazhdan operators and Mazzeo-Melrose 0-calculus. Iterates this decomposition to create "iterated-tt" representatives. For Poincaré disk, shows X-ray transform splits into orthogonal components relative to L² structure.

Result: For even tensor fields, provides complete data space decomposition with range characterization of I_{2n} in terms of moment conditions and spectral decay. Gives explicit reconstruction approaches for even tensors in iterated-tt form using knowledge of geodesically invariant distributions with one-sided Fourier content.

Conclusion: Introduces a new gauge representative (iterated-tt) for tensor field reconstruction from X-ray data, distinct from solenoidal gauge, with explicit reconstruction methods and complete range characterization for even tensor fields on asymptotically hyperbolic surfaces.

Abstract: We initiate a study of the inversion of the geodesic X-ray transform $I_m$
over symmetric $m$-tensor fields on asymptotically hyperbolic surfaces. This
operator has a non-trivial kernel whenever $m\ge 1$. To propose a gauge
representative to be reconstructed from X-ray data, we first prove a
"tt-potential-conformal" decomposition theorem for $m$-tensor fields (where
"tt" stands for transverse traceless), previously used in integral geometry on
compact Riemannian manifolds with boundary in Sharafutdinov, 2007; Dairbekov
and Sharafutdinov, 2011. The proof is based on elliptic decompositions of the
Guillemin-Kazhdan operators $\eta_\pm$ (Guillemin and Kazhdan, 1980) and
leverages in the current setting the 0-calculus of Mazzeo-Melrose (Mazzeo and
Melrose, 1987; Mazzeo, 1991). Iterating this decomposition gives rise to an
"iterated-tt" representative modulo $\ker I_m$ for a tensor field, which is
distinct from the often-used solenoidal representative.
  In the case of the Poincar\'e disk, we show that the X-ray transform of a
tensor in iterated-tt form splits into components that are orthogonal relative
to a specific $L^2$ structure in data space. For even tensor fields, we provide
a full picture of the data space decomposition, in particular a range
characterization of $I_{2n}$ for every $n$ in terms of moment conditions and
spectral decay. Finally, we give explicit approaches for the reconstruction of
even tensors in iterated-tt form from their X-ray transform or its normal
operator, using specific knowledge of geodesically invariant distributions with
one-sided Fourier content, whose properties are analyzed in detail.

</details>


### [96] [Nonhomothetic complete periodic metrics with constant scalar curvature](https://arxiv.org/abs/2510.04351)
*João H. Andrade,Jeffrey S. Case,Paolo Piccione,Juncheng Wei*

Main category: math.DG

TL;DR: Infinitely many nonhomothetic, complete periodic metrics with constant scalar curvature conformal to round metric on S^n\S^k, obtained from Yamabe metrics on products of spheres and hyperbolic manifolds.


<details>
  <summary>Details</summary>
Motivation: To construct and classify constant scalar curvature metrics on punctured spheres, extending understanding of Yamabe metrics and conformal geometry.

Method: Pull back Yamabe metrics from products of S^{n-k-1} and compact hyperbolic (k+1)-manifolds, using classical rigidity theorems by Obata and Ferrand.

Result: Infinitely many pairwise nonhomothetic solutions exist, generically distinct up to homothety, on S^n\S^k for k < (n-2)/2.

Conclusion: The construction provides a rich family of constant scalar curvature metrics, with distinctness established through conformal group characterization of round spheres.

Abstract: We show that there are infinitely many pairwise nonhomothetic, complete,
periodic metrics with constant scalar curvature that are conformal to the round
metric on $S^n\setminus S^k$, where $k < \frac{n-2}{2}$. These metrics are
obtained by pulling back Yamabe metrics defined on products of $S^{n-k-1}$ and
compact hyperbolic $(k+1)$-manifolds. Our main result proves that these
solutions are generically distinct up to homothety. The core of our argument
relies on classical rigidity theorems due to Obata and Ferrand, which
characterize the round sphere by its conformal group.

</details>


### [97] [Minimal surfaces and comparison geometry](https://arxiv.org/abs/2510.04481)
*Otis Chodosh*

Main category: math.DG

TL;DR: Applications of minimal surfaces in comparison geometry


<details>
  <summary>Details</summary>
Motivation: To explore how minimal surfaces can be used in comparison geometry

Method: Discussion of applications and connections between minimal surfaces and comparison geometry

Result: Established relationships and applications of minimal surfaces in geometric comparison contexts

Conclusion: Minimal surfaces provide valuable tools and insights for comparison geometry

Abstract: We discuss applications of minimal surfaces to comparison geometry.

</details>


### [98] [Poincaré-Einstein 4-manifolds with conformally Kähler geometry](https://arxiv.org/abs/2510.04928)
*Mingyang Li,Hongyi Liu*

Main category: math.DG

TL;DR: Study of 4D Poincaré-Einstein manifolds with Kähler conformal classes, where Einstein metrics are non-Kähler and admit Killing fields. When the Killing field integrates to an S¹-action, a Dirichlet boundary value problem yields infinite-dimensional families of new Poincaré-Einstein metrics with non-positive Yamabe type conformal infinities.


<details>
  <summary>Details</summary>
Motivation: To investigate Poincaré-Einstein manifolds whose conformal class contains a Kähler metric, exploring the relationship between Einstein metrics and Kähler structures in 4 dimensions.

Method: Formulate a Dirichlet boundary value problem when the Killing field integrates to an S¹-action, using Toda-type equations derived from the Einstein equation. Establish existence and uniqueness theory for this construction.

Result: Construction provides non-perturbative realization of infinite-dimensional families of new Poincaré-Einstein metrics whose conformal infinities are of non-positive Yamabe type.

Conclusion: The approach successfully generates infinite-dimensional families of Poincaré-Einstein metrics with specific geometric properties, demonstrating the existence of such structures in the non-Kähler Einstein setting.

Abstract: We study 4-dimensional Poincar\'e-Einstein manifolds whose conformal class
contains a K\"ahler metric. Such Einstein metrics are non-K\"ahler and admit a
Killing field extending to the conformal infinity, and the Einstein equation
reduces to a Toda-type equation. When the Killing field integrates to an
$\mathbb{S}^1$-action, we formulate a Dirichlet boundary value problem and
establish existence and uniqueness theory. This construction provides a
non-perturbative realization of infinite-dimensional families of new
Poincar\'e-Einstein metrics whose conformal infinities are of non-positive
Yamabe type.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [99] [Dynamic Landau-Lifshitz-Bloch-Slonczewski equations for spintronics](https://arxiv.org/abs/2510.04562)
*Pascal Thibaudeau,Mouad Fattouhi,Liliana D. Buda-Prejbeanu*

Main category: cond-mat.mes-hall

TL;DR: A new dynamic Landau-Lifshitz-Bloch-Slonczewski equation set is developed to model magnetization dynamics under Joule heating, overcoming limitations of traditional constant-magnitude approaches.


<details>
  <summary>Details</summary>
Motivation: The standard Landau-Lifshitz-Gilbert equation fails in spintronic devices with significant Joule heating because it assumes constant magnetization magnitude, which doesn't account for heating-induced demagnetization effects.

Method: Developed a statistical framework treating magnetization magnitude as a dynamic variable coupled to thermal bath, deriving dynamic Landau-Lifshitz-Bloch-Slonczewski equations that capture transient heating-induced demagnetization.

Result: The dynamic equations accurately model energy landscape and switching dynamics in high-anisotropy systems under high-current operation, providing improved predictions of critical currents and switching times.

Conclusion: This approach enables accurate and accelerated modeling of spintronic device behavior under high-current conditions where heating-induced demagnetization is significant.

Abstract: The atomistic Landau-Lifshitz-Gilbert equation is challenged when modeling
spintronic devices where Joule heating is significant, due to its core
assumption of a constant magnetization magnitude. Based on a statistical
framework that treats the magnetization magnitude as a dynamic variable coupled
to a thermal bath, we derive a dynamic Landau-Lifshitz-Bloch-Slonczewski set of
equations for torques, that captures the transient, heating-induced
demagnetization that occurs during high-current operation. Integrating these
dynamic equations and comparing them to their stochastic equivalents reveals
that both the energy landscape and switching dynamics in high-anisotropy
systems are similarly modified. This approach yields accurate and accelerated
predictions of critical currents and switching times.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [100] [The method of the approximate inverse for limited-angle CT](https://arxiv.org/abs/2510.04369)
*Bernadette Hahn,Gael Rigaud,Richard Schmähl*

Main category: eess.IV

TL;DR: Proposes CLARK, a model-driven approach for limited-angle CT reconstruction that combines spectral filtering, approximate inverse method, and edge-preserving denoising to eliminate streak artifacts while handling ill-conditioning from limited-angle data.


<details>
  <summary>Details</summary>
Motivation: Limited-angle CT enables faster acquisition and safer scans but standard methods like FBP and total-variation produce artifacts. Deep learning alternatives require large datasets, motivating a model-driven approach.

Method: Uses reconstruction kernels precomputed as solutions to auxiliary problems (LARK), then develops CLARK with spectral filtering, approximate inverse method, and custom edge-preserving denoising to stabilize the ill-conditioned reconstruction.

Result: The method fully reconstructs objects without streak artifacts even for large limited angles, though it inherits ill-conditioning that requires regularization. Validated on synthetic and real data.

Conclusion: CLARK provides an effective model-driven alternative to deep learning for limited-angle CT reconstruction, serving as a potential starting point for future learning strategies while handling the inherent ill-conditioning of limited-angle problems.

Abstract: Limited-angle computerized tomography stands for one of the most difficult
challenges in imaging. Although it opens the way to faster data acquisition in
industry and less dangerous scans in medicine, standard approaches, such as the
filtered backprojection (FBP) algorithm or the widely used total-variation
functional, often produce various artefacts that hinder the diagnosis. With the
rise of deep learning, many modern techniques have proven themselves successful
in removing such artefacts but at the cost of large datasets. In this paper, we
propose a new model-driven approach based on the method of the approximate
inverse, which could serve as new starting point for learning strategies in the
future. In contrast to FBP-type approaches, our reconstruction step consists in
evaluating linear functionals on the measured data using reconstruction kernels
that are precomputed as solution of an auxiliary problem. With this problem
being uniquely solvable, the derived limited-angle reconstruction kernel (LARK)
is able to fully reconstruct the object without the well-known streak
artefacts, even for large limited angles. However, it inherits severe
ill-conditioning which leads to a different kind of artefacts arising from the
singular functions of the limited-angle Radon transform. The problem becomes
particularly challenging when working on semi-discrete (real or analytical)
measurements. We develop a general regularization strategy, named constrained
limited-angle reconstruction kernel (CLARK), by combining spectral filter, the
method of the approximate inverse and custom edge-preserving denoising in order
to stabilize the whole process. We further derive and interpret error estimates
for the application on real, i.e. semi-discrete, data and we validate our
approach on synthetic and real data.

</details>


### [101] [Adaptive double-phase Rudin--Osher--Fatemi denoising model](https://arxiv.org/abs/2510.04382)
*Wojciech Górny,Michał Łasica,Alexandros Matsoukas*

Main category: eess.IV

TL;DR: A new image denoising model using variable-growth total variation regularization with adaptive weight to reduce staircasing while preserving edges.


<details>
  <summary>Details</summary>
Motivation: To address the staircasing effect in the classical Rudin-Osher-Fatemi model while maintaining good edge preservation capabilities.

Method: Variable-growth total variation regularization of double-phase type with adaptive weight, implemented and tested on synthetic and natural images in 1D and 2D.

Result: The model was tested across various noise levels and showed improved performance in reducing staircasing artifacts.

Conclusion: The proposed model effectively reduces staircasing while preserving image edges, making it a promising alternative to traditional total variation denoising methods.

Abstract: We propose a new image denoising model based on a variable-growth total
variation regularization of double-phase type with adaptive weight. It is
designed to reduce staircasing with respect to the classical
Rudin--Osher--Fatemi model, while preserving the edges of the image in a
similar fashion. We implement the model and test its performance on synthetic
and natural images in 1D and 2D over a range of noise levels.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [102] [Field-Theoretic Simulation of Dean-Kawasaki Dynamics for Interacting Particles](https://arxiv.org/abs/2510.05042)
*Jaehyeok Jin,Chen Liu,David R. Reichman*

Main category: cond-mat.stat-mech

TL;DR: First numerical investigation of weakly interacting fluids using regularized Dean-Kawasaki equation framework, examining regularization effects on structural correlations.


<details>
  <summary>Details</summary>
Motivation: To address concerns about validity and utility of stochastic partial differential equations from microscopic Dean-Kawasaki mappings due to singular nature of density distributions, and establish rigorous coarse-graining procedure.

Method: Numerical investigation of weakly interacting fluids within regularized Dean-Kawasaki equation framework, building on recent rigorous coarse-graining procedures.

Result: Reveals effects of regularization on Dean-Kawasaki formalism at the level of structural correlations.

Conclusion: Paves the way for improved numerical approaches to simulate fluctuating hydrodynamics in liquids.

Abstract: The formulation of a fluctuating hydrodynamic theory for interacting
particles is a crucial step in the theoretical description of liquids. The
microscopic mappings proposed decades ago by Dean and Kawasaki have played a
central role in the analytical treatment of such problems. However, the
singular mathematical nature of the density distributions used in these
derivations raises concerns about the validity and practical utility of the
resulting stochastic partial differential equations, particularly for direct
numerical simulations. Recent efforts have centered on establishing a rigorous
coarse-graining procedure to regularize the effective Dean-Kawasaki equation.
Building on this foundation, we numerically investigate weakly interacting
fluids within such a regularized framework for the first time. Our work
reveals, at the level of structural correlations, the effects of regularization
on the Dean-Kawasaki formalism and paves the way for improved numerical
approaches to simulate fluctuating hydrodynamics in liquids.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [103] [The Feynman propagator for massive Klein-Gordon fields on radiative asymptotically flat spacetimes](https://arxiv.org/abs/2510.04308)
*Mikhail Molodyk,András Vasy*

Main category: gr-qc

TL;DR: The paper defines a global Feynman propagator for massive Klein-Gordon fields on asymptotically flat spacetimes using microlocal analysis, extends the limiting absorption principle, and proves a new localized radial point estimate.


<details>
  <summary>Details</summary>
Motivation: To establish a well-defined Feynman propagator on radiative perturbations of Minkowski space and address the complex Hamilton flow structure in such spacetimes.

Method: Uses microlocal approach to non-elliptic Fredholm theory within Sussman's de,sc-pseudodifferential algebra, and develops a new localized radial point estimate inspired by Haber-Vasy.

Result: Successfully defines a distinguished global Feynman propagator and extends the limiting absorption principle to asymptotically flat spacetimes with radiative perturbations.

Conclusion: The approach provides rigorous foundations for Feynman propagators in curved spacetimes and offers new tools for handling complex Hamilton flow structures through localized radial point estimates.

Abstract: On a large class of asymptotically flat spacetimes which includes radiative
perturbations of Minkowski space, we define a distinguished global Feynman
propagator for massive Klein-Gordon fields by means of the microlocal approach
to non-elliptic Fredholm theory, working in the de,sc-pseudodifferential
algebra due to Sussman. We extend the limiting absorption principle (the
"$i\varepsilon$ prescription" for the Feynman propagator) to this setting.
Motivated by the complicated Hamilton flow structure arising in this problem,
we also prove a new localized radial point estimate in the spirit of Haber-Vasy
which, under appropriate nondegeneracy assumptions, allows one to propagate
microlocal regularity into a single radial point belonging to a larger radial
set which can be a source, sink, or saddle for the Hamilton flow.

</details>
