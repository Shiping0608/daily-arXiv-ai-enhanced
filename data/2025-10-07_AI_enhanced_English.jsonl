{"id": "2510.03420", "pdf": "https://arxiv.org/pdf/2510.03420", "abs": "https://arxiv.org/abs/2510.03420", "authors": ["Manh Tuan Hoang", "Matthias Ehrhardt"], "title": "A Generalized Second-Order Positivity-Preserving Numerical Method for Non-Autonomous Dynamical Systems with Applications", "categories": ["math.NA", "cs.NA", "65L05, 65Z05"], "comment": "6 Tables, 3 Figures", "summary": "In this work, we propose a generalized, second-order, nonstandard finite\ndifference (NSFD) method for non-autonomous dynamical systems. The proposed\nmethod combines the NSFD framework with a new non-local approximation of the\nright-hand side function. This method achieves second-order convergence and\nunconditionally preserves the positivity of solutions for all step sizes.\nEspecially, it avoids the restrictive conditions required by many existing\npositivity-preserving, second-order NSFD methods. The method is easy to\nimplement and computationally efficient. Numerical experiments, including an\nimproved NSFD scheme for an SIR epidemic model, confirm the theoretical\nresults. Additionally, we demonstrate the method's applicability to nonlinear\npartial differential equations and boundary value problems with positive\nsolutions, showcasing its versatility in real-world modeling.", "AI": {"tldr": "A second-order nonstandard finite difference method for non-autonomous systems that preserves positivity unconditionally and avoids restrictive conditions of existing methods.", "motivation": "To develop a generalized NSFD method that achieves second-order convergence while unconditionally preserving solution positivity without the restrictive conditions of existing approaches.", "method": "Combines NSFD framework with a new non-local approximation of the right-hand side function to create a second-order method that maintains positivity for all step sizes.", "result": "The method achieves second-order convergence, unconditionally preserves positivity, is easy to implement, computationally efficient, and works well for SIR epidemic models, nonlinear PDEs, and boundary value problems.", "conclusion": "The proposed NSFD method successfully provides a versatile, positivity-preserving second-order scheme applicable to various real-world modeling problems without restrictive conditions."}}
{"id": "2510.03510", "pdf": "https://arxiv.org/pdf/2510.03510", "abs": "https://arxiv.org/abs/2510.03510", "authors": ["Tam\u00e1s D\u00f3zsa", "Matthias Voigt", "Zolt\u00e1n Szab\u00f3", "J\u00f3zsef Bokor", "P\u00e9ter Kov\u00e1cs"], "title": "Generalized rational Prony and Bernoulli methods", "categories": ["math.NA", "cs.NA"], "comment": "Submitted to the journal IOP Inverse Problems for consideration", "summary": "The generalized operator-based Prony method is an important tool for\ndescribing signals which can be written as finite linear combinations of\neigenfunctions of certain linear operators. On the other hand, Bernoulli's\nalgorithm and its generalizations can be used to recover the parameters of\nrational functions belonging to finite-dimensional subspaces of $H_2$\nHardy-Hilbert spaces. In this work, we discuss several results related to these\nmethods. We discuss a rational variant of the generalized operator-based Prony\nmethod and show that in fact, any Prony problem can be treated this way. This\nrealization establishes the connection between Prony and Bernoulli methods and\nallows us to address some well-known numerical pitfalls. Several numerical\nexperiments are provided to showcase the usefulness of the introduced methods.\nThese include problems related to the identification of time-delayed linear\nsystems and parameter recovery problems in reproducing kernel Hilbert spaces.", "AI": {"tldr": "This paper connects the generalized operator-based Prony method with Bernoulli's algorithm, showing that any Prony problem can be treated as a rational variant and addressing numerical issues.", "motivation": "To establish the connection between Prony and Bernoulli methods and address numerical pitfalls in signal parameter recovery problems.", "method": "Develops a rational variant of the generalized operator-based Prony method and shows its equivalence to Bernoulli methods for parameter recovery in finite-dimensional subspaces.", "result": "Demonstrates that any Prony problem can be treated using the rational variant approach, successfully applying the methods to time-delayed linear systems and reproducing kernel Hilbert spaces.", "conclusion": "The connection between Prony and Bernoulli methods provides a unified framework for parameter recovery with improved numerical stability, as validated through various applications."}}
{"id": "2510.03563", "pdf": "https://arxiv.org/pdf/2510.03563", "abs": "https://arxiv.org/abs/2510.03563", "authors": ["Stefano Berrone", "Karol L. Cascavita", "Enrique Delgado \u00c1vila", "Samuele Rubino", "Maria Strazzullo", "Fabio Vicini"], "title": "General Order Virtual Element Approximation for the Smagorinsky turbulence model", "categories": ["math.NA", "cs.NA", "35Q30, 65N22, 76F25"], "comment": null, "summary": "In this paper, we investigate a Smagorinsky model in a virtual element\nframework to simulate convection-dominated Navier-Stokes equations. We conduct\na two-dimensional numerical investigation to assess the performance of the\ngeneral order virtual element approximation in this context. First, we examine\nnumerically the convergence of the method with respect to the meshsize to\ncertify the novel virtual element numerical discretization, which includes, for\nthe first time, a discretization of the Smagorinsky term. Moreover, we present\na numerical study of a lid-driven cavity for different Reynolds numbers (up to\n10000) and meshes (uniform, anisotropic, and isotropic with hanging nodes). The\nresults highlight the main advantage of using the virtual elements method in\nthis context: the isotropic refinement with hanging nodes enhances the accuracy\nof the solution compared to the anisotropic mesh, uses fewer degrees of freedom\nwith respect to the uniform mesh, and yields the most stable behavior in terms\nof convergence of the Newton solver.", "AI": {"tldr": "The paper presents a Smagorinsky model in a virtual element framework for simulating convection-dominated Navier-Stokes equations, with numerical validation and performance analysis on lid-driven cavity flows.", "motivation": "To develop and validate a novel virtual element discretization that includes the Smagorinsky term for simulating convection-dominated flows, addressing the need for accurate and stable numerical methods for high Reynolds number flows.", "method": "A two-dimensional numerical investigation using general order virtual element approximation with Smagorinsky model. The study examines convergence with meshsize and tests on lid-driven cavity with different Reynolds numbers (up to 10000) and mesh types (uniform, anisotropic, and isotropic with hanging nodes).", "result": "The virtual element method with isotropic refinement using hanging nodes provides enhanced accuracy compared to anisotropic meshes, uses fewer degrees of freedom than uniform meshes, and demonstrates the most stable convergence behavior for the Newton solver.", "conclusion": "The virtual elements method with isotropic refinement and hanging nodes offers significant advantages for simulating convection-dominated Navier-Stokes equations, providing improved accuracy, computational efficiency, and solver stability compared to traditional mesh approaches."}}
{"id": "2510.03670", "pdf": "https://arxiv.org/pdf/2510.03670", "abs": "https://arxiv.org/abs/2510.03670", "authors": ["Liet Vo", "Hung Dang Nguyen"], "title": "Fully discrete finite element methods for the stochastic Kuramoto-Sivashinsky equation with multiplicative noise", "categories": ["math.NA", "cs.NA", "math.PR"], "comment": "43 pages", "summary": "We investigate a fully discrete finite element approximation for the\nstochastic Kuramoto-Sivashinsky equation, combining the standard finite element\nmethods in spatial discretization with the implicit Euler-Maruyama scheme in\ntime. Rigorous error estimates are established for two distinct noise regimes.\nIn the case of bounded multiplicative noise, we prove optimal strong\nconvergence rates in full expectation. The analysis relies crucially on a\nstochastic Gronwall inequality and an exponential stability estimate for the\nPDE solution, which together control the interplay between the nonlinear drift\nand the multiplicative stochastic forcing. For general multiplicative noise,\nwhere boundedness no longer holds, we derive sub-optimal convergence rates in\nprobability by introducing a localization technique based on carefully\nconstructed subsets of the sample space. This dual framework demonstrates that\nthe proposed fully discrete scheme achieves strong convergence under bounded\nnoise and probabilistic convergence under general multiplicative noise, thus\nproviding the first comprehensive error analysis for numerical approximations\nof the stochastic Kuramoto-Sivashinsky equation.", "AI": {"tldr": "Finite element analysis of stochastic Kuramoto-Sivashinsky equation with implicit Euler-Maruyama time discretization, establishing error estimates for bounded and general multiplicative noise regimes.", "motivation": "To provide the first comprehensive error analysis for numerical approximations of the stochastic Kuramoto-Sivashinsky equation, addressing both bounded and general multiplicative noise cases.", "method": "Combines standard finite element methods for spatial discretization with implicit Euler-Maruyama scheme for time discretization, using stochastic Gronwall inequality and exponential stability estimates for bounded noise, and localization technique for general noise.", "result": "Optimal strong convergence rates in full expectation for bounded multiplicative noise, and sub-optimal convergence rates in probability for general multiplicative noise using localization on sample space subsets.", "conclusion": "The fully discrete scheme achieves strong convergence under bounded noise and probabilistic convergence under general multiplicative noise, providing comprehensive error analysis for stochastic Kuramoto-Sivashinsky equation approximations."}}
{"id": "2510.03414", "pdf": "https://arxiv.org/pdf/2510.03414", "abs": "https://arxiv.org/abs/2510.03414", "authors": ["Amierul Aqil Khairi", "Elyse Lian", "Uri Shumlak"], "title": "Spectroscopic measurements of graphite electrode erosion on the ZaP-HD sheared-flow-stabilized Z-pinch device", "categories": ["physics.plasm-ph"], "comment": "10 pages, 13 figures, submitted to Review of Scientific Instruments", "summary": "The ionizations per photon, or S/XB, method uses spectroscopic measurements\nof radiating impurity ions to determine the influx from a solid surface. It is\nhighly useful as a non-perturbing, in-situ measure of the gross erosion flux of\nplasma-facing components (PFCs). In sheared-flow-stabilized (SFS) Z-pinch\ndevices, the electrode supplies the plasma current and directly contacts the\ncore Z-pinch plasma. Electrode erosion due to the large particle and heat\nfluxes affects electrode durability, which is an important factor in existing\nand future devices. An improved understanding of these plasma-electrode\ninteractions is required, in particular as energy density increases.\nExperiments on the ZaP-HD device investigate erosion of the graphite electrode\nby applying the S/XB method for C-III emission at 229.7 nm. The S/XB\ncoefficients are determined from electron density and temperature profiles\nobtained from Digital Holographic Interferometry (DHI) measurements. An\napproach for expanding these profiles to represent plasma contacting the\nelectrode is described. In both cases, the measured erosion fluxes are on the\norder of 10$^{30}$-10$^{31}$ atoms m$^{-2}$s$^{-1}$. These values are\nsignificantly larger than the expected erosion flux due to physical sputtering\nof H$^+$ ions on carbon, but are comparable to theoretical sublimation fluxes.\nThis suggests that the source of carbon erosion flux is primarily from\nsublimation as opposed to sputtering. The dominance of sublimation over\nsputtering processes implies a difference in energy of the eroded neutrals\nwhich may provide insight on redeposition and net erosion behavior.", "AI": {"tldr": "The paper applies the S/XB method to measure graphite electrode erosion in Z-pinch devices, finding erosion fluxes of 10^30-10^31 atoms m^-2s^-1, primarily from sublimation rather than sputtering.", "motivation": "Understanding electrode erosion in sheared-flow-stabilized Z-pinch devices is crucial for electrode durability, especially as energy density increases. Electrodes directly contact core plasma and face large particle and heat fluxes.", "method": "Used S/XB method with C-III emission at 229.7 nm on ZaP-HD device. Determined S/XB coefficients from electron density and temperature profiles obtained via Digital Holographic Interferometry (DHI), with an approach to expand profiles to represent plasma contacting the electrode.", "result": "Measured erosion fluxes were 10^30-10^31 atoms m^-2s^-1, significantly larger than expected physical sputtering but comparable to theoretical sublimation fluxes, suggesting sublimation is the primary erosion mechanism.", "conclusion": "Carbon electrode erosion in Z-pinch devices is dominated by sublimation rather than sputtering, which implies differences in eroded neutral energies that may provide insight into redeposition and net erosion behavior."}}
{"id": "2510.03649", "pdf": "https://arxiv.org/pdf/2510.03649", "abs": "https://arxiv.org/abs/2510.03649", "authors": ["Diba Behnoudfar", "Kyle E. Niemeyer"], "title": "Uncertainty quantification of reacting fluids interacting with porous media using a hybrid physics-based and data-driven approach", "categories": ["physics.comp-ph"], "comment": null, "summary": "Accurately simulating coupled physical processes under uncertainty is\nessential for reliable modeling and design in performance-critical applications\nsuch as combustion systems. Ablative heat shield design, as a specific example\nof this class, involves modeling multi-physics interactions between reacting\nflows and a porous material. Repeatedly evaluating these models to quantify\nparametric uncertainties would be prohibitively computationally expensive. In\nthis work, we combine physics-based modeling using a single-domain approach\nwith data-driven reduced-order modeling to quantify uncertainty via the\noperator inference method. The detailed physics-based simulations reproduce the\nmeasured surface temperature of an object exposed to high-enthalpy flow in a\nplasma wind tunnel experiment within 5%. We further use the model to\ndemonstrate the effect of complex flow situations on the dynamic interactions\nbetween the porous heat shield material and the surrounding gas. The parametric\nreduced-order model, built on physics-based simulation data, successfully\ncaptures variations in quantities of interest resulting from changes in the\npermeability and heat transfer coefficient of the porous material in two\nseparate studies: solid fuel combustion and emission of buoyant reacting plumes\nin quiescent air and ablation in a wind tunnel.", "AI": {"tldr": "Combines physics-based modeling with data-driven reduced-order modeling to efficiently quantify uncertainty in coupled physical processes like ablative heat shield design.", "motivation": "Repeatedly evaluating detailed multi-physics models for uncertainty quantification is computationally prohibitive, especially for performance-critical applications like combustion systems and heat shield design.", "method": "Uses operator inference method to combine single-domain physics-based modeling with data-driven reduced-order modeling, building parametric reduced-order models from physics-based simulation data.", "result": "Physics-based simulations reproduce measured surface temperature within 5% accuracy. The reduced-order model successfully captures variations from parameter changes in permeability and heat transfer coefficient across different scenarios.", "conclusion": "The hybrid approach enables efficient uncertainty quantification for coupled physical processes while maintaining accuracy, demonstrated through successful applications in solid fuel combustion and ablation scenarios."}}
{"id": "2510.03408", "pdf": "https://arxiv.org/pdf/2510.03408", "abs": "https://arxiv.org/abs/2510.03408", "authors": ["Sebastian Acosta", "Benjamin Palacios"], "title": "Inverse photoacoustic tomography problem in media with fractional attenuation", "categories": ["math.AP", "35R30, 35L05, 35R11"], "comment": null, "summary": "We investigate the inverse problem of recovering an initial source for the\nwave equation with fractional attenuation, motivated by photoacoustic\ntomography (PAT). The attenuation is modeled by a Caputo fractional derivative\nof order $\\alpha\\in(0,1)$. We establish uniqueness under a geometric foliation\ncondition via an adaptation of two types of Carleman estimates to the\nfractional setting, prove stability through continuity inequalities for\nfractional time-derivatives of wave solutions, and derive a reconstruction\nscheme based on a Neumann series. While our results apply directly to PAT, we\nexpect that the analytic approach and tools employed might be of broader\nrelevance to the analysis of PDEs with memory effects and to inverse problems\nfor attenuated wave models.", "AI": {"tldr": "The paper studies inverse source recovery for the wave equation with fractional attenuation, with applications to photoacoustic tomography. It establishes uniqueness under geometric conditions, proves stability via continuity inequalities, and develops a Neumann series-based reconstruction scheme.", "motivation": "Motivated by photoacoustic tomography (PAT), the research aims to solve the inverse problem of recovering initial sources in wave equations with fractional attenuation, which models memory effects in wave propagation.", "method": "The approach adapts Carleman estimates to the fractional setting, uses continuity inequalities for fractional time-derivatives of wave solutions to prove stability, and develops a reconstruction scheme based on Neumann series.", "result": "Established uniqueness under geometric foliation conditions, proved stability through continuity inequalities, and derived a practical reconstruction scheme using Neumann series.", "conclusion": "The results directly apply to PAT and the analytical methods developed are expected to be broadly relevant for PDEs with memory effects and inverse problems for attenuated wave models."}}
{"id": "2510.03778", "pdf": "https://arxiv.org/pdf/2510.03778", "abs": "https://arxiv.org/abs/2510.03778", "authors": ["Maatank Parashar", "Tejas Dhulipalla"], "title": "A Variational Method for Conformable Fractional Equations Using Rank-One Updates", "categories": ["math.NA", "cs.NA", "math.AP"], "comment": null, "summary": "We make a complete variational treatment of rank-one Proper Generalised\nDecomposition for separable fractional partial differential equations with\nconformable derivatives. The setting is Hilbertian, the energy is induced by a\nsymmetric coercive bilinear form, and the residual is placed in the dual space.\nA greedy rank-one update is obtained by maximizing an energy Rayleigh quotient\nover the rank-one manifold, followed by an exact line search. An exact one step\nenergy decrease identity is proved, together with geometric decay of the energy\nerror under a weak greedy condition that measures how well the search captures\nthe Riesz representer of the residual. The alternating least squares\nrealization is analyzed at the level of operators, including well posedness of\nthe alternating subproblems, a characterization of stationary points, and\nmonotonicity of the Rayleigh quotient along the inner iteration.\nDiscretizations based on weighted finite elements and on Gr\\\"unwald type\nschemes are described in detail, including assembly, boundary conditions,\ncomplexity, and memory. Two model problems, a stationary fractional Poisson\nproblem and a space time fractional diffusion problem, are treated from the\ncontinuous level down to matrices.", "AI": {"tldr": "Complete variational analysis of rank-one Proper Generalised Decomposition for separable fractional PDEs with conformable derivatives, including greedy updates, energy decay proofs, and discretization methods.", "motivation": "To develop a rigorous mathematical framework for Proper Generalised Decomposition methods applied to fractional partial differential equations with conformable derivatives, addressing both theoretical foundations and practical implementations.", "method": "Variational treatment in Hilbert space using symmetric coercive bilinear forms; greedy rank-one updates via energy Rayleigh quotient maximization; alternating least squares approach; discretization with weighted finite elements and Gr\u00fcnwald schemes.", "result": "Proved exact one-step energy decrease identity and geometric decay of energy error under weak greedy conditions; established well-posedness of alternating subproblems and monotonicity properties; implemented methods for fractional Poisson and space-time fractional diffusion problems.", "conclusion": "The framework provides rigorous mathematical foundations for PGD methods in fractional PDEs, with proven convergence properties and practical discretization schemes that bridge continuous theory to matrix implementations."}}
{"id": "2510.03960", "pdf": "https://arxiv.org/pdf/2510.03960", "abs": "https://arxiv.org/abs/2510.03960", "authors": ["Jiyoung Yoo", "Jingwei Hu", "Lee F. Ricketson"], "title": "An Explicit Energy-Conserving Particle Method for the Vlasov-Fokker-Planck Equation", "categories": ["physics.plasm-ph", "math-ph", "math.MP"], "comment": null, "summary": "We propose an explicit particle method for the Vlasov-Fokker-Planck equation\nthat conserves energy at the fully discrete level. The method features two key\ncomponents: a deterministic and conservative particle discretization for the\nnonlinear Fokker-Planck operator (also known as the Lenard-Bernstein or\nDougherty operator), and a second-order explicit time integrator that ensures\nenergy conservation through an accuracy-justifiable correction. We validate the\nmethod on several plasma benchmarks, including collisional Landau damping and\ntwo-stream instability, demonstrating its effectiveness.", "AI": {"tldr": "An explicit particle method for Vlasov-Fokker-Planck equation that conserves energy at discrete level using deterministic particle discretization and second-order time integrator.", "motivation": "To develop a numerical method that maintains energy conservation properties at the fully discrete level for the Vlasov-Fokker-Planck equation, which is important for accurate plasma simulations.", "method": "Combines deterministic conservative particle discretization for nonlinear Fokker-Planck operator with second-order explicit time integrator featuring accuracy-justifiable correction for energy conservation.", "result": "Method successfully validated on plasma benchmarks including collisional Landau damping and two-stream instability, demonstrating effectiveness.", "conclusion": "The proposed explicit particle method achieves discrete energy conservation while maintaining computational efficiency, making it suitable for plasma physics simulations."}}
{"id": "2510.04385", "pdf": "https://arxiv.org/pdf/2510.04385", "abs": "https://arxiv.org/abs/2510.04385", "authors": ["Byung Kyu Na", "Stefan Possanner", "Xin Wang"], "title": "Structure-Preserving MHD-Driftkinetic Discretization for Wave-Particle Interactions", "categories": ["physics.comp-ph", "cs.NA", "math.NA", "physics.plasm-ph"], "comment": "29 pages, 9 figures, to be submitted to Journal of Computational\n  Physics", "summary": "We present a structure-preserving discretization of the hybrid\nmagnetohydrodynamics (MHD)-driftkinetic system for simulations of low-frequency\nwave-particle interactions. The model equations are derived from a variational\nprinciple, assuring energetically consistent couplings between MHD fluids and\ndriftkinetic particles. The spatial discretization is based on a\nfinite-element-exterior-calculus (FEEC) framework for the MHD and a\nparticle-in-cell (PIC) method for the driftkinetic. A key feature of the scheme\nis the inclusion of the non-quadratic particle magnetic moment energy term in\nthe Hamiltonian, which is introduced by the guiding-center approximation. The\nresulting discrete Hamiltonian structure naturally organizes the dynamics into\nskew-symmetric subsystems, enabling balanced energy exchange. To handle the\nnon-quadratic energy term, we develop energy-preserving time integrators based\non discrete gradient methods. The algorithm is implemented in the open-source\nPython package \\texttt{STRUPHY}. Numerical experiments confirm the\nenergy-conserving property of the scheme and demonstrate the capability to\nsimulate energetic particles (EP) induced excitation of toroidal Alfv\\'en\neigenmodes (TAE) without artificial dissipation or mode filtering. This\ncapability highlights the potential of structure-preserving schemes for\nhigh-fidelity simulations of hybrid systems.", "AI": {"tldr": "Structure-preserving discretization of hybrid MHD-driftkinetic system for simulating low-frequency wave-particle interactions with energy-conserving properties.", "motivation": "To develop high-fidelity simulations of hybrid magnetohydrodynamics-driftkinetic systems for studying wave-particle interactions, particularly energetic particle-induced excitations in plasma physics.", "method": "Variational principle derivation, finite-element-exterior-calculus (FEEC) for MHD, particle-in-cell (PIC) for driftkinetic particles, discrete gradient time integrators for non-quadratic energy terms.", "result": "Numerical experiments confirm energy conservation and demonstrate capability to simulate energetic particle-induced excitation of toroidal Alfv\u00e9n eigenmodes without artificial dissipation.", "conclusion": "Structure-preserving schemes show potential for high-fidelity simulations of hybrid systems, enabling accurate modeling of wave-particle interactions in plasma physics."}}
{"id": "2510.03412", "pdf": "https://arxiv.org/pdf/2510.03412", "abs": "https://arxiv.org/abs/2510.03412", "authors": ["Pasquale Ambrosio", "Simone Ciani"], "title": "Local boundedness for weak solutions to strongly degenerate orthotropic parabolic equations", "categories": ["math.AP", "35B45, 35B65, 35K10, 35K65, 35K92"], "comment": "16 pages", "summary": "We prove the local boundedness of local weak solutions to the parabolic\nequation \\[ \\partial_{t}u\\,=\\,\\sum_{i=1}^{n}\\partial_{x_{i}}\\left[(\\vert\nu_{x_{i}}\\vert-\\delta_{i})_{+}^{p-1}\\frac{u_{x_{i}}}{\\vert\nu_{x_{i}}\\vert}\\right]\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\mathrm{in}\\,\\,\\,\\Omega_{T}=\\Omega\\times(0,T]\\,,\n\\] where $\\Omega$ is a bounded domain in $\\mathbb{R}^{n}$ with $n\\geq2$,\n$p\\geq2$, $\\delta_{1},\\ldots,\\delta_{n}$ are non-negative numbers and\n$\\left(\\,\\cdot\\,\\right)_{+}$ denotes the positive part. The main novelty here\nis that the above equation combines an orthotropic structure with a strongly\ndegenerate behavior. The core result of this paper thus extends a classical\nboundedness theorem, originally proved for the parabolic $p$-Laplacian, to a\nwidely degenerate anisotropic setting. As a byproduct, we also obtain the local\nboundedness of local weak solutions to the isotropic counterpart of the above\nequation.", "AI": {"tldr": "The paper proves local boundedness of weak solutions to a degenerate anisotropic parabolic equation combining orthotropic structure with strong degeneracy.", "motivation": "To extend classical boundedness theorems from the parabolic p-Laplacian to a more complex anisotropic and degenerate setting where the equation has both orthotropic structure and strong degeneracy.", "method": "The authors analyze local weak solutions to the given parabolic equation with orthotropic structure and degenerate behavior, using mathematical analysis techniques to prove boundedness properties.", "result": "Successfully proved local boundedness of local weak solutions to the degenerate anisotropic parabolic equation, extending classical results to this more complex setting.", "conclusion": "The main novelty is combining orthotropic structure with strong degeneracy, and the core result extends classical boundedness theorems to widely degenerate anisotropic settings, with the isotropic counterpart also covered as a byproduct."}}
{"id": "2510.03826", "pdf": "https://arxiv.org/pdf/2510.03826", "abs": "https://arxiv.org/abs/2510.03826", "authors": ["Yunyun Ma", "Jiguang Sun"], "title": "Fourier-Galerkin method for scattering poles of sound soft obstacles", "categories": ["math.NA", "cs.NA", "65N30, 45C05"], "comment": null, "summary": "The computation of scattering poles for a sound-soft obstacle is\ninvestigated. These poles correspond to the eigenvalues of two boundary\nintegral operators. We construct novel decompositions of these operators and\nshow that they are Fredholm. Then a Fourier-Galerkin method is proposed for\ndiscretization. By establishing the regular convergence of the discrete\noperators, an error estimate is established using the abstract approximation\ntheory for eigenvalue problems of holomorphic Fredholm operator functions. We\ngive details of the numerical implementation. Several examples are presented to\nvalidate the theory and demonstrate the effectiveness of the proposed method.", "AI": {"tldr": "Novel method for computing scattering poles of sound-soft obstacles using boundary integral operators and Fourier-Galerkin discretization with error analysis.", "motivation": "To develop an effective computational method for determining scattering poles, which are important in wave scattering problems and correspond to eigenvalues of boundary integral operators.", "method": "Construct novel decompositions of boundary integral operators, prove they are Fredholm, use Fourier-Galerkin method for discretization, and apply abstract approximation theory for eigenvalue problems.", "result": "Established regular convergence of discrete operators, derived error estimates, and validated the method through numerical examples showing its effectiveness.", "conclusion": "The proposed method provides a rigorous and effective computational approach for computing scattering poles with proven convergence properties and error estimates."}}
{"id": "2510.04298", "pdf": "https://arxiv.org/pdf/2510.04298", "abs": "https://arxiv.org/abs/2510.04298", "authors": ["Ziang Zhu", "Yifan Liu", "Jun Li", "Han Wen", "Shihui Cao", "Yin Shi", "Qing Jia", "Chaoxin Chen", "Yaoyuan Liu", "Hang Zhao", "Tao Gong", "Zhichao Li", "Dong Yang", "Jian Zheng"], "title": "A Particle-in-Cell Simulation Framework for Thomson Scattering Analysis in Inertial Confinement Fusion", "categories": ["physics.plasm-ph"], "comment": null, "summary": "In inertial confinement fusion (ICF), Thomson scattering (TS) is a widely\nused diagnostic technique for probing plasma conditions. We present a\nfirst-principles numerical approach to obtaining scattered light signals of ion\nacoustic features with high resolution in angle and frequency space using\nparticle-in-cell simulations under typical ICF conditions. Our method\ndemonstrates good agreement with existing theories for thermal collective TS.\nIn the super-thermal collective regime, the results align with theory when the\ndriven plasma modes are well-matched in wave vectors to the probe and\ncollecting beams. Moreover, we also find that TS signals can remain significant\neven under imperfect wave-vector matching-a result that contradicts the\nconventional expectation that the TS spectrum strictly follows the plasma\ndensity spectrum. We attribute this discrepancy to a beating wave mechanism\narising from the interaction between the probe beam and driven plasma density\nmodulations. Our work thus provides a practical framework for interpreting TS\nsignals from driven ion modes, a common yet complex feature in ICF plasmas.", "AI": {"tldr": "First-principles numerical approach for Thomson scattering analysis in ICF plasmas shows TS signals remain significant even with imperfect wave-vector matching, challenging conventional expectations.", "motivation": "To develop a practical framework for interpreting Thomson scattering signals from driven ion modes in inertial confinement fusion plasmas, which are common but complex features.", "method": "Used particle-in-cell simulations under typical ICF conditions to obtain scattered light signals of ion acoustic features with high resolution in angle and frequency space.", "result": "Good agreement with existing theories for thermal collective TS; in super-thermal regime, alignment with theory when wave vectors match; TS signals remain significant even with imperfect wave-vector matching due to beating wave mechanism.", "conclusion": "Provides a practical framework for interpreting TS signals from driven ion modes in ICF plasmas, challenging conventional understanding that TS spectrum strictly follows plasma density spectrum."}}
{"id": "2510.04917", "pdf": "https://arxiv.org/pdf/2510.04917", "abs": "https://arxiv.org/abs/2510.04917", "authors": ["Daniele Macuglia", "Giovanni Ciccotti", "Beno\u00eet Roux"], "title": "The dawn of alchemical free-energy methods in biomolecular simulations", "categories": ["physics.comp-ph", "physics.chem-ph"], "comment": "29 pages, 13 figures, 135 references, 39 footnotes", "summary": "From the onset of fundamental statistical mechanical constructs formulated in\nthe late 19th century, alchemical free-energy methods slowly emerged and\ntransitioned to become operational tools of biomolecular simulation applicable\nto a wide range of problems including protein-ligand binding for drug discovery\nresearch. This article reconstructs how statistical mechanical approaches such\nas thermodynamic integration and free-energy perturbation were reconfigured in\nthe early 1980's to address the complexities of increasingly heterogeneous\nbiomolecular systems. Drawing on oral history interviews and primary\nliterature, the study examines the technical, institutional, theoretical, and\ninfrastructural conditions under which these methods were implemented, and\nbecame progressively operational. These conditions encompassed the\nconsolidation of lab-specific software infrastructures, the formulation of\npractical simulation protocols, as well as essential statistical mechanical\nclarifications. From this perspective, the progress of free-energy methods\nproceeded less from a unified convergence than from an iterative\ntroubleshooting process of alignment involving practical and theoretical\nconsiderations. The aim of the present article is to offer a historically\ngrounded account of how free-energy techniques acquired practical and\nfunctional reliability.", "AI": {"tldr": "This paper traces the historical development of alchemical free-energy methods from 19th century statistical mechanics to operational tools for biomolecular simulation, particularly in drug discovery.", "motivation": "To understand how statistical mechanical approaches like thermodynamic integration and free-energy perturbation became practical tools for studying complex biomolecular systems, examining the conditions that enabled their operationalization.", "method": "Historical analysis using oral history interviews and primary literature to examine technical, institutional, theoretical, and infrastructural conditions that facilitated implementation of free-energy methods.", "result": "Found that progress occurred through iterative troubleshooting and alignment of practical and theoretical considerations rather than unified convergence, involving lab-specific software, simulation protocols, and statistical mechanical clarifications.", "conclusion": "Free-energy methods acquired practical reliability through a historically situated process of alignment between theoretical constructs and practical implementation requirements in biomolecular research contexts."}}
{"id": "2510.03600", "pdf": "https://arxiv.org/pdf/2510.03600", "abs": "https://arxiv.org/abs/2510.03600", "authors": ["S. E. Chorfi", "F. Et-tahri", "L. Maniar", "M. Yamamoto"], "title": "Forward and backward problems for abstract time-fractional Schr\u00f6dinger equations", "categories": ["math.AP", "math-ph", "math.MP", "35R11, 35R30, 35R25"], "comment": null, "summary": "We investigate forward and backward problems associated with abstract\ntime-fractional Schr\\\"odinger equations $\\mathrm{i}^\\nu \\partial_t^\\alpha u(t)\n+ A u(t)=0$, $\\alpha \\in (0,1)\\cup (1,2)$ and $\\nu\\in\\{1,\\alpha\\}$, where $A$\nis a self-adjoint operator with compact resolvent on a Hilbert space $H$. This\nkind of equation, which incorporates the Caputo time-fractional derivative of\norder $\\alpha$, models quantum systems with memory effects and anomalous wave\npropagation. We first establish the well-posedness of the forward problems in\ntwo scenarios: ($\\nu=1,\\,$ $\\alpha \\in (0,1)$) and ($\\nu=\\alpha,\\,$ $\\alpha \\in\n(0,1)\\cup (1,2)$). Then, we prove well-posedness and stability results for the\nbackward problems depending on the two cases $\\nu=1$ and $\\nu=\\alpha$. Our\napproach employs the solution's eigenvector expansion along with the properties\nof the Mittag-Leffler functions, including the distribution of zeros and\nasymptotic expansions. Finally, we conclude with a discussion of some open\nproblems.", "AI": {"tldr": "Analysis of forward and backward problems for abstract time-fractional Schr\u00f6dinger equations with Caputo derivatives, establishing well-posedness and stability results using eigenvector expansions and Mittag-Leffler function properties.", "motivation": "To model quantum systems with memory effects and anomalous wave propagation using time-fractional Schr\u00f6dinger equations, which incorporate Caputo derivatives to capture non-local time behavior.", "method": "Employ solution's eigenvector expansion combined with properties of Mittag-Leffler functions, including zero distribution and asymptotic expansions, to analyze two cases: \u03bd=1 with \u03b1\u2208(0,1) and \u03bd=\u03b1 with \u03b1\u2208(0,1)\u222a(1,2).", "result": "Established well-posedness for forward problems in both scenarios and proved well-posedness and stability results for backward problems, with analysis depending on the two cases \u03bd=1 and \u03bd=\u03b1.", "conclusion": "Successfully analyzed time-fractional Schr\u00f6dinger equations with memory effects, providing rigorous mathematical foundations for both forward and backward problems, while identifying remaining open problems for future research."}}
{"id": "2510.03927", "pdf": "https://arxiv.org/pdf/2510.03927", "abs": "https://arxiv.org/abs/2510.03927", "authors": ["Qiwei Feng", "Bin Han", "Michelle Michelle", "Jiwoon Sim"], "title": "High-order, Compact, and Symmetric Finite Difference Methods for a $d$-Dimensional Hypercube", "categories": ["math.NA", "cs.NA", "65N06, 35J25"], "comment": null, "summary": "This paper presents compact, symmetric, and high-order finite difference\nmethods (FDMs) for the variable Poisson equation on a $d$-dimensional\nhypercube. Our scheme produces a symmetric linear system: an important property\nthat does not immediately hold for a high-order FDM. Since the model problem is\ncoercive, the linear system is in fact symmetric positive definite, and\nconsequently many fast solvers are applicable. Furthermore, the symmetry\ncombined with the minimum support of the stencil keeps the storage requirement\nminimal. Theoretically speaking, we prove that a compact, symmetric 1D FDM on a\nuniform grid can achieve arbitrary consistency order. On the other hand, in the\n$d$-dimensional setting, where $d \\ge 2$, the maximum consistency order that a\ncompact, symmetric FDM on a uniform grid can achieve is 4. If $d=2$ and the\ndiffusion coefficient satisfies a certain derivative condition, the maximum\nconsistency order is 6. Moreover, the finite compact, symmetric, 4th-order FDMs\nfor $d\\ge 3$, can be conveniently expressed as a linear combination of two\ntypes of FDMs: one that depends on partial derivatives along one axis, and the\nother along two axes. All finite difference stencils are explicitly provided\nfor ease of reproducibility.", "AI": {"tldr": "Compact symmetric high-order finite difference methods for variable Poisson equation on hypercubes, achieving up to 4th order in d\u22652 dimensions with symmetric positive definite linear systems.", "motivation": "To develop finite difference methods that produce symmetric linear systems for variable Poisson equations, enabling use of fast solvers while maintaining compact stencils and minimal storage requirements.", "method": "Develop compact symmetric finite difference methods on uniform grids for d-dimensional hypercubes, with theoretical analysis of maximum achievable consistency orders under different conditions.", "result": "Proved that 1D compact symmetric FDMs can achieve arbitrary order, while d\u22652 dimensions have maximum order 4 (or 6 for d=2 under certain conditions). Provided explicit stencils for 4th-order methods in d\u22653 as linear combinations of single-axis and two-axis derivative FDMs.", "conclusion": "Compact symmetric high-order FDMs are feasible for variable Poisson equations with proven consistency order bounds, offering symmetric positive definite systems suitable for fast solvers with minimal storage."}}
{"id": "2510.04356", "pdf": "https://arxiv.org/pdf/2510.04356", "abs": "https://arxiv.org/abs/2510.04356", "authors": ["Abetharan Antony", "Robert Kingham", "Stefan Mijin", "Marty Marinak"], "title": "Non-local transport in Radiation-Hydrodynamics codes for ICF by efficient coupling to an external Vlasov-Fokker-Planck code", "categories": ["physics.plasm-ph"], "comment": null, "summary": "Accurately incorporating non-local transport into radiation-hydrodynamics\ncodes, and indeed any fluid system, has long been elusive. To date, a\nsimplified and accurate theory that can be easily integrated has not been\navailable. This limitation affects modeling in inertial confinement fusion and\nmagnetic confinement fusion systems, among others, where non-local transport is\nwell-known to be present. Here, we present a coupling methodology between a\nfull Vlasov-Fokker-Planck (VFP) electron kinetic code and\nradiation-hydrodynamics (rad-hydro) codes. The VFP code is used to adjust\nnative electron transport in the rad-hydro code, thus enabling improved\ntransport without the need to integrate a full electron VFP solver into the\nrad-hydro code. This approach necessitates only occasional invocation of the\nVFP code, reducing computational intensity compared to following the dynamic\nevolution entirely with the VFP code on fluid time scales. We illustrate that\nthe methodology is more accurate than other simplified methods in thermal decay\nsystems relevant to inertial confinement fusion and can replicate standard\ntheoretical results with high accuracy.", "AI": {"tldr": "A coupling methodology between Vlasov-Fokker-Planck electron kinetic code and radiation-hydrodynamics codes to improve non-local transport modeling without full VFP integration.", "motivation": "Non-local transport has been difficult to incorporate accurately into radiation-hydrodynamics codes, affecting fusion modeling where it's known to be present.", "method": "Coupling VFP electron kinetic code with rad-hydro codes, using VFP to adjust native electron transport without full VFP solver integration, requiring only occasional VFP invocation.", "result": "Method is more accurate than other simplified methods in thermal decay systems relevant to ICF and can replicate standard theoretical results with high accuracy.", "conclusion": "The approach enables improved transport modeling with reduced computational intensity compared to full VFP evolution on fluid time scales."}}
{"id": "2510.03325", "pdf": "https://arxiv.org/pdf/2510.03325", "abs": "https://arxiv.org/abs/2510.03325", "authors": ["Giuseppe Di Somma", "Giorgio Carelli", "Angela D. V. Di Virgilio", "Francesco Fuso", "Enrico Maccioni", "Paolo Marsili"], "title": "Fast frequency reconstruction using Deep Learning for event recognition in ring laser data", "categories": ["cs.LG", "physics.comp-ph", "physics.data-an", "physics.geo-ph"], "comment": null, "summary": "The reconstruction of a frequency with minimal delay from a sinusoidal signal\nis a common task in several fields; for example Ring Laser Gyroscopes, since\ntheir output signal is a beat frequency. While conventional methods require\nseveral seconds of data, we present a neural network approach capable of\nreconstructing frequencies of several hundred Hertz within approximately 10\nmilliseconds. This enables rapid trigger generation. The method outperforms\nstandard Fourier-based techniques, improving frequency estimation precision by\na factor of 2 in the operational range of GINGERINO, our Ring Laser\nGyroscope.\\\\ In addition to fast frequency estimation, we introduce an\nautomated classification framework to identify physical disturbances in the\nsignal, such as laser instabilities and seismic events, achieving accuracy\nrates between 99\\% and 100\\% on independent test datasets for the seismic\nclass. These results mark a step forward in integrating artificial intelligence\ninto signal analysis for geophysical applications.", "AI": {"tldr": "Neural network approach for fast frequency reconstruction from sinusoidal signals, achieving ~10ms processing time and 2x better precision than Fourier methods, with automated classification of physical disturbances.", "motivation": "Need for rapid frequency reconstruction from sinusoidal signals in applications like Ring Laser Gyroscopes, where conventional methods require several seconds of data.", "method": "Neural network approach for frequency estimation and automated classification framework to identify physical disturbances like laser instabilities and seismic events.", "result": "Frequency reconstruction within ~10ms (vs seconds for conventional methods), 2x improvement in frequency estimation precision, and 99-100% accuracy for seismic event classification on test datasets.", "conclusion": "Successful integration of AI into signal analysis for geophysical applications, enabling rapid trigger generation and improved disturbance identification."}}
{"id": "2510.03708", "pdf": "https://arxiv.org/pdf/2510.03708", "abs": "https://arxiv.org/abs/2510.03708", "authors": ["Mourad Choulli"], "title": "Inverse spectral problems for higher-order coefficients", "categories": ["math.AP", "35R30, 35R01, 35P99"], "comment": null, "summary": "We establish uniqueness and stability inequalities for the problem of\ndetermining the higher-order coefficients of an elliptic operator from the\ncorresponding boundary spectral data (BSD). Our analysis relies on the\nrelationship between boundary spectral data and elliptic and hyperbolic\nDirichlet to Neumann (DtN) maps. We also show how to adapt our analysis to\nobtain uniqueness and stability inequalities for determining the conductivity\nor the potential in an elliptic operator from the corresponding BSD.", "AI": {"tldr": "Uniqueness and stability inequalities for determining higher-order coefficients of elliptic operators from boundary spectral data, with extensions to conductivity and potential determination.", "motivation": "To establish rigorous mathematical foundations for inverse problems involving elliptic operators, specifically determining coefficients from boundary spectral data.", "method": "Analysis based on relationship between boundary spectral data and elliptic/hyperbolic Dirichlet-to-Neumann maps, adapting approach for different coefficient types.", "result": "Proved uniqueness and stability inequalities for higher-order coefficients, conductivity, and potential determination from boundary spectral data.", "conclusion": "The paper provides mathematical guarantees for coefficient determination in elliptic operators using boundary spectral data, with applications to conductivity and potential recovery."}}
{"id": "2510.03972", "pdf": "https://arxiv.org/pdf/2510.03972", "abs": "https://arxiv.org/abs/2510.03972", "authors": ["Tsiry Avisoa Randrianasolo"], "title": "A discrete data assimilation algorithm for the reconstruction of Gray--Scott dynamics", "categories": ["math.NA", "cs.NA", "math.AP", "35K57, 35Q92, 65M08, 65M12, 65M15, 65M20, 93B52"], "comment": "20 pages, 4 figures", "summary": "The Gray--Scott model governs the interaction of two chemical species via a\nsystem of reaction-diffusion equations. Despite its simple form, it produces\nextremely rich patterns such as spots, stripes, waves, and labyrinths. That\nmakes it ideal for studying emergent behavior, self-organization, and\ninstability-driven pattern formation. It is also known for its sensitivity to\npoorly observed initial conditions. Using such initial conditions alone quickly\nleads simulations to deviate from the true dynamics. The present paper\naddresses this challenge with a nudging-based data assimilation algorithm:\ncoarse, cell-averaged measurements are injected into the model through a\nfeedback (nudging) term, implemented as a finite-volume interpolant. We prove\ntwo main results. (i) For the continuous problem, the nudged solution\nsynchronizes with the true dynamics, and the $L^2$-error decays exponentially\nunder conditions that tie observation resolution, nudging gains, and diffusion.\n(ii) For the fully discrete semi-implicit finite-volume scheme, the same\nsynchronization holds, up to a mild time-step restriction. Numerical tests on\nlabyrinthine patterns support the theory. They show recovery of fine structure\nfrom sparse data and clarify how the observation resolution, the nudging gain,\nand the frequency of updates affect the decay rate.", "AI": {"tldr": "The paper presents a nudging-based data assimilation algorithm for the Gray-Scott reaction-diffusion model to address sensitivity to initial conditions, proving synchronization between nudged and true solutions with exponential error decay.", "motivation": "The Gray-Scott model is highly sensitive to poorly observed initial conditions, causing simulations to quickly deviate from true dynamics, which motivates the need for data assimilation methods.", "method": "A nudging-based data assimilation algorithm using coarse, cell-averaged measurements injected through a feedback term implemented as a finite-volume interpolant, with both continuous and discrete formulations.", "result": "Theoretical proofs show: (i) for continuous problem, nudged solution synchronizes with true dynamics with exponential L\u00b2-error decay; (ii) for discrete scheme, same synchronization holds with mild time-step restriction. Numerical tests confirm recovery of fine structure from sparse data.", "conclusion": "The nudging method effectively addresses initial condition sensitivity in the Gray-Scott model, enabling accurate pattern recovery from sparse observations through proper parameter tuning of observation resolution, nudging gain, and update frequency."}}
{"id": "2510.04658", "pdf": "https://arxiv.org/pdf/2510.04658", "abs": "https://arxiv.org/abs/2510.04658", "authors": ["Manthan Verma", "Abhishek K. Jha", "Shashwat Nirgudkar", "Mahendra K. Verma"], "title": "Numerical Demonstration of Kolmogorov Scaling in Magnetohydrodynamic Turbulence", "categories": ["physics.plasm-ph", "physics.flu-dyn"], "comment": "21 pages, 11 figures", "summary": "The two leading models of isotropic magnetohydrodynamic (MHD) turbulence have\ncompeting predictions: $k^{-5/3}$ (Kolmogorov) and $k^{-3/2}$\n(Iroshnikov-Kraichnan) scalings. This paper identifies the valid MHD turbulence\nmodel using high-resolution numerical and diagnostics-structure functions,\nintermittency exponents, and energy spectra and fluxes of imbalance MHD. The\nenergy spectra of our forced MHD simulations on $8192^2$, $4096^2$, $1024^3$,\nand $512^3$ support Kolmogorov's k^{-5/3} spectrum over Iroshnikov-Kraichnan's\nk^{-3/2} spectrum, but the difference in the spectral exponents is small.\nHowever, the numerically computed third-order structure functions and\nintermittency exponents support Kolmogorov scaling in both two and three\ndimensions. Also, the energy fluxes of the imbalance MHD follow the predictions\nof Kolmogorov scaling. These results would help in better modelling of solar\nwind, solar corona, and dynamos.", "AI": {"tldr": "This paper identifies the valid MHD turbulence model using high-resolution simulations, showing that Kolmogorov's k^{-5/3} scaling is supported over Iroshnikov-Kraichnan's k^{-3/2} scaling through energy spectra, structure functions, intermittency exponents, and energy flux analysis.", "motivation": "To resolve the competing predictions between Kolmogorov's k^{-5/3} and Iroshnikov-Kraichnan's k^{-3/2} scalings in isotropic magnetohydrodynamic (MHD) turbulence using high-resolution numerical simulations.", "method": "Used high-resolution forced MHD simulations on grids of 8192^2, 4096^2, 1024^3, and 512^3, analyzing energy spectra, third-order structure functions, intermittency exponents, and energy fluxes of imbalance MHD.", "result": "Energy spectra support Kolmogorov's k^{-5/3} spectrum over Iroshnikov-Kraichnan's k^{-3/2}, though the difference in spectral exponents is small. Third-order structure functions and intermittency exponents consistently support Kolmogorov scaling in both 2D and 3D. Energy fluxes of imbalance MHD follow Kolmogorov scaling predictions.", "conclusion": "Kolmogorov scaling is the valid model for MHD turbulence, which will help improve modeling of solar wind, solar corona, and dynamos."}}
{"id": "2510.03383", "pdf": "https://arxiv.org/pdf/2510.03383", "abs": "https://arxiv.org/abs/2510.03383", "authors": ["Noh Zainal Abidin", "Frederic Grondin", "Pol Muller", "Jean-Fran\u00e7ois Sigrist"], "title": "Assessment of Hybrid RANS-LES and WALE Formulations for Wake and Resistance Prediction of the BB2 Submarine", "categories": ["physics.flu-dyn", "physics.class-ph", "physics.comp-ph"], "comment": "Proceedings of the 27th Numerical Towing Tank Symposium (NuTTS 2025),\n  Cloud Towing Tank; University of Duisburg-Essen, Sep 2025, Zagreb, Croatia", "summary": "Submarine hydrodynamics presents unique challenges in accurately predicting\nflow separation, wake structure, and resistance due to complex geometry and\nturbulent behaviour at high Reynolds (Re) numbers. Traditional\nReynolds-Averaged Navier-Stokes (RANS) approaches are often limited in\nresolving unsteady flow structures and turbulence in the near and far regions.\nTo address these limitations, hybrid RANS-LES models such as Detached Eddy\nSimulation (DES) and Large Eddy Simulation (LES) offer improved performance in\ncapturing near-wall vortical structures. The capturing of turbulent vortices\nand wake structures significantly contributes to conduct hydrodynamic noise\nanalysis. Detailed resolution and understanding of these coherent structures\nhelp minimize hydroacoustic signatures, essential for submarines' stealth\ncharacteristics. Based on prior studies, Breuer et al. (2003) reported that\nRANS failed to capture unsteady vortex shedding, producing only steady results\neven in 3D simulations. In contrast, DES and LES successfully resolved\nasymmetric shedding across different grid resolutions. Spalart (2009) reported\nthat DES is more effective than RANS or LES for high Re flows, although it\nsuffers from challenges related to ambiguous grids and nonmonotonic grid\nrefinement behaviour. Whereas Liang \\& Xue (2014) found that DES predicts tip\nvortex flow characteristics more accurately than RANS-SA and can capture\ncomplex 3D vortex structures. In addition, Guilmineau et al. (2018)\ndemonstrated that the IDDES model accurately predicts recirculation bubbles and\naligns more closely with experimental data for flow prediction. Long et al.\n(2021) confirmed the capability of DDES in simulating cavitating flows around\nhydrofoils and marine propellers. Lungu (2022) highlighted the efficiency and\naccuracy of the hybrid IDDES-SST model in DARPA submarine simulations. Zhang et\nal. (2023) also noted that URANS struggles with resolving small-scale\nturbulence structures, whereas IDDES is better suited for predicting complex\nphenomena such as ship air wake asymmetry. Nevertheless, capturing the\nunsteadiness and turbulence fluctuations scales is extremely challenging\nbecause the cell size requirements should suit each turbulence model employed.\nThus, as a continuation of the prior work of Abidin et al. (2024), the unsteady\nsimulation with high mesh resolution at $U_m=1.8235$ m/s and Re of $3.6\\times\n10^6$ to generate the asymmetrical wake dynamic and vortical structure. The\ncurrent research expands the methodology by parameterizing the meshes and\nnumerical scheme based on Taylor microscale refinement with respect to the\ncharacteristic length of (L, B and D) of submarine, particularly focusing on\nhybrid turbulence model and WALE to observe the ability to resolve turbulence\nin the wake region. The transient simulations were performed initially using\nwall-resolved mesh (76x10^6 cells) at y^+ < 5 and then wall-modelled mesh\n(56x10^6 to 74x10^6 cells) at y^+ > 30, which produced notably different and\nmore detailed results (vortices and turbulence fluctuation) than previous\nsteady-state RANS simulations without risking the accuracy of quantity of\ninterest (global resistance).", "AI": {"tldr": "This paper analyzes submarine hydrodynamics using hybrid RANS-LES models to better capture flow separation, wake structures, and turbulence at high Reynolds numbers, addressing limitations of traditional RANS approaches.", "motivation": "Traditional RANS methods fail to resolve unsteady flow structures and turbulence in submarine hydrodynamics, particularly for predicting flow separation, wake structure, and resistance at high Reynolds numbers. Hybrid models are needed to improve accuracy for hydroacoustic analysis and stealth characteristics.", "method": "Used hybrid turbulence models (DES, LES, IDDES) with wall-resolved and wall-modelled meshes (56-76 million cells) at different y+ values. Parameterized meshes based on Taylor microscale refinement and employed WALE model to resolve turbulence in wake regions at Re=3.6\u00d710^6.", "result": "Hybrid models successfully captured asymmetric vortex shedding and detailed turbulent structures that RANS missed. Wall-resolved mesh (y+<5) and wall-modelled mesh (y+>30) both produced more detailed vortices and turbulence fluctuations than steady-state RANS while maintaining accuracy for global resistance prediction.", "conclusion": "Hybrid RANS-LES models like DES and IDDES are superior to traditional RANS for submarine hydrodynamics, effectively resolving complex 3D vortex structures and unsteady flow phenomena essential for hydroacoustic analysis and stealth optimization."}}
{"id": "2510.03740", "pdf": "https://arxiv.org/pdf/2510.03740", "abs": "https://arxiv.org/abs/2510.03740", "authors": ["Yu Xiao", "Can Zhang"], "title": "Rapid boundary stabilization of 1D nonlinear parabolic equations", "categories": ["math.AP", "math.OC", "35K55, 93C20, 93D23"], "comment": "28 pages", "summary": "In this paper, we focus on the rapid boundary stabilization of 1D nonlinear\nparabolic equations via the modal decomposition method. The nonlinear term is\nassumed to satisfy certain local Lipschitz continuity and global growth\nconditions. Through the modal decomposition, we construct a feedback control\nthat modifies only the unstable eigenvalues to achieve spectral reduction.\nUnder this control, we establish locally rapid stabilization by estimating the\nnonlinearity in Lyapunov stability analysis. Furthermore, utilizing the\ndissipative property, we derive a globally rapid stabilization result for\ndissipative systems such as the Burgers equation and the Allen-Cahn equation.", "AI": {"tldr": "Rapid boundary stabilization of 1D nonlinear parabolic equations using modal decomposition method to modify unstable eigenvalues for spectral reduction.", "motivation": "To achieve rapid stabilization of nonlinear parabolic equations through boundary control, addressing both local and global stabilization challenges.", "method": "Modal decomposition method with feedback control that modifies unstable eigenvalues, combined with Lyapunov stability analysis and dissipative property utilization.", "result": "Established locally rapid stabilization through nonlinearity estimation in Lyapunov analysis, and globally rapid stabilization for dissipative systems like Burgers and Allen-Cahn equations.", "conclusion": "The modal decomposition approach successfully achieves both local and global rapid stabilization for 1D nonlinear parabolic equations with appropriate nonlinearity conditions."}}
{"id": "2510.04060", "pdf": "https://arxiv.org/pdf/2510.04060", "abs": "https://arxiv.org/abs/2510.04060", "authors": ["Tong Mao", "Jinchao Xu"], "title": "Sharp Lower Bounds for Linearized ReLU^k Approximation on the Sphere", "categories": ["math.NA", "cs.LG", "cs.NA"], "comment": null, "summary": "We prove a saturation theorem for linearized shallow ReLU$^k$ neural networks\non the unit sphere $\\mathbb S^d$. For any antipodally quasi-uniform set of\ncenters, if the target function has smoothness $r>\\tfrac{d+2k+1}{2}$, then the\nbest $\\mathcal{L}^2(\\mathbb S^d)$ approximation cannot converge faster than\norder $n^{-\\frac{d+2k+1}{2d}}$. This lower bound matches existing upper bounds,\nthereby establishing the exact saturation order $\\tfrac{d+2k+1}{2d}$ for such\nnetworks. Our results place linearized neural-network approximation firmly\nwithin the classical saturation framework and show that, although ReLU$^k$\nnetworks outperform finite elements under equal degrees $k$, this advantage is\nintrinsically limited.", "AI": {"tldr": "The paper establishes the exact saturation order for linearized shallow ReLU^k neural networks on the unit sphere, showing they cannot converge faster than n^{-(d+2k+1)/(2d)} for target functions with smoothness r > (d+2k+1)/2.", "motivation": "To understand the fundamental limitations of ReLU^k neural network approximation and place linearized neural-network approximation within classical saturation theory, comparing their performance with finite element methods.", "method": "Proving a saturation theorem for linearized shallow ReLU^k networks on the unit sphere using antipodally quasi-uniform sets of centers and analyzing approximation rates in L^2(S^d) norm.", "result": "The best L^2(S^d) approximation cannot converge faster than order n^{-(d+2k+1)/(2d)} when the target function has smoothness r > (d+2k+1)/2, matching existing upper bounds and establishing the exact saturation order.", "conclusion": "ReLU^k networks outperform finite elements under equal degrees k, but this advantage is intrinsically limited by the saturation order (d+2k+1)/(2d), placing linearized neural-network approximation firmly within classical saturation framework."}}
{"id": "2510.05058", "pdf": "https://arxiv.org/pdf/2510.05058", "abs": "https://arxiv.org/abs/2510.05058", "authors": ["Trishul Dhalia", "Rohit Juneja", "Amita Das"], "title": "Pushing the Frontiers of Light: Magnetized Plasma Lenses and Chirp Tailoring for Extreme Intensities", "categories": ["physics.plasm-ph", "physics.optics"], "comment": "7 pages,6 figures", "summary": "In this work, an innovative scheme is proposed that exploits the response of\nmagnetized plasmas to realize a refractive index exceeding unity for right\ncircularly polarized (RCP) waves. Using two- and three-dimensional\nParticle-in-Cell (PIC) simulations with the OSIRIS 4.0 framework, it is shown\nthat a shaped magnetized plasma lens (MPL) can act as a glass/solid-state-based\nconvex lens, amplifying laser intensity via transverse focusing. Moreover, by\nintegrating three key ingredients, a tailored plasma lens geometry, a spatially\nstructured strong magnetic field, and a suitably chirped laser pulse,\nsimultaneous focusing and compression of the pulse has been achieved. The\nsimulations reveal up to a 100-fold increase in laser intensity, enabled by the\ncombined action of the MPL and the chirped pulse profile. With recent advances\nin high-field magnet technology, shaped plasma targets, and controlled chirped\nlaser systems, this approach offers a promising pathway toward experimentally\nreaching extreme intensities.", "AI": {"tldr": "A magnetized plasma lens scheme achieves refractive index >1 for RCP waves, enabling 100x laser intensity amplification through combined focusing and compression.", "motivation": "To develop a method for achieving extreme laser intensities using magnetized plasmas as optical elements, leveraging recent advances in magnet technology and laser systems.", "method": "Used 2D/3D PIC simulations with OSIRIS 4.0 framework, combining tailored plasma lens geometry, structured magnetic field, and chirped laser pulse for simultaneous focusing and compression.", "result": "Achieved up to 100-fold increase in laser intensity through transverse focusing and pulse compression enabled by the magnetized plasma lens system.", "conclusion": "This approach provides a promising experimental pathway to reach extreme laser intensities using current magnet, plasma target, and chirped laser technologies."}}
{"id": "2510.03388", "pdf": "https://arxiv.org/pdf/2510.03388", "abs": "https://arxiv.org/abs/2510.03388", "authors": ["Vidushi Sharma", "Lee A. Collins", "Alexander J. White"], "title": "Mixed Stochastic-Deterministic Density Functional Theoretic Decomposition of Kubo-Greenwood Conductivities in the Projector Augmented Wave Formalism", "categories": ["physics.chem-ph", "astro-ph.HE", "cond-mat.mtrl-sci", "physics.comp-ph", "physics.plasm-ph"], "comment": "11 pages, 5 figures + supplemental material = 12 pages, 6 figures", "summary": "Pairing the accuracy of Kohn-Sham density-functional framework with the\nefficiency of a stochastic algorithmic approach, mixed stochastic-deterministic\nDensity Functional Theory (mDFT) achieves a favorable computational scaling\nwith system sizes and electronic temperatures. We employ the recently developed\nmDFT formalism to investigate the dynamic charge-transport properties of\nsystems in the warm dense matter regime. The optical conductivity spectra are\ncomputed for single- and multi- component mixtures of carbon, hydrogen, and\nberyllium using two complementary approaches: Kubo-Greenwood in the mDFT\npicture and real-time Time-Dependent mDFT. We further devise a decomposition of\nthe Onsager coefficients leading up to the Kubo-Greenwood spectra to exhibit\ncontributions from the deterministic, stochastic, and mixed electronic state\ntransitions at different incident photon energies.", "AI": {"tldr": "Mixed stochastic-deterministic DFT (mDFT) combines accuracy and efficiency to study dynamic charge-transport in warm dense matter, using Kubo-Greenwood and real-time TD-mDFT approaches.", "motivation": "To investigate dynamic charge-transport properties in warm dense matter systems with favorable computational scaling using a mixed stochastic-deterministic approach.", "method": "Employed mDFT formalism with two complementary approaches: Kubo-Greenwood in mDFT picture and real-time Time-Dependent mDFT. Also developed decomposition of Onsager coefficients to analyze electronic state transitions.", "result": "Computed optical conductivity spectra for single- and multi-component mixtures of carbon, hydrogen, and beryllium in warm dense matter regime.", "conclusion": "mDFT provides an efficient framework for studying charge-transport in warm dense matter with decomposition revealing contributions from different electronic state transitions."}}
{"id": "2510.03794", "pdf": "https://arxiv.org/pdf/2510.03794", "abs": "https://arxiv.org/abs/2510.03794", "authors": ["Farid Bozorgnia", "Avetik Arakelyan"], "title": "Gamma Convergence of Partially Segregated Elliptic Systems", "categories": ["math.AP", "35J50, 49J45, 35R35, 49Q05"], "comment": "25 pages, 1 figure; Keywords: Gamma-convergence, segregation\n  problems, penalty methods, triple junctions, free boundary problems", "summary": "We study partially segregated elliptic systems through the use of penalized\nenergy functionals. These systems arise from the minimization of\nGross-Pitaevskii-type energies that capture the behavior of multi-component\nultracold gas mixtures and other systems involving multiple interacting fluid\nor gas species. In the case when the domain is planar, i.e., in $\\mathbb{R}^2$,\nour main result is the Gamma convergence of penalized energy to the constrained\nDirichlet energy with strict segregation. The proof combines lower\nsemicontinuity arguments with a recovery sequence construction based on\ngeometric decompositions near interfaces and triple junctions. This establishes\na rigorous variational link between the penalized and constrained formulations.", "AI": {"tldr": "Gamma convergence of penalized energy to constrained Dirichlet energy with strict segregation for partially segregated elliptic systems in 2D.", "motivation": "Study multi-component ultracold gas mixtures and systems with multiple interacting fluid/gas species through Gross-Pitaevskii-type energies.", "method": "Use penalized energy functionals and combine lower semicontinuity arguments with recovery sequence construction based on geometric decompositions near interfaces and triple junctions.", "result": "Established Gamma convergence in planar domains (R^2) from penalized energy to constrained Dirichlet energy with strict segregation.", "conclusion": "Provides rigorous variational link between penalized and constrained formulations for partially segregated elliptic systems."}}
{"id": "2510.04112", "pdf": "https://arxiv.org/pdf/2510.04112", "abs": "https://arxiv.org/abs/2510.04112", "authors": ["Liang Pan", "Wei Chen", "Jianxian Qiu", "Tao Xiong"], "title": "High order well-balanced and total-energy-conserving local discontinuous Galerkin methods for compressible self-gravitating Euler equations", "categories": ["math.NA", "cs.NA", "35L65, 65M60, 76L05"], "comment": null, "summary": "In this paper, we develop a high order structure-preserving local\ndiscontinuous Galerkin (DG) scheme for the compressible self-gravitating Euler\nequations, which pose great challenges due to the presence of time-dependent\ngravitational potential. The designed scheme is well-balanced for general\npolytropic equilibrium state and total energy conserving for multiple spatial\ndimensions without an assumption of spherical symmetry. The well-balanced\nproperty is achieved by decomposing the gravitational potential into\nequilibrium and perturbation parts, employing a modified Harten-Lax-van\nLeer-contact flux and a modification of the discretization for the source term.\nConservation of total energy is particularly challenging in the presence of\nself-gravity, especially when aiming for high order accuracy. To address this,\nwe rewrite the energy equation into a conservative form, and carefully design\nan energy flux with the aid of weak formulation from the DG method to maintain\nconservation as well as high order accuracy. The resulting scheme can be\nextended to high order in time discretizations. Numerical examples for two and\nthree dimensional problems are provided to verify the desired properties of our\nproposed scheme, including shock-capturing, high order accuracy, well balance,\nand total energy conservation.", "AI": {"tldr": "A high-order local discontinuous Galerkin scheme for compressible self-gravitating Euler equations that achieves well-balanced property for general polytropic equilibrium and total energy conservation in multiple dimensions.", "motivation": "The compressible self-gravitating Euler equations pose challenges due to time-dependent gravitational potential, requiring schemes that maintain well-balanced properties and energy conservation, especially for high-order accuracy.", "method": "Decompose gravitational potential into equilibrium and perturbation parts, use modified HLLC flux and source term discretization for well-balance, rewrite energy equation in conservative form with carefully designed energy flux using DG weak formulation.", "result": "The scheme achieves well-balanced property for general polytropic equilibrium states and total energy conservation in multiple spatial dimensions without spherical symmetry assumption, with high-order accuracy in time.", "conclusion": "Numerical examples in 2D and 3D verify the scheme's shock-capturing, high-order accuracy, well-balanced property, and total energy conservation capabilities."}}
{"id": "2510.05084", "pdf": "https://arxiv.org/pdf/2510.05084", "abs": "https://arxiv.org/abs/2510.05084", "authors": ["Ethan Kahn"], "title": "Electrospray Thruster Plume Impingement on CubeSat Solar Arrays: A Particle-Tracking Study", "categories": ["physics.plasm-ph"], "comment": "16 pages, 3 figures, Preprint", "summary": "Electrospray thrusters are emerging as a leading propulsion technology for\nCubeSats, offering high specific impulse ($I_{sp} > 1000$ s) and low power\nrequirements. However, the divergent ion plumes can impinge on spacecraft\nsurfaces, particularly body-mounted solar arrays, causing contamination and\nthrust efficiency losses. This study presents a validated particle-tracking\nsimulation to quantify the effects of thruster placement on thrust efficiency\nand surface contamination for 1U, 3U, and 6U CubeSats. The plume model employs\na cosine power distribution ($k=1.8$) with half-angle $46^\\circ$, validated\nagainst experimental data with errors below 7%. Results show that thrust\nefficiency ranges from 53.6% for rear-mounted thrusters on 3U body-mounted\nconfigurations to 100% for side-mounted configurations with deployable arrays.\nCubeSat size significantly affects impingement: 3U platforms experience 46.4%\ncontamination with rear-mounted thrusters compared to 16.6% for 1U. Deployable\nsolar arrays reduce contamination by 77% compared to body-mounted arrays, while\nside-mounted thrusters eliminate impingement entirely at the cost of only 1.6%\nefficiency loss. Corner-mounted configurations at $30^\\circ$ cant provide\nintermediate performance with 88.9% efficiency and 11.1% contamination. These\nquantitative design guidelines enable mission planners to optimize thruster\nintegration based on power budget and propellant mass constraints, with\nstatistical uncertainty below 0.15% across all configurations.", "AI": {"tldr": "Study analyzes electrospray thruster placement on CubeSats to optimize thrust efficiency and minimize surface contamination using validated particle-tracking simulations.", "motivation": "Electrospray thrusters are promising for CubeSats but their divergent ion plumes can contaminate spacecraft surfaces, particularly body-mounted solar arrays, reducing thrust efficiency.", "method": "Validated particle-tracking simulation using cosine power distribution (k=1.8) with 46\u00b0 half-angle, validated against experimental data with <7% errors, analyzing various thruster placements on 1U, 3U, and 6U CubeSats.", "result": "Thrust efficiency ranges from 53.6% (rear-mounted on 3U) to 100% (side-mounted with deployable arrays). Deployable arrays reduce contamination by 77%, side-mounted thrusters eliminate impingement with only 1.6% efficiency loss. 3U platforms have 46.4% contamination vs 16.6% for 1U.", "conclusion": "Quantitative design guidelines enable mission planners to optimize thruster integration based on power budget and propellant mass constraints, with statistical uncertainty below 0.15% across configurations."}}
{"id": "2510.03557", "pdf": "https://arxiv.org/pdf/2510.03557", "abs": "https://arxiv.org/abs/2510.03557", "authors": ["Nicholas Frontiere", "J. D. Emberson", "Michael Buehlmann", "Esteban M. Rangel", "Salman Habib", "Katrin Heitmann", "Patricia Larsen", "Vitali Morozov", "Adrian Pope", "Claude-Andr\u00e9 Faucher-Gigu\u00e8re", "Antigoni Georgiadou", "Damien Lebrun-Grandi\u00e9", "Andrey Prokopenko"], "title": "Cosmological Hydrodynamics at Exascale: A Trillion-Particle Leap in Capability", "categories": ["cs.DC", "astro-ph.CO", "astro-ph.IM", "cs.PF", "physics.comp-ph"], "comment": null, "summary": "Resolving the most fundamental questions in cosmology requires simulations\nthat match the scale, fidelity, and physical complexity demanded by\nnext-generation sky surveys. To achieve the realism needed for this critical\nscientific partnership, detailed gas dynamics, along with a host of\nastrophysical effects, must be treated self-consistently with gravity for\nend-to-end modeling of structure formation. As an important step on this\nroadmap, exascale computing enables simulations that span survey-scale volumes\nwhile incorporating key subgrid processes that shape complex cosmic structures.\nWe present results from CRK-HACC, a cosmological hydrodynamics code built for\nthe extreme scalability requirements set by modern cosmological surveys. Using\nseparation-of-scale techniques, GPU-resident tree solvers, in situ analysis\npipelines, and multi-tiered I/O, CRK-HACC executed Frontier-E: a four trillion\nparticle full-sky simulation, over an order of magnitude larger than previous\nefforts. The run achieved 513.1 PFLOPs peak performance, processing 46.6\nbillion particles per second and writing more than 100 PB of data in just over\none week of runtime.", "AI": {"tldr": "CRK-HACC cosmological hydrodynamics code executed Frontier-E, a four trillion particle full-sky simulation, achieving 513.1 PFLOPs peak performance and processing 46.6 billion particles per second.", "motivation": "To address fundamental cosmology questions and match the scale, fidelity, and complexity required by next-generation sky surveys through realistic simulations that incorporate detailed gas dynamics and astrophysical effects.", "method": "Used CRK-HACC code with separation-of-scale techniques, GPU-resident tree solvers, in situ analysis pipelines, and multi-tiered I/O for extreme scalability.", "result": "Successfully ran Frontier-E simulation with four trillion particles, achieving 513.1 PFLOPs peak performance, processing 46.6 billion particles per second, and writing over 100 PB of data in one week.", "conclusion": "Exascale computing enables unprecedented cosmological simulations that incorporate key subgrid processes and span survey-scale volumes, advancing the realism needed for scientific partnership with next-generation sky surveys."}}
{"id": "2510.03835", "pdf": "https://arxiv.org/pdf/2510.03835", "abs": "https://arxiv.org/abs/2510.03835", "authors": ["Yuanjie Lei", "Shuangqian Liu", "Qinghua Xiao", "Huijiang Zhao"], "title": "The non-cutoff Vlasov-Poisson-Boltzmann system with weak collisions", "categories": ["math.AP"], "comment": null, "summary": "We prove global existence of smooth solutions near Maxwellians for the\nnon-cutoff Vlasov-Poisson-Boltzmann system in the weakly collisional regime. To\naddress the weak dissipation of the non-cutoff linearized Boltzmann operator,\nwe develop a refined velocity-weighted energy framework combined with\nvector-field techniques to control the transport term, nonlinear collisions,\nand the self-consistent electric field. This approach yields uniform-in-time\nbounds, captures enhanced dissipation of the solution, and establishes Landau\ndamping for both the density and electric field, providing the first\nglobal-in-time result of this type for the non-cutoff Vlasov-Poisson-Boltzmann\nsystem. Our approach is inspired by the recent work of Chaturvedi-Luk-Nguyen\n({\\it J. Amer. Math. Soc.} {\\bf 36} (2023), no. 4, 1103--1189.)", "AI": {"tldr": "Global existence of smooth solutions near Maxwellians for non-cutoff Vlasov-Poisson-Boltzmann system in weakly collisional regime, establishing Landau damping for density and electric field.", "motivation": "To address the weak dissipation of the non-cutoff linearized Boltzmann operator and establish the first global-in-time result for the non-cutoff Vlasov-Poisson-Boltzmann system.", "method": "Developed a refined velocity-weighted energy framework combined with vector-field techniques to control transport term, nonlinear collisions, and self-consistent electric field.", "result": "Proved global existence of smooth solutions near Maxwellians, obtained uniform-in-time bounds, captured enhanced dissipation, and established Landau damping for both density and electric field.", "conclusion": "Successfully established the first global-in-time result for the non-cutoff Vlasov-Poisson-Boltzmann system with Landau damping effects."}}
{"id": "2510.04123", "pdf": "https://arxiv.org/pdf/2510.04123", "abs": "https://arxiv.org/abs/2510.04123", "authors": ["Wei Chen", "Shumo Cui", "Kailiang Wu", "Tao Xiong", "Baoyue Yu"], "title": "Bound-Preserving WENO Schemes for Temple-class systems", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "This paper explores numerical schemes for Temple-class systems, which are\nintegral to various applications including one-dimensional two-phase flow,\nelasticity, traffic flow, and sedimentation. Temple-class systems are\ncharacterized by conservative equations, with different pressure function\nexpressions leading to specific models such as the Aw-Rascle-Zhang (ARZ)\ntraffic model and the sedimentation model. Our work extends existing studies by\nintroducing a moving mesh approach to address the challenges of preserving\nnon-convex invariant domains, a common issue in the numerical simulation of\nsuch systems. Our study outlines a novel bound-preserving (BP) and conservative\nnumerical scheme, designed specifically for non-convex sets in Temple-class\nsystems, which is critical for avoiding non-physical solutions and ensuring\nrobustness in simulations. We develop both local and global BP methods based on\nfinite difference schemes, with numerical experiments demonstrating the\neffectiveness and reliability of our methods. Furthermore, a parameterized flux\nlimiter is introduced to restrict high-order fluxes and maintain bound\npreservation. This innovation marks the first time such a parameterized\napproach has been applied to non-convex sets, offering significant improvements\nover traditional methods. The findings presented extend beyond theoretical\nimplications, as they are applicable to general Temple-class systems and can be\ntailored to ARZ traffic flow networks, highlighting the versatility and broad\napplicability of our approach. The paper contributes significantly to the field\nby providing a comprehensive method that maintains the physical and\nmathematical constrains of Temple-class systems.", "AI": {"tldr": "This paper introduces a novel bound-preserving numerical scheme for Temple-class systems using a moving mesh approach and parameterized flux limiter to handle non-convex invariant domains, with applications to traffic flow, elasticity, and sedimentation models.", "motivation": "To address challenges in preserving non-convex invariant domains in numerical simulations of Temple-class systems, which is crucial for avoiding non-physical solutions and ensuring robustness in applications like traffic flow, elasticity, and sedimentation.", "method": "Developed local and global bound-preserving methods based on finite difference schemes with a moving mesh approach. Introduced a parameterized flux limiter to restrict high-order fluxes and maintain bound preservation for non-convex sets.", "result": "Numerical experiments demonstrated the effectiveness and reliability of the proposed methods. The parameterized flux limiter approach is the first application to non-convex sets, offering significant improvements over traditional methods.", "conclusion": "The paper provides a comprehensive method that maintains physical and mathematical constraints of Temple-class systems, with broad applicability to general Temple-class systems and specific models like ARZ traffic flow networks, representing a significant contribution to the field."}}
{"id": "2510.02955", "pdf": "https://arxiv.org/pdf/2510.02955", "abs": "https://arxiv.org/abs/2510.02955", "authors": ["Theodore D. Drivas", "Tarek M. Elgindi", "Daniel Ginsberg"], "title": "On the existence of fibered three-dimensional perfect fluid equilibria without continuous Euclidean symmetry", "categories": ["math.AP", "physics.flu-dyn", "physics.plasm-ph", "35Q31, 76B03, 76W05"], "comment": null, "summary": "Following Lortz, we construct a family of smooth steady states of the ideal,\nincompressible Euler equation in three dimensions that possess no continuous\nEuclidean symmetry. As in Lortz, they do possess a planar reflection symmetry\nand, as such, all the orbits of the velocity are closed. Different from Lortz,\nour construction has a discrete m-fold symmetry and is foliated by invariant\ncylindrical level sets of a non-degenerate Bernoulli pressure. Such examples\nnarrow the scope of validity of Grad's conjecture that the only solutions with\na continuous symmetry can be fibered by pressure surfaces.", "AI": {"tldr": "Construction of 3D steady Euler flows with no continuous symmetry but discrete m-fold symmetry, contradicting Grad's conjecture about pressure surface fibering.", "motivation": "To challenge Grad's conjecture that only solutions with continuous symmetry can be fibered by pressure surfaces, by constructing counterexamples.", "method": "Following Lortz's approach but with discrete m-fold symmetry, constructing smooth steady states of ideal incompressible Euler equations in 3D with planar reflection symmetry.", "result": "Successfully created a family of steady states with no continuous Euclidean symmetry but possessing discrete m-fold symmetry and invariant cylindrical level sets of non-degenerate Bernoulli pressure.", "conclusion": "These examples narrow the validity scope of Grad's conjecture, showing that pressure surface fibering can occur without continuous symmetry."}}
{"id": "2510.04197", "pdf": "https://arxiv.org/pdf/2510.04197", "abs": "https://arxiv.org/abs/2510.04197", "authors": ["R. M. Str\u00e4ssle", "S. A. Hosseini", "I. V. Karlin"], "title": "Consistent kinetic modeling of compressible flows with variable Prandtl numbers: Double-distribution quasi-equilibrium approach", "categories": ["physics.flu-dyn", "math-ph", "math.MP", "nlin.CG", "physics.comp-ph"], "comment": null, "summary": "A consistent kinetic modeling and discretization strategy for compressible\nflows across all Prandtl numbers and specific heat ratios is developed using\nthe quasi-equilibrium approach within two of the most widely used\ndouble-distribution frameworks. The methodology ensures accurate recovery of\nthe Navier-Stokes-Fourier equations, including all macroscopic moments and\ndissipation rates, through detailed hydrodynamic limit analysis and careful\nconstruction of equilibrium and quasi-equilibrium attractors. Discretization is\nperformed using high-order velocity lattices with a static reference frame in a\ndiscrete velocity Boltzmann context to isolate key modeling aspects such as the\nnecessary requirements on expansion and quadrature orders. The proposed models\ndemonstrate high accuracy, numerical stability and Galilean invariance across a\nwide range of Mach numbers and temperature ratios. Separate tests for strict\nconservation and measurements of all dissipation rates confirm these insights\nfor all Prandtl numbers and specific heat ratios. Simulations on a sensitive\ntwo-dimensional shock-vortex interaction excellently reproduce viscous\nNavier-Stokes-Fourier-level physics. The proposed models establish an accurate,\nefficient and scalable framework for kinetic simulations of compressible flows\nwith moderate supersonic speeds and discontinuities at arbitrary Prandtl\nnumbers and specific heat ratios, offering a valuable tool for studying complex\nproblems in fluid dynamics and paving the way for future extensions to the\nlattice Boltzmann context, by application of correction terms, as well as\nhigh-Mach and hypersonic regimes, employing target-designed reference frames.", "AI": {"tldr": "Developed a consistent kinetic modeling and discretization strategy for compressible flows across all Prandtl numbers and specific heat ratios using quasi-equilibrium approach in double-distribution frameworks.", "motivation": "To create an accurate and efficient framework for kinetic simulations of compressible flows that works across all Prandtl numbers and specific heat ratios, enabling study of complex fluid dynamics problems.", "method": "Used quasi-equilibrium approach within double-distribution frameworks with detailed hydrodynamic limit analysis, careful construction of equilibrium and quasi-equilibrium attractors, and high-order velocity lattices with static reference frame in discrete velocity Boltzmann context.", "result": "Models demonstrate high accuracy, numerical stability, and Galilean invariance across wide Mach numbers and temperature ratios. Simulations successfully reproduce viscous Navier-Stokes-Fourier-level physics in sensitive shock-vortex interactions.", "conclusion": "The proposed models establish an accurate, efficient and scalable framework for kinetic simulations of compressible flows with moderate supersonic speeds and discontinuities, offering a valuable tool for fluid dynamics and paving way for future extensions."}}
{"id": "2510.03849", "pdf": "https://arxiv.org/pdf/2510.03849", "abs": "https://arxiv.org/abs/2510.03849", "authors": ["Jihoon Ok", "Giovanni Scilla", "Bianca Stroffolini"], "title": "Partial regularity for parabolic systems of double phase type", "categories": ["math.AP", "35D30, 35K55, 35K65"], "comment": "25 pages", "summary": "We study partial regularity for nondegenerate parabolic systems of double\nphase type, where the growth function is given by $H(z,s)=s^p+a(z)s^q$,\n$z=(x,t)\\in\\Omega_T$, with $\\tfrac{2n}{n+2}<p\\le q$ and $a(z)$ a nonnegative\n$C^{0,\\alpha,\\frac{\\alpha}{2}}$-continuous function for some $\\alpha\\in(0,1]$.\nAs the main result we prove that if $q< \\min \\{p+\\tfrac{\\alpha p }{n+2}, p+1\n\\}$ the spatial gradient of any weak solution is locally H\\\"older continuous,\nexcept on a set of measure zero.", "AI": {"tldr": "The paper proves partial regularity for parabolic systems of double phase type, showing that under certain conditions on the exponents p and q, the spatial gradient of weak solutions is locally H\u00f6lder continuous except on a measure zero set.", "motivation": "To establish regularity results for parabolic systems with double phase growth conditions, extending previous work on elliptic systems to the parabolic case and determining optimal conditions for H\u00f6lder continuity of gradients.", "method": "The authors study weak solutions to nondegenerate parabolic systems with growth function H(z,s)=s^p+a(z)s^q, where a(z) is a nonnegative H\u00f6lder continuous function. They use techniques from partial regularity theory for parabolic systems.", "result": "The main result shows that if q < min{p + \u03b1p/(n+2), p+1}, then the spatial gradient of any weak solution is locally H\u00f6lder continuous except on a set of measure zero.", "conclusion": "The paper provides optimal conditions for partial regularity in parabolic systems of double phase type, establishing H\u00f6lder continuity of gradients under the specified range of exponents p and q."}}
{"id": "2510.04170", "pdf": "https://arxiv.org/pdf/2510.04170", "abs": "https://arxiv.org/abs/2510.04170", "authors": ["Longze Tan"], "title": "Robust and efficient solvers for nonlinear partial differential equations based on random feature method", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "The random feature method (RFM), a mesh-free machine learning-based\nframework, has emerged as a promising alternative for solving PDEs on complex\ndomains. However, for large three-dimensional nonlinear problems, attaining\nhigh accuracy typically requires domain partitioning with many collocation\npoints and random features per subdomain, which leads to extremely large and\nill-conditioned nonlinear least-squares systems. To overcome these challenges,\nwe propose two randomized Newton-type solvers. The first is an inexact Newton\nmethod with right preconditioning (IPN), in which randomized Jacobian\ncompression and QR factorization are used to construct an efficient\npreconditioner that substantially reduces the condition number. Each Newton\nstep is then approximately solved by LSQR, and a derivative-free line search is\nincorporated to ensure residual reduction and stable convergence. Building upon\nthis framework, we further develop an adaptive multi-step inexact\npreconditioned Newton method (AMIPN). In this approach, the preconditioned\nJacobian is reused across multiple inner iterations, while a prescribed maximum\nnumber of inner iterations together with an adaptive early-stopping criterion\ndetermines whether the current preconditioner can be retained in subsequent\nouter iterations. These mechanisms effectively avoid redundant computations and\nenhance robustness. Extensive numerical experiments on both three-dimensional\nsteady-state and two-dimensional time-dependent PDEs with complex geometries\nconfirm the remarkable effectiveness of the proposed solvers. Compared with\nclassical discretization techniques and recent machine-learning-based\napproaches, the methods consistently deliver substantial accuracy improvements\nand robust convergence, thereby establishing the RFM combined with IPN/AMIPN as\nan efficient framework for large-scale nonlinear PDEs. .", "AI": {"tldr": "Proposes two randomized Newton-type solvers (IPN and AMIPN) to efficiently solve large-scale nonlinear PDEs using the random feature method, overcoming computational challenges through preconditioning and adaptive strategies.", "motivation": "The random feature method faces challenges with large 3D nonlinear PDEs, requiring many collocation points and features that lead to extremely large and ill-conditioned systems.", "method": "IPN uses randomized Jacobian compression and QR factorization for preconditioning, solved by LSQR with line search. AMIPN reuses preconditioned Jacobians across iterations with adaptive stopping criteria.", "result": "Extensive experiments on 3D steady-state and 2D time-dependent PDEs show substantial accuracy improvements and robust convergence compared to classical and recent ML-based approaches.", "conclusion": "RFM combined with IPN/AMIPN establishes an efficient framework for large-scale nonlinear PDEs with complex geometries."}}
{"id": "2510.04294", "pdf": "https://arxiv.org/pdf/2510.04294", "abs": "https://arxiv.org/abs/2510.04294", "authors": ["Gwonhak Lee", "Minhyeok Kang", "Jungsoo Hong", "Stepan Fomichev", "Joonsuk Huh"], "title": "Filtered Quantum Phase Estimation", "categories": ["quant-ph", "physics.comp-ph"], "comment": "42 pages, 13 figures", "summary": "Accurate state preparation is a critical bottleneck in many quantum\nalgorithms, particularly those for ground state energy estimation. Even in\nfault-tolerant quantum computing, preparing a quantum state with sufficient\noverlap to the desired eigenstate remains a major challenge. To address this,\nwe develop a unified framework for filtered-state preparation that enhances the\noverlap of a given input state through spectral filtering. This framework\nencompasses the polynomial and trigonometric realizations of filters, allowing\na transparent analysis of the trade-offs between overlap amplification and\npreparation cost. As examples, we introduce signal-processing-inspired filters,\nsuch as Gaussian filters and Krylov subspace-based filters, that adaptively\nsuppress excited-state contributions using low-rank projections. Within this\nframework, we further develop a filtered variant of QPE (FQPE) that mitigates\nthe unfavorable dependence on the initial overlap present in standard QPE.\nNumerical experiments on Fermi-Hubbard models show that FQPE reduces the total\nruntime by more than two orders of magnitude in the high-precision regime, with\noverlap amplification exceeding a factor of one hundred.", "AI": {"tldr": "A unified framework for filtered-state preparation that enhances eigenstate overlap through spectral filtering, including polynomial and trigonometric filters, with applications to quantum phase estimation that significantly reduces runtime.", "motivation": "Accurate state preparation is a critical bottleneck in quantum algorithms, particularly for ground state energy estimation, where achieving sufficient overlap with desired eigenstates remains challenging even in fault-tolerant quantum computing.", "method": "Developed a unified framework for filtered-state preparation using spectral filtering (polynomial and trigonometric realizations), introduced signal-processing-inspired filters like Gaussian filters and Krylov subspace-based filters, and created a filtered variant of QPE (FQPE) that mitigates dependence on initial overlap.", "result": "Numerical experiments on Fermi-Hubbard models show FQPE reduces total runtime by more than two orders of magnitude in high-precision regime, with overlap amplification exceeding a factor of one hundred.", "conclusion": "The filtered-state preparation framework provides an effective approach to overcome state preparation bottlenecks in quantum algorithms, particularly through FQPE which dramatically improves efficiency for ground state energy estimation."}}
{"id": "2510.03936", "pdf": "https://arxiv.org/pdf/2510.03936", "abs": "https://arxiv.org/abs/2510.03936", "authors": ["Pierre Marie Ngougoue Ngougoue"], "title": "Local Well-Posedness For Barotropic Compressible Fluid-Viscoelastic Shell Interactions", "categories": ["math.AP", "35B65, 35Q74, 35R37, 76N10, 74F10, 74K25"], "comment": null, "summary": "We study a three-dimensional barotropic compressible Navier-Stokes flow\ninteracting with a viscoelastic shell that occupies a portion of the fluid\nboundary. The analysis is entirely Eulerian and the moving interface is\nparametrised by a localised Hanzawa transform supported near the shell patch,\nwhich preserves the transport structure of the continuity equation and avoids a\nglobal Lagrangian map. We prove local-in-time existence and uniqueness of\nstrong solutions for compatible data and without imposing a vanishing initial\nshell displacement. The proof combines a well-posedness theory for the\ncontinuity equation, solved by the method of characteristics in the Hanzawa\nframe, with an analysis of the momentum-structure subproblem carried out by the\nclassical linearisation-energy estimate -- fixed-point scheme on a fixed\nreference domain. A Banach fixed point then couples the two steps and closes\nthe argument on a short time interval. We work with a viscosity-weighted energy\nthat makes the scaling in the shear and bulk viscosities explicit. This yields\nbounds whose constants grow at most linearly in these parameters. In\nparticular, the estimates are inviscid-limit compatible. The result complements\nthe global finite-energy weak theory for compressible fluid-shell interaction\nby providing a local strong well-posedness statement in the same geometric\nconfiguration. It also extends strong boundary results beyond beams and plates\nto bending shells, and addresses the wave-to-bending direction suggested in the\nliterature on compressible fluid-structure interaction at structural\nboundaries, while remaining fully Eulerian.", "AI": {"tldr": "Local strong well-posedness for 3D compressible Navier-Stokes flow interacting with viscoelastic shell using Eulerian approach with Hanzawa transform, without vanishing initial displacement.", "motivation": "To establish local strong well-posedness for compressible fluid-shell interaction, extending beyond beams and plates to bending shells, addressing wave-to-bending direction in fluid-structure interaction literature.", "method": "Eulerian analysis using localized Hanzawa transform near shell patch, combining well-posedness theory for continuity equation with momentum-structure subproblem analysis via linearisation-energy estimate and fixed-point scheme on fixed reference domain.", "result": "Proved local-in-time existence and uniqueness of strong solutions for compatible data, with viscosity-weighted energy yielding bounds that grow linearly in viscosity parameters and are inviscid-limit compatible.", "conclusion": "Provides local strong well-posedness complementing global weak theory, extends strong boundary results to bending shells, and addresses wave-to-bending direction while remaining fully Eulerian."}}
{"id": "2510.04427", "pdf": "https://arxiv.org/pdf/2510.04427", "abs": "https://arxiv.org/abs/2510.04427", "authors": ["Lisen Ding", "Mingyi Wang", "Dongling Wang"], "title": "A note on spectral Monte-Carlo method for fractional Poisson equation on high-dimensional ball", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "Recently, a class of efficient spectral Monte-Carlo methods was developed in\n\\cite{Feng2025ExponentiallyAS} for solving fractional Poisson equations. These\nmethods fully consider the low regularity of the solution near boundaries and\nleverage the efficiency of walk-on-spheres algorithms, achieving spectral\naccuracy. However, the underlying formulation is essentially one-dimensional.\nIn this work, we extend this approach to radial solutions in general\nhigh-dimensional balls. This is accomplished by employing a different set of\neigenfunctions for the fractional Laplacian and deriving new interpolation\nformulas. We provide a comprehensive description of our methodology and a\ndetailed comparison with existing techniques. Numerical experiments confirm the\nefficacy of the proposed extension.", "AI": {"tldr": "Extension of efficient spectral Monte-Carlo methods from 1D to high-dimensional balls for solving fractional Poisson equations with radial solutions, using new eigenfunctions and interpolation formulas.", "motivation": "Previous methods were limited to one-dimensional formulations, despite achieving spectral accuracy. The goal is to extend these efficient methods to handle radial solutions in general high-dimensional settings.", "method": "Employ different set of eigenfunctions for the fractional Laplacian and derive new interpolation formulas to extend the approach from 1D to high-dimensional balls.", "result": "Numerical experiments confirm the efficacy of the proposed extension, showing successful application to radial solutions in high-dimensional settings.", "conclusion": "The methodology successfully extends efficient spectral Monte-Carlo methods to high-dimensional balls for fractional Poisson equations with radial solutions, providing a comprehensive framework with validated numerical performance."}}
{"id": "2510.04305", "pdf": "https://arxiv.org/pdf/2510.04305", "abs": "https://arxiv.org/abs/2510.04305", "authors": ["Thomas Jacob", "Siddhant Mohapatra", "Rajalingam A", "Sam Mathew", "Pallab Sinha Mahapatra"], "title": "Mixing of a binary passive particle system using smart active particles", "categories": ["cond-mat.soft", "physics.comp-ph"], "comment": "The Supplementary Information is available on request from the\n  authors", "summary": "Controlled activity of active entities interacting with a passive environment\ncan generate emergent system-level phenomena, positioning such systems as\npromising platforms for potential downstream applications in targeted drug\ndelivery, adaptive and reconfigurable materials, microfluidic transport and\nrelated fields. The present work aims to realise an optimal mixing of two\nsegregated species of passive particles by introducing a small fraction of\nactive particles (2% by composition) with adaptive and intelligent behaviour,\ndirected by a trained Artificial Neural Network-based agent. While conventional\nrun-and-tumble particles can induce mixing in the system, the smart active\nparticles demonstrate superior performance, achieving faster and more efficient\nmixing. Interestingly, an optimal mixing strategy doesn't involve a uniform\ndispersion of active particles in the domain, but rather limiting their motion\nto an eccentrically placed zone of activity, inducing a global rotational\nmotion of the passive particles about the system centre. A transition in the\ndirectionality of the passive particles' motion is observed along the radius\ntowards the centre, likening the active particles' motion to an ellipse-shaped\nvoid with a defined surface speed. Situated at the intersection of active\nmatter and machine learning, this work highlights the potential of integrating\nadaptive learning frameworks into traditional active matter models.", "AI": {"tldr": "Smart active particles guided by AI achieve superior mixing of passive particles through eccentric rotational motion, outperforming conventional methods.", "motivation": "To develop intelligent active matter systems for applications like drug delivery and microfluidic transport by optimizing mixing of passive particles using adaptive AI agents.", "method": "Used 2% active particles controlled by a trained Artificial Neural Network agent, creating eccentric zones of activity that induce global rotational motion in passive particles.", "result": "AI-guided active particles achieved faster and more efficient mixing than conventional run-and-tumble particles, with optimal strategy involving eccentric motion patterns creating rotational flow.", "conclusion": "Integrating machine learning with active matter enables superior control and emergent behaviors, demonstrating the potential of adaptive learning frameworks in active matter systems."}}
{"id": "2510.03961", "pdf": "https://arxiv.org/pdf/2510.03961", "abs": "https://arxiv.org/abs/2510.03961", "authors": ["Soobin Cho", "Renming Song"], "title": "Abnormal boundary decay for the fractional Laplacian", "categories": ["math.AP", "math.PR", "60J45, 60J50, 35K08, 47G20"], "comment": "31 pages", "summary": "In this paper, we show that, for $\\alpha \\in (0,2)$, the $C^{1, \\rm Dini}$\nregularity assumption on an open set $D\\subset \\mathbb R^d$ is optimal for the\nstandard boundary decay property for nonnegative $\\alpha$-harmonic functions in\n$D$ and for the standard boundary decay property of the heat kernel\n$p^D(t,x,y)$ of the Dirichlet fractional Laplacian $\\Delta^{\\alpha/2}|_D$ by\nproving the following: (i) If $D$ is a $C^{1, \\rm Dini}$ open set and $h$ is a\nnon-negative function which is $\\alpha$-harmonic in $D$ and vanishes near a\nportion of $\\partial D$, then the rate at which $h(x)$ decays to 0 near that\nportion of $\\partial D$ is ${\\rm dist} (x, D^c)^{\\alpha/2}$. (ii) If $D$ is a\n$C^{1, \\rm Dini}$ open set, then, as $x\\to \\partial D$, the rate at which\n$p^D(t,x,y)$ tends to 0 is ${\\rm dist} (x, D^c)^{\\alpha/2}$. (iii) For any\nnon-Dini modulus of continuity $\\ell$, there exist non-$C^{1, \\rm Dini}$ open\nsets $D$, with $\\partial D$ locally being the graph of a $C^{1, \\ell}$\nfunction, such that the standard boundary decay properties above do not hold\nfor $D$.", "AI": {"tldr": "The paper proves that C\u00b9, Dini regularity is the optimal boundary condition for standard boundary decay properties of \u03b1-harmonic functions and fractional heat kernels.", "motivation": "To determine the minimal boundary regularity required for standard boundary decay properties in fractional Laplacian problems, specifically identifying whether C\u00b9, Dini regularity is necessary and sufficient.", "method": "The authors prove three main results: (i) boundary decay rate for \u03b1-harmonic functions in C\u00b9, Dini domains, (ii) boundary decay rate for fractional heat kernels in C\u00b9, Dini domains, and (iii) counterexamples using non-Dini modulus of continuity functions to show the necessity of C\u00b9, Dini condition.", "result": "C\u00b9, Dini regularity is both sufficient and necessary for the standard boundary decay properties - functions decay at rate dist(x, D^c)^{\u03b1/2} near the boundary. Without C\u00b9, Dini regularity, these decay properties fail.", "conclusion": "The C\u00b9, Dini boundary regularity assumption is optimal for boundary decay properties of \u03b1-harmonic functions and fractional heat kernels, establishing a sharp threshold for these analytical properties."}}
{"id": "2510.04458", "pdf": "https://arxiv.org/pdf/2510.04458", "abs": "https://arxiv.org/abs/2510.04458", "authors": ["Alexander Kushkuley"], "title": "Some Remarks on Commuting Probability", "categories": ["math.NA", "cs.NA", "math.RT", "65-XX (Primary) 20Cxx, 20Pxx (Secondary)"], "comment": null, "summary": "We introduce a weighted sum of irreducible character ratios as an estimator\nfor commutator probabilities. The estimator yields Frobenius formula when\napplied to a regular representation", "AI": {"tldr": "Introduces a weighted sum of irreducible character ratios as an estimator for commutator probabilities, which yields Frobenius formula when applied to regular representation.", "motivation": "To develop an estimator for commutator probabilities using character theory methods.", "method": "Uses a weighted sum of irreducible character ratios as the estimator approach.", "result": "The estimator produces the Frobenius formula when applied to a regular representation.", "conclusion": "The proposed weighted sum of irreducible character ratios serves as an effective estimator for commutator probabilities."}}
{"id": "2510.04447", "pdf": "https://arxiv.org/pdf/2510.04447", "abs": "https://arxiv.org/abs/2510.04447", "authors": ["Lucas Happ"], "title": "FewBodyToolkit.jl: a Julia package for solving quantum few-body problems", "categories": ["quant-ph", "physics.comp-ph"], "comment": null, "summary": "Few-body physics explores quantum systems of a small number of particles,\nbridging the gap between single-particle and many-body regimes. To provide an\naccessible tool for such studies, we present FewBodyToolkit.jl, a Julia package\nfor quantum few-body simulations. The package supports general two- and\nthree-body systems in various spatial dimensions with arbitrary\npair-interactions, and allows to calculate bound and resonant states. The\nimplementation is based on the well-established Gaussian expansion method and\nwe illustrate the package's capabilities through benchmarks and research\nexamples. The package comes with documentation and examples, making it useful\nfor research, teaching, benchmarking, and method development.", "AI": {"tldr": "FewBodyToolkit.jl is a Julia package for quantum few-body simulations using Gaussian expansion method, supporting 2-3 body systems with arbitrary interactions in various dimensions.", "motivation": "To provide an accessible computational tool for studying quantum few-body systems that bridge single-particle and many-body physics regimes.", "method": "Implementation based on the Gaussian expansion method, supporting general two- and three-body systems with arbitrary pair-interactions in various spatial dimensions.", "result": "The package successfully calculates bound and resonant states, demonstrated through benchmarks and research examples with comprehensive documentation.", "conclusion": "FewBodyToolkit.jl is a useful tool for research, teaching, benchmarking, and method development in quantum few-body physics."}}
{"id": "2510.03991", "pdf": "https://arxiv.org/pdf/2510.03991", "abs": "https://arxiv.org/abs/2510.03991", "authors": ["Ping Zhang", "Yibin Zhang"], "title": "Long time evolution of a pair of 2D viscous point vortices", "categories": ["math.AP", "35Q30 76D05 76D17"], "comment": null, "summary": "This paper studies the long-time evolution of two point vortices under the 2D\nNavier-Stokes tokes equations. Starting from initial data given by a pair of\nDirac measures, we derive an asymptotic expansion for the vorticity over time\nscales significantly longer than the advection time, yet shorter than the\ndiffusion time. Building on previous works \\cite{GS24-1, DG24}, we construct\nsuitable approximate solutions $\\Omega_a$ and employ Arnold's method to define\na nonlinear energy functional $E_\\ve[\\om]$, with respect to which the\nlinearized operator $\\Lambda^{E,\\star}$ around $\\Omega_a$ is nearly\nskew-adjoint. A key innovation in this work is the introduction of\n``pseudo-momenta'': $\\varrho^e_a, \\varrho^o_a,\\varrho^{te}_a, \\varrho^{to}_a$,\nwhich correspond to eigenfunctions or other nontrivial elements in invariant\nsubspaces of $\\Lambda^E$, derived from the Lie structure of the 2D Euler\nequations.", "AI": {"tldr": "This paper analyzes the long-term evolution of two point vortices in 2D Navier-Stokes equations, deriving asymptotic expansions for vorticity over extended time scales and introducing pseudo-momenta as key innovations.", "motivation": "To understand the long-time behavior of two point vortices under 2D Navier-Stokes equations, particularly over time scales longer than advection but shorter than diffusion, building on previous research.", "method": "Constructs approximate solutions \u03a9_a, employs Arnold's method to define nonlinear energy functional E_\u03b5[\u03c9], and introduces pseudo-momenta (\u03f1^e_a, \u03f1^o_a, \u03f1^{te}_a, \u03f1^{to}_a) derived from the Lie structure of 2D Euler equations.", "result": "Develops asymptotic expansion for vorticity evolution and establishes that the linearized operator \u039b^{E,\u22c6} around approximate solutions is nearly skew-adjoint with respect to the energy functional.", "conclusion": "The introduction of pseudo-momenta provides a novel approach to analyzing vortex dynamics, leveraging the underlying Lie structure of 2D Euler equations for improved understanding of long-term vortex evolution."}}
{"id": "2510.04540", "pdf": "https://arxiv.org/pdf/2510.04540", "abs": "https://arxiv.org/abs/2510.04540", "authors": ["Lei Li", "Min Tang", "Yuqi Yang"], "title": "Convergence Analysis of the Random Ordinate Method for Mitigating the Ray Effect", "categories": ["math.NA", "cs.NA", "math.DS"], "comment": null, "summary": "The Discrete Ordinates Method (DOM) is widely used for velocity\ndiscretization in radiative transport simulations. However, DOM tends to\nexhibit the ray effect when the velocity discretization is not sufficiently\nrefined, a limitation that is well documented. To counter this, we have\ndeveloped the Random Ordinates Method (ROM) by integrating randomness into the\nvelocity discretization, which mitigates the ray effect without incurring\nadditional computational costs. ROM partitions the velocity space into n cells,\nselects a random ordinate from each cell, and solves a DOM system with these\nordinates. It leverages the average of multiple samples to achieve a higher\nconvergence order, especially for solutions with low regularity in the velocity\nvariable. In this work, we provide a detailed convergence analysis for ROM,\nfocusing on bias and single-run errors. This analysis is crucial for\ndetermining the necessary mesh size and the optimal number of samples required\nto attain a specified level of accuracy.", "AI": {"tldr": "The paper introduces the Random Ordinates Method (ROM) to address the ray effect in Discrete Ordinates Method (DOM) for radiative transport simulations. ROM uses random velocity discretization and averaging of multiple samples to achieve better convergence.", "motivation": "DOM suffers from ray effects when velocity discretization is insufficiently refined. This limitation motivates the development of ROM to mitigate ray effects without increasing computational costs.", "method": "ROM partitions velocity space into n cells, selects random ordinates from each cell, and solves DOM systems with these ordinates. It uses averaging of multiple samples to achieve higher convergence order, particularly for solutions with low velocity regularity.", "result": "ROM provides detailed convergence analysis focusing on bias and single-run errors. This helps determine optimal mesh size and number of samples needed for desired accuracy.", "conclusion": "ROM effectively mitigates ray effects in radiative transport simulations while maintaining computational efficiency through random velocity discretization and sample averaging."}}
{"id": "2510.04562", "pdf": "https://arxiv.org/pdf/2510.04562", "abs": "https://arxiv.org/abs/2510.04562", "authors": ["Pascal Thibaudeau", "Mouad Fattouhi", "Liliana D. Buda-Prejbeanu"], "title": "Dynamic Landau-Lifshitz-Bloch-Slonczewski equations for spintronics", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.comp-ph"], "comment": "5 pages, 5 figures", "summary": "The atomistic Landau-Lifshitz-Gilbert equation is challenged when modeling\nspintronic devices where Joule heating is significant, due to its core\nassumption of a constant magnetization magnitude. Based on a statistical\nframework that treats the magnetization magnitude as a dynamic variable coupled\nto a thermal bath, we derive a dynamic Landau-Lifshitz-Bloch-Slonczewski set of\nequations for torques, that captures the transient, heating-induced\ndemagnetization that occurs during high-current operation. Integrating these\ndynamic equations and comparing them to their stochastic equivalents reveals\nthat both the energy landscape and switching dynamics in high-anisotropy\nsystems are similarly modified. This approach yields accurate and accelerated\npredictions of critical currents and switching times.", "AI": {"tldr": "A new dynamic Landau-Lifshitz-Bloch-Slonczewski equation set is developed to model magnetization dynamics under Joule heating, overcoming limitations of traditional constant-magnitude approaches.", "motivation": "The standard Landau-Lifshitz-Gilbert equation fails in spintronic devices with significant Joule heating because it assumes constant magnetization magnitude, which doesn't account for heating-induced demagnetization effects.", "method": "Developed a statistical framework treating magnetization magnitude as a dynamic variable coupled to thermal bath, deriving dynamic Landau-Lifshitz-Bloch-Slonczewski equations that capture transient heating-induced demagnetization.", "result": "The dynamic equations accurately model energy landscape and switching dynamics in high-anisotropy systems under high-current operation, providing improved predictions of critical currents and switching times.", "conclusion": "This approach enables accurate and accelerated modeling of spintronic device behavior under high-current conditions where heating-induced demagnetization is significant."}}
{"id": "2510.04065", "pdf": "https://arxiv.org/pdf/2510.04065", "abs": "https://arxiv.org/abs/2510.04065", "authors": ["Prashanta Garain"], "title": "Two alternative proofs of weak Harnack inequality for mixed local and nonlocal $p$-Laplace equations with a nonhomogeneity", "categories": ["math.AP", "35B45, 35B65, 35D30, 35J92, 35R11"], "comment": "28 pages, comments are welcome", "summary": "We study a class of mixed local and nonlocal $p$-Laplace equations with\nprototype \\[ -\\Delta_p u + (-\\Delta_p)^s u = f \\quad \\text{in } \\Omega, \\]\nwhere $\\Omega \\subset \\mathbb{R}^n$ is bounded and open. We provide sufficient\ncondition on $f$ to ensure weak Harnack inequality with a tail term for\nsign-changing supersolutions. Two different proofs are presented, avoiding the\nKrylov--Safonov covering lemma and expansion of positivity: one via the\nJohn--Nirenberg lemma, the other via the Bombieri--Giusti lemma. To our\nknowledge, these approaches are new, even for $p = 2$ with $f \\equiv 0$, and\ninclude a new proof of the reverse H\\\"older inequality for supersolutions.\nFurther, we establish Harnack inequality for solutions by first deriving a\nlocal boundedness result, together with a tail estimate and an initial weak\nHarnack inequality.", "AI": {"tldr": "The paper studies mixed local and nonlocal p-Laplace equations and provides sufficient conditions on f to prove weak Harnack inequality for sign-changing supersolutions, presenting two novel proofs that avoid traditional methods.", "motivation": "To establish Harnack inequalities for mixed local-nonlocal p-Laplace equations, which are important for understanding regularity properties of solutions to these partial differential equations.", "method": "Two different proof approaches: one using the John-Nirenberg lemma and another using the Bombieri-Giusti lemma, both avoiding the Krylov-Safonov covering lemma and expansion of positivity. Also develops local boundedness results and tail estimates.", "result": "Proves weak Harnack inequality with tail term for sign-changing supersolutions, establishes Harnack inequality for solutions, provides new proof of reverse H\u00f6lder inequality for supersolutions.", "conclusion": "The paper presents novel approaches to proving Harnack inequalities for mixed local-nonlocal p-Laplace equations, offering alternative methods that work even for the classical case p=2 with f=0."}}
{"id": "2510.04693", "pdf": "https://arxiv.org/pdf/2510.04693", "abs": "https://arxiv.org/abs/2510.04693", "authors": ["P. N. Vabishchevich"], "title": "Computational identification of the source domain in an inverse problem of potential theory", "categories": ["math.NA", "cs.NA"], "comment": "20 pages, 26 figures", "summary": "The inverse potential problem consists in determining the density of the\nvolume potential from measurements outside the sources. Its ill-posedness is\ndue both to the non-uniqueness of the solution and to the instability of the\nsolution with respect to measurement errors. The inverse problem is solved\nunder additional assumptions about the sources using regularizing algorithms.\nIn this work, an inverse problem is posed for identifying the domain that\ncontains the sources. The computational algorithm is based on approximating the\nvolume potential by the single-layer potential on the boundary of the domain\ncontaining the sources. The inverse problem is considered in the class of a\npriori constraints of nonnegativity of the potential density. Residual\nminimization in the class of nonnegative solutions is performed using the\nclassical Nonnegative Least Squares algorithm. The capabilities of the proposed\napproach are illustrated by numerical experiments for a two-dimensional test\nproblem with an analytically prescribed potential on the observation surface.", "AI": {"tldr": "The paper addresses the inverse potential problem by identifying the domain containing sources using regularizing algorithms with nonnegativity constraints, solved via Nonnegative Least Squares.", "motivation": "The inverse potential problem is ill-posed due to non-uniqueness and instability with respect to measurement errors, requiring additional assumptions and regularization to obtain stable solutions.", "method": "The computational algorithm approximates the volume potential by the single-layer potential on the boundary of the source domain, with residual minimization in the class of nonnegative solutions using Nonnegative Least Squares.", "result": "Numerical experiments for a two-dimensional test problem with an analytically prescribed potential demonstrate the capabilities of the proposed approach.", "conclusion": "The proposed method effectively solves the inverse potential problem under nonnegativity constraints, providing a stable and computationally feasible solution through regularized optimization."}}
{"id": "2510.04589", "pdf": "https://arxiv.org/pdf/2510.04589", "abs": "https://arxiv.org/abs/2510.04589", "authors": ["Sai Anandhi Seetharaman", "Soumyadipta Maiti", "Ambesh Gupta", "Beena Rai"], "title": "Investigating into mechanisms of high temperature strength of refractory high-entropy alloys", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "comment": "32 pages, 9 figures", "summary": "The yield strength plateau of two BCC refractory high entropy alloys (RHEAs)\n- MoNbTaVW and MoNbTaW was examined through hybrid Monte Carlo and molecular\ndynamics (MC/MD) simulations. By analyzing atomic diffusivities derived from\nvacancy formation and migration energies around the edge dislocation cores, the\nnumber of critical atomic swaps were calculated at different temperatures.\nUsing hybrid MC/MD simulations of these critical swaps, we demonstrate that\nabove 1400K, the stress required to move the dislocations gets saturated,\nindicating the effect of Dynamic Strain Ageing (DSA) via cross core motion.\nFurther simulations on random solid solutions (0 MC swaps) revealed a similar\nplateau effect at the intermediate temperatures. This was attributed to the\nadditional athermal stress arising from lattice distortions due to solid\nsolution strengthening. Our findings suggest that the yield strength plateau\nresults from an interplay between the DSA-driven diffusion process and athermal\nstress. Specifically, the plateau emerges from DSA mechanisms in the presence\nof atomic diffusion, whereas in the absence of diffusion, it is governed by\nathermal statistical lattice distortions. This dual mechanism framework\nprovides a comprehensive explanation for the experimentally observed Yield\nstrength behavior in RHEAs at intermediate temperatures.", "AI": {"tldr": "The yield strength plateau in BCC refractory high entropy alloys is caused by dual mechanisms: Dynamic Strain Ageing (DSA) via atomic diffusion at high temperatures, and athermal stress from lattice distortions at intermediate temperatures.", "motivation": "To understand the underlying mechanisms behind the yield strength plateau observed in BCC refractory high entropy alloys (RHEAs) at intermediate temperatures, which has been experimentally observed but not fully explained.", "method": "Used hybrid Monte Carlo and molecular dynamics (MC/MD) simulations to analyze atomic diffusivities, vacancy formation/migration energies, and critical atomic swaps around dislocation cores at different temperatures.", "result": "Above 1400K, dislocation motion stress saturates due to DSA via cross core motion. At intermediate temperatures, similar plateau occurs from athermal stress due to lattice distortions from solid solution strengthening.", "conclusion": "The yield strength plateau in RHEAs results from an interplay between DSA-driven diffusion processes and athermal stress from lattice distortions, providing a comprehensive dual-mechanism explanation for experimental observations."}}
{"id": "2510.04082", "pdf": "https://arxiv.org/pdf/2510.04082", "abs": "https://arxiv.org/abs/2510.04082", "authors": ["Huanqing Guo", "Junyong Zhang", "Jiqiang Zheng"], "title": "Negative Order Bochner-Riesz Operators for the Critical Magnetic Schr\u00f6dinger Operator in $\\mathbb{R}^2$", "categories": ["math.AP"], "comment": "14 pages", "summary": "This paper studies the sharp $L^p$-$L^q$ boundedness of the Bochner-Riesz\noperator $S^{\\delta}_{\\lambda}(\\mathcal{L}_{\\mathbf{A}})$ associated with a\nscaling-critical magnetic Schr\\\"odinger operator $\\mathcal{L}_{\\mathbf{A}}$ on\n$\\mathbb{R}^2$, where $\\delta \\in (-3/2, 0)$. We determine the conditions on\nthe exponents $p$ and $q$ under which the operator is bounded from\n$L^p(\\mathbb{R}^2)$ to $L^q(\\mathbb{R}^2)$. Our main result characterizes the\nboundedness region as a pentagonal subset $\\Delta(\\delta)$ of the $(1/p,\n1/q)$-plane, extending previous uniform resolvent result in Fanelli, Zhang and\nZheng[Int. Math. Res. Not., 20(2023), 17656-17703].", "AI": {"tldr": "This paper characterizes the sharp L^p-L^q boundedness of the Bochner-Riesz operator associated with a magnetic Schr\u00f6dinger operator on R^2, identifying the boundedness region as a pentagonal subset in the (1/p, 1/q)-plane.", "motivation": "To extend previous uniform resolvent results and determine precise conditions for the boundedness of Bochner-Riesz operators associated with magnetic Schr\u00f6dinger operators.", "method": "Studying the sharp L^p-L^q boundedness of the Bochner-Riesz operator S^\u03b4_\u03bb(L_A) for \u03b4 \u2208 (-3/2, 0) associated with a scaling-critical magnetic Schr\u00f6dinger operator on R^2.", "result": "The boundedness region is characterized as a pentagonal subset \u0394(\u03b4) of the (1/p, 1/q)-plane, extending previous uniform resolvent results.", "conclusion": "The paper successfully determines the sharp conditions on exponents p and q for the boundedness of the Bochner-Riesz operator associated with magnetic Schr\u00f6dinger operators on R^2."}}
{"id": "2510.04751", "pdf": "https://arxiv.org/pdf/2510.04751", "abs": "https://arxiv.org/abs/2510.04751", "authors": ["Xinyi Wei", "Julian Braun", "Yangshuai Wang", "Lei Zhang"], "title": "Higher-Order Boundary Conditions for Atomistic Dislocation Simulations", "categories": ["math.NA", "cs.NA", "physics.comp-ph"], "comment": null, "summary": "We present a higher-order boundary condition for atomistic simulations of\ndislocations that address the slow convergence of standard supercell methods.\nThe method is based on a multipole expansion of the equilibrium displacement,\ncombining continuum predictor solutions with discrete moment corrections. The\ncontinuum predictors are computed by solving a hierarchy of singular elliptic\nPDEs via a Galerkin spectral method, while moment coefficients are evaluated\nfrom force-moment identities with controlled approximation error. A key feature\nis the coupling between accurate continuum predictors and moment evaluations,\nenabling the construction of systematically improvable high-order boundary\nconditions. We thus design novel algorithms, and numerical results for screw\nand edge dislocations confirm the predicted convergence rates in geometry and\nenergy norms, with reduced finite-size effects and moderate computational cost.", "AI": {"tldr": "A higher-order boundary condition method for atomistic dislocation simulations that improves convergence by combining continuum predictors with discrete moment corrections.", "motivation": "To address the slow convergence of standard supercell methods in atomistic simulations of dislocations.", "method": "Uses multipole expansion of equilibrium displacement, combining continuum predictor solutions (solved via Galerkin spectral method for singular elliptic PDEs) with discrete moment corrections (evaluated from force-moment identities).", "result": "Numerical results for screw and edge dislocations confirm predicted convergence rates in geometry and energy norms, with reduced finite-size effects and moderate computational cost.", "conclusion": "The method enables systematically improvable high-order boundary conditions through coupling accurate continuum predictors with moment evaluations."}}
{"id": "2510.04152", "pdf": "https://arxiv.org/pdf/2510.04152", "abs": "https://arxiv.org/abs/2510.04152", "authors": ["Tomasz Cie\u015blak", "Sebastian Owczarek", "Karolina Wielgos"], "title": "Entropy-energy solutions for Thermo-Visco-Elastic systems with Mr\u00f3z-type inelastic behavior", "categories": ["math.AP"], "comment": null, "summary": "In this article, we study a thermodynamically consistent thermo-visco-elastic\nmodel describing the balance of internal energy in a heat-conducting inelastic\nbody. In the considered problem, the temperature dependence appears in both the\nelastic and inelastic constitutive relations. For such a system, we introduce\nthe concept of a weak entropy-energy solution, which satisfies the entropy\nequality instead of the internal energy equation. Although the model does not\npossess any mathematically favorable structural properties, such as\nKelvin-Voigt type effects or simplifications eliminating temperature from the\nconstitutive relations, we prove the global-in-time existence of weak\nentropy-energy solutions for large initial data.", "AI": {"tldr": "Global existence of weak entropy-energy solutions for a thermodynamically consistent thermo-visco-elastic model with temperature-dependent elastic and inelastic constitutive relations.", "motivation": "To study a thermodynamically consistent thermo-visco-elastic model that describes internal energy balance in heat-conducting inelastic bodies, where temperature dependence appears in both elastic and inelastic constitutive relations.", "method": "Introduce the concept of weak entropy-energy solutions that satisfy the entropy equality instead of the internal energy equation, and prove existence results despite the model lacking mathematically favorable structural properties.", "result": "Prove the global-in-time existence of weak entropy-energy solutions for large initial data, even without Kelvin-Voigt type effects or simplifications eliminating temperature from constitutive relations.", "conclusion": "The proposed weak entropy-energy solution concept enables rigorous mathematical analysis of thermodynamically consistent thermo-visco-elastic models with temperature-dependent constitutive relations, establishing global existence results for challenging physical systems."}}
{"id": "2510.04825", "pdf": "https://arxiv.org/pdf/2510.04825", "abs": "https://arxiv.org/abs/2510.04825", "authors": ["Eleanor Jones", "Yuji Nakatsukasa"], "title": "SubApSnap: Solving parameter-dependent linear systems with a snapshot and subsampling", "categories": ["math.NA", "cs.NA", "65F99, 65F20"], "comment": null, "summary": "A growing number of problems in computational mathematics can be reduced to\nthe solution of many linear systems that are related, often depending smoothly\nor slowly on a parameter $p$, that is, $A(p)x(p)=b(p)$. We introduce an\nefficient algorithm for solving such parameter-dependent linear systems for\nmany values of $p$. The algorithm, which we call SubApSnap (for\n\\emph{Sub}sampled $A(p)$ times \\emph{Snap}shot), is based on combining ideas\nfrom model order reduction and randomised linear algebra: namely, taking a\nsnapshot matrix, and solving the resulting tall-skinny least-squares problems\nusing a subsampling-based dimension-reduction approach. We show that SubApSnap\nis a strict generalisation of the popular DEIM algorithm in nonlinear model\norder reduction. SubApSnap is a sublinear-time algorithm, and once the snapshot\nand subsampling are determined, it solves $A(p_*)x(p_*)=b(p_*)$ for a new value\nof $p_*$ at a dramatically improved speed: it does not even need to read the\nwhole matrix $A(p_*)$ to solve the linear system for a new value of $p_*$. We\nprove under natural assumptions that, given a good subsampling and snapshot,\nSubApSnap yields solutions with small residual for all parameter values of\ninterest. We illustrate the efficiency and performance of the algorithm with\nproblems arising in PDEs, model reduction, and kernel ridge regression, where\nSubApSnap achieves speedups of many orders of magnitude over a standard\nsolution; for example over $20,000\\times$ for a $10^7\\times 10^7$ problem,\nwhile providing good accuracy.", "AI": {"tldr": "SubApSnap is a sublinear-time algorithm for solving parameter-dependent linear systems A(p)x(p)=b(p) by combining model order reduction and randomized linear algebra, achieving dramatic speedups of over 20,000\u00d7 for large problems.", "motivation": "Many computational mathematics problems involve solving many related linear systems that depend smoothly on parameters, which can be computationally expensive when done naively for each parameter value.", "method": "Combines model order reduction (snapshot matrix) with randomized linear algebra (subsampling-based dimension reduction) to solve tall-skinny least-squares problems efficiently without reading entire matrices.", "result": "SubApSnap achieves speedups of many orders of magnitude (e.g., over 20,000\u00d7 for 10^7\u00d710^7 problems) while maintaining good accuracy, and is a strict generalization of the DEIM algorithm.", "conclusion": "The algorithm provides an efficient solution for parameter-dependent linear systems, requiring only subsampled data to solve for new parameter values, making it particularly valuable for large-scale problems in PDEs, model reduction, and kernel ridge regression."}}
{"id": "2510.04795", "pdf": "https://arxiv.org/pdf/2510.04795", "abs": "https://arxiv.org/abs/2510.04795", "authors": ["Cole Brower", "Samuel Rodriguez Bernabeu", "Jeff Hammond", "John Gunnels", "Sotiris S. Xanthea", "Martin Ganahl", "Andor Menczer", "\u00d6rs Legeza"], "title": "Mixed-precision ab initio tensor network state methods adapted for NVIDIA Blackwell technology via emulated FP64 arithmetic", "categories": ["physics.chem-ph", "cond-mat.str-el", "physics.comp-ph"], "comment": "11 pages, 10 figures", "summary": "We report cutting-edge performance results via mixed-precision spin adapted\nab initio Density Matrix Renormalization Group (DMRG) electronic structure\ncalculations utilizing the Ozaki scheme for emulating FP64 arithmetic through\nthe use of fixed-point compute resources. By approximating the underlying\nmatrix and tensor algebra with operations on a modest number of fixed-point\nrepresentatives (``slices''), we demonstrate on smaller benchmark systems and\nfor the active compounds of the FeMoco and cytochrome P450 (CYP) enzymes with\ncomplete active space (CAS) sizes of up to 113 electrons in 76 orbitals\n[CAS(113, 76)] and 63 electrons in 58 orbitals [CAS(63, 58)], respectively,\nthat the chemical accuracy can be reached with mixed-precision arithmetic. We\nalso show that, due to its variational nature, DMRG provides an ideal tool to\nbenchmark accuracy domains, as well as the performance of new hardware\ndevelopments and related numerical libraries. Detailed numerical error analysis\nand performance assessment are also presented for subcomponents of the DMRG\nalgebra by systematically interpolating between double- and\npseudo-half-precision. Our analyis represents the first quantum chemistry\nevaluation of FP64 emulation for correlated calculations capable of achieving\nchemical accuracy and emulation based on fixed-point arithmetic, and it paves\nthe way for the utilization of state-of-the-art Blackwell technology in\ntree-like tensor network state electronic structure calculations, opening new\nresearch directions in materials sciences and beyond.", "AI": {"tldr": "Mixed-precision DMRG calculations using FP64 emulation via fixed-point arithmetic achieve chemical accuracy for large active spaces (up to CAS(113,76)) while providing a benchmark for new hardware.", "motivation": "To demonstrate that chemical accuracy can be achieved in large-scale electronic structure calculations using mixed-precision arithmetic with FP64 emulation, enabling the use of modern fixed-point hardware.", "method": "Mixed-precision spin adapted ab initio DMRG using Ozaki scheme for FP64 emulation through fixed-point compute resources, approximating matrix/tensor algebra with fixed-point \"slices\", with systematic interpolation between double- and pseudo-half-precision.", "result": "Chemical accuracy achieved for FeMoco (CAS(113,76)) and cytochrome P450 (CAS(63,58)) enzymes, with detailed numerical error analysis showing viability of mixed-precision approach.", "conclusion": "DMRG provides ideal benchmarking for new hardware and numerical libraries, and this work enables utilization of Blackwell technology in tensor network state calculations, opening new research directions."}}
{"id": "2510.04270", "pdf": "https://arxiv.org/pdf/2510.04270", "abs": "https://arxiv.org/abs/2510.04270", "authors": ["Iulia Cristian", "Juan J. L. Vel\u00e1zquez"], "title": "Mass concentration in a spatially inhomogeneous coagulation model with fast sedimentation", "categories": ["math.AP"], "comment": "83 pages, no figures, comments welcome", "summary": "We study a spatially inhomogeneous coagulation model that contains a\ntransport term in the spatial variable. The transport term models the vertical\nmotion of particles due to gravity, thereby incorporating their fall into the\ndynamics. Local existence of mass-conserving solutions for a class of\ncoagulation rates for which in the spatially homogeneous case instantaneous\ngelation (i.e., instantaneous loss of mass) occurs has been proved in\n[Cristian-Niethammer-Vel\\'azquez, 2024]. In order to obtain some insight into\nhow to prove global existence of solutions, we allow a fast sedimentation\nspeed. For very fast sedimentation speed, we rigorously prove that solutions\nconverge to a Dirac measure in the space variable. We also formally obtain in\nthe limit a one-dimensional coagulation equation with diagonal kernel, i.e.,\nonly particles of the same size interact. This provides a physical intuition on\nhow coagulation models with a diagonal kernel emerge.", "AI": {"tldr": "Study of a spatially inhomogeneous coagulation model with transport term modeling gravity-induced particle motion. Local existence proved for mass-conserving solutions, and global existence insights obtained by allowing fast sedimentation speed.", "motivation": "To understand how to prove global existence of solutions for coagulation models with transport terms, particularly in cases where instantaneous gelation occurs in the homogeneous case.", "method": "Analysis of a spatially inhomogeneous coagulation model with transport term. For very fast sedimentation speed, rigorous proof that solutions converge to a Dirac measure in space variable, and formal derivation of a one-dimensional coagulation equation with diagonal kernel.", "result": "For very fast sedimentation speed, solutions converge to a Dirac measure in the spatial variable. In the limit, a one-dimensional coagulation equation with diagonal kernel emerges, where only particles of the same size interact.", "conclusion": "The study provides physical intuition on how coagulation models with diagonal kernels emerge from spatially inhomogeneous models with fast sedimentation, offering insights for proving global existence of solutions."}}
{"id": "2510.04826", "pdf": "https://arxiv.org/pdf/2510.04826", "abs": "https://arxiv.org/abs/2510.04826", "authors": ["Rui Wang", "Yunfeng Xiong", "Zhengru Zhang"], "title": "Efficient structure-preserving scheme for chemotaxis PDEs with singular sensitivity in crime and epidemic modeling", "categories": ["math.NA", "cs.NA", "35K61, 65M06, 65M15, 82C26, 92C17"], "comment": null, "summary": "The system of chemotaxis PDEs with singular sensitivity was originally\nproposed by Short et al. [Math. Mod. Meth. Appl. Sci., 18:1249-1267, 2008] as\nthe continuum limit of a biased random walk model to account for the formation\nof crime hotspots and environmental feedback successfully. Recently, this idea\nhas also been applied to epidemiology to model the impact of human social\nbehaviors on disease transmission. In order to characterize the phase\ntransition, pattern formation and statistical properties in the long-term\ndynamics, a stable and accurate numerical scheme is urgently demanded, which\nstill remains challenging due to the positivity constraint on the singular\nsensitivity and the absence of an energy functional. To address these numerical\nchallenges, this paper constructs an efficient positivity-preserving,\nimplicit-explicit scheme with second-order accuracy. A rigorous error\nestimation is provided with the Lagrange multiplier correction to deal with the\nsingular sensitivity. The whole framework is extended to a multi-agent epidemic\nmodel with degenerate diffusion, in which both positivity and mass conservation\nare achieved. Typical numerical examples are conducted to validate our\ntheoretical results and to demonstrate the necessity of correction strategy.\nThe proposed scheme allows us to study the nucleation, spread, and dissipation\nof crime hotspots, as well as validate that clustering of population density\nmay accelerate virus transmission in epidemic dynamics and potentially\naggravate the second infectious wave.", "AI": {"tldr": "This paper develops a second-order accurate, positivity-preserving implicit-explicit numerical scheme for chemotaxis PDEs with singular sensitivity, originally used for crime hotspot modeling and extended to epidemic dynamics.", "motivation": "There is an urgent need for stable and accurate numerical schemes to characterize phase transitions, pattern formation, and statistical properties in long-term dynamics of chemotaxis PDEs with singular sensitivity, which face challenges due to positivity constraints and absence of energy functionals.", "method": "Constructed an efficient positivity-preserving implicit-explicit scheme with second-order accuracy, using Lagrange multiplier correction to handle singular sensitivity. Extended the framework to multi-agent epidemic models with degenerate diffusion while maintaining positivity and mass conservation.", "result": "The proposed scheme successfully handles singular sensitivity through rigorous error estimation with Lagrange multiplier correction. Numerical examples validate theoretical results and demonstrate the necessity of the correction strategy for studying crime hotspots and epidemic dynamics.", "conclusion": "The developed scheme enables study of crime hotspot nucleation, spread, and dissipation, and validates that population density clustering accelerates virus transmission in epidemics, potentially aggravating second infectious waves."}}
{"id": "2510.05042", "pdf": "https://arxiv.org/pdf/2510.05042", "abs": "https://arxiv.org/abs/2510.05042", "authors": ["Jaehyeok Jin", "Chen Liu", "David R. Reichman"], "title": "Field-Theoretic Simulation of Dean-Kawasaki Dynamics for Interacting Particles", "categories": ["cond-mat.stat-mech", "cond-mat.soft", "physics.chem-ph", "physics.comp-ph"], "comment": "21 pages, 10 figures (Supplemental Material: 17 pages, 7 figures, 2\n  tables)", "summary": "The formulation of a fluctuating hydrodynamic theory for interacting\nparticles is a crucial step in the theoretical description of liquids. The\nmicroscopic mappings proposed decades ago by Dean and Kawasaki have played a\ncentral role in the analytical treatment of such problems. However, the\nsingular mathematical nature of the density distributions used in these\nderivations raises concerns about the validity and practical utility of the\nresulting stochastic partial differential equations, particularly for direct\nnumerical simulations. Recent efforts have centered on establishing a rigorous\ncoarse-graining procedure to regularize the effective Dean-Kawasaki equation.\nBuilding on this foundation, we numerically investigate weakly interacting\nfluids within such a regularized framework for the first time. Our work\nreveals, at the level of structural correlations, the effects of regularization\non the Dean-Kawasaki formalism and paves the way for improved numerical\napproaches to simulate fluctuating hydrodynamics in liquids.", "AI": {"tldr": "First numerical investigation of weakly interacting fluids using regularized Dean-Kawasaki equation framework, examining regularization effects on structural correlations.", "motivation": "To address concerns about validity and utility of stochastic partial differential equations from microscopic Dean-Kawasaki mappings due to singular nature of density distributions, and establish rigorous coarse-graining procedure.", "method": "Numerical investigation of weakly interacting fluids within regularized Dean-Kawasaki equation framework, building on recent rigorous coarse-graining procedures.", "result": "Reveals effects of regularization on Dean-Kawasaki formalism at the level of structural correlations.", "conclusion": "Paves the way for improved numerical approaches to simulate fluctuating hydrodynamics in liquids."}}
{"id": "2510.04433", "pdf": "https://arxiv.org/pdf/2510.04433", "abs": "https://arxiv.org/abs/2510.04433", "authors": ["Maria Filipkovska"], "title": "Existence and qualitative behavior of solutions of abstract differential-algebraic equations", "categories": ["math.AP"], "comment": null, "summary": "Abstract differential-algebraic equations (ADAEs) of a semilinear type are\nstudied. Theorems on the existence and uniqueness of solutions and the maximal\ninterval of existence, on the global solvability of the ADAEs, the boundedness\nof solutions and the blow-up of solutions are presented. Previously, an ADAE is\nreduced to a system of explicit differential equations and algebraic equations\nby using projectors. The number of equations of the system depends on the index\nof the characteristic pencil of the ADAE. We consider the pencil of an\narbitrarily high index.", "AI": {"tldr": "Study of semilinear abstract differential-algebraic equations (ADAEs) focusing on existence, uniqueness, and solution behavior including global solvability, boundedness, and blow-up.", "motivation": "To establish comprehensive theoretical foundations for semilinear ADAEs with arbitrarily high index characteristic pencils, extending previous work on reduced systems.", "method": "Reduce ADAEs to explicit differential and algebraic equations using projectors, with system size determined by the index of the characteristic pencil.", "result": "Proved theorems on existence/uniqueness of solutions, maximal existence intervals, global solvability, boundedness, and blow-up behavior for high-index ADAEs.", "conclusion": "Established complete theoretical framework for semilinear ADAEs with arbitrary index, providing fundamental results on solution properties and behavior."}}
{"id": "2510.04904", "pdf": "https://arxiv.org/pdf/2510.04904", "abs": "https://arxiv.org/abs/2510.04904", "authors": ["Congpei An", "Jiashu Ran", "Hao-Ning Wu"], "title": "The path of hyperinterpolation: A survey", "categories": ["math.NA", "cs.NA", "65D15, 41A10, 65D32"], "comment": "15 pages", "summary": "This paper surveys hyperinterpolation, a quadrature-based approximation\nscheme. We cover classical results, provide examples on several domains, review\nrecent progress on relaxed quadrature exactness, introduce methodological\nvariants, and discuss applications to differential and integral equations.", "AI": {"tldr": "Survey of hyperinterpolation methods, covering classical results, examples on domains, recent progress on relaxed quadrature exactness, methodological variants, and applications to differential/integral equations.", "motivation": "To provide a comprehensive overview of hyperinterpolation as a quadrature-based approximation scheme and its developments.", "method": "Survey approach covering classical theory, domain examples, recent advances in relaxed quadrature exactness, and methodological variants.", "result": "Comprehensive review of hyperinterpolation methods, their theoretical foundations, and practical implementations across various domains.", "conclusion": "Hyperinterpolation is a versatile quadrature-based approximation method with broad applications in differential and integral equations, with ongoing developments in methodology and theory."}}
{"id": "2510.04501", "pdf": "https://arxiv.org/pdf/2510.04501", "abs": "https://arxiv.org/abs/2510.04501", "authors": ["Chiun-Chuan Chen", "Ting-Yang Hsiao", "Shun-Chieh Wang"], "title": "Non-Monotone Traveling Waves of the Weak Competition Lotka-Volterra System", "categories": ["math.AP", "math.DS", "q-bio.PE"], "comment": null, "summary": "We investigate traveling wave solutions in the two-species reaction-diffusion\nLotka-Volterra competition system under weak competition. For the strict weak\ncompetition regime $(b<a<1/c,\\,d>0)$, we construct refined upper and lower\nsolutions combined with the Schauder fixed point theorem to establish the\nexistence of traveling waves for all wave speeds $s\\geq\ns^*:=\\max\\{2,2\\sqrt{ad}\\}$, and provide verifiable sufficient conditions for\nthe emergence of non-monotone waves. Such conditions for non-monotonic waves\nhave not been explicitly addressed in previous studies. It is interesting to\npoint out that our result for non-monotone waves also hold for the critical\nspeed case $s=s^*$. In addition, in the critical weak competition case\n$(b<a=1/c,\\,d>0)$, we rigorously prove, for the first time, the existence of\nfront-pulse traveling waves.", "AI": {"tldr": "Existence of traveling wave solutions in two-species Lotka-Volterra competition system under weak competition, including non-monotone waves and front-pulse waves.", "motivation": "To establish existence of traveling waves in weak competition regime and provide explicit conditions for non-monotone waves, which were not addressed in previous studies.", "method": "Construct refined upper and lower solutions combined with Schauder fixed point theorem.", "result": "Traveling waves exist for all wave speeds s \u2265 s* = max{2,2\u221a(ad)}, with sufficient conditions for non-monotone waves. Front-pulse traveling waves exist in critical weak competition case.", "conclusion": "Successfully established existence of traveling waves in weak competition regime, including non-monotone waves and front-pulse waves, filling gaps in previous research."}}
{"id": "2510.04920", "pdf": "https://arxiv.org/pdf/2510.04920", "abs": "https://arxiv.org/abs/2510.04920", "authors": ["Yury Zabegaev", "Inga Berre", "Eirik Keilegavlen"], "title": "Data-driven linear solver selection and performance tuning for multiphysics simulations in porous media", "categories": ["math.NA", "cs.NA", "65F08, 68T05, 65M08, 76S05, 35Q74, 74S10, 74M15", "G.1.3; G.1.8; I.2.6; J.2; G.4"], "comment": "21 pages, 21 figures", "summary": "Modeling multiphysics processes in porous media requires preconditioned\niterative linear solvers to enable efficient simulations at industry-relevant\nscales. These solvers are typically composed of sub-algorithms that target\nindividual physical processes. Various options are available for each\nalgorithm, with the corresponding ranges of numerical parameters. The choices\nof sub-algorithms and their parameters significantly affects simulation\nperformance and robustness. Optimizing these choices for each simulation is\nchallenging due to the vast number of possible combinations. Moreover,\noptimization relies on performance data from past simulations, which becomes\nless representative as the simulation setup changes. This paper addresses the\nproblem of automated selection and tuning of preconditioned linear solvers for\nmultiphysics simulations. The proposed solver selection algorithm collects\nperformance data during the run of the target simulation and continuously\nupdates a machine learning model responsible for solver selection, resulting in\nan adaptively refined selection policy. The algorithm is evaluated on two\ntime-dependent nonlinear model problems: (i) coupled fluid flow and heat\ntransfer in porous media and (ii) thermo-poromechanics in porous media with\nfractures, governed by frictional contact mechanics. These experiments\ndemonstrate that the algorithm selects efficient and robust solvers with\nnegligible overhead and performs comparably to a reference selection policy\nthat has full access to the performance data of prior simulations. Our results\nindicate that the proposed approach effectively addresses the challenge of\nsolver selection and tuning, providing particular value to simulation engineers\nand researchers, especially when expert knowledge on linear solver tuning is\nnot readily available.", "AI": {"tldr": "Automated selection and tuning of preconditioned linear solvers for multiphysics simulations using machine learning with adaptive refinement during runtime.", "motivation": "Optimizing linear solver choices for multiphysics simulations is challenging due to vast algorithm combinations and changing simulation setups that make past performance data less representative.", "method": "Proposes a solver selection algorithm that collects performance data during simulation runtime and continuously updates a machine learning model for adaptive solver selection.", "result": "The algorithm selects efficient and robust solvers with negligible overhead, performing comparably to reference policies with full access to prior simulation data.", "conclusion": "The approach effectively addresses solver selection challenges, providing value especially when expert knowledge on linear solver tuning is unavailable."}}
{"id": "2510.04566", "pdf": "https://arxiv.org/pdf/2510.04566", "abs": "https://arxiv.org/abs/2510.04566", "authors": ["Takashi Kagaya", "Masatomo Takahashi"], "title": "Inverse curvature flow of closed Legendre curves", "categories": ["math.AP", "math.DG"], "comment": "18 pages, 13 figures", "summary": "In this paper, we deal with an inverse curvature flow of $\\ell$-convex\nLegendre curves. Since the Legendre curve is a natural generalization of\nregular curve, the flow is a generalization of the classical inverse curvature\nflow of regular curves. For the initial value problem, we study on the unique\nexistence of the flow in global time, the monotonicity of the number of the\nsingular cusps with respect to t > 0 and the asymptotic behavior of the flow as\n$t \\to \\infty$. Regarding the asymptotics, the flow asymptotically converges to\none of the self similar solutions by scaling appropriately, and the convergence\nis completely categorized depending on the initial curve.", "AI": {"tldr": "Analysis of inverse curvature flow for \u2113-convex Legendre curves, studying global existence, cusp behavior, and asymptotic convergence to self-similar solutions.", "motivation": "To generalize classical inverse curvature flow from regular curves to Legendre curves, which are natural generalizations of regular curves, and understand their geometric evolution properties.", "method": "Study the initial value problem for inverse curvature flow of \u2113-convex Legendre curves, analyzing global existence, monotonicity of singular cusps over time, and asymptotic behavior as t\u2192\u221e.", "result": "The flow exists globally, the number of singular cusps decreases monotonically with time, and the flow asymptotically converges to self-similar solutions after appropriate scaling, with convergence behavior fully categorized by initial curve properties.", "conclusion": "Inverse curvature flow for Legendre curves exhibits well-behaved global dynamics with predictable cusp evolution and complete classification of asymptotic convergence to self-similar solutions based on initial conditions."}}
{"id": "2510.00538", "pdf": "https://arxiv.org/pdf/2510.00538", "abs": "https://arxiv.org/abs/2510.00538", "authors": ["Koji Wada", "Takao Namiki"], "title": "Cluster Analysis for Globally Coupled Map using Optimal Transport Distance and Complexity of Attractor-ruin", "categories": ["math.DS", "cs.NA", "math.NA", "37D45, 39A33, 62H30"], "comment": "15 pages, 10 figures", "summary": "In this paper, we show the results of the strength of attractorruins for a\nglobally coupled map. The globally coupled map (GCM) is a discrete dynamical\nsystem, and here we consider a model in which the logistic map is globally\ncoupled. An attractor-ruin is a set in which the attractor is destabilized by a\nchange in parameters, which is characterized by a Milnor attractor.\nIntermittent phenomena called chaotic itinerancy, in which orbits transition\nbetween attractor-ruin, have been observed in various complex systems, and\ntheir onset mechanisms and statistical properties have attracted attention. In\nthis study, the instability of orbits of GCM is analyzed from the perspective\nof clustering using the optimal transport distance, and the strength of\nattractor-ruins is numerically evaluated by applying this method. As a result,\nit was found that the strength of various attractor-ruins is high in the\nparameter region called the partially ordered phase, where chaotic itinerancy\noccurs.", "AI": {"tldr": "Analysis of attractor-ruin strength in globally coupled logistic maps using optimal transport distance, showing high strength in the partially ordered phase where chaotic itinerancy occurs.", "motivation": "To understand the instability mechanisms and statistical properties of chaotic itinerancy - intermittent transitions between attractor-ruins observed in complex systems.", "method": "Analyzed orbit instability in globally coupled maps using optimal transport distance for clustering, then numerically evaluated attractor-ruin strength.", "result": "Found that various attractor-ruins have high strength specifically in the partially ordered phase parameter region where chaotic itinerancy occurs.", "conclusion": "The strength of attractor-ruins is closely related to the occurrence of chaotic itinerancy in the partially ordered phase of globally coupled maps."}}
{"id": "2510.04614", "pdf": "https://arxiv.org/pdf/2510.04614", "abs": "https://arxiv.org/abs/2510.04614", "authors": ["Huaian Diao", "Xieling Fan", "Hongyu Liu"], "title": "A New Quasi-Singularity Formation Mechanism for Second-order Hyperbolic Equations", "categories": ["math.AP", "35L71, 35L67, 35L15, 35B44"], "comment": null, "summary": "This paper investigates a novel mechanism for quasi-singularity formation in\nboth linear and nonlinear hyperbolic wave equations in two and three\ndimensions. We prove that over any finite time interval, there exist inputs\nsuch that the H\\\"older norm of the resulting wave field exceeds any prescribed\nbound. Conversely, the set of such almost-blowup points has vanishing measure\nwhen the aforementioned bound goes to infinity. This phenomenon thus defines a\nquasi-singular state, intermediate between classical singularity and\nregularity. Crucially, both the equation coefficients and the inputs can be\narbitrarily smooth; the quasi-singularity arises intrinsically from the\nstructure of the hyperbolic wave equation combined with specific input\ncharacteristics.", "AI": {"tldr": "The paper studies quasi-singularity formation in hyperbolic wave equations, showing that over any finite time, there exist inputs causing arbitrarily large H\u00f6lder norms, while the set of such blowup points has vanishing measure.", "motivation": "To understand intermediate states between classical singularity and regularity in wave equations, and investigate how quasi-singularities can arise from smooth coefficients and inputs.", "method": "Analysis of linear and nonlinear hyperbolic wave equations in 2D and 3D, proving existence of inputs causing arbitrarily large H\u00f6lder norms over finite time intervals.", "result": "Proved that quasi-singular states exist where wave field H\u00f6lder norms can exceed any bound, but the set of such blowup points has vanishing measure as the bound increases.", "conclusion": "Quasi-singularity is an intrinsic phenomenon in hyperbolic wave equations that arises from equation structure and specific input characteristics, even with smooth coefficients and inputs."}}
{"id": "2510.03303", "pdf": "https://arxiv.org/pdf/2510.03303", "abs": "https://arxiv.org/abs/2510.03303", "authors": ["Enrique Zuazua"], "title": "Machine Learning and Control: Foundations, Advances, and Perspectives", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "Control theory of dynamical systems offers a powerful framework for tackling\nchallenges in deep neural networks and other machine learning architectures. We\nshow that concepts such as simultaneous and ensemble controllability offer new\ninsights into the classification and representation properties of deep neural\nnetworks while the control and optimization of static systems can be employed\nto better understand the performance of shallow networks. Inspired by the\nclassical concept of turnpike, we also explore the relationship between dynamic\nand static neural networks, where depth is traded for width, and the role of\ntransformers as mechanisms for accelerating classical neural network tasks. We\nalso exploit the expressive power of neural networks (exemplified, for\ninstance, by the Universal Approximation Theorem) to develop a novel hybrid\nmodeling methodology, the Hybrid-Cooperative Learning (HYCO), combining\nmechanics and data-driven methods in a game-theoretic setting. Finally, we\ndescribe how classical properties of diffusion processes, long established in\nthe context of partial differential equations, contribute to explaining the\nsuccess of modern generative artificial intelligence (AI). We present an\noverview of our recent results in these areas, illustrating how control,\nmachine learning, numerical analysis, and partial differential equations come\ntogether to motivate a fertile ground for future research.", "AI": {"tldr": "Control theory provides insights into deep neural networks' properties, explores depth-width tradeoffs, introduces hybrid modeling methods, and explains generative AI success through diffusion processes.", "motivation": "To apply control theory concepts to understand deep neural networks and machine learning architectures, bridging control theory with machine learning, numerical analysis, and PDEs.", "method": "Uses simultaneous/ensemble controllability for network analysis, explores depth-width relationships via turnpike concept, develops HYCO hybrid modeling combining mechanics and data, and applies diffusion process theory.", "result": "Establishes connections between control theory and neural networks, provides theoretical foundations for network properties, creates hybrid modeling framework, and explains generative AI mechanisms.", "conclusion": "Control theory, machine learning, numerical analysis, and PDEs form a fertile interdisciplinary research area with significant potential for advancing neural network understanding and applications."}}
{"id": "2510.04672", "pdf": "https://arxiv.org/pdf/2510.04672", "abs": "https://arxiv.org/abs/2510.04672", "authors": ["Giacomo Bertazzoni", "Petteri Harjulehto", "Peter H\u00e4st\u00f6", "Elvira Zappale"], "title": "Relaxation of quasi-convex functionals with variable exponent growth", "categories": ["math.AP", "math.FA", "26A45, 26B30, 46E35, 46E99, 49J45"], "comment": null, "summary": "We prove a relaxation result for a quasi-convex bulk integral functional with\nvariable exponent growth in a suitable space of bounded variation type. A key\ntool is a decomposition under mild assumptions of the energy into absolutely\ncontinuous and singular parts weighted via a recession function.", "AI": {"tldr": "Relaxation result for quasi-convex bulk integral functional with variable exponent growth in BV-type spaces, using energy decomposition into absolutely continuous and singular parts weighted by recession function.", "motivation": "To establish relaxation results for integral functionals with variable exponent growth in spaces of bounded variation, which are important in calculus of variations and partial differential equations.", "method": "Prove relaxation by decomposing the energy functional into absolutely continuous and singular parts weighted through a recession function under mild assumptions.", "result": "Successfully proved the relaxation result for quasi-convex bulk integral functional with variable exponent growth in bounded variation type spaces.", "conclusion": "The relaxation approach using energy decomposition with recession function weighting provides an effective framework for handling variable exponent growth functionals in BV-type spaces."}}
{"id": "2510.03426", "pdf": "https://arxiv.org/pdf/2510.03426", "abs": "https://arxiv.org/abs/2510.03426", "authors": ["Franz A. Heinsen", "Leo Kozachkov"], "title": "Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "comment": "18 pages, 4 figures (main text). 14 pages, 21 figures (appendix)", "summary": "Many domains, from deep learning to finance, require compounding real numbers\nover long sequences, often leading to catastrophic numerical underflow or\noverflow. We introduce generalized orders of magnitude (GOOMs), a principled\nextension of traditional orders of magnitude that incorporates floating-point\nnumbers as a special case, and which in practice enables stable computation\nover significantly larger dynamic ranges of real numbers than previously\npossible. We implement GOOMs, along with an efficient custom parallel prefix\nscan, to support native execution on parallel hardware such as GPUs. We\ndemonstrate that our implementation of GOOMs outperforms traditional approaches\nwith three representative experiments, all of which were previously considered\nimpractical or impossible, and now become possible and practical: (1)\ncompounding real matrix products far beyond standard floating-point limits; (2)\nestimating spectra of Lyapunov exponents in parallel, orders of magnitude\nfaster than with previous methods, applying a novel selective-resetting method\nto prevent state colinearity; and (3) capturing long-range dependencies in deep\nrecurrent neural networks with non-diagonal recurrent states, computed in\nparallel via a prefix scan, without requiring any form of stabilization. Our\nresults show that our implementation of GOOMs, combined with efficient parallel\nscanning, offers a scalable and numerically robust alternative to conventional\nfloating-point numbers for high-dynamic-range applications.", "AI": {"tldr": "GOOMs enable stable computation over large dynamic ranges of real numbers, outperforming traditional floating-point approaches in compounding operations, Lyapunov exponent estimation, and deep recurrent neural networks.", "motivation": "Many domains require compounding real numbers over long sequences, leading to catastrophic numerical underflow or overflow with traditional floating-point numbers.", "method": "Introduce generalized orders of magnitude (GOOMs) with efficient custom parallel prefix scan implementation for native execution on parallel hardware like GPUs.", "result": "GOOMs enable previously impractical/impossible experiments: compounding real matrix products beyond floating-point limits, faster Lyapunov exponent estimation with selective-resetting, and stable long-range dependencies in deep RNNs without stabilization.", "conclusion": "GOOMs combined with efficient parallel scanning offer a scalable and numerically robust alternative to conventional floating-point numbers for high-dynamic-range applications."}}
{"id": "2510.04763", "pdf": "https://arxiv.org/pdf/2510.04763", "abs": "https://arxiv.org/abs/2510.04763", "authors": ["Shammi Malhotra", "Sarika Goyal", "K. Sreenadh"], "title": "Asymptotic behaviour and existence of positive solutions for mixed local nonlocal elliptic equations with Hardy potential", "categories": ["math.AP"], "comment": null, "summary": "We investigate the existence and multiplicity of positive solutions to the\nfollowing problem driven by the superposition of the Laplacian and the\nfractional Laplacian with Hardy potential \\begin{equation*} \\left\\{\n\\begin{aligned}\n  -\\Delta u + (-\\Delta)^s u - \\mu \\frac{u}{|x|^2} &= \\lambda |u|^{p-2} u +\n|u|^{2^*-2} u \\quad \\text{in } \\Omega \\subset \\mathbb{R}^N,\n  u &= 0 \\quad \\text{in } \\mathbb{R}^N \\setminus \\Omega, \\end{aligned} \\right.\n\\end{equation*} where $ \\Omega \\subset \\mathbb{R}^N $ is a bounded domain with\nsmooth boundary, $ 0 < s < 1 $, $ 1 < p < 2^* $, with $ 2^* = \\frac{2N}{N-2} $,\n$ \\lambda > 0 $, and $ \\mu \\in (0, \\bar{\\mu}) $ where $\\bar \\mu = \\left(\n\\frac{N-2}{2} \\right)^2$.\n  The aim of this paper is twofold. First, we establish uniform asymptotic\nestimates for solutions of the problem by means of a suitable transformation.\nThen, according to the value of the exponent $p$, we analyze three distinct\ncases and prove the existence of a positive solution. Moreover, in the\nsublinear regime $1 < p < 2$, we demonstrate the existence of multiple positive\nsolutions for small perturbations of the fractional Laplacian.", "AI": {"tldr": "The paper studies positive solutions to a mixed Laplacian-fractional Laplacian equation with Hardy potential, establishing asymptotic estimates and proving existence/multiplicity results depending on the exponent p.", "motivation": "To investigate the existence and multiplicity of positive solutions for a nonlinear elliptic problem combining classical and fractional Laplacians with Hardy potential, which arises in various physical applications.", "method": "Uses uniform asymptotic estimates via suitable transformations and analyzes three distinct cases based on exponent p values (sublinear, critical, superlinear). Employs variational methods and perturbation techniques.", "result": "Proves existence of positive solutions for all three p-regimes. In sublinear case (1 < p < 2), demonstrates multiple positive solutions for small perturbations of the fractional Laplacian.", "conclusion": "The mixed Laplacian-fractional Laplacian operator with Hardy potential admits positive solutions, with multiplicity occurring in the sublinear regime under small fractional perturbations."}}
{"id": "2510.03505", "pdf": "https://arxiv.org/pdf/2510.03505", "abs": "https://arxiv.org/abs/2510.03505", "authors": ["Lamsahel Noureddine", "Carole Rosier"], "title": "A Direct Approach for Detection of Bottom Topography in Shallow Water", "categories": ["math-ph", "cs.NA", "math.MP", "math.NA"], "comment": null, "summary": "We propose a fast, stable, and direct analytic method to detect underwater\nchannel topography from surface wave measurements, based on one-dimensional\nshallow water equations. The technique requires knowledge of the free surface\nand its first two time derivatives at a single instant $t^{\\star}$ above the\nfixed, bounded open segment of the domain. We first restructure the forward\nshallow water equations to obtain an inverse model in which the bottom profile\nis the only unknown, and then discretize this model using a second-order\nfinite-difference scheme to infer the floor topography. We demonstrate that the\napproach satisfies a Lipschitz stability and is independent of the initial\nconditions of the forward problem. The well-posedness of this inverse model\nrequires that, at the chosen measurement time $t^{\\star}$, the discharge be\nstrictly positive across the fixed portion of the open channel, which is\nautomatically satisfied for steady and supercritical flows. For unsteady\nsubcritical and transcritical flows, we derive two empirically validated\nsufficient conditions ensuring strict positivity after a sufficiently large\ntime. The proposed methodology is tested on a range of scenarios, including\nclassical benchmarks and different types of inlet discharges and bathymetries.\nWe find that this analytic approach yields high approximation accuracy and that\nthe bed profile reconstruction is stable under noise. In addition, the\nsufficient conditions are met across all tests.", "AI": {"tldr": "A fast, stable analytic method for detecting underwater channel topography from surface wave measurements using shallow water equations, requiring only surface data at a single time instant.", "motivation": "To develop an efficient and reliable method for reconstructing underwater channel topography from easily measurable surface wave data, overcoming limitations of traditional methods.", "method": "Restructured forward shallow water equations into an inverse model, discretized with second-order finite-difference scheme, requiring only free surface and its first two time derivatives at a single measurement time.", "result": "The method achieves high approximation accuracy, stable reconstruction under noise, and satisfies Lipschitz stability independent of initial conditions. Sufficient conditions for positivity are met across all tested scenarios.", "conclusion": "The proposed analytic approach provides an effective, stable, and direct method for underwater topography detection that works across various flow regimes and bathymetry types."}}
{"id": "2510.04801", "pdf": "https://arxiv.org/pdf/2510.04801", "abs": "https://arxiv.org/abs/2510.04801", "authors": ["Sourav Mitra", "Sebastian Schwarzacher"], "title": "Thermal effects in fluid structure interactions", "categories": ["math.AP", "35Q30, 35R37, 35Q35, 74F10, 76D05"], "comment": null, "summary": "In this article we consider two different heat conducting fluids each\nmodelled by the incompressible Navier-Stokes-Fourier system separated by a\nnon-linear elastic Koiter shell. The motion of the shell changes the domain of\ndefinition of the two separated fluids. For this setting we show the existence\nof a weak solution. The heat capacity of the shell is given energetically. It\nallows to consider transmission laws ranging from insulation to\nsuperconductivity.We follow a variational approach for fluid-structure\ninteractions. To include temperature a novel two step minimization scheme is\nused to produce an approximation. The weak solutions are energetically closed\nand include a strictly positive temperature.", "AI": {"tldr": "Existence of weak solutions for two heat-conducting fluids separated by a nonlinear elastic Koiter shell, with varying transmission laws from insulation to superconductivity.", "motivation": "To model and analyze the interaction between two heat-conducting fluids separated by an elastic shell, where the shell's motion affects the fluid domains and heat transmission properties.", "method": "Variational approach for fluid-structure interactions using a novel two-step minimization scheme to incorporate temperature effects.", "result": "Demonstrated existence of weak solutions that are energetically closed and include strictly positive temperature.", "conclusion": "Successfully established the existence of weak solutions for this complex fluid-structure interaction problem with heat conduction and varying transmission properties."}}
{"id": "2510.03535", "pdf": "https://arxiv.org/pdf/2510.03535", "abs": "https://arxiv.org/abs/2510.03535", "authors": ["William Anderson", "Seung Whan Chung", "Youngsoo Choi"], "title": "Sequential decoder training for improved latent space dynamics identification", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML"], "comment": null, "summary": "Accurate numerical solutions of partial differential equations are essential\nin many scientific fields but often require computationally expensive solvers,\nmotivating reduced-order models (ROMs). Latent Space Dynamics Identification\n(LaSDI) is a data-driven ROM framework that combines autoencoders with equation\ndiscovery to learn interpretable latent dynamics. However, enforcing latent\ndynamics during training can compromise reconstruction accuracy of the model\nfor simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that\nimproves reconstruction and prediction accuracy by sequentially learning\nadditional decoders to correct residual errors from previous stages. Applied to\nthe 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI,\nachieving lower prediction errors and reduced training time across a wide range\nof architectures.", "AI": {"tldr": "Multi-stage LaSDI (mLaSDI) improves reduced-order modeling by sequentially learning additional decoders to correct residual errors, achieving better accuracy and faster training than standard LaSDI for the Vlasov equation.", "motivation": "Standard LaSDI framework can compromise reconstruction accuracy when enforcing latent dynamics during training, creating a need for improved methods that maintain both interpretable dynamics and high accuracy.", "method": "Multi-stage LaSDI sequentially learns additional decoders to correct residual errors from previous stages, building upon the autoencoder and equation discovery framework of standard LaSDI.", "result": "mLaSDI consistently outperforms standard LaSDI on the 1D-1V Vlasov equation, achieving lower prediction errors and reduced training time across various architectures.", "conclusion": "The multi-stage approach effectively improves both reconstruction and prediction accuracy while maintaining the interpretable latent dynamics framework, making it a superior alternative to standard LaSDI."}}
{"id": "2510.04809", "pdf": "https://arxiv.org/pdf/2510.04809", "abs": "https://arxiv.org/abs/2510.04809", "authors": ["Davide Buoso", "Riccardo Molinarolo"], "title": "On the eigenvalues of the biharmonic operator on annuli", "categories": ["math.AP"], "comment": null, "summary": "We show that the fundamental tone of the bilaplacian with Dirichlet or Navier\nboundary conditions on radially symmetric domains is always simple in dimension\n$N\\ge3$. In dimension $N=2$ we show that it is simple if the inner radius is\nbig enough.", "AI": {"tldr": "The paper analyzes the simplicity of the fundamental tone of the bilaplacian with Dirichlet or Navier boundary conditions on radially symmetric domains.", "motivation": "To understand the spectral properties of the bilaplacian operator on symmetric domains, particularly whether the fundamental tone is simple or degenerate.", "method": "Mathematical analysis of the bilaplacian operator with Dirichlet or Navier boundary conditions on radially symmetric domains, examining different dimensions.", "result": "In dimensions N\u22653, the fundamental tone is always simple. In dimension N=2, it is simple when the inner radius is sufficiently large.", "conclusion": "The fundamental tone of the bilaplacian exhibits different behavior based on dimension and domain geometry, with guaranteed simplicity in higher dimensions and conditional simplicity in 2D."}}
{"id": "2510.03650", "pdf": "https://arxiv.org/pdf/2510.03650", "abs": "https://arxiv.org/abs/2510.03650", "authors": ["Amir Sadikov"], "title": "LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "cs.NE", "math.NA"], "comment": null, "summary": "Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo\n(QMC) methods for high-dimensional integration. We cast two long-standing QMC\ndesign problems as program synthesis and solve them with an LLM-guided\nevolutionary loop that mutates and selects code under task-specific fitness:\n(i) constructing finite 2D/3D point sets with low star discrepancy, and (ii)\nchoosing Sobol' direction numbers that minimize randomized QMC error on\ndownstream integrands. Our two-phase procedure combines constructive code\nproposals with iterative numerical refinement. On finite sets, we rediscover\nknown optima in small 2D cases and set new best-known 2D benchmarks for N >=\n40, while matching most known 3D optima up to the proven frontier (N <= 8) and\nreporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol'\nparameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC)\nmean-squared error for several 32-dimensional option-pricing tasks relative to\nwidely used Joe--Kuo parameters, while preserving extensibility to any sample\nsize and compatibility with standard randomizations. Taken together, the\nresults demonstrate that LLM-driven evolutionary program synthesis can automate\nthe discovery of high-quality QMC constructions, recovering classical designs\nwhere they are optimal and improving them where finite-N structure matters.\nData and code are available at\nhttps://github.com/hockeyguy123/openevolve-star-discrepancy.git.", "AI": {"tldr": "LLM-guided evolutionary program synthesis automates the discovery of high-quality quasi-Monte Carlo constructions, improving finite point sets and Sobol' sequences for numerical integration.", "motivation": "To solve long-standing QMC design problems by automating the construction of low-discrepancy point sets and optimal Sobol' direction numbers, which are traditionally challenging to design manually.", "method": "Two-phase procedure combining constructive code proposals with iterative numerical refinement, using an LLM-guided evolutionary loop that mutates and selects code under task-specific fitness functions.", "result": "Achieved new best-known 2D benchmarks for N >= 40, matched most known 3D optima up to N <= 8, and improved 3D benchmarks beyond. For Sobol' sequences, consistently reduced rQMC error in 32-dimensional option pricing compared to Joe-Kuo parameters.", "conclusion": "LLM-driven evolutionary program synthesis can automate QMC construction discovery, recovering classical designs where optimal and improving them where finite-N structure matters, demonstrating practical automation of numerical design problems."}}
{"id": "2510.04810", "pdf": "https://arxiv.org/pdf/2510.04810", "abs": "https://arxiv.org/abs/2510.04810", "authors": ["Dong Qiu", "Xiang Xu", "Yeqiong Ye", "Ting Zhou"], "title": "Uniqueness Result For Semi-linear Wave Equations With Sources", "categories": ["math.AP", "35R30"], "comment": "25 pages", "summary": "This paper addresses the inverse problem of simultaneously recovering\nmultiple unknown parameters for semilinear wave equations from boundary\nmeasurements. We consider an initial-boundary value problem for a wave equation\nwith a general semilinear term and an internal source. The inverse problem is\nto determine the nonlinear coefficients (potentials), the source term, and the\ninitial data from the Dirichlet-to-Neumann (DtN) map. Our approach combines\nhigher-order linearization and the construction of complex geometrical optics\n(CGO) solutions. The main results establish that while unique recovery is not\nalways possible, we can precisely characterize the gauge equivalence classes in\nthe solutions to this inverse problem. For a wave equation with a polynomial\nnonlinearity of degree $n$, we prove that only the highest-order coefficient\ncan be uniquely determined from the DtN map; the lower-order coefficients and\nthe source can only be recovered up to a specific gauge transformation\ninvolving a function $\\psi$. Furthermore, we provide sufficient conditions\nunder which unique determination of all parameters is guaranteed. We also\nextend these results to various specific non-polynomial nonlinearities,\ndemonstrating that the nature of the nonlinearity critically influences whether\nunique recovery or a gauge symmetry is obtained.", "AI": {"tldr": "This paper studies the inverse problem of recovering multiple parameters for semilinear wave equations from boundary measurements. It shows that unique recovery is not always possible, but characterizes the gauge equivalence classes and identifies which parameters can be uniquely determined versus those subject to gauge transformations.", "motivation": "To understand the limitations and possibilities of parameter recovery in semilinear wave equations from boundary measurements, particularly addressing the inverse problem of determining nonlinear coefficients, source terms, and initial data from the Dirichlet-to-Neumann map.", "method": "Combines higher-order linearization and construction of complex geometrical optics (CGO) solutions to analyze the inverse problem and characterize gauge equivalence classes.", "result": "For polynomial nonlinearities of degree n, only the highest-order coefficient can be uniquely determined; lower-order coefficients and source terms can only be recovered up to specific gauge transformations. Provides sufficient conditions for unique determination of all parameters.", "conclusion": "The nature of the nonlinearity critically influences whether unique recovery or gauge symmetry is obtained, with precise characterization of gauge equivalence classes for various nonlinearities including polynomial and non-polynomial types."}}
{"id": "2510.03745", "pdf": "https://arxiv.org/pdf/2510.03745", "abs": "https://arxiv.org/abs/2510.03745", "authors": ["Michael Etienne Van Huffel", "Nathan Kirk", "Makram Chahine", "Daniela Rus", "T. Konstantin Rusch"], "title": "Neural Low-Discrepancy Sequences", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "Low-discrepancy points are designed to efficiently fill the space in a\nuniform manner. This uniformity is highly advantageous in many problems in\nscience and engineering, including in numerical integration, computer vision,\nmachine perception, computer graphics, machine learning, and simulation.\nWhereas most previous low-discrepancy constructions rely on abstract algebra\nand number theory, Message-Passing Monte Carlo (MPMC) was recently introduced\nto exploit machine learning methods for generating point sets with lower\ndiscrepancy than previously possible. However, MPMC is limited to generating\npoint sets and cannot be extended to low-discrepancy sequences (LDS), i.e.,\nsequences of points in which every prefix has low discrepancy, a property\nessential for many applications. To address this limitation, we introduce\nNeural Low-Discrepancy Sequences ($NeuroLDS$), the first machine learning-based\nframework for generating LDS. Drawing inspiration from classical LDS, we train\na neural network to map indices to points such that the resulting sequences\nexhibit minimal discrepancy across all prefixes. To this end, we deploy a\ntwo-stage learning process: supervised approximation of classical constructions\nfollowed by unsupervised fine-tuning to minimize prefix discrepancies. We\ndemonstrate that $NeuroLDS$ outperforms all previous LDS constructions by a\nsignificant margin with respect to discrepancy measures. Moreover, we\ndemonstrate the effectiveness of $NeuroLDS$ across diverse applications,\nincluding numerical integration, robot motion planning, and scientific machine\nlearning. These results highlight the promise and broad significance of Neural\nLow-Discrepancy Sequences. Our code can be found at\nhttps://github.com/camail-official/neuro-lds.", "AI": {"tldr": "NeuroLDS is the first machine learning framework for generating low-discrepancy sequences (LDS) that outperform classical constructions, enabling applications in numerical integration, robotics, and scientific ML.", "motivation": "Existing machine learning approaches like MPMC can only generate point sets but not sequences where every prefix has low discrepancy, which is essential for many applications. Classical LDS constructions rely on abstract algebra and number theory, limiting their performance.", "method": "Train a neural network to map indices to points using a two-stage process: supervised approximation of classical constructions followed by unsupervised fine-tuning to minimize prefix discrepancies across all sequence lengths.", "result": "NeuroLDS significantly outperforms all previous LDS constructions in discrepancy measures and demonstrates effectiveness in numerical integration, robot motion planning, and scientific machine learning applications.", "conclusion": "The framework shows promise for broad applications and represents a significant advancement over traditional methods for generating low-discrepancy sequences."}}
{"id": "2510.04833", "pdf": "https://arxiv.org/pdf/2510.04833", "abs": "https://arxiv.org/abs/2510.04833", "authors": ["Pablo Hidalgo-Palencia", "Cody Hutcheson", "Joseph Kasel"], "title": "The parabolic Dirichlet problem with continuous and H\u00f6lder boundary data, and rough coefficients", "categories": ["math.AP", "math.CA", "35K20 (Primary), 31B25, 35B65 (Secondary)"], "comment": "41 pages, 7 figures", "summary": "We provide very mild sufficient conditions for space-time domains\n(non-necessarily cylindrical) which ensure that the continuous Dirichlet\nproblem and the H\\\"older Dirichlet problem are well-posed, for any parabolic\noperator in divergence form with merely bounded coefficients. Concretely, we\nshow that the parabolic measure exists, even for unbounded domains, hence\nsolving an open problem posed by Genschaw and Hofmann (2020).\n  This problem has inherent difficulties because of its parabolic nature, as\nthe behavior of solutions near the boundary may depend strongly on the values\nof the coefficients of the operator. One of our sufficient conditions, the\ntime-backwards capacity density condition, is a quantitative version of the\nparabolic Wiener's criterion, and hence is adapted to the operator under\nconsideration. The other condition, the time-backwards Hausdorff content\ncondition, is (albeit slightly stronger) purely geometrical and independent of\nthe operator, hence much easier to check in practice.", "AI": {"tldr": "The paper establishes mild sufficient conditions for well-posedness of Dirichlet problems for parabolic operators with bounded coefficients in general space-time domains, solving an open problem about parabolic measure existence.", "motivation": "To address the open problem posed by Genschaw and Hofmann (2020) regarding the existence of parabolic measure for unbounded domains, overcoming inherent difficulties due to the parabolic nature where solution behavior near boundaries depends strongly on operator coefficients.", "method": "Introduces two sufficient conditions: (1) time-backwards capacity density condition (quantitative version of parabolic Wiener's criterion, adapted to the operator), and (2) time-backwards Hausdorff content condition (purely geometrical, operator-independent).", "result": "Proves that both conditions ensure well-posedness of continuous and H\u00f6lder Dirichlet problems for any parabolic operator in divergence form with bounded coefficients, establishing existence of parabolic measure even for unbounded domains.", "conclusion": "The paper successfully solves the open problem by providing practical, verifiable conditions that guarantee parabolic measure existence and well-posedness of Dirichlet problems in general space-time domains."}}
{"id": "2510.03803", "pdf": "https://arxiv.org/pdf/2510.03803", "abs": "https://arxiv.org/abs/2510.03803", "authors": ["Chenglong Bao", "Zanyu Li", "Yunan Yang"], "title": "Well-Posedness and Efficient Algorithms for Inverse Optimal Transport with Bregman Regularization", "categories": ["math.OC", "cs.NA", "math.NA"], "comment": null, "summary": "This work analyzes the inverse optimal transport (IOT) problem under Bregman\nregularization. We establish well-posedness results, including existence,\nuniqueness (up to equivalence classes of solutions), and stability, under\nseveral structural assumptions on the cost matrix. On the computational side,\nwe investigate the existence of solutions to the optimization problem with\ngeneral constraints on the cost matrix and provide a sufficient condition\nguaranteeing existence. In addition, we propose an inexact block coordinate\ndescent (BCD) method for the problem with a strongly convex penalty term. In\nparticular, when the penalty is quadratic, the subproblems admit a diagonal\nHessian structure, which enables highly efficient element-wise Newton updates.\nWe establish a linear convergence rate for the algorithm and demonstrate its\npractical performance through numerical experiments, including the validation\nof stability bounds, the investigation of regularization effects, and the\napplication to a marriage matching dataset.", "AI": {"tldr": "Analysis of inverse optimal transport under Bregman regularization, establishing well-posedness and proposing an efficient inexact block coordinate descent method with linear convergence.", "motivation": "To address the inverse optimal transport problem with regularization, establishing theoretical foundations and developing efficient computational methods for practical applications.", "method": "Established well-posedness results (existence, uniqueness, stability) under structural assumptions on cost matrix. Proposed inexact block coordinate descent method with strongly convex penalty, using element-wise Newton updates for quadratic penalties.", "result": "Proved existence and uniqueness of solutions under sufficient conditions. Developed efficient algorithm with linear convergence rate. Demonstrated practical performance through numerical experiments including stability validation and marriage matching applications.", "conclusion": "The proposed framework provides rigorous theoretical guarantees and efficient computational methods for inverse optimal transport with Bregman regularization, enabling practical applications across various domains."}}
{"id": "2510.04881", "pdf": "https://arxiv.org/pdf/2510.04881", "abs": "https://arxiv.org/abs/2510.04881", "authors": ["Stefano Almi", "Maicol Caponi", "Manuel Friedrich", "Francesco Solombrino"], "title": "Riesz fractional gradient functionals defined on partitions: nonlocal-to-local variational limits", "categories": ["math.AP", "49J45, 26A33, 35B27, 35R11, 26B30, 49Q20"], "comment": null, "summary": "This paper addresses the asymptotics of functionals with linear growth\ndepending on the Riesz $s$-fractional gradient on piecewise constant functions.\nWe consider a general class of varying energy densities and, as $s\\to 1$, we\ncharacterize their local limiting functionals in the sense of\n$\\Gamma$-convergence.", "AI": {"tldr": "Study of asymptotics for functionals with linear growth depending on Riesz s-fractional gradient on piecewise constant functions, characterizing local limiting functionals via \u0393-convergence as s\u21921.", "motivation": "To understand the limiting behavior of fractional gradient functionals with linear growth as the fractional parameter approaches 1, particularly for piecewise constant functions.", "method": "Analysis of general class of varying energy densities depending on Riesz s-fractional gradient, using \u0393-convergence techniques to characterize local limiting functionals as s\u21921.", "result": "Characterization of local limiting functionals in the \u0393-convergence sense for the asymptotics of these fractional gradient functionals.", "conclusion": "The paper successfully establishes the asymptotic behavior and identifies the limiting functionals for fractional gradient problems with linear growth as the fractional parameter approaches 1."}}
{"id": "2510.03949", "pdf": "https://arxiv.org/pdf/2510.03949", "abs": "https://arxiv.org/abs/2510.03949", "authors": ["Kyurae Kim", "Samuel Gruffaz", "Ji Won Park", "Alain Oliviero Durmus"], "title": "Analysis of kinetic Langevin Monte Carlo under the stochastic exponential Euler discretization from underdamped all the way to overdamped", "categories": ["stat.CO", "cs.NA", "math.NA", "math.PR", "stat.ML"], "comment": null, "summary": "Simulating the kinetic Langevin dynamics is a popular approach for sampling\nfrom distributions, where only their unnormalized densities are available.\nVarious discretizations of the kinetic Langevin dynamics have been considered,\nwhere the resulting algorithm is collectively referred to as the kinetic\nLangevin Monte Carlo (KLMC) or underdamped Langevin Monte Carlo. Specifically,\nthe stochastic exponential Euler discretization, or exponential integrator for\nshort, has previously been studied under strongly log-concave and log-Lipschitz\nsmooth potentials via the synchronous Wasserstein coupling strategy. Existing\nanalyses, however, impose restrictions on the parameters that do not explain\nthe behavior of KLMC under various choices of parameters. In particular, all\nknown results fail to hold in the overdamped regime, suggesting that the\nexponential integrator degenerates in the overdamped limit. In this work, we\nrevisit the synchronous Wasserstein coupling analysis of KLMC with the\nexponential integrator. Our refined analysis results in Wasserstein\ncontractions and bounds on the asymptotic bias that hold under weaker\nrestrictions on the parameters, which assert that the exponential integrator is\ncapable of stably simulating the kinetic Langevin dynamics in the overdamped\nregime, as long as proper time acceleration is applied.", "AI": {"tldr": "This paper provides a refined analysis of kinetic Langevin Monte Carlo (KLMC) using exponential integrator discretization, showing it works in the overdamped regime with proper time acceleration, overcoming limitations of previous analyses.", "motivation": "Previous analyses of KLMC with exponential integrator had restrictive parameter limitations and failed to explain behavior in the overdamped regime, suggesting the method degenerates in that limit.", "method": "The authors revisit the synchronous Wasserstein coupling analysis of KLMC with exponential integrator, using refined analytical techniques to derive Wasserstein contractions and bounds on asymptotic bias.", "result": "The refined analysis shows that exponential integrator can stably simulate kinetic Langevin dynamics in the overdamped regime when proper time acceleration is applied, with weaker parameter restrictions than previous work.", "conclusion": "The exponential integrator for KLMC is capable of stable simulation in the overdamped regime, contrary to previous understanding, as long as appropriate time acceleration is used."}}
{"id": "2510.04975", "pdf": "https://arxiv.org/pdf/2510.04975", "abs": "https://arxiv.org/abs/2510.04975", "authors": ["R. Kumar", "A. Ortega"], "title": "Fractional critical systems with mixed boundary conditions", "categories": ["math.AP", "Primary: 35J50, 35B33, 35R11, 35S15, Secondary: 35J61, 35Q55"], "comment": null, "summary": "In this paper, we analyze the existence of solution for a fractional elliptic\nsystem coupled by critical nonlinearities and endowed with mixed\nDirichlet-Neumann boundary conditions. By means of variational methods and an\northogonalization-like process in the corresponding Sobolev space, we establish\nthe existence of at least one weak solution.", "AI": {"tldr": "Existence of weak solutions for fractional elliptic systems with critical nonlinearities and mixed Dirichlet-Neumann boundary conditions", "motivation": "To analyze the existence of solutions for fractional elliptic systems coupled by critical nonlinearities with mixed boundary conditions", "method": "Variational methods and an orthogonalization-like process in Sobolev spaces", "result": "Established existence of at least one weak solution", "conclusion": "Successfully proved solution existence using variational techniques and orthogonalization methods"}}
{"id": "2510.03989", "pdf": "https://arxiv.org/pdf/2510.03989", "abs": "https://arxiv.org/abs/2510.03989", "authors": ["Xue-Cheng Tai", "Hao Liu", "Lingfeng Li", "Raymond H. Chan"], "title": "A Mathematical Explanation of Transformers for Large Language Models and GPTs", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "comment": null, "summary": "The Transformer architecture has revolutionized the field of sequence\nmodeling and underpins the recent breakthroughs in large language models\n(LLMs). However, a comprehensive mathematical theory that explains its\nstructure and operations remains elusive. In this work, we propose a novel\ncontinuous framework that rigorously interprets the Transformer as a\ndiscretization of a structured integro-differential equation. Within this\nformulation, the self-attention mechanism emerges naturally as a non-local\nintegral operator, and layer normalization is characterized as a projection to\na time-dependent constraint. This operator-theoretic and variational\nperspective offers a unified and interpretable foundation for understanding the\narchitecture's core components, including attention, feedforward layers, and\nnormalization. Our approach extends beyond previous theoretical analyses by\nembedding the entire Transformer operation in continuous domains for both token\nindices and feature dimensions. This leads to a principled and flexible\nframework that not only deepens theoretical insight but also offers new\ndirections for architecture design, analysis, and control-based\ninterpretations. This new interpretation provides a step toward bridging the\ngap between deep learning architectures and continuous mathematical modeling,\nand contributes a foundational perspective to the ongoing development of\ninterpretable and theoretically grounded neural network models.", "AI": {"tldr": "The paper proposes a continuous mathematical framework that interprets the Transformer architecture as a discretization of a structured integro-differential equation, providing a unified theoretical foundation.", "motivation": "To develop a comprehensive mathematical theory that explains the Transformer architecture's structure and operations, which currently lacks rigorous theoretical understanding despite its revolutionary impact on sequence modeling and LLMs.", "method": "The authors formulate a continuous framework where the Transformer is interpreted as a discretization of a structured integro-differential equation. Self-attention emerges as a non-local integral operator, and layer normalization is characterized as projection to time-dependent constraints.", "result": "The framework provides a unified and interpretable foundation for understanding Transformer components (attention, feedforward layers, normalization) by embedding the entire operation in continuous domains for both token indices and feature dimensions.", "conclusion": "This work bridges the gap between deep learning architectures and continuous mathematical modeling, offering new directions for architecture design, analysis, and control-based interpretations while contributing to theoretically grounded neural network models."}}
{"id": "2510.04099", "pdf": "https://arxiv.org/pdf/2510.04099", "abs": "https://arxiv.org/abs/2510.04099", "authors": ["Zhiqiang Xu", "Zili Xu", "Xinyue Zhang"], "title": "Optimal frames for Phase Retrieval from Edge Vectors of Optimal Polygons", "categories": ["cs.IT", "cs.NA", "math.FA", "math.IT", "math.MG", "math.NA"], "comment": null, "summary": "This paper aims to characterize the optimal frame for phase retrieval,\ndefined as the frame whose condition number for phase retrieval attains its\nminimal value. In the context of the two-dimensional real case, we reveal the\nconnection between optimal frames for phase retrieval and the\nperimeter-maximizing isodiametric problem, originally proposed by Reinhardt in\n1922. Our work establishes that every optimal solution to the\nperimeter-maximizing isodiametric problem inherently leads to an optimal frame\nin ${\\mathbb R}^2$. By recasting the optimal polygons problem as one concerning\nthe discrepancy of roots of unity, we characterize all optimal polygons.\nBuilding upon this connection, we then characterize all optimal frames with $m$\nvectors in ${\\mathbb R}^2$ for phase retrieval when $m \\geq 3$ has an odd\nfactor. As a key corollary, we show that the harmonic frame $E_m$ is {\\em not}\noptimal for any even integer $m \\geq 4$. This finding disproves a conjecture\nproposed by Xia, Xu, and Xu (Math. Comp., 90(356): 2931-2960). Previous work\nhas established that the harmonic frame $E_m \\subset {\\mathbb R}^2$ is indeed\noptimal when $m$ is an odd integer.\n  Exploring the connection between phase retrieval and discrete geometry, this\npaper aims to illuminate advancements in phase retrieval and offer new\nperspectives on the perimeter-maximizing isodiametric problem.", "AI": {"tldr": "This paper characterizes optimal frames for phase retrieval in 2D real space by connecting them to the perimeter-maximizing isodiametric problem, showing that optimal solutions to this geometric problem yield optimal frames.", "motivation": "To characterize optimal frames for phase retrieval and establish connections between phase retrieval and discrete geometry problems, specifically the perimeter-maximizing isodiametric problem.", "method": "By revealing the connection between optimal frames for phase retrieval and the perimeter-maximizing isodiametric problem, and recasting the optimal polygons problem as one concerning the discrepancy of roots of unity.", "result": "Characterized all optimal frames with m vectors in R\u00b2 for phase retrieval when m \u2265 3 has an odd factor, and proved that harmonic frame E_m is not optimal for any even integer m \u2265 4.", "conclusion": "The work establishes fundamental connections between phase retrieval and discrete geometry, disproves a previous conjecture about harmonic frames, and provides complete characterization of optimal frames under certain conditions."}}
{"id": "2510.04102", "pdf": "https://arxiv.org/pdf/2510.04102", "abs": "https://arxiv.org/abs/2510.04102", "authors": ["Ramzi Dakhmouche", "Hossein Gorji"], "title": "Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws", "categories": ["cs.LG", "cs.NA", "math.NA", "math.PR"], "comment": null, "summary": "Motivated by the remarkable success of Foundation Models (FMs) in language\nmodeling, there has been growing interest in developing FMs for time series\nprediction, given the transformative power such models hold for science and\nengineering. This culminated in significant success of FMs in short-range\nforecasting settings. However, extrapolation or long-range forecasting remains\nelusive for FMs, which struggle to outperform even simple baselines. This\ncontrasts with physical laws which have strong extrapolation properties, and\nraises the question of the fundamental difference between the structure of\nneural networks and physical laws. In this work, we identify and formalize a\nfundamental property characterizing the ability of statistical learning models\nto predict more accurately outside of their training domain, hence explaining\nperformance deterioration for deep learning models in extrapolation settings.\nIn addition to a theoretical analysis, we present empirical results showcasing\nthe implications of this property on current deep learning architectures. Our\nresults not only clarify the root causes of the extrapolation gap but also\nsuggest directions for designing next-generation forecasting models capable of\nmastering extrapolation.", "AI": {"tldr": "The paper identifies a fundamental property that explains why deep learning models struggle with extrapolation in time series forecasting, contrasting with physical laws that have strong extrapolation capabilities.", "motivation": "Foundation Models (FMs) have succeeded in short-range time series forecasting but fail at long-range extrapolation, performing worse than simple baselines. This contrasts with physical laws' strong extrapolation properties, raising questions about fundamental differences between neural networks and physical laws.", "method": "The authors identify and formalize a fundamental property characterizing statistical learning models' ability to predict accurately outside their training domain. They provide theoretical analysis and empirical results demonstrating this property's implications on current deep learning architectures.", "result": "The research clarifies the root causes of the extrapolation gap in deep learning models and shows how their performance deteriorates in extrapolation settings compared to their training domain performance.", "conclusion": "The findings not only explain the extrapolation gap but also suggest directions for designing next-generation forecasting models capable of mastering extrapolation, potentially bridging the gap between neural networks and physical laws."}}
{"id": "2510.04144", "pdf": "https://arxiv.org/pdf/2510.04144", "abs": "https://arxiv.org/abs/2510.04144", "authors": ["Nikolas Eptaminitakis", "Fran\u00e7ois Monard", "Yuzhou Joey Zou"], "title": "Tensor tomography on asymptotically hyperbolic surfaces", "categories": ["math.DG", "math.AP"], "comment": "47 pages, 2 figures", "summary": "We initiate a study of the inversion of the geodesic X-ray transform $I_m$\nover symmetric $m$-tensor fields on asymptotically hyperbolic surfaces. This\noperator has a non-trivial kernel whenever $m\\ge 1$. To propose a gauge\nrepresentative to be reconstructed from X-ray data, we first prove a\n\"tt-potential-conformal\" decomposition theorem for $m$-tensor fields (where\n\"tt\" stands for transverse traceless), previously used in integral geometry on\ncompact Riemannian manifolds with boundary in Sharafutdinov, 2007; Dairbekov\nand Sharafutdinov, 2011. The proof is based on elliptic decompositions of the\nGuillemin-Kazhdan operators $\\eta_\\pm$ (Guillemin and Kazhdan, 1980) and\nleverages in the current setting the 0-calculus of Mazzeo-Melrose (Mazzeo and\nMelrose, 1987; Mazzeo, 1991). Iterating this decomposition gives rise to an\n\"iterated-tt\" representative modulo $\\ker I_m$ for a tensor field, which is\ndistinct from the often-used solenoidal representative.\n  In the case of the Poincar\\'e disk, we show that the X-ray transform of a\ntensor in iterated-tt form splits into components that are orthogonal relative\nto a specific $L^2$ structure in data space. For even tensor fields, we provide\na full picture of the data space decomposition, in particular a range\ncharacterization of $I_{2n}$ for every $n$ in terms of moment conditions and\nspectral decay. Finally, we give explicit approaches for the reconstruction of\neven tensors in iterated-tt form from their X-ray transform or its normal\noperator, using specific knowledge of geodesically invariant distributions with\none-sided Fourier content, whose properties are analyzed in detail.", "AI": {"tldr": "Study of geodesic X-ray transform inversion for symmetric m-tensor fields on asymptotically hyperbolic surfaces, introducing an \"iterated-tt\" gauge representative distinct from solenoidal gauge.", "motivation": "The geodesic X-ray transform has a non-trivial kernel for m\u22651, requiring gauge representatives to reconstruct tensor fields from X-ray data. Previous approaches used solenoidal representatives, but this work proposes an alternative \"iterated-tt\" representative.", "method": "Proves a \"tt-potential-conformal\" decomposition theorem using elliptic decompositions of Guillemin-Kazhdan operators and Mazzeo-Melrose 0-calculus. Iterates this decomposition to create \"iterated-tt\" representatives. For Poincar\u00e9 disk, shows X-ray transform splits into orthogonal components relative to L\u00b2 structure.", "result": "For even tensor fields, provides complete data space decomposition with range characterization of I_{2n} in terms of moment conditions and spectral decay. Gives explicit reconstruction approaches for even tensors in iterated-tt form using knowledge of geodesically invariant distributions with one-sided Fourier content.", "conclusion": "Introduces a new gauge representative (iterated-tt) for tensor field reconstruction from X-ray data, distinct from solenoidal gauge, with explicit reconstruction methods and complete range characterization for even tensor fields on asymptotically hyperbolic surfaces."}}
{"id": "2510.04108", "pdf": "https://arxiv.org/pdf/2510.04108", "abs": "https://arxiv.org/abs/2510.04108", "authors": ["Ramzi Dakhmouche", "Adrien Letellier", "Hossein Gorji"], "title": "Can Linear Probes Measure LLM Uncertainty?", "categories": ["cs.LG", "cs.NA", "math.NA", "math.ST", "stat.TH"], "comment": null, "summary": "Effective Uncertainty Quantification (UQ) represents a key aspect for\nreliable deployment of Large Language Models (LLMs) in automated\ndecision-making and beyond. Yet, for LLM generation with multiple choice\nstructure, the state-of-the-art in UQ is still dominated by the naive baseline\ngiven by the maximum softmax score. To address this shortcoming, we demonstrate\nthat taking a principled approach via Bayesian statistics leads to improved\nperformance despite leveraging the simplest possible model, namely linear\nregression. More precisely, we propose to train multiple Bayesian linear\nmodels, each predicting the output of a layer given the output of the previous\none. Based on the obtained layer-level posterior distributions, we infer the\nglobal uncertainty level of the LLM by identifying a sparse combination of\ndistributional features, leading to an efficient UQ scheme. Numerical\nexperiments on various LLMs show consistent improvement over state-of-the-art\nbaselines.", "AI": {"tldr": "The paper proposes a Bayesian approach using linear regression models to improve uncertainty quantification in LLMs for multiple-choice tasks, outperforming current methods.", "motivation": "Current uncertainty quantification for LLMs in multiple-choice scenarios is dominated by naive maximum softmax score baselines, which is insufficient for reliable deployment in automated decision-making.", "method": "Train multiple Bayesian linear models that predict each layer's output from the previous layer, then use layer-level posterior distributions to infer global uncertainty through sparse combination of distributional features.", "result": "Numerical experiments on various LLMs show consistent improvement over state-of-the-art baselines in uncertainty quantification.", "conclusion": "A principled Bayesian approach with simple linear models can effectively improve uncertainty quantification in LLMs for multiple-choice generation tasks."}}
{"id": "2510.04308", "pdf": "https://arxiv.org/pdf/2510.04308", "abs": "https://arxiv.org/abs/2510.04308", "authors": ["Mikhail Molodyk", "Andr\u00e1s Vasy"], "title": "The Feynman propagator for massive Klein-Gordon fields on radiative asymptotically flat spacetimes", "categories": ["gr-qc", "math-ph", "math.AP", "math.MP", "35L05, 58J40, 81T20"], "comment": "76 pages, 8 figures", "summary": "On a large class of asymptotically flat spacetimes which includes radiative\nperturbations of Minkowski space, we define a distinguished global Feynman\npropagator for massive Klein-Gordon fields by means of the microlocal approach\nto non-elliptic Fredholm theory, working in the de,sc-pseudodifferential\nalgebra due to Sussman. We extend the limiting absorption principle (the\n\"$i\\varepsilon$ prescription\" for the Feynman propagator) to this setting.\nMotivated by the complicated Hamilton flow structure arising in this problem,\nwe also prove a new localized radial point estimate in the spirit of Haber-Vasy\nwhich, under appropriate nondegeneracy assumptions, allows one to propagate\nmicrolocal regularity into a single radial point belonging to a larger radial\nset which can be a source, sink, or saddle for the Hamilton flow.", "AI": {"tldr": "The paper defines a global Feynman propagator for massive Klein-Gordon fields on asymptotically flat spacetimes using microlocal analysis, extends the limiting absorption principle, and proves a new localized radial point estimate.", "motivation": "To establish a well-defined Feynman propagator on radiative perturbations of Minkowski space and address the complex Hamilton flow structure in such spacetimes.", "method": "Uses microlocal approach to non-elliptic Fredholm theory within Sussman's de,sc-pseudodifferential algebra, and develops a new localized radial point estimate inspired by Haber-Vasy.", "result": "Successfully defines a distinguished global Feynman propagator and extends the limiting absorption principle to asymptotically flat spacetimes with radiative perturbations.", "conclusion": "The approach provides rigorous foundations for Feynman propagators in curved spacetimes and offers new tools for handling complex Hamilton flow structures through localized radial point estimates."}}
{"id": "2510.04205", "pdf": "https://arxiv.org/pdf/2510.04205", "abs": "https://arxiv.org/abs/2510.04205", "authors": ["Di Zhang"], "title": "PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "math.OC", "68T07, 41A15, 52B11", "F.2.2; G.1.2; I.2.6"], "comment": "10", "summary": "Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to\ntraditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability\nand a strong mathematical foundation. However, their parameter efficiency\nremains a significant challenge for practical deployment. This paper introduces\nPolyKAN, a novel theoretical framework for KAN compression that provides formal\nguarantees on both model size reduction and approximation error. By leveraging\nthe inherent piecewise polynomial structure of KANs, we formulate the\ncompression problem as one of optimal polyhedral region merging. We establish a\nrigorous polyhedral characterization of KANs, develop a complete theory of\n$\\epsilon$-equivalent compression, and design an optimal dynamic programming\nalgorithm that guarantees minimal compression under specified error bounds. Our\ntheoretical analysis demonstrates that PolyKAN achieves provably minimal\ncompression while maintaining strict error control, with polynomial-time\ncomplexity in all network parameters. The framework provides the first formal\nfoundation for KAN compression with mathematical guarantees, opening new\ndirections for efficient deployment of interpretable neural architectures.", "AI": {"tldr": "PolyKAN provides a theoretical framework for compressing Kolmogorov-Arnold Networks (KANs) with formal guarantees on model size reduction and approximation error through optimal polyhedral region merging.", "motivation": "KANs offer enhanced interpretability over MLPs but suffer from parameter inefficiency, limiting practical deployment. There is a need for compression methods with mathematical guarantees.", "method": "Leverages the piecewise polynomial structure of KANs, formulates compression as optimal polyhedral region merging, establishes polyhedral characterization, develops \u03b5-equivalent compression theory, and designs an optimal dynamic programming algorithm.", "result": "PolyKAN achieves provably minimal compression with strict error control and polynomial-time complexity, providing the first formal foundation for KAN compression with mathematical guarantees.", "conclusion": "The framework enables efficient deployment of interpretable neural architectures and opens new directions for KAN compression with rigorous mathematical foundations."}}
{"id": "2510.04345", "pdf": "https://arxiv.org/pdf/2510.04345", "abs": "https://arxiv.org/abs/2510.04345", "authors": ["Anthony Carbery", "Zane Kun Li", "Yixuan Pang", "Po-Lam Yung"], "title": "A weighted formulation of refined decoupling and inequalities of Mizohata-Takeuchi-type for the moment curve", "categories": ["math.CA", "math.AP"], "comment": "47 pages", "summary": "Let $\\Gamma$ be a compact patch of a well-curved $C^{n+1}$ curve in\n$\\mathbb{R}^n$ with induced Lebesgue measure ${\\rm d} \\lambda$, and let $g\n\\mapsto \\widehat{g \\,{\\rm d}\\lambda}$ be the Fourier extension operator for\n$\\Gamma$. Then we have, for arbitrary non-negative weights $w$,\n\\begin{equation*}\n  \\int_{B_R} |\\widehat{g \\,{\\rm d}\\lambda}|^2w \\leq C_{n,a} R^{a} \\sup_S\n\\left(\\int_S w\\right)\\int_\\Gamma |g|^2 \\, {\\rm d} \\lambda\n  \\end{equation*} for any $a> \\frac{n-3}{2} + \\frac{2}{n} -\n\\frac{2}{n^2(n+1)}$, where the $\\sup$ is over all $1$-neighbourhoods $S$ of\nhyperplanes whose normals are parallel to the tangent at some point of\n$\\Gamma$. This represents partial progress on the Mizohata-Takeuchi conjecture\nfor curves in dimensions $n \\geq 3$, improving upon the exponent $a=n-1$ which\ncan be obtained as a consequence of the Agmon-H\\\"ormander trace inequality. Our\nmain tool in establishing this inequality will be a weighted formulation of\nrefined decoupling for well-curved curves. We also discuss the sharpness of the\nexponents we obtain in this and in auxiliary results, and further explore this\nin the context of axiomatic decoupling for curves.", "AI": {"tldr": "The paper proves improved Fourier extension estimates for curves in dimensions n\u22653, making partial progress on the Mizohata-Takeuchi conjecture by establishing a weighted inequality with exponent a > (n-3)/2 + 2/n - 2/(n\u00b2(n+1)), which improves upon the previous bound a=n-1 from the Agmon-H\u00f6rmander trace inequality.", "motivation": "To make progress on the Mizohata-Takeuchi conjecture for curves in higher dimensions (n\u22653), which concerns Fourier extension estimates and has implications for understanding restriction phenomena in harmonic analysis.", "method": "Uses weighted formulation of refined decoupling for well-curved curves as the main tool, building on axiomatic decoupling theory for curves.", "result": "Established improved weighted Fourier extension inequality with exponent a > (n-3)/2 + 2/n - 2/(n\u00b2(n+1)) for curves in R\u207f, representing partial progress on the Mizohata-Takeuchi conjecture.", "conclusion": "The paper provides improved bounds for Fourier extension on curves in dimensions n\u22653, discusses sharpness of exponents, and explores connections with axiomatic decoupling theory, advancing our understanding of restriction phenomena."}}
{"id": "2510.04322", "pdf": "https://arxiv.org/pdf/2510.04322", "abs": "https://arxiv.org/abs/2510.04322", "authors": ["Akshay Govind Srinivasan", "Anuj Jagannath Said", "Sathwik Pentela", "Vikas Dwivedi", "Balaji Srinivasan"], "title": "Towards Fast Option Pricing PDE Solvers Powered by PIELM", "categories": ["cs.CE", "cs.LG", "cs.NA", "math.NA", "J.2; I.6.3; G.1.7; G.1.8"], "comment": "6 Pages, 5 Figures, 3 Tables", "summary": "Partial differential equation (PDE) solvers underpin modern quantitative\nfinance, governing option pricing and risk evaluation. Physics-Informed Neural\nNetworks (PINNs) have emerged as a promising approach for solving the forward\nand inverse problems of partial differential equations (PDEs) using deep\nlearning. However they remain computationally expensive due to their iterative\ngradient descent based optimization and scale poorly with increasing model\nsize. This paper introduces Physics-Informed Extreme Learning Machines (PIELMs)\nas fast alternative to PINNs for solving both forward and inverse problems in\nfinancial PDEs. PIELMs replace iterative optimization with a single\nleast-squares solve, enabling deterministic and efficient training. We\nbenchmark PIELM on the Black-Scholes and Heston-Hull-White models for forward\npricing and demonstrate its capability in inverse model calibration to recover\nvolatility and interest rate parameters from noisy data. From experiments we\nobserve that PIELM achieve accuracy comparable to PINNs while being up to\n$30\\times$ faster, highlighting their potential for real-time financial\nmodeling.", "AI": {"tldr": "PIELMs provide a faster alternative to PINNs for solving financial PDEs by replacing iterative optimization with single least-squares solves, achieving comparable accuracy with 30x speedup.", "motivation": "PINNs are computationally expensive and scale poorly with model size due to iterative gradient descent optimization, limiting their practical use in real-time financial modeling.", "method": "Physics-Informed Extreme Learning Machines (PIELMs) replace the iterative optimization of PINNs with a single least-squares solve for deterministic and efficient training.", "result": "PIELMs achieve accuracy comparable to PINNs while being up to 30x faster on Black-Scholes and Heston-Hull-White models for both forward pricing and inverse calibration tasks.", "conclusion": "PIELMs offer significant computational advantages over PINNs for financial PDE solving, making them suitable for real-time applications in quantitative finance."}}
{"id": "2510.04351", "pdf": "https://arxiv.org/pdf/2510.04351", "abs": "https://arxiv.org/abs/2510.04351", "authors": ["Jo\u00e3o H. Andrade", "Jeffrey S. Case", "Paolo Piccione", "Juncheng Wei"], "title": "Nonhomothetic complete periodic metrics with constant scalar curvature", "categories": ["math.DG", "math.AP"], "comment": "This is a companion paper to arXiv:2310.15798 which discusses only\n  the case of the Yamabe and singular Yamabe problem for constant scalar\n  curvature metrics. We split the papers in two due to the broader interest in\n  scalar curvature; this paper also includes a more detailed discussion of\n  nonuniqueness. 13 pages", "summary": "We show that there are infinitely many pairwise nonhomothetic, complete,\nperiodic metrics with constant scalar curvature that are conformal to the round\nmetric on $S^n\\setminus S^k$, where $k < \\frac{n-2}{2}$. These metrics are\nobtained by pulling back Yamabe metrics defined on products of $S^{n-k-1}$ and\ncompact hyperbolic $(k+1)$-manifolds. Our main result proves that these\nsolutions are generically distinct up to homothety. The core of our argument\nrelies on classical rigidity theorems due to Obata and Ferrand, which\ncharacterize the round sphere by its conformal group.", "AI": {"tldr": "Infinitely many nonhomothetic, complete periodic metrics with constant scalar curvature conformal to round metric on S^n\\S^k, obtained from Yamabe metrics on products of spheres and hyperbolic manifolds.", "motivation": "To construct and classify constant scalar curvature metrics on punctured spheres, extending understanding of Yamabe metrics and conformal geometry.", "method": "Pull back Yamabe metrics from products of S^{n-k-1} and compact hyperbolic (k+1)-manifolds, using classical rigidity theorems by Obata and Ferrand.", "result": "Infinitely many pairwise nonhomothetic solutions exist, generically distinct up to homothety, on S^n\\S^k for k < (n-2)/2.", "conclusion": "The construction provides a rich family of constant scalar curvature metrics, with distinctness established through conformal group characterization of round spheres."}}
{"id": "2510.04323", "pdf": "https://arxiv.org/pdf/2510.04323", "abs": "https://arxiv.org/abs/2510.04323", "authors": ["Amod Agashe", "Matthew Winters"], "title": "Reducibility and rational torsion in modular abelian varieties", "categories": ["math.NT", "cs.NA", "math.NA"], "comment": null, "summary": "Let N be a square-free positive integer and let f be a newform of weight 2 on\n\\Gamma_0(N). Let A denote the abelian subvariety of J_0(N) associated to f and\nlet m be a maximal ideal of the Hecke algebra T that contains Ann_T(f) and has\nresidue characteristic r such that r does not divide 6N. We show that if either\nA[m] or the canonical representation \\rho_m over T/m associated to m is\nreducible, then r divides the order of the cuspidal subgroup of J_0(N) and A[m]\nhas a nontrivial rational point. We mention some applications of this result,\nincluding an application to the second part of the Birch and Swinnerton-Dyer\nconjecture for A.", "AI": {"tldr": "This paper shows that for square-free N and weight 2 newforms f, if certain Galois representations are reducible, then the prime r divides the cuspidal subgroup order and there exists a nontrivial rational point in A[m].", "motivation": "To understand connections between reducibility of Galois representations, cuspidal subgroups, and rational points on abelian varieties, with applications to the Birch and Swinnerton-Dyer conjecture.", "method": "Using properties of newforms, abelian varieties associated to modular forms, Hecke algebras, and Galois representations to establish divisibility conditions and existence of rational points.", "result": "When either A[m] or the canonical representation \u03c1_m is reducible, then r divides the order of the cuspidal subgroup of J\u2080(N) and A[m] has a nontrivial rational point.", "conclusion": "The reducibility of certain Galois representations implies arithmetic consequences for cuspidal subgroups and rational points, with implications for the Birch and Swinnerton-Dyer conjecture."}}
{"id": "2510.04481", "pdf": "https://arxiv.org/pdf/2510.04481", "abs": "https://arxiv.org/abs/2510.04481", "authors": ["Otis Chodosh"], "title": "Minimal surfaces and comparison geometry", "categories": ["math.DG", "math.AP"], "comment": "submitted to the Proceedings of the ICM 2026", "summary": "We discuss applications of minimal surfaces to comparison geometry.", "AI": {"tldr": "Applications of minimal surfaces in comparison geometry", "motivation": "To explore how minimal surfaces can be used in comparison geometry", "method": "Discussion of applications and connections between minimal surfaces and comparison geometry", "result": "Established relationships and applications of minimal surfaces in geometric comparison contexts", "conclusion": "Minimal surfaces provide valuable tools and insights for comparison geometry"}}
{"id": "2510.04346", "pdf": "https://arxiv.org/pdf/2510.04346", "abs": "https://arxiv.org/abs/2510.04346", "authors": ["Nahshon Mokua Obiri", "Kristof Van Laerhoven"], "title": "Environment-Aware Indoor LoRaWAN Path Loss: Parametric Regression Comparisons, Shadow Fading, and Calibrated Fade Margins", "categories": ["cs.NI", "cs.LG", "cs.NA", "eess.SP", "math.NA"], "comment": "Code: https://github.com/nahshonmokua/LoRaWAN-Indoor-PL-parametrics", "summary": "Indoor LoRaWAN propagation is shaped by structural and time-varying context\nfactors, which challenge log-distance models and the assumption of log-normal\nshadowing. We present an environment-aware, statistically disciplined path loss\nframework evaluated using leakage-safe cross-validation on a 12-month campaign\nin an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is\naugmented with environmental covariates (relative humidity, temperature, carbon\ndioxide, particulate matter, and barometric pressure), as well as the\nsignal-to-noise ratio. We compare multiple linear regression with regularized\nvariants, Bayesian linear regression, and a selective second-order polynomial\napplied to continuous drivers. Predictor relevance is established using\nheteroscedasticity-robust Type II and III analysis of variance and nested\npartial F tests. Shadow fading is profiled with kernel density estimation and\nnon-parametric families, including Normal, Skew-Normal, Student's t, and\nGaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07\nto 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are\nnon-Gaussian; a 3-component mixture captures a sharp core with a light, broad\ntail. We convert accuracy into reliability by prescribing the fade margin as\nthe upper-tail quantile of cross-validated residuals, quantifying uncertainty\nvia a moving-block bootstrap, and validating on a held-out set. At 99% packet\ndelivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7\nto 27.9 dB for linear baselines. This result presents a deployment-ready,\ninterpretable workflow with calibrated reliability control for indoor Internet\nof Things planning, aligned with 6G targets.", "AI": {"tldr": "The paper presents an environment-aware path loss framework for indoor LoRaWAN that improves prediction accuracy and reliability by incorporating environmental factors and using polynomial models, reducing required fade margins for 99% packet delivery.", "motivation": "Traditional log-distance models for indoor LoRaWAN propagation are challenged by structural and time-varying environmental factors, leading to inaccurate predictions and unreliable network planning.", "method": "Used a 12-month measurement campaign in an office environment, augmented log-distance multi-wall mean with environmental covariates, compared multiple regression methods, analyzed predictor relevance with ANOVA, and modeled shadow fading with various distributions including Gaussian mixtures.", "result": "The polynomial mean reduced cross-validated RMSE from 8.07 to 7.09 dB and increased R\u00b2 from 0.81 to 0.86. For 99% packet delivery ratio, the environment-aware polynomial required only 25.7 dB fade margin compared to 27.7-27.9 dB for linear baselines.", "conclusion": "The framework provides a deployment-ready, interpretable workflow with calibrated reliability control for indoor IoT planning, aligning with 6G targets by improving prediction accuracy and reducing required fade margins."}}
{"id": "2510.04557", "pdf": "https://arxiv.org/pdf/2510.04557", "abs": "https://arxiv.org/abs/2510.04557", "authors": ["Huiqiu Lin", "Lianping Liu", "Zhe You", "Da Zhao"], "title": "Estimates of the first Dirichlet eigenvalue of graphs", "categories": ["math.CO", "math.AP", "05C50, 35R02, 47A75, 49J40, 49R05"], "comment": "24 pages, 2 figures", "summary": "Inspired by the Li--Yau eigenvalue-diameter estimates, we investigate lower\nbounds for the first Dirichlet eigenvalue in terms of the diameter (or\ninscribed radius) of a graph. Let $G = (V, E)$ be a graph with boundary $B$.\nAssume that the interior $\\Omega = V \\setminus B$ is connected. Let $r$ be the\ninscribed radius of $(G, B)$ and $d$ be the maximum degree of $G$. We prove\nthat $$\\lambda_1(G, B) \\geq \\frac{d - 1}{r d^r},$$ which can be viewed as an\nanalogue of the Lin--Yau bound and the Meng--Lin bound for normalized\nDirichlet/Laplacian eigenvalues. We also derive the inequality $$\\lambda_1(G,\nB) \\geq \\frac{1}{r |\\Omega|}.$$ In particular, for a tree $T$ with at least $3$\nvertices, we show that $$\\lambda_1(T) \\geq 4 \\sin^2 \\frac{\\pi}{4r + 6} \\geq\n\\frac{1}{(r + 1)^2}.$$ Notably, both of the two preceding bounds are sharp up\nto a constant factor. We additionally examine upper bounds on the first\nDirichlet eigenvalue under constraints on the numbers of interior and boundary\nvertices.", "AI": {"tldr": "The paper establishes lower bounds for the first Dirichlet eigenvalue of graphs in terms of diameter and inscribed radius, proving sharp inequalities that generalize classical Li-Yau estimates to discrete settings.", "motivation": "The research is motivated by extending classical Li-Yau eigenvalue-diameter estimates from Riemannian geometry to discrete graphs, seeking analogous bounds for Dirichlet eigenvalues in terms of geometric parameters like inscribed radius and diameter.", "method": "The authors use graph-theoretic and combinatorial methods to derive eigenvalue bounds, working with graphs having boundary vertices and connected interiors. They employ inscribed radius, maximum degree, and vertex counts as key parameters in their analysis.", "result": "Several sharp lower bounds are proven: \u03bb\u2081(G,B) \u2265 (d-1)/(r d^r) where d is maximum degree and r is inscribed radius; \u03bb\u2081(G,B) \u2265 1/(r|\u03a9|); and for trees with \u22653 vertices, \u03bb\u2081(T) \u2265 4 sin\u00b2(\u03c0/(4r+6)) \u2265 1/(r+1)\u00b2. All bounds are sharp up to constant factors.", "conclusion": "The paper successfully establishes discrete analogues of classical Li-Yau bounds for Dirichlet eigenvalues, providing sharp geometric estimates that connect spectral properties with graph diameter and inscribed radius, with particularly strong results for trees."}}
{"id": "2510.04369", "pdf": "https://arxiv.org/pdf/2510.04369", "abs": "https://arxiv.org/abs/2510.04369", "authors": ["Bernadette Hahn", "Gael Rigaud", "Richard Schm\u00e4hl"], "title": "The method of the approximate inverse for limited-angle CT", "categories": ["eess.IV", "cs.CV", "cs.NA", "math.NA"], "comment": null, "summary": "Limited-angle computerized tomography stands for one of the most difficult\nchallenges in imaging. Although it opens the way to faster data acquisition in\nindustry and less dangerous scans in medicine, standard approaches, such as the\nfiltered backprojection (FBP) algorithm or the widely used total-variation\nfunctional, often produce various artefacts that hinder the diagnosis. With the\nrise of deep learning, many modern techniques have proven themselves successful\nin removing such artefacts but at the cost of large datasets. In this paper, we\npropose a new model-driven approach based on the method of the approximate\ninverse, which could serve as new starting point for learning strategies in the\nfuture. In contrast to FBP-type approaches, our reconstruction step consists in\nevaluating linear functionals on the measured data using reconstruction kernels\nthat are precomputed as solution of an auxiliary problem. With this problem\nbeing uniquely solvable, the derived limited-angle reconstruction kernel (LARK)\nis able to fully reconstruct the object without the well-known streak\nartefacts, even for large limited angles. However, it inherits severe\nill-conditioning which leads to a different kind of artefacts arising from the\nsingular functions of the limited-angle Radon transform. The problem becomes\nparticularly challenging when working on semi-discrete (real or analytical)\nmeasurements. We develop a general regularization strategy, named constrained\nlimited-angle reconstruction kernel (CLARK), by combining spectral filter, the\nmethod of the approximate inverse and custom edge-preserving denoising in order\nto stabilize the whole process. We further derive and interpret error estimates\nfor the application on real, i.e. semi-discrete, data and we validate our\napproach on synthetic and real data.", "AI": {"tldr": "Proposes CLARK, a model-driven approach for limited-angle CT reconstruction that combines spectral filtering, approximate inverse method, and edge-preserving denoising to eliminate streak artifacts while handling ill-conditioning from limited-angle data.", "motivation": "Limited-angle CT enables faster acquisition and safer scans but standard methods like FBP and total-variation produce artifacts. Deep learning alternatives require large datasets, motivating a model-driven approach.", "method": "Uses reconstruction kernels precomputed as solutions to auxiliary problems (LARK), then develops CLARK with spectral filtering, approximate inverse method, and custom edge-preserving denoising to stabilize the ill-conditioned reconstruction.", "result": "The method fully reconstructs objects without streak artifacts even for large limited angles, though it inherits ill-conditioning that requires regularization. Validated on synthetic and real data.", "conclusion": "CLARK provides an effective model-driven alternative to deep learning for limited-angle CT reconstruction, serving as a potential starting point for future learning strategies while handling the inherent ill-conditioning of limited-angle problems."}}
{"id": "2510.04702", "pdf": "https://arxiv.org/pdf/2510.04702", "abs": "https://arxiv.org/abs/2510.04702", "authors": ["Atsuhide Ishida"], "title": "Inverse scattering for $N$-body time-decaying harmonic oscillators", "categories": ["math-ph", "math.AP", "math.MP"], "comment": null, "summary": "In the previous study (Ishida, 2025), the author proved the uniqueness of\nshort-range potential functions using the Enss-Weder time-dependent method\n(Enss and Weder, 1995) for a two-body quantum system described by time-decaying\nharmonic oscillators. In this study, we extend the result of Ishida (2025) to\nthe $N$-body case. We use the approaches developed in Enss and Weder (1995),\nWeder (1996), and Valencia and Weder (2012) to prove that the high-velocity\nlimit of the scattering operator uniquely determines all the pairwise\ninteraction potentials among the $N$ particles, focusing respectively on each\nfixed pair of particles.", "AI": {"tldr": "Extension of uniqueness proof for potential functions from two-body to N-body quantum systems using high-velocity scattering operator limits.", "motivation": "To generalize previous uniqueness results for short-range potential functions from two-body quantum systems to N-body systems with time-decaying harmonic oscillators.", "method": "Uses Enss-Weder time-dependent method and approaches from Enss-Weder (1995), Weder (1996), and Valencia-Weder (2012) to analyze high-velocity limit of scattering operator.", "result": "Proves that high-velocity limit of scattering operator uniquely determines all pairwise interaction potentials among N particles.", "conclusion": "Successfully extends uniqueness theorem for potential functions to N-body quantum systems, with focus on determining each fixed pair of particles' interaction potentials."}}
{"id": "2510.04382", "pdf": "https://arxiv.org/pdf/2510.04382", "abs": "https://arxiv.org/abs/2510.04382", "authors": ["Wojciech G\u00f3rny", "Micha\u0142 \u0141asica", "Alexandros Matsoukas"], "title": "Adaptive double-phase Rudin--Osher--Fatemi denoising model", "categories": ["eess.IV", "cs.CV", "cs.NA", "math.NA"], "comment": "21 pages, 18 figures, supplementary material available at:\n  https://github.com/wojciechgorny/double-phase-ROF-model/", "summary": "We propose a new image denoising model based on a variable-growth total\nvariation regularization of double-phase type with adaptive weight. It is\ndesigned to reduce staircasing with respect to the classical\nRudin--Osher--Fatemi model, while preserving the edges of the image in a\nsimilar fashion. We implement the model and test its performance on synthetic\nand natural images in 1D and 2D over a range of noise levels.", "AI": {"tldr": "A new image denoising model using variable-growth total variation regularization with adaptive weight to reduce staircasing while preserving edges.", "motivation": "To address the staircasing effect in the classical Rudin-Osher-Fatemi model while maintaining good edge preservation capabilities.", "method": "Variable-growth total variation regularization of double-phase type with adaptive weight, implemented and tested on synthetic and natural images in 1D and 2D.", "result": "The model was tested across various noise levels and showed improved performance in reducing staircasing artifacts.", "conclusion": "The proposed model effectively reduces staircasing while preserving image edges, making it a promising alternative to traditional total variation denoising methods."}}
{"id": "2510.04893", "pdf": "https://arxiv.org/pdf/2510.04893", "abs": "https://arxiv.org/abs/2510.04893", "authors": ["Patricio Guzm\u00e1n", "Agust\u00edn Huerta", "Hugo Parada"], "title": "Rapid stabilization for a wave equation with boundary disturbance", "categories": ["math.OC", "cs.SY", "eess.SY", "math.AP"], "comment": null, "summary": "In this paper, we study the rapid stabilization of an unstable wave equation,\nin which an unknown disturbance is located at the boundary condition. We\naddress two different boundary conditions: Dirichlet- Dirichlet and\nDirichlet-Neumann. In both cases, we design a feedback law, located at the same\nplace as the unknown disturbance, that forces the exponential decay of the\nenergy for any desired decay rate while suppressing the effects of the unknown\ndisturbance. For the feedback design, we employ the backstepping method,\nLyapunov techniques and the sign multivalued operator. The well-posedness of\nthe closed-loop system, which is a differential inclusion, is shown with the\nmaximal monotone operator theory.", "AI": {"tldr": "Rapid stabilization of unstable wave equations with unknown boundary disturbances using backstepping and Lyapunov methods.", "motivation": "Address stabilization of unstable wave equations when unknown disturbances affect boundary conditions, requiring robust control that suppresses disturbance effects.", "method": "Used backstepping method, Lyapunov techniques, and sign multivalued operator for feedback design; employed maximal monotone operator theory for well-posedness analysis.", "result": "Designed feedback laws that achieve exponential decay of energy at any desired rate while suppressing unknown boundary disturbances for both Dirichlet-Dirichlet and Dirichlet-Neumann cases.", "conclusion": "Successfully developed stabilization methods for wave equations with unknown boundary disturbances, proving well-posedness of the resulting differential inclusion system."}}
{"id": "2510.04894", "pdf": "https://arxiv.org/pdf/2510.04894", "abs": "https://arxiv.org/abs/2510.04894", "authors": ["Louis-Pierre Chaintron", "Antoine Diez"], "title": "Mean-field limits \u00e0 la Tanaka and large deviations for particle systems with network interactions", "categories": ["math.PR", "math.AP", "35Q70, 82C22, 82C31, 60H10, 60H30, 60F10, 05C80"], "comment": null, "summary": "This article proposes a unified framework to study non-exchangeable\nmean-field particle systems with some general interaction mechanisms. The\nstarting point is a fixed-point formulation of particle systems originally due\nto Tanaka that allows us to prove mean-field limit and large deviation results\nin an abstract setting. While it has been recently shown that such formulation\nencompasses a large class of exchangeable particle systems, we propose here a\nsetting for the non-exchangeable case, including the case of adaptive\ninteraction networks. We introduce sufficient conditions on the network\nstructure that imply the mean-field limit and a new large deviations principle\nfor the interaction measure. Finally, we formally highlight important models\nfor which it is possible to derive a closed PDE characterization of the limit.", "AI": {"tldr": "A unified framework for non-exchangeable mean-field particle systems with general interactions, extending Tanaka's fixed-point formulation to non-exchangeable cases including adaptive networks.", "motivation": "To extend mean-field analysis beyond exchangeable particle systems to include non-exchangeable cases with adaptive interaction networks, which are common in real-world applications.", "method": "Uses Tanaka's fixed-point formulation of particle systems, introduces sufficient conditions on network structure, and develops abstract framework for proving mean-field limits and large deviations.", "result": "Establishes mean-field limit and new large deviations principle for interaction measures in non-exchangeable particle systems with adaptive networks.", "conclusion": "The framework successfully extends mean-field theory to non-exchangeable systems and provides conditions for deriving PDE characterizations of the limit in important models."}}
{"id": "2510.04478", "pdf": "https://arxiv.org/pdf/2510.04478", "abs": "https://arxiv.org/abs/2510.04478", "authors": ["Hongli Zhao", "Mihai Anitescu", "Sen Na"], "title": "Overlapping Schwarz Scheme for Linear-Quadratic Programs in Continuous Time", "categories": ["math.OC", "cs.CE", "cs.DC", "cs.NA", "math.DS", "math.NA"], "comment": "34 pages, 2 figures", "summary": "We present an optimize-then-discretize framework for solving linear-quadratic\noptimal control problems (OCP) governed by time-inhomogeneous ordinary\ndifferential equations (ODEs). Our method employs a modified overlapping\nSchwarz decomposition based on the Pontryagin Minimum Principle, partitioning\nthe temporal domain into overlapping intervals and independently solving\nHamiltonian systems in continuous time. We demonstrate that the convergence is\nensured by appropriately updating the boundary conditions of the individual\nHamiltonian dynamics. The cornerstone of our analysis is to prove that the\nexponential decay of sensitivity (EDS) exhibited in discrete-time OCPs carries\nover to the continuous-time setting. Unlike the discretize-then-optimize\napproach, our method can flexibly incorporate different numerical integration\nmethods for solving the resulting Hamiltonian two-point boundary-value\nsubproblems, including adaptive-time integrators. A numerical experiment on a\nlinear-quadratic OCP illustrates the practicality of our approach in broad\nscientific applications.", "AI": {"tldr": "An optimize-then-discretize framework for linear-quadratic optimal control problems using overlapping Schwarz decomposition based on Pontryagin Minimum Principle, with convergence ensured by boundary condition updates and exponential decay of sensitivity.", "motivation": "To develop a flexible framework for solving linear-quadratic optimal control problems governed by time-inhomogeneous ODEs that can incorporate different numerical integration methods, unlike the discretize-then-optimize approach.", "method": "Modified overlapping Schwarz decomposition based on Pontryagin Minimum Principle, partitioning temporal domain into overlapping intervals and independently solving Hamiltonian systems in continuous time with boundary condition updates.", "result": "The method ensures convergence by appropriately updating boundary conditions and proves that exponential decay of sensitivity from discrete-time OCPs carries over to continuous-time setting.", "conclusion": "The framework provides a practical approach for linear-quadratic OCPs that can flexibly use different numerical integration methods, including adaptive-time integrators, as demonstrated in numerical experiments."}}
{"id": "2510.04928", "pdf": "https://arxiv.org/pdf/2510.04928", "abs": "https://arxiv.org/abs/2510.04928", "authors": ["Mingyang Li", "Hongyi Liu"], "title": "Poincar\u00e9-Einstein 4-manifolds with conformally K\u00e4hler geometry", "categories": ["math.DG", "math-ph", "math.AP", "math.MP"], "comment": null, "summary": "We study 4-dimensional Poincar\\'e-Einstein manifolds whose conformal class\ncontains a K\\\"ahler metric. Such Einstein metrics are non-K\\\"ahler and admit a\nKilling field extending to the conformal infinity, and the Einstein equation\nreduces to a Toda-type equation. When the Killing field integrates to an\n$\\mathbb{S}^1$-action, we formulate a Dirichlet boundary value problem and\nestablish existence and uniqueness theory. This construction provides a\nnon-perturbative realization of infinite-dimensional families of new\nPoincar\\'e-Einstein metrics whose conformal infinities are of non-positive\nYamabe type.", "AI": {"tldr": "Study of 4D Poincar\u00e9-Einstein manifolds with K\u00e4hler conformal classes, where Einstein metrics are non-K\u00e4hler and admit Killing fields. When the Killing field integrates to an S\u00b9-action, a Dirichlet boundary value problem yields infinite-dimensional families of new Poincar\u00e9-Einstein metrics with non-positive Yamabe type conformal infinities.", "motivation": "To investigate Poincar\u00e9-Einstein manifolds whose conformal class contains a K\u00e4hler metric, exploring the relationship between Einstein metrics and K\u00e4hler structures in 4 dimensions.", "method": "Formulate a Dirichlet boundary value problem when the Killing field integrates to an S\u00b9-action, using Toda-type equations derived from the Einstein equation. Establish existence and uniqueness theory for this construction.", "result": "Construction provides non-perturbative realization of infinite-dimensional families of new Poincar\u00e9-Einstein metrics whose conformal infinities are of non-positive Yamabe type.", "conclusion": "The approach successfully generates infinite-dimensional families of Poincar\u00e9-Einstein metrics with specific geometric properties, demonstrating the existence of such structures in the non-K\u00e4hler Einstein setting."}}
{"id": "2510.04831", "pdf": "https://arxiv.org/pdf/2510.04831", "abs": "https://arxiv.org/abs/2510.04831", "authors": ["Boyang Wu", "Miguel Onorato", "Zaher Hani", "Yulin Pan"], "title": "Validity condition of normal form transformation for the $\u03b2$-FPUT system", "categories": ["math-ph", "cs.NA", "math.MP", "math.NA"], "comment": "12 pages, 1 figure", "summary": "In this work, we provide a validity condition for the normal form\ntransformation to remove the non-resonant cubic terms in the $\\beta$-FPUT\nsystem. We show that for a wave field with random phases, the normal form\ntransformation is valid by dominant probability if $\\beta \\ll\n1/N^{1+\\epsilon}$, with $N$ the number of masses and $\\epsilon$ an arbitrarily\nsmall constant. To obtain this condition, a bound is needed for a summation in\nthe transformation equation, which we prove rigorously in the paper. The\ncondition also suggests that the importance of the non-resonant terms in the\nevolution equation is governed by the parameter $\\beta N$. We design numerical\nexperiments to demonstrate that this is indeed the case for spectra at both\nthermal-equilibrium and out-of-equilibrium conditions. The methodology\ndeveloped in this paper is applicable to other Hamiltonian systems where a\nnormal form transformation needs to be applied.", "AI": {"tldr": "The paper establishes a validity condition for normal form transformation in the \u03b2-FPUT system, showing it's valid with high probability when \u03b2 \u226a 1/N^(1+\u03b5), and demonstrates that \u03b2N governs the importance of non-resonant terms.", "motivation": "To provide rigorous validity conditions for normal form transformations in Hamiltonian systems, specifically addressing when non-resonant cubic terms can be removed in the \u03b2-FPUT system.", "method": "Developed a bound for a summation in the transformation equation, proved rigorously, and designed numerical experiments to test the condition for both thermal-equilibrium and out-of-equilibrium spectra.", "result": "The normal form transformation is valid with dominant probability when \u03b2 \u226a 1/N^(1+\u03b5), and numerical experiments confirm that \u03b2N governs the importance of non-resonant terms.", "conclusion": "The methodology is applicable to other Hamiltonian systems requiring normal form transformations, and the parameter \u03b2N determines the significance of non-resonant terms in the evolution equation."}}
{"id": "2510.04995", "pdf": "https://arxiv.org/pdf/2510.04995", "abs": "https://arxiv.org/abs/2510.04995", "authors": ["Xuefeng Xu", "Graham Cormode"], "title": "Power Transform Revisited: Numerically Stable, and Federated", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": "25 pages", "summary": "Power transforms are popular parametric techniques for making data more\nGaussian-like, and are widely used as preprocessing steps in statistical\nanalysis and machine learning. However, we find that direct implementations of\npower transforms suffer from severe numerical instabilities, which can lead to\nincorrect results or even crashes. In this paper, we provide a comprehensive\nanalysis of the sources of these instabilities and propose effective remedies.\nWe further extend power transforms to the federated learning setting,\naddressing both numerical and distributional challenges that arise in this\ncontext. Experiments on real-world datasets demonstrate that our methods are\nboth effective and robust, substantially improving stability compared to\nexisting approaches.", "AI": {"tldr": "Power transforms suffer from numerical instabilities in direct implementations. This paper analyzes the instability sources, proposes remedies, and extends power transforms to federated learning.", "motivation": "Power transforms are widely used for making data Gaussian-like in preprocessing, but direct implementations have severe numerical instabilities that can cause incorrect results or crashes.", "method": "Comprehensive analysis of instability sources and development of effective remedies. Extension of power transforms to federated learning setting, addressing both numerical and distributional challenges.", "result": "Experiments on real-world datasets show the methods are effective and robust, with substantial stability improvements compared to existing approaches.", "conclusion": "The proposed methods successfully address numerical instabilities in power transforms and enable their reliable use in federated learning contexts."}}
