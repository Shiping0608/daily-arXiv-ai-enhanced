<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 15]
- [math.AP](#math.AP) [Total: 10]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [hep-ex](#hep-ex) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Finite element approximation to linear, second order, parabolic problems with $L^1$ data](https://arxiv.org/abs/2510.05331)
*Gabriel Barrenechea,Abner J. Salgado*

Main category: math.NA

TL;DR: The paper analyzes convergence of implicit Euler scheme with mass lumping for heat equation with L^1 data, proving convergence to renormalized solution under inverse CFL condition.


<details>
  <summary>Details</summary>
Motivation: To handle heat equations with low integrability data (L^1 right hand side and initial conditions) where standard solutions may not be well-posed, requiring renormalized solutions for well-posedness.

Method: Uses standard implicit Euler scheme with mass lumping under an inverse CFL condition to approximate the renormalized solution of the heat equation.

Result: Proves convergence in L^∞(0,T;L^1(Ω)) and L^q(0,T;W^{1,q}_0(Ω)) for q < (d+2)/(d+1) to the renormalized solution.

Conclusion: The implicit Euler scheme with mass lumping provides a valid numerical approximation for heat equations with L^1 data when solutions are understood in the renormalized sense.

Abstract: We consider the approximation to the solution of the initial boundary value
problem for the heat equation with right hand side and initial condition that
merely belong to $L^1$. Due to the low integrability of the data, to guarantee
well-posedness, we must understand solutions in the renormalized sense. We
prove that, under an inverse CFL condition, the solution of the standard
implicit Euler scheme with mass lumping converges, in
$L^\infty(0,T;L^1(\Omega))$ and $L^q(0,T;W^{1,q}_0(\Omega))$
($q<\tfrac{d+2}{d+1}$), to the renormalized solution of the problem.

</details>


### [2] [Domain Decomposition-Based Coupling of High-Fidelity Finite Element and Reduced Order Operator Inference Models Using the Schwarz Alternating Method](https://arxiv.org/abs/2510.05350)
*Ian Moore,Anthony Gruber,Chris Wentland,Irina Tezaur*

Main category: math.NA

TL;DR: Hybrid domain decomposition method coupling finite element models with reduced order models using Schwarz alternating method for accelerated computation with geometry flexibility.


<details>
  <summary>Details</summary>
Motivation: To accelerate the Schwarz process while maintaining geometry and mesh flexibility in solving complex PDE problems.

Method: Couples sub-domain-local high-fidelity FE models with non-intrusive Operator Inference ROMs using Schwarz alternating method.

Result: Achieved stable and accurate predictive solutions for convection-dominated convection-diffusion-reaction problem with improved ROM training process.

Conclusion: The OpInf-FE method effectively accelerates Schwarz process while enabling geometry flexibility and stable solutions for complex PDE problems.

Abstract: We propose a novel hybrid domain decomposition method that couples
sub-domain-local high-fidelity finite element (FE) models with reduced order
models (ROMs) using the Schwarz alternating method. By integrating the
noninstrusive Operator Inference (OpInf) ROM, our approach accelerates the
Schwarz process while allowing for geometry and mesh flexibility. We
demonstrate the effectiveness of the new OpInf-FE method on a
convection-dominated convection-diffusion-reaction problem, achieving stable
and accurate predictive solutions while improving the ROM training process.

</details>


### [3] [A highly efficient second-order accurate long-time dynamics preserving scheme for some geophysical fluid models](https://arxiv.org/abs/2510.05360)
*Daozhi Han,Xiaoming Wang*

Main category: math.NA

TL;DR: A second-order time-marching scheme for geophysical fluid models that efficiently approximates invariant measures and ensures long-time stability with minimal computational cost per step.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for long-time simulations of geophysical fluid models that accurately captures the stationary statistical properties (climate) of the underlying dynamical system while maintaining computational efficiency.

Method: Uses a mean-reverting scalar auxiliary variable (mr-SAV) formulation that preserves dissipative structure, requires solving only a fixed symmetric positive-definite linear system with constant coefficients at each step, and employs fractional-order function spaces for compactness.

Result: The scheme achieves second-order accuracy in time, guarantees long-time stability with uniformly bounded solutions, preserves dissipativity for models like the barotropic quasi-geostrophic equation, and ensures numerical solutions remain bounded in compactly embedded function spaces.

Conclusion: The method rigorously proves convergence of both global attractors and invariant measures from discrete to continuous systems in the vanishing time-step limit, making it particularly suitable for climate simulations and long-time statistical analysis of geophysical flows.

Abstract: We develop and analyze a highly efficient, second-order time-marching scheme
for infinite-dimensional nonlinear geophysical fluid models, designed to
accurately approximate invariant measures-that is, the stationary statistical
properties (or climate) of the underlying dynamical system. Beyond second-order
accuracy in time, the scheme is particularly well suited for long-time
simulations due to two key features: it requires solving only a fixed symmetric
positive-definite linear system with constant coefficients at each step; and it
guarantees long-time stability, producing uniformly bounded solutions in time
for any bounded external forcing, regardless of initial data. For prototypical
models such as the barotropic quasi-geostrophic equation, the method preserves
dissipativity, ensuring that numerical solutions remain bounded in a function
space compactly embedded in the phase space as time tends to infinity.
Leveraging this property, we rigorously prove convergence of both global
attractors and invariant measures of the discrete system to those of the
continuous model in the vanishing time-step limit. A central innovation of the
method is a mean-reverting scalar auxiliary variable (mr-SAV) formulation,
which preserves the dissipative structure of externally forced systems within
an appropriate phase space. For the infinite-dimensional models considered, we
additionally employ fractional-order function spaces to establish compactness
of numerical solutions in topologies compatible with the phase space.

</details>


### [4] [Finite element analysis of an eigenvalue problem arising from neutron transport](https://arxiv.org/abs/2510.05368)
*Nicolás A. Barnafi,Felipe Lepe,Francisca Muñoz Riquelme*

Main category: math.NA

TL;DR: Analysis of a finite element method for neutron transport eigenvalue problems in 2D and 3D, proving convergence and error estimates for piecewise polynomial approximations.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze a finite element method for approximating solutions to non-symmetric eigenvalue problems arising from neutron transport.

Method: Finite element approximation using piecewise polynomials of degree k ≥ 1, analyzed within the framework of compact operators theory.

Result: Proved convergence and derived error estimates for the proposed method, with numerical tests confirming theoretical results.

Conclusion: The finite element method is effective for solving neutron transport eigenvalue problems, with theoretical convergence and error estimates validated by numerical experiments.

Abstract: In two and three dimensions, we analyze a finite element method to
approximate the solutions of an eigenvalue problem arising from neutron
transport. We derive the eigenvalue problem of interest, which results to be
non-symmetric. Under a standard finite element approximation based on piecewise
polynomials of degree $k \geq 1$, and under the framework of the compact
operators theory, we prove convergence and error estimates of the proposed
method. We report a series of numerical tests in order confirm the theoretical
results.

</details>


### [5] [A convergent adaptive finite element method for a phase-field model of dynamic fracture](https://arxiv.org/abs/2510.05407)
*Ram Manohar,S. M. Mallikarjuaniah*

Main category: math.NA

TL;DR: An adaptive finite element method for dynamic brittle fracture using phase-field modeling with mesh adaptation driven by residual-based estimators, enabling efficient capture of complex fracture phenomena.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for dynamic brittle fracture that avoids explicit crack tracking and can capture complex phenomena like crack branching and tortuosity.

Method: Combines staggered time-stepping with variational inequality formulation for damage irreversibility, using adaptive finite elements with residual-based a posteriori estimators for mesh refinement.

Result: Proved rigorous convergence of discrete solutions to critical points of energy functional, demonstrated capability to capture complex fracture patterns with computational savings over uniform refinement.

Conclusion: The proposed adaptive finite element method provides an efficient and rigorous approach for simulating dynamic brittle fracture with complex crack patterns while maintaining mathematical guarantees.

Abstract: We propose and analyze an adaptive finite element method for a phase-field
model of dynamic brittle fracture. The model couples a second-order hyperbolic
equation for elastodynamics with the Ambrosio-Tortorelli regularization of the
Francfort-Marigo variational fracture energy, which circumvents the need for
explicit crack tracking. Our numerical scheme combines a staggered
time-stepping algorithm with a variational inequality formulation to strictly
enforce the irreversibility of damage. The mesh adaptation is driven by a
residual-based a posteriori-type estimator, enabling efficient resolution of
the evolving fracture process zone. The main theoretical contribution is a
rigorous convergence analysis, where we prove that the sequence of discrete
solutions generated by the AFEM converges (up to a tolerance) to a critical
point of the governing energy functional. Numerical experiments for a
two-dimensional domain containing an edge-crack under dynamic anti-plane shear
loading demonstrate our method's capability of autonomously capturing complex
phenomena, including crack branching and tortuosity, with significant
computational savings over uniform refinement.

</details>


### [6] [Data-Driven Filtering of the Spherical Harmonics Method](https://arxiv.org/abs/2510.05452)
*Benjamin Plumridge,Cory Hauck,Steffen Schotthofer*

Main category: math.NA

TL;DR: A neural network is trained to determine optimal filter strength for the filtered spherical harmonics (FPN) method in radiation transport, improving accuracy over constant filter approaches.


<details>
  <summary>Details</summary>
Motivation: Selecting optimal filter strength in FPN method is challenging without true solution reference, requiring a data-driven approach to automate this parameter tuning.

Method: Model filter strength as neural network using local state variables and material cross-sections, trained via PDE-constrained optimization in PyTorch using discretize-then-optimize formulation.

Result: Neural-network filter substantially improves FPN accuracy; outperforms constant filter in 2-D problems, though constant filter sometimes better in 1-D cases.

Conclusion: Data-driven neural network approach effectively tunes FPN filter strength, providing automated parameter selection that generally enhances solution accuracy, especially for higher-dimensional problems.

Abstract: We investigate a data-driven approach for tuning the filtered spherical
harmonics method (\fpn) when solving the radiation transport equation (RTE).
The \fpn method extends the classical spherical harmonics approach (\pn) by
introducing regularization through a filter operator, which mitigates spurious
oscillations caused by Gibbs' phenomenon. This filter includes a tunable
parameter, the {filter strength}, that controls the degree of smoothing applied
to the solution. However, selecting an optimal filter strength is nontrivial,
often requiring inaccessible information such as the true or a high-order
reference solution. To overcome this limitation, we model the filter strength
as a neural network whose inputs include local state variables and material
cross-sections. The optimal filter strength is formulated as the solution to a
PDE-constrained optimization problem, and the neural network is trained using a
discretize-then-optimize formulation in PyTorch. We evaluate the learned filter
strength across a suite of test problems and compare the results to those from
a simple, but tunable constant filter strength. In all cases, the
neural-network-driven filter substantially improves the accuracy of the \pn
approximation. For 1-D test cases, the constant filter outperforms the
neural-network filter in some cases, but in 2-D problems, the neural-network
filter generally performs better.

</details>


### [7] [Optimal $L^2$ Error Estimates for Non-symmetric Nitsche's Methods](https://arxiv.org/abs/2510.05597)
*Gang Chen,Chaoran Liu,Yangwen Zhang*

Main category: math.NA

TL;DR: Optimal L²-error estimates for non-symmetric Nitsche method are established, resolving previous suboptimal convergence analyses through a specially constructed dual problem that restores adjoint consistency.


<details>
  <summary>Details</summary>
Motivation: Existing analyses of the non-symmetric Nitsche method yielded only suboptimal L² convergence, which contradicted consistently optimal numerical results, creating a discrepancy between theory and practice.

Method: Introduce a specially constructed dual problem that restores adjoint consistency to analyze both super-penalty and penalty-free variants on quasi-uniform meshes, and extend to general shape-regular meshes without quasi-uniformity.

Result: The analysis provides optimal L²-error estimates that match the consistently optimal numerical performance observed in practice, with numerical experiments in 2D and 3D confirming the sharpness of the theoretical results.

Conclusion: The specially constructed dual problem successfully resolves the discrepancy between previous suboptimal theoretical analyses and optimal numerical performance of the non-symmetric Nitsche method, establishing optimal convergence rates across various mesh types.

Abstract: We establish optimal $L^2$-error estimates for the non-symmetric Nitsche
method. Existing analyses yield only suboptimal $L^2$ convergence, in contrast
to consistently optimal numerical results. We resolve this discrepancy by
introducing a specially constructed dual problem that restores adjoint
consistency. Our analysis covers both super-penalty and penalty-free variants
on quasi-uniform meshes, as well as the practically important case on general
shape-regular meshes without quasi-uniformity. Numerical experiments in two and
three dimensions confirm the sharpness of our theoretical results.

</details>


### [8] [A finite element model for thermomechanical stress-strain fields in transversely isotropic strain-limiting materials](https://arxiv.org/abs/2510.05704)
*Saugata Ghosh,Dambaru Bhatta,S. M. Mallikarjunaiah*

Main category: math.NA

TL;DR: A computational framework using strain-limiting elasticity theory to address unphysical strain singularities at crack tips in transversely isotropic materials under thermo-mechanical loading, showing bounded strains despite high stress concentrations.


<details>
  <summary>Details</summary>
Motivation: Classical linear elasticity fails to predict physically realistic behavior near stress concentrations in transversely isotropic materials, leading to unphysical strain singularities at crack tips.

Method: Employ strain-limiting elasticity theory with algebraically nonlinear constitutive relationship, formulate loosely coupled PDE system, and develop finite element method with conforming discretization in continuous Galerkin framework for 2D boundary value problems.

Result: Numerical results show significant departure from classical predictions - stress concentrates intensely at crack tip but strain grows much slower and remains bounded throughout the domain.

Conclusion: The strain-limiting model effectively regularizes thermo-elastic crack-tip fields and provides reliable computational foundation for predictive modeling of thermally driven crack initiation and evolution in anisotropic materials.

Abstract: This paper presents a comprehensive computational framework for investigating
thermo-elastic fracture in transversely isotropic materials, where classical
linear elasticity fails to predict physically realistic behavior near stress
concentrations. We address the challenge of unphysical strain singularities at
crack tips by employing a strain-limiting theory of elasticity. This theory is
characterized by an algebraically nonlinear constitutive relationship between
stress and strain, which intrinsically enforces a limit on the norm of the
strain tensor. This approach allows the development of very large stresses, as
expected near a crack tip, while ensuring that the corresponding strains remain
physically bounded. A loosely coupled system of linear and nonlinear partial
differential equations governing the response of a thermo-mechanical
transversely isotropic solid is formulated. We develop a robust numerical
solution based on the finite element method, utilizing a conforming finite
element discretization within a continuous Galerkin framework to solve the
two-dimensional boundary value problem. The model is applied to analyze the
stress and strain fields near an edge crack under severe thermo-mechanical
loading. Our numerical results reveal a significant departure from classical
predictions: while stress concentrates intensely at the crack tip, the strain
grows at a substantially slower rate and remains bounded throughout the domain.
This work validates the efficacy of the strain-limiting model in regularizing
thermo-elastic crack-tip fields and establishes a reliable computational
foundation for the predictive modeling of thermally driven crack initiation and
evolution in advanced anisotropic materials.

</details>


### [9] [Reconstruction of Boundary Data in the Helmholtz Equation Using Particle Swarm Optimization](https://arxiv.org/abs/2510.05755)
*Jamal Daoudi,Chakir Tajani*

Main category: math.NA

TL;DR: A bio-inspired method combining Particle Swarm Optimization with Tikhonov regularization is proposed to solve the ill-posed Helmholtz equation data completion problem, achieving accurate and stable solutions.


<details>
  <summary>Details</summary>
Motivation: To identify unknown boundary conditions on inaccessible boundary regions using measurements from accessible areas, addressing the ill-posed nature of such inverse problems that makes finding stable solutions difficult.

Method: Combines Particle Swarm Optimization (a bio-inspired algorithm) with Tikhonov regularization to handle the ill-posedness of the Helmholtz equation data completion problem.

Result: Numerical experiments show the approach yields accurate and stable solutions that converge reliably, effectively handling the inherent instability of these inverse problems.

Conclusion: The proposed bio-inspired method provides a promising approach for solving Helmholtz equation data completion problems, offering both accuracy and stability despite the ill-posed nature of such inverse problems.

Abstract: This paper tackles the data completion problem related to the Helmholtz
equation. The goal is to identify unknown boundary conditions on parts of the
boundary that cannot be accessed directly, by making use of measurements
collected from accessible regions. Such inverse problems are known to be
ill-posed in the Hadamard sense, which makes finding stable and dependable
solutions particularly difficult. To address these challenges, we propose a
bio-inspired method that combines Particle Swarm Optimization with Tikhonov
regularization. The results of our numerical experiments suggest that this
approach can yield solutions that are both accurate and stable, converging
reliably. Overall, this method provides a promising way to handle the inherent
instability and sensitivity of these types of inverse problems.

</details>


### [10] [Generalized capillary-rise models: existence and fast solvers in integral Hölder spaces](https://arxiv.org/abs/2510.05801)
*Josefa Caballero,Łukasz Płociniczak,Kishin Sadarangani*

Main category: math.NA

TL;DR: Analysis of nonlinear Volterra integral equations with nonsmooth kernels, establishing existence in integral Hölder spaces, developing sharp interpolation error estimates, and proposing collocation methods with proven convergence.


<details>
  <summary>Details</summary>
Motivation: To extend classical capillary rise models by accommodating nonsmooth kernels and nonlinearities in Volterra integral equations, requiring analysis in specialized function spaces.

Method: Work in function spaces with prescribed modulus of continuity and integral Hölder spaces; establish existence results; analyze linear interpolation with sharp error estimates; develop piecewise linear collocation and spectral collocation methods.

Result: Proved existence of solutions in integral Hölder spaces; obtained sharp optimal error estimates for interpolation; developed convergent collocation methods; demonstrated effectiveness through numerical experiments.

Conclusion: The framework successfully handles generalized capillary rise models with nonsmooth kernels, provides rigorous analysis in specialized function spaces, and offers efficient numerical methods with proven convergence properties.

Abstract: We study a class of nonlinear Volterra integral equations that generalize the
classical capillary rise models, allowing for nonsmooth kernels and
nonlinearities. To accommodate such generalities, we work in two families of
function spaces: spaces with prescribed modulus of continuity and integral
H\"older spaces. We establish existence results for solutions within the
integral H\"older space framework. Furthermore, we analyze the behavior of
linear interpolation in these spaces and provide, for the first time, sharp
error estimates, demonstrating their optimality. Building on this foundation,
we propose a piecewise linear collocation method tailored to solutions in
integral H\"older spaces and prove its convergence. For problems admitting
smoother solutions, we develop an efficient spectral collocation scheme based
on Legendre nodes. Finally, several numerical experiments illustrate the
theoretical results and highlight the performance of the proposed methods.

</details>


### [11] [A Warm-basis Method for Bridging Learning and Iteration: a Case Study in Fluorescence Molecular Tomography](https://arxiv.org/abs/2510.05926)
*Ruchi Guo,Jiahua Jiang,Bangti Jin,Wuwei Ren,Jianru Zhang*

Main category: math.NA

TL;DR: A novel warm-basis iterative projection method (WB-IPM) combines learning-based approaches with iterative schemes to improve Fluorescence Molecular Tomography (FMT) depth reconstruction accuracy without requiring large training datasets.


<details>
  <summary>Details</summary>
Motivation: FMT faces accuracy challenges in depth reconstruction, with conventional iterative methods having poor z-resolution and supervised learning requiring impractical large training datasets. Need to combine learning with iterative methods for better accuracy and stability.

Method: Proposed warm-basis iterative projection method (WB-IPM) that integrates learning-based approaches with iterative schemes, using a weaker loss function based only on directional component differences to reduce training effort.

Result: Achieves significantly more accurate reconstructions than both learning-based and iterative methods alone, with reduced training requirements as demonstrated by error analysis and experiments on simulated and real data.

Conclusion: WB-IPM effectively combines learning and iterative approaches for FMT, providing superior reconstruction accuracy with reduced training effort through directional component-based loss functions.

Abstract: Fluorescence Molecular Tomography (FMT) is a widely used non-invasive optical
imaging technology in biomedical research. It usually faces significant
accuracy challenges in depth reconstruction, and conventional iterative methods
struggle with poor $z$-resolution even with advanced regularization. Supervised
learning approaches can improve recovery accuracy but rely on large,
high-quality paired training dataset that is often impractical to acquire in
practice. This naturally raises the question of how learning-based approaches
can be effectively combined with iterative schemes to yield more accurate and
stable algorithms. In this work, we present a novel warm-basis iterative
projection method (WB-IPM) and establish its theoretical underpinnings. The
method is able to achieve significantly more accurate reconstructions than the
learning-based and iterative-based methods. In addition, it allows a weaker
loss function depending solely on the directional component of the difference
between ground truth and neural network output, thereby substantially reducing
the training effort. These features are justified by our error analysis as well
as simulated and real-data experiments.

</details>


### [12] [A Geometrical Acoustics based Focusing Algorithm for Layered Media in Medical Ultrasound](https://arxiv.org/abs/2510.05951)
*Simon Hackl,Simon Hubmer,Ronny Ramlau*

Main category: math.NA

TL;DR: GOAT algorithm corrects ultrasound image aberrations caused by sound speed variations in layered media.


<details>
  <summary>Details</summary>
Motivation: Ultrasound imaging assumes constant sound speed, but variations cause blurred/distorted images that need correction.

Method: Geometrical Acoustics based Focusing Algorithm (GOAT) for layered media with differentiable boundaries, with existence/uniqueness analysis.

Result: Numerical simulations show method precision, phantom experiments demonstrate image quality improvements.

Conclusion: GOAT effectively corrects ultrasound aberrations in layered media settings.

Abstract: Ultrasound imaging is a widely used, non-invasive diagnostic tool in modern
medicine. A crucial assumption is a constant sound speed in the observed
medium. For large scale sound speed variations, this assumption leads to
blurred and distorted images. In this paper, we present a Geometrical Acoustics
based Focusing Algorithm (GOAT) which is able to correct for these aberrations,
given a known layered medium setting with continuously differentiable medium
boundaries. Existence and uniqueness conditions for a solution to the
underlying system of equations are given. Using numerical simulations, the
precision of our method is evaluated. Finally, the resulting image quality
improvements are demonstrated in a phantom-based experimental setup.

</details>


### [13] [Approximation by neural network operators of convolution type activated by deformed and parametrized half hyperbolic tangent function](https://arxiv.org/abs/2510.05980)
*Asiye Arif,Tugba Yurdakadim*

Main category: math.NA

TL;DR: The paper introduces three neural network convolution operators activated by q-deformed and β-parametrized half hyperbolic tangent functions, with convergence analysis and smoothness preservation properties.


<details>
  <summary>Details</summary>
Motivation: To develop novel neural network operators with specific activation functions and analyze their mathematical properties including convergence and smoothness preservation.

Method: Introduces three convolution-type neural network operators using q-deformed and β-parametrized half hyperbolic tangent activation functions, with quantitative convergence analysis using modulus of continuity.

Result: Obtained quantitative convergence results to the identity operator and demonstrated global smoothness preservation properties of the operators.

Conclusion: The proposed operators with q-deformed and β-parametrized activation functions exhibit desirable convergence properties and smoothness preservation, with iterated versions also considered.

Abstract: Here, we introduce three kinds of neural network operators of convolution
type which are activated by q-deformed and \b{eta}-parametrized half hyperbolic
tangent function. We obtain quantitative convergence results to the identity
operator with the use of modulus of continuity. Global smoothness preservation
of our operators are also presented and the iterated versions of them are taken
into the consideration.

</details>


### [14] [Stochastic BDDC algorithms](https://arxiv.org/abs/2510.05993)
*Xuemin Tu,Jinjin Zhang*

Main category: math.NA

TL;DR: Stochastic BDDC algorithms with online/offline stages for sampling solutions of linear stochastic elliptic equations with random coefficients, using Polynomial Chaos expansions and outperforming mean-based BDDC.


<details>
  <summary>Details</summary>
Motivation: To develop efficient sampling methods for solutions of linear stochastic elliptic equations with random coefficients, improving upon deterministic BDDC algorithms by handling stochasticity more effectively.

Method: Stochastic BDDC algorithms with offline stage for constructing Polynomial Chaos expansions of components using subdomain local parametrization, and online stage for efficient sample-dependent implementation.

Result: Condition number of stochastic BDDC preconditioned operator is estimated under assumptions, and numerical experiments show it outperforms BDDC using mean coefficient values.

Conclusion: Stochastic BDDC algorithms provide efficient sampling with theoretical guarantees and superior performance compared to mean-based approaches for stochastic elliptic equations.

Abstract: Stochastic balancing domain decomposition by constraints (BDDC) algorithms
are developed and analyzed for the sampling of the solutions of linear
stochastic elliptic equations with random coefficients. Different from the
deterministic BDDC algorithms, the stochastic BDDC algorithms have online and
offline stages. At the offline stage, the Polynomial Chaos (PC) expansions of
different components of the BDDC algorithms are constructed based on the
subdomain local parametrization of the stochastic coefficients. During the
online stage, the sample-dependent BDDC algorithm can be implemented with a
small cost. Under some assumptions, the condition number of the stochastic BDDC
preconditioned operator is estimated. Numerical experiments confirm the theory
and show that the stochastic BDDC algorithm outperforms the BDDC preconditioner
constructed using the mean value of the stochastic coefficients.

</details>


### [15] [A novel viewpoint for Bayesian inversion based on the Poisson point process](https://arxiv.org/abs/2510.05994)
*Zhiliang Deng,Zhiyuan Wang,Xiaomei Yang,Xiaofei Guan*

Main category: math.NA

TL;DR: A Bayesian framework interprets the posterior distribution as a Poisson point process intensity measure, using kernel density estimation and PPP superposition for efficient sampling.


<details>
  <summary>Details</summary>
Motivation: To provide an efficient method for exploring posterior distributions in inverse problems by generating independent and identically distributed samples.

Method: Interpret posterior as PPP intensity measure, approximate with kernel density estimation, exploit PPP superposition property for sampling from kernel components.

Result: Enables efficient sampling from posterior distribution and generation of independent identically distributed samples.

Conclusion: The framework offers a novel approach for posterior exploration in inverse problems, improving solution analysis through efficient sampling.

Abstract: We present a novel Bayesian framework for inverse problems in which the pos
terior distribution is interpreted as the intensity measure of a Poisson point
process
  (PPP). The posterior density is approximated using kernel density estimation,
and
  the superposition property of PPPs is then exploited to enable efficient
sampling
  from each kernel component. This methodology offers a new means of exploring
the
  posterior distribution and facilitates the generation of independent and
identically
  distributed samples, thereby enhancing the analysis of inverse problem
solutions.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [16] [Directional Poincaré inequality on compact Lie groups](https://arxiv.org/abs/2510.05409)
*Paulo L. Dattori da Silva,André Pedroso Kowacs*

Main category: math.AP

TL;DR: Extension of directional Poincaré inequality from torus to compact Lie groups, with conditions based on vector field eigenvalues and global solvability.


<details>
  <summary>Details</summary>
Motivation: To generalize Steinerberger's directional Poincaré inequality from the torus setting to the more general framework of compact Lie groups.

Method: Analyze eigenvalues of the global symbol of vector fields and establish connections between inequality existence and global solvability of vector fields.

Result: Found necessary and sufficient conditions for directional Poincaré inequality existence based on vector field eigenvalues. Proved equivalence between inequality holding and global solvability for left-invariant vector fields.

Conclusion: Directional Poincaré inequality extends to compact Lie groups, with existence equivalent to global solvability of the vector field, and extends further to tube-type vector fields on T¹×G.

Abstract: We extend the directional Poincar\'e inequality on the torus, introduced by
Steinerberger in [Ark. Mat. 54 (2016), pp. 555--569], to the setting of compact
Lie groups. We provide necessary and sufficient conditions for the existence of
such an inequality based on estimates on the eigenvalues of the global symbol
of the corresponding vector field. We also prove that such refinement of the
Poincar\'e inequality holds for a left-invariant vector field on a compact Lie
group $G$ if and only if the vector field is globally solvable, and extend this
equivalence to tube-type vector fields on $\mathbb{T}^1\times G$.

</details>


### [17] [$L^2$ restriction bounds for analytic continuations of quantum ergodic Laplace eigenfunctions](https://arxiv.org/abs/2510.05570)
*John A. Toth,Xiao Xiao*

Main category: math.AP

TL;DR: Quantum ergodic restriction theorem for real hypersurfaces in Grauert tubes, with applications to FBI transform restrictions of Laplace eigenfunctions.


<details>
  <summary>Details</summary>
Motivation: To establish quantum ergodic restriction results for real hypersurfaces in Grauert tube settings, extending quantum ergodicity to restriction problems.

Method: Proving a quantum ergodic restriction theorem for real hypersurfaces in Grauert tubes associated with real-analytic compact Riemannian manifolds.

Result: Obtained h-independent upper and lower bounds for L^2 restrictions of FBI transform of Laplace eigenfunctions on generic geometric hypersurfaces.

Conclusion: Successfully established quantum ergodic restriction theory with applications to eigenfunction restriction bounds in Grauert tube geometry.

Abstract: We prove a quantum ergodic restriction (QER) theorem for real hypersurfaces
$\Sigma \subset X,$ where $X$ is the Grauert tube associated with a
real-analytic, compact Riemannian manifold. As an application, we obtain $h$
independent upper and lower bounds for the $L^2$ - restrictions of the FBI
transform of Laplace eigenfunctions restricted to $\Sigma$ satisfying certain
generic geometric conditions.

</details>


### [18] [Bilinear embedding for divergence-form operators with negative potentials](https://arxiv.org/abs/2510.05714)
*Andrea Poggio*

Main category: math.AP

TL;DR: The paper extends the bilinear inequality for elliptic operators with mixed boundary conditions to include complex potentials, introducing a new coefficient condition that generalizes p-ellipticity.


<details>
  <summary>Details</summary>
Motivation: To generalize previous results on maximal regularity and semigroup properties from nonnegative potentials to complex potentials with subcritical negative parts, extending classical Schrödinger operator theory.

Method: Introduces a novel condition on matrix coefficients that reduces to standard p-ellipticity for nonnegative potentials, and applies this to study the operator L = -div(A∇) + V with mixed boundary conditions.

Result: Shows that solutions to the parabolic problem u'(t) + L u(t) = f(t) have maximal regularity on L^p(Ω), and establishes mapping properties of the semigroup generated by -L.

Conclusion: The new coefficient condition successfully extends classical results to complex potentials, providing maximal regularity and semigroup mapping properties for elliptic operators with mixed boundary conditions.

Abstract: Let $\Omega \subseteq \mathbb{R}^d$ be open, $A$ a complex uniformly strictly
accretive $d\times d$ matrix-valued function on $\Omega$ with $L^\infty$
coefficients, and $V$ a locally integrable function on $\Omega$ whose negative
part is subcritical. We consider the operator $\mathscr{L} =
-\mathrm{div}(A\nabla) + V$ with mixed boundary conditions on $\Omega$. We
extend the bilinear inequality of Carbonaro and Dragi\v{c}evi\'c [15],
originally established for nonnegative potentials, by introducing a novel
condition on the coefficients that reduces to standard $p$-ellipticity when $V$
is nonnegative. As a consequence, we show that the solution to the parabolic
problem $u'(t) + \mathscr{L} u(t) = f(t)$ with $u(0)=0$ has maximal regularity
on $L^p(\Omega)$, in the same spirit as [13]. Moreover, we study mapping
properties of the semigroup generated by $-\mathscr{L}$ under this new
condition, thereby extending classical results for the Schr\"{o}dinger operator
$-\Delta + V$ on $\mathbb{R}^d$ [8,47].

</details>


### [19] [A contractible Schiffer counterexample on the half-sphere](https://arxiv.org/abs/2510.05732)
*Gonzalo Cao-Labora,Antonio J. Fernández*

Main category: math.AP

TL;DR: Existence of nontrivial smooth contractible domains on sphere with Neumann eigenfunctions constant on boundary, using bifurcation from geodesic disks and computer-assisted verification.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that rigidity results for Serrin-type problems don't extend to Neumann eigenfunctions on spheres, showing existence of domains with special boundary behavior.

Method: Local bifurcation argument around geodesic disks centered at north pole, using anisotropic Hölder spaces and computer-assisted techniques to verify bifurcation conditions.

Result: Found family of nontrivial smooth contractible domains on half-sphere admitting Neumann eigenfunctions constant on boundary.

Conclusion: Rigidity literature for Serrin-type problems doesn't apply to Neumann eigenfunctions on spheres, as shown by existence of domains with constant boundary values.

Abstract: We show the existence of a family of nontrivial smooth contractible domains
on the sphere that admit Neumann eigenfunctions of the Laplacian which are
constant on the boundary. These domains are contained on the half-sphere, in
stark contrast with the rigidity literature for Serrin-type problems. The proof
relies on a local bifurcation argument around the family of geodesic disks
centered at the north pole. We combine the use of anisotropic H\"older spaces
for the functional setting with computer-assisted techniques to check the
bifurcation conditions.

</details>


### [20] [Existence of global weak solutions to a parabolic $p$-Laplacian problem with convective term](https://arxiv.org/abs/2510.05847)
*Angelica Pia Di Feola,Michael Ruzicka*

Main category: math.AP

TL;DR: Existence of global weak solutions for parabolic p-Laplacian systems with convective terms in 3D domains


<details>
  <summary>Details</summary>
Motivation: To establish existence results for nonlinear parabolic systems with convective terms without requiring divergence constraints

Method: Proving existence of global weak solutions on bounded domains with C^2 boundary for p-Laplacian systems with convective terms

Result: Global weak solutions exist on (0,T) for any p in (1,2) and satisfy maximum principle

Conclusion: Successfully established existence theory for parabolic p-Laplacian systems with convective terms in three-dimensional domains

Abstract: For a given bounded domain $\Omega \subset \mathbb R^3$, with $C^2$ boundary,
and a given instant of time $T>0$, we prove the existence of a global weak
solution on $(0,T)$, which satisfies a maximum principle, to a parabolic
$p$-Laplacian system with convective term without divergence constraint for any
$p\in (1,2)$.

</details>


### [21] [Quantitative Gaffney and Korn inequalities](https://arxiv.org/abs/2510.05870)
*Wadim Gerner*

Main category: math.AP

TL;DR: The paper proves homogeneous, quantitative versions of Ehrling's inequality for function spaces on C^{1,1}-domains, and derives similar versions of Gaffney's inequality (for electromagnetism) and Korn's inequality (for elasticity theory), with constants that are dimensional rather than domain-dependent.


<details>
  <summary>Details</summary>
Motivation: To establish inequalities with dimensional constants that are independent of domain geometry, providing more universal mathematical tools for applications in electromagnetism and elasticity theory.

Method: Proving homogeneous quantitative versions of Ehrling's inequality for H^1(Ω)⊂⊂L^2(∂Ω) and H^1(Ω)↪L^2(Ω) spaces, then using these results to derive similar versions of Gaffney's and Korn's inequalities.

Result: Obtained inequalities with dimensional constants instead of domain-dependent constants, provided explicit upper bounds for these constants, and showed that the upper bound for the tangential homogeneous Korn inequality is asymptotically sharp as dimension n→∞.

Conclusion: The paper successfully establishes homogeneous quantitative inequalities with dimensional constants and raises the question of determining the optimal values of these dimensional constants.

Abstract: We prove a homogeneous, quantitative version of Ehrling's inequality for the
function spaces $H^1(\Omega)\subset\subset L^2(\partial\Omega)$,
$H^1(\Omega)\hookrightarrow L^2(\Omega)$ which reflects geometric properties of
a given $C^{1,1}$-domain $\Omega\subset\mathbb{R}^n$.
  We use this result to derive quantitative homogeneous versions of Gaffney's
inequality, of relevance in electromagnetism as well as Korn's inequality, of
relevance in elasticity theory.
  The main difference to the corresponding classical results is that the
constants appearing in our inequalities turn out to be dimensional constants.
We provide explicit upper bounds for these constants and show that in the case
of the tangential homogeneous Korn inequality our upper bound is asymptotically
sharp as $n\rightarrow \infty$.
  Lastly, we raise the question of the optimal values of these dimensional
constants.

</details>


### [22] [Eigenstructure of the linearized electrical impedance tomography problem under radial perturbations](https://arxiv.org/abs/2510.05966)
*Markus Hirvensalo*

Main category: math.AP

TL;DR: Analysis of Fréchet derivative for conductivity perturbations in the unit ball, showing eigenfunctions are spherical harmonics and eigenvalues decay uniformly, enabling finite-rank approximations for rotationally symmetric perturbations.


<details>
  <summary>Details</summary>
Motivation: To understand the mathematical structure of the Fréchet derivative in conductivity problems, which is important for numerical algorithms and inverse problems involving boundary measurements.

Method: Analyzed the Fréchet derivative mapping conductivity perturbations to boundary measurement changes in the unit ball, focusing on rotationally symmetric perturbations from L^2(B) space.

Result: Found that eigenfunctions correspond to spherical harmonics, established explicit eigenvalue formulas, and showed uniform eigenvalue decay for bounded perturbations. Demonstrated that the Fréchet derivative can be approximated by finite-rank operators for rotationally symmetric perturbations.

Conclusion: The established structure and approximability properties are favorable for numerical analysis and algorithms involving the Fréchet derivative in conductivity problems.

Abstract: We analyze the Fr\'echet derivative $F$, that maps a perturbation in
conductivity to the linearized change in boundary measurements governed by the
conductivity equation. The domain is taken to be the unit ball $B \subset
\mathbb{R}^d$ with $d \geq 2$, and we choose perturbations $\eta$ from the
Hilbert space $L^2(B)$. Under the condition that the perturbations are
rotationally symmetric, we show that the eigenfunctions of the linear
approximation $F \eta$ correspond to the spherical harmonics. Furthermore, we
establish an explicit formula for the associated eigenvalues and show that for
perturbations from any bounded subset, the decay of these eigenvalues is
uniform with respect to the degree of the spherical harmonics. The established
structure of $F \eta$ enables us to show that the Fr\'echet derivative $F$ can
be approximated by finite-rank operators when restricted to rotationally
symmetric perturbations. Both the extension to $L^2(B)$ perturbations and the
approximability by finite-rank operators are favorable properties for further
analysis of $F$ in numerical algorithms.

</details>


### [23] [Estimates of a possible gap related to the energy equality for a class of non-Newtonian fluids](https://arxiv.org/abs/2510.05990)
*Francesca Crispo,Angelica Pia Di Feola,Carlo Romano Grisanti*

Main category: math.AP

TL;DR: Construction of weak solutions with energy equality for 3D power-law fluids with shear-dependent viscosity in periodic domains, for initial data in J²(Ω) and p ∈ (9/5,2).


<details>
  <summary>Details</summary>
Motivation: To establish existence of weak solutions that satisfy energy equality for power-law fluids, extending results similar to Navier-Stokes equations.

Method: Analysis of 3D initial value problem for power-law fluids with shear-dependent viscosity in spatially periodic domains.

Result: Existence of weak solutions with energy equality is proven for initial data v₀ ∈ J²(Ω) and p ∈ (9/5,2).

Conclusion: The results align with Navier-Stokes equations, with additional dissipation expressed only in terms of energy quantities.

Abstract: The paper is concerned with the 3D-initial value problem for power-law fluids
with shear dependent viscosity in a spatially periodic domain. The goal is the
construction of a weak solution enjoying an energy equality. The results hold
assuming an initial data $v_0\in J^2(\Omega)$ and for $p\in \left(\frac
95,2\right)$. It is interesting to observe that the result is in complete
agreement with the one known for the Navier-Stokes equations. Further, in both
cases, the additional dissipation, which measures the possible gap with the
classical energy equality, is only expressed in terms of energy quantities.

</details>


### [24] [Existence and Nonexistence Breaking Results For a Weighted Elliptic Problem in Half-Space](https://arxiv.org/abs/2510.05999)
*J. M. Do Ó,R. F. Freire,J. Giacomoni,E. S. Medeiros*

Main category: math.AP

TL;DR: The paper studies a weighted elliptic boundary value problem and shows that introducing a weighted operator can reverse the classical solvability behavior compared to the Laplacian case.


<details>
  <summary>Details</summary>
Motivation: To investigate how weighted operators affect the existence and nonexistence of solutions to elliptic boundary value problems, particularly reversing known results from the classical Laplacian case.

Method: Established regularity for weak solutions and used a variational approach combined with a new Pohozaev-type identity to analyze solvability.

Result: Identified regimes where the weighted problem admits solutions despite nonexistence for the classical Laplacian case, and vice versa, thus inverting classical existence/nonexistence results.

Conclusion: The introduction of weighted operators can fundamentally change the solvability behavior of elliptic boundary value problems, reversing classical existence patterns.

Abstract: In this paper we study the problem $-\mathrm{div}(\rho(x_N)\nabla
u)=a|u|^{p-2}u$ in $\mathbb{R}^N_+$, $-\partial u/\partial x_N=b|u|^{q-2}u$ in
$\mathbb{R}^{N-1}$ where $a,b \in \mathbb{R}$, $p,q\in (1,\infty)$ and $\rho$
is a positive weight. We establish regularity results for weak solutions and,
using a variational approach combined with a new Pohozaev-type identity, we
show that the introduction of the weighted operator
$-\mathrm{div}(\rho(x_N)\nabla u)$ can reverse the known solvability behavior
of the classical Laplacian case. Specifically, we identify regimes where the
problem admits solutions despite nonexistence for the corresponding case with
$-\Delta$, and vice versa, thus inverting the classical existence and
nonexistence results.

</details>


### [25] [Energy equality of the weak solutions to the fractional Navier-Stokes / MHD equations](https://arxiv.org/abs/2510.06024)
*Yi Feng,Weihua Wang*

Main category: math.AP

TL;DR: This paper studies energy equality for weak solutions of 3D incompressible fractional Navier-Stokes/MHD equations, establishing sufficient conditions in Sobolev multiplier spaces that ensure energy equality holds.


<details>
  <summary>Details</summary>
Motivation: To investigate when energy equality holds for weak solutions of fractional Navier-Stokes and MHD equations, as these energy equations are related to solution uniqueness.

Method: Using symmetrization and interpolation techniques to derive sufficient conditions in Sobolev multiplier spaces.

Result: Obtained new sufficient conditions that ensure energy equality for weak solutions of fractional MHD equations, with corresponding results for fractional Navier-Stokes equations.

Conclusion: The established conditions guarantee energy equality for weak solutions, which is important for understanding solution uniqueness in fractional Navier-Stokes and MHD equations.

Abstract: In this paper, we study the problem of energy equality for weak solutions of
the 3D incompressible fractional Navier-Stokes / MHD equations. With the help
of the technique of symmetrization and interpolation method, we obtain some new
sufficient conditions including the Sobolev multiplier spaces, which insures
the validity of the energy equality of the weak solution to fractional MHD
equations. Correspondingly, the results of fractional Navier-Stokes equations
are obtained. And these energy equations are usually related to the uniqueness
of solutions to the corresponding fractional Navier-Stokes / MHD equations.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [26] [SHarmonic: A fast and accurate implementation of spherical harmonics for electronic-structure calculations](https://arxiv.org/abs/2510.05282)
*Xavier Andrade,Jacopo Simoni,Yuan Ping,Tadashi Ogitsu,Alfredo A. Correa*

Main category: physics.comp-ph

TL;DR: SHarmonic is a new spherical harmonics implementation for electronic-structure calculations that uses explicit formulas with normalized Cartesian coordinates, achieving high precision with significantly better computational efficiency and GPU acceleration.


<details>
  <summary>Details</summary>
Motivation: To create a more computationally efficient and precise implementation of spherical harmonics for electronic-structure calculations, avoiding error-prone and cumbersome implementations in other codes.

Method: Uses explicit formulas for spherical harmonics written in terms of normalized Cartesian coordinates, enabling GPU acceleration and optimized computational performance.

Result: The implementation is as precise as other methods while being at least one order of magnitude more computationally efficient, with GPU execution providing an additional order of magnitude speed improvement.

Conclusion: SHarmonic provides a simple, open-source library that offers high precision and significant performance improvements for spherical harmonics calculations in electronic-structure codes.

Abstract: The authors present SHarmonic, a new implementation of the spherical
harmonics targeted for electronic-structure calculations. Their approach is to
use explicit formulas for the harmonics written in terms of normalized
Cartesian coordinates. This approach results in a code that is as precise as
other implementations while being at least one order of magnitude more
computationally efficient. The library can run on graphics processing units
(GPUs) as well, achieving an additional order of magnitude in execution speed.
This new implementation is simple to use and is provided under an open source
license, it can be readily used by other codes to avoid the error-prone and
cumbersome implementation of the spherical harmonics.

</details>


### [27] [New GPU developments in the Madgraph CUDACPP plugin: kernel splitting, helicity streams, cuBLAS color sums](https://arxiv.org/abs/2510.05392)
*Andrea Valassi*

Main category: physics.comp-ph

TL;DR: The paper presents optimizations for the CUDACPP plugin in Madgraph5_aMC@NLO by splitting large GPU kernels into smaller ones for better performance.


<details>
  <summary>Details</summary>
Motivation: To further optimize matrix element calculations in the CUDACPP plugin by improving GPU performance through kernel decomposition.

Method: Splitting the single large GPU kernel used for matrix element calculations into several smaller kernels for better optimization.

Result: Developed additional optimizations ready for inclusion in a new software release, improving GPU performance.

Conclusion: The new kernel-splitting approach provides significant optimizations for GPU-based matrix element calculations in particle physics simulations.

Abstract: The first production release of the CUDACPP plugin for the Madgraph5_aMC@NLO
generator, which speeds up matrix element (ME) calculations for leading-order
(LO) QCD processes using a data parallel approach on vector CPUs and GPUs, was
delivered in October 2024. This has been described in previous publications by
the team behind that effort. In this paper, I describe my work on some
additional developments providing further optimizations of CUDACPP for GPUs,
which I consider ready for inclusion in a new release of the software. The new
approach mainly consists in splitting the calculation of the ME, which has been
so far performed using a single large GPU kernel, into several smaller kernels.
I also take this opportunity to describe more in detail some features of the
CUDACPP software that are relevant to these new developments and that have not
yet been documented.

</details>


### [28] [msmJAX: Fast and Differentiable Electrostatics on the GPU in Python](https://arxiv.org/abs/2510.05961)
*Florian Buchner,Johannes Schörghuber,Nico Unglert,Jesús Carrete,Georg K. H. Madsen*

Main category: physics.comp-ph

TL;DR: msmJAX is a Python package implementing the multilevel summation method for efficient evaluation of long-range interactions in particle simulations, built on JAX for ML integration and GPU acceleration.


<details>
  <summary>Details</summary>
Motivation: To provide a linear-scaling algorithm for electrostatic and long-range interactions that integrates with machine learning methods in chemistry and materials science, combining high performance with Python accessibility.

Method: Implements multilevel summation method with B-spline interpolation using JAX framework, enabling GPU deployment, automatic differentiation, and modular design for adaptation.

Result: The package demonstrates linear scaling, stability in molecular-dynamics simulations, and offers high performance with easy deployment on GPUs.

Conclusion: msmJAX successfully bridges computational efficiency with machine learning integration, providing a powerful tool for particle-based simulations in chemistry and materials science.

Abstract: We present msmJAX, a Python package implementing the multilevel summation
method with B-spline interpolation, a linear-scaling algorithm for efficiently
evaluating electrostatic and other long-range interactions in particle-based
simulations. Built on the JAX framework, msmJAX integrates naturally with the
machine-learning methods that are transforming chemistry and materials science,
while also serving as a powerful tool in its own right. It combines high
performance with Python's accessibility, offers easy deployment on GPUs, and
supports automatic differentiation. We outline the modular design of msmJAX,
enabling users to adapt or extend the code, and present benchmarks and
examples, including a verification of linear scaling, and demonstrations of its
stability in molecular-dynamics simulations.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [29] [A note on thermal effects in non-linear models for plasma-based acceleration](https://arxiv.org/abs/2510.05274)
*D. Simeoni,G. Parise,A. R. Rossi,A. Frazzitta,F. Guglietta,M. Sbragaglia*

Main category: physics.plasm-ph

TL;DR: Study shows how non-zero background temperature affects relativistic plasma wakefields in the blowout regime, causing contraction of both longitudinal and transverse bubble sizes.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of non-negligible background temperature on plasma wakefields, building on previous models that neglected temperature effects.

Method: Combined theoretical modeling with PIC simulations to characterize bubble size changes as a function of temperature in the blowout regime.

Result: Non-zero background temperature causes contraction of both longitudinal and transverse bubble sizes, with model predictions validated by PIC simulations.

Conclusion: Background temperature significantly affects plasma wakefield bubble dimensions, with both transverse and longitudinal sizes decreasing as temperature increases.

Abstract: We investigate the impact of a non-negligible background temperature on
relativistic plasma wakefields generated when a beam of charged particles
passes through a neutral plasma at rest. Our analysis focuses on the blowout
regime, where the plasma response is highly non-linear: plasma electrons are
radially blown out and expelled away from the propagation axis of the beam
particles, creating a region (bubble) of ions without electrons. Our study
builds upon earlier investigations for non-linear models of plasma wakefields
developed neglecting plasma temperature. In the presence of a non-zero
background temperature, we characterize the bubble in terms of its transversal
and longitudinal sizes as a function of the temperature. Model predictions and
parametrizations are studied in combination with PIC simulations, and correctly
reproduce the temperature induced contraction of both the longitudinal and
transverse bubble sizes.

</details>


### [30] [Difference in Neoclassical Edge Flows Between Strongly Negative and Positive Triangularities in the XGC Gyrokinetic Simulation](https://arxiv.org/abs/2510.05287)
*S. Ku,C. S. Chang,R. Hager,L. W. Schmitz,A. O. Nelson*

Main category: physics.plasm-ph

TL;DR: Neoclassical simulation comparing negative and positive triangularity plasmas shows significant differences in X-point orbit loss physics and rotation sources, with validation limited to middle pedestal slope.


<details>
  <summary>Details</summary>
Motivation: To establish a baseline for multiphysics studies of negative triangularity plasmas by comparing edge rotation physics between positive and negative triangularity configurations.

Method: Used XGC gyrokinetic code to simulate DIII-D-like plasma with carbon ions, comparing artificial positive triangularity equilibrium with experimental negative triangularity discharge while keeping other geometric parameters constant.

Result: Found significant differences in deuteron vs carbon rotation due to Pfirsch-Schluter rotation, and major differences in X-point orbit loss physics between triangularity types. Neoclassical simulation only validated in middle pedestal slope.

Conclusion: Edge turbulence is important for full validation, and this study provides baseline for future multiphysics studies including turbulence in negative triangularity plasmas.

Abstract: The neoclassical baseline study of a strongly negative triangularity (NT)
plasma and the corresponding positive triangularity plasma is performed using
the edge-specialized, total-f gyrokinetic code XGC. A DIII-D-like plasma is
used, based on the negative triangularity discharge of DIII-D \#193793. An
artificial positive triangularity (PT) equilibrium has been constructed to
compare the edge rotation physics at the same triangularity strength, but with
opposite sign, while keeping the same elongation and other geometric
parameters. Carbon(+6) ions are added to the deuterium plasma at an
experimentally relevant level. By using the experimental profile of carbon
toroidal rotation profile as an input, XGC finds that the deuteron rotation is
significantly different from the carbon rotation at the inboard and outboard
midplanes, mostly caused by the difference in the Pfirsch-Schluter rotation.
More importantly, significant difference in the X-point orbit loss physics,
thus the rotation source, is found between the positive and negative
triangularity equilibrium models. However, it is also found that the agreement
between the present neoclassical simulation and the experimental NT data is
validated only within the middle of pedestal slope, indicating the importance
of edge turbulence. This study could establish baseline for the multiphysics,
multiscale studies that include turbulence of negative triangularity plasmas.

</details>


### [31] [Deposition rate and energy to substrate in chopped and standard HiPIMS: identifying optimal pulse parameters](https://arxiv.org/abs/2510.05656)
*Mina Farahani,Jiří Čapek,Tomáš Kozák*

Main category: physics.plasm-ph

TL;DR: Chopped HiPIMS with segmented pulses increases energy flux and deposition rates compared to standard HiPIMS at same total pulse length, but standard HiPIMS with short pulses outperforms when pulse durations are similar.


<details>
  <summary>Details</summary>
Motivation: HiPIMS offers higher ionized flux but lower deposition rates than DCMS. Chopped HiPIMS was proposed to mitigate ion back-attraction and promote gas recovery, requiring optimization of deposition conditions.

Method: Used chopped and standard HiPIMS with varying micropulse lengths, delay times, and magnetic field strengths. Measured energy flux, deposition rate, and ionized flux fraction using thermal probe, QCM, and mass spectrometer at substrate position with constant average power.

Result: Chopped HiPIMS showed higher energy flux and deposition rates than standard HiPIMS at same total pulse length. Weaker magnetic field enhanced deposition rates and ion transport. In chopped mode, longer micropulses decreased performance while longer delays improved it. Standard HiPIMS with short pulses performed best.

Conclusion: Consistent short pulse durations and sufficient off-times for complete gas refill are crucial for maximizing ion fluxes and deposition rates in HiPIMS processes.

Abstract: High-Power Impulse Magnetron Sputtering (HiPIMS) offers higher ionized flux
fractions at the cost of lower deposition rates compared to conventional DCMS.
A fine optimization of the deposition conditions is crucial for specific
applications. Chopped or multi-pulse HiPIMS (segmenting pulses into shorter
micropulses) has been proposed to mitigate ion back-attraction and promote
working gas recovery. This study investigates how micropulse length, delay time
between segments, and magnetic field strength influence energy flux, deposition
rate, and ionized flux fraction in chopped and standard HiPIMS. These
quantities are evaluated by passive thermal probe, biasable QCM and mass
spectrometer measurements at the substrate position. Deposition-averaged and
pulse-averaged power is kept constant for all conditions to facilitate
meaningful comparison. Results indicate that chopping the HiPIMS pulse
consistently leads to higher energy flux and total deposition rate compared to
standard HiPIMS at the same total pulse length, primarily due to increased ion
flux. A weaker unbalanced magnetic field configuration enhances deposition
rates and ion transport. In chopped HiPIMS, increasing micropulse length
decreased energy flux and deposition rates, whereas increasing the delay time
between micropulses substantially improved these parameters. Importantly,
standard HiPIMS, which operated at higher frequencies and short pulse lengths,
demonstrated superior performance (with higher total energy and particle
fluxes) than chopped HiPIMS when compared at similar short pulse durations.
This suggests that consistent short pulse durations and sufficient off-times
for complete gas refill are paramount for maximizing ion fluxes and deposition
rates.

</details>


### [32] [Investigating the Lower Hybrid Drift Instability in Reconnecting Current Sheets Using a Hybrid Kinetic Model (ssV Code)](https://arxiv.org/abs/2510.05912)
*S. Thatikonda,F. N. De Oliveira-Lopes,A. Mustonen,K. Pommois,D. Told,F. Jenko*

Main category: physics.plasm-ph

TL;DR: Hybrid kinetic simulations show the lower hybrid drift instability (LHDI) evolves differently in electrostatic vs electromagnetic regimes, with electromagnetic LHDI generating turbulence that kinks current sheets and enhances resistivity, potentially facilitating fast magnetic reconnection.


<details>
  <summary>Details</summary>
Motivation: To understand how the nonlinear evolution of LHDI affects reconnecting current sheets and bridge theoretical predictions with simulations about kinetic instabilities in reconnection physics.

Method: Used hybrid kinetic simulation model in Super Simple Vlasov (ssV) code with kinetic ions and drift-kinetic electrons, solving coupled electrostatic and electromagnetic fields. Conducted parametric study varying mass ratio, temperature ratio, plasma beta, and sheet thickness.

Result: In electrostatic cases, LHDI remains localized at sheet edges and flattens density gradients. In electromagnetic regimes, LHDI-induced turbulence generates magnetic perturbations that kink current sheets and enhance anomalous resistivity.

Conclusion: LHDI dynamics may facilitate fast magnetic reconnection under certain conditions, bridging theory and simulations while emphasizing the importance of kinetic instabilities in reconnection physics.

Abstract: We investigate the nonlinear evolution of the lower hybrid drift instability
(LHDI) in reconnecting current sheets using a hybrid kinetic simulation model
implemented in the Super Simple Vlasov (ssV) code. The model treats ions
kinetically and electrons with a drift-kinetic approximation, solving
self-consistent coupled electrostatic and electromagnetic fields. A parametric
study explores the effects of mass ratio, temperature ratio, plasma beta, and
sheet thickness. In electrostatic cases, LHDI remains localized at the sheet
edges, flattening density gradients. In electromagnetic regimes, turbulence
induced by LHDI generates magnetic perturbations that kink the current sheet
and enhance anomalous resistivity. These dynamics may facilitate fast magnetic
reconnection under certain conditions. Our results bridge prior theoretical
predictions and simulations, emphasizing the importance of kinetic
instabilities in reconnection physics.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [33] [Mechanistic-statistical inference of mosquito dynamics from mark-release-recapture data](https://arxiv.org/abs/2510.06080)
*Nga Nguyen,Olivier Bonnefon,René Gato,Luis Almeida,Lionel Roques*

Main category: q-bio.PE

TL;DR: A mechanistic-statistical framework for estimating mosquito dispersal and survival from mark-release-recapture data, combining individual-based diffusion models with reaction-diffusion limits for biological control applications.


<details>
  <summary>Details</summary>
Motivation: Biological control strategies like sterile insect technique require reliable estimates of male mosquito dispersal and survival, which current empirical methods struggle to provide.

Method: Combines individual-based 2D diffusion model with reaction-diffusion limit, uses Poisson observation model for trap counts, and quantifies uncertainty via parametric bootstrap.

Result: Applied to urban MRR data in Cuba, estimated mean life expectancy of ~5 days and typical displacement of ~180m for sterile Aedes aegypti males.

Conclusion: The mechanistic approach jointly estimates movement, mortality, and capture parameters, providing biologically interpretable results and a principled framework for designing SIT interventions.

Abstract: Biological control strategies against mosquito-borne diseases--such as the
sterile insect technique (SIT), RIDL, and Wolbachia-based releases--require
reliable estimates of dispersal and survival of released males. We propose a
mechanistic--statistical framework for mark--release--recapture (MRR) data
linking an individual-based 2D diffusion model with its reaction--diffusion
limit. Inference is based on solving the macroscopic system and embedding it in
a Poisson observation model for daily trap counts, with uncertainty quantified
via a parametric bootstrap. We validate identifiability using simulated data
and apply the model to an urban MRR campaign in El Cano (Havana, Cuba)
involving four weekly releases of sterile Aedes aegypti males. The
best-supported model suggests a mean life expectancy of about five days and a
typical displacement of about 180 m. Unlike empirical fits of survival or
dispersal, our mechanistic approach jointly estimates movement, mortality, and
capture, yielding biologically interpretable parameters and a principled
framework for designing and evaluating SIT-based interventions.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [34] [Large deviation principle for a stochastic nonlinear damped Schrodinger equation](https://arxiv.org/abs/2510.06110)
*Sandip Roy,Debopriya Mukherjee,Manil Thankamani Mohan*

Main category: math.PR

TL;DR: The paper proves the Laplace principle and large deviation principle for the stochastic nonlinear Schrödinger equation with polynomial nonlinearity, linear damping, and mixed Ito-Stratonovich noise.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for stochastic nonlinear Schrödinger equations with mixed noise types, which are important in quantum mechanics and nonlinear optics.

Method: Uses weak convergence framework of Budhiraja and Dupuis, Banach fixed point theorem for local well-posedness, Yosida approximation for global well-posedness, and truncation methods with stopping time arguments.

Result: Successfully proved Laplace principle and Wentzell-Freidlin type large deviation principle, established local and global well-posedness for both deterministic and stochastic controlled equations.

Conclusion: The analysis provides complete mathematical framework for stochastic nonlinear Schrödinger equations with mixed noise, including conservation laws and rigorous well-posedness results.

Abstract: The present paper focuses on the stochastic nonlinear Schrodinger equation
with polynomial nonlinearity, and a zero-order (no derivatives involved) linear
damping. Here, the random forcing term appears as a mix of a nonlinear noise in
the Ito sense and a linear multiplicative noise in the Stratonovich sense. We
prove the Laplace principle for the family of solutions to the stochastic
system in a suitable Polish space, using the weak convergence framework of
Budhiraja and Dupuis. This analysis is nontrivial, since it requires uniform
estimates for the solutions of the associated controlled stochastic equation in
the underlying solution space in order to verify the weak convergence
criterion. The Wentzell Freidlin type large deviation principle is proved using
Varadhan's lemma and Bryc's converse to Varadhan's lemma. The local
well-posedness of the skeleton equation (deterministic controlled system) is
established by employing the Banach fixed point theorem, and the global well
posedness is established via Yosida approximation. We show that the
conservation law holds in the absence of the linear damping and Ito noise. The
well posedness of the stochastic controlled equation is also nontrivial in this
case. We use a truncation method, a stopping time argument, and the Yosida
technique to get the global well-posedness of the stochastic controlled
equation.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [35] [Functional Connectivity Networks for Transportation Delay Analysis: from Theory to Software](https://arxiv.org/abs/2510.05143)
*Carlson Moses Büth,Massimiliano Zanin*

Main category: physics.soc-ph

TL;DR: This paper introduces a framework and Python package (delaynet) for creating functional network representations of transportation delay propagation, addressing the lack of coherent methodology in this field.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of a coherent framework for creating functional network representations of delay propagation in transportation systems, where many fundamental decisions are currently left to researcher judgment.

Method: The authors provide theoretical background on functional networks for transportation systems, detail the main steps and pitfalls, and introduce a Python package called 'delaynet' to support network reconstruction and analysis.

Result: The paper presents an analysis of delay propagation in the Swiss train system using their framework and demonstrates the practical application of their methodology.

Conclusion: The authors conclude by discussing future research steps and the potential for their framework and tool to standardize and improve delay propagation analysis in transportation networks.

Abstract: Within the endeavour of modelling and understanding the propagation of delays
in transportation networks, an approach that has attracted increasing interest
in the last decade is the creation of functional network representations. These
graphs map elements of interest (e.g. airports or stations) as nodes, and
derive pairwise propagation patterns from their dynamics through correlation
and causality tests. In spite of multiple notable results, this approach still
lacks a coherent framework, with decisions related to many fundamental steps
being left to the judgement of the researcher. We here provide an introduction
to the theory behind functional networks for transportation systems, detailing
the main steps and the associated pitfalls. We further introduce a Python
package, delaynet, designed to support the researcher in the reconstruction and
analysis of such networks. We finally present an analysis of the propagation of
delays in the Swiss train system; and discuss future research steps.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [36] [Bilevel optimization for learning hyperparameters: Application to solving PDEs and inverse problems with Gaussian processes](https://arxiv.org/abs/2510.05568)
*Nicholas H. Nelsen,Houman Owhadi,Andrew M. Stuart,Xianjin Yang,Zongren Zou*

Main category: stat.ML

TL;DR: Efficient hyperparameter optimization using Gauss-Newton linearization for PDE-constrained problems, eliminating costly nested optimization.


<details>
  <summary>Details</summary>
Motivation: Hyperparameter choice critically affects accuracy, stability, and generalization in scientific computing methods, but bilevel optimization is computationally demanding for PDE-constrained problems.

Method: Gauss-Newton linearization of inner optimization step provides closed-form updates, reducing each outer iteration to a single linearized PDE solve followed by explicit gradient-based hyperparameter updates.

Result: Substantial improvements in accuracy and robustness compared to random hyperparameter initialization, demonstrated on nonlinear PDEs and inverse problems with Gaussian process models.

Conclusion: The method is scalable and effective for high-dimensional hyperparameter optimization, working well with additive kernels and neural network-parameterized deep kernels.

Abstract: Methods for solving scientific computing and inference problems, such as
kernel- and neural network-based approaches for partial differential equations
(PDEs), inverse problems, and supervised learning tasks, depend crucially on
the choice of hyperparameters. Specifically, the efficacy of such methods, and
in particular their accuracy, stability, and generalization properties,
strongly depends on the choice of hyperparameters. While bilevel optimization
offers a principled framework for hyperparameter tuning, its nested
optimization structure can be computationally demanding, especially in
PDE-constrained contexts. In this paper, we propose an efficient strategy for
hyperparameter optimization within the bilevel framework by employing a
Gauss-Newton linearization of the inner optimization step. Our approach
provides closed-form updates, eliminating the need for repeated costly PDE
solves. As a result, each iteration of the outer loop reduces to a single
linearized PDE solve, followed by explicit gradient-based hyperparameter
updates. We demonstrate the effectiveness of the proposed method through
Gaussian process models applied to nonlinear PDEs and to PDE inverse problems.
Extensive numerical experiments highlight substantial improvements in accuracy
and robustness compared to conventional random hyperparameter initialization.
In particular, experiments with additive kernels and neural
network-parameterized deep kernels demonstrate the method's scalability and
effectiveness for high-dimensional hyperparameter optimization.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [37] [Ancient Gauss Curvature Flows of Bounded Width](https://arxiv.org/abs/2510.05614)
*Beomjun Choi,Kyeongsu Choi,Dongjun Noh*

Main category: math.DG

TL;DR: Construction of pancake-like ancient compact solutions to Gauss curvature flow in a slab, and sausage-like ancient compact solutions to α-Gauss curvature flow (α>1/2) asymptotic to round cylinders.


<details>
  <summary>Details</summary>
Motivation: To construct specific types of ancient compact solutions to Gauss curvature flows with particular geometric properties and asymptotic behaviors.

Method: Mathematical construction of pancake-like solutions with flat sides contained in a slab, and sausage-like solutions for α-Gauss curvature flow with α>1/2.

Result: Successfully constructed both pancake-like ancient compact solutions with flat sides in a slab, and sausage-like ancient compact solutions asymptotic to round cylinders for α-Gauss curvature flow with α>1/2.

Conclusion: The paper demonstrates the existence of specific geometric ancient solutions to Gauss curvature flows with prescribed geometric properties and asymptotic behaviors.

Abstract: In this paper, we construct a pancake-like ancient compact solution with flat
sides to the Gauss curvature flow, contained in a slab. Also, we construct
sausage-like ancient compact solutions to the $\alpha$-Gauss curvature flow
with $\alpha >\frac{1}{2}$, asymptotic to a round cylinder.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [38] [Exploring Complexity Measures for Analysis of Solar Wind Structures and Streams](https://arxiv.org/abs/2510.05873)
*Venla Koikkalainen,Emilia Kilpua,Simon Good,Adnane Osmane*

Main category: astro-ph.SR

TL;DR: The paper analyzes solar wind time series using entropy-complexity and information planes with permutation entropy and horizontal visibility graph methods to distinguish different solar wind types.


<details>
  <summary>Details</summary>
Motivation: To study structure within solar wind time series and identify differences between various solar wind types (fast streams, slow streams, magnetic clouds, sheath regions) using information theory metrics.

Method: Used statistical complexity and information theory metrics including entropy-complexity and information planes, with permutation entropy and horizontal visibility graph (HVG) degree distribution for entropy measurement, compared to Jensen complexity and Fisher information measure.

Result: Magnetic cloud intervals were distinct in all approaches, especially for magnetic field magnitude. Fisher-Shannon showed larger spread in entropy-complexity plane. Differences between solar wind types were more distinct for larger time lags. Fluctuations were generally stochastic.

Conclusion: Information theory tools can effectively identify structures in solar wind time series and provide insight into their origin and formation, with magnetic clouds being particularly distinguishable.

Abstract: In this paper we use statistical complexity and information theory metrics to
study structure within solar wind time series. We explore this using
entropy-complexity and information planes, where the measure for entropy is
formed using either permutation entropy or the degree distribution of a
horizontal visibility graph (HVG). The entropy is then compared to the Jensen
complexity (Jensen-Shannon complexity plane) and Fisher information measure
(Fisher-Shannon information plane), formed both from permutations and the HVG
approach. Additionally we characterise the solar wind time series by studying
the properties of the HVG degree distribution. Four types of solar wind
intervals have been analysed, namely fast streams, slow streams, magnetic
clouds and sheath regions, all of which have distinct origins and
interplanetary characteristics. Our results show that, overall, different
metrics give similar results but Fisher-Shannon, which gives a more local
measure of complexity, leads to a larger spread of values in the
entropy-complexity plane. Magnetic cloud intervals stood out in all approaches,
in particular when analysing the magnetic field magnitude. Differences between
solar wind types (except for magnetic clouds) were typically more distinct for
larger time lags, suggesting universality in fluctuations for small scales. The
fluctuations within the solar wind time series were generally found to be
stochastic, in agreement with previous studies. The use of information theory
tools in the analysis of solar wind time series can help to identify structures
and provide insight into their origin and formation.

</details>


### [39] [Kinetic collisionless model of the solar transition region and corona with spatially intermittent heating](https://arxiv.org/abs/2510.05954)
*Luca Barbieri,Pascal Démoulin*

Main category: astro-ph.SR

TL;DR: A 3D kinetic model shows that intermittent heating at the chromospheric surface can create the solar transition region and hot corona through collisionless plasma dynamics and gravitational filtering, without local coronal heating.


<details>
  <summary>Details</summary>
Motivation: To understand how the solar transition region and corona form, particularly investigating whether intermittent heating at the chromosphere alone can produce the observed temperature inversion and hot corona.

Method: Developed a 3D kinetic model with collisionless plasma in uniform magnetic field, using surface coarse-graining for intermittent heating at chromospheric boundary, solving Vlasov equation with non-thermal boundary conditions.

Result: The model produces suprathermal particle populations, temperature inversion via gravitational filtering, realistic temperature/density profiles with thin transition region and hot corona, matching solar observations.

Conclusion: Spatial intermittency of heating at the chromospheric interface is sufficient to account for transition region formation and high-temperature corona, without requiring local coronal heating mechanisms.

Abstract: We develop a three-dimensional kinetic model of the solar transition region
and corona in which the plasma above the chromosphere is collisionless and
embedded in a uniform magnetic field. Heating occurs intermittently at discrete
locations on the chromospheric surface, modeled through a surface
coarse-graining procedure that produces non-thermal boundary conditions for the
Vlasov equation. The resulting stationary distribution functions generate
suprathermal particle populations and naturally lead to a temperature inversion
via gravitational filtering, without any local coronal heating. The model
reproduces realistic temperature and density profiles with a thin transition
region and a hot corona, consistent with solar observations. These results
demonstrate that the spatial intermittency of heating at the chromospheric
interface is sufficient to account for the formation of the transition region
and the high-temperature corona.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [Physics-Informed Neural Networks with Fourier Features and Attention-Driven Decoding](https://arxiv.org/abs/2510.05385)
*Rohan Arni,Carlos Blanco*

Main category: cs.LG

TL;DR: The paper proposes Spectral PINNSformer (S-Pformer), a redesigned Transformer-based PINN architecture that eliminates encoder redundancy and integrates Fourier feature embeddings to mitigate spectral bias, achieving better performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To address two key issues in PINNSformer architectures: encoder redundancy (increased parameter count) and spectral bias, which limits the ability to capture multiscale behaviors in PDE solutions.

Method: Redesigned PINNSformer by removing the encoder (found unnecessary for spatiotemporal correlations with self-attention) and integrating Fourier feature embeddings for adaptive frequency domain encoding to mitigate spectral bias.

Result: S-Pformer outperforms encoder-decoder PINNSformer architectures across all benchmarks, achieves or outperforms MLP performance while significantly reducing parameter count.

Conclusion: The encoder in PINNSformers is redundant when using self-attention, and Fourier feature embeddings effectively mitigate spectral bias, enabling more efficient and accurate PDE solution approximation.

Abstract: Physics-Informed Neural Networks (PINNs) are a useful framework for
approximating partial differential equation solutions using deep learning
methods. In this paper, we propose a principled redesign of the PINNsformer, a
Transformer-based PINN architecture. We present the Spectral PINNSformer
(S-Pformer), a refinement of encoder-decoder PINNSformers that addresses two
key issues; 1. the redundancy (i.e. increased parameter count) of the encoder,
and 2. the mitigation of spectral bias. We find that the encoder is unnecessary
for capturing spatiotemporal correlations when relying solely on
self-attention, thereby reducing parameter count. Further, we integrate Fourier
feature embeddings to explicitly mitigate spectral bias, enabling adaptive
encoding of multiscale behaviors in the frequency domain. Our model outperforms
encoder-decoder PINNSformer architectures across all benchmarks, achieving or
outperforming MLP performance while reducing parameter count significantly.

</details>


### [41] [Monte Carlo-Type Neural Operator for Differential Equations](https://arxiv.org/abs/2510.05620)
*Salah Eddine Choutri,Prajwal Chauhan,Othmane Mazhar,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: MCNO is a Monte Carlo-type neural operator framework for learning 1D PDE solution operators by directly learning kernel functions and using Monte Carlo integration, without assuming translation-invariant kernels like FNOs.


<details>
  <summary>Details</summary>
Motivation: To provide an alternative to Fourier Neural Operators that doesn't rely on spectral representations or translation-invariant kernel assumptions, and to explore Monte Carlo integration in neural operator frameworks for continuous-domain PDEs.

Method: Represent the kernel as a learnable tensor over sampled input-output pairs, perform uniform random sampling once from a discretized grid, and use an interpolation step to map between arbitrary input and output grids. The Monte Carlo estimator approximates the integral operator.

Result: MCNO achieves competitive accuracy with efficient computational cost on standard 1D PDE benchmarks, and theoretical analysis shows the Monte Carlo estimator has bounded bias and variance under mild regularity assumptions.

Conclusion: MCNO provides a theoretically supported alternative to spectral methods and graph-based Monte Carlo approaches, with potential for natural extension beyond one-dimensional problems to any spatial dimension.

Abstract: The Monte Carlo-type Neural Operator (MCNO) introduces a framework for
learning solution operators of one-dimensional partial differential equations
(PDEs) by directly learning the kernel function and approximating the
associated integral operator using a Monte Carlo-type approach. Unlike Fourier
Neural Operators (FNOs), which rely on spectral representations and assume
translation-invariant kernels, MCNO makes no such assumptions. The kernel is
represented as a learnable tensor over sampled input-output pairs, and sampling
is performed once, uniformly at random from a discretized grid. This design
enables generalization across multiple grid resolutions without relying on
fixed global basis functions or repeated sampling during training, while an
interpolation step maps between arbitrary input and output grids to further
enhance flexibility. Experiments on standard 1D PDE benchmarks show that MCNO
achieves competitive accuracy with efficient computational cost. We also
provide a theoretical analysis proving that the Monte Carlo estimator yields a
bounded bias and variance under mild regularity assumptions. This result holds
in any spatial dimension, suggesting that MCNO may extend naturally beyond
one-dimensional problems. More broadly, this work explores how Monte Carlo-type
integration can be incorporated into neural operator frameworks for
continuous-domain PDEs, providing a theoretically supported alternative to
spectral methods (such as FNO) and to graph-based Monte Carlo approaches (such
as the Graph Kernel Neural Operator, GNO).

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [42] [TeMFpy: a Python library for converting fermionic mean-field states into tensor networks](https://arxiv.org/abs/2510.05227)
*Simon H. Hille,Attila Szabó*

Main category: cond-mat.str-el

TL;DR: TeMFpy is a Python library that converts fermionic mean-field states to matrix product state (MPS) form, supporting both Slater determinants and Pfaffian states with efficient algorithms.


<details>
  <summary>Details</summary>
Motivation: To provide tools for building variational wave functions for strongly correlated electron systems like quantum spin liquids by converting mean-field states to MPS form.

Method: Developed new efficient algorithms for converting Slater determinants and Pfaffian states to finite/infinite MPS, built on top of TeNPy for seamless integration with existing MPS algorithms.

Result: Created TeMFpy library with detailed implemented algorithms and example workflows that enable construction of variational wave functions for strongly correlated systems.

Conclusion: TeMFpy successfully provides accessible tools for converting fermionic mean-field states to MPS form, facilitating research on quantum spin liquids and other strongly correlated electron systems.

Abstract: We introduce TeMFpy, a Python library for converting fermionic mean-field
states to finite or infinite matrix product state (MPS) form. TeMFpy includes
new, efficient, and easy-to-understand algorithms for both Slater determinants
and Pfaffian states. Together with Gutzwiller projection, these also allow the
user to build variational wave functions for various strongly correlated
electron systems, such as quantum spin liquids. We present all implemented
algorithms in detail and describe how they can be accessed through TeMFpy,
including full example workflows. TeMFpy is built on top of TeNPy and,
therefore, integrates seamlessly with existing MPS-based algorithms.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [43] [Tunable electronic energy level alignment and exciton diversity in organic-inorganic van der Waals heterostructures](https://arxiv.org/abs/2510.05267)
*Aurélie Champagne,Olugbenga Adeniran,Jonah B. Haber,Antonios M. Alvertis,Zhen-Fei Liu,Jeffrey B. Neaton*

Main category: cond-mat.mtrl-sci

TL;DR: Hybrid organic-inorganic van der Waals bilayers combining perylene-based molecular crystals with TMD monolayers (MoS2, WS2) show tunable electronic and excitonic properties through polarization effects and energy level alignment control.


<details>
  <summary>Details</summary>
Motivation: To explore hybrid van der Waals bilayers incorporating molecular monolayers for enhanced tunability and functionality beyond traditional inorganic-only systems.

Method: Using ab initio many-body perturbation theory within GW approximation and Bethe-Salpeter equation approach to predict emergent properties in hybrid bilayers.

Result: Substantial band gap renormalization of molecular crystals due to TMD-induced polarization, tunable energy level alignment, and control over diverse excitons including hybrid and charge-transfer excitons.

Conclusion: Organic-inorganic van der Waals heterostructures are promising for tunable optoelectronic devices and quantum excitonic phenomena, expanding design possibilities for low-dimensional systems.

Abstract: van der Waals stacking of two-dimensional (2D) materials offers a powerful
platform for engineering material interfaces with tailored electronic and
optical properties. While most van der Waals multilayers have featured
inorganic monolayers, incorporating molecular monolayers introduces new degrees
of tunability and functionality. Here, we investigate hybrid bilayers composed
of atomically thin perylene-based molecular crystals interfaced with monolayer
transition metal dichalcogenides (TMDs), specifically MoS2 and WS2. Using ab
initio many-body perturbation theory within the GW approximation and the
Bethe-Salpeter equation approach, we predict emergent properties beyond those
of the isolated constituent systems. Notably, we find substantial
renormalization of monolayer molecular crystal band gap due to TMD-induced
polarization. Furthermore, by varying the TMD monolayer, we demonstrate tuning
of the energy level alignment of the bilayer and subsequent control over a
diversity of lowest-energy excitons, which include strongly bound hybrid
excitons and long-lived charge-transfer excitons. These findings establish
organic-inorganic van der Waals heterostructures as a promising class of
materials for tunable optoelectronic devices and quantum excitonic phenomena,
expanding the design space for low-dimensional systems.

</details>


### [44] [Machine Learning Interatomic Potentials Enable Molecular Dynamics Simulations of Doped MoS2](https://arxiv.org/abs/2510.05339)
*Abrar Faiyad,Ashlie Martini*

Main category: cond-mat.mtrl-sci

TL;DR: First computational framework using Meta's Universal MLIP for molecular dynamics simulation of MoS2 doped with 25 elements, enabling high-throughput screening at orders-of-magnitude reduced cost compared to DFT.


<details>
  <summary>Details</summary>
Motivation: To bridge the accuracy-efficiency gap between first-principles methods and empirical potentials for doped 2D materials, enabling large-scale materials discovery and application-oriented design.

Method: Used Meta's Universal Model for Atoms machine learning interatomic potential (MLIP) to simulate MoS2 doped with 25 elements, benchmarked against DFT calculations, and performed heating-cooling simulations of doped-MoS2 supercells.

Result: MLIP accurately captures complex phenomena including dopant clustering, MoS2 layer fracturing, interlayer diffusion, and chemical compound formation at significantly reduced computational cost compared to DFT.

Conclusion: The framework provides an open-source computational workflow for high-throughput screening of dopant candidates and optimization of compositions for targeted performance in tribological, electronic, and optoelectronic applications.

Abstract: We present the first computational framework for molecular dynamics
simulation of MoS2 doped with 25 elements spanning metals, non-metals, and
transition metals using Meta's Universal Model for Atoms machine learning
interatomic potential (MLIP). Benchmarking against density functional theory
calculations demonstrates the accuracy of the MLIP for simulating doped-MoS2
systems and highlights opportunities for improvement. Using the MLIP, we
perform heating-cooling simulations of doped-MoS2 supercells. The simulations
capture complex phenomena including dopant clustering, MoS2 layer fracturing,
interlayer diffusion, and chemical compound formation at orders-of-magnitude
reduced computational cost compared to density functional theory. This work
provides an open-source computational workflow for application-oriented design
of doped-MoS2, enabling high-throughput screening of dopant candidates and
optimization of compositions for targeted tribological, electronic, and
optoelectronic performance. The MLIP bridges the accuracy-efficiency gap
between first-principles methods and empirical potentials, and the framework
offers unprecedented opportunities for large-scale materials discovery in
two-dimensional doped material systems.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [45] [Simulation of Muon-induced Backgrounds for the Colorado Underground Research Institute (CURIE)](https://arxiv.org/abs/2510.06150)
*Dakota K. Keblbeck,Eric Mayotte,Uwe Greife,Kyle G. Leach,Wouter Van De Pontseele,Caitlyn Stone-Whitehead,Luke Wanner,Grace Wagner*

Main category: hep-ex

TL;DR: Monte Carlo simulation of muon-induced backgrounds for CURIE underground facility using coupled MUTE and Geant4 frameworks, predicting neutron and gamma-ray fluxes with site-specific geological considerations.


<details>
  <summary>Details</summary>
Motivation: To provide accurate background predictions for experimental design at shallow-underground facilities by accounting for local geology and overburden geometry effects on muon-induced secondaries.

Method: Used coupled MUTE and Geant4 frameworks with angular-dependent muon energy distributions to simulate production and transport of muon-induced secondaries through site-specific rock compositions and geometries.

Result: Predicted muon-induced neutron fluxes of (3.78±0.61)×10⁻³ m⁻²s⁻¹ and (3.97±0.65)×10⁻³ m⁻²s⁻¹ for two research spaces, and gamma-ray fluxes of (5.54±0.91)×10⁻¹ m⁻²s⁻¹ and (6.51±1.06)×10⁻¹ m⁻²s⁻¹ respectively, with expected energy spectra components.

Conclusion: Local geology and overburden geometry significantly influence muon-induced secondary yields, requiring site-specific simulations for accurate underground background characterization. The simulation framework is publicly available for community use.

Abstract: We present a comprehensive Monte Carlo simulation of muon-induced backgrounds
for the Colorado Underground Research Institute (CURIE), a shallow-underground
facility with $\approx 415$~m.w.e. overburden. Using coupled \textsc{mute} and
\textsc{geant4} frameworks, we characterize the production and transport of
muon-induced secondaries through site-specific rock compositions and
geometries, establishing a proof-of-concept for high-precision, end-to-end
simulations. Our simulations employ angular-dependent muon energy
distributions, which improve secondary flux accuracy. For the Subatomic
Particle Hideout and Cryolab I research spaces, we predict total muon-induced
neutron fluxes of $(3.78 \pm 0.61_{\text{sys}}) \times
10^{-3}$~m$^{-2}$s$^{-1}$ and $(3.97 \pm 0.65_{\text{sys}}) \times
10^{-3}$~m$^{-2}$s$^{-1}$, respectively, consistent with empirical depth
parameterizations. The simulated neutron energy spectra exhibit the expected
thermal, epithermal, evaporation, and spallation components extending to GeV
energies. Electromagnetic backgrounds are expected to dominate the total flux,
with $\gamma$-ray components of $(5.54 \pm 0.91_{\text{sys}}) \times
10^{-1}$~m$^{-2}$s$^{-1}$ and $(6.51 \pm 1.06_{\text{sys}}) \times
10^{-1}$~m$^{-2}$s$^{-1}$ for the Subatomic Particle Hideout and Cryolab I
facilities, respectively. These results provide quantitative background
predictions for experimental design and sensitivity projections at shallow- and
deep-underground facilities. They further demonstrate that local geology and
overburden geometry influence muon-induced secondary yields and energy spectra,
emphasizing the need for site-specific simulations for accurate underground
background characterization. Therefore, the simulation framework has been made
publicly available for the broader low-background physics community to enable
meaningful inter-facility comparisons.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [46] [Time-reassigned synchrosqueezing frequency-domain chirplet transform for multicomponent signals with intersecting group delay curves](https://arxiv.org/abs/2510.06173)
*Shuixin Li,Jiecheng Chen,Qingtang Jiang,Lin Li*

Main category: eess.SP

TL;DR: This paper proposes a novel time-frequency-group delay dispersion (TF-GDD) representation using frequency-domain chirplet transform to address limitations of existing TSST methods in handling multicomponent signals with intersecting group delay curves.


<details>
  <summary>Details</summary>
Motivation: Existing TSST methods perform well for transient signals but fail with multicomponent signals having intersecting group delay curves, compromising feature extraction accuracy and signal component recovery, especially critical in broadband systems requiring precise group delay dispersion measurement.

Method: Proposes a three-dimensional TF-GDD representation based on frequency-domain chirplet transform, introduces time-reassigned synchrosqueezing frequency-domain chirplet transform (TSFCT) for sharper distributions, and develops frequency-domain group signal separation operation (FGSSO) for mode retrieval.

Result: Experimental results show that the proposed TSFCT and FGSSO effectively estimate group delays and retrieve modes even for signals with intersecting group delay trajectories, overcoming the fundamental limitation of previous methods.

Conclusion: The proposed frequency-domain approach provides superior capability for analyzing rapidly frequency-varying signals and enables accurate group delay dispersion measurement and mode separation in complex multicomponent scenarios.

Abstract: To analyze signals with rapid frequency variations or transient components,
the time-reassigned synchrosqueezing transform (TSST) and its variants have
been recently proposed. Unlike the traditional synchrosqueezing transform, TSST
squeezes the time-frequency (TF) coefficients along the group delay (GD)
trajectories rather than the instantaneous frequency trajectories. Although
TSST methods perform well in analyzing transient signals, they are
fundamentally limited in processing multicomponent signals with intersecting GD
curves. This limitation compromises the accuracy of both feature extraction and
signal component recovery, thereby significantly reducing the interpretability
of time-frequency representations (TFRs). This is particularly problematic in
broadband signal processing systems, where the linearity of the phase response
is critical and precise measurement of group delay dispersion (GDD) is
essential.
  Motivated by the superior capability of frequency-domain signal modeling in
characterizing rapidly frequency-varying signals, this paper proposes a novel
three-dimensional time-frequency-group delay dispersion (TF-GDD) representation
based on the frequency-domain chirplet transform. A subsequent time-reassigned
synchrosqueezing frequency-domain chirplet transform (TSFCT) is introduced to
achieve a sharper TF-GDD distribution and more accurate GD estimation. For mode
retrieval, a novel frequency-domain group signal separation operation (FGSSO)
is proposed.The theoretical contributions include a derivation of the
approximation error for the GD and GDD reference functions and an establishment
of the error bounds for FGSSO-based mode retrieval. Experimental results
demonstrate that the proposed TSFCT and FGSSO effectively estimate GDs and
retrieve modes--even for modes with intersecting GD trajectories.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [47] [A curvilinear surface ALE formulation for self-evolving Navier-Stokes manifolds - Stabilized finite element formulation](https://arxiv.org/abs/2510.05119)
*Roger A. Sauer*

Main category: physics.flu-dyn

TL;DR: Stabilized finite element formulation for ALE surface theory of Navier-Stokes flow on self-evolving manifolds, applicable to fluidic surfaces like soap films and membranes.


<details>
  <summary>Details</summary>
Motivation: To model complex and inherently unstable physical systems such as soap films, capillary menisci, and lipid membranes that involve large deformations and require frame-invariant formulations.

Method: Stabilized pressure-velocity formulation using quadratic finite elements, implicit time integration, and ALE mesh motion determined by membrane elasticity for in-plane stabilization without affecting physical behavior.

Result: Optimal convergence rates achieved in challenging examples including shear flow on self-evolving surfaces and inflating soap bubbles with partial inflow on evolving boundaries.

Conclusion: The monolithic coupled three-field system with C1-continuous surface discretizations (e.g., NURBS) is particularly advantageous for modeling fluidic surfaces with large deformations.

Abstract: This work presents a stabilized finite element formulation of the arbitrary
Lagrangian-Eulerian (ALE) surface theory for Navier-Stokes flow on
self-evolving manifolds developed in Sauer (2025). The formulation is
physically frame-invariant, applicable to large deformations, and relevant to
fluidic surfaces such as soap films, capillary menisci and lipid membranes,
which are complex and inherently unstable physical systems. It is applied here
to area-incompressible surface flows using a stabilized pressure-velocity (or
surface tension-velocity) formulation based on quadratic finite elements and
implicit time integration. The unknown ALE mesh motion is determined by
membrane elasticity such that the in-plane mesh motion is stabilized without
affecting the physical behavior of the system. The resulting three-field system
is monolithically coupled, and fully linearized within the Newton-Rhapson
solution method. The new formulation is demonstrated on several challenging
examples including shear flow on self-evolving surfaces and inflating soap
bubbles with partial inflow on evolving boundaries. Optimal convergence rates
are obtained in all cases. Particularly advantageous are C1-continuous surface
discretizations, for example based on NURBS.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [48] [Adaptive Pruning for Increased Robustness and Reduced Computational Overhead in Gaussian Process Accelerated Saddle Point Searches](https://arxiv.org/abs/2510.06030)
*Rohit Goswami,Hannes Jónsson*

Main category: physics.chem-ph

TL;DR: This paper improves Gaussian process regression for saddle point searches by using geometry-aware optimal transport measures and active pruning to reduce computational overhead and enhance stability.


<details>
  <summary>Details</summary>
Motivation: Standard GP regression for saddle point searches suffers from high computational overhead in hyperparameter optimization and can fail when searching regions not well represented by the GP model.

Method: Uses geometry-aware optimal transport measures, active pruning with Wasserstein-1 distances for farthest-point sampling, permutation-invariant metrics for trust radius, and logarithmic barrier penalty for signal variance growth.

Result: Reduces mean computational time by more than half on 238 challenging configurations from a chemical reaction dataset.

Conclusion: The improved GP approach establishes a robust and scalable algorithm for accelerating saddle point searches when energy and force evaluations are computationally expensive.

Abstract: Gaussian process (GP) regression provides a strategy for accelerating saddle
point searches on high-dimensional energy surfaces by reducing the number of
times the energy and its derivatives with respect to atomic coordinates need to
be evaluated. The computational overhead in the hyperparameter optimization
can, however, be large and make the approach inefficient. Failures can also
occur if the search ventures too far into regions that are not represented well
enough by the GP model. Here, these challenges are resolved by using
geometry-aware optimal transport measures and an active pruning strategy using
a summation over Wasserstein-1 distances for each atom-type in farthest-point
sampling, selecting a fixed-size subset of geometrically diverse configurations
to avoid rapidly increasing cost of GP updates as more observations are made.
Stability is enhanced by permutation-invariant metric that provides a reliable
trust radius for early-stopping and a logarithmic barrier penalty for the
growth of the signal variance. These physically motivated algorithmic changes
prove their efficacy by reducing to less than a half the mean computational
time on a set of 238 challenging configurations from a previously published
data set of chemical reactions. With these improvements, the GP approach is
established as, a robust and scalable algorithm for accelerating saddle point
searches when the evaluation of the energy and atomic forces requires
significant computational effort.

</details>
