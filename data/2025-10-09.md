<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 14]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [quant-ph](#quant-ph) [Total: 6]
- [cs.CE](#cs.CE) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [math.DS](#math.DS) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Finite element approximation and very weak solution existence in a two-dimensional, degenerate Keller-Segel model](https://arxiv.org/abs/2510.06341)
*Juan Vicente Guti√©rrez-Santacreu*

Main category: math.NA

TL;DR: A numerical algorithm for approximating solutions of degenerate cross-diffusion systems modeling taxis-type migration with local sensing, using finite elements and Euler time stepping that preserves solution properties.


<details>
  <summary>Details</summary>
Motivation: To develop numerical methods for degenerate cross-diffusion systems that model taxis-type migration processes, where degeneracy leads to very weak solutions with low regularity.

Method: Combines finite element spatial discretization with Euler time stepping, preserving solution properties (positivity, maximum principle, mass conservation, dual estimates) at discrete level.

Result: The discrete solutions preserve the continuous problem's properties, enabling derivation of compactness arguments and convergence (up to subsequence) to very weak solutions on 2D polygonal domains.

Conclusion: The proposed numerical scheme successfully approximates very weak solutions of degenerate cross-diffusion systems while preserving key mathematical properties at the discrete level.

Abstract: This paper is devoted to the design and analysis of a numerical algorithm for
approximating solutions of a degenerate cross-diffusion system, which models
particular instances of taxis-type migration processes under local sensing
mechanisms. The degeneracy leads to solutions that are very weak due to the low
regularity themselves. Specifically, the solutions satisfy pointwise bounds
(such as positivity and the maximum principle), integrability (such as mass
conservation), and dual a priori estimates.
  The proposed numerical scheme combines a finite element spatial
discretization with Euler time stepping. The discrete solutions preserve the
above-mentioned properties at the discrete level, enabling the derivation of
compactness arguments and the convergence (up to a subsequence) of the
numerical solutions to a very weak solution of the continuous problem on
two-dimensional polygonal domains.

</details>


### [2] [Structurally informed data assimilation in two dimensions](https://arxiv.org/abs/2510.06369)
*Tongtong Li,Anne Gelb,Yoonsang Lee*

Main category: math.NA

TL;DR: A structurally informed data assimilation framework using ensemble transform Kalman filtering (ETKF) that introduces gradient-based weighting matrices to handle discontinuous state variables more effectively than conventional approaches.


<details>
  <summary>Details</summary>
Motivation: Conventional covariance-based ensemble Kalman filter approaches often fail to effectively balance observations and model information near sharp features in systems with piecewise-smooth or discontinuous state variables.

Method: Developed a structurally informed DA framework using ETKF with gradient-based weighting matrices constructed from finite difference statistics of the forecast ensemble, allowing dynamic adjustment of observation and prior estimate influence based on local roughness.

Result: Numerical experiments demonstrate that the new structurally informed data assimilation framework consistently yields greater accuracy compared to conventional approaches.

Conclusion: The proposed framework effectively addresses the challenge of data assimilation for systems with discontinuous state variables through dynamic gradient-based weighting, showing improved performance over traditional methods.

Abstract: Accurate data assimilation (DA) for systems with piecewise-smooth or
discontinuous state variables remains a significant challenge, as conventional
covariance-based ensemble Kalman filter approaches often fail to effectively
balance observations and model information near sharp features. In this paper
we develop a structurally informed DA framework using ensemble transform Kalman
filtering (ETKF). Our approach introduces gradient-based weighting matrices
constructed from finite difference statistics of the forecast ensemble, thereby
allowing the assimilation process to dynamically adjust the influence of
observations and prior estimates according to local roughness. The design is
intentionally flexible so that it can be suitably refined for sparse data
environments. Numerical experiments demonstrate that our new structurally
informed data assimilation framework consistently yields greater accuracy when
compared to more conventional approaches.

</details>


### [3] [Media Coverage of War Victims: Journalistic Biases in Reporting on Israel and Gaza](https://arxiv.org/abs/2510.06453)
*Bedoor AlShebli,Bruno Gabriel Salvador Casara,Anne Maass*

Main category: math.NA

TL;DR: Analysis of over 14,000 news articles from Western and non-Western media reveals systematic biases in reporting on Israeli-Palestinian conflict, including differential victim portrayal, false balance in suffering, and credibility undermining of Palestinian casualty figures.


<details>
  <summary>Details</summary>
Motivation: To investigate media bias in reporting on asymmetrical warfare during the Gaza war starting October 7th 2023, examining how different media outlets portray Israeli and Palestinian victims.

Method: Analyzed over 14,000 news articles from three Western outlets (NYT, BBC, CNN) and one non-Western outlet (Al Jazeera English) published during the first year of the Gaza war, focusing on media narratives about victims experiencing hardship.

Result: Found three systematic biases in Western media: 1) Israeli victims portrayed as identifiable individuals vs. Palestinian victims as undifferentiated collectives; 2) False balance created by equating suffering despite actual disparity; 3) Language casting doubt on credibility of Palestinian casualty figures. These biases were absent or greatly reduced in Al Jazeera.

Conclusion: Western media exhibits systematic journalistic biases in conflict reporting that dehumanize Palestinian victims, create false equivalences, and undermine trust in information about Palestinian suffering, while Al Jazeera shows significantly less bias.

Abstract: October 7th 2023 marked the start of a war against Gaza, which is considered
one of the most devastating wars in modern history and has led to a stark
attitudinal divide within and between countries. To investigate the role of
media bias in reporting on this asymmetrical warfare, we analyzed over 14,000
news articles published during the first year of war in three Western (The New
York Times, BBC, CNN) and one non-Western English-language outlets (Al Jazeera
English). Exploring the media narratives concerning Israeli and Palestinian
victims experiencing hardship, we found three systematic biases in Western
media. 1) Compared to Palestinian victims, represented mainly as
undifferentiated collectives, Israeli victims were more likely to be portrayed
as identifiable individual human beings. 2) Despite the striking difference in
all forms of hardship (casualties, displacement, etc.), Western journalists
created a false balance, equating Israeli and Palestinian suffering, by
persistently referring back to the 7th of October massacre, even in the absence
of new events involving Israeli victims. 3) When reporting on numbers of
Palestinian (vs. Israeli) victims, journalists used language that casts doubt
about the credibility of the information and the reputation of the source
providing it, thereby selectively undermining the reader's trust in the
information regarding Palestinian suffering. Together, our analysis reveals a
series of systematic journalistic biases in high-profile Western media that are
absent or greatly reduced in Al Jazeera.

</details>


### [4] [A Precise Performance Analysis of the Randomized Singular Value Decomposition](https://arxiv.org/abs/2510.06490)
*Danil Akhtiamov,Reza Ghane,Babak Hassibi*

Main category: math.NA

TL;DR: Derives precise asymptotic expressions for RSVD approximation error, evaluates them for power law and bilevel singular value distributions, and extends analysis to polynomial-filtered RSVD.


<details>
  <summary>Details</summary>
Motivation: To quantify the gap between existing theoretical bounds and actual RSVD performance, and provide insights for optimal filter selection in polynomial-filtered RSVD.

Method: Derives precise asymptotic expressions for RSVD approximation error that depend only on matrix singular values, and evaluates them for power law and bilevel singular value distributions.

Result: Obtained precise asymptotic characterizations of RSVD approximation error for growing matrix dimensions, with specific evaluations for important matrix ensembles.

Conclusion: The analysis provides quantitative understanding of RSVD performance gap and offers insights for optimal filter design in polynomial-filtered variants.

Abstract: The Randomized Singular Value Decomposition (RSVD) is a widely used algorithm
for efficiently computing low-rank approximations of large matrices, without
the need to construct a full-blown SVD. Of interest, of course, is the
approximation error of RSVD compared to the optimal low-rank approximation
error obtained from the SVD. While the literature provides various upper and
lower error bounds for RSVD, in this paper we derive precise asymptotic
expressions that characterize its approximation error as the matrix dimensions
grow to infinity. Our expressions depend only on the singular values of the
matrix, and we evaluate them for two important matrix ensembles: those with
power law and bilevel singular value distributions. Our results aim to quantify
the gap between the existing theoretical bounds and the actual performance of
RSVD. Furthermore, we extend our analysis to polynomial-filtered RSVD, deriving
performance characterizations that provide insights into optimal filter
selection.

</details>


### [5] [Convergence of the Immersed Boundary Method for an Elastically Bound Particle Immersed in a 2D Navier-Stokes Fluid Fluid](https://arxiv.org/abs/2510.06586)
*Alexandre X. Milewski,Charles S. Peskin*

Main category: math.NA

TL;DR: The paper proves convergence of the immersed boundary (IB) method for co-dimension 2 problems where the fluid is 2 dimensions greater than the immersed structure, addressing theoretical gaps in previous empirical verifications.


<details>
  <summary>Details</summary>
Motivation: The IB method has been empirically verified but lacks theoretical proof due to singular forcing terms in governing equations. This paper addresses this gap specifically for co-dimension 2 problems where the immersed boundary is mollified.

Method: The authors leverage the mollification of the immersed boundary in co-dimension 2 problems to provide a theoretical convergence proof for the IB method applied to moving elastically bound particles in non-linear fluids.

Result: The paper successfully proves convergence of the immersed boundary method for the specific case of co-dimension 2 problems with mollified boundaries.

Conclusion: This work provides the first theoretical convergence proof for the IB method in co-dimension 2 scenarios, establishing a rigorous foundation for its application in fluid-membrane interaction simulations.

Abstract: The immersed boundary (IB) method has been used as a means to simulate
fluid-membrane interactions in a wide variety of biological and engineering
applications. Although the numerical convergence of the method has been
empirically verified, it is theoretically unproved because of the singular
forcing terms present in the governing equations. This paper is motivated by a
specific variant of the IB method, in which the fluid is 2 dimensions greater
than the dimension of the immersed structure. In these co-dimension 2 problems
the immersed boundary is necessarily mollified in the continuous formulation.
In this paper we leverage this fact to prove convergence of the IB method as
applied to a moving elastically bound particle in a fully non-linear fluid.

</details>


### [6] [Algorithm for constructing optimal explicit finite-difference formulas in the Hilbert space](https://arxiv.org/abs/2510.06643)
*R. S. Karimov,D. D. Atoev*

Main category: math.NA

TL;DR: This paper presents functional methods for constructing and optimizing finite-difference formulas in Hilbert spaces, specifically focusing on Adams-type explicit formulas in the space W‚ÇÇ^(m,m-1)(0,1) for m‚â•3.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of constructing finite-difference formulas using functional methods in Hilbert spaces, particularly for optimization problems in the space W‚ÇÇ^(m,m-1)(0,1).

Method: Uses functional methods and optimization techniques in the Hilbert space W‚ÇÇ^(m,m-1)(0,1) to construct finite-difference formulas, specifically focusing on explicit Adams-type formulas.

Result: Found representations of optimal coefficients for explicit finite-difference formulas of the Adams type on classes W‚ÇÇ^(m,m-1)(0,1) for any m‚â•3.

Conclusion: The functional approach successfully enables the construction and optimization of finite-difference formulas in Hilbert spaces, providing explicit coefficient representations for Adams-type formulas in the specified function spaces.

Abstract: This work presents problems of constructing finite-difference formulas in the
Hilbert space, i.e., setting problems of constructing finite-difference
formulas using functional methods. The work presents a functional statement of
the problem of optimizing finite-difference formulas in the space
$W_{2}^{\left(m,m-1\right)} \left(0,1\right)$. Here, representations of optimal
coefficients of explicit finite-difference formulas of the Adams type on
classes $W_{2}^{\left(m,m-1\right)} \left(0,1\right)$ for any $m\ge 3$ will be
found.

</details>


### [7] [Mass-Lumped Virtual Element Method with Strong Stability-Preserving Runge-Kutta Time Stepping for Two-Dimensional Parabolic Problems](https://arxiv.org/abs/2510.06653)
*Paulo Akira F. Enabe,Rodrigo Provasi*

Main category: math.NA

TL;DR: Mass-lumped Virtual Element Method with explicit SSP-RK time integration for 2D parabolic problems on polygonal meshes, achieving optimal convergence with classical CFL condition Œît=O(h¬≤).


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for parabolic problems on general polygonal meshes that maintains stability, energy decay, and maximum principles while using explicit time integration.

Method: Combines mass-lumped VEM with diagonal mass matrix construction via row-sum operations and flooring, and explicit SSP-RK time integration. Stabilization terms vanish under row summation, making lumped weights computable through polynomial systems.

Result: Achieves optimal convergence rates (O(h) in H¬π, O(h¬≤) in L¬≤) on distorted meshes, maintains mesh-robust spectral bounds, and preserves energy decay, positivity, and discrete maximum principles under Œît=O(h¬≤) CFL condition.

Conclusion: The mass-lumped VEM with SSP-RK integration provides an effective framework for parabolic problems on polygonal meshes, delivering optimal accuracy and strong stability properties without degradation from geometric distortion.

Abstract: This paper presents a mass-lumped Virtual Element Method (VEM) with explicit
Strong Stability-Preserving Runge-Kutta (SSP-RK) time integration for
two-dimensional parabolic problems on general polygonal meshes. A diagonal mass
matrix is constructed via row-sum operations combined with flooring to ensure
uniform positivity. Stabilization terms vanish identically under row summation,
so the lumped weights derive solely from the $L^2$ projector and are computable
through a small polynomial system at cost $\mathcal{O}(N_k^3)$ per element. The
resulting lumped bilinear form satisfies $L^2$-equivalence with
edge-count-independent constants, yielding a symmetric positive definite
discrete inner product. A mesh-robust spectral bound
$\lambda_{\max}\big((\hat{\mathbf{M}}_h)^{-1}\mathbf{K}_h\big) \le
C_{\mathrm{inv}}^2/\hat{\beta}_* \cdot h^{-2}$ is established with constants
depending only on spatial dimension, polynomial order, and mesh regularity.
This delivers the classical diffusion-type CFL condition $\Delta
t=\mathcal{O}(h^2)$ for forward Euler stability and extends to higher-order
SSP-RK schemes, guaranteeing preservation of energy decay, positivity, and
discrete maximum principles. Numerical experiments on distorted quadrilaterals,
serendipity elements, and Voronoi polygons validate the theoretical
predictions: the lumped VEM with $k=1$ achieves optimal convergence rates
($\mathcal{O}(h)$ in $H^1$, $\mathcal{O}(h^2)$ in $L^2$) with no degradation
from geometric distortion or mass lumping, while SSP-RK integrators remain
stable under the predicted $\Delta t\propto h^{2}$ scaling

</details>


### [8] [An Integral Equation Method for Linear Two-Point Boundary Value Systems](https://arxiv.org/abs/2510.06678)
*Tianze Zhang,Yixuan Ma,Jun Wang*

Main category: math.NA

TL;DR: A fast, accurate black-box solver for two-point boundary value problems using well-conditioned integral equations with high-order Nystrom discretization and fast direct solvers.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for solving two-point boundary value systems that addresses conditioning issues in integral formulations.

Method: Uses integral equation-based approach with careful selection of background Green's function, high-order Nystrom discretization, and fast direct solvers on continuous level.

Result: Excellent performance in speed, accuracy, and robustness demonstrated through challenging numerical examples, with numerical study showing well-conditioned formulations.

Conclusion: The method provides an effective black-box solver for two-point boundary value problems with superior performance characteristics.

Abstract: We present an integral equation-based method for the numerical solution of
two-point boundary value systems. Special care is devoted to the mathematical
formulation, namely the choice of the background Green's function that leads to
a well-conditioned integral equation. We then make use of a high-order Nystrom
discretization and a fast direct solver on the continuous level to obtain a
black-box solver that is fast and accurate. A numerical study of the
conditioning of different integral formulations is carried out. Excellent
performance in speed, accuracy, and robustness is demonstrated with several
challenging numerical examples.

</details>


### [9] [Randomized Quasi-Monte Carlo with Importance Sampling for Functions under Generalized Growth Conditions and Its Applications in Finance](https://arxiv.org/abs/2510.06705)
*Jianlong Chen,Yu Xu,Xiaoqun Wang*

Main category: math.NA

TL;DR: This paper extends RQMC methods for functions with critical exponential growth (e^A||x||^2 where A=1/2), combining RQMC with importance sampling to maintain high-efficiency convergence rates, and integrates preintegration for discontinuous financial integrands.


<details>
  <summary>Details</summary>
Motivation: High-dimensional integrals of discontinuous functions with significant growth challenge RQMC error analysis, particularly in financial derivative pricing where functions exhibit generalized exponential growth conditions.

Method: Combines RQMC with importance sampling techniques, uses Gaussian proposals from Optimal Drift Importance Sampling, and integrates preintegration for discontinuous integrands to preserve growth conditions.

Result: Derived new error bounds for critical growth scenarios (A=1/2), verified light-tail conditions for ODIS proposals, proved preintegration preserves exponential growth, and validated effectiveness through numerical experiments achieving expected convergence rates.

Conclusion: The proposed RQMC-IS framework with preintegration successfully handles discontinuous functions with critical exponential growth in financial applications, extending theoretical guarantees beyond previous limitations.

Abstract: Many problems can be formulated as high-dimensional integrals of
discontinuous functions that often exhibit significant growth, challenging the
error analysis of randomized quasi-Monte Carlo (RQMC) methods. This paper
studies RQMC methods for functions with generalized exponential growth
conditions, with a special focus on financial derivative pricing. The main
contribution of this work is threefold. First, by combining RQMC and importance
sampling (IS) techniques, we derive a new error bound for a class of integrands
with the critical growth condition $e^{A\|\boldsymbol{x}\|^2}$ where $A = 1/2$.
This theory extends existing results in the literature, which are limited to
the case $A < 1/2$, and we demonstrate that by imposing a light-tail condition
on the proposal distribution in the IS, the RQMC method can maintain its
high-efficiency convergence rate even in this critical growth scenario. Second,
we verify that the Gaussian proposals used in Optimal Drift Importance Sampling
(ODIS) satisfy the required light-tail condition, providing rigorous
theoretical guarantees for RQMC-ODIS in critical growth scenarios. Third, for
discontinuous integrands from finance, we combine the preintegration technique
with RQMC-IS. We prove that this integrand after preintegration preserves the
exponential growth condition. This ensures that the preintegrated discontinuous
functions can be seamlessly incorporated into our RQMC-IS convergence
framework. Finally, numerical results validate our theory, showing that the
proposed method is effective in handling these problems with discontinuous
payoffs, successfully achieving the expected convergence rates.

</details>


### [10] [An Inertial Langevin Algorithm](https://arxiv.org/abs/2510.06723)
*Alexander Falk,Andreas Habring,Christoph Griesbacher,Thomas Pock*

Main category: math.NA

TL;DR: A novel accelerated sampling method for Gibbs distributions using inertial Langevin dynamics with noise rescaling, providing faster convergence than standard approaches.


<details>
  <summary>Details</summary>
Motivation: To improve sampling efficiency from Gibbs distributions by accelerating the unadjusted Langevin algorithm through inertia and noise rescaling.

Method: Introduces an inertia term similar to Polyak's heavy ball method with noise rescaling, interpreting it as discretization of kinetic Langevin dynamics.

Result: Proves ergodicity for strongly convex and smooth potentials, bounds discretization bias in Wasserstein-2 distance, and shows experimental acceleration in various applications including image denoising and molecular structure generation.

Conclusion: The proposed inertial Langevin method provides significant acceleration over standard approaches, working effectively even beyond the theoretical assumptions of strong convexity and smoothness.

Abstract: We present a novel method for drawing samples from Gibbs distributions with
densities of the form $\pi(x) \propto \exp(-U(x))$. The method accelerates the
unadjusted Langevin algorithm by introducing an inertia term similar to
Polyak's heavy ball method, together with a corresponding noise rescaling.
Interpreting the scheme as a discretization of \emph{kinetic} Langevin
dynamics, we prove ergodicity (in continuous and discrete time) for twice
continuously differentiable, strongly convex, and $L$-smooth potentials and
bound the bias of the discretization to the target in Wasserstein-2 distance.
In particular, the presented proofs allow for smaller friction parameters in
the kinetic Langevin diffusion compared to existing literature. Moreover, we
show the close ties of the proposed method to the over-relaxed Gibbs sampler.
The scheme is tested in an extensive set of numerical experiments covering
simple toy examples, total variation image denoising, and the complex task of
maximum likelihood learning of an energy-based model for molecular structure
generation. The experimental results confirm the acceleration provided by the
proposed scheme even beyond the strongly convex and $L$-smooth setting.

</details>


### [11] [Optimal network pricing with oblivious users: a new model and algorithm](https://arxiv.org/abs/2510.07157)
*Yixuan Li,Andersen Ang,Sebastian Stein*

Main category: math.NA

TL;DR: Proposes a new optimal network pricing model with oblivious users, formulates it as a constrained nonconvex stochastic quadratic program, and develops an efficient algorithm achieving 5x speedup.


<details>
  <summary>Details</summary>
Motivation: Traffic modeling is important for modern society, and existing models have constraints that need relaxation. The work addresses optimal network pricing under oblivious user behavior.

Method: Mathematically derives a new Onp formulation with decision-dependent modeling, expresses it as a constrained nonconvex stochastic quadratic program, and proposes an efficient algorithm using graph theory, sparse linear algebra, and stochastic approximation.

Result: The proposed algorithm achieves a 5x speedup by exploiting the sparsity structure of the model. The effectiveness of the algorithm and usefulness of the new Onp formulation are demonstrated.

Conclusion: The work successfully develops a new optimal network pricing formulation and an efficient solving algorithm that significantly improves computational performance while maintaining modeling accuracy.

Abstract: Traffic modeling is important in modern society. In this work we propose a
new model on the optimal network pricing (Onp) with the assumption of oblivious
users, in which the users remain oblivious to real-time traffic conditions and
others' behavior. Inspired by works on transportation research and network
pricing for selfish traffic, we mathematically derive and prove a new
formulation of Onp with decision-dependent modeling that relax certain existing
modeling constraints in the literature. Then, we express the Onp formulation as
a constrained nonconvex stochastic quadratic program with uncertainty, and we
propose an efficient algorithm to solve the problem, utilizing graph theory,
sparse linear algebra and stochastic approximation. Lastly, we showcase the
effectiveness of the proposed algorithm and the usefulness of the new Onp
formulation. The proposed algorithm achieves a 5x speedup by exploiting the
sparsity structure of the model.

</details>


### [12] [Greedy Thiele continued-fraction approximation on continuum domains in the complex plane](https://arxiv.org/abs/2510.07295)
*Tobin A. Driscoll,Yuxing Zhou*

Main category: math.NA

TL;DR: An adaptive greedy algorithm for Thiele continued-fraction approximation on continuum domains in the complex plane, with new efficient evaluation methods.


<details>
  <summary>Details</summary>
Motivation: To develop a faster alternative to the AAA algorithm for function approximation on continuum domains using Thiele continued fractions.

Method: Adaptive greedy algorithm that selects interpolation nodes from adaptively refined sample points on domain boundary, plus new algorithms for evaluating Thiele continued fractions using single floating-point division.

Result: Numerical experiments show continuum TCF is 2.5 to 8 times faster than AAA algorithm on functions defined on [-1,1] and unit circle.

Conclusion: The proposed continuum TCF method provides significantly faster function approximation compared to AAA while maintaining accuracy.

Abstract: We describe an adaptive greedy algorithm for Thiele continued-fraction
approximation of a function defined on a continuum domain in the complex plane.
The algorithm iteratively selects interpolation nodes from an adaptively
refined set of sample points on the domain boundary. We also present new
algorithms for evaluating Thiele continued fractions and their accessory
weights using only a single floating-point division. Numerical experiments
comparing the greedy TCF method with the AAA algorithm on several challenging
functions defined on the interval $[-1,1]$ and on the unit circle show that
continuum TCF is consistently 2.5 to 8 times faster than AAA.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [Log-free estimate for the resonant paraproduct in the 3D Navier-Stokes equations](https://arxiv.org/abs/2510.06246)
*Pylyp Cherevan*

Main category: math.AP

TL;DR: The paper establishes an a priori estimate for the resonant paraproduct in 3D Navier-Stokes equations without logarithmic loss, using geometric integration, wave-packet discretization, and anisotropic Strichartz estimates.


<details>
  <summary>Details</summary>
Motivation: To improve the understanding of the nonlinear term (u¬∑‚àá)u in 3D Navier-Stokes equations by obtaining better bounds for the resonant paraproduct component, eliminating the logarithmic loss that typically appears in such estimates.

Method: Combines phase-geometric integration by parts along adapted frames, wave-packet discretization at scale N^{-1/2}, anisotropic Strichartz estimates, bilinear decoupling on rank-3 phase surfaces, and energy arguments in H^{-1} for different angular regions.

Result: Proves the bound ‚à•R_N(u)‚à•_{H^{-1}} ‚â≤ N^{-1} ‚à•u‚à•_{H^{1/2}} ‚à•u‚à•_{H^{1}} with constant independent of dyadic frequency N, achieving scale-consistent estimate without logarithmic loss or smallness assumptions.

Conclusion: The analysis successfully removes the logarithmic loss in resonant paraproduct estimates for 3D Navier-Stokes, though restricted to a single component, with potential for extensions to broader contexts.

Abstract: We consider the resonant paraproduct (high-high $\to$ low regime) in the
nonlinearity $(u\cdot\nabla)u$ for the three-dimensional Navier-Stokes
equations. For sufficiently smooth, divergence-free u, we establish the a
priori estimate without logarithmic loss $$\|R_N(u)\|{\dot H^{-1}} \lesssim
N^{-1}\,\|u\|{\dot H^{1/2}}\,\|u\|_{\dot H^{1}},$$ with a constant independent
of the dyadic frequency $N$. The proof combines phase-geometric integration by
parts along an adapted frame, wave-packet discretization at scale $N^{-1/2}$,
and an anisotropic Strichartz estimate on time windows of length $N^{-1/2}$. In
the wide angular region we apply bilinear decoupling on a rank-3 phase surface;
in the geometry at hand the minimal curvature yields a gain of order
$N^{-1/6+o(1)}$ (with $o(1)\to 0$ as $N\to\infty$), which suffices to remove
the logarithmic loss. The contribution from the narrow region is handled
separately by an energy argument in $\dot H^{-1}$ using null-form suppression
near the interaction diagonal. The resulting bound is scale-consistent and
requires no smallness assumptions, only the divergence-free condition on $u$.
The analysis is restricted to a single resonant component of the paraproduct;
potential extensions are discussed.

</details>


### [14] [Regularity theory for mixed local-nonlocal problem involving general stable operators](https://arxiv.org/abs/2510.06569)
*Pedro Fellype Pontes,Minbo Yang*

Main category: math.AP

TL;DR: Study of regularity for solutions to linear elliptic equations with mixed local-nonlocal operators involving stable L√©vy type operators and H√∂lder continuous weights.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of solutions to elliptic equations that combine both local (divergence form) and nonlocal (L√©vy type) operators, which arise in various applications including mathematical physics and finance.

Method: Establish maximum principle and Liouville-type result in entire space, then use these to derive interior and boundary regularity under appropriate assumptions on forcing term f(x) and weight a(x).

Result: Successfully proved interior regularity and boundary regularity for solutions to the mixed local-nonlocal elliptic equation.

Conclusion: The developed maximum principle and Liouville-type result provide effective tools for analyzing regularity of solutions to mixed local-nonlocal elliptic equations with stable L√©vy operators and H√∂lder continuous coefficients.

Abstract: In this paper, we study the regularity of solutions to a linear elliptic
equation involving a mixed local-nonlocal operator of the form $$Lu -
\operatorname{div}\big(a(x)\nabla u(x)\big)= f, \quad \text{in } \Omega \subset
\mathbb{R}^n,$$ where $L$ is a general stable L\'{e}vy type operator and
$a(\cdot)$ is a positive H\"{o}lder continuous weight. By establishing a
maximum principle and a Liouville-type result in the entire space, we are able
to derive the interior regularity and the regularity up to the boundary of the
solutions under suitable assumptions on $f(x)$ and $a(x)$ .

</details>


### [15] [On a new region for the Lane-Emden conjecture in higher dimensions](https://arxiv.org/abs/2510.06613)
*Kui Li,Mingxiang Li,Juncheng Wei*

Main category: math.AP

TL;DR: The paper proves the Lane-Emden conjecture for dimensions n‚â•5 in the subcritical regime using Obata-type integral inequality, Picone's identity, and scaling invariance.


<details>
  <summary>Details</summary>
Motivation: To establish the non-existence of non-trivial, non-negative solutions to the Lane-Emden system in the subcritical regime, addressing a long-standing conjecture in partial differential equations.

Method: Employed Obata-type integral inequality, Picone's identity, and exploited the scaling invariance properties of the Lane-Emden system.

Result: Proved that the Lane-Emden conjecture holds for any dimension n‚â•5 and exponents satisfying p‚â•1, q‚â•1 with the given inequality condition.

Conclusion: The Lane-Emden conjecture is valid in the specified parameter range, confirming the absence of non-trivial solutions to the system in higher dimensions.

Abstract: We study the Lane-Emden conjecture, which asserts the non-existence of
non-trivial, non-negative solutions to the Lane-Emden system
  \[
  -\Delta u = v^p, \quad -\Delta v = u^q, \quad x \in \mathbb{R}^n\]
  in the subcritical regime. By employing an Obata-type integral inequality,
Picone's identity, and exploiting the scaling invariance of the system, we
prove that the conjecture holds for any dimension $n \geq 5$ and exponents
satisfying $p\geq 1,q\geq 1$, and
  \[
  \frac{1}{p+1} + \frac{1}{q+1} \geq 1 - \frac{2}{n} + \frac{4}{n^2}.
  \]

</details>


### [16] [Global weak solutions to nonlinear kinetic Fokker--Planck equations in bounded domains under physical initial data](https://arxiv.org/abs/2510.06656)
*Young-Pil Choi,Sihyun Song*

Main category: math.AP

TL;DR: Global existence of weak solutions for nonlinear kinetic Fokker-Planck equations with degenerate diffusion under inflow or partial absorption-reflection boundary conditions, using only physical assumptions on initial/boundary data.


<details>
  <summary>Details</summary>
Motivation: To establish existence theory for kinetic Fokker-Planck equations under only physically relevant conditions (finite mass, kinetic energy, entropy) without requiring additional regularity assumptions.

Method: Developed a new compactness principle based on weighted Fisher information to overcome lack of uniform ellipticity, yielding strong L^1 convergence of approximate solutions.

Result: Proved global existence of weak solutions under inflow or partial absorption-reflection boundary conditions with only physical assumptions on initial and boundary data.

Conclusion: The framework provides a robust existence theory that applies uniformly to both inflow and reflection boundary settings under physically relevant conditions only.

Abstract: We establish the global existence of weak solutions to a nonlinear kinetic
Fokker--Planck equation with degenerate diffusion, under either inflow or
partial absorption-reflection boundary conditions. The novelty of our approach
lies in constructing solutions under solely the physical assumptions on the
initial and boundary data, namely finite mass, kinetic energy, and entropy,
with no additional regularity imposed. To overcome the lack of uniform
ellipticity, we develop a new compactness principle based on weighted Fisher
information, which yields strong $L^1$ convergence of approximate solutions.
This framework provides a robust existence theory under only the physically
relevant conditions, and applies uniformly to both inflow and reflection
boundary settings.

</details>


### [17] [Bifurcations of viscous boundary layers in the half space](https://arxiv.org/abs/2510.06715)
*Dongfen Bian,Emmanuel Grenier,G√©rard Iooss*

Main category: math.AP

TL;DR: The paper proves that shear flows undergo a Hopf bifurcation near their upper marginally stable curve, leading to space periodic traveling wave solutions of the Navier-Stokes equations, which can be stable under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To establish the existence of Hopf bifurcation in shear flows near the upper marginally stable curve, addressing challenges posed by the essential spectrum of the linearized operator.

Method: Mathematical analysis and proof techniques applied to the incompressible Navier-Stokes equations, overcoming difficulties from the essential spectrum containing the negative real axis.

Result: Existence of space periodic traveling wave solutions near the upper marginally stable curve, with potential linear and nonlinear asymptotic stability in super-critical cases.

Conclusion: Shear flows exhibit Hopf bifurcation behavior near the upper marginally stable curve, producing stable periodic solutions under appropriate conditions.

Abstract: It is well-established that shear flows are linearly unstable provided the
viscosity is small enough, when the horizontal Fourier wave number lies in some
interval, between the so-called lower and upper marginally stable curves. In
this article, we prove that, under a natural spectral assumption, shear flows
undergo a Hopf bifurcation near their upper marginally stable curve. In
particular, close to this curve, there exists space periodic traveling waves
solutions of the full incompressible Navier-Stokes equations. For the
linearized operator, the occurrence of an essential spectrum containing the
entire negative real axis causes certain difficulties which are overcome.
Moreover, if this Hopf bifurcation is super-critical, these time and space
periodic solutions are linearly and nonlinearly asymptotically stable.

</details>


### [18] [On some divergence-form singular elliptic equations with codimension-two boundary: $L^p$-estimates](https://arxiv.org/abs/2510.06716)
*Jie Ji,Jingang Xiong*

Main category: math.AP

TL;DR: Global weighted L^p gradient estimate for divergence-form elliptic equations with VMO coefficients and singularities on co-dimension two boundary.


<details>
  <summary>Details</summary>
Motivation: To establish gradient estimates for elliptic equations with singular coefficients and boundary singularities, which are important for understanding regularity properties in domains with lower-dimensional boundaries.

Method: Develop weighted L^p estimates for solutions to divergence-form elliptic equations where coefficients belong to weighted VMO spaces and equations have singularities on co-dimension two boundaries.

Result: Proved global weighted L^p estimates for the gradient of solutions to such elliptic equations.

Conclusion: Successfully established gradient regularity results for elliptic equations with singular coefficients and boundary singularities, extending previous results to more general settings.

Abstract: We establish a global weighted $L^p$ estimate for the gradient of the
solution to a divergence-form elliptic equations, where the coefficients are in
a weighted VMO space and the equations have singularities on a co-dimension two
boundary.

</details>


### [19] [Accelerated and fast magnetic reconnection through enhanced resistive dissipation for MHD equations](https://arxiv.org/abs/2510.06801)
*Gennaro Ciampa,Renato Luc√†*

Main category: math.AP

TL;DR: Magnetic reconnection occurs faster than resistive time scales in 3D MHD due to advective effects, with new enhanced diffusion estimates.


<details>
  <summary>Details</summary>
Motivation: To understand magnetic reconnection in 3D MHD and demonstrate how advection can accelerate reconnection beyond resistive time scales.

Method: Developed new analytical estimates for enhanced diffusion of high Sobolev norms and applied them to 3D periodic MHD equations.

Result: First analytical proof showing advection actively drives magnetic reconnection on time scales shorter than resistive ones.

Conclusion: Advective effects enable faster magnetic reconnection than previously thought, with new diffusion estimates having broader applications.

Abstract: We consider the phenomenon of magnetic reconnection, namely a change in the
topology of magnetic lines, for sufficiently regular solutions of the
three-dimensional periodic magnetohydrodynamic (MHD) equations. We provide
examples where magnetic reconnection occurs on time scales shorter than the
resistive one, due to enhanced dissipation emerging from advective effects.
This is the first analytical result where the advection term plays an active
role in the reconnection process. A key aspect of our approach is a new
estimate for enhanced diffusion of high Sobolev norms, which is of independent
interest beyond its application to the MHD equations.

</details>


### [20] [Functional calculus for Safarov pseudo-differential operators](https://arxiv.org/abs/2510.06859)
*Santiago G√≥mez Cobos,Michael Ruzhansky*

Main category: math.AP

TL;DR: Develops holomorphic functional calculus for pseudo-differential operators on Riemannian manifolds with linear connections, leading to Szeg≈ë-type theorems, heat kernel expansions, and spectral Œ∂-function calculations.


<details>
  <summary>Details</summary>
Motivation: To extend functional calculus techniques to pseudo-differential operators defined on Riemannian manifolds with arbitrary linear connections, not necessarily metric-compatible.

Method: Uses Safarov's global pseudo-differential classes Œ®·µ®,Œ¥·µê(Œ©·µè,‚àá,œÑ) and develops holomorphic functional calculus for operators in these classes.

Result: Establishes Szeg≈ë-type theorem, derives asymptotic expansion of heat kernel trace, and calculates associated spectral Œ∂-functions.

Conclusion: The developed functional calculus provides powerful tools for spectral analysis of pseudo-differential operators on manifolds with general linear connections.

Abstract: Given a smooth, closed Riemannian manifold $(M,g)$ equipped with a linear
connection $\nabla$ (not necessarily metric), we develop the holomorphic
functional calculus for operators belonging to the global pseudo-differential
classes $\Psi_{\rho, \delta}^m\left(\Omega^\kappa, \nabla, \tau\right)$
introduced by Safarov. As a consequence of our main result, we establish a
Szeg\"o type-theorem, derive asymptotic expansion of the heat kernel trace, and
calculate some associated spectral $\zeta$-functions.

</details>


### [21] [On Morawetz estimates for the elastic wave equation](https://arxiv.org/abs/2510.06958)
*Seongyeon Kim,Ihyeok Seo*

Main category: math.AP

TL;DR: Morawetz-type estimates for elastic wave equations with singular weights show space-time weights allow stronger singularities and weaker regularity than spatial weights.


<details>
  <summary>Details</summary>
Motivation: To establish improved Morawetz-type estimates for elastic wave equations using singular weights, comparing spatial vs space-time weight performance.

Method: Develop Morawetz-type estimates for elastic wave equations with singular weights of form |x|^{-Œ±} and |(x,t)|^{-Œ±}, analyzing singularity strength and regularity requirements.

Result: Space-time weights |(x,t)|^{-Œ±} permit stronger singularities and require weaker regularity assumptions on initial data compared to spatial weights |x|^{-Œ±}.

Conclusion: Space-time singular weights provide more favorable conditions for Morawetz estimates in elastic wave equations than purely spatial weights.

Abstract: We establish Morawetz-type estimates for solutions to the elastic wave
equation with singular weights of the form $|x|^{-\alpha}$ or
$|(x,t)|^{-\alpha}$. In particular, we show that space-time weights
$|(x,t)|^{-\alpha}$ admit stronger singularities and require weaker regularity
assumptions on the initial data compared to purely spatial weights
$|x|^{-\alpha}$.

</details>


### [22] [Non-uniqueness in Mean Curvature Flow: Non-canonical solutions via the parabolic Allen--Cahn](https://arxiv.org/abs/2510.06979)
*J. M. Daniels-Holgate*

Main category: math.AP

TL;DR: The paper shows that interior weak mean curvature flows exist strictly inside fattening regions, constructed as limits of parabolic Œµ-Allen-Cahn solutions, providing first examples of closed, non-trivial, non-canonical integral Brakke motions.


<details>
  <summary>Details</summary>
Motivation: To determine whether weak mean curvature flows can exist strictly inside fattening regions when the flow evolves non-uniquely, beyond the known outermost flows on the boundary.

Method: General construction of non-outermost flows as limits of solutions to the parabolic Œµ-Allen-Cahn equation from low regularity initial data, including fractal sets.

Result: Demonstrates existence of interior flows inside fattening, providing first examples of closed, non-trivial, non-canonical integral Brakke motions, and establishes existence of integral Brakke motions from fractal sets.

Conclusion: Interior weak mean curvature flows do exist strictly inside fattening regions, resolving an open question and extending the theory to include flows from low regularity initial data like fractal sets.

Abstract: When mean curvature flow evolves non-uniquely, the flow is said to fatten.
The work of Ilmanen shows that any weak MCF is supported inside the fattening,
and work of Hershkovits--White identified canonical weak flows supported on the
boundary of the fattening, known as the outermost flows. It is natural to ask,
when the flow fattens, are there weak mean curvature flows supported strictly
inside the fattening? Outside of some special cases (e.g. flow from cones),
this question was entirely open. We show these interior flows exist, providing
a general construction for non-outermost flows as limits of solutions to the
parabolic $\varepsilon$-Allen--Cahn. This gives the first examples of closed,
non-trivial, non-canonical, integral Brakke motions. As part of this
construction, we study the $\varepsilon$-Allen--Cahn flow from low regularity
initial data, and our results demonstrate the existence of integral Brakke
motions from fractal sets. This includes the existence portion of Hershkovits's
work on mean curvature flow from Reifenberg sets.

</details>


### [23] [Relaxation of Non-Convex Integral Functionals in the Multidimensional Scalar Case](https://arxiv.org/abs/2510.07085)
*Tommaso Bertin,Paulin Huguet*

Main category: math.AP

TL;DR: Study of integral functionals in Sobolev spaces, focusing on non-convex cases and preventing the Lavrentiev phenomenon by determining lower semicontinuous envelopes with fixed Lipschitz boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To address challenges in non-convex integral functionals and prevent the Lavrentiev phenomenon, which causes discrepancies between different minimization approaches.

Method: Analyze integral functionals of the form E[f]:u‚Ü¶‚à´_Œ© f(x,u(x),‚àáu(x)) dx, determining lower semicontinuous envelopes with respect to various topologies under fixed Lipschitz Dirichlet boundary conditions.

Result: Developed formulations for the lower semicontinuous envelope of the functional E[f] under different topological settings with boundary constraints.

Conclusion: The research provides mathematical tools to handle non-convex integral functionals and prevent the Lavrentiev phenomenon through proper formulation of lower semicontinuous envelopes with boundary conditions.

Abstract: We study integral functionals defined on scalar Sobolev spaces of the form
$$E[f]:u\mapsto \int_\Omega f(x,u(x),\nabla u(x)) d x,$$
  with an emphasis on the non-convex case, and the difficulties it involves to
prevent the Lavrentiev phenomenon. We determine a formulation of the lower
semicontinuous envelope of $E[f]$ with respect to various topologies and with
fixed Lipschitz Dirichlet boundary conditions.

</details>


### [24] [Stability of non-conservative cross diffusion model and approximation by stochastic particle systems](https://arxiv.org/abs/2510.07138)
*Vincent Bansaye,Alexandre Bertolino,Ayman Moussa*

Main category: math.AP

TL;DR: Non-conservative deterministic cross diffusion models can be approximated by stochastic population models when populations become locally large, with quantitative convergence estimates.


<details>
  <summary>Details</summary>
Motivation: To establish the relationship between deterministic cross diffusion models and stochastic population models, and provide quantitative approximation results for large populations.

Method: Extension of stability estimates via duality approach under smallness condition, development of large deviation estimates for structured population models, and martingale estimates in H^{-1} space.

Result: Proved that deterministic cross diffusion models are approximated by stochastic population models when populations become locally large, with convergence as population per site and number of sites go to infinity.

Conclusion: The study provides rigorous mathematical foundation connecting deterministic and stochastic population models, with improved approximation results even in the conservative case.

Abstract: We study the stability of non-conservative deterministic cross diffusion
models and prove that they are approximated by stochastic population models
when the populations become locally large. In this model, the individuals of
two species move, reproduce and die with rates sensitive to the local densities
of the two species. Quantitative estimates are given and convergence is
obtained soon as the population per site and the number of sites go to
infinity. The proofs rely on the extension of stability estimates via duality
approach under a smallness condition and the development of large deviation
estimates for structured population models, which are of independent interest.
The proofs also involve martingale estimates in H^{-1} and improve the
approximation results in the conservative case as well.

</details>


### [25] [Derivation of the fourth-order DLSS equation with nonlinear mobility via chemical reactions](https://arxiv.org/abs/2510.07149)
*Alexander Mielke,Andr√© Schlichting,Artur Stephan*

Main category: math.AP

TL;DR: The paper derives the fourth-order DLSS equation from a chemical reaction network interpretation, showing it can yield either classical DLSS or a variant with nonlinear mobility depending on rates, and identifies the gradient structure via EDP convergence.


<details>
  <summary>Details</summary>
Motivation: To provide a physical interpretation of the DLSS equation through chemical reaction networks and understand its limiting gradient structure with nonlinear mobility.

Method: Derivation based on chemical reaction network interpretation, considering rate equations on discretized circle for particle jumping processes, and using EDP convergence to identify limiting gradient structure.

Result: Obtained either classical DLSS equation or variant with nonlinear mobility depending on rates; identified gradient structure driven by entropy with generalized diffusive transport; found traveling wave solutions with algebraic tails or compactly supported polynomials.

Conclusion: The DLSS equation with power-type mobility shares qualitative similarities with fast diffusion and porous medium equations, exhibiting different types of traveling wave solutions depending on the mobility type.

Abstract: We provide a derivation of the fourth-order DLSS equation based on an
interpretation as a chemical reaction network. We consider the rate equation on
the discretized circle for a process in which pairs of particles occupying the
same site simultaneously jump to the two neighboring sites; the reverse process
involves pairs of particles at adjacent sites simultaneously jumping back to
the site located between them. Depending on the rates, in the
vanishing-mesh-size limit we obtain either the classical DLSS equation or a
variant with nonlinear mobility of power type. Via EDP convergence, we identify
the limiting gradient structure to be driven by entropy with respect to a
generalization of diffusive transport with nonlinear mobility. Interestingly,
the DLSS equation with power-type mobility shares qualitative similarities with
the fast diffusion and porous medium equation, since we find traveling wave
solutions with algebraic tails or compactly supported polynomials,
respectively.

</details>


### [26] [L^p-quasicontractiveness and Kernel estimates for semigroups generated by systems of elliptic operators](https://arxiv.org/abs/2510.07216)
*L. Angiuli,E. M. Mangino,L. Lorenzi*

Main category: math.AP

TL;DR: This paper studies systems of strongly coupled elliptic operators with unbounded coefficients, showing they generate quasi-contractive semigroups in L^p-spaces and establishing new kernel estimates using a distance function that accounts for coefficient growth.


<details>
  <summary>Details</summary>
Motivation: To analyze systems of elliptic operators with unbounded coefficients and establish functional analytic properties like semigroup generation and kernel estimates, which are important for understanding the behavior of solutions to such systems.

Method: The authors associate quasi-contractive semigroups in L^p-spaces with the elliptic operators and derive kernel estimates using a distance function that incorporates the growth of diffusion coefficients and potential terms at infinity.

Result: The paper shows that these elliptic operators generate quasi-contractive semigroups in L^p-spaces for p in an interval containing 2, and establishes new kernel estimates with respect to a growth-accounting distance function.

Conclusion: The results provide important functional analytic properties for systems of strongly coupled elliptic operators with unbounded coefficients, including semigroup generation and refined kernel estimates that account for coefficient growth behavior.

Abstract: This paper focuses on systems of strongly coupled elliptic operators whose
coefficients may be unbounded and are defined on a domain $\Omega \subseteq
\mathbb{R}^d$. It is shown that a quasi-contractive semigroup in $L^p$-spaces
can be associated with such operators for values of $p$ belonging to an
interval that contains $2$ as an interior point. Then, under refined
assumptions and considering systems of elliptic operators coupled up to first
order, new kernel estimates are established with respect to a distance function
that accounts for the growth of the diffusion coefficients and the potential
term at infinity.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [27] [Integration of Silica in G4CMP for Phonon Simulations: Framework and Tools for Material Integration](https://arxiv.org/abs/2510.06595)
*Caitlyn Stone-Whitehead,Israel Hernandez,Connor Bray,Allison Davenport,Spencer Fretwell,Abigail Gillespie,Joren Husic,Mingyu Li,Andrew Marino,Kyle Leach,Bismah Rizwan,Wouter Van De Pontseele,Grace Wagner*

Main category: physics.comp-ph

TL;DR: G4CMP provides tools for phonon and charge dynamics in cryogenic materials, with new formalism for implementing phonon simulations in custom materials, demonstrated through silica phonon transport analysis for BeEST experiments.


<details>
  <summary>Details</summary>
Motivation: To support the superconducting qubit and low-threshold detector community in implementing phonon simulations in custom materials using G4CMP, particularly for BSM physics experiments.

Method: Developed technical formalism for phonon simulations in custom materials, conducted detailed analysis of silica phonon transport properties as case study, and created Python-based tools for material implementation.

Result: Successfully implemented phonon simulation capabilities in G4CMP, with specific analysis of silica phonon transport relevant to BeEST-style experiments, and provided user tools for custom material implementation.

Conclusion: G4CMP now offers enhanced capabilities for phonon simulations in custom materials, supporting the superconducting detector community in BSM physics research with practical tools and demonstrated applications.

Abstract: Superconducting detectors with sub-eV energy resolution have demonstrated
success setting limits on Beyond the Standard Model (BSM) physics due to their
unique sensitivity to low-energy events. G4CMP, a Geant4-based extension for
condensed matter physics, provides a comprehensive toolkit for modeling phonon
and charge dynamics in cryogenic materials. This paper introduces a technical
formalism to support the superconducting qubit and low-threshold detector
community in implementing phonon simulations in custom materials into the
G4CMP. As a case study, we present the results of a detailed analysis of silica
phonon transport properties relevant for simulating substrate backgrounds in
Beryllium Electron capture in Superconducting Tunnel junctions (BeEST)-style
experiments using G4CMP. Additionally, Python-based tools were developed to aid
users in implementing their own materials and are available on the G4CMP
repository.

</details>


### [28] [GSM: GPU Accelerated Rare Events Sampling with Machine Learning Potentials](https://arxiv.org/abs/2510.06873)
*Haoting Zhang,Jiuyang Shi,Qiuhan Jia,Junjie Wang,Jian Sun*

Main category: physics.comp-ph

TL;DR: GSM is a GPU-based Metadynamics package that enables full-life-cycle enhanced sampling for large-scale molecular dynamics simulations using machine learning potentials.


<details>
  <summary>Details</summary>
Motivation: With the rise of GPU computing and machine learning potentials, high-accuracy large-scale MD simulations became feasible, but existing GPU-based enhanced sampling tools haven't kept pace with this progress.

Method: Developed GPU Sampling MetaD (GSM) package that leverages machine learning potentials to perform Metadynamics simulations on single GPUs for systems with millions of atoms.

Result: The package successfully performs high-precision rare event sampling for large systems on single GPUs, demonstrating capability across diverse atomic systems with particular efficiency in large-scale systems.

Conclusion: GSM provides a potential solution to many size-dependent problems by enabling full GPU-accelerated Metadynamics simulations for large systems using machine learning potentials.

Abstract: Enhanced sampling has achieved considerable success in molecular dynamics
(MD) simulations of rare events. Metadynamics (MetaD), owing to its excellent
compatibility with MD engines, became one of the most popular enhanced sampling
methods. With the boom of GPU computing and the advent of machine learning
potentials (MLPs), high-accuracy, large-scale MD simulations have gradually
become feasible. However, the corresponding GPU-based enhanced sampling tools
have not yet been well adapted to this progress. To enable full-life-cycle GPU
MetaD simulations, we propose the GPU Sampling MetaD (GSM) package. By
leveraging MLPs, it is feasible to perform high-precision rare event sampling
for systems comprising millions of atoms on a typical single GPU, which offers
a potential solution to many size-dependent problems. By conducting sampling in
several classical systems, the results sufficiently demonstrate the capability
of this package to simulate diverse atomic systems, especially efficient in
large scale systems.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [29] [Influence of coil geometry and coil-plasma distance on the magnetic field approximation error](https://arxiv.org/abs/2510.06773)
*Wadim Gerner*

Main category: physics.plasm-ph

TL;DR: The paper analyzes how coil geometry and coil-plasma distance affect magnetic field errors from electric current noise, finding that larger reach and smaller enclosed volume reduce noise effects, and that pointwise field error is controlled by average error times (distance)^{-3/2}.


<details>
  <summary>Details</summary>
Motivation: To understand how coil geometry and positioning affect magnetic field precision in plasma applications, specifically addressing electric current noise propagation and field error control.

Method: Analytical investigation using geometric quantities (reach and enclosed volume) and mathematical formulations to derive quantitative relationships between coil parameters and magnetic field errors.

Result: Found that larger coil reach and smaller enclosed volume reduce current noise effects on magnetic fields, and established that pointwise field error scales with average error times (coil-plasma distance)^{-3/2}.

Conclusion: Coil geometry optimization (maximizing reach, minimizing volume) and proper coil-plasma distance selection are crucial for controlling magnetic field precision in plasma systems.

Abstract: We investigate analytically two questions:
  1) How does the coil geometry influence the effect of electric current noise
on the induced magnetic field?
  2) How does the coil-plasma distance influence our ability to control the
pointwise magnetic field error in terms of the average magnetic field error?
  Regarding (1), we argue that the main geometric quantities of interest are
the notion of reach and the volume of the region enclosed by the coils. Our
main finding is a quantitative formula which shows that the larger the reach
and the smaller the volume of the region enclosed by the coils, the smaller is
the influence of the electric current uncertainty on the magnetic fields.
  Regarding (2), we show that the pointwise magnetic field error can be
controlled (modulo an explicit constant) by the average square-magnetic
field-error times $(\text{coil-plasma distance})^{-\frac{3}{2}}$.

</details>


### [30] [3D simulations of negative streamers in CO$_2$ with admixtures of C$_4$F$_7$N](https://arxiv.org/abs/2510.06794)
*Thomas J. G. Smits,Jannis Teunissen,Ute Ebert*

Main category: physics.plasm-ph

TL;DR: The paper investigates whether conventional fluid models can simulate streamer discharges in CO2 with C4F7N admixtures (1% or 10%) as eco-friendly alternatives to SF6 gas insulation. It compares 3D fluid simulations using local field approximation (LFA) and local energy approximation (LEA) against particle simulations for negative streamers.


<details>
  <summary>Details</summary>
Motivation: CO2 with C4F7N admixtures could serve as eco-friendly alternatives to the extreme greenhouse gas SF6 in high-voltage insulation. Streamer discharges in these gases differ from air due to rapid conductivity decay and lack of effective photoionization mechanisms, making discharge growth more stochastic.

Method: The study reviews cross section databases for C4F7N and CO2, compares two-term Boltzmann solver with Monte Carlo method for computing reaction and transport coefficients, and performs 3D simulations comparing fluid models (LFA and LEA) against particle simulations for negative streamers.

Result: Particle and fluid models show generally similar results. Key differences include particle simulations being intrinsically stochastic leading to more branching, and the LEA model not showing better agreement with particle simulations than the LFA model. Different boundary conditions on the negative rod electrode also affect results.

Conclusion: Conventional fluid models can be used to simulate streamers in CO2 with C4F7N admixtures, with fluid and particle models producing generally similar results despite some differences in stochastic behavior and branching patterns.

Abstract: CO$_2$ with an admixture of C$_4$F$_7$N could serve as an eco-friendly
alternative to the extreme greenhouse gas SF$_6$ in high-voltage insulation.
Streamer discharges in such gases are different from those in air due to the
rapid conductivity decay in the streamer channels. Furthermore, since no
effective photoionisation mechanism is known, we expect discharge growth to be
more stochastic than in air. In this paper we investigate whether conventional
fluid models can be used to simulate streamers in CO$_2$ with admixtures of
C$_4$F$_7$N of 1 or 10%. We focus on 3D simulations of negative streamers.
First we review cross section databases for C$_4$F$_7$N and CO$_2$. Then we
compare a two-term Boltzmann solver with a Monte Carlo method to compute
reaction and transport coefficients from the cross sections. Afterwards we
compare 3D fluid simulations with the local field (LFA) or local energy
approximation (LEA) against particle simulations. In general, we find that the
results of particle and fluid models are quite similar. One difference we
observe is that particle simulations are intrinsically stochastic, leading to
more branching. Furthermore, the LEA model does not show better agreement with
the particle simulations than the LFA model. We also discuss the effect and
choice of different boundary conditions on the negative rod electrode.

</details>


### [31] [Chaotic Motion of Ions In Finite-amplitude Low-frequency Alfv√©n Waves](https://arxiv.org/abs/2510.07144)
*Jingyu Peng,Jiansen He*

Main category: physics.plasm-ph

TL;DR: Analysis of nonlinear interactions between monochromatic Alfv√©n waves and ions, showing chaotic particle motion quantified by Lyapunov exponents and Chaos Ratio, with chaos threshold determined by wave-induced field line curvature.


<details>
  <summary>Details</summary>
Motivation: Finite-amplitude low-frequency Alfv√©n waves are common in space plasmas and play crucial roles in ion heating, but their nonlinear interactions with ions need better understanding.

Method: Quantified chaotic behavior using maximum Lyapunov exponent and introduced Chaos Ratio (CR) to characterize proportion of chaotic particles. Analyzed magnetic moment changes and identified chaos mechanism as pitch-angle scattering from wave-induced field line curvature.

Result: Found chaos depends on particles' initial states. Determined chaos threshold as CR=0.01 contour line. Established chaos condition as effective relative curvature radius P_eff < C. Analytical chaos region in (k_x, k_z, B_w) parameter space matches well with CR=0.01 threshold.

Conclusion: Chaotic ion motion in Alfv√©n waves is driven by wave-induced field line curvature causing pitch-angle scattering, with analytical chaos conditions that agree well with numerical simulations.

Abstract: Finite-amplitude low-frequency Alfv\'en waves (AWs) are commonly found in
plasma environments, such as space plasmas, and play a crucial role in ion
heating. In this study, we examine the nonlinear interactions between
monochromatic AWs and ions. When the wave amplitude and propagation angle lie
within certain ranges, particle motion becomes chaotic. We quantify this
chaotic behavior using the maximum Lyapunov exponent, $\lambda_m$, and find
that chaos depends on the particles' initial states. To characterize the
proportion of chaotic particles across different initial states, we introduce
the Chaos Ratio ($CR$). The threshold for the onset of global chaos is
calculated as the contour line of $CR=0.01$. We analyze changes in the magnetic
moment during particle motion and identify the physical image of chaos as
pitch-angle scattering caused by wave-induced field line curvature (WFLC).
Consequently, the condition for chaos can be expressed as the effective
relative curvature radius $P_{eff.}<C$, with $C$ being a constant. We
analytically determine the chaos region in the $(k_x,\,k_z,\,B_w)$ parameter
space, and the results show excellent agreement with the global chaos threshold
given by $CR=0.01$.

</details>


### [32] [GyroSwin: 5D Surrogates for Gyrokinetic Plasma Turbulence Simulations](https://arxiv.org/abs/2510.07314)
*Fabian Paischer,Gianluca Galletti,William Hornsby,Paul Setinek,Lorenzo Zanisi,Naomi Carey,Stanislas Pamela,Johannes Brandstetter*

Main category: physics.plasm-ph

TL;DR: GyroSwin is a scalable 5D neural surrogate that models nonlinear gyrokinetic simulations to capture plasma turbulence effects neglected by reduced-order models, achieving three orders of magnitude speedup while maintaining physical accuracy.


<details>
  <summary>Details</summary>
Motivation: Plasma turbulence significantly impairs plasma confinement in nuclear fusion, and current reduced-order models omit nonlinear effects from full 5D dynamics. There's a need for accurate yet efficient models that capture these physical phenomena for reactor design.

Method: Extends hierarchical Vision Transformers to 5D, introduces cross-attention and integration modules for latent 3D-5D interactions between electrostatic potential fields and distribution function, and performs channelwise mode separation inspired by nonlinear physics.

Result: Outperforms widely used reduced numerics on heat flux prediction, captures turbulent energy cascade, reduces computational cost by three orders of magnitude compared to fully resolved nonlinear gyrokinetics, and shows promising scaling up to one billion parameters.

Conclusion: GyroSwin provides a scalable neural surrogate for gyrokinetic simulations that maintains physical verifiability while dramatically improving computational efficiency, paving the way for practical plasma turbulence modeling in fusion energy research.

Abstract: Nuclear fusion plays a pivotal role in the quest for reliable and sustainable
energy production. A major roadblock to viable fusion power is understanding
plasma turbulence, which significantly impairs plasma confinement, and is vital
for next-generation reactor design. Plasma turbulence is governed by the
nonlinear gyrokinetic equation, which evolves a 5D distribution function over
time. Due to its high computational cost, reduced-order models are often
employed in practice to approximate turbulent transport of energy. However,
they omit nonlinear effects unique to the full 5D dynamics. To tackle this, we
introduce GyroSwin, the first scalable 5D neural surrogate that can model 5D
nonlinear gyrokinetic simulations, thereby capturing the physical phenomena
neglected by reduced models, while providing accurate estimates of turbulent
heat transport.GyroSwin (i) extends hierarchical Vision Transformers to 5D,
(ii) introduces cross-attention and integration modules for latent
3D$\leftrightarrow$5D interactions between electrostatic potential fields and
the distribution function, and (iii) performs channelwise mode separation
inspired by nonlinear physics. We demonstrate that GyroSwin outperforms widely
used reduced numerics on heat flux prediction, captures the turbulent energy
cascade, and reduces the cost of fully resolved nonlinear gyrokinetics by three
orders of magnitude while remaining physically verifiable. GyroSwin shows
promising scaling laws, tested up to one billion parameters, paving the way for
scalable neural surrogates for gyrokinetic simulations of plasma turbulence.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [33] [Local Order Average-Atom Interatomic Potentials](https://arxiv.org/abs/2510.06459)
*Chloe A. Zeller,Ronald E. Miller,Ellad B. Tadmor*

Main category: cond-mat.mtrl-sci

TL;DR: A new local-order average-atom interatomic potential (LOAA IP) is developed to account for short-range order effects in materials, extending the standard average-atom approach that only works for random alloys.


<details>
  <summary>Details</summary>
Motivation: Standard average-atom interatomic potentials are limited to random alloys and cannot capture local ordering effects that are important in complex materials like high-entropy alloys.

Method: The LOAA IP incorporates information from partial radial distribution functions to model short-range order, requiring smaller system sizes for statistically converged results at reduced computational cost.

Result: Validation on 2D binary hexagonal crystals and 3D Fe-Ni-Cr and Ni-Al alloys shows LOAA accurately captures elastic properties, material properties, and phase transformations, matching results from standard interatomic potentials.

Conclusion: The LOAA method provides an efficient computational approach for modeling materials with local ordering effects while maintaining accuracy comparable to more expensive standard methods.

Abstract: An extension to the effective average-atom (AA) interatomic potential (IP)
that accounts for local ordering effects is derived. While the AA approach is
only valid for random alloys, the new local-order average-atom (LOAA) IP
accounts for short-range order within a material by utilizing information from
partial radial distribution functions. Simulations with a LOAA potential
require smaller system sizes to achieve statistically converged results and
therefore can be used to model complex materials where short-range order
effects are important, such as high-entropy alloys, at a fraction of the
computational cost of standard IPs. The method is validated for a
two-dimensional (2D) binary hexagonal crystal with Lennard-Jones (LJ)
interactions, and for three-dimensional (3D) Fe$_{(1-x)/2}$Ni$_{(1-x)/2}$Cr$_x$
and Ni$_{0.67}$Al$_{0.33}$ alloys modeled via embedded-atom method (EAM)
potentials. For the 2D crystal we obtain a local ordering phase diagram in
terms of the LJ parameters and demonstrate that in all cases the LOAA
formulation obtains elastic properties that match the true-species case using
standard IPs. The 3D alloy examples further demonstrate the ability of this
method to accurately capture other material properties and phase
transformations.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [34] [Machine Learning enhanced parametric Reynolds-averaged Navier-Stokes equations at the full and reduced order levels](https://arxiv.org/abs/2510.06992)
*Davide Oberto,Maria Strazzullo,Stefano Berrone*

Main category: physics.flu-dyn

TL;DR: Machine learning enhances RANS models for turbulent flow prediction, with PODNN outperforming PODG in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To accelerate predictions for real-time applications and many-query scenarios by improving Reynolds-Averaged Navier-Stokes (RANS) models using machine learning at both Full Order Model (FOM) and Reduced Order Model (ROM) levels.

Method: Integrates machine learning with RANS models using ŒΩt-Vector Basis Neural Network for FOM snapshots, compares POD Galerkin (PODG) and POD Neural Network (PODNN) approaches for ROM construction, and validates on turbulent flow in a square duct benchmark.

Result: PODG method proves unstable and inaccurate for turbulent flow prediction, while PODNN demonstrates superior performance in terms of accuracy and computational efficiency.

Conclusion: POD Neural Network approach is more effective than traditional POD Galerkin method for turbulent flow reduced order modeling, offering better accuracy and computational performance.

Abstract: In this contribution, we focus on the Reynolds-Averaged Navier-Stokes (RANS)
models and their exploitation to build reliable reduced order models to further
accelerate predictions for real-time applications and many-query scenarios.
Specifically, we investigate how machine learning can be employed to enhance
the predictive capabilities of the model, both at the Full Order Model (FOM)
and Reduced Order Model (ROM) levels. We explore a novel integration of these
two areas. We generate the FOM snapshots, essential for ROM construction, using
a data-driven RANS model: the $\nu_t$-Vector Basis Neural Network. This is the
first time that these machine learning procedure generalizes a large parametric
variation, and we propose tailored training strategies to increase the accuracy
of the FOM model. At the ROM level, we compare the results obtained by standard
Proper Orthogonal Decomposition in an intrusive Galerkin setting (PODG) and POD
Neural Network approach (PODNN). The numerical validation is based on a classic
turbulent flow benchmark: the flow in a square duct. Our investigation reveals
that the PODG method, proves unstable and inaccurate for turbulent flow
prediction, while PODNN demonstrates superior performance in terms of accuracy
and computational efficiency.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [35] [Prescribed $p$-curvature problem on Riemannian manifolds with negative curvature](https://arxiv.org/abs/2510.06577)
*Jiaogen Zhang*

Main category: math.DG

TL;DR: This paper solves the prescribed curvature problem for p-fold sums of modified Shouten tensor on compact negatively curved Riemannian manifolds, establishing existence of conformal metrics with constant p-curvature.


<details>
  <summary>Details</summary>
Motivation: To extend Gursky and Viaclovsky's work on prescribed curvature problems from p=1 to general p-fold sums of modified Shouten tensor on negatively curved manifolds.

Method: Investigates the prescribed curvature problem defined by p-fold sums of modified Shouten tensor using conformal geometry techniques on compact Riemannian manifolds with negative curvature.

Result: Establishes existence of a metric conformal to the original metric such that the p-curvature (p-fold sum of modified Shouten tensor) is constant.

Conclusion: The paper resolves the prescribed curvature problem for general p, with the p=1 case recovering Gursky and Viaclovsky's known result.

Abstract: Let $(N,g)$ be a compact Riemannian manifold with negative curvature of
dimension $n\geq 3$. This paper investigates the prescribed curvature problem
defined by the $p$-fold sum of the modified Shouten tensor for $1\leq p\leq n$.
As an application, we establish the existence of a metric $\tilde{g}$ conformal
to $g$ such that \[ \mathcal{M}_{p}(\mathrm{Ric}_{\tilde{g}})=constant. \] In
the case $p=1$, this resolves a well-known result of Gursky and Viaclovsky
[Indiana Univ. Math. J. 52 (2003), no. 2, 399--419 MR1976082].

</details>


### [36] [A Liouville theorem for CR Yamabe type equation on Sasakian manifolds](https://arxiv.org/abs/2510.06779)
*Biqiang Zhao*

Main category: math.DG

TL;DR: The paper proves a rigidity result for CR Yamabe type equations on complete noncompact Sasakian manifolds with nonnegative curvature, showing they are CR isometric to the Heisenberg group.


<details>
  <summary>Details</summary>
Motivation: To study the CR Yamabe type equation on complete noncompact Sasakian manifolds and establish rigidity results under curvature conditions.

Method: Uses Jerison-Lee's differential identity combined with integral estimates to prove the rigidity theorem.

Result: Under certain assumptions, complete noncompact Sasakian manifolds with nonnegative curvature satisfying the CR Yamabe type equation are CR isometric to the Heisenberg group.

Conclusion: The paper establishes a rigidity theorem showing that Sasakian manifolds with the specified properties must be CR equivalent to the Heisenberg group.

Abstract: In this paper, we study the CR Yamabe type equation \begin{align}
  \Delta_b u+F(u)=0 \nonumber
  \end{align} on complete noncompact $(2n+1)$-dimensional Sasakian manifolds
with nonnegative curvature. Under some assumptions, we prove a rigidity result,
that is, the manifold is CR isometric to Heisenberg group $\mathbb{H}^n$. The
proofs are based on the Jerison-Lee's differential identity combining with
integral estimates.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [37] [Asymptotic Vanishing of the Success Probability in Shor's Algorithm](https://arxiv.org/abs/2510.06271)
*Jo√£o P. da Cruz*

Main category: quant-ph

TL;DR: Shor's factoring algorithm loses its guaranteed success probability in the asymptotic regime as N ‚Üí ‚àû, with success probability decaying as 1/œÜ(N) and no global expectation existing for polynomial-time order finding.


<details>
  <summary>Details</summary>
Motivation: To understand why Shor's algorithm, which guarantees at least 50% success probability for fixed N, fails to maintain this guarantee when extended to arbitrarily large moduli N.

Method: Analyzed the asymptotic behavior of multiplicative groups Œ©_N = (Z/NZ)^√ó as probability spaces, showing they form a non-tight family. Used mathematical analysis of probability measures and Monte Carlo simulations up to N ‚â§ 10^6.

Result: Success probability decays as 1/œÜ(N) as N ‚Üí ‚àû, the ensemble of uniform measures admits no weak limit, and there is asymptotic loss of ergodicity. No stationary success probability exists in the limit.

Conclusion: The 'expected polynomial time' in order finding is only locally defined - no global expectation exists as the arithmetic domain expands. This explains the empirical absence of large-N Shor's algorithm implementations and sets a fundamental limit on quantum factoring scalability.

Abstract: Shor's factoring algorithm guarantees a success probability of at least one
half for any fixed modulus N = pq with distinct primes p and q. We show that
this guarantee does not extend to the asymptotic regime. As N -> infinity, the
multiplicative groups Omega_N = (Z/NZ)^x form a non-tight family of probability
spaces, and the probability weight associated with successful bases,
proportional to p(success | a', N) p(a' | N), decays as 1/phi(N). The ensemble
of uniform measures {mu_N} therefore admits no weak limit, implying an
asymptotic loss of ergodicity. Monte Carlo simulations up to N <= 10^6 confirm
this decay and the absence of a stationary success probability. These results
demonstrate that the "expected polynomial time" in order finding is only
locally defined: no global expectation exists once the arithmetic domain
expands. The asymptotic vanishing of success probability explains the empirical
absence of large-N implementations of Shor's algorithm and sets a fundamental
limit on the scalability of quantum factoring.

</details>


### [38] [Prediction of Molecular Single-Photon Emitters: A Materials-Modelling Approach](https://arxiv.org/abs/2510.06407)
*Erik Karlsson √ñhman,Daqing Wang,R. Matthias Geilhufe,Christian Sch√§fer*

Main category: quant-ph

TL;DR: A computational framework combining database analysis and microscopic predictions to explore molecular single-photon emitters, identifying promising candidates including chiral emitters.


<details>
  <summary>Details</summary>
Motivation: Molecular emitters offer design flexibility for quantum light-matter interfaces, but exploring the vast space of possible molecular configurations is challenging.

Method: Integration of database analysis with microscopic predictions, using dibenzoterrylene in anthracene host as a benchmark system.

Result: Identified promising new molecular emitter candidates, including a chiral molecular emitter.

Conclusion: Future extensions with machine learning could unlock the full potential of molecular quantum light-matter interfaces.

Abstract: Interfacing light with quantum systems is an integral part of quantum
technology, with the most essential building block being single-photon
emitters. Although various platforms exist, each with its individual strengths,
molecular emitters boast a unique advantage -- namely the flexibility to tailor
their design to fit the requirements of a specific task. However, the
characteristics of the vast space of possible molecular configurations are
challenging to understand and explore. Here, we present a theoretical and
computational framework to initiate exploration of this vast potential by
integrating database analysis with microscopic predictions. Using a model
system of dibenzoterrylene in an anthracene host as benchmark, our approach
identifies promising new candidates, among them a chiral molecular emitter.
Future extensions of our approach integrated with machine learning routines
hold the promise of ultimately unlocking the full potential of molecular
quantum light-matter interfaces.

</details>


### [39] [Quantum matrix arithmetics with Hamiltonian evolution](https://arxiv.org/abs/2510.06316)
*Christopher Kang,Yuan Su*

Main category: quant-ph

TL;DR: The paper develops methods for performing matrix arithmetic operations using Hamiltonian evolutions with results encoded in off-diagonal blocks, requiring only ‚â§2 ancilla qubits. It covers matrix multiplication, addition, inversion, conjugation, scaling, and singular value transformation, along with an overlap estimation algorithm.


<details>
  <summary>Details</summary>
Motivation: Efficient implementation of matrix arithmetic operations is crucial for quantum algorithm speedups. The goal is to perform matrix operations using Hamiltonian evolutions while maintaining Hamiltonian block encoding for composable operations with minimal ancilla qubits.

Method: Uses Hamiltonian evolutions with results encoded in off-diagonal blocks (Hamiltonian block encoding). Employs Lie group commutator product formula and higher-order generalizations for matrix multiplication, and dominated polynomial approximation for singular value transformation. Also presents overlap estimation algorithm without extra qubits.

Result: Achieves matrix arithmetic operations (multiplication, addition, inversion, conjugation, scaling, singular value transformation) with ‚â§2 ancilla qubits. Applies to doubly factorized tensor hypercontracted Hamiltonians in quantum chemistry, obtaining improvements for initial states with fixed particle numbers using only 1 ancilla qubit.

Conclusion: The developed Hamiltonian block encoding framework enables efficient matrix arithmetic operations with minimal ancilla qubits, providing practical improvements for quantum simulations, particularly in quantum chemistry applications.

Abstract: The efficient implementation of matrix arithmetic operations underpins the
speedups of many quantum algorithms. We develop a suite of methods to perform
matrix arithmetics -- with the result encoded in the off-diagonal blocks of a
Hamiltonian -- using Hamiltonian evolutions of input operators. We show how to
maintain this $\textit{Hamiltonian block encoding}$, so that matrix operations
can be composed one after another, and the entire quantum computation takes
$\leq 2$ ancilla qubits. We achieve this for matrix multiplication, matrix
addition, matrix inversion, Hermitian conjugation, fractional scaling, integer
scaling, complex phase scaling, as well as singular value transformation for
both odd and even polynomials. We also present an overlap estimation algorithm
to extract classical properties of Hamiltonian block encoded operators,
analogous to the well known Hadmard test, at no extra cost of qubit. Our
Hamiltonian matrix multiplication uses the Lie group commutator product formula
and its higher-order generalizations due to Childs and Wiebe. Our Hamiltonian
singular value transformation employs a dominated polynomial approximation,
where the approximation holds within the domain of interest, while the
constructed polynomial is upper bounded by the target function over the entire
unit interval. We describe a circuit for simulating a class of sum-of-squares
Hamiltonians, attaining a commutator scaling in step count, while leveraging
the power of matrix arithmetics to reduce the cost of each simulation step. In
particular, we apply this to the doubly factorized tensor hypercontracted
Hamiltonians from recent studies of quantum chemistry, obtaining further
improvements for initial states with a fixed number of particles. We achieve
this with $1$ ancilla qubit.

</details>


### [40] [A Quantum Linear Systems Pathway for Solving Differential Equations](https://arxiv.org/abs/2510.06837)
*Abhishek Setty*

Main category: quant-ph

TL;DR: A systematic quantum pathway for solving differential equations using block encoding and Quantum Singular Value Transformation (QSVT), demonstrated on tridiagonal systems, heat equation, and nonlinear Burgers' equation.


<details>
  <summary>Details</summary>
Motivation: To advance quantum linear system methods toward large-scale applications by developing a systematic approach for solving differential equations within the quantum computing framework.

Method: Combines block encoding with Quantum Singular Value Transformation (QSVT), applied to tridiagonal linear systems, heat equation with mixed boundary conditions, and Carleman-linearized nonlinear Burgers' equation.

Result: Successfully demonstrated the approach on complex systems, identified circuit-depth overhead as a key bottleneck for heat equation, and showed efficient block encoding for nonlinear dynamics.

Conclusion: The pathway lays foundation for advancing quantum linear system methods but highlights limitations including need for efficient minimum singular value estimation, depth-reduction techniques, and classical benchmarks.

Abstract: We present a systematic pathway for solving differential equations within the
quantum linear systems framework by combining block encoding with Quantum
Singular Value Transformation (QSVT). The approach is demonstrated on a complex
tridiagonal linear system and extended to problems in computational fluid
dynamics: the heat equation with mixed boundary conditions and the nonlinear
Burgers' equation. Our scaling analysis of the heat equation shows how
discretization influences the minimum singular value and the polynomial degree
required for QSVT, identifying circuit-depth overhead as a key bottleneck. For
Burgers' equation, we illustrate how Carleman-linearized nonlinear dynamics can
be efficiently block encoded and solved within the QSVT framework. These
results highlight both the potential and limitations of current methods,
underscoring the need for efficient estimation of minimum singular value,
depth-reduction techniques, and benchmarks against classical reachability. This
pathway lays a foundation for advancing quantum linear system methods toward
large-scale applications.

</details>


### [41] [Universal initial state preparation for first quantized quantum simulations](https://arxiv.org/abs/2510.07278)
*Jack S. Baker,Gaurav Saxena,Thi Ha Kyaw*

Main category: quant-ph

TL;DR: A universal method for efficiently mapping symmetry-adapted initial states from second-quantized to first-quantized representation on quantum computers using Jordan-Schwinger mapping and quantum Schur transform.


<details>
  <summary>Details</summary>
Motivation: Preparing symmetry-adapted initial states is a major bottleneck in first-quantized quantum simulation, requiring efficient conversion from second-quantized representations.

Method: Uses Jordan-Schwinger Lie algebra homomorphism to map number-conserving operators between representations, then applies inverse quantum Schur transform after preparing encoded superposition of Schur labels via block-encoded linear combination of unitaries.

Result: Algorithm runs in poly(L, N, d, log Œµ‚Åª¬π) time for L configurations of N particles over d modes to accuracy Œµ, and works universally for fermions, bosons, and Green's paraparticles in arbitrary single-particle bases.

Conclusion: The method enables practical symmetry-adapted state preparation within leading first-quantized pipelines, with potential for further improvements through statistics-aware or faster quantum Schur transforms.

Abstract: Preparing symmetry-adapted initial states is a principal bottleneck in
first-quantized quantum simulation. We present a universal approach that
efficiently maps any polynomial-size superposition of occupation-number
configurations to the first-quantized representation on a digital quantum
computer. The method exploits the Jordan--Schwinger Lie algebra homomorphism,
which identifies number-conserving second-quantized operators with their
first-quantized action and induces an equivariant bijection between Fock
occupations and $\mathfrak{su}(d)$ weight states within the Schur--Weyl
decomposition. Operationally, we prepare an encoded superposition of Schur
labels via a block-encoded linear combination of unitaries and then apply the
inverse quantum Schur transform. The algorithm runs in time $\text{poly}(L, N,
d, \log \epsilon^{-1})$ for $L$ configurations of $N$ particles over $d$ modes
to accuracy $\epsilon$, and applies universally to fermions, bosons, and
Green's paraparticles in arbitrary single-particle bases. Resource estimates
indicate practicality within leading first-quantized pipelines;
statistics-aware or faster quantum Schur transforms promise further reductions.

</details>


### [42] [End-to-End Quantum Algorithm for Topology Optimization in Structural Mechanics](https://arxiv.org/abs/2510.07280)
*Leonhard H√∂lscher,Oliver Ahrend,Lukas Karch,Carlotta L'Estocq,Marc Marfany Andreu,Tobias Stollenwerk,Frank K. Wilhelm,Julia Kowalski*

Main category: quant-ph

TL;DR: Quantum algorithm for topology optimization using Grover's search on binary design variables, achieving quadratic speedup over classical unstructured search.


<details>
  <summary>Details</summary>
Motivation: Topology optimization requires evaluating enormous design spaces which is infeasible classically, so quantum algorithms can provide exponential speedup.

Method: Reformulate compliance minimization as combinatorial satisfiability problem solved with Grover's algorithm, using quantum FEM with block-encoding, QSVT for matrix inversion, Hadamard test, and QAE.

Result: Implemented and validated on MBB beam problem, showing exponential evaluation of structures in superposition with polynomial time complexity.

Conclusion: Quantum workflow demonstrates practical advancement in computational engineering with Grover's quadratic speedup maintained in global search.

Abstract: Topology optimization is a key methodology in engineering design for finding
efficient and robust structures. Due to the enormous size of the design space,
evaluating all possible configurations is typically infeasible. In this work,
we present an end-to-end, fault-tolerant quantum algorithm for topology
optimization that operates on the exponential Hilbert space representing the
design space. We demonstrate the algorithm on the two-dimensional
Messerschmitt-B\"olkow-Blohm (MBB) beam problem. By restricting design
variables to binary values, we reformulate the compliance minimization task as
a combinatorial satisfiability problem solved using Grover's algorithm. Within
Grover's oracle, the compliance is computed through the finite-element method
(FEM) using established quantum algorithms, including block-encoding of the
stiffness matrix, Quantum Singular Value Transformation (QSVT) for matrix
inversion, Hadamard test, and Quantum Amplitude Estimation (QAE). The complete
algorithm is implemented and validated using classical quantum-circuit
simulations. A detailed complexity analysis shows that the method evaluates the
compliance of exponentially many structures in quantum superposition in
polynomial time. In the global search, our approach maintains Grover's
quadratic speedup compared to classical unstructured search. Overall, the
proposed quantum workflow demonstrates how quantum algorithms can advance the
field of computational science and engineering.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [43] [TOMATOES: Topology and Material Optimization for Latent Heat Thermal Energy Storage Devices](https://arxiv.org/abs/2510.07057)
*Rahul Kumar Padhy,Krishnan Suresh,Aaditya Chandrasekhar*

Main category: cs.CE

TL;DR: This paper presents an automated design framework for concurrent optimization of material choice and topology in latent heat thermal energy storage systems, overcoming the challenge of discrete material selection using variational autoencoders.


<details>
  <summary>Details</summary>
Motivation: Conventional topology optimization limits to fixed materials and doesn't leverage expanding material databases, leaving the co-design of material and geometry for LHTES systems unexplored.

Method: Uses data-driven variational autoencoder to project discrete material databases onto continuous latent spaces, integrated into an end-to-end differentiable transient nonlinear finite-element solver that accounts for phase change.

Result: Demonstrated effectiveness through illustrative examples showing maximization of discharged energy within specified time under cost constraints.

Conclusion: The framework successfully enables concurrent optimization of material choice and topology for LHTES systems, overcoming the discrete material selection challenge.

Abstract: Latent heat thermal energy storage (LHTES) systems are compelling candidates
for energy storage, primarily owing to their high storage density. Improving
their performance is crucial for developing the next-generation efficient and
cost effective devices. Topology optimization (TO) has emerged as a powerful
computational tool to design LHTES systems by optimally distributing a
high-conductivity material (HCM) and a phase change material (PCM). However,
conventional TO typically limits to optimizing the geometry for a fixed,
pre-selected materials. This approach does not leverage the large and expanding
databases of novel materials. Consequently, the co-design of material and
geometry for LHTES remains a challenge and unexplored.
  To address this limitation, we present an automated design framework for the
concurrent optimization of material choice and topology. A key challenge is the
discrete nature of material selection, which is incompatible with the
gradient-based methods used for TO. We overcome this by using a data-driven
variational autoencoder (VAE) to project discrete material databases for both
the HCM and PCM onto continuous and differentiable latent spaces. These
continuous material representations are integrated into an end-to-end
differentiable, transient nonlinear finite-element solver that accounts for
phase change. We demonstrate this framework on a problem aimed at maximizing
the discharged energy within a specified time, subject to cost constraints. The
effectiveness of the proposed method is validated through several illustrative
examples.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [44] [Quantitative boundary H√∂lder estimates for the inhomogeneous Poisson problem through a probabilistic approach](https://arxiv.org/abs/2510.06906)
*Iulian C√Æmpean,Ionel Popescu,Arghir Zarnescu*

Main category: math.PR

TL;DR: This paper derives explicit quantitative boundary H√∂lder estimates for the inhomogeneous Poisson problem in bounded domains, with applications to neural network approximations that avoid the curse of dimensionality.


<details>
  <summary>Details</summary>
Motivation: To provide explicit constants for boundary regularity estimates in the Poisson problem, which can help overcome the curse of dimensionality when using neural networks to approximate solutions.

Method: Two-step approach: (1) prove boundary H√∂lder regularity controlled by boundary data seminorm, forcing term norm, and Brownian motion exit time moments; (2) derive explicit estimates for exit time moments using geometric domain properties like exterior ball/cone conditions.

Result: Obtained explicit boundary H√∂lder estimates for Poisson problem in various domain types, along with explicit constants for Green function and solution gradient pointwise estimates. These enable neural network approximations with polynomial dimension scaling.

Conclusion: The explicit boundary regularity estimates provide tools to bypass the curse of dimensionality in neural network approximations of Poisson problems, achieving polynomial scaling that can be optimal in some cases.

Abstract: In this paper we derive quantitative boundary H\"older estimates, with
explicit constants, for the inhomogeneous Poisson problem in a bounded open set
$D\subset \mathbb{R}^d$.
  Our approach has two main steps: firstly, we consider an arbitrary $D$ as
above and prove that the boundary $\alpha$-H\"older regularity of the solution
the Poisson equation is controlled, with explicit constants, by the H\"older
seminorm of the boundary data, the $L^ \gamma$-norm of the forcing term with
$\gamma>d/2$, and the $\alpha/2$-moment of the exit time from $D$ of the
Brownian motion.
  Secondly, we derive explicit estimates for the $\alpha/2$-moment of the exit
time in terms of the distance to the boundary, the regularity of the domain
$D$, and $\alpha$. Using this approach, we derive explicit estimates for the
same problem in domains satisfying exterior ball conditions, respectively
exterior cone/wedge conditions, in terms of simple geometric features.
  As a consequence we also obtain explicit constants for pointwise estimates
for the Green function and for the gradient of the solution.
  The obtained estimates can be employed to bypass the curse of high dimensions
when aiming to approximate the solution of the Poisson problem using neural
networks, obtaining polynomial scaling with dimension, which in some cases can
be shown to be optimal.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [45] [Benchmarking AI-evolved cosmological structure formation](https://arxiv.org/abs/2510.06731)
*Xiaofeng Dong,Nesar Ramachandra,Salman Habib,Katrin Heitmann*

Main category: astro-ph.CO

TL;DR: Deep learning image-to-image translation can approximate cosmological simulations faster. U-Net models structure formation from analytical and numerical datasets. Performance is evaluated using physical metrics, showing limitations at small scales. A custom density-weighted loss function improves small-scale accuracy.


<details>
  <summary>Details</summary>
Motivation: To provide a fast, approximate alternative to computationally expensive cosmological simulations for covariance studies, systematics investigations, and cosmological parameter inference.

Method: Use U-Net convolutional neural network for cosmological matter field evolution. Generate datasets using Zel'dovich approximation (analytical) and Particle-Mesh N-body method (numerical). Validate with comprehensive physical metrics and introduce custom density-weighted loss function.

Result: U-Net performs well on some physical metrics but accuracy decreases at smaller scales with large density dynamic range. The density-weighted loss function significantly improves small-scale accuracy.

Conclusion: Physically motivated benchmarks can guide optimization schemes to enhance scientific machine learning accuracy by focusing on relevant features, as demonstrated with the density-weighted loss improvement.

Abstract: The potential of deep learning-based image-to-image translations has recently
attracted significant attention. One possible application of such a framework
is as a fast, approximate alternative to cosmological simulations, which would
be particularly useful in various contexts, including covariance studies,
investigations of systematics, and cosmological parameter inference. To
investigate different aspects of learning-based cosmological mappings, we
choose two approaches for generating suitable cosmological matter fields as
datasets: a simple analytical prescription provided by the Zel'dovich
approximation, and a numerical N-body method using the Particle-Mesh approach.
The evolution of structure formation is modeled using U-Net, a widely employed
convolutional image translation framework. Because of the lack of a controlled
methodology, validation of these learned mappings requires multiple benchmarks
beyond simple visual comparisons and summary statistics. A comprehensive list
of metrics is considered, including higher-order correlation functions,
conservation laws, topological indicators, and statistical independence of
density fields. We find that the U-Net approach performs well only for some of
these physical metrics, and accuracy is worse at increasingly smaller scales,
where the dynamic range in density is large. By introducing a custom
density-weighted loss function during training, we demonstrate a significant
improvement in the U-Net results at smaller scales. This study provides an
example of how a family of physically motivated benchmarks can, in turn, be
used to fine-tune optimization schemes -- such as the density-weighted loss
used here -- to significantly enhance the accuracy of scientific machine
learning approaches by focusing attention on relevant features.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [46] [Experimental Results from Early Non-Planar NI-HTS Magnet Prototypes for the Columbia Stellarator eXperiment (CSX)](https://arxiv.org/abs/2510.07042)
*D. Schmeling,M. Russo,B. T. Gebreamlak,T. J. Kiker,A. R. Skrypek,A. R. Hightower,J. Xue,S. Chen,S. Sohaib,C. Martinez,K. F. Richardson,L. Filor,S. Komatsu,L. Liu,C. Paz-Soldan*

Main category: physics.ins-det

TL;DR: The paper presents a staged prototype program (P1-P3) for developing high-temperature superconducting (HTS) coils for the Columbia Stellarator eXperiment (CSX), addressing challenges in adapting ReBCO superconductors to non-planar stellarator geometries through 3D-printed aluminum frames, gimballed winding mechanics, and solder potting.


<details>
  <summary>Details</summary>
Motivation: To demonstrate a university-scale quasi-axisymmetric stellarator using HTS technology at 0.5 T magnetic field, requiring new strategies to handle the strain sensitivity of ReBCO superconductors in non-planar geometries.

Method: A three-stage prototype program using 3D-printed sectional aluminum coil frames with winding channels, gimballed constant-tension winding mechanics, and solder potting for radial current redistribution and passive quench mitigation.

Result: P1 successfully tested manufacturing and achieved predicted fields at 77 K; P2 was wound to 42 turns and energized at 30-40 K producing expected magnetic fields; P3 with dual double-pancakes and 200 turns approaches the 70 kAt target at 20 K, with sub-microhm lap joints developed.

Conclusion: The prototype program successfully de-risks manufacturing, cooling interfaces, quench management, and diagnostics, paving the way for full-size non-planar HTS stellarator coils for CSX.

Abstract: The Columbia Stellarator eXperiment (CSX) is an upgrade of the Columbia
Non-neutral Torus (CNT) that aims to demonstrate a university-scale,
quasi-axisymmetric stellarator using high-temperature superconducting (HTS)
technology at an on-axis magnetic field target of 0.5 T. Due to the strain
sensitivity of ReBCO (Rare-earth Barium Copper Oxides), adapting it to
non-planar stellarator geometries requires new winding, structural, and cooling
strategies. We report on the results of a staged prototype program (P1, P2, P3)
employing 3D-printed, sectional aluminum coil frames with winding channels,
gimballed constant-tension winding mechanics, and solder potting for radial
current redistribution and passive quench mitigation. The first prototype, P1
(planar elliptical, double-pancake) tested additive manufacture, sectional
joining and baseline winding, achieving predicted fields at 77 K. P2
(non-planar, higher strain) was wound to 42 turns, energized at 30-40 K to
produce expected magnetic fields, and studied thermal gradients and resistance
at up to 4.5 kAt. Design evolution in P3 introduces concave geometry with dual
double-pancakes, 200 turns, and approaches the 70 kAt target at 20 K. In
parallel, sub-microhm lap joints have been developed. Together, these results
de-risk manufacturing, cooling interfaces, quench management, and diagnostics,
paving the way for full-size non-planar HTS stellarator coils for CSX.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [47] [Influence of Solar Sails on Magnetic Field Measurements in Space Plasmas](https://arxiv.org/abs/2510.06510)
*Konstantinos Horaites,Juan V. Rodriguez,Ying Liu*

Main category: physics.space-ph

TL;DR: Solar sail technology may interfere with magnetometer readings due to plasma interactions, particularly through eddy currents and magnetic pileup effects.


<details>
  <summary>Details</summary>
Motivation: To prepare for solar sail missions with magnetometers by characterizing sail-plasma interactions that could affect magnetic field measurements, which are crucial for space weather prediction.

Method: Investigation of two electromagnetic effects: eddy currents induced in the metallic sail and magnetic pileup when sail size matches electron kinetic scales of surrounding plasma.

Result: Eddy currents significantly perturb local magnetic field at high frequencies, and magnetic pileup can influence spacecraft environment under specific size conditions.

Conclusion: This research provides initial guidance for determining when sail-plasma interactions could impact magnetometer performance in future solar sail missions.

Abstract: Solar sail technology is ready to be deployed in a satellite mission carrying
a science-grade magnetometer. In preparation for such a mission, it is
essential to characterize the interactions between the sail and the ambient
plasma that could affect the magnetometer readings. The solar wind magnetic
field is a key parameter in space weather prediction, because it governs the
energy-releasing magnetic reconnection process at Earth's magnetopause. This
paper investigates the influence of solar sails on the ambient magnetic field,
particularly focusing on two critical electromagnetic effects: eddy currents
and magnetic pileup. We find the induced eddy currents in the metallic sail can
significantly perturb the local magnetic field at high frequencies. We also
suggest that magnetic pileup can influence the spacecraft's environment when
the sail size is comparable to the electron kinetic scales of the surrounding
plasma. This research provides an initial guide for determining when
sail-plasma interactions could impact magnetometer performance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Lagrangian neural ODEs: Measuring the existence of a Lagrangian with Helmholtz metrics](https://arxiv.org/abs/2510.06367)
*Luca Wolf,Tobias Buck,Bjoern Malte Schaefer*

Main category: cs.LG

TL;DR: The paper introduces Helmholtz metrics to quantify how closely an ODE resembles Euler-Lagrange equations, and develops Lagrangian neural ODEs that can learn these equations directly from positional data with no extra inference cost.


<details>
  <summary>Details</summary>
Motivation: Neural ODEs are powerful but not all solutions are physical Euler-Lagrange equations. There's a need to quantify this resemblance and learn physically meaningful equations directly.

Method: Developed Helmholtz metrics to measure ODE resemblance to Euler-Lagrange equations, combined with second order neural ODEs to create Lagrangian neural ODEs that learn from positional data.

Result: The method can distinguish Lagrangian from non-Lagrangian systems, improves neural ODE solutions, and works with noisy fundamental systems.

Conclusion: Lagrangian neural ODEs provide a direct way to learn Euler-Lagrange equations with zero additional inference cost, enabling more physically meaningful machine learning for physics applications.

Abstract: Neural ODEs are a widely used, powerful machine learning technique in
particular for physics. However, not every solution is physical in that it is
an Euler-Lagrange equation. We present Helmholtz metrics to quantify this
resemblance for a given ODE and demonstrate their capabilities on several
fundamental systems with noise. We combine them with a second order neural ODE
to form a Lagrangian neural ODE, which allows to learn Euler-Lagrange equations
in a direct fashion and with zero additional inference cost. We demonstrate
that, using only positional data, they can distinguish Lagrangian and
non-Lagrangian systems and improve the neural ODE solutions.

</details>


### [49] [AutoBalance: An Automatic Balancing Framework for Training Physics-Informed Neural Networks](https://arxiv.org/abs/2510.06684)
*Kang An,Chenhao Si,Ming Yan,Shiqian Ma*

Main category: cs.LG

TL;DR: AutoBalance introduces a novel 'post-combine' training paradigm for PINNs that assigns independent adaptive optimizers to each loss component and aggregates preconditioned updates, overcoming limitations of existing gradient manipulation methods.


<details>
  <summary>Details</summary>
Motivation: Training PINNs is difficult due to conflicting objectives and different curvatures in multiple loss terms (PDE residuals and boundary conditions). Existing gradient manipulation methods before optimization are fundamentally limited as they disrupt the optimizer's internal preconditioning.

Method: AutoBalance uses a 'post-combine' strategy where each loss component gets its own independent adaptive optimizer. The preconditioned updates from these individual optimizers are aggregated afterwards rather than combining gradients before optimization.

Result: Extensive experiments on challenging PDE benchmarks show AutoBalance consistently outperforms existing frameworks, achieving significant reductions in solution error measured by both MSE and L‚àû norms.

Conclusion: AutoBalance provides an effective training paradigm for PINNs that is orthogonal and complementary to other popular methodologies, amplifying their effectiveness on demanding benchmarks.

Abstract: Physics-Informed Neural Networks (PINNs) provide a powerful and general
framework for solving Partial Differential Equations (PDEs) by embedding
physical laws into loss functions. However, training PINNs is notoriously
difficult due to the need to balance multiple loss terms, such as PDE residuals
and boundary conditions, which often have conflicting objectives and vastly
different curvatures. Existing methods address this issue by manipulating
gradients before optimization (a "pre-combine" strategy). We argue that this
approach is fundamentally limited, as forcing a single optimizer to process
gradients from spectrally heterogeneous loss landscapes disrupts its internal
preconditioning. In this work, we introduce AutoBalance, a novel "post-combine"
training paradigm. AutoBalance assigns an independent adaptive optimizer to
each loss component and aggregates the resulting preconditioned updates
afterwards. Extensive experiments on challenging PDE benchmarks show that
AutoBalance consistently outperforms existing frameworks, achieving significant
reductions in solution error, as measured by both the MSE and $L^{\infty}$
norms. Moreover, AutoBalance is orthogonal to and complementary with other
popular PINN methodologies, amplifying their effectiveness on demanding
benchmarks.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [50] [Phase segregation of liquid-vapor systems with a gravitational field](https://arxiv.org/abs/2510.06399)
*A. Lamura*

Main category: cond-mat.soft

TL;DR: Study of liquid-vapor phase separation under gravity using lattice Boltzmann method, showing gravity accelerates coarsening and leads to layered structure formation.


<details>
  <summary>Details</summary>
Motivation: Few studies exist on liquid-vapor segregation under gravity despite extensive work on solid mixtures under external forces.

Method: Isothermal lattice Boltzmann model with van der Waals equation of state for liquid-vapor systems under gravitational force.

Result: Without gravity: growth exponent 2/3 (inertial regime). With gravity: increased growth exponent, accelerated coarsening, liquid accumulation at bottom. Layer thickness transitions from L ‚àù t^{2/3} to L ‚àù g t^{5/3}. Final steady state shows two overlapped liquid-vapor layers matching theoretical predictions.

Conclusion: Gravity significantly affects liquid-vapor phase separation dynamics, accelerating coarsening and leading to stratified steady states with predictable density profiles.

Abstract: Phase separation in the presence of external forces has attracted
  considerable attention since the initial works for solid mixtures.
  Despite this, only very few studies are available which address
  the segregation process of liquid-vapor systems under gravity.
  We present here an extensive study which takes into account both
  hydrodynamic and gravitational effects on the coarsening dynamics.
  An isothermal formulation of a lattice Boltzmann model for a liquid-vapor
  system with the van der Waals equation of state is adopted.
  In the absence of gravity, the growth of domains follows a power law
  with the exponent $2/3$ of the inertial regime.
  The external force deeply affects the observed morphology
  accelerating the coarsening of domains and favoring the liquid accumulation
  at the bottom of the system. Along the force direction,
  the growth exponent is found to increase with the gravity strength
  still preserving sharp interfaces since the Porod's law is found to be
  verified.
  The time evolution of the average thickness $L$
  of the layers of accumulated material at confining walls
  shows a transition from an initial regime where $L \simeq t^{2/3}$
  ($t$: time) to a late-time regime $L \simeq g t^{5/3}$ with $g$ the
  gravitational acceleration.
  The final steady state, made of two overlapped layers of liquid and
  vapor, shows a density profile in agreement with theoretical predictions.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [51] [Mass Conservation on Rails -- Rethinking Physics-Informed Learning of Ice Flow Vector Fields](https://arxiv.org/abs/2510.06286)
*Kim Bente,Roman Marchant,Fabio Ramos*

Main category: physics.ao-ph

TL;DR: Divergence-free neural networks (dfNNs) enforce exact mass conservation for Antarctic ice flow interpolation, outperforming physics-informed and unconstrained neural networks.


<details>
  <summary>Details</summary>
Motivation: To improve reliability of sea level rise projections by ensuring ice sheet models respect physical principles like mass conservation when interpolating sparse ice flow measurements.

Method: Proposed divergence-free neural networks (dfNNs) that enforce local mass conservation exactly using vector calculus, compared with physics-informed neural networks (PINNs) and unconstrained neural networks on ice flux interpolation over Byrd Glacier.

Result: dfNNs with 'mass conservation on rails' yielded more reliable estimates than PINNs and unconstrained NNs. Directional guidance from continent-wide satellite velocity data boosted performance across all models.

Conclusion: Exact enforcement of mass conservation through dfNNs provides more reliable ice flow interpolation than soft physics constraints, and leveraging broader satellite data improves model performance.

Abstract: To reliably project future sea level rise, ice sheet models require inputs
that respect physics. Embedding physical principles like mass conservation into
models that interpolate Antarctic ice flow vector fields from sparse & noisy
measurements not only promotes physical adherence but can also improve accuracy
and robustness. While physics-informed neural networks (PINNs) impose physics
as soft penalties, offering flexibility but no physical guarantees, we instead
propose divergence-free neural networks (dfNNs), which enforce local mass
conservation exactly via a vector calculus trick. Our comparison of dfNNs,
PINNs, and unconstrained NNs on ice flux interpolation over Byrd Glacier
suggests that "mass conservation on rails" yields more reliable estimates, and
that directional guidance, a learning strategy leveraging continent-wide
satellite velocity data, boosts performance across models.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [52] [Local well-posedness for a class of semilinear Moore-Gibson-Thompson equations with subcritical nonlinearities](https://arxiv.org/abs/2510.06944)
*Flank D. M. Bezerra,Lu√≠s M. Salge*

Main category: math.DS

TL;DR: This paper studies higher-order semilinear evolution equations based on the Moore-Gibson-Thompson model, focusing on local well-posedness under subcritical nonlinearities.


<details>
  <summary>Details</summary>
Motivation: To extend the analysis of evolution equations inspired by the Moore-Gibson-Thompson model to higher-order operators and clarify the role of extrapolation spaces and fractional domains.

Method: The authors use sectorial operators with zero in the resolvent set, construct fractional powers spaces, and analyze spectral properties to prove local well-posedness.

Result: The paper establishes local well-posedness of the semilinear Cauchy problem for higher-order elliptic operators under subcritical nonlinearities.

Conclusion: The framework successfully handles the lack of accretivity and bounded imaginary powers through extrapolation spaces and fractional domains.

Abstract: In this paper, we study a class of higher-order semilinear evolution
equations inspired by the Moore-Gibson-Thompson model introduced by Dell'Oro,
Liverani and Pata (2023), involving strongly elliptic operators of order ($2m$)
with homogeneous boundary conditions. The associated unbounded linear operator
is a sectorial operator with zero belonging to the resolvent set, allowing the
construction of fractional powers spaces, and the analysis of their spectral
properties. We prove the local well-posedness of the corresponding semilinear
Cauchy problem under subcritical nonlinearities. Our framework clarifies the
role of extrapolation spaces and fractional domains in handling the lack of
accretivity and bounded imaginary powers of the operator.

</details>
