<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 17]
- [math.AP](#math.AP) [Total: 16]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [math.PR](#math.PR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [math.NT](#math.NT) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Determination of Range Conditions for General Projection Pair Operators](https://arxiv.org/abs/2510.07480)
*Richard Huber,Rolf Clackdoyle,Laurent Desbat*

Main category: math.NA

TL;DR: This paper analyzes the range of projection pair operators (N=2) in medical tomography, finding that regular range conditions can be determined through kernel conditions, with applications to exponential fanbeam and mixed parallel-fanbeam transforms.


<details>
  <summary>Details</summary>
Motivation: Characterizing the range of N-projections operators is mathematically interesting and practically useful for geometric calibration, motion detection, and model parameter identification in medical tomography.

Method: The authors investigate projection pair operators in the plane (N=2) applied to functions in R², analyzing the set of annihilators to the range and developing kernel conditions to determine range conditions explicitly.

Result: The set of annihilators to the range of projection pair operators that are regular distributions contains at most one dimension. For the exponential fanbeam transform, no regular range condition exists, meaning arbitrary data can be approximated in an L² sense by projections of smooth functions.

Conclusion: The developed theory provides a framework for analyzing range conditions of projection pair operators, with demonstrated applications to specific transforms like exponential fanbeam and mixed parallel-fanbeam projection pairs.

Abstract: Tomographic techniques are vital in modern medicine, allowing doctors to
observe patients' interior features. Individual steps in the measurement
process are modeled by `single projection operators' $p$. These are line
integral operators over a collection of curves that covers the regions of
interest. Then, the entire measurement process can be understood as a finite
collection of such single projections, and thus be modeled by an
$N$-projections operator $P=(p_1,\dots,p_N)$. The most well-known example of an
$N$-projections operator is the restriction of the Radon transform to finitely
many projection angles. Characterizations of the range of $N$-projections
operators are of intrinsic mathematical interest and can also help in practical
applications such as geometric calibration, motion detection, or model
parameter identification. In this work, we investigate the range of projection
pair operators $\mathcal{P}$ in the plane, i.e., operators formed by two
projections ($N=2$) applied to functions in $\mathbb{R}^2$. We find that the
set of annihilators to $\mathrm{rg}(\mathcal{P})$ that are regular
distributions contains at most one dimension and a range condition can be
explicitly determined by what we refer to as `kernel conditions'. With this
tool, we examine the exponential fanbeam transform for which no range
conditions were known, finding that no (regular) range condition exists, and
therefore, arbitrary data can be approximated in an $L^2$ sense by projections
of smooth functions. We also illustrate the use of this theory on a mixed
parallel-fanbeam projection pair operator.

</details>


### [2] [A control-based spatial source reconstruction in fractional heat equations](https://arxiv.org/abs/2510.07528)
*Galina García,Joaquín Vidal,Sebastián Zamorano*

Main category: math.NA

TL;DR: This paper develops a method to reconstruct the spatial component of source terms in fractional heat equations using partial observations of the system state and its time derivative.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse source problem for nonlocal heat equations with fractional Laplacian, enabling source reconstruction from limited measurement data.

Method: Derives reconstruction formula for Fourier coefficients using spectral analysis and Volterra integral equations, leveraging null controllability of fractional heat equation for s∈(1/2,1).

Result: Numerical experiments demonstrate the accuracy and stability of the proposed reconstruction approach.

Conclusion: The method provides a robust framework for recovering spatial sources in fractional heat equations under limited observation data.

Abstract: This article addresses the inverse source problem for a nonlocal heat
equation involving the fractional Laplacian. The primary goal is to reconstruct
the spatial component of the source term from partial observations of the
system's state and its time derivative over a subset of the domain. A
reconstruction formula for the Fourier coefficients of the unknown source is
derived, leveraging the null controllability property of the fractional heat
equation when the fractional order lies in the interval $s\in(1/2,1)$. The
methodology builds on spectral analysis and Volterra integral equations,
providing a robust framework for recovering spatial sources under limited
measurement data. Numerical experiments confirm the accuracy and stability of
the proposed approach.

</details>


### [3] [Semi-implicit strategies for the Serre-Green-Naghdi equations in hyperbolic form. Is hyperbolic relaxation really a good idea?](https://arxiv.org/abs/2510.07539)
*Emanuele Macca,Walter Boscheri,Mario Ricchiuto*

Main category: math.NA

TL;DR: A semi-implicit integration strategy for hyperbolic Serre-Green-Naghdi (hSGN) equations that treats stiff acoustic terms implicitly while keeping advective components explicit, overcoming stability constraints of explicit schemes.


<details>
  <summary>Details</summary>
Motivation: The elliptic formulation of SGN equations increases computational cost compared to Saint-Venant equations. While hyperbolic models (hSGN) help, explicit schemes face restrictive stability constraints as relaxation parameter increases.

Method: Semi-implicit (SI) integration using IMEX Runge-Kutta framework: stiff acoustic terms treated implicitly, advective components remain explicit.

Result: Mitigates CFL stability restriction, maintains dispersive accuracy at moderate computational cost. Numerical results confirm efficiency and accuracy.

Conclusion: Combination of hyperbolization and semi-implicit time integration provides efficient and accurate alternative to both classical SGN and fully explicit hSGN solvers.

Abstract: The Serre-Green-Naghdi (SGN) equations provide a valuable framework for
modelling fully nonlinear and weakly dispersive shallow-water flows. However,
their elliptic formulation can considerably increase the computational cost
compared to the Saint-Venant equations. To overcome this difficulty, hyperbolic
models (hSGN) have been proposed that replace the elliptic operators with
first-order hyperbolic formulations augmented by relaxation terms, which
recover the original elliptic formulation in the stiff limit. Yet, as the
relaxation parameter \lambda increases, explicit schemes face restrictive
stability constraints that may offset these advantages. To mitigate this
limitation, we introduce a semi-implicit (SI) integration strategy for the hSGN
system, where the stiff acoustic terms are treated implicitly within an IMEX
Runge-Kutta framework, while the advective components remain explicit. The
proposed approach mitigates the CFL stability restriction and maintains
dispersive accuracy at a moderate computational cost. Numerical results confirm
that the combination of hyperbolization and semi-implicit time integration
provides an efficient and accurate alternative to both classical SGN and fully
explicit hSGN solvers.

</details>


### [4] [Stochastic Gradient Descent for Incomplete Tensor Linear Systems](https://arxiv.org/abs/2510.07630)
*Anna Ma,Deanna Needell,Alexander Xue*

Main category: math.NA

TL;DR: The paper adapts a stochastic gradient descent method for solving large tensor linear systems with missing data, extending it beyond uniform missing patterns to other missing data models.


<details>
  <summary>Details</summary>
Motivation: Solving large tensor linear systems is challenging due to high data volume, and becomes even more difficult when data is missing. Existing methods assume uniform missing patterns, which limits their applicability.

Method: Modified the update direction of a stochastic gradient descent-based method to make it applicable under various missing data models, not just uniform patterns.

Result: Proved convergence results for the modified method and experimentally verified these results on synthetic data.

Conclusion: The adapted stochastic gradient descent method successfully handles tensor linear systems with missing data under various missing patterns, with proven convergence guarantees.

Abstract: Solving large tensor linear systems poses significant challenges due to the
high volume of data stored, and it only becomes more challenging when some of
the data is missing. Recently, Ma et al. showed that this problem can be
tackled using a stochastic gradient descent-based method, assuming that the
missing data follows a uniform missing pattern. We adapt the technique by
modifying the update direction, showing that the method is applicable under
other missing data models. We prove convergence results and experimentally
verify these results on synthetic data.

</details>


### [5] [Parallel-in-Time Solution of Allen-Cahn Equations by Integrating Operator Learning into the Parareal Method](https://arxiv.org/abs/2510.07672)
*Yuwei Geng,Junqi Yin,Eric C. Cyr,Guannan Zhang,Lili Ju*

Main category: math.NA

TL;DR: Deep learning-based coarse propagator integrated with Parareal method accelerates time-dependent PDE simulations while maintaining accuracy comparable to traditional numerical solvers.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy of deep learning solutions for time-dependent PDEs while achieving computational speedup, particularly for equations with rapid changes like sharp transitions.

Method: Use convolutional neural networks (CNNs) to learn the fully discrete time-stepping operator from traditional numerical schemes, then integrate as coarse propagator in Parareal algorithm with traditional method as fine solver.

Result: Achieves significant computational speedup compared to traditional fine solvers while converging to high-accuracy solutions in only a few Parareal iterations, especially effective on multiple GPUs.

Conclusion: Integration of neural networks into parallel-in-time frameworks enables efficient and accurate simulations of time-dependent PDEs, with trained models applicable to various initial conditions without retraining.

Abstract: While recent advances in deep learning have shown promising efficiency gains
in solving time-dependent partial differential equations (PDEs), matching the
accuracy of conventional numerical solvers still remains a challenge. One
strategy to improve the accuracy of deep learning-based solutions for
time-dependent PDEs is to use the learned solution as the coarse propagator in
the Parareal method and a traditional numerical method as the fine solver.
However, successful integration of deep learning into the Parareal method
requires consistency between the coarse and fine solvers, particularly for PDEs
exhibiting rapid changes such as sharp transitions. To ensure such consistency,
we propose to use the convolutional neural networks (CNNs) to learn the fully
discrete time-stepping operator defined by the traditional numerical scheme
used as the fine solver. We demonstrate the effectiveness of the proposed
method in solving the classical and mass-conservative Allen-Cahn (AC)
equations. Through iterative updates in the Parareal algorithm, our approach
achieves a significant computational speedup compared to traditional fine
solvers while converging to high-accuracy solutions. Our results highlight that
the proposed Parareal algorithm effectively accelerates simulations,
particularly when implemented on multiple GPUs, and converges to the desired
accuracy in only a few iterations. Another advantage of our method is that the
CNNs model is trained on trajectories-based on random initial conditions, such
that the trained model can be used to solve the AC equations with various
initial conditions without re-training. This work demonstrates the potential of
integrating neural network methods into the parallel-in-time frameworks for
efficient and accurate simulations of time-dependent PDEs.

</details>


### [6] [Ergodicity and error estimate of laws for a random splitting Langevin Monte Carlo](https://arxiv.org/abs/2510.07676)
*Lei Li,Chen Wang,Mengchao Wang*

Main category: math.NA

TL;DR: The paper analyzes random splitting Langevin Monte Carlo (RSLMC) to mitigate first-order bias in LMC. It develops an analysis framework for sampling error under Wasserstein distance, establishes geometric ergodicity, and proves O(τ²) accuracy for approximating the Gibbs distribution.


<details>
  <summary>Details</summary>
Motivation: To address the first-order bias in standard Langevin Monte Carlo methods while avoiding the computational complexity of other high-order schemes, by developing a theoretical framework for analyzing random splitting LMC.

Method: Developed an analysis framework using relative entropy approach and explicit formulas for semi-group commutators. Established pointwise gradient/Hessian estimates via Bernstein-type PDE approach. Used reflection coupling for geometric ergodicity. Combined local error estimates with ergodicity for uniform-in-time sampling error bounds.

Result: Obtained sharp local truncation error, established geometric ergodicity, and proved that the invariant measure of RSLMC approximates the true Gibbs distribution with O(τ²) accuracy, where τ is the time step. Numerical experiments validated the theoretical results.

Conclusion: Random splitting Langevin Monte Carlo effectively mitigates first-order bias with computational efficiency comparable to standard LMC, achieving second-order accuracy in approximating the target distribution while maintaining theoretical guarantees on convergence and sampling error.

Abstract: The random splitting Langevin Monte Carlo could mitigate the first order bias
in Langevin Monte Carlo with little extra work compared other high order
schemes. We develop in this work an analysis framework for the sampling error
under Wasserstein distance regarding the random splitting Langevin Monte Carlo.
First, the sharp local truncation error is obtained by the relative entropy
approach together with the explicit formulas for the commutator of related
semi-groups. The necessary pointwise estimates of the gradient and Hessian of
the logarithmic density are established by the Bernstein type approach in PDE
theory. Second, the geometric ergodicity is established by accommodation of the
reflection coupling. Combining the ergodicity with the local error estimate, we
establish a uniform-in-time sampling error bound, showing that the invariant
measure of the method approximates the true Gibbs distribution with $O(\tau^2)$
accuracy where $\tau$ is the time step. Lastly, we perform numerical
experiments to validate the theoretical results.

</details>


### [7] [Smoother-type a posteriori error estimates for finite element methods](https://arxiv.org/abs/2510.07677)
*Yuwen Li,Han Shui*

Main category: math.NA

TL;DR: This paper develops user-friendly a posteriori error estimates for finite element methods using linear iterative solvers like Jacobi or Gauss-Seidel on an auxiliary finer mesh.


<details>
  <summary>Details</summary>
Motivation: To create simple and practical error estimation methods that are easy to implement and outperform traditional residual-type estimators.

Method: Uses simple smoothers (Jacobi or Gauss-Seidel) on an auxiliary finer mesh to process finite element residuals, requiring only coarse-to-fine prolongation operator.

Result: Numerical experiments show the proposed smoother-type error estimators are more accurate than residual-type estimators and robust with respect to parameters and polynomial degrees.

Conclusion: For symmetric problems, the method provides reliable and efficient error estimation under saturation assumption, offering a practical alternative to traditional approaches.

Abstract: This work develops user-friendly a posteriori error estimates of finite
element methods, based on smoothers of linear iterative solvers. The proposed
method employs simple smoothers, such as Jacobi or Gauss--Seidel iteration, on
an auxiliary finer mesh to process the finite element residual for a posteriori
error control. The implementation requires only a coarse-to-fine prolongation
operator. For symmetric problems, we prove the reliability and efficiency of
smoother-type error estimators under a saturation assumption. Numerical
experiments for various PDEs demonstrate that the proposed smoother-type error
estimators outperform residual-type estimators in accuracy and exhibit
robustness with respect to parameters and polynomial degrees.

</details>


### [8] [Elastic-plastic cell-based smoothed finite element method solving geotechnical problems](https://arxiv.org/abs/2510.07687)
*Yang Yang,Mingjiao Yan,Zongliang Zhang,Miao Zhang,Feidong Zheng,Dong Pana,Xiaozi Lina*

Main category: math.NA

TL;DR: An elastic-plastic cell-based smoothed finite element method (CSFEM) is developed for geotechnical analysis of soils and rocks with nonlinear, path-dependent behaviors, improving stress accuracy and reducing mesh sensitivity while maintaining polygonal element flexibility.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of analyzing soils and rocks with nonlinear and path-dependent behaviors in geotechnical engineering, where conventional methods may suffer from stress inaccuracies, volumetric locking, and mesh distortion sensitivity.

Method: The method introduces strain smoothing over subcell domains and employs a consistent stress return-mapping algorithm, implemented in ABAQUS via user-defined elements, and validated through various benchmark and practical problems.

Result: Numerical results show excellent agreement with analytical solutions and conventional FEM, with smoother stress fields, improved convergence, and higher accuracy in ultimate load prediction across various test cases.

Conclusion: CSFEM provides a stable and efficient framework for elastic-plastic analysis of complex geotechnical problems, demonstrating enhanced performance over conventional methods while retaining computational flexibility.

Abstract: An elastic-plastic cell-based smoothed finite element method (CSFEM) is
proposed for geotechnical analysis of soils and rocks exhibiting nonlinear and
path-dependent behaviors. By introducing strain smoothing over subcell domains
and employing a consistent stress return-mapping algorithm, the method enhances
stress accuracy, alleviates volumetric locking, and reduces sensitivity to mesh
distortion while retaining the flexibility of polygonal elements. The
formulation is implemented in ABAQUS via a user-defined element and validated
through benchmark and practical problems, including a pressurized thick
cylinder, biaxial soil test, strip footing bearing capacity, tunnel excavation,
and slope stability. Numerical results show excellent agreement with analytical
solutions and conventional FEM, with smoother stress fields, improved
convergence, and higher accuracy in ultimate load prediction. These findings
demonstrate that CSFEM provides a stable and efficient framework for
elastic-plastic analysis of complex geotechnical problems.

</details>


### [9] [Scaling crossover of the generalized Jeffreys-type law](https://arxiv.org/abs/2510.07930)
*Fugui Ma*

Main category: math.NA

TL;DR: This paper develops a novel numerical scheme (CIM-CLG algorithm) for solving multi-term time-fractional Jeffreys-type equations, providing spectral accuracy with reduced computational complexity and full parallelizability.


<details>
  <summary>Details</summary>
Motivation: To provide a physical explanation for the generalized Jeffreys-type law from first principles and develop efficient computational methods for complex nonlocal problems exhibiting scaling crossover phenomena.

Method: Used continuous-time random walk framework with generalized waiting time distribution, derived equation from overdamped Langevin equation with stochastic time-change, employed Laplace transform for analysis, and developed CIM-CLG numerical scheme with spectral accuracy.

Result: Established well-posedness and Sobolev regularity of the equation, achieved computational complexity of O(N) in time and O(M log M) in space, validated efficiency and accuracy through extensive 1D/2D numerical experiments.

Conclusion: The work integrates stochastic modeling, mathematical analysis, and numerical computation to advance understanding of generalized Jeffreys-type law and provide a rigorous, efficient framework for complex nonlocal problems.

Abstract: The generalized Jeffreys-type law is formulated as a multi-term
time-fractional Jeffreys-type equation, whose dynamics exhibit rich scaling
crossover phenomena entailing different diffusion mechanisms. In this work, we
provide a novel physical explanation for the equation from first principles,
beginning with a microscopic description based on the continuous-time random
walk framework with a generalized waiting time distribution and further
deriving the equation from an overdamped Langevin equation subject to a
stochastic time-change (subordination). Employing the Laplace transform method,
we conduct a rigorous analysis of the equation, establishing its well-posedness
and providing a detailed Sobolev regularity analysis. We also develop a novel
numerical scheme, termed the CIM-CLG algorithm, which achieves spectral
accuracy in both time and space while substantially relaxing the temporal
regularity requirements on the solution. The algorithm reduces the
computational complexity to $\mathcal{O}(N)$ in time and $\mathcal{O}(M\log M)$
in space and is fully parallelizable. Detailed implementation guidelines and
new technical error estimates are provided. Extensive numerical experiments in
1D and 2D settings validate the efficiency, robustness, and accuracy of the
proposed method. By integrating stochastic modeling, mathematical analysis, and
numerical computation, this work advances the understanding of the generalized
Jeffreys-type law and offers a mathematically rigorous and computationally
efficient framework for tackling complex nonlocal problems.

</details>


### [10] [Likelihood-informed Model Reduction for Bayesian Inference of Static Structural Loads](https://arxiv.org/abs/2510.07950)
*Jakob Scheffels,Elizabeth Qian,Iason Papaioannou,Elisabeth Ullmann*

Main category: math.NA

TL;DR: A new projection-based model reduction method for Bayesian inverse problems that exploits low-dimensional structure in the likelihood-informed subspace to accelerate computations.


<details>
  <summary>Details</summary>
Motivation: Bayesian inverse problems are computationally expensive due to many forward model evaluations, but often exhibit low-dimensional structure where data are informative only in a small subspace.

Method: Projects the governing PDE onto the likelihood-informed subspace, creating a reduced model for the case where the unknown parameter is the right-hand-side forcing.

Result: Numerical experiments show the method successfully exploits intrinsic low-dimensionality, achieving O(10^{-10}) relative errors with 10x-100x lower-dimensional models.

Conclusion: The proposed approach effectively accelerates Bayesian inverse problem solutions by leveraging the low-dimensional structure inherent in many structural engineering applications.

Abstract: Bayesian inverse problems use data to update a prior probability distribution
on uncertain parameter values to a posterior distribution. Such problems arise
in many structural engineering applications, but computational solution of
Bayesian inverse problems is often expensive because standard solution
approaches require many evaluations of the forward model mapping the parameter
value to predicted observations. In many settings, this forward model is
expensive because it requires the solution of a high-dimensional discretization
of a partial differential equation. However, Bayesian inverse problems often
exhibit low-dimensional structure because the available data are primarily
informative (relative to the prior) in a low-dimensional subspace, sometimes
called the likelihood-informed subspace (LIS). This paper proposes a new
projection-based model reduction method for static linear systems that exploits
this low-dimensional structure in the setting where the unknown parameter is
the right-hand-side forcing. The proposed method projects the governing partial
differential equation onto the likelihood-informed subspace, yielding a
computationally efficient reduced model that can be used to accelerate the
solution of the inverse problem. Numerical experiments on two structural
engineering model problems demonstrate that the proposed approach can
successfully exploit the intrinsic low-dimensionality of the problem, obtaining
relative errors of O(10^{-10}) in the inverse problem solution with a 10x-100x
lower-dimensional model.

</details>


### [11] [LDMD with Temporally Adaptive Segmentation](https://arxiv.org/abs/2510.08065)
*Qiuqi Li,Chang Liu,Yifei Yang*

Main category: math.NA

TL;DR: Localized DMD (LDMD) framework improves long-term prediction accuracy by segmenting temporal domain and performing localized predictions, with adaptive segmentation and error analysis.


<details>
  <summary>Details</summary>
Motivation: Standard DMD struggles with poor long-term predictive accuracy, so a localized approach is needed to improve forecasting performance.

Method: Segment temporal domain into subintervals, construct snapshot matrices within each segment, perform localized predictions, and use adaptive segmentation strategy.

Result: LDMD significantly enhances long-term predictive accuracy while maintaining high computational efficiency on benchmark problems including Burgers', Allen-Cahn, nonlinear Schrodinger, and Maxwell's equations.

Conclusion: The proposed LDMD framework effectively addresses DMD's long-term prediction limitations through temporal segmentation and localized forecasting.

Abstract: Dynamic mode decomposition (DMD) is a widely used data-driven algorithm for
predicting the future states of dynamical systems. However, its standard
formulation often struggles with poor long-term predictive accuracy. To address
this limitation, we propose a localized DMD (LDMD) framework that improves
prediction performance by integrating DMD's strong linear forecasting
capabilities with time-domain segmentation techniques. In this framework, the
temporal domain is segmented into multiple subintervals, within which snapshot
matrices are constructed and localized predictions are performed. We first
present the localized DMD method with predefined segmentation, and then explore
an adaptive segmentation strategy to further enhance computational efficiency
and prediction robustness. Furthermore, we conduct an error analysis that
provides the upper bound of the local and global truncation error for the
proposed framework. The effectiveness of LDMD is demonstrated on four benchmark
problems-Burgers', Allen-Cahn, nonlinear Schrodinger, and Maxwell's equations.
Numerical results show that LDMD significantly enhances long-term predictive
accuracy while preserving high computational efficiency.

</details>


### [12] [Semi-Implicit Central scheme for Hyperbolic Systems of Balance Laws with Relaxed Source Term](https://arxiv.org/abs/2510.08134)
*Sudipta Sahu,Emanuele Macca,Rathan Samala*

Main category: math.NA

TL;DR: A new IMEX finite volume scheme combining midpoint rule in space and trapezoidal rule in time with backward semi-implicit Taylor expansion for stiff hyperbolic systems.


<details>
  <summary>Details</summary>
Motivation: Quasi-linear hyperbolic systems with stiff source terms present computational challenges that need stable and accurate numerical methods.

Method: Finite volume Nessyahu-Tadmor central scheme with new IMEX approach: stiff source term handled semi-implicitly using midpoint rule in space, trapezoidal rule in time, and backward semi-implicit Taylor expansion.

Result: Method maintains stability near stiffness and discontinuities while preserving second-order accuracy, validated through theoretical analysis and numerical tests on benchmark models.

Conclusion: The proposed IMEX scheme effectively solves stiff hyperbolic systems with good stability and accuracy properties.

Abstract: Quasi-linear hyperbolic systems with source terms introduce significant
computational challenges due to the presence of a stiff source term. To address
this, a finite volume Nessyahu-Tadmor (NT) central numerical scheme is explored
and applied to benchmark models such as the Jin-Xin relaxation model, the
shallow-water model, the Broadwell model, the Euler equations with heat
transfer, and the Euler system with stiff friction to assess their
effectiveness. The core part of this numerical scheme lies in developing a new
implicit-explicit (IMEX) scheme, where the stiff source term is handled in an
semi-implicit manner constructed by combining the midpoint rule in space, the
trapezoidal rule in time with a backward semi-implicit Taylor expansion. The
advantage of the proposed method lies in its stability region and maintains
robustness near stiffness and discontinuities, while asymptotically preserving
second-order accuracy.
  Theoretical analysis and numerical validation confirm the stability and
accuracy of the method, highlighting its potential for efficiently solving the
stiff hyperbolic systems of balance laws.

</details>


### [13] [Dual-primal Isogeometric Tearing and Interconnecting Solvers for adaptively refined multi-patch configurations](https://arxiv.org/abs/2510.08148)
*Stefan Takacs,Stefan Tyoler*

Main category: math.NA

TL;DR: The paper applies the dual-primal Isogeometric Tearing and Interconnecting method (IETI-DP) to adaptive multi-patch geometries generated by recursive patch splitting, providing solvability conditions and a preconditioner with optimal condition number bounds.


<details>
  <summary>Details</summary>
Motivation: Adaptive refinement is essential in Isogeometric Analysis since splines achieve full approximation power only with regular solutions, but solutions are usually not regular everywhere. The paper addresses the challenge of handling hierarchical, non-matching multi-patch configurations with T-junctions that arise from adaptive refinement.

Method: The authors investigate the application of IETI-DP method to adaptive multi-patch geometries generated by recursive patch splitting. They provide sufficient conditions for solvability of local problems and propose a preconditioner for the iterative solver.

Result: The paper establishes a condition number bound that matches the bound previously shown for fully matching cases. Numerical experiments confirm the theoretical findings and demonstrate the efficiency of the proposed approach in adaptive refinement scenarios.

Conclusion: The IETI-DP method can be successfully applied to adaptive multi-patch geometries with T-junctions, maintaining optimal convergence properties while handling the complexities of adaptive refinement in Isogeometric Analysis.

Abstract: Isogeometric Analysis is a variant of the finite element method, where spline
functions are used for the representation of both the geometry and the
solution. Splines, particularly those with higher degree, achieve their full
approximation power only if the solution is sufficiently regular. Since
solutions are usually not regular everywhere, adaptive refinement is essential.
Recently, a multi-patch-based adaptive refinement strategy based on recursive
patch splitting has been proposed, which naturally generates hierarchical,
non-matching multi-patch configurations with T-junctions, but preserves the
tensor-product structure within each patch.
  In this work, we investigate the application of the dual-primal Isogeometric
Tearing and Interconnecting method (IETI-DP) to such adaptive multi-patch
geometries. We provide sufficient conditions for the solvability of the local
problems and propose a preconditioner for the overall iterative solver. We
establish a condition number bound that coincides with the bound previously
shown for the fully matching case. Numerical experiments confirm the
theoretical findings and demonstrate the efficiency of the proposed approach in
adaptive refinement scenarios.

</details>


### [14] [Full moment error estimates in strong norms for numerical approximations of stochastic Navier-Stokes equations with multiplicative noise, Part I: time discretization](https://arxiv.org/abs/2510.08291)
*Xiaobing Feng,Liet Vo*

Main category: math.NA

TL;DR: Derives optimal-order full moment error estimates for Euler-Maruyama discretization of stochastic Navier-Stokes equations with multiplicative noise, and introduces a new framework for analyzing nonlinear SPDEs.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous error estimates for numerical approximations of stochastic Navier-Stokes equations and develop a general framework for analyzing nonlinear SPDEs with multiplicative noise.

Method: Uses exponential stability estimates for SPDE solutions, discrete stochastic Gronwall inequality, and bootstrap arguments to analyze the Euler-Maruyama time discretization.

Result: Achieves optimal-order full moment error estimates in strong norms for both velocity and pressure approximations.

Conclusion: The paper provides a novel framework for numerical analysis of nonlinear SPDEs with multiplicative noise, successfully applied to stochastic Navier-Stokes equations with rigorous error bounds.

Abstract: This paper focuses on deriving optimal-order full moment error estimates in
strong norms for both velocity and pressure approximations in the
Euler-Maruyama time discretization of the stochastic Navier-Stokes equations
with multiplicative noise. Additionally, it introduces a novel approach and
framework for the numerical analysis of nonlinear stochastic partial
differential equations (SPDEs) with multiplicative noise in general. The main
ideas of this approach include establishing exponential stability estimates for
the SPDE solution, leveraging a discrete stochastic Gronwall inequality, and
employing a bootstrap argument.

</details>


### [15] [Surface finite element approximation of parabolic SPDEs with Whittle--Matérn noise](https://arxiv.org/abs/2510.08443)
*Øyvind Stormark Auestad,Geir-Arne Fuglstad,Annika Lang*

Main category: math.NA

TL;DR: A new fully discrete surface finite element method for linear parabolic stochastic evolution equations with additive noise, using surface finite element approximation of noise with covariance operators from elliptic operators.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for stochastic PDEs on surfaces, particularly for equations with noise having covariance operators defined by powers of elliptic operators like Whittle-Matern random fields.

Method: Surface finite element discretization of both the solution and the noise, tailored for additive noise with covariance operators from elliptic operators. The method is fully discrete.

Result: Derived strong and pathwise convergence rates for the approximation, which were verified through numerical experiments.

Conclusion: The proposed surface finite element method provides an effective approach for approximating linear parabolic stochastic evolution equations with additive noise, with proven convergence rates that are validated numerically.

Abstract: We propose and analyse a new type of fully discrete surface finite element
approximation of a class of linear parabolic stochastic evolution equations
with additive noise. Our discretization uses a surface finite element
approximation of the noise, and is tailored for equations with noise having
covariance operator defined by (negative powers of) elliptic operators, like
Whittle--Mat\'ern random fields. We derive strong and pathwise convergence
rates of our approximation, and verify these by numerical experiments.

</details>


### [16] [Refinement-based Christoffel sampling for least squares approximation in non-orthogonal bases](https://arxiv.org/abs/2510.08461)
*Astird Herremans,Ben Adcock*

Main category: math.NA

TL;DR: A refinement-based Christoffel sampling algorithm for least squares approximation that avoids costly discrete orthogonalization for non-orthogonal basis functions.


<details>
  <summary>Details</summary>
Motivation: Standard Christoffel sampling requires i.i.d. sampling from a distribution proportional to the inverse Christoffel function, which needs orthonormal basis computation - a costly process for non-orthogonal bases via discrete orthogonalization.

Method: Proposes an iterative refinement-based algorithm inspired by approximate leverage score sampling that avoids discrete orthogonalization. Uses a numerical variant of Christoffel function to account for finite-precision effects.

Result: The algorithm achieves near-best approximations with O(n log(n)) samples while computational cost grows only logarithmically with ||k_n||_{L^∞(X)}, compared to proportional growth in existing methods.

Conclusion: The method is efficient and robust, with extensive numerical experiments supporting its performance and a convergence proof validating the approach.

Abstract: We introduce a refinement-based Christoffel sampling (RCS) algorithm for
least squares approximation in the span of a given, generally non-orthogonal
set of functions $\Phi_n = \{\phi_1, \dots, \phi_n\}$. A standard sampling
strategy for this problem is Christoffel sampling, which achieves near-best
approximations in probability using only $\mathcal{O}(n \log(n))$ samples.
However, it requires i.i.d.\ sampling from a distribution whose density is
proportional to the inverse Christoffel function $k_n$, the computation of
which requires an orthonormal basis. As a result, existing approaches for
non-orthogonal bases $\Phi_n$ typically rely on costly discrete
orthogonalization. We propose a new iterative algorithm, inspired by recent
advances in approximate leverage score sampling, that avoids this bottleneck.
Crucially, while the computational cost of discrete orthogonalization grows
proportionally with $\|k_n\|_{L^\infty(X)}$, the cost of our approach increases
only logarithmically in $\|k_n\|_{L^\infty(X)}$. In addition, we account for
finite-precision effects by considering a numerical variant of the Christoffel
function, ensuring that the algorithm relies only on computable quantities.
Alongside a convergence proof, we present extensive numerical experiments
demonstrating the efficiency and robustness of the proposed method.

</details>


### [17] [Where Have All the Kaczmarz Iterates Gone?](https://arxiv.org/abs/2510.08563)
*El Houcine Bergou,Soumia Boucherouite,Aritra Dutta,Xin Li,Anna Ma*

Main category: math.NA

TL;DR: This paper analyzes the asymptotic behavior of the randomized Kaczmarz algorithm on noisy and inconsistent linear systems, studying where the iterates converge and deriving convergence bounds based on noise levels and system characteristics.


<details>
  <summary>Details</summary>
Motivation: The randomized Kaczmarz algorithm is efficient for large-scale linear systems, but practical applications often involve noisy and inconsistent systems. While RK convergence is well-understood for consistent systems, its behavior on noisy systems is limited.

Method: The paper investigates the asymptotic behavior of RK iterates in expectation for noisy systems, analyzes the roles of singular vectors of the coefficient matrix, and derives bounds on the convergence horizon based on noise levels and system characteristics.

Result: The study provides theoretical findings on the locations of limit points and convergence bounds, validated by extensive numerical experiments that offer practical insights into RK's performance under realistic noisy conditions.

Conclusion: The results establish a deeper understanding of RK algorithm's limitations and robustness in noisy environments, enabling optimized applications in real-world scientific and engineering problems.

Abstract: The randomized Kaczmarz (RK) algorithm is one of the most computationally and
memory-efficient iterative algorithms for solving large-scale linear systems.
However, practical applications often involve noisy and potentially
inconsistent systems. While the convergence of RK is well understood for
consistent systems, the study of RK on noisy, inconsistent linear systems is
limited. This paper investigates the asymptotic behavior of RK iterates in
expectation when solving noisy and inconsistent systems, addressing the
locations of their limit points. We explore the roles of singular vectors of
the (noisy) coefficient matrix and derive bounds on the convergence horizon,
which depend on the noise levels and system characteristics. Finally, we
provide extensive numerical experiments that validate our theoretical findings,
offering practical insights into the algorithm's performance under realistic
conditions. These results establish a deeper understanding of the RK
algorithm's limitations and robustness in noisy environments, paving the way
for optimized applications in real-world scientific and engineering problems.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [18] [Consistency of some well-posed five-field theories of dissipative relativistic fluid dynamics](https://arxiv.org/abs/2510.07526)
*Heinrich Freistuhler*

Main category: math.AP

TL;DR: The paper analyzes causal hyperbolic five-field theories from the FTBDNK family of relativistic Navier-Stokes formulations, showing they are O(ε²) equivalent to Landau-Lifshitz formulation with O(ε³) excess entropy production.


<details>
  <summary>Details</summary>
Motivation: To establish consistency properties for causal hyperbolic five-field theories obtained from Landau-Lifshitz formulation via Eulerian gradient shifts (EGS(L) models).

Method: Analysis of EGS(L) family of models that generalize previous classes, examining their properties relative to the Landau-Lifshitz formulation with dissipation coefficient magnitude ε.

Result: Any element of EGS(L) is O(ε²) equivalent to Landau-Lifshitz formulation, has O(ε³) excess entropy production, and represents heterogeneous local thermodynamic equilibria cleanly.

Conclusion: The EGS(L) models provide consistent causal hyperbolic formulations that closely approximate the Landau-Lifshitz theory while maintaining desirable thermodynamic properties.

Abstract: Within the FTBDNK family of formulations of relativistic Navier-Stokes (H.
Freist\"uhler and B. Temple, Proc. R. Soc. A 470, 20140055 (2014), Proc. R.
Soc. A 473 (2017), 20160729; F. S. Bemfica, M. Disconzi, and J. Noronha, Phys.
Rev. D 98, 104064 (2018), Phys. Rev. D 100, 104020 (2019); P. Kovtun, J. High
Energy Phys. 2019, 034 (2019)), this paper collects some consistency properties
for certain causal hyperbolic five-field theories obtained from the
Landau-Lifshitz formulation via Eulerian gradient shifts, a family, EGS(L), of
models that slightly generalize a class identified in H. Freist\"uhler, J.
Math.\ Phys. 61, 033101 (2020). With $\epsilon$ the magnitude of the
dissipation coefficients that quantify viscosity and heat conduction, the paper
shows that any element of EGS(L) is $O(\epsilon^2)$ equivalent to the
Landau-Lifshitz formulation, has an $O(\epsilon^3)$ excess entropy production,
and represents heterogeneous local thermodynamic equilibria cleanly.

</details>


### [19] [First order equation on random measures as superposition of weak solutions to the McKean-Vlasov equation](https://arxiv.org/abs/2510.07542)
*Alessandro Pinzi*

Main category: math.AP

TL;DR: The paper defines an evolution equation for random probability measures with non-local drift and diffusion, shows solutions can be lifted to superposition solutions of non-linear KFP and McKean-Vlasov equations, and transfers existence/uniqueness results.


<details>
  <summary>Details</summary>
Motivation: To develop a framework for analyzing evolution equations of random probability measures with non-local interactions and establish connections between different mathematical formulations.

Method: Define evolution equation for random measures, prove lifting to superposition solutions of non-linear Kolmogorov-Fokker-Planck and McKean-Vlasov equations, transfer existence/uniqueness results.

Result: Solutions to the random measure equation can be represented as superpositions of solutions to non-linear KFP and McKean-Vlasov equations, enabling transfer of existence and uniqueness properties.

Conclusion: The superposition approach provides a powerful method to connect different formulations and transfer existence/uniqueness results between random measure equations and associated non-linear PDEs and SDEs.

Abstract: The goal of this paper is to define an evolution equation for a curve of
random probability measures $(M_t)_{t\in[0,T]}\subset
\mathcal{P}(\mathcal{P}(\mathbb{R}^d))$ associated to a non-local drift
$b:[0,T]\times\mathbb{R}^d \times \mathcal{P}(\mathbb{R}^d) \to \mathbb{R}^d$
and a non-local diffusion term $a:[0,T]\times \mathbb{R}^d \times
\mathcal{P}(\mathbb{R}^d) \to \operatorname{Sym}_+(\mathbb{R}^{d\times d})$.
Then, we show that any solution to that equation can be lifted to a
superposition of solutions to a non-linear Kolmogorov-Fokker-Planck equation
and also to a superposition of weak solutions to the McKean-Vlasov equations.
Finally, we use this superposition result to show how existence and uniqueness
can be transferred from the equation on random measures to the associated
non-linear Kolmogorov-Fokker-Planck equation and to the McKean-Vlasov equation,
assuming uniqueness of the linearized KFP.

</details>


### [20] [Nonlinear Stability of the Rayleigh-Taylor Problem in Quantum Navier-Stokes Equations](https://arxiv.org/abs/2510.07695)
*Fei Jiang,Yajie Zhang,Zhipeng Zhang,Youyi Zhao*

Main category: math.AP

TL;DR: The paper proves that quantum effects can inhibit Rayleigh-Taylor instability in nonlinear quantum Navier-Stokes equations under proper conditions, showing algebraic stability when the scaled Planck constant exceeds a threshold.


<details>
  <summary>Details</summary>
Motivation: To extend the known linear theory of quantum effect stabilization of Rayleigh-Taylor instability to the nonlinear case and rigorously prove inhibition of RT instability in quantum fluids.

Method: Mathematical proof using a complicated multi-layer energy method with anisotropic norms of spatial derivatives for the nonlinear quantum Navier-Stokes equations in a slab with Navier boundary conditions.

Result: If the RT density profile satisfies an additional stabilizing condition and the scaled Planck constant exceeds a threshold ε_c, then small perturbation solutions around an RT equilibrium state are algebraically stable in time.

Conclusion: Quantum effects can completely inhibit Rayleigh-Taylor instability in the nonlinear regime under proper conditions, extending the linear theory results to more realistic physical scenarios.

Abstract: It is well-known that the Rayleigh--Taylor (abbr. RT) instability can be
completely inhibited by the quantum effect stabilization in proper
circumstances leading to a cutoff wavelength in the \emph{linear} motion
equations. Motivated by the linear theory, we further investigate the
{stability} for the \emph{nonlinear} RT problem of quantum Navier--Stokes
equations in a slab with Navier boundary condition, and rigorously prove the
inhibition of RT instability by the quantum effect under a proper setting. More
precisely, if the RT density profile $\bar\rho$ satisfies an additional
stabilizing condition, then there is a threshold ${\varepsilon_{{c}}}$ of the
scaled Planck constant, such that if the scaled Planck constant is bigger than
${\varepsilon_{{c}}}$, the small perturbation solutions around an RT
equilibrium state are algebraically stable in time. The mathematical proof is
realized by a complicated multi-layer energy method with anisotropic norms of
spacial derivatives.

</details>


### [21] [A log-free estimate for the diagonal paraproduct high $\times$ high $\to$ low in the 3D Navier-Stokes equation](https://arxiv.org/abs/2510.07848)
*Pylyp Cherevan*

Main category: math.AP

TL;DR: The paper analyzes the diagonal paraproduct in the 3D Navier-Stokes nonlinearity (u·∇)u, obtaining log-free L²_t Ḣ^{-1}_x estimates for the projection P_{<N^{1-δ}}∇(u_N⊗v_N) in scale-critical windows for 1/6<δ≤5/8.


<details>
  <summary>Details</summary>
Motivation: To understand the critical energy scheme in 3D Navier-Stokes equations by analyzing the diagonal paraproduct structure in the nonlinear term (u·∇)u.

Method: Uses phase-geometric integration, anisotropic local estimates on cylinders, bilinear ℓ² decoupling on finite-rank surfaces, and controls the narrow diagonal zone via null form suppression.

Result: Obtains log-free estimates at L²_t Ḣ^{-1}_x level for the projection P_{<N^{1-δ}}∇(u_N⊗v_N) in the range 1/6<δ≤5/8, consistent with critical energy scheme.

Conclusion: The analysis is restricted to a single resonant component; extensions to the full (u·∇)u structure and sup_t versions remain for future work.

Abstract: We consider the diagonal paraproduct arising in the nonlinearity $(u\cdot
\nabla) u$ for the three-dimensional Navier-Stokes equations. On scale-critical
windows and in the range $1/6 < \delta \le 5/8$ we obtain a log-free estimate
at the level $L^2_t {\dot H}^{-1}_x$ for the projection $P_{< N^{1-\delta}}
\nabla(u_N \otimes v_N)$, consistent with the critical energy scheme. The main
tools are phase-geometric integration, anisotropic local estimates on
cylinders, and bilinear $ell^2$ decoupling on a finite-rank surface; the narrow
diagonal zone is controlled via suppression of the null form. The work is
restricted to a single resonant component; extensions to the full structure $(u
\cdot\nabla) u$ and to sup$_t$ versions are left for further analysis.

</details>


### [22] [The $\eps-\eps^β$ property for clusters with double density](https://arxiv.org/abs/2510.07907)
*A. Pratelli,V. Scattaglia*

Main category: math.AP

TL;DR: Extends the "ε-ε^β property" to clusters in Euclidean space with double density


<details>
  <summary>Details</summary>
Motivation: To generalize the ε-ε^β property beyond standard density settings to more complex double density configurations

Method: Mathematical extension of the ε-ε^β property framework to handle clusters with double density in Euclidean space

Result: Successfully extends the property to the double density case, establishing theoretical foundations

Conclusion: The extension provides a more comprehensive framework for analyzing clusters with complex density structures in Euclidean spaces

Abstract: This article is devoted to extend the "$\eps-\eps^\beta$ property" to the
case of clusters in an Euclidean space with a double density.

</details>


### [23] [Fractional p-Laplacian Kirchhoff-type problem involving a singular term via Nehari manifold](https://arxiv.org/abs/2510.07911)
*Djamel Abid*

Main category: math.AP

TL;DR: Existence of nontrivial positive solutions for Kirchhoff-type problems with sign-changing nonlinearities and singular terms


<details>
  <summary>Details</summary>
Motivation: To study the existence of nontrivial positive solutions for Kirchhoff-type problems that involve sign-changing nonlinearities and singular terms, which are challenging due to the combination of these features

Method: Using the Nehari manifold approach and Ekeland's variational principle to analyze the problem

Result: For appropriate choice of parameter λ, the problem has at least two positive solutions in both subcritical and critical cases

Conclusion: The combination of Nehari manifold and Ekeland's variational principle successfully establishes the existence of multiple positive solutions for Kirchhoff-type problems with sign-changing nonlinearities and singular terms

Abstract: This paper is dedicated to studying the existence of nontrivial positive
solutions for a Kirchhoff-type problem with sign change nonlinearities and a
singular term, Using the Nehari manifold and EkelandS variational principle we
prove that for the appropriate choice of {\lambda} our problem has at least two
positive solutions for both subcritical and critical cases.

</details>


### [24] [A cross-diffusion system with independent drifts and fast diffusion](https://arxiv.org/abs/2510.07937)
*Charles Elbar,Filippo Santambrogio*

Main category: math.AP

TL;DR: The paper extends existence results for global weak solutions in a one-dimensional cross-diffusion system from linear diffusion to fast-diffusion cases.


<details>
  <summary>Details</summary>
Motivation: To generalize previous results on cross-diffusion systems with linear diffusion to include fast-diffusion cases with exponent 0 < α ≤ 1.

Method: Analysis of a one-dimensional cross-diffusion system on the torus with fast-diffusion law and different external potentials, using non-negative L¹ initial data with bounded entropy and a mixing condition.

Result: Proves existence of global weak solutions for arbitrary non-negative L¹ initial data satisfying bounded entropy and mixing conditions.

Conclusion: Successfully extends the recent result of Mészáros and Parker from linear diffusion (α=1) to fast-diffusion cases (0 < α ≤ 1).

Abstract: We study a one-dimensional cross-diffusion system for two interacting
populations on the torus, with a fast-diffusion law with exponent $0< \alpha\le
1$ and different external potentials. For arbitrary non-negative $L^{1}$
initial data with bounded entropy and a mixing condition we prove the existence
of global weak solutions. This extends the recent result of M\'esz\'aros,
Parker from the linear diffusion ($\alpha=1$) to the fast-diffusion.

</details>


### [25] [Gradient regularity for widely degenerate parabolic equations](https://arxiv.org/abs/2510.07999)
*Michael Strunk*

Main category: math.AP

TL;DR: The paper establishes continuity of certain functions of the gradient of weak solutions to degenerate parabolic equations where the diffusion vanishes on a bounded convex set containing the origin.


<details>
  <summary>Details</summary>
Motivation: To extend C^1-regularity results from the elliptic case to the parabolic setting for equations with degenerate diffusion that vanishes on a bounded convex set.

Method: Analysis of weak solutions to parabolic equations with diffusion function F that is elliptic outside a bounded convex set E and vanishes inside E, assuming f belongs to L^{n+2+σ}(Ω_T).

Result: Proved that K(Du) ∈ C^0(Ω_T) for any continuous function K that vanishes on E, establishing continuity of certain functions of the gradient.

Conclusion: Successfully extended regularity results from elliptic to parabolic equations with degenerate diffusion, showing continuity properties of gradient-related functions.

Abstract: In this paper, we are interested in the regularity of weak solutions
$u\colon\Omega_T\to\mathbb{R}$ to parabolic equations of the type
\begin{equation*}
  \partial_t u - \mathrm{div} \nabla \mathcal{F}(x,t,Du) = f\qquad\mbox{in
$\Omega_T$}, \end{equation*} where $\mathcal{F}$ is only elliptic for values of
$Du$ outside a bounded and convex set $E\subset \mathbb{R}^n$ with the property
that $0\in \mathrm{Int}{E}$. Here, $\Omega_T
:=\Omega\times(0,T)\subset\mathbb{R}^{n+1}$ denotes a space-time cylinder taken
over a bounded domain $\Omega\subset\mathbb{R}^n$ for some finite time $T>0$.
The function $\mathcal{F} : \Omega_T\times\mathbb{R}^n \to\mathbb{R}_{\geq 0}$
present in the diffusion is assumed to satisfy: the partial mapping $\xi\mapsto
\mathcal{F}(x,t,\xi)$ is regular whenever $\xi$ lies outside of $E$, and
vanishes entirely whenever $\xi$ lies within this set. Additionally, the datum
$f$ is assumed to be of class $L^{n+2+\sigma}(\Omega_T)$ for some parameter
$\sigma > 0$. As our main result we establish that
  \begin{equation*}
  \mathcal{K}(Du)\in C^0(\Omega_T)
  \end{equation*} for any continuous function $\mathcal{K}\in
C^0(\mathbb{R}^n)$ that vanishes on $E$. This article aims to extend the
$C^1$-regularity result for the elliptic case to the parabolic setting.

</details>


### [26] [Stability of Traveling Fronts of the FitzHugh-Nagumo Equations on Cylindrical Surfaces](https://arxiv.org/abs/2510.08028)
*Afroditi Talidou*

Main category: math.AP

TL;DR: Traveling front solutions of FitzHugh-Nagumo equations are nonlinearly stable on cylindrical surfaces, including standard cylinders and warped cylinders with slowly varying radius.


<details>
  <summary>Details</summary>
Motivation: To extend the known nonlinear stability of traveling front solutions in one spatial dimension to cylindrical surfaces, including both standard and warped geometries.

Method: Mathematical analysis of traveling front solutions on cylindrical surfaces, with extension to warped cylinders with slowly varying radius, supported by numerical simulations.

Result: Traveling fronts are nonlinearly stable on standard cylinders and persist on warped cylinders with slowly varying radius.

Conclusion: The stability properties of FitzHugh-Nagumo traveling fronts extend to cylindrical geometries, providing theoretical foundation for front propagation on curved surfaces.

Abstract: The FitzHugh-Nagumo equations are known to admit traveling front solutions in
one spatial dimension that are nonlinearly stable. This paper concerns the
stability of traveling front solutions propagating on cylindrical surfaces. It
is shown that such traveling fronts are nonlinearly stable on the surface of
standard cylinders of constant radius. The analysis is extended to warped
cylinders with slowly varying radius, where persistence of front-like solutions
is established. Numerical simulations support the theoretical findings.

</details>


### [27] [Determining a magnetic Schrödinger equation by a single far-field measurement](https://arxiv.org/abs/2510.08198)
*Chaohua Duan,Zhen Xue*

Main category: math.AP

TL;DR: Single far-field measurement uniquely determines support of magnetic and electric potentials for polyhedral scatterers. Transmission eigenfunctions vanish at corners in 2D and edge corners in 3D when angle ≠ π.


<details>
  <summary>Details</summary>
Motivation: To advance theoretical understanding of inverse scattering with magnetic potentials and demonstrate minimal measurement data suffices for shape reconstruction in practical cases.

Method: Variational approach for direct problem well-posedness, complex geometric optics solutions with asymptotic analysis near singular points.

Result: Unique determination of potential support from single far-field measurement for polyhedral structures. Transmission eigenfunctions vanish at corners.

Conclusion: Work provides new insights into quantum effects and singular geometries, with applications in quantum imaging, material characterization, and nondestructive testing.

Abstract: This paper investigates the inverse scattering problem for the magnetic
Schr\"odinger equation. We first establish the well-posedness of the direct
problem through a variational approach under physically meaningful assumptions
on the magnetic and electric potentials. Our main results demonstrate that a
single far-field measurement uniquely determines the support of the potential
functions when the scatterer has polyhedral structures.
  A significant theoretical byproduct of our analysis reveals that transmission
eigenfunctions must vanish at corners in two dimensions and edge corners in
three dimensions, provided the angle is not $\pi$. This geometric property of
eigenfunctions extends previous results for the non-magnetic case and provides
new insights into the interaction between quantum effects and singular
geometries. The proof combines complex geometric optics solutions with careful
asymptotic analysis near singular points.
  From an inverse problems perspective, our work shows that minimal measurement
data suffices for shape reconstruction in important practical cases, advancing
the theoretical understanding of inverse scattering with magnetic potentials.
The results have potential applications in quantum imaging, material
characterization, and nondestructive testing where magnetic fields play a
crucial role.

</details>


### [28] [A survey on the optimal partition problem](https://arxiv.org/abs/2510.08241)
*Roberto Ognibene,Bozhidar Velichkov*

Main category: math.AP

TL;DR: Survey on regularity theory for optimal partition problems involving vector-valued Sobolev functions with disjoint supports, covering both interior and boundary regularity of solutions and free boundaries.


<details>
  <summary>Details</summary>
Motivation: The optimal partition problem has emerged in diverse contexts, necessitating a coherent overview and up-to-date account of progress in regularity theory for solutions and their free boundaries.

Method: Synthesizes current state of the art by analyzing non-negative vector-valued Sobolev functions with mutually disjoint support components, focusing on local minimizers of Dirichlet energy and critical points satisfying variational inequalities.

Result: Provides comprehensive synthesis of regularity results for solutions and their free boundaries in optimal partition problems, covering both interior and boundary cases.

Conclusion: This survey offers a unified perspective and current understanding of regularity theory in optimal partition problems, addressing both interior and boundary regularity aspects.

Abstract: This survey synthesizes the current state of the art on the regularity theory
for solutions to the optimal partition problem. Namely, we consider
non-negative, vector-valued Sobolev functions whose components have mutually
disjoint support, and which are either local minimizers of the Dirichlet energy
or, more generally, critical points satisfying a system of variational
inequalities. This is particularly meaningful as the problem has emerged on
several occasions and in diverse contexts: our aim is then to provide a
coherent point of view and an up-to-date account of the progress concerning
regularity of the solutions and their free boundaries, both in the interior and
up to a fixed boundary.

</details>


### [29] [Hölder regularity of the solutions of Fredholm integral equations on upper Ahlfors regular sets](https://arxiv.org/abs/2510.08264)
*M. Lanza de Cristoforis,M. Norman*

Main category: math.AP

TL;DR: The paper extends Hölder continuity results for solutions of Fredholm integral equations to metric measured spaces with upper Ahlfors growth conditions, including nondoubling measures.


<details>
  <summary>Details</summary>
Motivation: To generalize classical Hölder continuity results for integral equations beyond the traditional doubling measure context, allowing for more flexible measure spaces including those with nondoubling measures.

Method: Extending the analysis of Fredholm integral equations of the second kind to metric measured spaces that satisfy upper Ahlfors growth conditions, which are less restrictive than doubling conditions.

Result: The paper establishes the validity of (generalized) Hölder continuity for solutions of Fredholm integral equations in these more general metric measured spaces.

Conclusion: The classical Hölder continuity results for integral equations can be successfully extended to metric measured spaces with upper Ahlfors growth conditions, significantly broadening the applicability beyond doubling measures.

Abstract: We extend to the context of metric measured spaces, with a measure that
satisfies upper Ahlfors growth conditions the validity of (generalized)
H\"{o}lder continuity results for the solution of a Fredholm integral equation
of the second kind. Here we note that upper Ahlfors growth conditions include
also cases of nondoubling measures.

</details>


### [30] [On the Cahn-Hilliard equation with nonlinear diffusion: the non-convex case](https://arxiv.org/abs/2510.08287)
*Monica Conti,Stefania Gatti,Andrea Giorgini,Giulio Schimperna*

Main category: math.AP

TL;DR: Analysis of Cahn-Hilliard equation with nonlinear diffusion and non-degenerate mobility, removing convexity assumptions and establishing new solution properties in 2D and 3D.


<details>
  <summary>Details</summary>
Motivation: Previous results required strong convexity of energy gradient, excluding relevant cases. This work aims to remove convexity condition and study phase separation in complex systems like crystals and polymers.

Method: Uses Lojasiewicz-Simon inequality tailored to nonlinear diffusion case to analyze longtime dynamics. Proves uniqueness and smoothing in 2D, local well-posedness and global existence near minimizers in 3D.

Result: In 2D: uniqueness of weak solutions, smoothing effect for positive times, convergence to equilibrium. In 3D: local well-posedness for arbitrary data, global existence near energy minimizers, Lyapunov stability principle.

Conclusion: Successfully removed convexity assumption and established comprehensive solution properties for Cahn-Hilliard equation with nonlinear diffusion, providing new insights into phase separation dynamics in complex systems.

Abstract: We investigate the Cahn-Hilliard equation with nonlinear diffusion and
non-degenerate mobility modeling phase separation phenomena in complex systems
(e.g., crystals and polymers). Previous results in the literature on this model
relied on the strong convexity assumption of the gradient part of the energy,
which excludes relevant cases. In this work, we remove the convexity condition
and establish new qualitative properties of solutions under general assumptions
on the diffusion and mobility functions. In two spatial dimensions, we prove
uniqueness of weak solutions, their smoothing effect for positive times, and
convergence to equilibrium as time tends to infinity. In three dimensions, we
show local well-posedness of strong solutions for arbitrary initial data and
global existence for data close to energy minimizers, yielding a Lyapunov
stability principle. A key ingredient of our analysis is a Lojasiewicz-Simon
inequality tailored to the nonlinear diffusion case, which enables us to
characterize the longtime dynamics.

</details>


### [31] [On a class of (non)local superposition operators of arbitrary order](https://arxiv.org/abs/2510.08345)
*Serena Dipierro,Sven Jarohs,Enrico Valdinoci*

Main category: math.AP

TL;DR: Systematic study of superposition operators of any positive order, including examples, counterexamples, and applications to nonlinear problems.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive framework for dealing with superposition operators of arbitrary positive order, addressing gaps in existing theory.

Method: General mathematical setting with systematic analysis, examples, counterexamples, and characterization of measures and functional spaces.

Result: Established theoretical framework for superposition operators and existence theory for nonlinear problems involving fractional order operators.

Conclusion: Provides a unified approach for superposition operators of any positive order with practical applications in nonlinear analysis.

Abstract: In this paper we introduce a very general setting dealing with the
superposition of operators of any positive order and provide a systematic study
of them. We also provide examples and counterexamples, as well as
characterizing properties of the measures and the functional spaces under
consideration. Moreover, we present some applications regarding the existence
theory for a class of nonlinear problems involving superposition operators of
arbitrary (possibly fractional) order.

</details>


### [32] [The Space-Time Connectivity Theorem for Normal Currents](https://arxiv.org/abs/2510.08360)
*Paolo Bonicatto,Filip Rindler,Harry Turnbull*

Main category: math.AP

TL;DR: Establishes a Space-Time Connectivity Theorem for normal currents, extending classical results to provide progressive-in-time deformations between sequences and their limits.


<details>
  <summary>Details</summary>
Motivation: To generalize classical connectivity theorems by Federer and Fleming to normal currents, enabling the witnessing of weak* convergence through space-time deformations.

Method: Develops a space-time framework where connecting currents include a time coordinate, allowing progressive deformation from sequence elements to their limit.

Result: Proves that uniformly bounded sequences of boundaryless normal currents can be connected to their weak* limit via space-time normal currents.

Conclusion: The theorem provides a constructive way to understand convergence in normal currents through time-progressive deformations, distinguishing from classical static approaches.

Abstract: This work establishes a Space-Time Connectivity Theorem for normal currents.
In analogy to classical results by Federer and Fleming as well as a recent
theorem for integral currents by the second author, this result allows one to
witness the weak* convergence of a uniformly bounded sequence of boundaryless
normal currents with a space-time normal current that connects the elements of
the sequence with their limit. The space-time setting is distinguished from the
classical case in that this connecting current has a time coordinate and thus
constitutes a progressive-in-time way to deform an element of the sequence to
the limit.

</details>


### [33] [Analysis of the transmission eigenvalue problem for biharmonic scattering considering penetrable scatterers](https://arxiv.org/abs/2510.08444)
*Rafael Ceja Ayala,Isaac Harris,Andreas Kleefeld*

Main category: math.AP

TL;DR: Analytical study of transmission eigenvalues for biharmonic scattering with a penetrable obstacle in a 2D Kirchhoff-Love plate, proving existence, discreteness, and monotonicity with respect to refractive index.


<details>
  <summary>Details</summary>
Motivation: Previous studies focused on transmission eigenvalues for acoustic scattering, but this work extends the analysis to biharmonic scattering in elastic plates.

Method: Analytical proofs of existence and discreteness of transmission eigenvalues, study of dependence on refractive index, and numerical validation experiments.

Result: Proved existence and discreteness of transmission eigenvalues, established monotonicity of the first transmission eigenvalue with respect to refractive index.

Conclusion: The theoretical framework for transmission eigenvalues in biharmonic scattering is established and validated numerically, extending previous acoustic scattering results to elastic plate problems.

Abstract: In this paper, we provide an analytical study of the transmission eigenvalue
problem in the context of biharmonic scattering with a penetrable obstacle. We
will assume that the underlying physical model is given by an infinite elastic
two--dimensional Kirchhoff--Love plate in $\mathbb{R}^2$, where the plate's
thickness is small relative to the wavelength of the incident wave. In previous
studies, transmission eigenvalues have been studied for acoustic scattering,
whereas in this case, we consider biharmonic scattering. We prove the existence
and discreteness of the transmission eigenvalues as well as study the
dependence on the refractive index. We are able to prove the monotonicity of
the first transmission eigenvalue with respect to the refractive index. Lastly,
we provide numerical experiments to validate the theoretical work.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [34] [A Virtual Fields Method-Genetic Algorithm (VFM-GA) calibration framework for isotropic hyperelastic constitutive models with application to an elastomeric foam material](https://arxiv.org/abs/2510.07683)
*Zicheng Yan,Jialiang Tao,Christian Franck,David L. Henann*

Main category: physics.comp-ph

TL;DR: A calibration framework combining Virtual Fields Method and Genetic Algorithm for automated parameter identification in isotropic hyperelastic constitutive models, validated on compressible elastomeric foams.


<details>
  <summary>Details</summary>
Motivation: To automate material parameter identification for complex hyperelastic models while ensuring physical validity and handling both homogeneous and inhomogeneous deformation fields.

Method: Combines Virtual Fields Method (VFM) for objective function formulation using DIC displacement fields and load data, with Genetic Algorithm (GA) optimization to navigate complex parameter spaces while assessing material stability.

Result: Outperforms manual fitting, demonstrating robust and efficient parameter identification for complex hyperelastic models using both homogeneous and inhomogeneous deformation data from foam specimens.

Conclusion: The VFM-GA framework provides an effective automated solution for material parameter identification in hyperelastic constitutive models, handling complex parameter spaces while ensuring physical validity.

Abstract: This work introduces a calibration framework for material parameter
identification in isotropic hyperelastic constitutive models. The framework
synergizes the Virtual Fields Method (VFM) to define an objective function with
a Genetic Algorithm (GA) as the optimization method to facilitate automated
calibration. The formulation of the objective function uses experimental
displacement fields measured from Digital Image Correlation (DIC) synchronized
with load cell data and can accommodate data from experiments involving
homogeneous or inhomogeneous deformation fields. The framework places no
restrictions on the target isotropic hyperelastic constitutive model,
accommodating models with coupled dependencies on deformation invariants and
specialized functional forms with a number of material parameters, and assesses
material stability, eliminating sets of material parameters that potentially
lead to non-physical behavior for the target hyperelastic constitutive model.
To minimize the objective function, a GA is deployed as the optimization tool
due to its ability to navigate the intricate landscape of material parameter
space. The VFM-GA framework is evaluated by applying it to a hyperelastic
constitutive model for compressible elastomeric foams. The evaluation process
entails a number of tests that employ both homogeneous and inhomogeneous
displacement fields collected from DIC experiments on open-cell foam specimens.
The results outperform manual fitting, demonstrating the framework's robust and
efficient capability to handle material parameter identification for complex
hyperelastic constitutive models.

</details>


### [35] [Iterated Agent for Symbolic Regression](https://arxiv.org/abs/2510.08317)
*Zhuo-Yang Song,Zeyu Cai,Shutao Zhang,Jiashen Wei,Jichen Pan,Shi Qiu,Qing-Hong Cao,Tie-Jiun Hou,Xiaohui Liu,Ming-xing Luo,Hua Xing Zhu*

Main category: physics.comp-ph

TL;DR: IdeaSearchFitter uses LLMs as semantic operators in evolutionary search for symbolic regression, generating interpretable mathematical expressions guided by natural-language rationales.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of traditional symbolic regression methods that suffer from combinatorial explosion, overfitting, and produce overly complex, uninterpretable models.

Method: Evolutionary search framework with LLMs as semantic operators that generate candidate expressions guided by natural-language rationales, biasing discovery toward conceptually coherent models.

Result: Achieves competitive noise-robust performance on Feynman Symbolic Regression Database, discovers mechanistically aligned models on real-world data with good accuracy-complexity trade-offs, and derives compact physically-motivated parametrizations for high-energy physics applications.

Conclusion: IdeaSearchFitter effectively combines LLMs with evolutionary search to produce accurate, interpretable, and conceptually coherent mathematical models across diverse scientific domains.

Abstract: Symbolic regression (SR), the automated discovery of mathematical expressions
from data, is a cornerstone of scientific inquiry. However, it is often
hindered by the combinatorial explosion of the search space and a tendency to
overfit. Popular methods, rooted in genetic programming, explore this space
syntactically, often yielding overly complex, uninterpretable models. This
paper introduces IdeaSearchFitter, a framework that employs Large Language
Models (LLMs) as semantic operators within an evolutionary search. By
generating candidate expressions guided by natural-language rationales, our
method biases discovery towards models that are not only accurate but also
conceptually coherent and interpretable. We demonstrate IdeaSearchFitter's
efficacy across diverse challenges: it achieves competitive, noise-robust
performance on the Feynman Symbolic Regression Database (FSReD), outperforming
several strong baselines; discovers mechanistically aligned models with good
accuracy-complexity trade-offs on real-world data; and derives compact,
physically-motivated parametrizations for Parton Distribution Functions in a
frontier high-energy physics application. IdeaSearchFitter is a specialized
module within our broader iterated agent framework, IdeaSearch, which is
publicly available at https://www.ideasearch.cn/.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [36] [Does Turbulence at the Correlation Scale Regulate the Statistics of Magnetic Reconnection?](https://arxiv.org/abs/2510.07502)
*M. B. Khan,M. A. Shay,S. Oughton,W. H. Matthaeus,C. C. Haggerty,S. Adhikari,P. A. Cassak,S. Fordin,D. O'Donnell,Y. Yang,R. Bandyopadhyay,S. Roy*

Main category: physics.plasm-ph

TL;DR: Study shows reconnection rates in MHD turbulence are strongly correlated with global turbulent magnetic field magnitude at correlation scale, not dissipation scales, suggesting magnetic reconnection plays major role in energy dissipation.


<details>
  <summary>Details</summary>
Motivation: To understand how magnetic reconnection events behave within strong background MHD turbulence and their relationship to global turbulent field statistics.

Method: Direct numerical simulations of magnetic reconnection events embedded in MHD turbulence, analyzing reconnection rates and their correlation with turbulent magnetic field statistics.

Result: Reconnection rates distribution is strongly correlated with global turbulent magnetic field magnitude at correlation scale, with average rates and dissipation rates much larger than predicted using fluctuation amplitudes at dissipation/kinetic scales.

Conclusion: Magnetic reconnection likely plays a major role in energy dissipation in astrophysical and heliospheric turbulence due to its enhanced rates in turbulent environments.

Abstract: We study the statistics of dynamical quantities associated with magnetic
reconnection events embedded in a sea of strong background magnetohydrodynamic
(MHD) turbulence using direct numerical simulations. We focus on the
relationship of the reconnection properties to the statistics of global
turbulent fields. For the first time, we show that the distribution in
turbulence of reconnection rates (determined by upstream fields) is strongly
correlated with the magnitude of the global turbulent magnetic field at the
correlation scale. The average reconnection rates, and associated dissipation
rates, during turbulence are thus much larger than predicted by using turbulent
magnetic field fluctuation amplitudes at the dissipation or kinetic scales.
Magnetic reconnection may therefore be playing a major role in energy
dissipation in astrophysical and heliospheric turbulence.

</details>


### [37] [Full-wave computation of SUb-atmospheric Radio-frequency Engine (SURE)](https://arxiv.org/abs/2510.07849)
*Dingzhou Li,Lei Chang,Ye Tao*

Main category: physics.plasm-ph

TL;DR: Simulation study of inductively coupled plasma for near-space electric propulsion, comparing single-turn vs five-turn antennas and analyzing effects of gas pressure, input power, frequency, and gas types on power absorption efficiency.


<details>
  <summary>Details</summary>
Motivation: Near-space vehicles need efficient propulsion systems, and previous work proposed an electric propulsion system using plasma thrust, but lacked understanding of how parameters affect power absorption and electromagnetic behavior.

Method: Used computer simulations to examine how gas pressure (200-1000 Pa), input power (200-600 W), frequency (13.56-52.24 MHz), and gas types (Ar, N2, H2, He) influence inductively coupled plasma in a quartz tube, comparing single-turn and five-turn antenna designs.

Result: Single-turn antenna consistently absorbed power better than five-turn antenna. Higher frequencies significantly influence plasma power absorption and magnetic field. Optimal power absorption at 400 Pa gas pressure. Input power showed initial decrease then increasing trend. Molecular gases (N2, H2) had reduced power absorption efficiency due to inelastic collision mechanisms.

Conclusion: The simulation results provide guidance for designing future experiments for this electric propulsion concept, with single-turn antenna and 400 Pa pressure being optimal conditions.

Abstract: Near-space, which covers altitudes from 20 to 100 kilometers, has been
receiving more and more attention because of its special strategic value.
Airships and high-altitude balloons are two common types of low-speed vehicles
that operate in this region. They can be used for jobs like monitoring,
communication, and remote sensing, but they need efficient propulsion systems
to work well. Earlier, we proposed a new type of electric propulsion system
that can ionize the surrounding air to create plasma and produce thrust for
near-space vehicles. However, in past experiments, not enough was known about
how certain parameters affect power absorption and electromagnetic behavior.
Therefore, in this study, we used computer simulations to examine how gas
pressure (200 to 1000 Pa), input power (200 to 600 W), frequency (13.56 to
52.24 MHz), and different gas types ($Ar$, $N_2$, $H_2$, $He$) influence
inductively coupled plasma inside a quartz tube. We especially focused on
comparing two antenna designs: one with a single turn and one with five turns.
In all the simulations, the single-turn antenna consistently absorbed power
better than the five-turns antenna. Higher frequencies significantly influence
both plasma power absorption and magnetic field characteristics. The optimal
power absorption occurs at a filling gas pressure of 400 Pa. When varying the
input power, we observed an initial decrease followed by an increasing trend,
which may be related to ionization mechanisms. In comparisons among different
gas types, the inelastic collision mechanisms in molecular gases lead to a
notable reduction in plasma power absorption efficiency. The results from this
work will help guide the design of future experiments for this electric
propulsion concept.

</details>


### [38] [Ion Stochastic Heating by Low-frequency Alfvén Wave Spectrum](https://arxiv.org/abs/2510.07875)
*Jingyu Peng,Jiansen He*

Main category: physics.plasm-ph

TL;DR: The paper studies nonlinear interactions between oblique Alfvén wave spectra and ions, showing that increasing wave modes leads to chaotic ion motion and stochastic heating, characterized by effective relative curvature radius Peff.


<details>
  <summary>Details</summary>
Motivation: Finite-amplitude low-frequency Alfvén waves are common in space plasmas and play a crucial role in ion heating, motivating the study of their nonlinear interactions with ions.

Method: The study analyzes nonlinear interaction between oblique Alfvén wave spectra and ions using test particle simulations and characterizes stochastic heating threshold with effective relative curvature radius Peff.

Result: Results show excellent agreement between theoretical predictions and chaotic regions from simulations, with stochastic heating rate Q expressed as Q/(Ωi mi vA²) = H(α)ṽ³B̃w²ω̃₁, where parameters depend on wave conditions.

Conclusion: The study successfully characterizes stochastic heating threshold and rate for ions interacting with oblique Alfvén wave spectra, explaining anisotropic heating characteristics through a uniform solid angle distribution model.

Abstract: Finite-amplitude low-frequency Alfv\'en waves are commonly found in plasma
environments, such as space plasmas, and play a crucial role in ion heating.
The nonlinear interaction between oblique Alfv\'en wave spectra and ions has
been studied. As the number of wave modes increases, ions are more likely to
exhibit chaotic motion and experience stochastic heating. The stochastic
heating threshold in the parameter space can be characterized by a single
parameter, the effective relative curvature radius $P_{{eff.}}$. The results
show excellent agreement with the chaotic regions identified through test
particle simulations. The anisotropic characteristics of stochastic heating are
explained using a uniform solid angle distribution model. The stochastic
heating rate $Q=\dot{T}$ is calculated, and its relationship with wave
conditions is expressed as $Q/(\Omega_i m_i v_A^2) = H(\alpha) \tilde{v}^3
\tilde{B}_w^2 \tilde{\omega}_1$, where $\alpha$ is propagating angle,
$\Omega_i$ is the gyrofrequency, $m_i$ is the ion mass, $v_A$ is the Alfv\'en
speed, $\tilde{v}$ is the dimensionless speed, $\tilde{B}_w$ is the
dimensionless wave amplitude, and $\tilde{\omega}_1$ is the lowest
dimensionless wave frequency.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [39] [Sharp Non-uniqueness in Law for Stochastic Differential Equations on the Whole Space](https://arxiv.org/abs/2510.08248)
*Huaxiang Lü,Michael Röckner*

Main category: math.PR

TL;DR: The paper constructs divergence-free drift fields in certain function spaces that lead to non-unique weak solutions for stochastic differential equations, showing this is sharp compared to known uniqueness results.


<details>
  <summary>Details</summary>
Motivation: To investigate when stochastic differential equations have non-unique weak solutions, particularly for drift fields in borderline function spaces where uniqueness transitions to non-uniqueness.

Method: Construct divergence-free drift fields in L_t^rL^p ∩ C_tL^{d-} spaces using convex integration methods adapted to all of R^d (not just the torus), together with refined heat kernel estimates.

Result: For any finite collection of initial measures and drifts satisfying d/p + 1/r > 1, there exist divergence-free drift fields that yield at least two distinct weak solutions from each initial measure. This is sharp compared to uniqueness results for C_tL^{d+} drifts.

Conclusion: The paper establishes a sharp threshold for non-uniqueness in stochastic differential equations with divergence-free drift fields, demonstrating that the condition d/p + 1/r > 1 is critical for transitioning from uniqueness to non-uniqueness of weak solutions.

Abstract: In this paper, we investigate the stochastic differential equation on
$\mathbb{R}^d,d\geq2$: \begin{align*}
  \dif X_t&=v(t,X_t)\dif t+\sqrt{2} \dif W_t. \end{align*} For any finite
collection of initial probability measures $\{\mu^i_0\}_{1\leq i\leq M}$ on
$\mathbb{R}^d$ and $\frac{d}{p}+\frac{1}{r}>1$, we construct a divergence-free
drift field $v\in L_t^rL^p\cap C_tL^{d-}$ such that the associated SDE admits
at least two distinct weak solutions originating from each initial measure
$\mu^i_0$. This result is sharp in view of the well-known uniqueness of strong
solutions for drifts in $C_tL^{d+}$, as established in \cite{KR05}. As a
corollary, there exists a measurable set $A\subset\mathbb{R}^d$ with positive
Lebesgue measure such that for any $x\in A$, the SDE with drift $v$ admits at
least two weak solutions when with start in $x\in A$. The proof proceeds by
constructing two distinct probability solutions to the associated Fokker-Planck
equation via a convex integration method adapted to all of $\mathbb{R}^d$
(instead of merely the torus), together with refined heat kernel estimate.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [40] [Topology optimization of nonlinear forced response curves via reduction on spectral submanifolds](https://arxiv.org/abs/2510.07900)
*Hongming Liang,Matteo Pozzi,Jacopo Marconi,Shobhit Jain,Mingwu Li*

Main category: eess.SY

TL;DR: Using spectral submanifolds (SSMs) reduction theory to enable efficient topology optimization of nonlinear dynamic systems by reformulating periodic responses as reduced-order model equilibria, allowing analytic computation of responses and sensitivities.


<details>
  <summary>Details</summary>
Motivation: Topology optimization has potential for tuning nonlinear dynamic responses but is limited in high-dimensional systems due to high computational costs of repeated response and sensitivity analyses.

Method: Employ SSM reduction theory to create reduced-order models that reformulate periodic responses as equilibria, enabling efficient analytic evaluation of response amplitudes and sensitivities for optimization problems targeting peak amplitude, hardening/softening behavior, and bifurcation distances.

Result: Successfully applied to nonlinear MEMS device design, achieving targeted performance optimization with practical efficiency.

Conclusion: The SSM-based framework provides an efficient strategy for incorporating nonlinear dynamic effects into topology optimization of structures.

Abstract: Forced response curves (FRCs) of nonlinear systems can exhibit complex
behaviors, including hardening/softening behavior and bifurcations. Although
topology optimization holds great potential for tuning these nonlinear dynamic
responses, its use in high-dimensional systems is limited by the high cost of
repeated response and sensitivity analyses. To address this challenge, we
employ the spectral submanifolds (SSMs) reduction theory, which reformulates
the periodic response as the equilibria of an associated reduced-order model
(ROM). This enables efficient and analytic evaluation of both response
amplitudes and their sensitivities. Based on the SSM-based ROM, we formulate
optimization problems that optimize the peak amplitude, the hardening/softening
behavior, and the distance between two saddle-node bifurcations for an FRC. The
proposed method is applied to the design of nonlinear MEMS devices, achieving
targeted performance optimization. This framework provides a practical and
efficient strategy for incorporating nonlinear dynamic effects into the
topology optimization of structures.

</details>


### [41] [A Stable, Accurate and Well-Conditioned Time-Domain PMCHWT Formulation](https://arxiv.org/abs/2510.07989)
*Van Chien Le,Cedric Munger,Francesco P. Andriulli,Kristof Cools*

Main category: eess.SY

TL;DR: A new boundary element method for transient electromagnetic scattering using time-domain PMCHWT equations with Calderon preconditioning and quasi-Helmholtz rescaling to address stability issues.


<details>
  <summary>Details</summary>
Motivation: To develop a stable and accurate boundary element formulation for transient electromagnetic scattering by dielectric objects that overcomes dense-mesh breakdown, large-timestep breakdown, and late-time instability issues.

Method: Uses time-domain PMCHWT equation with multiplicative Calderon preconditioner, quasi-Helmholtz projectors for rescaling Helmholtz components, and temporal differentiation/integration as rescaling operators. Solved with marching-on-in-time scheme and iterative solvers.

Result: Numerical experiments on simply- and multiply-connected dielectric scatterers with non-smooth geometries demonstrate accurate, stable, and efficient performance.

Conclusion: The proposed approach successfully resolves key stability issues in transient electromagnetic scattering simulations while maintaining accuracy and efficiency across various dielectric scatterer geometries.

Abstract: This paper introduces a new boundary element formulation for transient
electromagnetic scattering by homogeneous dielectric objects based on the
time-domain PMCHWT equation. To address dense-mesh breakdown, a multiplicative
Calderon preconditioner utilizing a modified static electric field integral
operator is employed. Large-timestep breakdown and late-time instability are
simultaneously resolved by rescaling the Helmholtz components leveraging the
quasi-Helmholtz projectors and using temporal differentiation and integration
as rescaling operators. This rescaling also balances the loop and star
components at large timesteps, improving solution accuracy. The resulting
discrete system is solved using a marching-on-in-time scheme and iterative
solvers. Numerical experiments for simply- and multiply-connected dielectric
scatterers, including highly non-smooth geometries, corroborate the accuracy,
stability, and efficiency of the proposed approach.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [42] [A Geomechanically-Informed Framework for Wellbore Trajectory Prediction: Integrating First-Principles Kinematics with a Rigorous Derivation of Gated Recurrent Networks](https://arxiv.org/abs/2510.07564)
*Shubham Kumar,Anshuman Sahoo*

Main category: physics.geo-ph

TL;DR: A geomechanically-informed data-driven framework using GRU networks for wellbore trajectory prediction, with mathematical derivations from first principles and validation on Gulfaks oil field data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurate wellbore trajectory prediction by moving beyond empirical modeling to a geomechanically-informed approach that treats petrophysical logs as proxies for rock mechanical properties.

Method: Uses LAS and DEV data from 14 wells; derives wellbore kinematic models from vector calculus and differential geometry; implements GRU network with detailed forward propagation and BPTT training; includes feature normalization, depth resampling, and sequence preprocessing.

Result: Model performance evaluated using MAE, RMSE, and R2 metrics on trajectory prediction accuracy.

Conclusion: The framework provides a mathematically rigorous, geomechanically-informed approach to wellbore trajectory prediction that bridges theoretical principles with practical data-driven modeling.

Abstract: Accurate wellbore trajectory prediction is a paramount challenge in
subsurface engineering, governed by complex interactions between the drilling
assembly and heterogeneous geological formations. This research establishes a
comprehensive, mathematically rigorous framework for trajectory prediction that
moves beyond empirical modeling to a geomechanically-informed, data-driven
surrogate approach.The study leverages Log ASCII Standard (LAS) and wellbore
deviation (DEV) data from 14 wells in the Gulfaks oil field, treating
petrophysical logs not merely as input features, but as proxies for the
mechanical properties of the rock that fundamentally govern drilling dynamics.
A key contribution of this work is the formal derivation of wellbore kinematic
models, including the Average Angle method and Dogleg Severity, from the first
principles of vector calculus and differential geometry, contextualizing them
as robust numerical integration schemes. The core of the predictive model is a
Gated Recurrent Unit (GRU) network, for which we provide a complete,
step-by-step derivation of the forward propagation dynamics and the
Backpropagation Through Time (BPTT) training algorithm. This detailed
theoretical exposition, often omitted in applied studies, clarifies the
mechanisms by which the network learns temporal dependencies. The methodology
encompasses a theoretically justified data preprocessing pipeline, including
feature normalization, uniform depth resampling, and sequence generation.
Trajectory post-processing and error analysis are conducted using Mean Absolute
Error (MAE), Root Mean Square Error (RMSE), and the Coefficient of
Determination (R2).

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [43] [Quantum Grid Path Planning Using Parallel QAOA Circuits Based on Minimum Energy Principle](https://arxiv.org/abs/2510.07413)
*Jun Liu*

Main category: quant-ph

TL;DR: This paper proposes a quantum path planning solution using parallel QAOA architecture to overcome classical NP problems and NISQ-era limitations, mapping grid path planning to finding minimum quantum energy states.


<details>
  <summary>Details</summary>
Motivation: To address bottlenecks in classical path planning for NP problems and overcome limitations of current quantum path planning frameworks in the NISQ era.

Method: Map grid path planning to minimum quantum energy state problem; build two parallel QAOA circuits for connectivity energy and path energy calculations; use classical algorithm to filter unreasonable solutions; merge results from parallel circuits.

Result: With appropriate filter parameters, quantum states with low occurrence probabilities can be filtered out, increasing target state probability. Even with only 1 circuit layer, optimal path coding can be found using filter's critical role.

Conclusion: Parallel circuits show significant advantage over serial circuits, finding optimal feasible path coding combinations with highest probability.

Abstract: To overcome the bottleneck of classical path planning schemes in solving NP
problems and address the predicament faced by current mainstream quantum path
planning frameworks in the Noisy Intermediate-Scale Quantum (NISQ) era, this
study attempts to construct a quantum path planning solution based on parallel
Quantum Approximate Optimization Algorithm (QAOA) architecture. Specifically,
the grid path planning problem is mapped to the problem of finding the minimum
quantum energy state. Two parallel QAOA circuits are built to simultaneously
execute two solution processes, namely connectivity energy calculation and path
energy calculation. A classical algorithm is employed to filter out
unreasonable solutions of connectivity energy, and finally, the approximate
optimal solution to the path planning problem is obtained by merging the
calculation results of the two parallel circuits. The research findings
indicate that by setting appropriate filter parameters, quantum states
corresponding to position points with extremely low occurrence probabilities
can be effectively filtered out, thereby increasing the probability of
obtaining the target quantum state. Even when the circuit layer number p is
only 1, the theoretical solution of the optimal path coding combination can
still be found by leveraging the critical role of the filter. Compared with
serial circuits, parallel circuits exhibit a significant advantage, as they can
find the optimal feasible path coding combination with the highest probability.

</details>


### [44] [When Less is More: Approximating the Quantum Geometric Tensor with Block Structures](https://arxiv.org/abs/2510.08430)
*Ahmedeo Shokry,Alessandro Santini,Filippo Vicentini*

Main category: quant-ph

TL;DR: A block-diagonal quantum geometric tensor is introduced to reduce computational cost in neural quantum states optimization by partitioning the metric by network layers, similar to K-FAC methods.


<details>
  <summary>Details</summary>
Motivation: The natural gradient method in neural quantum states is limited by the high computational cost of computing and inverting the quantum geometric tensor (quantum Fisher information matrix).

Method: Introduce a block-diagonal quantum geometric tensor that partitions the metric by network layers, removing cross-layer correlations while preserving essential curvature information.

Result: Experiments on Heisenberg and frustrated J1-J2 models show faster convergence, lower energy, and improved stability compared to standard methods.

Conclusion: The layer-wise approximation improves conditioning and scalability of quantum geometric tensor computations while maintaining optimization performance.

Abstract: The natural gradient is central in neural quantum states optimizations but it
is limited by the cost of computing and inverting the quantum geometric tensor,
the quantum analogue of the Fisher information matrix. We introduce a
block-diagonal quantum geometric tensor that partitions the metric by network
layers, analogous to block-structured Fisher methods such as K-FAC. This
layer-wise approximation preserves essential curvature while removing noisy
cross-layer correlations, improving conditioning and scalability. Experiments
on Heisenberg and frustrated $J_1$-$J_2$ models show faster convergence, lower
energy, and improved stability.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [45] [Non-Hermitian many-body localization in asymmetric chains with long-range interaction](https://arxiv.org/abs/2510.08277)
*Wen Wang,Han-Ze Li,Jian-Xin Zhong*

Main category: cond-mat.dis-nn

TL;DR: Discovery of coexisting static/dynamic spectral real-complex phase transitions and many-body ergodic-localized phase transitions in clean, long-range interaction-induced non-Hermitian many-body localization systems.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between many-body localization and spectra in non-Hermitian many-body systems, particularly in clean systems with long-range interactions.

Method: Study of a one-dimensional clean, long-range interaction-induced non-Hermitian many-body localization system, with proposed experimental realization in cold-atom systems.

Result: Found coexisting static and dynamic spectral real-complex phase transitions alongside many-body ergodic-localized phase transitions. Phase diagrams show similar non-monotonic boundary trends but do not overlap, revealing distinct properties from conventional disorder-induced non-Hermitian many-body localization.

Conclusion: The findings provide valuable insights for understanding the relationship between non-Hermitian many-body localization and non-Hermitian spectra in long-range interacting systems.

Abstract: Understanding the relationship between many-body localization and spectra in
non-Hermitian many-body systems is crucial. In a one-dimensional clean,
long-range interaction-induced non-Hermitian many-body localization system, we
have discovered the coexistence of static and dynamic spectral real-complex
phase transitions, along with many-body ergodic-localized phase transitions.
The phase diagrams of these two types of transitions show similar non-monotonic
boundary trends but do not overlap, highlighting properties distinct from
conventional disorder-induced non-Hermitian many-body localization. We also
propose a potential experimental realization of this model in cold-atom
systems. Our findings provide valuable insights for further understanding the
relationship between non-Hermitian many-body localization and non-Hermitian
spectra in long-range interacting systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [Targeted Digital Twin via Flow Map Learning and Its Application to Fluid Dynamics](https://arxiv.org/abs/2510.07549)
*Qifan Chen,Zhongshu Xu,Jinjin Zhang,Dongbin Xiu*

Main category: cs.LG

TL;DR: A framework for creating targeted digital twins that directly model quantities of interest using memory-based flow map learning from short trajectory data bursts, enabling efficient long-term predictions without full system simulations.


<details>
  <summary>Details</summary>
Motivation: To achieve computational efficiency by modeling only the relevant quantities of interest rather than running expensive full digital twin simulations, particularly for long-term dynamic analysis.

Method: Memory-based flow map learning (FML) using short bursts of trajectory data from repeated full DT executions to construct data-driven models of quantities of interest.

Result: Successfully demonstrated in a CFD example of cylinder flow, where tDTs accurately predicted hydrodynamic forces long-term while bypassing full flow simulations.

Conclusion: The proposed tDT framework enables substantial computational savings through offline construction and efficient online prediction of quantities of interest without full system simulations.

Abstract: We present a numerical framework for constructing a targeted digital twin
(tDT) that directly models the dynamics of quantities of interest (QoIs) in a
full digital twin (DT). The proposed approach employs memory-based flow map
learning (FML) to develop a data-driven model of the QoIs using short bursts of
trajectory data generated through repeated executions of the full DT. This
renders the construction of the FML-based tDT an entirely offline computational
process. During online simulation, the learned tDT can efficiently predict and
analyze the long-term dynamics of the QoIs without requiring simulations of the
full DT system, thereby achieving substantial computational savings. After
introducing the general numerical procedure, we demonstrate the construction
and predictive capability of the tDT in a computational fluid dynamics (CFD)
example: two-dimensional incompressible flow past a cylinder. The QoIs in this
problem are the hydrodynamic forces exerted on the cylinder. The resulting tDTs
are compact dynamical systems that evolve these forces without explicit
knowledge of the underlying flow field. Numerical results show that the tDTs
yield accurate long-term predictions of the forces while entirely bypassing
full flow simulations.

</details>


### [47] [Unveiling the Power of Multiple Gossip Steps: A Stability-Based Generalization Analysis in Decentralized Training](https://arxiv.org/abs/2510.07980)
*Qinglun Li,Yingqi Liu,Miao Zhang,Xiaochun Cao,Quanjun Yin,Li Shen*

Main category: cs.LG

TL;DR: Multi-Gossip Steps (MGS) bridges decentralized and centralized training, reducing performance gaps but cannot fully eliminate them due to fundamental theoretical limitations.


<details>
  <summary>Details</summary>
Motivation: To understand why MGS improves decentralized training performance and whether it can fully close the gap with centralized training, addressing open theoretical questions.

Method: Used stability analysis to derive upper bounds on generalization error and excess error of MGS, analyzing factors like learning rate, data heterogeneity, node count, and communication topology.

Result: MGS reduces optimization error exponentially and tightens generalization bounds, but a non-negligible gap remains compared to centralized training even with infinite MGS steps.

Conclusion: MGS significantly improves decentralized training but cannot completely eliminate the performance gap with centralized methods due to inherent theoretical limitations, as validated by experiments on CIFAR datasets.

Abstract: Decentralized training removes the centralized server, making it a
communication-efficient approach that can significantly improve training
efficiency, but it often suffers from degraded performance compared to
centralized training. Multi-Gossip Steps (MGS) serve as a simple yet effective
bridge between decentralized and centralized training, significantly reducing
experiment performance gaps. However, the theoretical reasons for its
effectiveness and whether this gap can be fully eliminated by MGS remain open
questions. In this paper, we derive upper bounds on the generalization error
and excess error of MGS using stability analysis, systematically answering
these two key questions. 1). Optimization Error Reduction: MGS reduces the
optimization error bound at an exponential rate, thereby exponentially
tightening the generalization error bound and enabling convergence to better
solutions. 2). Gap to Centralization: Even as MGS approaches infinity, a
non-negligible gap in generalization error remains compared to centralized
mini-batch SGD ($\mathcal{O}(T^{\frac{c\beta}{c\beta +1}}/{n m})$ in
centralized and $\mathcal{O}(T^{\frac{2c\beta}{2c\beta +2}}/{n
m^{\frac{1}{2c\beta +2}}})$ in decentralized). Furthermore, we provide the
first unified analysis of how factors like learning rate, data heterogeneity,
node count, per-node sample size, and communication topology impact the
generalization of MGS under non-convex settings without the bounded gradients
assumption, filling a critical theoretical gap in decentralized training.
Finally, promising experiments on CIFAR datasets support our theoretical
findings.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [48] [Time-causal and time-recursive wavelets](https://arxiv.org/abs/2510.05834)
*Tony Lindeberg*

Main category: eess.SP

TL;DR: This paper presents a time-causal wavelet analysis method based on temporal scale-space theory, using truncated exponential kernels and their derivatives to enable real-time processing without accessing future data.


<details>
  <summary>Details</summary>
Motivation: To enable wavelet analysis for real-time temporal signals where future data cannot be accessed, requiring truly time-causal computational mechanisms throughout the signal processing pipeline.

Method: Based on temporal scale-space theory, using convolution with truncated exponential kernels in cascade as the only permissible class of kernels, along with their temporal derivatives to satisfy wavelet admissibility conditions. A specific way of choosing time constants ensures temporal scale covariance and self-similarity.

Result: Developed connections between wavelet theory and scale-space theory, characterizing how continuous scaling properties transfer to discrete implementation. The method can reflect the duration of locally dominant temporal structures in input signals.

Conclusion: The proposed time-causal wavelet analysis is a valuable tool for real-time signal processing tasks, especially for signals with local variations over multiple temporal scales, and for analyzing physical or biophysical phenomena requiring physically realistic time-causal analysis.

Abstract: When to apply wavelet analysis to real-time temporal signals, where the
future cannot be accessed, it is essential to base all the steps in the signal
processing pipeline on computational mechanisms that are truly time-causal.
  This paper describes how a time-causal wavelet analysis can be performed
based on concepts developed in the area of temporal scale-space theory,
originating from a complete classification of temporal smoothing kernels that
guarantee non-creation of new structures from finer to coarser temporal scale
levels. By necessity, convolution with truncated exponential kernels in cascade
constitutes the only permissable class of kernels, as well as their temporal
derivatives as a natural complement to fulfil the admissibility conditions of
wavelet representations. For a particular way of choosing the time constants in
the resulting infinite convolution of truncated exponential kernels, to ensure
temporal scale covariance and thus self-similarity over temporal scales, we
describe how mother wavelets can be chosen as temporal derivatives of the
resulting time-causal limit kernel.
  By developing connections between wavelet theory and scale-space theory, we
characterize and quantify how the continuous scaling properties transfer to the
discrete implementation, demonstrating how the proposed time-causal wavelet
representation can reflect the duration of locally dominant temporal structures
in the input signals.
  We propose that this notion of time-causal wavelet analysis could be a
valuable tool for signal processing tasks, where streams of signals are to be
processed in real time, specifically for signals that may contain local
variations over a rich span of temporal scales, or more generally for analysing
physical or biophysical temporal phenomena, where a fully time-causal analysis
is called for to be physically realistic.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [49] [Asymptotically flat black holes with a singular Cauchy horizon and a spacelike singularity](https://arxiv.org/abs/2510.07431)
*Maxime Van de Moortel*

Main category: gr-qc

TL;DR: Construction of asymptotically flat black hole spacetimes with coexisting null and spacelike singularities using new gluing methods for Einstein-Maxwell-scalar-field equations.


<details>
  <summary>Details</summary>
Motivation: To provide concrete examples of black holes with coexisting null and spacelike singularities, particularly significant for modeling gravitational collapse where this phenomenon is conjectured to be generic.

Method: New spacelike-characteristic gluing method between uncharged spherically symmetric solutions and charged dynamical black hole event horizons; construction of one-ended and two-ended asymptotically flat black hole spacetimes.

Result: First examples of black holes with coexisting null and spacelike singularities; terminal boundary in black hole interior has two components: weakly singular null Cauchy horizon and strong spacelike singularity.

Conclusion: Successfully constructed black hole spacetimes exhibiting coexisting null and spacelike singularities, providing important models for gravitational collapse where this phenomenon is conjectured to be generic.

Abstract: In our recent work [Van de Moortel, The coexistence of null and spacelike
singularities inside spherically symmetric black holes], we analyzed the
transition between null and spacelike singularities in spherically symmetric
dynamical black holes and demonstrated that the spacelike portion is described
by a Kasner metric with positive varying exponents that degenerate to $(1,0,0)$
near the null-spacelike transition. In the present paper, we provide examples
of global spacetimes satisfying the assumptions of this previous result and
apply its analysis to obtain a large class of asymptotically flat (spherically
symmetric) black hole spacetimes that exhibit coexisting null and spacelike
singularities. Our main results include:
  _The construction of one-ended asymptotically flat black hole spacetimes
solving the Einstein-Maxwell-charged-scalar-field equations. The proof relies
on a new spacelike-characteristic gluing method between any uncharged
spherically symmetric solution and the event horizon of a charged dynamical
black hole.
  _The construction of a large class of two-ended asymptotically flat black
hole spacetimes solving the Einstein-Maxwell-(uncharged)-scalar-field
equations.
  In both cases, we show that the terminal boundary in the black hole interior
only has two distinct components: a weakly singular (null) Cauchy horizon
$\mathcal{CH}_{i^+}$ where curvature blows up and a strong singularity
$\mathcal{S}=\{r=0\}$.
  Our construction provides the first examples of black holes with coexisting
null and spacelike singularities. These examples hold particular significance
in the one-ended case as a model of gravitational collapse, where this
phenomenon is conjecturally generic for the Einstein-scalar-field model, even
beyond spherical symmetry.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [50] [Stress concentration via quasi-Minnaert resonance in bubble-elastic structures and applications](https://arxiv.org/abs/2510.06892)
*Ruixiang Tang,Huaian Diao,Hongyu Liu,Weisheng Zhou*

Main category: math-ph

TL;DR: This paper analyzes stress concentration in bubbly-elastic structures using quasi-Minnaert resonance, which creates boundary localization and high-oscillation wave patterns near bubble boundaries to enhance stress concentration for engineering and medical applications.


<details>
  <summary>Details</summary>
Motivation: Stress concentration in bubble-elastic scattering has important applications in engineering blasting and medical treatments, but the complex interaction between acoustic and elastic waves across bubble boundaries presents significant analytical challenges.

Method: The study employs layer potential theory and asymptotic analysis to rigorously establish stress concentration and quasi-Minnaert resonance phenomena in a radially symmetric bubble-elastic model. Extensive numerical experiments are conducted for various bubble geometries including unit disk, corner domain, apple-shaped domain in R², and ball in R³.

Result: The research demonstrates how to leverage quasi-Minnaert resonance to induce stress concentration in the elastic total wave field near air bubble boundaries by appropriately selecting incident elastic waves and high-contrast structures. The resonance manifests as two distinct wave patterns: boundary localization and high-oscillation phenomena.

Conclusion: The findings enhance understanding of stress concentration mechanisms and their applications in engineering blasting and medical therapies, providing a comprehensive mathematical framework for analyzing stress concentration in bubbly-elastic structures induced by quasi-Minnaert resonance.

Abstract: Stress concentration in bubble-elastic scattering scenarios has significant
applications in engineering blasting and medical treatments. This study
provides a comprehensive mathematical analysis of stress concentration in
bubbly-elastic structures, induced by the quasi-Minnaert resonance. The
quasi-Minnaert resonance manifests as two distinct wave patterns near the
bubble's boundary: boundary localization and high-oscillation phenomena. We
demonstrate how to leverage the quasi-Minnaert resonance to induce stress
concentration in the elastic total wave field near the air bubble's boundary by
appropriately selecting the incident elastic wave and high-contrast structure.
The interaction between the air bubble and the elastic background couples two
physical wave fields-acoustic and elastic waves-across the bubble's boundary.
The intricate transmission conditions, combined with the scalar nature of
acoustic waves and the vectorial nature of elastic waves, present significant
analytical challenges. To address these, we employ layer potential theory and
asymptotic analysis to rigorously establish the stress concentration and
quasi-Minnaert resonance phenomena in a radially geometry bubble-elastic model.
Extensive numerical experiments are conducted to demonstrate the stress
concentration phenomenon alongside quasi-Minnaert resonance for various bubble
geometries, including a unit disk, a corner domain, an apple-shaped domain in
$\mathbb{R}^2$, and a ball in $\mathbb{R}^3$. The findings of this study
enhance the understanding of stress concentration mechanisms and their
applications in engineering blasting and medical therapies.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [51] [Minimal Denominators Lying in Subsets of the Ring of Polynomials over a Finite Field](https://arxiv.org/abs/2510.07787)
*Noy Soffer Aranov*

Main category: math.NT

TL;DR: The paper proves that for polynomials over finite fields, the distribution functions for smallest denominator approximations in both continuous and discrete settings are exactly equal, which is stronger than the analogous real number case.


<details>
  <summary>Details</summary>
Motivation: To study the distribution of smallest denominators in rational approximations over finite fields and compare with the real number setting where only asymptotic closeness was known.

Method: Analyzes the distribution of smallest denominators Q in subsets of F_q[x] for both continuous approximations (P/Q in balls) and discrete approximations (with fixed denominator degree), proving equality of their probability distribution functions.

Result: For any infinite subset S ⊆ F_q[x], any degree n, and any dimension m, the probability distribution functions for both continuous and discrete smallest denominator approximations are exactly equal.

Conclusion: The finite field setting yields stronger results than the real number case, with exact equality rather than just asymptotic closeness of the distribution functions.

Abstract: Given a subset $\mathcal{S}\subseteq \mathbb{F}_q[x]$ and fixed $n,m\in
\mathbb{N}$, one can study the distribution of the value of the smallest
denominator $Q\in \mathcal{S}$, for which there exists $\mathbf{P}\in
\mathbb{F}_q[x]^m$ such that $\frac{P}{Q}\in B(\boldsymbol{\alpha},q^{-n})$,
where $Q\in \mathcal{S}$. On the other hand, one can study the discrete
analogue, when $N\in \mathbb{F}_q[x]$ is a polynomial with $\deg(N)=n$ and
$\boldsymbol{\alpha}\in \frac{1}{N}\mathbb{F}_q[x]^m$ as a discrete probability
distribution function. We prove that for any infinite subset
$\mathcal{S}\subseteq \mathbb{F}_q[x]$, for any $n\in \mathbb{N}$, and for any
dimension $m$, the probability distribution functions of both these random
variables are equal to one another. This is significantly stronger than the
real setting, where Balazard and Martin proved that these functions have
asymptotically close averages, when there are no restrictions on the
denominators.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [52] [Scalings and simulation requirements in two-phase flows](https://arxiv.org/abs/2510.07727)
*Luis H. Hatashita,Pranav Nathan,Suhas S. Jain*

Main category: physics.flu-dyn

TL;DR: Derived grid-point and time-step scalings for two-phase flow DNS based on Re, We, and Ca numbers, with computational cost estimates of We^{12/5} and Ca^4 for different regimes.


<details>
  <summary>Details</summary>
Motivation: Lack of DNS guidelines for two-phase flows compared to single-phase turbulence, requiring systematic quantification of resolution requirements.

Method: High-fidelity simulations of stationary two-phase homogeneous isotropic turbulence, evaluating convergence of interfacial area, size distribution, SMD, and curvature distribution.

Result: Established length scale ratios: η_KH/η ~ We_L^{-3/5}Re_L^{3/4} (inertia-dominated) and η_KV/η ~ Ca_L^{-1}Re_L^{3/4} (viscous-dominated). Minimum grid resolution k_max η_KH ≥ 60 for second-order schemes.

Conclusion: Provides valuable guidelines for grid resolution and time-step requirements in two-phase flow DNS, accelerating physics discovery and model development.

Abstract: In this work, important two-phase flow scalings are derived, which enable the
quantification of grid-point and time-step requirements as functions of Re, We,
and Ca numbers. The adequate grid resolution is determined in the
inertia-dominated regime with the aid of high-fidelity simulations of
stationary two-phase homogeneous isotropic turbulence by evaluating convergence
of total interfacial area, size distribution, SMD, and curvature distribution.
Although standards for DNS for single-phase turbulence flow exist, there is a
lack of similar guidance in two-phase flows. Therefore, length scale ratios of
the Kolmogorov-Hinze to the Kolmogorov scale of \eta_{KH}/\eta \sim
We_{L}^{-3/5}Re_{L}^{3/4} in the inertia-dominated regime and the
Kolmogorov-viscous to Kolmogorov scale of \eta_{KV}/\eta \sim
Ca_{L}^{-1}Re_{L}^{3/4} for the viscous-dominated regime, are constructed.
These scalings imply a computational cost increase like We_{L}^{12/5} and
Ca_{L}^4, in the inertia-dominated and viscous-dominated regimes, respectively.
A novel dimensionless number, coined as the ratio of interface scales (Ris), is
proposed to aid in the classification of the turbulence regimes in the presence
of an interface. Convergence of the total interfacial area, size distribution,
SMD, and curvature distribution are observed for grid resolutions of k_{\max}
\eta_{KH} \geq 60$ for second-order schemes. Furthermore, it is observed that
this lower bound is the minimum required to capture intermittent events
responsible for the increase of instantaneous total interfacial area. This
criterion will be a valuable tool for determining grid resolution and time-step
requirements a-priori for DNS of two-phase flows and for estimating the
corresponding computational cost. This work provides guidelines and best
practices for numerical simulations of two-phase flows, which will accelerate
physics discovery and model development.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [53] [Dimension- and Facet-Dependent Altermagnetic Triferroics and Biferroics in CrSb](https://arxiv.org/abs/2510.07771)
*Long Zhang,Guoying Gao*

Main category: cond-mat.mtrl-sci

TL;DR: This paper investigates CrSb as a model system for altermagnetic multiferroics, predicting various ferroic properties across different phases and facets, including the discovery of triferroics in WZ-phase (110) facets with reversible spin splitting.


<details>
  <summary>Details</summary>
Motivation: Altermagnets are gaining interest for their unique spin properties, but altermagnetic multiferroics, especially triferroics, remain rare. The study aims to explore how dimensionality and facet orientation affect ferroic properties in CrSb.

Method: Using first-principles calculations, the researchers analyzed CrSb in NiAs, MnP, wurtzite, zincblende, and rocksalt phases, examining bulk and various facet orientations to determine ferroic properties.

Result: Predicted altermagnetism in MnP-phase CrSb, identified AM-ferroelastic biferroics in NiAs- and MnP-phase (110) facets, and discovered FM/AM-ferroelectric-ferroelastic triferroics in WZ-phase (110) facets with moderate switching barriers. Both FE and FC switching can reverse AM spin splitting while maintaining high spin polarization.

Conclusion: The work establishes a framework for designing altermagnetic multiferroics through polymorphic, dimensional, and facet engineering, offering promising opportunities for multifunctional spintronic applications.

Abstract: Altermagnets have recently garnered significant interest due to their
vanishing net magnetic moment and non-relativistic momentum-dependent spin
splitting. However, altermagnetic (AM) multiferroics especially triferroics
remain scarce. We investigate the experimentally synthesized non-van der Waals
CrSb as a model system to explore the effects of dimensionality and facet
orientation on its ferroic properties. NiAs, MnP, wurtzite (WZ), zincblende
(ZB), and rocksalt (RS) phases are considered. Using first-principles
calculations, we predict the altermagnetism of CrSb in MnP phase which has
comparable stability with experimental NiAs phase. Both NiAs- and MnP-phase
(110) facets exhibit AM-ferroelastic (FC) biferroics, while the WZ-phase bulk
and (001) facets host ferromagnetic (FM) or AM-ferroelectric (FE) biferroics.
Notably, the WZ-phase (110) facets are identified as FM/AM-FE-FC triferroics,
with moderate energy barriers of 0.129 and 0.363 eV atom-1 for FE and FC
switching, respectively. Both FE and FC switching can reverse the AM spin
splitting in antiferromagnetic (AFM) configurations while preserving the high
spin polarization in FM states. The magnetic anisotropy is highly tunable,
exhibiting either uniaxial or in-plane behavior depending on the phase,
dimension, and facet. This work establishes a framework for designing AM
multiferroics through polymorphic, dimensional, and facet engineering, offering
promising avenues for multifunctional spintronic applications.

</details>


### [54] [Modulating thermal conductivity of bulk BAs based on targeted phonon excitation](https://arxiv.org/abs/2510.07934)
*Tianhao Li,Yangjun Qin,Dongkai Pan,Han Meng,Nuo Yang*

Main category: cond-mat.mtrl-sci

TL;DR: Reversible phonon excitation strategy to dynamically modulate thermal conductivity of boron arsenide (BAs) using selective excitation of specific phonon modes, enabling both enhancement and suppression of thermal conductivity.


<details>
  <summary>Details</summary>
Motivation: Address opposing thermal conductivity requirements in electronics (high conductivity needed) and thermoelectrics (low conductivity preferred) by developing dynamic control over thermal transport properties.

Method: First-principles calculations and Boltzmann transport equation to study selective excitation of specific phonon modes in BAs, analyzing effects at different excitation multipliers (5 and 25).

Result: At excitation multiplier of 25: thermal conductivity enhanced by 2% or suppressed by 35% relative to intrinsic value of 2235 W m^-1 K^-1. At multiplier of 5: increased by 2% or decreased by 11%. Modulation depends on excitation frequency, multiplier, and phonon properties.

Conclusion: The approach provides dynamic and reversible thermal conductivity tuning through phonon excitation, with applications in thermal management and thermoelectric energy conversion, where different mechanisms dominate at low vs high excitation levels.

Abstract: This study proposes a reversible phonon excitation strategy to dynamically
modulate the thermal conductivity of boron arsenide (BAs), addressing the
opposing thermal conductivity requirements in electronics and thermoelectrics.
Using first-principles calculations and Boltzmann transport equation, we
demonstrate that selective excitation of specific phonon modes enables active
control over thermal transport. At an excitation multiplier of 25, the thermal
conductivity of BAs can be enhanced by up to 2% or suppressed by up to 35%
relative to its intrinsic value of 2235 W m^-1 K^-1. At a lower multiplier of
5, thermal conductivity can be increased by 2% or decreased by 11%. The
modulation effect depends on excitation frequency, multiplier, and intrinsic
phonon properties, with certain frequencies exhibiting opposite trends under
different excitation intensities. Mechanistic analysis shows that at low
excitation levels, phonon splitting suppresses Umklapp scattering, reducing the
scattering rate, while at high levels, it enhances Normal scattering,
increasing the scattering rate. This approach offers a dynamic and reversible
route to tuning thermal conductivity, with applications in thermal management
and thermoelectric energy conversion.

</details>


### [55] [Rare-Earth Engineering of NaAlO3 Perovskites Unlocks Unified Optoelectronic, Thermoelectric, and Spintronic Functionalities](https://arxiv.org/abs/2510.08130)
*Muhammad Imran,Sikander Azam,Qaiser Rafiq,Amin Ur Rahman*

Main category: cond-mat.mtrl-sci

TL;DR: Rare-earth doping (Eu3+, Gd3+, Tb3+) in NaAlO3 perovskite reduces band gap from 6.2 eV to ~3.1 eV, enables visible-light absorption, and enhances thermoelectric performance with potential for multifunctional applications.


<details>
  <summary>Details</summary>
Motivation: Wide-gap perovskite oxides like NaAlO3 have limitations in deep-UV absorption and carrier transport, motivating the exploration of rare-earth doping to improve their electronic and optical properties for energy and quantum technologies.

Method: First-principles GGA+U+SOC calculations were used to investigate Eu3+, Gd3+, and Tb3+ doped NaAlO3, evaluating electronic, optical, elastic, and thermoelectric properties.

Result: Rare-earth substitution is thermodynamically favorable (1.2-1.6 eV formation energies), reduces band gap to ~3.1 eV, induces strong f-p hybridization, creates spin-dependent electronic behaviors (half-metallicity with Gd, spin-selective metallicity with Eu, p-type semiconducting with Tb), and enhances thermoelectric performance (Seebeck >210 uV/K, ZT~0.45 at 500K).

Conclusion: Rare-earth-doped NaAlO3 serves as a multifunctional perovskite platform suitable for photovoltaics, photocatalysis, thermoelectrics, and spintronics applications due to its improved electronic, optical, and thermoelectric properties.

Abstract: Perovskite oxides are promising for energy and quantum technologies, but
wide-gap hosts such as NaAlO3 suffer from deep-UV absorption and limited
carrier transport. Using first-principles GGA+U+SOC calculations, we
investigate Eu3+-, Gd3+-, and Tb3+-doped NaAlO3 and evaluate their electronic,
optical, elastic, and thermoelectric properties. Rare-earth substitution is
thermodynamically favorable (formation energies 1.2-1.6 eV) and induces strong
f-p hybridization, reducing the pristine band gap (about 6.2 eV) to about 3.1
eV for Tb. Spin-resolved band structures reveal Gd-driven half-metallicity,
Eu-induced spin-selective metallicity, and Tb-stabilized p-type semiconducting
behavior. The optical spectra show a red-shifted absorption edge (about 2.0-2.2
eV), a large static dielectric response (epsilon1(0) about 95 for Eu), and
plasmonic resonances near 4 eV, enabling visible-light harvesting. Elastic
analysis indicates mild lattice softening with preserved ductility (Pugh ratio
B/G about 1.56-1.57). Thermoelectric performance is enhanced, with Seebeck
coefficients greater than 210 uV/K for Eu and Tb and ZT about 0.45 at 500 K.
These results identify rare-earth-doped NaAlO3 as a multifunctional perovskite
platform for photovoltaics, photocatalysis, thermoelectrics, and spintronics.

</details>
