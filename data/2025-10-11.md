<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 17]
- [math.AP](#math.AP) [Total: 16]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [math.PR](#math.PR) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [eess.SY](#eess.SY) [Total: 2]
- [math-ph](#math-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [math.NT](#math.NT) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Determination of Range Conditions for General Projection Pair Operators](https://arxiv.org/abs/2510.07480)
*Richard Huber,Rolf Clackdoyle,Laurent Desbat*

Main category: math.NA

TL;DR: This paper analyzes the range of projection pair operators (two projections) in 2D tomography, finding that regular range conditions are limited to at most one dimension and developing explicit kernel conditions to characterize the range.


<details>
  <summary>Details</summary>
Motivation: To understand the mathematical properties of N-projection operators in tomography, which has practical applications in medical imaging calibration, motion detection, and parameter identification.

Method: Investigates projection pair operators (N=2) in the plane, analyzing their range and developing explicit kernel conditions to characterize the annihilators of the range space.

Result: Found that the set of regular annihilators to the range of projection pair operators contains at most one dimension. Applied this to the exponential fanbeam transform, discovering no regular range conditions exist, meaning arbitrary data can be approximated by projections of smooth functions.

Conclusion: The theory provides a framework for analyzing range conditions in tomography, with practical applications demonstrated on mixed parallel-fanbeam projection pairs, showing limited constraints on achievable projection data.

Abstract: Tomographic techniques are vital in modern medicine, allowing doctors to
observe patients' interior features. Individual steps in the measurement
process are modeled by `single projection operators' $p$. These are line
integral operators over a collection of curves that covers the regions of
interest. Then, the entire measurement process can be understood as a finite
collection of such single projections, and thus be modeled by an
$N$-projections operator $P=(p_1,\dots,p_N)$. The most well-known example of an
$N$-projections operator is the restriction of the Radon transform to finitely
many projection angles. Characterizations of the range of $N$-projections
operators are of intrinsic mathematical interest and can also help in practical
applications such as geometric calibration, motion detection, or model
parameter identification. In this work, we investigate the range of projection
pair operators $\mathcal{P}$ in the plane, i.e., operators formed by two
projections ($N=2$) applied to functions in $\mathbb{R}^2$. We find that the
set of annihilators to $\mathrm{rg}(\mathcal{P})$ that are regular
distributions contains at most one dimension and a range condition can be
explicitly determined by what we refer to as `kernel conditions'. With this
tool, we examine the exponential fanbeam transform for which no range
conditions were known, finding that no (regular) range condition exists, and
therefore, arbitrary data can be approximated in an $L^2$ sense by projections
of smooth functions. We also illustrate the use of this theory on a mixed
parallel-fanbeam projection pair operator.

</details>


### [2] [A control-based spatial source reconstruction in fractional heat equations](https://arxiv.org/abs/2510.07528)
*Galina García,Joaquín Vidal,Sebastián Zamorano*

Main category: math.NA

TL;DR: This paper presents a method to reconstruct the spatial component of a source term in a fractional heat equation using partial observations of the system state and its time derivative.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse source problem for nonlocal heat equations with fractional Laplacian, enabling source reconstruction from limited measurement data.

Method: Derives a reconstruction formula for Fourier coefficients using null controllability of fractional heat equation (s∈(1/2,1)), spectral analysis, and Volterra integral equations.

Result: Numerical experiments demonstrate the accuracy and stability of the proposed reconstruction approach.

Conclusion: The method provides a robust framework for recovering spatial sources in fractional heat equations under limited measurement conditions.

Abstract: This article addresses the inverse source problem for a nonlocal heat
equation involving the fractional Laplacian. The primary goal is to reconstruct
the spatial component of the source term from partial observations of the
system's state and its time derivative over a subset of the domain. A
reconstruction formula for the Fourier coefficients of the unknown source is
derived, leveraging the null controllability property of the fractional heat
equation when the fractional order lies in the interval $s\in(1/2,1)$. The
methodology builds on spectral analysis and Volterra integral equations,
providing a robust framework for recovering spatial sources under limited
measurement data. Numerical experiments confirm the accuracy and stability of
the proposed approach.

</details>


### [3] [Semi-implicit strategies for the Serre-Green-Naghdi equations in hyperbolic form. Is hyperbolic relaxation really a good idea?](https://arxiv.org/abs/2510.07539)
*Emanuele Macca,Walter Boscheri,Mario Ricchiuto*

Main category: math.NA

TL;DR: A semi-implicit integration strategy for hyperbolic Serre-Green-Naghdi (hSGN) equations that treats stiff acoustic terms implicitly while keeping advective components explicit, overcoming stability constraints of explicit schemes.


<details>
  <summary>Details</summary>
Motivation: The elliptic formulation of SGN equations increases computational cost compared to Saint-Venant equations. While hyperbolic models (hSGN) help, explicit schemes face restrictive stability constraints as relaxation parameter increases.

Method: Semi-implicit (SI) integration using IMEX Runge-Kutta framework: stiff acoustic terms treated implicitly, advective components remain explicit.

Result: The approach mitigates CFL stability restriction, maintains dispersive accuracy at moderate computational cost, and provides efficient alternative to classical SGN and fully explicit hSGN solvers.

Conclusion: Combination of hyperbolization and semi-implicit time integration offers an efficient and accurate solution for fully nonlinear and weakly dispersive shallow-water flow modeling.

Abstract: The Serre-Green-Naghdi (SGN) equations provide a valuable framework for
modelling fully nonlinear and weakly dispersive shallow-water flows. However,
their elliptic formulation can considerably increase the computational cost
compared to the Saint-Venant equations. To overcome this difficulty, hyperbolic
models (hSGN) have been proposed that replace the elliptic operators with
first-order hyperbolic formulations augmented by relaxation terms, which
recover the original elliptic formulation in the stiff limit. Yet, as the
relaxation parameter \lambda increases, explicit schemes face restrictive
stability constraints that may offset these advantages. To mitigate this
limitation, we introduce a semi-implicit (SI) integration strategy for the hSGN
system, where the stiff acoustic terms are treated implicitly within an IMEX
Runge-Kutta framework, while the advective components remain explicit. The
proposed approach mitigates the CFL stability restriction and maintains
dispersive accuracy at a moderate computational cost. Numerical results confirm
that the combination of hyperbolization and semi-implicit time integration
provides an efficient and accurate alternative to both classical SGN and fully
explicit hSGN solvers.

</details>


### [4] [Stochastic Gradient Descent for Incomplete Tensor Linear Systems](https://arxiv.org/abs/2510.07630)
*Anna Ma,Deanna Needell,Alexander Xue*

Main category: math.NA

TL;DR: This paper adapts stochastic gradient descent for tensor linear systems with missing data, extending beyond uniform missing patterns to other missing data models.


<details>
  <summary>Details</summary>
Motivation: Solving large tensor linear systems is challenging, especially when data is missing. Previous methods assumed uniform missing patterns, but real-world data often follows different missing patterns.

Method: Modified stochastic gradient descent approach by changing the update direction to handle various missing data models beyond uniform patterns.

Result: Proved convergence results and experimentally verified the method's effectiveness on synthetic data.

Conclusion: The adapted stochastic gradient descent method successfully handles tensor linear systems with different types of missing data patterns, extending beyond the uniform missing assumption.

Abstract: Solving large tensor linear systems poses significant challenges due to the
high volume of data stored, and it only becomes more challenging when some of
the data is missing. Recently, Ma et al. showed that this problem can be
tackled using a stochastic gradient descent-based method, assuming that the
missing data follows a uniform missing pattern. We adapt the technique by
modifying the update direction, showing that the method is applicable under
other missing data models. We prove convergence results and experimentally
verify these results on synthetic data.

</details>


### [5] [Parallel-in-Time Solution of Allen-Cahn Equations by Integrating Operator Learning into the Parareal Method](https://arxiv.org/abs/2510.07672)
*Yuwei Geng,Junqi Yin,Eric C. Cyr,Guannan Zhang,Lili Ju*

Main category: math.NA

TL;DR: The paper proposes using CNNs to learn discrete time-stepping operators from traditional numerical schemes, integrating them as coarse propagators in the Parareal method to accelerate solving time-dependent PDEs like Allen-Cahn equations while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods for PDEs show efficiency gains but struggle to match traditional solver accuracy. The Parareal method can leverage neural networks as coarse propagators, but requires consistency between coarse and fine solvers, especially for PDEs with rapid changes.

Method: Use convolutional neural networks to learn the fully discrete time-stepping operator defined by traditional numerical schemes. Train CNNs on trajectories from random initial conditions, then integrate them as coarse propagators in the Parareal algorithm with traditional methods as fine solvers.

Result: Achieves significant computational speedup compared to traditional fine solvers while converging to high-accuracy solutions. The method converges to desired accuracy in few iterations and shows particular effectiveness when implemented on multiple GPUs.

Conclusion: The approach demonstrates successful integration of neural networks into parallel-in-time frameworks for efficient and accurate simulations of time-dependent PDEs, with trained models applicable to various initial conditions without retraining.

Abstract: While recent advances in deep learning have shown promising efficiency gains
in solving time-dependent partial differential equations (PDEs), matching the
accuracy of conventional numerical solvers still remains a challenge. One
strategy to improve the accuracy of deep learning-based solutions for
time-dependent PDEs is to use the learned solution as the coarse propagator in
the Parareal method and a traditional numerical method as the fine solver.
However, successful integration of deep learning into the Parareal method
requires consistency between the coarse and fine solvers, particularly for PDEs
exhibiting rapid changes such as sharp transitions. To ensure such consistency,
we propose to use the convolutional neural networks (CNNs) to learn the fully
discrete time-stepping operator defined by the traditional numerical scheme
used as the fine solver. We demonstrate the effectiveness of the proposed
method in solving the classical and mass-conservative Allen-Cahn (AC)
equations. Through iterative updates in the Parareal algorithm, our approach
achieves a significant computational speedup compared to traditional fine
solvers while converging to high-accuracy solutions. Our results highlight that
the proposed Parareal algorithm effectively accelerates simulations,
particularly when implemented on multiple GPUs, and converges to the desired
accuracy in only a few iterations. Another advantage of our method is that the
CNNs model is trained on trajectories-based on random initial conditions, such
that the trained model can be used to solve the AC equations with various
initial conditions without re-training. This work demonstrates the potential of
integrating neural network methods into the parallel-in-time frameworks for
efficient and accurate simulations of time-dependent PDEs.

</details>


### [6] [Ergodicity and error estimate of laws for a random splitting Langevin Monte Carlo](https://arxiv.org/abs/2510.07676)
*Lei Li,Chen Wang,Mengchao Wang*

Main category: math.NA

TL;DR: Random splitting Langevin Monte Carlo reduces first-order bias with minimal extra computational cost compared to higher-order schemes. The paper develops an analysis framework for sampling error under Wasserstein distance and establishes O(τ²) accuracy.


<details>
  <summary>Details</summary>
Motivation: To mitigate first-order bias in Langevin Monte Carlo methods while maintaining computational efficiency compared to other high-order schemes.

Method: Developed analysis framework using relative entropy approach and explicit formulas for commutator of semi-groups. Used Bernstein type PDE approach for gradient/Hessian estimates. Established geometric ergodicity via reflection coupling. Combined local error estimates with ergodicity for uniform-in-time sampling error bounds.

Result: Established that the invariant measure of random splitting Langevin Monte Carlo approximates the true Gibbs distribution with O(τ²) accuracy, where τ is the time step. Numerical experiments validated theoretical results.

Conclusion: Random splitting Langevin Monte Carlo effectively reduces first-order bias with O(τ²) accuracy while requiring little extra computational work compared to other high-order schemes.

Abstract: The random splitting Langevin Monte Carlo could mitigate the first order bias
in Langevin Monte Carlo with little extra work compared other high order
schemes. We develop in this work an analysis framework for the sampling error
under Wasserstein distance regarding the random splitting Langevin Monte Carlo.
First, the sharp local truncation error is obtained by the relative entropy
approach together with the explicit formulas for the commutator of related
semi-groups. The necessary pointwise estimates of the gradient and Hessian of
the logarithmic density are established by the Bernstein type approach in PDE
theory. Second, the geometric ergodicity is established by accommodation of the
reflection coupling. Combining the ergodicity with the local error estimate, we
establish a uniform-in-time sampling error bound, showing that the invariant
measure of the method approximates the true Gibbs distribution with $O(\tau^2)$
accuracy where $\tau$ is the time step. Lastly, we perform numerical
experiments to validate the theoretical results.

</details>


### [7] [Smoother-type a posteriori error estimates for finite element methods](https://arxiv.org/abs/2510.07677)
*Yuwen Li,Han Shui*

Main category: math.NA

TL;DR: Develops user-friendly a posteriori error estimates for finite element methods using linear iterative solver smoothers like Jacobi or Gauss-Seidel on finer meshes.


<details>
  <summary>Details</summary>
Motivation: To create simple and practical error estimation methods that are easy to implement and outperform traditional residual-type estimators.

Method: Uses simple smoothers (Jacobi/Gauss-Seidel) on auxiliary finer meshes to process finite element residuals, requiring only coarse-to-fine prolongation operators.

Result: For symmetric problems, reliability and efficiency are proven under saturation assumption. Numerical experiments show better accuracy than residual-type estimators and robustness across parameters and polynomial degrees.

Conclusion: Smoother-type error estimators provide accurate, robust, and easily implementable alternatives to traditional error estimation methods in finite element analysis.

Abstract: This work develops user-friendly a posteriori error estimates of finite
element methods, based on smoothers of linear iterative solvers. The proposed
method employs simple smoothers, such as Jacobi or Gauss--Seidel iteration, on
an auxiliary finer mesh to process the finite element residual for a posteriori
error control. The implementation requires only a coarse-to-fine prolongation
operator. For symmetric problems, we prove the reliability and efficiency of
smoother-type error estimators under a saturation assumption. Numerical
experiments for various PDEs demonstrate that the proposed smoother-type error
estimators outperform residual-type estimators in accuracy and exhibit
robustness with respect to parameters and polynomial degrees.

</details>


### [8] [Elastic-plastic cell-based smoothed finite element method solving geotechnical problems](https://arxiv.org/abs/2510.07687)
*Yang Yang,Mingjiao Yan,Zongliang Zhang,Miao Zhang,Feidong Zheng,Dong Pana,Xiaozi Lina*

Main category: math.NA

TL;DR: A cell-based smoothed finite element method (CSFEM) is developed for elastic-plastic analysis of soils and rocks, improving stress accuracy and reducing mesh sensitivity while maintaining polygonal element flexibility.


<details>
  <summary>Details</summary>
Motivation: To address nonlinear and path-dependent behaviors in geotechnical materials (soils and rocks) while overcoming limitations of conventional FEM such as volumetric locking and mesh distortion sensitivity.

Method: Uses strain smoothing over subcell domains with consistent stress return-mapping algorithm, implemented in ABAQUS via user-defined elements.

Result: Excellent agreement with analytical solutions and conventional FEM, showing smoother stress fields, improved convergence, and higher accuracy in ultimate load prediction across various geotechnical problems.

Conclusion: CSFEM provides a stable and efficient framework for elastic-plastic analysis of complex geotechnical problems, offering enhanced performance over traditional methods.

Abstract: An elastic-plastic cell-based smoothed finite element method (CSFEM) is
proposed for geotechnical analysis of soils and rocks exhibiting nonlinear and
path-dependent behaviors. By introducing strain smoothing over subcell domains
and employing a consistent stress return-mapping algorithm, the method enhances
stress accuracy, alleviates volumetric locking, and reduces sensitivity to mesh
distortion while retaining the flexibility of polygonal elements. The
formulation is implemented in ABAQUS via a user-defined element and validated
through benchmark and practical problems, including a pressurized thick
cylinder, biaxial soil test, strip footing bearing capacity, tunnel excavation,
and slope stability. Numerical results show excellent agreement with analytical
solutions and conventional FEM, with smoother stress fields, improved
convergence, and higher accuracy in ultimate load prediction. These findings
demonstrate that CSFEM provides a stable and efficient framework for
elastic-plastic analysis of complex geotechnical problems.

</details>


### [9] [Scaling crossover of the generalized Jeffreys-type law](https://arxiv.org/abs/2510.07930)
*Fugui Ma*

Main category: math.NA

TL;DR: This paper develops a mathematical framework for the generalized Jeffreys-type law using multi-term time-fractional equations, provides physical explanations from microscopic principles, establishes rigorous analysis, and creates an efficient numerical algorithm with spectral accuracy.


<details>
  <summary>Details</summary>
Motivation: To provide a novel physical explanation for the generalized Jeffreys-type law from first principles and develop a mathematically rigorous framework for analyzing complex nonlocal problems with rich scaling crossover phenomena.

Method: Uses continuous-time random walk framework with generalized waiting time distribution, derives from overdamped Langevin equation with stochastic time-change, employs Laplace transform for analysis, and develops CIM-CLG numerical algorithm with spectral accuracy.

Result: Established well-posedness and Sobolev regularity of the equation, developed efficient numerical scheme with O(N) time and O(M log M) space complexity, validated through extensive 1D and 2D numerical experiments showing efficiency and accuracy.

Conclusion: The work advances understanding of generalized Jeffreys-type law by integrating stochastic modeling, mathematical analysis, and numerical computation, providing a rigorous and computationally efficient framework for complex nonlocal problems.

Abstract: The generalized Jeffreys-type law is formulated as a multi-term
time-fractional Jeffreys-type equation, whose dynamics exhibit rich scaling
crossover phenomena entailing different diffusion mechanisms. In this work, we
provide a novel physical explanation for the equation from first principles,
beginning with a microscopic description based on the continuous-time random
walk framework with a generalized waiting time distribution and further
deriving the equation from an overdamped Langevin equation subject to a
stochastic time-change (subordination). Employing the Laplace transform method,
we conduct a rigorous analysis of the equation, establishing its well-posedness
and providing a detailed Sobolev regularity analysis. We also develop a novel
numerical scheme, termed the CIM-CLG algorithm, which achieves spectral
accuracy in both time and space while substantially relaxing the temporal
regularity requirements on the solution. The algorithm reduces the
computational complexity to $\mathcal{O}(N)$ in time and $\mathcal{O}(M\log M)$
in space and is fully parallelizable. Detailed implementation guidelines and
new technical error estimates are provided. Extensive numerical experiments in
1D and 2D settings validate the efficiency, robustness, and accuracy of the
proposed method. By integrating stochastic modeling, mathematical analysis, and
numerical computation, this work advances the understanding of the generalized
Jeffreys-type law and offers a mathematically rigorous and computationally
efficient framework for tackling complex nonlocal problems.

</details>


### [10] [Likelihood-informed Model Reduction for Bayesian Inference of Static Structural Loads](https://arxiv.org/abs/2510.07950)
*Jakob Scheffels,Elizabeth Qian,Iason Papaioannou,Elisabeth Ullmann*

Main category: math.NA

TL;DR: A new projection-based model reduction method for Bayesian inverse problems that exploits low-dimensional likelihood-informed subspace structure to accelerate computations.


<details>
  <summary>Details</summary>
Motivation: Bayesian inverse problems are computationally expensive due to many forward model evaluations, but often exhibit low-dimensional structure where data is informative only in a small subspace.

Method: Projects the governing PDE onto the likelihood-informed subspace (LIS) to create a computationally efficient reduced model for cases where the unknown parameter is the right-hand-side forcing.

Result: Numerical experiments show the method successfully exploits intrinsic low-dimensionality, achieving relative errors of O(10^{-10}) with 10x-100x lower-dimensional models.

Conclusion: The proposed approach effectively accelerates Bayesian inverse problem solutions by leveraging the low-dimensional structure inherent in such problems.

Abstract: Bayesian inverse problems use data to update a prior probability distribution
on uncertain parameter values to a posterior distribution. Such problems arise
in many structural engineering applications, but computational solution of
Bayesian inverse problems is often expensive because standard solution
approaches require many evaluations of the forward model mapping the parameter
value to predicted observations. In many settings, this forward model is
expensive because it requires the solution of a high-dimensional discretization
of a partial differential equation. However, Bayesian inverse problems often
exhibit low-dimensional structure because the available data are primarily
informative (relative to the prior) in a low-dimensional subspace, sometimes
called the likelihood-informed subspace (LIS). This paper proposes a new
projection-based model reduction method for static linear systems that exploits
this low-dimensional structure in the setting where the unknown parameter is
the right-hand-side forcing. The proposed method projects the governing partial
differential equation onto the likelihood-informed subspace, yielding a
computationally efficient reduced model that can be used to accelerate the
solution of the inverse problem. Numerical experiments on two structural
engineering model problems demonstrate that the proposed approach can
successfully exploit the intrinsic low-dimensionality of the problem, obtaining
relative errors of O(10^{-10}) in the inverse problem solution with a 10x-100x
lower-dimensional model.

</details>


### [11] [LDMD with Temporally Adaptive Segmentation](https://arxiv.org/abs/2510.08065)
*Qiuqi Li,Chang Liu,Yifei Yang*

Main category: math.NA

TL;DR: Proposes a localized DMD (LDMD) framework that improves long-term predictive accuracy by segmenting temporal domain and performing localized predictions, with adaptive segmentation and error analysis.


<details>
  <summary>Details</summary>
Motivation: Standard DMD struggles with poor long-term predictive accuracy, so the authors aim to enhance prediction performance while maintaining computational efficiency.

Method: Segments temporal domain into subintervals, constructs snapshot matrices within each segment, performs localized predictions, and develops adaptive segmentation strategy with error analysis.

Result: LDMD significantly enhances long-term predictive accuracy on benchmark problems (Burgers', Allen-Cahn, nonlinear Schrodinger, Maxwell's equations) while preserving computational efficiency.

Conclusion: The localized DMD framework effectively addresses DMD's long-term prediction limitations through temporal segmentation and localized forecasting, with proven error bounds and demonstrated effectiveness.

Abstract: Dynamic mode decomposition (DMD) is a widely used data-driven algorithm for
predicting the future states of dynamical systems. However, its standard
formulation often struggles with poor long-term predictive accuracy. To address
this limitation, we propose a localized DMD (LDMD) framework that improves
prediction performance by integrating DMD's strong linear forecasting
capabilities with time-domain segmentation techniques. In this framework, the
temporal domain is segmented into multiple subintervals, within which snapshot
matrices are constructed and localized predictions are performed. We first
present the localized DMD method with predefined segmentation, and then explore
an adaptive segmentation strategy to further enhance computational efficiency
and prediction robustness. Furthermore, we conduct an error analysis that
provides the upper bound of the local and global truncation error for the
proposed framework. The effectiveness of LDMD is demonstrated on four benchmark
problems-Burgers', Allen-Cahn, nonlinear Schrodinger, and Maxwell's equations.
Numerical results show that LDMD significantly enhances long-term predictive
accuracy while preserving high computational efficiency.

</details>


### [12] [Semi-Implicit Central scheme for Hyperbolic Systems of Balance Laws with Relaxed Source Term](https://arxiv.org/abs/2510.08134)
*Sudipta Sahu,Emanuele Macca,Rathan Samala*

Main category: math.NA

TL;DR: A new IMEX finite volume scheme is developed for stiff hyperbolic systems, combining midpoint and trapezoidal rules with backward Taylor expansion to handle stiff source terms while maintaining second-order accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: To address computational challenges in quasi-linear hyperbolic systems with stiff source terms, which are difficult to solve numerically due to stiffness and discontinuities.

Method: Finite volume Nessyahu-Tadmor central scheme with new IMEX approach: stiff source term handled semi-implicitly using midpoint rule in space, trapezoidal rule in time, and backward semi-implicit Taylor expansion.

Result: The method demonstrates stability, robustness near stiffness and discontinuities, and preserves second-order accuracy asymptotically. Validated on multiple benchmark models.

Conclusion: The proposed IMEX scheme effectively solves stiff hyperbolic systems of balance laws with confirmed stability and accuracy, showing potential for efficient computation.

Abstract: Quasi-linear hyperbolic systems with source terms introduce significant
computational challenges due to the presence of a stiff source term. To address
this, a finite volume Nessyahu-Tadmor (NT) central numerical scheme is explored
and applied to benchmark models such as the Jin-Xin relaxation model, the
shallow-water model, the Broadwell model, the Euler equations with heat
transfer, and the Euler system with stiff friction to assess their
effectiveness. The core part of this numerical scheme lies in developing a new
implicit-explicit (IMEX) scheme, where the stiff source term is handled in an
semi-implicit manner constructed by combining the midpoint rule in space, the
trapezoidal rule in time with a backward semi-implicit Taylor expansion. The
advantage of the proposed method lies in its stability region and maintains
robustness near stiffness and discontinuities, while asymptotically preserving
second-order accuracy.
  Theoretical analysis and numerical validation confirm the stability and
accuracy of the method, highlighting its potential for efficiently solving the
stiff hyperbolic systems of balance laws.

</details>


### [13] [Dual-primal Isogeometric Tearing and Interconnecting Solvers for adaptively refined multi-patch configurations](https://arxiv.org/abs/2510.08148)
*Stefan Takacs,Stefan Tyoler*

Main category: math.NA

TL;DR: This paper applies the dual-primal Isogeometric Tearing and Interconnecting method (IETI-DP) to adaptive multi-patch geometries generated by recursive patch splitting, providing solvability conditions and a preconditioner with optimal condition number bounds.


<details>
  <summary>Details</summary>
Motivation: Isogeometric Analysis requires adaptive refinement for irregular solutions, and recent multi-patch adaptive refinement generates hierarchical non-matching configurations with T-junctions, necessitating efficient solution methods for such complex geometries.

Method: The authors apply IETI-DP method to adaptive multi-patch geometries, provide sufficient conditions for solvability of local problems, and propose a preconditioner for the iterative solver.

Result: The method establishes a condition number bound that matches previous bounds for fully matching cases, and numerical experiments confirm theoretical findings and demonstrate efficiency in adaptive refinement scenarios.

Conclusion: IETI-DP is successfully applied to adaptive multi-patch geometries with optimal condition number bounds, providing an efficient solution approach for adaptive refinement in Isogeometric Analysis.

Abstract: Isogeometric Analysis is a variant of the finite element method, where spline
functions are used for the representation of both the geometry and the
solution. Splines, particularly those with higher degree, achieve their full
approximation power only if the solution is sufficiently regular. Since
solutions are usually not regular everywhere, adaptive refinement is essential.
Recently, a multi-patch-based adaptive refinement strategy based on recursive
patch splitting has been proposed, which naturally generates hierarchical,
non-matching multi-patch configurations with T-junctions, but preserves the
tensor-product structure within each patch.
  In this work, we investigate the application of the dual-primal Isogeometric
Tearing and Interconnecting method (IETI-DP) to such adaptive multi-patch
geometries. We provide sufficient conditions for the solvability of the local
problems and propose a preconditioner for the overall iterative solver. We
establish a condition number bound that coincides with the bound previously
shown for the fully matching case. Numerical experiments confirm the
theoretical findings and demonstrate the efficiency of the proposed approach in
adaptive refinement scenarios.

</details>


### [14] [Full moment error estimates in strong norms for numerical approximations of stochastic Navier-Stokes equations with multiplicative noise, Part I: time discretization](https://arxiv.org/abs/2510.08291)
*Xiaobing Feng,Liet Vo*

Main category: math.NA

TL;DR: Optimal-order moment error estimates for Euler-Maruyama discretization of stochastic Navier-Stokes equations with multiplicative noise, plus a new framework for nonlinear SPDE analysis.


<details>
  <summary>Details</summary>
Motivation: To derive strong norm error estimates for velocity and pressure in stochastic Navier-Stokes equations and develop a general numerical analysis framework for nonlinear SPDEs with multiplicative noise.

Method: Established exponential stability estimates for SPDE solutions, used discrete stochastic Gronwall inequality, and applied bootstrap argument techniques.

Result: Achieved optimal-order full moment error estimates in strong norms for both velocity and pressure approximations.

Conclusion: The paper presents successful error analysis for stochastic Navier-Stokes equations and introduces a novel framework applicable to general nonlinear SPDEs with multiplicative noise.

Abstract: This paper focuses on deriving optimal-order full moment error estimates in
strong norms for both velocity and pressure approximations in the
Euler-Maruyama time discretization of the stochastic Navier-Stokes equations
with multiplicative noise. Additionally, it introduces a novel approach and
framework for the numerical analysis of nonlinear stochastic partial
differential equations (SPDEs) with multiplicative noise in general. The main
ideas of this approach include establishing exponential stability estimates for
the SPDE solution, leveraging a discrete stochastic Gronwall inequality, and
employing a bootstrap argument.

</details>


### [15] [Surface finite element approximation of parabolic SPDEs with Whittle--Matérn noise](https://arxiv.org/abs/2510.08443)
*Øyvind Stormark Auestad,Geir-Arne Fuglstad,Annika Lang*

Main category: math.NA

TL;DR: A new fully discrete surface finite element method for linear parabolic stochastic evolution equations with additive noise, using surface finite element approximation of noise with covariance from elliptic operators.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for stochastic PDEs on surfaces, particularly for equations with noise having covariance operators defined by powers of elliptic operators like Whittle-Matern random fields.

Method: Uses surface finite element approximation of the noise and fully discrete surface finite element discretization tailored for equations with additive noise having specific covariance structure.

Result: Derived strong and pathwise convergence rates for the approximation method, with numerical experiments verifying the theoretical convergence rates.

Conclusion: The proposed method provides an effective fully discrete surface finite element approach for stochastic evolution equations with additive noise, with proven convergence properties.

Abstract: We propose and analyse a new type of fully discrete surface finite element
approximation of a class of linear parabolic stochastic evolution equations
with additive noise. Our discretization uses a surface finite element
approximation of the noise, and is tailored for equations with noise having
covariance operator defined by (negative powers of) elliptic operators, like
Whittle--Mat\'ern random fields. We derive strong and pathwise convergence
rates of our approximation, and verify these by numerical experiments.

</details>


### [16] [Refinement-based Christoffel sampling for least squares approximation in non-orthogonal bases](https://arxiv.org/abs/2510.08461)
*Astird Herremans,Ben Adcock*

Main category: math.NA

TL;DR: A refinement-based Christoffel sampling algorithm for least squares approximation that avoids costly discrete orthogonalization for non-orthogonal bases, with computational cost growing only logarithmically rather than proportionally with the Christoffel function norm.


<details>
  <summary>Details</summary>
Motivation: Standard Christoffel sampling requires i.i.d. sampling from a distribution proportional to the inverse Christoffel function, which needs an orthonormal basis. For non-orthogonal bases, existing approaches rely on expensive discrete orthogonalization.

Method: Propose an iterative refinement-based Christoffel sampling algorithm inspired by approximate leverage score sampling. Uses a numerical variant of the Christoffel function to account for finite-precision effects and relies only on computable quantities.

Result: The algorithm achieves near-best approximations in probability using O(n log(n)) samples. Computational cost increases only logarithmically with ||k_n||_{L^∞(X)} instead of proportionally as in discrete orthogonalization approaches.

Conclusion: The proposed method is efficient and robust, avoiding the bottleneck of discrete orthogonalization while maintaining theoretical guarantees, as demonstrated through convergence proof and extensive numerical experiments.

Abstract: We introduce a refinement-based Christoffel sampling (RCS) algorithm for
least squares approximation in the span of a given, generally non-orthogonal
set of functions $\Phi_n = \{\phi_1, \dots, \phi_n\}$. A standard sampling
strategy for this problem is Christoffel sampling, which achieves near-best
approximations in probability using only $\mathcal{O}(n \log(n))$ samples.
However, it requires i.i.d.\ sampling from a distribution whose density is
proportional to the inverse Christoffel function $k_n$, the computation of
which requires an orthonormal basis. As a result, existing approaches for
non-orthogonal bases $\Phi_n$ typically rely on costly discrete
orthogonalization. We propose a new iterative algorithm, inspired by recent
advances in approximate leverage score sampling, that avoids this bottleneck.
Crucially, while the computational cost of discrete orthogonalization grows
proportionally with $\|k_n\|_{L^\infty(X)}$, the cost of our approach increases
only logarithmically in $\|k_n\|_{L^\infty(X)}$. In addition, we account for
finite-precision effects by considering a numerical variant of the Christoffel
function, ensuring that the algorithm relies only on computable quantities.
Alongside a convergence proof, we present extensive numerical experiments
demonstrating the efficiency and robustness of the proposed method.

</details>


### [17] [Where Have All the Kaczmarz Iterates Gone?](https://arxiv.org/abs/2510.08563)
*El Houcine Bergou,Soumia Boucherouite,Aritra Dutta,Xin Li,Anna Ma*

Main category: math.NA

TL;DR: Analysis of the randomized Kaczmarz algorithm's asymptotic behavior on noisy and inconsistent linear systems, including limit point locations, convergence bounds, and numerical validation.


<details>
  <summary>Details</summary>
Motivation: Practical applications often involve noisy and inconsistent linear systems, but the study of RK algorithm behavior in such conditions is limited despite its computational efficiency.

Method: Investigated asymptotic behavior of RK iterates in expectation, analyzed roles of singular vectors of noisy coefficient matrix, derived convergence horizon bounds, and conducted extensive numerical experiments.

Result: Established understanding of RK algorithm's limitations and robustness in noisy environments, with bounds on convergence horizon that depend on noise levels and system characteristics.

Conclusion: The findings provide practical insights for optimized RK applications in real-world scientific and engineering problems with noisy data.

Abstract: The randomized Kaczmarz (RK) algorithm is one of the most computationally and
memory-efficient iterative algorithms for solving large-scale linear systems.
However, practical applications often involve noisy and potentially
inconsistent systems. While the convergence of RK is well understood for
consistent systems, the study of RK on noisy, inconsistent linear systems is
limited. This paper investigates the asymptotic behavior of RK iterates in
expectation when solving noisy and inconsistent systems, addressing the
locations of their limit points. We explore the roles of singular vectors of
the (noisy) coefficient matrix and derive bounds on the convergence horizon,
which depend on the noise levels and system characteristics. Finally, we
provide extensive numerical experiments that validate our theoretical findings,
offering practical insights into the algorithm's performance under realistic
conditions. These results establish a deeper understanding of the RK
algorithm's limitations and robustness in noisy environments, paving the way
for optimized applications in real-world scientific and engineering problems.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [18] [Consistency of some well-posed five-field theories of dissipative relativistic fluid dynamics](https://arxiv.org/abs/2510.07526)
*Heinrich Freistuhler*

Main category: math.AP

TL;DR: This paper analyzes causal hyperbolic five-field theories from the FTBDNK family of relativistic Navier-Stokes formulations, specifically focusing on EGS(L) models derived from Landau-Lifshitz formulation via Eulerian gradient shifts.


<details>
  <summary>Details</summary>
Motivation: To establish consistency properties for causal hyperbolic theories in relativistic fluid dynamics, particularly those obtained from the Landau-Lifshitz formulation through Eulerian gradient shifts.

Method: Analysis of EGS(L) models within the FTBDNK family, examining their properties relative to the Landau-Lifshitz formulation using perturbation theory with dissipation coefficient magnitude ε.

Result: EGS(L) models are O(ε²) equivalent to Landau-Lifshitz formulation, have O(ε³) excess entropy production, and represent heterogeneous local thermodynamic equilibria cleanly.

Conclusion: The EGS(L) family provides consistent causal hyperbolic theories that closely approximate the Landau-Lifshitz formulation while maintaining desirable thermodynamic properties.

Abstract: Within the FTBDNK family of formulations of relativistic Navier-Stokes (H.
Freist\"uhler and B. Temple, Proc. R. Soc. A 470, 20140055 (2014), Proc. R.
Soc. A 473 (2017), 20160729; F. S. Bemfica, M. Disconzi, and J. Noronha, Phys.
Rev. D 98, 104064 (2018), Phys. Rev. D 100, 104020 (2019); P. Kovtun, J. High
Energy Phys. 2019, 034 (2019)), this paper collects some consistency properties
for certain causal hyperbolic five-field theories obtained from the
Landau-Lifshitz formulation via Eulerian gradient shifts, a family, EGS(L), of
models that slightly generalize a class identified in H. Freist\"uhler, J.
Math.\ Phys. 61, 033101 (2020). With $\epsilon$ the magnitude of the
dissipation coefficients that quantify viscosity and heat conduction, the paper
shows that any element of EGS(L) is $O(\epsilon^2)$ equivalent to the
Landau-Lifshitz formulation, has an $O(\epsilon^3)$ excess entropy production,
and represents heterogeneous local thermodynamic equilibria cleanly.

</details>


### [19] [First order equation on random measures as superposition of weak solutions to the McKean-Vlasov equation](https://arxiv.org/abs/2510.07542)
*Alessandro Pinzi*

Main category: math.AP

TL;DR: This paper defines an evolution equation for random probability measures with non-local drift and diffusion, shows solutions can be lifted to superposition solutions of non-linear KFP and McKean-Vlasov equations, and transfers existence/uniqueness results.


<details>
  <summary>Details</summary>
Motivation: To develop a framework for studying evolution equations of random probability measures with non-local interactions and establish connections between different formulations of stochastic processes.

Method: Define evolution equation for random measures, prove lifting to superposition solutions of non-linear KFP and McKean-Vlasov equations, transfer existence/uniqueness results assuming uniqueness of linearized KFP.

Result: Solutions to the random measure equation can be represented as superpositions of solutions to non-linear KFP and McKean-Vlasov equations, enabling transfer of existence/uniqueness properties.

Conclusion: The paper establishes fundamental connections between different formulations of stochastic processes with non-local interactions and provides a framework for transferring existence and uniqueness results across these formulations.

Abstract: The goal of this paper is to define an evolution equation for a curve of
random probability measures $(M_t)_{t\in[0,T]}\subset
\mathcal{P}(\mathcal{P}(\mathbb{R}^d))$ associated to a non-local drift
$b:[0,T]\times\mathbb{R}^d \times \mathcal{P}(\mathbb{R}^d) \to \mathbb{R}^d$
and a non-local diffusion term $a:[0,T]\times \mathbb{R}^d \times
\mathcal{P}(\mathbb{R}^d) \to \operatorname{Sym}_+(\mathbb{R}^{d\times d})$.
Then, we show that any solution to that equation can be lifted to a
superposition of solutions to a non-linear Kolmogorov-Fokker-Planck equation
and also to a superposition of weak solutions to the McKean-Vlasov equations.
Finally, we use this superposition result to show how existence and uniqueness
can be transferred from the equation on random measures to the associated
non-linear Kolmogorov-Fokker-Planck equation and to the McKean-Vlasov equation,
assuming uniqueness of the linearized KFP.

</details>


### [20] [Nonlinear Stability of the Rayleigh-Taylor Problem in Quantum Navier-Stokes Equations](https://arxiv.org/abs/2510.07695)
*Fei Jiang,Yajie Zhang,Zhipeng Zhang,Youyi Zhao*

Main category: math.AP

TL;DR: The paper proves that quantum effects can inhibit Rayleigh-Taylor instability in nonlinear quantum Navier-Stokes equations, establishing a threshold for the scaled Planck constant that ensures algebraic stability.


<details>
  <summary>Details</summary>
Motivation: To extend the known linear theory of quantum effect stabilization in Rayleigh-Taylor instability to the nonlinear case and rigorously prove inhibition of instability under proper conditions.

Method: A complicated multi-layer energy method with anisotropic norms of spatial derivatives is used to mathematically prove the stability results.

Result: If the Rayleigh-Taylor density profile satisfies an additional stabilizing condition, there exists a threshold for the scaled Planck constant such that when exceeded, small perturbation solutions around an equilibrium state are algebraically stable in time.

Conclusion: Quantum effects can completely inhibit Rayleigh-Taylor instability in the nonlinear regime under proper conditions, with the scaled Planck constant playing a crucial role in determining stability.

Abstract: It is well-known that the Rayleigh--Taylor (abbr. RT) instability can be
completely inhibited by the quantum effect stabilization in proper
circumstances leading to a cutoff wavelength in the \emph{linear} motion
equations. Motivated by the linear theory, we further investigate the
{stability} for the \emph{nonlinear} RT problem of quantum Navier--Stokes
equations in a slab with Navier boundary condition, and rigorously prove the
inhibition of RT instability by the quantum effect under a proper setting. More
precisely, if the RT density profile $\bar\rho$ satisfies an additional
stabilizing condition, then there is a threshold ${\varepsilon_{{c}}}$ of the
scaled Planck constant, such that if the scaled Planck constant is bigger than
${\varepsilon_{{c}}}$, the small perturbation solutions around an RT
equilibrium state are algebraically stable in time. The mathematical proof is
realized by a complicated multi-layer energy method with anisotropic norms of
spacial derivatives.

</details>


### [21] [A log-free estimate for the diagonal paraproduct high $\times$ high $\to$ low in the 3D Navier-Stokes equation](https://arxiv.org/abs/2510.07848)
*Pylyp Cherevan*

Main category: math.AP

TL;DR: The paper analyzes the diagonal paraproduct in the 3D Navier-Stokes nonlinearity (u·∇)u, obtaining log-free L²_t Ḣ⁻¹_x estimates for scale-critical windows in the range 1/6 < δ ≤ 5/8.


<details>
  <summary>Details</summary>
Motivation: To understand the nonlinear structure of the 3D Navier-Stokes equations, particularly the diagonal paraproduct arising from (u·∇)u, and establish critical energy estimates.

Method: Uses phase-geometric integration, anisotropic local estimates on cylinders, bilinear ℓ² decoupling on finite-rank surfaces, and suppression of the null form to control narrow diagonal zones.

Result: Obtained log-free estimates at L²_t Ḣ⁻¹_x level for the projection P_{< N¹⁻ᵟ}∇(u_N ⊗ v_N) in scale-critical windows for 1/6 < δ ≤ 5/8.

Conclusion: The analysis is currently restricted to single resonant components; extensions to the full (u·∇)u structure and sup_t versions require further investigation.

Abstract: We consider the diagonal paraproduct arising in the nonlinearity $(u\cdot
\nabla) u$ for the three-dimensional Navier-Stokes equations. On scale-critical
windows and in the range $1/6 < \delta \le 5/8$ we obtain a log-free estimate
at the level $L^2_t {\dot H}^{-1}_x$ for the projection $P_{< N^{1-\delta}}
\nabla(u_N \otimes v_N)$, consistent with the critical energy scheme. The main
tools are phase-geometric integration, anisotropic local estimates on
cylinders, and bilinear $ell^2$ decoupling on a finite-rank surface; the narrow
diagonal zone is controlled via suppression of the null form. The work is
restricted to a single resonant component; extensions to the full structure $(u
\cdot\nabla) u$ and to sup$_t$ versions are left for further analysis.

</details>


### [22] [The $\eps-\eps^β$ property for clusters with double density](https://arxiv.org/abs/2510.07907)
*A. Pratelli,V. Scattaglia*

Main category: math.AP

TL;DR: Extends the ε-ε^β property to clusters in Euclidean space with double density


<details>
  <summary>Details</summary>
Motivation: To generalize the ε-ε^β property from standard settings to more complex scenarios involving clusters with double density in Euclidean space

Method: Mathematical extension of the ε-ε^β property concept to handle clusters with double density structure

Result: Successfully extended the ε-ε^β property framework to accommodate double density clusters in Euclidean space

Conclusion: The extension provides a more comprehensive mathematical framework for analyzing clusters with complex density structures

Abstract: This article is devoted to extend the "$\eps-\eps^\beta$ property" to the
case of clusters in an Euclidean space with a double density.

</details>


### [23] [Fractional p-Laplacian Kirchhoff-type problem involving a singular term via Nehari manifold](https://arxiv.org/abs/2510.07911)
*Djamel Abid*

Main category: math.AP

TL;DR: Existence of nontrivial positive solutions for Kirchhoff-type problems with sign-changing nonlinearities and singular terms, proving at least two positive solutions exist for appropriate parameter choices in both subcritical and critical cases.


<details>
  <summary>Details</summary>
Motivation: To study Kirchhoff-type problems with sign-changing nonlinearities and singular terms, which present mathematical challenges due to the combination of nonlocal effects, changing sign nonlinearities, and singular behavior.

Method: Using the Nehari manifold approach and Ekeland's variational principle to analyze the existence of solutions.

Result: For appropriate choice of parameter λ, the problem has at least two positive solutions in both subcritical and critical cases.

Conclusion: The variational methods successfully establish the existence of multiple positive solutions for Kirchhoff-type problems with sign-changing nonlinearities and singular terms under suitable parameter conditions.

Abstract: This paper is dedicated to studying the existence of nontrivial positive
solutions for a Kirchhoff-type problem with sign change nonlinearities and a
singular term, Using the Nehari manifold and EkelandS variational principle we
prove that for the appropriate choice of {\lambda} our problem has at least two
positive solutions for both subcritical and critical cases.

</details>


### [24] [A cross-diffusion system with independent drifts and fast diffusion](https://arxiv.org/abs/2510.07937)
*Charles Elbar,Filippo Santambrogio*

Main category: math.AP

TL;DR: Existence of global weak solutions for a 1D cross-diffusion system with fast-diffusion law (0<α≤1) and external potentials, extending previous linear diffusion results.


<details>
  <summary>Details</summary>
Motivation: To extend the recent result of Mészáros and Parker from linear diffusion (α=1) to the fast-diffusion case for cross-diffusion systems of interacting populations.

Method: Study a one-dimensional cross-diffusion system on the torus with fast-diffusion law exponent 0<α≤1 and different external potentials, using non-negative L¹ initial data with bounded entropy and a mixing condition.

Result: Proved the existence of global weak solutions for arbitrary non-negative L¹ initial data with bounded entropy and mixing condition.

Conclusion: Successfully extended the existence theory from linear diffusion to fast-diffusion cases for cross-diffusion systems of interacting populations.

Abstract: We study a one-dimensional cross-diffusion system for two interacting
populations on the torus, with a fast-diffusion law with exponent $0< \alpha\le
1$ and different external potentials. For arbitrary non-negative $L^{1}$
initial data with bounded entropy and a mixing condition we prove the existence
of global weak solutions. This extends the recent result of M\'esz\'aros,
Parker from the linear diffusion ($\alpha=1$) to the fast-diffusion.

</details>


### [25] [Gradient regularity for widely degenerate parabolic equations](https://arxiv.org/abs/2510.07999)
*Michael Strunk*

Main category: math.AP

TL;DR: The paper establishes continuity of weak solutions to degenerate parabolic equations where the diffusion vanishes on a bounded convex set, extending elliptic regularity results to the parabolic case.


<details>
  <summary>Details</summary>
Motivation: To extend C^1-regularity results from elliptic equations to the parabolic setting, particularly for equations where the diffusion degenerates (vanishes) on bounded convex sets.

Method: Analyzes weak solutions to parabolic equations with degenerate diffusion structure, where the diffusion function F vanishes on a bounded convex set E and is regular outside E. Uses L^{n+2+σ} regularity assumptions on the forcing term f.

Result: Proves that for any continuous function K vanishing on E, the composition K(Du) is continuous in the space-time cylinder, establishing continuity of the gradient outside the degenerate set.

Conclusion: Successfully extends elliptic regularity theory to parabolic equations with degenerate diffusion, showing continuity properties of solutions even when the diffusion vanishes on certain sets.

Abstract: In this paper, we are interested in the regularity of weak solutions
$u\colon\Omega_T\to\mathbb{R}$ to parabolic equations of the type
\begin{equation*}
  \partial_t u - \mathrm{div} \nabla \mathcal{F}(x,t,Du) = f\qquad\mbox{in
$\Omega_T$}, \end{equation*} where $\mathcal{F}$ is only elliptic for values of
$Du$ outside a bounded and convex set $E\subset \mathbb{R}^n$ with the property
that $0\in \mathrm{Int}{E}$. Here, $\Omega_T
:=\Omega\times(0,T)\subset\mathbb{R}^{n+1}$ denotes a space-time cylinder taken
over a bounded domain $\Omega\subset\mathbb{R}^n$ for some finite time $T>0$.
The function $\mathcal{F} : \Omega_T\times\mathbb{R}^n \to\mathbb{R}_{\geq 0}$
present in the diffusion is assumed to satisfy: the partial mapping $\xi\mapsto
\mathcal{F}(x,t,\xi)$ is regular whenever $\xi$ lies outside of $E$, and
vanishes entirely whenever $\xi$ lies within this set. Additionally, the datum
$f$ is assumed to be of class $L^{n+2+\sigma}(\Omega_T)$ for some parameter
$\sigma > 0$. As our main result we establish that
  \begin{equation*}
  \mathcal{K}(Du)\in C^0(\Omega_T)
  \end{equation*} for any continuous function $\mathcal{K}\in
C^0(\mathbb{R}^n)$ that vanishes on $E$. This article aims to extend the
$C^1$-regularity result for the elliptic case to the parabolic setting.

</details>


### [26] [Stability of Traveling Fronts of the FitzHugh-Nagumo Equations on Cylindrical Surfaces](https://arxiv.org/abs/2510.08028)
*Afroditi Talidou*

Main category: math.AP

TL;DR: Traveling front solutions of FitzHugh-Nagumo equations are nonlinearly stable on cylindrical surfaces, including standard cylinders and warped cylinders with slowly varying radius.


<details>
  <summary>Details</summary>
Motivation: To extend the known nonlinear stability of traveling front solutions in one spatial dimension to cylindrical surfaces, addressing stability on curved geometries.

Method: Mathematical analysis of traveling front solutions on cylindrical surfaces, extension to warped cylinders with slowly varying radius, and numerical simulations to support theoretical results.

Result: Traveling fronts are nonlinearly stable on standard cylinders and persist on warped cylinders with slowly varying radius.

Conclusion: The stability properties of FitzHugh-Nagumo traveling fronts extend successfully from one-dimensional to cylindrical geometries, with numerical validation supporting the theoretical analysis.

Abstract: The FitzHugh-Nagumo equations are known to admit traveling front solutions in
one spatial dimension that are nonlinearly stable. This paper concerns the
stability of traveling front solutions propagating on cylindrical surfaces. It
is shown that such traveling fronts are nonlinearly stable on the surface of
standard cylinders of constant radius. The analysis is extended to warped
cylinders with slowly varying radius, where persistence of front-like solutions
is established. Numerical simulations support the theoretical findings.

</details>


### [27] [Determining a magnetic Schrödinger equation by a single far-field measurement](https://arxiv.org/abs/2510.08198)
*Chaohua Duan,Zhen Xue*

Main category: math.AP

TL;DR: Single far-field measurement uniquely determines support of magnetic and electric potentials for polyhedral scatterers. Transmission eigenfunctions vanish at corners in 2D and edge corners in 3D when angle ≠ π.


<details>
  <summary>Details</summary>
Motivation: To solve inverse scattering problem for magnetic Schrödinger equation and understand shape reconstruction with minimal data, advancing theoretical understanding of inverse scattering with magnetic potentials.

Method: Variational approach for well-posedness of direct problem, complex geometric optics solutions with asymptotic analysis near singular points, and analysis of transmission eigenfunctions.

Result: Established well-posedness of direct problem, proved unique determination of potential support from single measurement for polyhedral structures, showed eigenfunctions vanish at corners.

Conclusion: Minimal measurement data suffices for shape reconstruction in practical cases, with applications in quantum imaging, material characterization, and nondestructive testing involving magnetic fields.

Abstract: This paper investigates the inverse scattering problem for the magnetic
Schr\"odinger equation. We first establish the well-posedness of the direct
problem through a variational approach under physically meaningful assumptions
on the magnetic and electric potentials. Our main results demonstrate that a
single far-field measurement uniquely determines the support of the potential
functions when the scatterer has polyhedral structures.
  A significant theoretical byproduct of our analysis reveals that transmission
eigenfunctions must vanish at corners in two dimensions and edge corners in
three dimensions, provided the angle is not $\pi$. This geometric property of
eigenfunctions extends previous results for the non-magnetic case and provides
new insights into the interaction between quantum effects and singular
geometries. The proof combines complex geometric optics solutions with careful
asymptotic analysis near singular points.
  From an inverse problems perspective, our work shows that minimal measurement
data suffices for shape reconstruction in important practical cases, advancing
the theoretical understanding of inverse scattering with magnetic potentials.
The results have potential applications in quantum imaging, material
characterization, and nondestructive testing where magnetic fields play a
crucial role.

</details>


### [28] [A survey on the optimal partition problem](https://arxiv.org/abs/2510.08241)
*Roberto Ognibene,Bozhidar Velichkov*

Main category: math.AP

TL;DR: Survey on regularity theory for optimal partition problems involving vector-valued Sobolev functions with disjoint supports, covering both local minimizers of Dirichlet energy and critical points satisfying variational inequalities.


<details>
  <summary>Details</summary>
Motivation: The optimal partition problem has emerged in diverse contexts, requiring a coherent overview and up-to-date account of progress in understanding solution regularity and free boundaries.

Method: Synthesizes current state-of-the-art by analyzing non-negative vector-valued Sobolev functions with mutually disjoint support components, examining both local minimizers of Dirichlet energy and critical points satisfying variational inequalities.

Result: Provides comprehensive coverage of regularity results for solutions and their free boundaries, including both interior regularity and behavior up to fixed boundaries.

Conclusion: This survey offers a unified perspective on the regularity theory for optimal partition problems, consolidating recent advances in understanding solution properties and free boundary behavior across various mathematical contexts.

Abstract: This survey synthesizes the current state of the art on the regularity theory
for solutions to the optimal partition problem. Namely, we consider
non-negative, vector-valued Sobolev functions whose components have mutually
disjoint support, and which are either local minimizers of the Dirichlet energy
or, more generally, critical points satisfying a system of variational
inequalities. This is particularly meaningful as the problem has emerged on
several occasions and in diverse contexts: our aim is then to provide a
coherent point of view and an up-to-date account of the progress concerning
regularity of the solutions and their free boundaries, both in the interior and
up to a fixed boundary.

</details>


### [29] [Hölder regularity of the solutions of Fredholm integral equations on upper Ahlfors regular sets](https://arxiv.org/abs/2510.08264)
*M. Lanza de Cristoforis,M. Norman*

Main category: math.AP

TL;DR: Extension of Hölder continuity results for Fredholm integral equations to metric measured spaces with upper Ahlfors growth conditions, including nondoubling measures.


<details>
  <summary>Details</summary>
Motivation: To generalize existing Hölder continuity results for solutions of Fredholm integral equations beyond traditional doubling measure contexts, accommodating more general metric measured spaces.

Method: Extending mathematical analysis to metric measured spaces where the measure satisfies upper Ahlfors growth conditions, which are less restrictive than doubling conditions.

Result: Established validity of (generalized) Hölder continuity for solutions of Fredholm integral equations of the second kind in this broader context.

Conclusion: The results extend classical regularity theory to include cases with nondoubling measures, broadening the applicability of Hölder continuity analysis in integral equations.

Abstract: We extend to the context of metric measured spaces, with a measure that
satisfies upper Ahlfors growth conditions the validity of (generalized)
H\"{o}lder continuity results for the solution of a Fredholm integral equation
of the second kind. Here we note that upper Ahlfors growth conditions include
also cases of nondoubling measures.

</details>


### [30] [On the Cahn-Hilliard equation with nonlinear diffusion: the non-convex case](https://arxiv.org/abs/2510.08287)
*Monica Conti,Stefania Gatti,Andrea Giorgini,Giulio Schimperna*

Main category: math.AP

TL;DR: Analysis of Cahn-Hilliard equation with nonlinear diffusion and non-degenerate mobility for phase separation, removing convexity assumptions and establishing new solution properties in 2D and 3D.


<details>
  <summary>Details</summary>
Motivation: Previous results required strong convexity of energy gradient, excluding relevant cases. This work aims to analyze the model under more general assumptions on diffusion and mobility functions.

Method: Uses Lojasiewicz-Simon inequality tailored for nonlinear diffusion case to characterize longtime dynamics. Proves uniqueness, smoothing effects, and convergence in 2D; local well-posedness and global existence near minimizers in 3D.

Result: In 2D: uniqueness of weak solutions, smoothing effect for positive times, convergence to equilibrium. In 3D: local well-posedness for arbitrary initial data, global existence near energy minimizers, Lyapunov stability principle.

Conclusion: Successfully removed convexity condition and established comprehensive qualitative properties of solutions under general assumptions, providing new insights into phase separation dynamics in complex systems.

Abstract: We investigate the Cahn-Hilliard equation with nonlinear diffusion and
non-degenerate mobility modeling phase separation phenomena in complex systems
(e.g., crystals and polymers). Previous results in the literature on this model
relied on the strong convexity assumption of the gradient part of the energy,
which excludes relevant cases. In this work, we remove the convexity condition
and establish new qualitative properties of solutions under general assumptions
on the diffusion and mobility functions. In two spatial dimensions, we prove
uniqueness of weak solutions, their smoothing effect for positive times, and
convergence to equilibrium as time tends to infinity. In three dimensions, we
show local well-posedness of strong solutions for arbitrary initial data and
global existence for data close to energy minimizers, yielding a Lyapunov
stability principle. A key ingredient of our analysis is a Lojasiewicz-Simon
inequality tailored to the nonlinear diffusion case, which enables us to
characterize the longtime dynamics.

</details>


### [31] [On a class of (non)local superposition operators of arbitrary order](https://arxiv.org/abs/2510.08345)
*Serena Dipierro,Sven Jarohs,Enrico Valdinoci*

Main category: math.AP

TL;DR: Systematic study of superposition operators of any positive order, including examples, counterexamples, characterization of measures and functional spaces, and applications to nonlinear problems involving fractional order operators.


<details>
  <summary>Details</summary>
Motivation: To provide a general framework for dealing with superposition operators of any positive order and systematically study their properties.

Method: Developed a general setting for superposition operators, provided examples and counterexamples, characterized measures and functional spaces, and applied to nonlinear problems.

Result: Established a comprehensive theoretical framework for superposition operators of arbitrary positive order, including fractional cases.

Conclusion: The paper presents a systematic approach to superposition operators that covers operators of any positive order, with applications to nonlinear problems involving fractional operators.

Abstract: In this paper we introduce a very general setting dealing with the
superposition of operators of any positive order and provide a systematic study
of them. We also provide examples and counterexamples, as well as
characterizing properties of the measures and the functional spaces under
consideration. Moreover, we present some applications regarding the existence
theory for a class of nonlinear problems involving superposition operators of
arbitrary (possibly fractional) order.

</details>


### [32] [The Space-Time Connectivity Theorem for Normal Currents](https://arxiv.org/abs/2510.08360)
*Paolo Bonicatto,Filip Rindler,Harry Turnbull*

Main category: math.AP

TL;DR: Establishes a Space-Time Connectivity Theorem for normal currents, extending classical results to provide progressive-in-time deformations connecting sequences of boundaryless normal currents with their weak* limits.


<details>
  <summary>Details</summary>
Motivation: To extend classical connectivity theorems by Federer and Fleming to normal currents, providing a space-time framework that allows witnessing weak* convergence through progressive deformations over time.

Method: Develops a space-time connectivity theorem using space-time normal currents with time coordinates, building on classical results and recent work on integral currents.

Result: Proves that uniformly bounded sequences of boundaryless normal currents can be connected to their weak* limit via a space-time normal current that provides progressive-in-time deformations.

Conclusion: The theorem provides a new geometric perspective on weak* convergence in the space of normal currents, distinguishing from classical results by incorporating time progression in the deformation process.

Abstract: This work establishes a Space-Time Connectivity Theorem for normal currents.
In analogy to classical results by Federer and Fleming as well as a recent
theorem for integral currents by the second author, this result allows one to
witness the weak* convergence of a uniformly bounded sequence of boundaryless
normal currents with a space-time normal current that connects the elements of
the sequence with their limit. The space-time setting is distinguished from the
classical case in that this connecting current has a time coordinate and thus
constitutes a progressive-in-time way to deform an element of the sequence to
the limit.

</details>


### [33] [Analysis of the transmission eigenvalue problem for biharmonic scattering considering penetrable scatterers](https://arxiv.org/abs/2510.08444)
*Rafael Ceja Ayala,Isaac Harris,Andreas Kleefeld*

Main category: math.AP

TL;DR: Analytical study of transmission eigenvalues for biharmonic scattering with penetrable obstacles in elastic plates, proving existence, discreteness, and monotonicity with refractive index.


<details>
  <summary>Details</summary>
Motivation: Extend transmission eigenvalue analysis from acoustic scattering to biharmonic scattering in elastic plates, where plate thickness is small relative to wavelength.

Method: Mathematical analysis of transmission eigenvalue problem using analytical methods, proving theoretical properties and conducting numerical validation.

Result: Proved existence and discreteness of transmission eigenvalues, established monotonicity of first eigenvalue with respect to refractive index, and validated with numerical experiments.

Conclusion: Successfully extended transmission eigenvalue theory to biharmonic scattering, providing analytical framework and numerical verification for elastic plate scattering problems.

Abstract: In this paper, we provide an analytical study of the transmission eigenvalue
problem in the context of biharmonic scattering with a penetrable obstacle. We
will assume that the underlying physical model is given by an infinite elastic
two--dimensional Kirchhoff--Love plate in $\mathbb{R}^2$, where the plate's
thickness is small relative to the wavelength of the incident wave. In previous
studies, transmission eigenvalues have been studied for acoustic scattering,
whereas in this case, we consider biharmonic scattering. We prove the existence
and discreteness of the transmission eigenvalues as well as study the
dependence on the refractive index. We are able to prove the monotonicity of
the first transmission eigenvalue with respect to the refractive index. Lastly,
we provide numerical experiments to validate the theoretical work.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [34] [A Virtual Fields Method-Genetic Algorithm (VFM-GA) calibration framework for isotropic hyperelastic constitutive models with application to an elastomeric foam material](https://arxiv.org/abs/2510.07683)
*Zicheng Yan,Jialiang Tao,Christian Franck,David L. Henann*

Main category: physics.comp-ph

TL;DR: A calibration framework combining Virtual Fields Method and Genetic Algorithm for automated parameter identification in isotropic hyperelastic constitutive models, validated on elastomeric foams.


<details>
  <summary>Details</summary>
Motivation: To automate material parameter identification for complex hyperelastic models while ensuring physical validity and accommodating various deformation fields.

Method: Synergizes Virtual Fields Method for objective function formulation with Genetic Algorithm optimization, using DIC displacement fields and load cell data, with material stability assessment.

Result: Outperforms manual fitting, demonstrating robust and efficient parameter identification for complex hyperelastic constitutive models.

Conclusion: The VFM-GA framework provides an effective automated solution for material parameter calibration in hyperelastic models, handling complex parameter spaces and ensuring physical behavior.

Abstract: This work introduces a calibration framework for material parameter
identification in isotropic hyperelastic constitutive models. The framework
synergizes the Virtual Fields Method (VFM) to define an objective function with
a Genetic Algorithm (GA) as the optimization method to facilitate automated
calibration. The formulation of the objective function uses experimental
displacement fields measured from Digital Image Correlation (DIC) synchronized
with load cell data and can accommodate data from experiments involving
homogeneous or inhomogeneous deformation fields. The framework places no
restrictions on the target isotropic hyperelastic constitutive model,
accommodating models with coupled dependencies on deformation invariants and
specialized functional forms with a number of material parameters, and assesses
material stability, eliminating sets of material parameters that potentially
lead to non-physical behavior for the target hyperelastic constitutive model.
To minimize the objective function, a GA is deployed as the optimization tool
due to its ability to navigate the intricate landscape of material parameter
space. The VFM-GA framework is evaluated by applying it to a hyperelastic
constitutive model for compressible elastomeric foams. The evaluation process
entails a number of tests that employ both homogeneous and inhomogeneous
displacement fields collected from DIC experiments on open-cell foam specimens.
The results outperform manual fitting, demonstrating the framework's robust and
efficient capability to handle material parameter identification for complex
hyperelastic constitutive models.

</details>


### [35] [Iterated Agent for Symbolic Regression](https://arxiv.org/abs/2510.08317)
*Zhuo-Yang Song,Zeyu Cai,Shutao Zhang,Jiashen Wei,Jichen Pan,Shi Qiu,Qing-Hong Cao,Tie-Jiun Hou,Xiaohui Liu,Ming-xing Luo,Hua Xing Zhu*

Main category: physics.comp-ph

TL;DR: IdeaSearchFitter is a symbolic regression framework that uses LLMs as semantic operators in evolutionary search to generate interpretable mathematical expressions guided by natural-language rationales.


<details>
  <summary>Details</summary>
Motivation: Traditional symbolic regression methods suffer from combinatorial explosion and overfitting, often producing overly complex and uninterpretable models. There's a need for methods that discover both accurate and conceptually coherent expressions.

Method: Uses Large Language Models as semantic operators within evolutionary search to generate candidate expressions guided by natural-language rationales, biasing discovery toward interpretable models.

Result: Achieves competitive noise-robust performance on Feynman Symbolic Regression Database, outperforms baselines, discovers mechanistically aligned models with good accuracy-complexity trade-offs on real-world data, and derives compact physically-motivated parametrizations for Parton Distribution Functions.

Conclusion: IdeaSearchFitter effectively combines LLMs with evolutionary search to discover interpretable symbolic models across diverse scientific domains, demonstrating superior performance and conceptual coherence compared to traditional methods.

Abstract: Symbolic regression (SR), the automated discovery of mathematical expressions
from data, is a cornerstone of scientific inquiry. However, it is often
hindered by the combinatorial explosion of the search space and a tendency to
overfit. Popular methods, rooted in genetic programming, explore this space
syntactically, often yielding overly complex, uninterpretable models. This
paper introduces IdeaSearchFitter, a framework that employs Large Language
Models (LLMs) as semantic operators within an evolutionary search. By
generating candidate expressions guided by natural-language rationales, our
method biases discovery towards models that are not only accurate but also
conceptually coherent and interpretable. We demonstrate IdeaSearchFitter's
efficacy across diverse challenges: it achieves competitive, noise-robust
performance on the Feynman Symbolic Regression Database (FSReD), outperforming
several strong baselines; discovers mechanistically aligned models with good
accuracy-complexity trade-offs on real-world data; and derives compact,
physically-motivated parametrizations for Parton Distribution Functions in a
frontier high-energy physics application. IdeaSearchFitter is a specialized
module within our broader iterated agent framework, IdeaSearch, which is
publicly available at https://www.ideasearch.cn/.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [36] [Does Turbulence at the Correlation Scale Regulate the Statistics of Magnetic Reconnection?](https://arxiv.org/abs/2510.07502)
*M. B. Khan,M. A. Shay,S. Oughton,W. H. Matthaeus,C. C. Haggerty,S. Adhikari,P. A. Cassak,S. Fordin,D. O'Donnell,Y. Yang,R. Bandyopadhyay,S. Roy*

Main category: physics.plasm-ph

TL;DR: Reconnection rates in MHD turbulence are strongly correlated with global turbulent magnetic fields at correlation scale, not dissipation scales, leading to much higher energy dissipation than previously thought.


<details>
  <summary>Details</summary>
Motivation: To understand how magnetic reconnection events behave within strong background MHD turbulence and their role in energy dissipation in astrophysical and heliospheric contexts.

Method: Direct numerical simulations of magnetic reconnection events embedded in MHD turbulence, analyzing relationships between reconnection properties and global turbulent field statistics.

Result: Reconnection rate distribution is strongly correlated with magnitude of global turbulent magnetic field at correlation scale, not dissipation/kinetic scales. Average reconnection and dissipation rates are much larger than predicted by turbulent magnetic field fluctuations at smaller scales.

Conclusion: Magnetic reconnection likely plays a major role in energy dissipation in astrophysical and heliospheric turbulence due to its strong correlation with global turbulent fields.

Abstract: We study the statistics of dynamical quantities associated with magnetic
reconnection events embedded in a sea of strong background magnetohydrodynamic
(MHD) turbulence using direct numerical simulations. We focus on the
relationship of the reconnection properties to the statistics of global
turbulent fields. For the first time, we show that the distribution in
turbulence of reconnection rates (determined by upstream fields) is strongly
correlated with the magnitude of the global turbulent magnetic field at the
correlation scale. The average reconnection rates, and associated dissipation
rates, during turbulence are thus much larger than predicted by using turbulent
magnetic field fluctuation amplitudes at the dissipation or kinetic scales.
Magnetic reconnection may therefore be playing a major role in energy
dissipation in astrophysical and heliospheric turbulence.

</details>


### [37] [Full-wave computation of SUb-atmospheric Radio-frequency Engine (SURE)](https://arxiv.org/abs/2510.07849)
*Dingzhou Li,Lei Chang,Ye Tao*

Main category: physics.plasm-ph

TL;DR: This study uses computer simulations to analyze how gas pressure, input power, frequency, and gas types affect inductively coupled plasma in a quartz tube for near-space electric propulsion systems, comparing single-turn and five-turn antenna designs.


<details>
  <summary>Details</summary>
Motivation: Near-space vehicles need efficient propulsion systems, and previous experiments lacked sufficient understanding of how parameters affect power absorption and electromagnetic behavior in plasma-based electric propulsion systems.

Method: Computer simulations were used to examine the effects of gas pressure (200-1000 Pa), input power (200-600 W), frequency (13.56-52.24 MHz), and different gas types (Ar, N₂, H₂, He) on inductively coupled plasma, with focus on comparing single-turn and five-turn antenna designs.

Result: Single-turn antenna consistently absorbed power better than five-turn antenna. Higher frequencies significantly influence plasma power absorption and magnetic field characteristics. Optimal power absorption occurs at 400 Pa gas pressure. Power absorption shows initial decrease then increasing trend with input power. Molecular gases show reduced plasma power absorption efficiency due to inelastic collision mechanisms.

Conclusion: The simulation results provide guidance for designing future experiments for electric propulsion concepts in near-space vehicles.

Abstract: Near-space, which covers altitudes from 20 to 100 kilometers, has been
receiving more and more attention because of its special strategic value.
Airships and high-altitude balloons are two common types of low-speed vehicles
that operate in this region. They can be used for jobs like monitoring,
communication, and remote sensing, but they need efficient propulsion systems
to work well. Earlier, we proposed a new type of electric propulsion system
that can ionize the surrounding air to create plasma and produce thrust for
near-space vehicles. However, in past experiments, not enough was known about
how certain parameters affect power absorption and electromagnetic behavior.
Therefore, in this study, we used computer simulations to examine how gas
pressure (200 to 1000 Pa), input power (200 to 600 W), frequency (13.56 to
52.24 MHz), and different gas types ($Ar$, $N_2$, $H_2$, $He$) influence
inductively coupled plasma inside a quartz tube. We especially focused on
comparing two antenna designs: one with a single turn and one with five turns.
In all the simulations, the single-turn antenna consistently absorbed power
better than the five-turns antenna. Higher frequencies significantly influence
both plasma power absorption and magnetic field characteristics. The optimal
power absorption occurs at a filling gas pressure of 400 Pa. When varying the
input power, we observed an initial decrease followed by an increasing trend,
which may be related to ionization mechanisms. In comparisons among different
gas types, the inelastic collision mechanisms in molecular gases lead to a
notable reduction in plasma power absorption efficiency. The results from this
work will help guide the design of future experiments for this electric
propulsion concept.

</details>


### [38] [Ion Stochastic Heating by Low-frequency Alfvén Wave Spectrum](https://arxiv.org/abs/2510.07875)
*Jingyu Peng,Jiansen He*

Main category: physics.plasm-ph

TL;DR: The paper studies nonlinear interactions between oblique Alfvén wave spectra and ions, showing that increasing wave modes leads to chaotic ion motion and stochastic heating, characterized by an effective relative curvature radius parameter.


<details>
  <summary>Details</summary>
Motivation: To understand how finite-amplitude low-frequency Alfvén waves in space plasmas contribute to ion heating through nonlinear wave-particle interactions.

Method: Analyzed nonlinear interaction between oblique Alfvén wave spectra and ions using test particle simulations and developed a theoretical framework with an effective relative curvature radius parameter.

Result: Found that stochastic heating threshold is characterized by Peff parameter, showing excellent agreement with chaotic regions in simulations. Derived stochastic heating rate formula Q/(Ωi mi vA²) = H(α)ṽ³B̃w²ω̃₁.

Conclusion: The study provides a comprehensive framework for understanding stochastic ion heating by Alfvén waves, with the effective curvature radius parameter successfully characterizing heating thresholds and the derived heating rate formula quantifying the relationship with wave conditions.

Abstract: Finite-amplitude low-frequency Alfv\'en waves are commonly found in plasma
environments, such as space plasmas, and play a crucial role in ion heating.
The nonlinear interaction between oblique Alfv\'en wave spectra and ions has
been studied. As the number of wave modes increases, ions are more likely to
exhibit chaotic motion and experience stochastic heating. The stochastic
heating threshold in the parameter space can be characterized by a single
parameter, the effective relative curvature radius $P_{{eff.}}$. The results
show excellent agreement with the chaotic regions identified through test
particle simulations. The anisotropic characteristics of stochastic heating are
explained using a uniform solid angle distribution model. The stochastic
heating rate $Q=\dot{T}$ is calculated, and its relationship with wave
conditions is expressed as $Q/(\Omega_i m_i v_A^2) = H(\alpha) \tilde{v}^3
\tilde{B}_w^2 \tilde{\omega}_1$, where $\alpha$ is propagating angle,
$\Omega_i$ is the gyrofrequency, $m_i$ is the ion mass, $v_A$ is the Alfv\'en
speed, $\tilde{v}$ is the dimensionless speed, $\tilde{B}_w$ is the
dimensionless wave amplitude, and $\tilde{\omega}_1$ is the lowest
dimensionless wave frequency.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [39] [Non-Hermitian many-body localization in asymmetric chains with long-range interaction](https://arxiv.org/abs/2510.08277)
*Wen Wang,Han-Ze Li,Jian-Xin Zhong*

Main category: cond-mat.dis-nn

TL;DR: Discovery of coexisting static and dynamic spectral real-complex phase transitions with many-body ergodic-localized transitions in long-range non-Hermitian systems, showing distinct properties from disorder-induced localization.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between many-body localization and spectra in non-Hermitian many-body systems, particularly in long-range interacting systems.

Method: Study of a one-dimensional clean, long-range interaction-induced non-Hermitian many-body localization system.

Result: Found coexistence of static and dynamic spectral real-complex phase transitions with many-body ergodic-localized transitions. Phase diagrams show similar non-monotonic boundary trends but do not overlap, differing from conventional disorder-induced non-Hermitian MBL.

Conclusion: The findings provide valuable insights into the relationship between non-Hermitian many-body localization and spectra in long-range interacting systems, with potential experimental realization in cold-atom systems.

Abstract: Understanding the relationship between many-body localization and spectra in
non-Hermitian many-body systems is crucial. In a one-dimensional clean,
long-range interaction-induced non-Hermitian many-body localization system, we
have discovered the coexistence of static and dynamic spectral real-complex
phase transitions, along with many-body ergodic-localized phase transitions.
The phase diagrams of these two types of transitions show similar non-monotonic
boundary trends but do not overlap, highlighting properties distinct from
conventional disorder-induced non-Hermitian many-body localization. We also
propose a potential experimental realization of this model in cold-atom
systems. Our findings provide valuable insights for further understanding the
relationship between non-Hermitian many-body localization and non-Hermitian
spectra in long-range interacting systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [Targeted Digital Twin via Flow Map Learning and Its Application to Fluid Dynamics](https://arxiv.org/abs/2510.07549)
*Qifan Chen,Zhongshu Xu,Jinjin Zhang,Dongbin Xiu*

Main category: cs.LG

TL;DR: A framework for creating targeted digital twins (tDTs) that directly model quantities of interest using memory-based flow map learning from short trajectory data, enabling efficient long-term predictions without full system simulations.


<details>
  <summary>Details</summary>
Motivation: To enable efficient prediction and analysis of specific quantities of interest in digital twins without requiring computationally expensive full system simulations.

Method: Memory-based flow map learning (FML) using short bursts of trajectory data from repeated full digital twin executions to create data-driven models of quantities of interest.

Result: Successfully demonstrated in a CFD example (2D incompressible flow past a cylinder), where tDTs accurately predicted hydrodynamic forces on the cylinder long-term while bypassing full flow simulations.

Conclusion: The proposed tDT framework provides substantial computational savings by enabling efficient long-term dynamics prediction of quantities of interest without requiring full system simulations.

Abstract: We present a numerical framework for constructing a targeted digital twin
(tDT) that directly models the dynamics of quantities of interest (QoIs) in a
full digital twin (DT). The proposed approach employs memory-based flow map
learning (FML) to develop a data-driven model of the QoIs using short bursts of
trajectory data generated through repeated executions of the full DT. This
renders the construction of the FML-based tDT an entirely offline computational
process. During online simulation, the learned tDT can efficiently predict and
analyze the long-term dynamics of the QoIs without requiring simulations of the
full DT system, thereby achieving substantial computational savings. After
introducing the general numerical procedure, we demonstrate the construction
and predictive capability of the tDT in a computational fluid dynamics (CFD)
example: two-dimensional incompressible flow past a cylinder. The QoIs in this
problem are the hydrodynamic forces exerted on the cylinder. The resulting tDTs
are compact dynamical systems that evolve these forces without explicit
knowledge of the underlying flow field. Numerical results show that the tDTs
yield accurate long-term predictions of the forces while entirely bypassing
full flow simulations.

</details>


### [41] [Unveiling the Power of Multiple Gossip Steps: A Stability-Based Generalization Analysis in Decentralized Training](https://arxiv.org/abs/2510.07980)
*Qinglun Li,Yingqi Liu,Miao Zhang,Xiaochun Cao,Quanjun Yin,Li Shen*

Main category: cs.LG

TL;DR: Multi-Gossip Steps (MGS) bridges decentralized and centralized training, reducing performance gaps but cannot fully eliminate them. Theoretical analysis shows MGS reduces optimization error exponentially but leaves a non-negligible generalization gap compared to centralized training.


<details>
  <summary>Details</summary>
Motivation: Decentralized training is communication-efficient but suffers from performance degradation compared to centralized training. MGS helps bridge this gap, but the theoretical reasons for its effectiveness and whether it can fully eliminate the gap remained unknown.

Method: Used stability analysis to derive upper bounds on generalization error and excess error of MGS. Provided unified analysis of factors like learning rate, data heterogeneity, node count, per-node sample size, and communication topology impact under non-convex settings without bounded gradients assumption.

Result: 1) MGS reduces optimization error bound at exponential rate, tightening generalization error bound. 2) Even with infinite MGS, a non-negligible generalization gap remains compared to centralized SGD. Experiments on CIFAR datasets support theoretical findings.

Conclusion: MGS significantly improves decentralized training performance but cannot fully close the gap to centralized training. The analysis provides theoretical foundation for understanding MGS effectiveness and limitations in decentralized learning.

Abstract: Decentralized training removes the centralized server, making it a
communication-efficient approach that can significantly improve training
efficiency, but it often suffers from degraded performance compared to
centralized training. Multi-Gossip Steps (MGS) serve as a simple yet effective
bridge between decentralized and centralized training, significantly reducing
experiment performance gaps. However, the theoretical reasons for its
effectiveness and whether this gap can be fully eliminated by MGS remain open
questions. In this paper, we derive upper bounds on the generalization error
and excess error of MGS using stability analysis, systematically answering
these two key questions. 1). Optimization Error Reduction: MGS reduces the
optimization error bound at an exponential rate, thereby exponentially
tightening the generalization error bound and enabling convergence to better
solutions. 2). Gap to Centralization: Even as MGS approaches infinity, a
non-negligible gap in generalization error remains compared to centralized
mini-batch SGD ($\mathcal{O}(T^{\frac{c\beta}{c\beta +1}}/{n m})$ in
centralized and $\mathcal{O}(T^{\frac{2c\beta}{2c\beta +2}}/{n
m^{\frac{1}{2c\beta +2}}})$ in decentralized). Furthermore, we provide the
first unified analysis of how factors like learning rate, data heterogeneity,
node count, per-node sample size, and communication topology impact the
generalization of MGS under non-convex settings without the bounded gradients
assumption, filling a critical theoretical gap in decentralized training.
Finally, promising experiments on CIFAR datasets support our theoretical
findings.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [42] [Sharp Non-uniqueness in Law for Stochastic Differential Equations on the Whole Space](https://arxiv.org/abs/2510.08248)
*Huaxiang Lü,Michael Röckner*

Main category: math.PR

TL;DR: The paper constructs divergence-free drift fields in L_t^rL^p spaces that produce non-unique weak solutions to stochastic differential equations on R^d for d≥2.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that the well-known uniqueness results for strong solutions with regular drifts (C_tL^{d+}) are sharp, by showing non-uniqueness can occur for less regular drifts satisfying d/p + 1/r > 1.

Method: Constructs two distinct probability solutions to the Fokker-Planck equation using convex integration methods adapted to all of R^d (not just the torus), combined with refined heat kernel estimates.

Result: For any finite collection of initial measures and drifts in L_t^rL^p with d/p + 1/r > 1, the SDE admits at least two distinct weak solutions. There exists a measurable set A with positive measure where non-uniqueness occurs.

Conclusion: The results are sharp in view of known uniqueness for more regular drifts, establishing precise conditions for non-uniqueness in stochastic differential equations with irregular drift fields.

Abstract: In this paper, we investigate the stochastic differential equation on
$\mathbb{R}^d,d\geq2$: \begin{align*}
  \dif X_t&=v(t,X_t)\dif t+\sqrt{2} \dif W_t. \end{align*} For any finite
collection of initial probability measures $\{\mu^i_0\}_{1\leq i\leq M}$ on
$\mathbb{R}^d$ and $\frac{d}{p}+\frac{1}{r}>1$, we construct a divergence-free
drift field $v\in L_t^rL^p\cap C_tL^{d-}$ such that the associated SDE admits
at least two distinct weak solutions originating from each initial measure
$\mu^i_0$. This result is sharp in view of the well-known uniqueness of strong
solutions for drifts in $C_tL^{d+}$, as established in \cite{KR05}. As a
corollary, there exists a measurable set $A\subset\mathbb{R}^d$ with positive
Lebesgue measure such that for any $x\in A$, the SDE with drift $v$ admits at
least two weak solutions when with start in $x\in A$. The proof proceeds by
constructing two distinct probability solutions to the associated Fokker-Planck
equation via a convex integration method adapted to all of $\mathbb{R}^d$
(instead of merely the torus), together with refined heat kernel estimate.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [43] [A Geomechanically-Informed Framework for Wellbore Trajectory Prediction: Integrating First-Principles Kinematics with a Rigorous Derivation of Gated Recurrent Networks](https://arxiv.org/abs/2510.07564)
*Shubham Kumar,Anshuman Sahoo*

Main category: physics.geo-ph

TL;DR: This paper presents a geomechanically-informed, data-driven framework for wellbore trajectory prediction using GRU networks, with mathematical derivations of wellbore kinematic models and training algorithms.


<details>
  <summary>Details</summary>
Motivation: Accurate wellbore trajectory prediction is crucial in subsurface engineering but challenging due to complex drilling assembly-geological formation interactions. The research aims to move beyond empirical modeling to a more rigorous, physics-informed approach.

Method: Uses LAS and DEV data from 14 wells in the Gulffaks oil field. Derives wellbore kinematic models from first principles, implements a GRU network with detailed forward propagation and BPTT training algorithm derivation, and includes theoretically justified data preprocessing.

Result: The framework provides comprehensive trajectory prediction capabilities with error analysis using MAE, RMSE, and R2 metrics, though specific numerical results are not provided in the abstract.

Conclusion: The study establishes a mathematically rigorous, geomechanically-informed data-driven surrogate approach for wellbore trajectory prediction that bridges theoretical foundations with practical applications in drilling engineering.

Abstract: Accurate wellbore trajectory prediction is a paramount challenge in
subsurface engineering, governed by complex interactions between the drilling
assembly and heterogeneous geological formations. This research establishes a
comprehensive, mathematically rigorous framework for trajectory prediction that
moves beyond empirical modeling to a geomechanically-informed, data-driven
surrogate approach.The study leverages Log ASCII Standard (LAS) and wellbore
deviation (DEV) data from 14 wells in the Gulfaks oil field, treating
petrophysical logs not merely as input features, but as proxies for the
mechanical properties of the rock that fundamentally govern drilling dynamics.
A key contribution of this work is the formal derivation of wellbore kinematic
models, including the Average Angle method and Dogleg Severity, from the first
principles of vector calculus and differential geometry, contextualizing them
as robust numerical integration schemes. The core of the predictive model is a
Gated Recurrent Unit (GRU) network, for which we provide a complete,
step-by-step derivation of the forward propagation dynamics and the
Backpropagation Through Time (BPTT) training algorithm. This detailed
theoretical exposition, often omitted in applied studies, clarifies the
mechanisms by which the network learns temporal dependencies. The methodology
encompasses a theoretically justified data preprocessing pipeline, including
feature normalization, uniform depth resampling, and sequence generation.
Trajectory post-processing and error analysis are conducted using Mean Absolute
Error (MAE), Root Mean Square Error (RMSE), and the Coefficient of
Determination (R2).

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [44] [Dimension- and Facet-Dependent Altermagnetic Triferroics and Biferroics in CrSb](https://arxiv.org/abs/2510.07771)
*Long Zhang,Guoying Gao*

Main category: cond-mat.mtrl-sci

TL;DR: This paper investigates altermagnetic multiferroics in CrSb, predicting various ferroic properties across different phases and facets, including the discovery of triferroics in WZ-phase (110) facets with tunable magnetic anisotropy.


<details>
  <summary>Details</summary>
Motivation: Altermagnets are gaining interest for their unique spin properties, but altermagnetic multiferroics, especially triferroics, remain rare. The study aims to explore how dimensionality and facet orientation affect ferroic properties in CrSb.

Method: Using first-principles calculations, the researchers analyzed CrSb across multiple phases (NiAs, MnP, wurtzite, zincblende, rocksalt) and different facet orientations to study ferroic properties and magnetic behavior.

Result: Predicted altermagnetism in MnP-phase CrSb, identified AM-ferroelastic biferroics in NiAs- and MnP-phase (110) facets, and discovered FM/AM-ferroelectric-ferroelastic triferroics in WZ-phase (110) facets with moderate switching energy barriers. Magnetic anisotropy was found to be highly tunable.

Conclusion: The work establishes a framework for designing altermagnetic multiferroics through polymorphic, dimensional, and facet engineering, offering promising opportunities for multifunctional spintronic applications.

Abstract: Altermagnets have recently garnered significant interest due to their
vanishing net magnetic moment and non-relativistic momentum-dependent spin
splitting. However, altermagnetic (AM) multiferroics especially triferroics
remain scarce. We investigate the experimentally synthesized non-van der Waals
CrSb as a model system to explore the effects of dimensionality and facet
orientation on its ferroic properties. NiAs, MnP, wurtzite (WZ), zincblende
(ZB), and rocksalt (RS) phases are considered. Using first-principles
calculations, we predict the altermagnetism of CrSb in MnP phase which has
comparable stability with experimental NiAs phase. Both NiAs- and MnP-phase
(110) facets exhibit AM-ferroelastic (FC) biferroics, while the WZ-phase bulk
and (001) facets host ferromagnetic (FM) or AM-ferroelectric (FE) biferroics.
Notably, the WZ-phase (110) facets are identified as FM/AM-FE-FC triferroics,
with moderate energy barriers of 0.129 and 0.363 eV atom-1 for FE and FC
switching, respectively. Both FE and FC switching can reverse the AM spin
splitting in antiferromagnetic (AFM) configurations while preserving the high
spin polarization in FM states. The magnetic anisotropy is highly tunable,
exhibiting either uniaxial or in-plane behavior depending on the phase,
dimension, and facet. This work establishes a framework for designing AM
multiferroics through polymorphic, dimensional, and facet engineering, offering
promising avenues for multifunctional spintronic applications.

</details>


### [45] [Modulating thermal conductivity of bulk BAs based on targeted phonon excitation](https://arxiv.org/abs/2510.07934)
*Tianhao Li,Yangjun Qin,Dongkai Pan,Han Meng,Nuo Yang*

Main category: cond-mat.mtrl-sci

TL;DR: Reversible phonon excitation strategy dynamically modulates BAs thermal conductivity by up to +2% or -35% at high excitation, and +2% or -11% at low excitation, enabling active thermal management.


<details>
  <summary>Details</summary>
Motivation: Address opposing thermal conductivity requirements in electronics (needing high conductivity) and thermoelectrics (needing low conductivity) through dynamic control.

Method: First-principles calculations and Boltzmann transport equation to selectively excite specific phonon modes for active thermal transport control.

Result: Thermal conductivity modulation from -35% to +2% relative to intrinsic 2235 W/mK, depending on excitation frequency, multiplier, and phonon properties. Opposite trends observed at different excitation intensities.

Conclusion: Reversible phonon excitation provides dynamic thermal conductivity tuning for thermal management and thermoelectric applications, with mechanisms involving suppression of Umklapp scattering at low excitation and enhancement of Normal scattering at high excitation.

Abstract: This study proposes a reversible phonon excitation strategy to dynamically
modulate the thermal conductivity of boron arsenide (BAs), addressing the
opposing thermal conductivity requirements in electronics and thermoelectrics.
Using first-principles calculations and Boltzmann transport equation, we
demonstrate that selective excitation of specific phonon modes enables active
control over thermal transport. At an excitation multiplier of 25, the thermal
conductivity of BAs can be enhanced by up to 2% or suppressed by up to 35%
relative to its intrinsic value of 2235 W m^-1 K^-1. At a lower multiplier of
5, thermal conductivity can be increased by 2% or decreased by 11%. The
modulation effect depends on excitation frequency, multiplier, and intrinsic
phonon properties, with certain frequencies exhibiting opposite trends under
different excitation intensities. Mechanistic analysis shows that at low
excitation levels, phonon splitting suppresses Umklapp scattering, reducing the
scattering rate, while at high levels, it enhances Normal scattering,
increasing the scattering rate. This approach offers a dynamic and reversible
route to tuning thermal conductivity, with applications in thermal management
and thermoelectric energy conversion.

</details>


### [46] [Rare-Earth Engineering of NaAlO3 Perovskites Unlocks Unified Optoelectronic, Thermoelectric, and Spintronic Functionalities](https://arxiv.org/abs/2510.08130)
*Muhammad Imran,Sikander Azam,Qaiser Rafiq,Amin Ur Rahman*

Main category: cond-mat.mtrl-sci

TL;DR: Rare-earth doping (Eu3+, Gd3+, Tb3+) in NaAlO3 perovskite reduces band gap from 6.2 eV to ~3.1 eV, enables visible-light absorption, induces half-metallicity, and enhances thermoelectric performance for multifunctional applications.


<details>
  <summary>Details</summary>
Motivation: Wide-gap perovskite oxides like NaAlO3 suffer from deep-UV absorption and limited carrier transport, limiting their applications in energy and quantum technologies. Rare-earth doping is explored to overcome these limitations.

Method: First-principles GGA+U+SOC calculations to investigate electronic, optical, elastic, and thermoelectric properties of Eu3+, Gd3+, and Tb3+-doped NaAlO3.

Result: Doping is thermodynamically favorable (formation energies 1.2-1.6 eV), reduces band gap to ~3.1 eV, induces half-metallicity (Gd), spin-selective metallicity (Eu), and p-type semiconducting behavior (Tb). Optical absorption red-shifts to 2.0-2.2 eV, with large dielectric response and plasmonic resonances. Thermoelectric performance enhanced with Seebeck coefficients >210 uV/K and ZT ~0.45 at 500 K.

Conclusion: Rare-earth-doped NaAlO3 serves as a multifunctional perovskite platform suitable for photovoltaics, photocatalysis, thermoelectrics, and spintronics applications.

Abstract: Perovskite oxides are promising for energy and quantum technologies, but
wide-gap hosts such as NaAlO3 suffer from deep-UV absorption and limited
carrier transport. Using first-principles GGA+U+SOC calculations, we
investigate Eu3+-, Gd3+-, and Tb3+-doped NaAlO3 and evaluate their electronic,
optical, elastic, and thermoelectric properties. Rare-earth substitution is
thermodynamically favorable (formation energies 1.2-1.6 eV) and induces strong
f-p hybridization, reducing the pristine band gap (about 6.2 eV) to about 3.1
eV for Tb. Spin-resolved band structures reveal Gd-driven half-metallicity,
Eu-induced spin-selective metallicity, and Tb-stabilized p-type semiconducting
behavior. The optical spectra show a red-shifted absorption edge (about 2.0-2.2
eV), a large static dielectric response (epsilon1(0) about 95 for Eu), and
plasmonic resonances near 4 eV, enabling visible-light harvesting. Elastic
analysis indicates mild lattice softening with preserved ductility (Pugh ratio
B/G about 1.56-1.57). Thermoelectric performance is enhanced, with Seebeck
coefficients greater than 210 uV/K for Eu and Tb and ZT about 0.45 at 500 K.
These results identify rare-earth-doped NaAlO3 as a multifunctional perovskite
platform for photovoltaics, photocatalysis, thermoelectrics, and spintronics.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [47] [Topology optimization of nonlinear forced response curves via reduction on spectral submanifolds](https://arxiv.org/abs/2510.07900)
*Hongming Liang,Matteo Pozzi,Jacopo Marconi,Shobhit Jain,Mingwu Li*

Main category: eess.SY

TL;DR: The paper presents a method using spectral submanifolds (SSMs) reduction theory to enable efficient topology optimization of nonlinear systems by reformulating periodic responses as equilibria in reduced-order models, allowing analytic computation of responses and sensitivities.


<details>
  <summary>Details</summary>
Motivation: Topology optimization for nonlinear dynamic systems is limited by high computational costs of repeated response and sensitivity analyses, especially for high-dimensional systems with complex behaviors like hardening/softening and bifurcations.

Method: Uses spectral submanifolds (SSMs) reduction theory to create reduced-order models (ROMs) that reformulate periodic responses as equilibria, enabling efficient analytic evaluation of response amplitudes and their sensitivities for optimization.

Result: Successfully applied to nonlinear MEMS device design, achieving targeted performance optimization of peak amplitude, hardening/softening behavior, and distance between saddle-node bifurcations in forced response curves.

Conclusion: The SSM-based ROM framework provides a practical and efficient strategy for incorporating nonlinear dynamic effects into topology optimization of structures.

Abstract: Forced response curves (FRCs) of nonlinear systems can exhibit complex
behaviors, including hardening/softening behavior and bifurcations. Although
topology optimization holds great potential for tuning these nonlinear dynamic
responses, its use in high-dimensional systems is limited by the high cost of
repeated response and sensitivity analyses. To address this challenge, we
employ the spectral submanifolds (SSMs) reduction theory, which reformulates
the periodic response as the equilibria of an associated reduced-order model
(ROM). This enables efficient and analytic evaluation of both response
amplitudes and their sensitivities. Based on the SSM-based ROM, we formulate
optimization problems that optimize the peak amplitude, the hardening/softening
behavior, and the distance between two saddle-node bifurcations for an FRC. The
proposed method is applied to the design of nonlinear MEMS devices, achieving
targeted performance optimization. This framework provides a practical and
efficient strategy for incorporating nonlinear dynamic effects into the
topology optimization of structures.

</details>


### [48] [A Stable, Accurate and Well-Conditioned Time-Domain PMCHWT Formulation](https://arxiv.org/abs/2510.07989)
*Van Chien Le,Cedric Munger,Francesco P. Andriulli,Kristof Cools*

Main category: eess.SY

TL;DR: A new boundary element method for transient electromagnetic scattering using time-domain PMCHWT equations with Calderon preconditioning and quasi-Helmholtz rescaling to address stability issues.


<details>
  <summary>Details</summary>
Motivation: To develop a stable and accurate boundary element formulation for transient electromagnetic scattering by dielectric objects that overcomes dense-mesh breakdown, large-timestep breakdown, and late-time instability issues.

Method: Uses time-domain PMCHWT equation with multiplicative Calderon preconditioner, quasi-Helmholtz projectors for rescaling Helmholtz components, and temporal differentiation/integration operators to balance loop and star components. Solved with marching-on-in-time scheme and iterative solvers.

Result: Numerical experiments on simply- and multiply-connected dielectric scatterers, including highly non-smooth geometries, demonstrate the method's accuracy, stability, and efficiency.

Conclusion: The proposed approach successfully resolves multiple stability issues in transient electromagnetic scattering analysis and provides accurate results for complex dielectric geometries.

Abstract: This paper introduces a new boundary element formulation for transient
electromagnetic scattering by homogeneous dielectric objects based on the
time-domain PMCHWT equation. To address dense-mesh breakdown, a multiplicative
Calderon preconditioner utilizing a modified static electric field integral
operator is employed. Large-timestep breakdown and late-time instability are
simultaneously resolved by rescaling the Helmholtz components leveraging the
quasi-Helmholtz projectors and using temporal differentiation and integration
as rescaling operators. This rescaling also balances the loop and star
components at large timesteps, improving solution accuracy. The resulting
discrete system is solved using a marching-on-in-time scheme and iterative
solvers. Numerical experiments for simply- and multiply-connected dielectric
scatterers, including highly non-smooth geometries, corroborate the accuracy,
stability, and efficiency of the proposed approach.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [49] [Stress concentration via quasi-Minnaert resonance in bubble-elastic structures and applications](https://arxiv.org/abs/2510.06892)
*Ruixiang Tang,Huaian Diao,Hongyu Liu,Weisheng Zhou*

Main category: math-ph

TL;DR: This paper analyzes stress concentration in bubbly-elastic structures using quasi-Minnaert resonance, demonstrating how to induce stress concentration near bubble boundaries through appropriate wave selection and high-contrast structures.


<details>
  <summary>Details</summary>
Motivation: Stress concentration in bubble-elastic scattering has important applications in engineering blasting and medical treatments, motivating the need to understand and control these phenomena.

Method: The study employs layer potential theory and asymptotic analysis to rigorously analyze stress concentration and quasi-Minnaert resonance in a radially symmetric bubble-elastic model, with extensive numerical experiments on various bubble geometries.

Result: The research demonstrates that quasi-Minnaert resonance manifests as two wave patterns (boundary localization and high-oscillation) and can be leveraged to induce stress concentration near bubble boundaries through appropriate incident wave selection.

Conclusion: The findings enhance understanding of stress concentration mechanisms and their applications in engineering blasting and medical therapies, providing mathematical tools to control these phenomena.

Abstract: Stress concentration in bubble-elastic scattering scenarios has significant
applications in engineering blasting and medical treatments. This study
provides a comprehensive mathematical analysis of stress concentration in
bubbly-elastic structures, induced by the quasi-Minnaert resonance. The
quasi-Minnaert resonance manifests as two distinct wave patterns near the
bubble's boundary: boundary localization and high-oscillation phenomena. We
demonstrate how to leverage the quasi-Minnaert resonance to induce stress
concentration in the elastic total wave field near the air bubble's boundary by
appropriately selecting the incident elastic wave and high-contrast structure.
The interaction between the air bubble and the elastic background couples two
physical wave fields-acoustic and elastic waves-across the bubble's boundary.
The intricate transmission conditions, combined with the scalar nature of
acoustic waves and the vectorial nature of elastic waves, present significant
analytical challenges. To address these, we employ layer potential theory and
asymptotic analysis to rigorously establish the stress concentration and
quasi-Minnaert resonance phenomena in a radially geometry bubble-elastic model.
Extensive numerical experiments are conducted to demonstrate the stress
concentration phenomenon alongside quasi-Minnaert resonance for various bubble
geometries, including a unit disk, a corner domain, an apple-shaped domain in
$\mathbb{R}^2$, and a ball in $\mathbb{R}^3$. The findings of this study
enhance the understanding of stress concentration mechanisms and their
applications in engineering blasting and medical therapies.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [50] [Quantum Grid Path Planning Using Parallel QAOA Circuits Based on Minimum Energy Principle](https://arxiv.org/abs/2510.07413)
*Jun Liu*

Main category: quant-ph

TL;DR: A quantum path planning solution using parallel QAOA architecture that maps grid path planning to finding minimum quantum energy states, with two parallel circuits for connectivity and path energy calculations, filtered by classical algorithms to find optimal paths.


<details>
  <summary>Details</summary>
Motivation: To overcome NP problem bottlenecks in classical path planning and address limitations of quantum path planning frameworks in the NISQ era by developing a more efficient quantum solution.

Method: Map grid path planning to minimum quantum energy state problem; build two parallel QAOA circuits for connectivity energy and path energy calculations; use classical filtering to remove unreasonable solutions; merge results from both circuits.

Result: With appropriate filter parameters, quantum states with low occurrence probabilities can be filtered out, increasing target state probability. Even with only 1 circuit layer (p=1), optimal path coding combinations can be found using the filter's critical role.

Conclusion: Parallel QAOA circuits significantly outperform serial circuits, finding optimal feasible path coding combinations with the highest probability, demonstrating effective quantum-classical hybrid approach for path planning.

Abstract: To overcome the bottleneck of classical path planning schemes in solving NP
problems and address the predicament faced by current mainstream quantum path
planning frameworks in the Noisy Intermediate-Scale Quantum (NISQ) era, this
study attempts to construct a quantum path planning solution based on parallel
Quantum Approximate Optimization Algorithm (QAOA) architecture. Specifically,
the grid path planning problem is mapped to the problem of finding the minimum
quantum energy state. Two parallel QAOA circuits are built to simultaneously
execute two solution processes, namely connectivity energy calculation and path
energy calculation. A classical algorithm is employed to filter out
unreasonable solutions of connectivity energy, and finally, the approximate
optimal solution to the path planning problem is obtained by merging the
calculation results of the two parallel circuits. The research findings
indicate that by setting appropriate filter parameters, quantum states
corresponding to position points with extremely low occurrence probabilities
can be effectively filtered out, thereby increasing the probability of
obtaining the target quantum state. Even when the circuit layer number p is
only 1, the theoretical solution of the optimal path coding combination can
still be found by leveraging the critical role of the filter. Compared with
serial circuits, parallel circuits exhibit a significant advantage, as they can
find the optimal feasible path coding combination with the highest probability.

</details>


### [51] [When Less is More: Approximating the Quantum Geometric Tensor with Block Structures](https://arxiv.org/abs/2510.08430)
*Ahmedeo Shokry,Alessandro Santini,Filippo Vicentini*

Main category: quant-ph

TL;DR: A block-diagonal quantum geometric tensor is introduced to reduce computational cost of natural gradient optimization in neural quantum states by partitioning the metric by network layers.


<details>
  <summary>Details</summary>
Motivation: The natural gradient is important for neural quantum states optimization but limited by expensive computation and inversion of the quantum geometric tensor.

Method: Introduce a block-diagonal quantum geometric tensor that partitions the metric by network layers, similar to block-structured Fisher methods like K-FAC, preserving curvature while removing cross-layer correlations.

Result: Experiments on Heisenberg and frustrated J1-J2 models show faster convergence, lower energy, and improved stability.

Conclusion: The layer-wise approximation improves conditioning and scalability while maintaining essential curvature information.

Abstract: The natural gradient is central in neural quantum states optimizations but it
is limited by the cost of computing and inverting the quantum geometric tensor,
the quantum analogue of the Fisher information matrix. We introduce a
block-diagonal quantum geometric tensor that partitions the metric by network
layers, analogous to block-structured Fisher methods such as K-FAC. This
layer-wise approximation preserves essential curvature while removing noisy
cross-layer correlations, improving conditioning and scalability. Experiments
on Heisenberg and frustrated $J_1$-$J_2$ models show faster convergence, lower
energy, and improved stability.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [52] [Time-causal and time-recursive wavelets](https://arxiv.org/abs/2510.05834)
*Tony Lindeberg*

Main category: eess.SP

TL;DR: This paper presents a time-causal wavelet analysis method based on temporal scale-space theory, using truncated exponential kernels and their derivatives to enable real-time signal processing without accessing future data.


<details>
  <summary>Details</summary>
Motivation: To enable wavelet analysis for real-time temporal signals where future data cannot be accessed, requiring truly time-causal computational mechanisms that are physically realistic.

Method: Based on temporal scale-space theory, using convolution with truncated exponential kernels in cascade and their temporal derivatives to fulfill wavelet admissibility conditions. The time-causal limit kernel ensures temporal scale covariance and self-similarity.

Result: Developed a time-causal wavelet representation that can reflect the duration of locally dominant temporal structures in input signals, with continuous scaling properties transferring to discrete implementation.

Conclusion: The proposed time-causal wavelet analysis is a valuable tool for real-time signal processing tasks, particularly for signals with local variations across multiple temporal scales or for analyzing physical/biophysical phenomena requiring fully time-causal analysis.

Abstract: When to apply wavelet analysis to real-time temporal signals, where the
future cannot be accessed, it is essential to base all the steps in the signal
processing pipeline on computational mechanisms that are truly time-causal.
  This paper describes how a time-causal wavelet analysis can be performed
based on concepts developed in the area of temporal scale-space theory,
originating from a complete classification of temporal smoothing kernels that
guarantee non-creation of new structures from finer to coarser temporal scale
levels. By necessity, convolution with truncated exponential kernels in cascade
constitutes the only permissable class of kernels, as well as their temporal
derivatives as a natural complement to fulfil the admissibility conditions of
wavelet representations. For a particular way of choosing the time constants in
the resulting infinite convolution of truncated exponential kernels, to ensure
temporal scale covariance and thus self-similarity over temporal scales, we
describe how mother wavelets can be chosen as temporal derivatives of the
resulting time-causal limit kernel.
  By developing connections between wavelet theory and scale-space theory, we
characterize and quantify how the continuous scaling properties transfer to the
discrete implementation, demonstrating how the proposed time-causal wavelet
representation can reflect the duration of locally dominant temporal structures
in the input signals.
  We propose that this notion of time-causal wavelet analysis could be a
valuable tool for signal processing tasks, where streams of signals are to be
processed in real time, specifically for signals that may contain local
variations over a rich span of temporal scales, or more generally for analysing
physical or biophysical temporal phenomena, where a fully time-causal analysis
is called for to be physically realistic.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [53] [Minimal Denominators Lying in Subsets of the Ring of Polynomials over a Finite Field](https://arxiv.org/abs/2510.07787)
*Noy Soffer Aranov*

Main category: math.NT

TL;DR: The paper proves that for polynomials over finite fields, the distribution functions of smallest denominators in rational approximations are exactly equal for both continuous and discrete settings, unlike the real number case where they are only asymptotically close.


<details>
  <summary>Details</summary>
Motivation: To understand the distribution of smallest denominators in rational approximations over finite fields, comparing continuous and discrete settings, and showing stronger results than in the real number case.

Method: Studying the distribution of smallest denominators Q in subsets of polynomials over finite fields, for both continuous approximations (balls around points) and discrete approximations (polynomial denominators of fixed degree).

Result: Proved that for any infinite subset of polynomials, any degree n, and any dimension m, the probability distribution functions of both random variables (continuous and discrete) are exactly equal.

Conclusion: The equality of distribution functions in finite field settings is significantly stronger than the real number case, where only asymptotic closeness of averages was previously established.

Abstract: Given a subset $\mathcal{S}\subseteq \mathbb{F}_q[x]$ and fixed $n,m\in
\mathbb{N}$, one can study the distribution of the value of the smallest
denominator $Q\in \mathcal{S}$, for which there exists $\mathbf{P}\in
\mathbb{F}_q[x]^m$ such that $\frac{P}{Q}\in B(\boldsymbol{\alpha},q^{-n})$,
where $Q\in \mathcal{S}$. On the other hand, one can study the discrete
analogue, when $N\in \mathbb{F}_q[x]$ is a polynomial with $\deg(N)=n$ and
$\boldsymbol{\alpha}\in \frac{1}{N}\mathbb{F}_q[x]^m$ as a discrete probability
distribution function. We prove that for any infinite subset
$\mathcal{S}\subseteq \mathbb{F}_q[x]$, for any $n\in \mathbb{N}$, and for any
dimension $m$, the probability distribution functions of both these random
variables are equal to one another. This is significantly stronger than the
real setting, where Balazard and Martin proved that these functions have
asymptotically close averages, when there are no restrictions on the
denominators.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [54] [Scalings and simulation requirements in two-phase flows](https://arxiv.org/abs/2510.07727)
*Luis H. Hatashita,Pranav Nathan,Suhas S. Jain*

Main category: physics.flu-dyn

TL;DR: This paper derives scaling laws for grid resolution and time-step requirements in two-phase flow DNS, establishing criteria for adequate resolution based on Re, We, and Ca numbers, with computational cost implications.


<details>
  <summary>Details</summary>
Motivation: There are established standards for DNS in single-phase turbulence flows, but similar guidance is lacking for two-phase flows, creating a need for systematic resolution criteria.

Method: High-fidelity simulations of stationary two-phase homogeneous isotropic turbulence were used to evaluate convergence of interfacial area, size distribution, SMD, and curvature distribution. Length scale ratios were constructed for different turbulence regimes.

Result: Grid resolution of k_max η_KH ≥ 60 for second-order schemes is required to capture intermittent events and ensure convergence. Computational costs increase as We_L^{12/5} in inertia-dominated regime and Ca_L^4 in viscous-dominated regime.

Conclusion: The proposed scalings and resolution criteria provide valuable tools for determining grid resolution and time-step requirements a-priori for DNS of two-phase flows, accelerating physics discovery and model development.

Abstract: In this work, important two-phase flow scalings are derived, which enable the
quantification of grid-point and time-step requirements as functions of Re, We,
and Ca numbers. The adequate grid resolution is determined in the
inertia-dominated regime with the aid of high-fidelity simulations of
stationary two-phase homogeneous isotropic turbulence by evaluating convergence
of total interfacial area, size distribution, SMD, and curvature distribution.
Although standards for DNS for single-phase turbulence flow exist, there is a
lack of similar guidance in two-phase flows. Therefore, length scale ratios of
the Kolmogorov-Hinze to the Kolmogorov scale of \eta_{KH}/\eta \sim
We_{L}^{-3/5}Re_{L}^{3/4} in the inertia-dominated regime and the
Kolmogorov-viscous to Kolmogorov scale of \eta_{KV}/\eta \sim
Ca_{L}^{-1}Re_{L}^{3/4} for the viscous-dominated regime, are constructed.
These scalings imply a computational cost increase like We_{L}^{12/5} and
Ca_{L}^4, in the inertia-dominated and viscous-dominated regimes, respectively.
A novel dimensionless number, coined as the ratio of interface scales (Ris), is
proposed to aid in the classification of the turbulence regimes in the presence
of an interface. Convergence of the total interfacial area, size distribution,
SMD, and curvature distribution are observed for grid resolutions of k_{\max}
\eta_{KH} \geq 60$ for second-order schemes. Furthermore, it is observed that
this lower bound is the minimum required to capture intermittent events
responsible for the increase of instantaneous total interfacial area. This
criterion will be a valuable tool for determining grid resolution and time-step
requirements a-priori for DNS of two-phase flows and for estimating the
corresponding computational cost. This work provides guidelines and best
practices for numerical simulations of two-phase flows, which will accelerate
physics discovery and model development.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [55] [Asymptotically flat black holes with a singular Cauchy horizon and a spacelike singularity](https://arxiv.org/abs/2510.07431)
*Maxime Van de Moortel*

Main category: gr-qc

TL;DR: Construction of asymptotically flat black hole spacetimes with coexisting null and spacelike singularities using Einstein-Maxwell-scalar-field equations, featuring both one-ended and two-ended configurations.


<details>
  <summary>Details</summary>
Motivation: To provide concrete examples of black holes with coexisting null and spacelike singularities, building on previous theoretical work that analyzed the transition between these singularity types in spherically symmetric dynamical black holes.

Method: Uses a new spacelike-characteristic gluing method to connect uncharged spherically symmetric solutions with charged dynamical black hole event horizons, constructing both one-ended and two-ended asymptotically flat black hole spacetimes.

Result: Successfully constructed the first examples of black holes with coexisting null and spacelike singularities, showing the terminal boundary has only two components: a weakly singular null Cauchy horizon and a strong singularity at r=0.

Conclusion: These constructions provide significant examples of black holes with coexisting singularity types, particularly important for the one-ended case as a model of gravitational collapse where this phenomenon is conjectured to be generic even beyond spherical symmetry.

Abstract: In our recent work [Van de Moortel, The coexistence of null and spacelike
singularities inside spherically symmetric black holes], we analyzed the
transition between null and spacelike singularities in spherically symmetric
dynamical black holes and demonstrated that the spacelike portion is described
by a Kasner metric with positive varying exponents that degenerate to $(1,0,0)$
near the null-spacelike transition. In the present paper, we provide examples
of global spacetimes satisfying the assumptions of this previous result and
apply its analysis to obtain a large class of asymptotically flat (spherically
symmetric) black hole spacetimes that exhibit coexisting null and spacelike
singularities. Our main results include:
  _The construction of one-ended asymptotically flat black hole spacetimes
solving the Einstein-Maxwell-charged-scalar-field equations. The proof relies
on a new spacelike-characteristic gluing method between any uncharged
spherically symmetric solution and the event horizon of a charged dynamical
black hole.
  _The construction of a large class of two-ended asymptotically flat black
hole spacetimes solving the Einstein-Maxwell-(uncharged)-scalar-field
equations.
  In both cases, we show that the terminal boundary in the black hole interior
only has two distinct components: a weakly singular (null) Cauchy horizon
$\mathcal{CH}_{i^+}$ where curvature blows up and a strong singularity
$\mathcal{S}=\{r=0\}$.
  Our construction provides the first examples of black holes with coexisting
null and spacelike singularities. These examples hold particular significance
in the one-ended case as a model of gravitational collapse, where this
phenomenon is conjecturally generic for the Einstein-scalar-field model, even
beyond spherical symmetry.

</details>
