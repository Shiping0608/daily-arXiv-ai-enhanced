<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 17]
- [math.AP](#math.AP) [Total: 16]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [eess.SP](#eess.SP) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [math.PR](#math.PR) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [math.NT](#math.NT) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Determination of Range Conditions for General Projection Pair Operators](https://arxiv.org/abs/2510.07480)
*Richard Huber,Rolf Clackdoyle,Laurent Desbat*

Main category: math.NA

TL;DR: This paper analyzes the range of projection pair operators in tomography, focusing on operators formed by two projections in the plane. It develops kernel conditions to characterize the range and applies this theory to the exponential fanbeam transform and mixed parallel-fanbeam operators.


<details>
  <summary>Details</summary>
Motivation: Characterizing the range of N-projections operators is important for mathematical understanding and practical applications in medical imaging, including geometric calibration, motion detection, and model parameter identification. No range conditions were previously known for the exponential fanbeam transform.

Method: The authors investigate projection pair operators (N=2) in the plane, finding that the set of annihilators to the range contains at most one dimension. They develop explicit kernel conditions to determine range conditions for such operators.

Result: For the exponential fanbeam transform, the analysis shows that no regular range condition exists - arbitrary data can be approximated in an L² sense by projections of smooth functions. The theory is also demonstrated on mixed parallel-fanbeam projection pairs.

Conclusion: The developed kernel conditions provide a powerful tool for characterizing the range of projection pair operators, with applications to various tomographic transforms including the previously uncharacterized exponential fanbeam transform.

Abstract: Tomographic techniques are vital in modern medicine, allowing doctors to
observe patients' interior features. Individual steps in the measurement
process are modeled by `single projection operators' $p$. These are line
integral operators over a collection of curves that covers the regions of
interest. Then, the entire measurement process can be understood as a finite
collection of such single projections, and thus be modeled by an
$N$-projections operator $P=(p_1,\dots,p_N)$. The most well-known example of an
$N$-projections operator is the restriction of the Radon transform to finitely
many projection angles. Characterizations of the range of $N$-projections
operators are of intrinsic mathematical interest and can also help in practical
applications such as geometric calibration, motion detection, or model
parameter identification. In this work, we investigate the range of projection
pair operators $\mathcal{P}$ in the plane, i.e., operators formed by two
projections ($N=2$) applied to functions in $\mathbb{R}^2$. We find that the
set of annihilators to $\mathrm{rg}(\mathcal{P})$ that are regular
distributions contains at most one dimension and a range condition can be
explicitly determined by what we refer to as `kernel conditions'. With this
tool, we examine the exponential fanbeam transform for which no range
conditions were known, finding that no (regular) range condition exists, and
therefore, arbitrary data can be approximated in an $L^2$ sense by projections
of smooth functions. We also illustrate the use of this theory on a mixed
parallel-fanbeam projection pair operator.

</details>


### [2] [A control-based spatial source reconstruction in fractional heat equations](https://arxiv.org/abs/2510.07528)
*Galina García,Joaquín Vidal,Sebastián Zamorano*

Main category: math.NA

TL;DR: This paper solves the inverse source problem for fractional heat equations by reconstructing spatial sources from partial measurements using Fourier analysis and controllability properties.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reconstructing spatial source terms in nonlocal heat equations with fractional Laplacian from limited measurement data, which has applications in various physical and engineering problems.

Method: The method uses spectral analysis and Volterra integral equations to derive a reconstruction formula for Fourier coefficients of the unknown source, leveraging the null controllability property of fractional heat equations for orders s∈(1/2,1).

Result: Numerical experiments demonstrate that the proposed approach achieves accurate and stable reconstruction of spatial source terms from partial observations of the system state and its time derivative.

Conclusion: The developed methodology provides a robust framework for solving inverse source problems in fractional heat equations, enabling reliable source reconstruction with limited measurement data through spectral analysis and controllability theory.

Abstract: This article addresses the inverse source problem for a nonlocal heat
equation involving the fractional Laplacian. The primary goal is to reconstruct
the spatial component of the source term from partial observations of the
system's state and its time derivative over a subset of the domain. A
reconstruction formula for the Fourier coefficients of the unknown source is
derived, leveraging the null controllability property of the fractional heat
equation when the fractional order lies in the interval $s\in(1/2,1)$. The
methodology builds on spectral analysis and Volterra integral equations,
providing a robust framework for recovering spatial sources under limited
measurement data. Numerical experiments confirm the accuracy and stability of
the proposed approach.

</details>


### [3] [Semi-implicit strategies for the Serre-Green-Naghdi equations in hyperbolic form. Is hyperbolic relaxation really a good idea?](https://arxiv.org/abs/2510.07539)
*Emanuele Macca,Walter Boscheri,Mario Ricchiuto*

Main category: math.NA

TL;DR: A semi-implicit integration strategy for hyperbolic Serre-Green-Naghdi equations that treats stiff acoustic terms implicitly while keeping advective terms explicit, overcoming stability limitations of explicit schemes.


<details>
  <summary>Details</summary>
Motivation: The elliptic formulation of SGN equations increases computational cost compared to Saint-Venant equations, and explicit schemes for hyperbolic models face restrictive stability constraints as relaxation parameters increase.

Method: Semi-implicit IMEX Runge-Kutta framework where stiff acoustic terms are treated implicitly and advective components remain explicit.

Result: The approach mitigates CFL stability restrictions, maintains dispersive accuracy at moderate computational cost, and provides efficient alternative to classical SGN and fully explicit hSGN solvers.

Conclusion: Combining hyperbolization with semi-implicit time integration offers an efficient and accurate solution for fully nonlinear and weakly dispersive shallow-water flow modeling.

Abstract: The Serre-Green-Naghdi (SGN) equations provide a valuable framework for
modelling fully nonlinear and weakly dispersive shallow-water flows. However,
their elliptic formulation can considerably increase the computational cost
compared to the Saint-Venant equations. To overcome this difficulty, hyperbolic
models (hSGN) have been proposed that replace the elliptic operators with
first-order hyperbolic formulations augmented by relaxation terms, which
recover the original elliptic formulation in the stiff limit. Yet, as the
relaxation parameter \lambda increases, explicit schemes face restrictive
stability constraints that may offset these advantages. To mitigate this
limitation, we introduce a semi-implicit (SI) integration strategy for the hSGN
system, where the stiff acoustic terms are treated implicitly within an IMEX
Runge-Kutta framework, while the advective components remain explicit. The
proposed approach mitigates the CFL stability restriction and maintains
dispersive accuracy at a moderate computational cost. Numerical results confirm
that the combination of hyperbolization and semi-implicit time integration
provides an efficient and accurate alternative to both classical SGN and fully
explicit hSGN solvers.

</details>


### [4] [Stochastic Gradient Descent for Incomplete Tensor Linear Systems](https://arxiv.org/abs/2510.07630)
*Anna Ma,Deanna Needell,Alexander Xue*

Main category: math.NA

TL;DR: This paper extends stochastic gradient descent methods for solving tensor linear systems with missing data beyond uniform missing patterns.


<details>
  <summary>Details</summary>
Motivation: Solving large tensor linear systems is challenging, especially with missing data. Previous methods assumed uniform missing patterns, which limits practical applicability.

Method: Adapted stochastic gradient descent by modifying the update direction to handle various missing data models beyond uniform patterns.

Result: Proved convergence results for the modified method and experimentally verified them on synthetic data.

Conclusion: The modified stochastic gradient descent approach successfully handles tensor linear systems with non-uniform missing data patterns.

Abstract: Solving large tensor linear systems poses significant challenges due to the
high volume of data stored, and it only becomes more challenging when some of
the data is missing. Recently, Ma et al. showed that this problem can be
tackled using a stochastic gradient descent-based method, assuming that the
missing data follows a uniform missing pattern. We adapt the technique by
modifying the update direction, showing that the method is applicable under
other missing data models. We prove convergence results and experimentally
verify these results on synthetic data.

</details>


### [5] [Parallel-in-Time Solution of Allen-Cahn Equations by Integrating Operator Learning into the Parareal Method](https://arxiv.org/abs/2510.07672)
*Yuwei Geng,Junqi Yin,Eric C. Cyr,Guannan Zhang,Lili Ju*

Main category: math.NA

TL;DR: Deep learning-based coarse propagator integrated with Parareal method accelerates time-dependent PDE simulations while maintaining accuracy, demonstrated on Allen-Cahn equations.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy of deep learning solutions for time-dependent PDEs by ensuring consistency between neural network coarse solvers and traditional fine solvers, especially for PDEs with sharp transitions.

Method: Use convolutional neural networks to learn the fully discrete time-stepping operator from traditional numerical schemes, then integrate as coarse propagator in Parareal algorithm with traditional method as fine solver.

Result: Achieves significant computational speedup compared to traditional solvers while converging to high-accuracy solutions in only a few Parareal iterations, with effective GPU acceleration.

Conclusion: The proposed approach demonstrates successful integration of neural networks into parallel-in-time frameworks for efficient and accurate simulation of time-dependent PDEs, with trained models reusable for various initial conditions.

Abstract: While recent advances in deep learning have shown promising efficiency gains
in solving time-dependent partial differential equations (PDEs), matching the
accuracy of conventional numerical solvers still remains a challenge. One
strategy to improve the accuracy of deep learning-based solutions for
time-dependent PDEs is to use the learned solution as the coarse propagator in
the Parareal method and a traditional numerical method as the fine solver.
However, successful integration of deep learning into the Parareal method
requires consistency between the coarse and fine solvers, particularly for PDEs
exhibiting rapid changes such as sharp transitions. To ensure such consistency,
we propose to use the convolutional neural networks (CNNs) to learn the fully
discrete time-stepping operator defined by the traditional numerical scheme
used as the fine solver. We demonstrate the effectiveness of the proposed
method in solving the classical and mass-conservative Allen-Cahn (AC)
equations. Through iterative updates in the Parareal algorithm, our approach
achieves a significant computational speedup compared to traditional fine
solvers while converging to high-accuracy solutions. Our results highlight that
the proposed Parareal algorithm effectively accelerates simulations,
particularly when implemented on multiple GPUs, and converges to the desired
accuracy in only a few iterations. Another advantage of our method is that the
CNNs model is trained on trajectories-based on random initial conditions, such
that the trained model can be used to solve the AC equations with various
initial conditions without re-training. This work demonstrates the potential of
integrating neural network methods into the parallel-in-time frameworks for
efficient and accurate simulations of time-dependent PDEs.

</details>


### [6] [Ergodicity and error estimate of laws for a random splitting Langevin Monte Carlo](https://arxiv.org/abs/2510.07676)
*Lei Li,Chen Wang,Mengchao Wang*

Main category: math.NA

TL;DR: Analysis of random splitting Langevin Monte Carlo showing it achieves O(τ²) sampling error under Wasserstein distance with geometric ergodicity.


<details>
  <summary>Details</summary>
Motivation: To mitigate first-order bias in Langevin Monte Carlo with minimal computational overhead compared to other high-order schemes.

Method: Developed analysis framework using relative entropy approach, explicit commutator formulas for semi-groups, Bernstein-type PDE estimates for gradient/Hessian bounds, and reflection coupling for geometric ergodicity.

Result: Established uniform-in-time sampling error bound showing invariant measure approximates true Gibbs distribution with O(τ²) accuracy.

Conclusion: Random splitting Langevin Monte Carlo effectively reduces bias to second order while maintaining computational efficiency, validated by numerical experiments.

Abstract: The random splitting Langevin Monte Carlo could mitigate the first order bias
in Langevin Monte Carlo with little extra work compared other high order
schemes. We develop in this work an analysis framework for the sampling error
under Wasserstein distance regarding the random splitting Langevin Monte Carlo.
First, the sharp local truncation error is obtained by the relative entropy
approach together with the explicit formulas for the commutator of related
semi-groups. The necessary pointwise estimates of the gradient and Hessian of
the logarithmic density are established by the Bernstein type approach in PDE
theory. Second, the geometric ergodicity is established by accommodation of the
reflection coupling. Combining the ergodicity with the local error estimate, we
establish a uniform-in-time sampling error bound, showing that the invariant
measure of the method approximates the true Gibbs distribution with $O(\tau^2)$
accuracy where $\tau$ is the time step. Lastly, we perform numerical
experiments to validate the theoretical results.

</details>


### [7] [Smoother-type a posteriori error estimates for finite element methods](https://arxiv.org/abs/2510.07677)
*Yuwen Li,Han Shui*

Main category: math.NA

TL;DR: A posteriori error estimates for finite element methods using linear iterative solver smoothers (Jacobi/Gauss-Seidel) on finer meshes, requiring only coarse-to-fine prolongation.


<details>
  <summary>Details</summary>
Motivation: To develop user-friendly error estimates that are simpler to implement than traditional methods while maintaining accuracy.

Method: Use simple smoothers (Jacobi/Gauss-Seidel) on auxiliary finer mesh to process finite element residual for error control, requiring only coarse-to-fine prolongation operator.

Result: For symmetric problems, reliability and efficiency proven under saturation assumption. Numerical experiments show proposed estimators outperform residual-type estimators in accuracy and exhibit robustness to parameters and polynomial degrees.

Conclusion: Smoother-type error estimators provide accurate, robust, and user-friendly alternatives to traditional residual-type estimators for finite element methods.

Abstract: This work develops user-friendly a posteriori error estimates of finite
element methods, based on smoothers of linear iterative solvers. The proposed
method employs simple smoothers, such as Jacobi or Gauss--Seidel iteration, on
an auxiliary finer mesh to process the finite element residual for a posteriori
error control. The implementation requires only a coarse-to-fine prolongation
operator. For symmetric problems, we prove the reliability and efficiency of
smoother-type error estimators under a saturation assumption. Numerical
experiments for various PDEs demonstrate that the proposed smoother-type error
estimators outperform residual-type estimators in accuracy and exhibit
robustness with respect to parameters and polynomial degrees.

</details>


### [8] [Elastic-plastic cell-based smoothed finite element method solving geotechnical problems](https://arxiv.org/abs/2510.07687)
*Yang Yang,Mingjiao Yan,Zongliang Zhang,Miao Zhang,Feidong Zheng,Dong Pana,Xiaozi Lina*

Main category: math.NA

TL;DR: A cell-based smoothed finite element method (CSFEM) is developed for elastic-plastic analysis of soils and rocks, improving stress accuracy and reducing mesh sensitivity while maintaining polygonal element flexibility.


<details>
  <summary>Details</summary>
Motivation: To address nonlinear and path-dependent behaviors in geotechnical analysis while overcoming issues like volumetric locking and mesh distortion sensitivity in conventional FEM.

Method: Uses strain smoothing over subcell domains with consistent stress return-mapping algorithm, implemented in ABAQUS via user-defined elements.

Result: Validated through benchmark problems showing excellent agreement with analytical solutions and conventional FEM, with smoother stress fields and improved convergence.

Conclusion: CSFEM provides a stable and efficient framework for elastic-plastic analysis of complex geotechnical problems with enhanced accuracy and reduced mesh sensitivity.

Abstract: An elastic-plastic cell-based smoothed finite element method (CSFEM) is
proposed for geotechnical analysis of soils and rocks exhibiting nonlinear and
path-dependent behaviors. By introducing strain smoothing over subcell domains
and employing a consistent stress return-mapping algorithm, the method enhances
stress accuracy, alleviates volumetric locking, and reduces sensitivity to mesh
distortion while retaining the flexibility of polygonal elements. The
formulation is implemented in ABAQUS via a user-defined element and validated
through benchmark and practical problems, including a pressurized thick
cylinder, biaxial soil test, strip footing bearing capacity, tunnel excavation,
and slope stability. Numerical results show excellent agreement with analytical
solutions and conventional FEM, with smoother stress fields, improved
convergence, and higher accuracy in ultimate load prediction. These findings
demonstrate that CSFEM provides a stable and efficient framework for
elastic-plastic analysis of complex geotechnical problems.

</details>


### [9] [Scaling crossover of the generalized Jeffreys-type law](https://arxiv.org/abs/2510.07930)
*Fugui Ma*

Main category: math.NA

TL;DR: This paper develops a comprehensive framework for the generalized Jeffreys-type law, providing physical derivation from first principles, rigorous mathematical analysis, and a novel spectral-accurate numerical algorithm with optimal computational complexity.


<details>
  <summary>Details</summary>
Motivation: To provide a novel physical explanation for the generalized Jeffreys-type equation from first principles and develop efficient computational methods for solving complex nonlocal problems with rich scaling crossover phenomena.

Method: Derived the equation from microscopic continuous-time random walk framework and overdamped Langevin equation with stochastic time-change. Used Laplace transform for rigorous analysis of well-posedness and Sobolev regularity. Developed CIM-CLG algorithm with spectral accuracy in time and space.

Result: Established equation well-posedness and regularity properties. Created efficient numerical scheme with O(N) time complexity and O(M log M) space complexity that is fully parallelizable. Validated method through extensive 1D and 2D numerical experiments.

Conclusion: The work integrates stochastic modeling, mathematical analysis, and numerical computation to advance understanding of generalized Jeffreys-type law and provides a rigorous, computationally efficient framework for complex nonlocal problems.

Abstract: The generalized Jeffreys-type law is formulated as a multi-term
time-fractional Jeffreys-type equation, whose dynamics exhibit rich scaling
crossover phenomena entailing different diffusion mechanisms. In this work, we
provide a novel physical explanation for the equation from first principles,
beginning with a microscopic description based on the continuous-time random
walk framework with a generalized waiting time distribution and further
deriving the equation from an overdamped Langevin equation subject to a
stochastic time-change (subordination). Employing the Laplace transform method,
we conduct a rigorous analysis of the equation, establishing its well-posedness
and providing a detailed Sobolev regularity analysis. We also develop a novel
numerical scheme, termed the CIM-CLG algorithm, which achieves spectral
accuracy in both time and space while substantially relaxing the temporal
regularity requirements on the solution. The algorithm reduces the
computational complexity to $\mathcal{O}(N)$ in time and $\mathcal{O}(M\log M)$
in space and is fully parallelizable. Detailed implementation guidelines and
new technical error estimates are provided. Extensive numerical experiments in
1D and 2D settings validate the efficiency, robustness, and accuracy of the
proposed method. By integrating stochastic modeling, mathematical analysis, and
numerical computation, this work advances the understanding of the generalized
Jeffreys-type law and offers a mathematically rigorous and computationally
efficient framework for tackling complex nonlocal problems.

</details>


### [10] [Likelihood-informed Model Reduction for Bayesian Inference of Static Structural Loads](https://arxiv.org/abs/2510.07950)
*Jakob Scheffels,Elizabeth Qian,Iason Papaioannou,Elisabeth Ullmann*

Main category: math.NA

TL;DR: A new projection-based model reduction method for Bayesian inverse problems that exploits low-dimensional structure in likelihood-informed subspaces to accelerate computations.


<details>
  <summary>Details</summary>
Motivation: Bayesian inverse problems in structural engineering are computationally expensive due to many forward model evaluations, but often exhibit low-dimensional structure where data primarily inform parameters in a low-dimensional subspace.

Method: Projects the governing PDE onto the likelihood-informed subspace (LIS), creating a computationally efficient reduced model for solving inverse problems with right-hand-side forcing parameters.

Result: Numerical experiments show the method successfully exploits intrinsic low-dimensionality, achieving relative errors of O(10^{-10}) with 10x-100x lower-dimensional models.

Conclusion: The proposed approach effectively accelerates Bayesian inverse problem solutions by leveraging low-dimensional structure through projection onto likelihood-informed subspaces.

Abstract: Bayesian inverse problems use data to update a prior probability distribution
on uncertain parameter values to a posterior distribution. Such problems arise
in many structural engineering applications, but computational solution of
Bayesian inverse problems is often expensive because standard solution
approaches require many evaluations of the forward model mapping the parameter
value to predicted observations. In many settings, this forward model is
expensive because it requires the solution of a high-dimensional discretization
of a partial differential equation. However, Bayesian inverse problems often
exhibit low-dimensional structure because the available data are primarily
informative (relative to the prior) in a low-dimensional subspace, sometimes
called the likelihood-informed subspace (LIS). This paper proposes a new
projection-based model reduction method for static linear systems that exploits
this low-dimensional structure in the setting where the unknown parameter is
the right-hand-side forcing. The proposed method projects the governing partial
differential equation onto the likelihood-informed subspace, yielding a
computationally efficient reduced model that can be used to accelerate the
solution of the inverse problem. Numerical experiments on two structural
engineering model problems demonstrate that the proposed approach can
successfully exploit the intrinsic low-dimensionality of the problem, obtaining
relative errors of O(10^{-10}) in the inverse problem solution with a 10x-100x
lower-dimensional model.

</details>


### [11] [LDMD with Temporally Adaptive Segmentation](https://arxiv.org/abs/2510.08065)
*Qiuqi Li,Chang Liu,Yifei Yang*

Main category: math.NA

TL;DR: The paper proposes a localized Dynamic Mode Decomposition (LDMD) framework that improves long-term prediction accuracy by segmenting the temporal domain and performing localized predictions within each segment.


<details>
  <summary>Details</summary>
Motivation: Standard DMD struggles with poor long-term predictive accuracy, so the authors aim to enhance prediction performance by combining DMD's linear forecasting strengths with time-domain segmentation.

Method: The LDMD framework segments the temporal domain into multiple subintervals, constructs snapshot matrices within each segment, and performs localized predictions. It includes both predefined segmentation and an adaptive segmentation strategy for better efficiency and robustness.

Result: Numerical experiments on four benchmark problems (Burgers', Allen-Cahn, nonlinear Schrodinger, and Maxwell's equations) show that LDMD significantly enhances long-term predictive accuracy while maintaining high computational efficiency.

Conclusion: The proposed LDMD framework effectively addresses the long-term prediction limitations of standard DMD, providing improved accuracy and computational efficiency through localized temporal domain segmentation.

Abstract: Dynamic mode decomposition (DMD) is a widely used data-driven algorithm for
predicting the future states of dynamical systems. However, its standard
formulation often struggles with poor long-term predictive accuracy. To address
this limitation, we propose a localized DMD (LDMD) framework that improves
prediction performance by integrating DMD's strong linear forecasting
capabilities with time-domain segmentation techniques. In this framework, the
temporal domain is segmented into multiple subintervals, within which snapshot
matrices are constructed and localized predictions are performed. We first
present the localized DMD method with predefined segmentation, and then explore
an adaptive segmentation strategy to further enhance computational efficiency
and prediction robustness. Furthermore, we conduct an error analysis that
provides the upper bound of the local and global truncation error for the
proposed framework. The effectiveness of LDMD is demonstrated on four benchmark
problems-Burgers', Allen-Cahn, nonlinear Schrodinger, and Maxwell's equations.
Numerical results show that LDMD significantly enhances long-term predictive
accuracy while preserving high computational efficiency.

</details>


### [12] [Semi-Implicit Central scheme for Hyperbolic Systems of Balance Laws with Relaxed Source Term](https://arxiv.org/abs/2510.08134)
*Sudipta Sahu,Emanuele Macca,Rathan Samala*

Main category: math.NA

TL;DR: A new IMEX finite volume scheme combining midpoint rule in space and trapezoidal rule in time with backward semi-implicit Taylor expansion for stiff hyperbolic systems with source terms.


<details>
  <summary>Details</summary>
Motivation: Quasi-linear hyperbolic systems with stiff source terms present significant computational challenges that need specialized numerical methods.

Method: Finite volume Nessyahu-Tadmor central scheme with new IMEX approach: stiff source term handled semi-implicitly using midpoint rule in space, trapezoidal rule in time with backward semi-implicit Taylor expansion.

Result: Method maintains stability and robustness near stiffness and discontinuities while preserving second-order accuracy asymptotically.

Conclusion: The proposed IMEX scheme is effective for efficiently solving stiff hyperbolic systems of balance laws, with theoretical and numerical validation confirming stability and accuracy.

Abstract: Quasi-linear hyperbolic systems with source terms introduce significant
computational challenges due to the presence of a stiff source term. To address
this, a finite volume Nessyahu-Tadmor (NT) central numerical scheme is explored
and applied to benchmark models such as the Jin-Xin relaxation model, the
shallow-water model, the Broadwell model, the Euler equations with heat
transfer, and the Euler system with stiff friction to assess their
effectiveness. The core part of this numerical scheme lies in developing a new
implicit-explicit (IMEX) scheme, where the stiff source term is handled in an
semi-implicit manner constructed by combining the midpoint rule in space, the
trapezoidal rule in time with a backward semi-implicit Taylor expansion. The
advantage of the proposed method lies in its stability region and maintains
robustness near stiffness and discontinuities, while asymptotically preserving
second-order accuracy.
  Theoretical analysis and numerical validation confirm the stability and
accuracy of the method, highlighting its potential for efficiently solving the
stiff hyperbolic systems of balance laws.

</details>


### [13] [Dual-primal Isogeometric Tearing and Interconnecting Solvers for adaptively refined multi-patch configurations](https://arxiv.org/abs/2510.08148)
*Stefan Takacs,Stefan Tyoler*

Main category: math.NA

TL;DR: The paper applies IETI-DP method to adaptive multi-patch geometries from recursive patch splitting, providing solvability conditions and a preconditioner with optimal condition number bounds.


<details>
  <summary>Details</summary>
Motivation: Isogeometric Analysis requires adaptive refinement for irregular solutions, but adaptive multi-patch configurations with T-junctions need efficient solvers that preserve tensor-product structure.

Method: Apply dual-primal Isogeometric Tearing and Interconnecting method (IETI-DP) to hierarchical non-matching multi-patch geometries from recursive splitting, with proposed preconditioner.

Result: Established solvability conditions for local problems and proved condition number bound matching the fully matching case, confirmed by numerical experiments.

Conclusion: IETI-DP method is effective for adaptive multi-patch geometries, maintaining optimal convergence rates in adaptive refinement scenarios.

Abstract: Isogeometric Analysis is a variant of the finite element method, where spline
functions are used for the representation of both the geometry and the
solution. Splines, particularly those with higher degree, achieve their full
approximation power only if the solution is sufficiently regular. Since
solutions are usually not regular everywhere, adaptive refinement is essential.
Recently, a multi-patch-based adaptive refinement strategy based on recursive
patch splitting has been proposed, which naturally generates hierarchical,
non-matching multi-patch configurations with T-junctions, but preserves the
tensor-product structure within each patch.
  In this work, we investigate the application of the dual-primal Isogeometric
Tearing and Interconnecting method (IETI-DP) to such adaptive multi-patch
geometries. We provide sufficient conditions for the solvability of the local
problems and propose a preconditioner for the overall iterative solver. We
establish a condition number bound that coincides with the bound previously
shown for the fully matching case. Numerical experiments confirm the
theoretical findings and demonstrate the efficiency of the proposed approach in
adaptive refinement scenarios.

</details>


### [14] [Full moment error estimates in strong norms for numerical approximations of stochastic Navier-Stokes equations with multiplicative noise, Part I: time discretization](https://arxiv.org/abs/2510.08291)
*Xiaobing Feng,Liet Vo*

Main category: math.NA

TL;DR: Derives optimal-order full moment error estimates for Euler-Maruyama discretization of stochastic Navier-Stokes equations with multiplicative noise, and introduces a new framework for analyzing nonlinear SPDEs.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous error estimates for numerical approximations of stochastic Navier-Stokes equations and develop a general framework for analyzing nonlinear SPDEs with multiplicative noise.

Method: Uses exponential stability estimates for SPDE solutions, discrete stochastic Gronwall inequality, and bootstrap arguments to analyze the Euler-Maruyama time discretization.

Result: Achieves optimal-order full moment error estimates in strong norms for both velocity and pressure approximations in the stochastic Navier-Stokes equations.

Conclusion: The paper presents a novel analytical framework that successfully provides optimal error estimates for stochastic Navier-Stokes equations and can be generalized to other nonlinear SPDEs with multiplicative noise.

Abstract: This paper focuses on deriving optimal-order full moment error estimates in
strong norms for both velocity and pressure approximations in the
Euler-Maruyama time discretization of the stochastic Navier-Stokes equations
with multiplicative noise. Additionally, it introduces a novel approach and
framework for the numerical analysis of nonlinear stochastic partial
differential equations (SPDEs) with multiplicative noise in general. The main
ideas of this approach include establishing exponential stability estimates for
the SPDE solution, leveraging a discrete stochastic Gronwall inequality, and
employing a bootstrap argument.

</details>


### [15] [Surface finite element approximation of parabolic SPDEs with Whittle--Matérn noise](https://arxiv.org/abs/2510.08443)
*Øyvind Stormark Auestad,Geir-Arne Fuglstad,Annika Lang*

Main category: math.NA

TL;DR: A new fully discrete surface finite element method for linear parabolic stochastic evolution equations with additive noise, using surface finite element approximation of noise with covariance from elliptic operators.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for stochastic PDEs on surfaces, particularly for equations with noise having covariance structure defined by elliptic operators like Whittle-Matérn fields.

Method: Fully discrete surface finite element approximation with specialized noise discretization tailored for covariance operators from elliptic operators.

Result: Derived strong and pathwise convergence rates for the approximation method, with numerical experiments verifying the theoretical convergence rates.

Conclusion: The proposed surface finite element method provides effective discretization for stochastic evolution equations with additive noise, with proven convergence properties.

Abstract: We propose and analyse a new type of fully discrete surface finite element
approximation of a class of linear parabolic stochastic evolution equations
with additive noise. Our discretization uses a surface finite element
approximation of the noise, and is tailored for equations with noise having
covariance operator defined by (negative powers of) elliptic operators, like
Whittle--Mat\'ern random fields. We derive strong and pathwise convergence
rates of our approximation, and verify these by numerical experiments.

</details>


### [16] [Refinement-based Christoffel sampling for least squares approximation in non-orthogonal bases](https://arxiv.org/abs/2510.08461)
*Astird Herremans,Ben Adcock*

Main category: math.NA

TL;DR: A refinement-based Christoffel sampling algorithm for least squares approximation that avoids costly discrete orthogonalization for non-orthogonal basis functions.


<details>
  <summary>Details</summary>
Motivation: Standard Christoffel sampling requires expensive discrete orthogonalization for non-orthogonal bases, which becomes computationally prohibitive as the Christoffel function norm increases.

Method: Iterative refinement algorithm inspired by approximate leverage score sampling, using a numerical variant of Christoffel function that accounts for finite-precision effects.

Result: The method achieves near-best approximations with O(n log(n)) samples while reducing computational cost from linear to logarithmic growth in the Christoffel function norm.

Conclusion: The proposed algorithm is efficient and robust, providing a practical alternative to traditional Christoffel sampling for non-orthogonal basis functions.

Abstract: We introduce a refinement-based Christoffel sampling (RCS) algorithm for
least squares approximation in the span of a given, generally non-orthogonal
set of functions $\Phi_n = \{\phi_1, \dots, \phi_n\}$. A standard sampling
strategy for this problem is Christoffel sampling, which achieves near-best
approximations in probability using only $\mathcal{O}(n \log(n))$ samples.
However, it requires i.i.d.\ sampling from a distribution whose density is
proportional to the inverse Christoffel function $k_n$, the computation of
which requires an orthonormal basis. As a result, existing approaches for
non-orthogonal bases $\Phi_n$ typically rely on costly discrete
orthogonalization. We propose a new iterative algorithm, inspired by recent
advances in approximate leverage score sampling, that avoids this bottleneck.
Crucially, while the computational cost of discrete orthogonalization grows
proportionally with $\|k_n\|_{L^\infty(X)}$, the cost of our approach increases
only logarithmically in $\|k_n\|_{L^\infty(X)}$. In addition, we account for
finite-precision effects by considering a numerical variant of the Christoffel
function, ensuring that the algorithm relies only on computable quantities.
Alongside a convergence proof, we present extensive numerical experiments
demonstrating the efficiency and robustness of the proposed method.

</details>


### [17] [Where Have All the Kaczmarz Iterates Gone?](https://arxiv.org/abs/2510.08563)
*El Houcine Bergou,Soumia Boucherouite,Aritra Dutta,Xin Li,Anna Ma*

Main category: math.NA

TL;DR: Analysis of the randomized Kaczmarz algorithm's asymptotic behavior on noisy, inconsistent linear systems, including limit point locations, convergence bounds, and numerical validation.


<details>
  <summary>Details</summary>
Motivation: Practical applications often involve noisy and inconsistent linear systems, but the study of RK algorithm on such systems is limited despite its computational efficiency for large-scale problems.

Method: Investigated asymptotic behavior of RK iterates in expectation, analyzed roles of singular vectors of noisy coefficient matrix, derived convergence horizon bounds, and conducted extensive numerical experiments.

Result: Established theoretical understanding of RK algorithm's limitations and robustness in noisy environments, with bounds on convergence horizon depending on noise levels and system characteristics.

Conclusion: The findings provide practical insights into RK algorithm performance under realistic conditions and pave the way for optimized applications in real-world scientific and engineering problems.

Abstract: The randomized Kaczmarz (RK) algorithm is one of the most computationally and
memory-efficient iterative algorithms for solving large-scale linear systems.
However, practical applications often involve noisy and potentially
inconsistent systems. While the convergence of RK is well understood for
consistent systems, the study of RK on noisy, inconsistent linear systems is
limited. This paper investigates the asymptotic behavior of RK iterates in
expectation when solving noisy and inconsistent systems, addressing the
locations of their limit points. We explore the roles of singular vectors of
the (noisy) coefficient matrix and derive bounds on the convergence horizon,
which depend on the noise levels and system characteristics. Finally, we
provide extensive numerical experiments that validate our theoretical findings,
offering practical insights into the algorithm's performance under realistic
conditions. These results establish a deeper understanding of the RK
algorithm's limitations and robustness in noisy environments, paving the way
for optimized applications in real-world scientific and engineering problems.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [18] [Consistency of some well-posed five-field theories of dissipative relativistic fluid dynamics](https://arxiv.org/abs/2510.07526)
*Heinrich Freistuhler*

Main category: math.AP

TL;DR: This paper analyzes causal hyperbolic five-field relativistic Navier-Stokes theories derived from Landau-Lifshitz formulation via Eulerian gradient shifts, showing their O(ε²) equivalence to Landau-Lifshitz with O(ε³) excess entropy production.


<details>
  <summary>Details</summary>
Motivation: To establish consistency properties for causal hyperbolic theories within the FTBDNK family of relativistic Navier-Stokes formulations, particularly those obtained from Landau-Lifshitz via Eulerian gradient shifts.

Method: Analysis of the EGS(L) family of models derived from Landau-Lifshitz formulation through Eulerian gradient shifts, examining properties with respect to dissipation coefficient magnitude ε.

Result: Any element of EGS(L) is O(ε²) equivalent to Landau-Lifshitz formulation, has O(ε³) excess entropy production, and cleanly represents heterogeneous local thermodynamic equilibria.

Conclusion: The EGS(L) family provides consistent causal hyperbolic theories that maintain fundamental thermodynamic properties while generalizing previous formulations, with controlled deviations from the original Landau-Lifshitz framework.

Abstract: Within the FTBDNK family of formulations of relativistic Navier-Stokes (H.
Freist\"uhler and B. Temple, Proc. R. Soc. A 470, 20140055 (2014), Proc. R.
Soc. A 473 (2017), 20160729; F. S. Bemfica, M. Disconzi, and J. Noronha, Phys.
Rev. D 98, 104064 (2018), Phys. Rev. D 100, 104020 (2019); P. Kovtun, J. High
Energy Phys. 2019, 034 (2019)), this paper collects some consistency properties
for certain causal hyperbolic five-field theories obtained from the
Landau-Lifshitz formulation via Eulerian gradient shifts, a family, EGS(L), of
models that slightly generalize a class identified in H. Freist\"uhler, J.
Math.\ Phys. 61, 033101 (2020). With $\epsilon$ the magnitude of the
dissipation coefficients that quantify viscosity and heat conduction, the paper
shows that any element of EGS(L) is $O(\epsilon^2)$ equivalent to the
Landau-Lifshitz formulation, has an $O(\epsilon^3)$ excess entropy production,
and represents heterogeneous local thermodynamic equilibria cleanly.

</details>


### [19] [First order equation on random measures as superposition of weak solutions to the McKean-Vlasov equation](https://arxiv.org/abs/2510.07542)
*Alessandro Pinzi*

Main category: math.AP

TL;DR: This paper defines an evolution equation for random probability measures with non-local drift and diffusion, shows solutions can be lifted to superpositions of solutions to non-linear KFP and McKean-Vlasov equations, and transfers existence/uniqueness results.


<details>
  <summary>Details</summary>
Motivation: To establish connections between evolution equations for random probability measures and classical equations like Kolmogorov-Fokker-Planck and McKean-Vlasov equations, enabling transfer of existence and uniqueness properties.

Method: Define evolution equation for curves of random probability measures with non-local drift and diffusion terms, then prove lifting results to superposition solutions of non-linear KFP and McKean-Vlasov equations.

Result: Solutions to the random measure evolution equation can be lifted to superpositions of solutions to non-linear KFP equations and weak solutions to McKean-Vlasov equations.

Conclusion: Existence and uniqueness properties can be transferred from the random measure equation to the associated non-linear KFP and McKean-Vlasov equations under uniqueness assumptions for the linearized KFP.

Abstract: The goal of this paper is to define an evolution equation for a curve of
random probability measures $(M_t)_{t\in[0,T]}\subset
\mathcal{P}(\mathcal{P}(\mathbb{R}^d))$ associated to a non-local drift
$b:[0,T]\times\mathbb{R}^d \times \mathcal{P}(\mathbb{R}^d) \to \mathbb{R}^d$
and a non-local diffusion term $a:[0,T]\times \mathbb{R}^d \times
\mathcal{P}(\mathbb{R}^d) \to \operatorname{Sym}_+(\mathbb{R}^{d\times d})$.
Then, we show that any solution to that equation can be lifted to a
superposition of solutions to a non-linear Kolmogorov-Fokker-Planck equation
and also to a superposition of weak solutions to the McKean-Vlasov equations.
Finally, we use this superposition result to show how existence and uniqueness
can be transferred from the equation on random measures to the associated
non-linear Kolmogorov-Fokker-Planck equation and to the McKean-Vlasov equation,
assuming uniqueness of the linearized KFP.

</details>


### [20] [Nonlinear Stability of the Rayleigh-Taylor Problem in Quantum Navier-Stokes Equations](https://arxiv.org/abs/2510.07695)
*Fei Jiang,Yajie Zhang,Zhipeng Zhang,Youyi Zhao*

Main category: math.AP

TL;DR: The paper proves that quantum effects can completely inhibit Rayleigh-Taylor instability in nonlinear quantum Navier-Stokes equations under proper conditions, establishing algebraic stability for small perturbations.


<details>
  <summary>Details</summary>
Motivation: To extend linear theory findings about quantum effect stabilization of Rayleigh-Taylor instability to the nonlinear regime and rigorously prove inhibition under proper quantum conditions.

Method: A complicated multi-layer energy method with anisotropic norms of spatial derivatives is used to mathematically prove the stability results.

Result: There exists a threshold ε_c for the scaled Planck constant such that if the quantum parameter exceeds this threshold and the density profile satisfies a stabilizing condition, small perturbations around equilibrium are algebraically stable.

Conclusion: Quantum effects can completely inhibit Rayleigh-Taylor instability in the nonlinear regime, providing rigorous mathematical proof of stabilization under proper quantum conditions.

Abstract: It is well-known that the Rayleigh--Taylor (abbr. RT) instability can be
completely inhibited by the quantum effect stabilization in proper
circumstances leading to a cutoff wavelength in the \emph{linear} motion
equations. Motivated by the linear theory, we further investigate the
{stability} for the \emph{nonlinear} RT problem of quantum Navier--Stokes
equations in a slab with Navier boundary condition, and rigorously prove the
inhibition of RT instability by the quantum effect under a proper setting. More
precisely, if the RT density profile $\bar\rho$ satisfies an additional
stabilizing condition, then there is a threshold ${\varepsilon_{{c}}}$ of the
scaled Planck constant, such that if the scaled Planck constant is bigger than
${\varepsilon_{{c}}}$, the small perturbation solutions around an RT
equilibrium state are algebraically stable in time. The mathematical proof is
realized by a complicated multi-layer energy method with anisotropic norms of
spacial derivatives.

</details>


### [21] [A log-free estimate for the diagonal paraproduct high $\times$ high $\to$ low in the 3D Navier-Stokes equation](https://arxiv.org/abs/2510.07848)
*Pylyp Cherevan*

Main category: math.AP

TL;DR: The paper analyzes the diagonal paraproduct in the 3D Navier-Stokes nonlinearity (u·∇)u, obtaining log-free L²_t Ḣ^{-1}_x estimates for scale-critical windows in the range 1/6 < δ ≤ 5/8.


<details>
  <summary>Details</summary>
Motivation: To understand the nonlinear structure of the 3D Navier-Stokes equations by studying the diagonal paraproduct component, which is crucial for energy-critical schemes.

Method: Uses phase-geometric integration, anisotropic local estimates on cylinders, bilinear ℓ² decoupling on finite-rank surfaces, and suppresses the null form to control the narrow diagonal zone.

Result: Achieves log-free estimates at the L²_t Ḣ^{-1}_x level for the projection P_{< N^{1-δ}} ∇(u_N ⊗ v_N), consistent with critical energy schemes.

Conclusion: The analysis successfully handles a single resonant component, with extensions to the full (u·∇)u structure and sup_t versions left for future work.

Abstract: We consider the diagonal paraproduct arising in the nonlinearity $(u\cdot
\nabla) u$ for the three-dimensional Navier-Stokes equations. On scale-critical
windows and in the range $1/6 < \delta \le 5/8$ we obtain a log-free estimate
at the level $L^2_t {\dot H}^{-1}_x$ for the projection $P_{< N^{1-\delta}}
\nabla(u_N \otimes v_N)$, consistent with the critical energy scheme. The main
tools are phase-geometric integration, anisotropic local estimates on
cylinders, and bilinear $ell^2$ decoupling on a finite-rank surface; the narrow
diagonal zone is controlled via suppression of the null form. The work is
restricted to a single resonant component; extensions to the full structure $(u
\cdot\nabla) u$ and to sup$_t$ versions are left for further analysis.

</details>


### [22] [The $\eps-\eps^β$ property for clusters with double density](https://arxiv.org/abs/2510.07907)
*A. Pratelli,V. Scattaglia*

Main category: math.AP

TL;DR: This paper extends the "ε-ε^β property" to clusters in Euclidean space with double density.


<details>
  <summary>Details</summary>
Motivation: To generalize the ε-ε^β property from single density settings to more complex scenarios involving clusters with double density in Euclidean space.

Method: Theoretical extension and mathematical analysis of the ε-ε^β property applied to clusters with double density in Euclidean space.

Result: Successfully extended the ε-ε^β property framework to handle clusters with double density configurations.

Conclusion: The extension provides a more comprehensive mathematical framework for analyzing clusters with complex density structures in Euclidean spaces.

Abstract: This article is devoted to extend the "$\eps-\eps^\beta$ property" to the
case of clusters in an Euclidean space with a double density.

</details>


### [23] [Fractional p-Laplacian Kirchhoff-type problem involving a singular term via Nehari manifold](https://arxiv.org/abs/2510.07911)
*Djamel Abid*

Main category: math.AP

TL;DR: Existence of nontrivial positive solutions for Kirchhoff-type problems with sign-changing nonlinearities and singular terms, proven using Nehari manifold and Ekeland's variational principle.


<details>
  <summary>Details</summary>
Motivation: To study the existence of positive solutions for Kirchhoff-type problems that involve sign-changing nonlinearities and singular terms, which are challenging due to their complex mathematical structure.

Method: Using the Nehari manifold approach combined with Ekeland's variational principle to analyze the problem.

Result: For appropriate choice of parameter λ, the problem has at least two positive solutions in both subcritical and critical cases.

Conclusion: The combination of Nehari manifold and Ekeland's variational principle successfully establishes the existence of multiple positive solutions for Kirchhoff-type problems with sign-changing nonlinearities and singular terms.

Abstract: This paper is dedicated to studying the existence of nontrivial positive
solutions for a Kirchhoff-type problem with sign change nonlinearities and a
singular term, Using the Nehari manifold and EkelandS variational principle we
prove that for the appropriate choice of {\lambda} our problem has at least two
positive solutions for both subcritical and critical cases.

</details>


### [24] [A cross-diffusion system with independent drifts and fast diffusion](https://arxiv.org/abs/2510.07937)
*Charles Elbar,Filippo Santambrogio*

Main category: math.AP

TL;DR: The paper extends existence results for global weak solutions in a one-dimensional cross-diffusion system from linear diffusion to fast-diffusion cases.


<details>
  <summary>Details</summary>
Motivation: To generalize previous results on cross-diffusion systems with linear diffusion to include fast-diffusion scenarios, expanding the mathematical understanding of such systems.

Method: Study a one-dimensional cross-diffusion system on the torus with fast-diffusion law (exponent 0<α≤1) and different external potentials, under specific initial conditions including bounded entropy and a mixing condition.

Result: Proves existence of global weak solutions for arbitrary non-negative L¹ initial data with bounded entropy and mixing condition, extending previous linear diffusion results to fast-diffusion cases.

Conclusion: The extension from linear diffusion (α=1) to fast-diffusion (0<α≤1) is successfully achieved, providing broader mathematical foundation for cross-diffusion systems with fast-diffusion laws.

Abstract: We study a one-dimensional cross-diffusion system for two interacting
populations on the torus, with a fast-diffusion law with exponent $0< \alpha\le
1$ and different external potentials. For arbitrary non-negative $L^{1}$
initial data with bounded entropy and a mixing condition we prove the existence
of global weak solutions. This extends the recent result of M\'esz\'aros,
Parker from the linear diffusion ($\alpha=1$) to the fast-diffusion.

</details>


### [25] [Gradient regularity for widely degenerate parabolic equations](https://arxiv.org/abs/2510.07999)
*Michael Strunk*

Main category: math.AP

TL;DR: The paper establishes continuity of certain functions of the gradient of weak solutions to degenerate parabolic equations where the diffusion vanishes on a bounded convex set.


<details>
  <summary>Details</summary>
Motivation: To extend C^1-regularity results from elliptic equations to the parabolic setting for equations with degenerate diffusion that vanishes on a bounded convex set.

Method: Analyze weak solutions to parabolic equations with diffusion operator that is only elliptic outside a bounded convex set E, using L^{n+2+σ} regularity assumptions on the forcing term.

Result: Proved that for any continuous function vanishing on E, the composition with the gradient of the solution is continuous in the space-time domain.

Conclusion: Successfully extended elliptic regularity theory to the parabolic case for degenerate diffusion equations, establishing continuity properties for functions of the gradient.

Abstract: In this paper, we are interested in the regularity of weak solutions
$u\colon\Omega_T\to\mathbb{R}$ to parabolic equations of the type
\begin{equation*}
  \partial_t u - \mathrm{div} \nabla \mathcal{F}(x,t,Du) = f\qquad\mbox{in
$\Omega_T$}, \end{equation*} where $\mathcal{F}$ is only elliptic for values of
$Du$ outside a bounded and convex set $E\subset \mathbb{R}^n$ with the property
that $0\in \mathrm{Int}{E}$. Here, $\Omega_T
:=\Omega\times(0,T)\subset\mathbb{R}^{n+1}$ denotes a space-time cylinder taken
over a bounded domain $\Omega\subset\mathbb{R}^n$ for some finite time $T>0$.
The function $\mathcal{F} : \Omega_T\times\mathbb{R}^n \to\mathbb{R}_{\geq 0}$
present in the diffusion is assumed to satisfy: the partial mapping $\xi\mapsto
\mathcal{F}(x,t,\xi)$ is regular whenever $\xi$ lies outside of $E$, and
vanishes entirely whenever $\xi$ lies within this set. Additionally, the datum
$f$ is assumed to be of class $L^{n+2+\sigma}(\Omega_T)$ for some parameter
$\sigma > 0$. As our main result we establish that
  \begin{equation*}
  \mathcal{K}(Du)\in C^0(\Omega_T)
  \end{equation*} for any continuous function $\mathcal{K}\in
C^0(\mathbb{R}^n)$ that vanishes on $E$. This article aims to extend the
$C^1$-regularity result for the elliptic case to the parabolic setting.

</details>


### [26] [Stability of Traveling Fronts of the FitzHugh-Nagumo Equations on Cylindrical Surfaces](https://arxiv.org/abs/2510.08028)
*Afroditi Talidou*

Main category: math.AP

TL;DR: Traveling front solutions of FitzHugh-Nagumo equations are nonlinearly stable on cylindrical surfaces, including standard cylinders and warped cylinders with slowly varying radius.


<details>
  <summary>Details</summary>
Motivation: To extend the known nonlinear stability of traveling front solutions in one spatial dimension to cylindrical surfaces.

Method: Mathematical analysis of traveling front solutions on cylindrical surfaces, extended to warped cylinders with slowly varying radius, supported by numerical simulations.

Result: Traveling fronts are nonlinearly stable on standard cylinders and persist on warped cylinders with slowly varying radius.

Conclusion: The stability properties of FitzHugh-Nagumo traveling fronts extend from one-dimensional to cylindrical geometries, including surfaces with slowly varying curvature.

Abstract: The FitzHugh-Nagumo equations are known to admit traveling front solutions in
one spatial dimension that are nonlinearly stable. This paper concerns the
stability of traveling front solutions propagating on cylindrical surfaces. It
is shown that such traveling fronts are nonlinearly stable on the surface of
standard cylinders of constant radius. The analysis is extended to warped
cylinders with slowly varying radius, where persistence of front-like solutions
is established. Numerical simulations support the theoretical findings.

</details>


### [27] [Determining a magnetic Schrödinger equation by a single far-field measurement](https://arxiv.org/abs/2510.08198)
*Chaohua Duan,Zhen Xue*

Main category: math.AP

TL;DR: Single far-field measurement uniquely determines support of magnetic and electric potentials for polyhedral scatterers. Transmission eigenfunctions vanish at corners in 2D and edge corners in 3D when angle ≠ π.


<details>
  <summary>Details</summary>
Motivation: To understand inverse scattering for magnetic Schrödinger equation and establish minimal data requirements for shape reconstruction in practical cases with magnetic potentials.

Method: Variational approach for direct problem well-posedness, complex geometric optics solutions combined with asymptotic analysis near singular points.

Result: Proved unique determination of potential support from single measurement for polyhedral structures. Showed transmission eigenfunctions vanish at corners (2D) and edge corners (3D) when angle ≠ π.

Conclusion: Minimal measurement data suffices for shape reconstruction in magnetic scattering problems, advancing theoretical understanding with applications in quantum imaging and material characterization.

Abstract: This paper investigates the inverse scattering problem for the magnetic
Schr\"odinger equation. We first establish the well-posedness of the direct
problem through a variational approach under physically meaningful assumptions
on the magnetic and electric potentials. Our main results demonstrate that a
single far-field measurement uniquely determines the support of the potential
functions when the scatterer has polyhedral structures.
  A significant theoretical byproduct of our analysis reveals that transmission
eigenfunctions must vanish at corners in two dimensions and edge corners in
three dimensions, provided the angle is not $\pi$. This geometric property of
eigenfunctions extends previous results for the non-magnetic case and provides
new insights into the interaction between quantum effects and singular
geometries. The proof combines complex geometric optics solutions with careful
asymptotic analysis near singular points.
  From an inverse problems perspective, our work shows that minimal measurement
data suffices for shape reconstruction in important practical cases, advancing
the theoretical understanding of inverse scattering with magnetic potentials.
The results have potential applications in quantum imaging, material
characterization, and nondestructive testing where magnetic fields play a
crucial role.

</details>


### [28] [A survey on the optimal partition problem](https://arxiv.org/abs/2510.08241)
*Roberto Ognibene,Bozhidar Velichkov*

Main category: math.AP

TL;DR: Survey on regularity theory for optimal partition problems involving vector-valued Sobolev functions with disjoint supports, covering local minimizers and critical points.


<details>
  <summary>Details</summary>
Motivation: The optimal partition problem has emerged in various contexts, requiring a unified perspective and updated account of regularity results for solutions and free boundaries.

Method: Synthesizes current state-of-the-art by analyzing non-negative vector-valued Sobolev functions with mutually disjoint support components, examining both interior and boundary regularity.

Result: Provides comprehensive overview of progress in regularity theory for solutions and their free boundaries in optimal partition problems.

Conclusion: Offers coherent perspective and up-to-date account of regularity developments for optimal partition problems across different mathematical contexts.

Abstract: This survey synthesizes the current state of the art on the regularity theory
for solutions to the optimal partition problem. Namely, we consider
non-negative, vector-valued Sobolev functions whose components have mutually
disjoint support, and which are either local minimizers of the Dirichlet energy
or, more generally, critical points satisfying a system of variational
inequalities. This is particularly meaningful as the problem has emerged on
several occasions and in diverse contexts: our aim is then to provide a
coherent point of view and an up-to-date account of the progress concerning
regularity of the solutions and their free boundaries, both in the interior and
up to a fixed boundary.

</details>


### [29] [Hölder regularity of the solutions of Fredholm integral equations on upper Ahlfors regular sets](https://arxiv.org/abs/2510.08264)
*M. Lanza de Cristoforis,M. Norman*

Main category: math.AP

TL;DR: Extension of Hölder continuity results for Fredholm integral equations to metric measured spaces with upper Ahlfors growth conditions, including nondoubling measures.


<details>
  <summary>Details</summary>
Motivation: To generalize existing Hölder continuity results for Fredholm integral equations beyond traditional doubling measure spaces to include more general metric measured spaces with upper Ahlfors growth conditions.

Method: Extending the analysis framework to metric measured spaces where the measure satisfies upper Ahlfors growth conditions, which allows treatment of nondoubling measures.

Result: Established validity of (generalized) Hölder continuity results for solutions of Fredholm integral equations of the second kind in this broader context.

Conclusion: The paper successfully extends classical Hölder continuity results to more general metric measured spaces, including cases with nondoubling measures, broadening the applicability of these analytical tools.

Abstract: We extend to the context of metric measured spaces, with a measure that
satisfies upper Ahlfors growth conditions the validity of (generalized)
H\"{o}lder continuity results for the solution of a Fredholm integral equation
of the second kind. Here we note that upper Ahlfors growth conditions include
also cases of nondoubling measures.

</details>


### [30] [On the Cahn-Hilliard equation with nonlinear diffusion: the non-convex case](https://arxiv.org/abs/2510.08287)
*Monica Conti,Stefania Gatti,Andrea Giorgini,Giulio Schimperna*

Main category: math.AP

TL;DR: Analysis of Cahn-Hilliard equation with nonlinear diffusion and non-degenerate mobility, removing convexity assumptions and establishing new solution properties in 2D and 3D.


<details>
  <summary>Details</summary>
Motivation: Previous results required strong convexity assumptions that excluded relevant cases in phase separation modeling for complex systems like crystals and polymers.

Method: Established new qualitative properties under general diffusion/mobility assumptions, used Lojasiewicz-Simon inequality tailored to nonlinear diffusion case.

Result: In 2D: uniqueness, smoothing effect, convergence to equilibrium. In 3D: local well-posedness, global existence near minimizers, Lyapunov stability.

Conclusion: Successfully removed convexity condition and characterized longtime dynamics using tailored Lojasiewicz-Simon inequality.

Abstract: We investigate the Cahn-Hilliard equation with nonlinear diffusion and
non-degenerate mobility modeling phase separation phenomena in complex systems
(e.g., crystals and polymers). Previous results in the literature on this model
relied on the strong convexity assumption of the gradient part of the energy,
which excludes relevant cases. In this work, we remove the convexity condition
and establish new qualitative properties of solutions under general assumptions
on the diffusion and mobility functions. In two spatial dimensions, we prove
uniqueness of weak solutions, their smoothing effect for positive times, and
convergence to equilibrium as time tends to infinity. In three dimensions, we
show local well-posedness of strong solutions for arbitrary initial data and
global existence for data close to energy minimizers, yielding a Lyapunov
stability principle. A key ingredient of our analysis is a Lojasiewicz-Simon
inequality tailored to the nonlinear diffusion case, which enables us to
characterize the longtime dynamics.

</details>


### [31] [On a class of (non)local superposition operators of arbitrary order](https://arxiv.org/abs/2510.08345)
*Serena Dipierro,Sven Jarohs,Enrico Valdinoci*

Main category: math.AP

TL;DR: Systematic study of superposition operators of any positive order, with examples, counterexamples, and applications to nonlinear problems involving fractional order operators.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive framework for dealing with superposition operators of arbitrary positive order, addressing gaps in existing literature regarding general superposition operators and their properties.

Method: Introduces a general mathematical setting for superposition operators, provides systematic analysis with examples and counterexamples, characterizes measures and functional spaces, and applies the framework to nonlinear problems.

Result: Establishes a comprehensive theoretical foundation for superposition operators of any positive order, including fractional orders, with characterization of relevant mathematical objects and spaces.

Conclusion: The paper successfully develops a general framework for superposition operators that handles operators of arbitrary positive order, providing tools and results applicable to nonlinear problems involving fractional operators.

Abstract: In this paper we introduce a very general setting dealing with the
superposition of operators of any positive order and provide a systematic study
of them. We also provide examples and counterexamples, as well as
characterizing properties of the measures and the functional spaces under
consideration. Moreover, we present some applications regarding the existence
theory for a class of nonlinear problems involving superposition operators of
arbitrary (possibly fractional) order.

</details>


### [32] [The Space-Time Connectivity Theorem for Normal Currents](https://arxiv.org/abs/2510.08360)
*Paolo Bonicatto,Filip Rindler,Harry Turnbull*

Main category: math.AP

TL;DR: Establishes a Space-Time Connectivity Theorem for normal currents, extending classical results by Federer and Fleming and recent work on integral currents.


<details>
  <summary>Details</summary>
Motivation: To provide a progressive-in-time method for witnessing weak* convergence of boundaryless normal currents, distinguishing from classical approaches by incorporating a time coordinate.

Method: Develops a space-time framework where connecting currents have a time coordinate, allowing deformation of sequence elements to their limit in a time-progressive manner.

Result: Proves a theorem that enables witnessing weak* convergence of uniformly bounded sequences of boundaryless normal currents using space-time normal currents.

Conclusion: The space-time connectivity approach offers a novel perspective on current convergence, providing time-progressive deformation paths between sequence elements and their limits.

Abstract: This work establishes a Space-Time Connectivity Theorem for normal currents.
In analogy to classical results by Federer and Fleming as well as a recent
theorem for integral currents by the second author, this result allows one to
witness the weak* convergence of a uniformly bounded sequence of boundaryless
normal currents with a space-time normal current that connects the elements of
the sequence with their limit. The space-time setting is distinguished from the
classical case in that this connecting current has a time coordinate and thus
constitutes a progressive-in-time way to deform an element of the sequence to
the limit.

</details>


### [33] [Analysis of the transmission eigenvalue problem for biharmonic scattering considering penetrable scatterers](https://arxiv.org/abs/2510.08444)
*Rafael Ceja Ayala,Isaac Harris,Andreas Kleefeld*

Main category: math.AP

TL;DR: Analytical study of transmission eigenvalue problem for biharmonic scattering with penetrable obstacle in elastic Kirchhoff-Love plate, proving existence, discreteness, monotonicity with refractive index, and numerical validation.


<details>
  <summary>Details</summary>
Motivation: Extend transmission eigenvalue analysis from acoustic scattering to biharmonic scattering in elastic plates, where plate thickness is small relative to wavelength.

Method: Analytical proofs of existence and discreteness of transmission eigenvalues, study of dependence on refractive index, proof of monotonicity for first eigenvalue, and numerical experiments.

Result: Successfully proved existence and discreteness of transmission eigenvalues, established monotonicity of first eigenvalue with respect to refractive index, and validated results numerically.

Conclusion: Theoretical framework for transmission eigenvalues in biharmonic scattering is established with analytical proofs and numerical verification, extending previous acoustic scattering results to elastic plate problems.

Abstract: In this paper, we provide an analytical study of the transmission eigenvalue
problem in the context of biharmonic scattering with a penetrable obstacle. We
will assume that the underlying physical model is given by an infinite elastic
two--dimensional Kirchhoff--Love plate in $\mathbb{R}^2$, where the plate's
thickness is small relative to the wavelength of the incident wave. In previous
studies, transmission eigenvalues have been studied for acoustic scattering,
whereas in this case, we consider biharmonic scattering. We prove the existence
and discreteness of the transmission eigenvalues as well as study the
dependence on the refractive index. We are able to prove the monotonicity of
the first transmission eigenvalue with respect to the refractive index. Lastly,
we provide numerical experiments to validate the theoretical work.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [34] [A Virtual Fields Method-Genetic Algorithm (VFM-GA) calibration framework for isotropic hyperelastic constitutive models with application to an elastomeric foam material](https://arxiv.org/abs/2510.07683)
*Zicheng Yan,Jialiang Tao,Christian Franck,David L. Henann*

Main category: physics.comp-ph

TL;DR: A calibration framework combining Virtual Fields Method and Genetic Algorithm for automated parameter identification in isotropic hyperelastic constitutive models, validated on compressible elastomeric foams.


<details>
  <summary>Details</summary>
Motivation: To develop an automated calibration framework for material parameter identification in complex hyperelastic constitutive models, overcoming limitations of manual fitting methods.

Method: Synergizes Virtual Fields Method (VFM) for objective function formulation with Genetic Algorithm (GA) optimization, using DIC displacement fields and load cell data, with material stability assessment.

Result: Outperforms manual fitting, demonstrating robust and efficient parameter identification for complex hyperelastic models using both homogeneous and inhomogeneous deformation fields.

Conclusion: The VFM-GA framework provides an effective automated solution for material parameter identification in isotropic hyperelastic constitutive models, accommodating complex functional forms and ensuring physical behavior.

Abstract: This work introduces a calibration framework for material parameter
identification in isotropic hyperelastic constitutive models. The framework
synergizes the Virtual Fields Method (VFM) to define an objective function with
a Genetic Algorithm (GA) as the optimization method to facilitate automated
calibration. The formulation of the objective function uses experimental
displacement fields measured from Digital Image Correlation (DIC) synchronized
with load cell data and can accommodate data from experiments involving
homogeneous or inhomogeneous deformation fields. The framework places no
restrictions on the target isotropic hyperelastic constitutive model,
accommodating models with coupled dependencies on deformation invariants and
specialized functional forms with a number of material parameters, and assesses
material stability, eliminating sets of material parameters that potentially
lead to non-physical behavior for the target hyperelastic constitutive model.
To minimize the objective function, a GA is deployed as the optimization tool
due to its ability to navigate the intricate landscape of material parameter
space. The VFM-GA framework is evaluated by applying it to a hyperelastic
constitutive model for compressible elastomeric foams. The evaluation process
entails a number of tests that employ both homogeneous and inhomogeneous
displacement fields collected from DIC experiments on open-cell foam specimens.
The results outperform manual fitting, demonstrating the framework's robust and
efficient capability to handle material parameter identification for complex
hyperelastic constitutive models.

</details>


### [35] [Iterated Agent for Symbolic Regression](https://arxiv.org/abs/2510.08317)
*Zhuo-Yang Song,Zeyu Cai,Shutao Zhang,Jiashen Wei,Jichen Pan,Shi Qiu,Qing-Hong Cao,Tie-Jiun Hou,Xiaohui Liu,Ming-xing Luo,Hua Xing Zhu*

Main category: physics.comp-ph

TL;DR: IdeaSearchFitter uses LLMs as semantic operators in evolutionary search for symbolic regression, generating interpretable mathematical expressions guided by natural-language rationales.


<details>
  <summary>Details</summary>
Motivation: Traditional symbolic regression methods suffer from combinatorial explosion and overfitting, often producing complex, uninterpretable models. The paper aims to discover accurate yet conceptually coherent expressions.

Method: Framework employing Large Language Models as semantic operators within evolutionary search, generating candidate expressions guided by natural-language rationales to bias discovery toward interpretable models.

Result: Achieves competitive noise-robust performance on Feynman Symbolic Regression Database, discovers mechanistically aligned models on real-world data with good accuracy-complexity trade-offs, and derives compact physically-motivated parametrizations for Parton Distribution Functions.

Conclusion: IdeaSearchFitter effectively combines LLMs with evolutionary search to produce interpretable symbolic regression models across diverse scientific domains, demonstrating superior performance compared to traditional methods.

Abstract: Symbolic regression (SR), the automated discovery of mathematical expressions
from data, is a cornerstone of scientific inquiry. However, it is often
hindered by the combinatorial explosion of the search space and a tendency to
overfit. Popular methods, rooted in genetic programming, explore this space
syntactically, often yielding overly complex, uninterpretable models. This
paper introduces IdeaSearchFitter, a framework that employs Large Language
Models (LLMs) as semantic operators within an evolutionary search. By
generating candidate expressions guided by natural-language rationales, our
method biases discovery towards models that are not only accurate but also
conceptually coherent and interpretable. We demonstrate IdeaSearchFitter's
efficacy across diverse challenges: it achieves competitive, noise-robust
performance on the Feynman Symbolic Regression Database (FSReD), outperforming
several strong baselines; discovers mechanistically aligned models with good
accuracy-complexity trade-offs on real-world data; and derives compact,
physically-motivated parametrizations for Parton Distribution Functions in a
frontier high-energy physics application. IdeaSearchFitter is a specialized
module within our broader iterated agent framework, IdeaSearch, which is
publicly available at https://www.ideasearch.cn/.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [36] [Does Turbulence at the Correlation Scale Regulate the Statistics of Magnetic Reconnection?](https://arxiv.org/abs/2510.07502)
*M. B. Khan,M. A. Shay,S. Oughton,W. H. Matthaeus,C. C. Haggerty,S. Adhikari,P. A. Cassak,S. Fordin,D. O'Donnell,Y. Yang,R. Bandyopadhyay,S. Roy*

Main category: physics.plasm-ph

TL;DR: The study shows that magnetic reconnection rates in MHD turbulence are strongly correlated with global turbulent magnetic fields at the correlation scale, not dissipation scales, suggesting reconnection plays a major role in energy dissipation in astrophysical turbulence.


<details>
  <summary>Details</summary>
Motivation: To understand how magnetic reconnection events embedded in MHD turbulence relate to global turbulent field statistics and their role in energy dissipation.

Method: Direct numerical simulations of magnetic reconnection in MHD turbulence, analyzing relationships between reconnection properties and global turbulent field statistics.

Result: Reconnection rates distribution strongly correlates with global turbulent magnetic field magnitude at correlation scale; average reconnection and dissipation rates are much larger than predicted using field fluctuations at dissipation/kinetic scales.

Conclusion: Magnetic reconnection likely plays a major role in energy dissipation in astrophysical and heliospheric turbulence due to its strong correlation with global turbulent fields.

Abstract: We study the statistics of dynamical quantities associated with magnetic
reconnection events embedded in a sea of strong background magnetohydrodynamic
(MHD) turbulence using direct numerical simulations. We focus on the
relationship of the reconnection properties to the statistics of global
turbulent fields. For the first time, we show that the distribution in
turbulence of reconnection rates (determined by upstream fields) is strongly
correlated with the magnitude of the global turbulent magnetic field at the
correlation scale. The average reconnection rates, and associated dissipation
rates, during turbulence are thus much larger than predicted by using turbulent
magnetic field fluctuation amplitudes at the dissipation or kinetic scales.
Magnetic reconnection may therefore be playing a major role in energy
dissipation in astrophysical and heliospheric turbulence.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [37] [Non-Hermitian many-body localization in asymmetric chains with long-range interaction](https://arxiv.org/abs/2510.08277)
*Wen Wang,Han-Ze Li,Jian-Xin Zhong*

Main category: cond-mat.dis-nn

TL;DR: Study of non-Hermitian many-body localization in long-range interacting systems reveals coexistence of static/dynamic spectral phase transitions and ergodic-localized transitions with distinct phase boundaries.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between many-body localization and spectra in non-Hermitian many-body systems, particularly in clean systems with long-range interactions.

Method: Analysis of a one-dimensional clean system with long-range interaction-induced non-Hermitian many-body localization, examining spectral phase transitions and ergodic-localized transitions.

Result: Discovered coexistence of static and dynamic spectral real-complex phase transitions with many-body ergodic-localized phase transitions. Phase diagrams show similar non-monotonic boundary trends but do not overlap, revealing properties distinct from conventional disorder-induced systems.

Conclusion: The findings provide valuable insights into non-Hermitian many-body localization and spectra relationships in long-range interacting systems, with potential experimental realization in cold-atom systems.

Abstract: Understanding the relationship between many-body localization and spectra in
non-Hermitian many-body systems is crucial. In a one-dimensional clean,
long-range interaction-induced non-Hermitian many-body localization system, we
have discovered the coexistence of static and dynamic spectral real-complex
phase transitions, along with many-body ergodic-localized phase transitions.
The phase diagrams of these two types of transitions show similar non-monotonic
boundary trends but do not overlap, highlighting properties distinct from
conventional disorder-induced non-Hermitian many-body localization. We also
propose a potential experimental realization of this model in cold-atom
systems. Our findings provide valuable insights for further understanding the
relationship between non-Hermitian many-body localization and non-Hermitian
spectra in long-range interacting systems.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [38] [Asymptotically flat black holes with a singular Cauchy horizon and a spacelike singularity](https://arxiv.org/abs/2510.07431)
*Maxime Van de Moortel*

Main category: gr-qc

TL;DR: Construction of asymptotically flat black hole spacetimes with coexisting null and spacelike singularities using Einstein-Maxwell-scalar-field equations, featuring new gluing methods.


<details>
  <summary>Details</summary>
Motivation: To provide concrete examples of black holes with both null and spacelike singularities, addressing conjectured genericity in gravitational collapse models.

Method: Spacelike-characteristic gluing method between uncharged solutions and charged black hole event horizons; construction of one-ended and two-ended asymptotically flat spacetimes.

Result: First examples of black holes with coexisting null and spacelike singularities; terminal boundary has weakly singular null Cauchy horizon and strong spacelike singularity.

Conclusion: Successfully constructed black hole spacetimes exhibiting coexisting singularities, providing important models for gravitational collapse and supporting conjectured genericity beyond spherical symmetry.

Abstract: In our recent work [Van de Moortel, The coexistence of null and spacelike
singularities inside spherically symmetric black holes], we analyzed the
transition between null and spacelike singularities in spherically symmetric
dynamical black holes and demonstrated that the spacelike portion is described
by a Kasner metric with positive varying exponents that degenerate to $(1,0,0)$
near the null-spacelike transition. In the present paper, we provide examples
of global spacetimes satisfying the assumptions of this previous result and
apply its analysis to obtain a large class of asymptotically flat (spherically
symmetric) black hole spacetimes that exhibit coexisting null and spacelike
singularities. Our main results include:
  _The construction of one-ended asymptotically flat black hole spacetimes
solving the Einstein-Maxwell-charged-scalar-field equations. The proof relies
on a new spacelike-characteristic gluing method between any uncharged
spherically symmetric solution and the event horizon of a charged dynamical
black hole.
  _The construction of a large class of two-ended asymptotically flat black
hole spacetimes solving the Einstein-Maxwell-(uncharged)-scalar-field
equations.
  In both cases, we show that the terminal boundary in the black hole interior
only has two distinct components: a weakly singular (null) Cauchy horizon
$\mathcal{CH}_{i^+}$ where curvature blows up and a strong singularity
$\mathcal{S}=\{r=0\}$.
  Our construction provides the first examples of black holes with coexisting
null and spacelike singularities. These examples hold particular significance
in the one-ended case as a model of gravitational collapse, where this
phenomenon is conjecturally generic for the Einstein-scalar-field model, even
beyond spherical symmetry.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [39] [Dimension- and Facet-Dependent Altermagnetic Triferroics and Biferroics in CrSb](https://arxiv.org/abs/2510.07771)
*Long Zhang,Guoying Gao*

Main category: cond-mat.mtrl-sci

TL;DR: This paper investigates CrSb as a model system for altermagnetic multiferroics, predicting various ferroic properties across different phases and facets, including the discovery of triferroics in WZ-phase (110) facets with reversible spin switching.


<details>
  <summary>Details</summary>
Motivation: Altermagnets have gained interest for their unique spin properties, but altermagnetic multiferroics, especially triferroics, remain rare. The study aims to explore how dimensionality and facet orientation affect ferroic properties in CrSb.

Method: Using first-principles calculations, the authors analyzed CrSb in various phases (NiAs, MnP, wurtzite, zincblende, rocksalt) and examined different facets to study their ferroic properties and magnetic anisotropy.

Result: Predicted altermagnetism in MnP phase CrSb, identified AM-ferroelastic biferroics in NiAs- and MnP-phase (110) facets, and discovered FM/AM-FE-FC triferroics in WZ-phase (110) facets with moderate switching barriers. Both FE and FC switching can reverse AM spin splitting while preserving high spin polarization.

Conclusion: The work establishes a framework for designing altermagnetic multiferroics through polymorphic, dimensional, and facet engineering, offering promising opportunities for multifunctional spintronic applications.

Abstract: Altermagnets have recently garnered significant interest due to their
vanishing net magnetic moment and non-relativistic momentum-dependent spin
splitting. However, altermagnetic (AM) multiferroics especially triferroics
remain scarce. We investigate the experimentally synthesized non-van der Waals
CrSb as a model system to explore the effects of dimensionality and facet
orientation on its ferroic properties. NiAs, MnP, wurtzite (WZ), zincblende
(ZB), and rocksalt (RS) phases are considered. Using first-principles
calculations, we predict the altermagnetism of CrSb in MnP phase which has
comparable stability with experimental NiAs phase. Both NiAs- and MnP-phase
(110) facets exhibit AM-ferroelastic (FC) biferroics, while the WZ-phase bulk
and (001) facets host ferromagnetic (FM) or AM-ferroelectric (FE) biferroics.
Notably, the WZ-phase (110) facets are identified as FM/AM-FE-FC triferroics,
with moderate energy barriers of 0.129 and 0.363 eV atom-1 for FE and FC
switching, respectively. Both FE and FC switching can reverse the AM spin
splitting in antiferromagnetic (AFM) configurations while preserving the high
spin polarization in FM states. The magnetic anisotropy is highly tunable,
exhibiting either uniaxial or in-plane behavior depending on the phase,
dimension, and facet. This work establishes a framework for designing AM
multiferroics through polymorphic, dimensional, and facet engineering, offering
promising avenues for multifunctional spintronic applications.

</details>


### [40] [Modulating thermal conductivity of bulk BAs based on targeted phonon excitation](https://arxiv.org/abs/2510.07934)
*Tianhao Li,Yangjun Qin,Dongkai Pan,Han Meng,Nuo Yang*

Main category: cond-mat.mtrl-sci

TL;DR: A reversible phonon excitation strategy enables dynamic modulation of boron arsenide's thermal conductivity, allowing both enhancement (up to 2%) and suppression (up to 35%) through selective phonon mode excitation.


<details>
  <summary>Details</summary>
Motivation: Address the conflicting thermal conductivity requirements in electronics (needing high conductivity) and thermoelectrics (needing low conductivity) by developing a dynamic control method.

Method: Using first-principles calculations and Boltzmann transport equation to selectively excite specific phonon modes and analyze their effects on thermal transport properties.

Result: Thermal conductivity can be enhanced by 2% or suppressed by 35% at excitation multiplier of 25, and enhanced by 2% or decreased by 11% at multiplier of 5. Modulation depends on excitation frequency, multiplier, and intrinsic phonon properties.

Conclusion: This reversible phonon excitation approach provides a dynamic route to tune thermal conductivity for applications in thermal management and thermoelectric energy conversion.

Abstract: This study proposes a reversible phonon excitation strategy to dynamically
modulate the thermal conductivity of boron arsenide (BAs), addressing the
opposing thermal conductivity requirements in electronics and thermoelectrics.
Using first-principles calculations and Boltzmann transport equation, we
demonstrate that selective excitation of specific phonon modes enables active
control over thermal transport. At an excitation multiplier of 25, the thermal
conductivity of BAs can be enhanced by up to 2% or suppressed by up to 35%
relative to its intrinsic value of 2235 W m^-1 K^-1. At a lower multiplier of
5, thermal conductivity can be increased by 2% or decreased by 11%. The
modulation effect depends on excitation frequency, multiplier, and intrinsic
phonon properties, with certain frequencies exhibiting opposite trends under
different excitation intensities. Mechanistic analysis shows that at low
excitation levels, phonon splitting suppresses Umklapp scattering, reducing the
scattering rate, while at high levels, it enhances Normal scattering,
increasing the scattering rate. This approach offers a dynamic and reversible
route to tuning thermal conductivity, with applications in thermal management
and thermoelectric energy conversion.

</details>


### [41] [Rare-Earth Engineering of NaAlO3 Perovskites Unlocks Unified Optoelectronic, Thermoelectric, and Spintronic Functionalities](https://arxiv.org/abs/2510.08130)
*Muhammad Imran,Sikander Azam,Qaiser Rafiq,Amin Ur Rahman*

Main category: cond-mat.mtrl-sci

TL;DR: Rare-earth doping (Eu3+, Gd3+, Tb3+) in NaAlO3 perovskite reduces band gap from 6.2 eV to ~3.1 eV, enables visible-light absorption, and creates diverse electronic behaviors including half-metallicity, spin-selective metallicity, and p-type semiconducting properties with enhanced thermoelectric performance.


<details>
  <summary>Details</summary>
Motivation: Perovskite oxides like NaAlO3 have potential for energy and quantum technologies but suffer from wide band gaps (deep-UV absorption) and limited carrier transport, limiting their practical applications.

Method: First-principles GGA+U+SOC calculations to investigate electronic, optical, elastic, and thermoelectric properties of Eu3+, Gd3+, and Tb3+-doped NaAlO3.

Result: Rare-earth substitution is thermodynamically favorable (formation energies 1.2-1.6 eV), reduces band gap to ~3.1 eV, creates diverse electronic behaviors (half-metallicity, spin-selective metallicity, p-type semiconducting), red-shifts absorption edge to 2.0-2.2 eV, enhances dielectric response (ε₁(0) ~95 for Eu), and improves thermoelectric performance (Seebeck >210 μV/K, ZT ~0.45 at 500 K).

Conclusion: Rare-earth-doped NaAlO3 serves as a multifunctional perovskite platform suitable for photovoltaics, photocatalysis, thermoelectrics, and spintronics applications.

Abstract: Perovskite oxides are promising for energy and quantum technologies, but
wide-gap hosts such as NaAlO3 suffer from deep-UV absorption and limited
carrier transport. Using first-principles GGA+U+SOC calculations, we
investigate Eu3+-, Gd3+-, and Tb3+-doped NaAlO3 and evaluate their electronic,
optical, elastic, and thermoelectric properties. Rare-earth substitution is
thermodynamically favorable (formation energies 1.2-1.6 eV) and induces strong
f-p hybridization, reducing the pristine band gap (about 6.2 eV) to about 3.1
eV for Tb. Spin-resolved band structures reveal Gd-driven half-metallicity,
Eu-induced spin-selective metallicity, and Tb-stabilized p-type semiconducting
behavior. The optical spectra show a red-shifted absorption edge (about 2.0-2.2
eV), a large static dielectric response (epsilon1(0) about 95 for Eu), and
plasmonic resonances near 4 eV, enabling visible-light harvesting. Elastic
analysis indicates mild lattice softening with preserved ductility (Pugh ratio
B/G about 1.56-1.57). Thermoelectric performance is enhanced, with Seebeck
coefficients greater than 210 uV/K for Eu and Tb and ZT about 0.45 at 500 K.
These results identify rare-earth-doped NaAlO3 as a multifunctional perovskite
platform for photovoltaics, photocatalysis, thermoelectrics, and spintronics.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [42] [Time-causal and time-recursive wavelets](https://arxiv.org/abs/2510.05834)
*Tony Lindeberg*

Main category: eess.SP

TL;DR: This paper presents a time-causal wavelet analysis method for real-time temporal signals using truncated exponential kernels in cascade, ensuring no future information access while maintaining temporal scale covariance.


<details>
  <summary>Details</summary>
Motivation: Real-time signal processing requires truly time-causal computational mechanisms where future signals cannot be accessed, necessitating wavelet analysis methods that operate solely on past and present data.

Method: Based on temporal scale-space theory, the method uses convolution with truncated exponential kernels in cascade and their temporal derivatives to create time-causal wavelets. A specific choice of time constants ensures temporal scale covariance and self-similarity across scales.

Result: The proposed time-causal wavelet representation can reflect the duration of locally dominant temporal structures in input signals and maintains continuous scaling properties in discrete implementation.

Conclusion: This time-causal wavelet analysis is valuable for real-time signal processing tasks, especially for signals with rich temporal scale variations or physical/biophysical phenomena requiring physically realistic time-causal analysis.

Abstract: When to apply wavelet analysis to real-time temporal signals, where the
future cannot be accessed, it is essential to base all the steps in the signal
processing pipeline on computational mechanisms that are truly time-causal.
  This paper describes how a time-causal wavelet analysis can be performed
based on concepts developed in the area of temporal scale-space theory,
originating from a complete classification of temporal smoothing kernels that
guarantee non-creation of new structures from finer to coarser temporal scale
levels. By necessity, convolution with truncated exponential kernels in cascade
constitutes the only permissable class of kernels, as well as their temporal
derivatives as a natural complement to fulfil the admissibility conditions of
wavelet representations. For a particular way of choosing the time constants in
the resulting infinite convolution of truncated exponential kernels, to ensure
temporal scale covariance and thus self-similarity over temporal scales, we
describe how mother wavelets can be chosen as temporal derivatives of the
resulting time-causal limit kernel.
  By developing connections between wavelet theory and scale-space theory, we
characterize and quantify how the continuous scaling properties transfer to the
discrete implementation, demonstrating how the proposed time-causal wavelet
representation can reflect the duration of locally dominant temporal structures
in the input signals.
  We propose that this notion of time-causal wavelet analysis could be a
valuable tool for signal processing tasks, where streams of signals are to be
processed in real time, specifically for signals that may contain local
variations over a rich span of temporal scales, or more generally for analysing
physical or biophysical temporal phenomena, where a fully time-causal analysis
is called for to be physically realistic.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [43] [Stress concentration via quasi-Minnaert resonance in bubble-elastic structures and applications](https://arxiv.org/abs/2510.06892)
*Ruixiang Tang,Huaian Diao,Hongyu Liu,Weisheng Zhou*

Main category: math-ph

TL;DR: This paper analyzes stress concentration in bubbly-elastic structures using quasi-Minnaert resonance, which creates localized stress patterns near bubble boundaries through wave interactions between acoustic and elastic fields.


<details>
  <summary>Details</summary>
Motivation: Stress concentration in bubble-elastic scattering has important applications in engineering blasting and medical treatments, but the complex interaction between acoustic and elastic waves presents analytical challenges.

Method: The study employs layer potential theory and asymptotic analysis to rigorously establish stress concentration and quasi-Minnaert resonance phenomena in a radially symmetric bubble-elastic model. Numerical experiments are conducted for various bubble geometries.

Result: The research demonstrates how quasi-Minnaert resonance induces stress concentration near bubble boundaries through appropriate selection of incident elastic waves and high-contrast structures. Two distinct wave patterns are observed: boundary localization and high-oscillation phenomena.

Conclusion: The findings enhance understanding of stress concentration mechanisms and their practical applications in engineering blasting and medical therapies, providing mathematical foundations for controlling stress patterns in bubbly-elastic systems.

Abstract: Stress concentration in bubble-elastic scattering scenarios has significant
applications in engineering blasting and medical treatments. This study
provides a comprehensive mathematical analysis of stress concentration in
bubbly-elastic structures, induced by the quasi-Minnaert resonance. The
quasi-Minnaert resonance manifests as two distinct wave patterns near the
bubble's boundary: boundary localization and high-oscillation phenomena. We
demonstrate how to leverage the quasi-Minnaert resonance to induce stress
concentration in the elastic total wave field near the air bubble's boundary by
appropriately selecting the incident elastic wave and high-contrast structure.
The interaction between the air bubble and the elastic background couples two
physical wave fields-acoustic and elastic waves-across the bubble's boundary.
The intricate transmission conditions, combined with the scalar nature of
acoustic waves and the vectorial nature of elastic waves, present significant
analytical challenges. To address these, we employ layer potential theory and
asymptotic analysis to rigorously establish the stress concentration and
quasi-Minnaert resonance phenomena in a radially geometry bubble-elastic model.
Extensive numerical experiments are conducted to demonstrate the stress
concentration phenomenon alongside quasi-Minnaert resonance for various bubble
geometries, including a unit disk, a corner domain, an apple-shaped domain in
$\mathbb{R}^2$, and a ball in $\mathbb{R}^3$. The findings of this study
enhance the understanding of stress concentration mechanisms and their
applications in engineering blasting and medical therapies.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Targeted Digital Twin via Flow Map Learning and Its Application to Fluid Dynamics](https://arxiv.org/abs/2510.07549)
*Qifan Chen,Zhongshu Xu,Jinjin Zhang,Dongbin Xiu*

Main category: cs.LG

TL;DR: A numerical framework for creating targeted digital twins (tDTs) that directly model quantities of interest using memory-based flow map learning from short trajectory data bursts, enabling efficient long-term predictions without full system simulations.


<details>
  <summary>Details</summary>
Motivation: To achieve substantial computational savings by developing compact digital twins that can efficiently predict long-term dynamics of specific quantities of interest without requiring expensive full system simulations.

Method: Uses memory-based flow map learning (FML) with short bursts of trajectory data from repeated full digital twin executions to create data-driven models of quantities of interest, making the construction entirely offline.

Result: The framework successfully creates compact dynamical systems that accurately predict long-term hydrodynamic forces on a cylinder in CFD simulations, entirely bypassing full flow simulations while maintaining accuracy.

Conclusion: The proposed tDT framework provides an effective approach for computationally efficient long-term prediction of quantities of interest, demonstrated through successful application to computational fluid dynamics problems with significant computational savings.

Abstract: We present a numerical framework for constructing a targeted digital twin
(tDT) that directly models the dynamics of quantities of interest (QoIs) in a
full digital twin (DT). The proposed approach employs memory-based flow map
learning (FML) to develop a data-driven model of the QoIs using short bursts of
trajectory data generated through repeated executions of the full DT. This
renders the construction of the FML-based tDT an entirely offline computational
process. During online simulation, the learned tDT can efficiently predict and
analyze the long-term dynamics of the QoIs without requiring simulations of the
full DT system, thereby achieving substantial computational savings. After
introducing the general numerical procedure, we demonstrate the construction
and predictive capability of the tDT in a computational fluid dynamics (CFD)
example: two-dimensional incompressible flow past a cylinder. The QoIs in this
problem are the hydrodynamic forces exerted on the cylinder. The resulting tDTs
are compact dynamical systems that evolve these forces without explicit
knowledge of the underlying flow field. Numerical results show that the tDTs
yield accurate long-term predictions of the forces while entirely bypassing
full flow simulations.

</details>


### [45] [Unveiling the Power of Multiple Gossip Steps: A Stability-Based Generalization Analysis in Decentralized Training](https://arxiv.org/abs/2510.07980)
*Qinglun Li,Yingqi Liu,Miao Zhang,Xiaochun Cao,Quanjun Yin,Li Shen*

Main category: cs.LG

TL;DR: Multi-Gossip Steps (MGS) bridge decentralized and centralized training, reducing performance gaps but cannot fully eliminate them. Theoretical analysis shows MGS exponentially reduces optimization error but leaves a non-negligible generalization gap compared to centralized training.


<details>
  <summary>Details</summary>
Motivation: Decentralized training is communication-efficient but suffers from performance degradation compared to centralized training. MGS helps reduce this gap, but the theoretical reasons for its effectiveness and whether the gap can be fully eliminated remain unknown.

Method: Used stability analysis to derive upper bounds on generalization error and excess error of MGS. Provided unified analysis of factors like learning rate, data heterogeneity, node count, per-node sample size, and communication topology impact on MGS generalization under non-convex settings without bounded gradients assumption.

Result: 1) MGS reduces optimization error bound at exponential rate, tightening generalization error bound. 2) Even with infinite MGS, a non-negligible generalization gap remains compared to centralized mini-batch SGD. Experiments on CIFAR datasets support theoretical findings.

Conclusion: MGS significantly improves decentralized training performance but cannot fully close the gap to centralized training. The theoretical analysis provides comprehensive understanding of how various factors impact MGS generalization performance in decentralized settings.

Abstract: Decentralized training removes the centralized server, making it a
communication-efficient approach that can significantly improve training
efficiency, but it often suffers from degraded performance compared to
centralized training. Multi-Gossip Steps (MGS) serve as a simple yet effective
bridge between decentralized and centralized training, significantly reducing
experiment performance gaps. However, the theoretical reasons for its
effectiveness and whether this gap can be fully eliminated by MGS remain open
questions. In this paper, we derive upper bounds on the generalization error
and excess error of MGS using stability analysis, systematically answering
these two key questions. 1). Optimization Error Reduction: MGS reduces the
optimization error bound at an exponential rate, thereby exponentially
tightening the generalization error bound and enabling convergence to better
solutions. 2). Gap to Centralization: Even as MGS approaches infinity, a
non-negligible gap in generalization error remains compared to centralized
mini-batch SGD ($\mathcal{O}(T^{\frac{c\beta}{c\beta +1}}/{n m})$ in
centralized and $\mathcal{O}(T^{\frac{2c\beta}{2c\beta +2}}/{n
m^{\frac{1}{2c\beta +2}}})$ in decentralized). Furthermore, we provide the
first unified analysis of how factors like learning rate, data heterogeneity,
node count, per-node sample size, and communication topology impact the
generalization of MGS under non-convex settings without the bounded gradients
assumption, filling a critical theoretical gap in decentralized training.
Finally, promising experiments on CIFAR datasets support our theoretical
findings.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [46] [Sharp Non-uniqueness in Law for Stochastic Differential Equations on the Whole Space](https://arxiv.org/abs/2510.08248)
*Huaxiang Lü,Michael Röckner*

Main category: math.PR

TL;DR: The paper constructs divergence-free drift fields in L_t^rL^p spaces that lead to non-unique weak solutions for stochastic differential equations on R^d (d≥2), with sharpness established by comparison to known uniqueness results.


<details>
  <summary>Details</summary>
Motivation: To investigate the boundary between uniqueness and non-uniqueness for SDEs with irregular drift coefficients, specifically constructing examples where multiple weak solutions exist despite the drift satisfying certain integrability conditions.

Method: Using convex integration methods adapted to all of R^d (not just the torus) and refined heat kernel estimates to construct two distinct probability solutions to the associated Fokker-Planck equation.

Result: For drifts in L_t^rL^p with d/p + 1/r > 1, the SDE admits at least two distinct weak solutions from each initial measure, and there exists a measurable set A with positive Lebesgue measure where every point gives rise to non-unique solutions.

Conclusion: The result is sharp as it matches the known uniqueness threshold (C_tL^{d+}), demonstrating the precise conditions under which SDEs with irregular drifts exhibit non-uniqueness of weak solutions.

Abstract: In this paper, we investigate the stochastic differential equation on
$\mathbb{R}^d,d\geq2$: \begin{align*}
  \dif X_t&=v(t,X_t)\dif t+\sqrt{2} \dif W_t. \end{align*} For any finite
collection of initial probability measures $\{\mu^i_0\}_{1\leq i\leq M}$ on
$\mathbb{R}^d$ and $\frac{d}{p}+\frac{1}{r}>1$, we construct a divergence-free
drift field $v\in L_t^rL^p\cap C_tL^{d-}$ such that the associated SDE admits
at least two distinct weak solutions originating from each initial measure
$\mu^i_0$. This result is sharp in view of the well-known uniqueness of strong
solutions for drifts in $C_tL^{d+}$, as established in \cite{KR05}. As a
corollary, there exists a measurable set $A\subset\mathbb{R}^d$ with positive
Lebesgue measure such that for any $x\in A$, the SDE with drift $v$ admits at
least two weak solutions when with start in $x\in A$. The proof proceeds by
constructing two distinct probability solutions to the associated Fokker-Planck
equation via a convex integration method adapted to all of $\mathbb{R}^d$
(instead of merely the torus), together with refined heat kernel estimate.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [47] [Scalings and simulation requirements in two-phase flows](https://arxiv.org/abs/2510.07727)
*Luis H. Hatashita,Pranav Nathan,Suhas S. Jain*

Main category: physics.flu-dyn

TL;DR: The paper derives scaling laws for grid resolution and computational cost in two-phase flow DNS, proposing new length scale ratios and a dimensionless number (Ris) to classify turbulence regimes, with convergence criteria established for accurate simulations.


<details>
  <summary>Details</summary>
Motivation: There are established standards for DNS in single-phase turbulent flows, but similar guidance is lacking for two-phase flows, creating a need for systematic resolution requirements and cost estimation methods.

Method: High-fidelity simulations of stationary two-phase homogeneous isotropic turbulence were used to evaluate convergence of interfacial area, size distribution, SMD, and curvature distribution, deriving length scale ratios for different flow regimes.

Result: Key scalings were derived: η_KH/η ∼ We_L^{-3/5}Re_L^{3/4} for inertia-dominated regime and η_KV/η ∼ Ca_L^{-1}Re_L^{3/4} for viscous-dominated regime, with computational costs increasing as We_L^{12/5} and Ca_L^4 respectively. Grid resolution of k_maxη_KH ≥ 60 is required for second-order schemes.

Conclusion: The proposed criteria provide valuable tools for determining grid resolution and time-step requirements a-priori for two-phase flow DNS, accelerating physics discovery and model development through established guidelines and best practices.

Abstract: In this work, important two-phase flow scalings are derived, which enable the
quantification of grid-point and time-step requirements as functions of Re, We,
and Ca numbers. The adequate grid resolution is determined in the
inertia-dominated regime with the aid of high-fidelity simulations of
stationary two-phase homogeneous isotropic turbulence by evaluating convergence
of total interfacial area, size distribution, SMD, and curvature distribution.
Although standards for DNS for single-phase turbulence flow exist, there is a
lack of similar guidance in two-phase flows. Therefore, length scale ratios of
the Kolmogorov-Hinze to the Kolmogorov scale of \eta_{KH}/\eta \sim
We_{L}^{-3/5}Re_{L}^{3/4} in the inertia-dominated regime and the
Kolmogorov-viscous to Kolmogorov scale of \eta_{KV}/\eta \sim
Ca_{L}^{-1}Re_{L}^{3/4} for the viscous-dominated regime, are constructed.
These scalings imply a computational cost increase like We_{L}^{12/5} and
Ca_{L}^4, in the inertia-dominated and viscous-dominated regimes, respectively.
A novel dimensionless number, coined as the ratio of interface scales (Ris), is
proposed to aid in the classification of the turbulence regimes in the presence
of an interface. Convergence of the total interfacial area, size distribution,
SMD, and curvature distribution are observed for grid resolutions of k_{\max}
\eta_{KH} \geq 60$ for second-order schemes. Furthermore, it is observed that
this lower bound is the minimum required to capture intermittent events
responsible for the increase of instantaneous total interfacial area. This
criterion will be a valuable tool for determining grid resolution and time-step
requirements a-priori for DNS of two-phase flows and for estimating the
corresponding computational cost. This work provides guidelines and best
practices for numerical simulations of two-phase flows, which will accelerate
physics discovery and model development.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [48] [A Geomechanically-Informed Framework for Wellbore Trajectory Prediction: Integrating First-Principles Kinematics with a Rigorous Derivation of Gated Recurrent Networks](https://arxiv.org/abs/2510.07564)
*Shubham Kumar,Anshuman Sahoo*

Main category: physics.geo-ph

TL;DR: This paper develops a geomechanically-informed data-driven framework using GRU networks for wellbore trajectory prediction, with mathematical derivations from first principles and validation on Gulfaks oil field data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurate wellbore trajectory prediction by moving beyond empirical modeling to a comprehensive, mathematically rigorous framework that accounts for complex drilling assembly-geological formation interactions.

Method: Uses LAS and DEV data from 14 wells; treats petrophysical logs as proxies for rock mechanical properties; derives wellbore kinematic models from vector calculus/differential geometry; implements GRU network with complete forward propagation and BPTT derivations; includes data preprocessing pipeline with normalization, resampling, and sequence generation.

Result: The model performance is evaluated using MAE, RMSE, and R2 metrics for trajectory prediction accuracy, though specific numerical results are not provided in the abstract.

Conclusion: The research establishes a comprehensive framework that combines geomechanical principles with data-driven approaches for improved wellbore trajectory prediction, with detailed mathematical foundations that clarify the learning mechanisms of temporal dependencies.

Abstract: Accurate wellbore trajectory prediction is a paramount challenge in
subsurface engineering, governed by complex interactions between the drilling
assembly and heterogeneous geological formations. This research establishes a
comprehensive, mathematically rigorous framework for trajectory prediction that
moves beyond empirical modeling to a geomechanically-informed, data-driven
surrogate approach.The study leverages Log ASCII Standard (LAS) and wellbore
deviation (DEV) data from 14 wells in the Gulfaks oil field, treating
petrophysical logs not merely as input features, but as proxies for the
mechanical properties of the rock that fundamentally govern drilling dynamics.
A key contribution of this work is the formal derivation of wellbore kinematic
models, including the Average Angle method and Dogleg Severity, from the first
principles of vector calculus and differential geometry, contextualizing them
as robust numerical integration schemes. The core of the predictive model is a
Gated Recurrent Unit (GRU) network, for which we provide a complete,
step-by-step derivation of the forward propagation dynamics and the
Backpropagation Through Time (BPTT) training algorithm. This detailed
theoretical exposition, often omitted in applied studies, clarifies the
mechanisms by which the network learns temporal dependencies. The methodology
encompasses a theoretically justified data preprocessing pipeline, including
feature normalization, uniform depth resampling, and sequence generation.
Trajectory post-processing and error analysis are conducted using Mean Absolute
Error (MAE), Root Mean Square Error (RMSE), and the Coefficient of
Determination (R2).

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [49] [Quantum Grid Path Planning Using Parallel QAOA Circuits Based on Minimum Energy Principle](https://arxiv.org/abs/2510.07413)
*Jun Liu*

Main category: quant-ph

TL;DR: This paper proposes a quantum path planning solution using parallel QAOA architecture to overcome classical NP problems and NISQ-era limitations, achieving optimal path finding through quantum energy state mapping and filtering.


<details>
  <summary>Details</summary>
Motivation: To address the bottleneck of classical path planning in solving NP problems and overcome limitations of current quantum path planning frameworks in the NISQ era.

Method: Map grid path planning to finding minimum quantum energy state; build two parallel QAOA circuits for connectivity energy and path energy calculations; use classical algorithm to filter unreasonable solutions; merge results from parallel circuits.

Result: With appropriate filter parameters, quantum states with low occurrence probabilities can be filtered out, increasing target state probability. Even with circuit layer p=1, optimal path coding can be found using filter's critical role. Parallel circuits outperform serial ones in finding optimal feasible path combinations.

Conclusion: Parallel QAOA circuits with filtering mechanism provide an effective quantum path planning solution that can find optimal paths even with shallow circuits, demonstrating significant advantages over serial approaches in the NISQ era.

Abstract: To overcome the bottleneck of classical path planning schemes in solving NP
problems and address the predicament faced by current mainstream quantum path
planning frameworks in the Noisy Intermediate-Scale Quantum (NISQ) era, this
study attempts to construct a quantum path planning solution based on parallel
Quantum Approximate Optimization Algorithm (QAOA) architecture. Specifically,
the grid path planning problem is mapped to the problem of finding the minimum
quantum energy state. Two parallel QAOA circuits are built to simultaneously
execute two solution processes, namely connectivity energy calculation and path
energy calculation. A classical algorithm is employed to filter out
unreasonable solutions of connectivity energy, and finally, the approximate
optimal solution to the path planning problem is obtained by merging the
calculation results of the two parallel circuits. The research findings
indicate that by setting appropriate filter parameters, quantum states
corresponding to position points with extremely low occurrence probabilities
can be effectively filtered out, thereby increasing the probability of
obtaining the target quantum state. Even when the circuit layer number p is
only 1, the theoretical solution of the optimal path coding combination can
still be found by leveraging the critical role of the filter. Compared with
serial circuits, parallel circuits exhibit a significant advantage, as they can
find the optimal feasible path coding combination with the highest probability.

</details>


### [50] [When Less is More: Approximating the Quantum Geometric Tensor with Block Structures](https://arxiv.org/abs/2510.08430)
*Ahmedeo Shokry,Alessandro Santini,Filippo Vicentini*

Main category: quant-ph

TL;DR: A block-diagonal quantum geometric tensor is introduced to reduce computational cost in neural quantum states optimization by partitioning the metric by network layers, improving conditioning and scalability.


<details>
  <summary>Details</summary>
Motivation: The natural gradient is crucial for neural quantum states optimization but limited by the high computational cost of computing and inverting the quantum geometric tensor.

Method: Introduce a block-diagonal quantum geometric tensor that partitions the metric by network layers, similar to block-structured Fisher methods like K-FAC, to preserve essential curvature while removing noisy cross-layer correlations.

Result: Experiments on Heisenberg and frustrated J1-J2 models demonstrate faster convergence, lower energy, and improved stability compared to standard approaches.

Conclusion: The layer-wise approximation of the quantum geometric tensor provides an effective method to reduce computational costs while maintaining optimization performance in neural quantum states.

Abstract: The natural gradient is central in neural quantum states optimizations but it
is limited by the cost of computing and inverting the quantum geometric tensor,
the quantum analogue of the Fisher information matrix. We introduce a
block-diagonal quantum geometric tensor that partitions the metric by network
layers, analogous to block-structured Fisher methods such as K-FAC. This
layer-wise approximation preserves essential curvature while removing noisy
cross-layer correlations, improving conditioning and scalability. Experiments
on Heisenberg and frustrated $J_1$-$J_2$ models show faster convergence, lower
energy, and improved stability.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [51] [Minimal Denominators Lying in Subsets of the Ring of Polynomials over a Finite Field](https://arxiv.org/abs/2510.07787)
*Noy Soffer Aranov*

Main category: math.NT

TL;DR: The paper proves that for any infinite subset of polynomials over finite fields, the probability distribution functions of two different approximation problems are exactly equal, which is stronger than the analogous result in the real setting.


<details>
  <summary>Details</summary>
Motivation: To study the distribution of smallest denominators in rational approximations over finite fields and compare it with discrete analogues, extending results from the real number setting to the function field setting.

Method: The authors analyze two related problems: (1) the distribution of smallest denominators in rational approximations of vectors in function fields, and (2) the discrete analogue using polynomial denominators with fixed degree. They prove equality of the probability distribution functions for any infinite subset of polynomials.

Result: For any infinite subset of polynomials over finite fields, any degree n, and any dimension m, the probability distribution functions of both random variables (smallest denominator problem and discrete analogue) are exactly equal.

Conclusion: The result shows a stronger equivalence in the function field setting compared to the real number setting, where only asymptotic closeness of averages was previously established without denominator restrictions.

Abstract: Given a subset $\mathcal{S}\subseteq \mathbb{F}_q[x]$ and fixed $n,m\in
\mathbb{N}$, one can study the distribution of the value of the smallest
denominator $Q\in \mathcal{S}$, for which there exists $\mathbf{P}\in
\mathbb{F}_q[x]^m$ such that $\frac{P}{Q}\in B(\boldsymbol{\alpha},q^{-n})$,
where $Q\in \mathcal{S}$. On the other hand, one can study the discrete
analogue, when $N\in \mathbb{F}_q[x]$ is a polynomial with $\deg(N)=n$ and
$\boldsymbol{\alpha}\in \frac{1}{N}\mathbb{F}_q[x]^m$ as a discrete probability
distribution function. We prove that for any infinite subset
$\mathcal{S}\subseteq \mathbb{F}_q[x]$, for any $n\in \mathbb{N}$, and for any
dimension $m$, the probability distribution functions of both these random
variables are equal to one another. This is significantly stronger than the
real setting, where Balazard and Martin proved that these functions have
asymptotically close averages, when there are no restrictions on the
denominators.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [52] [Topology optimization of nonlinear forced response curves via reduction on spectral submanifolds](https://arxiv.org/abs/2510.07900)
*Hongming Liang,Matteo Pozzi,Jacopo Marconi,Shobhit Jain,Mingwu Li*

Main category: eess.SY

TL;DR: The paper presents a topology optimization framework using spectral submanifolds (SSMs) reduction to efficiently optimize nonlinear dynamic responses in high-dimensional systems, particularly for MEMS devices.


<details>
  <summary>Details</summary>
Motivation: Topology optimization for nonlinear systems is limited by high computational costs of repeated response and sensitivity analyses in high-dimensional systems.

Method: Uses spectral submanifolds (SSMs) reduction theory to reformulate periodic responses as equilibria of reduced-order models, enabling analytic evaluation of responses and sensitivities.

Result: Successfully applied to optimize peak amplitude, hardening/softening behavior, and saddle-node bifurcation distances in nonlinear MEMS devices.

Conclusion: Provides a practical and efficient strategy for incorporating nonlinear dynamic effects into topology optimization of structures.

Abstract: Forced response curves (FRCs) of nonlinear systems can exhibit complex
behaviors, including hardening/softening behavior and bifurcations. Although
topology optimization holds great potential for tuning these nonlinear dynamic
responses, its use in high-dimensional systems is limited by the high cost of
repeated response and sensitivity analyses. To address this challenge, we
employ the spectral submanifolds (SSMs) reduction theory, which reformulates
the periodic response as the equilibria of an associated reduced-order model
(ROM). This enables efficient and analytic evaluation of both response
amplitudes and their sensitivities. Based on the SSM-based ROM, we formulate
optimization problems that optimize the peak amplitude, the hardening/softening
behavior, and the distance between two saddle-node bifurcations for an FRC. The
proposed method is applied to the design of nonlinear MEMS devices, achieving
targeted performance optimization. This framework provides a practical and
efficient strategy for incorporating nonlinear dynamic effects into the
topology optimization of structures.

</details>


### [53] [A Stable, Accurate and Well-Conditioned Time-Domain PMCHWT Formulation](https://arxiv.org/abs/2510.07989)
*Van Chien Le,Cedric Munger,Francesco P. Andriulli,Kristof Cools*

Main category: eess.SY

TL;DR: A new boundary element method for transient electromagnetic scattering using time-domain PMCHWT equations with Calderon preconditioning and quasi-Helmholtz rescaling to overcome dense-mesh, large-timestep, and late-time instability issues.


<details>
  <summary>Details</summary>
Motivation: To develop a stable and accurate boundary element formulation for transient electromagnetic scattering by dielectric objects that addresses common numerical breakdown problems including dense-mesh breakdown, large-timestep breakdown, and late-time instability.

Method: Uses time-domain PMCHWT equation with multiplicative Calderon preconditioner, quasi-Helmholtz projectors for rescaling Helmholtz components, temporal differentiation/integration as rescaling operators, and marching-on-in-time scheme with iterative solvers.

Result: Numerical experiments on simply- and multiply-connected dielectric scatterers with non-smooth geometries demonstrate the method's accuracy, stability, and efficiency.

Conclusion: The proposed approach successfully resolves multiple numerical breakdown issues in transient electromagnetic scattering analysis while maintaining accuracy and stability across various dielectric scatterer geometries.

Abstract: This paper introduces a new boundary element formulation for transient
electromagnetic scattering by homogeneous dielectric objects based on the
time-domain PMCHWT equation. To address dense-mesh breakdown, a multiplicative
Calderon preconditioner utilizing a modified static electric field integral
operator is employed. Large-timestep breakdown and late-time instability are
simultaneously resolved by rescaling the Helmholtz components leveraging the
quasi-Helmholtz projectors and using temporal differentiation and integration
as rescaling operators. This rescaling also balances the loop and star
components at large timesteps, improving solution accuracy. The resulting
discrete system is solved using a marching-on-in-time scheme and iterative
solvers. Numerical experiments for simply- and multiply-connected dielectric
scatterers, including highly non-smooth geometries, corroborate the accuracy,
stability, and efficiency of the proposed approach.

</details>
