<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 8]
- [math.AP](#math.AP) [Total: 15]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [quant-ph](#quant-ph) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [math.CA](#math.CA) [Total: 2]
- [cs.LG](#cs.LG) [Total: 3]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Augmented data and neural networks for robust epidemic forecasting: application to COVID-19 in Italy](https://arxiv.org/abs/2510.09192)
*Giacomo Dimarco,Federica Ferrarese,Lorenzo Pareschi*

Main category: math.NA

TL;DR: A data augmentation method using compartmental models with uncertainty to generate synthetic training data improves neural network prediction accuracy, with NAR models excelling in short-term forecasting and PINNs capturing long-term qualitative behavior.


<details>
  <summary>Details</summary>
Motivation: To enhance neural network training and prediction accuracy by addressing data limitations through synthetic data generation that incorporates physical constraints and uncertainty.

Method: Generate synthetic data using compartmental models calibrated with real data, then integrate with deep learning. Compare two architectures: Physics-Informed Neural Networks (PINNs) and Nonlinear Autoregressive (NAR) models.

Result: Neural networks trained on augmented datasets show significantly improved predictive performance. NAR models provide accurate short-term forecasts while PINNs capture qualitative long-term behavior.

Conclusion: The proposed data augmentation strategy effectively improves neural network predictions, with NAR models being computationally efficient for short-term forecasting and PINNs suitable for exploring long-term dynamics.

Abstract: In this work, we propose a data augmentation strategy aimed at improving the
training phase of neural networks and, consequently, the accuracy of their
predictions. Our approach relies on generating synthetic data through a
suitable compartmental model combined with the incorporation of uncertainty.
The available data are then used to calibrate the model, which is further
integrated with deep learning techniques to produce additional synthetic data
for training. The results show that neural networks trained on these augmented
datasets exhibit significantly improved predictive performance. We focus in
particular on two different neural network architectures: Physics-Informed
Neural Networks (PINNs) and Nonlinear Autoregressive (NAR) models. The NAR
approach proves especially effective for short-term forecasting, providing
accurate quantitative estimates by directly learning the dynamics from data and
avoiding the additional computational cost of embedding physical constraints
into the training. In contrast, PINNs yield less accurate quantitative
predictions but capture the qualitative long-term behavior of the system,
making them more suitable for exploring broader dynamical trends. Numerical
simulations of the second phase of the COVID-19 pandemic in the Lombardy region
(Italy) validate the effectiveness of the proposed approach.

</details>


### [2] [LR-WaveHoltz: A Low-Rank Helmholtz Solver](https://arxiv.org/abs/2510.09352)
*Andreas Granath,Daniel Appelö,Siyang Wang*

Main category: math.NA

TL;DR: A low-rank method for solving the Helmholtz equation using WaveHoltz approach with SVD in 2D and tensor trains in 3D, featuring rank control via step-truncation and low-rank Anderson acceleration.


<details>
  <summary>Details</summary>
Motivation: To develop efficient computational methods for solving the Helmholtz equation, particularly for large-scale problems in multiple dimensions where traditional methods may be computationally expensive.

Method: Uses WaveHoltz method that filters time-domain wave equation solutions. Discretizes wave equation with high-order multiblock summation-by-parts finite differences. Employs SVD for 2D and tensor trains for 3D compression. Controls rank growth with step-truncation during time stepping and low-rank Anderson acceleration for fixed point iteration.

Result: Extensive numerical experiments demonstrate convergence and efficacy of the iterative scheme for free- and half-space problems in 2D and 3D with constant and piecewise constant wave speeds.

Conclusion: The proposed low-rank method effectively solves Helmholtz equations across multiple dimensions with controlled computational complexity through rank-reduction techniques.

Abstract: We propose a low-rank method for solving the Helmholtz equation. Our approach
is based on the WaveHoltz method, which computes Helmholtz solutions by
applying a time-domain filter to the solution of a related wave equation. The
wave equation is discretized by high-order multiblock summation-by-parts finite
differences. In two dimensions we use the singular value decomposition and in
three dimensions we use tensor trains to compress the numerical solution. To
control rank growth we use step-truncation during time stepping and a low-rank
Anderson acceleration for the WaveHoltz fixed point iteration. We have carried
out extensive numerical experiments demonstrating the convergence and efficacy
of the iterative scheme for free- and half-space problems in two and three
dimensions with constant and piecewise constant wave speeds.

</details>


### [3] [Inverse obstacle scattering with a single moving emitter](https://arxiv.org/abs/2510.09385)
*Yu Sun,Bo Chen,Peng Gao,Qiuyi Li,Yao Sun*

Main category: math.NA

TL;DR: This paper develops methods for forward and inverse scattering problems using a single moving point source emitter, including approximate forward solutions and a novel direct sampling method for scatterer reconstruction.


<details>
  <summary>Details</summary>
Motivation: To address time domain scattering problems with moving emitters, which is relevant for practical applications where sources are not stationary.

Method: Developed approximate solutions for forward scattering and created two indicator functions (basic and novel) for the direct sampling method to recover scatterers.

Result: Numerical experiments show the algorithms effectively reconstruct both 2D and 3D scatterers using a single moving emitter.

Conclusion: The proposed methods successfully solve both forward and inverse scattering problems with moving emitters and demonstrate practical applicability through numerical validation.

Abstract: This paper is concerned with time domain forward scattering and inverse
scattering problems with a single moving point source as the emitter.
Approximate solutions are provided for the forward scattering problem with a
moving emitter. Regarding the inverse problem, in addition to a basic indicator
function based on the approximate solutions, a novel indicator function is
developed to construct the direct sampling method to recover both point-like
and extended scatterers. Numerical experiments demonstrate that the proposed
algorithms are effective in reconstructing both two-dimensional and
three-dimensional scatterers with a single moving emitter.

</details>


### [4] [A time-space B-spline integrator for the Burgers' equation](https://arxiv.org/abs/2510.09408)
*Idris Dag,Serkan Uğurluoğlu,Nihat Adar*

Main category: math.NA

TL;DR: A new space-time B-spline collocation method for solving Burgers' equation using quadratic B-splines in time and cubic B-splines in space.


<details>
  <summary>Details</summary>
Motivation: To develop a novel numerical approach for Burgers' equation that uses B-splines for integration in both time and space dimensions, which hasn't been done before.

Method: First integrate Burgers' equation using quadratic B-spline functions in time, then solve the time-integrated equation using cubic B-spline collocation method in space, resulting in a recursive algebraic equation.

Result: The method successfully obtained both shock wave and front propagation solutions of Burgers' equation, demonstrating the effectiveness of the space-time B-spline collocation approach.

Conclusion: The proposed space-time B-spline collocation method is effective for solving Burgers' equation and represents a novel application of B-splines for simultaneous time and space integration in partial differential equations.

Abstract: The purpose of this paper is to propose a new algorithm for obtaining
approximate solutions to the Burgers' equation (BE). Integration in time by a
quadratic B-spline collocation method is shown. To the best of our knowledge,
B-splines have not previously been used to integrate partial differential
equations in both time and space. First, the BE is integrated using quadratic
B-spline functions in time, and then the time-integrated BE is further solved
in space via the cubic B-spline collocation method. The resulting recursive
algebraic equation is used to obtain both shock wave and front propagation
solutions of the BE, demonstrating the effectiveness of the space--time
B-spline collocation method.

</details>


### [5] [A Localized Orthogonal Decomposition method for heterogeneous mixed-dimensional problems](https://arxiv.org/abs/2510.09442)
*Moritz Hauck,Axel Målqvist,Malin Mosquera*

Main category: math.NA

TL;DR: A multiscale method using Localized Orthogonal Decomposition for mixed-dimensional elliptic problems with heterogeneous coefficients, achieving optimal convergence independent of coefficient regularity.


<details>
  <summary>Details</summary>
Motivation: To solve mixed-dimensional elliptic problems with highly heterogeneous coefficients (e.g., in fractured porous media modeling) without requiring the coarse mesh to resolve coefficient oscillations.

Method: Based on LOD framework, constructs locally supported, problem-adapted basis functions on coarse mesh by solving localized fine-scale problems in parallel.

Result: Method achieves optimal convergence with coarse mesh size, independent of coefficient regularity, with exponentially decaying localization error.

Conclusion: Numerical experiments validate theoretical findings and demonstrate computational viability of the method.

Abstract: We propose a multiscale method for mixed-dimensional elliptic problems with
highly heterogeneous coefficients arising, for example, in the modeling of
fractured porous media. The method is based on the Localized Orthogonal
Decomposition (LOD) framework and constructs locally supported, problem-adapted
basis functions on a coarse mesh that does not need to resolve the coefficient
oscillations. These basis functions are obtained in parallel by solving
localized fine-scale problems. Our a priori error analysis shows that the
method achieves optimal convergence with respect to the coarse mesh size,
independent of the coefficient regularity, with an exponentially decaying
localization error. Numerical experiments validate these theoretical findings
and demonstrate the computational viability of the method.

</details>


### [6] [A posteriori analysis for nonlinear convection-diffusion systems](https://arxiv.org/abs/2510.09449)
*Andreas Dedner,Jan Giesselmann,Kiwoong Kwon,Tristan Pryer*

Main category: math.NA

TL;DR: This paper develops reliable a posteriori error estimates for Runge-Kutta discontinuous Galerkin methods applied to nonlinear convection-diffusion systems, particularly focusing on convection-dominated and degenerate parabolic problems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide reliable error estimation for practical simulations of convection-diffusion systems, especially for convection-dominated cases where traditional methods may struggle, and to track error dependence on viscosity coefficients.

Method: The authors develop a posteriori error bounds for a family of discontinuous Galerkin spatial discretizations combined with various temporal discretizations, including explicit and implicit-explicit time-stepping schemes.

Result: The paper proves that the proposed estimators provide reliable upper bounds for numerical method errors and demonstrates through numerical evidence that these estimators achieve the same order of convergence as the actual error.

Conclusion: The work successfully establishes reliable a posteriori error estimation framework for discontinuous Galerkin methods applied to nonlinear convection-diffusion systems, with particular applicability to convection-dominated problems and tracking of viscosity coefficient dependence.

Abstract: This work provides reliable a posteriori error estimates for Runge-Kutta
discontinuous Galerkin approximations of nonlinear convection-diffusion
systems. The classes of systems we study are quite general with a focus on
convection-dominated and degenerate parabolic problems. Our a posteriori error
bounds are valid for a family of discontinuous Galerkin spatial discretizations
and various temporal discretizations that include explicit and
implicit-explicit time-stepping schemes, popular tools for practical
simulations of this class of problem. We prove that our estimators provide
reliable upper bounds for the error of the numerical method and present
numerical evidence showing that they achieve the same order of convergence as
the error. Since one of our main interests is the convection dominant case, we
also track the dependence of the estimator on the viscosity coefficient.

</details>


### [7] [Optimal higher-order convergence rates for parabolic multiscale problems](https://arxiv.org/abs/2510.09514)
*Balaje Kalyanaraman,Felix Krumbiegel,Roland Maier,Siyang Wang*

Main category: math.NA

TL;DR: Higher-order multiscale method for time-dependent problems with oscillatory coefficients using enriched LOD corrections to achieve higher-order convergence without restrictive coefficient assumptions.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of reduced convergence rates when applying higher-order LOD methods to time-dependent problems with highly oscillatory coefficients.

Method: Build on localized orthogonal decomposition (LOD) framework by constructing enriched correction operators to enrich multiscale spaces, applied to parabolic equations as model problems.

Result: Proved exponential decay of enriched corrections and established rigorous a priori error estimates. Numerical experiments confirm theoretical results.

Conclusion: The proposed higher-order multiscale method successfully achieves higher-order convergence for time-dependent problems with oscillatory coefficients without requiring restrictive assumptions on coefficients beyond boundedness.

Abstract: In this paper, we introduce a higher-order multiscale method for
time-dependent problems with highly oscillatory coefficients. Building on the
localized orthogonal decomposition (LOD) framework, we construct enriched
correction operators to enrich the multiscale spaces, ensuring higher-order
convergence without requiring assumptions on the coefficient beyond
boundedness. This approach addresses the challenge of a reduction of
convergence rates when applying higher-order LOD methods to time-dependent
problems. Addressing a parabolic equation as a model problem, we prove the
exponential decay of these enriched corrections and establish rigorous a priori
error estimates. Numerical experiments confirm our theoretical results.

</details>


### [8] [Multi-Level Hybrid Monte Carlo / Deterministic Methods for Particle Transport Problems](https://arxiv.org/abs/2510.09545)
*Vincent N. Novellino,Dmitriy Y. Anistratov*

Main category: math.NA

TL;DR: Multi-level hybrid transport (MLHT) methods using multi-level Monte Carlo (MLMC) approach for solving Boltzmann transport equation, with quasidiffusion and second-moment methods for efficient computational cost optimization.


<details>
  <summary>Details</summary>
Motivation: To develop efficient computational methods for solving the neutral particle Boltzmann transport equation by optimizing computational costs while maintaining target accuracy through multi-level grid approaches.

Method: Formulated MLHT methods on sequence of spatial grids using MLMC approach, employing quasidiffusion and second-moment methods with Monte Carlo techniques for closure computations and low-order equation solutions.

Result: Demonstrated weak convergence of functionals in 1-D slab transport problems, showing variance of correction factors decreases faster than computational cost increases, with coarse grid calculations driving overall variance and costs.

Conclusion: MLHT methods with MLMC approach provide efficient computational framework for Boltzmann transport problems, achieving optimized cost-accuracy balance through multi-level grid strategy and variance reduction techniques.

Abstract: This paper presents multi-level hybrid transport (MLHT) methods for solving
the neutral particle Boltzmann transport equation. The proposed MLHT methods
are formulated on a sequence of spatial grids using a multi-level Monte Carlo
(MLMC) approach. The general MLMC algorithm is defined by the recursive
estimation of the expected value of a solution functional's correction with
respect to a neighboring grid. MLMC theory optimizes the total computational
cost for estimating a functional to within a target accuracy. The proposed MLHT
algorithms are based on the quasidiffusion (Variable Eddington Factor) and
second-moment methods. For these methods, the low-order equations for the
angular moments of the high-order transport solution are discretized in space.
Monte Carlo techniques compute the closures for the low-order equations; then,
the equations are solved, yielding a single realization of the global flux
solution. The ensemble average of the realizations yields the level solution.
The results for 1-D slab transport problems demonstrates weak convergence of
the functionals considered. We observe that the variance of the correction
factors decreases faster than the increase in computational costs of generating
an MLMC sample. In the problems considered, the variance and costs of the MLMC
solution are driven by the coarse grid calculations.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [9] [Hausdorff dimension of the singular set for Griffith almost-minimizers in the plane](https://arxiv.org/abs/2510.08670)
*Manuel Friedrich,Camille Labourie,Kerrek Stinson*

Main category: math.AP

TL;DR: The paper proves that cracks in Griffith fracture energy minimizers are uniformly rectifiable and have singular sets with dimension less than 1, with applications to almost-minimizers and higher gradient integrability.


<details>
  <summary>Details</summary>
Motivation: To establish regularity properties of crack sets in brittle materials modeled by Griffith fracture energy, addressing fundamental questions about crack geometry and behavior.

Method: Combines uniform rectifiability analysis with previous epsilon-regularity results to bound the dimension of singular sets in crack configurations.

Result: Proved that crack sets are uniformly rectifiable, singular sets have dimension strictly less than 1, and established higher integrability for gradients of local minimizers.

Conclusion: The results provide rigorous mathematical foundations for Griffith fracture theory, including dimension bounds on singular sets and positive resolution of the analog of De Giorgi's conjecture for the Mumford-Shah functional.

Abstract: We consider regularity of the crack set associated to a minimizer of the
Griffith fracture energy, often used in modeling brittle materials. We show
that the crack is uniformly rectifiable which in conjunction with our previous
epsilon-regularity result allows us to prove that the singular set has
dimension strictly less than $1$. This size estimate also applies to
almost-minimizers. As a byproduct, we prove higher integrability for the
gradient of local minimizers of the Griffith energy, providing a positive
answer to the analog of De Giorgi's conjecture for the Mumford--Shah
functional.

</details>


### [10] [Stability Estimates for Commutativity Properties of the Dirichlet-to-Neumann Operator](https://arxiv.org/abs/2510.08822)
*Romain Speciel*

Main category: math.AP

TL;DR: The paper studies the relationship between the Laplacian on spheres and Dirichlet-to-Neumann maps, characterizing manifolds where this exact relationship holds and measuring geometric discrepancies when it doesn't.


<details>
  <summary>Details</summary>
Motivation: To understand which manifolds exhibit the exact relationship Δ_{S^{n-1}} = Λ² + (n-2)Λ found on spheres, and to quantify geometric deviations from this relationship.

Method: Uses stability estimates for smoothly bounded domains in R³, studies manifolds conformal to the ball, and employs Gohberg's lemma from microlocal analysis.

Result: Shows that small commutator [Λ,Δ_{S^{n-1}}] implies proximity to a ball, and that the exact relationship implies radial metric structure for conformal manifolds.

Conclusion: The relationship between Laplacian and Dirichlet-to-Neumann maps provides geometric characterization of manifolds, with stability estimates quantifying deviations from spherical geometry.

Abstract: The Laplacian $\Delta_{\mathbb{S}^{n-1}}$ on the unit sphere
$\mathbb{S}^{n-1}\subset \mathbb{R}^n$ has the property that it can explicitly
be expressed in terms of $\Lambda$, the Dirichlet-to-Neumann map of the unit
ball, as $\Delta_{\mathbb{S}^{n-1}}=\Lambda^2+(n-2)\Lambda$. In this paper, we
seek to characterize those manifolds for which such an exact relationship
holds, and more generally measure the discrepancy of such a relationship
holding in terms of geometric data. To this end, we obtain a stability estimate
which shows that, for a smoothly bounded domain in $\mathbb{R}^3$, if the
commutator $[\Lambda,\Delta_{\mathbb{S}^{n-1}}]$ is small then that domain is
itself close to a ball. We then study the case of manifolds conformal to the
ball, show that a relationship as above implies a radial metric structure, and
discuss stability in this setting. Finally, we provide a modern exposition of
Gohberg's lemma, a foundational result in microlocal analysis which we employ
as a starting step for our reasoning.

</details>


### [11] [On positive solutions of Lane-Emden equations on the integer lattice graphs](https://arxiv.org/abs/2510.08947)
*Huyuan Chen,Bobo Hua,Feng Zhou*

Main category: math.AP

TL;DR: This paper studies positive solutions to Lane-Emden equations on lattice graphs with Hardy-type potentials, establishing existence in Sobolev super-critical regions and nonexistence in Serrin sub-critical regions.


<details>
  <summary>Details</summary>
Motivation: To understand the existence and nonexistence of positive solutions to Lane-Emden equations on discrete domains (integer lattice graphs) with Hardy-type potentials, extending classical PDE results to discrete settings.

Method: Used variational methods to prove existence in Sobolev super-critical regions, and iterative analysis of decay behavior at infinity to prove nonexistence in Serrin sub-critical regions.

Result: Identified parameter regions where positive solutions exist (Sobolev super-critical) and don't exist (Serrin sub-critical), with intermediate regions in full-space/half-space where existence remains open.

Conclusion: The study provides a complete characterization of existence/nonexistence for Lane-Emden equations on lattice graphs, revealing domain-dependent differences in parameter regions.

Abstract: In this paper, we investigate the existence and nonexistence of positive
solutions to the Lane-Emden equations $$ -\Delta u = Q |u|^{p-2}u $$ on the
$d$-dimensional integer lattice graph $\mathbb{Z}^d$, as well as in the
half-space and quadrant domains, under the zero Dirichlet boundary condition in
the latter two cases. Here, $d \geq 2$, $p > 0$, and $Q$ denotes a Hardy-type
positive potential satisfying $Q(x) \sim (1+|x|)^{-\alpha}$ with $\alpha \in
[0, +\infty]$. \smallskip
  We identify the Sobolev super-critical regions of the parameter pair
$(\alpha, p)$ for which the existence of positive solutions is established via
variational methods. In contrast, within the Serrin sub-critical regions of
$(\alpha, p)$, we demonstrate nonexistence by iteratively analyzing the decay
behavior at infinity, ultimately leading to a contradiction. Notably, in the
full-space and half-space domains, there exists an intermediate regions between
the Sobolev critical line and the Serrin critical line where the existence of
positive solutions remains an open question. Such an intermediate region does
not exist in the quadrant domain.

</details>


### [12] [Energy distance and evolution problems: a promising tool for kinetic equations](https://arxiv.org/abs/2510.09123)
*Gennaro Auricchio,Giuseppe Toscani*

Main category: math.AP

TL;DR: Analysis of convergence rates to equilibrium for Fokker-Planck equations using Cramér and Energy distances, showing improved decay rates compared to relative entropy methods.


<details>
  <summary>Details</summary>
Motivation: To leverage Cramér and Energy distances, which are widely used in AI and machine learning tasks, for studying convergence rates in Fokker-Planck type equations with linear drift.

Method: Using Cramér and Energy distances to analyze the rate of convergence to equilibrium for solutions of Fokker-Planck type equations with linear drift.

Result: The rate of decay to equilibrium is improved compared to known results based on relative entropy decay.

Conclusion: Cramér and Energy distances provide enhanced convergence analysis for Fokker-Planck equations, offering better decay rates than traditional relative entropy approaches.

Abstract: We study the rate of convergence to equilibrium of the solutions to
Fokker-Planck type equations with linear drift by means of Cram\'er and Energy
distances, which have been recently widely used in problems related to AI, in
particular for tasks related to machine learning. In all cases in which the
Fokker-Planck type equations can be treated through these distances, it is
shown that the rate of decay is improved with respect to known results which
are based on the decay of relative entropy.

</details>


### [13] [Local Lipschitz continuity for energy integrals with fast growth and lower order terms](https://arxiv.org/abs/2510.09142)
*Andrea Torricelli*

Main category: math.AP

TL;DR: Local minimizers of integral functionals with fast growth and u-dependent lagrangians are locally Lipschitz continuous.


<details>
  <summary>Details</summary>
Motivation: To establish regularity properties for minimizers of integral functionals with fast growth conditions and explicit dependence on the function u.

Method: Mathematical analysis and proof techniques for integral functionals with fast growth conditions.

Result: Proved that local minimizers are locally Lipschitz continuous under the given conditions.

Conclusion: The paper establishes Lipschitz regularity for minimizers of fast-growing integral functionals with u-dependent lagrangians.

Abstract: We consider integral functionals with fast growth and the lagrangian
explicitly depending on $u$. We prove that the local minimizers are locally
Lipschitz continuous.

</details>


### [14] [Unique continuation and Hardy's uncertainty principle for hyperbolic Schrödinger equations](https://arxiv.org/abs/2510.09202)
*Torunn Jensen*

Main category: math.AP

TL;DR: The paper proves unique continuation properties for hyperbolic nonlinear Schrödinger equations and hyperbolic Schrödinger equations with potential, showing that solutions with Gaussian decay at two different times must be identically zero.


<details>
  <summary>Details</summary>
Motivation: To extend the Hardy uncertainty principle results from classical Schrödinger equations to hyperbolic settings, building on previous work by Escauriaza, Kenig, Ponce, and Vega.

Method: Uses Carleman estimates based on calculus and convexity arguments, with technical modifications to handle the hyperbolic character of the equations.

Result: Shows that under suitable conditions on nonlinearity or potential, solutions with Gaussian decay at two different times must vanish identically.

Conclusion: Successfully extends unique continuation properties to hyperbolic Schrödinger equations, providing rigorous justification for Carleman estimates in this setting.

Abstract: We prove unique continuation properties related to the Hardy uncertainty
principle for solutions of the hyperbolic nonlinear Schr\"odinger equation and
the hyperbolic Schr\"odinger equation with potential. Under suitable conditions
on the nonlinearity, or the potential, we show that if $u$ is a solution with
Gaussian decay at two different times, then $u\equiv 0$. These results extend
to the hyperbolic setting the work of Escauriaza, Kenig, Ponce, and Vega (JEMS,
10, 2008) for the classical Schr\"odinger equation. The proofs rely on Carleman
estimates based on calculus and convexity arguments, with the main challenge
being to provide a rigorous justification of these estimates. Although our
approach follows the general strategy of Escauriaza, Kenig, Ponce, and Vega,
several technical modifications are required to handle the hyperbolic character
of the equation.

</details>


### [15] [Li-Yau-Hamilton Inequality on the JKO Scheme for the Granular-Medium Equation](https://arxiv.org/abs/2510.09231)
*Fanch Coudreuse*

Main category: math.AP

TL;DR: Established Li-Yau-Hamilton inequality for Granular-Medium equation on torus, applied to derive quantitative bounds and Harnack inequality, and proved JKO scheme convergence for Fokker-Planck equation.


<details>
  <summary>Details</summary>
Motivation: To develop analytical tools for studying regularity and convergence properties of continuous and discrete flows in partial differential equations, particularly for the Granular-Medium equation and Fokker-Planck equation.

Method: Established Li-Yau-Hamilton inequality at both PDE level and for time-discrete JKO scheme approximation, then applied this estimate to derive quantitative results.

Result: Obtained Lipschitz and L∞ bounds, quantitative Harnack inequality, and proved JKO scheme convergence in L²_loc((0,+∞); H²(T^d)) for Fokker-Planck equation.

Conclusion: The Li-Yau-Hamilton inequality provides powerful regularity estimates that enable quantitative analysis and convergence proofs for both continuous and discrete flows in these PDE settings.

Abstract: We establish a version of the Li--Yau--Hamilton inequality for the
Granular-Medium equation on the torus, both at the PDE level and for its
time-discrete approximation given by the JKO scheme. We then apply this
estimate to derive further quantitative results for the continuous and discrete
JKO flows, including Lipschitz and $L^\infty$ bounds, as well as a quantitative
Harnack inequality. Finally, we use the regularity provided by this estimate to
show that the JKO scheme for the Fokker--Planck equation converges in
$L^2_{\mathrm{loc}}((0,+\infty); H^2(\mathbb{T}^d))$.

</details>


### [16] [Nonexistence of global solutions to the Grushin heat equation with nonlocal and local nonlinearities](https://arxiv.org/abs/2510.09258)
*Ahmad Z. Fino,Arlúcio Viana*

Main category: math.AP

TL;DR: Sharp nonexistence results for global positive solutions to Grushin-type heat equations with nonlinear reaction and memory terms, identifying Fujita-type critical exponents.


<details>
  <summary>Details</summary>
Motivation: To complete the understanding of global existence versus blow-up behavior for degenerate parabolic equations with memory effects, particularly for Grushin operators.

Method: Test function method adapted to handle the degeneracy of the Grushin operator and the influence of memory terms.

Result: Established sharp nonexistence results for global-in-time positive solutions and identified Fujita-type critical exponents in certain parameter regimes.

Conclusion: Successfully characterized the blow-up behavior and critical exponents for Grushin-type heat equations with nonlinear reaction and memory terms.

Abstract: In this paper, we investigate the nonexistence of global solutions to the
Grushin-type heat equation with nonlinear reaction terms, including cases
involving memory effects: $$
  \left\{\begin{array}{ll}
  \displaystyle {u_{t}-\Delta_{\mathcal{G}}
  u = k_1 \int_0^t(t-s)^{-\gamma}\abs{u}^{p_1-1}u(s)\,\mathrm{d}s} +
k_2|u|^{p_2-1}u, & (z,t)\in {\mathbb{R}}^{N+k}\times (0,\infty),
  \displaystyle{u(z,0)= u_0(z),\qquad\qquad}&\displaystyle{z\in
{\mathbb{R}}^{N+k},}
  \end{array}
  \right. $$
  where $\Delta_{\mathcal{G}}$ denotes the Grushin operator, $u_0 \in
L^1_{\mathrm{loc}}(\mathbb{R}^{N+k})$, $\gamma\in[0,1)$, $k_1,k_2 \geq 0$, and
$p_1,p_2>1$.
  We establish sharp nonexistence results for global-in-time positive
solutions, thereby completing the picture of global existence versus blow-up
and allow us to identify the corresponding Fujita-type critical exponents in
certain parameter regimes. The analysis relies on the test function method,
adapted to handle both the degeneracy of the Grushin operator and the influence
of the memory term.

</details>


### [17] [Uniqueness of solutions to MFG systems with large discount](https://arxiv.org/abs/2510.09280)
*Marco Cirant,Elisa Continelli*

Main category: math.AP

TL;DR: Solutions to Mean Field Game systems with discount are unique when discount factor is large and Lagrangian term is small, identifying an asymptotic uniqueness regime outside monotonicity conditions.


<details>
  <summary>Details</summary>
Motivation: To establish uniqueness conditions for Mean Field Game systems that don't rely on the usual monotonicity assumptions, exploring alternative regimes where uniqueness holds.

Method: Mathematical analysis of Mean Field Game systems with discount, proving uniqueness through conditions on the discount factor and Lagrangian term.

Result: Uniqueness is proven when the discount factor is sufficiently large and the Lagrangian term is proportionally small enough.

Conclusion: This work identifies a new asymptotic uniqueness regime for Mean Field Game systems that operates independently of monotonicity conditions.

Abstract: We prove that solutions to a class of Mean Field Game systems with discount
are unique provided that the discount factor is large enough, and the
Lagrangian term is (proportionally) small enough. This identifies an asymptotic
uniqueness regime that falls outside the usual ones involving monotonicity.

</details>


### [18] [Non-linear stability of shock profiles in dissipative hyperbolic-hyperbolic systems](https://arxiv.org/abs/2510.09287)
*Matthias Sroczinski,Kevin Zumbrun*

Main category: math.AP

TL;DR: First proof of nonlinear stability for smooth shock profiles in second-order dissipative hyperbolic-hyperbolic systems under spectral stability assumption, applicable to dimensions ≥2.


<details>
  <summary>Details</summary>
Motivation: To establish stability results for shock profiles in systems lacking the symmetric structure required for standard Kawashima-type energy estimates, including relativistic gas models and numerical relaxation systems.

Method: Developed a new para-differential type nonlinear damping estimate, similar to techniques used for constant state stability, allowing treatment of systems without symmetric structure.

Result: Successfully proved nonlinear stability of smooth small-amplitude shock profiles in dimensions ≥2 under spectral stability assumption.

Conclusion: The new para-differential damping estimate provides a powerful tool for analyzing stability in systems that don't satisfy the structural requirements of traditional energy methods.

Abstract: We give the first proof of nonlinear stability for smooth shock profiles of
second-order dissipative hyperbolic-hyperbolic systems under the assumption of
spectral stability, showing stability of smooth small-amplitude profiles in
dimensions greater than or equal to two. This class of systems notably includes
the two types of causal viscous relativistic gas models introduced respectively
by Freist\"uhler-Temple and Bemfica-Disconzi-Noronha, and (the equivalent
second-order form of) a class of first-order numerical relaxation systems
generalizing the well-known Jin-Xin relaxation schemes. A significant technical
innovation is a new para-differential type of nonlinear damping estimate
similar to that used by the first author to study stability of constant states,
allowing the treatment of systems far from the symmetric structure required for
the standard ``Kawashima-type'' energy estimates that are typically used for
that purpose.

</details>


### [19] [Global existence and asymptotic decay for small solutions of general quasilinear hyperbolic balance laws](https://arxiv.org/abs/2510.09318)
*Matthias Sroczinski*

Main category: math.AP

TL;DR: Global existence and asymptotic decay for small solutions to quasilinear hyperbolic balance laws without requiring entropy or symmetry assumptions, using para-differential calculus methods.


<details>
  <summary>Details</summary>
Motivation: To generalize previous works by removing the need for hyperbolic operators to admit entropy and source terms to satisfy symmetry assumptions, enabling analysis of systems like the multidimensional Jin-Xin relaxation system.

Method: Uses para-differential calculus methods recently developed for second-order hyperbolic systems, characterizing dissipative properties through three conditions for different wave number regimes in Fourier space.

Result: Establishes global existence and asymptotic decay for small solutions, leading to asymptotic stability of rest-states for multidimensional Jin-Xin relaxation system.

Conclusion: The approach successfully handles quasilinear hyperbolic balance laws under more general conditions than previous methods, achieving results not accessible through traditional techniques.

Abstract: This paper establishes global existence and asymptotic decay for small
solutions to quasilinear systems of hyperbolic balance laws, where,
generalizing previous works, the hyperbolic operator does not need to admit an
entropy nor does the source term need to satisfy any symmetry assumptions.
Dissipative properties are characterized by three conditions corresponding to
regimes of small, intermediate and large wave numbers in Fourier space and the
fully non-linear system is treated by using methods of para-differential
calculus recently developed in the context for proofs of global existence and
decay in second-order hyperbolic systems. The present work leads, in
particular, to asymptotic stability of rest-states for multidimensional Jin-Xin
relaxation system, a result not accessible through previous methods.

</details>


### [20] [Metaplectic time-frequency representations](https://arxiv.org/abs/2510.09322)
*Gianluca Giacchi*

Main category: math.AP

TL;DR: This paper surveys time-frequency representations using metaplectic operators, tracing from the 1932 Wigner distribution to recent generalizations by Cordero and Rodino that unify various representations under symplectic group theory.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive survey of time-frequency representations defined via metaplectic operators, particularly highlighting recent advances that unify classical representations under a common mathematical framework.

Method: The paper uses metaplectic operators (defined by Van Hove in mid-20th century) to generalize Wigner distributions and unify various time-frequency representations, building on the framework established by Bai, Li and Cheng in 2012.

Result: The survey demonstrates how metaplectic operators provide a unified framework where properties of time-frequency representations can be naturally explained by the structure of the symplectic group.

Conclusion: Metaplectic operators offer a powerful mathematical framework that effectively unifies and generalizes time-frequency representations, with recent contributions by Cordero and Rodino pushing these operators to their limits to create a comprehensive theory.

Abstract: Time-frequency representations stemmed in 1932 with the introduction of the
Wigner distribution. For most of the 20th century, research in this area
primarily focused on defining joint probability distributions for position and
momentum in quantum mechanics. Applications to electrical engineering were soon
established with the seminal works of Gabor and the researchers at Bell Labs.
In 2012, Bai, Li and Cheng used for the first time metaplectic operators,
defined in the middle of 20th century by Van Hove, to generalize the Wigner
distribution and unify effectively the most used time-frequency representations
under a common framework. This work serves as a comprehensive up-to-date survey
on time-frequency representations defined by means of metaplectic operators,
with particular emphasis on the recent contributions by Cordero and Rodino, who
exploited metaplectic operators to their limits to generalize the Wigner
distributions. Their idea provides a fruitful framework where properties of
time-frequency representations can be explained naturally by the structure of
the symplectic group.

</details>


### [21] [Shape optimization of a small favorable region in a periodically fragmented environment](https://arxiv.org/abs/2510.09346)
*Gianmaria Verzini*

Main category: math.AP

TL;DR: This paper analyzes shape optimization for species persistence in fragmented environments, showing optimal favorable habitats shrink to nearly spherical convex sets with exponentially decaying asymmetry.


<details>
  <summary>Details</summary>
Motivation: To understand how biological species persist in periodically fragmented environments by optimizing the shape of favorable habitat zones for population survival.

Method: Minimization of a weighted eigenvalue of the periodic Laplacian with bang-bang indefinite weight, using singular perturbation analysis as the favorable habitat volume vanishes.

Result: Optimal favorable zones shrink to connected, convex, nearly spherical sets in C¹,¹ sense, with spherical asymmetry decaying exponentially with respect to negative powers of volume in C¹,α sense for α<1.

Conclusion: The study provides complete characterization of optimal habitat shapes in vanishing volume regime, revealing strong geometric constraints favoring nearly spherical convex configurations.

Abstract: We consider a shape optimization problem for the persistence threshold of a
biological species dispersing in a periodically fragmented environment, the
unknown shape corresponding to the portion of the habitat which is favorable to
the population. Analytically, this translates in the minimization of a weighted
eigenvalue of the periodic Laplacian, with respect to a bang-bang indefinite
weight. For such problem, we exploit some recent results obtained in the
framework of Dirichlet or Neumann boundary conditions, to provide a full
description of the singularly perturbed regime in which the volume of the
favorable zone vanishes.
  First, we show that the optimal favorable zone shrinks to a connected,
convex, nearly spherical set, in $C^{1,1}$ sense. Secondly, we show that the
spherical asymmetry of the optimal favorable zone decays exponentially, with
respect to a negative power of its volume, in the $C^{1,\alpha}$ sense, for
every $\alpha<1$.

</details>


### [22] [Existence of martingale solutions for a stochastic weighted mean curvature flow of graphs](https://arxiv.org/abs/2510.09383)
*Qi Yan,Xiang-Dong Li*

Main category: math.AP

TL;DR: Existence of martingale solution for stochastic mean curvature flow with extra force over periodic domains, and derivation of small perturbation limit.


<details>
  <summary>Details</summary>
Motivation: To study stochastic mean curvature flow with additional forcing terms in periodic domains and establish mathematical foundations for such flows.

Method: Using compact embedding method of variational stochastic partial differential equations (SPDE) to prove existence of martingale solution.

Result: Proved existence of martingale solution for the stochastic mean curvature flow with extra force.

Conclusion: Successfully established existence of solutions and derived small perturbation limits for stochastic weighted mean curvature flow.

Abstract: We are concerned with a stochastic mean curvature flow of graphs with extra
force over a periodic domain of any dimension. Based on compact embedding
method of variational SPDE, we prove the existence of martingale solution.
Moreover, we derive the small perturbation limit of the stochastic weighted
mean curvature flow.

</details>


### [23] [Complex Gaussianity and spatio-frequential memory effect of random wave processes](https://arxiv.org/abs/2510.09402)
*Guillaume Bal,Anjali Nair*

Main category: math.AP

TL;DR: The paper analyzes wavefield speckle patterns in weak-coupling regime, showing macroscopic envelope follows deterministic diffusion while local speckle is Gaussian process with chromato-spatial memory effects.


<details>
  <summary>Details</summary>
Motivation: To understand the multiscale character of wavefield speckle patterns generated by interference of scattered coherent light in heterogeneous media.

Method: Using Itô-Schrödinger paraxial model for long-distance wave propagation in weak-coupling regime to analyze speckle patterns.

Result: Macroscopic envelope solves deterministic diffusion equation; local wavefield described by complex Gaussian process with spatial axial/lateral displacements and frequency/angular variations.

Conclusion: Results describe speckle patterns and corroborate chromato-spatial memory effects observed in laser light propagation through heterogeneous media.

Abstract: Wavefield speckle patterns are generated by interference of randomly
scattered coherent light. In the weak-coupling regime of the
It\^o-Schr\"odinger paraxial model for long-distance wave propagation, we show
the following multiscale character: a macroscopic envelope solves a
deterministic diffusion equation while the local wavefield (the speckle) is
described by a complex Gaussian process both in terms of spatial axial and
lateral displacements as well as frequency and angular variations of the
incident wavebeam. These results describe speckle patterns and corroborate
chromato-spatial memory effects observed in laser light propagation through
heterogeneous media.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [24] [Theoretical Analysis of Topotomography Using Small Intragranular Strain Approximations](https://arxiv.org/abs/2510.08712)
*Zheheng Liu,Nicola Vigano,Henry Proudhon,Wolfgang Ludwig*

Main category: physics.comp-ph

TL;DR: TT is a synchrotron-based X-ray diffraction imaging technique for characterizing grain shape and orientation in polycrystalline materials. This work provides mathematical foundations for 3D grain reconstruction from TT data, deriving projection geometry expressions and analyzing reconstruction feasibility.


<details>
  <summary>Details</summary>
Motivation: To establish fundamental mathematical understanding of 3D grain shape and orientation field reconstruction from Topo-Tomography diffraction data, addressing the need for reliable characterization of polycrystalline materials.

Method: Derived mathematical expressions for TT projection geometry considering grain shape, lattice rotations, and elastic strains under kinematical diffraction. Simplified expressions using small strain and grain size approximations. Proposed methods to expand data coverage including opposite scattering vectors and varying detector distance.

Result: Simplified expressions show integrated TT projection images correspond to projections of a "pseudo" distorted grain volume. Fourier analysis provides insights into orientation field reconstruction feasibility. Derived and validated a lower bound for orientation sampling resolution through simulations.

Conclusion: The mathematical framework enables understanding of TT reconstruction capabilities and provides methods to improve data coverage, establishing foundations for reliable 3D grain characterization in polycrystalline materials.

Abstract: Topo-Tomography (TT) is a synchrotron-based X-ray diffraction imaging
technique used to characterize grain shape and crystal orientation in
polycrystalline samples. This work aims to provide a decisive and fundamental
understanding of 3D grain shape and orientation field reconstruction from TT
diffraction data. We derive mathematical expressions for the TT projection
geometry, considering grain shape, intragranular lattice rotations, and elastic
strains, under the assumption of kinematical diffraction. These expressions are
simplified using approximations for small strain variations and grain size. The
simplified expressions show that integrated TT projection images correspond to
projections of a "pseudo" distorted grain volume. Its Fourier analysis provides
insights into the feasibility of orientation field reconstruction from TT
scans. We propose methods to expand data coverage, including using opposite
scattering vectors and varying detector distance. A lower bound for orientation
sampling resolution is derived and validated through simulations.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [25] [Emergence of advection-diffusion transport structure and nonlinear amplitude evolution of strongly driven instabilities](https://arxiv.org/abs/2510.08735)
*Emma G. Devin,Vinícius N. Duarte*

Main category: physics.plasm-ph

TL;DR: Analytic formulation for strongly driven instabilities in physical systems, showing linear growth followed by nonlinear evolution governed by a Bernoulli equation, with agreement to kinetic simulations.


<details>
  <summary>Details</summary>
Motivation: To understand transport structure and nonlinear amplitude evolution of discrete, strongly driven instabilities in various physical systems like plasmas, fluids, and self-gravitating systems.

Method: Developed analytic formulation describing mode evolution from linear phase to nonlinear phase governed by Bernoulli differential equation, with closed-form solution and advection-diffusion equation in energy coordinates.

Result: Found that instability evolves linearly until gradient exhaustion, then transitions to nonlinear phase with analytic solution, and distribution function satisfies advection-diffusion equation during nonlinear phase.

Conclusion: Analytical results closely agree with nonlinear kinetic simulations and are applicable to resonant transport in plasmas, galaxies, and viscous shear flows.

Abstract: Instabilities driven by strong gradients appear in a wide variety of physical
systems, including plasmas, neutral fluids, and self-gravitating systems. This
work develops an analytic formulation to describe the transport structure and
nonlinear amplitude evolution of a discrete, strongly driven instability in the
presence of energy sources and sinks. Initially, the mode is found to evolve
linearly until the gradient in the distribution has been exhausted. It then
transitions to a nonlinear phase governed by a Bernoulli differential equation,
for which a closed-form analytic solution is found, and continues to evolve
until the energy sources and sinks reach equilibrium. During the nonlinear
phase, the leading order distribution function is found to persistently satisfy
an advection-diffusion equation in time and energy coordinates. These
analytical results are shown to agree closely with nonlinear kinetic
simulations and to be readily applicable in the study of resonant transport in
plasmas, galaxies and viscous shear flows.

</details>


### [26] [Statistics of Current and Vorticity Structures in Relativistic Turbulence](https://arxiv.org/abs/2510.09126)
*Zachary Davis,Luca Comisso,Colby Haggerty,Joonas Nättilä*

Main category: physics.plasm-ph

TL;DR: Analysis of current and vorticity sheets in 3D relativistic turbulence using self-organizing maps and statistical analysis across different magnetizations and magnetic field strengths.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric and structural properties of coherent structures (current and vorticity sheets) in relativistic turbulence, which play key roles in energy dissipation and particle acceleration in astrophysical contexts.

Method: Used 3D particle-in-cell simulations of decaying relativistic turbulence in pair plasma, trained self-organizing maps to identify structures, and performed extensive statistical analysis across varying magnetizations ($\sigma$) and magnetic field fluctuation ratios ($\delta B_0/B_0$).

Result: Found power-law distributions for most geometric properties, except structure width which follows exponential distribution peaking at ~2 electron skin depths. Weak dependence on $\sigma$ but strong dependence on $\delta B_0/B_0$. Most current sheets are associated with at least one vorticity sheet neighbor, often situated between two vorticity sheets.

Conclusion: Provides detailed statistical framework for understanding coherent structure formation in relativistic turbulence, enabling incorporation into theoretical models for energy dissipation and particle acceleration processes relevant to high-energy astrophysical observations.

Abstract: Coherent structures created through turbulent cascades play a key role in
energy dissipation and particle acceleration. In this work, we investigate both
current and vorticity sheets in 3D particle-in-cell simulations of decaying
relativistic turbulence in pair plasma by training a self-organizing map to
recognize these structures. We subsequently carry out an extensive statistical
analysis to reveal their geometric and structural properties. This analysis is
systematically applied across a range of magnetizations ($\sigma$) and
fluctuating-to-mean magnetic field strengths ($\delta B_0/B_0$) to assess how
these parameters influence the resulting structures. We find that the
structures' geometric properties form power-law distributions in their
probability density functions (PDFs), with the exception of the structure
width, which generally exhibits an exponential distribution peaking around 2
electron skin depths. The measurements show weak dependence on $\sigma$ but a
strong dependence on $\delta B_0/B_0$. Finally, we investigate the spatial
relationship between current sheets and vorticity sheets. We find that most
current sheets are directly associated with at least one vorticity sheet
neighbor and are often situated between two vorticity sheets. These findings
provide a detailed statistical framework for understanding the formation and
organization of coherent structures in relativistic magnetized turbulence,
allowing for their incorporation into updated theoretical models for
structure-based energy dissipation and particle acceleration processes crucial
for interpreting high-energy astrophysical observations.

</details>


### [27] [Long Living Hot and Dense Plasma from Relativistic Laser-Nanowire Array Interaction](https://arxiv.org/abs/2510.09437)
*Ehsan Eftekhari-Zadeh,Mikhail Gyrdymov,Parysatis Tavana,Robert Loetzsch,Ingo Uschmann,Thomas Siefke,Thomas Käsebier,Uwe Zeitner,Adriana Szeghalmi,Alexander Pukhov,Dmitri Serebryakov,Evgeni Nerush,Igor Kostyukov,Olga Rosmej,Christian Spielmann,Daniil Kartashov*

Main category: physics.plasm-ph

TL;DR: Observation of nanosecond-scale, near-solid density, keV-temperature plasmas produced by irradiating composite nanowire arrays with intense femtosecond laser pulses, forming mm-scale jets sustained by kiloTesla magnetic fields.


<details>
  <summary>Details</summary>
Motivation: Long-living hot dense plasmas are critical for laser-driven nuclear physics, bright X-ray sources, and laboratory astrophysics research.

Method: Irradiated periodic arrays of composite nanowires with ultra-high contrast, relativistically intense femtosecond laser pulses, analyzed using 3D PIC simulations combined with collisional-radiative modeling (FLYCHK).

Result: Observed jet-like plasma structures extending up to 1 mm with densities of 10^20-10^22 cm^-3 and keV temperatures persisting for several nanoseconds, emitting K-shell radiation from He-like Ti^20+ ions.

Conclusion: The plasma jets are formed by kiloTesla-scale global magnetic fields generated during laser interaction, which drive current instabilities sustaining magnetic fields sufficient to confine hot dense plasma over nanosecond durations.

Abstract: Long-living, hot and dense plasmas generated by ultra-intense laser beams are
of critical importance for laser-driven nuclear physics, bright hard X-ray
sources, and laboratory astrophysics. We report the experimental observation of
plasmas with nanosecond-scale lifetimes, near-solid density, and keV-level
temperatures, produced by irradiating periodic arrays of composite nanowires
with ultra-high contrast, relativistically intense femtosecond laser pulses.
Jet-like plasma structures extending up to 1~mm from the nanowire surface were
observed, emitting K-shell radiation from He-like Ti$^{20+}$ ions.
High-resolution X-ray spectra were analyzed using 3D Particle-in-Cell (PIC)
simulations of the laser-plasma interaction combined with
collisional--radiative modeling (FLYCHK). The results indicate that the jets
consist of plasma with densities of $10^{20}$-$10^{22}$ cm$^{-3}$ and keV-scale
temperatures, persisting for several nanoseconds. We attribute the formation of
these jets to the generation of kiloTesla-scale global magnetic fields during
the laser interaction, as predicted by PIC simulations. These fields may drive
long-timescale current instabilities that sustain magnetic fields of several
hundred tesla, sufficient to confine hot, dense plasma over nanosecond
durations.

</details>


### [28] [Investigating Solid-Fluid Phase Coexistence in DC Plasma Bilayer Crystals: The Role of Particle Pairing and Mode Coupling](https://arxiv.org/abs/2510.09491)
*Siddhartha Mangamuri,Surabhi Jaiswal,Lénaïc Couëdel*

Main category: physics.plasm-ph

TL;DR: Investigation of solid-fluid phase coexistence in bilayer dusty plasma crystals under varying confinement bias, revealing fluid core with solid periphery and deviations from classical MCI theory.


<details>
  <summary>Details</summary>
Motivation: To understand phase behavior and melting transitions in bilayer dusty plasma systems, particularly the role of confinement effects and interlayer interactions.

Method: Used melamine formaldehyde particles in DC glow discharge argon plasma with electrically isolated confinement ring, systematically adjusting bias voltage while analyzing phonon spectra and stability.

Result: Observed distinct phase coexistence with melted fluid-like core surrounded by solid crystalline periphery. Found frequency shifts deviating from classical MCI theory predictions, with dynamic interlayer particle pairing and non-reciprocal interactions destabilizing the bilayer structure.

Conclusion: Interlayer coupling and confinement effects are crucial for tuning structural transitions in bilayer dusty plasmas, revealing previously underappreciated mechanisms driving melting transitions in complex plasma systems.

Abstract: This article presents a detailed investigation of solid-fluid phase
coexistence in a bilayer dusty plasma crystal subjected to varying confinement
ring bias voltages in a DC glow discharge argon plasma. Melamine formaldehyde
particles were employed to form a stable, hexagonally ordered bilayer crystal
within a confinement ring electrically isolated from the grounded cathode. By
systematically adjusting the confinement ring bias, a distinct phase
coexistence emerged: it is characterized by a fluid-like melted core surrounded
by a solid crystalline periphery. Crucially, analysis of the phonon spectra
revealed frequency shifts that deviate significantly from the predictions of
classical monolayer Mode-Coupling Instability (MCI) theory. Stability analysis
further demonstrated that dynamic interlayer particle pairing and
non-reciprocal interactions play a pivotal role in destabilizing the bilayer
structure. These findings highlight previously underappreciated mechanisms
driving the melting transition in bilayer dusty plasmas, offering a more
comprehensive understanding of phase behavior in complex plasma systems. The
results underscore the importance of interlayer coupling and confinement
effects in tuning structural transitions.

</details>


### [29] [Si3N4 membrane as entrance window for plasma-generated vacuum ultraviolet (VUV) radiation](https://arxiv.org/abs/2510.09524)
*Luka Hansen,Görkem Bilgin,Jan Benedikt*

Main category: physics.plasm-ph

TL;DR: VUV radiation from atmospheric pressure plasma measured down to 58.4nm using thin Si3N4 membrane to transfer radiation into vacuum monochromator without disturbing plasma or spectra.


<details>
  <summary>Details</summary>
Motivation: To measure vacuum ultraviolet (VUV) radiation from atmospheric pressure plasma without disturbing the plasma or the spectra, which requires a method to transfer VUV radiation into a vacuum monochromator.

Method: Used a 20nm thin Si3N4 membrane to transfer VUV radiation into a vacuum monochromator, allowing measurements without plasma disturbance. He2 absorption was observed by filling monochromator with He.

Result: Successfully measured VUV radiation down to 58.4nm. Indirectly measured transmission of Si3N4 membrane in He2 excimer continua region (58nm to 100nm), confirming literature values.

Conclusion: The thin Si3N4 membrane method effectively enables VUV radiation measurement from atmospheric pressure plasma without spectral disturbance, with transmission characteristics matching established literature values.

Abstract: Vacuum ultraviolet (VUV) radiation produced by an atmospheric pressure plasma
was successfully measured down to wavelengths of 58.4nm utilizing a 20nm thin
Si3N4 membrane to transfer the VUV radiation into a vacuum monochromator. This
method allows measurements without disturbing the plasma or the spectra. He2
absorption could be observed by filling the monochromator with He. Transmission
of the Si3N4 membrane in the region of the He2 excimer continua (58nm to 100
nm) could indirectly be measured and confirms literature values.

</details>


### [30] [The impact of plasma turbulence on atomic reaction rates in detached divertors](https://arxiv.org/abs/2510.09579)
*Konrad Eder,Wladimir Zholobenko,Andreas Stegmeir,Kaiyu Zhang,Frank Jenko*

Main category: physics.plasm-ph

TL;DR: Turbulent fluctuations significantly affect atomic reaction rates in plasma edge models, with mean-field approximations introducing systematic errors particularly in detached conditions with high fluctuation amplitudes.


<details>
  <summary>Details</summary>
Motivation: To analyze how turbulent fluctuations impact atomic reaction rates in plasma edge models, since these rates depend non-linearly on local density and temperature and are sensitive to turbulence.

Method: Used GRILLIX edge turbulence simulations in detached divertor conditions to evaluate ionization, recombination and radiation rates in both turbulent and corresponding mean-field states, analyzing differences and correlation effects.

Result: In detached conditions with high fluctuation amplitudes (>300% background), turbulent rates were up to ~2x lower than mean-field rates due to anti-correlation of electron density and temperature. When correlations were factored out, turbulent rates recovered to mean-field levels. Positive correlation increased turbulent ionization rate by ~3x.

Conclusion: Common practice of evaluating atomic rates with mean-field plasma inputs introduces systematic errors to particle and energy balances, especially in detached conditions with high turbulence.

Abstract: Numerical plasma models of the edge and scrape-off layer rely on reaction
rates to describe key atomic processes such as ionization, recombination,
charge-exchange, and line radiation. These rates depend non-linearly on local
density and temperature, and are thus sensitive to turbulent fluctuations. We
present an analysis of atomic rates and their dependence on turbulent
fluctuations, obtained from GRILLIX edge turbulence simulations in detached
divertor conditions. Ionization, recombination and radiation rates are
evaluated in turbulent and corresponding mean-field states, and their
differences are discussed. While the effect is minimal in attached conditions
with low fluctuation amplitudes (< 50% of the background), pronounced
discrepancies emerge in detached conditions with high fluctuation amplitudes (>
300% of the background). Local ionization and radiation rates obtained from
turbulent inputs are up to a factor of ~2 lower than rates obtained from
mean-field inputs. The rate reduction is the result of the particular
anti-correlation of electron density and temperature in detached conditions.
When factoring out correlations, the turbulent rate approximately recovers to
the mean-field rate. When arranged to correlate positively, the turbulent
ionization rate instead increases by a factor of ~3. Our results demonstrate
that the common method of evaluating rates with mean-field plasma inputs can
introduce systematic errors to particle and energy balances, particularly in
detached conditions.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [31] [Block encoding with low gate count for second-quantized Hamiltonians](https://arxiv.org/abs/2510.08644)
*Diyi Liu,Shuchen Zhu,Guang Hao Low,Lin Lin,Chao Yang*

Main category: quant-ph

TL;DR: New block encoding methods for second-quantized Hamiltonians reduce Clifford+T gate complexity and ancilla overhead, achieving T count scaling as O(√L) with improved constant factors, and enable more efficient fault-tolerant quantum simulation.


<details>
  <summary>Details</summary>
Motivation: Efficient block encoding of many-body Hamiltonians is crucial for quantum algorithms in scientific computing, especially in the early fault-tolerant era, to reduce resource overheads.

Method: Uses data lookup strategy based on SWAP architecture for sparsity oracle and direct sampling method for amplitude oracle with SELECT-SWAP architecture; designs block encoding targeting η-particle subspace; extends approach to electronic Hamiltonians with translation invariance or decaying structures.

Result: Achieves T count scaling as O(√L) with respect to number of interaction terms L, reduces subnormalization factor from O(L) to O(√L), and improves fault-tolerant efficiency for fixed particle number systems.

Conclusion: Provides practical path toward early fault-tolerant quantum simulation of many-body systems with substantially lower resource overheads compared to previous methods.

Abstract: Efficient block encoding of many-body Hamiltonians is a central requirement
for quantum algorithms in scientific computing, particularly in the early
fault-tolerant era. In this work, we introduce new explicit constructions for
block encoding second-quantized Hamiltonians that substantially reduce
Clifford+T gate complexity and ancilla overhead. By utilizing a data lookup
strategy based on the SWAP architecture for the sparsity oracle $O_C$, and a
direct sampling method for the amplitude oracle $O_A$ with SELECT-SWAP
architecture, we achieve a T count that scales as
$\mathcal{\tilde{O}}(\sqrt{L})$ with respect to the number of interaction terms
$L$ in general second-quantized Hamiltonians. We also achieve an improved
constant factor in the Clifford gate count of our oracle. Furthermore, we
design a block encoding that directly targets the $\eta$-particle subspace,
thereby reducing the subnormalization factor from $\mathcal{O}(L)$ to
$\mathcal{O}(\sqrt{L})$, and improving fault-tolerant efficiency when
simulating systems with fixed particle numbers. Building on the block encoding
framework developed for general many-body Hamiltonians, we extend our approach
to electronic Hamiltonians whose coefficient tensors exhibit translation
invariance or possess decaying structures. Our results provide a practical path
toward early fault-tolerant quantum simulation of many-body systems,
substantially lowering resource overheads compared to previous methods.

</details>


### [32] [A Decoy-like Protocol for Quantum Key Distribution: Enhancing the Performance with Imperfect Single Photon Sources](https://arxiv.org/abs/2510.09454)
*Chanaprom Cholsuk,Furkan Ağlarcı,Daniel K. L. Oi,Serkan Ateş,Tobias Vogl*

Main category: quant-ph

TL;DR: A new QKD protocol that relaxes the strict requirement for ultra-low g^(2)(0) values in single photon sources, enabling use of more practical SPSs while maintaining security through g^(2)(0) variation detection of PNS attacks.


<details>
  <summary>Details</summary>
Motivation: Achieving ultra-low g^(2)(0) values (<0.1) for single photon sources is experimentally challenging, limiting practical QKD implementation despite many SPSs routinely achieving g^(2)(0) > 0.1.

Method: Proposes a decoy-like QKD protocol that exploits g^(2)(0) variation as a diagnostic tool to detect photon-number-splitting attacks, similar to decoy-state methods but using correlation statistics instead of intensity modulation.

Result: Protocol enables secure key generation from both single- and two-photon pulses, outperforms GLLP framework under high channel loss, works with various solid-state SPSs, and requires no additional hardware since g^(2)(0) can be extracted from standard QKD experiments.

Conclusion: Establishes a practical route toward high-performance QKD without needing ultra-pure SPSs, simplifies laser systems for SPS generation, and is applicable to satellite-based communication.

Abstract: Quantum key distribution (QKD) relies on single photon sources (SPSs), e.g.
from solid-state systems, as flying qubits, where security strongly requires
sub-Poissonian photon statistics with low second-order correlation values
(\$g^{(2)}(0)\$). However, achieving such low \$g^{(2)}(0)\$ remains
experimentally challenging. We therefore propose a decoy-like QKD protocol that
relaxes this constraint while maintaining security. This enables the use of
many SPSs with \$g^{(2)}(0) > \$0.1, routinely achieved in experiments but
rarely considered viable for QKD. Monte Carlo simulations and our experiment
from defects in hexagonal boron nitride show that, under linear loss,
\$g^{(2)}(0)\$ remains constant, whereas photon-number-splitting (PNS) attacks
introduce nonlinear effects that modify the measured \$g^{(2)}(0)\$ statistics.
Exploiting this \$g^{(2)}(0)\$ variation as a diagnostic tool, our protocol
detects PNS attacks analogously to decoy-state methods. Both single- and
two-photon pulses consequently securely contribute to the secret key rate. Our
protocol outperforms the Gottesman--Lo--Lutkenhaus--Preskill (GLLP) framework
under high channel loss across various solid-state SPSs and is applicable to
the satellite-based communication. Since \$g^{(2)}(0)\$ can be extracted from
standard QKD experiments, no additional hardware is required. The relaxed
\$g^{(2)}(0)\$ requirement simplifies the laser system for SPS generation. This
establishes a practical route toward high-performance QKD without the need for
ultra-pure SPSs.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [33] [Atomistic origin of low thermal conductivity in quaternary chalcogenides Cu(Cd, Zn)$_2$InTe$_4$](https://arxiv.org/abs/2510.09040)
*Nirmalya Jana,Amit Agarwal,Koushik Pal*

Main category: cond-mat.mtrl-sci

TL;DR: The paper investigates the microscopic origins of low lattice thermal conductivity in quaternary chalcogenide semiconductors CuCd2InTe4 and CuZn2InTe4 using first-principles analysis of phonon transport mechanisms.


<details>
  <summary>Details</summary>
Motivation: Quaternary chalcogenide semiconductors exhibit low lattice thermal conductivity, making them vital for applications like barrier coatings and thermoelectrics, but the fundamental reasons for this low thermal conductivity remain poorly understood.

Method: Used a unified first-principles framework to analyze both Peierls (particle-like) and coherence (wave-like) phonon transport mechanisms, examining phonon anharmonicity, scattering, and acoustic-optical phonon interactions.

Result: Extended antibonding states below Fermi level enhance phonon anharmonicity and scatter heat-carrying phonons, suppressing thermal conductivity. Peierls mechanism dominates total thermal conductivity while coherence remains negligible. Heavier Cd ions in CuCd2InTe4 cause greater acoustic-optical phonon overlap and scattering compared to CuZn2InTe4.

Conclusion: The study establishes atomistic origins of low thermal conductivity in quaternary chalcogenides and provides design principles for developing low-thermal-conductivity semiconductors through understanding phonon transport mechanisms and material composition effects.

Abstract: Crystalline semiconductors with intrinsically low lattice thermal
conductivity ($\mathcal{K}$) are vital for device applications such as barrier
coatings and thermoelectrics. Quaternary chalcogenide semiconductors such as
CuCd$_2$InTe$_4$ and CuZn$_2$InTe$_4$ are experimentally shown to exhibit low
$\mathcal{K}$, yet its microscopic origin remains poorly understood. Here, we
analyse their thermal transport mechanisms using a unified first-principles
framework that captures both the Peierls (particle-like propagation,
$\mathcal{K}_P$) and coherence (wave-like tunneling, $\mathcal{K}_C$)
mechanisms of phonon transport. We show that extended antibonding states below
the Fermi level lead to enhanced phonon anharmonicity and strong scattering of
heat-carrying phonon modes, suppressing $\mathcal{K}$ in these chalcogenides.
We show that $\mathcal{K}_P$ dominates the total thermal conductivity, while
$\mathcal{K}_C$ remains negligible even under strong anharmonicity of the
phonon modes. The heavier Cd ions in CuCd$_2$InTe$_4$ induce greater
acoustic-optical phonon overlap and scattering compared to CuZn$_2$InTe$_4$,
further lowering thermal conductivity of the former. Additionally, grain
boundary scattering in realistic samples contributes to further suppression of
thermal transport. Our findings establish the atomistic origins of low
$\mathcal{K}$ in quaternary chalcogenides and offer guiding principles for
designing low-thermal-conductivity semiconductors.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [34] [Simulating dynamic bonding in soft materials](https://arxiv.org/abs/2510.08879)
*Tyla R. Holoman,B. P. Prajwal,Glen M. Hocky,Thomas M. Truskett*

Main category: cond-mat.soft

TL;DR: Review of molecular simulation methods for modeling dynamic bonding in soft materials


<details>
  <summary>Details</summary>
Motivation: Dynamic bonding is essential in soft materials, and molecular simulations provide unique insights that experiments alone cannot offer

Method: Review of molecular dynamics, Monte Carlo, and hybrid simulation methods for modeling bonding kinetics and thermodynamics

Result: Recent advances in simulation techniques for dynamic bonding are highlighted

Conclusion: The review identifies outstanding challenges and future directions for modeling dynamic bonding in soft matter

Abstract: Dynamic bonding is an essential feature of many soft materials. Molecular
simulations have proven to be a powerful tool for modeling bonding kinetics and
thermodynamics in these materials, providing insights into their properties
that cannot be obtained by experiments alone. Here, we review recent advances
in modeling dynamic bonding in soft matter via molecular dynamics, Monte Carlo,
and hybrid simulation methods, highlighting outstanding challenges and future
directions.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [35] [Influence of an external static magnetic field on prebreakdown electron emission and heating](https://arxiv.org/abs/2510.09113)
*Roni Koitermaa,Marzhan Toktaganova,Andreas Kyritsakis,Tauno Tiirats,Alexej Grudiev,Veronika Zadin,Flyura Djurabekova*

Main category: physics.acc-ph

TL;DR: High magnetic fields (10-30 T) can cause vacuum arcing by focusing electron beams to heat anode surfaces sufficiently for evaporation and plasma initiation.


<details>
  <summary>Details</summary>
Motivation: To understand how magnetic fields influence vacuum arcing and the roles of cathode and anode in the process.

Method: Used particle-in-cell (PIC) and finite element methods (FEM) to simulate electron beam behavior and heating under different electric and magnetic field configurations (0-30 T).

Result: Magnetic fields significantly focus electron beams onto anode surfaces, causing temperature increases sufficient for evaporation. Field directions play crucial roles in beam focusing.

Conclusion: Magnetic fields of 10-30 T can enable plasma initiation on anode side through electron beam focusing and heating, revealing anode's active role in vacuum arcing.

Abstract: High magnetic fields can increase the occurrence of vacuum arcing, suggesting
that both electric and magnetic fields can play a role in the vacuum arcing
process. The mechanism of vacuum arcing in high magnetic fields is believed to
involve both the cathode and the anode, with the cathode serving as the
originator of field-emitting nanoprotrusions or tips, while the anode serves a
secondary role. Significant heating of the anode surface can be achieved by
magnetic focusing of the emitted electron beam, leading to increased heat flux
due to greater current density. We simulated the emitted electron beam in
different configurations of the electric and magnetic fields using the
particle-in-cell (PIC) and finite element methods (FEM). The heating caused by
the impacting electron beam was simulated for magnetic fields ranging from 0 T
to 30 T. The directions of the electric and magnetic fields were found to play
a major role in the focusing of the electron beam. We found that a sufficient
temperature increase on the anode surface for evaporation can be reached at
magnetic fields on the order of 10-30 T, suggesting the possibility of plasma
initiation on the anode side.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [36] [Bayesian Model Inference using Bayesian Quadrature: the Art of Acquisition Functions and Beyond](https://arxiv.org/abs/2510.08974)
*Jingwen Song,Pengfei Wei*

Main category: stat.CO

TL;DR: The paper develops four new acquisition functions for Bayesian Quadrature to improve posterior estimation and model evidence calculation, addressing challenges like multi-modality and high-dimensional dependencies.


<details>
  <summary>Details</summary>
Motivation: Bayesian model inference faces challenges with complex posterior features like multi-modalities, nonlinear dependencies, and high sharpness. Bayesian Quadrature provides flexible balance between computational cost and accuracy, but its performance depends heavily on acquisition functions.

Method: Reexamines advanced acquisition functions from a prospective inference perspective, reformulates quadrature rules for prediction, and develops four new acquisition functions based on distinct intuitions about expected rewards. These functions measure prediction uncertainty of posterior, contribution to prediction uncertainty of evidence, and expected reduction of prediction uncertainties.

Result: The new acquisition functions provide flexibility for highly effective design of integration points and are extended to transitional BQ scheme with specific refinements to tackle complex posterior challenges efficiently and robustly.

Conclusion: The developed methods demonstrate effectiveness through extensive benchmark studies and application to an engineering example, providing efficient and robust solutions for Bayesian model inference with complex posterior features.

Abstract: Estimating posteriors and the associated model evidences is a core issue of
Bayesian model inference, and can be of great challenge given complex features
of the posteriors such as multi-modalities of unequal importance, nonlinear
dependencies and high sharpness. Bayesian Quadrature (BQ) has emerged as a
competitive framework for tackling this challenge, as it provides flexible
balance between computational cost and accuracy. The performance of a BQ scheme
is fundamentally dictated by the acquisition function as it exclusively governs
the generation of integration points. After reexamining one of the most
advanced acquisition function from a prospective inference perspective and
reformulating the quadrature rules for prediction, four new acquisition
functions, inspired by distinct intuitions on expected rewards, are primarily
developed, all of which are accompanied by elegant interpretations and highly
efficient numerical estimators. Mathematically, these four acquisition
functions measure, respectively, the prediction uncertainty of posterior, the
contribution to prediction uncertainty of evidence, as well as the expected
reduction of prediction uncertainties concerning posterior and evidence, and
thus provide flexibility for highly effective design of integration points.
These acquisition functions are further extended to the transitional BQ scheme,
along with several specific refinements, to tackle the above-mentioned
challenges with high efficiency and robustness. Effectiveness of the
developments is ultimately demonstrated with extensive benchmark studies and
application to an engineering example.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [37] [Spectral theory for Lévy and Lévy-Ornstein-Uhlenbeck semigroups on step 2 Carnot groups](https://arxiv.org/abs/2510.08866)
*Maria Gordina,Rohan Sarkar*

Main category: math.PR

TL;DR: Analysis of non-local perturbations of sub-Laplacians on step 2 Carnot groups using harmonic analysis to establish intertwining relationships and identify spectra.


<details>
  <summary>Details</summary>
Motivation: To study non-local perturbations acting along vertical directions in Carnot groups and understand their spectral properties and semigroup behavior.

Method: Using harmonic analysis on Carnot groups to obtain intertwining relationships between semigroups generated by perturbed operators and Euclidean semigroups with continuous spectrum.

Result: Identified spectrum of perturbed operators, proved ergodicity of L\'evy-Ornstein-Uhlenbeck semigroups, showed all L\'evy-OU generators are isospectral, and obtained explicit eigenspace descriptions.

Conclusion: The intertwining approach successfully characterizes spectral properties of non-local perturbations on Carnot groups and reveals isospectrality of L\'evy-OU generators.

Abstract: We consider non-local perturbations $\Delta^\psi_G$ of sub-Laplacians on a
step $2$ Carnot group $G$. The perturbations are by translation-invariant
non-local operators acting along the vertical directions in $G$. We use
harmonic analysis on $G$ to obtain intertwining relationship between the
semigroups generated by $\Delta^\psi_G$ and some strongly continuous
contraction semigroups on Euclidean spaces with purely continuous spectrum, and
as a result we identify the spectrum of $\Delta^\psi_G$. Further we introduce
the L\'evy-Ornstein-Uhlenbeck (OU) semigroup corresponding to $\Delta^\psi_G$.
We prove that these Markov semigroups are ergodic, though they are not normal
operators on $L^2$ space with respect to the invariant distribution
$\mathsf{p}_\psi$. The intertwining relationships allow us to show that all
L\'evy-OU generators on $G$ are isospectral, that is, they have the same
eigenvalues with the same multiplicities. As a byproduct, we obtain a precise
description of the eigenspaces, and also derive explicit formula for the
co-eigenfunctions corresponding to some eigenvalues.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [38] [Intelligent backpropagated neural networks application on Couette-Poiseuille flow of variable viscosity in a composite porous channel filled with an anisotropic porous layer](https://arxiv.org/abs/2510.08745)
*Timir Karmakar,Amrita Mandal*

Main category: physics.flu-dyn

TL;DR: This study analyzes Couette-Poiseuille flow with variable viscosity in a partially porous channel using anisotropic porous media. It combines numerical methods with machine learning (ALMM-BNN) to predict solutions across different parameter ranges, particularly in challenging intermediate ranges.


<details>
  <summary>Details</summary>
Motivation: To understand shear stress distribution in arterial blood flow with variable viscosity and inertial effects, and to develop frameworks for creating biological glycocalyx-mimicking microfluidic systems.

Method: Combines Navier-Stokes equations for free flow region with Brinkman-Forchheimer-extended Darcy's equation for porous medium. Uses iterative methods, regular perturbation, matched asymptotic expansion, and ALMM-BNN (artificial Levenberg-Marquardt method with back-propagated neural network) for intermediate parameter ranges.

Result: Asymptotic solutions show good agreement with numerical results for high/low Darcy numbers but fail in intermediate range. ALMM-BNN successfully captures overall trends and qualitative agreement in challenging intermediate ranges, though not exact solutions.

Conclusion: ALMM-BNN paradigm demonstrates potential as robust predictive tool for challenging parameter ranges where numerical solutions are difficult or expensive, providing valuable insights for arterial blood flow analysis and glycocalyx scaffolding development.

Abstract: This study examines Couette-Poiseuille flow of variable viscosity within a
channel that is partially filled with a porous medium. To enhance its practical
relevance, we assume that the porous medium is anisotropic with permeability
varying in all directions, making it a positive semidefinite matrix in the
momentum equation. We assume the Navier-Stokes equations govern the flow in the
free flow region, while the Brinkman-Forchheimer-extended Darcy's equation
governs the flow inside the porous medium. The coupled system contains a
nonlinear term from the Brinkman-Forchheimer equation. We propose an
approximate solution using an iterative method valid for a wide range of porous
media parameter values. For both high and low values of the Darcy number, the
asymptotic solutions derived from the regular perturbation method and matched
asymptotic expansion show good agreement with the numerical results. However,
these methods are not effective in the intermediate range. To address this, we
employ the artificial Levenberg-Marquardt method with a back-propagated neural
network (ALMM-BNN) paradigm to predict the solution in the intermediate range.
While it may not provide the exact solutions, it successfully captures the
overall trend and demonstrates good qualitative agreement with the numerical
results. This highlights the potential of the ALMM-BNN paradigm as a robust
predictive tool in challenging parameter ranges where numerical solutions are
either difficult to obtain or computationally expensive. The current model
provides valuable insights into the shear stress distribution of arterial blood
flow, taking into account the variable viscosity of the blood in the presence
of inertial effects. It also offers a framework for creating glycocalyx
scaffolding and other microfluidic systems that can mimic the biological
glycocalyx.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [39] [On elastic and non-elastic wave superpositions for the Euler system](https://arxiv.org/abs/2510.09576)
*Łukasz Chomienia,Alfred Michel Grundland*

Main category: math-ph

TL;DR: Analysis of elastic vs non-elastic wave superpositions in Euler system shows Lie algebra structure enables parametrization of non-elastic superposition region and reveals geometric properties.


<details>
  <summary>Details</summary>
Motivation: To understand the mathematical conditions governing elastic versus non-elastic wave superpositions in the Euler system and characterize their geometric structure.

Method: Using Lie algebra analysis of vector fields associated with sound waves, parametrizing non-elastic superposition regions, and studying geometric properties of the resulting manifold.

Result: Found that the Lie algebra is semidirect sum of infinite-dimensional Abelian and finite-dimensional non-Abelian algebras, enabling construction of reduced Euler system with local Lie group structure and torsion-free affine connection.

Conclusion: Non-elastic wave superpositions can be described geometrically as motion of quasi-rectifiable surfaces spanned by sound wave vector fields, with one-parameter subgroups serving as geodesics.

Abstract: The paper contains an analysis of the conditions for the existence of elastic
versus non-elastic wave superpositions governed by the Euler system in
(1+1)-dimensions. This system admits entropic solutions and two types of sound
wave solutions. It is shown that the smallest real Lie algebra containing
vector fields (associated with these waves) is isomorphic to a semidirect sum
of the infinite-dimensional Abelian real Lie algebra and the non-Abelian
finite-dimensional real Lie algebra. Based on this fact, we are able to find a
parametrization of the region of non-elastic wave superpositions which allows
for the construction of the reduced form of the Euler system. It is proven
that, in this case, the manifold spanned by the vector fields associated with
waves possesses the structure of a local Lie group. It is also shown that this
manifold has a torsion-free affine connection and the one-parameter subgroups
of the Lie group are geodesics. Based on this fact, a description of the
non-elastic wave superposition is given in terms of motion of the
quasi-rectifiable surfaces spanned by the pair of vector fields associated with
sound waves.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [40] [Quantum Trigonometric Bézier Curves](https://arxiv.org/abs/2510.09336)
*Çetin Dişibüyük*

Main category: math.CA

TL;DR: This paper introduces quantum trigonometric Bézier curves with shape parameters, analyzes their shape preserving properties through total positivity, presents recursive evaluation algorithms, and extends to rational quantum trigonometric Bézier curves.


<details>
  <summary>Details</summary>
Motivation: To develop quantum trigonometric Bézier curves with shape parameters that preserve geometric shapes and can be efficiently evaluated, extending classical Bézier curves to quantum trigonometric domain.

Method: Introduces one-parameter family of trigonometric Bernstein basis functions, studies total positivity for shape analysis, develops two recursive evaluation algorithms, and defines rational quantum trigonometric Bézier curves.

Result: The quantum trigonometric Bézier curves exhibit shape preserving properties through total positivity analysis, can be evaluated recursively, and their rational counterparts also maintain good shape preserving characteristics.

Conclusion: Quantum trigonometric Bézier curves with shape parameters are successfully constructed and shown to possess desirable shape preserving properties and efficient evaluation methods, with rational extensions maintaining these advantages.

Abstract: In order to construct quantum trigonometric B\'ezier curves with shape
parameter, one parameter family of trigonometric Bernstein basis functions are
introduced. We study the total positivity of the basis functions to analyze the
shape preserving properties of the quantum trigonometric B\'ezier curves. We
also showed that quantum trigonometric B\'ezier curves can be evaluated by two
different recursive evaluation algorithms. Finally, we have defined rational
counterpart of quantum trigonometric B\'ezier curves and show that the rational
quantum trigonometric B\'ezier curves posses nice shape preserving properties.

</details>


### [41] [Some capacitary strong type inequalities and related function spaces](https://arxiv.org/abs/2510.08982)
*Keng Hao Ooi,Nguyen Cong Phuc*

Main category: math.CA

TL;DR: The paper verifies Adams' conjecture on capacitary strong type inequalities, generalizing Maz'ya's classical result, and characterizes related function spaces through K"othe duality and isomorphism to concrete spaces.


<details>
  <summary>Details</summary>
Motivation: To prove Adams' conjecture about capacitary strong type inequalities that extend Maz'ya's classical work, and to provide better characterizations of related function spaces for practical applications in harmonic analysis.

Method: Using tools from nonlinear potential theory, weighted norm inequalities, and Banach function space theory to analyze the capacitary inequalities and establish isomorphisms between abstract and concrete function spaces.

Result: Successfully verified Adams' conjecture, characterized function spaces as K"othe duals to Sobolev multiplier spaces, and showed these spaces are isomorphic to more concrete spaces suitable for harmonic analysis.

Conclusion: The work provides a complete verification of Adams' conjecture and establishes practical characterizations of function spaces that bridge abstract theory with concrete applications in modern harmonic analysis.

Abstract: We verify a conjecture of D. R. Adams on a capacitary strong type inequality
that generalizes the classical capacitary strong type inequality of V. G.
Maz'ya. As a result, we characterize related function spaces as K\"othe duals
to a class of Sobolev multiplier type spaces. Moreover, using tools from
nonlinear potential theory, weighted norm inequalities, and Banach function
space theory, we show that these spaces are also isomorphic to more concrete
spaces that are easy to use and fit in well with the modern theory of function
spaces of harmonic analysis.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Multi-fidelity Batch Active Learning for Gaussian Process Classifiers](https://arxiv.org/abs/2510.08865)
*Murray Cutforth,Yiming Yang,Tiffany Fan,Serge Guillas,Eric Darve*

Main category: cs.LG

TL;DR: BPMI is a batch active learning algorithm for multi-fidelity GP classifiers that uses Taylor expansion to efficiently calculate mutual information for binary simulation outputs, outperforming baselines in accuracy.


<details>
  <summary>Details</summary>
Motivation: Many science and engineering problems use expensive computational simulations, and multi-fidelity approaches can accelerate parameter space exploration by efficiently allocating simulation budgets.

Method: Introduces Bernoulli Parameter Mutual Information (BPMI) - a batch active learning algorithm that uses first-order Taylor expansion of the link function to overcome intractability of mutual information calculation in probability space for multi-fidelity GP classifiers.

Result: BPMI demonstrated superior performance against several baselines on two synthetic test cases and a real-world laser-ignited rocket combustor simulation, achieving higher predictive accuracy for fixed computational budgets.

Conclusion: BPMI is an effective approach for efficient simulation budget allocation in multi-fidelity settings with binary outputs, providing improved accuracy over existing methods.

Abstract: Many science and engineering problems rely on expensive computational
simulations, where a multi-fidelity approach can accelerate the exploration of
a parameter space. We study efficient allocation of a simulation budget using a
Gaussian Process (GP) model in the binary simulation output case. This paper
introduces Bernoulli Parameter Mutual Information (BPMI), a batch active
learning algorithm for multi-fidelity GP classifiers. BPMI circumvents the
intractability of calculating mutual information in the probability space by
employing a first-order Taylor expansion of the link function. We evaluate BPMI
against several baselines on two synthetic test cases and a complex, real-world
application involving the simulation of a laser-ignited rocket combustor. In
all experiments, BPMI demonstrates superior performance, achieving higher
predictive accuracy for a fixed computational budget.

</details>


### [43] [Weights initialization of neural networks for function approximation](https://arxiv.org/abs/2510.08780)
*Xinwen Hu,Yunqing Huang,Nianyu Yi,Peimeng Yin*

Main category: cs.LG

TL;DR: A reusable initialization framework using basis function pretraining improves neural network training efficiency, generalization, and transferability by pretraining on polynomials and using domain mapping.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in neural network training including the need for training from scratch for each function, sensitivity to architecture/hyperparameters, and poor generalization beyond training domains.

Method: Basis neural networks are pretrained to approximate polynomial families on a reference domain, then used to initialize networks for complex functions. Domain mapping transforms inputs to preserve structural correspondence.

Result: Extensive experiments in 1D and 2D settings show substantial improvements in training efficiency, generalization, and model transferability.

Conclusion: Initialization-based strategies show promise for scalable and modular neural function approximation.

Abstract: Neural network-based function approximation plays a pivotal role in the
advancement of scientific computing and machine learning. Yet, training such
models faces several challenges: (i) each target function often requires
training a new model from scratch; (ii) performance is highly sensitive to
architectural and hyperparameter choices; and (iii) models frequently
generalize poorly beyond the training domain. To overcome these challenges, we
propose a reusable initialization framework based on basis function
pretraining. In this approach, basis neural networks are first trained to
approximate families of polynomials on a reference domain. Their learned
parameters are then used to initialize networks for more complex target
functions. To enhance adaptability across arbitrary domains, we further
introduce a domain mapping mechanism that transforms inputs into the reference
domain, thereby preserving structural correspondence with the pretrained
models. Extensive numerical experiments in one- and two-dimensional settings
demonstrate substantial improvements in training efficiency, generalization,
and model transferability, highlighting the promise of initialization-based
strategies for scalable and modular neural function approximation. The full
code is made publicly available on Gitee.

</details>


### [44] [Residual-Informed Learning of Solutions to Algebraic Loops](https://arxiv.org/abs/2510.09317)
*Felix Brandt,Andreas Heuermann,Philip Hannebohm,Bernhard Bachmann*

Main category: cs.LG

TL;DR: A residual-informed ML approach replaces algebraic loops in Modelica models with neural network surrogates, achieving 60% faster simulation while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To accelerate equation-based Modelica simulations by eliminating computationally expensive algebraic loops through machine learning surrogates.

Method: Uses feedforward neural networks trained with residual (error) of algebraic loops directly in loss function, eliminating need for supervised datasets and resolving ambiguous solution issues.

Result: Applied to IEEE 14-Bus system, achieved 60% reduction in simulation time compared to conventional methods while maintaining same accuracy through error control.

Conclusion: The residual-informed training approach successfully replaces algebraic loops with neural network surrogates, significantly accelerating simulations without compromising accuracy.

Abstract: This paper presents a residual-informed machine learning approach for
replacing algebraic loops in equation-based Modelica models with neural network
surrogates. A feedforward neural network is trained using the residual (error)
of the algebraic loop directly in its loss function, eliminating the need for a
supervised dataset. This training strategy also resolves the issue of ambiguous
solutions, allowing the surrogate to converge to a consistent solution rather
than averaging multiple valid ones. Applied to the large-scale IEEE 14-Bus
system, our method achieves a 60% reduction in simulation time compared to
conventional simulations, while maintaining the same level of accuracy through
error control mechanisms.

</details>
