<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 27]
- [math.AP](#math.AP) [Total: 25]
- [physics.comp-ph](#physics.comp-ph) [Total: 8]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 2]
- [math.CA](#math.CA) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 4]
- [math.SP](#math.SP) [Total: 2]
- [math.DG](#math.DG) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 5]
- [quant-ph](#quant-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [gr-qc](#gr-qc) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [math.RA](#math.RA) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [math.FA](#math.FA) [Total: 2]
- [cs.LG](#cs.LG) [Total: 7]
- [physics.optics](#physics.optics) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Tensor-based compression of the sea temperature data](https://arxiv.org/abs/2510.09778)
*Ilya Kosolapov,Tatiana Sheloput,Sergey Matveev*

Main category: math.NA

TL;DR: The paper shows that standard SVD-based tensor compression methods work well for spatiotemporal sea temperature data with missing values, and proposes spatial and temporal partitioning strategies that achieve 5x compression.


<details>
  <summary>Details</summary>
Motivation: To efficiently compress spatiotemporal sea temperature tensors with complex structure and significant missing values, avoiding the need for complex tensor completion algorithms.

Method: Uses standard SVD-based tensor compression formats (Tucker, Tensor-Train, Quantized-TT) with greedy spatial partitioning and temporal partitioning on 2-day intervals.

Result: Achieves robust 5x compression ratio across the entire dataset, with temporal partitioning proving nearly optimal at 2-day intervals.

Conclusion: Standard tensor compression methods combined with spatial and temporal partitioning strategies are effective for compressing complex spatiotemporal sea temperature data with missing values.

Abstract: In this work we investigate efficient data compression for spatiotemporal
Black, Azov and Marmara Seas temperature tensors that contain significant
number of missing values. These tensors have a complex structure influenced by
the coastlines and bathymetry, as well as temporal temperature changes. While
such missing data typically provokes utilization of tensor completion
algorithms, we demonstrate that standard SVD-based compression approaches
(including the Tucker, Tensor-Train (TT) and Quantized-TT formats) are
remarkably effective and yield comparable results. We propose a greedy spatial
data partitioning algorithm enhancing their performance. We divide the data
into the smaller subtensors before compression via exploitation of this trick.
  Furthermore, our analysis reveals a strong temporal dependency in the data's
compressibility caused by its nature. Fixing the level of precision we observe
a significant seasonal variation. Investigating this, we find that a temporal
partitioning on a scale of approximately two days is nearly optimal for all
tested tensor based formats. The combined application of these spatial and
temporal strategies with tensor methods ultimately achieves a robust
compression ratio of 5 times across the entire dataset.

</details>


### [2] [Fast and Accurate Intersections on a Sphere](https://arxiv.org/abs/2510.09892)
*Hongyu Chen,Paul A. Ullrich,Julian Panetta*

Main category: math.NA

TL;DR: A fast, high-precision algorithm for calculating intersections between great circle arcs and latitude lines on spheres using error-free transformations for machine precision accuracy with no computational overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional geoscience software has slow and numerically unstable intersection calculations, creating a need for high-performance algorithms suitable for applications like high-resolution climate data regridding.

Method: Proposed a simplified intersection point formula combined with error-free transformation (EFT) techniques to achieve machine precision accuracy, with vectorized and parallelized implementation.

Result: Achieved enhanced accuracy with no compute-time overhead compared to direct hardware floating point calculations, outperforming quadruple precision, arbitrary precision, and CGAL alternatives that inhibit vectorization.

Conclusion: Demonstrates that EFT techniques can be effectively combined to implement complex geometric calculations with both high accuracy and speed, making the algorithm suitable for performance-sensitive applications.

Abstract: We introduce a fast, high-precision algorithm for calculating intersections
between great circle arcs and lines of constant latitude on the unit sphere. We
first propose a simplified intersection point formula with improved speed and
numerical robustness over the ones traditionally implemented in geoscience
software. We then show how algorithms based on the concept of error-free
transformations (EFT) can be applied to evaluate this formula within a relative
error bound that is on the order of machine precision. We demonstrate that,
with a vectorized and parallelized implementation, this enhanced accuracy is
achieved with no compute-time overhead compared to a direct calculation in
hardware floating point, making our algorithm suitable for
performance-sensitive applications like regridding of high-resolution climate
data. In contrast, evaluating our formula using high-precision data types like
quadruple precision and arbitrary precision, or using the robust intersection
computation routines from the Computational Geometry Algorithms Library (CGAL),
leads to significant computational overhead, especially since these
alternatives inhibit vectorization. More generally, our work demonstrates how
EFT techniques can be combined and extended to implement nontrivial geometric
calculations with high accuracy and speed.

</details>


### [3] [Noncommutative Laplacian and numerical approximation of Laplace-Beltrami spectrum of compact Riemann surfaces](https://arxiv.org/abs/2510.09909)
*Damien Tageddine,Jean-Christophe Nave*

Main category: math.NA

TL;DR: Numerical approximation of Laplace-Beltrami operator on axially symmetric surfaces using noncommutative Laplace operator on hermitian matrices derived from S^1-action foliation.


<details>
  <summary>Details</summary>
Motivation: To develop numerical methods for approximating the Laplace-Beltrami operator on compact surfaces with axial symmetry embedded in 3D space.

Method: Use noncommutative Laplace operator defined on finite dimensional hermitian matrices, derived from foliation of the surface obtained under S^1-action.

Result: Present numerical results for sphere and generic ellipsoid cases.

Conclusion: Successfully derived numerical approximation method for Laplace-Beltrami operator on axially symmetric surfaces using matrix-based approach.

Abstract: We derive a numerical approximation of the Laplace-Beltrami operator on
compact surfaces embedded in $\mathbb{R}^3$ with an axial symmetry. To do so we
use a noncommutative Laplace operator defined on the space of finite
dimensional hermitian matrices. This operator is derived from a foliation of
the surface obtained under an $S^1$-action on the surface. We present numerical
results in the case of the sphere and a generic ellipsoid.

</details>


### [4] [Finite element analysis of a nonlinear heat Equation with damping and pumping effects](https://arxiv.org/abs/2510.10210)
*Rishabh Shukla,Wasim Akram,Manil T. Mohan*

Main category: math.NA

TL;DR: This paper analyzes a nonlinear heat equation with damping and pumping effects, establishing existence, uniqueness, and regularity of weak solutions, and provides finite element analysis with error estimates for various numerical methods.


<details>
  <summary>Details</summary>
Motivation: To study the mathematical properties of nonlinear reaction-diffusion equations with damping and pumping terms, which model various physical phenomena, and develop robust numerical methods for their solution.

Method: The authors use functional analysis to prove existence and uniqueness of weak solutions, then conduct finite element analysis using conforming, nonconforming, and discontinuous Galerkin methods with appropriate projection operators to derive error estimates.

Result: Existence and uniqueness of weak solutions are established for all dimensions and damping exponents. Regularity results are obtained with specific constraints on dimension and exponent. A priori error estimates are derived for semi- and fully discrete schemes.

Conclusion: The paper provides comprehensive mathematical analysis and numerical methods for nonlinear reaction-diffusion equations with damping and pumping, with rigorous error estimates that hold under specific regularity conditions.

Abstract: We study the following nonlinear heat equation with damping and pumping
effects (a reaction-diffusion equation) posed on a bounded simply connected
convex domain $\Omega \subset \mathbb{R}^d$, $d \geq 1$ with Lipschitz boundary
$\partial\Omega$: $$ \frac{\partial u(t)}{\partial t} - \nu \Delta u(t) +
\alpha |u(t)|^{p-2}u(t) - \sum_{\ell=1}^M \beta_{\ell} |u(t)|^{q_{\ell}-2}u(t)
= f(t), \quad t>0, $$ subject to homogeneous Dirichlet boundary conditions and
the initial condition $u(0)=u_0$, where $2 \leq p < \infty$ and $2 \leq
q_{\ell} < p$ for $1 \leq \ell \leq M$. For $u_0 \in L^2(\Omega)$ and $f \in
L^2(0,T;H^{-1}(\Omega))$, we establish the existence and uniqueness of a weak
solution for all dimensions $d \in \mathbb{N}$ and damping exponents $2 \leq p
< \infty$. Furthermore, for $u_0 \in H^2(\Omega) \cap H_0^1(\Omega)$ and $f \in
H^1(0,T;H^1(\Omega))$, we obtain regularity results: these hold for every $2
\leq p < \infty$ when $1 \leq d \leq 4$, and for $2 \leq p \leq
\frac{2d-6}{d-4}$ when $d \geq 5$. We further conduct finite element analysis
using conforming, nonconforming, and discontinuous Galerkin methods, deriving a
priori error estimates for both semi- and fully discrete schemes, supported by
numerical results. To relax restrictions on $p$ in the semidiscrete analysis,
we use appropriate projection/interpolation operators: the Ritz projection in
the conforming case ($2 \le p \le \frac{2d}{d-2}$), the Scott-Zhang
interpolation for $\frac{2d}{d-2} < p \le \frac{2d-6}{d-4}$, the Cl\'ement
interpolation in the nonconforming setting, and the $L^2$-projection in the DG
framework. In the fully discrete case, error estimates hold for the above
$p$-range under $u_0 \in D(A^{3/2})$ and $f \in H^1(0,T;H^1(\Omega))$.

</details>


### [5] [Weighted implicit-explicit discontinuous Galerkin methods for two-dimensional Ginzburg-Landau equations on general meshes](https://arxiv.org/abs/2510.10283)
*Zhen Guan,Xianxian Cao*

Main category: math.NA

TL;DR: A second-order linearized discontinuous Galerkin method is developed for 2D Ginzburg-Landau equations, providing unconditionally optimal error estimates through temporal-spatial step size analysis.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for solving the nonlinear Ginzburg-Landau equations that can handle general meshes and provide rigorous error analysis.

Method: Uses a second-order linearized discontinuous Galerkin method that includes BDF2 and Crank-Nicolson schemes as special cases, with analysis based on discontinuous Galerkin inverse inequality and mathematical induction.

Result: Achieved unconditionally optimal error estimate in L²-norm by analyzing two scenarios of temporal-spatial step size relationship: τ² ≤ h^{k+1} and τ² > h^{k+1}, where k is the polynomial degree.

Conclusion: The proposed method is validated through numerical examples with various grids and polynomial degrees, confirming the theoretical error estimates.

Abstract: In this paper, a second-order linearized discontinuous Galerkin method on
general meshes, which treats the backward differentiation formula of order two
(BDF2) and Crank-Nicolson schemes as special cases, is proposed for solving the
two-dimensional Ginzburg-Landau equations with cubic nonlinearity. By utilizing
the discontinuous Galerkin inverse inequality and the mathematical induction
method, the unconditionally optimal error estimate in $L^2$-norm is obtained.
The core of the analysis in this paper resides in the classification and
discussion of the relationship between the temporal step size and the spatial
step size, specifically distinguishing between the two scenarios of tau^2 \leq
h^{k+1}$and$\tau^2 > h^{k+1}$, where$k$denotes the degree of the discrete
spatial scheme. Finally, this paper presents two numerical examples involving
various grids and polynomial degrees to verify the correctness of the
theoretical results.

</details>


### [6] [Learning Operators through Coefficient Mappings in Fixed Basis Spaces](https://arxiv.org/abs/2510.10350)
*Chuqi Chen,Yang Xiang,Weihong Zhang*

Main category: math.NA

TL;DR: FB-C2CNet is a novel operator learning framework that maps between coefficient spaces of basis functions rather than pointwise values, reducing training complexity and enabling systematic analysis of basis choice effects.


<details>
  <summary>Details</summary>
Motivation: Classical operator learning approaches use pointwise-to-pointwise mapping which can be inefficient. The authors aim to develop a more efficient framework that operates in coefficient space to reduce training complexity and enable better understanding of basis function effects.

Method: FB-C2CNet projects input functions onto fixed basis functions (random features or finite element bases) and predicts solution coefficients in the same or different basis. This decouples basis selection from network training.

Result: Numerical experiments on Darcy flow, Poisson equations in various domains, and elasticity problems show FB-C2CNet achieves high accuracy and computational efficiency.

Conclusion: FB-C2CNet demonstrates strong potential for practical operator learning by providing an efficient coefficient-to-coefficient framework that enables systematic analysis of basis choice and generalization properties.

Abstract: Operator learning has emerged as a powerful paradigm for approximating
solution operators of partial differential equations (PDEs) and other
functional mappings. \textcolor{red}{}{Classical approaches} typically adopt a
pointwise-to-pointwise framework, where input functions are sampled at
prescribed locations and mapped directly to solution values. We propose the
Fixed-Basis Coefficient to Coefficient Operator Network (FB-C2CNet), which
learns operators in the coefficient space induced by prescribed basis
functions. In this framework, the input function is projected onto a fixed set
of basis functions (e.g., random features or finite element bases), and the
neural operator predicts the coefficients of the solution function in the same
or another basis. By decoupling basis selection from network training,
FB-C2CNet reduces training complexity, enables systematic analysis of how basis
choice affects approximation accuracy, and clarifies what properties of
coefficient spaces (such as effective rank and coefficient variations) are
critical for generalization. Numerical experiments on Darcy flow, Poisson
equations in regular, complex, and high-dimensional domains, and elasticity
problems demonstrate that FB-C2CNet achieves high accuracy and computational
efficiency, showing its strong potential for practical operator learning tasks.

</details>


### [7] [Staggered time discretization in finitely-strained heterogeneous visco-elastodynamics with damage or diffusion in the Eulerian frame](https://arxiv.org/abs/2510.10355)
*Tomáš Roubíček*

Main category: math.NA

TL;DR: Semi-implicit time discretization applied to compressible viscoelastic solid models in Eulerian description, proving numerical stability and convergence for 3D cases without requiring convexity of stored energy.


<details>
  <summary>Details</summary>
Motivation: To develop stable and convergent numerical methods for nonlinear viscoelastic solid models in actual deforming configurations, addressing challenges in computational mechanics for materials with complex rheological behavior.

Method: Uses semi-implicit (staggered) time discretization for compressible nonlinear viscoelastic solid models in Eulerian description with Kelvin-Voigt and Jeffreys rheologies, exploiting convexity of kinetic energy in linear momentum terms.

Result: Proved numerical stability and convergence towards weak solutions in three-dimensional cases, with enhancements outlined for damage and diffusion models.

Conclusion: The semi-implicit approach provides stable and convergent numerical schemes for viscoelastic solid models without requiring convexity of stored energy, offering potential extensions to more complex material behaviors.

Abstract: The semi-implicit (partly decoupled, also called staggered or fraction-step)
time discretization is applied to compressible nonlinear dynamical models of
viscoelastic solids in the Eulerian description, i.e.\ in the actual deforming
configuration, formulated fully in terms of rates. The Kelvin-Voigt rheology
and also, in the deviatoric part, the Jeffreys rheology are considered. The
numerical stability and, considering the Stokes-type viscosity multipolar of
the 2nd-grade, also convergence towards weak solutions are proved in
three-dimensional situations, exploiting the convexity of the kinetic energy
when written in terms of linear momentum. No (poly)convexity of the stored
energy is required and some enhancements (specifically towards damage and
diffusion models) are briefly outlined, too.

</details>


### [8] [Exact deflation for accurate SVD computation of nonnegative bidiagonal products of arbitrary rank](https://arxiv.org/abs/2510.10502)
*Rong Huang,Jungong Xue*

Main category: math.NA

TL;DR: A method for computing SVD of nonnegative bidiagonal products that handles zero singular values effectively, deflates them exactly, and computes nonzero singular values with high relative accuracy regardless of rank deficiency or ill conditioning.


<details>
  <summary>Details</summary>
Motivation: Zero singular values cause numerical difficulties in SVD computations, especially for rank-deficient and ill-conditioned matrices. Existing methods struggle with accurate computation when dealing with these challenging cases.

Method: Developed a method for computing SVD of nonnegative bidiagonal products that can handle arbitrary rank, full rank or rank-deficient factors, and square or rectangular matrices. The method exactly deflates zero singular values and computes nonzero singular values with high relative accuracy. Also supports accurate SVD computation for arbitrary submatrices.

Result: The method successfully deflates all zero singular values with favorable complexity, computes nonzero singular values with high relative accuracy regardless of how small they are, and handles rank deficiency and ill conditioning effectively. Error analysis and numerical experiments validate the claimed accuracy.

Conclusion: The proposed method provides a robust solution for SVD computation of nonnegative bidiagonal products, overcoming numerical challenges posed by zero singular values and enabling high-accuracy computation even for ill-conditioned and rank-deficient cases.

Abstract: Dealing with zero singular values can be quite challenging, as they have the
potential to cause numerous numerical difficulties. This paper presents a
method for computing the singular value decomposition (SVD) of a nonnegative
bidiagonal product of arbitrary rank, regardless of whether the factors are of
full rank or rank-deficient, square or rectangular. A key feature of our method
is its ability to exactly deflate all zero singular values with a favorable
complexity, irrespective of rank deficiency and ill conditioning. Furthermore,
it ensures the computation of nonzero singular values, no matter how small they
may be, with high relative accuracy. Additionally, our method is well-suited
for accurately computing the SVDs of arbitrary submatrices, leveraging an
approach to extract their representations from the original product. We have
conducted error analysis and numerical experiments to validate the claimed high
relative accuracy.

</details>


### [9] [Uniformly High Order Discontinuous Galerkin Gas Kinetic Scheme for Compressible flows](https://arxiv.org/abs/2510.10656)
*Mengqing Zhang,Shiyi Li,Dongmi Luo,Jianxian Qiu,Yibing Chen*

Main category: math.NA

TL;DR: A uniformly high-order discontinuous Galerkin gas kinetic scheme (DG-HGKS) is developed for compressible Euler equations, extending previous finite volume methods to achieve arbitrary high-order accuracy in space and time while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of existing one-stage DG-HGKS schemes that were restricted to no more than third-order accuracy, and to develop a more efficient high-order scheme for compressible flows.

Method: Extends CEHGKS framework to DG formulation by expanding numerical fluxes and volume integrals in time, replacing time derivatives with spatial derivatives, and implementing a limiter strategy combining KXRCF indicator with SHWENO reconstruction to handle discontinuities.

Result: The scheme achieves arbitrary high-order accuracy in both space and time, successfully demonstrated through 1D and 2D numerical tests showing robustness and effectiveness.

Conclusion: The proposed DG-HGKS scheme successfully breaks the third-order accuracy limitation of previous one-stage methods and provides an efficient high-order solution for compressible flow simulations.

Abstract: In this paper, a uniformly high-order discontinuous Galerkin gas kinetic
scheme (DG-HGKS) is proposed to solve the Euler equations of compressible
flows. The new scheme is an extension of the one-stage compact and efficient
high-order GKS (CEHGKS, Li et al. , 2021. J. Comput. Phys. 447, 110661) in the
finite volume framework. The main ideas of the new scheme consist of two parts.
Firstly, starting from a fully discrete DG formulation, the numerical fluxes
and volume integrals are expanded in time. Secondly, the time derivatives are
replaced by spatial derivatives using the techniques in CEHGKS. To suppress the
non-physical oscillations in the discontinuous regions while minimizing the
number of "troubled cells", an effective limiter strategy compatible with the
new scheme has been developed by combining the KXRCF indicator and the SHWENO
reconstruction technique. The new scheme can achieve arbitrary high-order
accuracy in both space and time, thereby breaking the previous limitation of no
more than third-order accuracy in existing one-stage DG-HGKS schemes. Numerical
tests in 1D and 2D have demonstrated the robustness and effectiveness of the
scheme.

</details>


### [10] [Novel superconvergence and ultraconvergence structures for the finite volume element method](https://arxiv.org/abs/2510.10668)
*Xiang Wang,Yuqing Zhang,Zhimin Zhang*

Main category: math.NA

TL;DR: Novel superconvergence and ultraconvergence structures for bi-k-order FVE method on rectangular meshes, achieving one-order-higher superconvergence and two-orders-higher ultraconvergence for derivatives.


<details>
  <summary>Details</summary>
Motivation: To develop tunable and asymmetric superconvergence/ultraconvergence points that standard bi-k-order finite elements cannot achieve, addressing the limitations in convergence properties of existing methods.

Method: Bi-k-order finite volume element method on rectangular meshes with tensorial k-k-order orthogonality via dual mesh constraints, using asymmetric-enabled M-decompositions (AMD-Super and AMD-Ultra) for theoretical analysis.

Result: Achieved one-order-higher superconvergence for derivatives and function values, and two-orders-higher ultraconvergence for derivatives under specific conditions (diagonal diffusion tensor, zero convection, FVE scheme with tensorial orthogonality).

Conclusion: The developed structures provide significant convergence improvements not available in standard methods, with numerical experiments confirming the theoretical findings. The 2D derivative ultraconvergence is non-trivial and more complex than 1D extensions due to directional coupling.

Abstract: This paper develops novel natural superconvergence and ultraconvergence
structures for the bi-$k$-order finite volume element (FVE) method on
rectangular meshes. These structures furnish tunable and possibly asymmetric
superconvergence and ultraconvergence points. We achieve one-order-higher
superconvergence for both derivatives and function values, and
two-orders-higher ultraconvergence for derivatives--a phenomenon that standard
bi-$k$-order finite elements do not exhibit. Derivative ultraconvergence
requires three conditions: a diagonal diffusion tensor, zero convection
coefficients, and the FVE scheme satisfying tensorial $k$-$k$-order
orthogonality (imposed via dual mesh constraints). This two-dimensional
derivative ultraconvergence is not a trivial tensor-product extension of the
one-dimensional phenomena; its analysis is also considerably more complex due
to directional coupling. Theoretically, we introduce the asymmetric-enabled
M-decompositions (AMD-Super and AMD-Ultra) to rigorously prove these phenomena.
Numerical experiments confirm the theory.

</details>


### [11] [Existence and numerical approximation of a one-dimensional Boussinesq system with variable coefficients on a finite interval](https://arxiv.org/abs/2510.10829)
*Juan Carlos Muñoz Grajales,Deissy Marcela Pizo*

Main category: math.NA

TL;DR: Study of well-posedness and numerical solutions for a nonlinear dispersive model describing surface waves in shallow water channels with irregular bottoms, including inverse problem analysis for initial condition reconstruction.


<details>
  <summary>Details</summary>
Motivation: To understand the mathematical properties and practical applications of nonlinear dispersive models for surface wave propagation in shallow water channels with variable bottom topography.

Method: Combined theoretical analysis of well-posedness with numerical simulations using a custom solver, and formulation of an optimization problem for inverse reconstruction of initial conditions from final state measurements.

Result: Development of analytical framework for model well-posedness and numerical methodology for both forward simulation and inverse problem solving of initial wave conditions.

Conclusion: The paper establishes theoretical foundations and provides numerical tools for analyzing nonlinear dispersive wave models in variable topography environments, with applications to both forward prediction and inverse reconstruction problems.

Abstract: In this paper, we investigate the well-posedness of a nonlinear dispersive
model with variable coefficients that describes the evolution of surface waves
propagating through a one-dimensional shallow water channel of finite length
with irregular bottom topography. To complement the theoretical analysis, we
utilize the numerical solver developed by the authors in \cite{PizoMunoz} to
approximate solutions of the model on a finite spatial interval, considering
various parameter values and forms of the variable coefficients in the
Boussinesq system under study. Additionally, we present preliminary numerical
experiments addressing an inverse problem: the reconstruction of the initial
wave elevation and fluid velocity from measurements taken at a final time. This
is achieved by formulating an optimization problem in which the initial
conditions are estimated as minimizers of a functional that quantifies the
discrepancy between the observed final state and the numerical solution evolved
from a trial initial state.

</details>


### [12] [USA Tariffs Effect: Machine Learning Insights into the Stock Market](https://arxiv.org/abs/2510.10877)
*Mridul Patel*

Main category: math.NA

TL;DR: This study analyzes how Trump's second-term tariffs, announced and implemented on 02-Apr-2025, affected the Australian S&P/ASX 200 index from Jan-Jul 2025 using EDA and machine learning regression models.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of Trump's second-term tariff policies on global markets, specifically Australia's stock market, and assess market responses to both announcement and implementation phases.

Method: Applied exploratory data analysis (EDA) techniques and machine learning-based regression models to evaluate stock performance, with comparative assessment of model predictive accuracy and robustness.

Result: The study captured significant market fluctuations and tariff-related responses in the Australian stock market, with machine learning models demonstrating varying levels of predictive accuracy.

Conclusion: Trump's second-term tariffs had measurable impacts on the Australian stock market, and machine learning regression models can effectively capture and predict tariff-related market responses.

Abstract: The imposition of tariffs by President Trump during his second term had
far-reaching consequences for global markets, including Australia. This study
investigates how both the announcement and subsequent implementation of these
tariffs, specifically on 02-Apr-2025, affected the Australian stock market,
focusing on the S\&P/ASX 200 index over the period from 21-Jan-2025 to
25-Jul-2025. To accurately capture the significance and behavior of market
fluctuations, the exploratory data analysis (EDA) techniques are applied.
Furthermore, the impact of tariffs on stock performance is evaluated using
machine learning-based regression models. A comparative assessment of these
models is conducted to determine their predictive accuracy and robustness in
capturing tariff-related market responses.

</details>


### [13] [Multilevel correction type of adaptive finite element method for Hartree-Fock equation](https://arxiv.org/abs/2510.10879)
*Fei Xu*

Main category: math.NA

TL;DR: Efficient algorithm combining multilevel correction with adaptive refinement to solve Hartree-Fock equation, avoiding large-scale nonlinear eigenvalue systems and dense matrix operations.


<details>
  <summary>Details</summary>
Motivation: To address the computational complexity and exponential scaling demands of traditional Hartree-Fock equation solvers while maintaining accuracy.

Method: Multilevel correction scheme with adaptive refinement, solving linearized boundary value problems and correcting solutions via small-scale Hartree-Fock equations in low-dimensional correction spaces.

Result: Significantly accelerated solution process with computational workload nearly independent of self-consistent field iterations, effectively mitigating exponential scaling demands.

Conclusion: The proposed algorithm efficiently solves Hartree-Fock equation with high accuracy while preserving low-dimensional characteristics and reducing computational complexity.

Abstract: This paper proposes an efficient algorithm for solving the Hartree--Fock
equation combining a multilevel correction scheme with an adaptive refinement
technique to improve computational efficiency. The algorithm integrates a
multilevel correction framework with an optimized implementation strategy.
Within this framework, a series of linearized boundary value problems are
solved, and their approximate solutions are corrected by solving small-scale
Hartree--Fock equations in low-dimensional correction spaces. The correction
space comprises a coarse space and the solution to the linearized boundary
value problem, enabling high accuracy while preserving low-dimensional
characteristics. The proposed algorithm efficiently addresses the inherent
computational complexity of the Hartree--Fock equation. Innovative correction
strategies eliminate the need for direct computation of large-scale nonlinear
eigenvalue systems and dense matrix operations. Furthermore, optimization
techniques based on precomputations within the correction space render the
total computational workload nearly independent of the number of
self-consistent field iterations. This approach significantly accelerates the
solution process of the Hartree--Fock equation, effectively mitigating the
traditional exponential scaling demands on computational resources while
maintaining precision.

</details>


### [14] [Multiscale Graph Reduction for Heterogeneous and Anisotropic Discrete Diffusion Processes](https://arxiv.org/abs/2510.10894)
*Maria Vasilyeva,James Brannick,Ben S. Southworth*

Main category: math.NA

TL;DR: Multiscale graph-based reduction algorithms for upscaling heterogeneous anisotropic diffusion problems using spectral clustering and energy-minimizing strategies.


<details>
  <summary>Details</summary>
Motivation: To develop efficient coarsening approaches for complex diffusion problems in heterogeneous and anisotropic media, addressing challenges in accurately capturing microscopic features while maintaining computational efficiency.

Method: Combines domain decomposition with spectral clustering using local generalized eigen-decompositions of signed graph Laplacian, followed by two energy-minimizing strategies: unconstrained minimization with local harmonic extensions and constrained energy minimization from non-local multi-continua upscaling.

Result: The algorithms demonstrate robustness across various test cases including perforated domains, channelized media, anisotropic settings, and pore network models, producing accurate coarse-scale models.

Conclusion: The proposed multiscale graph-based reduction approaches effectively handle complex diffusion problems and provide theoretically and numerically validated accurate coarse-scale representations.

Abstract: We present multiscale graph-based reduction algorithms for upscaling
heterogeneous and anisotropic diffusion problems. The proposed coarsening
approaches begin by constructing a partitioning of the computational domain
into a set of balanced local subdomains, resulting in a standard type of domain
decomposition. Given this initial decomposition, general coarsening techniques
based on spectral clustering are applied within each subgraph in order to
accurately identify the key microscopic features of a given system. The
spectral clustering algorithm is based on local generalized
eigen-decompositions applied to the signed graph Laplacian. The resulting
coarse-fine splittings are combined with two variants of energy-minimizing
strategies for constructing coarse bases for diffusion problems. The first is
an unconstrained minimization formulation in which local harmonic extensions
are applied column-wise to construct multi-vector preserving interpolation in
each region, whereas the second approach is a variant of the constrained energy
minimization formulations derived in the context of non-local multi-continua
upscaling techniques. We apply the resulting upscaling algorithms to a variety
of tests coming from the graph Laplacian, including diffusion in the perforated
domain, channelized media, highly anisotropic settings, and discrete pore
network models to demonstrate the potential and robustness of the proposed
coarsening approaches. We show numerically and theoretically that the proposed
approaches lead to accurate coarse-scale models.

</details>


### [15] [An efficient iteration method to reconstruct the drift term from the final measurement](https://arxiv.org/abs/2510.10940)
*Dakang Cen,Wenlong Zhang,Zhidong Zhang*

Main category: math.NA

TL;DR: The paper addresses the inverse drift problem in 1D parabolic equations using final time data, proving uniqueness via fixed-point iteration and developing a numerical algorithm with mollification to handle ill-posedness.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse drift problem in parabolic equations with final time data, which is inherently ill-posed and requires specialized mathematical approaches for uniqueness and numerical computation.

Method: Construct an operator whose fixed points correspond to the unknown drift, prove uniqueness through iterative convergence, and develop a numerical algorithm with mollification to regularize the ill-posed problem.

Result: The authors establish uniqueness of the drift solution and provide numerical results demonstrating the effectiveness of their iterative algorithm with mollification for handling the inverse problem's ill-posedness.

Conclusion: The proposed fixed-point approach successfully addresses the inverse drift problem, providing both theoretical uniqueness guarantees and practical numerical solutions through regularization techniques.

Abstract: This work investigates the inverse drift problem in the one-dimensional
parabolic equation with the final time data. The authors construct an operator
first, whose fixed points are the unknown drift, and then apply it to prove the
uniqueness. The proof of uniqueness contains an iteration converging to the
drift, which inspires the numerical algorithm. To handle the ill-posedness of
the inverse problem, the authors add the mollification on the data first in the
iterative algorithm, and then provide some numerical results.

</details>


### [16] [A Constrained Multi-Fidelity Bayesian Optimization Method](https://arxiv.org/abs/2510.10984)
*Jingyi Wang,Nai-Yuan Chiang,Tucker Hartland,J. Luc Peterson,Jerome Solberg,Cosmin G. Petra*

Main category: math.NA

TL;DR: Proposes a constrained multi-fidelity Bayesian optimization (CMFBO) method with novel acquisition functions that are analytical, easy to implement, and don't require feasible initial samples.


<details>
  <summary>Details</summary>
Motivation: Address challenges in constrained optimization using multi-fidelity Bayesian optimization framework, particularly in efficiently identifying feasible regions defined by constraints.

Method: Design novel acquisition functions with analytically closed-form expressions that are straightforward to implement and don't require feasible initial samples.

Result: Demonstrated effectiveness on synthetic test problems and applied to data-driven inertial confinement fusion design and high-current joint design using finite element simulations.

Conclusion: The proposed CMFBO method with new acquisition functions successfully addresses constrained optimization challenges in multi-fidelity Bayesian optimization applications.

Abstract: Recently, multi-fidelity Bayesian optimization (MFBO) has been successfully
applied to many engineering design optimization problems, where the cost of
high-fidelity simulations and experiments can be prohibitive. However,
challenges remain for constrained optimization problems using the MFBO
framework, particularly in efficiently identifying the feasible region defined
by the constraints. In this paper, we propose a constrained multi-fidelity
Bayesian optimization (CMFBO) method with novel acquisition functions.
Specifically, we design efficient acquisition functions that 1) have
analytically closed-form expressions; 2) are straightforward to implement; and
3) do not require feasible initial samples, an important feature often missing
in commonly used acquisition functions such as expected constrained improvement
(ECI). We demonstrate the effectiveness of our algorithms on synthetic test
problems using different combinations of acquisition functions. Then, we apply
the proposed method to a data-driven inertial confinement fusion (ICF) design
problem, and a high-current joint design problem using finite element
simulations with computational contact mechanics.

</details>


### [17] [Parareal in time and spectral in space fast L1 quasilinear subdiffusion solver](https://arxiv.org/abs/2510.11023)
*Josefa Caballero,Łukasz Płociniczak,Kishin Sadarangani*

Main category: math.NA

TL;DR: A fully discrete solver combining parareal algorithm with L1 finite-difference for Caputo derivative and spectral Galerkin discretization for quasilinear time-fractional diffusion equations, with rigorous convergence proof and demonstrated computational speedup.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient parallel solver for quasilinear time-fractional diffusion equations that combines temporal parallelization with high-order spatial discretization, addressing the computational challenges of fractional PDEs.

Method: Combines parareal algorithm in time with L1 finite-difference approximation of Caputo derivative and spectral Galerkin discretization in space. Uses energy norms and spectral basis orthogonality for convergence analysis.

Result: Parareal iterations converge exactly to fully serial L1-spectral solution in finite steps, with rates independent of fractional exponent. Spectral discretization provides exponential spatial accuracy, while parareal achieves clock speedup proportional to processors. Numerical experiments show up to order-of-magnitude computational time reduction.

Conclusion: The proposed parareal-L1-spectral method is highly efficient for solving quasilinear time-fractional diffusion equations, offering rigorous convergence guarantees, exponential spatial accuracy, and significant computational speedup through parallelization.

Abstract: We consider the initial-boundary value problem for a quasilinear
time-fractional diffusion equation, and develop a fully discrete solver
combining the parareal algorithm in time with a L1 finite-difference
approximation of the Caputo derivative and a spectral Galerkin discretization
in space. Our main contribution is the first rigorous convergence proof for the
parareal-L1 scheme in this nonlinear subdiffusive setting. By constructing
suitable energy norms and exploiting the orthogonality of the spectral basis,
we establish that the parareal iterations converge exactly to the fully serial
L1-spectral solution in a finite number of steps, with rates independent of the
fractional exponent. The spectral spatial discretization yields exponential
accuracy in space, while the parareal structure induces a clock speedup
proportional to the number of processors, making the overall method highly
efficient. Numerical experiments for both subdiffusive and classical diffusion
problems confirm our theoretical estimates and demonstrate up to an order of
magnitude reduction in computational time compared to the conventional
sequential solver. We observe that the speedup of the parareal method increases
linearly with the fine integrator degrees of freedom.

</details>


### [18] [On Runge-Kutta convolution quadrature based fractional variational integrators](https://arxiv.org/abs/2510.11082)
*Khaled Hariz,Sina Ober-Blöbaum,Fernando Jimenez*

Main category: math.NA

TL;DR: This paper develops fractional variational integrators (FVIs) using Runge-Kutta convolution quadrature (RKCQ) combined with higher-order Galerkin methods, achieving 2nd, 4th, and 6th order accuracy.


<details>
  <summary>Details</summary>
Motivation: To incorporate Lagrangian systems with fractional damping into variational formalism and develop efficient numerical schemes for such systems.

Method: Uses Runge-Kutta convolution quadrature (RKCQ) method for approximating fractional derivatives, combined with higher-order Galerkin methods, with special focus on CQ based on Lobatto IIIC.

Result: Developed FVIs with 2nd, 4th, and 6th order accuracy. Proved energy decay preservation and convergence properties for 2nd order schemes. Included discussion on midpoint fractional integrator.

Conclusion: The presented schemes successfully achieve high-order accuracy while preserving important physical properties like energy decay, making them suitable for simulating Lagrangian systems with fractional damping.

Abstract: Lagrangian systems subject to fractional damping can be incorporated into a
variational formalism. The construction can be made by doubling the state
variables and introducing fractional derivatives \cite{JiOb2}. The main
objective of this paper is to use the Runge-Kutta convolution quadrature (RKCQ)
method for approximating fractional derivatives, combined with higher order
Galerkin methods in order to derive fractional variational integrators (FVIs).
We are specially interested in the CQ based on Lobatto IIIC. Preservation
properties such as energy decay as well as convergence properties are
investigated numerically and proved for 2nd order schemes. The presented
schemes reach 2nd, 4th and 6th accuracy order. A brief discussion on the
midpoint fractional integrator is also included.

</details>


### [19] [A GPU-Accelerated Matrix-Free FAS Multigrid Solver for Navier-Stokes Equations with Memory-Efficient Implementations](https://arxiv.org/abs/2510.11152)
*Jiale Meng,Shuqi Tang,Steven M. Wise,Zhenlin Guo*

Main category: math.NA

TL;DR: A GPU-accelerated matrix-free multigrid solver using staggered finite differences with optimized memory usage and X-shape Multi-Color Gauss-Seidel smoother, achieving significant speedups (61× in 2D, 46× in 3D) and enabling large-scale simulations on single GPUs.


<details>
  <summary>Details</summary>
Motivation: To develop efficient GPU-based multigrid solvers for large-scale computational problems by optimizing memory usage and eliminating conditional branching, enabling complex simulations like Navier-Stokes and phase-field models on single GPUs.

Method: Matrix-free Full Approximation Storage (FAS) multigrid with staggered finite differences, GPU acceleration in MATLAB, X-shape Multi-Color Gauss-Seidel smoother that partitions grid into four submatrices, memory-efficient projection schemes reducing variables from 12/15 to 8.

Result: Achieved 61× speedup for 2D heat equation on 8192² grid and 46× for 3D at 512³; reduced memory footprint by 20-30%; enabled 512³ Navier-Stokes simulations on single GPU; reproduced grain growth scaling laws and simulated air-water bubble coalescence matching experiments.

Conclusion: The GPU-accelerated multigrid solver with optimized memory and X-MCGS smoother provides substantial performance improvements, making large-scale computational fluid dynamics and phase-field simulations feasible on consumer-grade GPUs.

Abstract: We develop a matrix-free Full Approximation Storage (FAS) multigrid solver
based on staggered finite differences and implemented on GPU in MATLAB. To
enhance performance, intermediate variables are reused, and an X-shape
Multi-Color Gauss-Seidel (X-MCGS) smoother is introduced, which eliminates
conditional branching by partitioning the grid into four submatrices.
Restriction and prolongation operators are also GPU-accelerated. Convergence
tests verify robustness and accuracy, while benchmarks show substantial
speedups: for the 2D heat equation on an $8192^2$ grid, the RTX~4090 achieves
$61\times$ over a single-core CPU, and in 3D at $512^3$, $46\times$. A
memory-efficient implementation of first- and second-order projection schemes
reduces GPU-resident variables from 12/15 to 8, lowering memory footprint and
improving performance by 20--30%, enabling $512^3$ Navier-Stokes simulations on
a single GPU. Grain growth on a $512^2$ grid accommodates up to $q=1189$ (2D)
and $q=123$ (3D) orientations, reproducing expected scaling laws. Coupled with
Cahn-Hilliard equations, air-water two-bubble coalescence is simulated on a
$256\times 256\times 1024$ grid, agreeing with experimental observations.

</details>


### [20] [Randomized flexible Krylov methods for $\ell_p$ regularization](https://arxiv.org/abs/2510.11237)
*Malena Sabaté Landman,Yuji Nakatsukasa*

Main category: math.NA

TL;DR: Proposes randomized flexible Krylov methods and sketch-to-precondition techniques to reduce computational complexity in solving large-scale sparse linear ill-posed problems using iteratively reweighted schemes.


<details>
  <summary>Details</summary>
Motivation: Existing flexible Krylov-Tikhonov methods for sparse solutions of large-scale linear ill-posed problems increase computational complexity at each iteration, and memory requirements can be prohibitive for inner-outer schemes.

Method: Uses randomized flexible Krylov methods combining flexible Krylov subspaces with sketch-and-solve efficiency, and sketch-to-precondition techniques to accelerate convergence in memory-constrained scenarios.

Result: The algorithms demonstrate improved performance through various numerical examples, showing reduced computational complexity while maintaining effectiveness.

Conclusion: The proposed randomized methods effectively alleviate computational burden in solving large-scale sparse ill-posed problems, offering practical alternatives to traditional approaches.

Abstract: The computation of sparse solutions of large-scale linear discrete ill-posed
problems remains a computationally demanding task. A powerful framework in this
context is the use of iteratively reweighted schemes, which are based on
constructing a sequence of quadratic tangent majorants of the $\ell_2$-$\ell_1$
regularization functional (with additional smoothing to ensure
differentiability at the origin), and solving them successively. Recently,
flexible Krylov-Tikhonov methods have been used to partially solve each problem
in the sequence efficiently. However, in order to guarantee convergence, the
complexity of the algorithm at each iteration increases with respect to more
traditional methods. We propose a randomized flexible Krylov method to
alleviate the increase of complexity, which leverages the adaptability of the
flexible Krylov subspaces with the efficiency of `sketch-and-solve' methods. A
possible caveat of the mentioned methods is their memory requirements. In this
case, one needs to rely instead on inner-outer schemes. In these scenarios, we
propose a `sketch-to-precondition' method to speed up the convergence of each
of the subproblems in the sequence. The performance of these algorithms is
shown through a variety of numerical examples.

</details>


### [21] [Forward and backward error bounds for a mixed precision preconditioned conjugate gradient algorithm](https://arxiv.org/abs/2510.11379)
*Thomas Bake,Erin Carson,Yuxin Ma*

Main category: math.NA

TL;DR: A new framework for preconditioned conjugate gradient (PCG) algorithm that provides rigorous error bounds without assuming the recursively updated residual norm goes below machine precision, and shows preconditioners can be applied in low precision without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: Current analyses of conjugate gradient algorithms in finite precision typically assume the recursively updated residual norm goes orders of magnitude below machine precision, which is a limiting assumption that this work aims to overcome.

Method: Developed a new framework for PCG algorithm with rigorous proofs for error bounds, analyzed relative backward and forward errors without relying on assumptions about residual norm behavior, and investigated preconditioner application in low precision.

Result: Proved that relative backward and forward errors of PCG can reach O(u) and O(u)κ(A)^{1/2} respectively after sufficient iterations, and demonstrated that applying preconditioners in low precision doesn't compromise final accuracy under reasonable conditions.

Conclusion: The proposed PCG framework provides more realistic error bounds without restrictive assumptions and enables efficient use of low-precision preconditioners while maintaining accuracy, as validated by numerical experiments.

Abstract: The preconditioned conjugate gradient (PCG) algorithm is one of the most
popular algorithms for solving large-scale linear systems Ax = b, where A is a
symmetric positive definite matrix. Rather than computing residuals directly,
it updates the residual vectors recursively. Current analyses of the conjugate
gradient (CG) algorithm in finite precision typically assume that the norm of
the recursively updated residual goes orders of magnitude below the machine
precision, focusing mainly on bounding the residual gap thereafter. This work
introduces a framework for the PCG algorithm and provides rigorous proofs that
the relative backward and forward errors of the computed results of PCG can
reach the levels O(u) and O(u)\kappa(A)^{1/2}, respectively, after a sufficient
number of iterations without relying on an assumption concerning the norm of
the recursively updated residual, where u represents the unit roundoff and
\kappa(A) is the condition number of A. Our PCG framework further shows that
applying preconditioners in low precision does not compromise the accuracy of
the final results, provided that reasonable conditions are satisfied. Our
theoretical results are illustrated through a set of numerical experiments.

</details>


### [22] [DE-Sinc approximation for unilateral rapidly decreasing functions and its computational error bound](https://arxiv.org/abs/2510.11411)
*Tomoaki Okayama*

Main category: math.NA

TL;DR: A new double-exponential transformation is proposed for Sinc approximation of unilateral rapidly decreasing functions, achieving almost exponential convergence instead of root-exponential convergence.


<details>
  <summary>Details</summary>
Motivation: Existing variable transformations for Sinc approximation of unilateral rapidly decreasing functions only achieve root-exponential convergence, which is not significantly improved by recent methods. The goal is to drastically improve the convergence rate.

Method: Proposes a new variable transformation t = 2sinh(log(log(1+exp(πsinh x)))), categorized as double-exponential (DE) transformation, and provides computational error bounds for this transformation.

Result: The proposed transformation enables the Sinc approximation to achieve almost exponential convergence, which is a significant improvement over the previous root-exponential convergence rates.

Conclusion: The new double-exponential transformation provides a much faster convergence rate for Sinc approximation of unilateral rapidly decreasing functions, with theoretical error bounds and numerical experiments confirming the improvement.

Abstract: The Sinc approximation is known to be a highly efficient approximation
formula for rapidly decreasing functions. For unilateral rapidly decreasing
functions, which rapidly decrease as $x\to\infty$ but does not as
$x\to-\infty$, an appropriate variable transformation makes the functions
rapidly decreasing. As such a variable transformation, Stenger proposed $t =
\sinh(\log(\operatorname{arsinh}(\exp x)))$, which enables the Sinc
approximation to achieve root-exponential convergence. Recently, another
variable transformation $t = 2\sinh(\log(\log(1+\exp x)))$ was proposed, which
improved the convergence rate. Furthermore, its computational error bound was
provided. However, the improvement is not significant because the convergence
rate is still root-exponential order. To improve the convergence rate
drastically, this study proposes a new transformation $t =
2\sinh(\log(\log(1+\exp(\pi\sinh x))))$, which is categorized as the
double-exponential (DE) transformation. Furthermore, this study provides its
computational error bound, which shows that the proposed approximation formula
may achieve almost exponential convergence. Numerical experiments that confirm
the theoretical result are also provided.

</details>


### [23] [Convergence Analysis of Galerkin Approximations for the Lindblad Master Equation](https://arxiv.org/abs/2510.11416)
*Rémi Robin,Pierre Rouchon*

Main category: math.NA

TL;DR: Numerical approximation of Lindblad master equation on infinite-dimensional Hilbert spaces using Galerkin discretization with convergence analysis.


<details>
  <summary>Details</summary>
Motivation: To develop effective numerical methods for solving the Lindblad master equation in infinite-dimensional quantum systems, particularly relevant for quantum error correction applications.

Method: Classical Galerkin approach for spatial discretization with a priori estimates to analyze convergence.

Result: Derived explicit convergence rates and demonstrated method effectiveness through autonomous quantum error correction examples.

Conclusion: The Galerkin discretization method provides reliable numerical approximation with proven convergence for Lindblad master equations in infinite-dimensional settings.

Abstract: This paper analyzes the numerical approximation of the Lindblad master
equation on infinite-dimensional Hilbert spaces. We employ a classical Galerkin
approach for spatial discretization and investigate the convergence of the
discretized solution to the exact solution. Using \textit{a priori} estimates,
we derive explicit convergence rates and demonstrate the effectiveness of our
method through examples motivated by autonomous quantum error correction.

</details>


### [24] [An adaptive time-stepping strategy for the modified phase field crystal model with a strong nonlinear vacancy potential](https://arxiv.org/abs/2510.11475)
*Wanrong Hao,Yunqing Huang*

Main category: math.NA

TL;DR: Developed three linear energy-stable schemes for a modified phase field crystal model with strong nonlinear vacancy potential, introducing an adaptive time-stepping strategy to handle numerical instabilities in strongly nonlinear systems.


<details>
  <summary>Details</summary>
Motivation: To enable realistic crystal growth simulation using a sixth-order phase-field model while addressing computational complexity and numerical stability issues in strongly nonlinear systems.

Method: Optimized Crank-Nicolson scheme using stabilized-SAV method, generalized positive auxiliary variable (GPAV), and modified exponential scalar auxiliary variable (ESAV) methods; developed Energy-Variation Moving Average (EV-MA) adaptive time-stepping strategy with moving average of energy variation and maximum change factor constraints.

Result: Proposed schemes demonstrate accuracy and energy stability in extensive numerical experiments; EV-MA strategy performs robustly across wide parameter ranges, effectively dampening oscillations and enhancing time step selection robustness.

Conclusion: The developed linear energy-stable schemes and EV-MA adaptive time-stepping strategy successfully address numerical challenges in strongly nonlinear phase field crystal models, enabling more robust and efficient crystal growth simulations.

Abstract: This paper develops three linear and energy-stable schemes for a modified
phase field crystal model with a strong nonlinear vacancy potential (VMPFC
model). This sixth-order phase-field model enables realistic crystal growth
simulation. Starting from a Crank-Nicolson scheme based on the stabilized-SAV
(S-SAV) method, we optimize it via the generalized positive auxiliary variable
(GPAV) and modified exponential scalar auxiliary variable (ESAV) methods,
thereby reducing computational complexity or eliminating the requirement for
the nonlinear free energy potential to be bounded from below. The newly
developed Energy-Variation Moving Average (EV-MA) adaptive time-stepping
strategy resolves numerical instabilities and mitigates the high parameter
sensitivity of the conventional adaptive time algorithm during rapid energy
decay in the strongly nonlinear system. Unlike conventional instantaneous
energy-derivative monitors, the EV-MA technique incorporates a moving average
of the energy variation. Additionally, the rate of change between adjacent time
steps is constrained by a maximum change factor. This design effectively
dampens spurious oscillations and enhances the robustness of time step
selection. Extensive numerical experiments are conducted to validate the
accuracy and energy stability of the proposed schemes. The EV-MA strategy is
also demonstrated to perform robustly across a wide range of parameters.

</details>


### [25] [Numerical Methods for Kernel Slicing](https://arxiv.org/abs/2510.11478)
*Nicolaj Rux,Johannes Hertrich,Sebastian Neumayer*

Main category: math.NA

TL;DR: The paper presents algorithms for recovering Fourier coefficients of kernels to enable efficient linear-complexity kernel sum computations using Fourier-slicing methods.


<details>
  <summary>Details</summary>
Motivation: Brute-force kernel sum computation scales quadratically with sample size, which is inefficient for large datasets. Fourier-slicing methods offer linear complexity but require known Fourier coefficients of the kernel.

Method: The authors treat the slicing relation as an inverse problem and develop two algorithms for recovering the Fourier coefficients needed for Fourier-slicing methods.

Result: Extensive numerical experiments show that the proposed methods achieve both speed and accuracy in computing kernel sums.

Conclusion: The presented algorithms successfully recover Fourier coefficients, enabling efficient linear-complexity kernel sum computations through Fourier-slicing approaches.

Abstract: Kernels are key in machine learning for modeling interactions. Unfortunately,
brute-force computation of the related kernel sums scales quadratically with
the number of samples. Recent Fourier-slicing methods lead to an improved
linear complexity, provided that the kernel can be sliced and its Fourier
coefficients are known. To obtain these coefficients, we view the slicing
relation as an inverse problem and present two algorithms for their recovery.
Extensive numerical experiments demonstrate the speed and accuracy of our
methods.

</details>


### [26] [Structure-preserving finite element approximations of a hybrid relativistic cold fluid-particle model](https://arxiv.org/abs/2510.11500)
*Tileuzhan Mukhamet,Katharina Kormann*

Main category: math.NA

TL;DR: Mixed finite element discretizations for cold relativistic fluid model preserving mass, energy and divergence constraints, with implicit/explicit time schemes and particle coupling.


<details>
  <summary>Details</summary>
Motivation: To develop numerical schemes that preserve fundamental conservation properties (mass, energy, divergence constraints) in relativistic fluid models, particularly for plasma wake field simulations.

Method: Derived mixed finite element discretizations from Poisson bracket approximations; used implicit energy-conserving average-vector field method and explicit strong-stability preserving Runge-Kutta scheme; coupled fluid model with relativistic particles.

Result: Numerical study shows convergence and conservation properties; successfully applied to plasma wake field simulation.

Conclusion: The proposed methods effectively preserve conservation properties while maintaining accuracy for relativistic fluid simulations, particularly in plasma wake field applications.

Abstract: We derive mixed finite element discretizations of a cold relativistics fluid
model from approximations of the Poisson bracket that preserve mass, energy and
the divergence constraints. For time-discretization we derive an implicit
energy-conserving average-vector field method or apply an explicit
strong-stability preserving Runge-Kutta scheme. We also consider a coupling of
the fluid model to relativistic particles. We perform a numerical study of the
scheme which shows convergence and conservation properties of the proposed
methods and apply the new scheme to a plasma wake field simulation.

</details>


### [27] [A fourth-order active flux method for parabolic problems with application to porous medium equation](https://arxiv.org/abs/2510.11527)
*Junming Duan*

Main category: math.NA

TL;DR: A fourth-order active flux method for parabolic problems using explicit SSP-RK time integration, avoiding pseudo-time iterations while maintaining conservation and achieving higher CFL stability than LDG methods.


<details>
  <summary>Details</summary>
Motivation: To develop a high-order finite volume method for parabolic problems that avoids the computational overhead of pseudo-time iterations in existing hyperbolic formulations while maintaining conservation properties.

Method: Uses a degenerate first-order system with auxiliary variables for derivatives, similar to LDG methods. Cell averages are updated via standard finite volume method, while point values use fourth-order central finite difference operators. Third-order SSP-RK time integration is employed.

Result: Achieves fourth-order accuracy in 1D with maximum CFL number of 0.27 for stability, larger than LDG methods. Successfully applied to porous medium equation with positivity-preserving limiters to ensure non-negative solutions.

Conclusion: The proposed active flux method provides an efficient fourth-order approach for parabolic problems with improved stability over LDG methods, validated through numerical experiments.

Abstract: The active flux (AF) method is a compact high-order finite volume method
originally proposed for solving hyperbolic conservation laws, in which cell
averages and point values at cell interfaces are evolved simultaneously. This
paper develops a fourth-order AF method for one- and two-dimensional parabolic
problems, employing the explicit strong-stability-preserving Runge-Kutta
(SSP-RK) method for time integration. The proposed method is built on a
degenerate first-order system with auxiliary variables representing the
derivatives of the primal variable, similar to local discontinuous Galerkin
(LDG) methods, which avoids introducing pseudo-time or performing iterations
within a physical time step in the existing hyperbolic formulations. The
evolution of cell averages follows the standard finite volume method, ensuring
conservation, while the point values of both the primal and auxiliary variables
are updated using fourth-order central finite difference operators. A discrete
Fourier analysis confirms the fourth-order accuracy in 1D. With the third-order
SSP-RK method, the maximum CFL number for stability is $0.27$ in 1D, as
obtained by von Neumann analysis, larger than that of LDG methods. The proposed
method is further applied to the porous medium equation, and
positivity-preserving limitings are incorporated to guarantee the
non-negativity of the numerical solutions. Several numerical experiments
validate the theoretical results and efficacy of the method.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [28] [Analyticity for Double Wall Carbon Nanotubes Modeled as Timoshenko Beams with Kelvin-Voigt and Intermediate Damping](https://arxiv.org/abs/2510.09865)
*Fredy Maglorio Sobrado Suárez,Gilson Tumelero,Jackson Luchesi,Marieli Musial Tumelero,Santos Richard Wieller Sanguino Bejarano*

Main category: math.AP

TL;DR: Mathematical modeling of double-walled carbon nanotubes using coupled Timoshenko beams with Van der Waals forces and various damping mechanisms, proving exponential stability and analyticity.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive mathematical model for double-walled carbon nanotubes that accounts for mechanical interactions and damping effects, enabling stability analysis.

Method: Used two coupled Timoshenko beams connected by Van der Waals forces, incorporating Kelvin-Voigt type dampings and fractional dampings in both beams.

Result: The proposed model is well-established, and the associated semigroup is exponentially stable and analytical for all (α, β) in [0,1]².

Conclusion: The model successfully describes double-walled carbon nanotubes with proven stability properties, and provides analytical results for Timoshenko systems.

Abstract: This manuscript studies a model of double-walled carbon nanotubes using two
Timoshenko beams which are coupled by the Van der Walls force $(y-u)$.
Kelvin-Voigt type dampings $(u_x-v)_{xt}$ and $(y_x-z)_{xt}$ and fractional
dampings $(-\partial_{xx})^\alpha v_t$ and $(-\partial_{xx})^\beta z_t$ in both
beams have been considered. We show that our proposed model is well established
and that the semigroup associated is exponentially stable and analytical for
any $(\alpha, \beta) \in [0, 1]^2$. As a consequence of this, a result on the
analyticity of a Timoshenko System is obtained.

</details>


### [29] [Vectorial Bernoulli Problems and Free Boundary Systems](https://arxiv.org/abs/2510.09875)
*Giorgio Tortone,Bozhidar Velichkov*

Main category: math.AP

TL;DR: Survey of recent results on regularity of vectorial free boundary problems of Bernoulli type and free boundary systems, covering methodologies and open questions.


<details>
  <summary>Details</summary>
Motivation: To provide an overview of recent developments in vectorial free boundary problems, illustrating general methodologies and highlighting important open questions in the field.

Method: Survey approach reviewing and synthesizing recent research results, analyzing general methodologies used in the field.

Result: Comprehensive overview of current state of knowledge in vectorial free boundary problems, identification of key methodologies and notable open questions.

Conclusion: The survey successfully illustrates the current methodologies and identifies important open research questions in vectorial free boundary problems, providing guidance for future research directions.

Abstract: In this survey we go through some of the recent results about the regularity
of vectorial free boundary problems of Bernoulli type and free boundary
systems. The aim is to illustrate the general methodologies as well as to
outline a selection of notable open questions.

</details>


### [30] [A no-contact result for a plate-fluid interaction system in dimension three](https://arxiv.org/abs/2510.09992)
*Mario Bukal,Igor Kukavica,Linfeng Li,Boris Muha*

Main category: math.AP

TL;DR: The paper proves that in 3D fluid-structure interaction between viscous fluid and elastic plate, the plate maintains uniform separation from rigid boundary without contact, even without damping.


<details>
  <summary>Details</summary>
Motivation: To understand the non-contact behavior in fluid-structure interaction systems where an elastic plate forms the moving upper boundary of a viscous fluid, particularly without requiring damping mechanisms.

Method: Analyzed weak solutions of incompressible Navier-Stokes equations coupled with elastic plate motion via velocity- and stress-matching conditions, under natural energy bounds and additional regularity assumptions.

Result: Proved uniform separation property showing the plate does not contact the rigid boundary, with this result holding even in the absence of damping in the plate equation.

Conclusion: The elastic plate maintains a uniform distance from the rigid boundary in 3D fluid-structure interaction systems, demonstrating inherent non-contact behavior without requiring damping.

Abstract: We address the fluid-structure interaction between a viscous incompressible
fluid and an elastic plate forming its moving upper boundary in three
dimensions. The fluid is described by the incompressible Navier-Stokes
equations with a free upper boundary that evolves according to the motion of
the structure, coupled via the velocity- and stress-matching conditions. Under
the natural energy bounds and additional regularity assumptions on the weak
solutions, we prove a non-contact property with a uniform separation of the
plate from the rigid boundary. The result does not require damping in the plate
equation.

</details>


### [31] [The eigentheory for nonlocal cooperative-advective system and its role in the study of free boundary system for directional epidemic models](https://arxiv.org/abs/2510.10024)
*Soufiane Bentout,Hoang-Hung Vo*

Main category: math.AP

TL;DR: A nonlocal cooperative reaction-diffusion system with free boundaries and drift terms is analyzed, establishing well-posedness, spectral properties, and a sharp vanishing-spreading dichotomy based on the basic reproduction number R₀ and initial conditions.


<details>
  <summary>Details</summary>
Motivation: The model is motivated by directional epidemic spread, addressing analytical challenges in nonlocal systems without variational structure that require sharper solution regularity.

Method: Established well-posedness and global existence of classical solutions, analyzed nonlocal eigenvalue problems using Fredholm theory, Crandall-Rabinowitz bifurcation theorem, and Hadamard-type derivative formulas to study principal eigenvalue properties.

Result: Proved a sharp vanishing-spreading dichotomy: when R₀≤1, all solutions vanish; when R₀>1, outcome depends on initial domain size h₀ and expansion rate μ, with critical habitat length L* and threshold μ̂ separating vanishing from spreading regimes.

Conclusion: Results provide rigorous framework for threshold dynamics in cooperative-advective nonlocal systems, offering mathematical insights for epidemic modeling, ecological invasion, and population dynamics.

Abstract: In this paper, we propose and analyze a nonlocal cooperative
reaction--diffusion system with free boundaries and drift terms, motivated by
directional epidemic spread. Lacking a variational structure but requiring
sharper regularity of solutions, the model poses substantial analytical
challenges compared with previous
works~\cite{Du,Berestycki2016a,Berestycki2016b,Cao2019,NguyenVo2022,Tang2024a,Tang2024b}.
We first establish the well-posedness of the local problem and the global
existence and uniqueness of classical solutions in $C^1$ space.
  We then study the associated nonlocal eigenvalue problem, proving the
existence, simplicity, qualitative properties, and asymptotic behavior of the
principal eigenvalue. The analysis employs Fredholm theory, the
Crandall--Rabinowitz bifurcation theorem, and Hadamard-type derivative formulas
to describe its parameter dependence and connection with the basic reproduction
number~$R_0$.
  Building on this spectral characterization, we show that the system admits a
\emph{sharp vanishing--spreading dichotomy} in its long-term dynamics. When
$R_0\le1$, all solutions vanish; for $R_0>1$, the outcome depends on the
initial domain size~$h_0$ and the free-boundary expansion rate~$\mu$. There
exists a critical habitat length~$\mathcal L^\ast$ such that if $h_0<\mathcal
L^\ast$, a threshold $\widehat{\mu}>0$ separates vanishing
($\mu\in(0,\widehat{\mu}]$) from spreading ($\mu>\widehat{\mu}$). In the
spreading regime, solutions converge to the unique positive steady state, while
in the vanishing regime they decay uniformly to zero. These results provide a
rigorous framework for the threshold dynamics of cooperative--advective
nonlocal systems and offer mathematical insight for further studies in epidemic
modeling, ecological invasion, and population dynamics.

</details>


### [32] [On the Profile of Singularity Formation for the Incompressible Hydrostatic Boussinesq system](https://arxiv.org/abs/2510.10090)
*Slim Ibrahim,Quyuan Lin,Lingjun Qian,Edriss S. Titi*

Main category: math.AP

TL;DR: The paper studies how non-constant temperature affects singularity formation in the primitive equations (hydrostatic Euler equations), finding that temperature variations don't impact singularity formation or stability in the velocity field.


<details>
  <summary>Details</summary>
Motivation: Previous research showed smooth solutions to inviscid primitive equations with constant temperature develop stable singularities, but the effect of non-constant temperature on singularity formation was unknown.

Method: Analyzed singularity formation stability for non-constant temperature in two scenarios: without temperature diffusion and with vertical diffusivity added to temperature dynamics.

Result: For both scenarios, temperature variation does not affect singularity formation or its stability in the velocity field.

Conclusion: The variation of temperature has no impact on the formation or stability of singularities in the primitive equations, regardless of whether temperature diffusion is present or not.

Abstract: The primitive equations (PEs) model planetary large-scale oceanic and
atmospheric dynamics. While it has been shown that there are smooth solutions
to the inviscid PEs (also called the hydrostatic Euler equations) with constant
temperature (isothermal) that develop stable singularities in finite time, the
effect of non-constant temperature on the singularity formation has not been
established yet. This paper studies the stability of singularity formation for
non-constant temperature in two scenarios: when there is no diffusion in the
temperature, or when a vertical diffusivity is added to the temperature
dynamics. For both scenarios, our results indicate that the variation of
temperature affects neither the formation of singularity, nor its stability, in
the velocity field, respectively.

</details>


### [33] [Weak solutions and weak-strong uniqueness for a compressible power-law-Oldroyd--B fluid model](https://arxiv.org/abs/2510.10096)
*Yong Lu,Milan Pokorny*

Main category: math.AP

TL;DR: Existence and uniqueness of weak solutions for a viscoelastic compressible flow model in R³ with shear thickening properties, requiring power law growth ≥ 5/2.


<details>
  <summary>Details</summary>
Motivation: To establish mathematical foundations for viscoelastic compressible flows with shear thickening behavior, particularly addressing the challenging case where stress tensor follows power law model while velocity divergence remains bounded.

Method: Analysis of a viscoelastic compressible flow model using power law stress tensor, proving existence of weak solutions through mathematical analysis techniques, and establishing uniqueness under additional integrability conditions.

Result: Proved existence of weak solutions when power law growth is ≥ 5/2, and showed uniqueness of sufficiently smooth solutions in the weak solution class with extra integrability conditions on initial stress tensor values.

Conclusion: The model admits weak solutions under specified growth conditions, and these solutions are unique when additional regularity and integrability assumptions are satisfied, providing rigorous mathematical validation for the viscoelastic compressible flow framework.

Abstract: We consider a model of a viscoelastic compressible flow in $R^{3}$ which is
additionally shear thickening (the stress tensor corresponds to the power law
model, however, the divergence of the velocity is due to the model bounded). We
prove existence of a weak solution to this model provided the growth in the
power law model is larger or equal than $\frac 52$. We also prove that any
sufficiently smooth solution of this model is unique in the class of weak
solution, provided extra integrability of the initial value for the extra
stress tensor is assumed.

</details>


### [34] [Multiple sign-changing solutions for semilinear subelliptic Dirichlet problem](https://arxiv.org/abs/2510.10120)
*Hua Chen,Hong-Ge Chen,Jin-Ning Li,Xin Liao*

Main category: math.AP

TL;DR: The paper establishes multiplicity results for sign-changing solutions of semilinear subelliptic equations using perturbation methods and invariant set techniques, with novel lower bounds for min-max values that reveal essential differences from classical elliptic equations.


<details>
  <summary>Details</summary>
Motivation: To study perturbation from symmetry problems for semilinear subelliptic equations associated with Hörmander vector fields, and understand how these differ from classical elliptic equations.

Method: Uses perturbation method with refined techniques for invariant sets, constructs two distinct lower bounds for min-max values: one from Dirichlet eigenvalues and another from Morse-type estimates and Cwikel-Lieb-Rozenblum type inequality in degenerate cases.

Result: Establishes multiplicity results for sign-changing solutions, with two different sufficient conditions that are not mutually inclusive, particularly in non-equiregular cases.

Conclusion: Sub-elliptic equations exhibit essential differences from classical elliptic framework, as demonstrated by the novel lower bounds and their distinct applicability conditions.

Abstract: We study the following perturbation from symmetry problem for the semilinear
subelliptic equation \[ \left\{
  \begin{array}{cc}
  -\triangle_{X} u=f(x,u)+g(x,u) & \mbox{in}~\Omega, \\[2mm]
  u\in H_{X,0}^{1}(\Omega),\hfill
  \end{array}
  \right. \] where $\triangle_{X}=-\sum_{i=1}^{m}X_{i}^{*}X_{i}$ is the
self-adjoint sub-elliptic operator associated with H\"{o}rmander vector fields
$X=(X_{1},X_{2},\ldots,X_{m})$, $\Omega$ is an open bounded subset in
$\mathbb{R}^n$, and $H_{X,0}^{1}(\Omega)$ denotes the weighted Sobolev space.
We establish multiplicity results for sign-changing solutions using a
perturbation method alongside refined techniques for invariant sets. The
pivotal aspect lies in the estimation of the lower bounds of min-max values
associated with sign-changing critical points. In this paper, we construct two
distinct lower bounds of these min-max values. The first one is derived from
the lower bound of Dirichlet eigenvalues of $-\triangle_{X}$, while the second
one is based on the Morse-type estimates and Cwikel-Lieb-Rozenblum type
inequality in degenerate cases. These lower bounds provide different sufficient
conditions for multiplicity results, each with unique advantages and are not
mutually inclusive, particularly in the general non-equiregular case. This
novel observation suggests that in some sense, the situation for sub-elliptic
equations would have essential difference from the classical elliptic
framework.

</details>


### [35] [Higher Hölder regularity for degenerate elliptic PDEs with data in Morrey spaces](https://arxiv.org/abs/2510.10359)
*Giuseppe Di Fazio,Rafayel Teymurazyan,José Miguel Urbano*

Main category: math.AP

TL;DR: Sharp local C^{1,α}-regularity for weak solutions to degenerate elliptic p-Laplacian equations with Morrey space data


<details>
  <summary>Details</summary>
Motivation: To establish precise regularity results for degenerate elliptic equations with data in Morrey spaces, extending classical regularity theory

Method: Uses Fefferman-Phong inequality and standard tools from nonlinear PDE regularity theory

Result: Proves sharp local C^{1,α}-regularity for weak solutions

Conclusion: Successfully establishes optimal regularity for p-Laplacian type equations with Morrey space data

Abstract: We establish sharp local $C^{1,\alpha}$-regularity for weak solutions to
degenerate elliptic equations of $p$-Laplacian type with data in Morrey spaces.
The proof relies on the Fefferman-Phong inequality and standard tools from
regularity theory for nonlinear PDEs.

</details>


### [36] [Abstract second-order boundary control systems](https://arxiv.org/abs/2510.10363)
*Till Preuster,Timo Reis,Manuel Schaller*

Main category: math.AP

TL;DR: This paper extends the analysis of second-order systems to non-self-adjoint operators, constructs boundary triplets and control systems, and provides equivalence transforms between different formulations.


<details>
  <summary>Details</summary>
Motivation: Previous work focused on self-adjoint uniformly positive operators, but many physical systems require more general cases where S* is symmetric (S* ⊂ S), allowing greater freedom in boundary conditions.

Method: Constructs boundary triplets for the operator matrix A, defines associated boundary control systems, characterizes impedance/scattering passivity, and introduces equivalence transforms via non-standard factorization of S.

Result: Fully characterizes when boundary control systems are impedance or scattering passive in terms of trace operators, and provides transformations between position-momentum and strain-momentum formulations.

Conclusion: The approach extends applicability to more general systems, demonstrated through wave equations and Maxwell equations, enabling analysis of systems with broader boundary conditions.

Abstract: We consider abstract second order systems of the form $\ddot{x}(t) + D
\dot{x}(t) + Sx(t)=0$, which are typically analyzed via the operator matrix
$\mathcal{A}=\left[\begin{smallmatrix}
  0 & I \\ -S & -D
  \end{smallmatrix}\right]$ governing the free dynamics of the corresponding
first-order in time formulation. While previous work (e.g. on spectral
properties of) $\mathcal{A}$ has focused on self-adjoint uniformly positive
$S$, we consider the more general case which comprises the situation where
$S^*$ is symmetric, i.e., $S^*\subset S$. As we will show, this relaxation
allows for a large freedom in view of boundary conditions. Our main
contribution is the construction of a boundary triplet for the operator
$\mathcal{A}$ and the definition of an associated boundary control system. We
fully characterize the cases in which the latter is impedance resp. scattering
passive in terms of the associated trace operators. Furthermore, based on a
non-standard factorization of $S$ we introduce an equivalence transform of
$\mathcal{A}$ that maps the abstract second-order system (e.g., $\mathcal{A} =
\left[\begin{smallmatrix}
  0 & I \\ \Delta & -D
  \end{smallmatrix}\right]$ for the wave equation in position-momentum
formulation) into widely-used alternative representation involving lower-order
spatial derivatives on the jet space (i.e., $\left[\begin{smallmatrix}
  0 & \nabla \\ \operatorname{div} & -D
  \end{smallmatrix}\right]$ corresponding to the wave equation in
strain-momentum formulation). We illustrate the suggested approach on the
example of a $n$-dimensional wave equation and a Maxwell equation.

</details>


### [37] [Monotonicity and local uniqueness for an isotropic nonlocal elliptic equation](https://arxiv.org/abs/2510.10408)
*Yi-Hsuan Lin*

Main category: math.AP

TL;DR: The paper extends monotonicity-based inversion methods to an inverse coefficient problem for isotropic nonlocal elliptic equations, establishing a monotonicity relation between coefficients and Dirichlet-to-Neumann maps, and proving local uniqueness.


<details>
  <summary>Details</summary>
Motivation: To develop inversion methods for nonlocal elliptic equations, specifically addressing the inverse coefficient problem for fractional Laplacian operators, which has applications in imaging and inverse problems.

Method: Extends monotonicity-based inversion methods to nonlocal equations, establishes monotonicity relation between coefficients and DN maps, and constructs localized potentials for the nonlocal equation.

Result: Shows that monotonicity ordering of coefficients implies corresponding ordering of DN maps, and obtains local uniqueness result for the fractional inverse problem.

Conclusion: Monotonicity methods can be successfully extended to nonlocal elliptic equations, providing theoretical foundations for coefficient recovery in fractional inverse problems.

Abstract: We extend monotonicity-based inversion methods to an inverse coefficient
problem for the isotropic nonlocal elliptic equation
  \[
  (-\nabla \cdot \sigma \nabla)^s u = 0 \quad \text{in } \Omega \subset
\mathbb{R}^n,
  \]
  where $0 < s < 1$, $n \geq 3$, and $\Omega$ is a bounded open set. We
establish a monotonicity relation between the leading coefficient $\sigma$ and
the (partial) exterior Dirichlet-to-Neumann (DN) map. Our main result shows
that a monotonicity ordering of the coefficients implies a corresponding
ordering of the DN maps. Furthermore, we construct localized potentials for the
nonlocal equation, which yield a local uniqueness result for the fractional
inverse problem.

</details>


### [38] [On the existence of self-similar solutions to the steady Navier-Stokes equations in high dimensions](https://arxiv.org/abs/2510.10488)
*Jeaheang Bang,Changfeng Gui,Hao Liu,Yun Wang,Chunjing Xie*

Main category: math.AP

TL;DR: The paper proves existence of scale-invariant homogeneous solutions to steady Navier-Stokes equations with homogeneous external forces in dimensions 4-16, with global uniqueness for small forces.


<details>
  <summary>Details</summary>
Motivation: To establish existence and uniqueness results for self-similar solutions to steady incompressible Navier-Stokes equations with homogeneous external forces, particularly focusing on higher dimensions.

Method: Exploits relation between radial velocity component and total head pressure under self-similarity assumption; uses energy estimates, maximum principle, and dimension-reduction effects from self-similarity.

Result: Proves existence of at least one (-1)-homogeneous solution for dimensions 4≤n≤16 with any given (-3)-homogeneous force; global uniqueness for small forces; extends to all n≥4 for nonnegative radial forces.

Conclusion: The paper successfully establishes existence and uniqueness of scale-invariant solutions to Navier-Stokes equations with homogeneous external forces, leveraging key structural relationships and dimension-dependent effects.

Abstract: We prove that the steady incompressible Navier-Stokes equations with any
given $(-3)$-homogeneous, locally Lipschitz external force on
$\mathbb{R}^n\setminus\{0\}$, $4\leq n\leq 16$, have at least one
$(-1)$-homogeneous solution which is scale-invariant and regular away from the
origin. The global uniqueness of the self-similar solution is obtained as long
as the external force is small. The key observation is to exploit a nice
relation between the radial component of the velocity and the total head
pressure under the self-similarity assumption. It plays an essential role in
establishing the energy estimates. If the external force has only the
nonnegative radial component, we can prove the existence of $(-1)$-homogeneous
solutions for all $n\geq 4$. The regularity of the solution follows from
integral estimates of the positive part of the total head pressure, which is
due to the maximum principle and a ``dimension-reduction" effect arising from
the self-similarity.

</details>


### [39] [Regularity Theory for the Space Homogeneous Polyatomic Boltzmann Flow](https://arxiv.org/abs/2510.10523)
*Ricardo Alonso,Milana Čolić*

Main category: math.AP

TL;DR: Study of polyatomic Boltzmann equation with continuous internal energy, establishing smoothing effects, derivative propagation, and exponential decay estimates for singularities.


<details>
  <summary>Details</summary>
Motivation: To analyze the mathematical properties of the polyatomic Boltzmann equation with physically relevant collision kernels, particularly focusing on smoothing and decay behaviors.

Method: Analysis of the Boltzmann equation with continuous internal energy using hard potentials type collision kernels with integrable angular part.

Result: Three main results: smoothing effects of gain collision operator, propagation of velocity/internal energy derivatives, and exponential decay of initial singularities.

Conclusion: Solutions decompose into smooth and rapidly decaying rough components, providing comprehensive understanding of solution structure.

Abstract: In this paper, we study the polyatomic Boltzmann equation based on continuous
internal energy, focusing on physically relevant collision kernels of the hard
potentials type with integrable angular part. We establish three main results:
smoothing effects of the gain collision operator, propagation of velocity and
internal energy first-order derivatives of solutions, and exponential decay
estimates for singularities of the initial data. These results ultimately lead
to a decomposition theorem, showing that any solution splits into a smooth part
and a rapidly decaying rough component.

</details>


### [40] [Determining nonlinear balance laws in product-type domains by a single local passive boundary observation](https://arxiv.org/abs/2510.10571)
*Chaohua Duan,Hongyu Liu,Qingle Meng,Li Wang*

Main category: math.AP

TL;DR: Operator-theoretic approach for solving inverse problems in nonlinear balance laws using boundary observations to recover flux and source operators.


<details>
  <summary>Details</summary>
Motivation: Shift focus from identifying specific functional forms to recovering input-output actions of flux and source operators in nonlinear balance laws.

Method: Established that single local passive boundary observation suffices to uniquely determine realizations of flux and source operators on product-type domains.

Result: Reveals holographic-type principle where macroscopic boundary data encodes microscopic dynamical information.

Conclusion: Framework has broad implications for fluid dynamics and reaction-diffusion systems, enabling operator recovery from boundary measurements.

Abstract: This paper introduces an operator-theoretic paradigm for solving inverse
problems in nonlinear balance laws, shifting the focus from identifying
specific functional forms to recovering the input-output actions of the
associated flux and source operators. It is established that a single local
passive boundary observation suffices to uniquely determine realizations of
these operators for systems posed on product-type domains. This framework,
which encompasses dynamical regimes, reveals a holographic-type principle where
macroscopic boundary data encodes microscopic dynamical information, with broad
implications for fluid dynamics and reaction-diffusion systems.

</details>


### [41] [Optimal gradient estimates for conductivity problems with imperfect low-conductivity interfaces](https://arxiv.org/abs/2510.10615)
*Hongjie Dong,Haigang Li,Yan Zhao*

Main category: math.AP

TL;DR: Optimal gradient estimates for field concentration between nearly touching conductors with imperfect interfaces, unifying bounded (γ>0) and singular blow-up (γ=0) cases.


<details>
  <summary>Details</summary>
Motivation: To understand how field gradients behave between nearly touching conductors with imperfect interfaces, bridging the gap between bounded gradients for γ>0 and singular blow-up for γ=0.

Method: Derived new regularity results for elliptic equations as γ→0, established pointwise gradient estimates depending on both γ and ε, and developed case dichotomy based on relative sizes of γ and distance function δ(x').

Result: Obtained optimal pointwise gradient estimates that explicitly depend on γ and ε, providing continuous characterization of gradient behavior throughout the γ transition for strictly relatively convex conductors in all dimensions n≥2.

Conclusion: The work provides a unified framework that completely characterizes gradient behavior between nearly touching conductors, covering both bounded and singular cases through explicit dependence on interfacial bonding parameter γ and separation distance ε.

Abstract: This paper studies field concentration between two nearly touching conductors
separated by imperfect low-conductivity interfaces, modeled by Robin boundary
conditions. It is known that for any sufficiently small interfacial bonding
parameter $\gamma > 0$, the gradient remains uniformly bounded with respect to
the separation distance $\varepsilon$. In contrast, for the perfect bonding
case ($\gamma = 0$, corresponding to the perfect conductivity problem), the
gradient may blow up as $\varepsilon \to 0$ at a rate depending on the
dimension. In this work, we establish optimal pointwise gradient estimates that
explicitly depend on both $\gamma$ and $\varepsilon$ in the regime where these
parameters are small. These estimates provide a unified framework that
encompasses both the previously known bounded case ($\gamma > 0$) and the
singular blow-up scenario ($\gamma = 0$), thus furnishing a complete and
continuous characterization of the gradient behavior throughout the transition
in $\gamma$. The key technical achievement is the derivation of new regularity
results for elliptic equations as $\gamma\to0$, along with a case dichotomy
based on the relative sizes of $\gamma$ and a distance function $\delta(x')$.
Our results hold for strictly relatively convex conductors in all dimensions $n
\geq 2$.

</details>


### [42] [Fine dissipative properties of Euler solutions with measure first derivatives](https://arxiv.org/abs/2510.10704)
*Marco Inversi*

Main category: math.AP

TL;DR: Analysis of bounded weak solutions to incompressible Euler equations with Radon measure derivatives, providing elementary proofs of local energy conservation for BV and BD solutions.


<details>
  <summary>Details</summary>
Motivation: To study fine properties of bounded weak solutions to Euler equations where first derivatives or their combinations are Radon measures, and to obtain simpler proofs of energy conservation without relying on convolution kernel choices.

Method: Exploits the specific form of Euler nonlinearity and analyzes solutions where derivatives are Radon measures, particularly focusing on BV and BD vector fields and cases where only vorticity is a measure.

Result: Obtained elementary proofs of local energy conservation for BV and BD solutions, and derived nontrivial conclusions when only vorticity is assumed to be a measure.

Conclusion: The method successfully provides simplified proofs of energy conservation for Euler equations but does not apply to linear transport equations, where renormalization for BD vector fields remains an open problem.

Abstract: We study fine properties of bounded weak solutions to the incompressible
Euler equations whose first derivatives, or only some combinations of them, are
Radon measures. As consequences we obtain elementary proofs of the local energy
conservation for solutions in BV and BD, without relying on the freedom in
choosing the convolution kernel appearing in the approximation of the
dissipation. The argument heavily exploits the form of the Euler nonlinearity
and it does not apply to the linear transport equations, where the
renormalization property for BD vector fields is an open problem. The methods
also yields to nontrivial conclusions when only the vorticity is assumed to be
a measure.

</details>


### [43] [Hydrodynamics of degenerate Fermi gases on spherical Fermi surfaces](https://arxiv.org/abs/2510.10897)
*Benjamin Anwasia,Diogo Arsénio*

Main category: math.AP

TL;DR: This paper analyzes Fermi gas of free electrons using Boltzmann-Fermi-Dirac equation, focusing on Fermi ground state and excited states on the Fermi sphere. It identifies low-temperature regimes where charge-density fluctuations concentrate on the Fermi sphere and derives macroscopic hydrodynamic equations for plasma oscillations in metals.


<details>
  <summary>Details</summary>
Motivation: To provide precise mathematical understanding of Fermi ground state and its first-order approximation of excited states on the Fermi sphere, and to understand how charge densities flow and propagate in metals through plasma oscillations.

Method: Using hydrodynamic limits in collisional kinetic theory framework to identify low-temperature regimes where charge-density fluctuations concentrate on the Fermi sphere. Characterizes thermodynamic equilibria and energy spectra of fluctuations in three dimensions or higher.

Result: Establishes that excited electrons and their energy can be distributed anisotropically on the Fermi sphere, contrary to common assumption of uniform distribution. Derives macroscopic hydrodynamic equations describing charge density flow and plasma oscillations in conductors.

Conclusion: The work extends applicability of classical acoustic limit proofs by allowing arbitrarily fast convergence rates of Knudsen number. Suggests low-temperature limits of Fermi gases provide promising research avenue toward complete understanding of compressible Euler limit.

Abstract: We consider the description of a Fermi gas of free electrons given by the
Boltzmann--Fermi--Dirac equation, and aim at providing a precise mathematical
understanding of the Fermi ground state and its first-order approximation of
excited states on the Fermi sphere.
  In order to achieve that, using the framework of hydrodynamic limits in
collisional kinetic theory, we identify the low-temperature regimes in which
charge-density fluctuations concentrate on the Fermi sphere. In three spatial
dimensions or higher, we also characterize the thermodynamic equilibra and
energy spectra of fluctuations. This allows us to derive the macroscopic
hydrodynamic equations describing how charge densities flow and propagate in
metals, thereby providing a precise description of plasma oscillations in
conductors. The two-dimensional case is fundamentally different and is handled
in a companion article.
  Remarkably, our results establish that excited electrons and their energy can
be distributed on the Fermi sphere anisotropically, which deviates from the
common intuitive assumption that electrons and their energy should be
distributed uniformly in all directions.
  The hydrodynamic regimes featured in this work are akin to the acoustic limit
of the classical Boltzmann equation. However, we emphasize that our derivation
holds for arbitrarily fast rates of convergence of the Knudsen number, which
significantly extends the applicability of the known proofs of the classical
acoustic limit. This suggest that low-temperature limits of Fermi gases provide
a promising avenue of research toward a complete understanding of the
compressible Euler limit.

</details>


### [44] [$L^2$ normal velocity implies strong solution for graphical Brakke flows](https://arxiv.org/abs/2510.11377)
*Kotaro Motegi*

Main category: math.AP

TL;DR: The paper proves that Brakke flows represented as graphs of continuous functions with continuous spatial derivatives are actually strong solutions to mean curvature flow, with the Brakke normal velocity coinciding with classical normal velocity.


<details>
  <summary>Details</summary>
Motivation: To establish the connection between weak solutions (Brakke flows) and classical solutions for mean curvature flow, particularly for graphical flows with sufficient regularity.

Method: The authors prove that if a one-parameter family of varifolds has L² normal velocity in Brakke's sense and is represented as a graph with continuous spatial derivative, then the function has weak derivatives in L². They combine this with parabolic regularity theory.

Result: Graphical Brakke flows with forcing terms in L^{p,q} and C^{0,α} are shown to be strong and classical solutions to the forced mean curvature flow equation, respectively.

Conclusion: The paper establishes that under appropriate regularity conditions, Brakke flows coincide with classical mean curvature flow solutions, bridging the gap between weak and classical formulations.

Abstract: We prove that if a one-parameter family of varifolds has an $L^2$ normal
velocity $v$ in the sense of Brakke, and if the family is represented as the
graph of a continuous function $f$ with continuous spatial derivative $\nabla
f$, then $f$ has weak derivatives $\partial_t f, \nabla^2 f \in L^2$, and $v$
coincides with the usual normal velocity of the graph. Moreover, by combining
this result with parabolic regularity theory, we show that graphical Brakke
flows with forcing term in $L^{p,q}$ and $C^{0,\alpha}$ are strong and
classical solutions to the forced mean curvature flow equation, respectively.

</details>


### [45] [Global-in-time Discontinuous Solutions for the Two-Phase Model of Compressible Fluids with Density-Dependent Viscosity](https://arxiv.org/abs/2510.11383)
*Marcel Zodji*

Main category: math.AP

TL;DR: Global existence and uniqueness of weak solutions for two compressible immiscible fluids with density-dependent viscosity, preserving the sharp interface regularity over time.


<details>
  <summary>Details</summary>
Motivation: To study the persistence of regularity of sharp interfaces between two compressible immiscible fluids with different pressure and viscosity laws, where density is discontinuous across the interface.

Method: Model described by three coupled equations: two hyperbolic equations (for volume fraction and density) and one parabolic equation for velocity. Initial density is α-Hölder continuous on both sides of a C¹⁺α-regular surface.

Result: Proved existence and uniqueness of global-in-time weak solutions in an intermediate regularity class that preserves piecewise Hölder regularity of density and C¹⁺α regularity of the sharp interface.

Conclusion: The sharp interface between two compressible immiscible fluids with density-dependent viscosity maintains its regularity over time, with global weak solutions existing and being unique.

Abstract: We are concerned with a model describing the motion of two compressible,
immiscible fluids with density-dependent viscosity in the whole $\mathbb R^3$.
The phases of the flow may have different pressure and viscosity laws and are
separated by a sharp interface, across which the (total) density is
discontinuous. Our goal is to study the persistence of the regularity of this
sharp interface over time. More precisely, the dynamics of the flow are
governed by three coupled equations: two hyperbolic equations (for the volume
fraction of one phase and for the density) and a parabolic equation for the
velocity field. We assume that, at the initial time, the density is
$\alpha$-H\"older continuous on both sides of a $\mathscr C^{1+\alpha}$-regular
surface across which it may be discontinuous. We prove the existence and
uniqueness of a global-in-time weak solution in an intermediate regularity
class that ensures the persistence of the piecewise H\"older regularity of the
density and the $\mathscr C^{1+\alpha}$ regularity of the sharp interface.

</details>


### [46] [Jost solutions and direct scattering for the continuum Calogero-Moser equation](https://arxiv.org/abs/2510.11403)
*Rupert L. Frank,Larry Read*

Main category: math.AP

TL;DR: The paper develops an inverse scattering transform for the continuum Calogero-Moser equation, including rigorous treatment of direct scattering via Jost solutions and distorted Fourier transform.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous mathematical framework for analyzing the continuum Calogero-Moser equation using inverse scattering methods.

Method: Constructs Jost solutions, introduces distorted Fourier transform, and derives trace formulas for eigenvalues of the Lax operator.

Result: Provides a complete inverse scattering transform framework for the continuum Calogero-Moser equation with rigorous mathematical foundations.

Conclusion: The developed inverse scattering transform enables systematic analysis of the continuum Calogero-Moser equation through scattering theory techniques.

Abstract: We propose an inverse scattering transform for the continuum Calogero-Moser
equation. We give a rigorous treatment of the direct scattering problem by
constructing the associated Jost solutions and introducing a distorted Fourier
transform, as well as deriving trace formulas for the eigenvalues of the Lax
operator.

</details>


### [47] [Introduction to quantitative De Giorgi methods](https://arxiv.org/abs/2510.11481)
*Giovanni Brigati,Clément Mouhot*

Main category: math.AP

TL;DR: Review of De Giorgi-Nash theory on Holder regularity for elliptic/parabolic PDEs with rough coefficients, extended to hypoelliptic equations in kinetic theory using quantitative trajectory methods.


<details>
  <summary>Details</summary>
Motivation: To present the classical De Giorgi method and its recent extension to hypoelliptic equations that appear naturally in kinetic theory, particularly for Kolmogorov equations with rough diffusion coefficients.

Method: Uses compactness arguments and quantitative methods based on trajectory construction, extending classical De Giorgi techniques to hypoelliptic equations.

Result: Provides a self-contained introduction to the topic with recent developments in quantitative approaches for hypoelliptic equations.

Conclusion: These lecture notes serve as a comprehensive introduction to De Giorgi-Nash theory and its modern extensions to kinetic theory applications.

Abstract: The theory of De Giorgi (1958) and Nash (1959) solves Hilbert's 19th problem
and constitutes a major advance in the analysis of PDEs in the 20th century.
This theory concerns the H\"older regularity of solutions to elliptic and
parabolic equations with non-regular coefficients, and it was extended by Moser
(1960) to include the Harnack inequality. This course reviews the classical De
Giorgi method in the elliptic and parabolic cases and introduces its recent
extension to hypoelliptic equations which appear naturally in kinetic theory.
The simplest case is the Kolmogorov equation with a rough diffusion
coefficients matrix in the kinetic variable. We present compactness arguments
but emphasize the recently developed quantitative methods based on the
construction of trajectories. These lecture notes are self-contained and can be
used as a general introduction to the topic.

</details>


### [48] [Gaussian beam interactions and inverse source problems for nonlinear wave equations](https://arxiv.org/abs/2510.11494)
*Matti Lassas,Tony Liimatainen,Valter Pohjola,Teemu Tyni*

Main category: math.AP

TL;DR: This paper solves the inverse source problem for semilinear wave equations on Lorentzian manifolds, recovering coefficients and source terms from local measurements using a novel Gaussian beam interaction calculus.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of linear inverse source problems where unique recovery is impossible, and to generalize previous works by removing the assumption that u=0 is a solution and accommodating quadratic nonlinearities.

Method: Developed a calculus for nonlinear interactions of Gaussian beams, providing explicit representations for waves corresponding to sources involving products of multiple Gaussian beams as an alternative to Fourier integral operator methods.

Result: Showed that coefficients q₁, q₂ and source term F can be recovered up to natural gauge symmetry from local measurements. When q₁ is known, F can be uniquely recovered - a striking contrast to linear equations where unique recovery is impossible.

Conclusion: The paper establishes unique recovery results for inverse source problems in semilinear wave equations and introduces a versatile Gaussian beam interaction calculus that can be applied to related problems.

Abstract: We study the inverse source problem for the semilinear wave equation \[
(\Box_g + q_1)u + q_2 u^2 = F, \] on a globally hyperbolic Lorentzian manifold.
We demonstrate that the coefficients $q_1$ and $q_2$, as well as the source
term $F$, can be recovered up to a natural gauge symmetry inherent in the
problem from local measurements. Furthermore, if $q_1$ is known, we establish
the unique recovery of the source $F$, which is in a striking contrast to
inverse source problems for linear equations where unique recovery is not
possible. Our results also generalize previous works by eliminating the
assumption that $u= 0$ is a solution, and by accommodating quadratic
nonlinearities.
  A key contribution is the development of a calculus for nonlinear
interactions of Gaussian beams. This framework provides an explicit
representation for waves that correspond to sources involving products of two
or more Gaussian beams. We anticipate this calculus will serve as a versatile
tool in related problems, offering a concrete alternative to Fourier integral
operator methods.

</details>


### [49] [An inverse problem for the Monge-Ampère equation](https://arxiv.org/abs/2510.11572)
*Tony Liimatainen,Yi-Hsuan Lin*

Main category: math.AP

TL;DR: The paper studies an inverse source problem for the Monge-Ampère equation, proving that the Dirichlet-to-Neumann map uniquely determines a positive source function F on convex planar domains.


<details>
  <summary>Details</summary>
Motivation: To extend inverse boundary value problems to fully nonlinear PDEs, specifically the Monge-Ampère equation, which presents challenges due to its full nonlinearity.

Method: Recover the Hessian of solutions as a Riemannian metric, use complex geometric optics solutions with asymptotic expansions, solve a nonlocal ∂̄-equation via unique continuation principles, and analyze the linearized equation with metric coefficients.

Result: The DN map uniquely determines the source function F for the Monge-Ampère equation on convex planar domains, and uniquely determines the conformal class of the metric g for related non-divergence form equations.

Conclusion: The developed techniques for handling full nonlinearity in inverse problems are expected to be broadly applicable to other nonlinear equations.

Abstract: We extend the study of inverse boundary value problems to the setting of
fully nonlinear PDEs by considering an inverse source problem for the
Monge-Amp\`ere equation
  \[
  \det D^2 u = F.
  \]
  We prove that, on a convex Euclidean domain in the plane, the associated
Dirichlet-to-Neumann (DN) map uniquely determines a positive source function
$F$. The proof relies on recovering the Hessian of a solution to the equation,
which is interpreted as a Riemannian metric $g$. Interestingly, although the
equation is posed on a Euclidean domain, the inverse problem becomes
anisotropic since the metric $g$ appears as a coefficient matrix in the
linearized equation.
  As an intermediate step, we prove that the DN map of the non-divergence form
equation
  \[
  g^{ab} \partial_{ab} v = 0
  \]
  uniquely determines the conformal class of the metric $g$ on a simply
connected planar domain, without the usual diffeomorphism invariance.
  To address the challenges of full nonlinearity, we develop asymptotic
expansions for complex geometric optics solutions in the planar setting and
solve a resulting nonlocal
  $\overline{\partial}$-equation by proving a unique continuation principle for
it. These techniques are expected to be applicable to a wide range of inverse
problems for nonlinear equations.

</details>


### [50] [Fujita-type results for parabolic equations with Hartree-type nonlinearities](https://arxiv.org/abs/2510.11648)
*Ahmad Z. Fino,Berikbol T. Torebek*

Main category: math.AP

TL;DR: This paper studies global solutions to a parabolic equation with Hartree-type nonlinearity, establishing conditions for global nonexistence and improving previous results by Filippucci and Ghergu.


<details>
  <summary>Details</summary>
Motivation: To investigate critical behavior of solutions to parabolic equations with fractional Laplacian and Hartree-type nonlinearities, addressing gaps in existing literature and improving previous results.

Method: Uses nonlinear capacity method and fixed-point principle combined with Hardy-Littlewood-Sobolev inequality to analyze existence and nonexistence of solutions.

Result: Established conditions for global nonexistence of solutions and proved local/global existence results for the case with Riesz potential convolution term.

Conclusion: The paper provides improved nonexistence conditions and existence results for parabolic equations with Hartree-type nonlinearities, advancing understanding of critical behavior in such systems.

Abstract: This paper investigates the critical behavior of global solutions to a
parabolic equation with a Hartree-type nonlinearity of the form
$$\left\{\begin{array}{ll} u_{t}+(-\Delta)^{\frac{\beta}{2}} u=
(\mathcal{K}\ast |u|^{p})|u|^{q},&\qquad x\in \mathbb{R}^n,\,\,\,t>0,
  u(x,0)=u_{0}(x),& \qquad x\in \mathbb{R}^n,\end{array}
  \right.$$ where $\beta\in(0,2]$, $n\geq1$, $p>1$, $q\geq 1$,
$(-\Delta)^{\frac{\beta}{2}},\,\beta\in(0,2)$ denotes the fractional Laplacian,
the symbol $\ast$ denotes the convolution operation in $\mathbb{R}^n$, and
$\mathcal{K}:(0,\infty)\rightarrow(0,\infty)$ is a continuous function such
that $\mathcal{K}(|\cdotp|)\in L^1_{{loc}}(\mathbb{R}^n)$ and is monotonically
decreasing in a neighborhood of infinity. We establish conditions for the
global nonexistence of solutions to the problem under consideration, thereby
partially improving some results of Filippucci and Ghergu in [Discrete Contin.
Dyn. Syst. A, 42 (2022) 1817-1833] and [Nonlinear Anal., 221 (2022) 112881]. In
addition, we establish local and global existence results in the case where the
convolution term corresponds to the Riesz potential. Our methodology relies on
the nonlinear capacity method and the fixed-point principle, combined with the
Hardy-Littlewood-Sobolev inequality.

</details>


### [51] [A family of interaction energy minimizers supported on two intervals](https://arxiv.org/abs/2510.11662)
*Steven B. Damelin,Ruiwen Shu*

Main category: math.AP

TL;DR: The paper studies the minimizer of a one-dimensional interaction energy with power-law potential W(x)=-|x|^b/b (1≤b≤2) and quartic external potential U(x)=|x|^4/4. The main result shows that for 1<b<2, the minimizer is supported on two intervals, demonstrating a transition from a single interval (b=1) to two points (b=2).


<details>
  <summary>Details</summary>
Motivation: To understand how the support structure of minimizers evolves as the interaction potential parameter b varies, particularly the transition from continuous support (b=1) to discrete support (b=2), and to develop analytical tools for studying such problems.

Method: Developed a new version of the iterated balayage algorithm, extending the original method by Benko et al. for logarithmic potentials to handle power-law potentials. This algorithm is used to characterize the minimizer's support structure.

Result: Proved that for 1<b<2, the minimizer of the interaction energy is supported on two intervals, establishing the transition behavior as b increases from 1 to 2.

Conclusion: The methodology provides a framework for analyzing minimizers of interaction energies in higher dimensions, with potential applications to problems where the support may be an annulus or other structured sets.

Abstract: In this paper, we consider the one-dimensional interaction energy
$\frac{1}{2}\int_{\mathbb{R}}(W*\rho)(x)d\rho(x) +
\int_{\mathbb{R}}U(x)d\rho(x)$ where the interaction potential $W(x)=
-\frac{|x|^b}{b},\,1\le b \le 2$ and the external potential
$U(x)=\frac{|x|^4}{4}$, and $\rho$ is a compactly supported probability measure
on the real line. Our main result shows that the minimizer is supported on two
intervals when $1<b<2$, showing in particular how the support of the minimizer
transits from an interval (when $b=1$) to two points (when $b=2$) as $b$
increases. As a crucial part of the proof, we develop a new version of the
iterated balayage algorithm, the original version of which was designed by
Benko, Damelin, Dragnev and Kuijlaars for logarithmic potentials in one
dimension. We expect the methodology in this paper can be generalized to study
minimizers of interaction energies in $\mathbb{R}^d$ whose support is possibly
an annulus.

</details>


### [52] [Stochastic Homogenization of the Hamilton-Jacobi Equation on Manifolds](https://arxiv.org/abs/2510.11714)
*Marco Pozza,Alfonso Sorrentino*

Main category: math.AP

TL;DR: Stochastic homogenization of Hamilton-Jacobi equations on Riemannian manifolds with stationary ergodic random environments, showing convergence to deterministic homogenized equations on R^b.


<details>
  <summary>Details</summary>
Motivation: To extend homogenization theory to stochastic settings on Riemannian manifolds and understand the relation between limit problems, limit spaces, and group complexity in random environments.

Method: Using a finitely generated abelian group acting by isometries on the manifold, with stationary ergodic Hamiltonians, and proving convergence of viscosity solutions under scaling.

Result: Viscosity solutions converge almost surely and locally uniformly to deterministic homogenized Hamilton-Jacobi equations on R^b, with effective Hamiltonian obtained via convex conjugate of effective Lagrangian.

Conclusion: This work generalizes Mather's β-function to stochastic settings and introduces stable-like norms for stationary ergodic Riemannian metrics, representing progress towards stationary-ergodic Aubry-Mather theory.

Abstract: This article establishes a stochastic homogenization result for the first
order Hamilton-Jacobi equation on a Riemannian manifold $M$, in the context of
a stationary ergodic random environment. The setting involves a finitely
generated abelian group $ \mathtt{G}$ of rank $b$ acting on $M$ by isometries
in a free, totally discontinuous, and co-compact manner, and a family of
Hamiltonians $H: T^*M \times \Omega \to \mathbb{R}$, parametrized over a
probability space $(\Omega, \mathbb{P})$, which are stationary with respect to
a $\mathbb{P}$-ergodic action of $\mathtt{G}$ on $\Omega$. Under standard
assumptions, including strict convexity and coercivity in the momentum
variable, we prove that as the scaling parameter $\varepsilon$ goes to $0$, the
viscosity solutions to the rescaled equation converge almost surely and locally
uniformly to the solution to a deterministic homogenized Hamilton-Jacobi
equation posed on $\mathbb{R}^b$, which corresponds to the asymptotic cone of
$\mathtt{G}$. In particular, this approach sheds light on the relation between
the limit problem, the limit space, and the complexity of the acting group. The
classical periodic case corresponds to a randomness set $\Omega$ that reduces
to a singleton; other interesting examples of this setting are also described.
  We remark that the effective Hamiltonian $\overline{H}$ is obtained as the
convex conjugate of an effective Lagrangian $\overline{L}$, which generalizes
Mather's $\beta$-function to the stochastic setting; this represents a first
step towards the development of a stationary-ergodic version of Aubry-Mather
theory. As a geometric application, we introduce a notion of stable-like norm
for stationary ergodic families of Riemannian metrics on $M$, which generalizes
the classical Federer-Gromov's stable norm for closed manifolds.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [53] [Enhancement of diffusivity and plastic deformation in ultrasound-assisted cold spray of tungsten: a molecular dynamics study](https://arxiv.org/abs/2510.09803)
*Md Tusher Ahmed,Farid Ahmed,Jianzhi Li*

Main category: physics.comp-ph

TL;DR: Ultrasound-assisted cold spray enhances tungsten's self-diffusivity and plastic deformation, enabling in situ alloy formation and improved coating properties for additive manufacturing applications.


<details>
  <summary>Details</summary>
Motivation: Tungsten's high melting point and brittleness make it challenging for additive manufacturing. Cold spray offers a solid-state alternative, but traditional CS has limited effectiveness for tungsten due to negligible diffusion and plastic deformation.

Method: Used atomistic simulations to study ultrasound-assisted cold spray of tungsten, examining different impact velocities, particle sizes, and ultrasound parameters to enhance viscoplasticity and self-diffusivity.

Result: Ultrasound perturbation significantly increased self-diffusivity and plastic deformation in tungsten compared to non-ultrasound CS. Enabled in situ alloy formation (V-W alloy) with distinct mechanical properties and reduced dislocation density in deposited coatings.

Conclusion: Ultrasound-assisted cold spray is a viable approach for fabricating uniform coatings and engineered alloys of refractory metals like tungsten, overcoming key limitations in additive manufacturing.

Abstract: Tungsten ($W$) is widely valued for its exceptional thermal stability,
mechanical strength, and corrosion resistance, making it an ideal candidate for
high-performance military and aerospace applications. However, its high melting
point and inherent brittleness pose significant challenges for processing $W$
using additive manufacturing (AM). Cold spray (CS), a solid-state AM process
that relies on high-velocity particle impact and plastic deformation, offers a
promising alternative. In this study, we employ atomistic simulations to
investigate the feasibility of CS for tungsten. We show that ultrasound
perturbation can significantly enhance the self-diffusivity and plastic
deformation of $W$ compared to the negligible diffusion and plastic deformation
observed in non-ultrasound-assisted CS of $W$. For different impact velocities,
particle sizes, and ultrasound parameters, we demonstrate that
ultrasound-assisted viscoplasticity enhances self-diffusivity by inhibiting
grain boundaries and incorporating softening in $W$. Moreover, we found that
this enhanced diffusion in ultrasound-assisted $W$ can be exploited to promote
interdiffusion at the particle-substrate interface, enabling in situ alloy
formation. Through the formation of an equimolar $V$-$W$ alloy on a $W$
substrate using ultrasound-assisted CS simulations, we observed distinct
mechanical properties and a reduced dislocation density in the deposited
coating compared to a pure tungsten substrate. These results highlight the
potential of ultrasound-assisted CS as a viable approach for fabricating
uniform coatings and engineered alloys, addressing key limitations in the AM of
refractory metals.

</details>


### [54] [CuPyMag: GPU-Accelerated Finite-Element Micromagnetics with Magnetostriction](https://arxiv.org/abs/2510.09812)
*Hongyi Guan,Ananya Renuka Balakrishna*

Main category: physics.comp-ph

TL;DR: CuPyMag is a GPU-accelerated Python framework for large-scale micromagnetic simulations with magnetostriction, achieving up to 100x speedup over CPU codes and enabling high-resolution simulations with 3M nodes in under 3 hours.


<details>
  <summary>Details</summary>
Motivation: To enable large-scale micromagnetic simulations with magnetostriction that account for realistic defect geometries and nanoscale magnetic structures, which are computationally intensive and challenging with traditional CPU-based approaches.

Method: Uses finite elements with GPU-resident workflow using CuPy's BLAS-accelerated backend, tensorizing key operations. Employs Gauss-Seidel projection method for time integration with stable time steps up to 11 ps and solves equations with 1-3 conjugate-gradient iterations without preconditioning.

Result: Achieves up to two orders of magnitude speedup compared to CPU codes, with linear/sublinear runtime growth. Successfully handles magnetoelastic coupling and far-field effects, solving high-resolution simulations with up to 3M nodes in under three hours on NVIDIA H200 GPU.

Conclusion: CuPyMag enables realistic, large-scale micromagnetic simulations with non-trivial defect geometries, expanding the scope towards problems that can guide experiments. Its Python-based design provides cross-platform compatibility and accessibility for diverse applications.

Abstract: We introduce CuPyMag, an open-source, Python-based framework for large-scale
micromagnetic simulations with magnetostriction. CuPyMag solves micromagnetics
with finite elements in a GPU-resident workflow in which key operations, such
as right-hand-side assembly, spatial derivatives, and volume averages, are
tensorized using CuPy's BLAS-accelerated backend. Benchmark tests show that the
GPU solvers in CuPyMag achieve a speedup of up to two orders of magnitude
compared to the CPU codes. Its runtime grows linearly/sublinearly with problem
size, demonstrating high efficiency. Additionally, CuPyMag uses the
Gauss-Seidel projection method for time integration, which not only allows
stable time steps (up to 11 ps) but also solves each governing equation with
only 1-3 conjugate-gradient iterations without preconditioning. CuPyMag
accounts for magnetoelastic coupling and far-field effects arising from the
boundary of the magnetic body, both of which play an important role in
magnetization reversal in the presence of local defects. CuPyMag solves these
computationally intensive multiphysics simulations with a high-resolution mesh
(up to 3M nodes) in under three hours on an NVIDIA H200 GPU. This acceleration
enables micromagnetic simulations with non-trivial defect geometries and
resolves nanoscale magnetic structures. It expands the scope of micromagnetic
simulations towards realistic, large-scale problems that can guide experiments.
More broadly, CuPyMag is developed using widely adopted Python libraries, which
provide cross-platform compatibility, ease of installation, and accessibility
for adaptations to diverse applications.

</details>


### [55] [Random State Approach to Quantum Computation of Electronic-Structure Properties](https://arxiv.org/abs/2510.09999)
*Yiran Bai,Feng Xiong,Xueheng Kuang*

Main category: physics.comp-ph

TL;DR: Random-state quantum algorithms for efficient electronic structure calculations in large-scale materials using few qubits


<details>
  <summary>Details</summary>
Motivation: Classical computation struggles with large-scale materials, and existing quantum algorithms lack practicality for real materials simulation

Method: Random state circuits with real-time evolution using Trotter decomposition and Hadamard test for density of states; modified quantum phase estimation for local density of states via direct measurements

Result: Successfully computed electronic properties for graphene, twisted bilayer graphene quasicrystals, and fractal lattices with system sizes from hundreds to thousands of atoms

Conclusion: Random-state quantum algorithms provide a general and qubit-efficient approach for simulating electronic properties of large-scale periodic and aperiodic materials

Abstract: Classical computation of electronic properties in large-scale materials
remains challenging. Quantum computation has the potential to offer advantages
in memory footprint and computational scaling. However, general and practical
quantum algorithms for simulating large-scale materials are still lacking. We
propose and implement random-state quantum algorithms to calculate
electronic-structure properties of real materials. Using a random state circuit
with only a few qubits, we employ real-time evolution with first-order Trotter
decomposition and Hadamard test to obtain electronic density of states, and we
develop a modified quantum phase estimation algorithm to calculate real-space
local density of states via direct quantum measurements. Furthermore, we
validate these algorithms by numerically computing the density of states and
spatial distributions of electronic states in graphene, twisted bilayer
graphene quasicrystals, and fractal lattices, covering system sizes from
hundreds to thousands of atoms. Our results manifest that the random-state
quantum algorithms provide a general and qubit-efficient route to simulating
electronic properties of large-scale periodic and aperiodic materials on
quantum computers.

</details>


### [56] [ChemGen: Code Generation for Multispecies Chemical Kinetics in Computational Physics Simulations](https://arxiv.org/abs/2510.10005)
*Ryan F. Johnson,Eric J. Ching,Ethan S. Genter,Joshua E. Lipman,Andrew D. Kercher,Jay Arcities,Hai Wang*

Main category: physics.comp-ph

TL;DR: ChemGen is a software package that generates C++ code to integrate multispecies thermodynamics and chemical kinetics into computational physics codes, achieving 4x speedup over OpenFOAM's native chemistry solver.


<details>
  <summary>Details</summary>
Motivation: To make chemical kinetics more accessible in existing simulation frameworks and bridge the gap between combustion modeling and computational physics.

Method: Uses code generation with decorators to generate flexible C++ code for thermodynamic properties, chemical source terms, analytical derivatives, implicit time integration schemes, linear solvers, and preconditioners.

Result: Verified components show agreement with Cantera and theoretical convergence rates. Integration into OpenFOAM achieved approximately 4x speedup over native chemistry solver.

Conclusion: ChemGen successfully bridges combustion modeling and computational physics through code generation, providing significant performance improvements while being released under NRL Open License as an ongoing project.

Abstract: This paper introduces ChemGen, a software package that uses code generation
to integrate multispecies thermodynamics and chemical kinetics into C+-based
computational physics codes. ChemGen aims to make chemical kinetics more
accessible in existing simulation frameworks and help bridge the gap between
combustion modeling and computational physics. The package employs the concept
of decorators which enable flexible C++ code generation to target established
software ecosystems. ChemGen generates code to evaluate thermodynamic
properties, chemical source terms, and their analytical derivatives for
Jacobian calculations. Also included are a variety of implicit time integration
schemes, linear solvers, and preconditioners. The various components of Chemgen
are verified by demonstrating agreement with Cantera and/or theoretical
convergence rates. Finally, we integrate ChemGen into OpenFOAM and achieve a
speedup over its native chemistry solver by approximately four times. ChemGen
is an ongoing project released under the NRL Open License, a source-available
license provided by the U.S. Naval Research Laboratory.

</details>


### [57] [invDFT: A CPU-GPU massively parallel tool to find exact exchange-correlation potentials from groundstate densities](https://arxiv.org/abs/2510.10529)
*Vishal Subramanian,Bikash Kanungo,Vikram Gavini*

Main category: physics.comp-ph

TL;DR: invDFT is an open-source framework that solves the inverse DFT problem to compute exact XC potentials from target densities, addressing numerical challenges through finite-element basis, asymptotic corrections, and HPC advances.


<details>
  <summary>Details</summary>
Motivation: Existing XC approximations in DFT lack general purpose chemical accuracy despite 50 years of research. The inverse DFT problem offers insights into XC functional nature and aids in developing more accurate functionals.

Method: Uses systematically convergent finite-element basis, asymptotic corrections to target density, and numerical/HPC advances including CPU-GPU hybrid architectures for efficiency and parallel scalability.

Result: Demonstrated accuracy and scalability using FCI densities and model densities for up to 100 electrons across weakly and strongly correlated molecules.

Conclusion: invDFT provides a robust framework to overcome numerical challenges in inverse DFT, enabling computation of exact XC potentials from target densities for improved functional development.

Abstract: Density functional theory (DFT) remains the most widely used electronic
structure method. Although exact in principle, in practice, it relies on
approximations to the exchange-correlation (XC) functional, which is known to
be a unique functional of the electron density. Despite 50 years of active
research, existing XC approximations remain far from general purpose chemical
accuracy of various thermochemical and materials properties. In that light, the
inverse DFT problem, of finding the exact XC potential corresponding to an
accurate groundstate density, offers an insightful tool to understand the
nature of the XC functional as well as aid in the development of more accurate
functionals. However, solving the inverse DFT problem is fraught with several
numerical challenges, such as non-uniqueness or spurious oscillations in the
solution and non-convergence. We present invDFT as an open-source framework to
address the outstanding challenges in inverse DFT and computed XC potentials
solely from a target density. We do so by use of a systematically convergent
finite-element basis and asymptotic corrections to the target density. We also
employ several numerical and high-performance computing (HPC) advances that
affords both efficiency and parallel scalability, on CPU-GPU hybrid
architectures. We demonstrate the accuracy and scalability of invDFT using
accurate full-configuration interaction (FCI) densities as well as model
densities, ranging up to 100 electrons and spanning both weakly and strongly
correlated molecules.

</details>


### [58] [Tuning Layer Orbital Hall Effect via Spin Rotation in Ferromagnetic Transition Metal Dichalcogenides](https://arxiv.org/abs/2510.11180)
*Shilei Ji,Jianping Yang,Li Gao,Xing'ao Li*

Main category: physics.comp-ph

TL;DR: The paper investigates orbital Hall effect in quantum anomalous Hall insulators using first-principles calculations, showing valley-selective orbital angular momentum excitation controlled by stacking symmetry.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of electronic devices by leveraging orbital angular momentum for information transmission, and to understand how crystal field effects and band topology influence orbital angular momentum manipulation.

Method: First-principles calculations on quantum anomalous Hall insulators with different stacking configurations (AA and AB stacking), analyzing layer orbital Hall effect through symmetry analysis and valley/orbital polarization.

Result: Orbital Hall effect originates from single valley due to band inversion; AA stacking shows intrinsic orbital polarization with OAM excitations from different valleys in two layers; AB stacking exhibits valley polarization protected by inversion symmetry.

Conclusion: Stacking symmetry controls valley-selective orbital angular momentum excitation, providing insights for orbitronics applications where orbital angular momentum can be manipulated through structural symmetry engineering.

Abstract: Orbitronics, which leverages the angular momentum of atomic orbitals for
information transmission, provides a novel strategy to overcome the limitations
of electronic devices. Unlike electron spin, orbital angular momentum (OAM) is
strongly influenced by crystal field effects and band topology, making its
orientation difficult to manipulate with external fields. In this work, by
using first principle calculations, we investigate quantum anomalous Hall
insulators (QAHIs) as a model system to study the layer orbital Hall effect
(OHE). Due to band inversion, only one valley remains orbital polarization, and
thus the OHE originates from a single valley. Based on stacking symmetry
analysis, we investigated both AA and AB stacking configurations, which possess
mirror and inversion symmetries, respectively. The excitation of OAM exhibits
valley selectivity, determined jointly by valley polarization and orbital
polarization. In AA stacking, the absence of inversion center gives rise to
intrinsic orbital polarization, leading to OAM excitations from different
valleys in the two layers. In contrast, AB stacking is protected by inversion
symmetry, which enforces valley polarization and

</details>


### [59] [Computational Crystal Plasticity Homogenization using Empirically Corrected Cluster Cubature (E3C) Hyper-Reduction](https://arxiv.org/abs/2510.11187)
*Stephan Wulfinghoff*

Main category: physics.comp-ph

TL;DR: A hyper-reduction method (E3C) is applied to computational homogenization of elastoplastic polycrystals, using novel reduced modes that replace strain modes and significantly reduce integration points, enabling efficient two-scale simulations.


<details>
  <summary>Details</summary>
Motivation: Computational homogenization of elastoplastic polycrystals is challenging due to the large number of grains, complex interactions, and crystal plasticity model complexity. Existing reduced order models have limitations, making mean field approaches preferred.

Method: The E3C hyper-reduction method identifies reduced modes (E3C-modes) that replace strain modes in projection-based Reduced Order Models. These use generalized integration points in strain space trained to satisfy orthogonality conditions, matching full-field model equilibrium states and stresses. Grain count is reduced while preserving texture features.

Result: The method successfully simulated two macroscopic engineering parts (untextured and textured) in three-dimensional two-scale applications with hundreds of thousands of macroscopic degrees of freedom and millions of grains. Computing times were in the order of hours on standard laptop hardware.

Conclusion: The E3C hyper-reduction method enables efficient computational homogenization of elastoplastic polycrystals, achieving significant computational speedup while maintaining accuracy in complex multi-scale simulations.

Abstract: The computational homogenization of elastoplastic polycrystals is a
challenging task due to the huge number of grains required, their complicated
interactions and due to the complexity of crystal plasticity models per se.
Despite a few successes of reduced order models, mean field and simplified
homogenization approaches often remain the preferred choice. In this work, a
recently proposed hyper-reduction method (called E3C) for projection-based
Reduced Order Models (pROMs) is applied to the problem of computational
homogenization of geometrically linearly deforming elastoplastic polycrystals.
The main novelty lies in the identification of reduced modes (the 'E3C-modes'),
which replace the strain modes of the reduced-order model, leading to a
significantly smaller number of integration points. The peculiarity, which
distinguishes the method from more conventional hyper-reduction techniques, is
that the E3C integration points are not taken from the set of FE integration
points. Instead, they can be interpreted as generalized integration points in
strain space which are trained such as to satisfy an orthogonality condition,
which ensures that the hyper-reduced model matches the equilibrium states and
macroscopic stresses of full-field model data as accurately as possible. In
addition, the number of grains is reduced, preserving the main features of the
original texture of the finite element model. Two macroscopic engineering parts
(untextured and textured) are simulated, illustrating the performance of the
method in three-dimensional two-scale applications involving hundreds of
thousands macroscopic degrees of freedom and millions of grains with computing
times in the order of hours (cumulated online and offline effort) on standard
laptop hardware.

</details>


### [60] [Optimal parallelisation strategies for flat histogram Monte Carlo sampling](https://arxiv.org/abs/2510.11562)
*Hubert J. Naguszewski,Christopher D. Woodgate,David Quigley*

Main category: physics.comp-ph

TL;DR: Benchmarking of parallelization schemes for flat histogram methods like Wang-Landau sampling, with focus on energy domain decomposition, replica exchange, and multiple walkers per sub-domain. A dynamic sub-domain sizing scheme is proposed and tested on a lattice model of AlTiCrMo alloy.


<details>
  <summary>Details</summary>
Motivation: To establish best practices for parallelizing flat histogram methods by benchmarking various schemes to determine which factors have the largest impact on parallel efficiency.

Method: Implemented and tested several parallelization schemes: energy domain decomposition with static and dynamic sub-domain sizing, replica exchange, multiple random walkers per sub-domain, and varying energy sub-domain overlap sizes. Applied to a lattice-based model of AlTiCrMo refractory high-entropy superalloy.

Result: All proposed strategies provided speedup, but parallelization across energy domains with non-uniform (dynamic) sizing offered the most significant performance improvements.

Conclusion: Concrete recommendations are provided for prioritizing parallelization strategies to optimally accelerate flat-histogram Monte Carlo simulations, with dynamic energy domain sizing being the most effective approach.

Abstract: Flat histogram methods, such as Wang-Landau sampling, provide a means for
high throughput calculation of phase diagrams of atomistic/lattice model
systems. Many parallelisation schemes with varying degrees of complexity have
been proposed to accelerate such sampling simulations. In this study, several
widely used schemes are benchmarked - both in isolation and in combination - to
establish best practice. The schemes studied include energy domain
decomposition with both static sizing of energy sub-domains, as well as a
dynamic sub-domain sizing scheme which we propose. We also assess the benefits
both of replica exchange and of including multiple random walkers per
sub-domain, to determine which factors have the largest impact on parallel
efficiency. Additionally, the influence of the choice of size of energy
sub-domain overlap regions is discussed. As an illustrative test case, we
implement and apply the aforementioned strategies to a lattice-based model
describing the internal energies of the AlTiCrMo refractory high-entropy
superalloy, which is understood to crystallographically order into a B2 (CsCl)
structure with decreasing temperature. We find that - while all of the proposed
strategies confer a non-negligible speedup - parallelisation across energy
domains which are non-uniform in size offers the most appreciable performance
improvements. This work offers concrete recommendations for which
parallelisation strategies should be prioritised to optimally accelerate
flat-histogram Monte Carlo simulations.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [61] [Thermal and Electrical Conductivities of Aluminum Up to 1000 eV: A First-Principles Prediction](https://arxiv.org/abs/2510.10112)
*Qianrui Liu,Xiantu He,Mohan Chen*

Main category: physics.plasm-ph

TL;DR: A new method combining Kubo formalism with mixed stochastic-deterministic DFT enables efficient first-principles calculation of thermal and electrical conductivities in dense plasmas at extremely high temperatures up to 1000 eV.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of thermal and electrical conductivities under extreme temperatures is crucial for understanding high-energy-density physics phenomena like stellar core dynamics, planetary magnetic field generation, and laser-driven plasma evolution, but traditional first-principles methods are computationally prohibitive.

Method: Integration of Kubo formalism with mixed stochastic-deterministic DFT (mDFT) method to enhance computational efficiency for calculating transport properties in dense plasmas.

Result: Successfully calculated thermal and electrical conductivities of Aluminum up to 1000 eV, revealing significant deviations from traditional transport models in the warm dense matter regime.

Conclusion: Quantum effects must be accounted for when investigating transport properties of warm dense matter, as first-principles calculations show substantial differences from conventional models.

Abstract: Accurate prediction of the thermal and electrical conductivities of materials
under extremely high temperatures is essential in high-energy-density physics.
These properties govern processes such as stellar core dynamics, planetary
magnetic field generation, and laser-driven plasma evolution. However,
first-principles methods like Kohn-Sham (KS) density functional theory (DFT)
face challenges in predicting these properties due to prohibitively high
computational costs. We propose a scheme that integrates the Kubo formalism
with a mixed stochastic-deterministic DFT (mDFT) method, which substantially
enhances efficiency in computing thermal and electrical conductivities of dense
plasmas under extremely high temperatures. As a showcase, this approach enables
{\it ab initio} calculations of the thermal and electrical conductivities of
Aluminum (Al) up to 1000 eV. Compared to traditional transport models, our
first-principles results reveal significant deviations in the thermal and
electrical conductivities of Al within the warm dense matter regime,
underscoring the importance of accounting for quantum effects when
investigating these transport properties of warm dense matter.

</details>


### [62] [Machine Learning-Integrated Hybrid Fluid-Kinetic Framework for Quantum Electrodynamic Laser Plasma Simulations](https://arxiv.org/abs/2510.11174)
*Sadra Saremi,Amirhossein Ahmadkhan Kordbacheh*

Main category: physics.plasm-ph

TL;DR: A machine learning-based 3D hybrid fluid-PIC system that automatically switches between fluid and kinetic regimes for laser-plasma simulations, achieving high accuracy with R² > 0.95.


<details>
  <summary>Details</summary>
Motivation: High-intensity laser plasma interactions involve both fluid and kinetic regimes, requiring models that maintain physical precision while keeping computational speed.

Method: Uses a hybrid fluid-PIC system with SwitchNet neural network to direct regime transitions, smooth ADK tunneling/multiphoton ionization transitions, Airy-function approximations for QED effects, physics-based loss functions, and Monte Carlo dropout for uncertainty.

Result: Achieves precise predictions with R² values above 0.95 and mean squared errors below 10^-4 for all field components.

Conclusion: The adaptive approach enhances accuracy and scalability of laser-plasma simulations, providing a unified predictive framework for high-energy-density and particle acceleration applications.

Abstract: High-intensity laser plasma interactions create complex computational
problems because they involve both fluid and kinetic regimes, which need models
that maintain physical precision while keeping computational speed. The
research introduces a machine learning-based three-dimensional hybrid
fluid-particle-in-cell (PIC) system, which links relativistic plasma behavior
to automatic regime transitions. The technique employs fluid approximations for
stable areas but activates the PIC solver when SwitchNet directs it to unstable
sections through its training on physics-based synthetic data. The model uses a
smooth transition between Ammosov-Delone-Krainov (ADK) tunneling and
multiphoton ionization rates to simulate ionization, while Airy-function
approximations simulate quantum electrodynamic (QED) effects for radiation
reaction and pair production. The convolutional neural network applies energy
conservation through physics-based loss functions, which operate on normalized
fields per channel. Monte Carlo dropout provides uncertainty measurement. The
hybrid model produces precise predictions with coefficient of determination
(R^2) values above 0.95 and mean squared errors below 10^-4 for all field
components. This adaptive approach enhances the accuracy and scalability of
laser-plasma simulations, providing a unified predictive framework for
high-energy-density and particle acceleration applications.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [63] [A Spatio-temporal CP decomposition analysis of New England region in the US](https://arxiv.org/abs/2510.10322)
*Fatoumata Sanogo*

Main category: stat.AP

TL;DR: Proposes a spatio-temporal principal component analysis method to initialize CP decomposition components for multidimensional climate data, leveraging both spatial and temporal structures.


<details>
  <summary>Details</summary>
Motivation: Modern technology has increased the availability of multidimensional spatio-temporal data, creating opportunities to better utilize both spatial and temporal structures in analysis methods.

Method: Uses NCAR Climate Data Gateway data (precipitation, max/min temperatures) to create multidimensional tensors, then applies spatio-temporal PCA to initialize CP decomposition components.

Result: The proposed method's performance was tested against popular initialization methods and further validated through clustering analysis.

Conclusion: The spatio-temporal PCA initialization method effectively leverages spatial and temporal structures in multidimensional data for improved CP decomposition analysis.

Abstract: Spatio temporal data consist of measurement for one or more raster fields
such as weather, traffic volume, crime rate, or disease incidents. Advances in
modern technology have increased the number of available information for this
type of data hence the rise of multidimensional data. In this paper we take
advantage of the multidimensional structure of the data but also its temporal
and spatial structure. In fact, we will be using the NCAR Climate Data Gateway
website which provides data discovery and access services for global and
regional climate model data. The daily values of total precipitation (prec),
maximum (tmax), and minimum (tmin) temperature are combined to create a
multidimensional data called tensor (a multidimensional array). In this paper,
we propose a spatio temporal principal component analysis to initialize CP
decomposition component. We take full advantage of the spatial and temporal
structure of the data in the initialization step for cp component analysis. The
performance of our method is tested via comparison with most popular
initialization method. We also run a clustering analysis to further show the
performance of our analysis.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [64] [Enhanced Sampling for Efficient Learning of Coarse-Grained Machine Learning Potentials](https://arxiv.org/abs/2510.11148)
*Weilong Chen,Franz Görlich,Paul Fuchs,Julija Zavadlav*

Main category: physics.chem-ph

TL;DR: Enhanced sampling improves coarse-grained machine learning potentials by generating biased data along CG degrees of freedom and recomputing forces with respect to unbiased potential, addressing poor sampling in transition regions and reducing simulation time.


<details>
  <summary>Details</summary>
Motivation: Current CG MLPs trained via force matching require long atomistic trajectories for convergence and suffer from poor sampling in transition regions, limiting their accuracy and reliability.

Method: Use enhanced sampling to bias along CG degrees of freedom for data generation, then recompute forces with respect to unbiased potential while preserving the correct potential of mean force.

Result: Demonstrated effectiveness on Müller-Brown potential and capped alanine, achieving notable improvements in sampling efficiency and model accuracy.

Conclusion: Enhanced sampling for force matching is a promising direction to improve the accuracy and reliability of coarse-grained machine learning potentials.

Abstract: Coarse-graining (CG) enables molecular dynamics (MD) simulations of larger
systems and longer timescales that are otherwise infeasible with atomistic
models. Machine learning potentials (MLPs), with their capacity to capture
many-body interactions, can provide accurate approximations of the potential of
mean force (PMF) in CG models. Current CG MLPs are typically trained in a
bottom-up manner via force matching, which in practice relies on configurations
sampled from the unbiased equilibrium Boltzmann distribution to ensure
thermodynamic consistency. This convention poses two key limitations: first,
sufficiently long atomistic trajectories are needed to reach convergence; and
second, even once equilibrated, transition regions remain poorly sampled. To
address these issues, we employ enhanced sampling to bias along CG degrees of
freedom for data generation, and then recompute the forces with respect to the
unbiased potential. This strategy simultaneously shortens the simulation time
required to produce equilibrated data and enriches sampling in transition
regions, while preserving the correct PMF. We demonstrate its effectiveness on
the M\"uller-Brown potential and capped alanine, achieving notable
improvements. Our findings support the use of enhanced sampling for force
matching as a promising direction to improve the accuracy and reliability of CG
MLPs.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [65] [Synchrosqueezed windowed linear canonical transform: A method for mode retrieval from multicomponent signals with crossing instantaneous frequencies](https://arxiv.org/abs/2510.10438)
*Shuixin Li,Jiecheng Chen,Qingtang Jiang,Jian Lu*

Main category: eess.SP

TL;DR: The paper proposes a novel windowed linear canonical transform (WLCT) that creates a 3D time-frequency-chirprate representation for analyzing superimposed non-stationary signals, and develops synchrosqueezed transforms for signal separation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of overlapping signal components in time-frequency domain analysis of superimposed non-stationary signals found in nature.

Method: Developed four types of windowed linear canonical transforms (WLCTs) that extend the chirplet transform concept, and used X-ray transforms to sharpen the time-frequency-chirprate representation.

Result: Created a new 3D time-frequency-chirprate representation framework and derived corresponding three-dimensional synchrosqueezed transforms.

Conclusion: WLCTs show great potential for three-dimensional signal separation of complex superimposed non-stationary signals.

Abstract: In nature, signals often appear in the form of the superposition of multiple
non-stationary signals. The overlap of signal components in the time-frequency
domain poses a significant challenge for signal analysis. One approach to
addressing this problem is to introduce an additional chirprate parameter and
use the chirplet transform (CT) to elevate the two-dimensional time-frequency
representation to a three-dimensional time-frequency-chirprate representation.
From a certain point of view, the CT of a signal can be regarded as a windowed
special linear canonical transform of that signal, undergoing a shift and a
modulation.
  In this paper, we develop this idea to propose a novel windowed linear
canonical transform (WLCT), which provides a new time-frequency-chirprate
representation. We discuss four types of WLCTs. In addition, we use a special
X-ray transform to further sharpen the time-frequency-chirprate representation.
Furthermore, we derive the corresponding three-dimensional synchrosqueezed
transform, demonstrating that the WLCTs have great potential for
three-dimensional signal separation.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [66] [Time-Dilation Methods for Extreme Multiscale Timestepping Problems](https://arxiv.org/abs/2510.09756)
*Philip F. Hopkins,Elias R. Most*

Main category: astro-ph.IM

TL;DR: A method using continuous space-time dilation factors to handle extreme timescale ranges in astrophysical simulations, enabling speedups over 10^4 by modulating time evolution locally.


<details>
  <summary>Details</summary>
Motivation: Astrophysical simulations face challenges with extreme dynamic range of timescales around special points (black holes, stars, etc.), where short numerical timesteps in subdomains dominate simulation costs and limit long-term evolution studies.

Method: Generalizes previous approaches using a variable continuous space-time dilation/stretch factor a(x,t) to modulate time evolution, extending reduced-speed-of-light and binary orbital dynamics methods while ensuring correct local steady-state solutions.

Result: The method avoids imprinting arbitrary scales without clear scale-separation, couples well with Lagrangian/Eulerian methods, and achieves effective speedup factors exceeding 10^4 in multiphysics simulations.

Conclusion: This flexible, easily-implemented approach provides a generalized framework for handling extreme timescale ranges in astrophysical simulations while maintaining physical accuracy and enabling significant computational speedups.

Abstract: Many astrophysical simulations involve extreme dynamic range of timescales
around 'special points' in the domain (e.g. black holes, stars, planets, disks,
galaxies, shocks, mixing interfaces), where processes on small scales couple
strongly to those on large scales. Adaptive resolution, multi-physics, and
hybrid numerical methods have enabled tremendous progress on the spatial,
physics, and numerical challenges involved. But often the limiter for following
the long timescales of global evolution is the extremely short numerical
timestep required in some subdomains (which leads to their dominating
simulation costs). Recently several approaches have been developed for tackling
this in problems where the short timescale solution is sampled and then
projected as an effective subgrid model over longer timescales (e.g. 'zooming
in and out'). We generalize these to a family of models where time evolution is
modulated by a variable but continuous in space-and-time dilation/stretch
factor $a({\bf x},\,t)$. This extends previous well-studied approaches
(including reduced-speed-of-light and binary orbital dynamics methods), and
ensures that the system comes to correct local steady-state solutions, and
derive criteria that the dilation factor/timesteps/resolution must obey to
ensure good behavior. We present a variety of generalizations to different
physics or coupling scales. Compared to previous approaches, this method makes
it possible to avoid imprinting arbitrary scales where there is no clear
scale-separation, and couples well to Lagrangian or Eulerian methods. It is
flexible and easily-implemented and we demonstrate its validity (and
limitations) in test problems. We discuss the relationship between these
methods and physical time dilation in GRMHD. We demonstrate how this can be
used to obtain effective speedup factors exceeding $\gtrsim 10^{4}$ in
multiphysics simulations.

</details>


### [67] [An efficient spectral Poisson solver for the nirvana-III code: the shearing-box case with vertical vacuum boundary conditions](https://arxiv.org/abs/2510.10070)
*S. Rendon Restrepo,O. Gressel*

Main category: astro-ph.IM

TL;DR: Developed two novel spectral methods for solving Poisson's equation in self-gravitating, vertically stratified protoplanetary discs using shearing box formalism with vertical vacuum boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To study differential rotation and self-gravity in astrophysical systems like protoplanetary discs and galaxies, requiring accurate Poisson solvers in shearing box framework with proper boundary conditions.

Method: Two spectral approaches using multi-dimensional FFT: one exploiting linearity of Poisson equation, and another using analytical Green's function satisfying shear-periodic and vacuum boundary conditions. Implemented in NIRVANA-III code with P3DFFT library for scalability.

Result: Methods show excellent accuracy with modest grid points, third-order convergence, good scalability up to 4096 CPU cores, consuming <6% total runtime. Overcame scalability limitations of slab decomposition.

Conclusion: Introduced two high-performance spectral Poisson solvers that enable high-resolution local studies of self-gravity in astrophysical systems, supporting vertical vacuum boundary conditions in shearing-box framework.

Abstract: The stability of a differentially rotating fluid subject to its own gravity
is a problem with applications across wide areas of astrophysics--from
protoplanetary discs (PPDs) to entire galaxies. The shearing box formalism
offers a conceptually simple framework for studying differential rotation in
the local approximation. Aimed at self-gravitating, and importantly, vertically
stratified PPDs, we develop two novel methods for solving Poisson's equation in
the framework of the shearing box with vertical vacuum boundary conditions
(BCs). Both approaches naturally make use of multi-dimensional fast Fourier
transforms for computational efficiency. While the first one exploits the
linearity properties of the Poisson equation, the second, which is slightly
more accurate, consists of finding the adequate discrete Green's function (in
Fourier space) adapted to the problem at hand. To this end, we have revisited
the method proposed by Vico et al. (2016) and have derived an analytical
Green's function satisfying the shear-periodic BCs in the plane as well as
vacuum BCs, vertically. Our spectral method demonstrates excellent accuracy,
even with a modest number of grid points, and exhibits third-order convergence.
It has been implemented in the NIRVANA-III code, where it exhibits good
scalability up to 4096 CPU cores, consuming less than 6% of the total runtime.
This was achieved through the use of P3DFFT, a fast Fourier Transform library
that employs pencil decomposition, overcoming the scalability limitations
inherent in libraries using slab decomposition. We have introduced two novel
spectral Poisson solvers that guarantees high accuracy, performance, and
intrinsically support vertical vacuum boundary conditions in the shearing-box
framework. Our solvers enable high-resolution local studies involving
self-gravity, such as MHD simulations of gravito-turbulence or gravitational
fragmentation.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [68] [Inequalities, identities, and bounds for divided differences of the exponential function](https://arxiv.org/abs/2510.10724)
*Qiulin Zeng,Nicholas Ezzell,Arman Babakhani,Itay Hen,Lev Barash*

Main category: math.CA

TL;DR: This paper proves log-submodularity of exponential divided differences, establishes a four-point inequality, provides bounds and asymptotics, and presents closed-form identities.


<details>
  <summary>Details</summary>
Motivation: To study the mathematical properties of exponential divided differences, particularly their submodular behavior and inequalities, which have applications in optimization and analysis.

Method: The authors use mathematical analysis techniques including inequality proofs, asymptotic analysis, and derivation of closed-form identities for exponential divided differences.

Result: Key results include: (i) proof of log-submodularity, (ii) establishment of a four-point inequality, (iii) sharp bounds and asymptotics, and (iv) closed-form convolution and summation identities.

Conclusion: The paper provides comprehensive mathematical analysis of exponential divided differences, establishing fundamental properties and identities that advance the understanding of these mathematical objects.

Abstract: Let $\exp[x_0,x_1,\dots,x_n]$ denote the divided difference of the
exponential function.
  (i) We prove that exponential divided differences are log-submodular.
  (ii) We establish the four-point inequality $
\exp[a,a,b,c]\,\exp[d,d,b,c]+\exp[b,b,a,d]\,\exp[c,c,a,d]-\exp[a,b,c,d]^2 \ge 0
$ for all $ a,b,c,d \in \mathbb{R} $.
  (iii) We obtain sharp two-sided bounds for $\exp[x_0,\dots,x_n]$ at fixed
mean and variance; as a consequence, we derive their large-input asymptotics.
  (iv) We present closed-form identities for divided differences of the
exponential function, including a convolution identity and summation formulas
for repeated arguments.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [69] [Coherent Rayleigh-Brillouin scattering: influences of intermolecular potentials and chirp rates](https://arxiv.org/abs/2510.09641)
*Lei Wu*

Main category: physics.flu-dyn

TL;DR: Developed MATLAB code to solve Boltzmann equation for chirped coherent Rayleigh-Brillouin scattering spectra computation, achieving ~1 minute per spectrum.


<details>
  <summary>Details</summary>
Motivation: Chirped CRBS is a high SNR, nanosecond-resolution flow diagnostic technique requiring comparison with theoretical predictions from Boltzmann equation.

Method: Created deterministic MATLAB code to solve Boltzmann equation for computing CRBS spectra.

Result: CRBS spectrum is highly sensitive to intermolecular potential; rapid chirping generates fine ripples around Rayleigh peak and spectral asymmetries.

Conclusion: Successfully developed efficient computational method for CRBS analysis with important findings about spectral sensitivity and chirping effects.

Abstract: Chirped coherent Rayleigh-Brillouin scattering (CRBS) is a flow diagnostic
technique that offers high signal-to-noise ratios and nanosecond temporal
resolution. To extract information of dilute gas flow, experimental spectra
must be compared with theoretical predictions derived from the Boltzmann
equation. In this work, we develop a MATLAB code that deterministically solves
the Boltzmann equation to compute CRBS spectra, enabling each line shape to be
obtained in about one minute. We find that the CRBS spectrum is highly
sensitive to the intermolecular potential, and that rapid chirping generates
fine ripples around the Rayleigh peak along with spectral asymmetries.

</details>


### [70] [A model for transport of soluble surfactants in two-phase flows](https://arxiv.org/abs/2510.10857)
*Suhas S. Jain*

Main category: physics.flu-dyn

TL;DR: A novel computational model for soluble surfactant transport in two-phase flows that accurately models adsorption/desorption processes using a central-difference scheme with the ACDI diffuse-interface method.


<details>
  <summary>Details</summary>
Motivation: To overcome numerical challenges in simulating soluble surfactants in two-phase flows, where sharp concentration gradients at interfaces and selective adsorption/desorption create difficulties for accurate modeling.

Method: Proposed a computational model using central-difference scheme discretization with the ACDI diffuse-interface method, maintaining positivity and discrete confinement properties while preventing artificial numerical diffusion.

Result: The model successfully maintains positivity of surfactant concentration and discrete confinement, preventing artificial diffusion between interface and bulk phases. Numerical simulations demonstrate accuracy, robustness, and proper surfactant confinement.

Conclusion: The proposed model provides an accurate and robust framework for simulating soluble surfactant transport in two-phase flows, with proven mathematical properties ensuring physical realizability and preventing numerical artifacts.

Abstract: In this work, we propose a novel transport model for soluble surfactants in
two-phase flows. In a two-phase flow, the soluble surfactants can adsorb/desorb
from/into the bulk of any of the phases to the interface and can modify the
interface properties. This results in sharp gradients in the surfactant
concentration on the interface and also between the two phases in the bulk when
there is selective adsorption/desorption, presenting a serious challenge for
the numerical simulations.
  To overcome this challenge, we propose a computational model for the
transport of soluble surfactants that can model the adsorption and desorption
processes accurately. The model is discretized using a central-difference
scheme, which leads to a non-dissipative implementation that is crucial for the
simulation of turbulent flows. The model is used with the ACDI
diffuse-interface method (Jain, 2022), but can also be used with other
algebraic-based interface-capturing methods. Furthermore, the provable
strengths of the proposed model are: (a) the model maintains the positivity
property of the surfactant concentration field, a physical realizability
requirement for the simulation of surfactants, when the proposed criterion is
satisfied, (b) the proposed model maintains discrete confinement of the
interfacial and bulk surfactants and prevents artificial numerical diffusion of
the surfactant between the interface and the bulk and between the two phases in
the bulk.
  Finally, we present numerical simulations using the proposed model for both
one-dimensional and multi-dimensional cases and assess: the accuracy and
robustness of the model, the validity of the positivity property of the scalar
concentration field, and the confinement of the surfactant at the interface. We
also study the effect of surfactants on an oscillating droplet and on a complex
droplet/bubble-laden turbulent flow.

</details>


### [71] [Analysis of bio-nanofluid flow over a stretching sheet with slip boundaries](https://arxiv.org/abs/2510.11149)
*Bahram Jalili,Salar Ghadiri Alamdari,Payam Jalili,Davood Domiri Ganji*

Main category: physics.flu-dyn

TL;DR: Study of 3D micropolar bio-nanofluid flow over a stretching sheet with magnetic field and slip boundaries, analyzed using finite difference method.


<details>
  <summary>Details</summary>
Motivation: To investigate the complex behavior of bio-nanofluids in three-dimensional flow with multiple physical parameters including magnetic field, slip boundaries, and various transport phenomena.

Method: Mathematical modeling with relevant transformations, numerical solution of non-linear ODEs using finite difference method, comprehensive parameter analysis.

Result: Velocity increases with λ and δ, decreases with A, δv, and M. Micro-rotation F(η) decreases with spin gradient viscosity, G(η) increases. Temperature decreases with Prandtl number, increases with thermophoresis. Concentration decreases with Schmidt number and Brownian motion. Microorganisms increase with Peclet number and slip parameters, decrease with bio-convection Schmidt number.

Conclusion: The study successfully modeled complex bio-nanofluid behavior and identified key parameter effects on various profiles, showing good agreement with previous research.

Abstract: A viscous, incompressible, micropolar bio-nanofluid flowing across a
stretching sheet in three dimensions while being driven to convect several slip
boundaries in the presence of a magnetic field was studied. With the assistance
of the relevant transformations, a mathematical model is presented. The finite
difference method numerically solves the converted non-linear ordinary
differential equations. A comprehensive assessment was conducted to examine the
impact of governing parameters on dimensionless velocity, micro-rotation,
temperature, nanoparticle volume fraction, microorganisms, and heat transfer
rate. The findings of this investigation showed a strong correlation when
compared to previous studies, indicating a high level of agreement and
consistency between the results. The study's conclusions indicate that the
velocity profile increases with higher values of {\lambda} and {\delta}, while
it decreases with higher values of A, {\delta}v, and M. The micro-rotation
profile F({\eta}) drops as the spin gradient viscosity parameter rises, but
G({\eta}) increases. The temperature profile decreases with higher Prandtl
numbers but increases with higher thermophoresis parameters. The concentration
profile decreases with higher Schmidt numbers and Brownian motion parameters.
The microorganism profile increases with higher Peclet numbers and
microorganism slip parameters but decreases with higher bio-convection Schmidt
numbers. Lastly, the local Nusselt number grows with increasing values of the
stretching parameter {\lambda}.

</details>


### [72] [Analytical and numerical investigation of heat transfer of porous fin in a local thermal non-equilibrium state](https://arxiv.org/abs/2510.11157)
*Payam Jalili,Salar Ghadiri Alamdari,Bahram Jalili,Amirali Shateri,Davood Domiri Ganji*

Main category: physics.flu-dyn

TL;DR: Analysis of heat transfer in porous fins using LTNE model with natural convection and radiation effects, solved by AGM method and validated against numerical methods.


<details>
  <summary>Details</summary>
Motivation: To understand heat transfer phenomena in porous fins considering natural convection and radiation effects, which is important for thermal management applications.

Method: Used local thermal non-equilibrium (LTNE) model with Darcy model for infiltration velocity and Boussinesq approximation for buoyancy. Applied Akbari-Ganji method (AGM) to solve governing equations and validated with finite difference and finite element methods.

Result: Decreasing Rayleigh and Biot numbers reduce solid phase temperature profiles. Low Rayleigh with high Biot reduces temperature difference between phases. Higher thermal conductivity ratio and dimensionless thickness increase solid phase temperatures. Nusselt number decreases with lower thermal conductivity ratio but increases with higher Rayleigh/Biot numbers and external radiation.

Conclusion: The LTNE model effectively captures heat transfer behavior in porous fins, with key parameters (Rayleigh, Biot, thermal conductivity ratio, dimensionless thickness) significantly influencing temperature profiles and Nusselt number.

Abstract: This research employs a local thermal non-equilibrium (LTNE) model to analyze
the heat transfer phenomenon through a porous fin, considering natural
convection and radiation effects. The infiltration velocity within the porous
medium is evaluated using the Darcy model, and buoyancy effects are accounted
for using the Boussinesq approximation. The Akbari-Ganji method (AGM) is
applied to address the governing energy equations. The accuracy of the proposed
solution is verified by comparing it with numerical results obtained from the
finite difference method (FDM), the finite element method (FEM), and earlier
investigations. The results are presented regarding the total average Nusselt
number and temperature profiles. These results shed light on the influence of
several important parameters, such as the thermal conductivity ratio,
dimensionless thickness, convectional heat transfer, and external and internal
radiation. The analysis reveals that decreasing Rayleigh and Biot numbers
reduces the temperature profiles of the solid phase. Additionally, when the
Rayleigh number is low but the assigned Biot number is high, the temperature
difference between the solid and fluid phases diminishes. Furthermore,
increased thermal conductivity ratio and dimensionless thickness for assigned
Biot and Rayleigh numbers lead to higher solid phase temperatures. The Nusselt
number exhibits a decreasing trend with a decreasing thermal conductivity ratio
but increases with higher Rayleigh and Biot numbers and increased external
radiation.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [73] [Spectrum of the wave equation with Dirac damping on a compact star graph](https://arxiv.org/abs/2510.09829)
*Mikuláš Kučera*

Main category: math.SP

TL;DR: Analysis of wave equation with Dirac damping on compact intervals and star graphs, focusing on spectral properties and Riesz basis criteria.


<details>
  <summary>Details</summary>
Motivation: To understand how Dirac damping affects the spectral properties of wave equations and establish criteria for Riesz basis properties of root vectors.

Method: Using zeroes of an entire function to determine spectrum, analyzing spectral changes at critical damping values, and generalizing to compact star graphs.

Result: Found that spectrum is determined by entire function zeroes, identified critical damping values causing spectral changes, and established Riesz basis criteria for arbitrary damping placement.

Conclusion: The study provides comprehensive understanding of Dirac damping effects on wave equations, with applications to star graphs and identification of critical damping behavior.

Abstract: We consider the wave equation with a distributional Dirac damping and
Dirichlet boundary conditions on a compact interval. It is shown that the
spectrum of the corresponding wave operator is fully determined by zeroes of an
entire function. Consequently, a considerable change of spectral properties is
shown for certain critical values of the damping parameter. We also derive a
definitive criterion for the Riesz basis property of the root vectors for an
arbitrary placement of a complex-valued Dirac damping. Finally, we consider a
generalisation of the problem for compact star graphs and provide insight into
the essence of the critical damping constant.

</details>


### [74] [Spectral and Dynamical Analysis of Fractional Discrete Laplacians on the Half-Lattice](https://arxiv.org/abs/2510.10680)
*Nassim Athmouni*

Main category: math.SP

TL;DR: Analysis of discrete fractional Laplacians on half-lattices showing they share essential spectrum with full-lattice operators, with applications to spectral theory and transport bounds.


<details>
  <summary>Details</summary>
Motivation: To understand how discrete fractional Laplacians behave on half-lattices compared to full lattices, particularly regarding spectral properties and dynamics.

Method: Expressed half-lattice operator as boundary restriction of full-lattice operator plus bounded correction, used conjugate-operator method for Mourre estimates, and applied limiting absorption principle.

Result: Half-lattice operators share essential spectrum and threshold structure with full-lattice operators, with strict Mourre estimates ensuring absence of singular continuous spectrum and finite eigenvalues.

Conclusion: Boundary corrections are relatively compact, preserving interior-energy spectral and dynamical results from full lattice to half-lattice without modification.

Abstract: We investigate discrete fractional Laplacians defined on the half-lattice in
several dimensions, allowing possibly different fractional orders along each
coordinate direction. By expressing the half-lattice operator as a boundary
restriction of the full-lattice one plus a bounded correction that is
relatively compact with respect to it, we show that both operators share the
same essential spectrum and the same interior threshold structure. For
perturbations by a decaying potential, the conjugate-operator method provides a
strict Mourre estimate on any compact energy window inside the continuous
spectrum, excluding threshold points. As a consequence, a localized Limiting
Absorption Principle holds, ensuring the absence of singular continuous
spectrum, the finiteness of eigenvalues, and weighted propagation (transport)
bounds. The form-theoretic construction also extends naturally to negative
fractional orders. Overall, the relative compactness of the boundary correction
guarantees that the interior-energy spectral and dynamical results obtained on
the full lattice remain valid on the half-lattice without modification.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [75] [Construction of harmonic coordinates for weak immersions](https://arxiv.org/abs/2510.10601)
*Dorian Martino,Tristan Rivière*

Main category: math.DG

TL;DR: The paper proves that weak immersions in critical Sobolev spaces in even dimensions ≥4 have global harmonic coordinates when the second fundamental form is small, generalizing a 2D result and providing tools for analyzing scale-invariant Lagrangians.


<details>
  <summary>Details</summary>
Motivation: To generalize Müller-Sverak's famous 2D result about harmonic coordinates to arbitrary even dimensions ≥4, and provide tools for analyzing scale-invariant Lagrangians of immersions like the Graham-Reichert functional.

Method: The authors prove existence of global harmonic coordinates for weak immersions in critical Sobolev spaces W^{n/2+1,2} in even dimensions n≥4, under smallness conditions on the second fundamental form in W^{n/2-1,2}.

Result: Established that weak immersions in critical Sobolev spaces with small second fundamental form admit global harmonic coordinates in even dimensions ≥4, and obtained a general local existence theorem for harmonic coordinates for metrics with Riemann tensor in L^p for p>n/2 in any dimension n≥3.

Conclusion: The main result provides a generalization of harmonic coordinate existence to higher even dimensions, with applications to scale-invariant Lagrangian analysis and intrinsic geometric analysis.

Abstract: We prove that any weak immersion in the critical Sobolev space
$W^{\frac{n}{2}+1,2}(\mathbb{R}^n;\mathbb{R}^d)$ in even dimension $n\geq 4$,
has global harmonic coordinates if its second fundamental form is small in the
Sobolev space $W^{\frac{n}{2}-1,2}(\mathbb{R}^n;\mathbb{R}^d)$. This is a
generalization to arbitrary even dimension $n\ge 4$ of a famous result of
M\"uller--Sverak \cite{muller1995} for $n=2$. The existence of such coordinates
is a key tool used by the authors in \cite{MarRiv20252} for the analysis of
scale-invariant Lagrangians of immersions, such as the Graham--Reichert
functional. From a purely intrinsic perspective, the proof of the main result
leads to a general local existence theorem of harmonic coordinates for general
metrics with Riemann tensor in $L^p$ for any $p>n/2$ in any dimension $n\geq
3$.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [76] [Studying the properties of reconnection-driven turbulence](https://arxiv.org/abs/2510.09978)
*Shi-Min Liang,Jian-Fu Zhang,Na-Na Gao,Nian-Yu Yi*

Main category: astro-ph.HE

TL;DR: Study investigates statistical properties of turbulent magnetic reconnection using numerical simulations, revealing distinct spectral characteristics and cascade behaviors compared to pure MHD turbulence.


<details>
  <summary>Details</summary>
Motivation: Current understanding of turbulent magnetic reconnection remains insufficient despite its ubiquity in astrophysical environments, requiring detailed statistical analysis of reconnection turbulence properties.

Method: Used AMUN open-source software to perform numerical simulations of turbulent magnetic reconnection, analyzed using traditional statistical methods including power spectrum and structure function analysis.

Result: Found velocity spectrum follows Kolmogorov type (E∝k^{-5/3}) while magnetic field spectrum is steeper; anisotropy cascade except with guide field; incompressible turbulence in adiabatic state; stronger velocity intermittency than magnetic field.

Conclusion: Steep magnetic field spectrum with Kolmogorov velocity spectrum characterizes reconnection turbulence, with guide field altering cascade isotropy, providing new insights for advancing self-driven reconnection theory.

Abstract: Magnetic reconnection, often accompanied by turbulence interaction, is a
ubiquitous phenomenon in astrophysical environments. However, the current
understanding of the nature of turbulent magnetic reconnection remains
insufficient. We investigate the statistical properties of reconnection
turbulence in the framework of the self-driven reconnection. Using the
open-source software package AMUN, we first perform numerical simulations of
turbulent magnetic reconnection. We then obtain the statistical results of
reconnection turbulence by traditional statistical methods such as the power
spectrum and structure function. Our numerical results demonstrate: (1) the
velocity spectrum of reconnection turbulence follows the classical Kolmogorov
type of $E\propto k^{-5/3}$, while the magnetic field spectrum is steeper than
the Kolmogorov spectrum, which are independent of limited resistivity, guide
field, and isothermal or adiabatic fluid states; (2) most of the simulations
show the anisotropy cascade, except that the presence of a guide field leads to
an isotropic cascade; (3) reconnection turbulence is incompressible in the
adiabatic state, with energy distribution dominated by the velocity solenoidal
component; (4) different from pure magnetohydrodynamic (MHD) turbulence, the
intermittency of the velocity field is stronger than that of the magnetic field
in reconnection turbulence. The steep magnetic field spectrum, together with
the velocity spectrum of Kolmogorov type, can characterize the feature of the
reconnection turbulence. In the case of the presence of the guide field, the
isotropy of the reconnection turbulence cascade is also different from the
cascade mode of pure MHD turbulence. Our experimental results provide new
insights into the properties of reconnection turbulence, which will contribute
to advancing the self-driven reconnection theory.

</details>


### [77] [Electromagnetic Observables of Weakly Collisional Black Hole Accretion](https://arxiv.org/abs/2510.11365)
*Vedant Dhruv,Ben Prather,Mani Chandra,Abhishek V. Joshi,Charles F. Gammie*

Main category: astro-ph.HE

TL;DR: Weakly collisional fluid simulations with viscosity and heat conduction show similar flow dynamics and imaging results for SgrA* compared to ideal fluid models, but with reduced light curve variability.


<details>
  <summary>Details</summary>
Motivation: To model black holes in hot, collisionless plasma more accurately by incorporating kinetic effects that are typically ignored in ideal fluid models.

Method: Used weakly collisional fluid simulations with viscosity and heat conduction to model leading order kinetic effects, and synthesized images and spectra of SgrA* assuming isotropic thermal electron population.

Result: Flow dynamics and synthesized images remain very similar to ideal fluid predictions, but weakly collisional models show lower light curve variability, especially in magnetically dominated models.

Conclusion: While weakly collisional models produce similar overall results to ideal fluid models, they systematically reduce variability in light curves, particularly for magnetically dominated systems.

Abstract: The black holes in the Event Horizon Telescope sources Messier 87* and
Sagittarius A* (SgrA*) are embedded in a hot, collisionless plasma that is
fully described in kinetic theory yet is usually modeled as an ideal,
magnetized fluid. In this Letter, we present results from a new set of weakly
collisional fluid simulations in which leading order kinetic effects are
modeled as viscosity and heat conduction. Consistent with earlier,
lower-resolution studies, we find that overall flow dynamics remain very
similar between ideal and non-ideal models. For the first time, we synthesize
images and spectra of SgrA* from weakly collisional models -- assuming an
isotropic, thermal population of electrons -- and find that these remain
largely indistinguishable from ideal fluid predictions. However, most weakly
collisional models exhibit lower light curve variability, with all magnetically
dominated models showing a small but systematic decrease in variability.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [78] [Linear Algebra Problems Solved by Using Damped Dynamical Systems on the Stiefel Manifold](https://arxiv.org/abs/2510.10535)
*M Gulliksson,A Oleynik,M Ogren,R Bakhshandeh-Chamazkoti*

Main category: math.OC

TL;DR: New damped dynamical systems method for solving minimization problems on the Stiefel Manifold with constraints satisfied in the limit.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient approach for solving constrained minimization problems on the Stiefel Manifold using dynamical systems theory.

Method: Uses damped dynamical systems where constraints are satisfied in the limit through an additional damped dynamical system.

Result: Method demonstrated through numerical experiments and compared favorably against state-of-the-art conjugate gradient method.

Conclusion: The proposed damped dynamical systems approach provides an effective alternative for Stiefel Manifold optimization problems.

Abstract: We develop a new method for solving minimization problems on the Stiefel
Manifold using damped dynamical systems. The constraints are satisfied in the
limit by an additional damped dynamical system. The method is illustrated by
numerical experiments and compared to a state-of-the-art conjugate gradient
method.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [79] [Disorder to Order Transition in 1D Nonreciprocal Cahn-Hilliard Model](https://arxiv.org/abs/2510.11580)
*Navdeep Rana,Ramin Golestanian*

Main category: cond-mat.soft

TL;DR: The paper studies 1D Nonreciprocal Cahn-Hilliard model with varying nonreciprocity and boundary conditions, revealing transitions from defect-laden states to ordered traveling waves and domain patterns.


<details>
  <summary>Details</summary>
Motivation: To understand how nonreciprocity and boundary conditions affect pattern formation and ordering transitions in one-dimensional nonreciprocal systems.

Method: Extensive numerical study of the 1D Nonreciprocal Cahn-Hilliard model with varying nonreciprocity parameter α and different boundary conditions (periodic, Dirichlet, Neumann).

Result: At small α: defect-laden configurations without global polar order; critical threshold α_c marks transition to ordered states; periodic boundaries yield traveling waves above α_c; Dirichlet/Neumann boundaries show fluctuating domains and eventual domain partitioning at large α.

Conclusion: Nonreciprocity drives transitions between different ordered states, with boundary conditions playing crucial role in determining the nature of ordering and pattern formation in the system.

Abstract: We extensively study the phenomenology of one dimensional Nonreciprocal Cahn
Hilliard model for varying nonreciprocity $(\alpha)$ and different boundary
conditions. At small $\alpha$, a perturbed uniform state evolves to a defect
laden configuration that lacks global polar order. Defects are the sources and
sinks of travelling waves and nonreciprocity selects defects with a unique wave
number that increases monotonically with $\alpha_c$. A critical threshold
$\alpha_c$ marks the onset of a transition to states with finite global polar
order. For periodic boundaries, above $\alpha_c$, the system shows travelling
waves that are completely ordered. In contrast, travelling waves are
incompatible with Dirichlet and Neumann boundaries. Instead, for $\alpha
\gtrsim \alpha_c$, we find fluctuating domains that show intermittent polar
order and at large $\alpha$, the system partitions into two domains with
opposite polar order.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [80] [Predicting Crystal Structures and Ionic Conductivity in Li$_{3}$YCl$_{6-x}$Br$_{x}$ Halide Solid Electrolytes Using a Fine-Tuned Machine Learning Interatomic Potential](https://arxiv.org/abs/2510.09861)
*Jonas Böhm,Aurélie Champagne*

Main category: cond-mat.mtrl-sci

TL;DR: Fine-tuning CHGNet ML potential for Li3YCl6-xBrx solid electrolytes to study ionic transport mechanisms using MD simulations and DFT calculations.


<details>
  <summary>Details</summary>
Motivation: To investigate ionic transport in ternary halide solid electrolytes for solid-state battery applications, particularly the Li3YCl6-xBrx family.

Method: Generated ordered structural models from experimental disordered structures, then used iterative fine-tuning workflow combining molecular dynamics simulations with static DFT calculations.

Result: Improved predictive accuracy in energy predictions, structure optimizations, and diffusion coefficient calculations; analyzed composition effects on ionic conductivity.

Conclusion: The approach is robust for modeling transport properties in complex solid electrolytes, demonstrating effectiveness of fine-tuning universal ML potentials.

Abstract: This work demonstrates the effectiveness of fine-tuning the CHGNet universal
machine learning interatomic potential (uMLIP) to investigate ionic transport
mechanisms in ternary halide solid electrolytes of the
Li$_{3}$YCl$_{6-x}$Br$_{x}$ family (x = 0 to 6), which are promising candidates
for solid-state battery applications. We present a strategy for generating
ordered structural models from experimentally derived disordered
Li$_{3}$YCl$_{6}$ (LYC) and Li$_{3}$YBr$_{6}$ (LYB) structures. These serve as
initial configurations for an iterative fine-tuning workflow that couples
molecular dynamics (MD) simulations with static density functional theory (DFT)
calculations. The fine-tuning process and the resulting improvements in
predictive accuracy are benchmarked across energy predictions, structure
optimizations, and diffusion coefficient calculations. Finally, we analyze the
influence of composition (varied x) on the predicted ionic conductivity in
Li$_{3}$YCl$_{6-x}$Br$_{x}$, demonstrating the robustness of our approach for
modeling transport properties in complex solid electrolytes.

</details>


### [81] [Atomic-Scale Origins of Oxidation Resistance in Amorphous Boron Nitride](https://arxiv.org/abs/2510.10326)
*Onurcan Kaya,Qiushi Deng,Thomas Souvignet,Catherine Marichy,Catherine Journet,Ivan Cole,Stephan Roche*

Main category: cond-mat.mtrl-sci

TL;DR: The oxidation resistance of amorphous boron nitride (α-BN) is determined by its atomic-scale structure, with dense, ordered networks resisting oxidation while porous, defective structures undergo bulk degradation.


<details>
  <summary>Details</summary>
Motivation: Amorphous boron nitride is promising for nanoelectronics but its chemical stability mechanisms are poorly understood, particularly how structure affects oxidation resistance.

Method: Combined machine-learning molecular dynamics simulations and angle-resolved X-ray photoelectron spectroscopy (XPS) to study structure-property relationships in α-BN oxidation.

Result: Dense, chemically ordered α-BN networks with high B-N bond fraction confine oxidation to surfaces, while porous structures with homonuclear B-B/N-N bonds allow oxygen penetration and bulk degradation. Higher-temperature grown films show more ordered structure and superior oxidation resistance.

Conclusion: Oxidation resistance of α-BN is tunable through atomic-scale morphology control, providing a framework for engineering chemically robust dielectric barriers for nanoelectronics.

Abstract: Amorphous boron nitride (\textrm{$\alpha$}-BN) is a promising ultrathin
barrier for nanoelectronics, yet the atomistic mechanisms governing its
chemical stability remain poorly understood. Here, we investigate the
structure-property relationship that dictates the oxidation of
\textrm{$\alpha$}-BN using a combination of machine-learning molecular dynamics
simulations and angle-resolved X-ray photoelectron spectroscopy. The
simulations reveal that the film structure, controlled by synthesis conditions,
is the critical factor determining oxidation resistance. Dense, chemically
ordered networks with a high fraction of B-N bonds effectively resist oxidation
by confining it to the surface, whereas porous, defect-rich structures with
abundant homonuclear B-B and N-N bonds permit oxygen penetration and undergo
extensive bulk degradation. These computational findings are consistent with
experimental trends observed in \textrm{$\alpha$}-BN films grown by chemical
vapour deposition. XPS analysis shows that a film grown at a higher temperature
develops a more ordered structure with a B/N ratio nearer to stoichiometric and
exhibits superior resistance to surface oxidation compared to its more
defective, lower-temperature counterpart. Together, these results demonstrate
that the oxidation resistance of \textrm{$\alpha$}-BN is a tunable property
directly linked to its atomic-scale morphology, providing a clear framework for
engineering chemically robust dielectric barriers for future nanoelectronic
applications.

</details>


### [82] [Electron-phonon coupling in magnetic materials using the local spin density approximation](https://arxiv.org/abs/2510.11350)
*Á. A. Carrasco Álvarez,M. Giantomassi,J. Lihm,G. E. Allemand,M. Mignolet,M. Verstraete,S. Poncé*

Main category: cond-mat.mtrl-sci

TL;DR: Extension of EPW package for electron-phonon coupling calculations in magnetic materials, validated on Fe and Ni showing suppressed superconductivity and different resistivity mechanisms.


<details>
  <summary>Details</summary>
Motivation: To study electron-phonon interactions in magnetic materials like Fe and Ni, which are crucial for applications in spintronics and data storage, and understand fundamental quantum phenomena.

Method: Extended EPW package using perturbation theory and maximally localized Wannier functions to interpolate electron-phonon matrix elements, enabling dense momentum grid calculations at reasonable computational cost.

Result: Confirmed absence of phonon-driven superconductivity in Fe and Ni (intrinsically suppressed), and found electron-phonon scattering dominates resistivity in Fe but contributes less than one-third in Ni.

Conclusion: The method successfully reveals fundamental differences in transport properties between Fe and Ni, with electron-phonon coupling playing varying roles in resistivity across magnetic materials.

Abstract: Magnetic materials are crucial for manipulating electron spin and magnetic
fields, enabling applications in data storage, spintronics, charge transport,
and energy conversion, while also providing insight into fundamental quantum
phenomena. In numerous applications, the interaction between electrons and
lattice vibrations, known as electron-phonon coupling, can be of significant
importance. In that regard, we extend the EPW package to be able to interpolate
the electron-phonon matrix elements combining perturbation theory and maximally
localized Wannier functions. This allows to use dense momentum grids at a
reasonable computational cost when computing electron-phonon-related quantities
and physical properties. We validate our implementation considering
ferromagnetic iron and nickel, where we explore the absence of phonon-driven
superconductivity, finding that superconductivity is intrinsically suppressed.
Furthermore, we evaluate the carrier resistivity at finite temperatures for
both systems, considering the role of the magnetic phase in carrier transport.
Our findings indicate that in the case of Fe, the primary contributor to
resistivity is electron-phonon scattering. In contrast, for Ni, electron-phonon
scattering constitutes less than one-third of the resistivity, underscoring a
fundamental difference in the transport properties of the two systems.

</details>


### [83] [A continued fraction approximation for the effective elasticity tensor of two-dimensional polycrystals as a function of the crystal elasticity tensor](https://arxiv.org/abs/2510.10297)
*Graeme W. Milton*

Main category: cond-mat.mtrl-sci

TL;DR: The paper shows that the effective elasticity tensor of 2D polycrystals can be approximated using continued fraction expansions similar to those of sequential laminates, and explores whether arbitrary polycrystal responses can be exactly mimicked by sequential laminates.


<details>
  <summary>Details</summary>
Motivation: To understand how the effective elasticity tensor of two-dimensional polycrystals relates to the constituent crystal properties, and whether complex polycrystal microstructures can be approximated or exactly replicated by simpler hierarchical sequential laminate structures.

Method: The approach uses continued fraction expansions to approximate the effective elasticity tensor function C*(C0). Sequential laminates are defined hierarchically, starting from pure crystal rotations and building up through lamination processes. The analysis extends to more general constitutive relations where strain is replaced by gradient fields and stress by divergence-free matrix fields.

Result: The paper demonstrates that continued fraction approximations for arbitrary polycrystal microstructures take a more general form than those of sequential laminates, with additional free parameters. It remains an open question whether these parameters can always be adjusted to exactly match sequential laminate responses.

Conclusion: The analysis establishes that 2D polycrystal elasticity can be approximated using continued fraction methods similar to sequential laminates, but whether arbitrary polycrystal responses can be exactly mimicked by sequential laminates remains unresolved. The framework extends to more general constitutive relations beyond standard elasticity theory.

Abstract: For two-dimensional polycrystals the effective elasticity tensor $C_*$ as a
function $C_*(C_0)$ of the elasticity tensor $C_0$ of the constituent crystal
is considered. It is shown that this function can be approximated by one with a
continued fraction expansion resembling that associated with a class of
microstructure known as sequential laminates. These are hierarchical
microstructures defined inductively. Rank 0 sequential laminates are simply
rotations of the pure crystal. Rank $j$ sequential laminates are obtained by
laminating together, on a length scale much larger that the existing
microstructure and with interfaces perpendicular to some direction $n_j$, rank
$j-1$ sequential laminates with a rotation of the pure crystal. The continued
fraction approximation for arbitrary polycrystal microstructures typically
takes a more general form than that of sequential laminates, but has some free
parameters. It is an open question as to whether these free parameters can
always be adjusted so the continued fraction approximation matches exactly that
of a sequential laminate. If so, one would have established that the elastic
response of two-dimensional polycrystals can always be mimicked by that of
sequential laminates. Our analysis carries over to the more general case where
the strain is replaced by a field $E(x)$ that is the gradient of a vector
potential $u(x)$, i.e. $E=\nabla u$ and the stress is replaced by a matrix
valued field $J(x)$ that need not be symmetric but has zero divergence
$\nabla\cdot J=0$. The tensor $L(x)$ entering the constitutive relation $J=L E$
is locally a rotation of the tensor $L_0$ of the pure crystal that need not
have any special symmetries and has 16 independent tensor elements.

</details>


### [84] [Ab-initio calculation of magnetic exchange interactions using the spin-spiral method in VASP: Self-consistent versus magnetic force theorem approaches](https://arxiv.org/abs/2510.11603)
*Umit Dogan Daglum,Maria Stamenova,Ersoy Sasioglu,Stefano Sanvito*

Main category: cond-mat.mtrl-sci

TL;DR: Comparative study of self-consistent vs magnetic force theorem approaches for calculating magnetic exchange interactions using spin-spiral method in VASP code.


<details>
  <summary>Details</summary>
Motivation: To evaluate the reliability and accuracy of different computational approaches (self-consistent vs magnetic force theorem) for determining magnetic exchange interactions in various magnetic systems.

Method: Used spin-spiral method in VASP code to compute magnon dispersion relations and extract Heisenberg exchange parameters via Fourier transformation for 3d ferromagnets (Fe, Co, Ni) and Mn-based Heusler compounds.

Result: Self-consistent calculations show excellent agreement with previous data, while MFT approach exhibits systematic deviations - overestimating in high-moment systems (Fe, Mn-Heuslers) and underestimating in low-moment systems (Ni), with discrepancies exceeding several hundred percent.

Conclusion: Self-consistency is crucial for accurate determination of magnetic exchange parameters, providing practical guidance for future first-principles studies of spin interactions.

Abstract: We present an ab initio investigation of magnetic exchange interactions using
the spin-spiral method implemented in the VASP code, with a comparative
analysis of the self-consistent (SC) and magnetic force theorem (MFT)
approaches. Using representative 3d ferromagnets (Fe, Co, Ni) and Mn-based full
Heusler compounds, we compute magnon dispersion relations directly from
spin-spiral total energies and extract real-space Heisenberg exchange
parameters via Fourier transformation. Curie temperatures are subsequently
estimated within both the mean-field and random-phase approximations. The SC
spin-spiral calculations yield exchange parameters and magnon spectra in
excellent agreement with previous theoretical data, confirming their
quantitative reliability across different classes of magnetic systems. In
contrast, the MFT approach exhibits systematic quantitative deviations: it
overestimates spin-spiral energies and exchange couplings in high-moment
systems such as bcc Fe and the Mn-based Heuslers, while underestimating them in
low-moment fcc Ni. The magnitude of these discrepancies increases strongly with
magnetic moment size, exceeding several hundred percent in the high-moment
compounds. These findings underscore the decisive role of self-consistency in
accurately determining magnetic exchange parameters and provide practical
guidance for future first-principles studies of spin interactions and
excitations using the spin-spiral technique.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [85] [Depth One Quantum Alternating Operator Ansatz as an Approximate Gibbs Distribution Sampler](https://arxiv.org/abs/2510.10345)
*Elijah Pelofske*

Main category: quant-ph

TL;DR: This study numerically investigates QAOA's ability to sample from Boltzmann distributions of disordered spin glass models, comparing standard X mixer and Grover mixer at depth one.


<details>
  <summary>Details</summary>
Motivation: To understand the thermal sampling properties of QAOA and its ability to approximate Boltzmann distributions from classical Ising models, specifically disordered spin glasses.

Method: Numerical investigation of QAOA at depth one, examining energy landscapes, Shannon entropy landscapes, and Boltzmann distribution sampling accuracy for both X mixer and Grover mixer across full parameter search space periods.

Result: At high temperatures, one-round Grover mixer QAOA samples Boltzmann distributions more accurately than X mixer QAOA. Both mixers can serve as approximate Boltzmann samplers, with performance heavily dependent on QAOA angle choice.

Conclusion: QAOA at depth one can approximate Boltzmann distribution sampling, with Grover mixer outperforming X mixer at high temperatures, but the quality of approximation depends significantly on parameter selection.

Abstract: This study numerically investigates the thermal sampling properties of QAOA,
the Quantum Alternating Operator Ansatz which was generalized from the original
Quantum Approximate Optimization Algorithm. Specifically, the ability of QAOA
to sample from the Gibbs distribution, equivalently the Boltzmann distribution,
defined by a classical Ising model, specifically a fully connected disordered
spin glass (Sherrington-Kirkpatrick) model. We focus on two different QAOA
mixers; the standard transverse field X mixer, and the Grover mixer. At a QAOA
depth of one we examine, for a single full QAOA parameter search space period,
the energy landscape, the Shannon entropy landscape of the QAOA probability
distribution, and the tradeoff between Boltzmann distribution sampling
temperature and error rate (how close to the true Boltzmann distribution is the
QAOA distribution). We find that at very high temperatures one-round Grover
mixer QAOA can sample from the Boltzmann distribution more accurately than the
standard X mixer QAOA at one round. Both X mixer and Grover mixer depth one
QAOA can serve as approximate Boltzmann distribution samplers, and how good
this approximation is depends heavily on the QAOA angle choice.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [86] [Stochastic and deterministic reaction-diffusion equations](https://arxiv.org/abs/2510.10842)
*Davide A. Bignamini,Paolo De Fazio*

Main category: math.PR

TL;DR: Well-posedness of non-autonomous deterministic and stochastic reaction-diffusion equations, with space-time regularity results for stochastic convolutions.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for non-autonomous reaction-diffusion equations in both deterministic and stochastic settings.

Method: Mathematical analysis and proof techniques for establishing well-posedness and regularity properties.

Result: Proved well-posedness for both deterministic and stochastic non-autonomous reaction-diffusion equations, and obtained space-time regularity results for non-autonomous stochastic convolutions.

Conclusion: The paper successfully establishes the well-posedness and regularity properties for non-autonomous reaction-diffusion equations in deterministic and stochastic frameworks.

Abstract: In this paper, we prove the well-posedness of non-autonomous deterministc and
stochatic reaction-diffiusion equations. Concerning the stochastic problem, we
prove also a result on the space-time regularity of the non-autonomous
stochastic convoltution.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [87] [Mass-Centered GCM Framework in Perturbations of Kerr(-Newman)](https://arxiv.org/abs/2510.10811)
*Allen Juntao Fang,Elena Giorgi,Jingbo Wan*

Main category: gr-qc

TL;DR: Develops a mass-centered GCM framework to overcome limitations in charged black hole stability analysis, replacing transport-based control with sphere-wise vanishing conditions for better handling of electromagnetic-gravitational coupling.


<details>
  <summary>Details</summary>
Motivation: The standard GCM construction breaks down for charged black holes due to electromagnetic-gravitational coupling disrupting the ℓ=1 mode behavior used in vacuum analysis, requiring an alternative approach for Reissner-Nordström and Kerr-Newman stability.

Method: Uses mass-centered GCM hypersurfaces with modified gauge constraints, replacing transport-based center-of-mass control with sphere-wise vanishing conditions on renormalized ℓ=1 modes, forming an elliptic-transport system determined by ℓ=1 basis fixing.

Result: Provides an alternative GCM construction that works for charged black holes and reduces to the vacuum case in the uncharged limit, overcoming the breakdown caused by electromagnetic-gravitational coupling.

Conclusion: The mass-centered GCM framework successfully addresses the limitations of standard GCM in charged settings, enabling progress toward nonlinear stability proofs for Reissner-Nordström and Kerr-Newman spacetimes.

Abstract: The nonlinear stability problem for black hole solutions of the Einstein
equations critically depends on choosing an appropriate geometric gauge. In the
vacuum setting, the use of Generally Covariant Modulated (GCM) spheres and
hypersurfaces has played a central role in the proof of stability for slowly
rotating Kerr spacetime. In this work, we develop an alternative GCM framework,
that we call mass-centered, designed to overcome the breakdown of the standard
GCM construction in the charged case, where electromagnetic-gravitational
coupling destroys the exceptional behavior of the $\ell=1$ mode of the
center-of-mass quantity used in the vacuum analysis. This construction is aimed
at the nonlinear stability of Reissner-Nordstr\"om and Kerr-Newman spacetimes.
Our approach replaces transport-based control of the center-of-mass quantity
with a sphere-wise vanishing condition on a renormalized $\ell=1$ mode,
yielding mass-centered GCM hypersurfaces with modified gauge constraints. The
resulting elliptic-transport system remains determined once an $\ell=1$ basis
is fixed via effective uniformization and provides an alternative construction
in vacuum in the uncharged limit.

</details>


### [88] [Einstein-Maxwell Equations on Mass-Centered GCM Hypersurfaces](https://arxiv.org/abs/2510.10814)
*Allen Juntao Fang,Elena Giorgi,Jingbo Wan*

Main category: gr-qc

TL;DR: The paper develops a mass-centered GCM framework to solve Einstein-Maxwell constraint equations on spacelike hypersurfaces, providing the first step toward nonlinear stability analysis of charged black holes (Reissner-Nordström and Kerr-Newman).


<details>
  <summary>Details</summary>
Motivation: To extend the successful GCM gauge approach from vacuum black hole stability to the charged setting, addressing additional difficulties in the Einstein-Maxwell system for nonlinear stability analysis.

Method: Using mass-centered Generally Covariant Modulated (GCM) spheres and hypersurfaces adapted for the Einstein-Maxwell system, solving constraint equations on spacelike GCM hypersurfaces and controlling geometric quantities via gauge-invariant fields describing coupled gravitational-electromagnetic radiation.

Result: Successfully solved Einstein-Maxwell equations on mass-centered spacelike GCM hypersurfaces, controlling all geometric quantities in terms of seed data corresponding to gauge-invariant radiation fields.

Conclusion: This work establishes the foundational framework for controlling gauge-dependent quantities in the nonlinear stability analysis of charged black hole families, building on previous vacuum results.

Abstract: The resolution of the nonlinear stability of black holes as solutions to the
Einstein equations relies crucially on imposing the right geometric gauge
conditions. In the vacuum case, the use of Generally Covariant Modulated (GCM)
spheres and hypersurfaces has been successful in the proof of stability for
slowly rotating Kerr spacetime. For the charged setting, our companion paper
introduced an alternative mass-centered GCM framework, adapted to the
additional difficulties of the Einstein-Maxwell system.
  In this work, we solve the Einstein-Maxwell equations on such a mass-centered
spacelike GCM hypersurface, which is equivalent to solving the constraint
equations there. We control all geometric quantities of the solution in terms
of some seed data, corresponding to the gauge-invariant fields describing
coupled gravitational-electromagnetic radiation in perturbations of
Reissner-Nordstr\"om or Kerr-Newman, first identified by the second author and
expected to be governed by favorable hyperbolic equations. This provides the
first step toward controlling gauge-dependent quantities in the nonlinear
stability analysis of the Reissner-Nordstr\"om and Kerr-Newman families.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [89] [Understanding the interplay of collagen and myocyte adaptation in cardiac volume overload: a multi-constituent growth and remodeling framework](https://arxiv.org/abs/2510.11670)
*Ludovica Maga,Mathias Peirlinck,Lise Noël*

Main category: physics.med-ph

TL;DR: A hybrid growth and remodeling framework shows collagen degradation controls passive mechanics while myocytes drive eccentric hypertrophy, with synergistic effects accelerating diastolic dysfunction in volume-overloaded hearts.


<details>
  <summary>Details</summary>
Motivation: To understand the relative contributions of collagen and myocyte changes in volume-overload induced cardiac remodeling, as traditional myocyte-only growth models cannot explain organ-scale changes.

Method: Developed a hybrid growth and remodeling framework integrating mixture-based constitutive modeling with kinematic growth formulation to assess multi-constituent contributions.

Result: Collagen dynamics control passive mechanical response, myocytes drive eccentric hypertrophy extent and phenotype, and collagen degradation exacerbates myocyte hypertrophy through synergistic interplay.

Conclusion: This provides mechanistic evidence for the synergistic roles of collagen and myocytes in volume-overload cardiac remodeling, advancing understanding of early compensatory stages toward diastolic dysfunction.

Abstract: Hearts subjected to volume overload (VO) are prone to detrimental anatomical
and functional changes in response to elevated mechanical stretches, ultimately
leading to heart failure. Experimental findings increasingly emphasize that
organ-scale changes following VO cannot be explained by myocyte growth alone,
as traditionally proposed in the literature. Collagen degradation, in
particular, has been associated with left ventricular adaptation in both acute
and chronic stages of VO. These hypotheses remain to be substantiated by
comprehensive mechanistic evidence, and the contribution of each constituent to
myocardial growth and remodeling (G&R) processes is yet to be quantified. In
this work, we establish a hybrid G&R framework in which we integrate a
mixture-based constitutive model with the kinematic growth formulation. This
multi-constituent model enables us to mechanistically assess the relative
contributions of collagen and myocyte changes to alterations in tissue
properties, ventricular dimensions, and growth phenotype. Our numerical results
confirm that collagen dynamics control the passive mechanical response of the
myocardium, whereas myocytes predominantly impact the extent and the phenotype
of eccentric hypertrophy. Importantly, collagen degradation exacerbates myocyte
hypertrophy, demonstrating a synergistic interplay that accelerates left
ventricular progression toward diastolic dysfunction. This work constitutes an
important step towards an integrated characterization of the early compensatory
stages of VO-induced cardiac G&R.

</details>


<div id='math.RA'></div>

# math.RA [[Back]](#toc)

### [90] [Local Rigidity of Quasi--Lie Brackets on Quaternionic Banach Modules and Applications to Nonlinear PDEs](https://arxiv.org/abs/2510.10124)
*Nassim Athmouni*

Main category: math.RA

TL;DR: Local rigidity theorem for quasi-Lie brackets on quaternionic Banach modules, with explicit bilinear correction preserving right H-linearity and restoring exact Lie property.


<details>
  <summary>Details</summary>
Motivation: Bridge quaternionic functional analysis with rigidity theory and develop applications to nonlinear PDEs, including well-posedness and continuation criteria.

Method: Combines radial homotopy operator, controlled Neumann-series inversion, and finite-rank adjustment with explicit operator estimates.

Result: Constructive framework that establishes local rigidity under quantitative control of antisymmetry and Jacobi defects.

Conclusion: Provides concrete applications to nonlinear PDEs with explicit thresholds for local well-posedness and Beale-Kato-Majda continuation criteria.

Abstract: We establish a local rigidity theorem for quasi--Lie brackets on quaternionic
Banach right modules. Under quantitative control of antisymmetry and Jacobi
defects, we construct an explicit bilinear correction that preserves right
$\mathbb{H}$--linearity and restores the exact Lie property. The approach
combines a radial homotopy operator, a controlled Neumann-series inversion, and
a finite-rank adjustment, all with explicit operator estimates. This
constructive framework bridges quaternionic functional analysis with rigidity
theory and yields concrete applications to nonlinear PDEs, including local
well-posedness and Beale--Kato--Majda continuation criteria with explicit
thresholds.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [91] [Performance of Machine Learning Methods for Gravity Inversion: Successes and Challenges](https://arxiv.org/abs/2510.09632)
*Vahid Negahdari,Shirin Samadi Bahrami,Seyed Reza Moghadasi,Mohammad Reza Razvan*

Main category: physics.geo-ph

TL;DR: This paper explores machine learning approaches for 2D gravity inversion, comparing CNN-based direct mapping with generative models (VAEs/GANs) and iterative solvers, finding CNNs provide the most reliable reconstructions.


<details>
  <summary>Details</summary>
Motivation: Gravity inversion is ill-posed and non-unique due to underdetermined systems with more model parameters than measurements, motivating data-driven machine learning approaches to improve inversion accuracy.

Method: Used CNN for direct mapping of gravity anomalies to density fields with customized data structure, employed VAEs and GANs for generative modeling, and tested iterative solvers (GD, GMRES, LGMRES, ICG) to refine CNN initial guesses.

Result: CNN inversion provided the most reliable reconstructions and significantly outperformed previous methods. Generative models showed promise but were unstable, while iterative solvers offered only marginal improvements.

Conclusion: CNN-based approaches are most effective for gravity inversion, while generative models need further development and iterative methods provide limited benefits due to the inherent ill-posedness of the problem.

Abstract: Gravity inversion is the problem of estimating subsurface density
distributions from observed gravitational field data. We consider the
two-dimensional (2D) case, in which recovering density models from
one-dimensional (1D) measurements leads to an underdetermined system with
substantially more model parameters than measurements, making the inversion
ill-posed and non-unique. Recent advances in machine learning have motivated
data-driven approaches for gravity inversion. We first design a convolutional
neural network (CNN) trained to directly map gravity anomalies to density
fields, where a customized data structure is introduced to enhance the
inversion performance. To further investigate generative modeling, we employ
Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs),
reformulating inversion as a latent-space optimization constrained by the
forward operator. In addition, we assess whether classical iterative solvers
such as Gradient Descent (GD), GMRES, LGMRES, and a recently proposed Improved
Conjugate Gradient (ICG) method can refine CNN-based initial guesses and
improve inversion accuracy. Our results demonstrate that CNN inversion not only
provides the most reliable reconstructions but also significantly outperforms
previously reported methods. Generative models remain promising but unstable,
and iterative solvers offer only marginal improvements, underscoring the
persistent ill-posedness of gravity inversion.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [92] [Young functions on varifolds. Part I. Functional analytic foundations](https://arxiv.org/abs/2510.05639)
*Hsin-Chuang Chou*

Main category: math.FA

TL;DR: Study of Young functions as measure-theoretic models for multiple-valued functions, convergence via graph measures, and development of test function spaces for differentiability analysis.


<details>
  <summary>Details</summary>
Motivation: To establish a framework for analyzing convergence of pairs of surfaces and functions, and to prepare for studying differentiability of Young functions in future work.

Method: Using graph measures associated with pairs of measures and Young functions to study convergence, and introducing test function spaces for differentiability analysis.

Result: Obtained a compactness theorem for convergence of pairs of surfaces and functions, and developed foundational test function spaces.

Conclusion: Established a measure-theoretic framework for Young functions that enables convergence analysis and provides the groundwork for studying differentiability in subsequent papers.

Abstract: In this paper, we study Young functions, a measure-theoretic model for
multiple-valued functions, and the convergence of pairs of measures and Young
functions via their associated graph measures. This setting allows us to study
the convergence of pairs of surfaces and functions thereon, and a compactness
theorem is immediate. To develop notions of differentiability of Young
functions in the upcoming papers, we also introduce and investigate several
test function spaces.

</details>


### [93] [On the boundedness of dilation operators in the context of Triebel-Lizorkin-Morrey spaces](https://arxiv.org/abs/2510.11439)
*Marc Hovemann,Markus Weimar*

Main category: math.FA

TL;DR: This paper analyzes dilation operators in Triebel-Lizorkin-Morrey spaces, establishing bounds for operator norms based on different parameter regimes.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of dilation operators in Triebel-Lizorkin-Morrey spaces, which are important function spaces in harmonic analysis and PDEs.

Method: Using Fourier analytic approach to Triebel-Lizorkin-Morrey spaces, proving upper and lower bounds for dilation operator norms, and developing advanced Fourier multiplier theorems.

Result: Found that dilation operator norms behave as λ^(s-d/u) for s>σ_p, with logarithmic corrections at borderline s=σ_p, and as λ^(-d/u) for s<σ_p with p≥1.

Conclusion: The study provides complete characterization of dilation operator behavior in Triebel-Lizorkin-Morrey spaces across different parameter regimes, with applications to Fourier analysis and equivalent norm characterizations.

Abstract: In this paper we study the behavior of dilation operators $ D_\lambda \colon
f \mapsto f(\lambda\,\cdot) $ with $ \lambda > 1 $ in the context of
Triebel-Lizorkin-Morrey spaces $\mathcal{E}^{s}_{u,p,q}(\mathbb{R}^d)$. For
that purpose we prove upper and lower bounds for the operator (quasi-)norm $\|
D_\lambda \,|\, \mathcal{L}\big(\mathcal{E}^s_{u,p,q}(\mathbb{R}^d)\big) \| $.
We show that for $s>\sigma_p $ the operator (quasi-)norm $\| D_\lambda \,|\,
\mathcal{L}\big(\mathcal{E}^s_{u,p,q}(\mathbb{R}^d)\big) \| $ up to constants
behaves as $\lambda^{s - \frac{d}{u}} $. For the borderline case $ s =
\sigma_{p} $ we observe a behavior of the form $\lambda^{\sigma_p-
\frac{d}{u}}$, multiplied with logarithmic terms of $\lambda$ that also depend
on the fine index $q$. For $s < \sigma_{p}$ and $p \geq 1$ we find the relation
$\| D_\lambda \,|\, \mathcal{L}\big(\mathcal{E}^s_{u,p,q}(\mathbb{R}^d)\big) \|
\sim \lambda^{ - \frac{d}{u}}$. The case $s < \sigma_{p}$ and $p < 1$ is
investigated as well. Our proofs are mainly based on the Fourier analytic
approach to Triebel-Lizorkin-Morrey spaces. As byproducts we show an advanced
Fourier multiplier theorem for band-limited functions in the context of Morrey
spaces and derive some new equivalent (quasi-)norms and characterizations of
$\mathcal{E}^{s}_{u,p,q}(\mathbb{R}^d)$.
  Keywords: Dilation Operator, Morrey space, Triebel-Lizorkin-Morrey space,
Fourier multiplier

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [94] [A physics-aware deep learning model for shear band formation around collapsing pores in shocked reactive materials](https://arxiv.org/abs/2510.09670)
*Xinlun Cheng,Bingzhe Chen,Joseph Choi,Yen T. Nguyen,Pradeep Seshadri,Mayank Verma,H. S. Udaykumar,Stephen Baek*

Main category: cs.LG

TL;DR: The paper advances PARCv2, a physics-aware neural network, to model hotspot formation in energetic materials under weak-to-moderate shock loading, demonstrating superior performance in capturing shear band dynamics compared to other physics-informed models.


<details>
  <summary>Details</summary>
Motivation: Modeling shock-to-detonation in energetic materials requires capturing complex physical processes like energy localization at hotspots. Weak-to-moderate shock loading conditions are critical for safe storage and handling but remain underexplored compared to strong shocks.

Method: Improved the PARCv2 (Physics-Aware Recurrent Convolutional Neural Network) architecture to rapidly predict shear localizations and plastic heating. Benchmarked against Fourier neural operator and neural ordinary differential equation models.

Result: PARCv2 demonstrated superior performance in capturing spatiotemporal dynamics of shear band formation compared to other physics-informed models. All models exhibited certain failure modes.

Conclusion: The findings underscore the importance of domain-specific considerations in developing robust AI-accelerated simulation tools for reactive materials.

Abstract: Modeling shock-to-detonation phenomena in energetic materials (EMs) requires
capturing complex physical processes such as strong shocks, rapid changes in
microstructural morphology, and nonlinear dynamics of chemical reaction fronts.
These processes participate in energy localization at hotspots, which initiate
chemical energy release leading to detonation. This study addresses the
formation of hotspots in crystalline EMs subjected to weak-to-moderate shock
loading, which, despite its critical relevance to the safe storage and handling
of EMs, remains underexplored compared to the well-studied strong shock
conditions. To overcome the computational challenges associated with direct
numerical simulations, we advance the Physics-Aware Recurrent Convolutional
Neural Network (PARCv2), which has been shown to be capable of predicting
strong shock responses in EMs. We improved the architecture of PARCv2 to
rapidly predict shear localizations and plastic heating, which play important
roles in the weak-to-moderate shock regime. PARCv2 is benchmarked against two
widely used physics-informed models, namely, Fourier neural operator and neural
ordinary differential equation; we demonstrate its superior performance in
capturing the spatiotemporal dynamics of shear band formation. While all models
exhibit certain failure modes, our findings underscore the importance of
domain-specific considerations in developing robust AI-accelerated simulation
tools for reactive materials.

</details>


### [95] [Scaling Laws and Symmetry, Evidence from Neural Force Fields](https://arxiv.org/abs/2510.09768)
*Khang Ngo,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: Equivariant architectures scale better than non-equivariant models in learning interatomic potentials, with higher-order representations showing superior scaling exponents. Compute-optimal training requires scaling data and model sizes together.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that equivariance matters more at larger scales and that fundamental inductive biases like symmetry should not be left for models to discover, especially as we scale, because they change task difficulty and scaling laws.

Method: Empirical study in geometric task of learning interatomic potentials, analyzing scaling behavior with respect to data, parameters, and compute with architecture-dependent exponents.

Result: Equivariant architectures scale better than non-equivariant models, with higher-order representations having better scaling exponents. Data and model sizes should scale together for compute-optimal training.

Conclusion: Contrary to common belief, fundamental inductive biases like symmetry should not be left for models to discover when scaling, as they significantly impact task difficulty and scaling laws.

Abstract: We present an empirical study in the geometric task of learning interatomic
potentials, which shows equivariance matters even more at larger scales; we
show a clear power-law scaling behaviour with respect to data, parameters and
compute with ``architecture-dependent exponents''. In particular, we observe
that equivariant architectures, which leverage task symmetry, scale better than
non-equivariant models. Moreover, among equivariant architectures, higher-order
representations translate to better scaling exponents. Our analysis also
suggests that for compute-optimal training, the data and model sizes should
scale in tandem regardless of the architecture. At a high level, these results
suggest that, contrary to common belief, we should not leave it to the model to
discover fundamental inductive biases such as symmetry, especially as we scale,
because they change the inherent difficulty of the task and its scaling laws.

</details>


### [96] [Gradient Enhanced Self-Training Physics-Informed Neural Network (gST-PINN) for Solving Nonlinear Partial Differential Equations](https://arxiv.org/abs/2510.10483)
*Narayan S Iyer,Bivas Bhaumik,Ram S Iyer,Satyasaran Changdar*

Main category: cs.LG

TL;DR: The paper proposes a Gradient Enhanced Self-Training PINN (gST-PINN) method that uses a gradient-based pseudo point self-learning algorithm to overcome limitations of traditional PINNs, achieving better accuracy and generalization in solving PDEs.


<details>
  <summary>Details</summary>
Motivation: Traditional Physics-Informed Neural Networks (PINNs) struggle with limited precision, slow training, lack of labeled data, and inadequate handling of multi-physics interactions. These challenges motivate the development of an improved method.

Method: The proposed gST-PINN method introduces a gradient-based pseudo point self-learning algorithm for solving PDEs. It generalizes both standard PINN and Gradient-enhanced PINN (gPINN) approaches and operates in a purely semi-supervised manner.

Result: gST-PINN significantly outperforms standard PINN, achieving MSE on the order of 10^-5 after 18,500 iterations compared to PINN's MSE of 10^-3 for Burgers' equation and 10^-4 for diffusion-sorption equation. The method shows continuous improvement while PINN performance plateaus.

Conclusion: The proposed gST-PINN consistently outperforms standard PINN in all tested cases, demonstrating better generalization and effectiveness in scenarios with low accuracy, convergence issues, and absence of labeled data.

Abstract: Partial differential equations (PDEs) provide a mathematical foundation for
simulating and understanding intricate behaviors in both physical sciences and
engineering. With the growing capabilities of deep learning, data$-$driven
approaches like Physics$-$Informed Neural Networks (PINNs) have been developed,
offering a mesh$-$free, analytic type framework for efficiently solving PDEs
across a wide range of applications. However, traditional PINNs often struggle
with challenges such as limited precision, slow training dynamics, lack of
labeled data availability, and inadequate handling of multi$-$physics
interactions. To overcome these challenging issues of PINNs, we proposed a
Gradient Enhanced Self$-$Training PINN (gST$-$PINN) method that specifically
introduces a gradient based pseudo point self$-$learning algorithm for solving
PDEs. We tested the proposed method on three different types of PDE problems
from various fields, each representing distinct scenarios. The effectiveness of
the proposed method is evident, as the PINN approach for solving the Burgers$'$
equation attains a mean square error (MSE) on the order of $10^{-3}$, while the
diffusion$-$sorption equation achieves an MSE on the order of $10^{-4}$ after
12,500 iterations, with no further improvement as the iterations increase. In
contrast, the MSE for both PDEs in the gST$-$PINN model continues to decrease,
demonstrating better generalization and reaching an MSE on the order of
$10^{-5}$ after 18,500 iterations. Furthermore, the results show that the
proposed purely semi$-$supervised gST$-$PINN consistently outperforms the
standard PINN method in all cases, even when solution of the PDEs are
unavailable. It generalizes both PINN and Gradient$-$enhanced PINN (gPINN), and
can be effectively applied in scenarios prone to low accuracy and convergence
issues, particularly in the absence of labeled data.

</details>


### [97] [Cross-Scale Reservoir Computing for large spatio-temporal forecasting and modeling](https://arxiv.org/abs/2510.11209)
*Nicola Alboré,Gabriele Di Antonio,Fabrizio Coccetti,Andrea Gabrielli*

Main category: cs.LG

TL;DR: A new reservoir computing method using multi-resolution inputs from coarser to finer layers for better spatiotemporal forecasting, tested on Sea Surface Temperature data.


<details>
  <summary>Details</summary>
Motivation: To improve forecasting of high-resolution spatiotemporal datasets by better capturing both local and global dynamics through cross-layer coupling.

Method: Multi-resolution reservoir computing architecture with inputs from coarser to finer layers, using cross-layer coupling between parallel reservoir models.

Result: Outperforms standard parallel reservoir models in long-term forecasting of Sea Surface Temperature data, with optimal network dynamics becoming increasingly linear in each layer.

Conclusion: The proposed multi-resolution approach with cross-layer coupling effectively improves predictive accuracy and reveals that slow modes are propagated to subsequent layers.

Abstract: We propose a new reservoir computing method for forecasting high-resolution
spatiotemporal datasets. By combining multi-resolution inputs from coarser to
finer layers, our architecture better captures both local and global dynamics.
Applied to Sea Surface Temperature data, it outperforms standard parallel
reservoir models in long-term forecasting, demonstrating the effectiveness of
cross-layers coupling in improving predictive accuracy. Finally, we show that
the optimal network dynamics in each layer become increasingly linear,
revealing the slow modes propagated to subsequent layers.

</details>


### [98] [Generative Models for Helmholtz Equation Solutions: A Dataset of Acoustic Materials](https://arxiv.org/abs/2510.09657)
*Riccardo Fosco Gramaccioni,Christian Marinoni,Fabrizio Frezza,Aurelio Uncini,Danilo Comminiello*

Main category: cs.LG

TL;DR: A dataset of 31,000 acoustic materials (HA30K) is introduced for learning Helmholtz equation solutions using deep learning, specifically Stable Diffusion with ControlNet, to accelerate wave propagation simulations.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical solvers like finite element methods are computationally expensive for large-scale or real-time acoustic simulations, creating a need for faster alternatives.

Method: Uses Stable Diffusion with ControlNet to represent acoustic pressure field solutions as images, leveraging GPU parallelization and adjustable diffusion steps for speed-quality trade-offs.

Result: The approach drastically reduces computation time compared to classical solvers while maintaining reasonable accuracy for early-stage research applications.

Conclusion: Deep learning-based methods are valuable for rapid exploration in acoustic material research, where speed is prioritized over absolute accuracy in initial stages.

Abstract: Accurate simulation of wave propagation in complex acoustic materials is
crucial for applications in sound design, noise control, and material
engineering. Traditional numerical solvers, such as finite element methods, are
computationally expensive, especially when dealing with large-scale or
real-time scenarios. In this work, we introduce a dataset of 31,000 acoustic
materials, named HA30K, designed and simulated solving the Helmholtz equations.
For each material, we provide the geometric configuration and the corresponding
pressure field solution, enabling data-driven approaches to learn Helmholtz
equation solutions. As a baseline, we explore a deep learning approach based on
Stable Diffusion with ControlNet, a state-of-the-art model for image
generation. Unlike classical solvers, our approach leverages GPU
parallelization to process multiple simulations simultaneously, drastically
reducing computation time. By representing solutions as images, we bypass the
need for complex simulation software and explicit equation-solving.
Additionally, the number of diffusion steps can be adjusted at inference time,
balancing speed and quality. We aim to demonstrate that deep learning-based
methods are particularly useful in early-stage research, where rapid
exploration is more critical than absolute accuracy.

</details>


### [99] [Deep Neural Networks Inspired by Differential Equations](https://arxiv.org/abs/2510.09685)
*Yongshuai Liu,Lianfang Wang,Kuilin Qin,Qinghua Zhang,Faqiang Wang,Li Cui,Jun Liu,Yuping Duan,Tieyong Zeng*

Main category: cs.LG

TL;DR: This paper reviews deep neural network architectures and dynamic modeling methods inspired by differential equations, covering ODE-based models, SDE-based stochastic models, and their applications in improving interpretability and generalization.


<details>
  <summary>Details</summary>
Motivation: To address persistent challenges in neural networks regarding theoretical understanding, interpretability, and generalization by adopting a differential equations perspective to provide a unified theoretical framework.

Method: Extensive review of deep neural network architectures and dynamic modeling methods inspired by differential equations, including ODE-based deterministic models, SDE-based stochastic models, and regularization techniques, with numerical comparisons.

Result: The review provides comprehensive insights into differential equation-inspired neural network models and their characteristics through numerical comparisons.

Conclusion: Integrating differential equations with deep learning offers promising research directions for developing intelligent computational methods with enhanced interpretability and generalization capabilities.

Abstract: Deep learning has become a pivotal technology in fields such as computer
vision, scientific computing, and dynamical systems, significantly advancing
these disciplines. However, neural Networks persistently face challenges
related to theoretical understanding, interpretability, and generalization. To
address these issues, researchers are increasingly adopting a differential
equations perspective to propose a unified theoretical framework and systematic
design methodologies for neural networks. In this paper, we provide an
extensive review of deep neural network architectures and dynamic modeling
methods inspired by differential equations. We specifically examine deep neural
network models and deterministic dynamical network constructs based on ordinary
differential equations (ODEs), as well as regularization techniques and
stochastic dynamical network models informed by stochastic differential
equations (SDEs). We present numerical comparisons of these models to
illustrate their characteristics and performance. Finally, we explore promising
research directions in integrating differential equations with deep learning to
offer new insights for developing intelligent computational methods that boast
enhanced interpretability and generalization capabilities.

</details>


### [100] [A Unified Framework for Lifted Training and Inversion Approaches](https://arxiv.org/abs/2510.09796)
*Xiaoyu Wang,Alexandra Valavanis,Azhir Mahmood,Andreas Mang,Martin Benning,Audrey Repetti*

Main category: cs.LG

TL;DR: Lifted training methods reformulate neural network training as constrained optimization using penalty terms, enabling distributed optimization, handling non-differentiable activations, and improving training landscape conditioning.


<details>
  <summary>Details</summary>
Motivation: Traditional gradient-based training faces challenges like vanishing/exploding gradients, issues with non-smooth activations, and limited parallelization due to sequential structure.

Method: Unified framework using Bregman distances for lifted training, including Method of Auxiliary Coordinates, Fenchel Lifted Networks, and Lifted Bregman Training. Implemented via block-coordinate descent with accelerated/adaptive optimization and implicit stochastic gradient methods.

Result: Numerical results show lifted Bregman approach is effective and stable compared to conventional training, especially for architectures with proximal activations in standard imaging tasks.

Conclusion: Lifted training provides a viable alternative to traditional methods, enabling distributed optimization, accommodating non-differentiable activations, and improving training stability across various network architectures.

Abstract: The training of deep neural networks predominantly relies on a combination of
gradient-based optimisation and back-propagation for the computation of the
gradient. While incredibly successful, this approach faces challenges such as
vanishing or exploding gradients, difficulties with non-smooth activations, and
an inherently sequential structure that limits parallelisation. Lifted training
methods offer an alternative by reformulating the nested optimisation problem
into a higher-dimensional, constrained optimisation problem where the
constraints are no longer enforced directly but penalised with penalty terms.
This chapter introduces a unified framework that encapsulates various lifted
training strategies, including the Method of Auxiliary Coordinates, Fenchel
Lifted Networks, and Lifted Bregman Training, and demonstrates how diverse
architectures, such as Multi-Layer Perceptrons, Residual Neural Networks, and
Proximal Neural Networks fit within this structure. By leveraging tools from
convex optimisation, particularly Bregman distances, the framework facilitates
distributed optimisation, accommodates non-differentiable proximal activations,
and can improve the conditioning of the training landscape. We discuss the
implementation of these methods using block-coordinate descent strategies,
including deterministic implementations enhanced by accelerated and adaptive
optimisation techniques, as well as implicit stochastic gradient methods.
Furthermore, we explore the application of this framework to inverse problems,
detailing methodologies for both the training of specialised networks (e.g.,
unrolled architectures) and the stable inversion of pre-trained networks.
Numerical results on standard imaging tasks validate the effectiveness and
stability of the lifted Bregman approach compared to conventional training,
particularly for architectures employing proximal activations.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [101] [Lattice Boltzmann method for electromagnetic wave scattering](https://arxiv.org/abs/2510.11042)
*Mohd. Meraj Khan,Sumesh P. Thampi,Anubhab Roy*

Main category: physics.optics

TL;DR: The paper proposes and validates the lattice Boltzmann method (LBM) as a numerical approach for electromagnetic scattering across various size parameters and geometries.


<details>
  <summary>Details</summary>
Motivation: To establish LBM as a viable alternative numerical method for electromagnetic scattering problems, covering different regimes (Rayleigh, Mie, geometric optics) and various geometries.

Method: Systematic validation of LBM through comparison with established reference solutions (Mie theory, Discretized Mie-Formalism) for circular cylinders (PEC and dielectric), dielectric spheres, and non-canonical geometries like hexagonal cylinders.

Result: LBM shows excellent agreement with analytical solutions for circular cylinders and spheres, and accurately captures edge diffraction and sharp-facet effects in non-canonical geometries.

Conclusion: LBM is established as a promising and versatile tool for computational electromagnetics, providing the first systematic benchmarking across 1D, 2D, and 3D configurations.

Abstract: In this paper, we propose the lattice Boltzmann method (LBM) as an
alternative numerical approach for electromagnetic scattering. The method is
systematically validated over a wide range of size parameters, thereby covering
the Rayleigh, Mie, and geometric optics regimes, through comparison with
established reference solutions. For circular cylinders, both perfect
electrically conducting (PEC) and dielectric, LBM results are benchmarked
against analytical Mie theory. For dielectric cylinders, comparisons are
performed over a broad range of relative permittivities to assess accuracy
across different material contrasts. Scattering from dielectric spheres is
likewise compared with exact Mie solutions, showing excellent agreement. To
assess performance for non-canonical geometries, we investigate a hexagonal
dielectric cylinder and validate the results against the Discretized
Mie-Formalism, demonstrating that LBM can accurately capture edge diffraction
and sharp-facet effects. Overall, the study provides the first systematic
benchmarking of LBM for electromagnetic scattering in one-, two-, and
three-dimensional configurations, establishing it as a promising and versatile
tool in computational electromagnetics.

</details>
