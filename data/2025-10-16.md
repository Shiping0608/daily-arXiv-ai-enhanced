<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 17]
- [math.AP](#math.AP) [Total: 18]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [math.PR](#math.PR) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [math.OC](#math.OC) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [math.FA](#math.FA) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A finite element method using a bounded auxiliary variable for solving the Richards equation](https://arxiv.org/abs/2510.13012)
*Abderrahmane Benfanich,Yves Bourgault,Abdelaziz Beljadid*

Main category: math.NA

TL;DR: A finite element method for solving the Richards equation using a bounded auxiliary variable to eliminate unbounded terms, with semi-implicit discretization and Newton's method for nonlinear systems.


<details>
  <summary>Details</summary>
Motivation: To develop a stable and accurate numerical method for modeling infiltration in porous media that can handle both dry and fully saturated zones without requiring regularization techniques.

Method: Introduces a bounded auxiliary variable to eliminate unbounded terms in weak formulation, uses semi-implicit discretization with Newton's method, and applies non-overlapping Schwarz domain decomposition for layered soils.

Result: Numerical experiments show the method maintains stability and accuracy, produces positive solutions even in totally dry zones, and effectively predicts flow dynamics in unsaturated soils using Havercamp and van Genuchten models.

Conclusion: The proposed method successfully eliminates the need for regularization, handles extreme saturation conditions, and demonstrates robust performance for infiltration modeling in layered porous media.

Abstract: The Richards equation, a nonlinear elliptic parabolic equation, is widely
used to model infiltration in porous media. We develop a finite element method
for solving the Richards equation by introducing a new bounded auxiliary
variable to eliminate unbounded terms in the weak formulation of the method.
This formulation is discretized using a semi-implicit scheme and the resulting
nonlinear system is solved using Newton's method. Our approach eliminates the
need of regularization techniques and offers advantages in handling both dry
and fully saturated zones. In the proposed techniques, a non-overlapping
Schwarz domain decomposition method is used for modeling infiltration in
layered soils. We apply the proposed method to solve the Richards equation
using the Havercamp and van Genuchten models for the capillary pressure.
Numerical experiments are performed to validate the proposed approach,
including tests such as modeling flows in fibrous sheets where the initial
medium is totally dry, two cases with fully saturated and dry regions, and an
infiltration problem in layered soils. The numerical results demonstrate the
stability and accuracy of the proposed numerical method. The numerical
solutions remain positive in the presence of totally dry zones. The numerical
investigations clearly demonstrated the capability of the proposed method to
effectively predict the dynamics of flows in unsaturated soils.

</details>


### [2] [Computation of stresses in jammed packings modeled with Tresca friction](https://arxiv.org/abs/2510.13021)
*Frédéric Marazzato,Shankar Venkataramani*

Main category: math.NA

TL;DR: This paper develops a computational method for stress analysis in jammed packings of rigid polygonal cells using Tresca friction law, combining constrained minimization, dual maximization, and finite element reconstruction.


<details>
  <summary>Details</summary>
Motivation: To accurately compute stresses within jammed packings of rigid polygonal cells that follow Tresca friction law, addressing the need for robust numerical methods in granular material analysis.

Method: Introduces constrained minimization of friction energy with non-interpenetration constraints, derives dual maximization problem for interface normal stresses, and uses lowest order Raviart-Thomas finite elements for stress field reconstruction.

Result: Numerical results demonstrate the consistency and robustness of the proposed methodology for stress computation in jammed cell packings.

Conclusion: The developed computational framework provides an effective approach for stress analysis in jammed polygonal cell systems with Tresca friction behavior.

Abstract: This paper is interested in the computation of stresses within jammed
packings of rigid polygonal cells. The cells are considered to follow a Tresca
friction law. First, a constrained minimization problem is introduced where the
friction energy is minimized while enforcing the non-interpenetration of
neighboring cells as inequality constraint. The corresponding dual maximization
problem is then deduced and its solution provides normal stresses at the
interface between cells. Finally, lowest order Raviart-Thomas finite elements
are used to reconstruct a consistent stress field by solving local problems.
Numerical results are presented to showcase the consistency and robustness of
the proposed methodology.

</details>


### [3] [Neural Approximate Inverse Preconditioners](https://arxiv.org/abs/2510.13034)
*Tianshi Xu,Rui Peng Li,Yuanzhe Xi*

Main category: math.NA

TL;DR: A neural network framework learns Green's functions for elliptic PDEs to build efficient inverse preconditioners with near-linear complexity.


<details>
  <summary>Details</summary>
Motivation: To develop data-driven approximate inverse preconditioners for elliptic PDEs that overcome traditional limitations in efficiency and scalability.

Method: Uses adaptive multiscale neural networks to learn Green's functions, with staged training, coarse-grid anchors, and domain decomposition for hierarchical matrix compression.

Result: Preconditioners achieve nearly linear complexity in setup and application while maintaining spectral properties for fast convergence in numerical experiments.

Conclusion: The neural network approach successfully constructs efficient preconditioners for challenging elliptic PDEs with consistent performance improvements.

Abstract: In this paper, we propose a data-driven framework for constructing efficient
approximate inverse preconditioners for elliptic partial differential equations
(PDEs) by learning the Green's function of the underlying operator with neural
networks (NNs). The training process integrates four key components: an
adaptive multiscale neural architecture ($\alpha$MSNN) that captures
hierarchical features across near-, middle-, and far-field regimes; the use of
coarse-grid anchor data to ensure physical identifiability; a
multi-$\varepsilon$ staged training protocol that progressively refines the
Green's function representation across spatial scales; and an overlapping
domain decomposition that enables local adaptation while maintaining global
consistency. Once trained, the NN-approximated Green's function is directly
compressed into either a hierarchical ($\mathcal{H}$-) matrix or a sparse
matrix-using only the mesh geometry and the network output. This geometric
construction achieves nearly linear complexity in both setup and application
while preserving the spectral properties essential for effective
preconditioning. Numerical experiments on challenging elliptic PDEs demonstrate
that the resulting preconditioners consistently yield fast convergence and
small iteration counts.

</details>


### [4] [Solving the BGK Model and Boltzmann equation by Fourier Neural Operator with conservative constraints](https://arxiv.org/abs/2510.13047)
*Boyun Hu,Kunlun Qi*

Main category: math.NA

TL;DR: A Fourier Neural Operator (FNO) framework is proposed to learn the Boltzmann collision operator and BGK model with embedded conservation constraints for improved physical consistency.


<details>
  <summary>Details</summary>
Motivation: The Boltzmann collision operator is challenging to approximate numerically due to its high dimensionality, nonlinear structure, and nonlocal integral form.

Method: FNO-based framework that learns the operator mapping between distribution functions, with conservation constraints embedded in the loss functional to enforce mass, momentum, and energy conservation.

Result: The modified FNO efficiently achieves accurate and physically consistent results across different dimensions, working in both sequence-to-sequence and point-to-point manners without requiring fine discretization or large datasets.

Conclusion: This approach shows promise as a framework for physics-constrained operator learning in kinetic theory and other nonlinear integro-differential equations.

Abstract: The numerical approximation of the Boltzmann collision operator presents
significant challenges arising from its high dimensionality, nonlinear
structure, and nonlocal integral form. In this work, we propose a Fourier
Neural Operator (FNO) based framework to learn the Boltzmann collision operator
and its simplified BGK model across different dimensions. The proposed operator
learning approach efficiently captures the mapping between the distribution
functions in either sequence-to-sequence or point to point manner, without
relying on fine grained discretization and large amount of data. To enhance
physical consistency, conservation constraints are embedded into the loss
functional to enforce improved adherence to the fundamental conservation laws
of mass, momentum, and energy compared with the original FNO framework. Several
numerical experiments are presented to demonstrate that the modified FNO can
efficiently achieve the accurate and physically consistent results,
highlighting its potential as a promising framework for physics constrained
operator learning in kinetic theory and other nonlinear integro-differential
equations.

</details>


### [5] [An Unconditionally Stable Explicit Robin-Robin Partitioned Scheme for Fluid-Structure Interaction](https://arxiv.org/abs/2510.13096)
*Shihan Guo,Ping Lin,Yifan Wang,Xiaohe Yue,Haibiao Zheng*

Main category: math.NA

TL;DR: An explicit partitioned scheme for fluid-structure interaction problems that uses Robin-Robin coupling conditions to enable independent solving of fluid and structure subproblems without staggered coupling or sub-iterations, achieving high computational efficiency and parallel scalability.


<details>
  <summary>Details</summary>
Motivation: To develop a computationally efficient method for fluid-structure interaction problems in modern engineering simulations that eliminates the need for costly sub-iterations and enables parallel computation.

Method: Uses explicit partitioned scheme with Robin-Robin coupling conditions where boundary terms are evaluated from previous-step solutions, allowing fluid and structure subproblems to be solved independently within each time step.

Result: The method demonstrates unconditional stability, accuracy, and superior computational efficiency in numerical experiments, while being inherently free from the added-mass effect.

Conclusion: The proposed explicit scheme shows strong potential for large-scale parallel FSI computations in engineering applications due to its efficiency, stability, and scalability.

Abstract: We propose an explicit partitioned (loosely coupled) scheme for fluid
structure interaction (FSI) problems, specifically designed to achieve high
computational efficiency in modern engineering simulations. The FSI problem
under consideration involves an incompressible viscous fluid, governed by the
Navier-Stokes equations, with a thick linear elastic structure. The scheme
adopts a Robin-Robin coupling condition, evaluating the right-hand side of the
Robin boundary terms at each time step solely from the previous-step solutions.
This explicit scheme allows the fluid and structure subproblems to be solved
entirely independently within each time step, eliminating the need for
staggered coupling or costly sub-iterations, which makes the method highly
efficient and scalable for parallel computation. %More importantly, the
proposed explicit scheme is inherently free from the added-mass effect
guarantees unconditional stability. Various of numerical experiments
demonstrate the stability, accuracy, and superior computational efficiency of
the proposed approach, highlighting its strong potential for large scale
parallel FSI computations in engineering applications.

</details>


### [6] [A Particle-Field Algorithm with Neural Interpolation for a Parabolic-Hyperbolic Chemotaxis System in 3D](https://arxiv.org/abs/2510.13199)
*Jongwon David Kim,Jack Xin*

Main category: math.NA

TL;DR: A neural stochastic interacting particle-field method is developed to solve a Parabolic-Hyperbolic Keller-Segel system for tumor angiogenesis, using particles for density and CNNs for chemoattractant concentration.


<details>
  <summary>Details</summary>
Motivation: To model tumor angiogenesis where tumor cells move towards blood vessels for nutrients, requiring numerical methods to solve PDE-based chemotaxis systems.

Method: Developed NSIPF method with density as empirical particle measures and chemoattractant concentration approximated by convolutional neural networks.

Result: The method successfully computes multi-bump solutions to the PHKS system in three dimensions.

Conclusion: NSIPF provides an effective computational approach for solving complex angiogenesis models using neural network approximations.

Abstract: Tumor angiogenesis involves a collection of tumor cells moving towards blood
vessels for nutrients to grow. Angiogenesis, and in general chemo- taxis,
systems have been modeled using partial differential equations (PDEs) and as
such require numerical methods to approximate their solutions. Here we study a
Parabolic-Hyperbolic Keller-Segel (PHKS) system in three space dimensions. The
model arises in the angiogenesis literature. To compute solutions to the PHKS
system, we develop a neural stochastic interacting particle-field (NSIPF)
method where the density variable is represented as empirical measures of
particles and the field variable (concentration of chemoattractant)
approximated by a convolutional neural network (CNN). We discuss the
performance of NSIPF in computing multi-bump solutions to the system.

</details>


### [7] [How many integrals should be evaluated at least in two-dimensional hyperinterpolation?](https://arxiv.org/abs/2510.13204)
*Maolin Che,Congpei An,Yimin Wei,Hong Yan*

Main category: math.NA

TL;DR: Novel approach combining matrix CUR decomposition with hyperinterpolation to approximate high-dimensional functions efficiently, overcoming the curse of dimensionality.


<details>
  <summary>Details</summary>
Motivation: Traditional Fourier-based hyperinterpolation suffers from exponential growth in coefficients with dimension (curse of dimensionality), requiring more efficient methods.

Method: Two strategies using matrix CUR decomposition: structured index selection for tensor compression and adaptive sampling for optimization, with theoretical error bounds and practical algorithms.

Result: Significantly reduces computational complexity and number of coefficients while maintaining accuracy, as demonstrated in numerical experiments.

Conclusion: Bridges matrix/tensor decomposition with function approximation, providing scalable solution for high-dimensional problems with applications in scientific computing and machine learning.

Abstract: This paper introduces a novel approach to approximating continuous functions
over high-dimensional hypercubes by integrating matrix CUR decomposition with
hyperinterpolation techniques. Traditional Fourier-based hyperinterpolation
methods suffer from the curse of dimensionality, as the number of coefficients
grows exponentially with the dimension. To address this challenge, we propose
two efficient strategies for constructing low-rank matrix CUR decompositions of
the coefficient matrix, significantly reducing computational complexity while
preserving accuracy.
  The first method employs structured index selection to form a compressed
representation of the tensor, while the second utilizes adaptive sampling to
further optimize storage and computation. Theoretical error bounds are derived
for both approaches, ensuring rigorous control over approximation quality.
Additionally, practical algorithms -- including randomized and adaptive
decomposition techniques -- are developed to efficiently compute the CUR
decomposition. Numerical experiments demonstrate the effectiveness of our
methods in drastically reducing the number of required coefficients without
compromising precision.
  Our results bridge matrix/tensor decomposition and function approximation,
offering a scalable solution for high-dimensional problems. This work advances
the field of numerical analysis by providing a computationally efficient
framework for hyperinterpolation, with potential applications in scientific
computing, machine learning, and data-driven modeling.

</details>


### [8] [On Convergence of the Secant Method](https://arxiv.org/abs/2510.13228)
*Yan Tan,Chenhao Ye,Qinghai Zhang,Shubo Zhao*

Main category: math.NA

TL;DR: Rigorous proof of secant method's Q-order convergence and computational efficiency comparison with Newton's method


<details>
  <summary>Details</summary>
Motivation: Most numerical analysis textbooks only briefly mention secant method's convergence order without providing rigorous mathematical proofs

Method: Established rigorous mathematical proof for the Q-order of convergence of the secant method

Result: Provided theoretical comparison of computational efficiency between secant method and Newton's method

Conclusion: The paper fills a gap in numerical analysis literature by providing rigorous convergence proofs for the secant method

Abstract: The secant method, as an important approach for solving nonlinear equations,
is introduced in nearly all numerical analysis textbooks. However, most
textbooks only briefly address the Q-order of convergence of this method, with
few providing rigorous mathematical proofs. This paper establishes a rigorous
proof for the Q-order of convergence of the secant method and theoretically
compares its computational efficiency with that of Newton's method.

</details>


### [9] [Functional tensor train neural network for solving high-dimensional PDEs](https://arxiv.org/abs/2510.13386)
*Yani Feng,Michael K. Ng,Kejun Tang,Zhiwen Zhang*

Main category: math.NA

TL;DR: A functional tensor train neural network (FTTNN) is developed to solve high-dimensional PDEs on non-uniform grids and irregular domains, overcoming limitations of traditional tensor train methods that require uniform grids.


<details>
  <summary>Details</summary>
Motivation: Traditional tensor train decomposition methods for high-dimensional PDEs are limited to uniform grids and regular domains, restricting their application to more complex geometries.

Method: Represent PDE solutions using functional tensor train format with TT-core functions approximated by neural networks, using functional tensor train rank in physics-informed loss functions computed via one-dimensional Gauss quadrature.

Result: FTTNN outperforms Physics Informed Neural Networks (PINN) in solving high-dimensional PDEs on both regular and irregular domains.

Conclusion: The proposed FTTNN method successfully extends tensor train decomposition to handle non-uniform grids and irregular domains while maintaining computational efficiency.

Abstract: Discrete tensor train decomposition is widely employed to mitigate the curse
of dimensionality in solving high-dimensional PDEs through traditional methods.
However, the direct application of the tensor train method typically requires
uniform grids of regular domains, which limits its application on non-uniform
grids or irregular domains. To address the limitation, we develop a functional
tensor train neural network (FTTNN) for solving high-dimensional PDEs, which
can represent PDE solutions on non-uniform grids or irregular domains. An
essential ingredient of our approach is to represent the PDE solutions by the
functional tensor train format whose TT-core functions are approximated by
neural networks. To give the functional tensor train representation, we propose
and study functional tensor train rank and employ it into a physics-informed
loss function for training. Because of tensor train representation, the
resulting high-dimensional integral in the loss function can be computed via
one-dimensional integrals by Gauss quadrature rules. Numerical examples
including high-dimensional PDEs on regular or irregular domains are presented
to demonstrate that the performance of the proposed FTTNN is better than that
of Physics Informed Neural Networks (PINN).

</details>


### [10] [An Enhanced Shifted QR Algorithm for Efficient Eigenvalue Computation of Square Non-Hermitian Matrices](https://arxiv.org/abs/2510.13409)
*Chahat Ahuja,Partha Chowdhury,Subhashree Mohapatra*

Main category: math.NA

TL;DR: Enhanced shifted QR algorithm for non-Hermitian matrix eigenvalues with improved convergence rate and maintained accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing QR algorithms fail to converge early for non-Hermitian matrices, requiring a better approach.

Method: Novel enhanced shifted QR algorithm designed specifically for non-Hermitian matrices.

Result: Significant improvement in convergence rate while maintaining accuracy for all test cases, including mid-large sized matrices and larger 50x50 matrices.

Conclusion: The proposed algorithm successfully addresses convergence issues in non-Hermitian matrices and shows promising results across various matrix sizes.

Abstract: This work presents a novel approach to compute the eigenvalues of
non-Hermitian matrices using an enhanced shifted QR algorithm. The existing QR
algorithms fail to converge early in the case of non-hermitian matrices, and
our approach shows significant improvement in convergence rate while
maintaining accuracy for all test cases. In this work, though our prior focus
will be to address the results for a class mid- large sized non-Hermitian
matrices, our algorithm has also produced significant improvements in the case
of comparatively larger matrices such as 50 x 50 non-Hermitian matrices

</details>


### [11] [Formal Verification of COO to CSR Sparse Matrix Conversion (Invited Paper)](https://arxiv.org/abs/2510.13412)
*Andrew W. Appel*

Main category: math.NA

TL;DR: Machine-checked proof of COO to CSR sparse matrix conversion algorithm correctness


<details>
  <summary>Details</summary>
Motivation: The classic COO to CSR conversion algorithm is concise but has intricate invariants that need formal verification

Method: Bottom-up methodology for deriving invariants from the program, using machine-checked proof

Result: Successfully verified the correctness of the COO to CSR conversion algorithm

Conclusion: The bottom-up approach effectively handles the intricate invariants of sparse matrix conversion algorithms

Abstract: We describe a machine-checked correctness proof of a C program that converts
a coordinate-form (COO) sparse matrix to a compressed-sparse-row (CSR) matrix.
The classic algorithm (sort the COO entries in lexicographic order by
row,column; fill in the CSR arrays left to right) is concise but has rather
intricate invariants. We illustrate a bottom-up methodology for deriving the
invariants from the program.

</details>


### [12] [Mechanizing Olver's Error Arithmetic](https://arxiv.org/abs/2510.13414)
*Max Fan,Ariel E. Kellison,Samuel D. Pollard*

Main category: math.NA

TL;DR: Mechanization of a floating-point rounding error model using relative precision as a metric for error analysis, showing it slightly overapproximates standard models but enables tractable certified error bounds.


<details>
  <summary>Details</summary>
Motivation: To provide a well-defined metric for floating-point error analysis that offers structured error propagation and facilitates certified rounding error bounds.

Method: Formal mechanization of relative precision properties, establishing conversion rules between relative precision and relative error, and demonstrating with inner product example.

Result: Relative precision forms a true metric, slightly overapproximates standard rounding error models, and enables tractable development of certified error bounds.

Conclusion: The relative precision-based rounding error model provides a viable alternative that facilitates certified error analysis while maintaining mathematical rigor.

Abstract: We mechanize the fundamental properties of a rounding error model for
floating-point arithmetic based on relative precision, a measure of error
proposed as a substitute for relative error in rounding error analysis. A key
property of relative precision is that it forms a true metric, providing a
well-defined measure of distance between exact results and their floating-point
approximations while offering a structured approach to propagating error bounds
through sequences of computations. Our mechanization formalizes this property,
establishes rules for converting between relative precision and relative error,
and shows that the rounding error model based on relative precision slightly
overapproximates the standard rounding error model. Finally, we demonstrate,
with a simple example of the inner product of two vectors, that this
alternative model facilitates a tractable approach to developing certified
rounding error bounds.

</details>


### [13] [A domain decomposition approach to pore-network modeling of porous media flow](https://arxiv.org/abs/2510.13429)
*Zhangchengrui Wang,Lei Zhang,Shuyu Sun,Jijing Zhao*

Main category: math.NA

TL;DR: A domain-decomposition pore-network method (DD-PNM) for single-phase Stokes flow in porous media that combines finite-element accuracy with sparse global coupling through interface unknowns.


<details>
  <summary>Details</summary>
Motivation: To develop a method that bridges the gap between accurate finite-element discretizations and efficient pore-network models for modeling Stokes flow in porous media.

Method: Combines finite-element discretizations on body-fitted meshes within pore subdomains with sparse global coupling through interface unknowns. Uses precomputed local Dirichlet-to-Neumann operators and a global Schur-complement system on internal interfaces.

Result: The method establishes solvability and discrete mass conservation of the global system, and constructively recovers classical pore-network models by fitting half-throat conductivities to local Dirichlet-to-Neumann maps.

Conclusion: The proposed DD-PNM provides a principled bridge between mesh-based and network-based frameworks, with numerical results demonstrating validity and effectiveness.

Abstract: We propose a domain-decomposition pore-network method (DD-PNM) for modeling
single-phase Stokes flow in porous media. The method combines the accuracy of
finite-element discretizations on body-fitted meshes within pore subdomains
with a sparse global coupling enforced through interface unknowns. Local
Dirichlet-to-Neumann operators are precomputed from finite-element solutions
for each pore subdomain, enabling a global Schur-complement system defined
solely on internal interfaces. Rigorous mathematical analysis establishes
solvability and discrete mass conservation of the global system. Moreover, we
constructively recover classical pore-network models by fitting half-throat
conductivities to local Dirichlet-to-Neumann maps, providing a principled
bridge between mesh-based and network-based frameworks. Numerical results are
presented to demonstrate the validity and effectiveness of the overall
methodology.

</details>


### [14] [Data-intrinsic approximation in metric spaces](https://arxiv.org/abs/2510.13496)
*Jürgen Dölz,Michael Multerer*

Main category: math.NA

TL;DR: This paper develops a mathematical framework for approximating labeled data using discrete modulus of continuity as a regularity measure, with efficient computation algorithms and connections to covering problems and combinatorial optimization.


<details>
  <summary>Details</summary>
Motivation: To reduce computational burden in data analysis by developing compression and approximation methods for labeled data samples, addressing the need for data-intrinsic regularity measures without structural assumptions.

Method: Uses discrete modulus of continuity as regularity measure, investigates its consistency in infinite data limit, proposes efficient computation algorithm, connects to covering problems and combinatorial optimization, and employs multilevel approximation spaces with Monte Carlo methods for statistical uncertainty.

Result: Developed consistent framework for sample-based approximation theory of labeled data, efficient algorithms for computing discrete modulus of continuity, and validated approach through extensive numerical studies.

Conclusion: The discrete modulus of continuity provides an effective data-intrinsic regularity measure for labeled data approximation, connecting approximation theory to covering problems and combinatorial optimization, with practical feasibility demonstrated through numerical validation.

Abstract: Analysis and processing of data is a vital part of our modern society and
requires vast amounts of computational resources. To reduce the computational
burden, compressing and approximating data has become a central topic. We
consider the approximation of labeled data samples, mathematically described as
site-to-value maps between finite metric spaces. Within this setting, we
identify the discrete modulus of continuity as an effective data-intrinsic
quantity to measure regularity of site-to-value maps without imposing further
structural assumptions. We investigate the consistency of the discrete modulus
of continuity in the infinite data limit and propose an algorithm for its
efficient computation. Building on these results, we present a sample based
approximation theory for labeled data. For data subject to statistical
uncertainty we consider multilevel approximation spaces and a variant of the
multilevel Monte Carlo method to compute statistical quantities of interest.
Our considerations connect approximation theory for labeled data in metric
spaces to the covering problem for (random) balls on the one hand and the
efficient evaluation of the discrete modulus of continuity to combinatorial
optimization on the other hand. We provide extensive numerical studies to
illustrate the feasibility of the approach and to validate our theoretical
results.

</details>


### [15] [On the prospects of interpolatory spline bases for accurate mass lumping strategies in isogeometric analysis](https://arxiv.org/abs/2510.13510)
*Yannis Voet,Espen Sande*

Main category: math.NA

TL;DR: The paper explores restoring interpolation properties in spline bases for isogeometric analysis to enable accurate mass lumping in explicit dynamic structural analysis.


<details>
  <summary>Details</summary>
Motivation: Interpolation is crucial for accurate mass lumping strategies in explicit dynamic analysis, but current isogeometric analysis methods sacrifice interpolation for other desirable properties.

Method: The authors investigate the possibility of restoring interpolation for spline bases within isogeometric analysis framework, drawing inspiration from spectral element methods.

Result: The technique presents both surprises and challenges that need to be critically assessed when implementing interpolatory spline bases.

Conclusion: Restoring interpolation in isogeometric analysis spline bases is a promising approach for mass lumping but comes with significant implementation challenges that require careful consideration.

Abstract: While interpolatory bases such as the Lagrange basis form the cornerstone of
classical finite element methods, they have been replaced in the more general
finite element setting of isogeometric analysis in favor of other desirable
properties. Yet, interpolation is a key property for devising accurate mass
lumping strategies that are ubiquitous in explicit dynamic analyses of
structures. In this article, we explore the possibility of restoring
interpolation for spline bases within isogeometric analysis for the purpose of
mass lumping. Although reminiscent of the spectral element method, this
technique comes with its lot of surprises and challenges, which are critically
assessed.

</details>


### [16] [On preconditioned Riemannian gradient methods for minimizing the Gross-Pitaevskii energy functional: algorithms, global convergence and optimal local convergence rate](https://arxiv.org/abs/2510.13516)
*Zixu Feng,Qinglin Tang*

Main category: math.NA

TL;DR: A unified framework for preconditioned Riemannian gradient methods to minimize Gross-Pitaevskii energy functionals with rotation on Riemannian manifolds, enabling analysis of existing methods and construction of efficient algorithms.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive framework for analyzing and constructing efficient preconditioned Riemannian gradient methods for Gross-Pitaevskii energy minimization with rotation, addressing challenges from phase and rotational invariances.

Method: Proposed a unified framework for preconditioned Riemannian gradient methods, proved energy dissipation and global convergence under mild assumptions, derived a sharp Polyak-Łojasiewicz inequality near minimizers assuming Morse-Bott property, and combined spectral analysis with PL inequality to identify optimal preconditioners.

Result: Established energy dissipation and global convergence, derived sharp PL inequality, characterized local convergence rate via condition number μ/L, identified optimal preconditioner achieving best possible local convergence rate (L-μ)/(L+μ)+ε, and validated results numerically on rotating Bose-Einstein condensates.

Conclusion: The framework enables rigorous analysis of preconditioned Riemannian gradient methods for GP functionals with rotation, provides first rigorous derivation of local convergence rate for such methods with two symmetry structures, and identifies optimal preconditioners validated by numerical experiments.

Abstract: In this article, we propose a unified framework for preconditioned Riemannian
gradient (P-RG) methods to minimize Gross-Pitaevskii (GP) energy functionals
with rotation on a Riemannian manifold. This framework enables comprehensive
analysis of existing projected Sobolev gradient methods and facilitates the
construction of highly efficient P-RG algorithms. Under mild assumptions on the
preconditioner, we prove energy dissipation and global convergence. Local
convergence is more challenging due to phase and rotational invariances.
Assuming the GP functional is Morse-Bott, we derive a sharp Polyak-\L
ojasiewicz (PL) inequality near minimizers. This allows precise
characterization of the local convergence rate via the condition number
$\mu/L$, where $\mu$ and $L$ are the lower and upper bounds of the spectrum of
a combined operator (preconditioner and Hessian) on a closed subspace. By
combining spectral analysis with the PL inequality, we identify an optimal
preconditioner achieving the best possible local convergence rate:
$(L-\mu)/(L+\mu)+\varepsilon$ ($\varepsilon>0$ small). To our knowledge, this
is the first rigorous derivation of the local convergence rate for P-RG methods
applied to GP functionals with two symmetry structures. Numerical experiments
on rapidly rotating Bose-Einstein condensates validate the theoretical results
and compare the performance of different preconditioners.

</details>


### [17] [High order regularization of nearly singular surface integrals](https://arxiv.org/abs/2510.13639)
*J. Thomas Beale,Svetlana Tlupova*

Main category: math.NA

TL;DR: High-order accurate regularization formulas for singular surface integrals in PDEs, enabling standard quadrature without special treatment near singularities.


<details>
  <summary>Details</summary>
Motivation: Need to accurately evaluate surface integrals with singular kernels at points on or near surfaces, particularly for nearly singular cases when surfaces are close together or grid points are near surfaces.

Method: Derive regularization formulas for single/double layer integrals for harmonic functions and Stokes flow with error O(δ^p) where δ is smoothing radius and p=3,5,7. Use δ=κh^q with q<1 to control discretization error.

Result: Achieve predicted convergence order O(h^{pq}) in examples. Obtain high-order accurate grid values for harmonic functions and Stokes flow around translating spheroids.

Conclusion: The regularization method enables efficient high-order accurate evaluation of singular surface integrals using standard quadrature, with applications to harmonic functions and Stokes flow problems.

Abstract: Solutions of partial differential equations can often be written as surface
integrals having a kernel related to a singular fundamental solution. Special
methods are needed to evaluate the integral accurately at points on or near the
surface. Here we derive formulas to regularize the integrals with high
accuracy, using analysis from Beale and Tlupova (Adv. Comput. Math., 2024), so
that a standard quadrature can be used without special care near the
singularity. We treat single or double layer integrals for harmonic functions
or for Stokes flow. The nearly singular case, evaluation at points close to the
surface, can be needed when surfaces are close to each other, or to find values
at grid points near a surface. We derive formulas for regularized kernels with
error $O(\delta^p)$ where $\delta$ is the smoothing radius and $p = 3$, $5$,
$7$. With spacing $h$ in the quadrature, we choose $\delta = \kappa h^q$ with
$q<1$ so that the discretization error is controlled as $h \to 0$. We see the
predicted order of convergence $O(h^{pq})$ in various examples. Values at all
grid points can be obtained from those near the surface in an efficient manner
suggested in A. Mayo (SIAM J. Statist. Comput., 1985). With this technique we
obtain high order accurate grid values for a harmonic function determined by
interfacial conditions and for the pressure and velocity in Stokes flow around
a translating spheroid.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [18] [The local regularity theory for the Stokes and Navier--Stokes equations near the curved boundary](https://arxiv.org/abs/2510.13028)
*Hui Chen,Su Liang,Tai-Peng Tsai*

Main category: math.AP

TL;DR: This paper studies local regularity of Stokes equations solutions near curved boundaries under no-slip or Navier boundary conditions, extending flat boundary estimates to curved boundaries with low regularity assumptions.


<details>
  <summary>Details</summary>
Motivation: To extend previous boundary regularity estimates from flat boundaries to curved boundaries, which is more complex and requires new techniques due to boundary straightening perturbations.

Method: Uses new techniques including 'normal form' after mollification with even-even-odd extension, recovering vertical derivative estimates from horizontal ones, and transferring temporal derivatives to spatial derivatives to handle higher order perturbation terms from boundary straightening.

Result: Successfully extends boundary estimates to curved boundaries under low regularity assumptions and proposes a new definition of boundary regular points for Navier-Stokes equations.

Conclusion: The developed techniques enable analysis of Stokes equations near curved boundaries and provide a new framework for defining boundary regular points that ensures higher spatial regularity in incompressible Navier-Stokes equations.

Abstract: In this paper, we study local regularity of the solutions to the Stokes
equations near a curved boundary under no-slip or Navier boundary conditions.
We extend previous boundary estimates near a flat boundary to that near a
curved boundary, under very low starting regularity assumptions. Compared with
the flat case, the proof for the curved case is more complicated and we adapt
new techniques such as the ``normal form" after the mollification with
even-even-odd extension, recovering vertical derivative estimates from
horizontal derivative estimates, and transferring temporal derivatives to
spatial derivatives, to deal with the higher order perturbation terms generated
by boundary straightening. As an application, we propose a new definition of
boundary regular points for the incompressible Navier--Stokes equations that
guarantees higher spatial regularity.

</details>


### [19] [Super Log-concavity of the First Eigenfunctions for Horo-convex Domains in Hyperbolic Space](https://arxiv.org/abs/2510.13072)
*Guofang Wei,Ling Xiao*

Main category: math.AP

TL;DR: The paper proves that the first eigenfunction of the Laplacian for horo-convex domains in hyperbolic space is super log-concave when the domain diameter is small, and shows this result is optimal with counterexamples.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric properties of eigenfunctions in hyperbolic space and establish conditions under which the first eigenfunction exhibits super log-concavity.

Method: Mathematical proof using geometric analysis techniques for horo-convex domains in hyperbolic space, with diameter constraints.

Result: Proved that the first eigenfunction is super log-concave for horo-convex domains with small diameter, and constructed counterexamples showing this fails when domains are not horo-convex or have large diameter.

Conclusion: The super log-concavity property holds precisely for horo-convex domains with small diameter in hyperbolic space, making the result optimal.

Abstract: In this paper, we prove that the first eigenfunction of the Laplacian for a
horo-convex domain $\Omega\subset\mathbb H^n$ is super log-concave when
$\text{diam}(\Omega)$ is not large. Our result is optimal in the sense that
there are counterexamples %are constructed for the cases when $\Omega$ is not
horo-convex or when $\text{diam}(\Omega)$ is large respectively

</details>


### [20] [Enhanced dissipation and Taylor dispersion by a parallel shear flow in an infinite cylinder with unbounded cross section](https://arxiv.org/abs/2510.13097)
*Te Li,Le Zhang*

Main category: math.AP

TL;DR: Study of passive scalar advection in infinite cylinder with unbounded cross section, showing enhanced dissipation occurs only for high frequencies while low frequencies exhibit Taylor dispersion decay.


<details>
  <summary>Details</summary>
Motivation: To understand long-time behavior of passive scalars in shear flows with small viscosity, particularly distinguishing between high and low frequency regimes in unbounded domains.

Method: Mathematical analysis of passive scalar advection using Fourier analysis in infinite cylinders with unbounded cross sections, considering viscosity regimes ν ≪ 1.

Result: Enhanced dissipation occurs only for high frequencies (ν ≤ |k|), while low frequencies (|k| ≤ ν) show Taylor dispersion decay. A non-degeneracy condition at infinity is required for unbounded cross sections.

Conclusion: The behavior differs significantly from bounded cross sections, with frequency-dependent dissipation regimes and additional conditions needed for unbounded domains.

Abstract: In this paper, we investigate the long-time behavior of a passive scalar
advected by a parallel shear flow in an infinite cylinder with unbounded cross
section, in the regime where the viscosity coefficient satisfies $\nu \ll 1$,
and in arbitrary spatial dimension. Under the assumption of an infinite
cylinder, that is, $x \in \mathbb{R}$, the corresponding Fourier frequency $k$
(often referred to as the streamwise wave number) also ranges over the whole
real line $\mathbb{R}$. In this setting, the enhanced dissipation phenomenon
only occurs for high frequencies $\nu \le |k|$, whereas for low frequencies
$|k| \le \nu$ only the decay of Taylor dispersion appears. It is worth noting
that, in the case where $x \in \mathbb{T}$, the Fourier frequency does not
contain low frequencies near zero, and thus enhanced dissipation occurs for all
nonzero modes. Previously, Coti Zelati and Gallay study in [8] the case of
infinite cylinders with bounded cross sections. For the unbounded case
considered here, we find that a non-degeneracy condition at infinity is also
required.

</details>


### [21] [The local well-posedness, blow-up phenomena and ill-posedness of a new fifth-order Camassa-Holm type equation](https://arxiv.org/abs/2510.13164)
*Yiyao Lian,Zhaoyang Yin*

Main category: math.AP

TL;DR: Analysis of a new fifth-order Camassa-Holm type equation including local well-posedness in Besov spaces, blow-up criteria, precise blow-up time determination, and ill-posedness in critical Sobolev space.


<details>
  <summary>Details</summary>
Motivation: To study the mathematical properties of a newly derived fifth-order Camassa-Holm type equation, including its well-posedness, blow-up behavior, and ill-posedness in critical spaces.

Method: Establish local well-posedness in Besov spaces, derive blow-up criteria, use conservation laws and local boundedness to determine precise blow-up time, and employ norm inflation argument to show ill-posedness in H^{1/2}.

Result: Proved local well-posedness in Besov spaces, obtained blow-up criteria, precisely determined blow-up time, and established ill-posedness in the critical Sobolev space H^{1/2}.

Conclusion: The new fifth-order Camassa-Holm type equation exhibits both well-posedness in Besov spaces and ill-posedness in the critical Sobolev space, with precise blow-up behavior characterized.

Abstract: In this paper, we study a new fifth-order Camassa-Holm type equation derived
by Li \cite{Li.Z}. We firstly establish the local well-posedness in the sense
of Hadamard for the Cauchy problem of the new fifth-order Camassa-Holm type
equation in Besov spaces. Secondly, we obtain blow-up criteria. Building upon
this, by utilizing the conservation laws and establishing local boundedness, we
derive a blow-up result that precisely determines the blow-up time. Finally,
the ill-posedness of the new fifth-order Camassa-Holm type equation in the
critical Sobolev space $H^{\frac{1}{2}}$ is established via a norm inflation
argument.

</details>


### [22] [The local well-posedness, global existence and ill-posedness for the fifth order Camassa-Holm model](https://arxiv.org/abs/2510.13165)
*Xiaoxin Chen,Zhaoyang Yin*

Main category: math.AP

TL;DR: The paper studies the fifth order Camassa-Holm model, improving local well-posedness, establishing blow-up criteria and global existence conditions, and proving ill-posedness in specific Besov spaces.


<details>
  <summary>Details</summary>
Motivation: To extend and improve the understanding of the fifth order Camassa-Holm model's well-posedness properties and blow-up behavior, building on previous work by Tang & Liu (2015) and FOCH (2021).

Method: Mathematical analysis of the fifth order Camassa-Holm equation, including improvement of local well-posedness results, derivation of blow-up criteria and global existence conditions, and investigation of ill-posedness through norm inflation analysis.

Result: Improved local well-posedness results, established blow-up criteria and global existence conditions, and proved ill-posedness in Besov spaces B^1_{∞,1} and B^{3/2}_{2,q} (for q∈(1,∞]) when b=5/3.

Conclusion: The fifth order Camassa-Holm model exhibits both well-posed and ill-posed behavior depending on the parameter b and function spaces, with significant implications for understanding its mathematical properties.

Abstract: In this paper, we consider the fifth order Camassa-Holm model. Firstly, we
improve the local well-posedness results in \cite{TangLiu2015,FOCH2021}.
Secondly, we give the blow up criteria and conditions for global existence.
Finally, when $b=\frac 53$ in the model, we obtain the ill-posedness in
$B^1_{\infty,1}$ and $B^{\frac 32}_{2,q}$ with $q\in(1,+\infty]$ in the sense
of norm inflation.

</details>


### [23] [Asymptotic stability of the Kolmogorov flow at high Reynolds numbers](https://arxiv.org/abs/2510.13181)
*Qi Chen,Hao Jia,Dongyi Wei,Zhifei Zhang*

Main category: math.AP

TL;DR: The paper proves asymptotic stability of Kolmogorov flow on non-square tori for perturbations with H^3 norm smaller than ν^{1/3}, revealing rich dynamical behavior and multiple time scales during transition.


<details>
  <summary>Details</summary>
Motivation: Kolmogorov flows are important metastable states in 2D Navier-Stokes equations at high Reynolds numbers, but stability analysis is challenging due to weakened enhanced dissipation from critical points.

Method: Established sharp vorticity depletion estimates near critical points, combined enhanced dissipation, inviscid damping, and vorticity depletion with quasilinear approximation and multiple-timescale analysis.

Result: Achieved ν^{1/3} stability threshold (same as Couette flow) despite weaker enhanced dissipation, showing rapid convergence to nearby shear flow before settling to Kolmogorov flow.

Conclusion: This is the first application of vorticity depletion estimates to improve nonlinear asymptotic stability thresholds in incompressible fluid equations, with expected sharpness in Sobolev spaces.

Abstract: In this paper we prove the asymptotic stability of the Kolmogorov flow on a
non-square torus for perturbations $\omega_0$ satisfying
$\|\omega_0\|_{H^3}\ll\nu^{1/3}$, where $0<\nu\ll1$ is the viscosity.
Kolmogorov flows are important metastable states to the two dimensional
incompressible Navier Stokes equations in the high Reynolds number regime. Our
result shows that the perturbed solution will rapidly converge to a shear flow
close to the Kolmogorov flow, before settling down to the Kolmogorov flow and
slowly decaying to $0$ as $t\to\infty$. In fact, our analysis reveals several
interesting time scales and rich dynamical behavior of the perturbation in the
transition period $0<t\leq 1/\nu$.
  The threshold $\nu^{1/3}$, which is the same as that for the Couette flow, is
quite surprising since one of the key stability mechanisms, enhanced
dissipation, becomes considerably weaker in the case of Kolmogorov flows due to
the presence of critical points. To overcome this essential new difficulty, we
establish sharp vorticity depletion estimates near the critical points to
obtain improved decay rates for the vorticity and velocity fields that are
comparable with those for Couette flows, at least for our purposes. We then
combine these estimates (enhanced dissipation, inviscid damping and vorticity
depletion) with a quasilinear approximation scheme and a multiple-timescale
analysis naturally adapted to the dynamics of the perturbation, to obtain the
$\nu^{1/3}$ threshold for dynamic stability of Kolmogorov flows. The threshold
is expected to be sharp when the perturbation is considered in Sobolev spaces.
This appears to be the first result that applies vorticity depletion estimates
to improve thresholds for nonlinear asymptotic stability in incompressible
fluid equations.

</details>


### [24] [Infinitely many solutions for the prescribed scalar curvature problem with volcano-like curvature](https://arxiv.org/abs/2510.13239)
*Tuoxin Li,Juncheng Wei,Haidong Yang*

Main category: math.AP

TL;DR: Existence of infinitely many positive solutions for prescribed scalar curvature problem with volcano-like potential function K(x), including non-degeneracy results.


<details>
  <summary>Details</summary>
Motivation: To study the prescribed scalar curvature problem with non-radial volcano-like potential functions and establish existence of infinitely many solutions, improving previous results.

Method: Analysis of the prescribed scalar curvature equation with volcano-like positive function K(x) having specific expansion properties, proving existence through construction methods.

Result: Proved existence of infinitely many positive solutions and showed that previously constructed solutions are non-degenerate in the entire D^{1,2} space.

Conclusion: First result of infinitely many solutions for prescribed scalar curvature problem with non-radial potential, with improved non-degeneracy results compared to previous work.

Abstract: In this paper, we consider the following prescribed scalar curvature problem:
  \begin{equation*}
  -\Delta u = K(x) u^{\frac{n+2}{n-2}}, \quad u>0\quad\hbox{in}\quad
\mathbb{R}^n, \quad
  u \in D^{1,2}(\mathbb{R}^n),
  \end{equation*}
  where $K(x)$ is a volcano-like positive function such that
  $$ K(x)= K(r_0)- c_0 | |x|- r_0|^m + O( | |x|- r_0|^{m+\theta}),\quad r_0-
\delta <|x| <r_0+\delta$$
  with $K(r_0), c_0, \delta>0, \theta >2, \min \{\frac{n-2}{2}, 2\} < m< n-2$.
  We first prove the existence of infinitely many positive solutions. A
consequence of our proof yields that the infinitely many solutions constructed
in \cite{WY} are non-degenerate in the whole $D^{1, 2}(\mathbb{R}^{n})$ space.
To our knowledge, it seems to be the first result of infinitely many solutions
of prescribed scalar curvature problem when the potential function $K(x)$ is
not radial. Our non-degeneracy results are also more complete and improve the
result in \cite{GMPS}.

</details>


### [25] [Non variational type critical growth nonlocal system](https://arxiv.org/abs/2510.13242)
*Ashutosh Dixit,Hichem Hajaiej,Tuhina Mukherjee*

Main category: math.AP

TL;DR: This paper studies positive solutions for a system of fractional differential equations with critical Sobolev exponents, establishing existence, uniqueness, and multiplicity results across different parameter regimes.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of positive solutions in fractional differential equation systems with critical nonlinearities, particularly when the system may or may not have variational structure.

Method: Mathematical analysis of a system of fractional differential equations using fractional Laplacian operators, with parameters including fractional critical Sobolev exponents and various coefficient conditions.

Result: The study establishes novel uniqueness and multiplicity results, showing that positive solution sets behave differently across three regimes: p_ij < 2, p_ij = 2, and 2 < p_ij < 2^*_s, with comprehensive characterization of exact solution numbers.

Conclusion: The research provides a complete characterization of positive solution behavior in fractional differential equation systems with critical exponents, revealing distinct solution patterns across different parameter ranges.

Abstract: This study investigates the existence, uniqueness, and multiplicity of
positive solutions for a system of fractional differential equations given by:
\begin{equation*} (-\Delta)^{s_i} u_{i}+\lambda_{i} u_{i}=\sum_{j=1}^{n}
\alpha_{i j}\left|u_{j}\right|^{q_{i j}}\left|u_{i}\right|^{p_{i j}-2} u_{i} ,
u_i\in {\mathscr{D}^{s_i,2}\left(\mathbb{R}^{N}\right)}, i=1,2,\cdots,n,
\end{equation*} where $N>2s=\max\{2s_i\}$, $s_i\in(0,1)$, $n\geq 2$,
$\lambda_{i} \geq 0$, $\alpha _{ij}>0$, $p_{ij}<2^{*}_{s}$, and
$p_{ij}+q_{ij}=2^{*}_{s}=\min\{{\frac{2N}{N-2s_i}\}}$ for $i\neq j \in
\{1,2,...,n\}$. $2^{*}_s$ called the fractional critical sobolev exponent and
$2^{*}_s=2 N /(N-2s)$ for $N > 2s$ and $2^{*}_s=+\infty$ for $N=2s$ or $N<2s$.
Our work establishes novel uniqueness and multiplicity results for positive
solutions, applicable whether the system possesses a variational structure or
not. We provide a comprehensive characterization of the exact number of
positive solutions under specific parameter configurations. Our analysis shows
that the positive solution set behaves differently across three distinct
regimes: $p_{ij}<2$, $p_{ij}=2$, and $2<p_{ij}<2^{*}_{s}$.

</details>


### [26] [The Boltzmann equation on smooth and cylindrical domains with Maxwell boundary conditions](https://arxiv.org/abs/2510.13260)
*Richard Medina Rodriguez*

Main category: math.AP

TL;DR: Study of well-posedness of Boltzmann equation near hydrodynamic limit on bounded domains with various boundary conditions and weight functions.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for the Boltzmann equation near its hydrodynamic limit on bounded domains, addressing different boundary conditions and weight functions for comprehensive analysis.

Method: Used quantitative methods with constructive constants, working with polynomial, stretched exponential and inverse gaussian weights to construct Cauchy theory near equilibrium. Considered C² domains with Maxwell boundary conditions (space-dependent accommodation coefficient) and cylindrical domains with mixed diffusive/specular reflection.

Result: Developed well-posedness theory for Boltzmann equation near hydrodynamic limit on bounded domains with specified boundary conditions and weight functions.

Conclusion: The paper establishes quantitative well-posedness results for Boltzmann equation near hydrodynamic limit on bounded domains, with all constants being constructive and tractable, covering various boundary conditions and weight functions.

Abstract: In this article we study the well-posedness of the Boltzmann equation near
its hydrodynamic limit on a bounded domain. We consider two types of domains,
namely $C^2$ domains with Maxwell boundary conditions where the accommodation
coefficient is a continuous space dependent function $\iota \in [\iota_0,1]$
for any $\iota_0 \in (0,1]$, or cylindrical domains with diffusive reflection
on the bases of the cylinder and specular reflection on the rest of the
boundary. Furthermore, we work with polynomial, stretched exponential and
inverse gaussian weights to construct the Cauchy theory near the equilibrium.
We remark that all methods are quantitative thus all the constants are
constructive and tractable.

</details>


### [27] [Universal Potential Estimates for Mixed Local and Nonlocal Nonlinear Measure Data Problems](https://arxiv.org/abs/2510.13269)
*Lingwei Ma,Qi Xiong,Zhenqiu Zhang*

Main category: math.AP

TL;DR: This paper develops nonlinear potential theory for mixed local and nonlocal p-Laplace equations with measure data, proving universal pointwise estimates for solutions and gradients using Riesz and Wolff potentials.


<details>
  <summary>Details</summary>
Motivation: To establish comprehensive pointwise estimates for solutions of mixed local/nonlocal p-Laplace equations with low regularity coefficients and measure data, addressing both superquadratic and subquadratic cases.

Method: Introduces a novel fractional maximum function that captures both local and nonlocal features, and establishes pointwise estimates for maximum operators of solutions and gradients via Riesz and Wolff potentials.

Result: Universal potential estimates that precisely characterize solution oscillations and identify borderline cases, refining earlier pointwise potential estimates under minimal regularity assumptions.

Conclusion: The developed nonlinear potential theory provides refined universal pointwise estimates that improve upon existing results by handling mixed local/nonlocal operators with low regularity coefficients and measure data.

Abstract: This paper presents the nonlinear potential theory for mixed local and
nonlocal $p$-Laplace type equations with coefficients and measure data,
involving both superquadratic and subquadratic cases. We prove a class of
universal pointwise estimates for the solution and its gradient via Riesz and
Wolff potentials. These are achieved by imposing various low regularity
conditions on the coefficient of the local term, while the kernel coefficient
for the nonlocal term is merely assumed to be measurable. The key to these
proofs lies in introducing a novel fractional maximum function that can capture
both local and nonlocal features simultaneously, and in establishing pointwise
estimates for such maximum operators of the solution and its gradient. Notably,
our universal potential estimates not only precisely characterize the
oscillations of solutions, but also identify the borderline case that bounds
their size, thereby refining the pointwise potential estimates available in
earlier work.

</details>


### [28] [Curvature penalization of strongly anisotropic interfaces models and their phase-field approximation](https://arxiv.org/abs/2510.13275)
*Jean-François Babadjian,Blanche Buet,Michael Goldman*

Main category: math.AP

TL;DR: The paper studies anisotropic interface models with nonconvex surface tension, showing how adding Willmore-type regularization restores lower semicontinuity and enables Γ-convergence results for free discontinuity problems.


<details>
  <summary>Details</summary>
Motivation: Strong anisotropy in interface models can cause instabilities due to lack of lower semicontinuity in the energy functional. The research aims to understand how higher-order regularization can stabilize these systems.

Method: Added a Willmore-type higher-order term to anisotropic nonconvex energies, analyzed two problems: anisotropic nonconvex perimeter and anisotropic nonconvex Mumford-Shah functional, established lower semicontinuity and Γ-convergence via phase field approximation.

Result: Successfully established lower semicontinuity properties and Γ-convergence results for both types of anisotropic nonconvex energies, enabling treatment of singular structures like crack-tips and multiple junctions.

Conclusion: Willmore-type regularization effectively stabilizes anisotropic nonconvex interface models, allowing mathematical analysis of complex singular structures in free discontinuity problems.

Abstract: This paper studies the effect of anisotropy on sharp or diffuse interfaces
models. When the surface tension is a convex function of the normal to the
interface, the anisotropy is said to be weak. This usually ensures the lower
semicontinuity of the associated energy. If, however, the surface tension
depends on the normal in a nonconvex way, this so-called strong anisotropy may
lead to instabilities related to the lack of lower semicontinuity of the
functional. We investigate the regularizing effects of adding a higher order
term of Willmore type to the energy. We consider two types of problems. The
first one is an anisotropic nonconvex generalization of the perimeter, and the
second one is an anisotropic nonconvex Mumford-Shah functional. In both cases,
lower semicontinuity properties of the energies with respect to a natural mode
of convergence are established, as well as $\Gamma$-convergence type results by
means of a phase field approximation. In comparison with related results for
curvature dependent energies, one of the original aspects of our work is that,
in the context of free discontinuity problems, we are able to consider singular
structures such as crack-tips or multiple junctions.

</details>


### [29] [An Allen-Cahn tumor growth model with temperature](https://arxiv.org/abs/2510.13283)
*Stefania Gatti,Erica Ipocoana,Alain Miranville*

Main category: math.AP

TL;DR: Proposes a new non-isothermal Allen-Cahn model for tumor growth derived using microforces approach, with proofs of local and global existence/uniqueness of solutions.


<details>
  <summary>Details</summary>
Motivation: To develop a more comprehensive mathematical model for tumor growth that incorporates thermal effects, extending beyond traditional isothermal approaches.

Method: Derived a non-isothermal Allen-Cahn (Ginzburg-Landau) model using microforces approach, then conducted mathematical analysis of the PDE system's well-posedness.

Result: Successfully proved the existence and uniqueness of both local and global-in-time solutions to the proposed PDE system.

Conclusion: The proposed non-isothermal tumor growth model is mathematically well-posed, establishing a solid foundation for further study of thermal effects in tumor dynamics.

Abstract: In this paper, we propose a new non-isothermal Allen-Cahn (Ginzburg-Landau)
model for tumor growth. After deriving it using a microforces approach, we
study its well-posedness. In particular, we are able to prove the existence and
uniqueness of a local and global-in-time solution to our PDE system.

</details>


### [30] [Bifurcation and multiplicity results for critical Grushin-Choquard problems](https://arxiv.org/abs/2510.13299)
*Suman Kanungo,Pawan Kumar Mishra,Giovanni Molica Bisci*

Main category: math.AP

TL;DR: The paper studies a nonlocal Brézis-Nirenberg type critical Choquard problem involving the Grushin operator, proving bifurcation from eigenvalues and establishing multiplicity results for nontrivial solutions.


<details>
  <summary>Details</summary>
Motivation: To extend the classical Brézis-Nirenberg problem to the nonlocal setting with the Grushin operator, which generalizes the Laplacian and appears in subelliptic PDEs, and to understand the bifurcation behavior and multiplicity of solutions.

Method: The authors analyze a critical Choquard equation with the Grushin operator using bifurcation theory. They consider the problem in bounded domains with Dirichlet boundary conditions and study the behavior near eigenvalues of the Grushin operator.

Result: The main results show that bifurcation occurs from any eigenvalue λ* of the Grushin operator, and in a suitable left neighborhood of λ*, there exist at least twice the multiplicity of λ* nontrivial solutions to the problem.

Conclusion: The paper establishes rigorous bifurcation and multiplicity results for nonlocal critical Choquard problems involving the Grushin operator, extending classical elliptic theory to this more general subelliptic framework.

Abstract: We consider the following nonlocal Br\'ezis-Nirenberg type critical Choquard
problem involving the Grushin operator
  \begin{equation*}
  \left\{
  \begin{aligned}
  -\Delta_\gamma & u =\lambda u + \left(\displaystyle\int_\Omega
\frac{|u(w)|^{2^*_{\gamma, \mu}}}{d(z-w)^\mu}dw\right) |u|^{2^*_{\gamma,
\mu}-2}u \quad &&\text{in} \ \Omega,
  u &= 0 \quad &&\text{on} \, \partial \Omega,
  \end{aligned}
  \right.
  \end{equation*}
  where $\Omega$ is an open bounded domain in $\mathbb{R}^N$, with $N \geq 3$,
and $\lambda >0$ is a parameter. Here, $\Delta_\gamma$ represents the Grushin
operator, defined as
  \[
  \Delta_\gamma u(z) = \Delta_x u(z) +(1+\gamma)^2 |x|^{2\gamma} \Delta_y u(z),
\quad \gamma \geq 0,
  \]
  where $z=(x,y)\in \Omega \subset \mathbb{R}^m\times \mathbb{R}^n$, $m+n=N
\geq 3$ and $2^*_{\gamma,\mu}= \frac{2N_\gamma-\mu}{N_\gamma-2}$ is the Sobolev
critical exponent in the Hardy-Littlewood context with $N_\gamma=
m+(1+\gamma)n$ is the homogeneous dimension associated to the Grushin operator
and $0<\mu<N_\gamma$. The homogeneous norm related to the Grushin operator is
denoted by
  $d(\cdot)$. In this article, we prove the existence of bifurcation from any
eigenvalue $\lambda^*$ of $-\Delta_\gamma$ under Dirichlet boundary conditions.
Furthermore, we show that in a suitable left neighborhood of $\lambda^*$, the
number of nontrivial solutions to the problem is at least twice the
multiplicity of $\lambda^*$.

</details>


### [31] [The Neumann problem for the fractional Laplacian: optimal regularity via the Mellin transform](https://arxiv.org/abs/2510.13340)
*Serena Dipierro,Xavier Ros-Oton,Enrico Valdinoci,Marvin Weidner*

Main category: math.AP

TL;DR: The paper establishes optimal regularity for solutions to the fractional Laplacian Neumann problem using a 1D Liouville theorem and complex analysis methods.


<details>
  <summary>Details</summary>
Motivation: To determine the optimal regularity of solutions to the fractional Laplacian Neumann problem, which is important for understanding the behavior of solutions to nonlocal PDEs with boundary conditions.

Method: Developed a 'meta-theorem' relating 1D solution classification to roots of meromorphic functions, using complex analysis and Mellin transform. Applied this to the fractional Laplacian with Neumann conditions.

Result: Proved solutions are C^{2s+α} when s≤1/2 and C^{s+1/2+α} when s≥1/2. Surprisingly discovered oscillating solutions of form u(x)=x^a cos(b log x) in 1D.

Conclusion: Established optimal regularity bounds for fractional Neumann problems and revealed unexpected oscillatory behavior in 1D solutions, providing new insights into nonlocal PDE theory.

Abstract: We establish the optimal regularity of solutions to the Neumann problem for
the fractional Laplacian, $(-\Delta)^s u=h$ in $\Omega$, with the external
condition $\mathcal N^s u=0$ in $\Omega^c$. For this, a key point is to
establish a 1D Liouville theorem for functions with growth, which we prove by
using complex analysis and the Mellin transform.
  More precisely, we prove a ``meta-theorem'' relating the classification of 1D
solutions to general linear homogeneous equations of the type $Lu=0$ in
$(0,\infty)$ to the (complex) roots of an explicit meromorphic function $f(z)$
that depends on $L$. In case of the fractional Laplacian with Neumann
conditions, we show that all solutions are $C^{2s+\alpha}$ when $s\leq 1/2$,
and $C^{s+\frac12+\alpha}$ when $s\geq1/2$.
  Moreover, quite surprisingly, we prove that even in 1D there exist highly
oscillating solutions of the type $u(x)=x^{a} \cos(b \log x)$ for $x>0$, with
$a>0$ and $b>0$ that depend on $s$, and $a<2s$ for $s\sim1$.

</details>


### [32] [Recovery of a matrix valued potential for the wave equation on stationary spacetimes](https://arxiv.org/abs/2510.13410)
*Spyridon Filippas,Lauri Oksanen,Miika Sarkkinen*

Main category: math.AP

TL;DR: The paper studies the inverse problem of recovering a time-dependent matrix-valued potential on a globally hyperbolic manifold from wave equation measurements, with sufficient conditions for solution when the manifold is stationary and connection term is time-independent.


<details>
  <summary>Details</summary>
Motivation: To develop methods for recovering time-dependent matrix-valued potentials in wave equations on globally hyperbolic manifolds, which has applications in inverse problems and mathematical physics.

Method: Two-step approach: (1) reduction to studying a non-Abelian light ray transform under global hyperbolicity, (2) establishing connection with a Riemannian analogue transform.

Result: Established sufficient conditions for solving the inverse problem when the manifold is stationary and the connection term is time-independent.

Conclusion: The inverse problem can be solved under specific geometric conditions by reducing it to the study of non-Abelian light ray transforms and connecting them to Riemannian analogues.

Abstract: We study the problem of recovering a time dependent matrix valued potential
on a globally hyperbolic manifold from the knowledge of the source to solution
map of a wave equation including a connection 1-form term. We exhibit
sufficient conditions for solving this inverse problem under the assumption
that the the manifold is stationary and that the connection term is time
independent. The proof is based on two ingredients. The first is reduction of
the problem to the study of a non-Abelian light ray transform and holds
assuming global hyperbolicity only. The second is the study of this transform
and establishing a link with a Riemannian analogue.

</details>


### [33] [Liouville properties for differential inequalities with $(p,q)$ Laplacian operator](https://arxiv.org/abs/2510.13576)
*Mousomi Bhakta,Anup Biswas,Roberta Filippucci*

Main category: math.AP

TL;DR: Liouville-type theorems for nonhomogeneous quasilinear inequalities involving p-Laplacian and q-Laplacian operators, establishing conditions for existence/nonexistence of positive solutions in exterior domains and entire space.


<details>
  <summary>Details</summary>
Motivation: To establish Liouville-type results for nonhomogeneous quasilinear inequalities with mixed p-Laplacian and q-Laplacian operators, extending classical Liouville theorems to more general settings with gradient-dependent nonlinearities.

Method: Proves various Liouville results using different techniques including proving optimal lower estimates for supersolutions and leveraging these estimates to establish nonexistence results, providing an alternative approach to the capacity method of Mitidieri-Pohozaev.

Result: Establishes critical exponents q* = q(N-1)/(N-q) as threshold for existence of positive solutions; proves triviality of solutions for certain parameter ranges; shows that for q ≥ N, only trivial solution exists; proves constant solutions for gradient-dependent inequalities under specific conditions.

Conclusion: The paper provides comprehensive Liouville-type theorems for mixed p-q Laplacian inequalities, identifying critical exponents and parameter ranges where only trivial or constant solutions exist, introducing new techniques for proving such results.

Abstract: In this paper, we establish several Liouville-type theorems for a class of
nonhomogenenous quasilinear inequalities. In the first part, we prove various
Liouville results associated with nonnegative solutions to
\begin{equation*}\tag{$P_s$} -\Delta_p u-\Delta_q u\geq u^{s-1} \, \text{ in
}\, \Omega, \end{equation*} where $1<q<p$, $s>1$ and $\Omega$ is any exterior
domain of $\mathbb{R}^N$. In particular, we prove that for $q<N$, inequality
$(P_s)$ does not admit any positive solution when $s<q_*$ and $(P_s)$ admits a
positive solution if $s>q_*$, where $q_*=\frac{q(N-1)}{N-q}$ is the Serrin
exponent for the $q$-Laplacian. Further, we show that when $s=q_*$ and $p<s$
the only nonnegative solution to $(P_s)$ is the trivial solution. On the other
hand, for $q\geq N$ we prove that $u\equiv 0$ is the only nonnegative solution
for $(P_s)$ for any $s>1$.
  In the second part, we consider the inequality
\begin{equation*}\tag{$P_{sm}$} -\Delta_p u-\Delta_q u \geq u^s |\nabla u|^m
\quad \text{ in }\mathbb{R}^N, \end{equation*} where $1<q<p$, $N>q$ and $s, \,
m\geq 0$. We prove that, for $\{0\leq m\leq q-1\}\cup\{m>p-1\}$, the only
positive solution to $(P_{sm})$ is constant, provided $s(N-q)+m(N-1)<N(q-1)$.
This, in particular, proves that if $\Omega=\mathbb{R}^N$ then any nonnegative
solution to $(P_s)$ with $1<q<N$ and $1<s<q_*$ is the trivial solution. To
prove Liouville in the range $0\leq m<q-1$, we first prove an almost optimal
lower estimate of any nonnegative supersolution of $(P_{sm})$ and then
leveraging this estimate we prove Liouville result. To the best of our
knowledge, this technique is completely new and provides an alternative
approach to the capacity method of Mitidieri-Pohozaev provided higher
regularity is available.

</details>


### [34] [Quantitative estimates for flows of regular Lagrangian flows for Hörmander singular kernels and LD vector fields](https://arxiv.org/abs/2510.13690)
*Henrique Borrin*

Main category: math.AP

TL;DR: The paper provides quantitative estimates for regular Lagrangian flows associated to vector fields with derivatives expressed as convolutions of singular kernels and summable functions.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous quantitative bounds for regular Lagrangian flows in cases where the vector field derivatives involve singular integral operators, which is important for understanding fluid dynamics and transport equations.

Method: The authors analyze vector fields whose derivatives are written as convolutions of fundamental singular kernels satisfying the Hörmander condition with summable functions in spacetime.

Result: Quantitative estimates are obtained for regular Lagrangian flows associated with these vector fields.

Conclusion: The paper successfully derives quantitative bounds for regular Lagrangian flows in the presence of singular integral operators in the vector field derivatives.

Abstract: In this paper, we obtain quantitative estimates of regular Lagrangian flows
associated to vector fields whose derivative can be written as convolution of a
fundamental singular kernel satisfying the ``H\"ormander'' condition convoluted
with summable function in spacetime.

</details>


### [35] [Strong solution for polymeric fluid-structure interaction with small initial acceleration](https://arxiv.org/abs/2510.13753)
*Prince Romeo Mensah*

Main category: math.AP

TL;DR: The paper develops a strong solution framework for a 3D-3D-2D coupled solute-solvent-structure system involving a flexible structure interacting with an Oldroyd-B polymeric fluid.


<details>
  <summary>Details</summary>
Motivation: To address the complex interaction between a flexible structure and a polymeric fluid without center-of-mass diffusion, requiring a rigorous mathematical framework for such coupled systems.

Method: Constructs strong solutions by decoupling the solute from the solvent-structure subsystem, solving each individually, and combining through a fixed-point argument. Relies on maximal regularity results for Stokes problems on moving domains.

Result: Develops a unique higher-order regularity notion of strong solutions for the coupled system, with the maximal regularity result for Stokes problems on moving domains being independently valuable.

Conclusion: Successfully establishes a mathematical framework for analyzing complex fluid-structure interaction problems involving polymeric fluids and flexible structures through decoupling and fixed-point techniques.

Abstract: We consider the problem of a 3D-3D-2D mutually coupled
solute-solvent-structure three-states system. This describes the interaction of
a flexible structure with a polymeric fluid of classical Oldroyd-B type without
centre-of-mass diffusion. We construct a unique higher-order regularity notion
of a strong solution for the system by decoupling the solute from the
solvent-structure subsystem, solving the decoupled system individually, and
glueing the solutions through a fixed-point argument. As a requirement for the
construction, we rely on a maximal regularity result for the Stokes problem on
moving domains with non-trivial boundary conditions; a result that is also of
independent interest.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [36] [Dependence of Microstructure Classification Accuracy on Crystallographic Data Representation](https://arxiv.org/abs/2510.13104)
*Shrunal Pothagoni,Dylan Miley,Tyrus Berry,Jeremy K. Mason,Benjamin Schweinhart*

Main category: physics.comp-ph

TL;DR: The paper examines different representations of crystallographic orientation in convolutional neural networks for microstructure classification, finding that spectral embedding with crystallographic symmetries performs best.


<details>
  <summary>Details</summary>
Motivation: To determine how crystallographic orientation should be represented in CNNs for microstructure analysis, as these networks are increasingly applied to EBSD micrographs for materials with unknown textures and phase compositions.

Method: Four different representations of orientation information were tested with CNNs to classify five synthetic microstructures with varying textures and grain geometries.

Result: Spectral embedding of crystallographic orientations that respects crystallographic symmetries performed significantly better than other representations, even with small training data volumes.

Conclusion: The choice of orientation representation significantly impacts CNN classification accuracy, with spectral embedding being the most effective approach for microstructure analysis.

Abstract: Convolutional neural networks are increasingly being used to analyze and
classify material microstructures, motivated by the possibility that they will
be able to identify relevant microstructural features more efficiently and
impartially than human experts. While up to now convolutional neural networks
have mostly been applied to light optimal microscopy and scanning electron
microscope micrographs, application to EBSD micrographs will be increasingly
common as rational design generates materials with unknown textures and phase
compositions. This raises the question of how crystallographic orientation
should be represented in such a convolutional neural network, and whether this
choice has a significant effect on the network's analysis and classification
accuracy. Four representations of orientation information are examined and are
used with convolutional neural networks to classify five synthetic
microstructures with varying textures and grain geometries. Of these, a
spectral embedding of crystallographic orientations in a space that respects
the crystallographic symmetries performs by far the best, even when the network
is trained on small volumes of data such as could be accessible by practical
experiments.

</details>


### [37] [Long-Range Chiral Pairing enables Topological Superconductivity in Triangular Lattices without Spin-Orbit Coupling and Magnetic Field](https://arxiv.org/abs/2510.13120)
*Yizhi Li,Yanyan Lu,Jianxin Zhong,Lijun Meng*

Main category: physics.comp-ph

TL;DR: Topological superconductivity achieved in monolayer triangular lattices through long-range pairing without spin-orbit coupling or magnetic fields, featuring spontaneous time-reversal symmetry breaking and stable topological phase.


<details>
  <summary>Details</summary>
Motivation: To develop a pathway to topological superconductivity that avoids conventional requirements of spin-orbit coupling and magnetic fields, simplifying the design of topological quantum devices.

Method: Used Berry curvature analysis and topological edge state calculations in zigzag and armchair-edge ribbons with various long-range pairing interactions (nearest-neighbor, next-nearest-neighbor, third-nearest-neighbor).

Result: Long-range pairing induces spontaneous time-reversal symmetry breaking and nontrivial topology. Topological edge states are regulated by ribbon boundary symmetry and pairing range, with particle-hole symmetry maintained in nearest-neighbor pairing but broken in longer-range pairings.

Conclusion: Demonstrates a viable mechanism for topological superconductivity without spin-orbit coupling or magnetic fields, providing theoretical foundation for simplified topological quantum device design.

Abstract: This paper demonstrates a pathway to topological superconductivity in
monolayer triangular lattices through long-range pairing without requiring
spin-orbit coupling and magnetic field, contrasting conventional frameworks
reliant on superconductivity and spin-orbit coupling and time-reversal symmetry
(TRS) breaking. Berry curvature analysis reveals spontaneous
TRS-breaking-induced peaks or valleys under long-range pairing, signaling
nontrivial topology superconducting state. Notably, the increase in the
long-range pairing strength only changes the size of the energy band-gap,
without triggering a topological phase transition. This characteristic is
verified by calculating Berry curvature and topological edge states. In zigzag
and armchair-edge ribbons of finite width, the topological edge states are
regulated by the ribbon boundary symmetry and the interact range of long-range
pairing. Under nearest-neighbor pairing, the topological edge states maintain
particle-hole symmetry and matches the corresponding Chern number. However,
next-nearest-neighbor and third-nearest-neighbor pairings break the
particle-hole symmetry of the topological edge states in armchair-edge ribbon.
This work proposes a mechanism for realizing topological superconductivity
without relying on spin-orbit coupling and magnetic field, offering a
theoretical foundation for simplifying the design of topological quantum
devices.

</details>


### [38] [GO-Diff: Data-free and amortized global structure optimization](https://arxiv.org/abs/2510.13448)
*Nikolaj Rønne,Tejs Vegge,Arghya Bhowmik*

Main category: physics.comp-ph

TL;DR: GO-Diff is a diffusion-based method for global structure optimization that directly samples low-energy atomic configurations without prior data, using only energy function guidance through Boltzmann-weighted score-matching.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient approach for global structure optimization that reduces the computational cost of traditional optimization methods while maintaining competitive performance.

Method: Two-stage self-sampling and model refinement loop trained from scratch using Boltzmann-weighted score-matching loss, leveraging only the known energy function without requiring prior data or explicit relaxation.

Result: Achieves competitive results with significantly fewer energy evaluations compared to traditional optimization pipelines, and supports amortized optimization by reusing pretrained models across related systems.

Conclusion: GO-Diff provides an effective diffusion-based framework for global structure optimization that reduces computational costs while maintaining performance, with the added benefit of transfer learning capabilities across related systems.

Abstract: We introduce GO-Diff, a diffusion-based method for global structure
optimization that learns to directly sample low-energy atomic configurations
without requiring prior data or explicit relaxation. GO-Diff is trained from
scratch using a Boltzmann-weighted score-matching loss, leveraging only the
known energy function to guide generation toward thermodynamically favorable
regions. The method operates in a two-stage loop of self-sampling and model
refinement, progressively improving its ability to target low-energy
structures. Compared to traditional optimization pipelines, GO-Diff achieves
competitive results with significantly fewer energy evaluations. Moreover, by
reusing pretrained models across related systems, GO-Diff supports amortized
optimization - enabling faster convergence on new tasks without retraining from
scratch.

</details>


### [39] [Rippled Moire Superlattices for Decoupled Ferroelectric Bits](https://arxiv.org/abs/2510.13568)
*Di Fan,Changming Ke,Shi Liu*

Main category: physics.comp-ph

TL;DR: The paper identifies strain-induced out-of-plane bending as the key symmetry-breaking mechanism for ferroelectricity in twisted bilayer h-BN, challenging the defect-pinning hypothesis and enabling nanoscale control of ferroelectric bits.


<details>
  <summary>Details</summary>
Motivation: To resolve the fundamental discrepancy between symmetry theory (predicting inversion symmetry preservation) and experimental observations (showing robust ferroelectricity) in moire superlattices like twisted bilayer h-BN.

Method: Used large-scale finite-field molecular dynamics simulations to analyze the system, focusing on strain-induced effects rather than defect-based explanations.

Result: Identified that in-plane compressive strain induces out-of-plane bending fields, which drive spatially heterogeneous interlayer sliding and distort moire domain walls, creating a four-state ferroelectric system. Showed that localized nanobubbles can serve as individually addressable ferroelectric bits.

Conclusion: Established a geometry-driven framework for understanding moire ferroelectrics, offering pathways for ultra-high-density memory and local control of moire potentials for manipulating quantum phases.

Abstract: Symmetry considerations suggest that moire superlattices formed by twisted
two-dimensional materials should preserve overall inversion symmetry. However,
experiments consistently report robust ferroelectricity in systems such as
twisted bilayer h-BN, posing a fundamental discrepancy between theory and
experiment regarding its microscopic origin. Here, using large-scale
finite-field molecular dynamics simulations, we challenge the prevailing
defect-pinning hypothesis and instead identify an out-of-plane bending field,
induced by in-plane compressive strain, as the key symmetry-breaking mechanism.
This strain-induced rippling drives spatially heterogeneous interlayer sliding
and distorts the moire domain wall network, resulting in a four-state
ferroelectric system. Remarkably, we show this mechanism can be harnessed at
the nanoscale, where localized nanobubbles designate the moire lattice's
fundamental hexagonal domain clusters as the smallest individually addressable
ferroelectric bits, thereby imposing local control on an otherwise globally
defined structure. Our findings establish a geometry-driven framework for
understanding and engineering moire ferroelectrics, offering not only a route
toward ultra-high-density, rewritable memory, but also a strategy for locally
tuning the moire potential itself, a critical step for manipulating emergent
correlated and topological quantum phases.

</details>


### [40] [Multiphysics Finite Element Modeling of Irradiation and Thermal Behavior Demonstrated on a Fuel-Assembly Problem](https://arxiv.org/abs/2510.13597)
*Fabrizio Aguzzi,Martín Armoa,Santiago M. Rabazzi,César Pairetti,Alejandro Albanesi*

Main category: physics.comp-ph

TL;DR: A micromechanical modeling framework for nuclear fuel rod components that couples VPSC-FEM with Code_Aster to analyze thermomechanical behavior under combined thermal, mechanical, and irradiation loading.


<details>
  <summary>Details</summary>
Motivation: To develop a predictive model for nuclear fuel rod elements (Zircaloy-2 cladding and spacer grids) under PWR conditions, capturing complex thermomechanical behavior driven by crystallographic texture and material anisotropy.

Method: VPSC-FEM coupling with finite element solver Code_Aster, incorporating thermal expansion and thermal creep to analyze anisotropic deformation, stress relaxation, and strain accumulation under combined loading conditions.

Result: The model captures anisotropic deformation driven by crystallographic texture and prismatic slip, with thermal creep causing early-stage stress relaxation and higher strain compared to irradiation-only cases. Irradiation mechanisms dominate long-term clearance behavior while thermal effects remain relevant in contact dynamics.

Conclusion: The high-resolution framework enables predictive modeling of spacer-cladding interaction and provides foundation for reduced-order models, with stress-strain response being more sensitive to micromechanical processes than elastic constants.

Abstract: This work presents a modeling framework to represent the thermomechanical
behavior of complex materials based on micromechanical dynamics. The framework
is applied to nuclear fuel rod elements composed of Zircaloy-2 cladding tubes
and spacer grids under typical Pressurized Water Reactor (PWR) conditions.
Thermal expansion and thermal creep are incorporated through a VPSC-FEM
coupling with the finite element solver Code_Aster, enabling analysis of
in-reactor behavior under combined thermal, mechanical, and irradiation
loading. The model captures anisotropic deformation driven by crystallographic
texture and prismatic slip activity under radial loading. Thermal creep, being
stress-sensitive, contributes to early-stage stress relaxation and strain
accumulation, leading to higher strain compared to the irradiation-only case.
The interaction of thermal creep with irradiation mechanisms modifies the
stress distribution and clearance evolution, with relaxation governed by
prismatic slip. For fuel rod components, irradiation-induced mechanisms
dominate the long-term clearance behavior, whereas thermal effects remain
relevant in contact dynamics during thermal preloading. The stress-strain
response is found to be more sensitive to micromechanical processes than to
elastic constants. This high-resolution formulation enables predictive modeling
of spacer-cladding interaction and provides a foundation for developing
reduced-order models.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [41] [Narrow Operator Models of Stellarator Equilibria in Fourier Zernike Basis](https://arxiv.org/abs/2510.13521)
*Timo Thun,Rory Conlin,Dario Panici,Daniel Böckenhoff*

Main category: physics.plasm-ph

TL;DR: First numerical approach to compute continuous distribution of ideal MHD equilibria with fixed boundary and rotational transform by varying pressure invariant using neural networks.


<details>
  <summary>Details</summary>
Motivation: Conventional approaches only solve for single stationary points, limiting understanding of equilibrium space. Need method to explore continuous distribution of equilibria for stellarator optimization and as starting points for more complex PDEs.

Method: Minimize force residual by optimizing parameters of multilayer perceptrons (MLPs) that map from scalar pressure multiplier to Fourier Zernike basis in DESC solver.

Result: Developed first numerical approach capable of solving for continuous distribution of ideal MHD equilibria with specified constraints.

Conclusion: New neural network-based method enables exploration of equilibrium space beyond single stationary points, advancing stellarator optimization capabilities.

Abstract: Numerical computation of the ideal Magnetohydrodynamic (MHD) equilibrium
magnetic field is at the base of stellarator optimisation and provides the
starting point for solving more sophisticated Partial Differential Equations
(PDEs) like transport or turbulence models. Conventional approaches solve for a
single stationary point of the ideal MHD equations, which is fully defined by
three invariants and the numerical scheme employed by the solver. We present
the first numerical approach that can solve for a continuous distribution of
equilibria with fixed boundary and rotational transform, varying only the
pressure invariant. This approach minimises the force residual by optimising
parameters of multilayer perceptrons (MLP) that map from a scalar pressure
multiplier to the Fourier Zernike basis as implemented in the modern
stellarator equilibrium solver DESC.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [42] [APRIL: Auxiliary Physically-Redundant Information in Loss - A physics-informed framework for parameter estimation with a gravitational-wave case study](https://arxiv.org/abs/2510.13677)
*Matteo Scialpi,Francesco Di Clemente,Leigh Smith,Michał Bejger*

Main category: gr-qc

TL;DR: APRIL (auxiliary physically-redundant information in loss) enhances PINNs by adding auxiliary loss terms that exploit physical redundancy relations among outputs, improving convergence for multi-system datasets with varying parameters.


<details>
  <summary>Details</summary>
Motivation: Standard PINNs scale poorly to datasets containing many realizations of the same underlying physics with varying parameters, limiting their effectiveness for multi-system problems.

Method: Augment standard supervised output-target loss with auxiliary terms that exploit exact physical redundancy relations among outputs, mathematically preserving the true physical minimum while reshaping the loss landscape.

Result: APRIL achieves up to an order-of-magnitude improvement in test accuracy for gravitational wave parameter estimation, especially for parameters that are otherwise difficult to learn.

Conclusion: APRIL provides physically consistent learning for large multi-system datasets and is well suited for future gravitational wave analyses with realistic noise and broader parameter ranges.

Abstract: Physics-Informed Neural Networks (PINNs) embed the partial differential
equations (PDEs) governing the system under study directly into the training of
Neural Networks, ensuring solutions that respect physical laws. While effective
for single-system problems, standard PINNs scale poorly to datasets containing
many realizations of the same underlying physics with varying parameters. To
address this limitation, we present a complementary approach by including
auxiliary physically-redundant information in loss (APRIL), i.e. augment the
standard supervised output-target loss with auxiliary terms which exploit exact
physical redundancy relations among outputs. We mathematically demonstrate that
these terms preserve the true physical minimum while reshaping the loss
landscape, improving convergence toward physically consistent solutions. As a
proof-of-concept, we benchmark APRIL on a fully-connected neural network for
gravitational wave (GW) parameter estimation (PE). We use simulated, noise-free
compact binary coalescence (CBC) signals, focusing on inspiral-frequency
waveforms to recover the chirp mass $\mathcal{M}$, the total mass
$M_\mathrm{tot}$, and symmetric mass ratio $\eta$ of the binary. In this
controlled setting, we show that APRIL achieves up to an order-of-magnitude
improvement in test accuracy, especially for parameters that are otherwise
difficult to learn. This method provides physically consistent learning for
large multi-system datasets and is well suited for future GW analyses involving
realistic noise and broader parameter ranges.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [43] [The $L_p$ dual Minkowski problem for capillary hypersurfaces](https://arxiv.org/abs/2510.12804)
*Ya Gao*

Main category: math.DG

TL;DR: The paper solves the L_p dual Minkowski problem for capillary hypersurfaces when p>q, establishing existence and uniqueness of smooth solutions for capillary convex bodies with prescribed capillary (p,q)-dual curvature measures.


<details>
  <summary>Details</summary>
Motivation: To address the L_p dual Minkowski problem specifically for capillary hypersurfaces in Euclidean half-space, extending classical Minkowski problems to capillary boundary conditions.

Method: Reduces the problem to a Monge-Ampère type equation with Robin boundary condition on the unit spherical cap, then proves existence and uniqueness of smooth solutions.

Result: Proves that there exists a unique smooth solution to the problem when θ ∈ (0, π/2), solving the capillary L_p dual Minkowski problem for p>q.

Conclusion: Successfully establishes the existence and uniqueness of capillary convex bodies with prescribed capillary (p,q)-dual curvature measures in the specified parameter range.

Abstract: In this paper, we consider the $L_p$ dual Minkowski problem for capillary
hypersurfaces for $p>q$, which aims to find a capillary convex body with a
prescribed capillary $(p,q)$-the dual curvature measure in the Euclidean
half-space. We reduce it to a Monge-Amp\`ere type equation with a Robin
boundary condition on the unit spherical cap, and prove that there exists a
unique smooth solution that solves this problem provided $\theta\in
(0,\frac{\pi}{2})$.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [44] [Design of an Imaging Air Cherenkov Telescope array layout with differential programming](https://arxiv.org/abs/2510.13506)
*Cyril Alispach,Matthieu Heller,Teresa Montaruli*

Main category: astro-ph.IM

TL;DR: The paper proposes using surrogate models and differential programming to optimize IACT array layouts, moving from brute-force simulations to more efficient methods that consider both sensitivity minimization and systematic uncertainty reduction.


<details>
  <summary>Details</summary>
Motivation: Current optimization of ground-based Cherenkov telescope arrays relies on computationally expensive brute-force simulations, requiring significant storage and computation time. There's a need for more efficient methods to explore the full phase space of telescope positioning.

Method: Develop differential programming with surrogate models based on high-level instrument response functions (IRFs) to reduce simulation time. Focus on modeling arrays from single telescope surrogate models and addressing the complexity of different telescope types.

Result: The approach aims to significantly reduce simulation time for single telescopes and develop a generic optimization pipeline for future ground-based cosmic-ray observatories.

Conclusion: The proposed surrogate model and differential programming approach provides a more efficient alternative to brute-force simulations for optimizing IACT array layouts, particularly for complex arrays with different telescope types like CTAO, balancing sensitivity optimization with systematic uncertainty reduction.

Abstract: Current optimization of ground-based Cherenkov telescopes arrays, also called
Imaging Air Cherenkov Telescope (IACT) arrays, relies on brute-force
human-driven approaches based on large simulations requiring both high amount
of storage and long computation time. To explore the full phase space of
telescope positioning of a given array even more simulations would be required.
To optimize any array layout, we explore the possibility of developing a
differential program with surrogate models of IACT arrays based on high-level
instrument response functions (IRFs). The simulation time of a single telescope
to a cosmic-ray event can be significantly reduced with its instrument response
function or with generative models. However, it is not straight forward to
model the array of telescopes from a set of single telescope surrogate models
as the array is a stereoscopic imaging system. The complexity increases as well
if the telescopes in the array are of different types. Additionally, the
optimum of the array layout depends on the scientific use case. Current array
layout optimization are obtained by minimizing the sensitivity of the array, a
metric that depends on several high-level parameters such as the trigger
efficiency, the energy and angular resolution, as well as the background
rejection capability. The variety of telescopes types in IACT arrays, such as
in the Cherenkov Telescope Array Observatory (CTAO), not only extends the
sensitive energy range but also allows for cross-calibration of the
instruments. Therefore, the optimal array layout is not only which minimizes
sensitivity but also which reduces the systematic uncertainties. We focus on
the optimization of a telescope array based on the SST-1M IACTs in Hanle,
Ladakh India aiming at building a generic optimization pipeline for future
ground-based cosmic-ray observatories

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [45] [Large deviation principles for the Gross Pitaevskii Gibbs measure at low temperature](https://arxiv.org/abs/2510.13206)
*Liam Packer,Kihoon Seong,Philippe Sosoe*

Main category: math.PR

TL;DR: Large deviation principle for conditional Gibbs measure in focusing Gross-Pitaevskii equation with mixed canonical/microcanonical ensemble in low temperature regime.


<details>
  <summary>Details</summary>
Motivation: Extend large deviation principles to singular settings with unbounded interaction potentials and diverging renormalization constants.

Method: Prove large deviation principle for conditional Gibbs measure associated with focusing Gross-Pitaevskii equation in low temperature regime.

Result: Gibbs measure concentrates along soliton manifold in low temperature limit.

Conclusion: Successfully extends previous work by Ellis et al. to more singular setting with unbounded potentials and diverging renormalization.

Abstract: We prove the large deviation principle for the conditional Gibbs measure
associated with the focusing Gross Pitaevskii equation in the low temperature
regime. This conditional measure is of mixed type, being canonical in energy
and microcanonical in particle number. In particular, our result extends the
large deviation principle for the mixed ensemble studied by Ellis, Jordan,
Otto, and Turkington to a more singular setting, where the interaction
potential is unbounded and the conditional event involves diverging
renormalization constants. As a consequence of the large deviation principle,
the Gibbs measure concentrates along the soliton manifold in the low
temperature limit.

</details>


### [46] [Quasi-Gaussianity of the 2D stochastic Navier-Stokes equations](https://arxiv.org/abs/2510.13460)
*James Coe,Martin Hairer,Leonardo Tolomeo*

Main category: math.PR

TL;DR: The paper proves that the invariant measure of 2D stochastic Navier-Stokes equations is equivalent to that of the corresponding Ornstein-Uhlenbeck process, using a generalized time-shifted Girsanov method.


<details>
  <summary>Details</summary>
Motivation: To understand the qualitative properties of solutions to 2D stochastic Navier-Stokes equations with white-in-time, colored-in-space forcing, particularly the relationship between the invariant measure of the nonlinear system and the corresponding linear system.

Method: Generalization of the time-shifted Girsanov method to compare laws of time marginals for dissipative SPDEs, allowing comparison between nonlinear equations and between nonlinear and linear equations. Applied to establish equivalence to a twisted nonlinear system that preserves Gaussian measure.

Result: Main result shows the unique invariant measure of the 2D stochastic Navier-Stokes system is equivalent to that of the corresponding Ornstein-Uhlenbeck process. Similar equivalence established for hypoviscous Navier-Stokes equations.

Conclusion: The generalized time-shifted Girsanov method provides a powerful tool for establishing measure equivalence in dissipative SPDEs, revealing fundamental connections between nonlinear stochastic systems and their linear counterparts.

Abstract: We study the qualitative properties of solutions to the 2D stochastic
Navier-Stokes equations with forcing that is white in time and coloured in
space. Our main result shows that the unique invariant measure of this system
is equivalent to that of the corresponding Ornstein-Uhlenbeck process.
  Our method relies on a generalization of the "time-shifted Girsanov method"
of [MS05, MRS22] to compare the laws of time marginals for dissipative SPDEs.
This generalisation allows to not only compare solutions to a nonlinear
equation to those of the corresponding linear equation, but also to directly
compare two nonlinear equations. We use this to establish equivalence of the
Navier-Stokes system to a "twisted" nonlinear system that leaves the Gaussian
measure invariant. We further apply this method to establish similar
equivalence statements for a family of hypoviscous Navier-Stokes equations.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [47] [Efficient Wall-Modelled Large Eddy Simulation of Rotors using Homogenized Lattice Boltzmann Methods](https://arxiv.org/abs/2510.13726)
*Adrian Kummerländer,Shota Ito,Maximilian Schecher,Davide Dapelo,Stephan Simonis,Mathias J. Krause,Fedor Bukreev*

Main category: physics.flu-dyn

TL;DR: A novel blade-resolved wall-modeled large eddy simulation (WMLES) approach using lattice Boltzmann method (LBM) for efficient simulation of rotor dynamics and wake effects in wind farms.


<details>
  <summary>Details</summary>
Motivation: Accurately capturing dynamic forces on rotors and wake effects is challenging due to high Reynolds numbers and large spatio-temporal scales in computational fluid dynamics.

Method: Combines homogenized hybrid regularized recursive collision scheme for filtered Brinkman-Navier-Stokes equations with novel wall-model, implemented in OpenLB framework for fluid-structure interaction.

Result: Excellent agreement with experimental and numerical data for model wind turbine forces and wake velocity profiles. Demonstrated computational efficiency with 384 rotors resolved by 41 billion lattice cells on supercomputer.

Conclusion: The framework enables efficient blade-resolved WMLES of entire wind farms and offers new methodology for complex wall-modeled fluid-structure interaction applications.

Abstract: Accurately capturing the dynamic forces acting on rotors as well as their
wake effects presents a significant challenge for computational fluid dynamics
(CFD) due to high Reynolds numbers and a large range of spatio-temporal scales.
The present work proposes a novel blade-resolved wall-modeled large eddy
simulation (WMLES) approach based on the lattice Boltzmann method (LBM).
  A homogenized hybrid regularized recursive collision scheme targeting the
filtered Brinkman--Navier--Stokes equations is combined with a novel
wall-model. This is implemented in the context of a platform-transparent
framework for fluid-structure interaction in the open source LBM framework
OpenLB.
  Convergence order and accuracy are validated against both experimental and
numerical data for a model wind turbine, demonstrating excellent agreement for
integral forces and wake velocity profiles. Computational efficiency and
parallel scalability was investigated by roofline analysis and weak scaling
studies for up to 384 rotors resolved by 41 billion lattice cells on the
Karolina supercomputer.
  The proposed framework enables efficient blade-resolved WMLES of entire wind
farms and offers a new methodology for other complex wall-modeled
fluid-structure interaction applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Machine Learning-Based Ultrasonic Weld Characterization Using Hierarchical Wave Modeling and Diffusion-Driven Distribution Alignment](https://arxiv.org/abs/2510.13023)
*Joshua R. Tempelman,Adam J. Wachtor,Eric B. Flynn*

Main category: cs.LG

TL;DR: This paper presents an end-to-end machine learning workflow for automated ultrasonic weld inspection that addresses data scarcity and signal corruption challenges through reduced-order modeling, diffusion-based distribution alignment, and U-Net segmentation.


<details>
  <summary>Details</summary>
Motivation: Automated ultrasonic weld inspection faces challenges due to limited training data (difficulty in curating experimental specimens or simulations) and environmental volatility causing signal corruption in industrial settings, making end-to-end ML workflows elusive.

Method: Proposes a workflow with: 1) Reduced-order Helmholtz model based on Lamb wave theory to generate comprehensive dataset over varying weld heterogeneity and crack defects, 2) Transfer learning using limited 3D elastodynamic simulations to refine models, 3) Guided diffusion to handle out-of-distribution real-world measurements by producing in-distribution representations of noisy LDV scans, 4) U-Net-based segmentation and inversion models.

Result: The integrated framework provides an end-to-end solution for automated weld inspection on real data by overcoming data scarcity through simulation and handling signal corruption through diffusion-based preprocessing.

Conclusion: The proposed workflow successfully addresses key challenges in automated ultrasonic weld inspection by combining reduced-order modeling, transfer learning, and diffusion-based distribution alignment to enable practical ML deployment in industrial settings with limited data and noisy measurements.

Abstract: Automated ultrasonic weld inspection remains a significant challenge in the
nondestructive evaluation (NDE) community to factors such as limited training
data (due to the complexity of curating experimental specimens or high-fidelity
simulations) and environmental volatility of many industrial settings
(resulting in the corruption of on-the-fly measurements). Thus, an end-to-end
machine learning (ML) workflow for acoustic weld inspection in realistic (i.e.,
industrial) settings has remained an elusive goal. This work addresses the
challenges of data curation and signal corruption by proposing workflow
consisting of a reduced-order modeling scheme, diffusion based distribution
alignment, and U-Net-based segmentation and inversion. A reduced-order
Helmholtz model based on Lamb wave theory is used to generate a comprehensive
dataset over varying weld heterogeneity and crack defects. The relatively
inexpensive low-order solutions provide a robust training dateset for inversion
models which are refined through a transfer learning stage using a limited set
of full 3D elastodynamic simulations. To handle out-of-distribution (OOD)
real-world measurements with varying and unpredictable noise distributions,
i.e., Laser Doppler Vibrometry scans, guided diffusion produces in-distribution
representations of OOD experimental LDV scans which are subsequently processed
by the inversion models. This integrated framework provides an end-to-end
solution for automated weld inspection on real data.

</details>


### [49] [Neural Triangular Transport Maps: A New Approach Towards Sampling in Lattice QCD](https://arxiv.org/abs/2510.13112)
*Andrey Bryutkin,Youssef Marzouk*

Main category: cs.LG

TL;DR: The paper proposes sparse triangular transport maps using monotone rectified neural networks to efficiently sample lattice field theories, addressing memory constraints and maintaining expressivity while enabling parallel evaluation with linear time complexity.


<details>
  <summary>Details</summary>
Motivation: Lattice field theories are challenging to sample due to multimodality and long-range correlations. Normalizing flows offer promise but face prohibitive memory requirements and expressivity challenges when applied to large lattices.

Method: Sparse triangular transport maps that exploit conditional independence structure using monotone rectified neural networks (MRNN), with a framework balancing exact sparsity (respecting conditional independence) and approximate sparsity (computational tractability).

Result: The method enables site-wise parallel evaluation and linear time complexity in lattice size N while preserving expressive, invertible structure. Tested on φ⁴ theory in 2D, analyzing how node labelings affect sparsity and performance.

Conclusion: The proposed sparse triangular transport maps provide an efficient alternative to traditional methods like Hybrid Monte Carlo and established flow approaches (RealNVP) for sampling lattice field theories.

Abstract: Lattice field theories are fundamental testbeds for computational physics;
yet, sampling their Boltzmann distributions remains challenging due to
multimodality and long-range correlations. While normalizing flows offer a
promising alternative, their application to large lattices is often constrained
by prohibitive memory requirements and the challenge of maintaining sufficient
model expressivity. We propose sparse triangular transport maps that explicitly
exploit the conditional independence structure of the lattice graph under
periodic boundary conditions using monotone rectified neural networks (MRNN).
We introduce a comprehensive framework for triangular transport maps that
navigates the fundamental trade-off between \emph{exact sparsity} (respecting
marginal conditional independence in the target distribution) and
\emph{approximate sparsity} (computational tractability without fill-ins).
Restricting each triangular map component to a local past enables site-wise
parallel evaluation and linear time complexity in lattice size $N$, while
preserving the expressive, invertible structure. Using $\phi^4$ in two
dimensions as a controlled setting, we analyze how node labelings (orderings)
affect the sparsity and performance of triangular maps. We compare against
Hybrid Monte Carlo (HMC) and established flow approaches (RealNVP).

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [50] [Probing non-Markovian qubit noise and modeling Post Markovian Master Equation](https://arxiv.org/abs/2510.12894)
*Chun-Tse Li,Jingming Tan,Vasil Gucev*

Main category: quant-ph

TL;DR: The paper studies non-Markovian noise in quantum processors using Post-Markovian Master Equation formalism, experimentally validates it on IBM Quantum devices, and shows crosstalk dominates non-Markovian effects.


<details>
  <summary>Details</summary>
Motivation: Typical qubit designs use Markovian approximation which doesn't fully capture realistic noise dynamics, especially with qubit-qubit coupling and extended bath correlation times that introduce non-Markovian effects.

Method: Employ Post-Markovian Master Equation (PMME) formalism to characterize memory effects in noise dynamics, and experimentally validate using superconducting qubits on IBM Quantum devices.

Result: Demonstrated clear non-Markovian behavior during circuit execution, and quantified that crosstalk dominates the observed non-Markovian effects in current quantum hardware using information-theoretic approach.

Conclusion: Non-Markovian noise effects are significant in quantum processors, with crosstalk being the dominant source of these effects, highlighting the need for better noise characterization beyond Markovian approximations.

Abstract: Understanding the noise characteristics of quantum processors is crucial when
achieving fault-tolerant quantum computing. However, typical qubit designs are
often studied under the Markovian approximation, which does not fully capture
realistic dynamics. Factors such as qubit-qubit coupling and extended bath
correlation times can introduce significant non-Markovian effects into the
noise processes. In this study, we employ the Post-Markovian Master Equation
(PMME) formalism to characterize memory effects in the noise dynamics. We
further experimentally validate the PMME framework using superconducting qubits
on an IBM Quantum device, demonstrating clear non-Markovian behavior during
circuit execution. Additionally, we quantify the crosstalk effect using an
information-theoretic approach and reveal that crosstalk can dominate the
observed non-Markovian effects in current quantum hardware.

</details>


### [51] [Performance Comparison of Gate-Based and Adiabatic Quantum Computing for Power Flow Analysis](https://arxiv.org/abs/2510.13378)
*Zeynab Kaseb,Matthias Moller,Peter Palensky,Pedro P. Vergara*

Main category: quant-ph

TL;DR: First direct comparison between gate-based quantum computing (GQC) and adiabatic quantum computing (AQC) for solving AC power flow equations, benchmarking QAOA against D-Wave's Advantage and Fujitsu's Digital Annealer.


<details>
  <summary>Details</summary>
Motivation: To address computational challenges in modern electricity networks and provide quantitative insights into performance trade-offs between GQC and AQC paradigms for power flow analysis in the NISQ era.

Method: Adapted Adiabatic Quantum Power Flow (AQPF) algorithm to QAOA, reformulated power flow equations as combinatorial optimization problem, and conducted numerical experiments on 4-bus test system comparing solution accuracy and computational time.

Result: Benchmarked QAOA results against D-Wave's Advantage system and Fujitsu's Quantum-Inspired Integrated Optimization software (QIIO), providing performance comparisons between different quantum computing approaches.

Conclusion: The study highlights the potential of quantum algorithms for power flow analysis and provides insights into scalability and practical viability of GQC versus AQC paradigms for addressing computational challenges in electricity networks.

Abstract: In this paper, we present the first direct comparison between gate-based
quantum computing (GQC) and adiabatic quantum computing (AQC) for solving the
AC power flow (PF) equations. Building on the Adiabatic Quantum Power Flow
(AQPF) algorithm originally designed for annealing platforms, we adapt it to
the Quantum Approximate Optimization Algorithm (QAOA). The PF equations are
reformulated as a combinatorial optimization problem. Numerical experiments on
a 4-bus test system assess solution accuracy and computational time. Results
from QAOA are benchmarked against those obtained using D-Wave's Advantage
system and Fujitsu's latest generation Digital Annealer, i.e., Quantum-Inspired
Integrated Optimization software (QIIO). The findings provide quantitative
insights into the performance trade-offs, scalability, and practical viability
of GQC versus AQC paradigms for PF analysis, highlighting the potential of
quantum algorithms to address the computational challenges associated with
modern electricity networks in the Noisy Intermediate-Scale Quantum (NISQ).

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [52] [Reciprocal Space Attention for Learning Long-Range Interactions](https://arxiv.org/abs/2510.13055)
*Hariharan Ramasubramanian,Alvaro Vazquez-Mayagoitia,Ganesh Sivaraman,Atul C. Thakur*

Main category: cond-mat.mtrl-sci

TL;DR: Reciprocal-Space Attention (RSA) is a framework that captures long-range interactions in Fourier domain to enhance machine learning interatomic potentials, addressing their limitations with long-range physics.


<details>
  <summary>Details</summary>
Motivation: Current MLIPs excel at local interactions but struggle with explicit treatment of long-range interactions like electrostatics and dispersion, requiring a solution that doesn't rely on empirical assumptions.

Method: Mapping linear-scaling attention mechanism into Fourier space to enable explicit modeling of long-range interactions, integrable with existing MLIP frameworks like MACE.

Result: RSA consistently captures long-range physics across diverse benchmarks including dimer binding curves, phosphorene exfoliation, and water dipole density.

Conclusion: RSA effectively addresses long-range interaction limitations in MLIPs and demonstrates broad applicability across chemical and materials systems.

Abstract: Machine learning interatomic potentials (MLIPs) have revolutionized the
modeling of materials and molecules by directly fitting to ab initio data.
However, while these models excel at capturing local and semi-local
interactions, they often prove insufficient when an explicit and efficient
treatment of long-range interactions is required. To address this limitation,
we introduce Reciprocal-Space Attention (RSA), a framework designed to capture
long-range interactions in the Fourier domain. RSA can be integrated with any
existing local or semi-local MLIP framework. The central contribution of this
work is the mapping of a linear-scaling attention mechanism into Fourier space,
enabling the explicit modeling of long-range interactions such as
electrostatics and dispersion without relying on predefined charges or other
empirical assumptions. We demonstrate the effectiveness of our method as a
long-range correction to the MACE backbone across diverse benchmarks, including
dimer binding curves, dispersion-dominated layered phosphorene exfoliation, and
the molecular dipole density of bulk water. Our results show that RSA
consistently captures long-range physics across a broad range of chemical and
materials systems. The code and datasets for this work is available at
https://github.com/rfhari/reciprocal_space_attention

</details>


### [53] [First-Principles Exploration of Pentagonal TiN$_8$ and MoN$_8$ Monolayers as New Magnetic Topological Insulator](https://arxiv.org/abs/2510.13107)
*Zheng Wang,Beichen Ruan,Zhuoheng Li,Shu-Shen Lyu,Kaixuan Chen*

Main category: cond-mat.mtrl-sci

TL;DR: Discovery of 2D pentagonal MN8 monolayers as magnetic topological materials exhibiting quantum anomalous Hall effect, with TiN8 showing C=-1 and MoN8 showing rare C=2 high-Chern-number QAH insulator behavior.


<details>
  <summary>Details</summary>
Motivation: To address the limited number of robust magnetic topological materials for quantum anomalous Hall effect applications, which often suffer from poor stability and complex synthesis.

Method: First-principles calculations to explore emergent magnetism and nontrivial band topology in two-dimensional pentagonal MN8 monolayers.

Result: Identified TiN8 as a QAH insulator with Chern number C=-1, and MoN8 as a rare high-Chern-number QAH insulator with C=2. These systems host out-of-plane ferromagnetic ground states with nontrivial topological properties driven by transition metal d-orbitals.

Conclusion: The penta-MN8 family provides a versatile platform for exotic topological quantum states, expanding material options for magnetic topological insulators and enabling next-generation spintronic and quantum computing devices.

Abstract: The quest for robust, intrinsically magnetic topological materials exhibiting
the quantum anomalous Hall (QAH) effect is a central challenge in condensed
matter physics and the application of revolutionary electronics. However,
progress has been hampered by the limited number of candidate materials, which
often suffer from poor stability and complex synthesis. Here, we introduce a
new paradigm by exploring the emergent magnetism and nontrivial band topology
in the largely overlooked family of two-dimensional (2D) pentagonal MN$_8$
monolayers. Employing first-principles calculations, we reveal that these
systems host out-of-plane ferromagnetic ground states, a key feature that
unlocks nontrivial topological properties driven by the localized $d$-orbitals
of the embedded transition metals. Remarkably, we identify TiN$_8$ as a QAH
insulator characterized by a Chern number of $C=-1$. Even more strikingly,
MoN$_8$ is predicted to be a rare high-Chern-number QAH insulator, boasting a
Chern number of $C=2$. Our findings establish the penta-MN$_8$ family as a
fertile and versatile platform for realizing exotic topological quantum states.
This work not only significantly expands the material landscape for magnetic
topological insulators but also provides a solid theoretical foundation for
designing next-generation spintronic and quantum computing devices.

</details>


### [54] [Towards Universal Material Property Prediction with Deep Learning and Single-Descriptor electronic Density](https://arxiv.org/abs/2510.13207)
*Feng Chen,Shu Li,Xin Chen,Dennis Wong,Biplab Sanyal,Duo Wang*

Main category: cond-mat.mtrl-sci

TL;DR: A universal machine learning framework using electronic charge density as a descriptor achieves accurate multi-property prediction with excellent transferability across 8 material properties.


<details>
  <summary>Details</summary>
Motivation: Overcome the lack of transferability in machine learning for materials science, where current frameworks are typically limited to specific properties due to the complex interplay of multiple degrees of freedom in materials.

Method: Develop a universal ML framework based solely on electronic charge density - a physically grounded and theoretically rigorous descriptor that captures essential material characteristics.

Result: Achieved accurate prediction of 8 different material properties with R² values up to 0.94, and demonstrated outstanding multi-task learning capability where prediction accuracy improves when more properties are incorporated.

Conclusion: This represents a significant step toward realizing a universal machine learning framework for unified prediction of all material properties, showing excellent transferability across multiple material characteristics.

Abstract: Owing to its high scalability and computational efficiency, machine learning
methods have been increasingly integrated into various scientific research
domains, including ab initio-based materials design. It has been demonstrated
that, by incorporating modern machine learning algorithms, one can predict
material properties with practically acceptable accuracy. However, one of the
most significant limitations that restrict the widespread application of
machine learning is its lack of transferability, as a given framework is
typically applicable only to a specific property. The origin of this limitation
is rooted in the fact that a material's properties are determined by multiple
degrees of freedom -- and their complex interplay -- associated with nuclei and
electrons, such as atomic type, structural symmetry, and the number and quantum
states of the valence electrons, among others. The inherent complexity rules
out the possibility of a single machine learning framework providing a full
description of these critical quantities. In this paper, we develop a universal
machine learning framework based solely on a physically grounded and
theoretically rigorous descriptor -- electronic charge density. Our framework
not only enables accurate prediction of eight different material properties
(with R$^2$ values up to 0.94), but also demonstrates outstanding multi-task
learning capability, as prediction accuracy improves when more target
properties are incorporated into a single training process, thereby indicating
excellent transferability. These results represent a significant step toward
realizing the long-standing goal of a universal machine learning framework for
the unified prediction of all material properties.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [55] [Average-case thresholds for exact regularization of linear programs](https://arxiv.org/abs/2510.13083)
*Michael P. Friedlander,Sharvaj Kubal,Yaniv Plan,Matthew S. Scott*

Main category: math.OC

TL;DR: This paper provides the first average-case analysis of exact regularization for linear programs, establishing probability bounds for success based on regularization strength and connecting it to polyhedral geometry via normal fans and Gaussian measures.


<details>
  <summary>Details</summary>
Motivation: To understand when small regularizers can preserve exact linear programming solutions, and provide probabilistic guarantees for exact regularization success in average-case scenarios.

Method: Uses average-case analysis with standard Gaussian cost vectors and fixed constraint sets, characterizes failure via Gaussian measure of inner cones, and develops novel two-sided bounds on the measure of shifted cones.

Result: Establishes dimension-dependent scaling laws for exact regularization probability, connects exact regularization to polyhedral geometry via normal fan and Gaussian solid-angle measures, and obtains computable bounds in canonical settings including regularized optimal transport.

Conclusion: Numerical experiments confirm the predicted scalings and thresholds, providing the first comprehensive average-case analysis of exact regularization for linear programs with practical computable bounds.

Abstract: Small regularizers can preserve linear programming solutions exactly. This
paper provides the first average-case analysis of exact regularization: with a
standard Gaussian cost vector and fixed constraint set, bounds are established
for the probability that exact regularization succeeds as a function of
regularization strength. Failure is characterized via the Gaussian measure of
inner cones, controlled by novel two-sided bounds on the measure of shifted
cones. Results reveal dimension-dependent scaling laws and connect exact
regularization of linear programs to their polyhedral geometry via the normal
fan and the Gaussian (solid-angle) measure of its cones. Computable bounds are
obtained in several canonical settings, including regularized optimal
transport. Numerical experiments corroborate the predicted scalings and
thresholds.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [56] [Unified kinetic theory of induced scattering: Compton, Brillouin, and Raman processes in magnetized electron and positron pair plasma](https://arxiv.org/abs/2510.12869)
*Rei Nishiura,Shoma F. Kamijima,Kunihito Ioka*

Main category: astro-ph.HE

TL;DR: A unified theoretical framework for induced scattering instabilities in magnetized electron-positron pair plasma, covering Compton, Brillouin, and Raman scattering with analytical growth rates.


<details>
  <summary>Details</summary>
Motivation: To understand scattering-parametric instabilities in extreme astrophysical environments like magnetar magnetospheres and FRB emission, where strong magnetic fields and pair plasma conditions exist.

Method: Solving dispersion relations from kinetic theory, accounting for ponderomotive force from wave interactions, to derive analytical expressions for linear growth rates of density fluctuation modes.

Result: Identified dominance conditions for different scattering types, suppression of perpendicular polarization in strong fields, and enabled SRS in charged modes (forbidden in unmagnetized pair plasma).

Conclusion: The framework provides comprehensive evaluation of induced scattering in extreme plasma environments, with applications to astrophysical phenomena like FRB emission in magnetar magnetospheres.

Abstract: We present a unified theoretical framework for induced (stimulated)
scattering-parametric instabilities of electromagnetic waves, including induced
Compton, stimulated Brillouin, and stimulated Raman scattering (SRS) in
strongly magnetized electron-positron pair plasma. By solving the dispersion
relations derived from kinetic theory, taking into account the ponderomotive
force due to the beat of incident and scattered waves, we obtain analytical
expressions for the linear growth rates of the ordinary, neutral, and charged
modes of density fluctuations. Our results clarify which type of scattering
dominates under different thermal coupling, resonance, and density conditions.
In strong magnetic fields, scattering of perpendicularly polarized waves is
generally suppressed, but by different powers of the cyclotron frequency.
Moreover, SRS, which is forbidden in unmagnetized electron and positron pair
plasma, becomes possible in the charged mode. This framework enables a
comprehensive evaluation of induced scattering in extreme astrophysical and
laboratory plasma, such as fast radio burst (FRB) emission and propagation in
magnetar magnetospheres.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [57] [An efficient approach with theoretical guarantees to simultaneously reconstruct activity and attenuation sinogram for TOF-PET](https://arxiv.org/abs/2510.13562)
*Liyang Hu,Chong Chen*

Main category: physics.med-ph

TL;DR: Proposes a new mathematical model for simultaneous reconstruction of activity and attenuation sinogram from TOF-PET emission data only, eliminating need for separate CT/MRI scans for attenuation correction.


<details>
  <summary>Details</summary>
Motivation: To address issues with current PET attenuation correction methods that require additional CT/MRI scans, which introduce radiation doses, longer scan times, and motion-induced misalignment errors.

Method: Developed a maximum likelihood estimation model that exploits the exponential form of attenuation correction factors and incorporates activity constraints in mask regions. Uses alternating update algorithm with proven convergence.

Result: Method demonstrates numerical convergence, robustness to noise, and outperforms state-of-the-art methods in accuracy and efficiency. Enables autonomous attenuation correction without additional scans.

Conclusion: The proposed approach successfully enables simultaneous reconstruction of activity and attenuation from TOF-PET emission data alone, providing accurate quantitative PET imaging without the drawbacks of conventional attenuation correction methods.

Abstract: In positron emission tomography (PET), it is indispensable to perform
attenuation correction in order to obtain the quantitatively accurate activity
map (tracer distribution) in the body. Generally, this is carried out based on
the estimated attenuation map obtained from computed tomography or magnetic
resonance imaging. However, except for errors in the attenuation correction
factors obtained, the additional scan not only brings in new radiation doses
and/or increases the scanning time but also leads to severe misalignment
induced by various motions during and between the two sequential scans. To
address these issues, based on maximum likelihood estimation, we propose a new
mathematical model for simultaneously reconstructing the activity and
attenuation sinogram from the time-of-flight (TOF)-PET emission data only.
Particularly, we make full use of the exclusively exponential form for the
attenuation correction factors, and consider the constraint of a total amount
of the activity in some mask region in the proposed model. Furthermore, we
prove its well-posedness, including the existence, uniqueness and stability of
the solution. We propose an alternating update algorithm to solve the model,
and also analyze its convergence. Finally, numerical experiments with various
TOF-PET emission data demonstrate that the proposed method is of numerical
convergence and robust to noise, and outperforms some state-of-the-art methods
in terms of accuracy and efficiency, and has the capability of autonomous
attenuation correction.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [58] [On operator fields in the upper triangular Toeplitz form](https://arxiv.org/abs/2510.12923)
*M. M. Chernin,A. Yu. Konyaev*

Main category: math.FA

TL;DR: The paper describes coordinate transformations that preserve upper triangular Toeplitz operator forms and their connection to Nijenhuis operators.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental problem of coordinate transformations that maintain upper triangular Toeplitz operator structure.

Method: Using implicit formulas involving matrix-valued functions to describe the transformations and Nijenhuis operators.

Result: Established the relationship between coordinate transformations preserving Toeplitz form and Nijenhuis operators in the same form.

Conclusion: The coordinate transformations preserving upper triangular Toeplitz form are closely related to Nijenhuis operators and can be described using implicit matrix-valued function formulas.

Abstract: In this work, we solve the fundamental problem of describing the coordinate
transformations that preserve the upper triangular Toeplitz form of the given
operator field. Surprisingly, this problem is closely related to the
description of all Nijenhuis operators in the same form. This description, as
well as the formulas for the aforementioned coordinate transformations, are
given by the implicit formulas involving matrix-valued functions.

</details>
