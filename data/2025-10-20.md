<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 13]
- [math.AP](#math.AP) [Total: 19]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [math.OC](#math.OC) [Total: 4]
- [quant-ph](#quant-ph) [Total: 2]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [math-ph](#math-ph) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Linearly Stable Generalizations of ESFR Schemes](https://arxiv.org/abs/2510.15043)
*Mathias Dufresne-Pich√©,Siva Nadarajah*

Main category: math.NA

TL;DR: Introduces Sobolev Stable discontinuous Galerkin (SSDG) schemes as a new conservative and linearly stable generalization of ESFR methods via the filtered DG framework, comparing them with existing EESFR methods.


<details>
  <summary>Details</summary>
Motivation: To develop a more flexible and efficient framework for high-order accurate schemes on unstructured grids by generalizing ESFR methods through the filtered DG framework.

Method: Proposes SSDG schemes as a new generalization of ESFR methods using the filtered discontinuous Galerkin framework, and analyzes their linear properties via Von Neumann analysis compared to EESFR methods.

Result: SSDG and EESFR schemes show different dispersive/dissipative behaviors but similar CFL limit increases and spectral accuracy. SSDG has broader parameter range for time-stepping improvements, while EESFR achieves p+1 order accuracy vs SSDG's p order under h-refinement.

Conclusion: SSDG schemes provide a viable alternative to EESFR with broader applicability for increasing explicit time-stepping limits, though with different accuracy characteristics under mesh refinement.

Abstract: The energy stable flux reconstruction (ESFR) method encompasses an infinite
family of high-order, linearly stable schemes and thus provides a flex- ible
and efficient framework for achieving high levels of accuracy on unstruc- tured
grids. One remarkable property of ESFR schemes is their ability to be expressed
equivalently as linearly filtered discontinuous Galerkin (FDG) schemes. In this
study, we introduce Sobolev Stable discontinuous Galerkin (SSDG) schemes, a new
conservative and linearly stable generalization of ESFR schemes via the FDG
framework. Additionally, we review existing generalizations of the ESFR method
and consider their relationship with the FDG framework. The linear properties
of SSDG schemes are studied via Von Neumann analysis and compared to those of
the existing extended ESFR (EESFR) method. It is found that while SSDG and
EESFR schemes exhibit fundamentally different dispersive and dissipative
behaviors, they can achieve a similar increase in CFL limit and exhibit a
similar spectral order of accuracy. Moreover, it is seen that the range of
scheme parameters over which SSDG schemes can be used to increase the explicit
time-stepping limit is much larger than for EESFR schemes. Finally, it is
observed that the order of accuracy of EESFR schemes under h-refinement is
generally p + 1 while that of SSDG schemes is p.

</details>


### [2] [Fast spectral separation method for kinetic equation with anisotropic non-stationary collision operator retaining micro-model fidelity](https://arxiv.org/abs/2510.15093)
*Yue Zhao,Huan Lei*

Main category: math.NA

TL;DR: A data-driven collisional operator for plasmas learned from MD simulations, extending kinetic models beyond weakly coupled regimes with anisotropic collision kernels and efficient spectral methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of classical Landau formulations that neglect particle correlations in weakly coupled plasmas, enabling accurate modeling in moderately coupled regimes.

Method: Developed a generalized collisional operator with anisotropic, non-stationary collision kernel learned from molecular dynamics simulations, using fast spectral separation with low-rank tensor representation and structure-preserving discretization.

Result: The model accurately captures plasma dynamics in moderately coupled regimes beyond standard Landau model, while maintaining computational efficiency (O(N log N) complexity) and preserving physical properties like conservation laws and H-theorem.

Conclusion: The proposed data-driven collisional operator successfully extends kinetic plasma modeling to moderately coupled regimes with improved accuracy while maintaining computational efficiency and physical structure preservation.

Abstract: We present a generalized, data-driven collisional operator for one-component
plasmas, learned from molecular dynamics simulations, to extend the collisional
kinetic model beyond the weakly coupled regime. The proposed operator features
an anisotropic, non-stationary collision kernel that accounts for particle
correlations typically neglected in classical Landau formulations. To enable
efficient numerical evaluation, we develop a fast spectral separation method
that represents the kernel as a low-rank tensor product of univariate basis
functions. This formulation admits an $O(N \log N)$ algorithm via fast Fourier
transforms and preserves key physical properties, including discrete
conservation laws and the H-theorem, through a structure-preserving central
difference discretization. Numerical experiments demonstrate that the proposed
model accurately captures plasma dynamics in the moderately coupled regime
beyond the standard Landau model while maintaining high computational
efficiency and structure-preserving properties.

</details>


### [3] [Reduced order method based Anderson-type acceleration method for nonlinear least square problems and large scale ill-posed problems](https://arxiv.org/abs/2510.15097)
*Kazufumi Ito,Tiancheng Xue*

Main category: math.NA

TL;DR: Anderson's acceleration method improves fixed point iteration convergence by minimizing residuals and using solution history as a reduced order method (ROM). The method combines variable step size gradient/quasi-Newton methods with ROM acceleration for rapid convergence.


<details>
  <summary>Details</summary>
Motivation: Most mathematical modeling problems are formulated as constrained optimization, with necessary optimality conditions written as fixed point problems. Anderson's method addresses slow convergence in standard fixed point iterations.

Method: Uses Anderson's acceleration with ROM approach: approximates solution as linear combination of iterates, minimizes equation error on linear manifold, applies reduced order Gauss-Newton method for least squares problems. Combines variable step size gradient/quasi-Newton methods with ROM acceleration.

Result: Method converges rapidly when condition number is not large. ROM acceleration is nearly optimal, regularizes and stabilizes convergence. Also developed acceleration for large-scale ill-posed linear systems using randomly overlapped Kaczmarz type method.

Conclusion: Anderson's method with ROM acceleration provides efficient convergence for optimization problems, combining historical solution information with optimal linear combinations to improve fixed point iteration performance.

Abstract: In this paper we discuss the Anderson's type acceleration method for
numerical optimizations. Most mathematical modeling problems can be formulated
as constrained optimization. The necessary optimality condition is written as a
fixed point problem in a Banach space. Anderson's acceleration method improves
the convergence of the standard fixed point iteration by minimizing the total
sum of residuals and updating solutions through an optimal linear combination
of a sequence of iterates. Thus, it is a form of iterative method of retard
which uses the history of solutions as a reduced order element method (ROM).
The weights are determined optimally by the least squares problem based on the
total residual. We analyze Anderson's method and the reduced order method (ROM)
for nonlinear least squares problems of minimizing |F(x)| squared. That is, the
solution is approximated by a linear combination of sequentially generated
solutions, and then we minimize the equation error on the linear manifold
spanned by the iterates. We use the reduced order Gauss Newton method to solve
the least squares problem for |F(x)| squared on the linear solution manifold.
For the linear equation case it is similar to Anderson's method. Anderson's
method approximates the solution to the nonlinear ROM. We consider variable
step size gradient and quasi Newton methods and the variable fixed point
iteration to generate the solution basis, then combine these with ROM
acceleration. It converges very rapidly if the condition number of matrix A is
not large. The variable iterate with ROM acceleration is nearly optimal, and
ROM also regularizes and stabilizes the convergence. We also consider a
randomly overlapped Kaczmarz type method and develop an acceleration approach
for large scale ill posed linear systems. Finally, we analyze the convergence
of the ROM to the operator equation.

</details>


### [4] [An Efficient Space-Time Two-Grid Compact Difference Scheme for the Two-Dimensional Viscous Burgers' Equation](https://arxiv.org/abs/2510.15274)
*Xiangyi Peng,Lisen Ding,Wenlin Qiu*

Main category: math.NA

TL;DR: An efficient space-time two-grid compact difference scheme for 2D viscous Burgers' equation that combines compact finite difference with two-grid strategy, achieving 70% CPU time reduction while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop a computationally efficient method for solving the 2D viscous Burgers' equation that reduces computational cost without sacrificing accuracy compared to traditional nonlinear approaches.

Method: Two-grid strategy with compact finite difference discretization: coarse grid uses fixed-point iteration for nonlinear system, fine grid uses linear temporal and cubic spatial Lagrange interpolations for initial approximations, followed by linearized correction scheme.

Result: Method achieves second-order accuracy in time and fourth-order accuracy in space, reduces CPU time by more than 70% compared to traditional nonlinear compact difference method while maintaining comparable accuracy.

Conclusion: The ST-TGCD scheme is a highly efficient alternative to conventional nonlinear approaches for solving the 2D viscous Burgers' equation, offering significant computational savings with maintained accuracy.

Abstract: This work proposes an efficient space-time two-grid compact difference
(ST-TGCD) scheme for solving the two-dimensional (2D) viscous Burgers' equation
subject to initial and periodic boundary conditions. The proposed approach
combines a compact finite difference discretization with a two-grid strategy to
achieve high computational efficiency without sacrificing accuracy. In the
coarse-grid stage, a fixed-point iteration is employed to handle the nonlinear
system, while in the fine-grid stage, linear temporal and cubic spatial
Lagrange interpolations are used to construct initial approximations. The final
fine-grid solution is refined through a carefully designed linearized
correction scheme. Rigorous analysis establishes unconditional convergence of
the method, demonstrating second-order accuracy in time and fourth-order
accuracy in space. Numerical experiments verify the theoretical results and
show that the ST-TGCD scheme reduces CPU time by more than 70\% compared with
the traditional nonlinear compact difference (NCD) method, while maintaining
comparable accuracy. These findings confirm the proposed scheme as a highly
efficient alternative to conventional nonlinear approaches.

</details>


### [5] [A Novel Preconditioning Framework for Solving Nonlinear PDEs based on Fenchel-Rockafellar Duality and Transformed Primal-Dual Techniques](https://arxiv.org/abs/2510.15351)
*Long Chen,Ruchi Guo,Jingrong Wei,Jun Zou*

Main category: math.NA

TL;DR: A DualTPD method for solving nonlinear PDEs using Fenchel-Rockafellar duality, TPD dynamics, and efficient preconditioners with multigrid solvers.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for solving nonlinear partial differential equations that addresses challenges with nonlinear terms and convergence issues.

Method: Decouples nonlinear terms via Fenchel-Rockafellar duality using discontinuous finite element spaces, applies transformed primal-dual dynamics for improved convergence, and designs efficient preconditioners with multigrid solvers for elliptic-type Schur complements.

Result: Extensive numerical experiments on elliptic p-Laplacian and nonlinear H(curl) problems show significant efficiency gains with global, mesh-independent convergence.

Conclusion: The DualTPD method provides an effective approach for nonlinear PDEs with improved convergence properties and computational efficiency.

Abstract: A DualTPD method is proposed for solving nonlinear partial differential
equations. The method is characterized by three main features. First,
decoupling via Fenchel--Rockafellar duality is achieved, so that nonlinear
terms are discretized by discontinuous finite element spaces, yielding
block-diagonal mass matrices and closed-form updates. Second, improved
convergence is obtained by applying transformed primal--dual (TPD) dynamics to
the nonlinear saddle-point system, which yields strongly monotone behavior.
Third, efficient preconditioners are designed for the elliptic-type Schur
complement arising from the separated differential operators, and multigrid
solvers are applied effectively. Extensive numerical experiments on elliptic
$p$-Laplacian and nonlinear $H(\curl)$ problems are presented, showing
significant efficiency gains with global, mesh-independent convergence.

</details>


### [6] [Second order explicit stabilized multirate method for stiff differential equations with error control](https://arxiv.org/abs/2510.15475)
*Mathieu Benninghoff,Gilles Vilmart*

Main category: math.NA

TL;DR: The paper introduces mROCK2, a second-order multirate explicit stabilized method that extends previous first-order approaches to handle local spatial mesh refinement more efficiently by splitting systems into fast stiff and slower mildly stiff components.


<details>
  <summary>Details</summary>
Motivation: Explicit stabilized methods lose efficiency with local spatial mesh refinement because stiffness is determined by the smallest mesh element, requiring a more efficient approach that handles different stiffness levels separately.

Method: Extends the first-order multirate explicit stabilized method (mRKC) to second order by introducing mROCK2, which splits the system into fast stiff and slower mildly stiff components and includes a step-size strategy with error control.

Result: Numerical experiments with the heat equation and local spatial mesh refinements confirm that mROCK2 achieves high precision and maintains efficiency.

Conclusion: The mROCK2 method successfully extends multirate explicit stabilized methods to second order, providing improved accuracy and efficient error-controlled step-size strategies for systems with local mesh refinement.

Abstract: Explicit stabilized methods are highly efficient time integrators for large
and stiff systems of ordinary differential equations especially when applied to
semi-discrete parabolic problems. However, when local spatial mesh refinement
is introduced, their efficiency decreases, since the stiffness is driven by
only the smallest mesh element. A natural approach is to split the system into
fast stiff and slower mildly stiff components. In this context, [A. Abdulle,
M.J. Grote and G. Rosilho de Souza 2022] proposed the order one multirate
explicit stabilized method (mRKC). We extend their approach to second order and
introduce the new multirate ROCK2 method (mROCK2), which achieves high
precision and allows a step-size strategy with error control. Numerical methods
including the heat equation with local spatial mesh refinements confirm the
accuracy and efficiency of the scheme.

</details>


### [7] [Rational methods for abstract linear initial boundary value problems without order reduction](https://arxiv.org/abs/2510.15481)
*Carlos Arranz-Sim√≥n,Bego√±a Cano,C√©sar Palencia*

Main category: math.NA

TL;DR: A-stable rational approximations to e^z are used to develop efficient time integration methods for IBVPs with time-dependent source terms and boundary values, achieving optimal order p with minimal function evaluations.


<details>
  <summary>Details</summary>
Motivation: To create practical numerical methods for abstract, well-posed initial-boundary value problems (IBVPs) that can handle time-dependent source terms and boundary values efficiently without requiring derivative evaluations of the data.

Method: Using A-stable rational approximations to e^z of order p, the authors develop numerical procedures that require only one evaluation of f (source term) and g (boundary value) per time step, eliminating the need for derivative evaluations.

Result: The proposed methods achieve optimal order p convergence and are practical for p ‚â§ 6. Full discretization analysis is provided and numerical experiments confirm the theoretical results.

Conclusion: The developed procedures provide efficient and practical time integration methods for IBVPs with time-dependent data, achieving optimal order accuracy with minimal computational cost (one function evaluation per step).

Abstract: Given an $A$-stable rational approximation to $e^z$ of order $p$, numerical
procedures are suggested to time integrate abstract, well-posed IBVPs, with
time-dependent source term $f$ and boundary value $g$. These procedures exhibit
the optimal order $p$ and can be implemented by using just one single
evaluation of $f$ and $g$ per step, i.e., no evaluations of the derivatives of
data are needed, and are of practical use at least for $p\le 6$. The full
discretization is also studied and the theoretical results are corroborated by
numerical experiments.

</details>


### [8] [A Hybrid High-Order Finite Element Method for a Nonlocal Nonlinear Problem of Kirchhoff Type](https://arxiv.org/abs/2510.15574)
*Gouranga Mallik*

Main category: math.NA

TL;DR: A hybrid high-order finite element method is developed for nonlocal nonlinear Kirchhoff-type problems, featuring arbitrary-order polynomial approximations on polytopal meshes, with existence/uniqueness proofs, optimal error estimates, and Newton-based numerical implementation.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient high-order finite element method for nonlocal nonlinear Kirchhoff-type problems that works on general polytopal meshes (both structured and unstructured), addressing the need for robust numerical schemes for such complex problems.

Method: Hybrid high-order (HHO) finite element method with arbitrary-order polynomial approximations on polytopal meshes. The discrete system is solved using Newton's iterations on sparse matrix systems.

Result: Proved existence and uniqueness of discrete solution, derived optimal-order error estimate in discrete energy norm, and validated theoretical results through numerical tests.

Conclusion: The HHO method provides an effective framework for solving nonlocal nonlinear Kirchhoff-type problems with arbitrary-order accuracy on general meshes, with rigorous mathematical foundation and practical numerical implementation.

Abstract: In this article, we design and analyze a hybrid high-order (HHO) finite
element approximation for the solution of a nonlocal nonlinear problem of
Kirchhoff type. The HHO method involves arbitrary-order polynomial
approximations on structured and unstructured polytopal meshes. We establish
the existence of a unique discrete solution to the nonlocal nonlinear discrete
problem. We derive an optimal-order error estimate in the discrete energy norm.
The discrete system is solved using Newton's iterations on the sparse matrix
system. We perform numerical tests to substantiate the theoretical results.

</details>


### [9] [High order Tensor-Train-Based Schemes for High-Dimensional Mean Field Games](https://arxiv.org/abs/2510.15603)
*Elisabetta Carlini,Luca Saluzzi*

Main category: math.NA

TL;DR: A fully discrete scheme combining semi-Lagrangian time discretizations with Tensor-Train decompositions to solve high-dimensional Mean Field Games systems, overcoming the curse of dimensionality.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of solving high-dimensional Mean Field Games systems, which suffer from exponential growth in storage and computational requirements with increasing dimensions.

Method: Couples semi-Lagrangian time discretizations with Tensor-Train decompositions, reformulating Hamilton-Jacobi-Bellman and Fokker-Planck equations as advection-diffusion-reaction subproblems within smoothed policy iteration.

Result: The method reduces storage and computational cost from exponential to polynomial in dimension, achieves theoretical convergence rates, shows modest growth in memory/runtime with dimension, and outperforms grid-based SL in accuracy per CPU second.

Conclusion: TT-accelerated SL methods provide an effective approach for solving high-dimensional Mean Field Games by taming the curse of dimensionality through tensor decompositions and semi-Lagrangian schemes.

Abstract: We introduce a fully discrete scheme to solve a class of high-dimensional
Mean Field Games systems. Our approach couples semi-Lagrangian (SL) time
discretizations with Tensor-Train (TT) decompositions to tame the curse of
dimensionality. By reformulating the classical Hamilton-Jacobi-Bellman and
Fokker-Planck equations as a sequence of advection-diffusion-reaction
subproblems within a smoothed policy iteration, we construct both first and
second order in time SL schemes. The TT format and appropriate quadrature rules
reduce storage and computational cost from exponential to polynomial in the
dimension. Numerical experiments demonstrate that our TT-accelerated SL methods
achieve their theoretical convergence rates, exhibit modest growth in memory
usage and runtime with dimension, and significantly outperform grid-based SL in
accuracy per CPU second.

</details>


### [10] [Convergence analysis of Sobolev Gradient flows for the rotating Gross-Pitaevskii energy functional](https://arxiv.org/abs/2510.15604)
*Chen Zhang,Patrick Henning,Mahima Yadav,Wenbin Chen*

Main category: math.NA

TL;DR: This paper analyzes numerical methods for finding ground states of rotating Bose-Einstein condensates using Sobolev gradient flow schemes, proving convergence for H‚ÇÄ¬π and a‚ÇÄ schemes in rotating cases and local linear convergence for all three schemes.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze numerical approximation methods for computing ground states of rotating Bose-Einstein condensates, which are modeled by minimizing the Gross-Pitaevskii energy functional under mass conservation constraints.

Method: The study considers three Sobolev gradient flow schemes: H‚ÇÄ¬π scheme, a‚ÇÄ scheme, and a_u scheme. The paper proves global convergence of H‚ÇÄ¬π and a‚ÇÄ schemes in rotating cases, and establishes local linear convergence for all three schemes near the ground state.

Result: Theoretical proofs show global convergence for H‚ÇÄ¬π and a‚ÇÄ schemes in rotating cases, and local linear convergence for all three schemes. Numerical experiments validate these theoretical findings.

Conclusion: The paper successfully extends convergence analysis to rotating Bose-Einstein condensates, providing rigorous mathematical foundations for the numerical schemes and confirming their effectiveness through both theoretical proofs and numerical verification.

Abstract: This paper studies the numerical approximation of the ground state of
rotating Bose--Einstein condensates, formulated as the minimization of the
Gross--Pitaevskii energy functional under a mass conservation constraint. To
solve this problem, we consider three Sobolev gradient flow schemes: the
$H_0^1$ scheme, the $a_0$ scheme, and the $a_u$ scheme. Convergence of these
schemes in the non-rotating case was established by Chen et al., and the
rotating $a_u$ scheme was analyzed in Henning et al. In this work, we prove the
global convergence of the $H_0^1$ and $a_0$ schemes in the rotating case, and
establish local linear convergence for all three schemes near the ground state.
Numerical experiments confirm our theoretical findings.

</details>


### [11] [Convergence of the Waveholtz Iteration on $\mathbb{R}^d$](https://arxiv.org/abs/2510.15606)
*Olof Runborg,Elliot Backman*

Main category: math.NA

TL;DR: The paper analyzes the Waveholtz method for solving Helmholtz equations, proving that iterates converge to the solution at a rate of 1/‚àön in weighted Sobolev norms.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous convergence analysis of the Waveholtz method, a time-domain iterative approach for solving Helmholtz equations, particularly in the constant-coefficient case.

Method: Analyzed the Waveholtz method by showing the difference between iterates and the solution satisfies a Helmholtz equation with specific forcing, then proving frequency-explicit estimates in weighted Sobolev norms.

Result: Proved that the difference between Waveholtz iterates and the solution decreases as 1/‚àön with iteration number n, guaranteeing convergence of real parts to the outgoing Helmholtz solution.

Conclusion: The Waveholtz method converges to the solution of Helmholtz equations, with proven convergence rate of 1/‚àön in weighted Sobolev norms for constant-coefficient cases.

Abstract: In this paper we analyse the Waveholtz method, a time-domain iterative method
for solving the Helmholtz iteration, in the constant-coefficient case in all of
$\mathbb{R}^d$. We show that the difference between a Waveholtz iterate and the
outgoing Helmholtz solution satisfies a Helmholtz equation with a particular
kind of forcing. For this forcing, we prove a frequency-explicit estimate in
weighted Sobolev norms, that shows a decrease of the differences as
$1/\sqrt{n}$ in terms of the iteration number $n$. This guarantees the
convergence of the real parts of the Waveholtz iterates to the real part of the
outgoing solution of the Helmholtz equation.

</details>


### [12] [Longer time accuracy for the Ladyzhenskya model with the EMAC formulation](https://arxiv.org/abs/2510.15819)
*Rihui Lan,Jorge Reyes*

Main category: math.NA

TL;DR: Incorporating EMAC formulation into Ladyzhenskaya model (LM) for LES to address over-dissipation issues of Smagorinsky model, with improved stability and accuracy at high Reynolds numbers.


<details>
  <summary>Details</summary>
Motivation: To overcome the known over-dissipation problems of classical Smagorinsky model in large eddy simulation by combining EMAC formulation's conservation properties with Ladyzhenskaya model.

Method: Developed finite element discretization for EMAC-LM system, analyzed stability, derived numerical error estimates, and conducted benchmark simulations.

Result: EMAC-LM model shows improved long-time behavior, better Gronwall constant independent of Reynolds number, and more accurate flow structures especially at high Reynolds numbers.

Conclusion: Combining EMAC formulation with Ladyzhenskaya model provides superior performance over standard LM approach, particularly for high Reynolds number flows.

Abstract: In this paper, we incorporate the EMAC formulation into the Ladyzhenskaya
model (LM), a large eddy simulation (LES) of incompressible flows. The EMAC
formulation, which conserves energy, linear momentum, and angular momentum even
with weak enforcement of incompressibility, has been shown to provide tangible
benefits over the popular skew-symmetric for direct numerical simulation and
regularized models of the Navier Stokes equations (NSE). The combination of
EMAC with the LM addresses the known over-dissipation issues associated with
the classical Smagorinsky model (SM). We develop a finite element
discretization for the EMAC-LM system and analyze its stability and derive
numerical error estimates, showing improved long-time behavior compared to the
standard LM approach, particularly due to EMAC's favorable Gronwall constant
independent of the Reynolds number. Benchmark simulations demonstrate that the
EMAC-LM model yields more accurate flow structures, especially at high Reynolds
numbers.

</details>


### [13] [Asymptotic-preserving conservative semi-Lagrangian discontinuous Galerkin schemes for the Vlasov-Poisson system in the quasi-neutral limit](https://arxiv.org/abs/2510.15854)
*Xiaofeng Cai,Linghui Kong,Dmitri Kuzmin,Li Shan*

Main category: math.NA

TL;DR: Conservative semi-Lagrangian discontinuous Galerkin (CSLDG) schemes for Vlasov-Poisson system with asymptotic preserving properties in quasi-neutral limit.


<details>
  <summary>Details</summary>
Motivation: To develop accurate and robust numerical methods for electrostatic plasmas that work reliably across different regimes, especially when spatial/temporal resolution doesn't fully resolve Debye length.

Method: Combines CSLDG discretization with reformulated Poisson equation (RPE). CSLDG ensures local mass conservation and bypasses CFL condition, while DG provides high-order accuracy. RPE is derived from Poisson equation coupled with Vlasov equation moments.

Result: Method is proven asymptotically stable, consistent, and satisfies AP properties. Maintains efficiency across non-quasi-neutral and quasi-neutral regimes. Numerical experiments verify accuracy, stability, and efficiency.

Conclusion: The CSLDG approach with RPE enables reliable simulation of complex electrostatic plasmas, providing essential properties for accurate and robust numerical modeling across different plasma regimes.

Abstract: We discretize the Vlasov-Poisson system using conservative semi-Lagrangian
(CSL) discontinuous Galerkin (DG) schemes that are asymptotic preserving (AP)
in the quasi-neutral limit. The proposed method (CSLDG) relies on two key
ingredients: the CSLDG discretization and a reformulated Poisson equation
(RPE). The use of the CSL formulation ensures local mass conservation and
circumvents the Courant-Friedrichs-Lewy condition, while the DG method provides
high-order accuracy for capturing fine-scale phase space structures of the
distribution function. The RPE is derived by the Poisson equation coupled with
moments of the Vlasov equation. The synergy between the CSLDG and RPE
components makes it possible to obtain reliable numerical solutions, even when
the spatial and temporal resolution might not fully resolve the Debye length.
We rigorously prove that the proposed method is asymptotically stable,
consistent and satisfies AP properties. Moreover, its efficiency is maintained
across non-quasi-neutral and quasi-neutral regimes. These properties of our
approach are essential for accurate and robust numerical simulation of complex
electrostatic plasmas. Several numerical experiments verify the accuracy,
stability and efficiency of the proposed CSLDG schemes.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [14] [Stability of the spatially homogeneous Landau equation in relative entropy and applications to score-based numerical methods](https://arxiv.org/abs/2510.15089)
*Vasily Ilin*

Main category: math.AP

TL;DR: Elementary proof of stability for Landau equation solutions using relative entropy, with applications to error bounds in score-based transport modeling.


<details>
  <summary>Details</summary>
Motivation: To provide a simple and explicit stability analysis for the Landau equation with Coulomb collisions, and connect this to practical error bounds in computational methods.

Method: Short elementary proof yielding explicit differential inequality for relative entropy under natural moment and regularity assumptions.

Result: Establishes stability of strong solutions and provides explicit error bounds linking training loss to relative-entropy error in numerical schemes.

Conclusion: The approach provides both theoretical stability guarantees and practical computational error bounds for Landau equation and related transport modeling.

Abstract: We give a short and elementary proof of stability for strong solutions of the
spatially homogeneous Landau equation with Coulomb collisions, measured in
relative entropy. The argument yields an explicit differential inequality for
relative entropy under natural moment and regularity assumptions. The same
computation provides an a posteriori error bound for score-based transport
modeling and related deterministic numerical schemes, linking the training loss
to the relative-entropy error.

</details>


### [15] [The role of the curvature of a surface in the shape of the solutions to elliptic equations](https://arxiv.org/abs/2510.15098)
*Francesca Gladiali,Massimo Grossi,Luigi Provenzano*

Main category: math.AP

TL;DR: Proves uniqueness and non-degeneracy of critical points for positive semi-stable solutions of -Œîu=f(u) with Dirichlet boundary conditions on star-shaped domains in sphere and hyperbolic plane under optimal geometric conditions.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical results about the behavior of solutions to semilinear elliptic equations on curved manifolds, specifically identifying when critical points are unique and non-degenerate.

Method: Mathematical analysis using geometric conditions on star-shaped domains in sphere (weaker than convexity) and hyperbolic plane (weaker than horoconvexity), with construction of examples to show optimality.

Result: Proved uniqueness and non-degeneracy of critical points for positive semi-stable solutions under specified geometric conditions, and demonstrated these conditions are optimal through counterexamples.

Conclusion: The geometric conditions identified are both sufficient and necessary for the uniqueness and non-degeneracy properties of critical points in these curved space settings.

Abstract: We prove uniqueness and non-degeneracy of the critical point of positive,
semi-stable solutions of $-\Delta u=f(u)$ with Dirichlet boundary conditions
for a class of star-shaped domains of the sphere and of the hyperbolic plane
satisfying a geometric condition. In the spherical case, this condition is
weaker than convexity, while in the hyperbolic case it is weaker than
horoconvexity. Finally, we construct examples showing that this geometric
condition is indeed optimal.

</details>


### [16] [Limiting absorption principle for a hybrid resonance in a two-dimensional cold plasma](https://arxiv.org/abs/2510.15153)
*Maryna Kachanovska,√âtienne Peillon*

Main category: math.AP

TL;DR: Analysis of hybrid plasma resonance boundary-value problem with vanishing coefficient, proving limiting absorption principle via weighted Sobolev spaces and solution decomposition.


<details>
  <summary>Details</summary>
Motivation: To study hybrid plasma resonance problems where the principal operator coefficient vanishes on a curve and changes sign, requiring special mathematical treatment.

Method: Establish a priori bounds in weighted Sobolev spaces, prove limiting absorption principle, decompose solution into regular and singular parts, introduce radiation-like condition.

Result: Successfully proved limiting absorption principle and established well-posed problem for the limiting absorption solution in bounded domain.

Conclusion: The approach enables rigorous treatment of hybrid plasma resonance with vanishing coefficients through weighted spaces and radiation conditions.

Abstract: We study a limiting absorption principle for the boundary-value problem
describing a hybrid plasma resonance, with a regular coefficient in the
principal part of the operator that vanishes on a curve inside the domain and
changes its sign across this curve. We prove the limiting absorption principle
by establishing a priori bounds on the solution in certain weighted Sobolev
spaces. Next, we show that the solution can be decomposed into regular and
singular parts. A peculiar property of this decomposition enables us to
introduce a radiation-like condition in a bounded domain and to state a
well-posed problem satisfied by the limiting absorption solution.

</details>


### [17] [Rogue waves and large deviations for 2D pure gravity deep water waves](https://arxiv.org/abs/2510.15159)
*Massimiliano Berti,Ricardo Grande,Alberto Maspero,Gigliola Staffilani*

Main category: math.AP

TL;DR: Rigorous characterization of rogue wave formation probability in deep water gravity wave equations, confirming oceanography conjectures about dispersive focusing mechanism.


<details>
  <summary>Details</summary>
Motivation: To quantify the probability of rogue wave formation under random Gaussian sea initial data, addressing a central problem in oceanography and mathematics.

Method: Combines normal forms and probabilistic methods to propagate statistical information over long timescales, tracking tail probability of solutions to Hamiltonian PDEs with integrable normal form and random Gaussian initial data.

Result: Large deviation result confirms conjectures about rogue wave formation through dispersive focusing in weakly nonlinear regime, holding up to optimal timescales allowed by deterministic well-posedness theory.

Conclusion: Rogue waves most likely arise through dispersive focusing where phase quasi-synchronization produces constructive amplification of water crest, with novel approach that doesn't require approximate solutions to be Gaussian.

Abstract: Rogue waves are extreme ocean events characterized by the sudden formation of
anomalously large crests, and remain an important subject of investigation in
oceanography and mathematics. A central problem is to quantify the probability
of their formation under random Gaussian sea initial data. In this work, we
rigorously characterize the tail probability for the formation of rogue waves
of the pure gravity water wave equations in deep water, the most accurate
quasilinear PDE modeling waves in open ocean. This large deviation result
confirms various conjectures from the oceanography literature in the weakly
nonlinear regime. Moreover, the result holds up to the optimal timescales
allowed by deterministic well-posedness theory. The proof shows that rogue
waves most likely arise through "dispersive focusing", where phase
quasi-synchronization produces constructive amplification of the water crest.
The main difficulty in justifying this mechanism is propagating statistical
information over such long timescales, which we overcome by combining normal
forms and probabilistic methods. Unlike prior work, this novel approach does
not require approximate solutions to be Gaussian. Our general method tracks the
tail probability of solutions to Hamiltonian PDEs with an integrable normal
form and random Gaussian initial data, even in the absence of (quasi-)invariant
measures.

</details>


### [18] [Dyadic microlocal partition for anisotropic metrics and uniform Weyl quantization](https://arxiv.org/abs/2510.15183)
*Vicente Vergara*

Main category: math.AP

TL;DR: The paper develops a dyadic microlocal partition for anisotropic metrics in phase space, proving uniform bounds for Weyl quantization and Moyal truncation with explicit remainder control. It provides tools for analyzing pseudodifferential and Fourier integral operators in non-homogeneous settings.


<details>
  <summary>Details</summary>
Motivation: To address the need for robust analysis tools in non-homogeneous settings where anisotropy varies with position, particularly for pseudodifferential and Fourier integral operators.

Method: Uses a dyadic microlocal partition adapted to anisotropic metrics, semiclassical per-band renormalization, and Cotlar-Stein almost-orthogonality scheme with explicit remainder control.

Result: Proves uniform bounds for localized Weyl quantization and Moyal truncation, recovers exact scaling on frequency bands, ensures global convergence, and provides operator-norm estimates with transparent dependence on symbol seminorms.

Conclusion: The approach provides a constructive toolset for uniform localization, composition, and recombination of pseudodifferential and Fourier integral operators, with applications including microlocal parametrix construction and Radon transform analysis.

Abstract: We develop a dyadic microlocal partition adapted to position-dependent
anisotropic metrics in phase space and prove uniform bounds for localized Weyl
quantization and for Moyal truncation with explicit control of remainders. A
semiclassical, per-band renormalization recovers the exact scaling on each
frequency band, while a Cotlar-Stein almost-orthogonality scheme ensures global
convergence and operator-norm estimates with transparent dependence on finitely
many symbol seminorms. The approach is robust in non-homogeneous settings where
anisotropy varies with position. As applications, we construct a microlocal
parametrix via truncated Moyal expansion and develop a local-global analysis of
the Radon transform viewed as a model Fourier integral operator. The results
provide a constructive toolset for uniform localization, composition, and
recombination of pseudodifferential and Fourier integral operators.

</details>


### [19] [H√∂lder damping for fractional wave equations](https://arxiv.org/abs/2510.15213)
*Jian Wang,Ruoyu P. T. Wang*

Main category: math.AP

TL;DR: Quantitative energy decay rates for fractional wave equations with low H√∂lder regularity damping under geometric control condition.


<details>
  <summary>Details</summary>
Motivation: To establish explicit energy decay rates for solutions of fractional wave equations when damping has low H√∂lder regularity, showing how decay rates depend on damping regularity.

Method: Analysis of fractional wave equations with H√∂lder regular damping functions under geometric control condition.

Result: Energy decay rates depend explicitly on H√∂lder regularity of damping; lower regularity below threshold gives slower decay.

Conclusion: Quantitative relationship established between damping regularity and energy decay rates in fractional wave equations.

Abstract: For fractional wave equations with low H\"older regularity damping, we
establish quantitative energy decay rates for their solutions when the
geometric control condition holds. The energy decay rates depend explicitly on
the H\"older regularity of the damping. In particular, we show damping
functions with lower H\"older regularities that below a certain threshold give
slower energy decay.

</details>


### [20] [Nonradial Quenching Profile for a MEMS Model](https://arxiv.org/abs/2510.15246)
*Hsuan-Lin Liao,Van Tien Nguyen*

Main category: math.AP

TL;DR: Construction of a non-radial quenching solution to the parabolic MEMS equation in 2D with explicit final profile near quenching point.


<details>
  <summary>Details</summary>
Motivation: To provide the first example of a quenching solution with genuinely non-radial profile for the MEMS equation, moving beyond radial symmetry assumptions.

Method: Construct approximate solution using perturbative expansion in self-similar variables, then justify true solution through spectral analysis and robust energy method.

Result: Successfully constructed quenching solution that quenches only at origin with explicit non-radial final profile involving x‚ÇÅ¬≤x‚ÇÇ¬≤ and Œ∏(x‚ÇÅ‚Å∂ + x‚ÇÇ‚Å∂) terms.

Conclusion: This represents the first genuine non-radial quenching solution for MEMS equations, demonstrating existence beyond radial symmetry constraints.

Abstract: We construct a quenching solution to the parabolic MEMS model \[ u_t = \Delta
u - \frac{1}{u^2} \quad \text{in } \mathcal{B} \times (0,T), \quad u|_{\partial
\mathcal{B}} = 1, \] where $\mathcal{B}$ is the unit disc in $\mathbb{R}^2$,
and $T > 0$ denotes the quenching time. The constructed solution quenches only
at the origin and admits the final profile \[ u(x,T) \sim \left(x_1^2 x_2^2 +
\theta(x_1^6 + x_2^6)\right)^{\frac{1}{3}} \quad \text{as } |x| \to 0, \] where
$\theta \in (0, \theta^*)$ for some $\theta^* > 0$. To our knowledge, this is
the first example of a quenching solution with a genuinely non-radial profile.
The proof relies on the construction of a good approximate solution, using a
perturbative expansion in self-similar variables. We then justify the true
solution that remains close to this approximation through a spectral analysis
combined with a robust energy method.

</details>


### [21] [The Wiener Criterion at $\infty$ for Degenerate Elliptic Equations](https://arxiv.org/abs/2510.15249)
*Ugur G. Abdulla,Denis Brazke*

Main category: math.AP

TL;DR: Establishes a Wiener criterion at infinity to characterize unique solvability of Dirichlet problems for degenerate elliptic equations with power-like weights in arbitrary open sets.


<details>
  <summary>Details</summary>
Motivation: To provide a criterion for determining when degenerate elliptic equations with power-like weights have unique solutions to the Dirichlet problem in arbitrary open sets.

Method: Develops a Wiener criterion at infinity that works in both measure-theoretical and topological contexts - analyzing A-harmonic measure and fine topology thinness.

Result: The criterion determines whether the A-harmonic measure of infinity is null or positive, and provides a test for thinness of the exterior set at infinity in the A-fine topology.

Conclusion: The Wiener criterion successfully characterizes unique solvability of Dirichlet problems for degenerate elliptic equations with power weights in arbitrary domains.

Abstract: This paper establishes a Wiener criterion at $\infty$ to characterise the
unique solvability of the Dirichlet problem for degenerate elliptic equations
with power-like weights in arbitrary open sets. In the measure-theoretical
context, the criterion determines whether the $\A$-harmonic measure of $\infty$
is null or positive. From the topological point of view, it presents a test for
the thinness of the exterior set at $\infty$ in the $\A$-fine topology.

</details>


### [22] [Global existence and stability in a class of chemotaxis systems with lethal interactions, nonlinear diffusion and production](https://arxiv.org/abs/2510.15276)
*Gnanasekaran Shanmugasundaram,Jitraj Saha*

Main category: math.AP

TL;DR: This paper analyzes chemotaxis systems with lethal interactions, examining both fully parabolic and parabolic-elliptic cases, and proves global bounded classical solutions and asymptotic stability using Lyapunov functionals.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of chemotaxis systems modeling lethal interactions in biological contexts, particularly focusing on solution behavior and stability across different mathematical formulations.

Method: Mathematical analysis of two chemotaxis system types: fully parabolic and parabolic-elliptic systems with homogeneous Neumann boundary conditions, using parameter constraints and Lyapunov functionals.

Result: Established existence of unique globally bounded classical solutions for arbitrary spatial dimensions n‚â•1 under appropriate parameter constraints, and obtained rigorous asymptotic stability results.

Conclusion: The study successfully demonstrates well-posedness and stability properties for chemotaxis systems with lethal interactions, providing mathematical foundations for understanding such biological phenomena.

Abstract: This paper investigates a class of chemotaxis systems modeling lethal
interactions in a smooth, bounded domain $\Omega \subset \mathbb{R}^n$ with
homogeneous Neumann boundary conditions. We examine two distinct cases: (i) a
fully parabolic system where both equations exhibit parabolic dynamics, and
(ii) a parabolic-elliptic system featuring a parabolic first equation coupled
with an elliptic second equation. Under appropriate parameter constraints, we
establish the existence of unique globally bounded classical solutions for
arbitrary spatial dimensions $n \geq 1$. Additionally, we employ carefully
constructed Lyapunov functionals to analyze the long-term behavior of
solutions, obtaining rigorous asymptotic stability results.

</details>


### [23] [Continuity estimates for variable growth variational problems in the Heisenberg group](https://arxiv.org/abs/2510.15359)
*Arka Mallick,Swarnendu Sil*

Main category: math.AP

TL;DR: This paper establishes regularity results for local minimizers of variable growth variational problems in Heisenberg groups, proving continuity properties of the horizontal gradient under different integrability assumptions on the exponent function.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend regularity theory to variable growth variational problems in the setting of Heisenberg groups, which has not been previously studied. The work aims to understand continuity properties of minimizers' horizontal gradients under different integrability conditions.

Method: The authors study local minimizers of a variable growth functional in Heisenberg groups, analyzing the horizontal gradient under two different integrability assumptions on the exponent function's horizontal gradient. They prove H√∂lder continuity and continuity results using variational methods.

Result: Two main results are proven: (a) If the horizontal gradient of p belongs to L^q with q>Q, then the horizontal gradient of u is H√∂lder continuous; (b) If the horizontal gradient of p belongs to a Lorentz-Zygmund space, then the horizontal gradient of u is continuous. The paper also establishes H√∂lder continuity without derivative assumptions on p in the non-borderline case.

Conclusion: This work provides the first regularity results for minimizers of variable growth variational problems in Heisenberg groups, establishing continuity properties of horizontal gradients under different integrability conditions on the exponent function.

Abstract: We study regularity results for local minimizers of variable growth
variational problem in Heisenberg groups under suitable integrability
assumption on the horizontal gradient of the exponent function. More precisely,
our main focus is on the continuity properties of the horizontal gradient
$\mathfrak{X} u$, where $u \in HW_{\text{loc}}^{1,1}$ is a local minimizer of
the functional
  \begin{align*}
  I [u]:= \int_{\Omega} \frac{1}{p(x)}\left\lvert \mathfrak{X} u
\right\rvert^{p(x)}\ \mathrm{d}x
  \end{align*} in a domain of $\Omega \subset \mathbb{H}_{n},$ where
$\mathbb{H}_{n}$ is the Heisenberg group with homogeneous dimension $Q=2n+2,$
where $p \in HW^{1,1}\left( \Omega\right)$ and we assume suitable integrability
hypothesis on $\mathfrak{X} p.$ We prove (a) if $\mathfrak{X} p \in L^{q}\left(
\Omega; \mathbb{R}^{2n}\right)$ with $q>Q,$ then $\mathfrak{X} u$ is H\"{o}lder
continuous and (b) if $\mathfrak{X} p \in L^{(Q,1)}\log L \left( \Omega;
\mathbb{R}^{2n}\right),$ then $\mathfrak{X} u$ is continuous.
  In fact, in the non-borderline case $(a)$, we prove H\"{o}lder continuity of
the horizontal gradient for the minima of more general variational problems,
assuming $p$ to be H\"{o}lder continuous, i.e. without any assumption on the
weak derivative of $p.$ To the best of our knowledge, the present work is the
first regularity result for minimizers of variable growth variational problems
in the setting of Heisenberg groups.

</details>


### [24] [Nonrelativistic limit of normalized solutions of nonlinear Dirac equations on noncompact metric graphs with localized nonlinearities](https://arxiv.org/abs/2510.15378)
*Zhentao He,Chao Ji*

Main category: math.AP

TL;DR: Study of nonrelativistic limit of normalized solutions for nonlinear Dirac equation on metric graphs with compact core, under mass constraint.


<details>
  <summary>Details</summary>
Motivation: First investigation of nonrelativistic limit for normalized solutions to nonlinear Dirac equation on metric graphs, addressing a gap in existing literature.

Method: Analysis of nonlinear Dirac equation on noncompact metric graphs with finitely many edges and compact core, using constraint ‚à´|u|¬≤dx=1 and studying limit behavior.

Result: Provides first results on nonrelativistic limit behavior of normalized solutions for this class of equations on metric graphs.

Conclusion: Pioneering work establishing foundation for studying nonrelativistic limits in nonlinear Dirac equations on metric graph structures.

Abstract: In this paper, we study the nonrelativistic limit of normalized solutions for
the following nonlinear Dirac equation (NLDE) on noncompact metric graph $\G$
with finitely many edges and a non-empty compact core $\K$ \begin{equation*}
  \D u - \omega u= \chi_\K\abs{u}^{p-2}u, \end{equation*} under the constraint
$\int_\G\abs{u}^2\,dx = 1$, where $\D$ is the Dirac operator on $\G$, $u: \G
\to \mathbb{C}^2$, the frequency $\omega \in \mathbb{R}$ is part of the
unknowns which arises as a Lagrange multiplier, $\chi_\K$ is the characteristic
function of the compact core $\K$, and $2<p<6$. To the best of our knowledge,
this is the first study to investigate the nonrelativistic limit of normalized
solutions to (NLDE) on metric graphs.

</details>


### [25] [Gradient Flows for the $p$-Laplacian Arising from Biological Network Models: A Novel Dynamical Relaxation Approach](https://arxiv.org/abs/2510.15379)
*Jan Haskovec,Peter Markowich,Stefano Zampini*

Main category: math.AP

TL;DR: The paper develops a scalar PDE model for biological transportation networks, deriving it as a continuum limit from discrete graphs and establishing existence of minimizers. It connects to p-Laplacian equations and proposes an efficient numerical scheme with optimal convergence.


<details>
  <summary>Details</summary>
Motivation: To understand biological transportation network formation through mathematical modeling and develop efficient computational methods for solving related p-Laplacian equations.

Method: Derived continuum energy functional as Œì-limit from discrete graph-based formulation on triangulations, established existence of minimizers, implemented finite element discretizations with novel dynamical relaxation scheme.

Result: Achieved optimal convergence rates in manufactured tests, mesh-independent performance with stable computational costs under refinement, and demonstrated ability to reproduce biologically relevant network patterns.

Conclusion: The scalar model effectively captures biological network formation and provides an efficient relaxation strategy for solving p-Laplacian equations with large exponents.

Abstract: We investigate a scalar partial differential equation model for the formation
of biological transportation networks. Starting from a discrete graph-based
formulation on equilateral triangulations, we rigorously derive the
corresponding continuum energy functional as the $\Gamma$-limit under network
refinement and establish the existence of global minimizers. The model
possesses a gradient-flow structure whose steady states coincide with solutions
of the $p$-Laplacian equation. Building on this connection, we implement finite
element discretizations and propose a novel dynamical relaxation scheme that
achieves optimal convergence rates in manufactured tests and exhibits
mesh-independent performance, with the number of time steps, nonlinear
iterations, and linear solves remaining stable under uniform mesh refinement.
Numerical experiments confirm both the ability of the scalar model to reproduce
biologically relevant network patterns and its effectiveness as a
computationally efficient relaxation strategy for solving $p$-Laplacian
equations for large exponents $p$.

</details>


### [26] [Asymptotic Blow-up Behavior for the Semilinear Heat Equation with Super-exponential Nonlinearities](https://arxiv.org/abs/2510.15402)
*Ryoto Ichiya*

Main category: math.AP

TL;DR: Study of blow-up behavior for semilinear heat equations with super-exponential nonlinearities in radial domains, proving asymptotic blow-up rates for type I solutions when n‚â§2.


<details>
  <summary>Details</summary>
Motivation: To extend previous results on blow-up behavior from exponential nonlinearities (f(u)=e^u) to more general super-exponential cases (f(u)=e^{u^p}u^q), which lack scale invariance properties.

Method: Treat the equation as a perturbation of the f(u)=e^u case using a transformation by Fujishima and Ioku (2018), and estimate the additional terms that appear after this transformation.

Result: Proved that for n‚â§2, nonnegative radial type I blow-up solutions satisfy the asymptotic relation lim_{t‚ÜíT} (T-t)/F(u(y‚àö(T-t),t)) = 1, where F(u)=‚à´_u^‚àû ds/f(s).

Conclusion: Successfully extended Liu's 1989 result for f(u)=e^u to super-exponential nonlinearities, establishing similar blow-up behavior despite the lack of scale invariance in the more general case.

Abstract: We consider the semilinear heat equation $u_t - \Delta u = f(u)$ in $\Omega =
B_R(0) \subset \mathbb{R}^n$ with super-exponential nonlinearities $f(u) =
e^{u^p}u^q$ ($p>1$, $q \in \{0\}\cup [1,\infty)$), nonnegative bounded radially
symmetric initial data and 0-Dirichlet boundary condition. In this paper, we
show the asymptotic blow-up behavior for nonnegative, radial type I blow-up
solution. More precisely, we prove that if $n \leq 2$, then such blow-up
solution satisfies \begin{equation*} \lim_{t \rightarrow T}
\frac{T-t}{F(u(y\sqrt{T-t},t))} = 1, \quad \text{where } F(u) =
\int_{u}^{\infty} \frac{ds}{f(s)}. \end{equation*} We note that this result
corresponds to the one which is proved by Liu in 1989 for the case of $f(u) =
e^u$, which has the scale invariance property unlike our super-exponential
case. To prove the main result, we see the equation as a perturbation of the
equation with $f(u) = e^u$ through a transformation introduced by Fujishima and
Ioku in 2018 and estimate the additional term which appears after the
transformation.

</details>


### [27] [Second-Order Gamma Limit for the Cahn-Hilliard Functional with Dirichlet Boundary Conditions](https://arxiv.org/abs/2510.15431)
*Francesco Colasanto,Pascal Steinke*

Main category: math.AP

TL;DR: Asymptotic development of order 2 by Gamma convergence for Cahn-Hilliard functional with Dirichlet boundary conditions and subquadratic potential growth near wells


<details>
  <summary>Details</summary>
Motivation: To study the asymptotic behavior and convergence properties of the Cahn-Hilliard functional, particularly focusing on cases with subquadratic growth near the potential wells under Dirichlet boundary conditions

Method: Using Gamma convergence techniques to analyze the asymptotic development of order 2 for the Cahn-Hilliard functional

Result: Established asymptotic development results for the Cahn-Hilliard functional with subquadratic potential growth near wells under Dirichlet boundary conditions

Conclusion: Successfully developed asymptotic analysis framework for Cahn-Hilliard functional with subquadratic potential growth using Gamma convergence methods

Abstract: This paper addresses the asymptotic development of order 2 by Gamma
convergence of the Cahn-Hillard functional with Dirichlet boundary conditions,
where the potential has subquadratic growth near the wells.

</details>


### [28] [Mixed local-nonlocal equations with critical nonlinearity on $\mathbb{R}^N$: Non-existence, Existence, and Multiplicity of positive solutions](https://arxiv.org/abs/2510.15507)
*Nirjan Biswas,Souptik Chakraborty,Paramananda Das*

Main category: math.AP

TL;DR: Study of critical problems with mixed local-nonlocal operators in R^N. For f=0, no nontrivial weak solutions exist in L^p spaces, contrasting with purely local/nonlocal cases. For small nonzero f, existence of positive weak solutions is proven.


<details>
  <summary>Details</summary>
Motivation: To understand the existence and multiplicity of solutions for critical problems involving mixed local-nonlocal operators, which exhibit different behavior from purely local or purely nonlocal cases.

Method: Proving a concentration compactness principle for the mixed operator -Œî_p + (-Œî_p)^s, and using variational methods to establish existence results for small nonzero forcing terms.

Result: When f=0, no nontrivial weak solutions exist in L^p(R^N). For small nonzero f, at least two positive weak solutions exist for the semilinear case (p=2) under dimensional restrictions, and at least one positive weak solution exists for the nonlinear case (p>1).

Conclusion: The mixed local-nonlocal operator exhibits unique behavior: trivial solution space for zero forcing, but existence of positive solutions for small nonzero forcing, requiring new compactness principles.

Abstract: We study the following critical problem involving the mixed local-nonlocal
operator: \begin{equation}\label{main_prob_abstract}\tag{$\mathcal{P}_2$}
  -\Delta u+(-\Delta)^s u=|u|^{2^*-2}u+f(x)\text{ in }\mathbb{R}^N,
\end{equation} where $N \ge 3,\, s\in (0,1),\, 2^*= \frac{2N}{N-2}$, and $f$ is
a nontrivial non-negative functional which lies in the dual space of the
ambient solution space. For $f \equiv0$, ($\mathcal{P}_2$) does not admit any
nontrivial weak solution in $L^2(\mathbb{R}^N)$. This phenomenon stands in
contrast to the purely local (for $N>4$) and purely nonlocal (for $N>4s$)
cases. On the other hand, when $f \not \equiv 0$, we prove the existence of at
least two positive weak solutions to ($\mathcal{P}_2$), provided that the
dimension $N$ satisfies certain restrictions and $\|f\|$ is small in the
corresponding dual space. Next, we consider the nonlinear analogue to
($\mathcal{P}_2$), namely
\begin{equation}\label{main_prob_abstract_1}\tag{$\mathcal{P}_p$}
  -\Delta_p u+(-\Delta_p)^s u=|u|^{p^*-2}u+f(x)\text{ in }\mathbb{R}^N,
\end{equation} where $p \in (1, \infty), N>p$, $p^*=\frac{Np}{N-p}$, $f$ is a
nontrivial non-negative functional in the dual space of the ambient solution
space. As in the semilinear case, no nontrivial weak solution exists for
($\mathcal{P}_p$) in $L^p(\mathbb{R}^N)$, when $f \equiv 0$. Moreover, for $f
\not \equiv 0$ with sufficiently small $\|f\|$, ($\mathcal{P}_p$) admits a
positive weak solution. For the existence, we prove a concentration compactness
principle for $-\Delta_p+(-\Delta_p)^s$.

</details>


### [29] [Construction and properties for the Green's function with Neumann boundary condition](https://arxiv.org/abs/2510.15544)
*Antoine Bricmont*

Main category: math.AP

TL;DR: Construction and analysis of the Green's function for the Neumann boundary value problem of the operator -Œî + a on smooth bounded domains in R^N (N‚â•3), including existence, uniqueness, and qualitative properties.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for the Green's function associated with the Neumann boundary value problem for elliptic operators of the form -Œî + a, which is fundamental for solving partial differential equations and understanding their qualitative behavior.

Method: Explicit construction of the Green's function G(x,y) under coercivity assumption of -Œî + a, with detailed analysis of pointwise estimates, derivative estimates near singularities, and comparison with the Laplacian's Green's function near boundaries.

Result: Proved existence and uniqueness of the Green's function, established pointwise estimates and derivative bounds near singularities, obtained boundary behavior comparisons, and demonstrated properties like symmetry and positivity.

Conclusion: The paper provides a complete theoretical framework for the Green's function of the Neumann problem for coercive elliptic operators, establishing its fundamental properties and behavior that are essential for applications in PDE theory.

Abstract: This article addresses the construction and analysis of the Green's function
for the Neumann boundary value problem associated with the operator $-\Delta +
a$ on a smooth bounded domain $\Omega \subset \mathbb{R}^N$ ($N \geq 3$) with
$a\in L^\infty(\Omega)$. Under the assumption that $-\Delta + a$ is coercive,
we obtain the existence, uniqueness, and qualitative properties of the Green's
function $G(x,y)$. The Green's function $G(x,y)$ is constructed explicitly,
satisfying pointwise estimates and derivative estimates near the singularity.
Also, near the boundary of $\Omega$, $G$ is compared to the Green's function of
the laplacian, with pointwise estimates. Other properties, like symmetry and
positivity among other things, are established.

</details>


### [30] [Optimal Sobolev Regularity for Second Order Divergence Elliptic Operators on Domains with Buried Boundary Parts](https://arxiv.org/abs/2510.15631)
*Joachim Rehberg,Elmar Schrohe*

Main category: math.AP

TL;DR: Study of regularity of solutions for elliptic second order boundary value problems in 3D with discontinuous coefficients and mixed boundary conditions, where Dirichlet boundary is partly 'buried' in the domain.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of solutions to elliptic boundary value problems with mixed boundary conditions and discontinuous coefficients, particularly when the Dirichlet boundary is partially embedded within the domain rather than just on the surface.

Method: Analysis of elliptic second order boundary value problems on bounded domains in R¬≥ with coefficients that are not necessarily continuous, using mixed boundary conditions (Dirichlet on one part, Neumann on another) where the Dirichlet boundary is partly 'buried' in the domain.

Result: The singularity of the solution along the border of the buried contact behaves exactly like the singularity for solutions of mixed boundary value problems along the border between Dirichlet and Neumann boundary parts.

Conclusion: The study establishes that the singularity behavior in solutions with buried Dirichlet boundaries follows the same pattern as traditional mixed boundary problems, providing important regularity results for these more complex geometric configurations.

Abstract: We study the regularity of solutions of elliptic second order boundary value
problems on a bounded domain $\Omega$ in $\mathbb R^3$. The coefficients are
not necessarily continuous and the boundary conditions may be mixed, i.e.
Dirichlet on one part $D$ of the boundary and Neumann on the complementing
part. The peculiarity is that $D$ is partly `buried' in $\Omega$ in the sense
that the topological interior of $\Omega \cup D$ properly contains $\Omega$.
The main result is that the singularity of the solution along the border of the
buried contact behaves exactly as the singularity for the solution of a mixed
boundary value problem along the border between the Dirichlet and the Neumann
boundary part.

</details>


### [31] [Derivation and quasi-invariant asymptotics of phenotype-structured integro-differential models](https://arxiv.org/abs/2510.15646)
*Emanuele Bernardi,Tommaso Lorenzi,Andrea Tosin*

Main category: math.AP

TL;DR: Developed a modeling framework for phenotype-structured populations that bridges individual-level mechanisms with population-scale evolutionary dynamics, deriving from stochastic agent-based models to mesoscopic integro-differential equations and non-local Fokker-Planck equations.


<details>
  <summary>Details</summary>
Motivation: To extend kinetic theory approaches to multi-agent systems where total mass is not conserved, enabling connection between individual-level mechanisms and population-scale evolutionary dynamics in phenotype-structured populations.

Method: Formulated stochastic agent-based model for population dynamics (proliferation, death, phenotype changes), derived corresponding mesoscopic integro-differential equation, and further derived non-local Fokker-Planck equation in quasi-invariant regime of small frequent phenotype changes.

Result: Successfully established a hierarchical modeling framework connecting microscopic agent-based dynamics to mesoscopic and macroscopic descriptions, with phenotype changes modeled via integral kernels and advection-diffusion terms.

Conclusion: The framework provides rigorous mathematical foundation for studying phenotype-structured population dynamics, bridging scales from individual agents to population-level evolutionary patterns, with numerical simulations validating the theoretical results.

Abstract: Building upon kinetic theory approaches for multi-agent systems and
generalising them to scenarios where the total mass of the system is not
conserved, we develop a modelling framework for phenotype-structured
populations that makes it possible to bridge individual-level mechanisms with
population-scale evolutionary dynamics. We start by formulating a stochastic
agent-based model, which describes the dynamics of single population members
undergoing proliferation, death, and phenotype changes. Then, we formally
derive the corresponding mesoscopic model, which consists of an
integro-differential equation for the distribution of population members over
the space of phenotypes, where phenotype changes are modelled via an integral
kernel. Finally, considering a quasi-invariant regime of small but frequent
phenotype changes, we rigorously derive a non-local Fokker-Planck-type equation
counterpart of this model, wherein phenotype changes are taken into account by
an advection-diffusion term. The theoretical results obtained are illustrated
through a sample of results of numerical simulations.

</details>


### [32] [Existence results for variational quasilinear elliptic systems involving the vectiorial $p$-Laplacian](https://arxiv.org/abs/2510.15694)
*Annamaria Canino,Simone Mauro*

Main category: math.AP

TL;DR: Existence and regularity results for p-Laplacian elliptic systems with Dirichlet boundary conditions, including classification of least energy solutions for Lane-Emden-type systems.


<details>
  <summary>Details</summary>
Motivation: To study existence, regularity, and classification of solutions for nonlinear elliptic systems involving the p-Laplacian operator, particularly focusing on Lane-Emden-type systems which generalize classical scalar equations to vector-valued settings.

Method: Mathematical analysis techniques including variational methods, existence theory for elliptic systems, regularity theory, and classification arguments for least energy solutions.

Result: Proved existence and regularity for general p-Laplacian systems, and showed that any least energy solution of the Lane-Emden system has the form (c¬πœâ,...,c·µêœâ) where c is on the unit sphere and œâ is a positive scalar solution.

Conclusion: The paper establishes fundamental existence and regularity results for p-Laplacian elliptic systems and provides a complete classification of least energy solutions for Lane-Emden-type systems, revealing their scalar-like structure.

Abstract: We prove existence and regularity results for the following elliptic system:
\[ \begin{cases}
-\textbf{div}(|D\boldsymbol{u}|^{p-2}D\boldsymbol{u})=\boldsymbol{f}(x,\boldsymbol{u})
& \text{in } \Omega \\ \boldsymbol{u}=0 & \text{on } \partial\Omega,
\end{cases} \] where $\boldsymbol{u}=(u^1,\dots,u^m)$, $p>1$, and
$\Omega\subset\mathbb{R}^N$ is a bounded domain. We also consider the special
case of the Lane-Emden-type system, taking
\[\boldsymbol{f}(x,\boldsymbol{u})=\lambda|\boldsymbol{u}|^{p-2}\boldsymbol{u}+|\boldsymbol{u}|^{q-2}\boldsymbol{u},\]
and we prove a classification result. In particular, we show that any least
energy solution for the Lane-Emden system is of the form
$(c^1\omega,\dots,c^m\omega)$, where $\boldsymbol{c}=(c^1,\dots,c^m)\in
S^{m-1}$ (the $(m-1)$-sphere in $\mathbb R^m$) and $\omega$ is a positive
solution of the corresponding scalar equation.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [33] [Random walk models of anisotropic diffusion on rectangular and hexagonal lattices](https://arxiv.org/abs/2510.15291)
*Luke P. Filippini,Adrianne L. Jenner,Elliot J. Carr*

Main category: physics.comp-ph

TL;DR: The paper presents discrete random walk models for anisotropic diffusion on rectangular and hexagonal lattices, showing that hexagonal lattices overcome tensor constraints found in rectangular implementations.


<details>
  <summary>Details</summary>
Motivation: To address limitations of deterministic diffusion models that can't capture inherent randomness and assume large particle numbers, by developing stochastic random walk alternatives.

Method: Discretize deterministic diffusion PDEs using finite volume method on rectangular/hexagonal lattices with forward Euler time stepping, creating homogeneous Markov chain random walk models with analytical transition probabilities.

Result: Good visual and quantitative agreement between deterministic models and random walk simulations. Rectangular lattices require diffusion tensor constraints while hexagonal lattices have no such restrictions.

Conclusion: Hexagonal lattice random walk models provide more flexible implementation for anisotropic diffusion without tensor constraints, with MATLAB code available for implementation.

Abstract: The diffusive transport of particles in anisotropic media is a fundamental
phenomenon in computational, medical and biological disciplines. While
deterministic models (partial differential equations) of such processes are
well established, their inability to capture inherent randomness, and the
assumption of a large number of particles, hinders their applicability. To
address these issues, we present several equivalent (discrete-space
discrete-time) random walk models of diffusion described by a
spatially-invariant tensor on a two-dimensional domain with no-flux boundary
conditions. Our approach involves discretising the deterministic model in space
and time to give a homogeneous Markov chain governing particle movement between
(spatial) lattice sites over time. The spatial discretisation is carried out
using a vertex-centred element-based finite volume method on rectangular and
hexagonal lattices, and a forward Euler discretisation in time yields a
nearest-neighbour random walk model with simple analytical expressions for the
transition probabilities. For each lattice configuration, analysis of these
expressions yields constraints on the time step duration, spatial steps and
diffusion tensor to ensure the probabilities are between zero and one. We find
that model implementation on a rectangular lattice can be achieved with a
constraint on the diffusion tensor, whereas a hexagonal lattice overcomes this
limitation (no restrictions on the diffusion tensor). Overall, the results
demonstrate good visual and quantitative (mean-squared error) agreement between
the deterministic model and random walk simulations for several test cases. All
results are obtained using MATLAB code available on GitHub
(https://github.com/lukefilippini/Filippini2025).

</details>


### [34] [Constrained bilinear optimal control of reactive evolution equations](https://arxiv.org/abs/2510.15293)
*Zhexian Li,Felipe de Barros,Ketan Savla*

Main category: physics.comp-ph

TL;DR: A novel optimize-then-discretize framework for constrained bilinear optimal control of second-order PDEs with both state and control constraints, using integral representations from unified transform method.


<details>
  <summary>Details</summary>
Motivation: Existing methods only handle control constraints, but many physical applications require both state and control constraints. Current approaches derive optimality conditions directly from PDEs, which is complex for constrained problems.

Method: Replace PDE constraint with equivalent integral representation using unified transform method, derive necessary optimality conditions via KKT conditions for infinite-dimensional optimization, then discretize to obtain smooth nonlinear equations.

Result: Framework successfully applied to nuclear reactivity control and water quality treatment, showing effectiveness without needing specialized algorithms.

Conclusion: The optimize-then-discretize approach with integral representations provides efficient solution for constrained bilinear optimal control problems with both state and control constraints.

Abstract: We consider constrained bilinear optimal control of second-order linear
evolution partial differential equations (PDEs) with a reaction term on the
half line, where control arises as a time-dependent reaction coefficient and
constraints are imposed on the state and control variables. These PDEs
represent a wide range of physical phenomena in fluid flow, heat, and mass
transfer. Existing computational methods for this type of control problems only
consider constraints on control variables. In this paper, we propose a novel
optimize-then-discretize framework for computing constrained bilinear optimal
control with both state and control constraints. Unlike existing methods that
derive optimality conditions directly from the PDE constraint, this framework
first replaces the PDE constraint with an equivalent integral representation of
the PDE solution. The integral representation, derived from the unified
transform method, does not involve differential operators, and thus explicit
expressions for necessary conditions of optimality can be derived using the
Karush-Kuhn-Tucker conditions for infinite-dimensional optimization.
Discretizing the optimality conditions results in a system of
finite-dimensional smooth nonlinear equations, which can be efficiently solved
using existing solvers without the need for specialized algorithms. This is in
contrast with discretize-then-optimize methods that discretize the PDE first
and then solve the optimality conditions of the approximated finite-dimensional
problem. Computational results for two applications, namely nuclear reactivity
control and water quality treatment in a reactor, are presented to illustrate
the effectiveness of the proposed framework.

</details>


### [35] [Towards In-Situ Failure Assessment: Deep Learning on DIC Results for Laminated Composites](https://arxiv.org/abs/2510.15424)
*Amir Mohammad Mirzaei*

Main category: physics.comp-ph

TL;DR: A deep learning framework using DIC strain field data predicts fracture loads in laminated composites with stress raisers, achieving R¬≤ values of 0.86 (MLP) and 0.82 (CNN) without needing finite element simulations.


<details>
  <summary>Details</summary>
Motivation: Predicting fracture load in composites with stress raisers is difficult due to complex failure mechanisms influenced by fiber orientation, layup sequence, and notch geometry. Existing methods require finite element simulations or empirical calibrations.

Method: Two deep learning architectures: MLP processing numerical maximum principal strain values from a targeted region with feature selection (mutual information, Lasso, SHAP), and CNN trained on full-field strain images with data augmentation. Validated on 116 tests across 31 configurations.

Result: MLP achieved R¬≤ of 0.86, CNN achieved R¬≤ of 0.82 across various composite configurations including different layups, notch types, and orientations. The framework captured diverse damage modes from brittle fiber fracture to ductile delamination.

Conclusion: The computationally efficient framework enables practical in-situ fracture load estimation using only DIC measurements, bypassing traditional simulation requirements while handling complex composite failure mechanisms.

Abstract: Predicting fracture load in laminated composites with stress raisers is
challenging due to complex failure mechanisms such as delamination, fibre
breakage, and matrix cracking, which are heavily influenced by fibre
orientation, layup sequence, and notch geometry. This study aims to address
this by developing a novel deep learning framework that leverages solely
experimental strain field data from Digital Image Correlation (DIC) for
accurate, in-situ predictions--bypassing the need for finite element
simulations or empirical calibrations. Two complementary architectures are
explored: a multi-layer perceptron (MLP) that processes numerical values of
maximum principal strain from a targeted rectangular region ahead of the notch,
enhanced by advanced feature selection (mutual information, Lasso, and SHAP) to
focus on critical data points; and a convolutional neural network (CNN) trained
on full-field strain images, bolstered by data augmentation to handle
variability and prevent overfitting. Validated across 116 quasi-static tests
encompassing 31 distinct configurations--including six layups (quasi-isotropic
to highly anisotropic) with four off-axis angles for open-hole specimens, and
one cross-ply layup with four off-axis and four on-axis notch orientations for
U-notched specimens--the MLP and CNN achieve coefficients of determination
(R^2) of 0.86 and 0.82, respectively. This framework captures a broad spectrum
of damage modes and responses, from brittle fibre-dominated fracture to ductile
delamination-driven failure, and due to its computational efficiency and
reliance only on DIC measurements, the approach enables practical in-situ
fracture load estimation.

</details>


### [36] [Boundary-Informed Method of Lines for Physics Informed Neural Networks](https://arxiv.org/abs/2510.15852)
*Maximilian Cederholm,Siyao Wang,Haochun Wang,Ruichen Xu,Yuefan Deng*

Main category: physics.comp-ph

TL;DR: A hybrid solver combining Method of Lines and Physics-Informed Neural Networks that achieves spectral accuracy with fewer collocation points than conventional methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional MOL that requires fine meshes due to truncation errors from finite-difference stencils, and to reduce memory footprint and data dependence.

Method: Trains a neural network to represent initial spatial profile, uses automatic differentiation for spectrally accurate gradients, replaces time integration with temporal PINN while maintaining spatial accuracy without mesh refinement.

Result: Achieves same or better accuracy than conventional MOL using an order of magnitude fewer collocation points, reduces memory footprint, lessens data dependence, and increases complexity robustness.

Conclusion: The boundary-informed MOL-PINN framework provides an efficient alternative to conventional methods that naturally extends to linear and nonlinear PDEs in any spatial dimension using only automatic differentiation and standard optimizers.

Abstract: We propose a hybrid solver that fuses the dimensionality-reduction strengths
of the Method of Lines (MOL) with the flexibility of Physics-Informed Neural
Networks (PINNs). Instead of approximating spatial derivatives with fixed
finite-difference stencils - whose truncation errors force extremely fine
meshes - our method trains a neural network to represent the initial spatial
profile and then employs automatic differentiation to obtain spectrally
accurate gradients at arbitrary nodes. These high-fidelity derivatives define
the right-hand side of the MOL-generated ordinary-differential system, and time
integration is replaced with a secondary temporal PINN while spatial accuracy
is retained without mesh refinement. The resulting "boundary-informed MOL-PINN"
matches or surpasses conventional MOL in accuracy using an order of magnitude
fewer collocation points, thereby shrinking memory footprints, lessening
dependence on large data sets, and increasing complexity robustness. Because it
relies only on automatic differentiation and standard optimizers, the framework
extends naturally to linear and nonlinear PDEs in any spatial dimension.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [37] [Characterization of proton focusing from variable-sized hemispherical targets](https://arxiv.org/abs/2510.15062)
*Jesse Griff-McMahon,Xavier Vaisseau,William Fox,Kirill Lezhnin,Krish Bhutwala,Ryan Nedbailo,Valeria Opsina-Boh√≥rquez,Timo Karpowski,Pravesh K. Patel,Sophia Malko*

Main category: physics.plasm-ph

TL;DR: Proton beam focusing from laser-driven hemispherical targets varies with size - smaller hemispheres focus near center while larger ones behave like flat foils with internal focus points.


<details>
  <summary>Details</summary>
Motivation: To systematically characterize how hemispherical target size affects laser-driven proton beam focusing behavior.

Method: Used mesh radiography to analyze proton beams from hemispherical targets of various diameters, comparing focusing behavior across different size ratios (Œ® = D_hemi/D_Laser).

Result: Smallest hemisphere (Œ®=6.1) focused protons near geometrical center, while larger hemispheres (Œ®=14.6) degraded focusing and behaved like flat foils with focal points inside the hemisphere. Virtual focus of 9¬±3 Œºm was measured.

Conclusion: Hemisphere size significantly impacts proton beam focusing - smaller targets provide better focusing near center, while larger targets exhibit degraded focusing behavior similar to flat foils.

Abstract: We systematically characterize the focusing behavior of laser-driven proton
beams from hemispherical targets of various diameters using mesh radiography.
The proton focal location is inferred to be near the geometrical center for the
smallest tested hemisphere ($\Psi=D_{hemi}/D_{Laser}=6.1$). However, larger
hemispheres ($\Psi=14.6$) degrade the focusing behavior and behave more like
flat foils with focal location significantly inside the hemisphere. We also
infer a tight virtual focus of $9\pm3~\mu$m through a mesh transition analysis.

</details>


### [38] [Modelling-driven requirements for Error Field Control Coil application to initial JT-60SA plasmas](https://arxiv.org/abs/2510.15504)
*L. Pigatto,G. Frello,Y. Q. Liu,L. Novello,M. Takechi,E. Tomasina,T. Bolzonella*

Main category: physics.plasm-ph

TL;DR: Modeling of Error Field Correction Coils (EFCC) application in JT-60SA tokamak scenarios using MARS-F code to assess magnetic perturbation impacts on plasma core and pedestal regions, and comparing operational parameters with EFCC specifications.


<details>
  <summary>Details</summary>
Motivation: Optimize future JT-60SA plasma scenarios through correct EFCC usage and prepare for ITER operations by gaining experience with error field detection and control strategies from JT-60SA experiments.

Method: Used linear resistive MHD code MARS-F to model EFCC application in JT-60SA Initial Research Phase I scenarios, including plasma response, and assessed impact of Resonant Magnetic Perturbations on core and pedestal regions.

Result: Analyzed dominant core response to error fields case by case and compared to mode locking thresholds from literature, then evaluated typical current/voltage amplitudes and waveforms against EFCC specifications.

Conclusion: The study helps assess safe operational space for EFCC usage in JT-60SA and provides valuable insights for error field control strategies applicable to ITER operations.

Abstract: JT-60SA is a large superconducting tokamak built in Naka, Japan. After the
successful achievement of its first MA-class plasma, the installation of
several additional sub-systems, including a set of non-axisymmetric Error Field
Correction Coils (EFCC), is ongoing. Optimization of future JT-60SA plasma
scenarios will critically depend on the correct use of EFCC, including careful
fulfillment of system specifications. In addition to that, preparation and risk
mitigation of early ITER operations will greatly benefit from the experience
gained by early EFCC application to JT-60SA experiments, in particular to
optimize error field detection and control strategies. In this work, EFCC
application in JT-60SA Initial Research Phase I perspective scenarios is
modeled including plasma response. Impact of (Resonant) Magnetic Perturbations
on the different plasma scenarios is assessed for both core and pedestal
regions by the linear resistive MHD code MARS-F. The dominant core response to
EFs is discussed case by case and compared to mode locking thresholds from
literature. Typical current/voltage amplitudes and wave-forms are then compared
to EFCC specifications in order to assess a safe operational space.

</details>


### [39] [Formation of a Tungsten Co-deposition Layer with Microparticles Using Pulsed Laser Deposition](https://arxiv.org/abs/2510.15641)
*S. Kodate,Y. Hayashi,S. Kajita*

Main category: physics.plasm-ph

TL;DR: Tungsten co-deposition experiments using pulsed laser deposition with helium/argon plasma exposure formed micron-sized spherical W particles that grew from nanoparticles via electrostatic collection of W ions.


<details>
  <summary>Details</summary>
Motivation: To study tungsten co-deposition behavior under plasma exposure conditions relevant to fusion applications.

Method: Used pulsed laser deposition with helium/argon plasma exposure in a linear plasma device for tungsten co-deposition experiments.

Result: Formed micron-sized spherical tungsten particles under co-deposition conditions, suggesting growth from nanoparticles via electrostatic collection of W ions in plasma plume.

Conclusion: The formation mechanism of micron-sized W particles involves nanoparticle growth through electrostatic collection of tungsten ions before deposition.

Abstract: This study performed tungsten (W) co-deposition experiments using pulsed
laser deposition under exposure to helium/argon plasma in a linear plasma
device. Micron-sized spherical W particles formed under co-deposition
conditions. It was suggested that these particles grew from nanoparticles via
the electrostatic collection of W ions in the plasma plume before deposition.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [40] [Cluster percolation and dynamical scaling in the Baxter--Wu model](https://arxiv.org/abs/2510.15124)
*Alexandros Vasilopoulos,Michail Akritidis,Nikolaos G. Fytas,Martin Weigel*

Main category: cond-mat.stat-mech

TL;DR: The paper studies percolation behavior of Fortuin-Kasteleyn clusters in the spin-1/2 Baxter-Wu model on triangular lattice, showing percolation temperature matches exact thermal critical point with consistent critical exponents.


<details>
  <summary>Details</summary>
Motivation: To investigate the relationship between percolation behavior of stochastic clusters and thermal phase transitions in the Baxter-Wu model with three-spin interactions.

Method: Monte Carlo simulations combined with finite-size scaling analysis, constructing clusters by randomly freezing one sublattice to create effective pairwise interactions.

Result: Percolation temperature of stochastic clusters coincides with exact thermal critical point, and critical exponents from cluster observables match those of thermal phase transition.

Conclusion: The cluster construction provides efficient algorithms with good scaling behavior, demonstrating strong connection between percolation properties and thermal critical phenomena.

Abstract: We investigate the percolation behavior of Fortuin-Kasteleyn--type clusters
in the spin-$1/2$ Baxter--Wu model with three-spin interactions on a triangular
lattice. The considered clusters are constructed by randomly freezing one of
the three sublattices, resulting in effective pairwise interactions among the
remaining spins. Using Monte Carlo simulations combined with a finite-size
scaling analysis, we determine the percolation temperature of these stochastic
clusters and show that it coincides with the exact thermal critical point of
the model. The critical exponents derived from cluster observables are
consistent with those of the underlying thermal phase transition. Finally, we
analyze the dynamical scaling of the multi-cluster and single-cluster
algorithms resulting from the cluster construction, highlighting their
efficiency and scaling behavior with system size.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [41] [Bayesian Inference for PDE-based Inverse Problems using the Optimization of a Discrete Loss](https://arxiv.org/abs/2510.15664)
*Lucas Amoudruz,Sergey Litvinov,Costas Papadimitriou,Petros Koumoutsakos*

Main category: stat.ME

TL;DR: B-ODIL is a Bayesian extension of ODIL that integrates PDE loss as prior knowledge with data likelihood to solve inverse problems with quantified uncertainties.


<details>
  <summary>Details</summary>
Motivation: Inverse problems require additional knowledge when measurements are incomplete or indirect. PDE-based models can close this gap, and extending ODIL to Bayesian framework enables uncertainty quantification.

Method: B-ODIL uses Bayesian formulation combining PDE loss as prior knowledge with data likelihood. It extends the ODIL method to handle inverse problems with uncertainty quantification.

Result: Demonstrated on synthetic benchmarks with PDEs in 1D, 2D, and 3D. Successfully applied to estimate tumor concentration and uncertainty in patient's brain from MRI scans using 3D tumor growth model.

Conclusion: B-ODIL provides an effective Bayesian framework for solving PDE-based inverse problems with uncertainty quantification, showing practical application in medical imaging for tumor analysis.

Abstract: Inverse problems are crucial for many applications in science, engineering
and medicine that involve data assimilation, design, and imaging. Their
solution infers the parameters or latent states of a complex system from noisy
data and partially observable processes. When measurements are an incomplete or
indirect view of the system, additional knowledge is required to accurately
solve the inverse problem. Adopting a physical model of the system in the form
of partial differential equations (PDEs) is a potent method to close this gap.
In particular, the method of optimizing a discrete loss (ODIL) has shown great
potential in terms of robustness and computational cost. In this work, we
introduce B-ODIL, a Bayesian extension of ODIL, that integrates the PDE loss of
ODIL as prior knowledge and combines it with a likelihood describing the data.
B-ODIL employs a Bayesian formulation of PDE-based inverse problems to infer
solutions with quantified uncertainties. We demonstrate the capabilities of
B-ODIL in a series of synthetic benchmarks involving PDEs in one, two, and
three dimensions. We showcase the application of B-ODIL in estimating tumor
concentration and its uncertainty in a patient's brain from MRI scans using a
three-dimensional tumor growth model.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [42] [Minimisation of Submodular Functions Using Gaussian Zeroth-Order Random Oracles](https://arxiv.org/abs/2510.15257)
*Amir Ali Farzin,Yuen-Man Pun,Philipp Braun,Tyler Summers,Iman Shames*

Main category: math.OC

TL;DR: A zeroth-order method using Gaussian smoothing is applied to submodular function minimization, achieving global convergence in offline settings and Hannan-consistency with static/dynamic regret bounds in online settings.


<details>
  <summary>Details</summary>
Motivation: To develop efficient algorithms for submodular function minimization that work in both offline and online settings, leveraging zeroth-order optimization techniques.

Method: Gaussian smoothing random oracle is used to estimate gradients for a zeroth-order optimization approach applied to submodular function minimization.

Result: The algorithm converges to global Œµ-approximate solutions offline, achieves Hannan-consistency with static regret online, and obtains O(‚àö(NP_N*)) dynamic regret where P_N* is path length.

Conclusion: The proposed zeroth-order method is effective for submodular minimization across both offline and online scenarios, with theoretical guarantees and practical performance demonstrated through numerical experiments.

Abstract: We consider the minimisation problem of submodular functions and investigate
the application of a zeroth-order method to this problem. The method is based
on exploiting a Gaussian smoothing random oracle to estimate the smoothed
function gradient. We prove the convergence of the algorithm to a global
$\epsilon$-approximate solution in the offline case and show that the algorithm
is Hannan-consistent in the online case with respect to static regret.
Moreover, we show that the algorithm achieves $O(\sqrt{NP_N^\ast})$ dynamic
regret, where $N$ is the number of iterations and $P_N^\ast$ is the path
length. The complexity analysis and hyperparameter selection are presented for
all the cases. The theoretical results are illustrated via numerical examples.

</details>


### [43] [Riemannian Bilevel Optimization with Gradient Aggregation](https://arxiv.org/abs/2510.15305)
*Zhuo Chen,Xinjian Xu,Shihui Ying,Tieyong Zeng*

Main category: math.OC

TL;DR: Proposes Riemannian bilevel optimization (RBLO) with bilevel descent aggregation for handling structural constraints in hierarchical decision-making problems.


<details>
  <summary>Details</summary>
Motivation: Existing bilevel optimization methods are mostly developed in Euclidean spaces, but many real-world problems involve structural constraints that can be abstracted to manifold structures.

Method: Transform constrained BLO to unconstrained RBLO by abstracting constraints to manifolds, use bilevel descent aggregation scheme to coordinate upper- and lower-level updates, and provide convergence analysis under geodesic convexity and Lipschitz smoothness.

Result: Numerical evaluation on 3sources datasets for multi-view hypergraph spectral clustering shows superior performance over Euclidean and manifold-based baselines.

Conclusion: The proposed RBLO algorithm effectively handles structural constraints in bilevel optimization problems and demonstrates improved performance compared to existing approaches.

Abstract: Bilevel optimization (BLO) offers a principled framework for hierarchical
decision-making and has been widely applied in machine learning tasks such as
hyperparameter optimization and meta-learning. While existing BLO methods are
mostly developed in Euclidean spaces, many real-world problems involve
structural constraints. In this paper, we propose a Riemannian bilevel
optimization (RBLO) algorithm that incorporates a bilevel descent aggregation
(BDA) scheme to jointly coordinate upper- and lower-level updates. Concretely,
first we abstract the constraints in the BLO to a manifold structure and then
transform the constrained BLO be a unconstrained RBLO problem. Second, to
address limitations of existing RBLO methods, particularly the restrictive
assumptions required for convergence, we reformulate the bilevel problem using
smooth manifold mappings and provide a convergence analysis under the
conditions of geodesic convexity and Lipschitz smoothness. Finally, we recall
the multi-view hypergraph spectral clustering task, and evaluate the proposed
approach on 3sources data sets. The numerical results validate the superior
performance over Euclidean and manifold-based baselines.

</details>


### [44] [Nonlinear Dimensionality Reduction Techniques for Bayesian Optimization](https://arxiv.org/abs/2510.15435)
*Luo Long,Coralia Cartis,Paz Fink Shustin*

Main category: math.OC

TL;DR: This paper proposes a novel Bayesian optimization approach that combines nonlinear dimensionality reduction using Variational Autoencoders with Sequential Domain Reduction in latent space, showing improved performance over random embeddings for high-dimensional optimization problems.


<details>
  <summary>Details</summary>
Motivation: Bayesian optimization struggles with scalability to high dimensions, and existing Latent-Space BO methods using linear random projections have limitations. The authors aim to develop more effective nonlinear dimensionality reduction techniques for BO.

Method: Uses Variational Autoencoders for nonlinear dimensionality reduction in Latent-Space BO, with deep metric loss for structured latent manifolds and VAE retraining. Combines this with Sequential Domain Reduction in latent space (SDR-LSBO) and implements in GPU-accelerated BoTorch with Matern-5/2 Gaussian process surrogates.

Result: The approach shows improved optimization quality across benchmark tasks, with structured latent manifolds enhancing BO performance. VAEs outperform random embeddings as dimensionality reduction mechanisms.

Conclusion: This is the first study to combine Sequential Domain Reduction with VAE-based Latent-Space BO, and the analysis provides critical design insights for metric shaping and retraining in scalable latent space BO.

Abstract: Bayesian optimisation (BO) is a standard approach for sample-efficient global
optimisation of expensive black-box functions, yet its scalability to high
dimensions remains challenging. Here, we investigate nonlinear dimensionality
reduction techniques that reduce the problem to a sequence of low-dimensional
Latent-Space BO (LSBO). While early LSBO methods used (linear) random
projections (Wang et al., 2013), building on Grosnit et al. (2021), we employ
Variational Autoencoders (VAEs) for LSBO, focusing on deep metric loss for
structured latent manifolds and VAE retraining to adapt the encoder-decoder to
newly sampled regions. We propose some changes in their implementation,
originally designed for tasks such as molecule generation, and reformulate the
algorithm for broader optimisation purposes. We then couple LSBO with
Sequential Domain Reduction (SDR) directly in the latent space (SDR-LSBO),
yielding an algorithm that narrows the latent search domains as evidence
accumulates. Implemented in a GPU-accelerated BoTorch stack with Matern-5/2
Gaussian process surrogates, our numerical results show improved optimisation
quality across benchmark tasks and that structured latent manifolds improve BO
performance. Additionally, we compare random embeddings and VAEs as two
mechanisms for dimensionality reduction, showing that the latter outperforms
the former. To the best of our knowledge, this is the first study to combine
SDR with VAE-based LSBO, and our analysis clarifies design choices for metric
shaping and retraining that are critical for scalable latent space BO. For
reproducibility, our source code is available at
https://github.com/L-Lok/Nonlinear-Dimensionality-Reduction-Techniques-for-Bayesian-Optimization.git.

</details>


### [45] [Optimization in Theory and Practice](https://arxiv.org/abs/2510.15734)
*Stephen J. Wright*

Main category: math.OC

TL;DR: This paper examines the interplay between theoretical analysis and practical performance in optimization algorithms, focusing on linear programming and unconstrained minimization of smooth functions, highlighting how advances in theory and practice influence each other.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between theoretical convergence/complexity analysis and practical performance of optimization algorithms, and explore how these two facets drive innovation in each other.

Method: Analysis of major algorithm classes in linear programming and unconstrained minimization of smooth functions, examining their theoretical properties (mainly non-asymptotic complexity bounds) and practical performance.

Result: The paper outlines how theoretical complexity analysis and practical algorithm performance have mutually influenced advances in optimization methods, particularly in the two focused areas.

Conclusion: There is a fascinating interaction between theoretical analysis and practical performance in optimization algorithms, with each driving innovation in the other, highlighting the importance of considering both aspects in algorithm development.

Abstract: Algorithms for continuous optimization problems have a rich history of design
and innovation over the past several decades, in which mathematical analysis of
their convergence and complexity properties plays a central role. Besides their
theoretical properties, optimization algorithms are interesting also for their
practical usefulness as computational tools for solving real-world problems.
There are often gaps between the practical performance of an algorithm and what
can be proved about it. These two facets of the field -- the theoretical and
the practical -- interact in fascinating ways, each driving innovation in the
other. This work focuses on the development of algorithms in two areas --
linear programming and unconstrained minimization of smooth functions --
outlining major algorithm classes in each area along with their theoretical
properties and practical performance, and highlighting how advances in theory
and practice have influenced each other in these areas. In discussing theory,
we focus mainly on non-asymptotic complexity, which are upper bounds on the
amount of computation required by a given algorithm to find an approximate
solution of problems in a given class.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [46] [Globalizing the Carleman linear embedding method for nonlinear dynamics](https://arxiv.org/abs/2510.15715)
*Ivan Novikau,Ilon Joseph*

Main category: quant-ph

TL;DR: Three global piecewise Carleman embedding methods are proposed to overcome convergence limitations of standard Carleman linearization in nonlinear systems with multiple fixed points.


<details>
  <summary>Details</summary>
Motivation: The standard Carleman embedding method fails to converge in regions with multiple fixed points, limiting its applicability for complex nonlinear dynamical systems.

Method: Three piecewise approaches: 1) Boundary-triggered switching between local linearization regions 2) Dynamic chart size adaptation for accuracy 3) Static grid partitioning with precomputed charts for speed

Result: All methods successfully handle nonlinear systems intractable for standard Carleman embedding, with adaptive methods excelling on chaotic systems and non-adaptive version being faster.

Conclusion: Piecewise Carleman embedding enables simulation of complex nonlinear systems, with adaptive methods for accuracy and non-adaptive version potentially suitable for quantum computing applications.

Abstract: The Carleman embedding method is a widely used technique for linearizing a
system of nonlinear differential equations, but fails to converge in regions
where there are multiple fixed points. We propose and test three different
versions of a global piecewise Carleman embedding technique, based on
partitioning space into multiple regions where the center and size of the
embedding region are chosen to control convergence. The first method switches
between local linearization regions of fixed size once the trajectory reaches
the boundary of the current linearization chart. During the transition, the
embedding is reconstructed within the newly created chart, centered at the
transition point. The second method also adapts the chart size dynamically,
enhancing accuracy in regions where multiple fixed points are located. The
third method partitions the state space using a static grid with precomputed
linearization charts of fixed size, making it more suitable for applications
that require high speed. All techniques are numerically tested on multiple
integrable and chaotic nonlinear dynamical systems demonstrating their
applicability for problems that are completely intractable for the standard
Carleman embedding method. Simulations of chaotic dynamical systems such as
various types of strange attractors demonstrate the power of the adaptive
methods, if a sufficiently low tolerance is imposed. Still, the non-adaptive
version of the method, with fixed centers and sizes of the linearization
charts, can be faster in simulating dynamical systems while providing similar
accuracy and may be more appropriate as the basis of algorithms for future
quantum computers.

</details>


### [47] [Second-order discretization of Dyson series: iterative method, numerical analysis and applications in open quantum systems](https://arxiv.org/abs/2510.15287)
*Zhenning Cai,Yixiao Sun,Geshuo Wang*

Main category: quant-ph

TL;DR: A novel discretization strategy for the Dyson series using Strang splitting and Taylor expansion, enabling efficient simulation of open quantum systems with reduced computational complexity.


<details>
  <summary>Details</summary>
Motivation: To develop a numerically exact method for simulating system-bath dynamics in open quantum systems that avoids high-dimensional integration and reduces computational costs compared to existing approaches.

Method: Proposed a general discretization strategy for the Dyson series combining Strang splitting with Taylor expansion. Developed first-order and second-order iterative schemes with rigorous convergence analysis. The second-order scheme omits unnecessary terms to reduce complexity.

Result: Established convergence orders: first-order method has O(Œît) global error, second-order method has O(Œît¬≤). Second-order scheme achieves time complexity O(M¬≥2¬≤·¥∑·µê·µÉÀ£K‚Çò‚Çê‚Çì¬≤) and space complexity O(M¬≤2¬≤·¥∑·µê·µÉÀ£K‚Çò‚Çê‚Çì), substantially reducing memory and computational effort for multilevel systems (M‚â•3).

Conclusion: The proposed method provides an efficient framework for simulating open quantum systems with rigorous convergence guarantees and significantly reduced computational complexity compared to existing approaches, validated through numerical experiments.

Abstract: We propose a general strategy to discretize the Dyson series without applying
direct numerical quadrature to high-dimensional integrals, and extend this
framework to open quantum systems. The resulting discretization can also be
interpreted as a Strang splitting combined with a Taylor expansion. Based on
this formulation, we develop a numerically exact iterative method for
simulation system-bath dynamics. We propose two numerical schemes, which are
first-order and second-order in time step $\Delta t$ respectively. We perform a
rigorous numerical analysis to establish the convergence orders of both
schemes, proving that the global error decreases as $\mathcal{O}(\Delta t)$ and
$\mathcal{O}(\Delta t^2)$ for the first- and second-order methods,
respectively. In the second-order scheme, we can safely omitted most terms
arising from the Strang splitting and Taylor expansion while maintaining
second-order accuracy, leading to a substantial reduction in computational
complexity. For the second-order method, we achieves a time complexity of
$\mathcal{O}(M^3 2^{2K_{\max}} K_{\max}^2)$ and a space complexity of
$\mathcal{O}(M^2 2^{2K_{\max}} K_{\max})$ where $M$ denotes the number of
system levels and $K_{\max}$ the number of time steps within the memory length.
Compared with existing methods, our approach requires substantially less memory
and computational effort for multilevel systems ($M\geqslant 3$). Numerical
experiments are carried out to illustrate the validity and efficiency of our
method.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [48] [Magnetohydrodynamic-guiding-center-particle-in-cell Method for Multiscale Plasma Kinetic Simulations](https://arxiv.org/abs/2510.15156)
*Zitao Hu,Xue-Ning Bai,Xiaochen Sun*

Main category: astro-ph.HE

TL;DR: The paper presents the MHD-gPIC method, which combines magnetohydrodynamics with guiding center particle-in-cell approach for studying particle acceleration and transport in systems where gyro-resonance is insignificant.


<details>
  <summary>Details</summary>
Motivation: To develop a computational method that can efficiently simulate systems with both thermal fluid components and high-energy particles, particularly for studying particle acceleration and transport in astrophysical systems.

Method: The MHD-gPIC method integrates magnetohydrodynamics with particle-in-cell approach using guiding center approximation for particles, including drift motion and carefully evaluated particle backreaction source terms. Implemented in Athena++ MHD code.

Result: The code was validated with a series of tests and shown to be applicable for studying particle acceleration and transport. Preliminary studies of particle acceleration during non-relativistic magnetic reconnection were also presented.

Conclusion: The MHD-gPIC method provides an effective computational framework for studying particle acceleration and transport in systems where gyro-resonance effects are not significant, with successful implementation and validation.

Abstract: We present the formulation, algorithm and numerical tests of the
magnetohydrodynamic-particle-in-cell (MHD-PIC) method with particles treated
under the guiding center approximation, which we term the MHD-gPIC method, and
it is implemented in the Athena++ MHD code. The new MHD-gPIC model consists of
thermal (cold) fluid and high-energy particles whose dynamics are integrated
through guiding center equations including drift motion, with carefully
evaluated source terms as particle backreaction. The code is validated with a
series of tests, and it is expected to be primarily applicable to study
particle acceleration and transport in systems where gyro-resonance is
considered insignificant. We also present preliminary studies of particle
acceleration during non-relativistic magnetic reconnection.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [49] [Aiming for Proxima Centauri b: Gravitational effects on relativistic spacecraft trajectories](https://arxiv.org/abs/2510.15827)
*Mark C. Baumann,Justin C. Feng,Nicky Ishaak*

Main category: gr-qc

TL;DR: Relativistic gravitational effects are crucial for interstellar spacecraft missions, requiring consideration for accurate targeting of Proxima Centauri b, with the Sun having the greatest influence and higher-order effects causing tens of kilometers displacement.


<details>
  <summary>Details</summary>
Motivation: To determine the importance of gravitational and relativistic effects for interstellar travel, particularly for laser-propelled spacecraft missions to neighboring stars like Proxima Centauri b.

Method: Used a Julia reimplementation of the PoMiN code, an N-body code modeling relativistic gravitational dynamics in the first post-Minkowskian approximation to general relativity, and developed numerical fine-tuning methods for precise initial data.

Result: The Sun has the greatest gravitational influence; relativistic effects must be considered for accuracy better than ~690,000 km; numerical methods achieve femtometer precision over 4.25 light years; higher-order relativistic effects from the Sun displace final position by tens of kilometers; launch phase errors can dominate miss distances.

Conclusion: Relativistic effects are significant for interstellar mission planning, with the Sun's gravitational influence being most important, and both relativistic corrections and launch phase precision are critical for successful targeting of distant exoplanets.

Abstract: How important are gravitational and relativistic effects for interstellar
travel? We consider this question in the context of proposed laser-propelled
spacecraft missions to neighboring stellar destinations. Our analysis applies
to any spacecraft traveling at relativistic speeds. As a concrete example, we
focus on a mission to Proxima Centauri b -- a terrestrial-sized planet in the
habitable zone around our nearest stellar neighbor, Proxima Centauri. We employ
a Julia reimplementation of the PoMiN code, an N-body code modeling
relativistic gravitational dynamics in the first post-Minkowskian (PM)
approximation to general relativity (valid to linear order in Newton's constant
$G$). We compute the gravitational influence of seven different celestial
bodies and find that the Sun has the greatest influence on the trajectory of
the interstellar spacecraft. We also study the differences between Newtonian
and PM gravity, and find that if mission planners wish to hit Proxima Centauri
b with an accuracy of better than about 690,000 kilometers, relativistic
effects must be taken into account. To solve for the precise initial data
needed to hit an intended target, we develop numerical fine-tuning methods and
demonstrate that these methods can (within a given model) be precise to about a
femtometer over a travel distance of $\sim4.25$ light years. However, we find
that for the spacecraft trajectories we consider, higher order general
relativistic effects (beyond the first PM approximation) from the Sun can
displace the final position of the spacecraft by tens of kilometers. We also
consider the variation in the initial direction of the spacecraft velocity and
find that, even with relativistic effects properly taken into account, the miss
distances can be dominated by the variation in the initial velocity that could
arise from errors during the launch and boost phase of the spacecraft mission.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [50] [Automotive Crash Dynamics Modeling Accelerated with Machine Learning](https://arxiv.org/abs/2510.15201)
*Mohammad Amin Nabian,Sudeep Chavare,Deepak Akhare,Rishikesh Ranade,Ram Cherukuri,Srinivas Tadepalli*

Main category: cs.LG

TL;DR: Machine learning surrogate models for crashworthiness assessment using NVIDIA PhysicsNeMo framework, showing feasibility and computational efficiency gains over traditional finite element simulations.


<details>
  <summary>Details</summary>
Motivation: Traditional finite element simulations for crashworthiness assessment are computationally expensive and time-consuming, creating need for faster alternatives.

Method: Comparative study of MeshGraphNet and Transolver neural network architectures with three transient dynamics modeling strategies (time-conditional, autoregressive, stability-enhanced autoregressive) on Body-in-White crash dataset with 150 LS-DYNA simulations.

Result: Models captured overall deformation trends with reasonable fidelity, achieving orders-of-magnitude reduction in computational cost while not yet matching full FE accuracy.

Conclusion: Machine learning approaches show promise for rapid design exploration and early-stage optimization in crashworthiness evaluation, demonstrating feasibility for structural crash dynamics applications.

Abstract: Crashworthiness assessment is a critical aspect of automotive design,
traditionally relying on high-fidelity finite element (FE) simulations that are
computationally expensive and time-consuming. This work presents an exploratory
comparative study on developing machine learning-based surrogate models for
efficient prediction of structural deformation in crash scenarios using the
NVIDIA PhysicsNeMo framework. Given the limited prior work applying machine
learning to structural crash dynamics, the primary contribution lies in
demonstrating the feasibility and engineering utility of the various modeling
approaches explored in this work. We investigate two state-of-the-art neural
network architectures for modeling crash dynamics: MeshGraphNet, and
Transolver. Additionally, we examine three strategies for modeling transient
dynamics: time-conditional, the standard Autoregressive approach, and a
stability-enhanced Autoregressive scheme incorporating rollout-based training.
The models are evaluated on a comprehensive Body-in-White (BIW) crash dataset
comprising 150 detailed FE simulations using LS-DYNA. The dataset represents a
structurally rich vehicle assembly with over 200 components, including 38 key
components featuring variable thickness distributions to capture realistic
manufacturing variability. Each model utilizes the undeformed mesh geometry and
component characteristics as inputs to predict the spatiotemporal evolution of
the deformed mesh during the crash sequence. Evaluation results show that the
models capture the overall deformation trends with reasonable fidelity,
demonstrating the feasibility of applying machine learning to structural crash
dynamics. Although not yet matching full FE accuracy, the models achieve
orders-of-magnitude reductions in computational cost, enabling rapid design
exploration and early-stage optimization in crashworthiness evaluation.

</details>


### [51] [Theoretical Refinement of CLIP by Utilizing Linear Structure of Optimal Similarity](https://arxiv.org/abs/2510.15508)
*Naoki Yoshida,Satoshi Hayakawa,Yuhta Takida,Toshimitsu Uesaka,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: KME-CLIP enhances CLIP by using kernel methods to better approximate pointwise mutual information (PMI) for similarity computation, improving performance on retrieval and classification tasks.


<details>
  <summary>Details</summary>
Motivation: Current CLIP implementations don't fully utilize the linear structure of PMI, which theory shows should be the optimal similarity metric between paired modalities.

Method: Proposed KME-CLIP uses inner product in reproducing kernel Hilbert space to leverage PMI's linear structure and approximate it with arbitrary accuracy.

Result: Empirically outperforms standard CLIP across several retrieval and classification tasks.

Conclusion: Kernel-based approach effectively captures PMI structure and improves multi-modal contrastive learning performance.

Abstract: In this study, we propose an enhancement to the similarity computation
mechanism in multi-modal contrastive pretraining frameworks such as CLIP. Prior
theoretical research has demonstrated that the optimal similarity metrics
between paired modalities should correspond to the pointwise mutual information
(PMI) between the two modalities. However, the current implementations of CLIP
and its variants fail to fully utilize the underlying linear structure of PMI.
We therefore propose KME-CLIP, which leverages this structure through the inner
product in a reproducing kernel Hilbert space. We theoretically prove that our
method can approximate PMI with arbitrary accuracy and empirically demonstrate
that our approach overall outperforms the standard CLIP formulation across
several retrieval and classification tasks.

</details>


### [52] [Deep Neural ODE Operator Networks for PDEs](https://arxiv.org/abs/2510.15651)
*Ziqian Li,Kang Liu,Yongcun Song,Hangrui Yue,Enrique Zuazua*

Main category: cs.LG

TL;DR: NODE-ONet is a neural ODE operator network that incorporates physics knowledge into PDE solving, improving temporal dynamics modeling and generalization beyond training time frames.


<details>
  <summary>Details</summary>
Motivation: Existing operator learning approaches often ignore domain knowledge in PDEs, leading to challenges in capturing temporal dynamics and poor generalization beyond training periods.

Method: Encoder-decoder architecture with three components: spatial encoder, neural ODE for latent temporal dynamics, and decoder for physical space reconstruction. Uses physics-encoded neural ODEs to incorporate PDE-specific properties.

Result: Demonstrates high accuracy, computational efficiency, and prediction capabilities beyond training time frames on nonlinear diffusion-reaction and Navier-Stokes equations. Reduces framework complexity while enhancing efficiency and generalization.

Conclusion: The framework offers flexibility with diverse encoders/decoders and generalizes across related PDE families, making it a scalable, physics-encoded tool for scientific machine learning.

Abstract: Operator learning has emerged as a promising paradigm for developing
efficient surrogate models to solve partial differential equations (PDEs).
However, existing approaches often overlook the domain knowledge inherent in
the underlying PDEs and hence suffer from challenges in capturing temporal
dynamics and generalization issues beyond training time frames. This paper
introduces a deep neural ordinary differential equation (ODE) operator network
framework, termed NODE-ONet, to alleviate these limitations. The framework
adopts an encoder-decoder architecture comprising three core components: an
encoder that spatially discretizes input functions, a neural ODE capturing
latent temporal dynamics, and a decoder reconstructing solutions in physical
spaces. Theoretically, error analysis for the encoder-decoder architecture is
investigated. Computationally, we propose novel physics-encoded neural ODEs to
incorporate PDE-specific physical properties. Such well-designed neural ODEs
significantly reduce the framework's complexity while enhancing numerical
efficiency, robustness, applicability, and generalization capacity. Numerical
experiments on nonlinear diffusion-reaction and Navier-Stokes equations
demonstrate high accuracy, computational efficiency, and prediction
capabilities beyond training time frames. Additionally, the framework's
flexibility to accommodate diverse encoders/decoders and its ability to
generalize across related PDE families further underscore its potential as a
scalable, physics-encoded tool for scientific machine learning.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [53] [Semiclassical limit of entropies and free energies](https://arxiv.org/abs/2510.15777)
*Zied Ammari,Michele Correggi,Marco Falconi,Rapha√´l Gautier*

Main category: math-ph

TL;DR: This paper studies the semiclassical convergence of von Neumann and Wehrl entropies, showing that von Neumann entropy converges to Wehrl entropy for quantum Gibbs states after renormalization, and that the free energy functional with Wehrl entropy Œì-converges to its classical counterpart.


<details>
  <summary>Details</summary>
Motivation: To establish connections between quantum and classical entropy concepts in semiclassical analysis, bridging statistical physics, information theory, and mathematical contexts like dynamical systems and von Neumann algebras.

Method: Using semiclassical analysis to study the convergence properties of von Neumann and Wehrl entropies, with particular focus on quantum Gibbs states and free energy functionals.

Result: Proved semiclassical convergence of von Neumann to Wehrl entropy for quantum Gibbs states after renormalization, and showed Œì-convergence of free energy functional with Wehrl entropy to classical counterpart.

Conclusion: The work establishes rigorous connections between quantum and classical entropy measures in the semiclassical limit, providing mathematical foundations for relating quantum statistical mechanics to classical thermodynamics.

Abstract: Entropy and free energy are central concepts in both statistical physics and
information theory, with quantum and classical facets. In mathematics these
concepts appear quite often in different contexts (dynamical systems,
probability theory, von Neumann algebras, etc.). In this work, we study the von
Neumann and Wehrl entropies from the point of view of semiclassical analysis.
We first prove the semiclassical convergence of the von Neumann to the Wehrl
entropy for quantum Gibbs states (thermal equilibrium), after a suitable
renormalization has been taken into account. Then, we show that, in the same
limit, the free energy functional defined with the Wehrl entropy $
\Gamma-$converges to its classical counterpart, so implying convergence of the
minima and the associated minimizers.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [54] [Impact of memory on clustering in spontaneous particle aggregation](https://arxiv.org/abs/2510.15335)
*Radek Erban,Jan Haskovec*

Main category: math.DS

TL;DR: Short-term memory promotes clustering while long-term memory inhibits aggregation in self-driven agents, with memory length controlling the transition between collective organization patterns.


<details>
  <summary>Details</summary>
Motivation: To understand how memory duration affects spontaneous aggregation in organisms, bridging individual-level dynamics with emergent spatial patterns.

Method: Stochastic agent-based model with memory via K internal variables, mean-field Fokker-Planck equation analysis, and systematic simulations in 1D/2D domains.

Result: Short-term memory promotes coarsening (fewer, larger clusters), long-term memory inhibits aggregation (more isolated individuals), with extended memory reducing environmental responsiveness.

Conclusion: Memory is a key factor controlling collective organization, providing a bridge between individual dynamics and emergent spatial patterns.

Abstract: The effect of short-term and long-term memory on spontaneous aggregation of
organisms is investigated using a stochastic agent-based model. Each individual
modulates the amplitude of its random motion according to the perceived local
density of neighbors. Memory is introduced via a chain of $K$~internal
variables that allow agents to retain information about previously encountered
densities. The parameter $K$ controls the effective length of memory. A formal
mean-field limit yields a macroscopic Fokker--Planck equation, which provides a
continuum description of the system in the large-population limit. Steady
states of this equation are characterized to interpret the emergence and
morphology of clusters. Systematic stochastic simulations in one- and
two-dimensional spatial domains reveal that short- or moderate-term memory
promotes coarsening, resulting in a smaller number of larger clusters, whereas
long-term memory inhibits aggregation and increases the proportion of isolated
individuals. Statistical analysis demonstrates that extended memory reduces the
agents' responsiveness to environmental stimuli, explaining the transition from
aggregation to dispersion as $K$ increases. These findings identify memory as a
key factor controlling the collective organization of self-driven agents and
provide a bridge between individual-level dynamics and emergent spatial
patterns.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [55] [A HHO formulation for variable density incompressible flows where the density is purely advected](https://arxiv.org/abs/2510.15733)
*Lorenzo Botti,Francesco Carlo Massa*

Main category: physics.flu-dyn

TL;DR: A Hybrid High-Order (HHO) method for incompressible Navier-Stokes equations with variable density that ensures exact volume conservation and pure density advection, using hybrid spaces and ESDIRK temporal discretization.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for simulating mixtures of immiscible incompressible fluids with exact volume conservation, pressure-robustness, and efficient computational performance.

Method: Spatial discretization using hybrid velocity-density-pressure spaces and temporal discretization via Explicit Singly Diagonal Implicit Runge-Kutta (ESDIRK) methods with upwind stabilization and static condensation.

Result: The method achieves exact volume conservation cell-by-cell, pressure-robustness, pure density advection, and successfully simulates Rayleigh-Taylor instability at Reynolds 1,000 and 5,000.

Conclusion: The proposed HHO formulation provides an effective framework for simulating immiscible fluid mixtures with attractive computational features including conservation properties, robustness, and high-order accuracy.

Abstract: We propose a Hybrid High-Order (HHO) formulation of the incompressible
Navier--Stokes equations with variable density that provides exact conservation
of volume and, accordingly, pure advection of the density variable. The spatial
discretization relies on hybrid velocity-density-pressure spaces and the
temporal discretization is based on Explicit Singly Diagonal Implicit
Runge-Kutta (ESDIRK) methods. The formulation possesses some attractive
features that can be fruitfully exploited for the simulation of mixtures of
immiscible incompressible fluids, namely: pressure-robustness, conservation of
volume enforced cell-by-cell up to machine precision, robustness in the
convection dominated regime thanks to upwind stabilizations, weak imposition of
boundary conditions, implicit high-order accurate time stepping encompassing
unsteady boundary conditions, reduced memory footprint thanks to static
condensation, possibility to exploit inherited $p$-multilevel solution
strategies to improve performance of iterative solvers. Numerical validation is
performed showcasing convergence rates, pressure-robustness and pure advection
of the density, thus we tackle the Rayleigh-Taylor instability at Reynolds
$1~000$ and $5~000$.

</details>


### [56] [On Turbulent Behavior of the Generalized Surface Quasigeostrophic Equations](https://arxiv.org/abs/2510.15077)
*Chengzhang Fu,Michael S. Jolly,Anuj Kumar,Vincent R. Martinez*

Main category: physics.flu-dyn

TL;DR: The paper analyzes turbulent behavior in generalized surface quasigeostrophic equations, deriving a new energy spectrum scaling and proving conditions for enstrophy cascade with numerical verification.


<details>
  <summary>Details</summary>
Motivation: To examine turbulent behavior in the two-parameter family of generalized surface quasigeostrophic equations and improve upon earlier energy spectrum predictions.

Method: Combined rigorous mathematical analysis using cascade mechanism arguments with direct numerical simulations to verify theoretical results.

Result: Derived an improved energy spectrum scaling of Œ∫^(2Œ≤/3-3) that fits better than earlier Œ∫^(Œ≤/3-3) prediction, and proved conditions for direct enstrophy cascade with bounds on dissipation rates.

Conclusion: The new energy spectrum scaling provides better fit to numerical data, and rigorous results on enstrophy cascade and dissipation bounds are numerically verified with parameter dependence demonstrated.

Abstract: Turbulent behavior of the two-parameter family of generalized surface
quasigeostrophic equations is examined both rigorously and numerically. We
adapt a cascade mechanism argument to derive an energy spectrum that scales as
$\kappa^{2\beta/3-3}$ where $\beta$ controls the regularity of the velocity
($\beta=1$ in the special case of the SQG). Direct numerical simulations
indicate that this fits better than $\kappa^{\beta/3-3}$ which was derived in
earlier work. Guided by earlier work on the 2D Navier-Stokes equations, we
prove a certain condition implies a direct cascade of enstrophy, as well as an
upper bound on the enstrophy dissipation rate, and sharp bounds on a
dissipation wavenumber. The dependence of these rigorous results on the two
parameters is demonstrated numerically.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [57] [The mechanics of $\textit{Less In More Out}$: modeling fabric-based soft robotic hearts](https://arxiv.org/abs/2510.14984)
*Marin Lauber,Mathias Peirlinck*

Main category: q-bio.TO

TL;DR: A computational model for fabric-based soft artificial hearts that predicts deformation, stress, and fatigue life, identifying key failure points and enabling design optimization.


<details>
  <summary>Details</summary>
Motivation: Fabric-based soft robots show promise for biomedical applications like artificial hearts, but predictive models are needed to understand their complex mechanics and improve reliability.

Method: Developed a computational model of the Less In More Out device - a fluidically actuated soft artificial heart made from heat-sealed woven fabric layers, validated against quasi-static experiments.

Result: The model accurately reproduces nonlinear deformation, strain fields, and pressure-volume relationships. Devices with fewer pouches deliver higher stroke volumes but 50% higher peak stresses. Heat-sealed seams and buckling regions are identified as critical fatigue points.

Conclusion: The framework enables detailed stress, buckling, and fatigue analysis that's difficult to obtain experimentally, providing a foundation for optimizing artificial hearts and other fluid-actuated fabric-based soft robotic systems.

Abstract: Fabric-based soft robots combine high load-carrying capacity, efficiency, and
low weight with the ability to bend, twist, contract, or extend with ease,
making them promising candidates for biomedical applications such as soft total
artificial hearts. While recent experiments have demonstrated their potential,
predictive numerical models are urgently needed to study their complex
mechanics, guide design optimization and improve their reliability. We develop
a computational model of the Less In More Out device, a fluidically actuated
soft total artificial heart constructed from heat-sealed layers of woven
fabric. Our model reproduces the nonlinear deformation, strain fields, and
pressure-volume relationships measured in quasi-static experiments. Devices
with fewer pouches deliver higher stroke volumes but exhibit up to 50% higher
peak von Mises stresses. Fatigue analysis using a strain-life approach
identifies heat-sealed seams and buckling regions as durability-limiting
features. Our framework enables detailed evaluation of stress concentrations,
buckling, and fatigue life, providing mechanistic insights that are difficult
to obtain experimentally. It also offers a foundation for the optimization of
artificial hearts and other fluid actuated fabric-based soft robotic systems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [58] [Atomic cluster expansion potential for the Si-H system](https://arxiv.org/abs/2510.15633)
*Louise A. M. Rosset,Volker L. Deringer*

Main category: cond-mat.mtrl-sci

TL;DR: A machine-learned interatomic potential based on atomic cluster expansion (ACE) is developed to model Si-H systems across various phases, enabling large-scale simulations of hydrogenated amorphous silicon for solar cell applications.


<details>
  <summary>Details</summary>
Motivation: Understanding atomic-scale effects of hydrogen in silicon matrices is crucial for solar-cell devices, but current knowledge gaps exist in hydrogenated silicon systems despite good understanding of elemental amorphous Si.

Method: Developed a machine-learned interatomic potential using the atomic cluster expansion (ACE) framework to model Si-H phases including crystalline/amorphous bulk, surfaces, and molecules.

Result: The potential was numerically and physically validated across hydrogen concentrations and showed good agreement with experimental findings.

Conclusion: This work advances the capability to explore large structural models of hydrogenated amorphous silicon (a-Si:H) at realistic device scales for solar cell applications.

Abstract: The silicon-hydrogen system is of key interest for solar-cell devices,
including both crystalline and amorphous modifications. Elemental amorphous Si
is now well understood, but the atomic-scale effects of hydrogenating the
silicon matrix remain to be fully explored. Here, we present a machine-learned
interatomic potential model based on the atomic cluster expansion (ACE)
framework that can describe a wide range of Si-H phases, from crystalline and
amorphous bulk structures to surfaces and molecules. We perform numerical and
physical validation across a range of hydrogen concentrations and compare our
results to experimental findings. Our work constitutes an advancement toward
the exploration of large structural models of a-Si:H at realistic device
scales.

</details>
