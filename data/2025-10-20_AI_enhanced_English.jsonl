{"id": "2510.15043", "pdf": "https://arxiv.org/pdf/2510.15043", "abs": "https://arxiv.org/abs/2510.15043", "authors": ["Mathias Dufresne-Pich\u00e9", "Siva Nadarajah"], "title": "Linearly Stable Generalizations of ESFR Schemes", "categories": ["math.NA", "cs.NA"], "comment": "31 pages, 29 figures", "summary": "The energy stable flux reconstruction (ESFR) method encompasses an infinite\nfamily of high-order, linearly stable schemes and thus provides a flex- ible\nand efficient framework for achieving high levels of accuracy on unstruc- tured\ngrids. One remarkable property of ESFR schemes is their ability to be expressed\nequivalently as linearly filtered discontinuous Galerkin (FDG) schemes. In this\nstudy, we introduce Sobolev Stable discontinuous Galerkin (SSDG) schemes, a new\nconservative and linearly stable generalization of ESFR schemes via the FDG\nframework. Additionally, we review existing generalizations of the ESFR method\nand consider their relationship with the FDG framework. The linear properties\nof SSDG schemes are studied via Von Neumann analysis and compared to those of\nthe existing extended ESFR (EESFR) method. It is found that while SSDG and\nEESFR schemes exhibit fundamentally different dispersive and dissipative\nbehaviors, they can achieve a similar increase in CFL limit and exhibit a\nsimilar spectral order of accuracy. Moreover, it is seen that the range of\nscheme parameters over which SSDG schemes can be used to increase the explicit\ntime-stepping limit is much larger than for EESFR schemes. Finally, it is\nobserved that the order of accuracy of EESFR schemes under h-refinement is\ngenerally p + 1 while that of SSDG schemes is p.", "AI": {"tldr": "Introduces Sobolev Stable discontinuous Galerkin (SSDG) schemes as a new conservative and linearly stable generalization of ESFR methods via the filtered DG framework, comparing them with existing EESFR methods.", "motivation": "To develop a more flexible and efficient framework for high-order accurate schemes on unstructured grids by generalizing ESFR methods through the filtered DG framework.", "method": "Proposes SSDG schemes as a new generalization of ESFR methods using the filtered discontinuous Galerkin framework, and analyzes their linear properties via Von Neumann analysis compared to EESFR methods.", "result": "SSDG and EESFR schemes show different dispersive/dissipative behaviors but similar CFL limit increases and spectral accuracy. SSDG has broader parameter range for time-stepping improvements, while EESFR achieves p+1 order accuracy vs SSDG's p order under h-refinement.", "conclusion": "SSDG schemes provide a viable alternative to EESFR with broader applicability for increasing explicit time-stepping limits, though with different accuracy characteristics under mesh refinement."}}
{"id": "2510.15093", "pdf": "https://arxiv.org/pdf/2510.15093", "abs": "https://arxiv.org/abs/2510.15093", "authors": ["Yue Zhao", "Huan Lei"], "title": "Fast spectral separation method for kinetic equation with anisotropic non-stationary collision operator retaining micro-model fidelity", "categories": ["math.NA", "cs.NA", "physics.comp-ph", "stat.ML"], "comment": null, "summary": "We present a generalized, data-driven collisional operator for one-component\nplasmas, learned from molecular dynamics simulations, to extend the collisional\nkinetic model beyond the weakly coupled regime. The proposed operator features\nan anisotropic, non-stationary collision kernel that accounts for particle\ncorrelations typically neglected in classical Landau formulations. To enable\nefficient numerical evaluation, we develop a fast spectral separation method\nthat represents the kernel as a low-rank tensor product of univariate basis\nfunctions. This formulation admits an $O(N \\log N)$ algorithm via fast Fourier\ntransforms and preserves key physical properties, including discrete\nconservation laws and the H-theorem, through a structure-preserving central\ndifference discretization. Numerical experiments demonstrate that the proposed\nmodel accurately captures plasma dynamics in the moderately coupled regime\nbeyond the standard Landau model while maintaining high computational\nefficiency and structure-preserving properties.", "AI": {"tldr": "A data-driven collisional operator for plasmas learned from MD simulations, extending kinetic models beyond weakly coupled regimes with anisotropic collision kernels and efficient spectral methods.", "motivation": "To overcome limitations of classical Landau formulations that neglect particle correlations in weakly coupled plasmas, enabling accurate modeling in moderately coupled regimes.", "method": "Developed a generalized collisional operator with anisotropic, non-stationary collision kernel learned from molecular dynamics simulations, using fast spectral separation with low-rank tensor representation and structure-preserving discretization.", "result": "The model accurately captures plasma dynamics in moderately coupled regimes beyond standard Landau model, while maintaining computational efficiency (O(N log N) complexity) and preserving physical properties like conservation laws and H-theorem.", "conclusion": "The proposed data-driven collisional operator successfully extends kinetic plasma modeling to moderately coupled regimes with improved accuracy while maintaining computational efficiency and physical structure preservation."}}
{"id": "2510.15097", "pdf": "https://arxiv.org/pdf/2510.15097", "abs": "https://arxiv.org/abs/2510.15097", "authors": ["Kazufumi Ito", "Tiancheng Xue"], "title": "Reduced order method based Anderson-type acceleration method for nonlinear least square problems and large scale ill-posed problems", "categories": ["math.NA", "cs.NA", "math.OC"], "comment": null, "summary": "In this paper we discuss the Anderson's type acceleration method for\nnumerical optimizations. Most mathematical modeling problems can be formulated\nas constrained optimization. The necessary optimality condition is written as a\nfixed point problem in a Banach space. Anderson's acceleration method improves\nthe convergence of the standard fixed point iteration by minimizing the total\nsum of residuals and updating solutions through an optimal linear combination\nof a sequence of iterates. Thus, it is a form of iterative method of retard\nwhich uses the history of solutions as a reduced order element method (ROM).\nThe weights are determined optimally by the least squares problem based on the\ntotal residual. We analyze Anderson's method and the reduced order method (ROM)\nfor nonlinear least squares problems of minimizing |F(x)| squared. That is, the\nsolution is approximated by a linear combination of sequentially generated\nsolutions, and then we minimize the equation error on the linear manifold\nspanned by the iterates. We use the reduced order Gauss Newton method to solve\nthe least squares problem for |F(x)| squared on the linear solution manifold.\nFor the linear equation case it is similar to Anderson's method. Anderson's\nmethod approximates the solution to the nonlinear ROM. We consider variable\nstep size gradient and quasi Newton methods and the variable fixed point\niteration to generate the solution basis, then combine these with ROM\nacceleration. It converges very rapidly if the condition number of matrix A is\nnot large. The variable iterate with ROM acceleration is nearly optimal, and\nROM also regularizes and stabilizes the convergence. We also consider a\nrandomly overlapped Kaczmarz type method and develop an acceleration approach\nfor large scale ill posed linear systems. Finally, we analyze the convergence\nof the ROM to the operator equation.", "AI": {"tldr": "Anderson's acceleration method improves fixed point iteration convergence by minimizing residuals and using solution history as a reduced order method (ROM). The method combines variable step size gradient/quasi-Newton methods with ROM acceleration for rapid convergence.", "motivation": "Most mathematical modeling problems are formulated as constrained optimization, with necessary optimality conditions written as fixed point problems. Anderson's method addresses slow convergence in standard fixed point iterations.", "method": "Uses Anderson's acceleration with ROM approach: approximates solution as linear combination of iterates, minimizes equation error on linear manifold, applies reduced order Gauss-Newton method for least squares problems. Combines variable step size gradient/quasi-Newton methods with ROM acceleration.", "result": "Method converges rapidly when condition number is not large. ROM acceleration is nearly optimal, regularizes and stabilizes convergence. Also developed acceleration for large-scale ill-posed linear systems using randomly overlapped Kaczmarz type method.", "conclusion": "Anderson's method with ROM acceleration provides efficient convergence for optimization problems, combining historical solution information with optimal linear combinations to improve fixed point iteration performance."}}
{"id": "2510.15062", "pdf": "https://arxiv.org/pdf/2510.15062", "abs": "https://arxiv.org/abs/2510.15062", "authors": ["Jesse Griff-McMahon", "Xavier Vaisseau", "William Fox", "Kirill Lezhnin", "Krish Bhutwala", "Ryan Nedbailo", "Valeria Opsina-Boh\u00f3rquez", "Timo Karpowski", "Pravesh K. Patel", "Sophia Malko"], "title": "Characterization of proton focusing from variable-sized hemispherical targets", "categories": ["physics.plasm-ph"], "comment": "9 pages, 9 figures", "summary": "We systematically characterize the focusing behavior of laser-driven proton\nbeams from hemispherical targets of various diameters using mesh radiography.\nThe proton focal location is inferred to be near the geometrical center for the\nsmallest tested hemisphere ($\\Psi=D_{hemi}/D_{Laser}=6.1$). However, larger\nhemispheres ($\\Psi=14.6$) degrade the focusing behavior and behave more like\nflat foils with focal location significantly inside the hemisphere. We also\ninfer a tight virtual focus of $9\\pm3~\\mu$m through a mesh transition analysis.", "AI": {"tldr": "Proton beam focusing from laser-driven hemispherical targets varies with size - smaller hemispheres focus near center while larger ones behave like flat foils with internal focus points.", "motivation": "To systematically characterize how hemispherical target size affects laser-driven proton beam focusing behavior.", "method": "Used mesh radiography to analyze proton beams from hemispherical targets of various diameters, comparing focusing behavior across different size ratios (\u03a8 = D_hemi/D_Laser).", "result": "Smallest hemisphere (\u03a8=6.1) focused protons near geometrical center, while larger hemispheres (\u03a8=14.6) degraded focusing and behaved like flat foils with focal points inside the hemisphere. Virtual focus of 9\u00b13 \u03bcm was measured.", "conclusion": "Hemisphere size significantly impacts proton beam focusing - smaller targets provide better focusing near center, while larger targets exhibit degraded focusing behavior similar to flat foils."}}
{"id": "2510.15274", "pdf": "https://arxiv.org/pdf/2510.15274", "abs": "https://arxiv.org/abs/2510.15274", "authors": ["Xiangyi Peng", "Lisen Ding", "Wenlin Qiu"], "title": "An Efficient Space-Time Two-Grid Compact Difference Scheme for the Two-Dimensional Viscous Burgers' Equation", "categories": ["math.NA", "cs.NA", "65M06, 65M15, 65M55"], "comment": null, "summary": "This work proposes an efficient space-time two-grid compact difference\n(ST-TGCD) scheme for solving the two-dimensional (2D) viscous Burgers' equation\nsubject to initial and periodic boundary conditions. The proposed approach\ncombines a compact finite difference discretization with a two-grid strategy to\nachieve high computational efficiency without sacrificing accuracy. In the\ncoarse-grid stage, a fixed-point iteration is employed to handle the nonlinear\nsystem, while in the fine-grid stage, linear temporal and cubic spatial\nLagrange interpolations are used to construct initial approximations. The final\nfine-grid solution is refined through a carefully designed linearized\ncorrection scheme. Rigorous analysis establishes unconditional convergence of\nthe method, demonstrating second-order accuracy in time and fourth-order\naccuracy in space. Numerical experiments verify the theoretical results and\nshow that the ST-TGCD scheme reduces CPU time by more than 70\\% compared with\nthe traditional nonlinear compact difference (NCD) method, while maintaining\ncomparable accuracy. These findings confirm the proposed scheme as a highly\nefficient alternative to conventional nonlinear approaches.", "AI": {"tldr": "An efficient space-time two-grid compact difference scheme for 2D viscous Burgers' equation that combines compact finite difference with two-grid strategy, achieving 70% CPU time reduction while maintaining high accuracy.", "motivation": "To develop a computationally efficient method for solving the 2D viscous Burgers' equation that reduces computational cost without sacrificing accuracy compared to traditional nonlinear approaches.", "method": "Two-grid strategy with compact finite difference discretization: coarse grid uses fixed-point iteration for nonlinear system, fine grid uses linear temporal and cubic spatial Lagrange interpolations for initial approximations, followed by linearized correction scheme.", "result": "Method achieves second-order accuracy in time and fourth-order accuracy in space, reduces CPU time by more than 70% compared to traditional nonlinear compact difference method while maintaining comparable accuracy.", "conclusion": "The ST-TGCD scheme is a highly efficient alternative to conventional nonlinear approaches for solving the 2D viscous Burgers' equation, offering significant computational savings with maintained accuracy."}}
{"id": "2510.15504", "pdf": "https://arxiv.org/pdf/2510.15504", "abs": "https://arxiv.org/abs/2510.15504", "authors": ["L. Pigatto", "G. Frello", "Y. Q. Liu", "L. Novello", "M. Takechi", "E. Tomasina", "T. Bolzonella"], "title": "Modelling-driven requirements for Error Field Control Coil application to initial JT-60SA plasmas", "categories": ["physics.plasm-ph", "cs.SY", "eess.SY"], "comment": null, "summary": "JT-60SA is a large superconducting tokamak built in Naka, Japan. After the\nsuccessful achievement of its first MA-class plasma, the installation of\nseveral additional sub-systems, including a set of non-axisymmetric Error Field\nCorrection Coils (EFCC), is ongoing. Optimization of future JT-60SA plasma\nscenarios will critically depend on the correct use of EFCC, including careful\nfulfillment of system specifications. In addition to that, preparation and risk\nmitigation of early ITER operations will greatly benefit from the experience\ngained by early EFCC application to JT-60SA experiments, in particular to\noptimize error field detection and control strategies. In this work, EFCC\napplication in JT-60SA Initial Research Phase I perspective scenarios is\nmodeled including plasma response. Impact of (Resonant) Magnetic Perturbations\non the different plasma scenarios is assessed for both core and pedestal\nregions by the linear resistive MHD code MARS-F. The dominant core response to\nEFs is discussed case by case and compared to mode locking thresholds from\nliterature. Typical current/voltage amplitudes and wave-forms are then compared\nto EFCC specifications in order to assess a safe operational space.", "AI": {"tldr": "Modeling of Error Field Correction Coils (EFCC) application in JT-60SA tokamak scenarios using MARS-F code to assess magnetic perturbation impacts on plasma core and pedestal regions, and comparing operational parameters with EFCC specifications.", "motivation": "Optimize future JT-60SA plasma scenarios through correct EFCC usage and prepare for ITER operations by gaining experience with error field detection and control strategies from JT-60SA experiments.", "method": "Used linear resistive MHD code MARS-F to model EFCC application in JT-60SA Initial Research Phase I scenarios, including plasma response, and assessed impact of Resonant Magnetic Perturbations on core and pedestal regions.", "result": "Analyzed dominant core response to error fields case by case and compared to mode locking thresholds from literature, then evaluated typical current/voltage amplitudes and waveforms against EFCC specifications.", "conclusion": "The study helps assess safe operational space for EFCC usage in JT-60SA and provides valuable insights for error field control strategies applicable to ITER operations."}}
{"id": "2510.15089", "pdf": "https://arxiv.org/pdf/2510.15089", "abs": "https://arxiv.org/abs/2510.15089", "authors": ["Vasily Ilin"], "title": "Stability of the spatially homogeneous Landau equation in relative entropy and applications to score-based numerical methods", "categories": ["math.AP", "35B35 (Primary) 35Q84, 82C40, 65M75 (Secondary)"], "comment": "9 pages", "summary": "We give a short and elementary proof of stability for strong solutions of the\nspatially homogeneous Landau equation with Coulomb collisions, measured in\nrelative entropy. The argument yields an explicit differential inequality for\nrelative entropy under natural moment and regularity assumptions. The same\ncomputation provides an a posteriori error bound for score-based transport\nmodeling and related deterministic numerical schemes, linking the training loss\nto the relative-entropy error.", "AI": {"tldr": "Elementary proof of stability for Landau equation solutions using relative entropy, with applications to error bounds in score-based transport modeling.", "motivation": "To provide a simple and explicit stability analysis for the Landau equation with Coulomb collisions, and connect this to practical error bounds in computational methods.", "method": "Short elementary proof yielding explicit differential inequality for relative entropy under natural moment and regularity assumptions.", "result": "Establishes stability of strong solutions and provides explicit error bounds linking training loss to relative-entropy error in numerical schemes.", "conclusion": "The approach provides both theoretical stability guarantees and practical computational error bounds for Landau equation and related transport modeling."}}
{"id": "2510.15291", "pdf": "https://arxiv.org/pdf/2510.15291", "abs": "https://arxiv.org/abs/2510.15291", "authors": ["Luke P. Filippini", "Adrianne L. Jenner", "Elliot J. Carr"], "title": "Random walk models of anisotropic diffusion on rectangular and hexagonal lattices", "categories": ["physics.comp-ph"], "comment": "30 pages, 11 figures, 1 table, submitted", "summary": "The diffusive transport of particles in anisotropic media is a fundamental\nphenomenon in computational, medical and biological disciplines. While\ndeterministic models (partial differential equations) of such processes are\nwell established, their inability to capture inherent randomness, and the\nassumption of a large number of particles, hinders their applicability. To\naddress these issues, we present several equivalent (discrete-space\ndiscrete-time) random walk models of diffusion described by a\nspatially-invariant tensor on a two-dimensional domain with no-flux boundary\nconditions. Our approach involves discretising the deterministic model in space\nand time to give a homogeneous Markov chain governing particle movement between\n(spatial) lattice sites over time. The spatial discretisation is carried out\nusing a vertex-centred element-based finite volume method on rectangular and\nhexagonal lattices, and a forward Euler discretisation in time yields a\nnearest-neighbour random walk model with simple analytical expressions for the\ntransition probabilities. For each lattice configuration, analysis of these\nexpressions yields constraints on the time step duration, spatial steps and\ndiffusion tensor to ensure the probabilities are between zero and one. We find\nthat model implementation on a rectangular lattice can be achieved with a\nconstraint on the diffusion tensor, whereas a hexagonal lattice overcomes this\nlimitation (no restrictions on the diffusion tensor). Overall, the results\ndemonstrate good visual and quantitative (mean-squared error) agreement between\nthe deterministic model and random walk simulations for several test cases. All\nresults are obtained using MATLAB code available on GitHub\n(https://github.com/lukefilippini/Filippini2025).", "AI": {"tldr": "The paper presents discrete random walk models for anisotropic diffusion on rectangular and hexagonal lattices, showing that hexagonal lattices overcome tensor constraints found in rectangular implementations.", "motivation": "To address limitations of deterministic diffusion models that can't capture inherent randomness and assume large particle numbers, by developing stochastic random walk alternatives.", "method": "Discretize deterministic diffusion PDEs using finite volume method on rectangular/hexagonal lattices with forward Euler time stepping, creating homogeneous Markov chain random walk models with analytical transition probabilities.", "result": "Good visual and quantitative agreement between deterministic models and random walk simulations. Rectangular lattices require diffusion tensor constraints while hexagonal lattices have no such restrictions.", "conclusion": "Hexagonal lattice random walk models provide more flexible implementation for anisotropic diffusion without tensor constraints, with MATLAB code available for implementation."}}
{"id": "2510.15351", "pdf": "https://arxiv.org/pdf/2510.15351", "abs": "https://arxiv.org/abs/2510.15351", "authors": ["Long Chen", "Ruchi Guo", "Jingrong Wei", "Jun Zou"], "title": "A Novel Preconditioning Framework for Solving Nonlinear PDEs based on Fenchel-Rockafellar Duality and Transformed Primal-Dual Techniques", "categories": ["math.NA", "cs.NA", "65Y20, 65N12, 49N15"], "comment": null, "summary": "A DualTPD method is proposed for solving nonlinear partial differential\nequations. The method is characterized by three main features. First,\ndecoupling via Fenchel--Rockafellar duality is achieved, so that nonlinear\nterms are discretized by discontinuous finite element spaces, yielding\nblock-diagonal mass matrices and closed-form updates. Second, improved\nconvergence is obtained by applying transformed primal--dual (TPD) dynamics to\nthe nonlinear saddle-point system, which yields strongly monotone behavior.\nThird, efficient preconditioners are designed for the elliptic-type Schur\ncomplement arising from the separated differential operators, and multigrid\nsolvers are applied effectively. Extensive numerical experiments on elliptic\n$p$-Laplacian and nonlinear $H(\\curl)$ problems are presented, showing\nsignificant efficiency gains with global, mesh-independent convergence.", "AI": {"tldr": "A DualTPD method for solving nonlinear PDEs using Fenchel-Rockafellar duality, TPD dynamics, and efficient preconditioners with multigrid solvers.", "motivation": "To develop an efficient method for solving nonlinear partial differential equations that addresses challenges with nonlinear terms and convergence issues.", "method": "Decouples nonlinear terms via Fenchel-Rockafellar duality using discontinuous finite element spaces, applies transformed primal-dual dynamics for improved convergence, and designs efficient preconditioners with multigrid solvers for elliptic-type Schur complements.", "result": "Extensive numerical experiments on elliptic p-Laplacian and nonlinear H(curl) problems show significant efficiency gains with global, mesh-independent convergence.", "conclusion": "The DualTPD method provides an effective approach for nonlinear PDEs with improved convergence properties and computational efficiency."}}
{"id": "2510.15641", "pdf": "https://arxiv.org/pdf/2510.15641", "abs": "https://arxiv.org/abs/2510.15641", "authors": ["S. Kodate", "Y. Hayashi", "S. Kajita"], "title": "Formation of a Tungsten Co-deposition Layer with Microparticles Using Pulsed Laser Deposition", "categories": ["physics.plasm-ph"], "comment": "to be published in Plasma Fusion Research", "summary": "This study performed tungsten (W) co-deposition experiments using pulsed\nlaser deposition under exposure to helium/argon plasma in a linear plasma\ndevice. Micron-sized spherical W particles formed under co-deposition\nconditions. It was suggested that these particles grew from nanoparticles via\nthe electrostatic collection of W ions in the plasma plume before deposition.", "AI": {"tldr": "Tungsten co-deposition experiments using pulsed laser deposition with helium/argon plasma exposure formed micron-sized spherical W particles that grew from nanoparticles via electrostatic collection of W ions.", "motivation": "To study tungsten co-deposition behavior under plasma exposure conditions relevant to fusion applications.", "method": "Used pulsed laser deposition with helium/argon plasma exposure in a linear plasma device for tungsten co-deposition experiments.", "result": "Formed micron-sized spherical tungsten particles under co-deposition conditions, suggesting growth from nanoparticles via electrostatic collection of W ions in plasma plume.", "conclusion": "The formation mechanism of micron-sized W particles involves nanoparticle growth through electrostatic collection of tungsten ions before deposition."}}
{"id": "2510.15098", "pdf": "https://arxiv.org/pdf/2510.15098", "abs": "https://arxiv.org/abs/2510.15098", "authors": ["Francesca Gladiali", "Massimo Grossi", "Luigi Provenzano"], "title": "The role of the curvature of a surface in the shape of the solutions to elliptic equations", "categories": ["math.AP"], "comment": null, "summary": "We prove uniqueness and non-degeneracy of the critical point of positive,\nsemi-stable solutions of $-\\Delta u=f(u)$ with Dirichlet boundary conditions\nfor a class of star-shaped domains of the sphere and of the hyperbolic plane\nsatisfying a geometric condition. In the spherical case, this condition is\nweaker than convexity, while in the hyperbolic case it is weaker than\nhoroconvexity. Finally, we construct examples showing that this geometric\ncondition is indeed optimal.", "AI": {"tldr": "Proves uniqueness and non-degeneracy of critical points for positive semi-stable solutions of -\u0394u=f(u) with Dirichlet boundary conditions on star-shaped domains in sphere and hyperbolic plane under optimal geometric conditions.", "motivation": "To establish rigorous mathematical results about the behavior of solutions to semilinear elliptic equations on curved manifolds, specifically identifying when critical points are unique and non-degenerate.", "method": "Mathematical analysis using geometric conditions on star-shaped domains in sphere (weaker than convexity) and hyperbolic plane (weaker than horoconvexity), with construction of examples to show optimality.", "result": "Proved uniqueness and non-degeneracy of critical points for positive semi-stable solutions under specified geometric conditions, and demonstrated these conditions are optimal through counterexamples.", "conclusion": "The geometric conditions identified are both sufficient and necessary for the uniqueness and non-degeneracy properties of critical points in these curved space settings."}}
{"id": "2510.15293", "pdf": "https://arxiv.org/pdf/2510.15293", "abs": "https://arxiv.org/abs/2510.15293", "authors": ["Zhexian Li", "Felipe de Barros", "Ketan Savla"], "title": "Constrained bilinear optimal control of reactive evolution equations", "categories": ["physics.comp-ph", "math.OC"], "comment": "38 pages, 7 figures", "summary": "We consider constrained bilinear optimal control of second-order linear\nevolution partial differential equations (PDEs) with a reaction term on the\nhalf line, where control arises as a time-dependent reaction coefficient and\nconstraints are imposed on the state and control variables. These PDEs\nrepresent a wide range of physical phenomena in fluid flow, heat, and mass\ntransfer. Existing computational methods for this type of control problems only\nconsider constraints on control variables. In this paper, we propose a novel\noptimize-then-discretize framework for computing constrained bilinear optimal\ncontrol with both state and control constraints. Unlike existing methods that\nderive optimality conditions directly from the PDE constraint, this framework\nfirst replaces the PDE constraint with an equivalent integral representation of\nthe PDE solution. The integral representation, derived from the unified\ntransform method, does not involve differential operators, and thus explicit\nexpressions for necessary conditions of optimality can be derived using the\nKarush-Kuhn-Tucker conditions for infinite-dimensional optimization.\nDiscretizing the optimality conditions results in a system of\nfinite-dimensional smooth nonlinear equations, which can be efficiently solved\nusing existing solvers without the need for specialized algorithms. This is in\ncontrast with discretize-then-optimize methods that discretize the PDE first\nand then solve the optimality conditions of the approximated finite-dimensional\nproblem. Computational results for two applications, namely nuclear reactivity\ncontrol and water quality treatment in a reactor, are presented to illustrate\nthe effectiveness of the proposed framework.", "AI": {"tldr": "A novel optimize-then-discretize framework for constrained bilinear optimal control of second-order PDEs with both state and control constraints, using integral representations from unified transform method.", "motivation": "Existing methods only handle control constraints, but many physical applications require both state and control constraints. Current approaches derive optimality conditions directly from PDEs, which is complex for constrained problems.", "method": "Replace PDE constraint with equivalent integral representation using unified transform method, derive necessary optimality conditions via KKT conditions for infinite-dimensional optimization, then discretize to obtain smooth nonlinear equations.", "result": "Framework successfully applied to nuclear reactivity control and water quality treatment, showing effectiveness without needing specialized algorithms.", "conclusion": "The optimize-then-discretize approach with integral representations provides efficient solution for constrained bilinear optimal control problems with both state and control constraints."}}
{"id": "2510.15475", "pdf": "https://arxiv.org/pdf/2510.15475", "abs": "https://arxiv.org/abs/2510.15475", "authors": ["Mathieu Benninghoff", "Gilles Vilmart"], "title": "Second order explicit stabilized multirate method for stiff differential equations with error control", "categories": ["math.NA", "cs.NA", "65L04, 65L06, 65L20"], "comment": null, "summary": "Explicit stabilized methods are highly efficient time integrators for large\nand stiff systems of ordinary differential equations especially when applied to\nsemi-discrete parabolic problems. However, when local spatial mesh refinement\nis introduced, their efficiency decreases, since the stiffness is driven by\nonly the smallest mesh element. A natural approach is to split the system into\nfast stiff and slower mildly stiff components. In this context, [A. Abdulle,\nM.J. Grote and G. Rosilho de Souza 2022] proposed the order one multirate\nexplicit stabilized method (mRKC). We extend their approach to second order and\nintroduce the new multirate ROCK2 method (mROCK2), which achieves high\nprecision and allows a step-size strategy with error control. Numerical methods\nincluding the heat equation with local spatial mesh refinements confirm the\naccuracy and efficiency of the scheme.", "AI": {"tldr": "The paper introduces mROCK2, a second-order multirate explicit stabilized method that extends previous first-order approaches to handle local spatial mesh refinement more efficiently by splitting systems into fast stiff and slower mildly stiff components.", "motivation": "Explicit stabilized methods lose efficiency with local spatial mesh refinement because stiffness is determined by the smallest mesh element, requiring a more efficient approach that handles different stiffness levels separately.", "method": "Extends the first-order multirate explicit stabilized method (mRKC) to second order by introducing mROCK2, which splits the system into fast stiff and slower mildly stiff components and includes a step-size strategy with error control.", "result": "Numerical experiments with the heat equation and local spatial mesh refinements confirm that mROCK2 achieves high precision and maintains efficiency.", "conclusion": "The mROCK2 method successfully extends multirate explicit stabilized methods to second order, providing improved accuracy and efficient error-controlled step-size strategies for systems with local mesh refinement."}}
{"id": "2510.15156", "pdf": "https://arxiv.org/pdf/2510.15156", "abs": "https://arxiv.org/abs/2510.15156", "authors": ["Zitao Hu", "Xue-Ning Bai", "Xiaochen Sun"], "title": "Magnetohydrodynamic-guiding-center-particle-in-cell Method for Multiscale Plasma Kinetic Simulations", "categories": ["astro-ph.HE", "astro-ph.SR", "physics.plasm-ph"], "comment": "28 pages, 10 figures, submitted to ApJS, comments welcomed", "summary": "We present the formulation, algorithm and numerical tests of the\nmagnetohydrodynamic-particle-in-cell (MHD-PIC) method with particles treated\nunder the guiding center approximation, which we term the MHD-gPIC method, and\nit is implemented in the Athena++ MHD code. The new MHD-gPIC model consists of\nthermal (cold) fluid and high-energy particles whose dynamics are integrated\nthrough guiding center equations including drift motion, with carefully\nevaluated source terms as particle backreaction. The code is validated with a\nseries of tests, and it is expected to be primarily applicable to study\nparticle acceleration and transport in systems where gyro-resonance is\nconsidered insignificant. We also present preliminary studies of particle\nacceleration during non-relativistic magnetic reconnection.", "AI": {"tldr": "The paper presents the MHD-gPIC method, which combines magnetohydrodynamics with guiding center particle-in-cell approach for studying particle acceleration and transport in systems where gyro-resonance is insignificant.", "motivation": "To develop a computational method that can efficiently simulate systems with both thermal fluid components and high-energy particles, particularly for studying particle acceleration and transport in astrophysical systems.", "method": "The MHD-gPIC method integrates magnetohydrodynamics with particle-in-cell approach using guiding center approximation for particles, including drift motion and carefully evaluated particle backreaction source terms. Implemented in Athena++ MHD code.", "result": "The code was validated with a series of tests and shown to be applicable for studying particle acceleration and transport. Preliminary studies of particle acceleration during non-relativistic magnetic reconnection were also presented.", "conclusion": "The MHD-gPIC method provides an effective computational framework for studying particle acceleration and transport in systems where gyro-resonance effects are not significant, with successful implementation and validation."}}
{"id": "2510.15153", "pdf": "https://arxiv.org/pdf/2510.15153", "abs": "https://arxiv.org/abs/2510.15153", "authors": ["Maryna Kachanovska", "\u00c9tienne Peillon"], "title": "Limiting absorption principle for a hybrid resonance in a two-dimensional cold plasma", "categories": ["math.AP", "math-ph", "math.MP", "35B65, 35B34, 35D40, 35M12, 35Q61"], "comment": "An extended version of the manuscript", "summary": "We study a limiting absorption principle for the boundary-value problem\ndescribing a hybrid plasma resonance, with a regular coefficient in the\nprincipal part of the operator that vanishes on a curve inside the domain and\nchanges its sign across this curve. We prove the limiting absorption principle\nby establishing a priori bounds on the solution in certain weighted Sobolev\nspaces. Next, we show that the solution can be decomposed into regular and\nsingular parts. A peculiar property of this decomposition enables us to\nintroduce a radiation-like condition in a bounded domain and to state a\nwell-posed problem satisfied by the limiting absorption solution.", "AI": {"tldr": "Analysis of hybrid plasma resonance boundary-value problem with vanishing coefficient, proving limiting absorption principle via weighted Sobolev spaces and solution decomposition.", "motivation": "To study hybrid plasma resonance problems where the principal operator coefficient vanishes on a curve and changes sign, requiring special mathematical treatment.", "method": "Establish a priori bounds in weighted Sobolev spaces, prove limiting absorption principle, decompose solution into regular and singular parts, introduce radiation-like condition.", "result": "Successfully proved limiting absorption principle and established well-posed problem for the limiting absorption solution in bounded domain.", "conclusion": "The approach enables rigorous treatment of hybrid plasma resonance with vanishing coefficients through weighted spaces and radiation conditions."}}
{"id": "2510.15424", "pdf": "https://arxiv.org/pdf/2510.15424", "abs": "https://arxiv.org/abs/2510.15424", "authors": ["Amir Mohammad Mirzaei"], "title": "Towards In-Situ Failure Assessment: Deep Learning on DIC Results for Laminated Composites", "categories": ["physics.comp-ph"], "comment": null, "summary": "Predicting fracture load in laminated composites with stress raisers is\nchallenging due to complex failure mechanisms such as delamination, fibre\nbreakage, and matrix cracking, which are heavily influenced by fibre\norientation, layup sequence, and notch geometry. This study aims to address\nthis by developing a novel deep learning framework that leverages solely\nexperimental strain field data from Digital Image Correlation (DIC) for\naccurate, in-situ predictions--bypassing the need for finite element\nsimulations or empirical calibrations. Two complementary architectures are\nexplored: a multi-layer perceptron (MLP) that processes numerical values of\nmaximum principal strain from a targeted rectangular region ahead of the notch,\nenhanced by advanced feature selection (mutual information, Lasso, and SHAP) to\nfocus on critical data points; and a convolutional neural network (CNN) trained\non full-field strain images, bolstered by data augmentation to handle\nvariability and prevent overfitting. Validated across 116 quasi-static tests\nencompassing 31 distinct configurations--including six layups (quasi-isotropic\nto highly anisotropic) with four off-axis angles for open-hole specimens, and\none cross-ply layup with four off-axis and four on-axis notch orientations for\nU-notched specimens--the MLP and CNN achieve coefficients of determination\n(R^2) of 0.86 and 0.82, respectively. This framework captures a broad spectrum\nof damage modes and responses, from brittle fibre-dominated fracture to ductile\ndelamination-driven failure, and due to its computational efficiency and\nreliance only on DIC measurements, the approach enables practical in-situ\nfracture load estimation.", "AI": {"tldr": "A deep learning framework using DIC strain field data predicts fracture loads in laminated composites with stress raisers, achieving R\u00b2 values of 0.86 (MLP) and 0.82 (CNN) without needing finite element simulations.", "motivation": "Predicting fracture load in composites with stress raisers is difficult due to complex failure mechanisms influenced by fiber orientation, layup sequence, and notch geometry. Existing methods require finite element simulations or empirical calibrations.", "method": "Two deep learning architectures: MLP processing numerical maximum principal strain values from a targeted region with feature selection (mutual information, Lasso, SHAP), and CNN trained on full-field strain images with data augmentation. Validated on 116 tests across 31 configurations.", "result": "MLP achieved R\u00b2 of 0.86, CNN achieved R\u00b2 of 0.82 across various composite configurations including different layups, notch types, and orientations. The framework captured diverse damage modes from brittle fiber fracture to ductile delamination.", "conclusion": "The computationally efficient framework enables practical in-situ fracture load estimation using only DIC measurements, bypassing traditional simulation requirements while handling complex composite failure mechanisms."}}
{"id": "2510.15481", "pdf": "https://arxiv.org/pdf/2510.15481", "abs": "https://arxiv.org/abs/2510.15481", "authors": ["Carlos Arranz-Sim\u00f3n", "Bego\u00f1a Cano", "C\u00e9sar Palencia"], "title": "Rational methods for abstract linear initial boundary value problems without order reduction", "categories": ["math.NA", "cs.NA", "65J10, 65M15"], "comment": "21 pages, 2 figures", "summary": "Given an $A$-stable rational approximation to $e^z$ of order $p$, numerical\nprocedures are suggested to time integrate abstract, well-posed IBVPs, with\ntime-dependent source term $f$ and boundary value $g$. These procedures exhibit\nthe optimal order $p$ and can be implemented by using just one single\nevaluation of $f$ and $g$ per step, i.e., no evaluations of the derivatives of\ndata are needed, and are of practical use at least for $p\\le 6$. The full\ndiscretization is also studied and the theoretical results are corroborated by\nnumerical experiments.", "AI": {"tldr": "A-stable rational approximations to e^z are used to develop efficient time integration methods for IBVPs with time-dependent source terms and boundary values, achieving optimal order p with minimal function evaluations.", "motivation": "To create practical numerical methods for abstract, well-posed initial-boundary value problems (IBVPs) that can handle time-dependent source terms and boundary values efficiently without requiring derivative evaluations of the data.", "method": "Using A-stable rational approximations to e^z of order p, the authors develop numerical procedures that require only one evaluation of f (source term) and g (boundary value) per time step, eliminating the need for derivative evaluations.", "result": "The proposed methods achieve optimal order p convergence and are practical for p \u2264 6. Full discretization analysis is provided and numerical experiments confirm the theoretical results.", "conclusion": "The developed procedures provide efficient and practical time integration methods for IBVPs with time-dependent data, achieving optimal order accuracy with minimal computational cost (one function evaluation per step)."}}
{"id": "2510.15159", "pdf": "https://arxiv.org/pdf/2510.15159", "abs": "https://arxiv.org/abs/2510.15159", "authors": ["Massimiliano Berti", "Ricardo Grande", "Alberto Maspero", "Gigliola Staffilani"], "title": "Rogue waves and large deviations for 2D pure gravity deep water waves", "categories": ["math.AP", "math-ph", "math.MP", "math.PR"], "comment": null, "summary": "Rogue waves are extreme ocean events characterized by the sudden formation of\nanomalously large crests, and remain an important subject of investigation in\noceanography and mathematics. A central problem is to quantify the probability\nof their formation under random Gaussian sea initial data. In this work, we\nrigorously characterize the tail probability for the formation of rogue waves\nof the pure gravity water wave equations in deep water, the most accurate\nquasilinear PDE modeling waves in open ocean. This large deviation result\nconfirms various conjectures from the oceanography literature in the weakly\nnonlinear regime. Moreover, the result holds up to the optimal timescales\nallowed by deterministic well-posedness theory. The proof shows that rogue\nwaves most likely arise through \"dispersive focusing\", where phase\nquasi-synchronization produces constructive amplification of the water crest.\nThe main difficulty in justifying this mechanism is propagating statistical\ninformation over such long timescales, which we overcome by combining normal\nforms and probabilistic methods. Unlike prior work, this novel approach does\nnot require approximate solutions to be Gaussian. Our general method tracks the\ntail probability of solutions to Hamiltonian PDEs with an integrable normal\nform and random Gaussian initial data, even in the absence of (quasi-)invariant\nmeasures.", "AI": {"tldr": "Rigorous characterization of rogue wave formation probability in deep water gravity wave equations, confirming oceanography conjectures about dispersive focusing mechanism.", "motivation": "To quantify the probability of rogue wave formation under random Gaussian sea initial data, addressing a central problem in oceanography and mathematics.", "method": "Combines normal forms and probabilistic methods to propagate statistical information over long timescales, tracking tail probability of solutions to Hamiltonian PDEs with integrable normal form and random Gaussian initial data.", "result": "Large deviation result confirms conjectures about rogue wave formation through dispersive focusing in weakly nonlinear regime, holding up to optimal timescales allowed by deterministic well-posedness theory.", "conclusion": "Rogue waves most likely arise through dispersive focusing where phase quasi-synchronization produces constructive amplification of water crest, with novel approach that doesn't require approximate solutions to be Gaussian."}}
{"id": "2510.15852", "pdf": "https://arxiv.org/pdf/2510.15852", "abs": "https://arxiv.org/abs/2510.15852", "authors": ["Maximilian Cederholm", "Siyao Wang", "Haochun Wang", "Ruichen Xu", "Yuefan Deng"], "title": "Boundary-Informed Method of Lines for Physics Informed Neural Networks", "categories": ["physics.comp-ph", "65N75"], "comment": "To appear in the SIAM Undergraduate Research Online proceedings,\n  March 2026", "summary": "We propose a hybrid solver that fuses the dimensionality-reduction strengths\nof the Method of Lines (MOL) with the flexibility of Physics-Informed Neural\nNetworks (PINNs). Instead of approximating spatial derivatives with fixed\nfinite-difference stencils - whose truncation errors force extremely fine\nmeshes - our method trains a neural network to represent the initial spatial\nprofile and then employs automatic differentiation to obtain spectrally\naccurate gradients at arbitrary nodes. These high-fidelity derivatives define\nthe right-hand side of the MOL-generated ordinary-differential system, and time\nintegration is replaced with a secondary temporal PINN while spatial accuracy\nis retained without mesh refinement. The resulting \"boundary-informed MOL-PINN\"\nmatches or surpasses conventional MOL in accuracy using an order of magnitude\nfewer collocation points, thereby shrinking memory footprints, lessening\ndependence on large data sets, and increasing complexity robustness. Because it\nrelies only on automatic differentiation and standard optimizers, the framework\nextends naturally to linear and nonlinear PDEs in any spatial dimension.", "AI": {"tldr": "A hybrid solver combining Method of Lines and Physics-Informed Neural Networks that achieves spectral accuracy with fewer collocation points than conventional methods.", "motivation": "To overcome limitations of traditional MOL that requires fine meshes due to truncation errors from finite-difference stencils, and to reduce memory footprint and data dependence.", "method": "Trains a neural network to represent initial spatial profile, uses automatic differentiation for spectrally accurate gradients, replaces time integration with temporal PINN while maintaining spatial accuracy without mesh refinement.", "result": "Achieves same or better accuracy than conventional MOL using an order of magnitude fewer collocation points, reduces memory footprint, lessens data dependence, and increases complexity robustness.", "conclusion": "The boundary-informed MOL-PINN framework provides an efficient alternative to conventional methods that naturally extends to linear and nonlinear PDEs in any spatial dimension using only automatic differentiation and standard optimizers."}}
{"id": "2510.15574", "pdf": "https://arxiv.org/pdf/2510.15574", "abs": "https://arxiv.org/abs/2510.15574", "authors": ["Gouranga Mallik"], "title": "A Hybrid High-Order Finite Element Method for a Nonlocal Nonlinear Problem of Kirchhoff Type", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "In this article, we design and analyze a hybrid high-order (HHO) finite\nelement approximation for the solution of a nonlocal nonlinear problem of\nKirchhoff type. The HHO method involves arbitrary-order polynomial\napproximations on structured and unstructured polytopal meshes. We establish\nthe existence of a unique discrete solution to the nonlocal nonlinear discrete\nproblem. We derive an optimal-order error estimate in the discrete energy norm.\nThe discrete system is solved using Newton's iterations on the sparse matrix\nsystem. We perform numerical tests to substantiate the theoretical results.", "AI": {"tldr": "A hybrid high-order finite element method is developed for nonlocal nonlinear Kirchhoff-type problems, featuring arbitrary-order polynomial approximations on polytopal meshes, with existence/uniqueness proofs, optimal error estimates, and Newton-based numerical implementation.", "motivation": "To develop an efficient high-order finite element method for nonlocal nonlinear Kirchhoff-type problems that works on general polytopal meshes (both structured and unstructured), addressing the need for robust numerical schemes for such complex problems.", "method": "Hybrid high-order (HHO) finite element method with arbitrary-order polynomial approximations on polytopal meshes. The discrete system is solved using Newton's iterations on sparse matrix systems.", "result": "Proved existence and uniqueness of discrete solution, derived optimal-order error estimate in discrete energy norm, and validated theoretical results through numerical tests.", "conclusion": "The HHO method provides an effective framework for solving nonlocal nonlinear Kirchhoff-type problems with arbitrary-order accuracy on general meshes, with rigorous mathematical foundation and practical numerical implementation."}}
{"id": "2510.15183", "pdf": "https://arxiv.org/pdf/2510.15183", "abs": "https://arxiv.org/abs/2510.15183", "authors": ["Vicente Vergara"], "title": "Dyadic microlocal partition for anisotropic metrics and uniform Weyl quantization", "categories": ["math.AP", "math-ph", "math.MP", "35S05, 35S30 (Primary), 35A18, 44A12, 81Q20, 42B20, 58J40\n  (Secondary)"], "comment": null, "summary": "We develop a dyadic microlocal partition adapted to position-dependent\nanisotropic metrics in phase space and prove uniform bounds for localized Weyl\nquantization and for Moyal truncation with explicit control of remainders. A\nsemiclassical, per-band renormalization recovers the exact scaling on each\nfrequency band, while a Cotlar-Stein almost-orthogonality scheme ensures global\nconvergence and operator-norm estimates with transparent dependence on finitely\nmany symbol seminorms. The approach is robust in non-homogeneous settings where\nanisotropy varies with position. As applications, we construct a microlocal\nparametrix via truncated Moyal expansion and develop a local-global analysis of\nthe Radon transform viewed as a model Fourier integral operator. The results\nprovide a constructive toolset for uniform localization, composition, and\nrecombination of pseudodifferential and Fourier integral operators.", "AI": {"tldr": "The paper develops a dyadic microlocal partition for anisotropic metrics in phase space, proving uniform bounds for Weyl quantization and Moyal truncation with explicit remainder control. It provides tools for analyzing pseudodifferential and Fourier integral operators in non-homogeneous settings.", "motivation": "To address the need for robust analysis tools in non-homogeneous settings where anisotropy varies with position, particularly for pseudodifferential and Fourier integral operators.", "method": "Uses a dyadic microlocal partition adapted to anisotropic metrics, semiclassical per-band renormalization, and Cotlar-Stein almost-orthogonality scheme with explicit remainder control.", "result": "Proves uniform bounds for localized Weyl quantization and Moyal truncation, recovers exact scaling on frequency bands, ensures global convergence, and provides operator-norm estimates with transparent dependence on symbol seminorms.", "conclusion": "The approach provides a constructive toolset for uniform localization, composition, and recombination of pseudodifferential and Fourier integral operators, with applications including microlocal parametrix construction and Radon transform analysis."}}
{"id": "2510.14984", "pdf": "https://arxiv.org/pdf/2510.14984", "abs": "https://arxiv.org/abs/2510.14984", "authors": ["Marin Lauber", "Mathias Peirlinck"], "title": "The mechanics of $\\textit{Less In More Out}$: modeling fabric-based soft robotic hearts", "categories": ["q-bio.TO", "physics.comp-ph"], "comment": null, "summary": "Fabric-based soft robots combine high load-carrying capacity, efficiency, and\nlow weight with the ability to bend, twist, contract, or extend with ease,\nmaking them promising candidates for biomedical applications such as soft total\nartificial hearts. While recent experiments have demonstrated their potential,\npredictive numerical models are urgently needed to study their complex\nmechanics, guide design optimization and improve their reliability. We develop\na computational model of the Less In More Out device, a fluidically actuated\nsoft total artificial heart constructed from heat-sealed layers of woven\nfabric. Our model reproduces the nonlinear deformation, strain fields, and\npressure-volume relationships measured in quasi-static experiments. Devices\nwith fewer pouches deliver higher stroke volumes but exhibit up to 50% higher\npeak von Mises stresses. Fatigue analysis using a strain-life approach\nidentifies heat-sealed seams and buckling regions as durability-limiting\nfeatures. Our framework enables detailed evaluation of stress concentrations,\nbuckling, and fatigue life, providing mechanistic insights that are difficult\nto obtain experimentally. It also offers a foundation for the optimization of\nartificial hearts and other fluid actuated fabric-based soft robotic systems.", "AI": {"tldr": "A computational model for fabric-based soft artificial hearts that predicts deformation, stress, and fatigue life, identifying key failure points and enabling design optimization.", "motivation": "Fabric-based soft robots show promise for biomedical applications like artificial hearts, but predictive models are needed to understand their complex mechanics and improve reliability.", "method": "Developed a computational model of the Less In More Out device - a fluidically actuated soft artificial heart made from heat-sealed woven fabric layers, validated against quasi-static experiments.", "result": "The model accurately reproduces nonlinear deformation, strain fields, and pressure-volume relationships. Devices with fewer pouches deliver higher stroke volumes but 50% higher peak stresses. Heat-sealed seams and buckling regions are identified as critical fatigue points.", "conclusion": "The framework enables detailed stress, buckling, and fatigue analysis that's difficult to obtain experimentally, providing a foundation for optimizing artificial hearts and other fluid-actuated fabric-based soft robotic systems."}}
{"id": "2510.15603", "pdf": "https://arxiv.org/pdf/2510.15603", "abs": "https://arxiv.org/abs/2510.15603", "authors": ["Elisabetta Carlini", "Luca Saluzzi"], "title": "High order Tensor-Train-Based Schemes for High-Dimensional Mean Field Games", "categories": ["math.NA", "cs.NA", "math.OC", "35Q91, 49J20, 49LXX, 82C31, 65C35"], "comment": null, "summary": "We introduce a fully discrete scheme to solve a class of high-dimensional\nMean Field Games systems. Our approach couples semi-Lagrangian (SL) time\ndiscretizations with Tensor-Train (TT) decompositions to tame the curse of\ndimensionality. By reformulating the classical Hamilton-Jacobi-Bellman and\nFokker-Planck equations as a sequence of advection-diffusion-reaction\nsubproblems within a smoothed policy iteration, we construct both first and\nsecond order in time SL schemes. The TT format and appropriate quadrature rules\nreduce storage and computational cost from exponential to polynomial in the\ndimension. Numerical experiments demonstrate that our TT-accelerated SL methods\nachieve their theoretical convergence rates, exhibit modest growth in memory\nusage and runtime with dimension, and significantly outperform grid-based SL in\naccuracy per CPU second.", "AI": {"tldr": "A fully discrete scheme combining semi-Lagrangian time discretizations with Tensor-Train decompositions to solve high-dimensional Mean Field Games systems, overcoming the curse of dimensionality.", "motivation": "To address the computational challenges of solving high-dimensional Mean Field Games systems, which suffer from exponential growth in storage and computational requirements with increasing dimensions.", "method": "Couples semi-Lagrangian time discretizations with Tensor-Train decompositions, reformulating Hamilton-Jacobi-Bellman and Fokker-Planck equations as advection-diffusion-reaction subproblems within smoothed policy iteration.", "result": "The method reduces storage and computational cost from exponential to polynomial in dimension, achieves theoretical convergence rates, shows modest growth in memory/runtime with dimension, and outperforms grid-based SL in accuracy per CPU second.", "conclusion": "TT-accelerated SL methods provide an effective approach for solving high-dimensional Mean Field Games by taming the curse of dimensionality through tensor decompositions and semi-Lagrangian schemes."}}
{"id": "2510.15213", "pdf": "https://arxiv.org/pdf/2510.15213", "abs": "https://arxiv.org/abs/2510.15213", "authors": ["Jian Wang", "Ruoyu P. T. Wang"], "title": "H\u00f6lder damping for fractional wave equations", "categories": ["math.AP", "math-ph", "math.MP", "35L05"], "comment": "10 pages, 1 figure", "summary": "For fractional wave equations with low H\\\"older regularity damping, we\nestablish quantitative energy decay rates for their solutions when the\ngeometric control condition holds. The energy decay rates depend explicitly on\nthe H\\\"older regularity of the damping. In particular, we show damping\nfunctions with lower H\\\"older regularities that below a certain threshold give\nslower energy decay.", "AI": {"tldr": "Quantitative energy decay rates for fractional wave equations with low H\u00f6lder regularity damping under geometric control condition.", "motivation": "To establish explicit energy decay rates for solutions of fractional wave equations when damping has low H\u00f6lder regularity, showing how decay rates depend on damping regularity.", "method": "Analysis of fractional wave equations with H\u00f6lder regular damping functions under geometric control condition.", "result": "Energy decay rates depend explicitly on H\u00f6lder regularity of damping; lower regularity below threshold gives slower decay.", "conclusion": "Quantitative relationship established between damping regularity and energy decay rates in fractional wave equations."}}
{"id": "2510.15604", "pdf": "https://arxiv.org/pdf/2510.15604", "abs": "https://arxiv.org/abs/2510.15604", "authors": ["Chen Zhang", "Patrick Henning", "Mahima Yadav", "Wenbin Chen"], "title": "Convergence analysis of Sobolev Gradient flows for the rotating Gross-Pitaevskii energy functional", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "This paper studies the numerical approximation of the ground state of\nrotating Bose--Einstein condensates, formulated as the minimization of the\nGross--Pitaevskii energy functional under a mass conservation constraint. To\nsolve this problem, we consider three Sobolev gradient flow schemes: the\n$H_0^1$ scheme, the $a_0$ scheme, and the $a_u$ scheme. Convergence of these\nschemes in the non-rotating case was established by Chen et al., and the\nrotating $a_u$ scheme was analyzed in Henning et al. In this work, we prove the\nglobal convergence of the $H_0^1$ and $a_0$ schemes in the rotating case, and\nestablish local linear convergence for all three schemes near the ground state.\nNumerical experiments confirm our theoretical findings.", "AI": {"tldr": "This paper analyzes numerical methods for finding ground states of rotating Bose-Einstein condensates using Sobolev gradient flow schemes, proving convergence for H\u2080\u00b9 and a\u2080 schemes in rotating cases and local linear convergence for all three schemes.", "motivation": "To develop and analyze numerical approximation methods for computing ground states of rotating Bose-Einstein condensates, which are modeled by minimizing the Gross-Pitaevskii energy functional under mass conservation constraints.", "method": "The study considers three Sobolev gradient flow schemes: H\u2080\u00b9 scheme, a\u2080 scheme, and a_u scheme. The paper proves global convergence of H\u2080\u00b9 and a\u2080 schemes in rotating cases, and establishes local linear convergence for all three schemes near the ground state.", "result": "Theoretical proofs show global convergence for H\u2080\u00b9 and a\u2080 schemes in rotating cases, and local linear convergence for all three schemes. Numerical experiments validate these theoretical findings.", "conclusion": "The paper successfully extends convergence analysis to rotating Bose-Einstein condensates, providing rigorous mathematical foundations for the numerical schemes and confirming their effectiveness through both theoretical proofs and numerical verification."}}
{"id": "2510.15246", "pdf": "https://arxiv.org/pdf/2510.15246", "abs": "https://arxiv.org/abs/2510.15246", "authors": ["Hsuan-Lin Liao", "Van Tien Nguyen"], "title": "Nonradial Quenching Profile for a MEMS Model", "categories": ["math.AP", "35B44, 35K55"], "comment": null, "summary": "We construct a quenching solution to the parabolic MEMS model \\[ u_t = \\Delta\nu - \\frac{1}{u^2} \\quad \\text{in } \\mathcal{B} \\times (0,T), \\quad u|_{\\partial\n\\mathcal{B}} = 1, \\] where $\\mathcal{B}$ is the unit disc in $\\mathbb{R}^2$,\nand $T > 0$ denotes the quenching time. The constructed solution quenches only\nat the origin and admits the final profile \\[ u(x,T) \\sim \\left(x_1^2 x_2^2 +\n\\theta(x_1^6 + x_2^6)\\right)^{\\frac{1}{3}} \\quad \\text{as } |x| \\to 0, \\] where\n$\\theta \\in (0, \\theta^*)$ for some $\\theta^* > 0$. To our knowledge, this is\nthe first example of a quenching solution with a genuinely non-radial profile.\nThe proof relies on the construction of a good approximate solution, using a\nperturbative expansion in self-similar variables. We then justify the true\nsolution that remains close to this approximation through a spectral analysis\ncombined with a robust energy method.", "AI": {"tldr": "Construction of a non-radial quenching solution to the parabolic MEMS equation in 2D with explicit final profile near quenching point.", "motivation": "To provide the first example of a quenching solution with genuinely non-radial profile for the MEMS equation, moving beyond radial symmetry assumptions.", "method": "Construct approximate solution using perturbative expansion in self-similar variables, then justify true solution through spectral analysis and robust energy method.", "result": "Successfully constructed quenching solution that quenches only at origin with explicit non-radial final profile involving x\u2081\u00b2x\u2082\u00b2 and \u03b8(x\u2081\u2076 + x\u2082\u2076) terms.", "conclusion": "This represents the first genuine non-radial quenching solution for MEMS equations, demonstrating existence beyond radial symmetry constraints."}}
{"id": "2510.15124", "pdf": "https://arxiv.org/pdf/2510.15124", "abs": "https://arxiv.org/abs/2510.15124", "authors": ["Alexandros Vasilopoulos", "Michail Akritidis", "Nikolaos G. Fytas", "Martin Weigel"], "title": "Cluster percolation and dynamical scaling in the Baxter--Wu model", "categories": ["cond-mat.stat-mech", "physics.comp-ph"], "comment": "10 pages, 7 figures, 1 table, REVTeX4.2", "summary": "We investigate the percolation behavior of Fortuin-Kasteleyn--type clusters\nin the spin-$1/2$ Baxter--Wu model with three-spin interactions on a triangular\nlattice. The considered clusters are constructed by randomly freezing one of\nthe three sublattices, resulting in effective pairwise interactions among the\nremaining spins. Using Monte Carlo simulations combined with a finite-size\nscaling analysis, we determine the percolation temperature of these stochastic\nclusters and show that it coincides with the exact thermal critical point of\nthe model. The critical exponents derived from cluster observables are\nconsistent with those of the underlying thermal phase transition. Finally, we\nanalyze the dynamical scaling of the multi-cluster and single-cluster\nalgorithms resulting from the cluster construction, highlighting their\nefficiency and scaling behavior with system size.", "AI": {"tldr": "The paper studies percolation behavior of Fortuin-Kasteleyn clusters in the spin-1/2 Baxter-Wu model on triangular lattice, showing percolation temperature matches exact thermal critical point with consistent critical exponents.", "motivation": "To investigate the relationship between percolation behavior of stochastic clusters and thermal phase transitions in the Baxter-Wu model with three-spin interactions.", "method": "Monte Carlo simulations combined with finite-size scaling analysis, constructing clusters by randomly freezing one sublattice to create effective pairwise interactions.", "result": "Percolation temperature of stochastic clusters coincides with exact thermal critical point, and critical exponents from cluster observables match those of thermal phase transition.", "conclusion": "The cluster construction provides efficient algorithms with good scaling behavior, demonstrating strong connection between percolation properties and thermal critical phenomena."}}
{"id": "2510.15606", "pdf": "https://arxiv.org/pdf/2510.15606", "abs": "https://arxiv.org/abs/2510.15606", "authors": ["Olof Runborg", "Elliot Backman"], "title": "Convergence of the Waveholtz Iteration on $\\mathbb{R}^d$", "categories": ["math.NA", "cs.NA", "65M12, 35J05"], "comment": "26 pages", "summary": "In this paper we analyse the Waveholtz method, a time-domain iterative method\nfor solving the Helmholtz iteration, in the constant-coefficient case in all of\n$\\mathbb{R}^d$. We show that the difference between a Waveholtz iterate and the\noutgoing Helmholtz solution satisfies a Helmholtz equation with a particular\nkind of forcing. For this forcing, we prove a frequency-explicit estimate in\nweighted Sobolev norms, that shows a decrease of the differences as\n$1/\\sqrt{n}$ in terms of the iteration number $n$. This guarantees the\nconvergence of the real parts of the Waveholtz iterates to the real part of the\noutgoing solution of the Helmholtz equation.", "AI": {"tldr": "The paper analyzes the Waveholtz method for solving Helmholtz equations, proving that iterates converge to the solution at a rate of 1/\u221an in weighted Sobolev norms.", "motivation": "To provide rigorous convergence analysis of the Waveholtz method, a time-domain iterative approach for solving Helmholtz equations, particularly in the constant-coefficient case.", "method": "Analyzed the Waveholtz method by showing the difference between iterates and the solution satisfies a Helmholtz equation with specific forcing, then proving frequency-explicit estimates in weighted Sobolev norms.", "result": "Proved that the difference between Waveholtz iterates and the solution decreases as 1/\u221an with iteration number n, guaranteeing convergence of real parts to the outgoing Helmholtz solution.", "conclusion": "The Waveholtz method converges to the solution of Helmholtz equations, with proven convergence rate of 1/\u221an in weighted Sobolev norms for constant-coefficient cases."}}
{"id": "2510.15249", "pdf": "https://arxiv.org/pdf/2510.15249", "abs": "https://arxiv.org/abs/2510.15249", "authors": ["Ugur G. Abdulla", "Denis Brazke"], "title": "The Wiener Criterion at $\\infty$ for Degenerate Elliptic Equations", "categories": ["math.AP", "35J25, 31C05, 31C15, 31C40"], "comment": "14 pages, 0 figures", "summary": "This paper establishes a Wiener criterion at $\\infty$ to characterise the\nunique solvability of the Dirichlet problem for degenerate elliptic equations\nwith power-like weights in arbitrary open sets. In the measure-theoretical\ncontext, the criterion determines whether the $\\A$-harmonic measure of $\\infty$\nis null or positive. From the topological point of view, it presents a test for\nthe thinness of the exterior set at $\\infty$ in the $\\A$-fine topology.", "AI": {"tldr": "Establishes a Wiener criterion at infinity to characterize unique solvability of Dirichlet problems for degenerate elliptic equations with power-like weights in arbitrary open sets.", "motivation": "To provide a criterion for determining when degenerate elliptic equations with power-like weights have unique solutions to the Dirichlet problem in arbitrary open sets.", "method": "Develops a Wiener criterion at infinity that works in both measure-theoretical and topological contexts - analyzing A-harmonic measure and fine topology thinness.", "result": "The criterion determines whether the A-harmonic measure of infinity is null or positive, and provides a test for thinness of the exterior set at infinity in the A-fine topology.", "conclusion": "The Wiener criterion successfully characterizes unique solvability of Dirichlet problems for degenerate elliptic equations with power weights in arbitrary domains."}}
{"id": "2510.15201", "pdf": "https://arxiv.org/pdf/2510.15201", "abs": "https://arxiv.org/abs/2510.15201", "authors": ["Mohammad Amin Nabian", "Sudeep Chavare", "Deepak Akhare", "Rishikesh Ranade", "Ram Cherukuri", "Srinivas Tadepalli"], "title": "Automotive Crash Dynamics Modeling Accelerated with Machine Learning", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "physics.app-ph", "physics.comp-ph"], "comment": null, "summary": "Crashworthiness assessment is a critical aspect of automotive design,\ntraditionally relying on high-fidelity finite element (FE) simulations that are\ncomputationally expensive and time-consuming. This work presents an exploratory\ncomparative study on developing machine learning-based surrogate models for\nefficient prediction of structural deformation in crash scenarios using the\nNVIDIA PhysicsNeMo framework. Given the limited prior work applying machine\nlearning to structural crash dynamics, the primary contribution lies in\ndemonstrating the feasibility and engineering utility of the various modeling\napproaches explored in this work. We investigate two state-of-the-art neural\nnetwork architectures for modeling crash dynamics: MeshGraphNet, and\nTransolver. Additionally, we examine three strategies for modeling transient\ndynamics: time-conditional, the standard Autoregressive approach, and a\nstability-enhanced Autoregressive scheme incorporating rollout-based training.\nThe models are evaluated on a comprehensive Body-in-White (BIW) crash dataset\ncomprising 150 detailed FE simulations using LS-DYNA. The dataset represents a\nstructurally rich vehicle assembly with over 200 components, including 38 key\ncomponents featuring variable thickness distributions to capture realistic\nmanufacturing variability. Each model utilizes the undeformed mesh geometry and\ncomponent characteristics as inputs to predict the spatiotemporal evolution of\nthe deformed mesh during the crash sequence. Evaluation results show that the\nmodels capture the overall deformation trends with reasonable fidelity,\ndemonstrating the feasibility of applying machine learning to structural crash\ndynamics. Although not yet matching full FE accuracy, the models achieve\norders-of-magnitude reductions in computational cost, enabling rapid design\nexploration and early-stage optimization in crashworthiness evaluation.", "AI": {"tldr": "Machine learning surrogate models for crashworthiness assessment using NVIDIA PhysicsNeMo framework, showing feasibility and computational efficiency gains over traditional finite element simulations.", "motivation": "Traditional finite element simulations for crashworthiness assessment are computationally expensive and time-consuming, creating need for faster alternatives.", "method": "Comparative study of MeshGraphNet and Transolver neural network architectures with three transient dynamics modeling strategies (time-conditional, autoregressive, stability-enhanced autoregressive) on Body-in-White crash dataset with 150 LS-DYNA simulations.", "result": "Models captured overall deformation trends with reasonable fidelity, achieving orders-of-magnitude reduction in computational cost while not yet matching full FE accuracy.", "conclusion": "Machine learning approaches show promise for rapid design exploration and early-stage optimization in crashworthiness evaluation, demonstrating feasibility for structural crash dynamics applications."}}
{"id": "2510.15819", "pdf": "https://arxiv.org/pdf/2510.15819", "abs": "https://arxiv.org/abs/2510.15819", "authors": ["Rihui Lan", "Jorge Reyes"], "title": "Longer time accuracy for the Ladyzhenskya model with the EMAC formulation", "categories": ["math.NA", "cs.NA", "physics.flu-dyn"], "comment": null, "summary": "In this paper, we incorporate the EMAC formulation into the Ladyzhenskaya\nmodel (LM), a large eddy simulation (LES) of incompressible flows. The EMAC\nformulation, which conserves energy, linear momentum, and angular momentum even\nwith weak enforcement of incompressibility, has been shown to provide tangible\nbenefits over the popular skew-symmetric for direct numerical simulation and\nregularized models of the Navier Stokes equations (NSE). The combination of\nEMAC with the LM addresses the known over-dissipation issues associated with\nthe classical Smagorinsky model (SM). We develop a finite element\ndiscretization for the EMAC-LM system and analyze its stability and derive\nnumerical error estimates, showing improved long-time behavior compared to the\nstandard LM approach, particularly due to EMAC's favorable Gronwall constant\nindependent of the Reynolds number. Benchmark simulations demonstrate that the\nEMAC-LM model yields more accurate flow structures, especially at high Reynolds\nnumbers.", "AI": {"tldr": "Incorporating EMAC formulation into Ladyzhenskaya model (LM) for LES to address over-dissipation issues of Smagorinsky model, with improved stability and accuracy at high Reynolds numbers.", "motivation": "To overcome the known over-dissipation problems of classical Smagorinsky model in large eddy simulation by combining EMAC formulation's conservation properties with Ladyzhenskaya model.", "method": "Developed finite element discretization for EMAC-LM system, analyzed stability, derived numerical error estimates, and conducted benchmark simulations.", "result": "EMAC-LM model shows improved long-time behavior, better Gronwall constant independent of Reynolds number, and more accurate flow structures especially at high Reynolds numbers.", "conclusion": "Combining EMAC formulation with Ladyzhenskaya model provides superior performance over standard LM approach, particularly for high Reynolds number flows."}}
{"id": "2510.15276", "pdf": "https://arxiv.org/pdf/2510.15276", "abs": "https://arxiv.org/abs/2510.15276", "authors": ["Gnanasekaran Shanmugasundaram", "Jitraj Saha"], "title": "Global existence and stability in a class of chemotaxis systems with lethal interactions, nonlinear diffusion and production", "categories": ["math.AP", "35A01, 35A09, 35B40, 35Q92, 37N25. 92C17"], "comment": null, "summary": "This paper investigates a class of chemotaxis systems modeling lethal\ninteractions in a smooth, bounded domain $\\Omega \\subset \\mathbb{R}^n$ with\nhomogeneous Neumann boundary conditions. We examine two distinct cases: (i) a\nfully parabolic system where both equations exhibit parabolic dynamics, and\n(ii) a parabolic-elliptic system featuring a parabolic first equation coupled\nwith an elliptic second equation. Under appropriate parameter constraints, we\nestablish the existence of unique globally bounded classical solutions for\narbitrary spatial dimensions $n \\geq 1$. Additionally, we employ carefully\nconstructed Lyapunov functionals to analyze the long-term behavior of\nsolutions, obtaining rigorous asymptotic stability results.", "AI": {"tldr": "This paper analyzes chemotaxis systems with lethal interactions, examining both fully parabolic and parabolic-elliptic cases, and proves global bounded classical solutions and asymptotic stability using Lyapunov functionals.", "motivation": "To understand the dynamics of chemotaxis systems modeling lethal interactions in biological contexts, particularly focusing on solution behavior and stability across different mathematical formulations.", "method": "Mathematical analysis of two chemotaxis system types: fully parabolic and parabolic-elliptic systems with homogeneous Neumann boundary conditions, using parameter constraints and Lyapunov functionals.", "result": "Established existence of unique globally bounded classical solutions for arbitrary spatial dimensions n\u22651 under appropriate parameter constraints, and obtained rigorous asymptotic stability results.", "conclusion": "The study successfully demonstrates well-posedness and stability properties for chemotaxis systems with lethal interactions, providing mathematical foundations for understanding such biological phenomena."}}
{"id": "2510.15633", "pdf": "https://arxiv.org/pdf/2510.15633", "abs": "https://arxiv.org/abs/2510.15633", "authors": ["Louise A. M. Rosset", "Volker L. Deringer"], "title": "Atomic cluster expansion potential for the Si-H system", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "comment": null, "summary": "The silicon-hydrogen system is of key interest for solar-cell devices,\nincluding both crystalline and amorphous modifications. Elemental amorphous Si\nis now well understood, but the atomic-scale effects of hydrogenating the\nsilicon matrix remain to be fully explored. Here, we present a machine-learned\ninteratomic potential model based on the atomic cluster expansion (ACE)\nframework that can describe a wide range of Si-H phases, from crystalline and\namorphous bulk structures to surfaces and molecules. We perform numerical and\nphysical validation across a range of hydrogen concentrations and compare our\nresults to experimental findings. Our work constitutes an advancement toward\nthe exploration of large structural models of a-Si:H at realistic device\nscales.", "AI": {"tldr": "A machine-learned interatomic potential based on atomic cluster expansion (ACE) is developed to model Si-H systems across various phases, enabling large-scale simulations of hydrogenated amorphous silicon for solar cell applications.", "motivation": "Understanding atomic-scale effects of hydrogen in silicon matrices is crucial for solar-cell devices, but current knowledge gaps exist in hydrogenated silicon systems despite good understanding of elemental amorphous Si.", "method": "Developed a machine-learned interatomic potential using the atomic cluster expansion (ACE) framework to model Si-H phases including crystalline/amorphous bulk, surfaces, and molecules.", "result": "The potential was numerically and physically validated across hydrogen concentrations and showed good agreement with experimental findings.", "conclusion": "This work advances the capability to explore large structural models of hydrogenated amorphous silicon (a-Si:H) at realistic device scales for solar cell applications."}}
{"id": "2510.15854", "pdf": "https://arxiv.org/pdf/2510.15854", "abs": "https://arxiv.org/abs/2510.15854", "authors": ["Xiaofeng Cai", "Linghui Kong", "Dmitri Kuzmin", "Li Shan"], "title": "Asymptotic-preserving conservative semi-Lagrangian discontinuous Galerkin schemes for the Vlasov-Poisson system in the quasi-neutral limit", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "We discretize the Vlasov-Poisson system using conservative semi-Lagrangian\n(CSL) discontinuous Galerkin (DG) schemes that are asymptotic preserving (AP)\nin the quasi-neutral limit. The proposed method (CSLDG) relies on two key\ningredients: the CSLDG discretization and a reformulated Poisson equation\n(RPE). The use of the CSL formulation ensures local mass conservation and\ncircumvents the Courant-Friedrichs-Lewy condition, while the DG method provides\nhigh-order accuracy for capturing fine-scale phase space structures of the\ndistribution function. The RPE is derived by the Poisson equation coupled with\nmoments of the Vlasov equation. The synergy between the CSLDG and RPE\ncomponents makes it possible to obtain reliable numerical solutions, even when\nthe spatial and temporal resolution might not fully resolve the Debye length.\nWe rigorously prove that the proposed method is asymptotically stable,\nconsistent and satisfies AP properties. Moreover, its efficiency is maintained\nacross non-quasi-neutral and quasi-neutral regimes. These properties of our\napproach are essential for accurate and robust numerical simulation of complex\nelectrostatic plasmas. Several numerical experiments verify the accuracy,\nstability and efficiency of the proposed CSLDG schemes.", "AI": {"tldr": "Conservative semi-Lagrangian discontinuous Galerkin (CSLDG) schemes for Vlasov-Poisson system with asymptotic preserving properties in quasi-neutral limit.", "motivation": "To develop accurate and robust numerical methods for electrostatic plasmas that work reliably across different regimes, especially when spatial/temporal resolution doesn't fully resolve Debye length.", "method": "Combines CSLDG discretization with reformulated Poisson equation (RPE). CSLDG ensures local mass conservation and bypasses CFL condition, while DG provides high-order accuracy. RPE is derived from Poisson equation coupled with Vlasov equation moments.", "result": "Method is proven asymptotically stable, consistent, and satisfies AP properties. Maintains efficiency across non-quasi-neutral and quasi-neutral regimes. Numerical experiments verify accuracy, stability, and efficiency.", "conclusion": "The CSLDG approach with RPE enables reliable simulation of complex electrostatic plasmas, providing essential properties for accurate and robust numerical modeling across different plasma regimes."}}
{"id": "2510.15359", "pdf": "https://arxiv.org/pdf/2510.15359", "abs": "https://arxiv.org/abs/2510.15359", "authors": ["Arka Mallick", "Swarnendu Sil"], "title": "Continuity estimates for variable growth variational problems in the Heisenberg group", "categories": ["math.AP", "Primary 35B65, 35H20, 35J62, Secondary 35H10, 35J70, 35J75 (2020)"], "comment": null, "summary": "We study regularity results for local minimizers of variable growth\nvariational problem in Heisenberg groups under suitable integrability\nassumption on the horizontal gradient of the exponent function. More precisely,\nour main focus is on the continuity properties of the horizontal gradient\n$\\mathfrak{X} u$, where $u \\in HW_{\\text{loc}}^{1,1}$ is a local minimizer of\nthe functional\n  \\begin{align*}\n  I [u]:= \\int_{\\Omega} \\frac{1}{p(x)}\\left\\lvert \\mathfrak{X} u\n\\right\\rvert^{p(x)}\\ \\mathrm{d}x\n  \\end{align*} in a domain of $\\Omega \\subset \\mathbb{H}_{n},$ where\n$\\mathbb{H}_{n}$ is the Heisenberg group with homogeneous dimension $Q=2n+2,$\nwhere $p \\in HW^{1,1}\\left( \\Omega\\right)$ and we assume suitable integrability\nhypothesis on $\\mathfrak{X} p.$ We prove (a) if $\\mathfrak{X} p \\in L^{q}\\left(\n\\Omega; \\mathbb{R}^{2n}\\right)$ with $q>Q,$ then $\\mathfrak{X} u$ is H\\\"{o}lder\ncontinuous and (b) if $\\mathfrak{X} p \\in L^{(Q,1)}\\log L \\left( \\Omega;\n\\mathbb{R}^{2n}\\right),$ then $\\mathfrak{X} u$ is continuous.\n  In fact, in the non-borderline case $(a)$, we prove H\\\"{o}lder continuity of\nthe horizontal gradient for the minima of more general variational problems,\nassuming $p$ to be H\\\"{o}lder continuous, i.e. without any assumption on the\nweak derivative of $p.$ To the best of our knowledge, the present work is the\nfirst regularity result for minimizers of variable growth variational problems\nin the setting of Heisenberg groups.", "AI": {"tldr": "This paper establishes regularity results for local minimizers of variable growth variational problems in Heisenberg groups, proving continuity properties of the horizontal gradient under different integrability assumptions on the exponent function.", "motivation": "The motivation is to extend regularity theory to variable growth variational problems in the setting of Heisenberg groups, which has not been previously studied. The work aims to understand continuity properties of minimizers' horizontal gradients under different integrability conditions.", "method": "The authors study local minimizers of a variable growth functional in Heisenberg groups, analyzing the horizontal gradient under two different integrability assumptions on the exponent function's horizontal gradient. They prove H\u00f6lder continuity and continuity results using variational methods.", "result": "Two main results are proven: (a) If the horizontal gradient of p belongs to L^q with q>Q, then the horizontal gradient of u is H\u00f6lder continuous; (b) If the horizontal gradient of p belongs to a Lorentz-Zygmund space, then the horizontal gradient of u is continuous. The paper also establishes H\u00f6lder continuity without derivative assumptions on p in the non-borderline case.", "conclusion": "This work provides the first regularity results for minimizers of variable growth variational problems in Heisenberg groups, establishing continuity properties of horizontal gradients under different integrability conditions on the exponent function."}}
{"id": "2510.15664", "pdf": "https://arxiv.org/pdf/2510.15664", "abs": "https://arxiv.org/abs/2510.15664", "authors": ["Lucas Amoudruz", "Sergey Litvinov", "Costas Papadimitriou", "Petros Koumoutsakos"], "title": "Bayesian Inference for PDE-based Inverse Problems using the Optimization of a Discrete Loss", "categories": ["stat.ME", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "Inverse problems are crucial for many applications in science, engineering\nand medicine that involve data assimilation, design, and imaging. Their\nsolution infers the parameters or latent states of a complex system from noisy\ndata and partially observable processes. When measurements are an incomplete or\nindirect view of the system, additional knowledge is required to accurately\nsolve the inverse problem. Adopting a physical model of the system in the form\nof partial differential equations (PDEs) is a potent method to close this gap.\nIn particular, the method of optimizing a discrete loss (ODIL) has shown great\npotential in terms of robustness and computational cost. In this work, we\nintroduce B-ODIL, a Bayesian extension of ODIL, that integrates the PDE loss of\nODIL as prior knowledge and combines it with a likelihood describing the data.\nB-ODIL employs a Bayesian formulation of PDE-based inverse problems to infer\nsolutions with quantified uncertainties. We demonstrate the capabilities of\nB-ODIL in a series of synthetic benchmarks involving PDEs in one, two, and\nthree dimensions. We showcase the application of B-ODIL in estimating tumor\nconcentration and its uncertainty in a patient's brain from MRI scans using a\nthree-dimensional tumor growth model.", "AI": {"tldr": "B-ODIL is a Bayesian extension of ODIL that integrates PDE loss as prior knowledge with data likelihood to solve inverse problems with quantified uncertainties.", "motivation": "Inverse problems require additional knowledge when measurements are incomplete or indirect. PDE-based models can close this gap, and extending ODIL to Bayesian framework enables uncertainty quantification.", "method": "B-ODIL uses Bayesian formulation combining PDE loss as prior knowledge with data likelihood. It extends the ODIL method to handle inverse problems with uncertainty quantification.", "result": "Demonstrated on synthetic benchmarks with PDEs in 1D, 2D, and 3D. Successfully applied to estimate tumor concentration and uncertainty in patient's brain from MRI scans using 3D tumor growth model.", "conclusion": "B-ODIL provides an effective Bayesian framework for solving PDE-based inverse problems with uncertainty quantification, showing practical application in medical imaging for tumor analysis."}}
{"id": "2510.15378", "pdf": "https://arxiv.org/pdf/2510.15378", "abs": "https://arxiv.org/abs/2510.15378", "authors": ["Zhentao He", "Chao Ji"], "title": "Nonrelativistic limit of normalized solutions of nonlinear Dirac equations on noncompact metric graphs with localized nonlinearities", "categories": ["math.AP", "35R02, 81Q35, 58E05, 35Q40"], "comment": "17 pages", "summary": "In this paper, we study the nonrelativistic limit of normalized solutions for\nthe following nonlinear Dirac equation (NLDE) on noncompact metric graph $\\G$\nwith finitely many edges and a non-empty compact core $\\K$ \\begin{equation*}\n  \\D u - \\omega u= \\chi_\\K\\abs{u}^{p-2}u, \\end{equation*} under the constraint\n$\\int_\\G\\abs{u}^2\\,dx = 1$, where $\\D$ is the Dirac operator on $\\G$, $u: \\G\n\\to \\mathbb{C}^2$, the frequency $\\omega \\in \\mathbb{R}$ is part of the\nunknowns which arises as a Lagrange multiplier, $\\chi_\\K$ is the characteristic\nfunction of the compact core $\\K$, and $2<p<6$. To the best of our knowledge,\nthis is the first study to investigate the nonrelativistic limit of normalized\nsolutions to (NLDE) on metric graphs.", "AI": {"tldr": "Study of nonrelativistic limit of normalized solutions for nonlinear Dirac equation on metric graphs with compact core, under mass constraint.", "motivation": "First investigation of nonrelativistic limit for normalized solutions to nonlinear Dirac equation on metric graphs, addressing a gap in existing literature.", "method": "Analysis of nonlinear Dirac equation on noncompact metric graphs with finitely many edges and compact core, using constraint \u222b|u|\u00b2dx=1 and studying limit behavior.", "result": "Provides first results on nonrelativistic limit behavior of normalized solutions for this class of equations on metric graphs.", "conclusion": "Pioneering work establishing foundation for studying nonrelativistic limits in nonlinear Dirac equations on metric graph structures."}}
{"id": "2510.15715", "pdf": "https://arxiv.org/pdf/2510.15715", "abs": "https://arxiv.org/abs/2510.15715", "authors": ["Ivan Novikau", "Ilon Joseph"], "title": "Globalizing the Carleman linear embedding method for nonlinear dynamics", "categories": ["quant-ph", "nlin.CD", "physics.comp-ph"], "comment": null, "summary": "The Carleman embedding method is a widely used technique for linearizing a\nsystem of nonlinear differential equations, but fails to converge in regions\nwhere there are multiple fixed points. We propose and test three different\nversions of a global piecewise Carleman embedding technique, based on\npartitioning space into multiple regions where the center and size of the\nembedding region are chosen to control convergence. The first method switches\nbetween local linearization regions of fixed size once the trajectory reaches\nthe boundary of the current linearization chart. During the transition, the\nembedding is reconstructed within the newly created chart, centered at the\ntransition point. The second method also adapts the chart size dynamically,\nenhancing accuracy in regions where multiple fixed points are located. The\nthird method partitions the state space using a static grid with precomputed\nlinearization charts of fixed size, making it more suitable for applications\nthat require high speed. All techniques are numerically tested on multiple\nintegrable and chaotic nonlinear dynamical systems demonstrating their\napplicability for problems that are completely intractable for the standard\nCarleman embedding method. Simulations of chaotic dynamical systems such as\nvarious types of strange attractors demonstrate the power of the adaptive\nmethods, if a sufficiently low tolerance is imposed. Still, the non-adaptive\nversion of the method, with fixed centers and sizes of the linearization\ncharts, can be faster in simulating dynamical systems while providing similar\naccuracy and may be more appropriate as the basis of algorithms for future\nquantum computers.", "AI": {"tldr": "Three global piecewise Carleman embedding methods are proposed to overcome convergence limitations of standard Carleman linearization in nonlinear systems with multiple fixed points.", "motivation": "The standard Carleman embedding method fails to converge in regions with multiple fixed points, limiting its applicability for complex nonlinear dynamical systems.", "method": "Three piecewise approaches: 1) Boundary-triggered switching between local linearization regions 2) Dynamic chart size adaptation for accuracy 3) Static grid partitioning with precomputed charts for speed", "result": "All methods successfully handle nonlinear systems intractable for standard Carleman embedding, with adaptive methods excelling on chaotic systems and non-adaptive version being faster.", "conclusion": "Piecewise Carleman embedding enables simulation of complex nonlinear systems, with adaptive methods for accuracy and non-adaptive version potentially suitable for quantum computing applications."}}
{"id": "2510.15257", "pdf": "https://arxiv.org/pdf/2510.15257", "abs": "https://arxiv.org/abs/2510.15257", "authors": ["Amir Ali Farzin", "Yuen-Man Pun", "Philipp Braun", "Tyler Summers", "Iman Shames"], "title": "Minimisation of Submodular Functions Using Gaussian Zeroth-Order Random Oracles", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "We consider the minimisation problem of submodular functions and investigate\nthe application of a zeroth-order method to this problem. The method is based\non exploiting a Gaussian smoothing random oracle to estimate the smoothed\nfunction gradient. We prove the convergence of the algorithm to a global\n$\\epsilon$-approximate solution in the offline case and show that the algorithm\nis Hannan-consistent in the online case with respect to static regret.\nMoreover, we show that the algorithm achieves $O(\\sqrt{NP_N^\\ast})$ dynamic\nregret, where $N$ is the number of iterations and $P_N^\\ast$ is the path\nlength. The complexity analysis and hyperparameter selection are presented for\nall the cases. The theoretical results are illustrated via numerical examples.", "AI": {"tldr": "A zeroth-order method using Gaussian smoothing is applied to submodular function minimization, achieving global convergence in offline settings and Hannan-consistency with static/dynamic regret bounds in online settings.", "motivation": "To develop efficient algorithms for submodular function minimization that work in both offline and online settings, leveraging zeroth-order optimization techniques.", "method": "Gaussian smoothing random oracle is used to estimate gradients for a zeroth-order optimization approach applied to submodular function minimization.", "result": "The algorithm converges to global \u03b5-approximate solutions offline, achieves Hannan-consistency with static regret online, and obtains O(\u221a(NP_N*)) dynamic regret where P_N* is path length.", "conclusion": "The proposed zeroth-order method is effective for submodular minimization across both offline and online scenarios, with theoretical guarantees and practical performance demonstrated through numerical experiments."}}
{"id": "2510.15379", "pdf": "https://arxiv.org/pdf/2510.15379", "abs": "https://arxiv.org/abs/2510.15379", "authors": ["Jan Haskovec", "Peter Markowich", "Stefano Zampini"], "title": "Gradient Flows for the $p$-Laplacian Arising from Biological Network Models: A Novel Dynamical Relaxation Approach", "categories": ["math.AP", "cs.NA", "math.NA", "math.OC", "35G61, 35K57, 35D30, 65M60, 65F08, 65Y05, 65H10, 92C42"], "comment": "25 pages, 7 figures", "summary": "We investigate a scalar partial differential equation model for the formation\nof biological transportation networks. Starting from a discrete graph-based\nformulation on equilateral triangulations, we rigorously derive the\ncorresponding continuum energy functional as the $\\Gamma$-limit under network\nrefinement and establish the existence of global minimizers. The model\npossesses a gradient-flow structure whose steady states coincide with solutions\nof the $p$-Laplacian equation. Building on this connection, we implement finite\nelement discretizations and propose a novel dynamical relaxation scheme that\nachieves optimal convergence rates in manufactured tests and exhibits\nmesh-independent performance, with the number of time steps, nonlinear\niterations, and linear solves remaining stable under uniform mesh refinement.\nNumerical experiments confirm both the ability of the scalar model to reproduce\nbiologically relevant network patterns and its effectiveness as a\ncomputationally efficient relaxation strategy for solving $p$-Laplacian\nequations for large exponents $p$.", "AI": {"tldr": "The paper develops a scalar PDE model for biological transportation networks, deriving it as a continuum limit from discrete graphs and establishing existence of minimizers. It connects to p-Laplacian equations and proposes an efficient numerical scheme with optimal convergence.", "motivation": "To understand biological transportation network formation through mathematical modeling and develop efficient computational methods for solving related p-Laplacian equations.", "method": "Derived continuum energy functional as \u0393-limit from discrete graph-based formulation on triangulations, established existence of minimizers, implemented finite element discretizations with novel dynamical relaxation scheme.", "result": "Achieved optimal convergence rates in manufactured tests, mesh-independent performance with stable computational costs under refinement, and demonstrated ability to reproduce biologically relevant network patterns.", "conclusion": "The scalar model effectively captures biological network formation and provides an efficient relaxation strategy for solving p-Laplacian equations with large exponents."}}
{"id": "2510.15733", "pdf": "https://arxiv.org/pdf/2510.15733", "abs": "https://arxiv.org/abs/2510.15733", "authors": ["Lorenzo Botti", "Francesco Carlo Massa"], "title": "A HHO formulation for variable density incompressible flows where the density is purely advected", "categories": ["physics.flu-dyn", "physics.comp-ph", "76T06, 35Q30, 65N30,"], "comment": "17 pages, 3 tables, 2 figures", "summary": "We propose a Hybrid High-Order (HHO) formulation of the incompressible\nNavier--Stokes equations with variable density that provides exact conservation\nof volume and, accordingly, pure advection of the density variable. The spatial\ndiscretization relies on hybrid velocity-density-pressure spaces and the\ntemporal discretization is based on Explicit Singly Diagonal Implicit\nRunge-Kutta (ESDIRK) methods. The formulation possesses some attractive\nfeatures that can be fruitfully exploited for the simulation of mixtures of\nimmiscible incompressible fluids, namely: pressure-robustness, conservation of\nvolume enforced cell-by-cell up to machine precision, robustness in the\nconvection dominated regime thanks to upwind stabilizations, weak imposition of\nboundary conditions, implicit high-order accurate time stepping encompassing\nunsteady boundary conditions, reduced memory footprint thanks to static\ncondensation, possibility to exploit inherited $p$-multilevel solution\nstrategies to improve performance of iterative solvers. Numerical validation is\nperformed showcasing convergence rates, pressure-robustness and pure advection\nof the density, thus we tackle the Rayleigh-Taylor instability at Reynolds\n$1~000$ and $5~000$.", "AI": {"tldr": "A Hybrid High-Order (HHO) method for incompressible Navier-Stokes equations with variable density that ensures exact volume conservation and pure density advection, using hybrid spaces and ESDIRK temporal discretization.", "motivation": "To develop a robust numerical method for simulating mixtures of immiscible incompressible fluids with exact volume conservation, pressure-robustness, and efficient computational performance.", "method": "Spatial discretization using hybrid velocity-density-pressure spaces and temporal discretization via Explicit Singly Diagonal Implicit Runge-Kutta (ESDIRK) methods with upwind stabilization and static condensation.", "result": "The method achieves exact volume conservation cell-by-cell, pressure-robustness, pure density advection, and successfully simulates Rayleigh-Taylor instability at Reynolds 1,000 and 5,000.", "conclusion": "The proposed HHO formulation provides an effective framework for simulating immiscible fluid mixtures with attractive computational features including conservation properties, robustness, and high-order accuracy."}}
{"id": "2510.15287", "pdf": "https://arxiv.org/pdf/2510.15287", "abs": "https://arxiv.org/abs/2510.15287", "authors": ["Zhenning Cai", "Yixiao Sun", "Geshuo Wang"], "title": "Second-order discretization of Dyson series: iterative method, numerical analysis and applications in open quantum systems", "categories": ["quant-ph", "cs.NA", "math.NA"], "comment": null, "summary": "We propose a general strategy to discretize the Dyson series without applying\ndirect numerical quadrature to high-dimensional integrals, and extend this\nframework to open quantum systems. The resulting discretization can also be\ninterpreted as a Strang splitting combined with a Taylor expansion. Based on\nthis formulation, we develop a numerically exact iterative method for\nsimulation system-bath dynamics. We propose two numerical schemes, which are\nfirst-order and second-order in time step $\\Delta t$ respectively. We perform a\nrigorous numerical analysis to establish the convergence orders of both\nschemes, proving that the global error decreases as $\\mathcal{O}(\\Delta t)$ and\n$\\mathcal{O}(\\Delta t^2)$ for the first- and second-order methods,\nrespectively. In the second-order scheme, we can safely omitted most terms\narising from the Strang splitting and Taylor expansion while maintaining\nsecond-order accuracy, leading to a substantial reduction in computational\ncomplexity. For the second-order method, we achieves a time complexity of\n$\\mathcal{O}(M^3 2^{2K_{\\max}} K_{\\max}^2)$ and a space complexity of\n$\\mathcal{O}(M^2 2^{2K_{\\max}} K_{\\max})$ where $M$ denotes the number of\nsystem levels and $K_{\\max}$ the number of time steps within the memory length.\nCompared with existing methods, our approach requires substantially less memory\nand computational effort for multilevel systems ($M\\geqslant 3$). Numerical\nexperiments are carried out to illustrate the validity and efficiency of our\nmethod.", "AI": {"tldr": "A novel discretization strategy for the Dyson series using Strang splitting and Taylor expansion, enabling efficient simulation of open quantum systems with reduced computational complexity.", "motivation": "To develop a numerically exact method for simulating system-bath dynamics in open quantum systems that avoids high-dimensional integration and reduces computational costs compared to existing approaches.", "method": "Proposed a general discretization strategy for the Dyson series combining Strang splitting with Taylor expansion. Developed first-order and second-order iterative schemes with rigorous convergence analysis. The second-order scheme omits unnecessary terms to reduce complexity.", "result": "Established convergence orders: first-order method has O(\u0394t) global error, second-order method has O(\u0394t\u00b2). Second-order scheme achieves time complexity O(M\u00b32\u00b2\u1d37\u1d50\u1d43\u02e3K\u2098\u2090\u2093\u00b2) and space complexity O(M\u00b22\u00b2\u1d37\u1d50\u1d43\u02e3K\u2098\u2090\u2093), substantially reducing memory and computational effort for multilevel systems (M\u22653).", "conclusion": "The proposed method provides an efficient framework for simulating open quantum systems with rigorous convergence guarantees and significantly reduced computational complexity compared to existing approaches, validated through numerical experiments."}}
{"id": "2510.15402", "pdf": "https://arxiv.org/pdf/2510.15402", "abs": "https://arxiv.org/abs/2510.15402", "authors": ["Ryoto Ichiya"], "title": "Asymptotic Blow-up Behavior for the Semilinear Heat Equation with Super-exponential Nonlinearities", "categories": ["math.AP", "35B44, 35B40, 35K05"], "comment": null, "summary": "We consider the semilinear heat equation $u_t - \\Delta u = f(u)$ in $\\Omega =\nB_R(0) \\subset \\mathbb{R}^n$ with super-exponential nonlinearities $f(u) =\ne^{u^p}u^q$ ($p>1$, $q \\in \\{0\\}\\cup [1,\\infty)$), nonnegative bounded radially\nsymmetric initial data and 0-Dirichlet boundary condition. In this paper, we\nshow the asymptotic blow-up behavior for nonnegative, radial type I blow-up\nsolution. More precisely, we prove that if $n \\leq 2$, then such blow-up\nsolution satisfies \\begin{equation*} \\lim_{t \\rightarrow T}\n\\frac{T-t}{F(u(y\\sqrt{T-t},t))} = 1, \\quad \\text{where } F(u) =\n\\int_{u}^{\\infty} \\frac{ds}{f(s)}. \\end{equation*} We note that this result\ncorresponds to the one which is proved by Liu in 1989 for the case of $f(u) =\ne^u$, which has the scale invariance property unlike our super-exponential\ncase. To prove the main result, we see the equation as a perturbation of the\nequation with $f(u) = e^u$ through a transformation introduced by Fujishima and\nIoku in 2018 and estimate the additional term which appears after the\ntransformation.", "AI": {"tldr": "Study of blow-up behavior for semilinear heat equations with super-exponential nonlinearities in radial domains, proving asymptotic blow-up rates for type I solutions when n\u22642.", "motivation": "To extend previous results on blow-up behavior from exponential nonlinearities (f(u)=e^u) to more general super-exponential cases (f(u)=e^{u^p}u^q), which lack scale invariance properties.", "method": "Treat the equation as a perturbation of the f(u)=e^u case using a transformation by Fujishima and Ioku (2018), and estimate the additional terms that appear after this transformation.", "result": "Proved that for n\u22642, nonnegative radial type I blow-up solutions satisfy the asymptotic relation lim_{t\u2192T} (T-t)/F(u(y\u221a(T-t),t)) = 1, where F(u)=\u222b_u^\u221e ds/f(s).", "conclusion": "Successfully extended Liu's 1989 result for f(u)=e^u to super-exponential nonlinearities, establishing similar blow-up behavior despite the lack of scale invariance in the more general case."}}
{"id": "2510.15827", "pdf": "https://arxiv.org/pdf/2510.15827", "abs": "https://arxiv.org/abs/2510.15827", "authors": ["Mark C. Baumann", "Justin C. Feng", "Nicky Ishaak"], "title": "Aiming for Proxima Centauri b: Gravitational effects on relativistic spacecraft trajectories", "categories": ["gr-qc", "physics.app-ph", "physics.comp-ph", "physics.space-ph", "83-08, 83-10"], "comment": "11 pages, 3 figures, 4 tables", "summary": "How important are gravitational and relativistic effects for interstellar\ntravel? We consider this question in the context of proposed laser-propelled\nspacecraft missions to neighboring stellar destinations. Our analysis applies\nto any spacecraft traveling at relativistic speeds. As a concrete example, we\nfocus on a mission to Proxima Centauri b -- a terrestrial-sized planet in the\nhabitable zone around our nearest stellar neighbor, Proxima Centauri. We employ\na Julia reimplementation of the PoMiN code, an N-body code modeling\nrelativistic gravitational dynamics in the first post-Minkowskian (PM)\napproximation to general relativity (valid to linear order in Newton's constant\n$G$). We compute the gravitational influence of seven different celestial\nbodies and find that the Sun has the greatest influence on the trajectory of\nthe interstellar spacecraft. We also study the differences between Newtonian\nand PM gravity, and find that if mission planners wish to hit Proxima Centauri\nb with an accuracy of better than about 690,000 kilometers, relativistic\neffects must be taken into account. To solve for the precise initial data\nneeded to hit an intended target, we develop numerical fine-tuning methods and\ndemonstrate that these methods can (within a given model) be precise to about a\nfemtometer over a travel distance of $\\sim4.25$ light years. However, we find\nthat for the spacecraft trajectories we consider, higher order general\nrelativistic effects (beyond the first PM approximation) from the Sun can\ndisplace the final position of the spacecraft by tens of kilometers. We also\nconsider the variation in the initial direction of the spacecraft velocity and\nfind that, even with relativistic effects properly taken into account, the miss\ndistances can be dominated by the variation in the initial velocity that could\narise from errors during the launch and boost phase of the spacecraft mission.", "AI": {"tldr": "Relativistic gravitational effects are crucial for interstellar spacecraft missions, requiring consideration for accurate targeting of Proxima Centauri b, with the Sun having the greatest influence and higher-order effects causing tens of kilometers displacement.", "motivation": "To determine the importance of gravitational and relativistic effects for interstellar travel, particularly for laser-propelled spacecraft missions to neighboring stars like Proxima Centauri b.", "method": "Used a Julia reimplementation of the PoMiN code, an N-body code modeling relativistic gravitational dynamics in the first post-Minkowskian approximation to general relativity, and developed numerical fine-tuning methods for precise initial data.", "result": "The Sun has the greatest gravitational influence; relativistic effects must be considered for accuracy better than ~690,000 km; numerical methods achieve femtometer precision over 4.25 light years; higher-order relativistic effects from the Sun displace final position by tens of kilometers; launch phase errors can dominate miss distances.", "conclusion": "Relativistic effects are significant for interstellar mission planning, with the Sun's gravitational influence being most important, and both relativistic corrections and launch phase precision are critical for successful targeting of distant exoplanets."}}
{"id": "2510.15305", "pdf": "https://arxiv.org/pdf/2510.15305", "abs": "https://arxiv.org/abs/2510.15305", "authors": ["Zhuo Chen", "Xinjian Xu", "Shihui Ying", "Tieyong Zeng"], "title": "Riemannian Bilevel Optimization with Gradient Aggregation", "categories": ["math.OC", "cs.NA", "math.NA", "49Q12, 65K10"], "comment": "Submitted to the Journal of Optimization Theory and Applications\n  (JOTA), under review. 25 pages, 4 figures", "summary": "Bilevel optimization (BLO) offers a principled framework for hierarchical\ndecision-making and has been widely applied in machine learning tasks such as\nhyperparameter optimization and meta-learning. While existing BLO methods are\nmostly developed in Euclidean spaces, many real-world problems involve\nstructural constraints. In this paper, we propose a Riemannian bilevel\noptimization (RBLO) algorithm that incorporates a bilevel descent aggregation\n(BDA) scheme to jointly coordinate upper- and lower-level updates. Concretely,\nfirst we abstract the constraints in the BLO to a manifold structure and then\ntransform the constrained BLO be a unconstrained RBLO problem. Second, to\naddress limitations of existing RBLO methods, particularly the restrictive\nassumptions required for convergence, we reformulate the bilevel problem using\nsmooth manifold mappings and provide a convergence analysis under the\nconditions of geodesic convexity and Lipschitz smoothness. Finally, we recall\nthe multi-view hypergraph spectral clustering task, and evaluate the proposed\napproach on 3sources data sets. The numerical results validate the superior\nperformance over Euclidean and manifold-based baselines.", "AI": {"tldr": "Proposes Riemannian bilevel optimization (RBLO) with bilevel descent aggregation for handling structural constraints in hierarchical decision-making problems.", "motivation": "Existing bilevel optimization methods are mostly developed in Euclidean spaces, but many real-world problems involve structural constraints that can be abstracted to manifold structures.", "method": "Transform constrained BLO to unconstrained RBLO by abstracting constraints to manifolds, use bilevel descent aggregation scheme to coordinate upper- and lower-level updates, and provide convergence analysis under geodesic convexity and Lipschitz smoothness.", "result": "Numerical evaluation on 3sources datasets for multi-view hypergraph spectral clustering shows superior performance over Euclidean and manifold-based baselines.", "conclusion": "The proposed RBLO algorithm effectively handles structural constraints in bilevel optimization problems and demonstrates improved performance compared to existing approaches."}}
{"id": "2510.15431", "pdf": "https://arxiv.org/pdf/2510.15431", "abs": "https://arxiv.org/abs/2510.15431", "authors": ["Francesco Colasanto", "Pascal Steinke"], "title": "Second-Order Gamma Limit for the Cahn-Hilliard Functional with Dirichlet Boundary Conditions", "categories": ["math.AP", "49J45"], "comment": null, "summary": "This paper addresses the asymptotic development of order 2 by Gamma\nconvergence of the Cahn-Hillard functional with Dirichlet boundary conditions,\nwhere the potential has subquadratic growth near the wells.", "AI": {"tldr": "Asymptotic development of order 2 by Gamma convergence for Cahn-Hilliard functional with Dirichlet boundary conditions and subquadratic potential growth near wells", "motivation": "To study the asymptotic behavior and convergence properties of the Cahn-Hilliard functional, particularly focusing on cases with subquadratic growth near the potential wells under Dirichlet boundary conditions", "method": "Using Gamma convergence techniques to analyze the asymptotic development of order 2 for the Cahn-Hilliard functional", "result": "Established asymptotic development results for the Cahn-Hilliard functional with subquadratic potential growth near wells under Dirichlet boundary conditions", "conclusion": "Successfully developed asymptotic analysis framework for Cahn-Hilliard functional with subquadratic potential growth using Gamma convergence methods"}}
{"id": "2510.15507", "pdf": "https://arxiv.org/pdf/2510.15507", "abs": "https://arxiv.org/abs/2510.15507", "authors": ["Nirjan Biswas", "Souptik Chakraborty", "Paramananda Das"], "title": "Mixed local-nonlocal equations with critical nonlinearity on $\\mathbb{R}^N$: Non-existence, Existence, and Multiplicity of positive solutions", "categories": ["math.AP", "35B09, 35B33, 35J20, 35J60"], "comment": "23 pages, comments are welcome", "summary": "We study the following critical problem involving the mixed local-nonlocal\noperator: \\begin{equation}\\label{main_prob_abstract}\\tag{$\\mathcal{P}_2$}\n  -\\Delta u+(-\\Delta)^s u=|u|^{2^*-2}u+f(x)\\text{ in }\\mathbb{R}^N,\n\\end{equation} where $N \\ge 3,\\, s\\in (0,1),\\, 2^*= \\frac{2N}{N-2}$, and $f$ is\na nontrivial non-negative functional which lies in the dual space of the\nambient solution space. For $f \\equiv0$, ($\\mathcal{P}_2$) does not admit any\nnontrivial weak solution in $L^2(\\mathbb{R}^N)$. This phenomenon stands in\ncontrast to the purely local (for $N>4$) and purely nonlocal (for $N>4s$)\ncases. On the other hand, when $f \\not \\equiv 0$, we prove the existence of at\nleast two positive weak solutions to ($\\mathcal{P}_2$), provided that the\ndimension $N$ satisfies certain restrictions and $\\|f\\|$ is small in the\ncorresponding dual space. Next, we consider the nonlinear analogue to\n($\\mathcal{P}_2$), namely\n\\begin{equation}\\label{main_prob_abstract_1}\\tag{$\\mathcal{P}_p$}\n  -\\Delta_p u+(-\\Delta_p)^s u=|u|^{p^*-2}u+f(x)\\text{ in }\\mathbb{R}^N,\n\\end{equation} where $p \\in (1, \\infty), N>p$, $p^*=\\frac{Np}{N-p}$, $f$ is a\nnontrivial non-negative functional in the dual space of the ambient solution\nspace. As in the semilinear case, no nontrivial weak solution exists for\n($\\mathcal{P}_p$) in $L^p(\\mathbb{R}^N)$, when $f \\equiv 0$. Moreover, for $f\n\\not \\equiv 0$ with sufficiently small $\\|f\\|$, ($\\mathcal{P}_p$) admits a\npositive weak solution. For the existence, we prove a concentration compactness\nprinciple for $-\\Delta_p+(-\\Delta_p)^s$.", "AI": {"tldr": "Study of critical problems with mixed local-nonlocal operators in R^N. For f=0, no nontrivial weak solutions exist in L^p spaces, contrasting with purely local/nonlocal cases. For small nonzero f, existence of positive weak solutions is proven.", "motivation": "To understand the existence and multiplicity of solutions for critical problems involving mixed local-nonlocal operators, which exhibit different behavior from purely local or purely nonlocal cases.", "method": "Proving a concentration compactness principle for the mixed operator -\u0394_p + (-\u0394_p)^s, and using variational methods to establish existence results for small nonzero forcing terms.", "result": "When f=0, no nontrivial weak solutions exist in L^p(R^N). For small nonzero f, at least two positive weak solutions exist for the semilinear case (p=2) under dimensional restrictions, and at least one positive weak solution exists for the nonlinear case (p>1).", "conclusion": "The mixed local-nonlocal operator exhibits unique behavior: trivial solution space for zero forcing, but existence of positive solutions for small nonzero forcing, requiring new compactness principles."}}
{"id": "2510.15435", "pdf": "https://arxiv.org/pdf/2510.15435", "abs": "https://arxiv.org/abs/2510.15435", "authors": ["Luo Long", "Coralia Cartis", "Paz Fink Shustin"], "title": "Nonlinear Dimensionality Reduction Techniques for Bayesian Optimization", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA"], "comment": "34 pages including appendixes, 8 figures. Keywords: global\n  optimisation, dimensionality reduction techniques, Bayesian methods,\n  Variational Autoencoders", "summary": "Bayesian optimisation (BO) is a standard approach for sample-efficient global\noptimisation of expensive black-box functions, yet its scalability to high\ndimensions remains challenging. Here, we investigate nonlinear dimensionality\nreduction techniques that reduce the problem to a sequence of low-dimensional\nLatent-Space BO (LSBO). While early LSBO methods used (linear) random\nprojections (Wang et al., 2013), building on Grosnit et al. (2021), we employ\nVariational Autoencoders (VAEs) for LSBO, focusing on deep metric loss for\nstructured latent manifolds and VAE retraining to adapt the encoder-decoder to\nnewly sampled regions. We propose some changes in their implementation,\noriginally designed for tasks such as molecule generation, and reformulate the\nalgorithm for broader optimisation purposes. We then couple LSBO with\nSequential Domain Reduction (SDR) directly in the latent space (SDR-LSBO),\nyielding an algorithm that narrows the latent search domains as evidence\naccumulates. Implemented in a GPU-accelerated BoTorch stack with Matern-5/2\nGaussian process surrogates, our numerical results show improved optimisation\nquality across benchmark tasks and that structured latent manifolds improve BO\nperformance. Additionally, we compare random embeddings and VAEs as two\nmechanisms for dimensionality reduction, showing that the latter outperforms\nthe former. To the best of our knowledge, this is the first study to combine\nSDR with VAE-based LSBO, and our analysis clarifies design choices for metric\nshaping and retraining that are critical for scalable latent space BO. For\nreproducibility, our source code is available at\nhttps://github.com/L-Lok/Nonlinear-Dimensionality-Reduction-Techniques-for-Bayesian-Optimization.git.", "AI": {"tldr": "This paper proposes a novel Bayesian optimization approach that combines nonlinear dimensionality reduction using Variational Autoencoders with Sequential Domain Reduction in latent space, showing improved performance over random embeddings for high-dimensional optimization problems.", "motivation": "Bayesian optimization struggles with scalability to high dimensions, and existing Latent-Space BO methods using linear random projections have limitations. The authors aim to develop more effective nonlinear dimensionality reduction techniques for BO.", "method": "Uses Variational Autoencoders for nonlinear dimensionality reduction in Latent-Space BO, with deep metric loss for structured latent manifolds and VAE retraining. Combines this with Sequential Domain Reduction in latent space (SDR-LSBO) and implements in GPU-accelerated BoTorch with Matern-5/2 Gaussian process surrogates.", "result": "The approach shows improved optimization quality across benchmark tasks, with structured latent manifolds enhancing BO performance. VAEs outperform random embeddings as dimensionality reduction mechanisms.", "conclusion": "This is the first study to combine Sequential Domain Reduction with VAE-based Latent-Space BO, and the analysis provides critical design insights for metric shaping and retraining in scalable latent space BO."}}
{"id": "2510.15544", "pdf": "https://arxiv.org/pdf/2510.15544", "abs": "https://arxiv.org/abs/2510.15544", "authors": ["Antoine Bricmont"], "title": "Construction and properties for the Green's function with Neumann boundary condition", "categories": ["math.AP"], "comment": null, "summary": "This article addresses the construction and analysis of the Green's function\nfor the Neumann boundary value problem associated with the operator $-\\Delta +\na$ on a smooth bounded domain $\\Omega \\subset \\mathbb{R}^N$ ($N \\geq 3$) with\n$a\\in L^\\infty(\\Omega)$. Under the assumption that $-\\Delta + a$ is coercive,\nwe obtain the existence, uniqueness, and qualitative properties of the Green's\nfunction $G(x,y)$. The Green's function $G(x,y)$ is constructed explicitly,\nsatisfying pointwise estimates and derivative estimates near the singularity.\nAlso, near the boundary of $\\Omega$, $G$ is compared to the Green's function of\nthe laplacian, with pointwise estimates. Other properties, like symmetry and\npositivity among other things, are established.", "AI": {"tldr": "Construction and analysis of the Green's function for the Neumann boundary value problem of the operator -\u0394 + a on smooth bounded domains in R^N (N\u22653), including existence, uniqueness, and qualitative properties.", "motivation": "To establish rigorous mathematical foundations for the Green's function associated with the Neumann boundary value problem for elliptic operators of the form -\u0394 + a, which is fundamental for solving partial differential equations and understanding their qualitative behavior.", "method": "Explicit construction of the Green's function G(x,y) under coercivity assumption of -\u0394 + a, with detailed analysis of pointwise estimates, derivative estimates near singularities, and comparison with the Laplacian's Green's function near boundaries.", "result": "Proved existence and uniqueness of the Green's function, established pointwise estimates and derivative bounds near singularities, obtained boundary behavior comparisons, and demonstrated properties like symmetry and positivity.", "conclusion": "The paper provides a complete theoretical framework for the Green's function of the Neumann problem for coercive elliptic operators, establishing its fundamental properties and behavior that are essential for applications in PDE theory."}}
{"id": "2510.15508", "pdf": "https://arxiv.org/pdf/2510.15508", "abs": "https://arxiv.org/abs/2510.15508", "authors": ["Naoki Yoshida", "Satoshi Hayakawa", "Yuhta Takida", "Toshimitsu Uesaka", "Hiromi Wakaki", "Yuki Mitsufuji"], "title": "Theoretical Refinement of CLIP by Utilizing Linear Structure of Optimal Similarity", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML"], "comment": null, "summary": "In this study, we propose an enhancement to the similarity computation\nmechanism in multi-modal contrastive pretraining frameworks such as CLIP. Prior\ntheoretical research has demonstrated that the optimal similarity metrics\nbetween paired modalities should correspond to the pointwise mutual information\n(PMI) between the two modalities. However, the current implementations of CLIP\nand its variants fail to fully utilize the underlying linear structure of PMI.\nWe therefore propose KME-CLIP, which leverages this structure through the inner\nproduct in a reproducing kernel Hilbert space. We theoretically prove that our\nmethod can approximate PMI with arbitrary accuracy and empirically demonstrate\nthat our approach overall outperforms the standard CLIP formulation across\nseveral retrieval and classification tasks.", "AI": {"tldr": "KME-CLIP enhances CLIP by using kernel methods to better approximate pointwise mutual information (PMI) for similarity computation, improving performance on retrieval and classification tasks.", "motivation": "Current CLIP implementations don't fully utilize the linear structure of PMI, which theory shows should be the optimal similarity metric between paired modalities.", "method": "Proposed KME-CLIP uses inner product in reproducing kernel Hilbert space to leverage PMI's linear structure and approximate it with arbitrary accuracy.", "result": "Empirically outperforms standard CLIP across several retrieval and classification tasks.", "conclusion": "Kernel-based approach effectively captures PMI structure and improves multi-modal contrastive learning performance."}}
{"id": "2510.15631", "pdf": "https://arxiv.org/pdf/2510.15631", "abs": "https://arxiv.org/abs/2510.15631", "authors": ["Joachim Rehberg", "Elmar Schrohe"], "title": "Optimal Sobolev Regularity for Second Order Divergence Elliptic Operators on Domains with Buried Boundary Parts", "categories": ["math.AP", "35J15, 35J25"], "comment": null, "summary": "We study the regularity of solutions of elliptic second order boundary value\nproblems on a bounded domain $\\Omega$ in $\\mathbb R^3$. The coefficients are\nnot necessarily continuous and the boundary conditions may be mixed, i.e.\nDirichlet on one part $D$ of the boundary and Neumann on the complementing\npart. The peculiarity is that $D$ is partly `buried' in $\\Omega$ in the sense\nthat the topological interior of $\\Omega \\cup D$ properly contains $\\Omega$.\nThe main result is that the singularity of the solution along the border of the\nburied contact behaves exactly as the singularity for the solution of a mixed\nboundary value problem along the border between the Dirichlet and the Neumann\nboundary part.", "AI": {"tldr": "Study of regularity of solutions for elliptic second order boundary value problems in 3D with discontinuous coefficients and mixed boundary conditions, where Dirichlet boundary is partly 'buried' in the domain.", "motivation": "To understand the behavior of solutions to elliptic boundary value problems with mixed boundary conditions and discontinuous coefficients, particularly when the Dirichlet boundary is partially embedded within the domain rather than just on the surface.", "method": "Analysis of elliptic second order boundary value problems on bounded domains in R\u00b3 with coefficients that are not necessarily continuous, using mixed boundary conditions (Dirichlet on one part, Neumann on another) where the Dirichlet boundary is partly 'buried' in the domain.", "result": "The singularity of the solution along the border of the buried contact behaves exactly like the singularity for solutions of mixed boundary value problems along the border between Dirichlet and Neumann boundary parts.", "conclusion": "The study establishes that the singularity behavior in solutions with buried Dirichlet boundaries follows the same pattern as traditional mixed boundary problems, providing important regularity results for these more complex geometric configurations."}}
{"id": "2510.15651", "pdf": "https://arxiv.org/pdf/2510.15651", "abs": "https://arxiv.org/abs/2510.15651", "authors": ["Ziqian Li", "Kang Liu", "Yongcun Song", "Hangrui Yue", "Enrique Zuazua"], "title": "Deep Neural ODE Operator Networks for PDEs", "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC", "AMS Subject Classification: 65M99, 47-08, 68T07, 65D15"], "comment": null, "summary": "Operator learning has emerged as a promising paradigm for developing\nefficient surrogate models to solve partial differential equations (PDEs).\nHowever, existing approaches often overlook the domain knowledge inherent in\nthe underlying PDEs and hence suffer from challenges in capturing temporal\ndynamics and generalization issues beyond training time frames. This paper\nintroduces a deep neural ordinary differential equation (ODE) operator network\nframework, termed NODE-ONet, to alleviate these limitations. The framework\nadopts an encoder-decoder architecture comprising three core components: an\nencoder that spatially discretizes input functions, a neural ODE capturing\nlatent temporal dynamics, and a decoder reconstructing solutions in physical\nspaces. Theoretically, error analysis for the encoder-decoder architecture is\ninvestigated. Computationally, we propose novel physics-encoded neural ODEs to\nincorporate PDE-specific physical properties. Such well-designed neural ODEs\nsignificantly reduce the framework's complexity while enhancing numerical\nefficiency, robustness, applicability, and generalization capacity. Numerical\nexperiments on nonlinear diffusion-reaction and Navier-Stokes equations\ndemonstrate high accuracy, computational efficiency, and prediction\ncapabilities beyond training time frames. Additionally, the framework's\nflexibility to accommodate diverse encoders/decoders and its ability to\ngeneralize across related PDE families further underscore its potential as a\nscalable, physics-encoded tool for scientific machine learning.", "AI": {"tldr": "NODE-ONet is a neural ODE operator network that incorporates physics knowledge into PDE solving, improving temporal dynamics modeling and generalization beyond training time frames.", "motivation": "Existing operator learning approaches often ignore domain knowledge in PDEs, leading to challenges in capturing temporal dynamics and poor generalization beyond training periods.", "method": "Encoder-decoder architecture with three components: spatial encoder, neural ODE for latent temporal dynamics, and decoder for physical space reconstruction. Uses physics-encoded neural ODEs to incorporate PDE-specific properties.", "result": "Demonstrates high accuracy, computational efficiency, and prediction capabilities beyond training time frames on nonlinear diffusion-reaction and Navier-Stokes equations. Reduces framework complexity while enhancing efficiency and generalization.", "conclusion": "The framework offers flexibility with diverse encoders/decoders and generalizes across related PDE families, making it a scalable, physics-encoded tool for scientific machine learning."}}
{"id": "2510.15646", "pdf": "https://arxiv.org/pdf/2510.15646", "abs": "https://arxiv.org/abs/2510.15646", "authors": ["Emanuele Bernardi", "Tommaso Lorenzi", "Andrea Tosin"], "title": "Derivation and quasi-invariant asymptotics of phenotype-structured integro-differential models", "categories": ["math.AP", "35R09, 35Q84, 35Q92"], "comment": null, "summary": "Building upon kinetic theory approaches for multi-agent systems and\ngeneralising them to scenarios where the total mass of the system is not\nconserved, we develop a modelling framework for phenotype-structured\npopulations that makes it possible to bridge individual-level mechanisms with\npopulation-scale evolutionary dynamics. We start by formulating a stochastic\nagent-based model, which describes the dynamics of single population members\nundergoing proliferation, death, and phenotype changes. Then, we formally\nderive the corresponding mesoscopic model, which consists of an\nintegro-differential equation for the distribution of population members over\nthe space of phenotypes, where phenotype changes are modelled via an integral\nkernel. Finally, considering a quasi-invariant regime of small but frequent\nphenotype changes, we rigorously derive a non-local Fokker-Planck-type equation\ncounterpart of this model, wherein phenotype changes are taken into account by\nan advection-diffusion term. The theoretical results obtained are illustrated\nthrough a sample of results of numerical simulations.", "AI": {"tldr": "Developed a modeling framework for phenotype-structured populations that bridges individual-level mechanisms with population-scale evolutionary dynamics, deriving from stochastic agent-based models to mesoscopic integro-differential equations and non-local Fokker-Planck equations.", "motivation": "To extend kinetic theory approaches to multi-agent systems where total mass is not conserved, enabling connection between individual-level mechanisms and population-scale evolutionary dynamics in phenotype-structured populations.", "method": "Formulated stochastic agent-based model for population dynamics (proliferation, death, phenotype changes), derived corresponding mesoscopic integro-differential equation, and further derived non-local Fokker-Planck equation in quasi-invariant regime of small frequent phenotype changes.", "result": "Successfully established a hierarchical modeling framework connecting microscopic agent-based dynamics to mesoscopic and macroscopic descriptions, with phenotype changes modeled via integral kernels and advection-diffusion terms.", "conclusion": "The framework provides rigorous mathematical foundation for studying phenotype-structured population dynamics, bridging scales from individual agents to population-level evolutionary patterns, with numerical simulations validating the theoretical results."}}
{"id": "2510.15734", "pdf": "https://arxiv.org/pdf/2510.15734", "abs": "https://arxiv.org/abs/2510.15734", "authors": ["Stephen J. Wright"], "title": "Optimization in Theory and Practice", "categories": ["math.OC", "cs.NA", "math.NA", "49K10, 65K05, 65Y20, 68Q25, 90C05, 90C30, 90C51"], "comment": null, "summary": "Algorithms for continuous optimization problems have a rich history of design\nand innovation over the past several decades, in which mathematical analysis of\ntheir convergence and complexity properties plays a central role. Besides their\ntheoretical properties, optimization algorithms are interesting also for their\npractical usefulness as computational tools for solving real-world problems.\nThere are often gaps between the practical performance of an algorithm and what\ncan be proved about it. These two facets of the field -- the theoretical and\nthe practical -- interact in fascinating ways, each driving innovation in the\nother. This work focuses on the development of algorithms in two areas --\nlinear programming and unconstrained minimization of smooth functions --\noutlining major algorithm classes in each area along with their theoretical\nproperties and practical performance, and highlighting how advances in theory\nand practice have influenced each other in these areas. In discussing theory,\nwe focus mainly on non-asymptotic complexity, which are upper bounds on the\namount of computation required by a given algorithm to find an approximate\nsolution of problems in a given class.", "AI": {"tldr": "This paper examines the interplay between theoretical analysis and practical performance in optimization algorithms, focusing on linear programming and unconstrained minimization of smooth functions, highlighting how advances in theory and practice influence each other.", "motivation": "To bridge the gap between theoretical convergence/complexity analysis and practical performance of optimization algorithms, and explore how these two facets drive innovation in each other.", "method": "Analysis of major algorithm classes in linear programming and unconstrained minimization of smooth functions, examining their theoretical properties (mainly non-asymptotic complexity bounds) and practical performance.", "result": "The paper outlines how theoretical complexity analysis and practical algorithm performance have mutually influenced advances in optimization methods, particularly in the two focused areas.", "conclusion": "There is a fascinating interaction between theoretical analysis and practical performance in optimization algorithms, with each driving innovation in the other, highlighting the importance of considering both aspects in algorithm development."}}
{"id": "2510.15694", "pdf": "https://arxiv.org/pdf/2510.15694", "abs": "https://arxiv.org/abs/2510.15694", "authors": ["Annamaria Canino", "Simone Mauro"], "title": "Existence results for variational quasilinear elliptic systems involving the vectiorial $p$-Laplacian", "categories": ["math.AP", "35A01, 35A15, 35J05, 35J20, 35J25, 35J62"], "comment": "Keywords: Subcritical nonlinearities, vectorial $p$-Laplacian, least\n  energy solutions, Dirichlet boundary conditions, quasilinear elliptic\n  systems, Lane-Emden equations", "summary": "We prove existence and regularity results for the following elliptic system:\n\\[ \\begin{cases}\n-\\textbf{div}(|D\\boldsymbol{u}|^{p-2}D\\boldsymbol{u})=\\boldsymbol{f}(x,\\boldsymbol{u})\n& \\text{in } \\Omega \\\\ \\boldsymbol{u}=0 & \\text{on } \\partial\\Omega,\n\\end{cases} \\] where $\\boldsymbol{u}=(u^1,\\dots,u^m)$, $p>1$, and\n$\\Omega\\subset\\mathbb{R}^N$ is a bounded domain. We also consider the special\ncase of the Lane-Emden-type system, taking\n\\[\\boldsymbol{f}(x,\\boldsymbol{u})=\\lambda|\\boldsymbol{u}|^{p-2}\\boldsymbol{u}+|\\boldsymbol{u}|^{q-2}\\boldsymbol{u},\\]\nand we prove a classification result. In particular, we show that any least\nenergy solution for the Lane-Emden system is of the form\n$(c^1\\omega,\\dots,c^m\\omega)$, where $\\boldsymbol{c}=(c^1,\\dots,c^m)\\in\nS^{m-1}$ (the $(m-1)$-sphere in $\\mathbb R^m$) and $\\omega$ is a positive\nsolution of the corresponding scalar equation.", "AI": {"tldr": "Existence and regularity results for p-Laplacian elliptic systems with Dirichlet boundary conditions, including classification of least energy solutions for Lane-Emden-type systems.", "motivation": "To study existence, regularity, and classification of solutions for nonlinear elliptic systems involving the p-Laplacian operator, particularly focusing on Lane-Emden-type systems which generalize classical scalar equations to vector-valued settings.", "method": "Mathematical analysis techniques including variational methods, existence theory for elliptic systems, regularity theory, and classification arguments for least energy solutions.", "result": "Proved existence and regularity for general p-Laplacian systems, and showed that any least energy solution of the Lane-Emden system has the form (c\u00b9\u03c9,...,c\u1d50\u03c9) where c is on the unit sphere and \u03c9 is a positive scalar solution.", "conclusion": "The paper establishes fundamental existence and regularity results for p-Laplacian elliptic systems and provides a complete classification of least energy solutions for Lane-Emden-type systems, revealing their scalar-like structure."}}
{"id": "2510.15077", "pdf": "https://arxiv.org/pdf/2510.15077", "abs": "https://arxiv.org/abs/2510.15077", "authors": ["Chengzhang Fu", "Michael S. Jolly", "Anuj Kumar", "Vincent R. Martinez"], "title": "On Turbulent Behavior of the Generalized Surface Quasigeostrophic Equations", "categories": ["physics.flu-dyn", "math.AP", "35Q30, 76F02, 76F25"], "comment": null, "summary": "Turbulent behavior of the two-parameter family of generalized surface\nquasigeostrophic equations is examined both rigorously and numerically. We\nadapt a cascade mechanism argument to derive an energy spectrum that scales as\n$\\kappa^{2\\beta/3-3}$ where $\\beta$ controls the regularity of the velocity\n($\\beta=1$ in the special case of the SQG). Direct numerical simulations\nindicate that this fits better than $\\kappa^{\\beta/3-3}$ which was derived in\nearlier work. Guided by earlier work on the 2D Navier-Stokes equations, we\nprove a certain condition implies a direct cascade of enstrophy, as well as an\nupper bound on the enstrophy dissipation rate, and sharp bounds on a\ndissipation wavenumber. The dependence of these rigorous results on the two\nparameters is demonstrated numerically.", "AI": {"tldr": "The paper analyzes turbulent behavior in generalized surface quasigeostrophic equations, deriving a new energy spectrum scaling and proving conditions for enstrophy cascade with numerical verification.", "motivation": "To examine turbulent behavior in the two-parameter family of generalized surface quasigeostrophic equations and improve upon earlier energy spectrum predictions.", "method": "Combined rigorous mathematical analysis using cascade mechanism arguments with direct numerical simulations to verify theoretical results.", "result": "Derived an improved energy spectrum scaling of \u03ba^(2\u03b2/3-3) that fits better than earlier \u03ba^(\u03b2/3-3) prediction, and proved conditions for direct enstrophy cascade with bounds on dissipation rates.", "conclusion": "The new energy spectrum scaling provides better fit to numerical data, and rigorous results on enstrophy cascade and dissipation bounds are numerically verified with parameter dependence demonstrated."}}
{"id": "2510.15335", "pdf": "https://arxiv.org/pdf/2510.15335", "abs": "https://arxiv.org/abs/2510.15335", "authors": ["Radek Erban", "Jan Haskovec"], "title": "Impact of memory on clustering in spontaneous particle aggregation", "categories": ["math.DS", "math.AP"], "comment": "27 pages, 7 figures", "summary": "The effect of short-term and long-term memory on spontaneous aggregation of\norganisms is investigated using a stochastic agent-based model. Each individual\nmodulates the amplitude of its random motion according to the perceived local\ndensity of neighbors. Memory is introduced via a chain of $K$~internal\nvariables that allow agents to retain information about previously encountered\ndensities. The parameter $K$ controls the effective length of memory. A formal\nmean-field limit yields a macroscopic Fokker--Planck equation, which provides a\ncontinuum description of the system in the large-population limit. Steady\nstates of this equation are characterized to interpret the emergence and\nmorphology of clusters. Systematic stochastic simulations in one- and\ntwo-dimensional spatial domains reveal that short- or moderate-term memory\npromotes coarsening, resulting in a smaller number of larger clusters, whereas\nlong-term memory inhibits aggregation and increases the proportion of isolated\nindividuals. Statistical analysis demonstrates that extended memory reduces the\nagents' responsiveness to environmental stimuli, explaining the transition from\naggregation to dispersion as $K$ increases. These findings identify memory as a\nkey factor controlling the collective organization of self-driven agents and\nprovide a bridge between individual-level dynamics and emergent spatial\npatterns.", "AI": {"tldr": "Short-term memory promotes clustering while long-term memory inhibits aggregation in self-driven agents, with memory length controlling the transition between collective organization patterns.", "motivation": "To understand how memory duration affects spontaneous aggregation in organisms, bridging individual-level dynamics with emergent spatial patterns.", "method": "Stochastic agent-based model with memory via K internal variables, mean-field Fokker-Planck equation analysis, and systematic simulations in 1D/2D domains.", "result": "Short-term memory promotes coarsening (fewer, larger clusters), long-term memory inhibits aggregation (more isolated individuals), with extended memory reducing environmental responsiveness.", "conclusion": "Memory is a key factor controlling collective organization, providing a bridge between individual dynamics and emergent spatial patterns."}}
{"id": "2510.15777", "pdf": "https://arxiv.org/pdf/2510.15777", "abs": "https://arxiv.org/abs/2510.15777", "authors": ["Zied Ammari", "Michele Correggi", "Marco Falconi", "Rapha\u00ebl Gautier"], "title": "Semiclassical limit of entropies and free energies", "categories": ["math-ph", "math.AP", "math.MP"], "comment": "24 pages", "summary": "Entropy and free energy are central concepts in both statistical physics and\ninformation theory, with quantum and classical facets. In mathematics these\nconcepts appear quite often in different contexts (dynamical systems,\nprobability theory, von Neumann algebras, etc.). In this work, we study the von\nNeumann and Wehrl entropies from the point of view of semiclassical analysis.\nWe first prove the semiclassical convergence of the von Neumann to the Wehrl\nentropy for quantum Gibbs states (thermal equilibrium), after a suitable\nrenormalization has been taken into account. Then, we show that, in the same\nlimit, the free energy functional defined with the Wehrl entropy $\n\\Gamma-$converges to its classical counterpart, so implying convergence of the\nminima and the associated minimizers.", "AI": {"tldr": "This paper studies the semiclassical convergence of von Neumann and Wehrl entropies, showing that von Neumann entropy converges to Wehrl entropy for quantum Gibbs states after renormalization, and that the free energy functional with Wehrl entropy \u0393-converges to its classical counterpart.", "motivation": "To establish connections between quantum and classical entropy concepts in semiclassical analysis, bridging statistical physics, information theory, and mathematical contexts like dynamical systems and von Neumann algebras.", "method": "Using semiclassical analysis to study the convergence properties of von Neumann and Wehrl entropies, with particular focus on quantum Gibbs states and free energy functionals.", "result": "Proved semiclassical convergence of von Neumann to Wehrl entropy for quantum Gibbs states after renormalization, and showed \u0393-convergence of free energy functional with Wehrl entropy to classical counterpart.", "conclusion": "The work establishes rigorous connections between quantum and classical entropy measures in the semiclassical limit, providing mathematical foundations for relating quantum statistical mechanics to classical thermodynamics."}}
