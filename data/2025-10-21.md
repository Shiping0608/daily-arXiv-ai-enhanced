<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 29]
- [math.AP](#math.AP) [Total: 34]
- [physics.comp-ph](#physics.comp-ph) [Total: 6]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [math-ph](#math-ph) [Total: 1]
- [math.CV](#math.CV) [Total: 1]
- [math.DS](#math.DS) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [physics.atom-ph](#physics.atom-ph) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 3]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [math.PR](#math.PR) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.DC](#cs.DC) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [math.OC](#math.OC) [Total: 5]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Is simplicity still possible for a more accurate approximation to the perimeter of the ellipse? or, Using the exponential function to further improve the second Ramanujan's approximation](https://arxiv.org/abs/2510.16191)
*Salvador E. Ayala-Raggi,Manuel Rendón-Marín*

Main category: math.NA

TL;DR: A new closed-form approximation for ellipse perimeter that improves Ramanujan's second formula, achieving very high accuracy with only four constants while maintaining simplicity.


<details>
  <summary>Details</summary>
Motivation: The perimeter of an ellipse lacks exact closed-form expression in elementary functions, and existing approximations need better balance between simplicity and accuracy.

Method: Enhanced Ramanujan's second formula by dividing it by 1 minus a binomial of two exponential terms, creating a stable approximation across wide eccentricity range.

Result: Achieves maximum relative error of ~0.57 ppm across b/a range [0.0001,1], 25x improvement over Cantrell approximation while maintaining compact four-constant expression.

Conclusion: Provides one of the simplest yet most accurate single-line closed-form approximations for ellipse perimeter, quasi-exact at both circular and degenerate limits.

Abstract: The perimeter of an ellipse has no exact closed-form expression in terms of
elementary functions, and numerous approximations have been proposed since the
eighteenth century. Classical formulas by Fagnano, Euler, and Ramanujan, as
well as modern refinements such as Cantrell and Koshy methods, aim to reduce
the approximation error while maintaining computational simplicity. In this
paper, we introduce a new closed-form expression that enhances Ramanujan second
formula by dividing it by 1 minus a binomial of two exponential terms resulting
in a very stable approximation in a range of b/a between 1 and 1/10000, or even
up to a smaller ratio. The resulting approximation remains compact, requiring
only four constants, and achieving a remarkable tradeoff between simplicity and
accuracy. Across the full eccentricity range of b/a in [0.0001,1], our method
attains a maximum relative error of approximately 0.57 ppm with respect to the
exact perimeter computed via elliptic integral. Our formula is quasi-exact at
the extremes, for the circle b/a=1 and for the degenerate flat ellipse b/a=0.
Compared with Cantrell approximation, the proposed method reduces the maximum
relative error by a factor of 25 while preserving a short and elegant
expression. This makes it one of the simplest yet most accurate closed-form and
single-line approximations to the ellipse perimeter currently available in the
literature.

</details>


### [2] [Applications of AAA rational approximation](https://arxiv.org/abs/2510.16237)
*Yuji Nakatsukasa,Lloyd N. Trefethen*

Main category: math.NA

TL;DR: The paper demonstrates applications of rational functions in numerical analysis using the AAA algorithm for rational approximation.


<details>
  <summary>Details</summary>
Motivation: To showcase the versatility and utility of rational functions across various areas of numerical analysis through practical implementations.

Method: Employing the AAA (Adaptive Antoulas-Anderson) algorithm for rational approximation to generate rational function approximations.

Result: Successful demonstration of rational function applications in multiple numerical analysis contexts using the AAA algorithm.

Conclusion: Rational functions, particularly when computed via the AAA algorithm, provide powerful and flexible tools for diverse numerical analysis applications.

Abstract: The AAA algorithm for rational approximation is employed to illustrate
applications of rational functions all across numerical analysis.

</details>


### [3] [Iterative solvers for partial differential equations with dissipative structure: Operator preconditioning and optimal control](https://arxiv.org/abs/2510.16399)
*Volker Mehrmann,Manuel Schaller,Martin Stoll*

Main category: math.NA

TL;DR: Analysis of iterative solvers for non-symmetric matrices in port-Hamiltonian PDEs, showing that preconditioning with the symmetric part yields mesh-independent condition numbers for elliptic/parabolic problems.


<details>
  <summary>Details</summary>
Motivation: To develop efficient iterative solvers for large-scale problems with non-symmetric matrices from port-Hamiltonian PDE discretizations, leveraging the structure for fast convergence.

Method: Use symmetric part H of operator A=H+S as preconditioner in Krylov subspace methods (GMRES, Widlund's/Rapoport's methods), analyze condition numbers for various PDEs, apply to optimal control via condensing and constraint preconditioning.

Result: Preconditioning with symmetric part yields condition numbers uniform in mesh size for elliptic/parabolic PDEs where H⁻¹S is bounded. Methods successfully applied to diffusion, fluid dynamics, elasticity problems and optimal control.

Conclusion: Symmetric part preconditioning provides mesh-independent convergence for port-Hamiltonian systems, with efficient implementations using incomplete Cholesky or algebraic multigrid, making it suitable for large-scale applications.

Abstract: This work considers the iterative solution of large-scale problems subject to
non-symmetric matrices or operators arising in discretizations of
(port-)Hamiltonian partial differential equations. We consider problems
governed by an operator $\mathcal{A}=\mathcal{H}+\mathcal{S}$ with symmetric
part $\mathcal{H}$ that is positive (semi-)definite and skew-symmetric part
$\mathcal{S}$. Prior work has shown that the structure and sparsity of the
associated linear system enables Krylov subspace solvers such as the
generalized minimal residual method (GMRES) or short recurrence variants such
as Widlund's or Rapoport's method using the symmetric part $\mathcal{H}$, or an
approximation of it, as preconditioner. In this work, we analyze the resulting
condition numbers, which are crucial for fast convergence of these methods, for
various partial differential equations (PDEs) arising in diffusion phenomena,
fluid dynamics, and elasticity. We show that preconditioning with the symmetric
part leads to a condition number uniform in the mesh size in case of elliptic
and parabolic PDEs where $\mathcal{H}^{-1}\mathcal{S}$ is a bounded operator.
Further, we employ the tailored Krylov subspace methods in optimal control by
means of a condensing approach and a constraint preconditioner for the
optimality system. We illustrate the results by various large-scale numerical
examples and discuss efficient evaluations of the preconditioner, such as
incomplete Cholesky factorization or the algebraic multigrid method.

</details>


### [4] [Applications of optimal error bounds for some generalized two-step iterative processes in Banach spaces](https://arxiv.org/abs/2510.16403)
*Tan-Phuc Nguyen,Thai-Hung Nguyen,Tien-Khai Nguyen,Cong-Duy-Nguyen Nguyen,Trung-Hieu Huynh*

Main category: math.NA

TL;DR: This paper extends previous work on optimal error bounds for iterative processes, focusing on more general iterative methods in Banach spaces to determine convergence rates and compare different processes.


<details>
  <summary>Details</summary>
Motivation: To build upon previous research on optimal error bounds for iterative processes and extend the analysis to more general iterative methods studied by other authors.

Method: The authors continue the determination of optimal error bounds for general iterative processes in Banach spaces, analyzing convergence under sufficient conditions on parameter sequences.

Result: Convergence results are obtained and convergence rates of these iterative processes are determined through the analysis of optimal error bounds.

Conclusion: The paper successfully extends the concept of optimal error bounds to more general iterative processes, enabling comparison between different methods and determination of convergence rates in Banach spaces.

Abstract: In a recent paper~\cite{paper2}, we proposed the concept of optimal error
bounds for an iterative process, which allows us to obtain the convergence
result of the iterative sequence to the common fixed point of the nonexpansive
mappings in Banach spaces. Moreover, we also achieve the comparison results
between different iterative processes via optimal error bounds. In this paper,
we continue to determine optimal error bounds for more general iterative
processes which were studied by many authors, such as in~\cite{DungHieu} and
references therein. From there, the convergence results are obtained and the
convergence rates of these iterative processes are determined under some
sufficient conditions on sequences of parameters.

</details>


### [5] [Parameter-related strong convergence rate and polynomial stability of a Euler's type method for time-changed stochastic differential equations](https://arxiv.org/abs/2510.16405)
*Ruchun Zuo*

Main category: math.NA

TL;DR: A Euler method with equidistant step size is proposed for time-changed SDEs with multiplicative noise, achieving strong convergence rate related to time-changing parameter, different from random step size methods. Also shows polynomial mean square stability matching the underlying equation's asymptotic behavior.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method for time-changed stochastic differential equations that uses equidistant step size instead of random step size, aiming to obtain convergence rates related to the time-changing parameter and study numerical stability.

Method: Proposed a Euler-type method with equidistant step size for time-changed stochastic differential equations driven by multiplicative noise.

Result: Obtained strong convergence rate related to the parameter of the time-changing process, which differs from existing results using random step size methods. Also demonstrated polynomial stability in mean square sense that aligns with the asymptotic behavior of the underlying equation.

Conclusion: The Euler method with equidistant step size provides an effective numerical approach for time-changed SDEs, yielding convergence rates tied to the time-changing parameter and maintaining stability properties consistent with the original equation's behavior.

Abstract: A Euler's type method with the equidistant step size is proposed for a class
of time-changed stochastic differential equations driven by the multiplicative
noise and the strong convergence rate that is related to the parameter of the
time changing process is obtained. Such a observation of the convergence rate
is significantly different from those existing results that employ methods with
the random step size. The polynomial stability in the mean square sense of the
numerical method is also studied, which is in line with the asymptotic behavior
of the underlying equation.

</details>


### [6] [A multilayer level-set method for eikonal-based traveltime tomography](https://arxiv.org/abs/2510.16413)
*Wenbin Li,Ken K. T. Hung,Shingyu Leung*

Main category: math.NA

TL;DR: A multilayer level-set method (MLSM) for eikonal-based traveltime tomography that captures multiple phases and interfaces using a sequence of level sets, with regularization strategies for stable inversion.


<details>
  <summary>Details</summary>
Motivation: Classical level-set methods rely only on the zero-level set, limiting their ability to represent multiple phases and complex discontinuous models in traveltime tomography.

Method: Uses multilayer level sets with local signed-distance behavior near each level set, computes traveltimes as eikonal viscosity solutions, obtains Frechet derivatives via adjoint state method, and applies regularization including multilayer reinitialization, arc-length penalization, and Sobolev smoothing.

Result: Numerical experiments show the MLSM efficiently recovers complex discontinuous slowness models with multiple phases and interfaces.

Conclusion: The proposed MLSM provides an effective Eulerian framework for first-arrival traveltime tomography that can handle arbitrarily many interfaces and subregions while maintaining inversion stability.

Abstract: We present a novel multilayer level-set method (MLSM) for eikonal-based
first-arrival traveltime tomography. Unlike classical level-set approaches that
rely solely on the zero-level set, the MLSM represents multiple phases through
a sequence of $i_n$-level sets ($n = 0, 1, 2, \cdots$). Near each $i_n$-level
set, the function is designed to behave like a local signed-distance function,
enabling a single level-set formulation to capture arbitrarily many interfaces
and subregions. Within this Eulerian framework, first-arrival traveltimes are
computed as viscosity solutions of the eikonal equation, and Fr\'{e}chet
derivatives of the misfit are obtained via the adjoint state method. To
stabilize the inversion, we incorporate several regularization strategies,
including multilayer reinitialization, arc-length penalization, and Sobolev
smoothing of model parameters. In addition, we introduce an illumination-based
error measure to assess reconstruction quality. Numerical experiments
demonstrate that the proposed MLSM efficiently recovers complex discontinuous
slowness models with multiple phases and interfaces.

</details>


### [7] [Determining the space dependent coefficients in space-time fractional diffusion equations via Krylov preconditioning](https://arxiv.org/abs/2510.16425)
*Asim Ilyas,Muhammad Faisal Khan,Rosita L. Sormani,Giacomo Tento,Stefano Serra-Capizzano*

Main category: math.NA

TL;DR: This paper addresses an inverse problem for reconstructing source terms in a time-space fractional diffusion equation using the quasiboundary value method for regularization, with numerical analysis based on GLT theory.


<details>
  <summary>Details</summary>
Motivation: To solve the ill-posed inverse problem of reconstructing source terms in fractional diffusion equations with variable coefficients, requiring regularization and efficient numerical methods.

Method: Used quasiboundary value method for regularization, finite difference approximation leading to block linear systems, and GLT theory for spectral analysis and preconditioner construction.

Result: Developed a numerical framework with preconditioning guided by GLT analysis, supported by numerical experiments demonstrating the approach's effectiveness.

Conclusion: The GLT-based preconditioning approach combined with regularization provides an effective numerical method for solving inverse problems in fractional diffusion equations with variable coefficients.

Abstract: We consider a time-space fractional diffusion equation with a variable
coefficient and investigate the inverse problem of reconstructing the source
term, after regularizing the problem with the quasiboundary value method to
mitigate the ill-posedness. The equation involves a Caputo fractional
derivative in the space variable and a tempered fractional derivative in the
time variable, both of order in (0, 1). A finite difference approximation leads
to a two-by-two block linear system of large dimensions. We conduct a spectral
analysis of the associated matrix sequences, employing tools from Generalized
Locally Toeplitz (GLT) theory, and construct the preconditioner guided by the
GLT analysis. Numerical experiments are reported and commented, followed by
concluding remarks.

</details>


### [8] [Dynamic-stabilization-based linear schemes for the Allen-Cahn equation with degenerate mobility: MBP and energy stability](https://arxiv.org/abs/2510.16447)
*Hongfei Fu,Dianming Hou,Zhonghua Qiao,Bingyin Zhang*

Main category: math.NA

TL;DR: Linear first- and second-order schemes for Allen-Cahn equation with degenerate mobility, featuring dynamic stabilization for unconditional MBP preservation and energy stability.


<details>
  <summary>Details</summary>
Motivation: To develop numerical schemes that guarantee maximum bound principle preservation and energy stability for Allen-Cahn equation with general (possibly degenerate) mobility, addressing limitations in existing methods.

Method: Dynamic stabilization approach with novel prediction strategy and cut-off preprocessing for second-order scheme; only one linear system per time level required.

Result: Unconditional preservation of MBP, energy stability with mobility robustness (valid even with degenerate mobility), rigorous maximum-norm error estimates established.

Conclusion: Proposed schemes successfully achieve desired properties with computational efficiency; numerical examples validate theoretical findings and demonstrate performance.

Abstract: In this paper, we investigate linear first- and second-order numerical
schemes for the Allen--Cahn equation with a general (possibly degenerate)
mobility. Compared with existing numerical methods, our schemes employ a novel
dynamic stabilization approach that guarantees unconditional preservation of
the maximum bound principle (MBP) and energy stability. A key advance is that
the discrete energy stability remains valid even in the presence of degenerate
mobility-a property we refer to as mobility robustness. Rigorous maximum-norm
error estimates are also established. In particular, for the second-order
scheme, we introduce a new prediction strategy with a cut-off preprocessing
procedure on the extrapolation solution, and only one linear system needs to be
solved per time level. Representative numerical examples are provided to
validate the theoretical findings and performance of the proposed schemes.

</details>


### [9] [Improving performance estimation of a PCM-integrated solar chimney through reduced-order based data assimilation](https://arxiv.org/abs/2510.16469)
*Diego R. Rivera,Ernesto Castillo,Felipe Galarce,Douglas R. Q. Pacheco*

Main category: math.NA

TL;DR: A ROM-DA framework with hybrid data-filling reconstructs temperature fields in PCM solar chimneys from sparse measurements, improving outlet velocity estimation accuracy by 20%.


<details>
  <summary>Details</summary>
Motivation: To enhance estimation accuracy of outlet airflow velocity in PCM-integrated solar chimneys using limited temperature measurements through advanced data assimilation.

Method: Combines reduced-order modeling from high-fidelity simulations with experimental data using regularized least-squares, plus hybrid boundary-layer and bicubic interpolation for missing data.

Result: Reconstructs transient temperature fields with <10% error for sparse data and <3% for expanded data, reducing outlet velocity RMS error by 20% compared to baseline.

Conclusion: First successful ROM-DA application to coupled multiphysics solar chimney with PCM, enabling near-real-time thermal state estimation and digital-twin development.

Abstract: This study evaluates a data assimilation framework based on reduced-order
modeling (ROM-DA), complemented by a hybrid data-filling strategy, to
reconstruct dynamic temperature fields in a phase-change-material (PCM)
integrated solar chimney from limited temperature measurements. The goal is to
enhance the estimation accuracy of the outlet airflow velocity. A regularized
least-squares formulation is employed to estimate temperature distributions
within an inclined solar chimney using RT-42 as the PCM. The methodology
combines (i) a reduced-order model derived from high-fidelity finite-volume
simulations of unsteady conjugate heat transfer with liquid-solid phase change
and surface radiation, and (ii) three experimental datasets with 22, 135, and
203 measurement points. Missing data are reconstructed using a hybrid filling
scheme based on boundary-layer and bicubic interpolations. The assimilated
temperature fields are integrated into the thermally coupled forward solver to
improve velocity predictions. Results show that the ROM-DA framework
reconstructs the transient temperature fields in both the air and PCM domains
with relative errors below 10 percent for sparse data and below 3 percent for
expanded datasets. When applied to experimental measurements, the approach
enhances the fidelity of temperature and velocity fields compared with the
baseline model, reducing the outlet velocity RMS error by 20 percent. This
represents the first application of a ROM-DA framework to a coupled
multiphysics solar chimney with PCM integration, demonstrating its potential
for near-real-time thermal state estimation and digital-twin development.

</details>


### [10] [Computing functions of $A^{-1}B$ where $A$ and $B$ are Hermitian matrices](https://arxiv.org/abs/2510.16473)
*Dario A. Bini,Massimiliano Fasi,Bruno Iannazzo*

Main category: math.NA

TL;DR: The paper analyzes numerical evaluation of Af(A⁻¹B) for Hermitian matrices, comparing algorithms using Schur decomposition with matrix square root vs Cholesky factorization.


<details>
  <summary>Details</summary>
Motivation: To develop efficient and accurate numerical methods for evaluating matrix functions of the form Af(A⁻¹B) where A is Hermitian positive definite and B is Hermitian.

Method: Combines Schur decomposition with either matrix square root or Cholesky factorization to develop multiple algorithms for computing Af(A⁻¹B).

Result: Algorithms based on Cholesky factorization are more accurate and efficient than those based on matrix square root.

Conclusion: Cholesky-based algorithms are recommended over matrix square root approaches for computing Af(A⁻¹B) due to superior numerical performance and computational efficiency.

Abstract: We consider the numerical evaluation of the quantity $Af(A^{-1}B)$, where $A$
is Hermitian positive definite, $B$ is Hermitian, and $f$ is a function defined
on the spectrum of $A^{-1}B$. We study the conditioning of the problem, and we
introduce several algorithms that combine the Schur decomposition with either
the matrix square root or the Cholesky factorization. We study the numerical
behavior of these algorithms in floating-point arithmetic, assess their
computational costs, and compare their numerical performance. Our analysis
suggests that the algorithms based on the Cholesky factorization will be more
accurate and efficient than those based on the matrix square root. This is
confirmed by our numerical experiments.

</details>


### [11] [High-order temporal parametric finite element methods for simulating solid-state dewetting](https://arxiv.org/abs/2510.16493)
*Xiaowen Gan,Yuqian Teng,Sisheng Wang*

Main category: math.NA

TL;DR: High-order parametric finite element methods for simulating solid-state dewetting using surface diffusion and contact point migration with energy-stable semi-implicit schemes.


<details>
  <summary>Details</summary>
Motivation: To develop temporally high-order numerical schemes for simulating solid-state dewetting processes in thin films, improving accuracy while maintaining stability.

Method: Incorporates predictor-corrector strategy and backward differentiation formula into energy-stable parametric finite element method, creating semi-implicit schemes that solve linear systems at each time step.

Result: Methods achieve desired temporal accuracy measured by manifold distance, maintain long-term mesh equidistribution, and demonstrate good mesh quality throughout evolution.

Conclusion: Successfully constructed temporally high-order schemes for solid-state dewetting simulation that are accurate, stable, and maintain mesh quality.

Abstract: We propose a class of temporally high-order parametric finite element methods
for simulating solid-state dewetting of thin films in two dimensions using a
sharp-interface model. The process is governed by surface diffusion and contact
point migration, along with appropriate boundary conditions. By incorporating
the predictor-corrector strategy and the backward differentiation formula for
time discretization into the energy-stable parametric finite element method
developed by Zhao et al. (2021), we successfully construct temporally
high-order schemes. The resulting numerical scheme is semi-implicit, requiring
the solution of a linear system at each time step. The well-posedness of the
fully discretized system is established. Moreover, the method maintains the
long-term mesh equidistribution property. Extensive numerical experiments
demonstrate that our methods achieve the desired temporal accuracy, measured by
the manifold distance, while maintaining good mesh quality throughout the
evolution.

</details>


### [12] [A linear unconditionally structure-preserving L1 scheme for the time-fractional Allen-Cahn equation](https://arxiv.org/abs/2510.16496)
*Dianming Hou,Zhonghua Qiao,Tao Tang*

Main category: math.NA

TL;DR: Development of linear, structure-preserving time-stepping schemes for the time-fractional Allen-Cahn equation that preserve maximum bound principle and variational energy dissipation law.


<details>
  <summary>Details</summary>
Motivation: To create numerical schemes for the time-fractional Allen-Cahn equation that maintain its key mathematical properties - maximum bound principle and variational energy dissipation law - while being computationally efficient.

Method: First-order and min{1+α, 2-α}-order L1 discretizations with fast implementations using sum-of-exponentials technique; linear schemes designed for general temporal meshes including graded meshes.

Result: The proposed schemes unconditionally preserve discrete maximum bound principle and variational energy dissipation law; sharp error estimates established using time-fractional Grönwall inequality; numerical experiments validate theoretical results.

Conclusion: The developed linear time-stepping schemes effectively preserve the mathematical structure of the time-fractional Allen-Cahn equation while maintaining computational efficiency, with proven theoretical guarantees and practical performance demonstrated through numerical experiments.

Abstract: As a variational phase-field model, the time-fractional Allen-Cahn (TFAC)
equation enjoys the maximum bound principle (MBP) and a variational energy
dissipation law. In this work, we develop and analyze linear,
structure-preserving time-stepping schemes for TFAC, including first-order and
$\min\{1+\alpha, 2-\alpha\}$-order L1 discretizations, together with fast
implementations based on the sum-of-exponentials (SOE) technique. A central
feature of the proposed linear schemes is their unconditional preservation of
both the discrete MBP and the variational energy dissipation law on general
temporal meshes, including graded meshes commonly used for these problems.
Leveraging the MBP of the numerical solutions, we establish sharp error
estimates by employing the time-fractional Gro\"nwall inequality. Finally,
numerical experiments validate the theoretical results and demonstrate the
effectiveness of the proposed schemes with an adaptive time-stepping strategy.

</details>


### [13] [Accelerated implicitization: Robust fixed-point iterations arising from an explicit scheme](https://arxiv.org/abs/2510.16535)
*Nicolas A. Barnafi,Felipe Galarce,Pablo Brubeck*

Main category: math.NA

TL;DR: A strategy for solving nonlinear implicit time discretization problems using explicit fixed-point subiterations with Anderson acceleration to improve convergence and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in solving nonlinear problems from implicit time discretizations, especially when matrix assembly is expensive or good preconditioners are unavailable for implicit systems.

Method: Uses explicit fixed-point subiterations for nonlinear problems combined with Anderson acceleration to mitigate instabilities and improve computational efficiency.

Result: Verified usability and scalability on three nonlinear differential equations. Method performs well across problems, particularly in cases like highly convective fluid flows where traditional approaches face difficulties.

Conclusion: The approach formalizes delaying implicit terms in time discretization, provides error analysis, and shows promising results supported by theory, establishing groundwork for future research.

Abstract: This work proposes a general strategy for solving possibly nonlinear problems
arising from implicit time discretizations as a sequence of explicit solutions.
The resulting sequence may exhibit instabilities similar to those of the base
explicit scheme, which can be mitigated through Anderson acceleration. The
approach uses explicit fixed-point subiterations for nonlinear problems,
combined with Anderson acceleration to improve convergence and computational
efficiency. Its usability and scalability are verified on three nonlinear
differential equations. An error analysis is presented to establish the
expected properties of the proposed strategy for both time and space-time
formulations. Several examples illustrate the simplicity of the implementation
and reveal the influence of parameter choices. The method proves simple to
implement and performs well across a range of problems, particularly when
matrix assembly is expensive or a good preconditioner for the implicit system
is unavailable, such as in highly convective fluid flows. This work formalizes
the delay of implicit terms in time discretization, provides a concise error
analysis, and enhances the approach using Anderson acceleration. The results
are encouraging and well supported by existing theory, laying the groundwork
for further research.

</details>


### [14] [Strong error analysis and first-order convergence of Milstein-type schemes for McKean-Vlasov SDEs with superlinear coefficients](https://arxiv.org/abs/2510.16801)
*Jingtao Zhu,Yuying Zhao,Siqing Gan*

Main category: math.NA

TL;DR: Developed unified Milstein-type methods for McKean-Vlasov SDEs with super-linear coefficients, achieving order-one strong convergence for interacting particle systems under mild regularity.


<details>
  <summary>Details</summary>
Motivation: Classical Milstein schemes fail for MV-SDEs with super-linear drift and diffusion terms, causing divergence and particle corruption. Existing methods lacked theoretical guarantees for such cases.

Method: Proposed unified class of Milstein-type discretizations (tamed-, tanh-, sine-Milstein) using discrete-time arguments and binomial expansions, avoiding standard continuous-time Itô approach.

Result: Established order-one strong convergence for interacting particle systems under mild regularity (only once differentiable coefficients), complementing previous numerical-only approaches.

Conclusion: The framework provides rigorous convergence theory for MV-SDEs with super-linear coefficients, validated by numerical experiments, filling a gap in existing literature.

Abstract: In the study of McKean-Vlasov stochastic differential equations (MV-SDEs),
numerical approximation plays a crucial role in understanding the behavior of
interacting particle systems (IPS). Classical Milstein schemes provide strong
convergence of order one under globally Lipschitz coefficients. Nevertheless,
many MV-SDEs arising from applications possess super-linearly growing drift and
diffusion terms, where classical methods may diverge and particle corruption
can occur. In the present work, we aim to fill this gap by developing a unified
class of Milstein-type discretizations handling both super-linear drift and
diffusion coefficients. The proposed framework includes the tamed-, tanh-, and
sine-Milstein methods as special cases and establishes order-one strong
convergence for the associated interacting particle system under mild
regularity assumptions, requiring only once differentiable coefficients. In
particular, our results complement Chen et al. (Electron. J. Probab., 2025),
where a taming-based Euler scheme was only tested numerically without
theoretical guarantees, by providing a rigorous convergence theory within a
broader Milstein-type framework. The analysis relies on discrete-time arguments
and binomial-type expansions, avoiding the continuous-time It\^o approach that
is standard in the literature. Numerical experiments are presented to
illustrate the convergence behavior and support the theoretical findings.

</details>


### [15] [Sparse variational regularization with oversmoothing penalty term in the scale of sequence spaces](https://arxiv.org/abs/2510.16821)
*Robert Plato,Bernd Hofmann*

Main category: math.NA

TL;DR: This paper analyzes variational regularization methods for linear ill-posed problems in sequence spaces, focusing on sparse regularization and oversmoothing cases where solutions don't belong to the penalty functional's domain.


<details>
  <summary>Details</summary>
Motivation: To address linear ill-posed problems in sequence spaces with deterministic noise, particularly focusing on sparse regularization and handling the challenging oversmoothing case where solutions fall outside the penalty functional's domain.

Method: Uses variational regularization with penalty functionals of the form ||·||_p^p (p>0) or ||·||_0 (counting measure for sparsity). For oversmoothing analysis, employs auxiliary elements defined via hard thresholding, which can also be used for post-processing to ensure sparsity.

Result: Presents stability results and convergence rates for suitable a priori parameter choices. The analysis covers both standard regularization and the oversmoothing scenario.

Conclusion: The framework successfully handles ill-posed problems in sequence spaces, including oversmoothing cases, using variational regularization with different penalty functionals and hard thresholding techniques to achieve sparsity and convergence.

Abstract: In this work, we consider a class of linear ill-posed problems with operators
that map from the sequence space $ \ell_r $ ($r \ge 1$) into a Banach space and
in addition satisfy a conditional stability estimate in the scale of sequence
spaces $ \ell_q, \, q \ge 0 $. For the regularization of such problems in the
presence of deterministic noise, we consider variational regularization with a
penalty functional either of the form $ \mathcal{R} =\Vert \cdot \Vert_p^p $
for some $ p > 0 $ or in form of the counting measure $\mathcal{R}_0 = \Vert
\cdot \Vert_0 $. The latter case guarantees sparsity of the corresponding
regularized solutions. In this framework, we present first stability and then
convergence rates for suitable a priori parameter choices. The results cover
the oversmoothing situation, where the desired solution does not belong to the
domain of definition of the considered penalty functional. The analysis of the
oversmoothing case utilizes auxiliary elements that are defined by means of
hard thresholding. Such technique can also be used for post processing to
guarantee sparsity.

</details>


### [16] [Unconditionally Stable, Variable Step DLN Methods for the Allen-Cahn Active Fluid Model: A Divergence-free Preserving Approach](https://arxiv.org/abs/2510.16860)
*Nan Zheng,Wenlong Pei,Qingguang Guan,Wenju Zhao*

Main category: math.NA

TL;DR: A divergence-free mixed FEM for nonlinear fourth-order Allen-Cahn phase field coupled active fluid equations using auxiliary variables and DLN time integration with adaptive stepping.


<details>
  <summary>Details</summary>
Motivation: To handle nonlinear fourth-order phase field equations with active fluids while easing regularity constraints and preserving divergence-free conditions.

Method: Convert fourth-order problem to second-order system using auxiliary variables w=Δu and ξ, apply divergence-free mixed FEM with variable-step DLN time integrator and adaptive time-stepping.

Result: Established boundedness under regularity assumptions, validated through numerical experiments showing effectiveness for complex active fluid dynamics.

Conclusion: The proposed method successfully handles fourth-order phase field equations with active fluids while maintaining accuracy and computational efficiency.

Abstract: This paper addresses the divergence-free mixed finite element method (FEM)
for nonlinear fourth-order Allen-Cahn phase field coupled active fluid
equations. By introducing an auxiliary variable $w = \Delta u$, the original
fourth-order problem is converted into a system of second-order equations,
thereby easing the regularity constraints imposed on standard $H^2$-comforming
finite element spaces. To further refine the formulation, an additional
auxiliary variable $\xi$, analogous to the pressure, is introduced, resulting
in a mixed finite element scheme that preserves the divergence-free condition
in $which = \Delta u$ inherited from the model. A fully discrete scheme is then
established by combining the spatial approximation by the divergence-free mixed
finite element method with the variable-step Dahlquist-Liniger-Nevanlinna (DLN)
time integrator. The boundedness of the scheme is rigorously derived under
suitable regularity assumptions. Additionally, an adaptive time-stepping
strategy based on the minimum dissipation criterion is carried out to enhance
computational efficiency. Several numerical experiments validate the
theoretical findings and demonstrate the method's effectiveness and accuracy in
simulating complex active fluid dynamics.

</details>


### [17] [HOQRI: Higher-order QR Iteration for Low Multilinear Rank Approximation of Large and Sparse Tensors](https://arxiv.org/abs/2510.16930)
*Yuchen Sun,Amit Bhat,Chunmei Wang,Kejun Huang*

Main category: math.NA

TL;DR: HOQRI is a new algorithm for computing low multilinear rank approximation of large sparse tensors, using orthogonalization instead of SVD and eliminating memory explosion via TTMcTC operation.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of HOOI algorithm for large sparse tensors, particularly the intermediate memory explosion problem and computational complexity of SVD steps.

Method: HOQRI uses simple orthogonalization steps instead of SVD, introduces TTMcTC sparse tensor operation to prevent memory explosion, and leverages manifold optimization framework on Stiefel manifolds.

Result: Numerical experiments demonstrate HOQRI's effectiveness on both synthetic and real data, with guaranteed convergence to stationary points.

Conclusion: HOQRI provides an efficient alternative to HOOI for large sparse tensor decomposition, with better memory efficiency and guaranteed convergence properties.

Abstract: We propose a new algorithm called higher-order QR iteration (HOQRI) for
computing low multilinear rank approximation (LMLRA), also known as the Tucker
decomposition, of large and sparse tensors. Compared to the celebrated
higher-order orthogonal iterations (HOOI), HOQRI relies on a simple
orthogonalization step in each iteration rather than a more sophisticated
singular value decomposition step as in HOOI. More importantly, when dealing
with extremely large and sparse data tensors, HOQRI completely eliminates the
intermediate memory explosion by defining a new sparse tensor operation called
TTMcTC (short for tensor times matrix chains times core). Furthermore,
recognizing that the orthonormal constraints form a Cartesian product of
Stiefel manifolds, we introduce the framework of manifold optimization and show
that HOQRI guarantees convergence to the set of stationary points. Numerical
experiments on synthetic and real data showcase the effectiveness of HOQRI.

</details>


### [18] [Numerical boundary control of multi-dimensional discrete-velocity kinetic models](https://arxiv.org/abs/2510.17246)
*Haitian Yang,Wen-An Yong*

Main category: math.NA

TL;DR: This paper extends multi-dimensional discrete-velocity models to numerical implementation using operator splitting and discrete Lyapunov functions to derive control laws ensuring exponential decay of numerical solutions.


<details>
  <summary>Details</summary>
Motivation: To develop numerical methods for multi-dimensional discrete-velocity models that can handle stiff source terms while ensuring stability and exponential decay properties.

Method: Uses operator splitting scheme, discrete Lyapunov function for deriving control laws, implicit scheme for collision part to handle stiffness, and proves stability of resulting schemes.

Result: Derived numerical control laws that ensure exponential decay of numerical solutions in time, with stability proven for the schemes. Validated through three numerical simulations for the two-dimensional coplanar model.

Conclusion: The proposed numerical framework successfully extends theoretical results to practical implementation, providing stable numerical schemes with proven exponential decay properties for multi-dimensional discrete-velocity models.

Abstract: This paper extends our recent results on multi-dimensional discrete-velocity
models to the numerical level. By adopting an operator splitting scheme and
introducing a suitable discrete Lyapunov function, we derive numerical control
laws that ensure the corresponding numerical solutions decay exponentially in
time. To handle the stiff source term, we also use an implicit scheme for the
collision part and prove the stability of the resulting schemes. The
theoretical results are validated through three numerical simulations for the
two-dimensional coplanar model.

</details>


### [19] [Counterexamples to the conjecture of the upper bound of the derivative of a rational Bézier curve](https://arxiv.org/abs/2510.17300)
*Mao Shi*

Main category: math.NA

TL;DR: Counterexamples to the upper bound of first-order derivatives of rational Bézier curves are presented, and the supremum of derivatives of all orders is investigated.


<details>
  <summary>Details</summary>
Motivation: To challenge existing upper bounds on derivatives of rational Bézier curves and explore the maximum possible derivative values across all orders.

Method: Present counterexamples that violate the established upper bound for first-order derivatives, then extend the analysis to investigate the supremum of derivatives for all higher orders.

Result: Demonstrated that the previously accepted upper bound for first-order derivatives of rational Bézier curves is incorrect through counterexamples, and established new findings about the supremum of derivatives across all orders.

Conclusion: The paper successfully refutes existing derivative bounds for rational Bézier curves and provides new insights into the maximum derivative behavior across all derivative orders.

Abstract: In this paper, we present counterexamples to the upper bound of the
first-order derivative of rational B\'ezier curves and further investigate the
supremum of derivatives of all orders for such curves.

</details>


### [20] [Optimal error estimates of the diffuse domain method for semilinear parabolic equations](https://arxiv.org/abs/2510.17319)
*Yuejin Xu*

Main category: math.NA

TL;DR: The paper analyzes convergence of diffuse domain method for semilinear parabolic equations with Neumann boundary conditions in irregular domains, proving numerical solution converges to exact solution as interface thickness approaches zero.


<details>
  <summary>Details</summary>
Motivation: To solve semilinear parabolic equations in irregular domains using diffuse domain method, which approximates irregular domains with phasefield functions and transforms problems to larger rectangular domains.

Method: Use phasefield function to approximate irregular domain, modify problem to rectangular domain, prove convergence in weighted Sobolev spaces as interface thickness parameter goes to zero, derive optimal error estimates in weighted L2 and H1 norms.

Result: Proved that numerical solution converges to exact solution when interface thickness parameter approaches zero, derived optimal error estimates, validated theoretical results with numerical experiments.

Conclusion: The diffuse domain method is effective for solving semilinear parabolic equations in irregular domains, with proven convergence and optimal error estimates as interface thickness vanishes.

Abstract: In this paper, we mainly discuss the convergence behavior of diffuse domain
method (DDM) for solving semilinear parabolic equations with Neumann boundary
condition defined in general irregular domains. We use a phasefield function to
approximate the irregular domain and when the interface thickness tends to
zero, the phasefield function will converge to indicator function of the
original domain. With this function, we can modify the problem to another one
defined on a larger rectangular domain that contains the targer physical
domain. Based on the weighted Sobolev spaces, we prove that when the interface
thickness parameter goes to zero, the numerical solution will converge to the
exact solution. Also, we derive the corresponding optimal error estimates under
the weighted L2 and H1 norms. Some numerical experiments are also carried out
to validate the theoretical results.

</details>


### [21] [A Review of Equation-Based and Data-Driven Reduced Order Models featuring a Hybrid cardiovascular application](https://arxiv.org/abs/2510.17331)
*Pierfrancesco Siena,Pasquale Claudio Africa,Michele Girfoglio,Gianluigi Rozza*

Main category: math.NA

TL;DR: A hybrid reduced order model combining projection-based and neural network approaches for efficient blood flow simulation in patient-specific aortic geometries.


<details>
  <summary>Details</summary>
Motivation: Cardiovascular diseases are a leading cause of death, driving the need for patient-specific and benchmark blood flow models that balance computational efficiency with accuracy.

Method: Hybrid ROM integrating projection-based techniques with neural network-enhanced data-driven components, using lifting function strategy to enforce physiologically realistic outflow pressure conditions.

Result: Substantial reduction in computational cost while maintaining high fidelity in reconstructing both velocity and pressure fields.

Conclusion: The hybrid ROM approach enables efficient patient-specific cardiovascular modeling, with detailed comparison showing advantages and limitations of reduced order models in this application domain.

Abstract: Cardiovascular diseases are a leading cause of death in the world, driving
the development of patient-specific and benchmark models for blood flow
analysis. This chapter provides a theoretical overview of the main categories
of Reduced Order Models (ROMs), focusing on both projection-based and
data-driven approaches within a classical setup. We then present a hybrid ROM
tailored for simulating blood flow in a patient-specific aortic geometry. The
proposed methodology integrates projection-based techniques with neural
network-enhanced data-driven components, incorporating a lifting function
strategy to enforce physiologically realistic outflow pressure conditions. This
hybrid methodology enables a substantial reduction in computational cost while
mantaining high fidelity in reconstructing both velocity and pressure fields.
We compare the full- and reduced-order solutions in details and critically
assess the advantages and limitations of ROMs in patient-specific
cardiovascular modeling.

</details>


### [22] [ParaSLRF: A High Performance Rational Filter Method for Solving Large Scale Eigenvalue Problems](https://arxiv.org/abs/2510.17334)
*Biyi Wang,Karl Meerbergen,Raf Vandebril,Hengbin An,Zeyao Mo*

Main category: math.NA

TL;DR: ParaSLRF is a parallel implementation of the SLRF method for computing eigenvalues in symmetric definite generalized eigenvalue problems, featuring two-level parallelization and iterative linear solvers for better load balance.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient parallel eigenvalue solver that overcomes load imbalance issues in existing rational filter methods like PFEAST by using iterative linear solvers and optimized pole selection.

Method: Two-level parallelization: (1) partitioning rational filter application among processor groups, (2) parallel linear system solving within groups using iterative methods. Includes converged eigenpair locking and good initial guesses for iterative solvers.

Result: ParaSLRF shows best parallel efficiency compared to other rational filter methods, demonstrates good two-level strong scalability, excellent load balance, and reduced waiting times in numerical experiments with finite element vibration models.

Conclusion: ParaSLRF provides an efficient parallel eigenvalue computation approach with superior load balancing and scalability compared to existing rational filter methods based on quadrature rules.

Abstract: In \emph{Wang et al., A Shifted Laplace Rational Filter for Large-Scale
Eigenvalue Problems}, the SLRF method was proposed to compute all eigenvalues
of a symmetric definite generalized eigenvalue problem lying in an interval on
the real positive axis. The current paper discusses a parallel implementation
of this method, abbreviated as ParaSLRF. The parallelization consists of two
levels: (1) on the highest level, the application of the rational filter to the
various vectors is partitioned among groups of processors; (2) within each
group, every linear system is solved in parallel.
  In ParaSLRF, the linear systems are solved by iterative methods instead of
direct ones, in contrast to other rational filter methods, such as, PFEAST.
Because of the specific selection of poles in ParaSLRF, the computational cost
of solving the associated linear systems for each pole, is almost the same.
This intrinsically leads to a better load balance between each group of
resources, and reduces waiting times of processes.
  We show numerical experiments from finite element models of mechanical
vibrations, and show a detailed parallel performance analysis. ParaSLRF shows
the best parallel efficiency, compared to other rational filter methods based
on quadrature rules for contour integration. To further improve performance,
the converged eigenpairs are locked, and a good initial guess of iterative
linear solver is proposed. These enhancements of ParaSLRF show good two-level
strong scalability and excellent load balance in our experiments.

</details>


### [23] [A non-local model for heterogeneous material flow on conveyor belts](https://arxiv.org/abs/2510.17500)
*Paola Goatin,Simone Göttlich,Fabian Ziegler*

Main category: math.NA

TL;DR: A finite volume scheme solves a 2D non-local macroscopic material flow model with boundaries, extending from scalar to heterogeneous material systems on bounded domains.


<details>
  <summary>Details</summary>
Motivation: To extend previous scalar case results to systems of heterogeneous materials on bounded domains, addressing challenges with boundary treatment in non-local terms.

Method: Uses finite volume approximation with Roe scheme and dimensional splitting, focusing on handling flux function discontinuities.

Result: Proves convergence of approximate solutions and shows good agreement with microscopic simulations in numerical tests.

Conclusion: The method successfully handles heterogeneous material systems and boundary discontinuities in non-local macroscopic flow models.

Abstract: In this paper, a finite volume approximation scheme is used to solve a
non-local macroscopic material flow model in two space dimensions, accounting
for the presence of boundaries in the non-local terms. Based on a previous
result for the scalar case, we extend the setting to a system of heterogeneous
material on bounded domains. We prove the convergence of the approximate
solutions constructed using the Roe scheme with dimensiona splitting, where the
major challenge lies in the treatment of the discontinuity occurring in the
flux function. Numerical tests show a good agreement with microscopic
simulations.

</details>


### [24] [A general framework for Krylov ODE residuals with applications to randomized Krylov methods](https://arxiv.org/abs/2510.17538)
*Emil Krieger,Marcel Schweitzer*

Main category: math.NA

TL;DR: This paper analyzes randomized Krylov subspace methods for solving ODEs via exponential integration, developing a general framework for error estimation using ODE residuals and demonstrating competitiveness through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: Randomized Krylov methods reduce orthogonalization costs for linear algebra tasks, but need reliable error estimation for ODE applications, particularly in exponential integration schemes.

Method: Develops a general framework for Krylov ODE residuals that unifies existing results, extends to various methods, and provides efficient implementation of sketched Krylov methods with a posteriori error estimation.

Result: Numerical experiments on large-scale real-world ODE models show the error estimate's quality and the competitiveness of sketched Krylov methods compared to other Krylov-based approaches.

Conclusion: The proposed framework provides reliable error monitoring for randomized Krylov methods in ODE applications, making them practical and competitive alternatives to traditional methods.

Abstract: Randomized Krylov subspace methods that employ the sketch-and-solve paradigm
to substantially reduce orthogonalization cost have recently shown great
promise in speeding up computations for many core linear algebra tasks (e.g.,
solving linear systems, eigenvalue problems and matrix equations, as well as
approximating the action of matrix functions on vectors) whenever a
nonsymmetric matrix is involved. An important application that requires
approximating the action of matrix functions on vectors is the implementation
of exponential integration schemes for ordinary differential equations. In this
paper, we specifically analyze randomized Krylov methods from this point of
view. In particular, we use the residual of the underlying differential
equation to derive a new, reliable a posteriori error estimate that can be used
to monitor convergence and decide when to stop the iteration. To do so, we
first develop a very general framework for Krylov ODE residuals that unifies
existing results, simplifies their derivation and allows extending the concept
to a wide variety of methods beyond randomized Arnoldi (e.g., rational Krylov
methods, Krylov methods using a non-standard inner product, ...). In addition,
we discuss certain aspects regarding the efficient implementation of sketched
Krylov methods. Numerical experiments on large-scale ODE models from real-world
applications illustrate the quality of the error estimate as well as the
general competitiveness of sketched Krylov methods for ODEs in comparison to
other Krylov-based methods.

</details>


### [25] [Numerical Error Analysis of the Poisson Equation under RHS Inaccuracies in Particle-in-Cell Simulations](https://arxiv.org/abs/2510.17580)
*Kai Zhang,Tao Xiao,Weizong Wang,Bijiao He*

Main category: math.NA

TL;DR: Analysis shows that RHS inaccuracies from charge density sampling near boundaries in PIC methods affect Poisson equation solutions differently: linear schemes become more accurate while quadratic schemes degrade, contrary to expectations.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of right-hand-side inaccuracies in Poisson equations for PIC simulations, particularly how charge density sampling errors near boundaries affect solution accuracy.

Method: Used analytical derivations in 1D and truncation error analyses in 2D, comparing embedded boundary finite difference schemes with linear and quadratic treatments, plus numerical experiments in 1D, 2D, and 3D domains.

Result: RHS inaccuracies reduce dominant truncation error in linear schemes but introduce zeroth-order terms in quadratic schemes, making linear schemes more accurate under typical PIC conditions. A calibration strategy was proposed to restore quadratic scheme accuracy.

Conclusion: Linear schemes outperform quadratic schemes under PIC-induced RHS inaccuracies, providing new insights into boundary-induced errors in Poisson problems and suggesting calibration methods for improved accuracy.

Abstract: Particle-in-Cell (PIC) simulations rely on accurate solutions of the
electrostatic Poisson equation, yet accuracy often deteriorates near irregular
Dirichlet boundaries on Cartesian meshes. While much research has addressed
discretization errors on the left-hand side (LHS) of the Poisson equation, the
impact of right-hand-side (RHS) inaccuracies - arising from charge density
sampling near boundaries in PIC methods - remains largely unexplored. This
study analyzes the numerical errors induced by underestimated RHS values at
near-boundary nodes when solving the Poisson equation using embedded boundary
finite difference schemes with linear and quadratic treatments. Analytical
derivations in one dimension and truncation error analyses in two dimensions
reveal that such RHS inaccuracies modify local truncation behavior differently:
they reduce the dominant truncation error in the linear scheme but introduce a
zeroth-order term in the quadratic scheme, leading to larger global errors.
Numerical experiments in one-, two-, and three-dimensional domains confirm
these findings. Contrary to expectations, the linear scheme yields superior
overall accuracy under typical PIC-induced RHS inaccuracies. A simple RHS
calibration strategy is further proposed to restore the accuracy of the
quadratic scheme. These results offer new insight into the interplay between
boundary-induced RHS errors and discretization accuracy in Poisson-type
problems.

</details>


### [26] [PDE-Free Mass-Constrained Learning of Complex Systems with Hidden States: The crowd dynamics case](https://arxiv.org/abs/2510.17657)
*Gianmaria Viola,Alessandro Della Pia,Lucia Russo,Ioannis Kevrekidis,Constantinos Siettos*

Main category: math.NA

TL;DR: A machine learning framework for modeling mass-constrained complex systems with hidden states using manifold learning to create reduced-order models without discovering governing PDEs.


<details>
  <summary>Details</summary>
Motivation: To model spatio-temporal dynamics of complex systems that can be described by PDEs but lack explicit models, extending the Equation-Free approach for data-driven reconstruction.

Method: Uses manifold learning with delayed coordinates to obtain latent space representations, employs linear (POD) and nonlinear (Diffusion Maps) methods for low-dimensional embeddings, learns predictive ROMs in latent space, and lifts back to original space while preserving mass constraints.

Result: DM-based nonlinear embeddings outperform POD in reconstruction accuracy, producing more parsimonious and stable ROMs that remain accurate and integrable over long time horizons.

Conclusion: The framework successfully reconstructs PDE solution operators without discovering the PDEs themselves, using manifold-informed objective maps to bridge multiple scales while preserving physical constraints like mass conservation.

Abstract: We introduce a machine learning framework for modeling the spatio-temporal
dynamics of mass-constrained complex systems with hidden states, whose behavior
can, in principle, be described by PDEs but lack explicit models. The method
extends the Equation-Free approach, enabling the data-driven reconstruction of
reduced-order models (ROMs) without needing to identify governing equations.
Using manifold learning, we obtain a latent space representation of system
evolution from data via delayed coordinates, in accordance with
Takens/Whitney's embedding theorems. Linear (Proper Orthogonal Decomposition,
POD) and nonlinear (Diffusion Maps, DMs) methods are employed to extract
low-dimensional embeddings that capture the essential dynamics. Predictive ROMs
are then learned within this latent space, and their evolution is lifted back
to the original high-dimensional space by solving a pre-image problem. We show
that both POD and k-nearest neighbor (k-NN) lifting operators preserve mass, a
key physical constraint in systems such as computational fluid dynamics and
crowd dynamics. Our framework effectively reconstructs the solution operator of
the underlying PDE without discovering the PDE itself, by leveraging a
manifold-informed objective map that bridges multiple scales. For our
illustrations, we use synthetic spatio-temporal data from the Hughes model,
which couples a continuity PDE with an Eikonal equation describing optimal path
selection in crowds. Results show that DM-based nonlinear embeddings outperform
POD in reconstruction accuracy, producing more parsimonious and stable ROMs
that remain accurate and integrable over long time horizons.

</details>


### [27] [A decoupled meshless Nyström scheme for 2D Fredholm integral equations of the second kind with smooth kernels](https://arxiv.org/abs/2510.17680)
*Bruno Degli Esposti,Alessandra Sestini*

Main category: math.NA

TL;DR: Generalizes Nyström method by decoupling solution nodes from quadrature nodes, improving performance especially for narrow kernels using meshless quadrature on scattered nodes.


<details>
  <summary>Details</summary>
Motivation: To improve the classical Nyström method's performance, particularly for narrow kernels, by separating solution and quadrature nodes.

Method: Decouples solution nodes from quadrature nodes, uses moment-free meshless quadrature formulas on scattered nodes, and requires reconstruction scheme to approximate values at quadrature nodes from solution nodes.

Result: Clear performance advantage over classical Nyström method, especially for narrow kernels, with convergence order being minimum between quadrature and reconstruction schemes.

Conclusion: The decoupled Nyström method provides improved efficiency while maintaining accuracy, with proven convergence properties under natural assumptions.

Abstract: The Nystr\"om method for the numerical solution of Fredholm integral
equations of the second kind is generalized by decoupling the set of solution
nodes from the set of quadrature nodes. The accuracy and efficiency of the new
method is investigated for smooth kernels and complex 2D domains using recently
developed moment-free meshless quadrature formulas on scattered nodes. Compared
to the classical Nystr\"om method, our variant has a clear performance
advantage, especially for narrow kernels. The decoupled Nystr\"om method
requires the choice of a reconstruction scheme to approximate values at
quadrature nodes from values at solution nodes. We prove that, under natural
assumptions, the overall order of convergence is the minimum between that of
the quadrature scheme and of the reconstruction scheme.

</details>


### [28] [Efficient Tensor Completion Algorithms for Highly Oscillatory Operators](https://arxiv.org/abs/2510.17734)
*Navjot Singh,Edgar Solomonik,Xiaoye Sherry Li,Yang Liu*

Main category: math.NA

TL;DR: Low-complexity tensor completion algorithms for reconstructing oscillatory operators using butterfly decomposition and tensor reshaping, achieving O(n log³ n) computational cost and improved reconstruction accuracy.


<details>
  <summary>Details</summary>
Motivation: To efficiently reconstruct highly oscillatory operators discretized as large matrices using tensor completion methods that leverage butterfly decomposition structure.

Method: Reshape input matrix into order O(log n) tensor, use butterfly decomposition as tensor decomposition, propose two completion algorithms (alternating least squares and gradient-based optimization) with novel low-rank matrix completion initialization strategy.

Result: Achieved O(n log³ n) computational cost with O(n log n) observed entries, orders of magnitude speedup per iteration compared to low-rank matrix and quantized tensor-train completion, and reconstruction errors smaller by an order of magnitude.

Conclusion: The proposed butterfly completion algorithms with novel initialization strategy enable efficient and accurate recovery of oscillatory operators, outperforming state-of-the-art completion methods in both computational efficiency and reconstruction accuracy.

Abstract: This paper presents low-complexity tensor completion algorithms and their
efficient implementation to reconstruct highly oscillatory operators
discretized as $n\times n$ matrices. The underlying tensor decomposition is
based on the reshaping of the input matrix and its butterfly decomposition into
an order $\mathcal{O} (\log n)$ tensor. The reshaping of the input matrix into
a tensor allows for representation of the butterfly decomposition as a tensor
decomposition with dense tensors. This leads to efficient utilization of the
existing software infrastructure for dense and sparse tensor computations. We
propose two tensor completion algorithms in the butterfly format, using
alternating least squares and gradient-based optimization, as well as a novel
strategy that uses low-rank matrix completion to efficiently generate an
initial guess for the proposed algorithms. To demonstrate the efficiency and
applicability of our proposed algorithms, we perform three numerical
experiments using simulated oscillatory operators in seismic applications. In
these experiments, we use $\mathcal {O} (n \log n)$ observed entries in the
input matrix and demonstrate an $\mathcal{O}(n\log^3 n)$ computational cost of
the proposed algorithms, leading to a speedup of orders of magnitudes per
iteration for large matrices compared to the low-rank matrix and quantized
tensor-train completion. Moreover, the proposed butterfly completion
algorithms, equipped with the novel initial guess generation strategy, achieve
reconstruction errors that are smaller by an order of magnitude, enabling
accurate recovery of the underlying structure compared to the state-of-the-art
completion algorithms.

</details>


### [29] [Local Solvers for High-Order Patch Smoothers via p-Multigrid](https://arxiv.org/abs/2510.17785)
*Michał Wichrowski*

Main category: math.NA

TL;DR: A vertex patch smoother using nested matrix-free p-multigrid for efficient local problem solving in multigrid frameworks.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and robust multigrid method that works well on unstructured meshes with geometric distortion and high-contrast coefficients.

Method: Vertex patch smoother with inexact local solutions via nested matrix-free p-multigrid, requiring O(p^{d+1}) operations per iteration.

Result: Limited sensitivity to geometric distortion and high-contrast coefficients; achieves robustness with respect to polynomial degree p and mesh refinement.

Conclusion: The proposed multigrid-within-multigrid framework provides an effective and robust solver for non-separable problems on unstructured meshes.

Abstract: I propose a vertex patch smoother where local problems are solved inexactly
by a nested, matrix-free p-multigrid, creating a multigrid-within-multigrid
framework. A single iteration of the local solver can be evaluated with
$\mathcal{O}(p^{d+1})$ operations, and the approach is applicable to
non-separable problems on unstructured meshes. Numerical experiments
demonstrate limited sensitivity to geometric distortion and high-contrast
coefficients. When used in a global geometric multigrid solver, the method
achieves robustness with respect to both polynomial degree $p$ and mesh
refinement, even on heavily distorted meshes.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [30] [On periodic solutions of the Benjamin-Bona-Mahony-Burgers equation](https://arxiv.org/abs/2510.16162)
*Chun Ho Lau,Taige Wang*

Main category: math.AP

TL;DR: Existence and stability of periodic solutions for BBM-Burgers equation with time-periodic forcing in H^1_0 space, with extensions to higher regularity and pseudo-parabolic regularization.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for periodic solutions of the BBM-Burgers equation under time-periodic forcing, extending analysis to higher regularity spaces and related pseudo-parabolic equations.

Method: High regularity analysis in Hilbert spaces H^ℓ for ℓ>1, and extension to pseudo-parabolic-regularized equation in ℋ^ℓ spaces for ℓ=1,2, considering periodic boundary value problems with time-periodic forcing.

Result: Established existence and stability of periodic solutions for BBM-Burgers equation in H^1_0([0,1]) with time-periodic forcing, with extensions to higher regularity spaces and pseudo-parabolic regularization.

Conclusion: The paper successfully proves existence and stability results for periodic solutions of BBM-Burgers equations under periodic forcing, providing comprehensive analysis across different regularity spaces and extensions to related equations.

Abstract: In this paper, we would establish the existence and stability of periodic
solutions to the Benjamin-Bona-Mahony-Burgers (BBM-Burgers) equation in
$H^1_0([0, 1])$, whose medium interior is applied with time-periodic force
$f(x, t)$ with period $\theta$. High regularity analysis has been conducted in
Hilbert spaces $H^\ell, \ell>1$. We also consider periodic solution to same
IBVP scenario of a pseudo-parabolic-regularized equation as an extension of the
BBM-Burgers in $\mathcal{H}^{\ell}, \ell=\{1, 2\}$.

</details>


### [31] [Propagation of singularities for equations with $C^{r}$ coefficients for $r>1$](https://arxiv.org/abs/2510.16182)
*Jan Rozendaal*

Main category: math.AP

TL;DR: For r>1 and s in an r-dependent interval, if p is a homogeneous pseudodifferential symbol with C^r regularity and u satisfies p(x,D)u in H^s, then the H^{s+m-1} wavefront set of u consists of maximally extended bicharacteristics of p.


<details>
  <summary>Details</summary>
Motivation: To understand the propagation of singularities for solutions to pseudodifferential equations with limited regularity coefficients, extending classical results to non-smooth settings.

Method: Analysis of wavefront sets and bicharacteristic curves for pseudodifferential operators with C^r regular symbols, using microlocal analysis techniques.

Result: The H^{s+m-1} wavefront set of u propagates along maximally extended bicharacteristics of p. For r=2, C^{1,1} regularity suffices and results apply to manifolds with bounded Ricci curvature.

Conclusion: Singularities propagate along bicharacteristics even with limited regularity coefficients, with applications to geometric settings including manifolds with bounded curvature.

Abstract: We observe that, for $r>1$, $s$ in an $r$-dependent interval, $p$ a
homogeneous pseudodifferential symbol of order $m$ having $C^{r}$ regularity in
space, and $u\in H^{s+m-r}(\mathbb{R}^{n})$ such that $p(x,D)u\in
H^{s}(\mathbb{R}^{n})$, each point in the $H^{s+m-1}$ wavefront set of $u$ lies
on a maximally extended bicharacteristic of $p$ which is contained in the
$H^{s+m-1}$ wavefront set of $u$. In fact, for $r=2$ slightly less than
$C^{1,1}$ regularity suffices, and here the results apply to manifolds with
bounded Ricci curvature.

</details>


### [32] [Weakly localized states of one dimensional Schrodinger equations have localized energy](https://arxiv.org/abs/2510.16283)
*Gavin Stewart,Avy Soffer*

Main category: math.AP

TL;DR: This paper studies the asymptotic behavior of solutions to the 1D Schrödinger equation with time-dependent potentials, showing solutions decompose into free waves and weakly bound components with localized derivatives.


<details>
  <summary>Details</summary>
Motivation: To extend previous results on Schrödinger equation asymptotics to lower-dimensional cases (d=1) where decay conditions on potentials don't preclude resonances, unlike in higher dimensions (d≥5).

Method: Analyze the Schrödinger equation with time-dependent potentials that decay rapidly at infinity, proving decomposition of solutions into free wave components and weakly bound parts with localized spatial derivatives.

Result: Solutions can be written as sum of free wave e^{-itΔ}u_+ and weakly bound component u_wb(t), where u_wb(t) = u_loc(t) + o_{Ḣ¹}(1) with ∂_x u_loc(t) localized near origin uniformly in time.

Conclusion: The work provides a natural extension of previous higher-dimensional results to the one-dimensional case, establishing asymptotic decomposition properties despite potential resonance issues in lower dimensions.

Abstract: We study the asymptotics of the Schr\"odinger equation with time-dependent
potential in dimension one. Assuming that the potential decays sufficiently
rapidly as $|x| \to \infty$, we prove that the solution can be written as the
sum of a free wave $e^{-it\Delta} u_+$ and a weakly bound component
$u_{\text{wb}}(t)$. Moreover, we show that the weakly bound part decomposes as
$u_{\text{wb}}(t) = u_{\text{loc}}(t) + o_{\dot{H}^1}(1)$, where $\partial_x
u_\text{loc}(t)$ is localized near the origin uniformly in time. Since decay
conditions on the potential do not preclude resonances unless $d \geq 5$, our
results can be seen as a natural extension of [Terence Tao. Dynamics of Partial
Differential Equations, 5(2), 2008] and [Avy Soffer, Xiaoxu Wu.
arXiv:2304.04245] to the lower-dimensional case.

</details>


### [33] [A study of general reaction-advection-diffusion equations describing dynamics between target, partaker, and guardian](https://arxiv.org/abs/2510.16286)
*Madi Yerlanov,Nancy Rodriguez*

Main category: math.AP

TL;DR: A reaction-advection-diffusion model for three-actor interactions (target, partaker, guardian) that captures crime hotspots and public attitude shifts, with applications to protest dynamics and escalating hostility in confined spaces.


<details>
  <summary>Details</summary>
Motivation: To develop a versatile mathematical framework that can model complex spatial-temporal interactions among multiple actors in social systems, ranging from urban crime patterns to public opinion shifts during critical events.

Method: A reaction-advection-diffusion system that accounts for movement and interactions of three actors through space, with mathematical proofs of local and global solution existence under realistic assumptions.

Result: The model successfully captures complex spatial patterns and temporal dynamics, demonstrated through numerical simulations of protest dynamics and escalating hostility in enclosed environments.

Conclusion: The proposed framework provides a powerful tool for analyzing multi-actor interactions in social systems, with proven mathematical foundations and practical applications in understanding spatial-temporal dynamics of social phenomena.

Abstract: This paper introduces a reaction-advection-diffusion system that models
interactions among three actors: a target, a partaker, and a guardian. The
framework is versatile, capturing phenomena ranging from the emergence and
movement of crime hotspots in urban areas to shifts in public attitudes during
critical events as individuals and control units move through space. We prove
local and global existence of solutions under realistic assumptions and
showcase the model through two applications: protest dynamics driven by new
demonstrators joining and escalating hostility in enclosed environments such as
classrooms or offices. Numerical simulations highlight the resulting complex
spatial patterns and temporal dynamics.

</details>


### [34] [Parameter Identifiability of RNA Dynamics in PDE Transport Models of Fluorescence Recovery After Photobleaching](https://arxiv.org/abs/2510.16304)
*Qinyu Xu*

Main category: math.AP

TL;DR: Developed a reaction-diffusion-advection PDE model for RNA dynamics in Xenopus oocytes, using FRAP data and identifiability analysis to determine which transport parameters can be uniquely estimated.


<details>
  <summary>Details</summary>
Motivation: To understand RNA transport and localization dynamics in Xenopus laevis oocytes, particularly how diffusion and active transport mechanisms contribute to RNA movement within cells.

Method: Used FRAP experimental data with a reaction-diffusion-advection PDE model, performed identifiability analysis via profile likelihood and reparametrization to assess parameter uniqueness from FRAP data.

Result: Transport velocity and diffusion coefficient are identifiable in all cell regions, while binding/unbinding rate combinations are only identifiable near the nucleus.

Conclusion: Parameter identifiability varies by cellular region, with transport velocity and diffusion being globally identifiable while binding kinetics require specific cellular contexts for reliable estimation.

Abstract: The transport and localization of RNA molecules, crucial for cellular
function and development, involve a combination of diffusion and active
transport mechanisms. Here, we are motivated by understanding the dynamics of
RNA in Xenopus laevis oocytes. Fluorescence Recovery After Photobleaching
(FRAP) is an experimental technique that is widely used to investigate the
dynamics of molecular movement within cells by observing the recovery of
fluorescence intensity in a photobleached region over time. To advance the
understanding of RNA dynamics, we develop a reaction-diffusion-advection
partial differential equation (PDE) model integrating both transport and
diffusion mechanisms. We propose a pipeline for identifiability analysis to
assess the model's ability to uniquely determine parameter values from observed
FRAP data. Based on profile likelihood analysis and reparametrization, we
examine the relationship between non- identifiable parameters, which improves
the robustness of parameter estimation. We find out that the identifiability of
the four parameters of interest is not exactly the same in different regions of
the cell. Specifically, transport velocity and diffusion coefficient are
identifiable in all regions of the cell, while some combinations of binding
rate and unbinding rate are found to be identifiable near the nucleus.

</details>


### [35] [Hele-Shaw flow with surface tension and kinetic undercooling as a sharp interface limit of a fully parabolic Patlak-Keller-Segel system with nonlinear diffusion](https://arxiv.org/abs/2510.16339)
*Michael Rozowski*

Main category: math.AP

TL;DR: The paper studies the large population limit of the parabolic-parabolic Patlak-Keller-Segel system with nonlinear diffusion, showing development of sharp interfaces and connection to Hele-Shaw free boundary problems.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior and interface formation in biological aggregation systems modeled by degenerate Patlak-Keller-Segel equations with nonlinear diffusion.

Method: Analysis of the large population limit of the parabolic-parabolic PKS system with porous medium-type diffusion, using energy convergence hypotheses and gradient flow techniques.

Result: Shows emergence of sharp interfaces separating regions with constant uniform density from empty regions, with the interface evolving according to a Hele-Shaw free boundary problem with surface tension and kinetic undercooling.

Conclusion: Phase separation in these systems results from compatibility between population pressure and chemical kinetics, and the PKS system's gradient flow structure corresponds to a penalized Modica-Mortola functional.

Abstract: A large population limit of the parabolic-parabolic Patlak-Keller-Segel (PKS)
system with degenerate, nonlinear diffusion, e.g., of porous medium-type
$-\frac{m}{m-1}\mathrm{div}(\rho \nabla \rho^{m-1})$, is studied. We show,
asymptotically, a sharp interface develops separating a region containing
organisms arranged in a constant-in-time, uniform density from a region without
organisms. Under an energy convergence hypothesis, we prove the emergent
interface evolves according to a Hele-Shaw free boundary problem with surface
tension and kinetic undercooling, and the free boundary satisfies a contact
angle-type condition with the fixed boundary.
  Further, we show that, for well-prepared initial data, phase separation in
these systems is, roughly, the result of some compatibility between an
antiderivative for the population pressure and the convex conjugate of an
antiderivative of the chemical destruction kinetics. When compatible, an energy
for which the parabolic-parabolic PKS system is a gradient flow is a penalized
Modica-Mortola functional.

</details>


### [36] [Elliptic and Pseudo-Parabolic Gradient System Arising from Anisotropic Image-Denoising with Orientation Adaptation](https://arxiv.org/abs/2510.16354)
*Naotaka Ukai*

Main category: math.AP

TL;DR: The paper proposes a new formulation for anisotropic image denoising that automatically computes initial orientation data by removing the time derivative of the orientation variable, addressing a gap in previous models.


<details>
  <summary>Details</summary>
Motivation: Previous anisotropic image denoising models lacked explicit procedures for determining initial orientation data. The authors aim to resolve this issue by developing a formulation that automatically computes the initial value within the equation.

Method: The authors remove the time derivative of the orientation variable to enable automatic computation of initial values. They use a time-discretization method to establish well-posedness and energy-dissipation properties, with a time-discrete scheme that determines initial orientation data.

Result: The proposed method successfully establishes well-posedness and energy-dissipation for the system. The time-discrete scheme automatically determines initial orientation data and ensures consistency with the continuous model.

Conclusion: The results guarantee stability and effectiveness for numerical implementation, providing a reliable method for determining initial orientation data in anisotropic image denoising through the time-discretization scheme.

Abstract: In this paper, we consider a coupled system of nonlinear elliptic and
pseudo-parabolic PDEs arising in anisotropic monochrome image-denoising with an
orientation adaptation. This system is motivated by the minimization of a
nonconvex energy functional. This study focuses on the treatment of the initial
data for the orientation variable. Previous models have not provided explicit
procedures or clarified a natural and convincing method for its determination.
To resolve this issue, we propose a formulation in which the time derivative of
the orientation variable is removed. This enables the initial value to be
computed automatically within the equation. This formulation weakens the
energy-dissipation property and introduces new challenges in constructing a
stable minimization process. Consequently, a different approach from previous
works is required. In light of the above, this paper aims to establish the
well-posedness and energy-dissipation for our system. The proofs are based on a
time-discretization method. The proposed time-discrete scheme determines the
initial orientation data and ensures consistency with the continuous model.
These results guarantee the stability and effectiveness of the proposed method
for numerical implementation and provide a method for determining the initial
data of orientation in the time-discretization scheme.

</details>


### [37] [Stability threshold of close-to-Couette shear flows with no-slip boundary conditions in 2D](https://arxiv.org/abs/2510.16378)
*Jacob Bedrossian,Siming He,Sameer Iyer,Linfeng Li,Fei Wang*

Main category: math.AP

TL;DR: The paper proves a stability threshold theorem for 2D Navier-Stokes equations on a channel with no-slip boundary conditions, showing that solutions remain close to evolving shear flows when initial perturbations have specific viscosity-dependent scaling.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous stability results for the 2D Navier-Stokes equations near Couette flow, particularly understanding how viscosity affects the stability threshold and demonstrating nonlinear enhanced dissipation and inviscid damping phenomena.

Method: Developed analytical framework to handle the time-dependent nonlocal term in linearized Navier-Stokes equations, with initial conditions where shear perturbation is small (independent of viscosity) and x-dependent fluctuation is O(ν^(1/2)|log ν|^(-2)).

Result: Proved nonlinear enhanced dissipation of vorticity and time-integrated inviscid damping for velocity, ensuring Navier-Stokes solutions stay close to evolving shear flows for all time despite the challenging nonlocal term.

Conclusion: The stability threshold theorem provides rigorous mathematical foundation for understanding how specific viscosity-dependent initial perturbations lead to long-term stability near Couette flow in 2D Navier-Stokes systems.

Abstract: In this paper, we develop a stability threshold theorem for the 2D
incompressible Navier-Stokes equations on the channel, supplemented with the
no-slip boundary condition. The initial datum is close to the Couette flow in
the following sense: the shear component of the perturbation is small, but
independent of the viscosity $\nu$. On the other hand, the $x$-dependent
fluctuation is assumed small in a viscosity-dependent sense, namely,
$O(\nu^{\frac12}|\log \nu|^{-2})$. Under this setup, we prove nonlinear
enhanced dissipation of the vorticity and a time-integrated inviscid damping
for the velocity. These stabilizing phenomena guarantee that the Navier-Stokes
solution stays close to an evolving shear flow for all time. The analytical
challenge stems from a time-dependent nonlocal term that appears in the
associated linearized Navier-Stokes equations.

</details>


### [38] [Equivalence of weak solution concepts for mean curvature flow](https://arxiv.org/abs/2510.16478)
*Tim Laux,Anton Ullrich*

Main category: math.AP

TL;DR: This paper establishes equivalence between viscosity solutions and variational solutions for mean curvature flow, proving that foliations by variational solutions yield unique viscosity solutions.


<details>
  <summary>Details</summary>
Motivation: To resolve an open question about connecting different weak solution concepts for mean curvature flow, bridging the gap between viscosity solutions (based on comparison principle) and variational solutions (Brakke flows and distributional solutions).

Method: Proving that if one has a foliation by variational solutions, then the resulting function is the unique viscosity solution. This connects level sets of viscosity solutions with variational solutions.

Result: Shows that almost every level set of the viscosity solution is a variational solution, establishing equivalence between these solution concepts. Also demonstrates generic uniqueness of variational solutions.

Conclusion: The paper successfully connects viscosity and variational solution concepts for mean curvature flow, answering an open question and showing their equivalence, which has implications for the mathematical understanding of geometric evolution equations.

Abstract: We provide a connection between weak solution concepts of mean curvature
flow. On the one side we have the viscosity solution which is based on the
comparison principle. On the other, variational solutions, which are combined
Brakke flows and distributional solutions. We prove that if one has a foliation
by variational solutions, then the resulting function is the unique viscosity
solution.
  This answers an open question suggested by the work of Evans and Spruck [J.
Geom. Anal., 5(1), 1995] and the authors [J. Geom. Anal., 34(12), 2024]. These
results show that almost every level set of the viscosity solution is a
variational solution. Thus, we establish the equivalence of these solution
concepts. Moreover, we show the generic uniqueness of variational solutions.

</details>


### [39] [Intermittent Solutions of the Stationary 2D Surface Quasi-Geostrophic Equation](https://arxiv.org/abs/2510.16583)
*Nicholas Gismondi,Alexandru F. Radu*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper we construct non-trivial solutions to the stationary
dissipative surface quasi-geostrophic equation on the two dimensional torus
which lie in $\dot{H}^{-1-\epsilon}(\mathbb{T}^2)$ for every $\epsilon > 0$.
Due to the fact that our solutions do not lie in
$\dot{H}^{-1/2}(\mathbb{T}^2)$, we must redefine the notion of solution. The
main new ingredient is the incorporation of intermittency into the construction
of the solutions, which allows for the exponent in the dissipation term to take
any value in the interval $(0,2]$ and also ensures that $\Lambda^{-1}\theta \in
L^p(\mathbb{T}^2)$ for all $1 \leq p < 2$.

</details>


### [40] [Fredholm Theory of Non-Elliptic Operators in the Presence of Normally Hyperbolic Trapping](https://arxiv.org/abs/2510.16642)
*Selim Amar*

Main category: math.AP

TL;DR: Improved Fredholm theory for non-elliptic operators with normally hyperbolic trapping, applied to wave operators on spacetimes.


<details>
  <summary>Details</summary>
Motivation: To develop a Fredholm theory for non-elliptic operators when classical dynamical systems exhibit normally hyperbolic trapping with smooth backward and forward trapped sets.

Method: Uses coisotropic Sobolev spaces with weak regularity at the backward trapped set Γ_u, where distributions v ∈ H^s satisfy Φ_u v ∈ H^{s+1} for some quantization Φ_u of the defining function φ_u of Γ_u.

Result: Establishes an improved Fredholm theory framework for such operators.

Conclusion: The developed theory is successfully applied to wave operators on spacetimes, providing a mathematical foundation for analyzing these operators in the presence of normally hyperbolic trapping.

Abstract: We present an improved Fredholm theory of non-elliptic operators for when the
corresponding classical dynamical system exhibits normally hyperbolic trapping
with smooth backward and forward trapped sets. It takes place on coisotropic
Sobolev spaces with weak regularity at the backward trapped set $\Gamma_u$,
which are roughly speaking made of distributions $v \in H^s$ satisfying $\Phi_u
v \in H^{s+1}$ for some quantization $\Phi_u$ of the defining function $\phi_u$
of $\Gamma_u$. We then apply it to the case of wave operators on spacetimes.

</details>


### [41] [The Dirichlet eigenvalue problems for some concave elliptic Hessian operators](https://arxiv.org/abs/2510.16748)
*Zhang Jiaogen*

Main category: math.AP

TL;DR: The paper establishes a priori estimates for Dirichlet eigenvalue problems involving concave elliptic Hessian operators, including Monge-Ampère and k-Hessian operators, proving existence of first nonzero eigenvalues and smooth eigenfunctions on strictly Γ-convex domains.


<details>
  <summary>Details</summary>
Motivation: To develop comprehensive a priori estimates for eigenvalue problems with concave elliptic Hessian operators, which have applications in various geometric and PDE contexts, and to establish existence and regularity results under mild operator constraints.

Method: The authors impose mild constraints on the concave elliptic Hessian operator F and work on smooth, strictly Γ-convex domains. They use analytical techniques to demonstrate existence of first nonzero eigenvalues and corresponding Γ-admissible eigenfunctions, proving regularity results through careful estimates.

Result: The paper proves the existence of the first nonzero eigenvalue and its corresponding Γ-admissible eigenfunction for concave elliptic Hessian operators. It establishes that the eigenfunction u₁ belongs to C∞(Ω) ∩ C¹,¹(Ω̄), and shows that every invariant Gårding-Dirichlet operator admits a unique first nonzero eigenvalue.

Conclusion: The study provides comprehensive a priori estimates for Dirichlet eigenvalue problems with concave elliptic Hessian operators, establishing existence, uniqueness, and regularity results under mild conditions, with applications extending to invariant Gårding-Dirichlet operators.

Abstract: In this manuscript, we investigate a priori estimates for the solution to the
Dirichlet eigenvalue problem for a broad class of concave elliptic Hessian
operators of the form \[ F(D^2u)=-\Lambda u \quad \textrm{in} \, \Omega, \qquad
u=0 \quad \textrm{on} \, \partial \Omega. \] These operators encompass the
Monge-Amp\`ere operator, the $k$-Hessian operators, and the $p$-Monge-Amp\`ere
operators. We impose a fairly mild constraint on the operator $F$, allowing us
to demonstrate the existence of the first nonzero eigenvalue and its
corresponding $\Gamma$-admissible eigenfunction on the smooth, strictly
$\Gamma$-convex domain $\Omega\subset \mathbb{R}^{n}$. Furthermore, we prove
that the eigenfunction $u_{1}$ belongs to $C^{\infty}(\Omega) \cap
C^{1,1}(\overline{\Omega})$. As an application, we prove that every invariant
G\r{a}rding-Dirichlet operator admits a unique first nonzero eigenvalue.

</details>


### [42] [On a repulsion model with Coulomb interaction and nonlinear mobility](https://arxiv.org/abs/2510.16894)
*Antonin Chodron de Courcel,Charles Elbar*

Main category: math.AP

TL;DR: Study of scalar conservation law with Coulomb interaction and nonlinear mobility, proving existence of entropy solutions, weak-strong uniqueness, and various solution properties including lower barriers, support growth, and convergence to spatial average.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of scalar conservation laws with Coulomb interactions and nonlinear mobility, particularly focusing on entropy solutions and their properties across different diffusion regimes.

Method: Mathematical analysis using entropy solutions framework, studying rearrangement techniques for porous media regime, and proving convergence properties through analytical methods.

Result: Proved existence of entropy solutions and weak-strong uniqueness principle. Established lower barriers in fast diffusion regime, instantaneous support growth and waiting time phenomena in porous media regime, and exponential convergence to spatial average.

Conclusion: The paper provides comprehensive analysis of scalar conservation laws with Coulomb interactions, establishing fundamental properties of entropy solutions across different diffusion regimes and proving convergence behavior.

Abstract: We study a scalar conservation law on the torus in which the flux
$\mathbf{j}$ is composed of a Coulomb interaction and a nonlinear mobility:
$\mathbf{j} = -u^m\nabla\mathsf{g}\ast u$. We prove existence of entropy
solutions and a weak-strong uniqueness principle. We also prove several
properties shared among entropy solutions, in particular a lower barrier in the
fast diffusion regime $m\lt 1$. In the porous media regime $m\ge 1$, we study
the decreasing rearrangement of solutions, which allows to prove an
instantaneous growth of the support and a waiting time phenomenon. We also show
exponential convergence of the solutions towards the spatial average in several
topologies.

</details>


### [43] [A hitchhiker's guide to first-order elliptic boundary value problems](https://arxiv.org/abs/2510.16919)
*Christian Baer,Lashi Bandara*

Main category: math.AP

TL;DR: A user's guide to first-order elliptic boundary value problems on manifolds with compact boundary, covering existence, regularity, Fredholmness, and a general class of elliptic boundary conditions including pseudo-local ones.


<details>
  <summary>Details</summary>
Motivation: To provide mathematical researchers with practical operator methods for geometry and topology applications, particularly for those working with elliptic boundary value problems.

Method: Analysis of first-order elliptic operators on manifolds with compact boundary, focusing on existence, regularity, and Fredholm properties for a general class of elliptic boundary conditions.

Result: Development of a comprehensive framework for elliptic boundary value problems, including a relative index theorem and a new characterization of a subclass of elliptic boundary conditions.

Conclusion: The paper establishes foundational results for first-order elliptic boundary value problems, providing tools for mathematical analysis in geometry and topology contexts.

Abstract: To empower the mathematical hitchhiker wishing to use operator methods in
geometry and topology, we present this user's guide to first-order elliptic
boundary value problems. Existence, regularity, and Fredholmness are discussed
for general first-order elliptic operators on manifolds with compact boundary.
The focus is on a very general class of elliptic boundary conditions, which
contain those that are pseudo-local as a special case, yielding the relative
index theorem. A new characterisation of a subclass of elliptic boundary
conditions is also given.

</details>


### [44] [Dirichlet eigenfunction and heat kernel estimates on annular domains](https://arxiv.org/abs/2510.17091)
*Brian Chao,Laurent Saloff-Coste*

Main category: math.AP

TL;DR: Analysis of thin annular domains in polar coordinates showing they satisfy volume doubling and Poincaré inequalities with respect to the principal Dirichlet Laplacian eigenfunction measure, leading to sharp heat kernel estimates.


<details>
  <summary>Details</summary>
Motivation: Motivated by Euclidean boxes, the study examines thin annular domains in polar coordinates to understand their geometric and analytic properties, particularly focusing on domains with spherical bases that are inner uniform domains.

Method: Uses eigenfunction comparison techniques, small scale Poincaré inequalities, and discretization techniques to prove volume doubling and Poincaré inequalities with respect to the eigenfunction measure φ_U².

Result: Shows that annular domains satisfy volume doubling and Poincaré inequalities uniformly over all locations and scales, provides sharp Dirichlet heat kernel estimates, and gives two-sided bounds for first eigenfunction and eigenvalue.

Conclusion: The methods yield uniform results for all annuli in ℝⁿ and also imply uniform Neumann heat kernel estimates for thin annular domains, demonstrating robustness under domain perturbations.

Abstract: Motivated by Euclidean boxes, we consider "thin" annular domains of the form
$U=(a,b)\times U_0\subseteq \mathbb{R}^n$ in polar coordinates, where the
spherical base $U_0\subseteq \mathbb{S}^{n-1}$ is an inner uniform domain. We
show that, with respect to the measure $\varphi_U^2$ determined by the
principal Dirichlet Laplacian eigenfunction $\varphi_U$, such annular domains
satisfy volume doubling and Poincar\'e inequalities uniformly over all
locations and scales. This implies sharp Dirichlet heat kernel estimates
expressed in terms of $\varphi_U$. Our results hold uniformly over the
collection of all annuli in $\mathbb{R}^n$. We also give matching two-sided
bounds for the first Dirichlet Laplacian eigenfunction and eigenvalue for some
annular domains including annuli in $\mathbb{R}^n$. Moreover, we prove
eigenfunction inequalities for $\varphi_U$ under domain perturbations of $U$.
The proofs of our main results utilize eigenfunction comparison techniques due
to Lierl and the authors (arXiv:1210.4586, arXiv:2504.18783), small scale
$\varphi_U^2$-Poincar\'e inequalities, as well as a discretization technique of
Coulhon and Saloff-Coste. Finally, our methods also imply uniform Neumann heat
kernel estimates for thin annular domains.

</details>


### [45] [A unified theory of existence of suitable weak solutions to the 3D incompressible Navier-Stokes equations for non-decaying initial data](https://arxiv.org/abs/2510.17107)
*A. Balakrishna,I. Kukavica,W. S. Ożański*

Main category: math.AP

TL;DR: The paper studies weak solutions to Navier-Stokes equations in Morrey-type spaces defined by ball coverings of R^3, providing a priori estimates and local existence results under specific geometric conditions on the cover.


<details>
  <summary>Details</summary>
Motivation: To extend the analysis of Navier-Stokes equations beyond classical L^2 spaces to more general Morrey-type spaces that include previously studied settings like L^2_uloc and dyadic spaces, while simplifying the construction of weak solutions.

Method: Introduces a new non-divergence-free approach to construct weak solutions, using ball coverings of R^3 with specific geometric conditions (intersection bounds and size comparability) and derives a priori estimates.

Result: Proves local existence of weak solutions in two cases: when ball sizes satisfy |B|^{1/3} ≲ |x_B|^{1-ε} or |B|^{1/3} ≳ 1+|x_B|, and shows these solutions are suitable in the sense of Caffarelli-Kohn-Nirenberg.

Conclusion: The new approach simplifies existence proofs in L^2_uloc settings, works without vanishing at infinity in dyadic settings, and produces solutions compatible with partial regularity theory.

Abstract: We consider any cover $\mathscr{C}$ of $\mathbb{R}^3$ by balls of radius
bigger or equal $1$ satisfying two conditions: (i) any ball intersects at most
$\sigma>0$ other balls, and (ii) intersecting balls have comparable sizes. We
consider a natural Morrey-type space such that the $L^2_{\mathrm{uloc}}$
setting of Lemari\'e-Rieusset (Recent Developments in the Navier-Stokes
Problem, 2002) and the dyadic-type space considered by Bradshaw and Kukavica
(J. Math. Fluid Mech., 22(1), 2020) are particular cases. We provide a priori
estimates and prove local existence of weak solutions in two cases; first, when
there exists $\epsilon>0$ such that $|B|^{1/3} \lesssim |x_B|^{1-\epsilon}$ for
all $B\in \mathscr{C}$, where $x_B$ denotes the center of~$B$, or when
$|B|^{1/3} \gtrsim 1+ |x_B|$ for all $B\in\mathscr{C}$. In particular, we
introduce a new non-divergence-free approach to the construction of weak
solutions, which simplifies the existence proof in the $L^2_{\mathrm{uloc}}$
setting. In addition, for the dyadic setting, we do not require vanishing at
the spatial infinity. The constructed solutions are suitable in the sense of
Caffarelli, Kohn, and Nirenberg, thus allowing an application of the partial
regularity theory.

</details>


### [46] [Nonlinear Schrödinger equation with Ornstein-Uhlenbeck operator](https://arxiv.org/abs/2510.17178)
*Xueying Yu,Haitian Yue,Zehua Zhao*

Main category: math.AP

TL;DR: Analysis of nonlinear Schrödinger equations with anisotropic dispersion using Ornstein-Uhlenbeck operators in divergence and non-divergence forms, establishing Strichartz estimates, Morawetz estimates, and different blow-up/scattering behaviors.


<details>
  <summary>Details</summary>
Motivation: To capture waveguide-type dispersive behavior in a Euclidean setting and provide the first rigorous analysis of NLS with Ornstein-Uhlenbeck operators in both divergence and non-divergence forms.

Method: Study two models: (Model Div) with divergence form operator and (Model Non-Div) with non-divergence form operator. Establish Strichartz estimates, Gaussian-weighted Morawetz estimates, virial-type blow-up analysis, and scattering analysis.

Result: For Model Div: proved virial-type finite-time blow-up. For Model Non-Div: established global well-posedness and small data scattering in 2D quintic and 3D cubic cases.

Conclusion: The work provides comprehensive analysis of NLS with anisotropic Ornstein-Uhlenbeck confinement, revealing different dynamical behaviors between divergence and non-divergence forms, with applications to waveguide modeling.

Abstract: In this work, we introduce and study nonlinear Schr\"odinger equations (NLS)
with anisotropic dispersion, where the standard Laplacian acts on the Euclidean
variable \(x \in \mathbb{R}^d\), and an Ornstein-Uhlenbeck ($\mathcal{OU}$)
operator governs the confined direction \(\alpha \in \mathbb{R}\). We consider
models with two natural variants of $\mathcal{OU}$-induced confinement: (Model
Div) based on the divergence form \(\nabla_\alpha \cdot
(e^{-\frac{\alpha^2}{2}} \nabla_\alpha)\), and (Model Non-Div) based on the
non-divergence form \(\Delta_\alpha - \alpha \cdot \nabla_\alpha\). For both
models, we establish the Strichartz estimates and Gaussian-weighted Morawetz
estimates. In addition, for (Model Div), we prove a virial-type finite-time
blow-up result; for (Model Non-Div), we establish global well-posedness and
small data scattering in the 2D quintic and 3D cubic cases. The primary
motivation of this work is to capture waveguide-type dispersive behavior in a
Euclidean setting. To the best of our knowledge, this is the first rigorous
analysis of NLS with $\mathcal{OU}$ operators in both divergence and
non-divergence forms.

</details>


### [47] [A Morawetz type energy estimate for wave equation in $\mathbb{R}^2$ and application to elastic waves](https://arxiv.org/abs/2510.17180)
*Ningan Lai,Silu Yin,Yi Zhou*

Main category: math.AP

TL;DR: The paper introduces a modified scaling Morawetz multiplier for the inhomogeneous wave equation in 2D, providing an alternative proof of global existence for quasilinear wave equations with small compact data, and extends to certain multi-speed wave systems.


<details>
  <summary>Details</summary>
Motivation: To develop a weighted Morawetz type energy estimate for the inhomogeneous wave equation in 2D that can provide alternative proofs of global existence results and extend to more complex wave systems.

Method: Using a modified scaling Morawetz multiplier to generate a non-negative weighted Morawetz type energy estimate for the inhomogeneous wave equation in R^2.

Result: Successfully obtained a weighted Morawetz energy estimate that enables an alternative proof of global existence for quasilinear wave equations with small compactly supported data, and extends to certain multi-speed wave systems including admissible harmonic elastic wave systems.

Conclusion: The modified scaling Morawetz multiplier provides an effective approach for establishing global existence results in 2D wave equations, with applications extending to more complex wave systems with multiple speeds.

Abstract: In this paper, we introduce a modified scaling Morawetz multiplier, which
produces a weighted Morawetz type energy (non-negative) estimate for the
inhomogeneous wave equation in $\mathbb{R}^2$. With this estimate in hand, an
alternative proof of global existence for the Cauchy problem of quasilinear
wave equation with small and compactly supported data is given. What is more,
such weighted Morawetz type energy estimate also works for certain wave system
with multiple speeds, which can be used to prove global existence of some
admissible harmonic elastic wave system in $\mathbb{R}^2$.

</details>


### [48] [Semi-convex viscosity solutions of the special Lagrangian equation](https://arxiv.org/abs/2510.17202)
*Connor Mooney,Ravi Shankar*

Main category: math.AP

TL;DR: The paper proves smoothness and interior derivative estimates for viscosity solutions to the special Lagrangian equation with almost negative phases and small semi-convexity, shows these conditions are sharp, and provides new Liouville theorems and optimal Hessian estimates.


<details>
  <summary>Details</summary>
Motivation: To establish regularity results and optimal estimates for solutions of the special Lagrangian equation under specific phase and convexity conditions.

Method: Using viscosity solution theory and constructing counterexamples to demonstrate sharpness of conditions.

Result: Proved smoothness and interior derivative estimates, showed conditions are sharp via counterexamples, established new Liouville theorem for subcritical phase solutions, and found optimal exponential Hessian estimates.

Conclusion: The paper provides complete regularity theory for special Lagrangian equations with almost negative phases and small semi-convexity, with all conditions shown to be optimal.

Abstract: We prove smoothness and interior derivative estimates for viscosity solutions
to the special Lagrangian equation with almost negative phases and small enough
semi-convexity. We show by example that the range of phases we consider and the
semi-convexity condition are sharp. As an application, we find a new Liouville
theorem for entire such solutions of the special Lagrangian equation with
subcritical phase. We also find effective Hessian estimates with exponential
dependence, which we show to be optimal.

</details>


### [49] [Blow-up rate for the subcritical semilinear heat equation in non-convex domains](https://arxiv.org/abs/2510.17229)
*Hideyuki Miura,Jin Takahashi,Erbol Zhanpeisov*

Main category: math.AP

TL;DR: The paper proves nonexistence of type II blow-up for semilinear heat equations in non-convex/unbounded domains for energy subcritical range, resolving a long-standing open question.


<details>
  <summary>Details</summary>
Motivation: To resolve a long-standing open question dating back to the 1980s about blow-up behavior in semilinear heat equations, particularly for type II blow-up in non-convex and unbounded domains.

Method: Analysis of the semilinear heat equation $u_t=\Delta u+|u|^{p-1} u$ in possibly non-convex and unbounded domains, focusing on the energy subcritical range $(n-2)p<n+2$.

Result: Shows nonexistence of type II blow-up for possibly sign-changing solutions in the specified range, and deduces blow-up of the scaling critical norm.

Conclusion: The main result resolves a fundamental open problem in the field and provides important insights into blow-up behavior for semilinear heat equations.

Abstract: We consider the semilinear heat equation $u_t=\Delta u+|u|^{p-1} u$ in
possibly non-convex and unbounded domains. Our main result shows the
nonexistence of type II blow-up for possibly sign-changing solutions in the
energy subcritical range $(n-2)p<n+2$. This resolves a long-standing open
question dating back to the 1980s and also deduces the blow-up of the scaling
critical norm.

</details>


### [50] [Energy minimizers in a periodic phase transition model of light-matter interaction in nematic liquid crystals](https://arxiv.org/abs/2510.17291)
*Panayotis Smyrnelis,Marcel G. Clerc,Manuel Diaz-Zuniga,Michał Kowalczyk*

Main category: math.AP

TL;DR: The paper completes the study of global minimizers in a forced, non-autonomous 1D phase transition model. It examines kinks (topological structures) in systems with two-period forcing, showing they can have up to three zeros and combine previous kink types.


<details>
  <summary>Details</summary>
Motivation: Motivated by recent findings of new topological structures in light configurations, the research extends previous work on phase transition models to systems with two-period forcing terms.

Method: The study analyzes a forced, non-autonomous 1D phase transition model with two-period forcing. It examines the structure of global minimizers (kinks) and their dependence on forcing strength, identifying thresholds that determine kink configurations.

Result: The research shows that depending on forcing strength, at most two thresholds determine kink structure. These kinks combine previous types and can have up to three zeros. The behavior is consistent with the original model of matter-light interaction in nematic liquid crystals.

Conclusion: Complex phase transition types exist in systems with two-period forcing, with kinks exhibiting combined characteristics from previous models and potentially having multiple zeros, consistent with the physical model of nematic liquid crystal interactions.

Abstract: In this paper we complete the study of global minimizers of a forced, non
autonomous, one dimensional, phase transition model, initiated in [8].
Motivated by the recent findings in [9], revealing new configurations of
topological structures in light, we consider a forcing term having two periods.
We show that depending on the strength of the forcing, at most two thresholds
that determine the structure of the minimizers (kinks) are attained. These
kinks are now a combination of the previous types encountered in [8], and they
may have at most three zeros. The existence of these complex types of phase
transition follows from a periodic one dimensional model of matter-light
interaction in nematic liquid crystal based on a thin sample limit of the
Oseen-Frank energy. We show that the qualitative behaviour of global minimizers
is consistent with the original model.

</details>


### [51] [On the {$L^\infty $} norms of spectral projectors on shrinking intervals: the cases of some spheres of revolution and of the Euclidean disk](https://arxiv.org/abs/2510.17295)
*Ambre Chabert,Yves Colin de Verdìère*

Main category: math.AP

TL;DR: The paper proves polynomial improvement on L² to L∞ norm bounds for spectral projectors on generic simple spheres of revolution and Euclidean disks, using quantum integrability and Lagrangian oscillatory function analysis.


<details>
  <summary>Details</summary>
Motivation: To improve understanding of spectral projector norms on Riemannian surfaces, particularly for spheres of revolution and disks, by leveraging their quantum integrable structure and caustic distributions.

Method: Uses quantum integrability to express norms via joint eigenfunctions of (√-Δ, ∂/∂θ), analyzes these as Lagrangian oscillatory functions with fold-type caustics, applies BKW decay away from caustics, and reduces problem to counting estimates.

Result: Achieves polynomial improvement on L² to L∞ norm bounds for spectral projectors on specified bandwidths for generic simple spheres of revolution (away from poles/equator) and Euclidean disks (away from center up to boundary).

Conclusion: The approach combining quantum integrability, Lagrangian oscillatory function analysis, and caustic distribution counting provides effective bounds for spectral projector norms on these special Riemannian surfaces.

Abstract: Given a compact Riemannian surface $M$, with Laplace-Beltrami operator
$\Delta$, for $\lambda > 0$, let $P\_{\lambda,\lambda^{-\frac{1}{3}}}$ be the
spectral projector on the bandwidth $[\lambda-\lambda^{-\frac{1}{3}}, \lambda +
\lambda^{\frac{1}{3}}]$ associated to $\sqrt{-\Delta}$. We prove a polynomial
improvement on the $L^2 \to L^{\infty}$ norm of
$P\_{\lambda,\lambda^{-\frac{1}{3}}}$ for generic simple spheres of revolution
(away from the poles and the equator) and for the Euclidean disk away from its
center but up to the boundary. We use the Quantum Integrability of those
surfaces to express the norm in terms of a joint basis of eigenfunctions for
$\left(\sqrt{-\Delta}, \frac{1}{i}\frac{\partial}{\partial \theta}\right)$.
Then, we use that those eigenfunctions are asymptotically Lagrangian
oscillatory functions, each supported on a Lagrangian torus with fold-type
caustic. Thus, studying the distribution of the caustics, and using BKW decay
away from the caustics, we are able to reduce the problem to counting
estimates.

</details>


### [52] [Convergence to equilibrium of weak solutions to the Cahn--Hilliard equation with non-degenerate mobility and singular potential](https://arxiv.org/abs/2510.17296)
*Maurizio Grasselli,Andrea Poiatti*

Main category: math.AP

TL;DR: The paper proves convergence to equilibrium for Cahn-Hilliard equations with non-degenerate mobility and singular potentials, even in 3D cases where regularity is limited, and extends this to coupled Cahn-Hilliard-Navier-Stokes systems.


<details>
  <summary>Details</summary>
Motivation: Previous results lacked convergence proofs for weak solutions of Cahn-Hilliard equations in 3D cases, especially with limited regularity and continuous mobility functions. The coupled fluid systems also presented open convergence problems.

Method: Uses a novel method that works with minimal assumptions - only requiring existence of global weak solutions satisfying an energy inequality, without needing additional regularity results.

Result: Proves convergence to single equilibrium for Cahn-Hilliard equations with non-degenerate mobility and singular potentials in all dimensions, including the previously open 3D case. Also proves convergence for coupled Cahn-Hilliard-Navier-Stokes systems with unmatched densities and viscosities.

Conclusion: The novel method provides a general approach for proving convergence to equilibrium in systems where regularization properties are unknown or unlikely, opening possibilities for analyzing other complex models with limited regularity.

Abstract: We consider the classical initial and boundary value problem for the
Cahn--Hilliard equation with non-degenerate mobility and singular (e.g.,
logarithmic) potential. We prove that any weak solution converges to a single
equilibrium using only minimal assumptions, that is, the existence of a global
weak solution which satisfies an energy inequality. This result appears to be
new in the literature and also holds in the three-dimensional case, which was
an open problem due to the lack of regularity results, especially when the
mobility is just a continuous function. We then prove the same result for a
Cahn--Hilliard-Navier--Stokes type system with unmatched densities and
viscosities proposed by Abels, Garcke, and Gr\"un (Math. Models Methods Appl.
Sci. 22, 2012), always assuming a non-degenerate mobility. We expect that this
novel method can be used to analyze the same issue for other models where the
regularization properties of the solutions are unknown or unlikely.

</details>


### [53] [Sobolev instability for perturbations of periodic transport equations](https://arxiv.org/abs/2510.17350)
*Gabriel Rivière,Maria Teresa Rotolo*

Main category: math.AP

TL;DR: Linear and time-dependent perturbations of periodic transport equations on the 2D torus cause exponential divergence of Sobolev norms for generic initial data, with similar results in higher dimensions under Morse-Smale conditions.


<details>
  <summary>Details</summary>
Motivation: To understand how perturbations affect the stability of periodic transport equations and whether they lead to exponential growth of Sobolev norms.

Method: Normal form procedure combined with analysis of Sobolev instabilities for time-dependent perturbations of Morse-Smale transport equations, using microlocal analysis and hyperbolic dynamics techniques.

Result: For generic perturbations, there exists a large class of initial data whose Sobolev norms diverge exponentially fast in 2D, and similar results hold in higher dimensions under Morse-Smale assumptions.

Conclusion: Perturbations of periodic transport equations generically lead to exponential instability in Sobolev norms, with the analysis extending to higher dimensions under appropriate dynamical conditions.

Abstract: We consider linear and time-dependent perturbations of periodic transport
equations on the two-dimensional torus. For generic perturbations, we prove the
existence of a large class of initial data whose Sobolev norms diverge
exponentially fast. In higher dimensions, this remains true under a Morse-Smale
assumption on the resonant part of the perturbation. In both cases, this is
achieved by a normal form procedure and by studying Sobolev instabilities for
time-dependent perturbations of Morse-Smale transport equations. The latter are
analyzed on general compact manifolds using techniques from microlocal analysis
and hyperbolic dynamics.

</details>


### [54] [On the blow-up of solutions to a Nakao-type problem with a time-dependent damping term](https://arxiv.org/abs/2510.17368)
*Yuequn Li,Alessandro Palmieri*

Main category: math.AP

TL;DR: The paper studies a semilinear weakly coupled system of wave equations with power nonlinearities, focusing on blow-up behavior and lifespan estimates for solutions in both scale-invariant and scattering producing damping cases.


<details>
  <summary>Details</summary>
Motivation: To analyze the blow-up behavior and lifespan of solutions for coupled wave equations with time-dependent damping coefficients, comparing results with classical damping cases.

Method: Applied an iteration argument to derive blow-up results and upper bound estimates for the lifespan of solutions in two damping scenarios: scale-invariant and scattering producing cases.

Result: In the scale-invariant case, found a shift in the space dimension for the blow-up region compared to classical damping; in the scattering producing case, obtained the same blow-up region as the classical Nakao problem.

Conclusion: The study reveals how different types of time-dependent damping coefficients affect the blow-up behavior of weakly coupled wave equation systems, with scale-invariant damping causing dimensional shifts while scattering producing damping yields classical results.

Abstract: In this paper, we study a semilinear weakly coupled system of wave equations
with power nonlinearities. More precisely, we couple (through the nonlinear
terms) a wave equation and a damped wave equation with a time-dependent
coefficient for the damping term. For the coefficient of the damping term we
consider two cases: the scale-invariant case and the scattering producing case.
By applying an iteration argument, we get a blow-up result and upper bound
estimates for the lifespan of the solutions. In the scale-invariant case, we
obtain a shift of the space dimension in the blow-up region for the same weakly
coupled system with a classical damping (i.e. with a constant coefficient),
while for the scattering producing case we find the same blow-up region as for
the classical Nakao problem.

</details>


### [55] [Quasilinear Equations with Neumann Boundary Conditions](https://arxiv.org/abs/2510.17374)
*Annamaria Canino,Simone Mauro*

Main category: math.AP

TL;DR: The paper proves multiplicity of non-constant weak solutions for a quasilinear elliptic equation with Neumann boundary conditions, and establishes boundedness of solutions under strong growth assumptions.


<details>
  <summary>Details</summary>
Motivation: To study the existence of multiple non-constant solutions for quasilinear elliptic equations with general subcritical and superlinear growth conditions, extending previous results for more specific cases.

Method: Using variational methods and analysis of weak solutions in H^1(Ω) for the quasilinear elliptic equation with Neumann boundary conditions, employing techniques from critical point theory.

Result: Existence of multiple non-constant weak solutions is proven, and all weak solutions are shown to be bounded under appropriate growth conditions on the nonlinear term.

Conclusion: The paper establishes multiplicity results for quasilinear elliptic equations with general growth conditions, providing both existence of multiple solutions and regularity properties (boundedness) of these solutions.

Abstract: We prove a multiplicity result for non-constant weak solutions $u \in
H^1(\Omega)$ for the quasilinear elliptic equation \[ \begin{cases}
\displaystyle-\text{div}(A(x,u)\nabla u) + \frac{1}{2} D_sA(x,u)\nabla u \cdot
\nabla u = g(x,u) - \lambda u & \text{in } \Omega \\ A(x,u)\nabla u \cdot \eta
= 0 & \text{on } \partial \Omega \end{cases} \] where $\lambda \in \mathbb{R}$,
$ \Omega$ is a bounded lipschitz domain, $ \eta $ is the outward normal to the
boundary $ \partial \Omega $, and $g(x,u)$ is a Carath\'eodory function that
satisfies a general subcritical (and superlinear) growth condition. We also
prove that any weak solution is bounded under a strong growth assumption.

</details>


### [56] [A unified relative entropy framework for macroscopic limits of Vlasov--Fokker--Planck equations](https://arxiv.org/abs/2510.17455)
*Young-Pil Choi,Jinwook Jung*

Main category: math.AP

TL;DR: A unified relative entropy framework for kinetic equations with Riesz interactions and Fokker-Planck relaxation, providing both strong and weak convergence results toward macroscopic limits.


<details>
  <summary>Details</summary>
Motivation: To develop a robust stability theory that handles nonlocal forces and singular scalings in kinetic equations, extending the scope of relative entropy methods to settings where traditional approaches face limitations.

Method: Combines entropy dissipation, Fisher-information control, and modulated interaction energies into a unified framework. Uses quantitative relative entropy estimates for strong convergence and bounded Lipschitz topologies for weak convergence.

Result: Establishes quantitative convergence toward three prototypical limits: diffusive limit (drift-diffusion equation), high-field limit (aggregation equation), and strong magnetic field limit (generalized surface quasi-geostrophic equation). Shows relative entropy works for both well-prepared and mildly prepared initial states.

Conclusion: Relative entropy serves as a unifying tool connecting microscopic dissipation with both strong and weak macroscopic convergence, providing a new mechanism to handle singular scalings and nonlocal interactions in kinetic equations.

Abstract: We develop a unified relative entropy framework for macroscopic limits of
kinetic equations with Riesz-type interactions and Fokker-Planck relaxation.
The method combines entropy dissipation, Fisher-information control, and
modulated interaction energies into a robust stability theory that yields both
strong and weak convergence results. For the strong convergence, we establish
quantitative relative entropy estimates toward macroscopic limits under
well-prepared data, extending the scope of the method to settings where
nonlocal forces and singular scalings play a decisive role. For the weak
convergence, we prove that quantitative convergence propagates in bounded
Lipschitz topologies, even when the initial relative entropy diverges with
respect to the singular scaling parameter. This dual perspective shows that
relative entropy provides not only a tool for strong convergence, but also a
new mechanism to handle mildly prepared initial states. We establish
quantitative convergence toward three prototypical limits: the diffusive limit
leading to a drift-diffusion equation, the high-field limit yielding the
aggregation equation in the repulsive regime, and the strong magnetic field
limit producing a generalized surface quasi-geostrophic equation. The analysis
highlights the unifying role of relative entropy in connecting microscopic
dissipation with both strong and weak macroscopic convergence.

</details>


### [57] [Global rigidity of two-dimensional bubbles](https://arxiv.org/abs/2510.17557)
*Lukas Niebel*

Main category: math.AP

TL;DR: Global rigidity of circular hollow vortices with surface tension is proven for small Weber numbers, supporting Crowdy and Wegmann's conjecture.


<details>
  <summary>Details</summary>
Motivation: To study stationary hollow vortices with surface tension in 2D, which are solutions to an overdetermined elliptic free boundary value problem involving mean curvature and Neumann boundary conditions.

Method: Analysis of an overdetermined elliptic free boundary value problem in exterior domains, variational approach considering perimeter and logarithmic potential energy, and linear analysis of close-to-circular solutions.

Result: Proved global rigidity of the circle for small Weber numbers, confirming the conjecture that circular shapes are the only solutions in this regime.

Conclusion: The circle is globally rigid for small Weber numbers, and linear analysis provides insights into near-circular solutions for both the free boundary and variational problems.

Abstract: We study stationary hollow vortices with surface tension in two dimensions.
Such objects are solutions to an overdetermined elliptic free boundary value
problem in an exterior domain, where an additional condition involving the mean
curvature and the Neumann trace on the boundary is imposed. We prove global
rigidity of the circle for small Weber numbers, supporting a conjecture of
Crowdy and Wegmann. This elliptic problem describes critical points of the sum
of perimeter and the logarithmic potential energy of bounded sets. The
variational problem is ill-posed in general, but we recover the global rigidity
for small Weber numbers in the class of sets bounded by a Jordan curve. A
linear analysis gives precise insights into close-to-circular solutions for
both problems.

</details>


### [58] [Relaxation for highly discontinuous, possibly unbounded, integral functionals](https://arxiv.org/abs/2510.17577)
*Tommaso Bertin,Giulia Treu*

Main category: math.AP

TL;DR: The paper proves that the Lavrentiev phenomenon does not occur for certain non-convex, non-continuous, unbounded Lagrangians in variational problems.


<details>
  <summary>Details</summary>
Motivation: To understand when the Lavrentiev phenomenon (gap between infima over different function spaces) occurs in variational problems with non-standard Lagrangians.

Method: Analysis of the functional F(u) = ∫f(∇u)dx over Lipschitz domains, using superlinear Borel functions f with very weak assumptions.

Result: Under superlinearity and weak assumptions on f, the Lavrentiev phenomenon does not occur, even for non-continuous, non-convex, unbounded Lagrangians.

Conclusion: The Lavrentiev phenomenon can be avoided for a broad class of Lagrangians, including pathological cases, when appropriate growth conditions are satisfied.

Abstract: We consider the functional \[ F(u)=\int_{\Omega} f(\nabla u)\,dx\qquad
u\in\varphi+W^{1,1}_0(\Omega) \] where $\Omega$ is a Lipschitz bounded open set
of $\R^N$, $f:\R^N\to\R\cup \{+\infty\}$ is a superlinear Borel function,
$\varphi\in W^{1,\infty}(\Omega)$.
  We prove that, if $f$ is superlinear and satisfies very weak assumptions,
then the Lavrentiev phenomenon does not occur. We underline that our
assumptions include the case of non continuous, non convex, and unbounded
Lagrangians.

</details>


### [59] [Formation of clusters and coarsening in weakly interacting diffusions](https://arxiv.org/abs/2510.17629)
*Nicolai Gerber,Rishabh Gvalani,Martin Hairer,Greg Pavliotis,André Schlichting*

Main category: math.AP

TL;DR: The paper studies clustering in weakly interacting diffusions with attractive potentials on 1D torus, linking it to discontinuous phase transitions in mean-field PDEs, and analyzes different coarsening mechanisms between particle systems and PDEs.


<details>
  <summary>Details</summary>
Motivation: To understand clustering behavior in weakly interacting diffusions and its connection to phase transitions in mean-field PDEs, particularly examining how different systems exhibit coarsening through different mechanisms.

Method: Uses strict Riesz rearrangement inequality to prove global minimizers are uniform or single-cluster states, analyzes timescales for particle systems vs PDEs, introduces new mass exchange model, and conducts numerical experiments.

Result: Shows that particle systems coarsen via coalescence and diffusive mass exchange, while PDE clusters are immobile and coarsen only through mass exchange, with the PDE exhibiting dynamical metastability.

Conclusion: The proposed mass exchange model for PDE clustering is validated through careful numerical experiments, demonstrating different coarsening behaviors between particle systems and mean-field PDEs.

Abstract: This paper studies the clustering behavior of weakly interacting diffusions
under the influence of sufficiently localized attractive interaction potentials
on the one-dimensional torus. We describe how this clustering behavior is
closely related to the presence of discontinuous phase transitions in the
mean-field PDE. For local attractive interactions, we employ a new variant of
the strict Riesz rearrangement inequality to prove that all global minimizers
of the free energy are either uniform or single-cluster states, in the sense
that they are symmetrically decreasing.
  We analyze different timescales for the particle system and the mean-field
(McKean-Vlasov) PDE, arguing that while the particle system can exhibit
coarsening by both coalescence and diffusive mass exchange between clusters,
the clusters in the mean-field PDE are unable to move and coarsening occurs via
the mass exchange of clusters. By introducing a new model for this mass
exchange, we argue that the PDE exhibits dynamical metastability. We conclude
by presenting careful numerical experiments that demonstrate the validity of
our model.

</details>


### [60] [The complex Ginzburg-Landau equation on a finite interval and chaos suppression via a finite-dimensional boundary feedback stabilizer](https://arxiv.org/abs/2510.17635)
*Dionyssios Mantzavinos,Türker Özsarı,Kemal Cem Yılmaz*

Main category: math.AP

TL;DR: This paper studies the well-posedness and boundary stabilization of the complex Ginzburg-Landau equation on a finite interval with Dirichlet-Neumann boundary conditions, using a nonlocal controller based on finite Fourier modes.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for controlling the complex Ginzburg-Landau equation, which models various physical phenomena including superconductivity and pattern formation, through boundary feedback stabilization.

Method: Used the unified transform (Fokas method) to derive linear estimates, designed a nonlocal controller using finite Fourier modes, and employed spatiotemporal estimates and Volterra-type integral transforms for analysis.

Result: Established local and global well-posedness, designed an effective boundary controller that stabilizes the system using finite Fourier modes, determined the minimum number of modes required for stabilization, and validated results numerically.

Conclusion: The complex Ginzburg-Landau equation can be effectively stabilized using boundary feedback control with finite Fourier modes, with theoretical results supported by numerical simulations and rigorous mathematical analysis.

Abstract: In this paper, we study the well-posedness and boundary stabilization of the
initial-boundary value problem for the complex Ginzburg-Landau (CGL) equation
on a finite interval. First, we establish a local well-posedness theory for the
open loop model in $L^2$-based fractional Sobolev spaces in the case of
Dirichlet-Neumann type inhomogeneous mixed boundary conditions. This local
well-posedness result is based on linear estimates derived by using the weak
solution formula obtained via the unified transform (also known as the Fokas
method). Next, we study the global well-posedness properties of the open loop
model in presence of inhomogeneous boundary conditions. Then, we turn our
attention to the rapid boundary feedback stabilization problem and design a
nonlocal controller which uses a finite number of Fourier modes of the state of
solution. This design relies on the fact that solutions of the CGL equation can
be separated into a slow, finite-dimensional component and a rapidly decaying
tail, with the former primarily governing long-term behavior. We determine the
necessary number of modes required to stabilize the system at a specified rate.
Additionally, we identify the minimum number of modes that ensure stabilization
at an unspecified decay rate. These theoretical results are validated by
numerical simulations. The spatiotemporal estimates established in the first
part of the paper are also employed to obtain local solutions of the controlled
system. The existence of global energy solutions follows from stabilization
estimates, while uniqueness follows from the uniqueness of an associated
initial-boundary value problem with homogeneous boundary conditions whose
solutions are in correspondence with the solutions of the original system
through a bounded invertible Volterra-type integral transform on Sobolev
spaces.

</details>


### [61] [Non-CMC solutions to a Lichnerowicz equation in the Einstein-scalar field theory on closed manifolds](https://arxiv.org/abs/2510.17669)
*Bartosz Bieganowski,Pietro d'Avenia,Jacopo Schino,Daniel Strzelecki*

Main category: math.AP

TL;DR: Existence of positive bounded solutions to Lichnerowicz equation in Einstein-scalar field theory on closed manifolds with non-constant mean curvature, addressing supercritical and singular terms.


<details>
  <summary>Details</summary>
Motivation: To study the Lichnerowicz equation in Einstein-scalar field theory on closed manifolds, particularly when non-constant mean curvature introduces supercritical terms alongside singular ones.

Method: Employ a recent fixed-point argument involving sub- and supersolutions to prove existence of solutions.

Result: Proved existence of a positive and essentially bounded solution to the Lichnerowicz equation under the specified conditions.

Conclusion: The paper establishes solution existence while also identifying conditions on equation coefficients that prevent existence of positive classical solutions.

Abstract: In the paper, we prove the existence of a positive and essentially bounded
solution to a Lichnerowicz equation in the Einstein-scalar field theory on a
closed manifold with non-constant mean curvature. In particular, the
non-constant mean curvature gives rise to supercritical terms in the equation,
on top of singular ones. We employ a recent fixed-point argument, which
involves sub- and supersolutions. Additionally, we provide several conditions
on the coefficients in the equation that prevent the existence of positive
classical solutions.

</details>


### [62] [Flat Standing Sphere Blow-up Solutions for the Nonlinear Heat Equation](https://arxiv.org/abs/2510.17718)
*Senhao Duan*

Main category: math.AP

TL;DR: Existence of singular standing sphere blow-up solution for nonlinear heat equation with radial symmetry, developing finite-time singularity on fixed-radius sphere with flat blow-up profile.


<details>
  <summary>Details</summary>
Motivation: To prove the existence of singular blow-up solutions that develop singularities specifically on spherical surfaces rather than at single points, extending understanding of blow-up phenomena in nonlinear PDEs.

Method: Refinement of Merle and Zaag's method that reduces infinite-dimensional dynamics to finite-dimensional problem in radial case, with explicit asymptotics near the singular ring.

Result: Successfully constructed a solution that develops finite-time singularity on a fixed-radius sphere with flat blow-up profile, remaining regular elsewhere.

Conclusion: The paper establishes the existence of standing sphere blow-up solutions, providing new insights into blow-up patterns beyond point singularities in nonlinear heat equations.

Abstract: In this paper, we prove the existence of a singular standing sphere blow-up
solution for the nonlinear heat equation with radial symmetry. This solution
develops a finite-time singularity on a fixed-radius sphere and exhibits a flat
blow-up profile. Our construction refines the method developed by Merle and
Zaag \cite{MZJEMS24} which reduces the infinite-dimensional dynamics to a
finite-dimensional problem in radial case. The solution satisfies explicit
asymptotics near the singular ring and remains regular elsewhere.

</details>


### [63] [Asymptotic stability of solitary waves for the 1D focusing cubic Schrödinger equation](https://arxiv.org/abs/2510.17763)
*Yongming Li*

Main category: math.AP

TL;DR: Full asymptotic stability of solitary waves for 1D cubic Schrödinger equation under small perturbations in weighted Sobolev spaces.


<details>
  <summary>Details</summary>
Motivation: To establish complete asymptotic stability of solitary wave solutions for the focusing cubic Schrödinger equation, building on previous results and addressing challenges from slow decay of radiation terms.

Method: Integrates space-time resonances approach using distorted Fourier transform with modulation techniques, employs normal form transformations to handle null structures in quadratic nonlinearities, and develops local smoothing estimates with moving centers.

Result: Proves modified scattering for radiation term and convergence for modulation parameters, achieving full asymptotic stability despite threshold resonances causing slow local decay.

Conclusion: Successfully establishes complete asymptotic stability of solitary waves through a comprehensive approach combining resonance analysis, modulation techniques, and specialized estimates to overcome slow decay challenges.

Abstract: We establish the full asymptotic stability of solitary wave solutions for the
1D focusing cubic Schr\"odinger equation on the line under small perturbations
in weighted Sobolev spaces, building upon our results in [58]. The proof
integrates the space-time resonances approach, based on the distorted Fourier
transform, with modulation techniques to show modified scattering for the
radiation term and convergence for the modulation parameters. A key challenge
throughout the nonlinear analysis is the slow local decay of the radiation
term, caused by threshold resonances in the linearized operator. The presence
of favorable null structures in the quadratic nonlinearities mitigates this
problem through the use of normal form transformations. Another essential step
in the proof involves developing a variant of the local smoothing estimate that
incorporates a moving center.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [64] [Design of Magnetic Lattices with Quantum Optimization Algorithms](https://arxiv.org/abs/2510.16349)
*Zekeriya Ender Eğer,Waris Khan,Priyabrata Maharana,Kandula Eswara Sai Kumar,Udbhav Sharma,Abhishek Chopra,Rut Lineswala,Pınar Acar*

Main category: physics.comp-ph

TL;DR: This paper uses quantum optimization (BQP) to identify magnetic spin distributions in ferromagnetic materials by minimizing free energy, validated against genetic algorithms for small lattices and extended to large 50×50 systems.


<details>
  <summary>Details</summary>
Motivation: To solve the computationally intractable problem of identifying magnetic spin distributions in large ferromagnetic lattices, where conventional methods become impractical due to the exponential growth of design variables (2^(n×n)).

Method: Construct magnetic lattices and compute free energy using an Ising model with spin-spin interactions and external field effects. Employ quantum optimization algorithm (BQP) to solve the high-dimensional optimization problem, validated against genetic algorithms for smaller systems.

Result: The BQP approach successfully identifies magnetic spin distributions and is validated against genetic algorithm solutions for smaller lattices. The method is extended to large-scale 50×50 lattice systems where traditional methods fail.

Conclusion: Quantum optimization (BQP) provides an effective solution for identifying magnetic spin distributions in large ferromagnetic systems, overcoming computational limitations of conventional methods for high-dimensional optimization problems.

Abstract: This article investigates the identification of magnetic spin distributions
in ferromagnetic materials by minimizing the system's free energy. Magnetic
lattices of varying sizes are constructed, and the free energy is computed
using an Ising model that accounts for spin-to-spin neighbor interactions and
the influence of an external magnetic field. The problem reduces to determining
the state of each spin, either up or down, leading to an optimization problem
with $2^{n \times n}$ design variables for an $n \times n$ lattice. To address
the high-dimensional and computationally intractable nature of this problem,
particularly for large domains, we employ a quantum optimization algorithm,
BQP. The BQP results are first validated against solutions obtained using a
genetic algorithm for smaller lattices. Finally, the approach is extended to
large-scale systems, including $50 \times 50$ lattices, where conventional
methods become impractical.

</details>


### [65] [Extended phase-space symplectic integration for electron dynamics](https://arxiv.org/abs/2510.16542)
*Francois Mauger,Cristel Chandre*

Main category: physics.comp-ph

TL;DR: Extended phase-space symplectic integration is applied to simulate electron dynamics in plasma physics and quantum chemistry, with stability analysis and accuracy metrics developed for broad application.


<details>
  <summary>Details</summary>
Motivation: To enable efficient and accurate simulation of electron dynamics in both classical plasma systems and quantum chemical systems using symplectic integration methods.

Method: Extended phase-space symplectic integration with high-order symplectic split-operator schemes, including stability analysis and on-the-fly accuracy estimation metrics.

Result: Successfully applied the method to two classes of electron dynamics: classical charged particle motion in magnetic fields with turbulent potentials, and Kohn-Sham time-dependent density-functional theory.

Conclusion: The work enables broad application of symplectic split-operator integration for Hamiltonian systems with both finite and infinite degrees of freedom.

Abstract: We investigate the use of extended phase-space symplectic integration [M.
Tao, Phys. Rev. E 94, 043303 (2016)] for simulating two different classes of
electron dynamics. The first one, with one and a half degrees of freedom, comes
from plasma physics and describes the classical dynamics of a charged particle
in a strong, constant, and uniform magnetic field perturbed by a turbulent
electrostatic potential. The second one, with an infinite number of degrees of
freedom, comes from physical chemistry and corresponds to Kohn-Sham
time-dependent density-functional theory. For both we lay out the extension
procedure and stability condition for numerical integration of the dynamics
using high-order symplectic split-operator schemes. We also identify a
computationally inexpensive metric that can be used for on-the-fly estimation
of the accuracy of simulations. Our work paves the way for broad application of
symplectic split-operator integration of classical and quantum Hamiltonian
systems with finite and infinite number of degrees of freedom.

</details>


### [66] [Scalable cell filter nudged elastic band (CFNEB) for large-scale transition-path calculations](https://arxiv.org/abs/2510.16721)
*Qiuhan Jia,Jiuyang Shi,Jian Sun*

Main category: physics.comp-ph

TL;DR: A scalable cell-filter nudged elastic band (CFNEB) method is developed to efficiently calculate transition pathways in large systems up to 10^5 atoms, overcoming computational limitations of conventional NEB methods.


<details>
  <summary>Details</summary>
Motivation: Conventional NEB methods face steep computational scaling with system size, making nucleation-type transitions in large supercells practically inaccessible.

Method: Combines deformation-based cell filtering (treating lattice vectors as generalized coordinates while removing rotational degrees of freedom) with adaptive image insertion/deletion strategy for dynamic path refinement. Implemented in ASE environment and GPU-accelerated version using GPUMD.

Result: Achieves throughput of ~10^6 atom·steps per second on consumer GPUs. Successfully applied to Ti3O5 β-λ transition and graphite-to-diamond transformation, capturing both concerted pathways and spontaneous symmetry breaking toward nucleated mechanisms in large cells.

Conclusion: CFNEB establishes a practical route for exploring realistic transition mechanisms in large-scale solid-state systems.

Abstract: The nudged elastic band (NEB) method is one of the most widely used
techniques for determining minimum-energy reaction pathways and activation
barriers between known initial and final states. However, conventional
implementations face steep computational scaling with system size, which makes
nucleation-type transitions in realistically large supercells practically
inaccessible. In this work, we develop a scalable cell-filter nudged elastic
band (CFNEB) framework that enables efficient transition-path calculations in
systems containing up to $10^5$ atoms. The method combines a deformation-based
cell filtering scheme, which treats lattice vectors as generalized coordinates
while removing spurious rotational degrees of freedom, with an adaptive image
insertion and deletion strategy that dynamically refines the reaction path. We
implement CFNEB both within the ASE environment and in a fully GPU-accelerated
version using the Graphics Processing Units Molecular Dynamics (GPUMD) engine,
achieving throughput on the order of $10^6$ atom$\cdot$steps per second on
consumer GPUs. We demonstrate the method on two representative systems: the
layer-by-layer $\beta$-$\lambda$ transition in $Ti_3O_5$ and the
nucleation-driven graphite-to-diamond transformation. These examples illustrate
that CFNEB not only reproduces known concerted pathways but also captures
spontaneous symmetry breaking toward nucleated mechanisms when the simulation
cell is sufficiently large. Our results establish CFNEB as a practical route to
exploring realistic transition mechanisms in large-scale solid-state systems.

</details>


### [67] [Thermal Conductivity Estimation of Thermoelectric Materials with Uncertainty Quantification Using Bayesian Physics-Informed Neural Networks](https://arxiv.org/abs/2510.16723)
*Hyeonbin Moon,Hanbin Cho,Wabi Demeke,Byungki Ryu,Seunghwa Ryu*

Main category: physics.comp-ph

TL;DR: A physics-informed deep learning framework that infers temperature-dependent thermal conductivity from sparse electric potential measurements alone, using both deterministic and Bayesian PINN approaches for robust property estimation with uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Characterizing temperature-dependent thermal conductivity is challenging due to strong temperature variation and difficulty in reliable heat flow measurement under experimental conditions, especially when direct temperature sensing is impractical.

Method: Developed a physics-informed neural network (PINN) that embeds coupled thermoelectric transport equations as soft constraints to simultaneously recover spatial temperature, voltage, and conductivity profiles without temperature data. Extended to Bayesian PINN using Hamiltonian Monte Carlo sampling for probabilistic parameter modeling and uncertainty quantification.

Result: Deterministic PINN achieves accurate inference under noise-free conditions but degrades with measurement noise. Bayesian PINN preserves predictive accuracy under noise, provides credible intervals for uncertainty quantification, and reveals inference bias for uncertainty-aware interpretation of material properties.

Conclusion: The framework establishes a scalable and generalizable alternative to conventional methods for determining temperature-dependent properties, offering physics-consistent and risk-aware property inference for thermoelectric systems and other functional materials.

Abstract: Characterizing the temperature-dependent thermal conductivity is challenging
because the property varies strongly with temperature and reliable heat flow
measurement, not just temperature sensing, is difficult under experimental
conditions. Here, we present a physics informed deep learning framework that
infers conductivity solely from sparse electric potential measurements. We
first develop a deterministic physics-informed neural network (PINN) that
embeds coupled thermoelectric transport equations as soft constraints, enabling
simultaneous recovery of spatial temperature, voltage, and conductivity
profiles without temperature data. The deterministic PINN achieves accurate
inference under noise-free conditions, yet its predictions degrade when
measurement noise is introduced. To address this, we extend the framework to a
Bayesian PINN, which models network parameters probabilistically and employs
Hamiltonian Monte Carlo (HMC) sampling for posterior inference. This extension
produces robust thermal conductivity estimates and, importantly, provides
credible intervals that quantify uncertainty from sparse and noisy data.
Numerical experiments confirm that the Bayesian PINN not only preserves
predictive accuracy under noise but also reveals inference bias and enables
uncertainty aware interpretation of material properties. Together, the
deterministic and Bayesian formulations establish a scalable and generalizable
alternative to conventional methods for determining temperature-dependent
properties, offering physics- consistent and risk-aware property inference for
thermoelectric systems and other functional materials where direct temperature
sensing is impractical

</details>


### [68] [Large-scale stochastic propagation method beyond the sequential approach](https://arxiv.org/abs/2510.17432)
*Zhichang Fu,Yunhai Li,Weiqing Zhou,Shengjun Yuan*

Main category: physics.comp-ph

TL;DR: A concurrent O(N) stochastic propagation method that eliminates sequential computation of intermediate states, achieving up to 10x speedup for billion-atom systems while maintaining precision.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of conventional sequential O(N) stochastic propagation methods that suffer from time step constraints and computational inefficiency in large-scale first-principles calculations.

Method: Introduces a concurrent strategy that minimizes information redundancy by eliminating sequential computation of intermediate states. Implements state-, moment-, and energy-based versions that work within the Nyquist-Shannon sampling theorem framework.

Result: Achieves up to an order-of-magnitude speedup compared to conventional methods, demonstrated through systematic benchmarking on one billion atoms within tight-binding models. Enables rapid computation of electronic, optical, and transport properties.

Conclusion: The concurrent method provides a performance breakthrough for large-scale stochastic propagation algorithms and offers valuable insights for enhancing other time-propagation algorithms, including those used in stochastic density functional theory.

Abstract: The $O(N)$ stochastic propagation method, which relies on the numerical
solution of the time-dependent Schr\"odinger equation using random initial
states, is widely used in large-scale first-principles calculations. In this
work, we eliminate the conventional sequential computation of intermediate
states by introducing a concurrent strategy that minimizes information
redundancy. The new method, in its state-, moment-, and energy-based
implementations, not only surpasses the time step constraint of sequential
propagation but also maintains precision within the framework of the
Nyquist-Shannon sampling theorem. Systematic benchmarking on one billion atoms
within the tight-binding model demonstrates that our new concurrent method
achieves up to an order-of-magnitude speedup, enabling the rapid computation of
a wide range of electronic, optical, and transport properties. This performance
breakthrough offers valuable insights for enhancing other time-propagation
algorithms, including those employed in large-scale stochastic density
functional theory.

</details>


### [69] [Performance of artificial neural networks in an inverse problem of laser beam diagnostics](https://arxiv.org/abs/2510.17507)
*Karol Pietrak,Radosław Muszyński,Adam Marek,Piotr Łapka*

Main category: physics.comp-ph

TL;DR: A method using artificial neural networks to identify unknown spatio-temporal heat flux distributions from laser beam excitation on aluminum plates, with comparison to Levenberg-Marquardt method.


<details>
  <summary>Details</summary>
Motivation: To develop an effective laser beam profiling method by identifying unknown boundary heat flux distributions resulting from pulsed laser excitation on thin aluminum plates.

Method: Utilizes artificial neural networks (Keras, Tensorflow) trained on temperature distributions generated with ANSYS Fluent solver, with focus on optimizing neural network hyperparameters.

Result: The paper presents numerical verification results comparing neural network identification performance with the previously used Levenberg-Marquardt method.

Conclusion: The neural network approach shows promise as part of a new laser beam profiling method, with performance comparisons made against traditional optimization methods.

Abstract: Results are presented for the numerical verification of a method devised to
identify an unknown spatio-temporal distribution of heat flux that occurs at
the surface of thin aluminum plate, as a result of pulsed, high-power laser
beam excitation. The presented identification of boundary heat flux function is
a part of newly-proposed laser beam profiling method and utilizes artificial
neural networks trained on temperature distributions generated with the ANSYS
Fluent solver. The paper focuses on the selection of the most effective neural
network hyperparameters (Keras, Tensorflow) and compares the results of neural
network identification with Levenberg-Marquardt method used earlier and
discussed in our previous articles.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [70] [Physics insights from a large-scale 2D UEDGE simulation database for detachment control in KSTAR](https://arxiv.org/abs/2510.16199)
*Menglong Zhao,Xueqiao Xu,Ben Zhu,Thomas Rognlien,Xinxing Ma,William Meyer,KyuBeen Kwon,David Eldon,Nami Li,Hyungho Lee,Junghoo Hwang*

Main category: physics.plasm-ph

TL;DR: A large-scale UEDGE simulation database for KSTAR reveals detachment onset at Te ~3-4 eV, shows weaker impurity sensitivity than 1D models, and identifies distinctive divertor asymmetries. Time-dependent simulations capture plasma response dynamics suitable for control applications.


<details>
  <summary>Details</summary>
Motivation: To study detachment physics in KSTAR and develop surrogate models for control applications by creating a comprehensive simulation database.

Method: Generated nearly 70,000 steady-state UEDGE simulations systematically scanning upstream density, input power, plasma current, impurity fraction, and transport coefficients, with magnetic and electric drifts included. Also performed complementary time-dependent simulations.

Result: Identified robust detachment indicators with strike-point Te consistently 3-4 eV at onset, weaker impurity sensitivity than 1D models, distinctive KSTAR divertor asymmetries, and quantified plasma response delays (5-15 ms outer strike point, ~40 ms LFS radiation front).

Conclusion: The database provides valuable insights into detachment physics and supports FOPDT models that capture plasma dynamics consistent with experimental observations, enabling effective detachment control in KSTAR.

Abstract: A large-scale database of two-dimensional UEDGE simulations has been
developed to study detachment physics in KSTAR and to support surrogate models
for control applications. Nearly 70,000 steady-state solutions were generated,
systematically scanning upstream density, input power, plasma current, impurity
fraction, and anomalous transport coefficients, with magnetic and electric
drifts across the magnetic field included. The database identifies robust
detachment indicators, with strike-point electron temperature at detachment
onset consistently Te around 3-4 eV, largely insensitive to upstream
conditions. Scaling relations reveal weaker impurity sensitivity than
one-dimensional models and show that heat flux widths follow Eich's scaling
only for uniform, low D and Chi. Distinctive in-out divertor asymmetries are
observed in KSTAR, differing qualitatively from DIII-D. Complementary
time-dependent simulations quantify plasma response to gas puffing, with delays
of 5-15 ms at the outer strike point and approximately 40 ms for the
low-magnetic-field-side (LFS) radiation front. These dynamics are well captured
by first-order-plus-dead-time (FOPDT) models and are consistent with
experimentally observed detachment-control behavior in KSTAR [Gupta et al.,
submitted to Plasma Phys. Control. Fusion (2025)]

</details>


### [71] [A flexible and differentiable coil proxy for stellarator equilibrium optimization](https://arxiv.org/abs/2510.16243)
*Lanke Fu,Dario Panici,Elizabeth Paul,Alan Kaptanoglu,Amitava Bhattacharjee*

Main category: physics.plasm-ph

TL;DR: A new "quasi-single-stage" method using differentiable coil proxy to balance plasma performance and coil complexity in stellarator design, avoiding issues of traditional two-stage optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional two-stage optimization for stellarator power plants makes it challenging to find optimal balance between plasma performance and coil cost. Single-stage approaches introduce dimensionality and ill-posedness problems.

Method: Developed a flexible, differentiable coil proxy based on adjoint differentiation of winding surface coil subproblem that directly predicts coil complexity during equilibrium optimization without adding new degrees of freedom.

Result: Initial numerical results demonstrate the proxy's effectiveness for single-stage optimization, successfully balancing coil and plasma performance.

Conclusion: The quasi-single-stage method provides an effective approach to optimize both plasma and coils simultaneously while avoiding the drawbacks of traditional single-stage optimization methods.

Abstract: Balancing plasma performance and coil cost is a significant challenge when
designing a stellarator power plant. Most present stellarator designs are
produced by two-stage optimization: the first for the equilibrium and the
second for a coil design reproducing its magnetic configuration. It is
challenging to find a compromise between plasma and coils with this approach.
In recent years, single-stage approaches have gained popularity, which attempt
to optimize both the plasma and coils simultaneously to improve the plasma-coil
balance. In exchange, it can substantially increase the problem's
dimensionality and introduce the ill-posedness of filamentary coil optimization
to equilibrium optimization. This paper introduces a new ``quasi-single-stage''
method representing a flexible and differentiable coil proxy that directly
predicts coil complexity during equilibrium optimization. The proxy is based on
the adjoint differentiation of a winding surface coil subproblem. Our proxy can
balance coil and plasma performance without introducing new degrees of freedom
or ill-posedness. We present initial numerical results that demonstrate the
proxy's effectiveness for single-stage optimization.

</details>


### [72] [Hamiltonian estimation of island width threshold for stochasticity onset on edge pedestal top in presence of a resonant magnetic perturbation](https://arxiv.org/abs/2510.16361)
*Zhifei Gui,Ping Zhu,Dominique Franck Escande*

Main category: physics.plasm-ph

TL;DR: The paper applies Hamiltonian methods to analyze nonlinear magnetic topology from RMPs in tokamaks, comparing three chaos analysis methods (renormalization, Lyapunov exponents, weighted Birkhoff average) that show consistent predictions of stochasticity thresholds.


<details>
  <summary>Details</summary>
Motivation: To understand chaotic behavior in tokamak magnetic fields induced by Resonant Magnetic Perturbations (RMPs) and establish quantitative criteria for optimizing ELM control strategies.

Method: Used Hamiltonian method to analyze nonlinear magnetic topology, comparing three chaos analysis approaches: renormalization method, Lyapunov exponents, and weighted Birkhoff average.

Result: Found strong consistency among all three methods in predicting large scale stochasticity threshold. Established magnetic island width threshold as quantitative criterion for RMP-based ELM control optimization.

Conclusion: The magnetic island width threshold bridges a critical gap in plasma control strategies, providing a quantitative basis for optimizing RMP-based edge localized mode (ELM) control in tokamaks.

Abstract: This study applies the Hamiltonian method to analyze the nonlinear magnetic
topology induced by Resonant Magnetic Perturbations (RMPs) in tokamaks. We
investigate the system's chaotic behavior by comparing three methods: the
renormalization method, Lyapunov exponents (LE), and weighted Birkhoff average
(WBA). A strong consistency is found among these methods in predicting the
large scale stochasticity threshold. The magnetic island width threshold
provides a quantitative criterion for optimizing RMP-based ELM control,
bridging a critical gap in plasma control strategies.

</details>


### [73] [Relativistic Magnetohydrodynamic Wave Excitation by Laser Pulse in a Magnetized Plasma](https://arxiv.org/abs/2510.16362)
*Zohreh Hashempour,Mehdi Nasri Nasrabadi,Nora Nassiri-Mofakham,Hamidreza Daniali*

Main category: physics.plasm-ph

TL;DR: Analysis of modulational instability in relativistic magnetized plasma using MHD equations, deriving NLSE and examining instability growth rates and wave dispersion relationships.


<details>
  <summary>Details</summary>
Motivation: To understand modulational instability in strong laser-plasma interactions where relativistic electron velocities induce wave instability, particularly in relativistic magnetized plasma contexts.

Method: Used magnetohydrodynamic (MHD) equations with perturbation techniques on electron fluid equations, derived Nonlinear Schrödinger Equation (NLSE), and applied Bogoliubov-Mitropolsky perturbation approach to analyze NLLD and growth-damping effects.

Result: Derived maximum growth rate of modulational instability and its dependence on plasma parameters and wave intensity, providing quantitative insights into instability dynamics in relativistic magnetized plasma.

Conclusion: Successfully modeled modulational instability in relativistic plasma using NLSE framework, revealing key relationships between wave dispersion, plasma parameters, and instability growth mechanisms including NLLD effects.

Abstract: In the study of plasma, particularly in applications involving strong
laser-plasma interactions, the propagation of a strong electromagnetic wave
induces relativistic velocities in the electron flow. Given such conditions,
the wave propagating through the plasma experiences modulational instability.
In this paper, we investigate this instability using magnetohydrodynamic (MHD)
equations. In the relativistic limit, the motion of ions can be neglected due
to their significant inertia, allowing us to treat the ions as a background
fluid. This simplification enables us to apply perturbation techniques to the
electron fluid equations, leading to the derivation of the nonlinear wave
equation in the form of the Nonlinear Schr\"odinger Equation (NLSE). We also
explore the relationship between wave dispersion and the conditions for
instability. We derive the maximum growth rate of the modulational instability
and analyze its dependence on plasma parameters and wave intensity in the
context of relativistic magnetized plasma, providing quantitative insights into
the instability dynamics. Finally, we examine aspects of the perturbed NLSE
using the Bogoliubov-Mitropolsky perturbation approach, treating real and
imaginary coefficients separately, which explicitly incorporates both Nonlinear
Landau Damping (NLLD) and growth-damping effects.

</details>


### [74] [Probing Flying-Focus Wakefields](https://arxiv.org/abs/2510.16950)
*Aaron Liberman,Anton Golovanov,Sheroy Tata,Anda-Maria Talposi,Victor Malka*

Main category: physics.plasm-ph

TL;DR: Flying-focus wakefields with tunable velocity can solve electron dephasing in laser-wakefield accelerators, enabling high acceleration gradients over long distances through spatio-temporal couplings and axiparabola focusing.


<details>
  <summary>Details</summary>
Motivation: To overcome electron dephasing limitations in laser-wakefield accelerators, which currently prevents achieving both high acceleration gradients and long acceleration lengths simultaneously.

Method: Combination of spatio-temporal couplings and focusing with an axiparabola to create quasi-Bessel beams, studied through direct imaging and simulations.

Result: Demonstrated stability of wakefield structures, analyzed evolution with density changes, studied ionization effects with various gases, and identified importance of focusing position.

Conclusion: The study provides crucial insights into flying-focus wakefield behavior, advancing understanding needed for realizing dephasingless acceleration in laser-wakefield accelerators.

Abstract: Flying-focus wakefields, which can propagate with a tunable velocity along
the optical axis, are promising solutions to electron dephasing in
laser-wakefield accelerators. This is accomplished by a combination of
spatio-temporal couplings and focusing with an axiparabola, a specialized
optical element which produces a quasi-Bessel beam. If implemented,
dephasingless acceleration would allow for a hitherto unachievable mixture of
high acceleration gradients and long acceleration lengths. Here, we conduct an
in-depth study of the structure and behavior of such a flying-focus wakefield,
through a combination of direct imaging and simulations. We show the stability
of the wakefield structures, explore how the wakefield evolves with changes of
density, study the effects of ionization on the wakefield structure with a
variety of gases, and analyze the importance of the focusing position. These
insights shed light onto this novel wakefield regime and bring understanding
that is important to the realization of dephasingless acceleration.

</details>


### [75] [Plasma Shape Control via Zero-shot Generative Reinforcement Learning](https://arxiv.org/abs/2510.17531)
*Niannian Wu,Rongpeng Li,Zongyu Yang,Yong Xiao,Ning Wei,Yihang Chen,Bo Li,Zhifeng Zhao,Wulyu Zhong*

Main category: physics.plasm-ph

TL;DR: A novel framework combining Generative Adversarial Imitation Learning with Hilbert space representation learning to create a versatile, zero-shot plasma shape control policy from historical PID data.


<details>
  <summary>Details</summary>
Motivation: Traditional PID controllers lack adaptability for plasma shape control, while task-specific RL methods have limited generalization and require repetitive retraining.

Method: Combines Generative Adversarial Imitation Learning (GAIL) with Hilbert space representation learning to mimic stable PID operational style and create geometrically structured latent space for goal-directed control.

Result: The foundation policy achieves precise and stable tracking of reference trajectories for key shape parameters across various plasma scenarios on HL-3 tokamak simulator without task-specific fine-tuning.

Conclusion: Presents a viable pathway toward developing highly flexible and data-efficient intelligent control systems for future fusion reactors.

Abstract: Traditional PID controllers have limited adaptability for plasma shape
control, and task-specific reinforcement learning (RL) methods suffer from
limited generalization and the need for repetitive retraining. To overcome
these challenges, this paper proposes a novel framework for developing a
versatile, zero-shot control policy from a large-scale offline dataset of
historical PID-controlled discharges. Our approach synergistically combines
Generative Adversarial Imitation Learning (GAIL) with Hilbert space
representation learning to achieve dual objectives: mimicking the stable
operational style of the PID data and constructing a geometrically structured
latent space for efficient, goal-directed control. The resulting foundation
policy can be deployed for diverse trajectory tracking tasks in a zero-shot
manner without any task-specific fine-tuning. Evaluations on the HL-3 tokamak
simulator demonstrate that the policy excels at precisely and stably tracking
reference trajectories for key shape parameters across a range of plasma
scenarios. This work presents a viable pathway toward developing highly
flexible and data-efficient intelligent control systems for future fusion
reactors.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [76] [A sufficient condition for the existence of smooth solutions of the relativistic cold plasma equations on any given time interval](https://arxiv.org/abs/2510.16613)
*Olga S. Rozanova,Evgeniy V. Chizhonkov*

Main category: math-ph

TL;DR: Found sufficient conditions for smooth solutions to relativistic cold plasma equations, requiring smallness of both initial data and their derivatives, unlike non-relativistic case.


<details>
  <summary>Details</summary>
Motivation: To establish sufficient conditions for smooth solutions to the Cauchy problem for one-dimensional relativistic cold plasma equations over arbitrary time intervals.

Method: Mathematical analysis of sufficient conditions for solution smoothness, and numerical experiments to verify accuracy and study singularity structure.

Result: Obtained sufficient conditions that require smallness of both initial data and their derivatives, with numerical verification of accuracy.

Conclusion: Relativistic plasma equations require stricter conditions than non-relativistic case, with both initial data and derivatives needing to be small for smooth solutions.

Abstract: In terms of initial data, a sufficient condition for the smoothness of the
solution to the Cauchy problem for one-dimensional relativistic cold plasma
equations over any given time interval is found. Unlike the non-relativistic
case, such sufficient conditions take into account the smallness properties of
not only the derivatives of the initial data but also the initial data
themselves. The accuracy of the obtained initial condition is investigated
using a numerical experiment. The structure of the emerging singularities is
also studied.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [77] [High Energy plurisubharmonic classes](https://arxiv.org/abs/2510.16412)
*Vincent Guedj,Ahmed Zeriahi*

Main category: math.CV

TL;DR: The paper studies finite energy classes of plurisubharmonic functions on bounded strongly pseudoconvex domains using Orlicz spaces with concave increasing weights, focusing on the range of the Monge-Ampère operator.


<details>
  <summary>Details</summary>
Motivation: To solve the long-standing open problem of characterizing the image of bounded plurisubharmonic functions under the Monge-Ampère operator, which has been unresolved for over 40 years in Pluripotential Theory.

Method: Introduces and studies finite energy classes E_χ(Ω) of plurisubharmonic functions using Orlicz space formalism with concave increasing weights χ: R^- → R^- where χ(0) = 0.

Result: The paper investigates the range of the Monge-Ampère operator on these finite energy classes.

Conclusion: The authors conjecture that their approach should lead to an integral characterization of the image of bounded plurisubharmonic functions, potentially solving this long-standing problem.

Abstract: Let $\Omega \Subset \C^n$ be a bounded strongly pseudoconvex domain. For any
concave increasing weight $\chi : \R^- \longrightarrow \R^-$ such that $\chi(0)
= 0$, we introduce and study finite energy classes $\mathcal E_\chi(\Omega)$ of
plurisubharmonic functions, using the Orlicz space formalism. We investigate
the range of the Monge-Amp\`ere operator on these classes, and conjecture that
this should lead to an integral characterization of the image of bounded
plurisubharmonic functions, an open problem since the birth of Pluripotential
Theory more than forty years ago.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [78] [Practicalities of State-Dependent and Threshold Delay Differential Equations](https://arxiv.org/abs/2510.17126)
*A. R. Humphries,A. S. Eremin,Z. Wang*

Main category: math.DS

TL;DR: This chapter explores state-dependent delays in modeling, covering discrete delays and threshold-defined delays, with methods for linearization and numerical implementation using standard software.


<details>
  <summary>Details</summary>
Motivation: Delays in applied problems are often state-dependent rather than constant, requiring specialized approaches for accurate modeling and analysis.

Method: The chapter discusses modeling with state-dependent delays, formulation as dynamical systems, linearization techniques, and numerical methods including tracking breaking points for discrete delays and velocity ratio terms for threshold conditions.

Result: The approach enables efficient simulation of solutions while preserving numerical method order, and makes linearization accessible without advanced functional analysis.

Conclusion: State-dependent delays can be effectively handled through proper formulation and numerical techniques, allowing them to be treated with standard software tools.

Abstract: Delays are ubiquitous in applied problems, but often do not arise as the
simple constant discrete delays that analysts and numerical analysts like to
treat. In this chapter we show how state-dependent delays arise naturally when
modeling and the consequences that follow. We treat discrete state-dependent
delays, and delays implicitly defined by threshold conditions. We will consider
modeling, formulation as dynamical systems, linearization, and numerical
techniques. For discrete state-dependent delays we show how breaking points can
be tracked efficiently to preserve the order of numerical methods for
simulating solutions. For threshold conditions we will discuss how a velocity
ratio term arises in models, and present a heuristic linearization method that
avoids Banach spaces and sun-star calculus, making the method accessible to a
wider audience. We will also discuss numerical implementations of threshold and
distributed delay problems which allows them to be treated numerically with
standard software.

</details>


### [79] [The heat flow driven by the Laplacian of a directed hypergraph](https://arxiv.org/abs/2510.17497)
*Delio Mugnolo*

Main category: math.DS

TL;DR: Spectral and semigroup theory for Laplacians on directed hypergraphs, revealing non-classical phenomena where heat flow loses standard properties but may regain them asymptotically under specific combinatorial conditions.


<details>
  <summary>Details</summary>
Motivation: To develop a theoretical framework for Laplacians on directed hypergraphs, where classical Markovian and symmetry principles of graph Laplacians break down, addressing the genuinely higher-order nature of these dynamics.

Method: Develop spectral and semigroup theory, derive spectral bounds, characterize eventual positivity and contractivity in infinity-norm, and identify conditions for Markovian behavior.

Result: Revealed new phenomena: heat flow may lose positivity, stochasticity, or diagonal dominance but regain them asymptotically under precise combinatorial conditions. Characterized eventual positivity and contractivity, and identified rare cases where flow remains Markovian.

Conclusion: The framework successfully captures the genuinely higher-order nature of dynamics on directed hypergraphs, demonstrating how classical properties can break down and be recovered under specific combinatorial conditions, with examples including hypergraphs dual to oriented graphs and directed realizations of the Fano plane.

Abstract: We develop a spectral and semigroup theory for Laplacians on directed
hypergraphs, a setting where classical Markovian and symmetry principles of
graph Laplacians break down. This framework reveals new phenomena: the heat
flow may lose positivity, stochasticity, or diagonal dominance, yet regain them
asymptotically under precise combinatorial conditions. We derive spectral
bounds, characterize eventual positivity and contractivity in $\infty$-norm,
and identify the rare cases where the flow remains Markovian. Several examples
-- culminating in hypergraphs that are dual of oriented graphs as well as
directed realizations of the Fano plane - highlight the genuinely higher-order
nature of these dynamics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [80] [Toward Autonomous Neural VMC: An Energy-Variance Convergence Criterion for Quantum Systems](https://arxiv.org/abs/2510.17490)
*Huan-Chen Shi,Er-Liang Cui,Dan Zhou*

Main category: quant-ph

TL;DR: Energy variance is proposed as a universal convergence criterion for neural-network variational Monte Carlo, with empirical threshold of 1e-3 guaranteeing <1% relative errors across diverse quantum systems.


<details>
  <summary>Details</summary>
Motivation: To establish a robust, practical convergence criterion for neural wave function optimization in VMC, as current methods lack systematic application of energy variance despite its theoretical foundation.

Method: Propose and validate energy variance as primary convergence metric, implement in lightweight neural solver, and test across harmonic oscillators, hydrogen atoms, charmonium hadrons, and other quantum systems.

Result: Energy variance below 1e-3 consistently guarantees relative errors under 1% across all tested systems, providing system-agnostic benchmark for automated optimization.

Conclusion: Energy-variance criterion enables robust, scalable convergence assessment and accelerates preliminary physical verification of quantum Hamiltonians through automated parameter scans.

Abstract: The optimization of neural wave functions in variational Monte Carlo(VMC)
crucially relies on a robust convergence criterion. While the energy variance
is theoretically a definitive measure of an eigenstate, its systematic
application as a primary, practical convergence criterion in neural-network VMC
has been underexplored. In this work, we propose and validate the energy
variance as a universal, quantitative criterion for convergence. Then its
reliability is demonstrated across diverse quantum systems-from harmonic
oscillators and hydrogen atoms to charmonium hadrons-showing that a variance
below 1*10^{-3} guarantees relative errors under 1%. This empirical threshold
provides a system-agnostic benchmark for convergence, enabling hands-off
operation of the optimization process. We implement this criterion within a
lightweight neural solver, thereby enabling automated parameter scans. Its
utility is showcased by efficiently mapping ground-state properties of a 2D
double-well potential, a hydrogen atom in a magnetic field, and a three-body
quantum dot. Our work positions the energy-variance criterion as a robust and
scalable tool that significantly accelerates the preliminary physical
verification of quantum Hamiltonians.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [81] [Optimized Single-Core PCF-Based SPR Biosensor for High-Performance Early-Stage Multi-Cancer Detection](https://arxiv.org/abs/2510.17283)
*Tonmoy Malakar,Miss Nourin Nurain Amina,Zarin Tasnim Nijhum,Nazmus Shakib Lalin*

Main category: physics.med-ph

TL;DR: A highly sensitive SPR-based biosensor using circular-lattice PCF with gold and V2O5 layers achieves exceptional sensitivity for early cancer detection in skin, blood, and adrenal gland tissues.


<details>
  <summary>Details</summary>
Motivation: To overcome the bulkiness and limited sensitivity of traditional SPR systems for early-stage cancer detection by integrating SPR with PCF technology.

Method: Used circular-lattice PCF with thin gold layer for plasmon excitation and V2O5 nanolayer for enhanced adhesion. Analyzed performance using Finite Element Method in COMSOL Multiphysics 6.1 to detect refractive index variations in cancer cells.

Result: Achieved wavelength sensitivities of 21,250 nm/RIU (skin), 53,571 nm/RIU (blood), and 103,571 nm/RIU (adrenal gland cancers), with maximum FOM of 306.424 RIU^-1 and spectral resolution of 9.57x10^-7 RIU.

Conclusion: The SPR-PCF sensor shows strong potential for real-time, label-free biosensing applications in precise and early cancer diagnostics.

Abstract: In this study, we present a highly sensitive Surface Plasmon Resonance
(SPR)-based biosensor integrated with a circular-lattice Photonic Crystal Fiber
(PCF) for early-stage cancer detection. The proposed sensor leverages the
synergy between SPR and PCF technologies to overcome the bulkiness and limited
sensitivity of traditional SPR systems. A thin gold (Au) layer, responsible for
plasmon excitation, is deposited on the fiber structure, while a nanolayer of
vanadium pentoxide (V2O5) is introduced to enhance adhesion between the gold
and the silica background, improving structural stability and field
confinement. The sensor is designed to detect refractive index (RI) variations
in biological analytes, specifically targeting cancerous cells from skin,
blood, and adrenal gland tissues. The optical characteristics and performance
of the sensor were thoroughly analyzed using the Finite Element Method (FEM) in
COMSOL Multiphysics 6.1, allowing for precise simulation and optimization. The
sensor demonstrates high sensitivity within the RI range of 1.360-1.395,
corresponding to the RI values of the target cancer cells. Remarkable
wavelength sensitivities of 21,250 nm/RIU, 53,571 nm/RIU, and 103,571 nm/RIU
were achieved for skin, blood, and adrenal gland cancers, respectively. In
addition, a maximum figure of merit (FOM) of 306.424 RIU^-1 and a spectral
resolution (SR) of 9.57x10^-7 RIU further affirm the sensor's exceptional
detection capabilities. These findings indicate the proposed SPR-PCF sensor's
strong potential for real-time, label-free biosensing applications,
particularly in precise and early cancer diagnostics.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [82] [Frequency domain laser ultrasound for inertial confinement fusion target wall thickness measurements](https://arxiv.org/abs/2510.15997)
*Martin Ryzy,Guqi Yan,Clemens Grünsteidl,Georg Watzl,Kevin Sequoia,Pavel Lapa,Haibo Huang*

Main category: physics.ins-det

TL;DR: A non-destructive laser ultrasound method using zero-group velocity guided elastic wave resonances to measure wall thickness of spherical capsules for inertial confinement fusion.


<details>
  <summary>Details</summary>
Motivation: Perfect capsule geometry is required for maximum implosion efficiency in inertial confinement fusion experiments, necessitating accurate wall thickness measurement methods.

Method: Frequency domain laser ultrasound microscopy system using zero-group velocity guided elastic wave resonances, complemented with plate dispersion calculations and finite element wave propagation simulations.

Result: Wall thickness measurements along the equator of a high-density carbon capsule (2mm diameter, 80μm wall) showed excellent agreement with infrared interferometry reference measurements.

Conclusion: The method is scalable and applicable to various target materials including metals and metal-doped targets for inertial confinement fusion applications.

Abstract: In inertial confinement fusion experiments hollow, spherical mm-sized
capsules are used as a container for nuclear fuel. To achieve maximum implosion
efficiency, a perfect capsule geometry is required. This paper presents a wall
thickness measurement method based on zero-group velocity guided elastic wave
resonances. They are measured with a non-destructive, contactless frequency
domain laser ultrasound microscopy system. Wall thickness measurements along
the equator of a high-density carbon capsule with a diameter of around 2 mm and
a wall thickness of around 80 $\unicode{x00B5}$m excellently agree with
infrared interferometry reference measurements. In addition, the multi-resonant
nature of a spherical shell is studied by complementing experimental
observations with plate dispersion calculations and finite element wave
propagation simulations. The presented method is scalable and can be applied to
a broad range of target materials, including metals, or metal-doped targets.

</details>


<div id='physics.atom-ph'></div>

# physics.atom-ph [[Back]](#toc)

### [83] [Numerical modeling of laser cooling in molecules: From simple diatomics to polyatomics and radioactive species](https://arxiv.org/abs/2510.16203)
*Felix Kogel,Tatsam Garg,Phillip Groß,Lukas Leczek,Marian Rockenhäuser,Neil Shah,Jakob Weiß,Andreas Schindewolf,Tim Langen*

Main category: physics.atom-ph

TL;DR: MoleCool is a Python toolbox for efficiently modeling molecular laser cooling dynamics using optical Bloch and rate equations, handling complex multi-level systems with applications to various molecular species.


<details>
  <summary>Details</summary>
Motivation: Molecular laser cooling requires modeling dozens to hundreds of energy levels, posing computational challenges that existing tools struggle with, necessitating an efficient numerical solution.

Method: Developed MoleCool, a Python toolbox that implements and solves differential equation systems for light-matter interactions using optical Bloch equations and rate equations.

Result: Successfully demonstrated MoleCool's capabilities through educational examples from simple Rabi oscillations to complex cooling schemes, including full hyperfine structure modeling of RaF, BaF, and YbOH molecules.

Conclusion: MoleCool provides an efficient and comprehensive tool for modeling molecular laser cooling dynamics, enabling detailed studies of complex molecular species with many energy levels.

Abstract: Optical Bloch equations and rate equations serve as powerful tools to model
light-matter interactions from textbook-like two-level atoms to the complex
internal dynamics of molecules. A particular challenge in this context is posed
by molecular laser cooling, where many dozens or hundreds of levels need to be
taken into account for a comprehensive modeling. Here, we present MoleCool, a
numerically efficient Python toolbox to implement and solve the corresponding
differential equation systems. We illustrate both the capabilities of the
toolbox and some of the intricacies of molecular laser cooling by educational
examples, which range from simple Rabi oscillations to spontaneous and coherent
cooling schemes for various currently studied or considered molecular species.
This includes, in particular, a comprehensive modeling of laser cooling
dynamics with full hyperfine structure resolution in radioactive radium
monofluoride (RaF), as well as studies of other complex species such as barium
monofluoride (BaF) and ytterbium monohydroxide (YbOH).

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [84] [Near-field radiative heat transfer in the dual nanoscale regime between polaritonic membranes](https://arxiv.org/abs/2510.16058)
*Livia Correa McCormack,Lei Tang,Mathieu Francoeur*

Main category: cond-mat.mes-hall

TL;DR: Near-field radiative heat transfer between polaritonic membranes (SiC, SiN, SiO2) can be significantly enhanced or attenuated due to corner and edge modes, with material losses playing a key role.


<details>
  <summary>Details</summary>
Motivation: To understand how subwavelength membrane geometry affects near-field radiative heat transfer and identify the mechanisms behind enhancement and attenuation effects.

Method: Used fluctuational electrodynamics simulations combined with modal analysis to study heat transfer between polaritonic membranes.

Result: Found that membranes support corner and edge modes, leading to 5.1-fold enhancement for SiC and 2.1-fold attenuation for SiO2 in heat transfer coefficient compared to infinite surfaces.

Conclusion: Material losses directly influence the enhancement or attenuation of heat transfer by reducing the density of available electromagnetic states between membranes.

Abstract: The enhancement and attenuation of near-field radiative heat transfer between
polaritonic SiC, SiN and SiO2 subwavelength membranes is analyzed.
Fluctuational electrodynamics simulations combined with a modal analysis show
that all membranes support corner and edge modes, which can induce a large
5.1-fold enhancement for SiC and a 2.1-fold attenuation for SiO2 of the heat
transfer coefficient with respect to that between infinite surfaces. The
enhancement or attenuation is directly related to material losses which reduce
the density of available electromagnetic states between the membranes.

</details>


### [85] [Geometry-Driven Charge and Spin Transport in $\beta12$ Borophene Quantum Dots](https://arxiv.org/abs/2510.17412)
*Seyed Mahdi Mastoor,Amirhossein Ahmadkhan Kordbacheh*

Main category: cond-mat.mes-hall

TL;DR: Study of geometry effects on charge/spin transport in β12 borophene quantum dots with different shapes (circular disc and regular hexagon) connected to zigzag and armchair nanoribbon leads, revealing distinct spin filtering behaviors based on edge configuration.


<details>
  <summary>Details</summary>
Motivation: To understand how geometry and edge termination affect charge and spin transport in confined borophene systems, which is crucial for developing borophene-based spintronic devices.

Method: Used five-band tight-binding Hamiltonian parameterized with first-principles data, calculated transport properties within non-equilibrium Green's function framework, computed spin resolved transmissions and polarization for various lead widths and exchange field strengths.

Result: Armchair-connected structures showed broader and more stable fully spin-polarized windows than zigzag-connected ones. Identified critical lead-width thresholds (≈1.01 nm for zigzag, ≈0.87 nm for armchair) and moderate exchange field required for complete spin filtering.

Conclusion: Edge termination and confinement geometry strongly influence transport properties, providing design guidelines for borophene-based nanoscale spintronic devices.

Abstract: Theoretical research has been conducted to study how geometry affects charge
and spin transport in $\beta\mathrm{12}$ borophene quantum dots, which are
confined systems. The study examined two distinct central regions, which
included a circular disc and a regular hexagonal area that connected to
semi-infinite zigzag and armchair borophene nanoribbon leads. The system was
described by a five-band tight-binding Hamiltonian parameterized using
first-principles data, and the transport properties were calculated within the
non-equilibrium Green's function framework. Spin resolved transmissions and
spin polarization were computed for a range of lead widths and
proximity-induced exchange field strengths. The analysis revealed distinct
transport characteristics determined by geometry and edge configuration:
armchair-connected structures exhibited broader and more stable fully
spin-polarized windows compared with zigzag-connected counterparts.
Furthermore, critical lead-width thresholds ($\approx 1.01$ nm for zigzag and
$\approx 0.87$ nm for armchair) and a moderate exchange field above which
complete spin filtering occurs were identified. The results highlight the
strong influence of edge termination and confinement geometry on transport
properties and provide useful design guidelines for developing borophene-based
nanoscale spintronic devices.

</details>


### [86] [Néel-Vector-Orientation Induced Intrinsic Half-Metallicity in Two-Dimensional Altermagnets](https://arxiv.org/abs/2510.17522)
*Xin Chen,Jin Zou,Lipeng Song,Wei Sun,Yiwen Wu,Luyao Zhu,Xu Cheng,Duo Wang,Biplab Sanyal*

Main category: cond-mat.mes-hall

TL;DR: The paper shows how rotating the Néel vector in monolayer Ta2TeSeO altermagnet enables deterministic spin channel selection through selective symmetry breaking, achieving fully spin-polarized transport and reversible switching with minimal energy inputs.


<details>
  <summary>Details</summary>
Motivation: Altermagnets offer zero net magnetization with giant spin splitting but require breaking C4zT symmetry to deterministically select conducting spin channels. The research aims to find efficient ways to achieve this control.

Method: Using axial vector transformation rules, the study analyzes how Néel vector rotation in Ta2TeSeO breaks specific mirror symmetries (Mx or My) while preserving others, creating selective symmetry breaking that affects spin channels differently.

Result: Aligning Néel vector along x or y direction breaks corresponding mirror symmetry, keeping one spin sector gapless while opening a gap in the opposite spin, yielding fully spin-polarized transport. C2z symmetry breaking makes Weyl points inequivalent, creating half metallic state. The mechanism enables reversible switching with minute strain or weak magnetic fields.

Conclusion: The Néel vector rotation provides a general low-power route for spin filtering and logic in 2D decorated Lieb altermagnets lacking Mz symmetry, with potential control via circularly polarized light and applications in efficient spintronics.

Abstract: Altermagnets combine zero net magnetization with giant spin splitting,
enabling spin-polarized transport without strong spin-orbit coupling (SOC).
Deterministically selecting the conducting spin channel, however, requires
breaking the 90 degree rotation and time-reversal antisymmetry (C4zT). Using
standard axial vector transformation rules as preliminaries, we show that in
monolayer Ta2TeSeO this can be achieved naturally and tuned in a symmetry
efficient way by rotating the Neel vector. Without considering the Neel vector,
Ta2TeSeO has one pair of mirror protected spin polarized Weyl points in each
spin channel. Aligning the Neel vector along the crystallographic x or y
direction breaks the mirror symmetry Mx or My, inducing selective mirror
symmetry breaking that keeps one spin sector gapless and opens a gap in the
opposite spin, yielding fully spin polarized transport. The C2z symmetry
breaking makes the preserved two Weyl points inequivalent, turning the half
semimetal into a half metallic state. The same orientation selective symmetry
reduction applies to lattice vibrations, implying phonon chirality splitting.
Owing to the near degenerate in plane anisotropy, reversible zero moment
switching is achievable with minute in plane strain or weak magnetic fields,
and the lattice coupling suggests control by circularly polarized light. The
mechanism extends to other two dimensional decorated Lieb altermagnets lacking
horizontal mirror Mz, providing a general low power route to spin filtering and
logic.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [87] [A volume-of-fluid model for biomass particle pyrolysis](https://arxiv.org/abs/2510.17588)
*Riccardo Caraccio,Edoardo Cipriano,Alessio Frassoldati,Tiziano Faravelli*

Main category: physics.flu-dyn

TL;DR: A single-grid model for biomass pyrolysis that fully resolves both solid particle evolution and gas-phase dynamics using VOF method, with novel porosity-shrinkage coupling and anisotropic biomass representation.


<details>
  <summary>Details</summary>
Motivation: Existing models focus on either solid particle evolution or gas-phase dynamics separately, neglecting their coupled interactions, which limits accurate prediction of pyrolysis processes.

Method: Eulerian two-phase system with Volume-Of-Fluid (VOF) method to track biomass-gas interface, including solid-phase pyrolysis reactions and novel porosity-shrinkage coupling approach with anisotropic biomass representation.

Result: Model demonstrates mass conservation and numerical convergence, shows excellent agreement with experimental data for mass/temperature profiles and volatiles trends, though shrinking profiles highlight need for better understanding of biomass structure evolution.

Conclusion: The model advances sustainable pyrolysis process development by providing fully coupled solid-gas phase resolution, with code and setups made publicly available in Basilisk framework.

Abstract: Numerical models of biomass particle pyrolysis focus on either the solid
particle evolution or on the surrounding gas-phase dynamics, neglecting the
coupled interactions between the two. This work addresses this limitation by
proposing a single-grid model that fully resolves both phases without relying
on sub-grid-scale correlations. The model adopts an Eulerian representation of
the two-phase system, using a Volume-Of-Fluid (VOF) method to track the
interface between the biomass and the surrounding gas phase. Solid-phase
pyrolysis reactions are included, and a novel approach is proposed to capture
the coupling between the evolution of biomass porosity and the particle
shrinkage. The anisotropic nature of the biomass particle is accounted for in
this multidimensional framework. The resulting model demonstrates mass
conservation and numerical convergence. Extensive validation with experimental
data shows excellent agreement in terms of mass and temperature profiles and
correct volatiles trends. Shrinking profiles reveal correct trends, but they
also highlight the need for a better fundamental understanding of the evolution
of the biomass structure. Overall, the model takes a step forward in aiding the
development of sustainable pyrolysis processes. The code and simulation setups,
developed within the open-source Basilisk framework, are made publicly
available.

</details>


### [88] [Fluid-structure interaction analysis with interface control principle](https://arxiv.org/abs/2510.17144)
*Chungil Lee,Yoshiaki Abe,Yu Kawano,Tomoki Yamazaki*

Main category: physics.flu-dyn

TL;DR: A data-driven interface control principle for unsteady fluid-structure interaction that minimizes residual forces between fluid and structural domains, enabling independent simulations with predetermined interface states.


<details>
  <summary>Details</summary>
Motivation: To develop a method that allows independent fluid and structural simulations without real-time communication by controlling interface motion to minimize residual forces.

Method: Data-driven interface modeling using sparse identification of nonlinear dynamics with control, combined with control theory to minimize residual force between fluid and structural domains.

Result: Successfully applied to vortex-induced vibration of a cylinder at Re=150, showing residual force minimization and good agreement with reference FSI simulations when residual force approaches zero.

Conclusion: The proposed interface control principle enables accurate unsteady FSI analysis with independent fluid and structural simulations by predetermining interface states and minimizing residual forces.

Abstract: An interface control principle is proposed for unsteady fluid-structure in-
teraction (FSI) analyses. This principle introduces a method of explicitly
controlling the interface motion in the temporal direction to minimize the
residual force on the interface, which is defined as the discrepancy between
the fluid and structural forces. The interface model is constructed using a
data-driven approach that involves sparse identification of nonlinear dy-
namics with control to evaluate the residual force. The interface model is
subsequently subjected to control theory in order to minimize the residual
force. Following the convergence of the residual force, the interface state is
controlled to be that of the original unsteady FSI system. The fluid and
structural simulations can be conducted independently without communication, as
the interface state information is predetermined as inputs for each system. The
proposed method is applied to the vortex-induced vibration (VIV) of a cylinder
at a Reynolds number of 150 under several reduced velocity conditions
corresponding to the lock-in regime with limit-cycle oscillations. The results
demonstrate that the residual force is sufficiently minimized in time, and when
the residual force is close to zero, the predicted fluid force and structural
displacement of the VIV show good agreement with the reference FSI simulation.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [89] [Electron Localization in Non-Compact Covalent Bonds Captured by the r2SCAN+V Approach](https://arxiv.org/abs/2510.16348)
*Yubo Zhang,Da Ke,Rohan Maniar,Timo Lebeda,Peihong Zhang,Jianwei Sun,John P. Perdew*

Main category: cond-mat.mtrl-sci

TL;DR: SCAN and r2SCAN functionals improve over PBE but fail in materials with non-compact covalent bonding. The proposed r2SCAN+V modification addresses this limitation.


<details>
  <summary>Details</summary>
Motivation: To understand why SCAN and r2SCAN functionals underperform in specific materials like graphene, Fe, Cr2, and VO2, and to develop a solution for these challenging cases.

Method: Identified non-compact covalent bonding as the common characteristic in problematic materials. Proposed r2SCAN+V approach with parameter V (4 eV for Fe, lower for others) to improve electron localization accuracy.

Result: r2SCAN+V successfully improves accuracy across all tested challenging materials, addressing the limitations of SCAN and r2SCAN in non-compact covalent bonding systems.

Conclusion: The study reveals that SCAN/r2SCAN's bias toward local atomic site localization causes issues with non-compact covalent bonds. r2SCAN+V provides a practical solution and insights for future functional development.

Abstract: In density functional theory, the SCAN (Strongly Constrained and
Appropriately Normed) and r2SCAN functionals significantly improve over
generalized gradient approximation functionals such as PBE
(Perdew-Burke-Ernzerhof) in predicting electronic, magnetic, and structural
properties across various materials, including transition-metal compounds.
However, there remain puzzling cases where SCAN and r2SCAN underperform, such
as in calculating the band structure of graphene, the magnetic moment of Fe,
the potential energy curve of the Cr2 molecule, and the bond length of VO2.
This research identifies a common characteristic among these challenging
materials: non-compact covalent bonding through s-s, p-p, or d-d electron
hybridization. While SCAN and r2SCAN excel at capturing electron localization
at local atomic sites, they struggle to accurately describe electron
localization in non-compact covalent bonds, resulting in a biased improvement.
To address this issue, we propose the r2SCAN+V approach as a practical
modification that improves accuracy across all the tested materials. The
parameter V is 4 eV for metallic Fe, but substantially lower for the other
cases. Our findings provide valuable insights for the future development of
advanced functionals.

</details>


### [90] [Intermediate-Band Formation in Tm3+-doped Ca2SnO4: A Wide-Gap Oxide Host for Visible-Light Absorption and Energy Applications](https://arxiv.org/abs/2510.16957)
*Shah Hussain,Sikander Azam,Umme Habiba,Qaiser Rafiq,Amin Ur Rahman,Hamada H. Amer,Yasir Saeed*

Main category: cond-mat.mtrl-sci

TL;DR: Rare earth doping transforms stable oxides into multifunctional materials. Tm3+ doping in Ca2SnO4 introduces localized 4f states that reduce band gap, create magnetic moments, enhance covalency, and enable visible light absorption while maintaining mechanical stability.


<details>
  <summary>Details</summary>
Motivation: To understand how localized 4f states from rare earth doping change the structural, electronic, magnetic, and optical properties of wide band gap oxides like Ca2SnO4, enabling multifunctionality.

Method: First principles calculations of pristine and Tm3+ doped Ca2SnO4, including electron localization function analysis to study bonding changes and optical response calculations.

Result: Tm doping introduces intermediate levels reducing optical band gap, creates strong local magnetic moments, enhances covalency with electron pockets stabilizing luminescent centers, and enables visible range absorption with low energy plasmon peaks while maintaining dielectric stability.

Conclusion: Tm doped Ca2SnO4 is a mechanically robust, optically tunable, and magnetically active oxide phosphor suitable for red emission, intermediate band photovoltaics, and spin photon coupling, demonstrating how targeted rare earth substitution enables multifunctionality in wide gap stannates.

Abstract: Rare earth doping is an effective way to convert chemically stable oxides
into multifunctional materials with coupled electronic, optical, and magnetic
properties. We present first principles calculations of pristine and Tm3+ doped
Ca2SnO4 to understand how localized 4f states change the structural,
electronic, magnetic, and optical behavior of the host. Pristine Ca2SnO4 is a
mechanically stable, wide band gap insulator with mostly ionic covalent bonding
and diamagnetic character. Replacing Ca2+ with Tm3+ introduces several key
changes: (i) localized Tm 4f states create intermediate levels inside the wide
gap, reducing the optical band gap; (ii) exchange and spin orbit interactions
generate strong local magnetic moments and spin asymmetry near the conduction
band; (iii) electron localization function analysis shows enhanced covalency
and electron pockets that stabilize luminescent centers; and (iv) the optical
response shows visible range absorption, refractive index features, and low
energy plasmon peaks while maintaining high energy dielectric stability. These
effects make Tm doped Ca2SnO4 a mechanically robust, optically tunable, and
magnetically active oxide phosphor suitable for red emission, intermediate band
photovoltaics, and spin photon coupling. More broadly, our results show how
targeted rare earth substitution can enable multifunctionality in wide gap
stannates and guide the design of next generation spintronic photonic oxides.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [91] [Free energy Wasserstein gradient flow and their particle counterparts: toy model, (degenerate) PL inequalities and exit times](https://arxiv.org/abs/2510.16506)
*Pierre Monmarché*

Main category: math.PR

TL;DR: Analysis of gradient flows with Brownian noise and entropic penalization in infinite-dimensional settings, focusing on particle systems and their finite-dimensional characterizations, with applications to the Curie-Weiss model.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time and metastable behavior of gradient flows perturbed by noise in infinite-dimensional settings, particularly for Wasserstein gradient flows approximated by mean-field interacting particles, which classical results do not cover.

Method: Study a simple class of models where infinite-dimensional behavior is characterized by finite-dimensional processes, using these as benchmarks. Apply insights to the continuous Curie-Weiss model with symmetric double-well potential.

Result: At critical temperature, the N-particle Gibbs measure does not satisfy uniform standard log-Sobolev inequality (optimal constant grows like √N), but satisfies a uniform Lojasiewicz inequality, leading to uniform polynomial convergence rates, propagation of chaos at stationarity and uniformly in time, and creation of chaos.

Conclusion: The proposed framework provides flexible benchmarks for studying infinite-dimensional gradient flows with noise, demonstrating that despite lack of uniform log-Sobolev inequalities, other functional inequalities can ensure desirable convergence properties in particle systems.

Abstract: In finite dimension, the long-time and metastable behavior of a gradient flow
perturbated by a small Brownian noise is well understood. A similar situation
arises when a Wasserstein gradient flow over a space of probability measure is
approximated by a system of mean-field interacting particles, but classical
results do not apply in these infinite-dimensional settings. This work is
concerned with the situation where the objective function of the optimization
problem contains an entropic penalization, so that the particle system is a
Langevin diffusion process. We consider a very simple class of models, for
which the infinite-dimensional behavior is fully characterized by a
finite-dimensional process. The goal is to have a flexible class of benchmarks
to fix some objectives, conjectures and (counter-)examples for the general
situation. Inspired by the systematic study of these toy models, one
application is presented on the continuous Curie-Weiss model in a symmetric
double-well potential. We show that, at the critical temperature, although the
$N$-particle Gibbs measure does not satisfy a uniform-in-$N$ standard
log-Sobolev inequality (the optimal constant growing like $\sqrt{N}$), it does
satisfy a more general Lojasiewicz inequality uniformly in $N$, inducing
uniform polynomial long-time convergence rates, propagation of chaos at
stationarity and uniformly in time, and creation of chaos.

</details>


### [92] [Local pathwise solutions and regularization by noises for the stochastic hyperbolic Keller-Segel equation](https://arxiv.org/abs/2510.17673)
*Tengyu Li,Lei Zhang*

Main category: math.PR

TL;DR: Analysis of stochastic hyperbolic Keller-Segel equation with multiplicative noise, establishing local existence/uniqueness and two global noise-induced regularization results.


<details>
  <summary>Details</summary>
Motivation: To study the Cauchy problem for stochastic hyperbolic Keller-Segel equation with multiplicative noises on the torus, addressing unresolved questions from deterministic setting.

Method: Establish local existence/uniqueness in Sobolev spaces H^s(T^d) for s>d/2+1, then analyze two global regularization scenarios: (1) polynomial-type nonlinear noises with specific intensity thresholds, (2) small initial data or linear multiplicative noises with large intensity.

Result: Local existence/uniqueness proven for appropriate noise regularity. Two global results: (1) unique pathwise solution for large initial data with probability one under polynomial-type noise conditions, (2) unique pathwise solution with high probability for small initial data or linear multiplicative noises with large intensity.

Conclusion: Multiplicative noises can induce regularization effects in stochastic hyperbolic Keller-Segel equation, providing solutions where deterministic counterparts fail, particularly for large initial data and specific noise configurations.

Abstract: In this paper, we investigate the Cauchy problem associated with the
stochastic hyperbolic Keller-Segel (SHKS) equation featuring multiplicative
noises on the torus $\mathbb{T}^d$. First, we establish the local existence and
uniqueness of pathwise solutions to the SHKS equation within Sobolev spaces
$H^s(\mathbb{T}^d)$ for $s>\frac{d}{2}+1$, under appropriate regularity
conditions imposed on the nonlinear multiplicative noises. Subsequently, we
explore two global results pertaining to noise-induced regularization: (1) The
first result demonstrates that for polynomial-type nonlinear noises, when the
noise intensity parameters meet specific threshold conditions, the SHKS
equation possesses a unique pathwise solution for large initial data with
probability one. This finding provides a partial answer to a question that has
remained unresolved in the deterministic setting; (2) The second result reveals
that, for small initial data, or equivalently when dealing with linear
multiplicative noises with sufficiently large intensity (allowed to be
negative), the SHKS equation admits a unique pathwise solution with high
probability.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [93] [Unsupervised Constitutive Model Discovery from Sparse and Noisy Data](https://arxiv.org/abs/2510.13559)
*Vahab Knauf Narouie,Jorge-Humberto Urrea-Quintero,Fehmi Cirak,Henning Wessels*

Main category: cs.CE

TL;DR: statFEM-EUCLID framework combines statistical finite element method with unsupervised constitutive model discovery to improve robustness against noise and data sparsity while maintaining consistency with physical laws.


<details>
  <summary>Details</summary>
Motivation: Existing VFM-based approaches like EUCLID are sensitive to measurement noise and data sparsity, while statFEM alone doesn't enforce constitutive consistency or yield interpretable models.

Method: Coupling statFEM (Bayesian framework for displacement reconstruction) with EUCLID (unsupervised constitutive model discovery) to create statFEM-EUCLID framework for isotropic hyperelastic materials.

Result: The integrated framework reduces sensitivity to noise and data sparsity while ensuring reconstructed fields remain consistent with both equilibrium and constitutive laws.

Conclusion: statFEM-EUCLID successfully combines the strengths of both approaches, providing more robust constitutive model discovery under practical measurement constraints.

Abstract: Recently, unsupervised constitutive model discovery has gained attention
through frameworks based on the Virtual Fields Method (VFM), most prominently
the EUCLID approach. However, the performance of VFM-based approaches,
including EUCLID, is affected by measurement noise and data sparsity, which are
unavoidable in practice. The statistical finite element method (statFEM) offers
a complementary perspective by providing a Bayesian framework for assimilating
noisy and sparse measurements to reconstruct the full-field displacement
response, together with quantified uncertainty. While statFEM recovers
displacement fields under uncertainty, it does not strictly enforce consistency
with constitutive relations or aim to yield interpretable constitutive models.
In this work, we couple statFEM with unsupervised constitutive model discovery
in the EUCLID framework, yielding statFEM--EUCLID. The framework is
demonstrated for isotropic hyperelastic materials. The results show that this
integration reduces sensitivity to noise and data sparsity, while ensuring that
the reconstructed fields remain consistent with both equilibrium and
constitutive laws.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [94] [Asymptotically well-balanced geostrophic reconstruction finite volumes numerical schemes for the 2D rotating NLSWE in spherical coordinates](https://arxiv.org/abs/2510.16002)
*Alejandro González del Pino,Manuel Jesús Castro Díaz,Jorge Macías Sánchez*

Main category: physics.ao-ph

TL;DR: Development of second- and third-order finite-volume schemes for 2D rotating shallow-water equations that preserve geostrophic equilibrium for simulating meteotsunamis.


<details>
  <summary>Details</summary>
Motivation: To design reliable forecasting models for meteotsunamis - long-wave events generated by atmospheric pressure disturbances that amplify near coasts.

Method: Second- and third-order finite-volume numerical schemes applied to 2D rotating shallow-water equations in spherical coordinates, designed to preserve geostrophic equilibrium as Rossby number approaches zero.

Result: Numerical results from analytical and real-world test cases demonstrate the importance of maintaining geostrophic equilibrium over time.

Conclusion: The developed schemes successfully preserve geostrophic equilibrium, making them suitable for reliable meteotsunami forecasting models.

Abstract: The dynamics of large-scale geophysical fluids is primarily governed by the
balance between the Coriolis force and the pressure gradient.
  This phenomenon, known as geostrophic equilibrium, is the basis for the
geostrophic model, which has proven to be extremely useful for understanding
and forecasting large-scale atmospheric and oceanic dynamics.
  In the present work, we develop second- and third-order finite-volume
numerical schemes applied to the 2D rotating shallow-water equations in
spherical coordinates. These schemes are designed to preserve the geostrophic
equilibrium in the limit as the Rossby number tends to zero.
  The final goal is to design reliable and efficient forecasting models for
simulating meteotsunamis, long-wave events generated in the ocean by
atmospheric pressure disturbances. These disturbances produce long waves of
small amplitude that gradually amplify as they approach the coast.
  The numerical results for various analytical and real-world test cases
underscore the importance of maintaining geostrophic equilibrium over time.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [95] [Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator](https://arxiv.org/abs/2510.16816)
*Ming Zhong,Zhenya Yan*

Main category: cs.LG

TL;DR: LANO is a neural operator that uses agent tokens to achieve linear complexity while maintaining softmax attention performance, improving accuracy by 19.5% over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the scalability-accuracy trade-off in transformer-based neural operators where softmax attention has quadratic complexity and linear attention variants suffer accuracy degradation.

Method: Introduces agent-based attention with a compact set of M agent tokens (M << N) that mediate global interactions among N tokens, creating linear complexity O(MNd) while preserving softmax attention's expressive power.

Result: Achieves 19.5% average accuracy improvement over state-of-the-art neural PDE solvers, including Transolver with slice-based softmax attention, across standard benchmarks.

Conclusion: LANO bridges the gap between linear complexity and softmax-level performance, establishing a scalable, high-accuracy foundation for scientific machine learning applications.

Abstract: Neural operators offer a powerful data-driven framework for learning mappings
between function spaces, in which the transformer-based neural operator
architecture faces a fundamental scalability-accuracy trade-off: softmax
attention provides excellent fidelity but incurs quadratic complexity
$\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,
while linear attention variants reduce cost to $\mathcal{O}(N d^2)$ but often
suffer significant accuracy degradation. To address the aforementioned
challenge, in this paper, we present a novel type of neural operators, Linear
Attention Neural Operator (LANO), which achieves both scalability and high
accuracy by reformulating attention through an agent-based mechanism. LANO
resolves this dilemma by introducing a compact set of $M$ agent tokens $(M \ll
N)$ that mediate global interactions among $N$ tokens. This agent attention
mechanism yields an operator layer with linear complexity $\mathcal{O}(MN d)$
while preserving the expressive power of softmax attention. Theoretically, we
demonstrate the universal approximation property, thereby demonstrating
improved conditioning and stability properties. Empirically, LANO surpasses
current state-of-the-art neural PDE solvers, including Transolver with
slice-based softmax attention, achieving average $19.5\%$ accuracy improvement
across standard benchmarks. By bridging the gap between linear complexity and
softmax-level performance, LANO establishes a scalable, high-accuracy
foundation for scientific machine learning applications.

</details>


### [96] [Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides](https://arxiv.org/abs/2510.17569)
*Jyler Menard,R. A. Mansbach*

Main category: cs.LG

TL;DR: The paper investigates using dimensionality reduction on variational autoencoder latent spaces to improve antimicrobial peptide design by enhancing interpretability, compression, and organization with physicochemical properties.


<details>
  <summary>Details</summary>
Motivation: Deep generative models like variational autoencoders are effective for peptide design but lack interpretability and rigorous quantification of latent space quality as a search space.

Method: The study investigates dimensionality reduction of latent spaces, interpretability analysis, and organization of latent spaces with physicochemical properties to improve antimicrobial activity optimization.

Result: Further reduction of latent space via dimensionality reduction is advantageous when organizing with relevant information, provides more interpretable search spaces, and allows organization with different physicochemical properties at various label availability levels.

Conclusion: Dimensionality reduction can improve latent space organization for antimicrobial peptide design, enhancing both interpretability and optimization efficiency when combined with physicochemical property guidance.

Abstract: Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat
bacterial infections. Discovering and designing such peptides is difficult
because of the vast number of possible sequences of amino acids. Deep
generative models, such as variational autoencoders, have shown value in
peptide design due to their ability to model sequence space with a
continuous-valued latent space. Although such models have already been used to
great effect in biomolecular design, they still suffer from a lack of
interpretability and rigorous quantification of latent space quality as a
search space. We investigate (1) whether further compression of the design
space via dimensionality reduction may facilitate optimization, (2) the
interpretability of the spaces, and (3) how organizing latent spaces with
physicochemical properties may improve the efficiency of optimizing
antimicrobial activity. We find that further reduction of the latent space via
dimensionality reduction can be advantageous when organizing the space with
more relevant information at data availability, that using the dimensionality
reduction search space can be more interpretable, and that we can organize the
latent space with different physicochemical properties even at different
percentages of available labels.

</details>


### [97] [Trace Regularity PINNs: Enforcing $\mathrm{H}^{\frac{1}{2}}(\partial Ω)$ for Boundary Data](https://arxiv.org/abs/2510.16817)
*Doyoon Kim,Junbin Song*

Main category: cs.LG

TL;DR: TRPINN is an enhanced PINN that enforces boundary loss in the correct Sobolev-Slobodeckij norm H^{1/2}, reducing computational cost and improving convergence stability.


<details>
  <summary>Details</summary>
Motivation: Standard PINNs have limitations in handling boundary conditions, particularly with oscillatory data, and may not converge properly due to improper norm enforcement.

Method: Uses Trace Regularity Physics-Informed Neural Network (TRPINN) with H^{1/2} boundary norm, computes only essential semi-norm portion, avoids denominator evaluations, and employs Neural Tangent Kernel analysis.

Result: TRPINN converges to true solution in H^1 sense, converges faster than standard PINNs, succeeds where standard PINNs fail on oscillatory boundary conditions, and achieves 1-3 decimal digit performance improvements.

Conclusion: TRPINN provides theoretically sound boundary enforcement with improved convergence and computational efficiency, making it superior to standard PINNs for problems with challenging boundary conditions.

Abstract: We propose an enhanced physics-informed neural network (PINN), the Trace
Regularity Physics-Informed Neural Network (TRPINN), which enforces the
boundary loss in the Sobolev-Slobodeckij norm $H^{1/2}(\partial \Omega)$, the
correct trace space associated with $H^1(\Omega)$. We reduce computational cost
by computing only the theoretically essential portion of the semi-norm and
enhance convergence stability by avoiding denominator evaluations in the
discretization. By incorporating the exact $H^{1/2}(\partial \Omega)$ norm, we
show that the approximation converges to the true solution in the
$H^{1}(\Omega)$ sense, and, through Neural Tangent Kernel (NTK) analysis, we
demonstrate that TRPINN can converge faster than standard PINNs. Numerical
experiments on the Laplace equation with highly oscillatory Dirichlet boundary
conditions exhibit cases where TRPINN succeeds even when standard PINNs fail,
and show performance improvements of one to three decimal digits.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [98] [Communication-Efficient and Memory-Aware Parallel Bootstrapping using MPI](https://arxiv.org/abs/2510.16284)
*Di Zhang*

Main category: cs.DC

TL;DR: Parallel bootstrapping algorithms using MPI that reduce communication overhead and memory constraints through local statistic aggregation and synchronized pseudo-random number generation.


<details>
  <summary>Details</summary>
Motivation: Bootstrapping becomes computationally prohibitive for large datasets or high resample counts due to its computational cost.

Method: Two novel strategies: Local Statistic Aggregation (transmitting sufficient statistics instead of full datasets) and Synchronized Pseudo-Random Number Generation (enabling distributed resampling when datasets don't fit on single processes).

Result: Significant reductions in communication volume and memory usage, enabling scalable parallel bootstrapping on large-scale systems.

Conclusion: The proposed methods facilitate efficient parallel bootstrapping by addressing key challenges of communication overhead and memory constraints in distributed environments.

Abstract: Bootstrapping is a powerful statistical resampling technique for estimating
the sampling distribution of an estimator. However, its computational cost
becomes prohibitive for large datasets or a high number of resamples. This
paper presents a theoretical analysis and design of parallel bootstrapping
algorithms using the Message Passing Interface (MPI). We address two key
challenges: high communication overhead and memory constraints in distributed
environments. We propose two novel strategies: 1) Local Statistic Aggregation,
which drastically reduces communication by transmitting sufficient statistics
instead of full resampled datasets, and 2) Synchronized Pseudo-Random Number
Generation, which enables distributed resampling when the entire dataset cannot
be stored on a single process. We develop analytical models for communication
and computation complexity, comparing our methods against naive baseline
approaches. Our analysis demonstrates that the proposed methods offer
significant reductions in communication volume and memory usage, facilitating
scalable parallel bootstrapping on large-scale systems.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [99] [Spontaneous rotation and propulsion of suspended capsules in active nematics](https://arxiv.org/abs/2510.17643)
*Júlio P. A. Santos,Margarida M. Telo da Gama,Rodrigo C. V. Coelho*

Main category: cond-mat.soft

TL;DR: Elastic capsules in 2D active nematics show diverse behaviors including rotation and directed motion, influenced by geometry, internal/external activity, and flexibility.


<details>
  <summary>Details</summary>
Motivation: To understand how shape, deformability, and topological defects interact to produce emergent motility in soft active matter systems.

Method: Lattice Boltzmann simulations of flexible membrane capsules suspended in two-dimensional active nematic fluids.

Result: Circular capsules rotate persistently due to internal +1/2 defects; axisymmetric capsules move directionally along symmetry axes; flexibility suppresses motility by dissipating active stresses into shape deformations.

Conclusion: Shape, deformability, and defect dynamics cooperate to produce emergent motility, with potential applications in microswimmer design and drug delivery.

Abstract: We investigate the dynamics of elastic capsules suspended in two-dimensional
active nematic fluids using lattice Boltzmann simulations. The capsules,
modeled as flexible membranes enclosing active internal regions, exhibit a rich
variety of behaviors shaped by their geometry and the interplay between
internal and external activity. Circular capsules with active interiors undergo
persistent rotation driven by internally confined +1/2 topological defects.
Axisymmetric capsules, such as boomerangs, develop directed motion along their
axis of symmetry due to unbalanced active forces generated by defect
distributions near their boundaries. We further show that capsule flexibility
suppresses motility and rotation, as active stresses are dissipated into shape
deformations. These findings reveal how shape, deformability, and defect
dynamics cooperate to produce emergent motility in soft active matter, with
potential applications in the design of microswimmers and drug delivery
vehicles.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [100] [Equivalence of additive and parametric pinning control protocols for systems of weakly coupled oscillators](https://arxiv.org/abs/2510.16766)
*Riccardo Muolo,Yuzuru Kato*

Main category: math.OC

TL;DR: Two pinning control approaches (additive and parametric) are shown to be equivalent for weakly coupled periodic oscillators using phase reduction theory.


<details>
  <summary>Details</summary>
Motivation: To understand and compare different pinning control methods for nonlinear networked systems, particularly for synchronization control given its wide applicability.

Method: Used phase reduction technique to analyze equivalence between additive pinning (external input) and parametric pinning (parameter variation), validated with numerical simulations on coupled Stuart-Landau oscillators.

Result: Demonstrated that both pinning control approaches are equivalent for weakly coupled systems exhibiting periodic oscillatory behaviors.

Conclusion: The equivalence between additive and parametric pinning control methods enables broader applications of pinning control in real-world systems.

Abstract: Controlling the behavior of nonlinear systems on networks is a paramount task
in control theory, in particular the control of synchronization, given its vast
applicability. In this work, we focus on pinning control and we examine two
different approaches: the first, more common in engineering applications, where
the control is implemented through an external input (additive pinning); the
other, where the parameters of the pinned nodes are varied (parametric
pinning). By means of the phase reduction technique, we show that the two
pinning approaches are equivalent for weakly coupled systems exhibiting
periodic oscillatory behaviors. Through numerical simulations, we validate the
claim for a system of coupled Stuart--Landau oscillators. Our results pave the
way for further applications of pinning control in real-world systems.

</details>


### [101] [Agent-Based Optimal Control for Image Processing](https://arxiv.org/abs/2510.16154)
*Alessio Oliviero,Simone Cacace,Giuseppe Visconti*

Main category: math.OC

TL;DR: This paper proposes using multi-agent systems for image processing tasks like color quantization and segmentation, framing it as an optimal control problem and solving it with primal-dual splitting and the method of multipliers.


<details>
  <summary>Details</summary>
Motivation: To explore multi-agent systems for solving classical image processing tasks, particularly color quantization and segmentation, by treating them as optimal control problems.

Method: Frame the task as an optimal control problem to steer multi-agent dynamics for color clustering, balancing total variation of color field and fidelity to original image. Use primal-dual splitting and method of multipliers for solution, with parallel CUDA implementation.

Result: Numerical experiments demonstrate the efficacy of the approach and its potential for handling high-dimensional data.

Conclusion: Multi-agent systems combined with optimal control formulation and primal-dual methods provide an effective approach for image processing tasks like color quantization and segmentation, with scalability to high-dimensional data.

Abstract: We investigate the use of multi-agent systems to solve classical image
processing tasks, such as colour quantization and segmentation. We frame the
task as an optimal control problem, where the objective is to steer the
multi-agent dynamics to obtain colour clusters that segment the image. To do
so, we balance the total variation of the colour field and fidelity to the
original image. The solution is obtained resorting to primal-dual splitting and
the method of multipliers. Numerical experiments, implemented in parallel with
CUDA, demonstrate the efficacy of the approach and its potential for
high-dimensional data.

</details>


### [102] [Optimal Trajectories for Optimal Transport in Nonuniform Environments](https://arxiv.org/abs/2510.17170)
*Luca Dieci,Daniyar Omarov*

Main category: math.OC

TL;DR: The paper solves discrete optimal transport problems in nonuniform environments by formulating Euler-Lagrange equations for optimal path finding and providing verifiable optimality conditions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of forming cost matrices for optimal transport in nonuniform environments, which requires finding optimal paths between points.

Method: Formulate and solve Euler-Lagrange equations for optimal path finding, develop new algorithms for solving the discrete optimal transport problem.

Result: Provides verifiable sufficient conditions for solution optimality and demonstrates algorithm performance through numerical examples.

Conclusion: The proposed approach successfully solves discrete optimal transport in nonuniform settings with theoretical guarantees and effective algorithms.

Abstract: In this work, we solve a discrete optimal transport problem in a nonuniform
environment. The key challenge is to form the cost matrix, which requires
finding the optimal path between two points, and for this task we formulate and
solve the associated Euler-Lagrange equations. A main theoretical result of
ours is to provide verifiable sufficient conditions of optimality of the
solution of the Euler-Lagrange equation. We propose new algorithms to solve the
problem, and illustrate our results and performance of the algorithms on
several numerical examples.

</details>


### [103] [An Inexact General Descent Method with Applications in Differential Equation-Constrained Optimization](https://arxiv.org/abs/2510.17581)
*Humberto Gimenes Macedo,Luís Felipe Bueno*

Main category: math.OC

TL;DR: The paper proposes an inexact general descent framework for optimization with approximate gradients, particularly relevant for differential equation-constrained optimization where gradients depend on iterative solvers.


<details>
  <summary>Details</summary>
Motivation: Gradient evaluations are often approximate in practical applications, especially in differential equation-constrained optimization where discrete adjoint gradients depend on iterative solvers. This motivates developing optimization methods that remain reliable under inexact first-order information.

Method: An inexact general descent framework with global convergence theory under two step-size regimes: bounded step sizes with error tolerance proportional to gradient norm, and diminishing step sizes with summable tolerance sequence. Implemented through inexact gradient descent and an inexact BFGS-like method.

Result: The framework was tested on a second-order ODE inverse problem and a two-dimensional Laplace inverse problem using discrete adjoint gradients with adaptive accuracy. Adaptive inexact gradients consistently reduced optimization time compared to fixed tight tolerances, and incorporating curvature information further improved efficiency.

Conclusion: The proposed inexact optimization framework with adaptive gradient accuracy effectively handles approximate gradient information while maintaining convergence, demonstrating practical efficiency improvements in differential equation-constrained optimization problems.

Abstract: In many applications, gradient evaluations are inherently approximate,
motivating the development of optimization methods that remain reliable under
inexact first-order information. A common strategy in this context is adaptive
evaluation, whereby coarse gradients are used in early iterations and refined
near a minimizer. This is particularly relevant in differential
equation-constrained optimization (DECO), where discrete adjoint gradients
depend on iterative solvers. Motivated by DECO applications, we propose an
inexact general descent framework and establish its global convergence theory
under two step-size regimes. For bounded step sizes, the analysis assumes that
the error tolerance in the computed gradient is proportional to its norm,
whereas for diminishing step sizes, the tolerance sequence is required to be
summable. The framework is implemented through inexact gradient descent and an
inexact BFGS-like method, whose performance is demonstrated on a second-order
ODE inverse problem and a two-dimensional Laplace inverse problem using
discrete adjoint gradients with adaptive accuracy. Across these examples,
adaptive inexact gradients consistently reduced optimization time relative to
fixed tight tolerances, while incorporating curvature information further
improved overall efficiency.

</details>


### [104] [Periodic limit for non-autonomous Lagrangian systems and applications to a Kuramoto type model](https://arxiv.org/abs/2510.17242)
*Veronica Danesi,Cristian Mendico,Xuan Tao,Kaizhi Wang*

Main category: math.OC

TL;DR: This paper studies non-autonomous Lagrangian systems where the Tonelli Lagrangian converges to a time-periodic function, showing convergence of Lax-Oleinik semigroups to periodic solutions and gradient graph convergence in Hausdorff distance.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of non-autonomous Lagrangian systems when they approach time-periodic limits, particularly focusing on convergence properties and applications to physical models.

Method: Constructs a suitable Lax-Oleinik semigroup for continuous initial conditions and analyzes its convergence properties toward periodic solutions, with gradient graph convergence in Hausdorff distance.

Result: Proves that the constructed Lax-Oleinik semigroup converges to a periodic solution, and its gradient graph converges to the gradient graph of the periodic limit function in Hausdorff distance.

Conclusion: The results are applied to a Kuramoto-type model, demonstrating the existence of an invariant torus given by the gradient graph of the limiting periodic solution of the Hamilton-Jacobi equation.

Abstract: This paper explores the asymptotic properties of non-autonomous Lagrangian
systems, assuming that the associated Tonelli Lagrangian converges to a
time-periodic function. Specifically, given a continuous initial condition, we
provide a suitable construction of a Lax-Oleinik semigroup such that it
converges toward a periodic solution of the equation. Moreover, the graph of
its gradient converges as time tends to infinity to the graph of the gradient
of the periodic limit function with respect to the Hausdorff distance. Finally,
we apply this result to a Kuramoto-type model, proving the existence of an
invariant torus given by the graph of the gradient of the limiting periodic
solution of the Hamilton-Jacobi equation.

</details>
