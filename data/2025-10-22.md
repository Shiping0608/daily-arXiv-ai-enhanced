<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 13]
- [math.AP](#math.AP) [Total: 20]
- [physics.comp-ph](#physics.comp-ph) [Total: 6]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [math.FA](#math.FA) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [cs.DC](#cs.DC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [math.DS](#math.DS) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Image reconstruction from structured subsampled 2D Fourier data](https://arxiv.org/abs/2510.18045)
*Gerlind Plonka,Anahita Riahi*

Main category: math.NA

TL;DR: Study of image reconstruction from incomplete 2D Fourier samples, focusing on parallel MRI sampling patterns with few acquired rows. Shows importance of low-pass rows around zero frequency and proposes hybrid TV minimization algorithm.


<details>
  <summary>Details</summary>
Motivation: Inspired by parallel MRI requirements, need to reconstruct images from incomplete Fourier samples with special sampling patterns containing few acquired rows.

Method: Proposed hybrid algorithm combining TV minimization via primal-dual optimization with specialized reconstruction exploiting sampling pattern properties.

Result: Achieves very good performance for natural and cartoon-like images with data reduction rates up to 8 for complex images and 16 for real images.

Conclusion: Non-adaptive approaches are insufficient; careful choice of low-pass row width is crucial, and the proposed hybrid algorithm provides optimal reconstruction results.

Abstract: In this paper we study the performance of image reconstruction methods from
incomplete samples of the 2D discrete Fourier transform. Inspired by
requirements in parallel MRI, we focus on a special sampling pattern with a
small number of acquired rows of the Fourier transformed image. We show the
importance of the low-pass set of acquired rows around zero in the Fourier
space for image reconstruction. A suitable choice of the width $L$ of this
index set depends on the image data and is crucial to achieve optimal
reconstruction results. We prove that non-adaptive reconstruction approaches
cannot lead to satisfying recovery results. We propose a new hybrid algorithm
which connects the TV minimization technique based on primal-dual optimization
with a recovery algorithm which exploits properties of the special sampling
pattern for reconstruction. Our method shows very good performance for natural
images as well as for cartoon-like images for a data reduction rate up to 8 in
the complex setting and even 16 for real images.

</details>


### [2] [Estimation of a Gas Diffusion Coefficient by Fitting Molecular Dynamics Trajectories to Finite-Difference Simulations](https://arxiv.org/abs/2510.18191)
*Isaac Viviano*

Main category: math.NA

TL;DR: A method to estimate argon diffusion coefficient in helium using MD simulations and continuum diffusion modeling with parameter optimization.


<details>
  <summary>Details</summary>
Motivation: To develop a computational procedure for estimating gas diffusion coefficients by combining molecular dynamics with continuum modeling.

Method: Used LAMMPS for 2D MD simulations of argon-helium system with Lennard-Jones potential, combined with finite-difference solution of diffusion equation, and optimized diffusion coefficient using nonlinear least squares.

Result: Showed effects of binning parameters and grid spacing on results, and compared estimated diffusion coefficient with experimental measurements.

Conclusion: Successfully demonstrated a computational approach for diffusion coefficient estimation that bridges molecular dynamics and continuum modeling.

Abstract: A procedure is presented to estimate the diffusion coefficient of a uniform
patch of argon gas in a uniform background of helium gas. Molecular Dynamics
(MD) simulations of the two gases interacting through the Lennard-Jones
potential are carried out using the LAMMPS software package. In addition,
finite-difference (FD) calculations are used to solve the continuum diffusion
equation for the argon concentration with a given diffusion coefficient. To
contain the computational cost and facilitate data visualization, both MD and
FD computations were done in two space dimensions. The MD argon trajectories
were binned to the FD grid, and the optimal diffusion coefficient was estimated
by minimizing the difference between the binned MD data and the FD solution
with a nonlinear least squares procedure (Levenberg-Marquardt algorithm).
Numerical results show the effect of the MD binning parameter and FD grid
spacing. The estimated diffusion coefficient is compared to an experimental
measurement.

</details>


### [3] [An Explicit Euler-type Scheme for Lévy-driven SDEs with Superlinear and Time-Irregular Coefficients](https://arxiv.org/abs/2510.18222)
*Sani Biswas,Joaquin Fontbona*

Main category: math.NA

TL;DR: A randomized tamed Euler scheme for Lévy-driven SDEs with superlinear random coefficients and Carathéodory-type drift achieves optimal strong convergence rate close to 0.5.


<details>
  <summary>Details</summary>
Motivation: To address numerical challenges in Lévy-driven SDEs with superlinear coefficients and time-irregular drifts, which haven't been covered in previous works.

Method: Combines drift randomization to handle low time-regularity with taming technique to control superlinear state dependence.

Result: The proposed scheme achieves optimal strong L²-convergence rate arbitrarily close to 0.5, and extends to Lévy-driven SDDEs with Markovian switching.

Conclusion: This is the first work addressing superlinear coefficients in numerical analysis of Carathéodory-type SDEs and ordinary differential equations.

Abstract: This paper introduces a randomized tamed Euler scheme tailored for
L\'evy-driven stochastic differential equations (SDEs) with superlinear random
coefficients and Carath\'eodory-type drift. Under assumptions that allow for
time-irregular drifts while ensuring appropriate time-regularity of the
diffusion and jump coefficients, the proposed scheme is shown to achieve the
optimal strong $\mathcal{L}^2$-convergence rate, arbitrarily close to $0.5$.
  A crucial component of our methodology is the incorporation of drift
randomization, which overcomes challenges due to low time-regularity, along
with a taming technique to handle the superlinear state dependence.
  Our analysis moreover covers settings where the coefficients are random,
providing for instance strong convergence of randomized tamed Euler schemes for
L\'evy-driven stochastic delay differential equations (SDDEs) with Markovian
switching. To our knowledge, this is the first {work} that addresses the case
of superlinear coefficients in the numerical analysis of Carath\'eodory-type
SDEs and even for ordinary differential equations.

</details>


### [4] [SPIKE: Stable Physics-Informed Kernel Evolution Method for Solving Hyperbolic Conservation Laws](https://arxiv.org/abs/2510.18266)
*Hua Su,Lei Zhang,Jin Zhao*

Main category: math.NA

TL;DR: SPIKE method resolves the paradox of using strong-form residual minimization to capture weak solutions with discontinuities in hyperbolic conservation laws through regularized kernel evolution.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental paradox of how strong-form residual minimization can capture weak solutions containing discontinuities in inviscid hyperbolic conservation laws.

Method: Uses reproducing kernel representations with Tikhonov-regularized parameter evolution, providing smooth transition through shock formation without explicit shock detection or artificial viscosity.

Result: Numerical validation across scalar and vector-valued conservation laws confirms the method effectively maintains conservation, tracks characteristics, and captures shocks satisfying Rankine-Hugoniot conditions.

Conclusion: SPIKE provides a unified framework for numerical computation of inviscid hyperbolic conservation laws that automatically handles discontinuities without specialized treatment.

Abstract: We introduce the Stable Physics-Informed Kernel Evolution (SPIKE) method for
numerical computation of inviscid hyperbolic conservation laws. SPIKE resolves
a fundamental paradox: how strong-form residual minimization can capture weak
solutions containing discontinuities. SPIKE employs reproducing kernel
representations with regularized parameter evolution, where Tikhonov
regularization provides a smooth transition mechanism through shock formation,
allowing the dynamics to traverse shock singularities. This approach
automatically maintains conservation, tracks characteristics, and captures
shocks satisfying Rankine-Hugoniot conditions within a unified framework
requiring no explicit shock detection or artificial viscosity. Numerical
validation across scalar and vector-valued conservation laws confirms the
method's effectiveness.

</details>


### [5] [Provably realizability-preserving finite volume method for quadrature-based moment models of kinetic equations](https://arxiv.org/abs/2510.18380)
*Chuan Fan,Qian Huang,Kailiang Wu*

Main category: math.NA

TL;DR: A provably realizability-preserving finite-volume method for five-moment systems using Gaussian-EQMOM and HyQMOM closures, with rigorous HLL flux construction and explicit CFL constraints.


<details>
  <summary>Details</summary>
Motivation: Preserving moment realizability in quadrature-based moment methods is essential but challenging due to strong nonlinear coupling and lack of explicit conservative-to-flux maps.

Method: Recast realizability condition into nonnegative quadratic form, construct tailored HLL flux with rigorously derived wave speeds and intermediate states, implement practical limiter for interface states.

Result: Proved sufficient realizability-preserving conditions under explicit CFL constraints, demonstrated accuracy, robustness in low-density regions, and realizability for both closures.

Conclusion: Framework unifies realizability preservation for hyperbolic moment systems with complex closures and extends naturally to higher-order discretizations.

Abstract: Quadrature-based moment methods (QBMM) provide tractable closures for
multiscale kinetic equations, with diverse applications across aerosols,
sprays, and particulate flows, etc. However, for the derived hyperbolic
moment-closure systems, seeking numerical schemes preserving moment
realizability is essential yet challenging due to strong nonlinear coupling and
the lack of explicit conservative-to-flux maps. This paper proposes and
analyzes a provably realizability-preserving finite-volume method for
five-moment systems closed by the two-node Gaussian-EQMOM and three-point
HyQMOM. Rather than relying on kinetic fluxes, we recast the realizability
condition into a nonnegative quadratic form in the moment vector, reducing the
original nonlinear constraints to bilinear inequalities amenable to analysis.
On this basis, we construct a tailored Harten--Lax--van Leer (HLL) flux with
rigorously derived wave speeds and intermediate states that embed realizability
directly into the flux evaluation. We prove sufficient realizability-preserving
conditions under explicit Courant--Friedrichs--Lewy (CFL) constraints in the
collisionless case, and for BGK relaxation, we obtain coupled time-step
conditions involving a realizability radius; a semi-implicit BGK variant
inherits the collisionless CFL. From a multiscale perspective, the analysis
yields stability conditions uniform in the relaxation time and supports
stiff-to-kinetic transitions. A practical limiter enforces strict realizability
of reconstructed interface states without degrading accuracy. Numerical
experiments demonstrate the accuracy, robustness in low-density regions, and
realizability for both closures. This framework unifies realizability
preservation for solving hyperbolic moment systems with complex closures and
extends naturally to higher-order space--time discretizations.

</details>


### [6] [Prescribed Eigenvalues via Optimal Perturbation of main-diagonal submatrix](https://arxiv.org/abs/2510.18486)
*M. R. Eslahchi,E. Kokabifar*

Main category: math.NA

TL;DR: This paper presents a method for computing optimal perturbations to specific diagonal blocks of a block-diagonal matrix to achieve prescribed eigenvalues, with applications and numerical validation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop an efficient method for optimally perturbing specific blocks in block-diagonal matrices to achieve desired eigenvalue properties, extending previous work in this area.

Method: The method improves and extends previous methodology, definitions, and lemmas for constructing optimal perturbations of preassigned diagonal blocks in block-diagonal matrices to achieve prescribed eigenvalues.

Result: Numerical experiments demonstrate the validity and effectiveness of the proposed method for computing optimal perturbations that achieve the desired eigenvalues.

Conclusion: The paper successfully presents a method for optimal block perturbation in block-diagonal matrices to achieve prescribed eigenvalues, with practical applications and numerical validation.

Abstract: Consider a given square matrix $\textrm {K}$ with square blocks
$A_{11},A_{22},\ldots,A_{nn}$ on the main diagonal. This paper aims to compute
an optimal perturbation $\Delta$ of a preassigned block
$A_{ii}\in\mathbb{C}^{d_i\times d_k}, \left(1\le i\le n\right)$,with respect to
the spectral norm distance, such that the perturbed matrix ${\textrm {K}_X}$
has $k \le d_i$ prescribed eigenvalues. This paper presents a method for
constructing the optimal perturbation by improving and extending the
methodology, necessary definitions and lemmas of previous related works. Some
conceivable applications of this subject are also presented. Numerical
experiments are provided to illustrate the validity of the method.

</details>


### [7] [Multi-subspace power method for decomposing all tensors](https://arxiv.org/abs/2510.18627)
*Kexin Wang,João M. Pereira,Joe Kileel,Anna Seigal*

Main category: math.NA

TL;DR: A generalized algorithm for decomposing low rank tensors of any symmetry type, extending the subspace power method to all tensors and introducing shifted power method for computing partially symmetric singular vector tuples.


<details>
  <summary>Details</summary>
Motivation: To develop a unified approach for tensor decomposition that works across all symmetry types (from fully asymmetric to fully symmetric), overcoming limitations of existing methods that are specialized for specific symmetry types.

Method: Transforms input tensors into tensors with orthonormal slices, then uses a shifted power method to compute partially symmetric singular vector tuples (pSVTs) with singular value one, which correspond to decomposition summands.

Result: The algorithm achieves higher accuracy and faster runtime than existing methods, with proven global convergence for the shifted power method used to compute pSVTs.

Conclusion: The proposed method provides a comprehensive solution for tensor decomposition across all symmetry types, with theoretical guarantees and superior practical performance compared to existing approaches.

Abstract: We present an algorithm for decomposing low rank tensors of any symmetry
type, from fully asymmetric to fully symmetric. It generalizes the recent
subspace power method from symmetric tensors to all tensors. The algorithm
transforms an input tensor into a tensor with orthonormal slices. We show that
for tensors with orthonormal slices and low rank, the summands of their
decomposition are in one-to-one correspondence with the partially symmetric
singular vector tuples (pSVTs) with singular value one. We use this to show
correctness of the algorithm. We introduce a shifted power method for computing
pSVTs and establish its global convergence. Numerical experiments demonstrate
that our decomposition algorithm achieves higher accuracy and faster runtime
than existing methods.

</details>


### [8] [AutoVARP -- a framework for automated reproducible inducibility testing in computational models of cardiac electrophysiology](https://arxiv.org/abs/2510.18635)
*Paolo Seghetti,Matthias Gsell,Anton Prassk,Martin Bishop,Gernot Plank*

Main category: math.NA

TL;DR: AutoVARP is a framework for standardizing virtual arrhythmia inducibility studies in cardiac electrophysiology simulations, addressing reproducibility and standardization issues through automated tools built on openCARP and carputils.


<details>
  <summary>Details</summary>
Motivation: To overcome the barriers preventing translation of cardiac electrophysiology simulations from research to clinical applications, specifically the lack of reproducibility and standardized procedures in virtual arrhythmia induction studies.

Method: The framework separates induction studies into four stages: pre-pacing with forCEPSS, S1 pacing for steady state, S2 induction with different extrastimuli, and testing sustenance of induced reentries. It uses input files for standardization and is built on openCARP/carputils.

Result: AutoVARP effectively addresses standardization and reproducibility gaps, provides uniform methodology for non-expert users, is highly scalable across different anatomies, and was demonstrated on a large virtual subject cohort and biventricular mesh.

Conclusion: AutoVARP provides automated tools for sharing setups without requiring process re-implementation, making virtual arrhythmia inducibility studies more accessible and reproducible, though it offers less flexibility than custom implementations.

Abstract: Simulations of Cardiac Electrophysiology are gaining momentum beyond basic
mechanistic studies, as an approach for supporting clinical decision making.
The potential for in silico technologies observed from the research community
is immense, with studies demonstrating significantly improved therapeutical
outcome with little to no additional burden for patients. Two main factors
hinder the translation of these technologies from pure research to
applications: virtually no reproducibility of results, and lack of standardized
procedures. Inspired by a previously published virtual induction study by
Arevalo et al. (2016), We address the issues of reproducibility and
standardization providing autoVARP, a framework for standardization of virtual
arrhythmia inducibility studies, built upon openCARP and the carputils
framework. Standardization relies on the previously published forCEPSS
framework and is ensured by defining the whole induction study with input files
that can be easily shared. Our approach also ensures numerical efficiency by
separating the induction study into four stages: (i) pre-pacing with forCEPSS,
(ii) S1 pacing tor each steady state, (iii) S2 induction with different
extrastimuli, (iv) testing of sustenance of induced reentries. We demonstrate
the approach in a large virtual subject cohort to investigate numerical
artifacts that may arise when improper setups are provided to perform virtual
induction, and additionally showcase autoVARP in a biventricular mesh. AutoVARP
addresses effectively the current gap in standardization and reproducibility of
results providing a uniform methodology that can be implemented even by non
expert users. AutoVARP is highly scalable and adaptable to markedly different
anatomies. Although less flexible than in house implementations it provides
automated tools to share setups and does not require re-implementation of any
process.

</details>


### [9] [The numerical solution of the Dirichlet generalized and classical harmonic problems for irregular n-sided pyramidal domains by the method of probabilistic solutions](https://arxiv.org/abs/2510.18667)
*M. Zakradze,Z. Tabagari,N. Koblishvili,T. Davitashvili,J. M. Sanchez-Saez,F.,Criado-Aldeanueva*

Main category: math.NA

TL;DR: The paper applies probabilistic solutions method (MPS) using Wiener process simulation to solve Dirichlet harmonic problems for irregular pyramidal domains with discontinuous boundary functions.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method for solving generalized Dirichlet harmonic problems on irregular pyramidal domains where boundary functions have discontinuities along pyramid edges.

Method: Uses probabilistic solutions method (MPS) with computer modeling of Wiener process, determines intersection points with pyramid surface, implements numerical code, and calculates function values at chosen points.

Result: The algorithm successfully solves the boundary problems numerically, with verification of accuracy and presentation of results from two illustrative examples.

Conclusion: The MPS approach provides an effective numerical solution for Dirichlet harmonic problems on irregular pyramidal domains with discontinuous boundary conditions.

Abstract: This paper describes the application of the method of probabilistic solutions
(MPS) to numerically solve the Dirichlet generalized and classical harmonic
problems for irregular n sided pyramidal domains. Here, generalized means that
the boundary function has a finite number of first kind discontinuity curves,
with the pyramid edges acting as these curves. The pyramid base is a convex
polygon, and its vertex projection lies within the base. The proposed algorithm
for solving boundary problems numerically includes the following steps: a)
applying MPS, which relies on computer modeling of the Wiener process; b)
determining the intersection point between the simulated Wiener process path
and the pyramid surface; c) developing a code for numerical implementation and
verifying the accuracy of the results; d) calculating the desired function
value at any chosen point. Two examples are provided for illustration, and the
results of the numerical experiments are presented and discussed.

</details>


### [10] [Adaptive hyperviscosity stabilisation for the RBF-FD method in solving advection-dominated transport equations](https://arxiv.org/abs/2510.18772)
*Miha Rot,Žiga Vaupotič,Andrej Kolar-Požun,Gregor Kosec*

Main category: math.NA

TL;DR: An adaptive hyperviscosity stabilization method for RBF-FD that automatically determines hyperviscosity constants using spectral analysis, works on general node layouts, and reduces computational costs through optimized monomial augmentation and hybrid spline strategies.


<details>
  <summary>Details</summary>
Motivation: To develop a general stabilization procedure for RBF-FD methods that avoids empirical tuning limitations and von Neumann-based estimates, while improving efficiency for advection-dominated transport equations on domains without boundaries.

Method: Uses PDE-independent algorithm based on spectral radius of RBF-FD evolution matrix to adaptively determine hyperviscosity constants. Implements lower monomial augmentation for hyperviscosity operator approximation and hybrid spline orders for advection/hyperviscosity operators to reduce computational costs and enhance stability.

Result: Demonstrated stable performance on linear advection and non-linear Burgers' equation with limited numerical dissipation. Method supports general node layouts and achieves consistent stabilization with improved efficiency.

Conclusion: Successfully developed a general hyperviscosity RBF-FD solution procedure for advection-dominated problems and provided comprehensive analysis of hyperviscosity behavior within RBF-FD framework, addressing parameter interplay and numerical influences.

Abstract: This paper presents an adaptive hyperviscosity stabilisation procedure for
the Radial Basis Function-generated Finite Difference (RBF-FD) method, aimed at
solving linear and non-linear advection-dominated transport equations on
domains without a boundary. The approach employs a PDE-independent algorithm
that adaptively determines the hyperviscosity constant based on the spectral
radius of the RBF-FD evolution matrix. The proposed procedure supports general
node layouts and is not tailored for specific equations, avoiding the
limitations of empirical tuning and von Neumann-based estimates. To reduce
computational cost, it is shown that lower monomial augmentation in the
approximation of the hyperviscosity operator can still ensure consistent
stabilisation, enabling the use of smaller stencils and improving overall
efficiency. A hybrid strategy employing different spline orders for the
advection and hyperviscosity operators is also implemented to enhance
stability. The method is evaluated on pure linear advection and non-linear
Burgers' equation, demonstrating stable performance with limited numerical
dissipation. The two main contributions are: (1) a general hyperviscosity
RBF-FD solution procedure demonstrated on both linear and non-linear
advection-dominated problems, and (2) an in-depth analysis of the behaviour of
hyperviscosity within the RBF-FD framework, addressing the interplay between
key free parameters and their influence on numerical results.

</details>


### [11] [Commuting quasi-interpolators and Maxwell compactness for a polytopal de Rham complex](https://arxiv.org/abs/2510.18835)
*Théophile Chaumont-Frelet,Jérôme Droniou,Simon Lemaire*

Main category: math.NA

TL;DR: The paper establishes Maxwell compactness results for Discrete De Rham (DDR) polytopal complexes, proving that sequences with bounded discrete H(curl) or H(div) norms and orthogonal to discrete gradients/curls have L²-compact potential reconstructions.


<details>
  <summary>Details</summary>
Motivation: To develop convergence proofs for DDR schemes for PDEs based on the de Rham complex under minimal-regularity assumptions, with generic mixed boundary conditions.

Method: Designs novel quasi-interpolators mapping minimal-regularity de Rham spaces to discrete DDR spaces, forming commuting diagrams. Establishes primal and adjoint consistency properties for these operators.

Result: Proves Maxwell compactness results for DDR polytopal complexes and establishes full consistency properties for the quasi-interpolators.

Conclusion: The analysis enables convergence proofs of DDR schemes for PDEs with minimal regularity requirements, using recently introduced liftings from DDR to continuous de Rham complex.

Abstract: We establish Maxwell compactness results for the Discrete De Rham (DDR)
polytopal complex: sequences in this polytopal complex with bounded discrete
$\boldsymbol{H}(\mathbf{curl})$ (resp. discrete $\boldsymbol{H}(\mathrm{div})$)
norm and orthogonal to discrete gradients (resp. discrete curls) have
$L^2$-relatively compact potential reconstructions. The proof of these results
hinges on the design of novel quasi-interpolators, that map the
minimal-regularity de Rham spaces onto the discrete DDR spaces and form a
commuting diagram. A full set of (primal and adjoint) consistency properties is
established for these quasi-interpolators, which paves the way to convergence
proofs, under minimal-regularity assumptions, of DDR schemes for partial
differential equations based on the de Rham complex. Our analysis is performed
with generic mixed boundary conditions, also covering the cases of no boundary
conditions or fully homogeneous boundary conditions, and leverages recently
introduced liftings from the DDR complex to the continuous de Rham complex.

</details>


### [12] [Flexible inner-product free Krylov methods for inverse problems](https://arxiv.org/abs/2510.18853)
*Malena Sabaté Landman*

Main category: math.NA

TL;DR: This paper introduces new flexible and inner-product free Krylov methods for inverse problems, including a flexible generalized Hessenberg method and randomized versions based on sketch-and-solve framework.


<details>
  <summary>Details</summary>
Motivation: To address challenges in explicit variational regularization with non-standard norms (e.g., ℓ_p norms for 0 < p ≤ 1) and improve computational efficiency by reducing memory requirements through low precision arithmetic.

Method: Develops new flexible Krylov methods including a flexible generalized Hessenberg method for iteration-dependent preconditioning, and introduces randomized versions using the sketch-and-solve framework from randomized numerical linear algebra.

Result: Theoretical analysis is provided and numerical experiments demonstrate the performance of the new methods with different variational regularization terms.

Conclusion: The proposed flexible and inner-product free Krylov methods, including their randomized variants, effectively address inverse problems with non-standard regularization terms while improving computational efficiency.

Abstract: Flexible Krylov methods are a common standpoint for inverse problems. In
particular, they are used to address the challenges associated with explicit
variational regularization when it goes beyond the two-norm, for example
involving an $\ell_p$ norm for $0 < p \leq 1$. Moreover, inner-product free
Krylov methods have been revisited in the context of ill-posed problems, to
speed up computations and improve memory requirements by means of using low
precision arithmetics. However, these are effectively quasi-minimal residual
methods, and can be used in combination with tools from randomized numerical
linear algebra to improve the quality of the results. This work presents new
flexible and inner-product free Krylov methods, including a new flexible
generalized Hessenberg method for iteration-dependent preconditioning.
Moreover, it introduces new randomized versions of the methods, based on the
sketch-and-solve framework. Theoretical considerations are given, and numerical
experiments are provided for different variational regularization terms to show
the performance of the new methods.

</details>


### [13] [New flexible and inexact Golub-Kahan algorithms for inverse problems](https://arxiv.org/abs/2510.18865)
*Malena Sabaté Landman,Silvia Gazzola*

Main category: math.NA

TL;DR: New flexible and inexact Golub-Kahan factorization algorithms for large-scale linear inverse problems, offering improved regularization and handling of general data fidelity functionals.


<details>
  <summary>Details</summary>
Motivation: To develop more flexible and efficient algorithms for solving large-scale linear inverse problems, particularly those involving general data fidelity functionals like p-norms, which are common in imaging applications.

Method: Proposed methods use flexible and inexact Golub-Kahan factorizations to iteratively compute regularized solutions by projecting onto adaptively generated subspaces with iteration-dependent preconditioners or inexactness.

Result: Numerical experiments in imaging applications (deblurring and computed tomography) show the methods are effective and competitive with existing popular approaches.

Conclusion: The new solvers provide a flexible and inexact Krylov subspace alternative that handles general data fidelity functionals effectively in practical imaging problems.

Abstract: This paper introduces a new class of algorithms for solving large-scale
linear inverse problems based on new flexible and inexact Golub-Kahan
factorizations. The proposed methods iteratively compute regularized solutions
by approximating a solution to (re)weighted least squares problems via
projection onto adaptively generated subspaces, where the constraint subspaces
for the residuals are (formally) equipped with iteration-dependent
preconditioners or inexactness. The new solvers offer a flexible and inexact
Krylov subspace alternative to other existing Krylov-based approaches for
handling general data fidelity functionals, e.g., those expressed in the
$p$-norm. Numerical experiments in imaging applications, such as image
deblurring and computed tomography, highlight the effectiveness and
competitiveness of the proposed methods with respect to other popular methods.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [14] [On the Harnack inequality for time-fractional and more general non-local in time subdiffusion equations](https://arxiv.org/abs/2510.17992)
*Katarzyna Ryszewska,Rico Zacher*

Main category: math.AP

TL;DR: Harnack inequality established for globally positive local solutions to nonlocal in time subdiffusion equations in one space dimension, including time-fractional diffusion equations with order <1.


<details>
  <summary>Details</summary>
Motivation: Previous research showed classical Harnack inequality fails for these equations in dimensions ≥2, so this work completes the analysis by proving it holds in one dimension.

Method: Analysis of globally positive local solutions to general class of nonlocal in time subdiffusion equations, focusing on one-dimensional case.

Result: Positive result: Harnack inequality is established and holds for these equations in one space dimension.

Conclusion: The analysis is completed by proving Harnack inequality for nonlocal subdiffusion equations in one dimension, complementing known negative results in higher dimensions.

Abstract: In this paper we establish the Harnack inequality for globally positive local
solutions to a general class of nonlocal in time subdiffusion equations in one
space dimension, which includes time-fractional diffusion equations with time
order less than one. It is already known that for these equations the classical
Harnack inequality does not hold if the space dimension is greater than or
equal to two. Here, we complete the analysis, by providing a positive result in
one space dimension.

</details>


### [15] [Traveling Waves in the McKean-Vlasov Equation under Sakaguchi-Kuramoto Interaction with Phase Frustration](https://arxiv.org/abs/2510.18059)
*Jesenko Vukadinovic*

Main category: math.AP

TL;DR: The paper studies the McKean-Vlasov equation for weakly coupled oscillators with Sakaguchi-Kuramoto interaction, showing a continuous phase transition from incoherence to coherence via an asymmetrically extended von Mises distribution.


<details>
  <summary>Details</summary>
Motivation: The Kuramoto model works well for small networks but fails for larger networks with time delays, which cause symmetry-breaking phase offsets. The Sakaguchi-Kuramoto interaction is the simplest generalization to account for these frustrations.

Method: Analysis of the McKean-Vlasov equation using an asymmetrically extended von Mises probability distribution function (AvMPDF). The traveling wave equation reduces to a system of two equations for the order parameter and wave speed, employing an asymmetrical extension of the modified Bessel function.

Result: Established existence of a continuous global phase transition from incoherence to coherence, characterized by a propagating asymmetrically extended von Mises distribution.

Conclusion: The Sakaguchi-Kuramoto interaction successfully captures phase transitions in larger networks with time delays, with the analysis providing a framework for understanding coherence in frustrated oscillator systems.

Abstract: We study the McKean-Vlasov equation for weakly coupled oscillators subject to
the Sakaguchi-Kuramoto interaction. While the Kuramoto interaction provides a
good approximation for small, densely connected networks, time delays in larger
networks lead to symmetry-breaking phase offsets (frustrations). The
Sakaguchi-Kuramoto interaction is the simplest such generalization, featuring a
single frustration parameter. We establish the existence of a continuous global
phase transition from incoherence to coherence, in the form of a propagating
asymmetrically extended von Mises probability distribution function (AvMPDF).
The corresponding traveling wave equation reduces to a system of two equations
in two unknowns: the order parameter for the AvMPDF and the wave speed. The
analysis relies on an appropriate asymmetrical extension of the modified Bessel
function.

</details>


### [16] [Fokas method for linear convection-diffusion equation with time-dependent coefficients and its extension to other evolution equations](https://arxiv.org/abs/2510.18100)
*Konstantinos Kalimeris,Türker Özsarı*

Main category: math.AP

TL;DR: The paper develops an explicit solution formula for linear convection-diffusion equations with time-dependent coefficients using the Unified Transform Method, and proves well-posedness and regularity estimates in fractional Sobolev spaces.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by physical models involving heat or mass transfer in non-stationary environments where diffusivity and transport coefficients vary with time.

Method: The authors apply and adapt the Unified Transform Method (UTM) to handle time-varying coefficients and nonzero boundary conditions, using spectral symmetry arguments to remove unknown boundary traces from the finite line Fourier transform.

Result: An explicit integral formula for the solution is obtained, and well-posedness is established in fractional Sobolev spaces with spatial and temporal regularity estimates. The smoothing effect of the heat operator is shown to persist under suitable coefficient assumptions.

Conclusion: The approach successfully extends to several evolution equations with time-dependent coefficients in one space dimension, providing a framework for analyzing such problems with explicit solution representations.

Abstract: In this paper, we study a linear convection-diffusion equation with
time-dependent coefficients on a bounded interval. The problem includes
inhomogeneous Dirichlet boundary conditions and is motivated by physical models
where the diffusivity and transport change with time, such as heat or mass
transfer in non-stationary environments. We apply and adapt the Unified
Transform Method (UTM), which handles both time-varying coefficients and
nonzero boundary data, to obtain an explicit integral formula for the solution.
%The method allows us to handle both time-varying coefficients and nonzero
boundary data by removing unknown boundary traces emerging upon finite line
Fourier transform (FLFT) through a spectral symmetry argument. Next, we study
well-posedness of the model in fractional Sobolev spaces and prove spatial and
temporal regularity estimates. We show that the smoothing effect of the heat
operator is still prevalent even when coefficients depend on time under
suitable sign and growth assumptions on the coefficients. Finally, we extend
this approach to obtain the solution for several evolution equations with
time-dependent coefficients, in one space variable.

</details>


### [17] [Remarks on the derivation of the virial identity for nonlinear Schrödinger equations](https://arxiv.org/abs/2510.18168)
*Tomoyuki Ikeda,Shuji Machihara,Hayato Miyazaki,Tohru Ozawa*

Main category: math.AP

TL;DR: Derivation of virial identity for nonlinear Schrödinger equations without constructing approximate solutions or using regularizing arguments.


<details>
  <summary>Details</summary>
Motivation: To derive the virial identity using solution properties rather than constructing approximate solution sequences or employing regularizing arguments for weights.

Method: Exploiting properties of solutions directly, avoiding the need for approximate solution sequences or weight regularization arguments used in previous approaches.

Result: Successfully derived the virial identity without the traditional methods of constructing approximate solutions or using regularizing arguments.

Conclusion: The virial identity can be obtained directly through solution properties, providing a more streamlined approach compared to previous methods.

Abstract: We revisit the derivation of the virial identity for nonlinear Schr\"odinger
equations. In \cite{O06, FM17}, several conservation laws, such as for the
charge and the energy, were derived without constructing a sequence of
approximate solutions. Their approach involves additional properties of
solutions due to Strichartz' estimate. In this paper, we derive the virial
identity without constructing the sequence of approximate solutions or
employing a regularizing argument for weights, by exploiting the properties of
solutions.

</details>


### [18] [A primer on Fourier Series](https://arxiv.org/abs/2510.18203)
*Serena Dipierro,David Pfefferlé,Enrico Valdinoci*

Main category: math.AP

TL;DR: A comprehensive textbook on Fourier Series designed for undergraduate and graduate students, featuring exercises with complete solutions.


<details>
  <summary>Details</summary>
Motivation: To provide an accessible educational resource for learning Fourier Series at multiple academic levels.

Method: Structured textbook format with theoretical explanations, practical exercises, and detailed solution guides.

Result: A complete educational package that facilitates understanding of Fourier Series through theory and practice.

Conclusion: This textbook serves as a valuable learning tool for students studying Fourier Series across different educational levels.

Abstract: This is a textbook on Fourier Series, suitable for both undergraduate and
graduate courses. The textbook is endowed with exercises, and full solutions
are provided at the end of the book.

</details>


### [19] [Quantitative Weighted Estimates for Schrödinger Pseudo-Multipliers and its Commutators](https://arxiv.org/abs/2510.18219)
*Sayan Bagchi,Riju Basak,Joydwip Singh,Manasa N. Vempati*

Main category: math.AP

TL;DR: Study of L^p-boundedness for pseudo-multipliers associated with Schrödinger operators, including weighted estimates using specialized weight classes beyond classical A_p-classes, and analysis of commutators.


<details>
  <summary>Details</summary>
Motivation: To extend the theory of pseudo-multipliers to the framework of Schrödinger operators, developing weighted boundedness results with more general weight classes than traditional Muckenhoupt A_p-classes.

Method: Prove quantitative reverse Hölder's inequality and quantitative weighted estimates for general sparse operators; study commutators of Schrödinger pseudo-multipliers.

Result: Established weighted boundedness for pseudo-multipliers using specialized weight classes; proved boundedness and compactness results for commutators on weighted L^p-spaces.

Conclusion: The developed framework provides comprehensive boundedness results for Schrödinger pseudo-multipliers and their commutators in weighted L^p-spaces with generalized weight classes.

Abstract: In this article, we investigate the unweighted and weighted $L^p$-boundedness
of pseudo-multipliers associated with a class of Schr\"odinger operators. The
weight classes we consider are tailored to this framework and strictly contain
the classical Muckenhoupt $A_p$-classes. To establish the weighted boundedness,
we prove a quantitative version of reverse H\"older's inequality and
quantitative weighted estimates for general sparse operators, which are of
independent interest. We also study commutators of Schr\"odinger
pseudo-multipliers, establishing their boundedness and compactness results on
these weighted $L^p$-spaces.

</details>


### [20] [Linearized equation and generic regularity in the Alt-Caffarelli problem](https://arxiv.org/abs/2510.18330)
*Xavier Fernández-Real,Hui Yu*

Main category: math.AP

TL;DR: The paper studies free boundary regularity for the Alt-Caffarelli problem, showing analytic free boundaries in 6D for generic boundary data and improving singular set dimensions generally.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the regularity properties of free boundaries in the Alt-Caffarelli problem, particularly focusing on generic boundary data cases.

Method: Analyze positive solutions to the linearized equation around homogeneous minimizers, prove a Harnack inequality, and establish a dimensional lower bound for the principal eigenvalue.

Result: In six dimensions, free boundaries are analytic for generic boundary data. General improvements in Hausdorff dimensions of singular sets are achieved.

Conclusion: The analysis of linearized equations around homogeneous minimizers provides key insights into free boundary regularity, leading to significant improvements in understanding singular sets and analyticity properties.

Abstract: For the Alt-Caffarelli problem, we study free boundary regularity of energy
minimizers. In six dimensions, we show that free boundaries are analytic for
generic boundary data. In general, we improve previous generic Hausdorff
dimensions of the singular sets.
  To achieve this, we analyze positive solutions to the linearized equation
around homogeneous minimizers (possibly with singular sections on the sphere).
For this equation, we prove a Harnack inequality and establish a dimensional
lower bound for its principal eigenvalue.

</details>


### [21] [Support growth of vorticity for bi-rotational Euler flows in high dimensions](https://arxiv.org/abs/2510.18335)
*In-Jee Jeong,Deokwoo Lim*

Main category: math.AP

TL;DR: Incompressible Euler equations in 4+ dimensions under bi-rotational symmetry show infinite growth of vortex patch support diameter.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of incompressible Euler equations in higher dimensions under specific symmetry conditions, particularly focusing on vortex patch dynamics.

Method: Study Euler equations in ℝ^d (d≥4) with bi-rotational symmetry without swirl, reducing the system to scalar vorticity advection in the first quadrant.

Result: Patch type initial vorticities exhibit infinite growth of the support diameter over time.

Conclusion: Under bi-rotational symmetry in 4+ dimensions, vortex patches in Euler equations undergo unbounded expansion of their spatial extent.

Abstract: We study incompressible Euler equations in $\mathbb{R}^d$ with $d \ge 4$
under bi-rotational symmetry without swirl, which reduces the Euler equations
to a scalar vorticity advection in the first quadrant. We show that patch type
initial vorticities exhibit infinite growth of the support diameter.

</details>


### [22] [Asymptotic stability of the symmetric flow via inviscid damping and enhanced dissipation](https://arxiv.org/abs/2510.18361)
*Qi Chen,Hao Li,Shunlin Shen,Zhifei Zhang*

Main category: math.AP

TL;DR: This paper proves inviscid damping and enhanced dissipation for linearized Navier-Stokes around symmetric flow in a finite channel with non-slip boundary, leading to asymptotic stability in high Reynolds number regime.


<details>
  <summary>Details</summary>
Motivation: To establish stability results for symmetric flows in viscous fluids, particularly understanding how inviscid damping and enhanced dissipation mechanisms work together in the presence of viscosity and boundary effects.

Method: Analysis of linearized Navier-Stokes system around symmetric flow in finite channel with non-slip boundary conditions, using mathematical estimates for inviscid damping and enhanced dissipation.

Result: Proved that if initial velocity perturbation satisfies ||u^in - (U(y),0)||_H^5 ≤ cν^(2/3), then inviscid damping and enhanced dissipation estimates hold for Navier-Stokes solutions, ensuring asymptotic stability.

Conclusion: The symmetric flow is asymptotically stable in high Reynolds number regime, with viscosity playing a dual role through both enhanced dissipation and boundary layer effects that interact with inviscid damping mechanisms.

Abstract: In this paper, we establish the inviscid damping and enhanced dissipation
estimates for the linearized Navier-Stokes system around the symmetric flow in
a finite channel with the non-slip boundary condition. As an immediate
consequence, we prove the asymptotic stability of the symmetric flow in the
high Reynolds number regime. Namely, if the initial velocity perturbation
$u^{\mathrm{in}}$ satisfies $\Vert u^{\mathrm{in}}-(U(y),0) \Vert_{H^5}\leq c
\nu^{\frac{2}{3}}$, then inviscid damping and enhanced dissipation estimates
also hold for the solution to the Navier-Stokes system.

</details>


### [23] [The optimal transition threshold for the 2D Couette flow in the infinite channel](https://arxiv.org/abs/2510.18365)
*Qionglei Chen,Zhen Li,Changxing Miao*

Main category: math.AP

TL;DR: The paper proves stability of 2-D Navier-Stokes equations in an infinite channel with Navier-slip boundary conditions, showing enhanced dissipation and inviscid damping effects for small initial perturbations around Couette flow.


<details>
  <summary>Details</summary>
Motivation: To understand the stability properties of the Couette flow in the infinite channel setting with Navier-slip boundary conditions, particularly investigating enhanced dissipation and inviscid damping effects.

Method: Two key techniques: (1) new vorticity decomposition ω=ω_L+ω_e where ω_L captures weak enhanced dissipation and inviscid damping; (2) dyadic decomposition for long time scales and 'infinite superposition principle' to control echo cascade growth.

Result: For initial perturbations satisfying ||ω_in||_{H^3_{x,y}∩L^1_x H^3_y} ≤ cν^{1/3}, the solution exhibits enhanced dissipation at x-frequencies |k| ≫ ν and inviscid damping effect.

Conclusion: The proposed decomposition and superposition methods successfully establish stability and reveal novel mechanisms for controlling echo cascades in the Navier-Stokes equations with Navier-slip boundary conditions.

Abstract: We investigate the stability of the 2-D Navier-Stokes equations in the
infinite channel $\mathbb{R}\times [-1,1]$ with the Navier-slip boundary
condition. We show that if the initial perturbations $\omega^{in}$ around the
Couette flow satisfy $\|\omega^{in}\|_{H^3_{x,y}\cap L^1_x H^3_y}\leq
c\nu^{\frac13}$, the solution admits enhanced dissipation at $x$-frequencies
$|k|\gg \nu$ and inviscid damping effect. The key contributions lie in two
parts: (1) we adopt the new decomposition of the vorticity
$\omega=\omega_{L}+\omega_e$, where $\omega_L$ effectively captures a ``weak"
enhanced dissipation $(1+\nu^{\frac13} t)^{-\frac14}e^{-\nu t}$ and the
corresponding velocity exhibits the inviscid damping effect; (2) we introduce
the dyadic decomposition for the long time scale $t\geq \nu^{-\frac16}$ and
apply the ``infinite superposition principle" to the equation for $\omega_e$ in
order to control the growth induced by echo cascades, which appears to be novel
and may hold independent significance.

</details>


### [24] [Quantitative stability for the 2D Couette flow on the infinite channel with non-slip boundary condition](https://arxiv.org/abs/2510.18376)
*Qionglei Chen,Zhen Li,Changxing Miao*

Main category: math.AP

TL;DR: The paper establishes quantitative stability for 2D Couette flow on an infinite channel with non-slip boundary conditions, focusing on long wave frequencies (0≤|k|<1) using resolvent estimates.


<details>
  <summary>Details</summary>
Motivation: To investigate stability of 2D Couette flow on infinite channels, extending previous work from periodic domains to infinite domains with non-slip boundary conditions.

Method: Developed resolvent estimate argument with key division at 10ν in frequency interval (0,1), using sharp Sobolev constant in Wirtinger's inequality and refined Airy function estimates. Established space-time estimates for low-frequency (0≤|k|≤10ν) and intermediate-frequency (10ν≤|k|<1) regimes.

Result: Obtained nonlinear transition threshold γ≤1/2 and showed enhanced dissipation effect occurs for frequencies |k|≥ν^{1-} in linearized Navier-Stokes equations.

Conclusion: Successfully established quantitative stability for 2D Couette flow on infinite channels with non-slip boundary conditions, providing transition threshold and demonstrating enhanced dissipation effects.

Abstract: In this paper, we investigate the quantitative stability for the 2D Couette
flow on the infinite channel $\mathbb{R}\times [-1,1]$ with non-slip boundary
condition. Compared to the case $\mathbb{T}\times [-1,1]$, we establish the
stability in the context of long wave associated with the frequency range
$0\leq |k|<1$ by developing the resolvent estimate argument. The new ingredient
is to discover the key division point at $10\nu$ in the frequency interval
$(0,1)$ by the sharp Sobolev constant in Wirtinger's inequality together with
the refined estimates of the Airy function in the interval $(0,1)$, and then we
establish the space-time estimates on the low-frequency $0\leq |k|\leq 10 \nu$
and the intermediate-frequency $ 10 \nu\leq |k|<1$, respectively. As an
application of the space-time estimates, we obtain the nonlinear transition
threshold to be $\gamma\leq\frac{1}{2}$.Meanwhile, we also show that when the
frequencies $|k|\geq \nu^{1-}$, the enhanced dissipation effect occurs for the
linearized Navier-Stokes equations.

</details>


### [25] [Blow-up results for a Nakao-type problem with a time-dependent damping term and derivative-type nonlinearities](https://arxiv.org/abs/2510.18378)
*Yuequn Li,Alessandro Palmieri*

Main category: math.AP

TL;DR: Blow-up results and lifespan estimates for semilinear damped wave equations coupled through derivative-type power nonlinearities, analyzed using iteration arguments on solution functionals.


<details>
  <summary>Details</summary>
Motivation: To study blow-up behavior and lifespan estimates for coupled damped wave equations with derivative-type nonlinearities, considering both constant and time-dependent damping coefficients.

Method: Iteration argument applied to functionals related to the components of local solutions, analyzing classical damped wave equations and wave equations with time-dependent damping in scale-invariant and scattering-producing cases.

Result: Proved blow-up results and derived upper bound estimates for the lifespan of local solutions to the coupled damped wave system.

Conclusion: The iteration method successfully establishes blow-up behavior and lifespan bounds for coupled damped wave equations with derivative-type power nonlinearities under various damping conditions.

Abstract: In this paper, we consider a semilinear system of damped wave equations
coupled through power nonlinearities of derivative-type. In particular, we
consider a classical damped wave equation, i.e., with constant coefficients,
and a wave equation with a time-dependent coefficient for the damping term. For
this time-dependent coefficient we analyze two cases: the scale-invariant case
and the scattering producing case. We prove blow-up results and derive upper
bound estimates for the lifespan of local solutions. Our approach is based on
an iteration argument for a couple of functionals related to the components of
a local solution.

</details>


### [26] [Crystalline motion of discrete interfaces in the Blume-Emery-Griffiths model](https://arxiv.org/abs/2510.18403)
*Marco Cicalese,Giuliana Fusco,Giovanni Savaré*

Main category: math.AP

TL;DR: Analysis of discrete-to-continuum evolution in a lattice system with two immiscible phases and surfactant, using Blume-Emery-Griffith model and minimizing-movements scheme.


<details>
  <summary>Details</summary>
Motivation: To understand how lattice systems with immiscible phases and surfactant evolve in the continuum limit, particularly focusing on the role of surfactant mass conservation.

Method: Uses minimizing-movements scheme on Blume-Emery-Griffith lattice model with dissipation functional containing Almgren-Taylor-Wang term and surfactant variation term. Studies different scaling parameters gamma.

Result: For gamma > 2, surfactant can lose mass and evolution reduces to crystalline mean curvature flow. For gamma < 2, surfactant mass conservation leads to complex evolution with non-uniqueness and partial pinning.

Conclusion: The scaling parameter gamma critically determines the system's evolution behavior, with mass conservation (gamma < 2) introducing significant complexity compared to mass loss scenarios (gamma > 2).

Abstract: We study the discrete-to-continuum evolution of a lattice system consisting
of two immiscible phases labelled by -1 and +1 in presence of a surfactant
phase labelled by 0. The system's energy is described by the classical
Blume-Emery-Griffith model on the lattice epsilon Z^2, and its continuum
evolution is obtained as epsilon tends to zero through a minimizing-movements
scheme with a time step proportional to epsilon. The dissipation functional we
choose contains two contributions: a standard Almgren-Taylor-Wang type term
penalizing the distance between successive configurations of the +1 phase, and
a term penalizing the variation of the surfactant mass and modeling surfactant
evaporation. The latter term depends on a scaling parameter gamma > 0, which
determines whether the surfactant mass is conserved at each time step. We focus
on the case in which the initial configuration consists of a single crystal of
phase 1 completely wetted by the surfactant. For gamma > 2 the surfactant can
lose mass and the evolution reduces to the crystalline mean curvature flow of
an Ising-type model, while for gamma < 2 the conservation of the surfactant
mass leads to a more complex evolution characterized by stronger non-uniqueness
and partial pinning.

</details>


### [27] [The Momentum Light Ray Transform](https://arxiv.org/abs/2510.18450)
*Sombuddha Bhattacharyya,Tuhin Mondal,Suman Kumar Sahoo*

Main category: math.AP

TL;DR: Study of Momentum Light Ray Transform (MLRT) on symmetric tensor fields, exploring injectivity conditions and developing inversion algorithms for full and restricted measurements.


<details>
  <summary>Details</summary>
Motivation: To understand the mathematical properties of MLRT, an integral transform that integrates tensor fields along light rays with polynomial weights, and establish conditions for its invertibility.

Method: Using tools from tensor tomography, geometry, and analysis to derive necessary and sufficient conditions for MLRT injectivity and develop inversion algorithms.

Result: Established injectivity conditions for MLRT over general order tensors in space dimension ≥2, and developed inversion algorithms for restricted measurement settings.

Conclusion: Successfully characterized the injectivity properties of MLRT and provided practical inversion methods, advancing the mathematical understanding of this integral transform.

Abstract: In this article, we study Momentum Light Ray Transform (MLRT) on symmetric
tensor fields. MLRT is an integral transform in time-space domain ($(t,x)\in
\mathbb{R}^{1+n}$), which integrates a scalar function or a tensor field along
the light rays with a polynomial type weight. We explore necessary and
sufficient conditions for injectivity of MLRT, over general order tensors on
space dimension $\geq 2$, from full and restricted measurements. Furthermore,
we develop an inversion algorithm for MLRTs in the restricted measurement
setting. To prove the results, we use tools from tensor tomography, geometry,
and analysis.

</details>


### [28] [Entanglement principle and fractional Calderón problem for nonlocal parabolic operators](https://arxiv.org/abs/2510.18641)
*Ru-Yu Lai,Yi-Hsuan Lin,Lili Yan*

Main category: math.AP

TL;DR: The paper introduces a novel entanglement principle for variable-coefficient nonlocal parabolic operators and proves unique determination of lower-order perturbations from Dirichlet-to-Neumann maps using a modified version of this principle.


<details>
  <summary>Details</summary>
Motivation: To solve inverse problems for variable-coefficient nonlocal parabolic operators (∂ₜ - Δ_g)^s where 0 < s < 1, particularly focusing on determining lower-order perturbations from boundary measurements.

Method: Developed a novel entanglement principle for these operators under smoothness conditions, then derived a modified entanglement principle to overcome insufficient solution regularity issues for inverse problems.

Result: Proved that lower-order perturbations can be uniquely determined from the Dirichlet-to-Neumann map using the modified entanglement principle.

Conclusion: The modified entanglement principle enables effective resolution of inverse problems for variable-coefficient nonlocal parabolic operators despite initial regularity limitations.

Abstract: We examine inverse problems for the variable-coefficient nonlocal parabolic
operator $(\partial_t - \Delta_g)^s$, where $0 < s < 1$. This article makes two
primary contributions. First, we introduce a novel entanglement principle for
these operators under suitable smoothness conditions. Second, we prove that
lower-order perturbations can be uniquely determined from the associated
Dirichlet-to-Neumann map using this principle. However, due to insufficient
solution regularity, direct application of the entanglement principle to the
inverse problem is not feasible. To address this, we derive a modified
entanglement principle, enabling the effective resolution of related inverse
problems.

</details>


### [29] [The Global Well-posedness of the Euler-Poisson System for Ions in 2D](https://arxiv.org/abs/2510.18655)
*Han Cui*

Main category: math.AP

TL;DR: Global well-posedness of Euler-Poisson system for ions in 2D established using methods from gravity-capillary water waves and Euler-Maxwell systems.


<details>
  <summary>Details</summary>
Motivation: To prove the global existence and uniqueness of solutions for the Euler-Poisson system for ions in two dimensions, addressing challenges from time resonance and slow decay.

Method: Applied techniques developed for gravity-capillary water waves and Euler-Maxwell systems to overcome difficulties with time resonance at low frequencies and slow decay.

Result: Successfully established global well-posedness of the Euler-Poisson system for ions in 2D.

Conclusion: The methods from related systems can effectively resolve the mathematical challenges in proving global well-posedness for the Euler-Poisson system.

Abstract: This paper aims to establish the global well-posedness of the Euler-Poisson
system for ions in 2D. The difficulties arising from time resonance at low
frequencies and slow decay will be overcome by applying the method developed
for the gravity-capillary water waves and Euler-Maxwell systems.

</details>


### [30] [A Generalization of the Sphere Covering Inequality](https://arxiv.org/abs/2510.18681)
*Changfeng Gui,Amir Moradifam*

Main category: math.AP

TL;DR: A quantitative generalization of the Sphere Covering Inequality that relaxes the boundary matching condition by allowing conformal factors to differ by a constant c ≥ 0, revealing stability properties.


<details>
  <summary>Details</summary>
Motivation: To establish a more flexible version of the Sphere Covering Inequality that accounts for boundary perturbations and reveals underlying stability structures in conformal geometry.

Method: Generalizing the original Sphere Covering Inequality by introducing a constant c ≥ 0 that measures the difference between conformal factors along the boundary, and analyzing how this affects the total-area bound.

Result: The Sphere Covering Inequality is shown to be stable with respect to boundary perturbations, with a precise quantitative description of how the total-area bound varies under such perturbations.

Conclusion: The generalized inequality provides new analytic and geometric tools for studying elliptic equations with exponential nonlinearities, conformal geometry, and mathematical physics problems.

Abstract: The Sphere Covering Inequality was introduced in \cite{GM} (\emph{Invent.
Math.}, 2018) as a sharp geometric inequality that provides a lower bound for
the total area of two distinct surfaces of Gaussian curvature 1. These surfaces
are assumed to be conformal to the Euclidean unit disk and share the same
conformal factor along the boundary. In this paper, we establish a quantitative
generalization that relaxes the boundary matching condition by allowing the
conformal factors to differ by a constant \( c \ge 0 \) on the boundary. This
refinement reveals a new stability-type structure underlying the inequality.
Our results show that the Sphere Covering Inequality is stable with respect to
perturbations in the boundary data and provide a precise quantitative
description of how the total-area bound varies under such perturbations. The
generalized inequality provides new analytic and geometric tools for the study
of elliptic equations with exponential nonlinearities, conformal geometry, and
related problems in mathematical physics.

</details>


### [31] [Quasilinear Elliptic Cooperative and Competitive Systems](https://arxiv.org/abs/2510.18758)
*Annamaria Canino,Simone Mauro*

Main category: math.AP

TL;DR: Existence and multiplicity of weak solutions for quasilinear elliptic systems with non-differentiable energy functionals, using nonsmooth critical point theory.


<details>
  <summary>Details</summary>
Motivation: Study quasilinear elliptic systems where the energy functional lacks differentiability, requiring advanced mathematical tools to analyze weak solutions.

Method: Employ nonsmooth critical point theory and variational methods based on weak slope concept to handle non-differentiable energy functionals.

Result: Proved existence of least energy solutions in both cooperative (β > 0) and competitive (β < 0) regimes.

Conclusion: Successfully established existence results for quasilinear elliptic systems using nonsmooth analysis techniques, covering both cooperative and competitive cases.

Abstract: We study the existence and multiplicity of weak solutions for the following
quasilinear elliptic system: \[ \begin{cases} -\mathrm{div}(A_1(x,u_1)\nabla
u_1) + \displaystyle\frac{1}{2} D_{u_1}A_1(x,u_1)\nabla u_1 \cdot \nabla u_1 =
\lambda_1 u_1 + g_{\beta,1}(u) & \text{in } \Omega, \\[3mm]
-\mathrm{div}(A_2(x,u_2)\nabla u_2) + \displaystyle\frac{1}{2}
D_{u_2}A_2(x,u_2)\nabla u_2 \cdot \nabla u_2 = \lambda_2 u_2 + g_{\beta,2}(u) &
\text{in } \Omega, \\[2mm] u_1 = u_2 = 0 & \text{on } \partial\Omega,
\end{cases} \] where $\lambda_1, \lambda_2 < \mu_1$, the first Dirichlet
eigenvalue of the Laplacian, and $\Omega$ is a bounded domain. The nonlinearity
derives from a potential $G_\beta$ with subcritical growth.
  Due to the lack of differentiability of the associated energy functional, we
employ nonsmooth critical point theory and variational methods based on the
concept of weak slope. We prove the existence of least energy solutions in both
the cooperative ($\beta > 0$) and competitive ($\beta < 0$) regimes.

</details>


### [32] [A revisit of patch solutions for the 2D Loglog-Euler type equation](https://arxiv.org/abs/2510.18759)
*Changhui Tan,Liutang Xue,Zhilong Xue*

Main category: math.AP

TL;DR: This paper studies patch solutions for 2D Loglog-Euler type equations, proving global existence/uniqueness of weak solutions and establishing global preservation of boundary regularity for patch solutions using a physical-space approach.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of patch solutions beyond the classical 2D Euler equation to a broader class of active scalar equations, and to provide alternative proofs that avoid frequency-space arguments for better applicability to different domains.

Method: Develops a physical-space-based approach to analyze patch solutions, proving Yudovich-type theorems for global existence/uniqueness and establishing boundary regularity preservation without using Littlewood-Paley theory.

Result: Proves global existence and uniqueness of weak solutions for Loglog-Euler type equations, and shows that patch boundaries preserve C^{1,μ-ε} regularity globally for any ε ∈ (0,μ). Also extends to higher-order boundary regularity and multiple patches.

Conclusion: The physical-space approach successfully establishes global regularity results for patch solutions in Loglog-Euler type equations, offering advantages over frequency-space methods and enabling extensions to more complex domains.

Abstract: In this paper, we revisit the patch solutions for a class of inviscid
whole-space active scalar equations that interpolate between the 2D Euler
equation and the $\alpha$-SQG equation. Compared with the 2D Euler equation in
vorticity form, there is an additional Fourier multiplier $m(\Lambda)$
($\Lambda = (-\Delta)^{1/2}$) in the Biot-Savart law. If the symbol $m$
satisfies the Osgood-type condition $$\int_2^{+\infty} \frac{1}{r (\log r)
m(r)} dr= +\infty$$ and certain mild assumptions, the system is referred to as
the 2D Loglog-Euler type equation.
  First, we prove a Yudovich-type theorem establishing the existence and
uniqueness of a global weak solution for the Loglog-Euler type equation
associated with bounded and integrable initial data. This result directly
applies to patch solutions, which are weak solutions corresponding to patch
initial data given by characteristic functions of disjoint, regular, bounded
domains.
  Next, we revisit the seminal result by Elgindi ( Arch. Ration. Mech. Anal.
211(3) 965-990, 2014 ) and provide a different proof under explicit assumptions
on $m$, showing that for the 2D Loglog-Euler type equation with $C^{1,\mu}$
($0<\mu<1$) single-patch initial data, the evolved patch boundary globally
preserves the $C^{1,\mu-\varepsilon}$ regularity for any $\varepsilon \in
(0,\mu)$. In contrast to the frequency-space argument in Elgindi's result, we
develop an entirely physical-space-based approach that avoids the
Littlewood-Paley theory and offers advantages for potential extensions to the
half-plane or bounded smooth domains.
  Furthermore, we investigate the global propagation of higher-order
$C^{n,\mu}$ boundary regularity for patch solutions with any $n \in
\mathbb{N}^\star$, and analyze the evolution of multiple patches.

</details>


### [33] [The minimal wave speed of time-periodic traveling waves arising from a diffusive Kermack-McKendrick model with seasonality and nonlocal delayed interactions](https://arxiv.org/abs/2510.18767)
*Shuang-Ming Wang*

Main category: math.AP

TL;DR: This paper proves the non-existence of time-periodic traveling wave solutions with speed below the critical speed c* for a diffusive Kermack-McKendrick epidemic model with seasonality and nonlocal interactions.


<details>
  <summary>Details</summary>
Motivation: To solve an open problem from previous research regarding the minimal wave speed for time-periodic traveling waves in epidemic models with seasonality and nonlocal interactions.

Method: Constructed upper and lower solutions on truncated intervals for an auxiliary linear equation to handle challenges from nonlocal delay and non-autonomous system.

Result: Confirmed that the critical value c* is indeed the minimal wave speed for time-periodic traveling waves in this epidemic model.

Conclusion: Completely solved the open problem from the referenced paper, establishing c* as the minimal wave speed for the epidemic model with seasonality and nonlocal interactions.

Abstract: This paper is concerned with the non-existence of time-periodic traveling
wave solution with speed less than the critical speed for diffusive
Kermack-McKendrick epidemic model incorporating seasonality and nonlocal
interactions induced by latent period. By a technical construction of upper and
lower solutions on truncated intervals for an auxiliary linear equation, we
overcome the challenges arising from the coupling of nonlocal delay and the
fact that the system is non-autonomous. Thus the critical value $c^*$ defined
in [S.-M. Wang et al., Nonlinear Anal. Real World Appl., 55 (2020) 103117] is
confirmed as the minimal wave speed of time-periodic traveling waves. We have
completely solved the open problem [S.-M. Wang et al., Nonlinear Anal. Real
World Appl., 55 (2020) 103117]

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [34] [A Physics-Informed Machine Learning Framework for Solid Boundary Treatment in Meshfree Particle Methods](https://arxiv.org/abs/2510.17813)
*Nariman Mehranfar,Ahmad Shakibaeinia*

Main category: physics.comp-ph

TL;DR: A physics-informed machine learning framework replaces traditional boundary treatments in meshfree particle methods by predicting boundary correction terms using a CNN-MLP hybrid network, eliminating the need for ghost particles while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional boundary treatments in meshfree particle methods (like SPH and MPS) cause errors, instabilities, and computational complexity, especially for irregular geometries. Ghost particles and analytical corrections are computationally expensive.

Method: Uses a hybrid CNN-MLP neural network trained on physics-informed features (local geometry, particle states, kernel properties) to directly predict boundary correction terms for all spatial differential operators.

Result: Achieves accuracy comparable to ghost-particle methods while reducing computational overhead. Generalizes well to unseen geometries, flow conditions, and dynamic domains.

Conclusion: Establishes a flexible, physics-informed ML paradigm for boundary treatment that improves both accuracy and scalability across meshfree particle methods.

Abstract: Meshfree particle methods, such as Smoothed Particle Hydrodynamics (SPH) and
the Moving Particle Semi-Implicit (MPS) method, are widely used to simulate
complex free-surface and multiphase flows. A key challenge in these methods is
the treatment of solid boundaries, where kernel truncation causes errors and
instabilities. Traditional treatments, such as ghost particles and
semi-analytical wall corrections, restore kernel completeness but add
significant computational cost and complexity, especially for irregular
geometries. We propose a physics-informed machine learning (ML) framework that
directly predicts boundary correction terms for particle approximations,
eliminating the need for ghost particles or analytical corrections. The
framework is based on a hybrid convolutional neural network-multilayer
perceptron (CNN-MLP) trained on physics-informed features that capture local
geometry, particle states, and kernel properties. Once trained, it provides
consistent boundary contributions across all spatial differential operators,
including gradients, divergences, and Laplacians. The approach is demonstrated
with MPS but is readily extensible to other particle methods such as SPH. Tests
with predefined fields, unsteady diffusion, and incompressible Navier-Stokes
flows demonstrate accuracy comparable to that of ghost-particle methods while
reducing computational overhead. The model generalizes well to unseen
geometries, flow conditions, and particle distributions, including dynamically
evolving domains. This work establishes a flexible, physics-informed ML
paradigm for boundary treatment in particle-based PDE solvers, improving both
accuracy and scalability across a broad class of meshfree methods.

</details>


### [35] [Committors without Descriptors](https://arxiv.org/abs/2510.18018)
*Peilin Kang,Jintu Zhang,Enrico Trizio,TingJun Hou,Michele Parrinello*

Main category: physics.comp-ph

TL;DR: The paper presents an automated committor-based enhanced sampling method using graph neural networks to study rare events in atomistic simulations, eliminating the need for predefined descriptors.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of studying rare events in atomistic simulations by developing a more automated approach that can directly process atomic coordinates without requiring predefined physical descriptors.

Method: Combines a committor-based enhanced sampling method with graph neural networks, which directly process atomic coordinates rather than requiring pre-defined descriptors, and uses a variational criterion to iteratively optimize the neural network parametrization.

Result: The method enables extensive sampling of transition state ensembles and demonstrates advantages in describing solvent molecule roles in systems like ion pair dissociation and ligand binding.

Conclusion: Graph neural networks provide an effective way to automate committor-based enhanced sampling methods, offering improved capability to capture complex molecular interactions and solvent effects in rare event studies.

Abstract: The study of rare events is one of the major challenges in atomistic
simulations, and several enhanced sampling methods towards its solution have
been proposed. Recently, it has been suggested that the use of the committor,
which provides a precise formal description of rare events, could be of use in
this context. We have recently followed up on this suggestion and proposed a
committor-based method that promotes frequent transitions between the
metastable states of the system and allows extensive sampling of the process
transition state ensemble. One of the strengths of our approach is being
self-consistent and semi-automatic, exploiting a variational criterion to
iteratively optimize a neural-network-based parametrization of the committor,
which uses a set of physical descriptors as input. Here, we further automate
this procedure by combining our previous method with the expressive power of
graph neural networks, which can directly process atomic coordinates rather
than descriptors. Besides applications on benchmark systems, we highlight the
advantages of a graph-based approach in describing the role of solvent
molecules in systems, such as ion pair dissociation or ligand binding.

</details>


### [36] [Special Relativistic Smoothed Particle Hydrodynamics Based on Riemann Solver](https://arxiv.org/abs/2510.18251)
*Kanta Kitajima,Shu-ichiro Inutsuka,Izumi Seno*

Main category: physics.comp-ph

TL;DR: A novel Godunov Smoothed Particle Hydrodynamics method for special relativistic fluid dynamics that uses Riemann solvers for shock handling and maintains conservation laws through convolution integrals.


<details>
  <summary>Details</summary>
Motivation: To develop a more accurate numerical method for special relativistic fluid dynamics that can handle strong shock waves while maintaining conservation laws and achieving higher accuracy.

Method: Uses Godunov Smoothed Particle Hydrodynamics with Riemann solvers for shock description, convolution integrals for physical quantity definition, and proposes novel number density calculation with non-equal baryon numbers and variable smoothing length.

Result: Method demonstrates robustness in one- and two-dimensional relativistic shock tube problems and accurately simulates Kelvin-Helmholtz instabilities.

Conclusion: SRGSPH is validated as a reliable approach for high-resolution relativistic simulations.

Abstract: This paper proposes a novel numerical method based on Godunov Smoothed
Particle Hydrodynamics for special relativistic fluid dynamics. Our method
utilizes a Riemann solver to describe shock, enhancing accuracy in strong shock
waves. The formulation maintains conservation laws and achieves higher accuracy
through convolution integrals that define physical quantities for SPH
particles. We also propose the number density calculation method that uses a
non-equal baryon number in each SPH particle and variable smoothing length in a
way different from the conventional method. Numerical experiments demonstrate
the method's robustness across one- and two-dimensional relativistic shock tube
problems, as well as its ability to simulate Kelvin-Helmholtz instabilities
accurately, validating SRGSPH as a reliable approach for high-resolution
relativistic simulations.

</details>


### [37] [Inverse analysis for the identification of temporal and spatial characteristics of a short super-Gaussian laser pulse interacting with a solid plate](https://arxiv.org/abs/2510.18372)
*Karol Pietrak,Piotr Łapka,Małgorzata Kujawińska*

Main category: physics.comp-ph

TL;DR: A novel laser beam profiling method using infrared thermography and inverse analysis to determine four key laser pulse parameters from temperature distributions.


<details>
  <summary>Details</summary>
Motivation: To develop a reliable method for characterizing short high-power laser pulses by measuring their parameters through thermal effects on a metal plate.

Method: Uses infrared thermography with laser illuminating a thin metal plate, applies Levenberg-Marquardt inverse method in GNU Octave, and solves forward problem with 3D transient heat transfer model in ANSYS Fluent.

Result: Successfully developed and verified an inverse algorithm that can determine Super-Gaussian profile coefficient, laser power, pulse start time, and duration from temperature distributions.

Conclusion: The method provides an effective approach for laser beam profiling, with both strengths and limitations discussed for practical applications.

Abstract: In this paper, the results of numerical experiments verifying a novel setup
for laser beam profiling are presented. The experimental setup is based on
infrared thermography and includes laser beam illuminating a thin metal plate.
The method allows to determine four parameters of the short high-power laser
pulse, namely the Super-Gaussian profile coefficient, laser power, pulse start
time and duration. The unknown parameters are retrieved based on temporal and
spatial temperature distributions at the rear side of the illuminated plate.
The applied inverse method is based on Levenberg-Marquardt technique and it is
implemented in the GNU Octave environment. Solutions of the forward problem are
obtained numerically, with the aid of three-dimensional transient heat transfer
model implemented in the commercial software ANSYS Fluent. The paper presents
the results of the sensitivity analysis as well as calibration and verification
of the developed inverse algorithm through application of numerically-generated
synthetic data. Strengths and weaknesses of the applied approach are widely
discussed.

</details>


### [38] [LENNs: Locally Enhanced Neural Networks for High-Fidelity Modeling in Solid Mechanics](https://arxiv.org/abs/2510.18565)
*Zhihong Lai,Luyang Zhao,Qian Shao*

Main category: physics.comp-ph

TL;DR: Proposes Locally Enhanced Neural Networks (LENNs) to address localized discontinuities in solid mechanics using multilevel modeling with global and local networks coupled through window functions.


<details>
  <summary>Details</summary>
Motivation: Address limitations of single network PINNs in resolving smooth global responses and near-tip singularities, and inadequacy in discontinuity representation, which cause unstable training and limited accuracy.

Method: Uses global network for bulk solution and activates local network in localized areas for non-smooth response, coupled through smooth window function. Local network embeds additional functions to encode discontinuous information. Composite solution optimized through total potential energy functional.

Result: LENNs perform well in addressing localized discontinuous problems and provide accurate predictions for both displacement and stress fields in numerical experiments.

Conclusion: The method resolves conflict of single network in representing both smooth global and singular local fields without additional interface-loss terms and amplifies contribution of localized critical features in energy optimization.

Abstract: Despite prior advances in PINNs, significant challenges remain in localized
solid mechanics problems because of the limitations of single network
formulations in simultaneous resolution of smooth global responses and near-tip
singularities, and inadequacy in discontinuity representation, leading to
unstable training and limited accuracy. To address the challenges, we propose
Locally Enhanced Neural Networks (LENNs) that characterize localized
discontinuities in solid mechanics via multilevel modeling. In particular, this
novel framework employs a global network for the bulk solution and activates a
local network in localized area for non-smooth response, coupled through a
smooth window function that enables weighted superposition of local and global
solutions. Moreover, the local network embeds additional functions that encode
the discontinuous information into the input to capture localized non-smooth
mechanical behaviors. Finally, the composite solution is substituted into the
total potential energy functional for unified optimization. With this
structure, the method resolves the conflict of single network in representing
both smooth global and singular local fields without additional interface-loss
terms and amplifies the contribution of localized critical features in energy
optimization. We focus on a series of numerical experiments in solid mechanics
to demonstrate the performance of the method. Results show that LENNs perform
well in addressing localized discontinuous problems and provide accurate
predictions for both displacement and stress fields.

</details>


### [39] [Stable rational approximations for parabolic equation methods](https://arxiv.org/abs/2510.18622)
*Adith Ramamurti,Joseph F. Lingevitch,Jonathan C. Lighthall,Michael D. Collins*

Main category: physics.comp-ph

TL;DR: The paper applies the adaptive Antoulas-Anderson (AAA) algorithm for rational approximations in parabolic equation methods for wave propagation, showing improved stability and accuracy compared to existing methods, particularly for fluid-elastic waveguides.


<details>
  <summary>Details</summary>
Motivation: Existing rational approximation methods for parabolic equation wave propagation have stability issues in certain fluid-elastic waveguides, particularly thin elastic layers over thick fluid layers, requiring a more robust approximation approach.

Method: Uses the adaptive Antoulas-Anderson (AAA) algorithm for rational approximations of fractional-powered differential operators in parabolic equation methods, and applies the split-step Padé method to fluid-elastic waveguides.

Result: AAA algorithm provides excellent agreement with reference solutions, with transmission loss errors comparable or better than rotated operator method, and enables computational efficiency gains through split-step Padé method.

Conclusion: The AAA algorithm is an effective alternative for stable and accurate rational approximations in fluid-elastic parabolic equation simulations, overcoming limitations of previous methods while offering computational advantages.

Abstract: Modern parabolic equation (PE) methods for wave propagation rely on
application of a variety of fractional-powered differential operators. Rational
approximations of these operators need to properly map their spectra onto the
complex plane, accurately handling propagating modes while annihilating
evanescent ones. Standard approaches for stable and accurate rational
approximations include rotating the branch cut of the operators or imposing
stability constraint equations, and have yielded accurate results for wave
propagation in a variety of fluid, elastic, and fluid-elastic waveguides. The
stability constraint method, however, does not yield operators that are stable
for all fluid-elastic waveguides, and a recent study of waveguides comprised of
a thin elastic layer overlaying a thick fluid layer revealed instabilities in
the approximations derived from rotated operators. In this paper, we
demonstrate the applicability of a different rational approximation method, the
recently-developed adaptive Antoulas-Anderson (AAA) algorithm, to simulations
of wave propagation using the fluid-elastic parabolic equation. We find that
simulations using operators approximated using the AAA algorithm provide
excellent agreement with reference solutions, with errors in transmission loss
comparable to, and often less than, that of simulations using the rotated
operator method. In addition, we find that the AAA algorithm allows for the
application of the split-step Pad\'e method to fluid-elastic waveguides, which
yields a large gain in computational efficiency.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [40] [Carrier envelope phase and laser pulse shape effects on Schwinger vacuum pair production in super-Gaussian asymmetric electric fields](https://arxiv.org/abs/2510.17856)
*Abhinav Jangir,Anees Ahmed*

Main category: physics.plasm-ph

TL;DR: Study examines how carrier envelope phase and laser pulse shape affect electron-positron pair production in asymmetric super-Gaussian electric fields, showing extreme sensitivity and potential 100-1000x enhancement in pair production.


<details>
  <summary>Details</summary>
Motivation: To understand how field characteristics like carrier envelope phase, pulse shape, and field asymmetry influence electron-positron pair production in strong laser fields.

Method: Solving quantum Vlasov equation and analyzing turning-point structures using WKB formalism, varying field asymmetry, pulse shape (Gaussian to super-Gaussian), and carrier envelope phase.

Result: Momentum distribution and number density of created pairs show extreme sensitivity to field parameters. Multiphoton pair production dominates in long falling-pulse asymmetry. Short falling pulse with flat-top super-Gaussian profile further facilitates pair production, with number density enhanced by 2-3 orders of magnitude for certain parameters.

Conclusion: Carrier envelope phase and laser pulse shape significantly impact electron-positron pair production, with super-Gaussian fields enabling substantial enhancement of pair creation rates through optimized field parameters.

Abstract: We investigate the combined effects of carrier envelope phase and laser pulse
shape on electron-positron pair production in the presence of an external
asymmetric super-Gaussian electric field by solving the quantum Vlasov
equation. By varying the field asymmetry, the pulse shape from Gaussian to
super-Gaussian, and the carrier envelope phase, we show the momentum
distribution and the number density of created pairs to exhibit extreme
sensitivity to these field characteristics. The effects are also qualitatively
explained by analyzing the turning-point structures within the WKB formalism.
We observed that multiphoton pair production dominates in the case of long
falling-pulse asymmetry. For a short falling pulse with a flat-top
super-Gaussian laser profile, pair production is further facilitated. For
certain field parameters, we demonstrate that the number density can be
enhanced by two to three orders of magnitude.

</details>


### [41] [Diagnostics of a Multicusp-Assisted Inductively-Coupled Radio-Frequency Plasma Source for Plasma Immersion Ion Implantation](https://arxiv.org/abs/2510.18384)
*Moreno Joel,Jimenez Marilyn,Okerstrom Daniel,Bradley Michael P.,Couëdel Lénaïc*

Main category: physics.plasm-ph

TL;DR: Characterization of a multicusp-assisted RF plasma source for PIII using LIF and Langmuir probe diagnostics to measure ion temperature and drift velocity in argon plasmas.


<details>
  <summary>Details</summary>
Motivation: To enable stable plasma operation at low pressures and characterize plasma behavior near immersed electrodes in plasma immersion ion implantation systems.

Method: Used laser-induced fluorescence (LIF) and RF-compensated Langmuir probe diagnostics to measure ion temperature and drift velocity in argon plasmas with multicusp magnetic field configuration.

Result: Multicusp configuration enhanced plasma density at low pressure (0.8 mTorr), time-averaged measurements showed no perturbation near pulsed electrode, LIF-derived potential profiles matched Riemann's presheath theory, and ion velocity distributions revealed acceleration consistent with sheath dynamics.

Conclusion: LIF is suitable for steady-state characterization of bulk and presheath regions in PIII systems, supporting its use for plasma diagnostics in such applications.

Abstract: In this article, we present a detailed characterisation of a
multicusp-assisted inductively coupled RF plasma source for plasma immersion
ion implantation (PIII). Using laser-induced fluorescence (LIF) and
RF-compensated Langmuir probe diagnostics, we measured ion temperature T i and
drift velocity v z in argon plasmas near an immersed electrode. The multicusp
configuration enhances plasma density at low pressure, enabling stable
operation down to 0.8 mTorr. Timeaveraged measurements show no detectable
perturbation near the pulsed electrode, indicating full plasma recovery between
high-voltage pulses. LIF-derived potential profiles match Riemann's presheath
theory, and ion velocity distributions reveal acceleration consistent with
sheath dynamics. These results support the use of LIF for steady-state
characterisation of the bulk and presheath regions in PIII systems.

</details>


### [42] [Experimental and numerical investigation of suprathermal electron dynamics using vertical electron cyclotron emission](https://arxiv.org/abs/2510.18487)
*L. Votta,M. Hoppe,J. Decker,E. Devlaminck,A. S. Tema Biwolé,L. Porte,J. Cazabonne,Y. Savoye-Peysson,the TCV team*

Main category: physics.plasm-ph

TL;DR: YODA is a new synthetic ECE diagnostic framework that simulates electron cyclotron emission for arbitrary electron distributions and antenna geometries, validated against SPECE and successfully reproducing experimental VECE measurements in TCV tokamak.


<details>
  <summary>Details</summary>
Motivation: Reconstructing electron distributions from ECE measurements is challenging due to harmonic overlap and thermal radiation noise, requiring a forward modeling approach using kinetic simulations.

Method: Developed YODA framework for simulating ECE emission/absorption, combined with LUKE Fokker-Planck code to model electron distributions in ECCD experiments, and validated against SPECE code using TCV discharge data.

Result: Synthetic spectra from LUKE-YODA framework successfully reproduced main features of experimental VECE measurements in both simulated discharges.

Conclusion: The combination of kinetic and synthetic ECE simulations enables identification of electron distribution features that create specific signatures in VECE signals.

Abstract: The Tokamak \`a Configuration Variable (TCV) is equipped with an advanced set
of diagnostics for studying suprathermal electron dynamics. Among these, the
vertical electron cyclotron emission (VECE) diagnostic offers valuable insights
into the electron energy distribution by measuring electron cyclotron emission
(ECE) along a vertical line-of-sight. However, reconstructing the electron
distribution from ECE measurements is inherently challenging due to harmonic
overlap and thermal radiation noise. A more practical approach leverages
forward modeling of ECE based on kinetic simulations. To this end, we introduce
YODA, a novel synthetic ECE diagnostic framework that simulates emission and
(re)absorption of electron cyclotron radiation for arbitrary electron
distributions and antenna geometries. The framework is validated against the
well-established synthetic ECE code SPECE, using an ohmic TCV discharge as a
reference case. In this study, the 3D bounce-averaged Fokker-Planck code LUKE
is used to model electron distributions in two electron cyclotron current drive
(ECCD) experiments. The synthetic spectra generated using the combined
LUKE-YODA framework successfully reproduce the main features of the
experimental VECE measurements in both simulated discharges. The combination of
kinetic and synthetic ECE simulations allow the identification of the features
in the electron distribution function which give rise to certain signatures in
the VECE signal.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [43] [Semi-analytical pricing of American options with hybrid dividends via integral equations and the GIT method](https://arxiv.org/abs/2510.18159)
*Andrey Itkin*

Main category: q-fin.PR

TL;DR: A semi-analytical method using Generalized Integral Transform (GIT) for pricing American options with discrete/continuous dividends, transforming the PDE problem into integral equations to handle dividend-induced discontinuities efficiently.


<details>
  <summary>Details</summary>
Motivation: Traditional continuous-dividend models fail for American options with discrete dividends due to abrupt price drops and complex exercise timing, requiring better methods than standard numerical techniques.

Method: Uses Generalized Integral Transform (GIT) to convert the free-boundary PDE problem into Volterra integral equations, handling discrete dividends via Dirac delta functions in a GBM model.

Result: The GIT method proves highly accurate and computationally efficient, outperforming binomial trees and finite difference methods that struggle with dividend jump conditions.

Conclusion: GIT provides a powerful alternative to standard numerical methods, effectively handling discrete dividend discontinuities while maintaining accuracy and computational efficiency.

Abstract: This paper introduces a semi-analytical method for pricing American options
on assets (stocks, ETFs) that pay discrete and/or continuous dividends. The
problem is notoriously complex because discrete dividends create abrupt price
drops and affect the optimal exercise timing, making traditional
continuous-dividend models unsuitable. Our approach utilizes the Generalized
Integral Transform (GIT) method introduced by the author and his co-authors in
a number of papers, which transforms the pricing problem from a complex partial
differential equation with a free boundary into an integral Volterra equation
of the second or first kind. In this paper we illustrate this approach by
considering a popular GBM model that accounts for discrete cash and
proportional dividends using Dirac delta functions. By reframing the problem as
an integral equation, we can sequentially solve for the option price and the
early exercise boundary, effectively handling the discontinuities caused by the
dividends. Our methodology provides a powerful alternative to standard
numerical techniques like binomial trees or finite difference methods, which
can struggle with the jump conditions of discrete dividends by losing accuracy
or performance. Several examples demonstrate that the GIT method is highly
accurate and computationally efficient, bypassing the need for extensive
computational grids or complex backward induction steps.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [44] [Electron Acceleration via Lower-Hybrid Drift Instability in Astrophysical Plasmas: Dependence on Plasma Beta and Suprathermal Electron Distributions](https://arxiv.org/abs/2510.18182)
*Ji-Hoon Ha,Elena S. Volnova*

Main category: astro-ph.HE

TL;DR: This study investigates electron acceleration via lower-hybrid drift instability (LHDI) across various astrophysical plasma environments using plasma beta and suprathermal electron spectral slope as key parameters.


<details>
  <summary>Details</summary>
Motivation: Density inhomogeneities in space plasmas drive instabilities like LHDI, which are observed in magnetized environments but previously studied mainly in Earth's magnetosphere. This work extends LHDI investigation to broader astrophysical contexts.

Method: Used quasilinear model with two key parameters: plasma beta (magnetization) and spectral slope of suprathermal electrons. Analyzed critical conditions by comparing instability growth rate and fluctuation damping rate, and studied time evolution of electron distribution function.

Result: Electron acceleration is most efficient in low-beta plasmas (β < 1), but suprathermal electrons significantly enhance acceleration even in high-beta plasmas (β > 1).

Conclusion: LHDI plays an important role in electron acceleration across diverse astrophysical plasma environments, with suprathermal electrons being a key factor in enabling efficient acceleration.

Abstract: Density inhomogeneities are ubiquitous in space and astrophysical plasmas,
particularly at magnetic reconnection sites, shock fronts, and within
compressible turbulence. The gradients associated with these inhomogeneous
plasma regions serve as free energy sources that can drive plasma
instabilities, including the lower-hybrid drift instability (LHDI). Notably,
lower-hybrid waves are frequently observed in magnetized space plasma
environments, such as Earth's magnetotail and magnetopause. Previous studies
have primarily focused on modeling particle acceleration via LHDI in these
regions using a quasilinear approach. This study expands the investigation of
LHDI to a broader range of environments, spanning weakly to strongly magnetized
media, including interplanetary, interstellar, intergalactic, and intracluster
plasmas. To explore the applicability of LHDI in various astrophysical
settings, we employ two key parameters: (1) plasma magnetization, characterized
by the plasma beta parameter, and (2) the spectral slope of suprathermal
electrons following a power-law distribution. Using a quasilinear model, we
determine the critical values of plasma beta and spectral slope that enable
efficient electron acceleration via LHDI by comparing the rate of growth of
instability and the damping rate of the resulting fluctuations. We further
analyze the time evolution of the electron distribution function to confirm
these critical conditions. Our results indicate that electron acceleration is
generally most efficient in low-beta plasmas ($\beta < 1$). However, the
presence of suprathermal electrons significantly enhances electron acceleration
via LHDI, even in high-beta plasmas ($\beta > 1$). Finally, we discuss the
astrophysical implications of our findings, highlighting the role of LHDI in
electron acceleration across diverse plasma environments.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [45] [Uniqueness of Angular Velocity Reconstruction in Parallel-Beam and Diffraction Tomography](https://arxiv.org/abs/2510.18829)
*Peter Elbau,Denise Schmutz*

Main category: math.FA

TL;DR: This paper analyzes conditions for uniquely determining rotational motion from continuous measurements in parallel-beam and diffraction tomography, particularly for biological samples manipulated by optical/acoustic tweezers.


<details>
  <summary>Details</summary>
Motivation: The challenge of imaging trapped biological samples that are rotated using optical or acoustic tweezers, where determining the exact rotational motion is crucial for accurate reconstruction.

Method: Using the infinitesimal common line and circle method to analyze conditions for unique recovery of rotation parameters from continuous time-dependent measurements.

Result: Provides explicit criteria for sample structure and induced motion that guarantee unique reconstruction of all rotation parameters, and shows that the set of objects where uniqueness fails is nowhere dense.

Conclusion: The rotation of unknown samples can be uniquely recovered under specific structural and motion conditions, with failure cases being mathematically sparse.

Abstract: This work addresses the problem of uniquely determining a rotational motion
from continuous time-dependent measurements within the frameworks of
parallel-beam and diffraction tomography. The motivation stems from the
challenge of imaging trapped biological samples manipulated and rotated using
optical or acoustic tweezers. We analyze the conditions under which the
rotation of the unknown sample can be uniquely recovered using the
infinitesimal common line and circle method, respectively. We provide explicit
criteria for the sample's structure and the induced motion that guarantee
unique reconstruction of all rotation parameters. Moreover, we demonstrate that
the set of objects for which uniqueness fails is nowhere dense.

</details>


### [46] [Quantitative Stability in Discrete Optimal Transport](https://arxiv.org/abs/2510.17407)
*William Ford*

Main category: math.FA

TL;DR: This paper studies quantitative stability in optimal transport and uniqueness of dual transport problems, presenting five main contributions across different aspects of optimal transport theory.


<details>
  <summary>Details</summary>
Motivation: To investigate and extend understanding of stability properties in optimal transport problems and establish stronger uniqueness results for dual transport formulations.

Method: The research employs Wasserstein distance analysis, strong convexity inequalities for Kantorovich functionals, glueing arguments, discrete transport problem analysis, and extension of uniqueness criteria using Lipschitz-path connected support properties.

Result: Established quantitative stability results for optimal transport plans, extended strong convexity inequalities to broader measure classes, described behavior of discrete transport under perturbations, and proved uniqueness of dual optimizers for measures with Lipschitz-path connected support and bounded support.

Conclusion: The work provides comprehensive stability and uniqueness results across various optimal transport settings, significantly extending existing theory particularly for dual transport problems with structured marginal measures.

Abstract: This work investigates several aspects related to quantitative stability in
optimal transport, as well as uniqueness of the dual transport problem. Our
main contributions are as follows. Chapter 1: Observations regarding the
quantitative stability of optimal transport plans with respect to Wasserstein
distance on the product space. Chapter 2: Extention of strong convexity
inequalities for the Kantorovich functional to a larger class of source
measures, using glueing arguments recently used for the quantitative stability
of optimal transport maps. Chapters 3/4: A qualitative description of the
behaviour of the fully discrete transport problem under perturbation of the
support positions, as well as quantitative stability under uniqueness
assumptions. Chapter 5: Extention of known uniqueness criteria for the dual
transport problem. We show that when one marginal measure has Lipschitz-path
connected support and the other has bounded support, the values of dual
optimisers are unique up to a constant for a large family of costs, including
$p$-costs for all $p>1$.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [47] [GoodRegressor: A General-Purpose Symbolic Regression Framework for Physically Interpretable Materials Modeling](https://arxiv.org/abs/2510.18325)
*Seong-Hoon Jang*

Main category: cond-mat.mtrl-sci

TL;DR: GoodRegressor is a C++ symbolic regression framework that builds interpretable materials models by systematically exploring descriptor combinations and nonlinear transformations, achieving high accuracy in predicting activation energies and Arrhenius prefactors for oxygen-ion conductors.


<details>
  <summary>Details</summary>
Motivation: To bridge data-driven prediction with physical understanding in materials modeling by creating transparent, interpretable models that reveal clear physical trends rather than acting as black boxes.

Method: A five-module workflow (parser, designer, curator, regressor, and designer post-process) that systematically explores descriptor combinations, nonlinear transformations, and interaction terms to construct compact and physically interpretable models.

Result: Successfully predicted activation energies (R²=0.804) and Arrhenius prefactors (R²=0.723) for oxygen-ion conductors, revealing clear physical trends linking ionic transport to coordination environment, elastic moduli, and lattice flexibility.

Conclusion: GoodRegressor provides a transparent and extensible platform for symbolic regression that bridges data-driven prediction with physical understanding, applicable across diverse materials systems.

Abstract: GoodRegressor, a general-purpose, C++-based symbolic regression framework for
interpretable materials modeling, is presented. The workflow consists of five
modules, parser, designer, curator, regressor, and designer (as a
post-process), that systematically explore descriptor combinations, nonlinear
transformations, and interaction terms to construct compact and physically
interpretable models. Applied to oxygen-ion conductors, the framework
successfully predicts activation energies and Arrhenius prefactors with high
accuracy ($R^2=0.804$ and $R^2=0.723$, respectively) and reveals clear physical
trends linking ionic transport (particularly activation energy) to coordination
environment, elastic moduli, and lattice flexibility. GoodRegressor bridges
data-driven prediction and physical understanding, providing a transparent and
extensible platform for symbolic regression applicable across diverse materials
systems.

</details>


### [48] [Stability Criteria and Optoelectronic Properties of Mg3ZBr3 (Z = As, Sb, Bi) Perovskites for Evaluating the Performance in PIN Photo Diode](https://arxiv.org/abs/2510.18579)
*Md Mohiuddin,Mohammed Mehedi Hasan,Alamgir Kabir*

Main category: cond-mat.mtrl-sci

TL;DR: Lead-free Mg3ZBr3 (Z=As,Sb,Bi) halide perovskites are investigated as stable, non-toxic alternatives to lead-based perovskites for optoelectronic applications, showing promising optical and electronic properties.


<details>
  <summary>Details</summary>
Motivation: To address toxicity and stability issues of lead-based perovskites by searching for non-toxic, durable alternatives for optoelectronic applications.

Method: First-principles calculations including hybrid functionals for band gaps, optical spectra analysis, phonon dispersion studies, elastic analyses, and drift-diffusion p-i-n simulations.

Result: Cubic Pm-3m structures with indirect band gaps (2.06 eV for Mg3AsBr3, 1.65 eV for Mg3SbBr3), dynamical stability, strong light-matter coupling, decreasing bulk moduli from 44 GPa to 35 GPa along As→Sb→Bi series.

Conclusion: Mg3ZBr3 perovskites are promising lead-free candidates for stable thin-film photodiode and photovoltaic applications due to their favorable optoelectronic properties and stability.

Abstract: The toxicity and stability issues of lead-based perovskites motivate the
search for non-toxic, durable alternatives. This work examines lead-free
$\mathrm{Mg_3ZBr_3}$ ($Z=\mathrm{As,Sb,Bi}$) halide perovskites as
optoelectronic materials, with emphasis on $\mathrm{Mg_3AsBr_3}$ and
$\mathrm{Mg_3SbBr_3}$. First-principles calculations establish cubic
$Pm\bar{3}m$ frameworks stabilized by strong Mg--Br linkages, and indirect band
gaps of $2.0645\,\mathrm{eV}$ for $\mathrm{Mg_3AsBr_3}$ and
$1.6533\,\mathrm{eV}$ for $\mathrm{Mg_3SbBr_3}$ obtained using hybrid
functionals. Optical spectra show a rapid rise in absorption above the gap and
an increasing static dielectric response along
$\mathrm{As}\rightarrow\mathrm{Sb}\rightarrow\mathrm{Bi}$, indicating
strengthened light--matter coupling. Phonon dispersions lack imaginary
branches, confirming dynamical stability, and exhibit large mode anharmonicity
(Gr\"uneisen signatures) consistent with soft-lattice heat transport. Moving
down the pnictogen series expands the lattice and lowers the Goldschmidt
tolerance factor, while enhanced pnictogen--Br $p$-orbital hybridization and
stereochemically active $n\mathrm{s}^{2}$ lone pairs (Sb, Bi) narrow the band
gap and increase the optical dielectric response. Elastic analyses confirm Born
stability and moderate stiffness, with Hill-averaged bulk moduli decreasing
from approximately $44\,\mathrm{GPa}$ ($\mathrm{Mg_3AsBr_3}$) to
$35\,\mathrm{GPa}$ ($\mathrm{Mg_3BiBr_3}$). Drift--diffusion $p$--$i$--$n$
simulations qualitatively track band-edge-limited spectra, aligning with the
computed gaps. Together, these results position these materials as promising
lead-free candidates for stable thin-film photodiode and photovoltaic
applications.

</details>


### [49] [First-Principles Investigation of Sr2PrSbO6 Double Perovskite: An Emerging Aspirant for Electrocatalysis, Plasmonic, Photonics, Thermoelectric and Solar Cell Applications](https://arxiv.org/abs/2510.18609)
*Md. Mohiuddin,Alamgir Kabir*

Main category: cond-mat.mtrl-sci

TL;DR: First-principles DFT study of Sr2PrSbO6 reveals it's a stable cubic perovskite with wide bandgap (3.488 eV), strong UV absorption, and promising thermoelectric performance (ZT=0.33).


<details>
  <summary>Details</summary>
Motivation: To evaluate Sr2PrSbO6's potential for next-generation electrocatalysts, optoelectronic devices, and thermoelectric systems.

Method: First-principles calculations based on Density Functional Theory (DFT) including structural optimization, electronic band structure, density of states, optical properties, and thermoelectric analysis.

Result: Sr2PrSbO6 has stable cubic perovskite structure, direct bandgap of 3.488 eV, strong UV absorption, and room temperature ZT value of 0.33.

Conclusion: Sr2PrSbO6 is a promising material for optoelectronic applications (UV LEDs, PV-TE systems) and thermoelectric devices, serving as benchmark for future research.

Abstract: In this study, we investigate the structural properties, chemical stability,
and electronic, optical, and thermoelectric properties of
$\mathrm{Sr_2PrSbO_6}$ using first-principles calculations based on Density
Functional Theory (DFT). The goal of this study is to evaluate its potential
contribution to next-generation electrocatalysts, optoelectronic devices, and
thermoelectric systems. The structural optimization reveals that
$\mathrm{Sr_2PrSbO_6}$ crystallizes in a stable cubic perovskite structure with
space group $Fm\bar{3}m$. The calculated formation energy indicates high
thermodynamic stability, confirming the viability of $\mathrm{Sr_2PrSbO_6}$ for
practical applications. The electronic band structure calculations show that
$\mathrm{Sr_2PrSbO_6}$ is a wide bandgap semiconductor with a direct bandgap of
$3.488~\mathrm{eV}$ at the $\Gamma$-point. The calculated density of states
(DOS) indicates significant contributions from O $2p$, Sb $5p$, and Pr $5d$
orbitals. Optical property calculations, including the dielectric function and
absorption coefficient, reveal strong absorption in the UV regions, making
$\mathrm{Sr_2PrSbO_6}$ a promising candidate for optoelectronic applications
such as UV light-emitting diodes (LEDs) and photovoltaic-thermoelectric (PV-TE)
tandem systems. At room temperature, the calculated dimensionless quantity $ZT$
is $0.33$, which indicates this material as a possible candidate for
thermoelectric applications. Our results will serve as a benchmark for future
experimental and theoretical research on the properties of this material.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [50] [PCMS: Parallel Coupler For Multimodel Simulations](https://arxiv.org/abs/2510.18838)
*Jacob S. Merson,Cameron W. Smith,Mark S. Shephard,Fuad Hasan,Abhiyan Paudel,Angel Castillo-Crooke,Joyal Mathew,Mohammad Elahi*

Main category: cs.DC

TL;DR: PCMS is a GPU-accelerated coupling framework for multimodel simulations on supercomputers, supporting up to 5D field mapping with physics constraints, demonstrated with plasma physics codes and scaling to 2,080 GPUs.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient coupling framework for complex multimodel simulations on leadership-class supercomputers that can handle high-dimensional field mapping with physics constraints.

Method: Developed PCMS with distributed control and field mapping methods for up to five dimensions, utilizing discretization and field information to accommodate physics constraints.

Result: Successfully demonstrated coupling of plasma physics codes (XGC-DEGAS2 and GNET-GTC), achieved weak scaling on 2,080 GPUs of Frontier with 85% efficiency.

Conclusion: PCMS provides an effective GPU-accelerated coupling framework for complex multimodel simulations, demonstrating good scalability and practical applicability to plasma physics problems.

Abstract: This paper presents the Parallel Coupler for Multimodel Simulations (PCMS), a
new GPU accelerated generalized coupling framework for coupling simulation
codes on leadership class supercomputers. PCMS includes distributed control and
field mapping methods for up to five dimensions. For field mapping PCMS can
utilize discretization and field information to accommodate physics
constraints. PCMS is demonstrated with a coupling of the gyrokinetic
microturbulence code XGC with a Monte Carlo neutral transport code DEGAS2 and
with a 5D distribution function coupling of an energetic particle transport
code (GNET) to a gyrokinetic microturbulence code (GTC). Weak scaling is also
demonstrated on up to 2,080 GPUs of Frontier with a weak scaling efficiency of
85%.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [51] [A Quantum Algorithm for the Finite Element Method](https://arxiv.org/abs/2510.18150)
*Ahmad M. Alkadri,Tyler D. Kharazi,K. Birgitta Whaley,Kranthi K. Mandadapu*

Main category: quant-ph

TL;DR: Qu-FEM is a fault-tolerant quantum algorithm for the finite element method that preserves geometric flexibility through new primitives (unit of interaction and local-to-global indicator matrix), enabling global array assembly with constant-size linear combination of unitaries.


<details>
  <summary>Details</summary>
Motivation: To develop quantum PDE solvers that maintain the geometric flexibility of classical FEM while enabling quantum advantage, overcoming limitations of other quantum PDE approaches.

Method: Introduces two primitives: unit of interaction and local-to-global indicator matrix. Uses block-encodings for global arrays, performs numerical integration for varying coefficients, and enforces Dirichlet boundary conditions via Lagrange multipliers.

Result: For constant coefficients, achieves block-encodings requiring only O(d²p²n) Clifford+T gates for d-dimensional problems with order-p elements on grids with 2^n degrees of freedom per dimension.

Conclusion: Provides a framework for extending geometric flexibility in quantum PDE solvers while preserving potential quantum advantage, with explicit circuits demonstrated for modified Poisson equation in Cartesian domains.

Abstract: The finite element method (FEM) is a cornerstone numerical technique for
solving partial differential equations (PDEs). Here, we present
$\textbf{Qu-FEM}$, a fault-tolerant era quantum algorithm for the finite
element method. In contrast to other quantum PDE solvers, Qu-FEM preserves the
geometric flexibility of FEM by introducing two new primitives, the unit of
interaction and the local-to-global indicator matrix, which enable the assembly
of global finite element arrays with a constant-size linear combination of
unitaries. We study the modified Poisson equation as an elliptic problem of
interest, and provide explicit circuits for Qu-FEM in Cartesian domains. For
problems with constant coefficients, our algorithm admits block-encodings of
global arrays that require only $\tilde{\mathcal{O}}\left(d^2 p^2 n\right)$
Clifford+$T$ gates for $d$-dimensional, order-$p$ tensor product elements on
grids with $2^n$ degrees of freedom in each dimension, where $n$ is the number
of qubits representing the $N=2^n$ discrete grid points. For problems with
spatially varying coefficients, we perform numerical integration directly on
the quantum computer to assemble global arrays and force vectors. Dirichlet
boundary conditions are enforced via the method of Lagrange multipliers,
eliminating the need to modify the block-encodings that emerge from the
assembly procedure. This work presents a framework for extending the geometric
flexibility of quantum PDE solvers while preserving the possibility of a
quantum advantage.

</details>


### [52] [Information geometry of nonmonotonic quantum natural gradient](https://arxiv.org/abs/2510.18286)
*Hideyuki Miyahara*

Main category: quant-ph

TL;DR: The paper investigates nonmonotonic quantum natural gradient (QNG) methods that relax monotonicity conditions, showing they can achieve faster convergence than conventional QNG using the SLD metric.


<details>
  <summary>Details</summary>
Motivation: To resolve ambiguity in quantizing information-theoretic quantities and explore whether relaxing monotonicity conditions can lead to improved optimization performance in quantum machine learning.

Method: The authors demonstrate local optimality of SLD metric under monotonicity, analyze non-monotone quantum Fisher metrics, consider alternative quantum divergences, extend analysis to non-full-rank cases, and use Petz functions to design geometries.

Result: Nonmonotonic QNG achieves faster convergence compared to conventional QNG, and the analysis is extended beyond previous limitations of specific divergences and full-rank assumptions.

Conclusion: Relaxing monotonicity conditions in quantum Fisher metrics can lead to improved optimization performance in quantum natural gradient methods, with numerical simulations demonstrating advantages in quantum circuit learning parameter estimation.

Abstract: Natural gradient is an advanced optimization method based on information
geometry, where the Fisher metric plays a crucial role.
  Its quantum counterpart, known as quantum natural gradient (QNG), employs the
symmetric logarithmic derivative (SLD) metric, one of the quantum Fisher
metrics.
  While quantization in physics is typically well-defined via the canonical
commutation relations, the quantization of information-theoretic quantities
introduces inherent arbitrariness.
  To resolve this ambiguity, monotonicity has been used as a guiding principle
for constructing geometries in physics, as it aligns with physical intuition.
  Recently, a variant of QNG, which we refer to as nonmonotonic QNG in this
paper, was proposed by relaxing the monotonicity condition.
  It was shown to achieve faster convergence compared to conventional QNG.
  In this paper, we investigate the properties of nonmonotonic QNG.
  To ensure the paper is self-contained, we first demonstrate that the SLD
metric is locally optimal under the monotonicity condition and that
non-monotone quantum Fisher metrics can lead to faster convergence in QNG.
  Previous studies primarily relied on a specific type of quantum divergence
and assumed that density operators are full-rank.
  Here, we explicitly consider an alternative quantum divergence and extend the
analysis to non-full-rank cases.
  Additionally, we explore how geometries can be designed using Petz functions,
given that quantum Fisher metrics are characterized through them.
  Finally, we present numerical simulations comparing different quantum Fisher
metrics in the context of parameter estimation problems in quantum circuit
learning.

</details>


### [53] [Optimal quantum learning in proximity to universality](https://arxiv.org/abs/2510.18623)
*Moein N. Ivaki,Matias Karjula,Tapio Ala-Nissila*

Main category: quant-ph

TL;DR: The paper introduces a tunable N-qubit random circuit model with probabilistic Clifford gate substitution to study the boundary between classically simulable and quantum-superior systems in quantum reservoir computing.


<details>
  <summary>Details</summary>
Motivation: To identify the fundamental boundary between classically simulable systems and those with true quantum computational advantage, and understand the physical resources enabling quantum superiority.

Method: A tunable N-qubit random circuit model where a fraction p of Clifford gates are probabilistically substituted with nonstabilizing conditional-T gates, analyzing entanglement spectrum statistics and long-range nonstabilizer resource content.

Result: The learnability and scalability of quantum reservoirs can be continuously controlled by parameter p, allowing navigation from classically tractable to maximally expressive quantum dynamics. Anti-flatness scaling indicates concentration of measures as an impediment to learning.

Conclusion: The results provide architecture-agnostic strategies for designing powerful and trainable quantum machine learning systems, clarifying the physical resources underpinning quantum computational advantage.

Abstract: The boundary between classically simulable and computationally superior
quantum systems is fundamental to identifying true quantum advantage. We
investigate this within the framework of quantum reservoir computing by
introducing a tunable $N$-qubit random circuit model, where a fraction $p$ of
Clifford gates are probabilistically substituted with nonstabilizing
conditional-$\hat{T}$ gates. We establish a direct correspondence between the
reservoir's performance on temporal processing tasks and its entanglement
spectrum statistics and long-range nonstabilizer resource content. To assess
scalability, we study the scaling of the anti-flatness of states in the
large-$N$ limit at a fixed circuit depth ratio $d/N \sim \mathcal{O}(1)$. This
is taken as a witness to concentration of measures, a known impediment to
learning in thermalizing systems. We demonstrate that the learnability and
scalability of the reservoir can be continuously controlled by the parameter
$p$, allowing us to navigate from classically tractable to maximally expressive
quantum dynamics. These architecture-agnostic results offer a general strategy
for designing powerful and trainable quantum machine learning systems and
clarify the physical resources underpinning quantum computational advantage.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [54] [Smoothed Dissipative Particle Dynamics for Mesoscale Advection-Diffusion-Reaction Problems](https://arxiv.org/abs/2510.18458)
*Marina Echeverria Ferrero,Nicolas Moreno,Marco Ellero*

Main category: physics.flu-dyn

TL;DR: Development of a smoothed dissipative particle dynamics (SDPD) model that incorporates transport of reactants at sub-particle scales and compositional field evolution for advection-diffusion-reaction systems.


<details>
  <summary>Details</summary>
Motivation: To create a particle-based method capable of modeling soft matter systems with thermodynamic consistency and direct control over transport properties, particularly for complex systems governed by advection-diffusion-reaction dynamics.

Method: Extended SDPD model incorporating reactant transport on scales smaller than discretizing particles and compositional field evolution, implemented in LAMMPS and validated using benchmark problems across diffusion-dominated, reaction-dominated, and coupled ADR regimes.

Result: The SDPD model effectively captures complex behaviors including Turing pattern formation, demonstrating its capability to handle various ADR regimes.

Conclusion: The proposed SDPD model shows promise for applications in biology, chemistry, materials science, and environmental engineering due to its ability to model complex advection-diffusion-reaction systems.

Abstract: Smoothed dissipative particle dynamics (SDPD) is a widely used particle-based
method for modelling soft matter systems at mesoscopic and macroscopic scales,
offering thermodynamic consistency and direct control over the fluid's
transport properties. Here, we present an SDPD model that incorporates the
transport of reactants on scales smaller than the discretising particles,
including the evolution of compositional fields. The proposed methodology is
well-suited for modelling complex systems governed by
advection-diffusion-reaction (ADR) dynamics. Implemented in LAMMPS, the model
is validated using a range of benchmark problems spanning diffusion-dominated,
reaction-dominated, and coupled ADR regimes. Our simulation results demonstrate
that the implemented SDPD model effectively captures complex behaviours, such
as Turing pattern formation. The proposed model holds promise for applications
across various fields, including biology, chemistry, materials science, and
environmental engineering.

</details>


### [55] [Hydrodynamics of Flapping Foils Undergoing Irregular Motion with Application to Wave-Assisted Propulsion](https://arxiv.org/abs/2510.18710)
*Harshal S. Raut,Jung-Hee Seo,Rajat Mittal*

Main category: physics.flu-dyn

TL;DR: Irregular flapping motions in wave-assisted propulsion systems can generate greater thrust than regular sinusoidal motions, with spring-based pitch-limiting mechanisms outperforming angle-limiters.


<details>
  <summary>Details</summary>
Motivation: Most studies focus on regular sinusoidal kinematics, but realistic environments produce irregular motions due to disturbances, fluid-structure interactions, and control inputs. Understanding these irregular motions is particularly relevant for wave-assisted propulsion systems.

Method: Used time-accurate flow simulations of elliptic WAP foils subjected to irregular waves at three different sea-states, and employed a previously developed leading-edge vortex (LEV) model to analyze mechanisms.

Result: Irregular heaving and pitching generated greater mean thrust than energetically equivalent sinusoidal heaving. Spring-based pitch-limiting mechanisms yielded higher thrust than angle-limiters under the same conditions.

Conclusion: Irregular flapping motions can enhance thrust in wave-assisted propulsion systems, with critical interactions between unsteady flow structures and foil dynamics driving performance improvements.

Abstract: Flapping foils are widely studied as bioinspired propulsors, yet most
investigations have focused on regular, sinusoidal kinematics. In realistic
environments, however, irregular motions arise naturally due to environmental
disturbances, fluid-structure interactions, and control inputs, but their
hydrodynamic consequences remain largely unexplored. One system where response
to irregular forcing is particularly relevant is wave-assisted propulsion (WAP)
systems where free-to-pitch submerged foils generate thrust due to wave-induced
heaving. We employ time-accurate flow simulations of elliptic WAP foils
subjected to irregular waves at three different sea-states to gain insights
into this system. Our results demonstrate that irregular heaving and pitching
can generate greater mean thrust than energetically equivalent sinusoidal
heaving. Moreover, a spring-based pitch-limiting mechanism yields higher thrust
than an angle-limiter under the same conditions. By leveraging a previously
developed leading-edge vortex (LEV) model, we uncover the mechanisms driving
this thrust enhancement and highlight the critical interactions between
unsteady flow structures and foil dynamics. These findings provide new insights
into flapping-foil propulsion in irregular environments and have direct
implications for the design and optimization of WAP systems.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [56] [Self-Consistent Model for Gate Control of Narrow-, Broken-, and Inverted-Gap (Topological) Heterostructures](https://arxiv.org/abs/2510.18778)
*Maximilian Hofer,Christopher Fuchs,Moritz Siebert,Christian Berger,Lena Fürst,Martin Stehno,Steffen Schreyeck,Hartmut Buhmann,Tobias Kießling,Wouter Beugeling,Laurens W. Molenkamp*

Main category: cond-mat.mes-hall

TL;DR: The paper presents a full-band envelope-function approach for modeling narrow-, broken-, and inverted-gap materials where conventional wide-gap methods fail due to strong band hybridization and crossing.


<details>
  <summary>Details</summary>
Motivation: Conventional wide-gap approaches fail in narrow-, broken-, and inverted-gap materials because electrostatic potentials dramatically influence band structure, making it impossible to distinguish between electrons and holes and justify flat charge carrier distributions at charge neutrality.

Method: Implemented the full-band envelope-function approach by Andlauer and Vogl into the open-source software package kdotpy, which provides numerically stable and quantitatively accurate modeling of these complex systems.

Result: The approach successfully modeled experimental subband density evolution with top-gate voltage in thick (26 nm - 110 nm) topologically inverted HgTe quantum wells, demonstrating stable and accurate results where conventional methods fail.

Conclusion: The openly-available implementation in kdotpy is expected to greatly benefit the investigation of narrow-, broken-, and inverted-gap materials by providing reliable computational tools for these challenging systems.

Abstract: Even small electrostatic potentials can dramatically influence the band
structure of narrow-, broken-, and inverted-gap materials. A quantitative
understanding often necessitates a self-consistent Hartree approach. The
valence and conduction band states strongly hybridize and/or cross in these
systems. This makes distinguishing between electrons and holes impossible and
the assumption of a flat charge carrier distribution at the charge neutrality
point hard to justify. Consequently the wide-gap approach often fails in these
systems. An alternative is the full-band envelope-function approach by Andlauer
and Vogl, which has been implemented into the open-source software package
kdotpy (arXiv:2407.12651). We show that this approach and implementation gives
numerically stable and quantitatively accurate results where the conventional
method fails by modeling the experimental subband density evolution with
top-gate voltage in thick (26 nm - 110 nm), topologically inverted HgTe quantum
wells. We expect our openly-available implementation to greatly benefit the
investigation of narrow-, broken-, and inverted-gap materials.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [57] [Spatio-temporal dynamics for a class of monotone evolution systems](https://arxiv.org/abs/2510.18698)
*Taishan Yi,Xiao-Qiang Zhao*

Main category: math.DS

TL;DR: Establishes spreading properties and existence/non-existence of spatially heterogeneous steady states for monotone evolution systems without translational monotonicity, with applications to integro-difference equations.


<details>
  <summary>Details</summary>
Motivation: To study global dynamics of monotone evolution systems that lack translational monotonicity, particularly when one limiting system has spreading speeds while another has uniform asymptotic annihilation.

Method: Develop theoretical framework under abstract setting for monotone evolution systems, then apply to asymptotically homogeneous integro-difference equations with counter-example demonstration.

Result: Established spreading properties and conditions for existence/non-existence of spatially heterogeneous steady states; provided counter-example showing nonlinear function values at finite locations can produce nontrivial fixed points.

Conclusion: The developed theory successfully characterizes global dynamics for monotone systems without translational monotonicity, with applications demonstrating practical relevance and counter-intuitive behaviors in integro-difference equations.

Abstract: In this paper, under an abstract setting we establish the spreading
properties and the existence, non-existence and global attractivity of
spatially heterogeneous steady states for a large class of monotone evolution
systems without the translational monotonicity under the assumption that one
limiting system has both leftward and rightward spreading speeds and the other
one has the uniform asymptotic annihilation. Then we apply the developed theory
to study the global dynamics of asymptotically homogeneous integro-difference
equations, and provide a counter-example to show that the value of the
nonlinear function at the finite range of location may give rise to nontrivial
fixed points.

</details>
