<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 29]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [hep-ph](#hep-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [math.PR](#math.PR) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Computing excited states with isometric tensor networks in two-dimensions](https://arxiv.org/abs/2510.20063)
*Alec Dektor,Runze Chi,Roel Van Beeumen,Chao Yang*

Main category: math.NA

TL;DR: A new subspace iteration method using block-isoPEPS ansatz for computing excited states of 2D quantum many-body Hamiltonians with nearest neighbor interactions.


<details>
  <summary>Details</summary>
Motivation: To extend the successful block MPS framework from 1D chains to 2D lattices for studying excitations in quantum many-body systems beyond one dimension.

Method: Subspace iteration method based on block-isoPEPS ansatz that enables exact block orthogonalization, controlled local truncation via SVD, and efficient observable evaluation.

Result: Successfully computed excitations of 2D transverse-field Ising and Heisenberg models, demonstrating scalability and comparing favorably with existing PEPS methods.

Conclusion: Block isometric tensor networks provide a scalable framework for studying excitations in quantum many-body systems in two dimensions.

Abstract: We present a new subspace iteration method for computing low-lying eigenpairs
(excited states) of high-dimensional quantum many-body Hamiltonians with
nearest neighbor interactions on two-dimensional lattices. The method is based
on a new block isometric projected entangled pair state (block-isoPEPS) ansatz
that generalizes the block matrix product state (MPS) framework, widely used
for Hamiltonians defined on one-dimensional chains, to two-dimensions. The
proposed block-isoPEPS ansatz offers several attractive features for PEPS-based
algorithms, including exact block orthogonalization, controlled local
truncation via singular value decompositions, and efficient evaluation of
observables. We demonstrate the proposed inexact subspace iteration for
block-isoPEPS by computing excitations of the two-dimensional transverse-field
Ising and Heisenberg models and compare our results with existing PEPS methods.
Our results demonstrate that block isometric tensor networks provide a scalable
framework for studying excitations in quantum many-body systems beyond one
dimension.

</details>


### [2] [Minimizing Residuals in ODE Integration Using Optimal Control](https://arxiv.org/abs/2510.20117)
*Robert M. Corless,C. Yalçın Kaya*

Main category: math.NA

TL;DR: The paper studies fitting curves through ODE solver solution points (skeleton) by minimizing ODE residual norms, reformulating it as a multi-stage optimal control problem and solving analytically for test problems and numerically for Van der Pol equation.


<details>
  <summary>Details</summary>
Motivation: To improve interpolation between discrete solution points from ODE solvers by minimizing the residual error of the original differential equation, rather than using standard interpolation methods.

Method: Reformulate the interpolation problem as a multi-stage optimal control problem, apply maximum principle for optimality conditions, solve analytically for test problems (Dahlquist, leaky bucket) and numerically for Van der Pol equation using optimization software.

Result: Developed interpolating curves with minimal residual norms, compared favorably with MATLAB's deval function residuals using skeletons from various MATLAB ODE solvers.

Conclusion: The proposed approach provides superior interpolation curves that better satisfy the original ODE compared to standard interpolation methods, as demonstrated through analytical solutions and numerical experiments.

Abstract: Given the set of discrete solution points or nodes, called the skeleton,
generated by an ODE solver, we study the problem of fitting a curve passing
through the nodes in the skeleton minimizing a norm of the residual vector of
the ODE. We reformulate this interpolation problem as a multi-stage optimal
control problem and, for the minimization of two different norms, we apply the
associated maximum principle to obtain the necessary conditions of optimality.
We solve the problem analytically for the Dahlquist test problem and a variant
of the leaky bucket problem, in terms of the given skeleton. We also consider
the Van der Pol equation, for which we obtain interpolating curves with minimal
residual norms by numerically solving a direct discretization of the problem
through optimization software. With the skeletons obtained by various ODE
solvers of MATLAB, we make comparisons between the residuals obtained by our
approach and those obtained by the MATLAB function deval.

</details>


### [3] [Joint Signal Recovery and Uncertainty Quantification via the Residual Prior Transform](https://arxiv.org/abs/2510.20136)
*Yao Xiao,Anne Gelb*

Main category: math.NA

TL;DR: The paper introduces a hierarchical Bayesian framework that reformulates the residual transform operator as a new prior, enabling uncertainty quantification and joint recovery from multimodal data without requiring prior structural information.


<details>
  <summary>Details</summary>
Motivation: Conventional signal recovery priors assume fixed signal variability types, which is problematic for complex signals with varying behaviors across domains. The residual transform operator addresses this but lacks uncertainty quantification and multimodal data fusion capabilities.

Method: Reformulates the residual transform operator into a hierarchical Bayesian prior framework, enabling principled uncertainty quantification and joint signal recovery from multimodal measurements by coherently fusing information from disparate data sources.

Result: Numerical experiments demonstrate that the residual prior yields high-fidelity signal and image recovery from multimodal data while providing robust uncertainty quantification through credible intervals.

Conclusion: The Bayesian residual prior framework successfully addresses limitations of conventional priors by enabling uncertainty quantification and multimodal data fusion without requiring prior structural information about the signal.

Abstract: Conventional priors used for signal recovery are often limited by the
assumption that the type of a signal's variability, such as piecewise constant
or linear behavior, is known and fixed. This assumption is problematic for
complex signals that exhibit different behaviors across the domain. The
recently developed {\em residual transform operator} effectively reduces such
variability-dependent error within the LASSO regression framework. Importantly,
it does not require prior information regarding structure of the underlying
signal. This paper reformulates the residual transform operator into a new
prior within a hierarchical Bayesian framework. In so doing, it unlocks two
powerful new capabilities. First, it enables principled uncertainty
quantification, providing robust credible intervals for the recovered signal,
and second, it provides a natural framework for the joint recovery of signals
from multimodal measurements by coherently fusing information from disparate
data sources. Numerical experiments demonstrate that the residual prior yields
high-fidelity signal and image recovery from multimodal data while providing
robust uncertainty quantification.

</details>


### [4] [General transformation neural networks: A class of parametrized functions for high-dimensional function approximation](https://arxiv.org/abs/2510.20142)
*Xiaoyang Wang,Yiqi Gu*

Main category: math.NA

TL;DR: Proposes GTNNs (general transformation neural networks) with cubic and quadratic variants (CTNNs/QTNNs) that generalize affine transformations to improve accuracy for oscillatory functions in high-dimensional approximation problems.


<details>
  <summary>Details</summary>
Motivation: Conventional deep neural networks perform poorly for oscillatory functions under gradient descent training, requiring more general transformations to improve approximation accuracy.

Method: Generalize affine transformations in neurons to more complex functions (cubic and quadratic transformations) that act as shape functions with larger capacity. Provide theoretical analysis including universal approximation properties and error bounds.

Result: CTNNs and QTNNs demonstrate superior accuracy and robustness compared to conventional fully connected neural networks in regression problems and PDE applications.

Conclusion: GTNNs, particularly CTNNs and QTNNs, offer improved performance for high-dimensional approximation tasks involving oscillatory functions through more flexible transformation capabilities.

Abstract: We propose a novel class of neural network-like parametrized functions, i.e.,
general transformation neural networks (GTNNs), for high-dimensional
approximation. Conventional deep neural networks sometimes perform less
accurately in approximation problems under gradient descent training,
especially when the target function is oscillatory. To improve accuracy, we
generalize the affine transformation of the abstract neuron to more general
functions, which act as complex shape functions and have larger capacities.
Specifically, we introduce two types of GTNNs: the cubic and quadratic
transformation neural networks (CTNNs and QTNNs). We perform approximation
error analysis for CTNNs and QTNNs, presenting their universal approximation
properties for continuous functions and error bounds for smooth functions and
Barron-type functions. Several numerical examples of regression problems and
partial differential equations are presented, demonstrating that CTNNs/QTNNs
have advantages in accuracy and robustness over conventional fully connected
neural networks.

</details>


### [5] [IEnSF: Iterative Ensemble Score Filter for Reducing Error in Posterior Score Estimation in Nonlinear Data Assimilation](https://arxiv.org/abs/2510.20159)
*Zezhong Zhang,Feng Bao,Guannan Zhang*

Main category: math.NA

TL;DR: The paper proposes an iterative ensemble score filter (IEnSF) that improves upon the Ensemble Score Filter by using an iterative algorithm to reduce posterior score estimation errors in nonlinear data assimilation problems.


<details>
  <summary>Details</summary>
Motivation: The current Ensemble Score Filter uses a heuristic weighted sum to combine prior and likelihood scores, introducing structural errors in nonlinear settings. This work aims to address this limitation.

Method: Developed IEnSF that applies an iterative algorithm as an outer loop around the reverse-time SDE solver, gradually reducing posterior score estimation error by improving approximation of the conditional expectation of the likelihood score function.

Result: Numerical experiments show IEnSF substantially reduces posterior score estimation error in nonlinear settings and improves accuracy of tracking high-dimensional dynamical systems.

Conclusion: The iterative approach in IEnSF successfully addresses the structural error in posterior score estimation for nonlinear data assimilation problems, with iteration requirements depending on prior-posterior distance and observation operator nonlinearity.

Abstract: The Ensemble Score Filter (EnSF) has emerged as a promising approach to
leverage score-based diffusion models for solving high-dimensional and
nonlinear data assimilation problems. While initial applications of EnSF to the
Lorenz-96 model and the quasi-geostrophic system showed potential, the current
method employs a heuristic weighted sum to combine the prior and the likelihood
score functions. This introduces a structural error into the estimation of the
posterior score function in the nonlinear setting. This work addresses this
challenge by developing an iterative ensemble score filter (IEnSF) that applies
an iterative algorithm as an outer loop around the reverse-time stochastic
differential equation solver. When the state dynamics or the observation
operator is nonlinear, the iterative algorithm can gradually reduce the
posterior score estimation error by improving the accuracy of approximating the
conditional expectation of the likelihood score function. The number of
iterations required depends on the distance between the prior and posterior
distributions and the nonlinearity of the observation operator. Numerical
experiments demonstrate that the IEnSF algorithm substantially reduces the
error in posterior score estimation in the nonlinear setting and thus improves
the accuracy of tracking high-dimensional dynamical systems.

</details>


### [6] [Anderson-type acceleration method for Deep Neural Network optimization](https://arxiv.org/abs/2510.20254)
*Kazufumi Ito,Tiancheng Xue*

Main category: math.NA

TL;DR: Anderson-type acceleration method for stochastic gradient descent improves DNN performance, applicable to DNN and CNN, with applications in computer tomography and inverse medium problems.


<details>
  <summary>Details</summary>
Motivation: To enhance neural network optimization for deep neural networks (DNN) by improving the stochastic gradient descent method.

Method: Developed Anderson-type acceleration method for stochastic gradient descent optimization.

Result: Significant improvement in neural network performance, demonstrated applicability for DNN and CNN architectures.

Conclusion: The proposed acceleration method effectively enhances DNN optimization and shows promise for applications in computer tomography and inverse medium problems.

Abstract: In this paper we consider the neural network optimization for DNN. We develop
Anderson-type acceleration method for the stochastic gradient decent method and
it improves the network permanence very much. We demonstrate the applicability
of the method for DNN and CNN. We discuss the application of the general class
of the neural network design for computer tomography and inverse medium
problems.

</details>


### [7] [Well-Posedness and Approximation of Weak Solutions to Time Dependent Maxwell's Equations with $L^2$-Data](https://arxiv.org/abs/2510.20752)
*Harbir Antil*

Main category: math.NA

TL;DR: Analysis of Maxwell's equations in conducting media with rough coefficients and L²-data, proving well-posedness and developing structure-preserving finite element methods.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for Maxwell's equations in realistic settings with imperfect media (rough coefficients) and develop numerical methods that preserve key physical properties like Gauss law and energy conservation.

Method: Direct proof of well-posedness using interior-in-time mollification for uniqueness and Galerkin method for existence. Structure-preserving semi-discrete finite element method based on Nédélec/Raviart-Thomas de Rham complex.

Result: Proved well-posedness (existence, uniqueness, energy identity, continuous dependence) for first-order weak formulation. Developed numerical scheme preserving discrete Gauss law and energy identity, with proven convergence to weak solution.

Conclusion: The paper provides complete mathematical analysis and structure-preserving numerical methods for Maxwell's equations in conducting media with rough coefficients, establishing rigorous foundations for both theoretical and computational approaches.

Abstract: We study Maxwell's equations in conducting media with perfectly conducting
boundary conditions on Lipschitz domains, allowing rough material coefficients
and $L^2$-data. Our first contribution is a direct proof of well-posedness of
the first-order weak formulation, including solution existence and uniqueness,
an energy identity, and continuous dependence on the data. The argument uses
interior-in-time mollification to show uniqueness while avoiding reflection
techniques. Existence is via the well-known Galerkin method (cf.~Duvaut and
Lions \cite[Eqns.~(4.31)--(4.32), p.~346; Thm.~4.1]{GDuvaut_JLLions_1976a}).
For completeness, and to make the paper self-contained, a complete proof has
been provided.
  Our second contribution is a structure-preserving semi-discrete finite
element method based on the N\'ed\'elec/Raviart--Thomas de Rham complex. The
scheme preserves a discrete Gauss law for all times and satisfies a
continuous-in-time energy identity with stability for nonnegative conductivity.
With a divergence-free initialization of the magnetic field (via potential
reconstruction or constrained $L^2$ projection), we prove convergence of the
semi-discrete solutions to the unique weak solution as the mesh is refined. The
analysis mostly relies on projector consistency, weak-* compactness in
time-bounded $L^2$ spaces, and identification of time derivatives in dual
spaces.

</details>


### [8] [Unique continuation for the wave equation: the stability landscape](https://arxiv.org/abs/2510.20359)
*Erik Burman,Lauri Oksanen,Janosch Preuss,Ziyao Zhao*

Main category: math.NA

TL;DR: Unique continuation for wave equations with volumetric data, achieving Hölder stability in subsets and Lipschitz stability with boundary trace information, enabling finite element method design.


<details>
  <summary>Details</summary>
Motivation: To address unique continuation problems for wave equations when data is only available in volumetric subsets rather than on lateral boundaries, which is common in practical applications.

Method: Mathematical analysis of unique continuation properties, proving Hölder stability in proper subsets and Lipschitz stability when boundary trace information is available in finite-dimensional spaces.

Result: Proved that solutions can be continued with Hölder stability into proper subsets without lateral boundary data, and with Lipschitz stability when boundary trace information is available.

Conclusion: The stability results enable the design of finite element methods that converge to exact solutions at rates matching the continuous problem's stability properties.

Abstract: We consider a unique continuation problem for the wave equation given data in
a volumetric subset of the space time domain. In the absence of data on the
lateral boundary of the space-time cylinder we prove that the solution can be
continued with H\"older stability into a certain proper subset of the
space-time domain. Additionally, we show that unique continuation of the
solution to the entire space-time cylinder with Lipschitz stability is possible
given the knowledge of a suitable finite dimensional space in which the trace
of the solution on the lateral boundary is contained. These results allow us to
design a finite element method that provably converges to the exact solution at
a rate that mirrors the stability properties of the continuous problem.

</details>


### [9] [Improving the accuracy of meshless methods via resolving power optimisation using multiple kernels](https://arxiv.org/abs/2510.20365)
*H. Broadley,J. R. C. King,S. J. Lind*

Main category: math.NA

TL;DR: A framework for optimizing resolving power in meshless methods by exploiting non-unique kernels to improve spatial derivative approximations, showing improved accuracy for turbulent flow simulations without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: Meshless methods are widely used for turbulent flow simulations but their accuracy has been assessed mainly through polynomial consistency, with little attention to resolving power. This work addresses the need to optimize resolving power for better accuracy in PDE solutions.

Method: Using linear combinations of kernels to maximize resolving power over a range of wavenumbers, exploiting the non-uniqueness of kernels in meshless methods. The approach considers both wavenumber magnitude and orientation dependencies.

Result: The optimized kernels show improved accuracy in convergence tests with little impact on stability for time-dependent problems. Significant accuracy gains are achieved for various PDE systems without extra computational cost per timestep in Eulerian frameworks.

Conclusion: The optimization procedure enables accurate simulation of PDE systems with short spatial scales, particularly beneficial for turbulent flow fields, by providing improved resolution characteristics in meshless methods.

Abstract: Meshless methods are commonly used to determine numerical solutions to
partial differential equations (PDEs) for problems involving free surfaces
and/or complex geometries, approximating spatial derivatives at collocation
points via local kernels with a finite size. Despite their common use in
turbulent flow simulations, the accuracy of meshless methods has typically been
assessed using their convergence characteristics resulting from the polynomial
consistency of approximations to operators, with little to no attention paid to
the resolving power of the approximation. Here we provide a framework for the
optimisation of resolving power by exploiting the non-uniqueness of kernels to
provide improvements to numerical approximations of spatial derivatives. We
first demonstrate that, unlike in finite-difference approximations, the
resolving power of meshless methods is dependent not only on the magnitude of
the wavenumber, but also its orientation, before using linear combinations of
kernels to maximise resolving power over a range of wavenumbers. The new
approach shows improved accuracy in convergence tests and has little impact on
stability of time-dependent problems for a range of Eulerian meshless methods.
Solutions to a variety of PDE systems are computed, with significant gains in
accuracy for no extra computational cost per timestep in Eulerian frameworks.
The improved resolution characteristics provided by the optimisation procedure
presented herein enable accurate simulation of systems of PDEs whose solution
contains short spatial scales such as flow fields with homogeneous isotropic
turbulence.

</details>


### [10] [Projecting onto the Unit Dual Quaternion Set](https://arxiv.org/abs/2510.20425)
*Ziyang Li,Chunfeng Cui,Jiaxin Xie*

Main category: math.NA

TL;DR: This paper systematically studies projections onto unit dual quaternion sets under the 2^R-norm, identifying distinct cases based on standard and dual parts relationships.


<details>
  <summary>Details</summary>
Motivation: Dual quaternions have wide applications in multi-agent formation control, 3D motion modeling, and robotics, making projections onto unit dual quaternion sets a fundamental research aspect.

Method: The authors systematically study projections under the 2^R-norm, identifying several distinct cases based on the relationship between standard and dual parts in vector form.

Result: The proposed algorithm demonstrates effectiveness through numerical experiments.

Conclusion: The systematic approach to dual quaternion projections provides practical solutions for applications requiring unit dual quaternion constraints.

Abstract: Dual quaternions have gained significant attention due to their wide
applications in areas such as multi-agent formation control, 3D motion
modeling, and robotics. A fundamental aspect in dual quaternion research
involves the projection onto unit dual quaternion sets. In this paper, we
systematically study such projections under the $2^R$-norm, which is commonly
used in practical applications. We identify several distinct cases based on the
relationship between the standard and dual parts in vector form, and
demonstrate the effectiveness of the proposed algorithm through numerical
experiments.

</details>


### [11] [Preconditioning of a pollution-free discretization of the Helmholtz equation](https://arxiv.org/abs/2510.20564)
*Harald Monsuur*

Main category: math.NA

TL;DR: A pollution-free FOSLS formulation for Helmholtz equation with block preconditioner that shows linear iteration dependence on wave number.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and easy-to-implement iterative solver for Helmholtz equation that avoids pollution effects and works on general domains including scattering problems.

Method: First order system least squares (FOSLS) formulation with block preconditioner consisting of Schur complement preconditioner and test space preconditioner using subspace correction to maintain Hermitian positive definiteness.

Result: Numerical experiments show linear dependence of MINRES iterations on wave number κ, and an algebraic error estimation approach prevents unnecessary iterations.

Conclusion: The proposed method provides an effective pollution-free solution for Helmholtz equation with good computational efficiency and applicability to general domains.

Abstract: We present a pollution-free first order system least squares (FOSLS)
formulation for the Helmholtz equation, solved iteratively using a block
preconditioner. This preconditioner consists of two components: one for the
Schur complement, which corresponds to a preconditioner on $L_2(\Omega)$, and
another defined on the test space, which we ensure remains Hermitian positive
definite using subspace correction techniques. The proposed method is easy to
implement and is directly applicable to general domains, including scattering
problems. Numerical experiments demonstrate a linear dependence of the number
of MINRES iterations on the wave number $\kappa$. We also introduce an approach
to estimate algebraic errors which prevents unnecessary iterations.

</details>


### [12] [Advancing Offshore Renewable Energy: Techno-Economic and Dynamic Performance of Hybrid Wind-Wave Systems](https://arxiv.org/abs/2510.20601)
*Alaa Ahmed,Maha N. Haji*

Main category: math.NA

TL;DR: Hybrid offshore wind-wave energy systems combining floating wind turbines with wave energy converters can enhance power generation, stabilize output, and reduce costs. Integration reduces wave energy converter costs by 15-83% and cuts power fluctuations by 50%.


<details>
  <summary>Details</summary>
Motivation: Offshore wind has matured but wave energy remains costly. Hybrid systems can enhance power generation, stabilize output, and reduce costs by leveraging synergies between technologies.

Method: Analyzed six configurations combining RM3 wave energy converter with 5MW and 15MW wind turbines on spar and semi-submersible platforms. Examined dynamic response, mooring loads, and power production under varying conditions, including wave converter float motion and reaction plate effects.

Result: Reaction plate improves damping for spar platform, enhancing wave energy absorption. Integration reduces wave energy converter levelized cost by 15-83% without affecting wind turbine costs. Power fluctuations reduced by 50%. Hybridization reduces costs with 5MW turbine but slightly increases with 15MW turbine.

Conclusion: Hybrid systems create mutualistic relationship where wave energy converter benefits substantially while wind turbine experiences slight improvements or negligible effects. Research demonstrates potential to lower costs and support sustainable energy solutions through optimized hybrid offshore renewable systems.

Abstract: Offshore wind and wave energy offer high energy density and availability.
While offshore wind has matured significantly, wave energy remains costly and
under development. Integrating both technologies into a hybrid system can
enhance power generation, stabilize output, and reduce costs. This study
explores the benefits of combining an offshore floating wind turbine with the
two-body heaving point absorber wave energy converter, Reference Model 3 (RM3).
Six configurations are analyzed: RM3 integrated with the National Renewable
Energy Laboratory 5 MW and the International Energy Agency 15 MW wind turbines,
each tested on both spar and semi-submersible platforms. The analysis examines
dynamic response, mooring loads, and power production under varying
environmental conditions, considering the influence of the wave energy
converter float motion and an optional reaction plate. Results indicate that
the reaction plate improves damping for the spar platform, enhancing wave
energy absorption and power output. A comparative analysis indicates that
integrating the wave energy converter reduces its levelized cost of energy by
15-83%, while leaving the wind turbine levelized cost of energy unaffected.
Hybridization significantly reduces power fluctuations by 50%, reduces the
levelized cost of energy with the 5 MW wind turbine, and slightly increases it
with the 15 MW wind turbine. The results highlight a mutualistic relationship
between the wave energy converter and the offshore wind turbine, where the
former benefits substantially while the latter experiences slight improvements
or negligible effects. Additional findings quantify hydrodynamic interactions,
mooring performance, and economic feasibility. This research provides insights
into optimizing hybrid offshore renewable systems, demonstrating their
potential to lower costs and support sustainable energy solutions.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [Non-uniqueness and failure of Calderón-Zygmund estimates below the critical exponent for non-monotone PDE with linear growth](https://arxiv.org/abs/2510.20024)
*Akshara Vincent*

Main category: math.AP

TL;DR: Counterexamples to uniqueness and a priori Calderón-Zygmund estimates for non-monotone elliptic equations using convex integration.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that uniqueness and certain a priori estimates fail for elliptic equations with non-monotone operators, even when they are smooth, uniformly elliptic, and have essentially linear growth.

Method: Uses convex integration argument to construct counterexamples for equations of the form div(A(∇u)) = 0 in the unit ball, where A is smooth, uniformly elliptic, has essentially linear growth but is not monotone.

Result: Provides explicit counterexamples showing that solutions to such equations are not unique and that Calderón-Zygmund estimates fail below L^2 regularity.

Conclusion: Monotonicity is essential for uniqueness and certain a priori estimates in elliptic equations, even when other favorable conditions like smoothness and uniform ellipticity are present.

Abstract: We provide counterexamples to uniqueness of solutions as well as a priori
Calder\'on-Zygmund estimates for solutions below $L^2$ using convex integration
argument for equations of the type $$ \text{div} (A (\nabla u)) = 0 \quad
\text{in } \mathbb{B}^2, $$ where $A: \mathbb{R}^{2} \to \mathbb{R}^2$ is
smooth, uniformly elliptic and has essentially linear growth, but fails to be
monotone.

</details>


### [14] [Well-posedness for a class of parabolic equations with singular-degenerate coefficients](https://arxiv.org/abs/2510.20051)
*Junyuan Fang,Tuoc Phan*

Main category: math.AP

TL;DR: This paper studies linear parabolic equations with measurable coefficients in divergence form, where heat capacity coefficients belong to Muckenhoupt weight classes, allowing for degenerate and singular coefficients. It introduces weighted parabolic cylinders and Sobolev spaces, and proves regularity estimates, existence, and uniqueness of weak solutions under small oscillation assumptions.


<details>
  <summary>Details</summary>
Motivation: To handle linear parabolic equations with coefficients that can be degenerate, singular, or both, which arise in various physical applications and require specialized mathematical frameworks beyond standard Sobolev spaces.

Method: Uses the level-set method by Caffarelli and Peral, introduces weighted parabolic cylinders with non-homogeneous quasi-distance, develops weighted parabolic Sobolev spaces, establishes weighted inequalities and a weighted Aubin-Lions compactness theorem.

Result: Proves regularity estimates, existence, and uniqueness of weak solutions in the weighted Sobolev spaces under small mean oscillation assumptions on coefficients.

Conclusion: Provides a comprehensive framework for analyzing degenerate and singular parabolic equations through weighted function spaces and establishes fundamental existence, uniqueness, and regularity results.

Abstract: This paper studies a class of linear parabolic equations with measurable
coefficients in divergence form whose volumetric heat capacity coefficients are
assumed to be in some Muckenhoupt class of weights. As such, the coefficients
can be degenerate, singular, or both degenerate and singular. A class of
weighted parabolic cylinders with a non-homogeneous quasi-distance function,
and a class of weighted parabolic Sobolev spaces intrinsically suitable for the
class of equations are introduced. Under some smallness assumptions on the mean
oscillations of the coefficients, regularity estimates, existence, and
uniqueness of weak solutions in the weighted Sobolev spaces are proved. To
achieve the results, we apply the level-set method introduced by Caffarelli and
Peral. Several weighted inequalities and a version of weighted Aubin-Lions
compactness theorem for sequences in weighted parabolic Sobolev spaces are
established.

</details>


### [15] [Time-periodic solutions to the cubic wave equation: an elementary constructive approach](https://arxiv.org/abs/2510.20054)
*Filip Ficek*

Main category: math.AP

TL;DR: Elementary proof of infinite family of time-periodic solutions for 1D cubic wave equation with Dirichlet BCs using perturbative expansion and Banach contraction principle.


<details>
  <summary>Details</summary>
Motivation: To establish existence of infinite family of time-periodic solutions with explicit frequency and structure information, improving on previous results.

Method: First order perturbative expansion combined with Banach contraction principle to prove existence of nearby solutions.

Result: Successfully proved existence of infinite family of time-periodic solutions with explicit frequency and structural information.

Conclusion: The approach provides an elementary proof with explicit information about solution frequencies and structures, advancing beyond previous results.

Abstract: We present an elementary proof of existence of infinite family of
time-periodic solutions to the one-dimensional nonlinear cubic wave equation
with Dirichlet boundary conditions. It relies on the first order perturbative
expansion and uses the Banach contraction principle to show existence of nearby
solutions. In contrast to the previous results, this approach provides us
explicit information about the frequencies and structures of the obtained
solutions.

</details>


### [16] [Existence and qualitative properties of ground state solutions for the Schrödinger-Bopp-Podolsky system](https://arxiv.org/abs/2510.20143)
*Sheng Wang,Juan Huang*

Main category: math.AP

TL;DR: The paper studies the Schrödinger-Bopp-Podolsky system, proving existence of nontrivial and ground state solutions using mountain-pass lemma, and analyzing their properties including positivity, symmetry, and decay behavior.


<details>
  <summary>Details</summary>
Motivation: To establish the existence and properties of solutions to the nonlinear nonlocal Schrödinger-Bopp-Podolsky system, which describes quantum fields coupled with electromagnetic fields in Bopp-Podolsky theory under electrostatic conditions.

Method: Applied mountain-pass lemma to obtain nontrivial solutions, used energy estimates to prove ground state existence, analyzed critical point paths to show mountain-pass type ground states, and studied solution properties including positivity, symmetry, and asymptotic behavior.

Result: Proved existence of nontrivial solutions and ground state solutions, demonstrated these are mountain-pass type ground states, established positivity, radial symmetry, rotational invariance, and exponential decay of solutions, and analyzed asymptotic behavior with respect to parameter a in radial case.

Conclusion: The Schrödinger-Bopp-Podolsky system admits ground state solutions with mountain-pass structure that possess desirable physical properties including positivity, symmetry, and decay behavior, with well-defined asymptotic limits.

Abstract: This paper concerns the existence and related properties of solutions to the
Schr\"{o}dinger-Bopp-Podolsky system, which reduces to a nonlinear and nonlocal
partial differential equation describing a Schr\"{o}dinger field coupled with
its electromagnetic field in Bopp-Podolsky theory under purely electrostatic
conditions. Firstly, by applying the mountain-pass lemma, we obtain the
existence of nontrivial solutions. Then, through some estimates of the ground
state energy, we prove the existence of ground state solutions. By exploring
the relationship between solutions and paths associated with critical points,
we further demonstrate that the obtained solutions are ground states of
mountain-pass type. Additionally, the positivity, radial symmetry, rotational
invariance, and exponential decay of the ground state solutions are considered.
Finally, in the radial case, we explore the asymptotic behavior of the obtained
solutions with respect to $a$.

</details>


### [17] [Asymptotic issue for fractional laplacian on long cylinders](https://arxiv.org/abs/2510.20263)
*Tahir Boudjeriou,Prosenjit Roy*

Main category: math.AP

TL;DR: Analysis of weak solutions to fractional p-Laplacian problems in unbounded cylindrical domains, extending local results to nonlocal setting.


<details>
  <summary>Details</summary>
Motivation: To study asymptotic behavior of weak solutions for elliptic and parabolic problems with fractional p-Laplacian in domains unbounded in one direction, addressing technical challenges from nonlocal operators.

Method: Developed a nonlocal abstract framework to analyze weak solutions, extending techniques from local setting to handle nonlocal operator difficulties.

Result: Main results extend and complement related properties previously established in the local setting for similar problems.

Conclusion: Successfully developed framework for analyzing fractional p-Laplacian problems in unbounded cylindrical domains, overcoming nonlocal operator challenges and extending local results.

Abstract: In this paper, we are concerned with the asymptotic behavior of weak
solutions to certain elliptic and parabolic problems involving the fractional
$p$-Laplacian in cylindrical domains that become unbounded in one direction.
The nonlocal nature of the operator describing the equations creates several
technical difficulties in treating problems of this type. The main results,
obtained within a nonlocal abstract framework, extend and complement related
properties established in the local setting.

</details>


### [18] [Qualitative Behavior of Solutions to a Forced Nonlocal Thin-Film Equation](https://arxiv.org/abs/2510.20289)
*Jinhong Zhao,Bin Guo*

Main category: math.AP

TL;DR: Analysis of a 1D nonlocal degenerate fourth-order parabolic equation for hydraulic fracture modeling, establishing global existence and long-time behavior of weak solutions under inhomogeneous forces.


<details>
  <summary>Details</summary>
Motivation: To understand hydraulic fracture modeling through degenerate parabolic equations with inhomogeneous forces, addressing both time-dependent and time-independent cases.

Method: Uses regularization scheme, modified energy/entropy methods, and novel differential inequality techniques to analyze weak solutions.

Result: For time-dependent force S(t,x), solution converges to spatial average plus time integral of force. For time-independent S(x), difference from linear function remains bounded in H^s norm.

Conclusion: Establishes rigorous mathematical framework for hydraulic fracture modeling with precise convergence and boundedness results for different force types.

Abstract: We study a one-dimensional nonlocal degenerate fourth-order parabolic
equation with inhomogeneous forces relevant to hydraulic fracture modeling.
Employing a regularization scheme, modified energy/entropy methods, and novel
differential inequality techniques, we establish global existence and long-time
behavior results for weak solutions under both time-dependent and
time-independent inhomogeneous forces. Specifically, for the time-dependent
force $S(t, x)$, we prove that the solution converges in $H^s (\Omega )$ to
$\bar{u}_0+\frac{1}{|\Omega|}\int_0^t \int_\Omega S(r, x)\, dxdr $, where
$\bar{u}_0=\frac{1}{|\Omega|}\int_{\Omega}u_{0}(x)\,dx$ is the spatial average
of the initial data. For the time-independent force $S(x)$, we prove that the
difference between the weak solution and the linear function $\bar{u}_0 +
\frac{t}{|\Omega|}\int_\Omega S(x)\, dx$ remains uniformly bounded in $H^s
(\Omega )$.

</details>


### [19] [On an Analytical Criterion for Detecting Intermittent Turbulent Behaviour of Solutions of Partial Differential Equations](https://arxiv.org/abs/2510.20290)
*Michele V Bartuccelli,Guido Gentile*

Main category: math.AP

TL;DR: The paper proposes an analytical criterion using crest factor to distinguish between time-intermittent turbulence and other behaviors in PDE solutions.


<details>
  <summary>Details</summary>
Motivation: To develop a method for understanding and classifying turbulent vs non-turbulent behavior in PDE solutions, particularly for intermittent turbulence.

Method: Using crest factor computation to analyze solutions of classical linear and nonlinear equations and distinguish between different turbulence patterns.

Result: The crest factor criterion successfully distinguishes time-intermittent turbulence from non-turbulent solutions and statistically stationary turbulence like in Kolmogorov's theory.

Conclusion: The crest factor provides an effective analytical tool for identifying and classifying turbulent behavior in PDE solutions, particularly for intermittent turbulence.

Abstract: A main question in the study of partial differential equations is the
following: how do we understand the nature of the solutions and, in particular,
how do we determine if a given solution shows turbulent or non-turbulent
behaviour? Being able to answer such a question would be a major advance in the
comprehension of the nature of turbulence. In this paper we focus on the case
of intermittent turbulence and provide an analytical criterion, based on the
crest factor, which captures the essential feature of the solutions. By
computing the crest factor for the solutions of some classical equations, both
linear and nonlinear, we illustrate the capability of the criterion for
discerning between solutions exhibiting time-intermittent turbulence behaviour
and solutions which either are not turbulent or show statistically stationary
turbulence, like, for example, in the case described by Kolmogorov's theory.

</details>


### [20] [Nonlinear stability of a composite wave to the Cauchy problem of 1-D full compressible Navier-Stokes-Allen-Cahn system](https://arxiv.org/abs/2510.20298)
*Dan Lei,Zhengzheng Chen*

Main category: math.AP

TL;DR: The paper proves global existence and large-time convergence to composite rarefaction waves for the 1D compressible Navier-Stokes-Allen-Cahn system modeling two immiscible compressible fluids, with large initial perturbations.


<details>
  <summary>Details</summary>
Motivation: To understand the large time behavior of solutions to the compressible Navier-Stokes-Allen-Cahn system, which models mixtures of two immiscible viscous compressible fluids, particularly when the corresponding Euler system admits composite rarefaction wave solutions.

Method: Used an elementary energy method that accounts for the phase field variable and nonlinear wave complexity, proving global strong solution existence and convergence to composite rarefaction waves.

Result: Proved that when the adiabatic exponent γ is close to 1, a global strong solution exists uniquely and converges to a composite wave (1-rarefaction + 3-rarefaction) as time → ∞, even with arbitrarily large initial perturbations (except temperature) and rarefaction wave strength.

Conclusion: The compressible Navier-Stokes-Allen-Cahn system exhibits stable large-time behavior converging to composite rarefaction waves under appropriate conditions, demonstrating the system's asymptotic stability despite complex phase field dynamics.

Abstract: The compressible Navier-Stokes-Allen-Cahn system models the motion of a
mixture of two macroscopically immiscible viscous compressible fluids. In this
paper, we are concerned with the large time behavior of solutions to the Cauchy
problem of the one-dimensional full compressible Navier-Stokes-Allen-Cahn
system. If the Riemann problem of the corresponding Euler system admits a
solution which is a linear combination of 1-rarefaction wave and 3-rarefaction
wave, we proved that a global strong solution to the compressible
Navier-Stokes-Allen-Cahn system exists uniquely and converges to the above
composite wave as time goes to infinity, provided that the adiabatic exponent
$\gamma$ is closed to $1$. Here the initial perturbations except for the
temperature function of the fluid, and the strength of rarefaction waves can be
arbitrarily large. The proof is given by an elementary energy method that takes
into account the effect of the phase field variable $\chi(t,x)$ and the
complexity of nonlinear waves.

</details>


### [21] [Continuous data assimilation applied to the Rayleigh-Benard problem for compressible fluid flows](https://arxiv.org/abs/2510.20316)
*Eduard Feireisl,Wladimir Neves*

Main category: math.AP

TL;DR: Continuous data assimilation applied to Navier-Stokes-Fourier system for compressible rotating thermally driven fluids, with rigorous proof of tracking property under low Mach and high Rossby/Froude numbers.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze data assimilation methods for complex fluid systems, specifically compressible rotating thermally driven fluids described by the Navier-Stokes-Fourier equations.

Method: Applied continuous data assimilation method to the Navier-Stokes-Fourier system, with rigorous mathematical proof conducted in asymptotic regime of low Mach numbers and high Rossby/Froude numbers.

Result: Successfully proved the tracking property for large data within the framework of weak solutions under specified asymptotic conditions.

Conclusion: Continuous data assimilation is effective for tracking solutions in compressible rotating thermally driven fluid systems under low Mach and high Rossby/Froude number regimes, even for large data weak solutions.

Abstract: We apply a continuous data assimilation method to the Navier-Stokes-Fourier
system governing the evolution of a compressible, rotating and thermally driven
fluid. A rigorous proof of the tracking property is given in the asymptotic
regime of low Mach and high Rossby and Froude numbers. Large data in the
framework of weak solutions are considered.

</details>


### [22] [Energy Decay in Measure Time: HUM Observability, Product-Exponential Envelopes, and GCC Calibration](https://arxiv.org/abs/2510.20371)
*Ben F. Tibola*

Main category: math.AP

TL;DR: The paper introduces a measure-valued clock (sigma) to unify continuous damping and impulsive exposure patterns, proving that energy decays exponentially with respect to sigma rather than wall-clock time t.


<details>
  <summary>Details</summary>
Motivation: Past attempts to unify continuous damping with impulses using wall-clock time t failed because there's no uniform exponential energy law in t for impulsive exposure patterns.

Method: Replace wall-clock time t with a measure-valued clock sigma that aggregates continuous exposure and atomic doses within a single Lyapunov ledger, and apply the Hilbert Uniqueness Method (HUM) framework.

Result: Proved an observability-dissipation principle: energy decays at least at a product-exponential rate with respect to sigma, with a structural constant c_sigma > 0. When sigma = t, this reduces to classical exponential stabilization.

Conclusion: The framework provides a qualitative dynamics backbone where observability implies sigma-exponential decay with sharp constants, unifying intermittent regimes with quiescent intervals punctuated by impulses.

Abstract: We prove that for impulsive exposure patterns there is no uniform exponential
energy law in wall-clock time t, which explains why past t-based unifications
of continuous damping with impulses fail. We therefore replace t by a
measure-valued clock, sigma, that aggregates absolutely continuous exposure and
atomic doses within a single Lyapunov ledger. On this ledger we prove an
observability-dissipation principle in the sense of the Hilbert Uniqueness
Method (HUM): there exists a structural constant c_sigma > 0 such that the
energy decays at least at a product-exponential rate with respect to sigma.
When sigma = t, the statement reduces to classical exponential stabilization
with the same constant. For the damped wave under the Geometric Control
Condition (GCC), the constant is calibrated by the usual observability and
geometric factors. The framework yields a monotonicity principle ("more
sigma-mass implies faster decay") and unifies intermittent regimes where
quiescent intervals are punctuated by impulses. As robustness, secondary to the
main contribution, the same decay law persists under structure-compatible
discretizations and along compact variational limits; a stochastic extension
supplies expectation and pathwise envelopes via the compensator. The
contribution is a qualitative dynamics backbone: observability implies
sigma-exponential decay with sharp constants.

</details>


### [23] [Optimal quantitative stability estimates for Alexandrov's Soap Bubble Theorem via Gagliardo-Nirenberg-type interpolation inequalities](https://arxiv.org/abs/2510.20399)
*João Gonçalves da Silva,Giorgio Poggesi*

Main category: math.AP

TL;DR: Optimal quantitative stability estimates for Alexandrov's Soap Bubble Theorem using Gagliardo-Nirenberg interpolation inequalities, establishing uniform closeness to balls for L^r deviations of mean curvature from constant.


<details>
  <summary>Details</summary>
Motivation: To provide sharp stability estimates for the Alexandrov Soap Bubble Theorem across different function spaces and domain regularities, addressing gaps in existing literature.

Method: Leveraging Gagliardo-Nirenberg-type interpolation inequalities to establish optimal stability estimates for C^{k,α} domains with k≥1 and 0<α≤1.

Result: New stability profiles for r≥(N-1)/2, with linear profiles for r>(N-1)/2 and non-linear profiles that improve with increasing k, becoming formally linear as k→∞. All estimates are proven optimal.

Conclusion: The paper provides complete and optimal quantitative stability estimates for Alexandrov's theorem across various parameter ranges, with novel results particularly for r≥(N-1)/2 cases.

Abstract: The paper provides optimal quantitative stability estimates for the
celebrated Alexandrov's Soap Bubble Theorem within the class of $C^{k,\alpha}$
domains, for any $k \ge 1$ and $0 < \alpha \le 1$, by leveraging
Gagliardo-Nirenberg-type interpolation inequalities.
  Optimal estimates of uniform closeness to a ball are established for $L^r$
deviations of the mean curvature from being constant, for any $r\ge 2$ (more
generally, for any $r>1$ such that $r\ge (2N-2)/(N+1)$).
  For $r>\frac{N-1}{2}$, the stability profile is linear, thus returning the
existing results established in the literature through computations for nearly
spherical sets. Surprisingly, all the stability estimates for $r\ge
\frac{N-1}{2}$, for which the profile is not linear, are new; even in the
particular case $r=2$ (which has been extensively studied, since it is a case
of interest for several critical applications), the sharp stability profile
that we obtain is new. Interestingly, we also prove that the (non-linear)
profile for $r \ge \frac{N-1}{2}$ improves as $k$ becomes larger to such an
extent that it becomes formally linear as $k$ goes to $\infty$.
  Finally, for any $r$, we show that all our estimates are optimal for any $k
\ge 1$ and $0< \alpha \le 1$, by providing explicit examples.

</details>


### [24] [Hölder regularity for a class of doubly non linear PDEs](https://arxiv.org/abs/2510.20432)
*Filippo Maria Cassanello,Eurica Henriques*

Main category: math.AP

TL;DR: Local Hölder continuity is proven for non-negative, locally bounded weak solutions to doubly nonlinear parabolic equations of the form ∂_t(u^q) - div(|Du|^{p-2}Du) = 0, with p > 2 and 0 < q < p-1.


<details>
  <summary>Details</summary>
Motivation: The motivation is to establish regularity properties for solutions to doubly nonlinear parabolic equations, which combine nonlinear diffusion with nonlinear time evolution, extending classical regularity theory to this more complex setting.

Method: The proof uses expansion of positivity results, an alternative approach related to DeGiorgi-type lemmas, and an exponential shift technique to handle the intrinsic geometry of the problem.

Result: The main result is the proof of local Hölder continuity for non-negative, locally bounded weak solutions to the specified class of doubly nonlinear parabolic equations.

Conclusion: The paper successfully establishes local Hölder continuity for solutions to doubly nonlinear parabolic equations through a combination of expansion of positivity, DeGiorgi-type alternatives, and geometric adaptations using exponential shifts.

Abstract: We prove local H\"older continuity for non negative, locally bounded, local
weak solutions to the class of doubly nonlinear parabolic equations $\partial_t
(u_q) - \text{div} (|Du|^{p-2} Du) = 0$ for $p > 2$, $ 0 < q < p-1$. The proof
relies on expansion of positivity results combined with the study of an
alternative (related to DeGiorgi-type lemmas) and an exponential shift which
allows us to deal with the intrinsic geometry associated to the problem.

</details>


### [25] [Global bifurcation of solutions to elliptic systems with system and domain symmetries](https://arxiv.org/abs/2510.20462)
*Piotr Stefaniak*

Main category: math.AP

TL;DR: Study of parameterized elliptic systems on symmetric domains with system symmetries, proving existence of continua of nontrivial solutions bifurcating from constant branches via equivariant gradient maps.


<details>
  <summary>Details</summary>
Motivation: To establish existence of nontrivial solutions in parameterized elliptic systems on symmetric domains with additional system symmetries, without requiring nondegeneracy assumptions.

Method: Using degree theory for equivariant gradient maps to prove bifurcation of continua of solutions from constant branches determined by critical points of the potential.

Result: Existence of continua of nontrivial solutions bifurcating from constant branches. On compact symmetric spaces, solutions break symmetry at every nonzero level. Under additional assumptions, continua are unbounded.

Conclusion: The method provides existence results for nontrivial solutions in symmetric elliptic systems without nondegeneracy conditions, with symmetry breaking properties on compact symmetric domains.

Abstract: We study parameterized elliptic systems on symmetric domains with additional
system symmetries. We prove the existence of continua of nontrivial solutions
bifurcating from the constant branch determined by a critical point of the
potential, without assuming nondegeneracy, via the degree for equivariant
gradient maps. Our assumptions are formulated in terms of the right-hand side.
When the domain is a compact symmetric space, the bifurcating solutions break
symmetry at every nonzero level. Under additional assumptions on the right-hand
side, the continua are unbounded.

</details>


### [26] [On dissipative turbulent solutions to the compressible anisotropic Navier-Stokes equations in unbounded domains](https://arxiv.org/abs/2510.20476)
*Ondřej Kreml,Šárka Nečasová,Tong Tang*

Main category: math.AP

TL;DR: Global existence of dissipative turbulent solutions for compressible Navier-Stokes equations with anisotropic viscous stress tensor on unbounded domains, complementing previous work on bounded domains.


<details>
  <summary>Details</summary>
Motivation: To extend existence results for compressible Navier-Stokes equations with anisotropic viscous stress tensor from bounded domains to more realistic unbounded domains that better represent geophysical contexts.

Method: Using the concept of dissipative turbulent solutions to relax assumptions on anisotropic tensor coefficients and pressure law coefficient, and proving existence on a large class of unbounded domains.

Result: Established global existence of dissipative turbulent solutions on unbounded domains and proved weak-strong uniqueness property for these solutions.

Conclusion: The work successfully extends the existence theory to more physically relevant unbounded domains while relaxing technical assumptions, making the results more applicable to geophysical problems.

Abstract: Inspired by Abbatiello, Feireisl and Novotn\'y, we prove the global existence
of dissipative turbulent solution for the compressible Navier-Stokes equations
with anisotropic viscous stress tensor on unbounded domain. Our work
complements the result of Bresch and Jabin, where the authors used the new
compactness method to prove the existence of a weak solution to the same system
in $\mathbb{T}^3$. By virtue of the concept of dissipative turbulent solutions,
we are able to relax assumptions on the anisotropic tensor coefficients and the
pressure law coefficient. We point out that we establish the existence result
on a large class of unbounded domains, which is more conform to geophysical
context. We also prove the weak-strong uniqueness property of acquired
dissipative turbulent solutions.

</details>


### [27] [Non-optimal domains for the helicity maximisation problem](https://arxiv.org/abs/2510.20533)
*Wadim Gerner*

Main category: math.AP

TL;DR: This paper extends previous work on the helicity isoperimetric problem by establishing new geometric constraints that optimal domains must satisfy, ruling out optimality for many solid tori.


<details>
  <summary>Details</summary>
Motivation: To further understand the helicity isoperimetric problem and identify geometric properties that optimal domains must possess, building on previous work that showed optimal domains must have toroidal boundaries.

Method: The authors establish additional geometric constraints that any optimal domain must satisfy, using mathematical analysis of the helicity maximization problem.

Result: The paper rules out the optimality of a broad class of solid tori by showing they cannot satisfy the newly established geometric constraints.

Conclusion: While significant progress is made in understanding the geometric requirements for optimal domains, the actual existence of such optimal domains remains an open problem in the field.

Abstract: In [J. Cantarella, D. DeTurck, H. Gluck and M. Teytel, J. Math. Phys. 41:5615
(2000)] the helicity isoperimetric problem which asks to find a smooth domain
of fixed volume which maximises Biot-Savart helicity among all other smooth
domains of fixed volume was initiated. It was shown that if an optimal domain
exists, all of its boundary components must be tori.
  The present work extends these results by establishing additional geometric
constraints which optimal domains, if they exist, must satisfy. This allows to
rule out the optimality of a broad class of solid tori. The existence of
optimal domains remains an open problem.

</details>


### [28] [Homogenization, dimension reduction and linearization of thin elastic plate](https://arxiv.org/abs/2510.20573)
*Amartya Chakrabortty,Georges Griso,Julia Orlik*

Main category: math.AP

TL;DR: Analysis of composite plate homogenization, dimension reduction, and linearization in non-linear elasticity with energy scaling ~h²ε^(2a+3). Two-part study: simultaneous limits (ε,h)→(0,0) without coupling assumption, and rigorous derivation of linearized elasticity from non-linear elasticity with small deformations.


<details>
  <summary>Details</summary>
Motivation: To understand the interplay between homogenization, dimension reduction, and linearization in composite plates under external loading, particularly investigating whether the order of taking limits affects the final energy expression.

Method: Two-part approach: 1) Simultaneous homogenization, dimension reduction and linearization (ε,h)→(0,0) without coupling assumption; 2) Rigorous derivation of linearized elasticity from non-linear elasticity with small deformation conditions. Uses Γ-convergence technique and decomposition of plate deformations and displacements.

Result: Limit energy remains unchanged regardless of order: (h→0 first, then ε→0) vs simultaneous (ε,h)→(0,0). Exact form of limit energy obtained. Existence of unique solution for limit linearized homogenized energy problem demonstrated via Γ-convergence. Results extended to periodic perforated plates.

Conclusion: The order of taking limits (homogenization, dimension reduction, linearization) does not affect the final energy expression in composite plate problems. The Γ-convergence approach successfully establishes existence and uniqueness for the limit problem, with applicability to perforated plates.

Abstract: This paper investigates the homogenization, dimension reduction, and
linearization of a composite plate subjected to external loading within the
framework of non-linear elasticity problem. The total elastic energy of the
problem is of order $\sim h^2\varepsilon^{2a+3}$, where $a\geq1$. The paper is
divided into two parts: The first part presents the simultaneous
homogenization, dimension reduction and linearization
($(\varepsilon,h)\to(0,0)$) of a composite plate without any coupling
assumption of $\varepsilon$ and $h$. The second part consists of the rigorous
derivation of linearized elasticity as a limit of non-linear elasticity with
small deformation and external loading conditions. The results obtained
demonstrate that the limit energy remains unchanged when the first
linearization ($h\to 0$) is performed, followed by simultaneous homogenization
dimension reduction ($\varepsilon\to0$) and when both limits approach zero
simultaneously, i.e. $(\varepsilon,h)\to (0,0)$. The exact form of the limit
energy(s) is obtained through the decomposition of plate deformations and plate
displacements. By using the $\Gamma$-convergence technique, the existence of a
unique solution for the limit linearized homogenized energy problem is
demonstrated. These results are then extended to certain periodic perforated
plates.

</details>


### [29] [Weak sequential stability of solutions to a nonisothermal kinetic model for incompressible dilute polymeric fluids](https://arxiv.org/abs/2510.20580)
*Miroslav Bulíček,Josef Málek,Endre Süli*

Main category: math.AP

TL;DR: Mathematical analysis of thermodynamically consistent kinetic models for nonisothermal flows of dilute polymeric fluids, showing convergence to global-in-time large-data weak solutions with energy inequality and temperature stability.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze kinetic models for nonisothermal polymeric fluid flows that are thermodynamically consistent, accounting for energy storage and entropy production mechanisms.

Method: Analysis of nonlinear PDE system coupling temperature-dependent Navier-Stokes equations with temperature-dependent Fokker-Planck equation and temperature evolution equation. Construction of sequences of smooth solutions with uniform bounds.

Result: Sequences of smooth solutions converge to global-in-time large-data weak solutions satisfying energy inequality. Absolute temperature satisfies renormalized variational inequality, ensuring weak sequential stability.

Conclusion: The mathematical model exhibits strong stability properties with convergence to weak solutions that maintain thermodynamic consistency through energy inequalities and temperature stability conditions.

Abstract: The paper is concerned with the mathematical analysis of a class of
thermodynamically consistent kinetic models for nonisothermal flows of dilute
polymeric fluids, based on the identification of energy storage mechanisms and
entropy production mechanisms in the fluid under consideration. The model
involves a system of nonlinear partial differential equations coupling the
unsteady incompressible temperature-dependent Navier--Stokes equations to a
temperature-dependent generalization of the classical Fokker--Planck equation
and an evolution equation for the absolute temperature. Sequences of smooth
solutions to the initial-boundary-value problem, satisfying the available
bounds that are uniform with respect to the given data of the model, are shown
to converge to a global-in-time large-data weak solution that satisfies an
energy inequality, where the absolute temperature satisfies a renormalized
variational inequality, implying weak sequential stability of the mathematical
model.

</details>


### [30] [Uniqueness and continuous dependence for a viscoelastic problem with memory in domains with time dependent cracks](https://arxiv.org/abs/2510.20583)
*Federico Cianci,Gianni Dal Maso*

Main category: math.AP

TL;DR: Analysis of hyperbolic partial integro-differential systems in domains with time-dependent cracks, focusing on uniqueness and continuous dependence on cracks.


<details>
  <summary>Details</summary>
Motivation: To understand how time-dependent cracks affect the behavior of hyperbolic partial integro-differential systems and establish mathematical foundations for solution uniqueness and stability.

Method: Mathematical analysis of hyperbolic partial integro-differential systems in domains with time-dependent cracks, developing conditions on crack behavior.

Result: Established conditions on cracks that guarantee solution uniqueness with prescribed initial-boundary conditions and continuous dependence on crack variations.

Conclusion: The study provides rigorous mathematical conditions ensuring well-posedness of hyperbolic partial integro-differential systems in cracked domains with time-dependent boundaries.

Abstract: We study some hyperbolic partial integro-differential systems in domains with
time dependent cracks. In particular, we give conditions on the cracks which
imply the uniqueness of the solution with prescribed initial-boundary
conditions, and its continuous dependence on the cracks.

</details>


### [31] [Dynamic crack growth in viscoelastic materials with memory](https://arxiv.org/abs/2510.20599)
*Federico Cianci*

Main category: math.AP

TL;DR: A dynamic crack growth model for viscoelastic materials with history-dependent damping, based on energy dissipation balance and maximal dissipation conditions.


<details>
  <summary>Details</summary>
Motivation: To develop a mathematical model for crack propagation in viscoelastic materials where damping depends on deformation history, addressing dynamic fracture mechanics.

Method: Uses dynamic energy dissipation balance and maximal dissipation conditions to formulate the crack growth model.

Result: Proves an existence theorem for the model in two dimensions under certain a priori regularity constraints on cracks.

Conclusion: The paper establishes mathematical foundations for modeling dynamic crack growth in viscoelastic materials with history-dependent damping properties.

Abstract: In this paper we introduce a model of dynamic crack growth in viscoelastic
material, where the damping term depends on the history of the deformation. The
model is based on a dynamic energy dissipation balance and on a maximal
dissipation condition. Our main result is an existence theorem in dimension two
under some a priori regularity constraints on the cracks.

</details>


### [32] [Analysis of coupled Maxwell-cable problems](https://arxiv.org/abs/2510.20619)
*Timo Reis,Nathanael Skrepek*

Main category: math.AP

TL;DR: Analysis of qualitative properties of a dynamical system modeling electromagnetic field interactions with radiating curved cables, showing autonomous dynamics generate a strongly continuous semigroup and establishing well-posedness conditions.


<details>
  <summary>Details</summary>
Motivation: To analyze the qualitative properties of a recently developed model for electromagnetic field interactions with radiating curved cables, building on previous work that coupled telegrapher's and Maxwell's equations.

Method: Analysis of the dynamical system derived from coupled telegrapher's and Maxwell's equations, with inputs/outputs as currents/voltages at cable ends and state comprising distributions along cables and electromagnetic fields in surrounding domain.

Result: The autonomous dynamics (with zero input) generate a strongly continuous semigroup, and sufficient conditions for well-posedness are established, ensuring continuous dependence of state and output trajectories on inputs and initial conditions.

Conclusion: The model exhibits mathematically sound behavior with well-defined autonomous dynamics and continuous dependence properties, providing a rigorous foundation for analyzing electromagnetic interactions with radiating curved cables.

Abstract: Building on the recently published work "Modeling of radiating curved cables
via coupled telegrapher's and Maxwell's equations", which introduces a model
for the interaction between electromagnetic fields and radiating (possibly
curved) cables, we analyze the qualitative properties of the resulting
dynamical system. The model features inputs and outputs given by the currents
and voltages at the cable ends, while the state comprises the corresponding
distributions along the cables and the electromagnetic fields in the
surrounding domain. We show that the autonomous dynamics (i.e., with zero
input) generate a strongly continuous semigroup and establish sufficient
conditions for well-posedness, meaning continuous dependence of the state and
output trajectories on the inputs and initial conditions.

</details>


### [33] [Dimension reduction for time-dependent von Kármán rods](https://arxiv.org/abs/2510.20623)
*Federico Cianci,Bernd Schmidt*

Main category: math.AP

TL;DR: Convergence of 3D nonlinear elastodynamics solutions for thin rods to 1D von Kármán equations as cross-section shrinks, with energy dissipation from torsional vibrations.


<details>
  <summary>Details</summary>
Motivation: To understand how solutions in three-dimensional nonlinear elastodynamics for thin rods converge to dimensionally reduced models when the cross-section shrinks to zero, particularly for displacements comparable to the rod's small radius.

Method: Assuming existence of solutions and proper control of torsional velocity, analyze convergence of 3D elastodynamics solutions to effective 1D model as cross-section shrinks. Consider effects of high-frequency torsional vibrations.

Result: Solutions converge to a version of time-dependent von Kármán equations for one-dimensional rods. High-frequency torsional vibrations cause energy dissipation in the limit, leading to additional contributions in the limiting equations.

Conclusion: The study establishes rigorous convergence from 3D nonlinear elastodynamics to dimensionally reduced 1D von Kármán equations for thin rods, with identified energy dissipation mechanisms from torsional vibrations.

Abstract: This paper aims to study the convergence of solutions in three-dimensional
nonlinear elastodynamics for a thin rod as its cross section shrinks to zero
for displacements that are comparable to the small radius of the rod. Assuming
the existence of solutions and proper control of the torsional velocity, we
show how these converge to the solutions of an effective dimensionally reduced
model which is a version of the the time dependent von K\'arm\'an equations for
a one-dimensional rod. In the presence of high-frequency torsional vibrations,
energy can dissipate in the limit and we obtain additional contributions in the
limiting equations.

</details>


### [34] [Rothe's method in direct and time-dependent inverse source problems for a semilinear pseudo-parabolic equation](https://arxiv.org/abs/2510.20642)
*Karel Van Bockstal,Khonatbek Khompysh,Arshyn Altybay*

Main category: math.AP

TL;DR: This paper studies the inverse problem of recovering an unknown time-dependent source term in a semilinear pseudo-parabolic equation with variable coefficients, using additional measurement data from weighted spatial averages of the solution.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve inverse problems involving time-dependent source terms in complex pseudo-parabolic equations with variable coefficients, which have practical applications in various fields requiring parameter identification from limited measurement data.

Method: The method employs Rothe's time-discretisation approach to prove existence and uniqueness of weak solutions under smallness conditions on problem data, and develops a numerical scheme for computational implementation.

Result: The paper establishes the existence and uniqueness of weak solutions for the inverse source problem under appropriate conditions, and provides a numerical framework for solving such problems.

Conclusion: The study successfully addresses the inverse source identification problem for semilinear pseudo-parabolic equations, providing both theoretical guarantees and practical computational methods for recovering unknown time-dependent source terms.

Abstract: In this paper, we investigate the inverse problem of determining an unknown
time-dependent source term in a semilinear pseudo-parabolic equation with
variable coefficients and a Dirichlet boundary condition. The unknown source
term is recovered from additional measurement data expressed as a weighted
spatial average of the solution. By employing Rothe's time-discretisation
method, we prove the existence and uniqueness of a weak solution under a
smallness condition on the problem data. We also present a numerical scheme for
computations.

</details>


### [35] [Nonrelativistic limit of bound-state solutions for nonlinear Dirac equation on noncompact quantum graphs](https://arxiv.org/abs/2510.20658)
*Guangze Gu,Michael Ruzhansky,Guoyan Wei,Zhipeng Yang*

Main category: math.AP

TL;DR: Study of bound-state solutions for nonlinear Dirac equation on noncompact quantum graphs, establishing existence, convergence to nonlinear Schrödinger equation in nonrelativistic limit, and proving uniform boundedness and exponential decay.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of nonlinear Dirac equation solutions on quantum graphs and their connection to nonlinear Schrödinger equation in the nonrelativistic regime.

Method: Mathematical analysis of the nonlinear Dirac equation on noncompact quantum graphs, studying the nonrelativistic limit as c→∞, and proving qualitative properties of solutions.

Result: Existence of bound-state solutions proven; solutions converge to nonlinear Schrödinger equation solutions in nonrelativistic limit; solutions are uniformly bounded and exhibit exponential decay.

Conclusion: The nonlinear Dirac equation on quantum graphs has well-behaved bound-state solutions that connect smoothly to the nonrelativistic regime, with important qualitative properties preserved uniformly in the speed of light parameter.

Abstract: In this paper, we investigate the nonrelativistic limit and qualitative
properties of bound-state solutions for the nonlinear Dirac equation (NLDE)
defined on noncompact quantum graphs: \[ -i c \frac{d}{d x} \sigma_1 \psi+m c^2
\sigma_3 \psi-\omega \psi=g(|\psi|) \psi, \quad \text { in } \mathcal{G} \]
where \( g : \mathbb{R}\rightarrow\mathbb{R} \) is a continuous nonlinear
function, \( c>0 \) represents the speed of light, \( m>0 \) is the particle's
mass, \( \omega\in\mathbb{R} \) is related to the frequency, \( \sigma_1 \) and
\( \sigma_3 \) denote the Pauli matrices, and \(\mathcal{G}\) is a noncompact
quantum graph. We establish the existence of bound-state solutions to the NLDE
on \(\mathcal{G}\), and prove that these solutions converge toward the
corresponding bound-state solutions of a nonlinear Schr\"odinger equation (NLS)
in the nonrelativistic limit (i.e., as the speed of light \( c \to \infty \))
for particles of small mass. Furthermore, we prove uniform boundedness and
exponential decay properties of the NLDE solutions, uniformly in \( c \),
thereby offering insight into their asymptotic behavior.

</details>


### [36] [The Cauchy problem for $p$-evolution equations with variable coefficients in Gelfand-Shilov spaces](https://arxiv.org/abs/2510.20702)
*Marco Cappiello,Eliakim Cleyton Machado*

Main category: math.AP

TL;DR: Well-posedness in Gelfand-Shilov spaces for linear evolution equations with time-space dependent coefficients under decay conditions.


<details>
  <summary>Details</summary>
Motivation: To establish well-posedness for linear evolution equations with coefficients depending on both time and space variables, particularly when lower-order terms decay at infinity.

Method: Studied Cauchy problem for arbitrary order linear evolution equations with time-space dependent coefficients, imposing decay assumptions on lower-order coefficients for large |x|.

Result: Proved well-posedness result in Gelfand-Shilov spaces under suitable decay conditions on coefficients.

Conclusion: The paper successfully establishes well-posedness for this class of evolution equations in Gelfand-Shilov spaces through decay assumptions on coefficients.

Abstract: We study the Cauchy problem for a class of linear evolution equations of
arbitrary order with coefficients depending both on time and space variables.
Under suitable decay assumptions on the coefficients of the lower order terms
for $|x|$ large, we prove a well-posedness result in Gelfand-Shilov spaces.

</details>


### [37] [Large field problem in coercive singular PDEs](https://arxiv.org/abs/2510.20716)
*Ilya Chevyrev,Massimiliano Gubinelli*

Main category: math.AP

TL;DR: The paper derives a priori estimates for singular differential equations with irregular distributions, using rough path theory for time derivatives and regularity structures for heat operators.


<details>
  <summary>Details</summary>
Motivation: To establish local and boundary-independent estimates for subcritical singular differential equations with irregular forcing terms, reducing the problem to small noise cases.

Method: Uses rough path theory for time derivative equations and regularity structures for heat operator equations, with an abstract scaling method to convert local to global coercivity.

Result: Developed local space-time estimates independent of boundary conditions, with an abstract result enabling reduction to small noise cases through scaling.

Conclusion: The approach successfully provides a priori estimates for singular differential equations by leveraging scaling properties to handle irregular distributions in subcritical regimes.

Abstract: We derive a priori estimates for singular differential equations of the form
\[ \mathcal{L} \phi = P(\phi,\nabla\phi) + f(\phi,\nabla\phi)\xi \] where $P$
is a polynomial, $f$ is a sufficiently well-behaved function, and $\xi$ is an
irregular distribution such that the equation is subcritical. The differential
operator $\mathcal L$ is either a derivative in time, in which case we
interpret the equation using rough path theory, or a heat operator, in which
case we interpret the equation using regularity structures. Our only assumption
on $P$ is that solutions with $\xi=0$ exhibit coercivity. Our estimates are
local in space and time, and independent of boundary conditions.
  One of our main results is an abstract estimate that allows one to pass from
a local coercivity property to a global one using scaling, for a large class of
equations. This allows us to reduce the problem of deriving a priori estimates
to the case when $\xi$ is small.

</details>


### [38] [First Critical Field in the pinned three-dimensional Ginzburg--Landau Model: A matching upper bound](https://arxiv.org/abs/2510.20720)
*Carlos Román*

Main category: math.AP

TL;DR: The paper establishes a matching upper bound for the first critical field H_c1 in extreme type-II superconductors, confirming the sharpness of previous lower bounds and connecting vorticity onset to a weighted isoflux problem.


<details>
  <summary>Details</summary>
Motivation: To complete the characterization of H_c1 by providing an upper bound that matches the previously established lower bound, thereby determining its leading-order behavior in extreme type-II superconductors.

Method: Uses the three-dimensional magnetic Ginzburg-Landau functional with a pinning term, building on previous lower bound results and employing an upper bound construction based on the Biot-Savart law.

Result: Successfully establishes a matching upper bound for H_c1, confirming the sharpness of the previously derived lower bound and identifying the leading-order behavior of the first critical field.

Conclusion: The work completes the characterization of H_c1 by providing both lower and upper bounds, elucidating the connection between vorticity onset and a weighted isoflux problem in extreme type-II superconductors.

Abstract: We continue our study of the first critical field $H_{c_1}$ for extreme
type-II superconductors governed by the three-dimensional magnetic
Ginzburg--Landau functional with a pinning term $a_\varepsilon$, as introduced
in our previous work [arXiv:2507.10915]. Building upon the lower bound for
$H_{c_1}$ and the characterization of the Meissner solution, we now establish a
matching upper bound for $H_{c_1}$, thereby identifying its leading-order
behavior. This result confirms the sharpness of the previously derived lower
bound and further elucidates the connection between the onset of vorticity and
a weighted variant of the \emph{isoflux problem}. Our argument is prompted by
the upper bound construction we developed in [arXiv:2510.14910], based on the
Biot--Savart law.

</details>


### [39] [Quantitative classification of potential Navier-Stokes singularities beyond the blow-up time](https://arxiv.org/abs/2510.20757)
*Tobias Barker*

Main category: math.AP

TL;DR: This paper provides the first quantitative classification of potentially singular solutions to the 3D Navier-Stokes equations for approximately axisymmetric initial data, with bounds that can be numerically tested near potential blow-up times.


<details>
  <summary>Details</summary>
Motivation: Motivated by investigating the viability of numerical candidates for singular solutions of the 3D Navier-Stokes equations, particularly Hou's numerical candidate.

Method: Establishes improved quantitative regions of regularity for approximately axisymmetric initial data, uses quantitative energy estimates, implements physical space analogue of Tao's strategy, and applies recursive quantitative Carleman inequality arguments with careful bookkeeping.

Result: Achieves quantitative classification of potentially singular solutions at any given time in the region of potential blow-up times, with bounds amenable to numerical testing.

Conclusion: The approach successfully provides quantitative lower bounds on solutions near potential blow-up times through careful implementation of Carleman inequalities and regularity analysis, enabling numerical verification of potential singularities.

Abstract: In \cite{hou}, Hou gave a compelling numerical candidate for a singular
solution of the 3D Navier-Stokes equations. We pioneer classifications of
potentially singular solutions, motivated by the issue of investigating the
viability of numerical candidates.For approximately axisymmetric initial data,
we give the first quantitative classification of potentially singular solutions
at \textit{any} given time in the region of potential blow-up times. Moreover,
the quantitative bounds in the vicinity of any potential blow-up time are in
principle amenable to numerical testing. To achieve this, we establish improved
quantitative regions of regularity for approximately axisymmetric initial data,
which may be of independent interest. Together with improved quantitative
energy estimates from \cite{TB24}, this allows us to get a quantitative lower
bound in the vicinity of a blow-up time by implementing the strategy of
\cite{BP21}, which is a physical space analogue of Tao's strategy \cite{Ta21}
for producing quantitative estimates for critically bounded solutions. To
obtain a quantitative lower bound on the solution at any time in the region of
potential blow-up times, we recursively apply quantitative Carleman inequality
arguments from \cite{Ta21}. This necessitates careful bookkeeping to avoid
exponential losses and to ensure that all forward-in-time iterations of
(localized) vorticity concentration remain within the region of quantitative
regularity of the solution.

</details>


### [40] [Ill-Posedness of the 2D Euler Equations in a Logarithmically Refined Critical Sobolev Space](https://arxiv.org/abs/2510.20773)
*Elaine Cozzi,Nicholas Harrison,Zachary Radke*

Main category: math.AP

TL;DR: The paper extends Bourgain and Li's work by proving strong ill-posedness of 2D Euler equations in logarithmically regularized spaces that are strictly contained in H² but contain H^s for all s>2.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of ill-posedness beyond the critical Sobolev space H² by considering logarithmically regularized spaces that provide finer gradations between H² and higher regularity spaces.

Method: Construct logarithmically regularized spaces using fractional logarithmic derivatives applied to the critical Sobolev norm, then analyze ill-posedness when the power α of the logarithmic derivative satisfies α≤1/2.

Result: The 2D Euler equations are strongly ill-posed in these logarithmically regularized spaces when α≤1/2, extending the known ill-posedness results to spaces strictly contained in H².

Conclusion: The ill-posedness phenomenon for 2D Euler equations persists in logarithmically regularized spaces that are more regular than H² but still below the threshold where well-posedness is expected.

Abstract: In their seminal work, Bourgain and Li establish strong ill-posedness of the
2D Euler equations for initial velocity in the critical Sobolev space
$H^2(\mathbb{R}^2)$. In this work, we extend those results by demonstrating
strong ill-posedness in logarithmically regularized spaces which are strictly
contained in $H^2(\mathbb{R}^2)$ and which contain $H^s(\mathbb{R}^2)$ for all
$s>2$. These spaces are constructed via application of a fractional logarithmic
derivative to the critical Sobolev norm. We show that if the power $\alpha$ of
the logarithmic derivative satisfies $\alpha\leq 1/2$, then the 2D Euler
equations are strongly ill-posed.

</details>


### [41] [A Weakly Nonlinear Theory for Pattern Formation in Structured Models with Localized Solutions](https://arxiv.org/abs/2510.20781)
*Wesley J. M. Ridgway,Mohit P. Dalwadi,Philip Pearce,S. Jonathan Chapman*

Main category: math.AP

TL;DR: A weakly nonlinear framework for structured PDE models with exponentially localized base states, using WKBJ asymptotics and Stokes phenomenon analysis to handle singular steady states.


<details>
  <summary>Details</summary>
Motivation: To extend classical pattern formation analysis tools to structured PDE models where traditional methods fail due to sharply peaked or singular steady states.

Method: Uses WKBJ asymptotics and Stokes phenomenon analysis to systematically resolve solution structure in the limit where steady states approach Dirac-delta functions, demonstrated on a chemically structured model of motile bacteria.

Result: Derived an amplitude equation governing solution dynamics near linear instability, predicting a pitchfork bifurcation with an effective parameter grouping determining subcritical/supercritical nature.

Conclusion: The framework successfully extends weakly nonlinear analysis to structured PDE models with singular steady states and is broadly applicable beyond the specific bacterial model example.

Abstract: Structured models, such as PDEs structured by age or phenotype, provide a
setting to study pattern formation in heterogeneous populations. Classical
tools to quantify the emergence of patterns, such as linear and weakly
nonlinear analyses, pose significant mathematical challenges for these models
due to sharply peaked or singular steady states. Here, we present a weakly
nonlinear framework that extends classical tools to structured PDE models in
settings where the base state is spatially uniform, but exponentially localized
in the structured variable. Our approach utilizes WKBJ asymptotics and an
analysis of the Stokes phenomenon to systematically resolve the solution
structure in the limit where the steady state tends to a Dirac-delta function.
To demonstrate our method, we consider a chemically structured (nonlocal) model
of motile bacteria that interact through quorum sensing. For this example, our
analysis yields an amplitude equation that governs the solution dynamics near a
linear instability, and predicts a pitchfork bifurcation. From the amplitude
equation, we deduce an effective parameter grouping whose sign determines
whether the pitchfork bifurcation is subcritical or supercritical. Although we
demonstrate our framework for a specific example, our techniques are broadly
applicable.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [42] [Investigation of the Mechanical Properties of Three Commercial and Five Variations of IOL Models](https://arxiv.org/abs/2510.20015)
*Taner Karateke,Abdullah MevlÜt Mutluel*

Main category: physics.comp-ph

TL;DR: FEM simulation of 8 haptic IOL models shows different mechanical stability under compression forces - commercial models perform better at small forces while variations excel at larger forces.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the mechanical stability of different haptic IOL models under compression to help develop more mechanically stable designs.

Method: Used Finite Element Method (FEM) to simulate quasi-static compression and measure mechanical biomarkers including axial displacement, elasticity modulus, and stress.

Result: Commercial IOL models showed better mechanical response for smaller compression forces, while variation models performed better for larger compression forces.

Conclusion: The findings can guide the development of more mechanically stable IOL models by understanding how different designs perform under varying compression conditions.

Abstract: This study aimed to simulate the mechanical stability of eight different
(three commercial and five variations) haptic IOL models using FEM to measure
mechanical biomarkers (axial displacement, elasticity modulus, and stress)
under quasi-static compression. The results revealed that a commercial IOL
model exhibited a better mechanical response for smaller compression forces
than the other models. Conversely, a variation model performed better for
larger compression forces. These findings may help in developing more
mechanically stable IOL models.

</details>


### [43] [ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature](https://arxiv.org/abs/2510.20362)
*Aritra Roy,Enrico Grisan,John Buckeridge,Chiara Gattinoni*

Main category: physics.comp-ph

TL;DR: ComProScanner is an autonomous multi-agent platform that extracts, validates, classifies, and visualizes chemical compositions and properties from scientific literature, achieving 0.82 accuracy with DeepSeek-V3-0324 for ceramic piezoelectric materials.


<details>
  <summary>Details</summary>
Motivation: There is a lack of accessible automated tools for constructing, validating, and visualizing datasets from scientific literature extraction, particularly for complex chemical compositions and properties.

Method: Developed ComProScanner, a multi-agent platform that integrates extraction, validation, classification, and visualization of chemical compositions and properties from journal articles. Evaluated with 100 articles using 10 different LLMs.

Result: DeepSeek-V3-0324 outperformed all models with 0.82 overall accuracy for extracting complex compositions and piezoelectric strain coefficients (d33) from ceramic piezoelectric materials.

Conclusion: The framework provides a user-friendly package for extracting complex experimental data from literature to build machine learning datasets, addressing the scarcity of large datasets for specialized materials.

Abstract: Since the advent of various pre-trained large language models, extracting
structured knowledge from scientific text has experienced a revolutionary
change compared with traditional machine learning or natural language
processing techniques. Despite these advances, accessible automated tools that
allow users to construct, validate, and visualise datasets from scientific
literature extraction remain scarce. We therefore developed ComProScanner, an
autonomous multi-agent platform that facilitates the extraction, validation,
classification, and visualisation of machine-readable chemical compositions and
properties, integrated with synthesis data from journal articles for
comprehensive database creation. We evaluated our framework using 100 journal
articles against 10 different LLMs, including both open-source and proprietary
models, to extract highly complex compositions associated with ceramic
piezoelectric materials and corresponding piezoelectric strain coefficients
(d33), motivated by the lack of a large dataset for such materials.
DeepSeek-V3-0324 outperformed all models with a significant overall accuracy of
0.82. This framework provides a simple, user-friendly, readily-usable package
for extracting highly complex experimental data buried in the literature to
build machine learning or deep learning datasets.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [44] [High Gain Fusion Target Design using Generative Artificial Intelligence](https://arxiv.org/abs/2510.20105)
*Michael E. Glinsky*

Main category: physics.plasm-ph

TL;DR: Using generative AI based on Ubuntu concept to design optimally entangled topological states for fusion targets, achieving 10 GJ energy yield from 3 MJ input across various fusion methods.


<details>
  <summary>Details</summary>
Motivation: To develop practical, room temperature fusion targets that can achieve high energy yields with minimal input energy by leveraging topological design principles and AI.

Method: Generative AI based on Ubuntu concept that replaces deep convolutional neural networks with a generating functional formula for canonical transformations, enabling renormalization and topological control of collective systems.

Result: Created practical fusion targets that yield up to 10 GJ of energy from only 3 MJ of absorbed energy, applicable to tokamaks, laser-driven, and pulsed-power schemes.

Conclusion: The approach enables topological characterization and control of complex fusion systems through canonical field theory and renormalization, achieving high energy efficiency in fusion target design.

Abstract: By returning to the topological basics of fusion target design, Generative
Artificial Intelligence (genAI) is used to specify how to initially configure
and drive the optimally entangled topological state, and stabilize that
topological state from disruption. This can be applied to all methods;
including tokamaks, laser-driven schemes, and pulsed-power driven schemes. The
result is practical, room temperature targets that can yield up to 10 GJ of
energy, driven by as little as 3 MJ of absorbed energy. The genAI is based on
the concept of Ubuntu that replaces the Deep Convolutional Neural Network
approximation of a functional, with the formula for the generating functional
of a canonical transformation from the domain of the canonical field momentums
and fields, to the domain of the canonical momentums and coordinates, that is
the Reduced Order Model. This formula is a logical process of renormalization,
enabling Heisenberg's canonical approach to field theory, via calculation of
the S-matrix, given observation of the fields. This can be viewed as
topological characterization and control of collective, that is complex,
systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [45] [Simultaneously Solving Infinitely Many LQ Mean Field Games In Hilbert Spaces: The Power of Neural Operators](https://arxiv.org/abs/2510.20017)
*Dena Firoozi,Anastasis Kratsios,Xuwei Yang*

Main category: math.OC

TL;DR: Neural operators can learn to solve linear-quadratic mean-field games by mapping problem rules to equilibrium strategies, with statistical guarantees for generalization to unseen problems.


<details>
  <summary>Details</summary>
Motivation: Traditional MFG solvers are inefficient for solving many related problems, especially in settings with perturbations or continuum-parameterized agents.

Method: Train neural operators to learn the rules-to-equilibrium map from problem data (dynamics and cost functionals) to equilibrium strategies, with controlled Lipschitz regularity.

Result: NOs trained on small samples can reliably solve unseen LQ MFG variants, even in infinite-dimensional settings, with controlled parameter requirements.

Conclusion: The approach provides statistical guarantees for solving MFGs using neural operators, overcoming limitations of instance-by-instance solvers through controlled Lipschitz approximation and sample complexity bounds.

Abstract: Traditional mean-field game (MFG) solvers operate on an instance-by-instance
basis, which becomes infeasible when many related problems must be solved
(e.g., for seeking a robust description of the solution under perturbations of
the dynamics or utilities, or in settings involving continuum-parameterized
agents.). We overcome this by training neural operators (NOs) to learn the
rules-to-equilibrium map from the problem data (``rules'': dynamics and cost
functionals) of LQ MFGs defined on separable Hilbert spaces to the
corresponding equilibrium strategy. Our main result is a statistical guarantee:
an NO trained on a small number of randomly sampled rules reliably solves
unseen LQ MFG variants, even in infinite-dimensional settings. The number of NO
parameters needed remains controlled under appropriate rule sampling during
training.
  Our guarantee follows from three results: (i) local-Lipschitz estimates for
the highly nonlinear rules-to-equilibrium map; (ii) a universal approximation
theorem using NOs with a prespecified Lipschitz regularity (unlike traditional
NO results where the NO's Lipschitz constant can diverge as the approximation
error vanishes); and (iii) new sample-complexity bounds for $L$-Lipschitz
learners in infinite dimensions, directly applicable as the Lipschitz constants
of our approximating NOs are controlled in (ii).

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [46] [The global nonlinear stability of Minkowski spacetime with self-gravitating massive Dirac fields](https://arxiv.org/abs/2510.20626)
*Philippe G. LeFloch,Yue Ma,Weidong Zhang*

Main category: gr-qc

TL;DR: The paper proves the nonlinear stability of massive spinor fields in the Einstein-Dirac system, showing that initial data close to Minkowski spacetime leads to globally hyperbolic developments that remain asymptotic to Minkowski spacetime.


<details>
  <summary>Details</summary>
Motivation: Previous results were limited to massless fields in the Einstein-Dirac system. This work extends the analysis to massive spinor fields, requiring new mathematical tools and approaches due to the specific structure of spinor fields and Dirac equations.

Method: Uses asymptotically hyperboloidal-Euclidean framework with gauge-invariant treatment of spinor fields via Lorentz Clifford algebras and principal fiber bundles. Employs light-bending wave coordinates and derives L2 estimates, pointwise estimates, and new Sobolev inequalities for spinor fields.

Result: Establishes global existence for the system of wave equations with constraints and Klein-Gordon-type equations. Proves nonlinear stability of massive spinor fields with specific hierarchy of estimates distinguishing translations, rotations, and boosts.

Conclusion: The Einstein-Dirac system for massive fields is nonlinearly stable when initial data is sufficiently close to Minkowski spacetime, with the solution remaining asymptotic to Minkowski spacetime in all directions.

Abstract: We consider the Einstein-Dirac system for a massive field, which describes
the evolution of self-gravitating massive spinor fields, and we investigate the
global evolution problem, when the initial data set is sufficiently close to
data describing a spacelike, asymptotically Euclidean slice of the Minkowski
spacetime. We establish the gauge-invariant nonlinear stability of such fields,
namely the existence of a globally hyperbolic development, which remains
asymptotic to Minkowski spacetime in future timelike, null, and spacelike
directions. Previous results on this problem have been limited to the
Einstein-Dirac system in the massless case. Our analysis follows the
asymptotically hyperboloidal-Euclidean framework introduced by LeFloch and Y.
Ma for the massive Klein-Gordon-Einstein system. The structure specific to
spinor fields and the Dirac equation necessitates significantly new elements in
the proof. In contrast with prior approaches, our treatment of spinor fields
and the Dirac equation is gauge-invariant, relying on the formalism of Lorentz
Clifford algebras, principal fiber bundles, etc. Our analysis is carried out
with the metric expressed in light-bending wave coordinates, as we call them.
This leads us to the study of a global existence problem for a system of wave
equations with constraints and a Klein-Gordon-type equations. We derive L2
estimates for the Dirac equation and its coupling with the Einstein equations,
along with $pointwise estimates. New Sobolev inequalities are proven for spinor
fields in a gauge-invariant manner in the hyperboloidal-Euclidean foliation.
The nonlinear coupling between the massive Dirac equation and the Einstein
equations is investigated, and we establish a hierarchy of estimates, which
distinguish between translations, rotations, and boosts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals](https://arxiv.org/abs/2510.19917)
*Trajan Murphy,Akshunna S. Dogra,Hanfeng Gu,Caleb Meredith,Mark Kon,Julio Enrique Castrillion-Candas*

Main category: cs.LG

TL;DR: FINDER is a classification framework for noisy datasets that uses stochastic analysis and Hilbert space mapping to create stochastic features, then applies KLE decomposition for eigen-based classification in challenging data-scarce domains.


<details>
  <summary>Details</summary>
Motivation: Address classification challenges in noisy datasets with low signal-to-noise ratios, small sample sizes, and faulty data collection, which remain key research frontiers with both theoretical and practical implications.

Method: Views datasets as realizations from random fields, maps them to Hilbert spaces to create stochastic features, uses Kosambi-Karhunen-Loève expansion for irreducible component decomposition, and performs classification via eigen-decomposition of operator spectra.

Result: Achieved state-of-the-art breakthroughs in Alzheimer's Disease stage classification and remote sensing deforestation detection on challenging, data-deficient scientific domains.

Conclusion: FINDER provides a rigorous framework for noisy dataset classification, with discussion on when it outperforms existing methods, its failure modes, and limitations.

Abstract: ''Noisy'' datasets (regimes with low signal to noise ratios, small sample
sizes, faulty data collection, etc) remain a key research frontier for
classification methods with both theoretical and practical implications. We
introduce FINDER, a rigorous framework for analyzing generic classification
problems, with tailored algorithms for noisy datasets. FINDER incorporates
fundamental stochastic analysis ideas into the feature learning and inference
stages to optimally account for the randomness inherent to all empirical
datasets. We construct ''stochastic features'' by first viewing empirical
datasets as realizations from an underlying random field (without assumptions
on its exact distribution) and then mapping them to appropriate Hilbert spaces.
The Kosambi-Karhunen-Lo\'eve expansion (KLE) breaks these stochastic features
into computable irreducible components, which allow classification over noisy
datasets via an eigen-decomposition: data from different classes resides in
distinct regions, identified by analyzing the spectrum of the associated
operators. We validate FINDER on several challenging, data-deficient scientific
domains, producing state of the art breakthroughs in: (i) Alzheimer's Disease
stage classification, (ii) Remote sensing detection of deforestation. We end
with a discussion on when FINDER is expected to outperform existing methods,
its failure modes, and other limitations.

</details>


### [48] [Alternatives to the Laplacian for Scalable Spectral Clustering with Group Fairness Constraints](https://arxiv.org/abs/2510.20220)
*Iván Ojeda-Ruiz,Young Ju-Lee,Malcolm Dickens,Leonardo Cambisaca*

Main category: cs.LG

TL;DR: The paper introduces Fair-SMW, an efficient spectral clustering algorithm that incorporates group fairness constraints using Lagrangian methods and SMW identity, achieving 2x faster computation and 2x better balance than state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing spectral clustering algorithms with group fairness constraints suffer from high computational costs, creating a need for more efficient solutions that maintain fair clustering outcomes.

Method: Reformulates the constrained optimization problem using Lagrangian method and Sherman-Morrison-Woodbury identity, creating Fair-SMW algorithm with three Laplacian matrix alternatives and different spectral gaps.

Result: Achieved 2x faster computation time and 2x better balance compared to state-of-the-art methods across real-world datasets (LastFM, FacebookNet, Deezer, German) using Stochastic Block Model evaluation.

Conclusion: Fair-SMW provides an efficient solution for fair spectral clustering with significant improvements in both computational performance and fairness outcomes.

Abstract: Recent research has focused on mitigating algorithmic bias in clustering by
incorporating fairness constraints into algorithmic design. Notions such as
disparate impact, community cohesion, and cost per population have been
implemented to enforce equitable outcomes. Among these, group fairness
(balance) ensures that each protected group is proportionally represented
within every cluster. However, incorporating balance as a metric of fairness
into spectral clustering algorithms has led to computational times that can be
improved. This study aims to enhance the efficiency of spectral clustering
algorithms by reformulating the constrained optimization problem using a new
formulation derived from the Lagrangian method and the
Sherman-Morrison-Woodbury (SMW) identity, resulting in the Fair-SMW algorithm.
Fair-SMW employs three alternatives to the Laplacian matrix with different
spectral gaps to generate multiple variations of Fair-SMW, achieving clustering
solutions with comparable balance to existing algorithms while offering
improved runtime performance. We present the results of Fair-SMW, evaluated
using the Stochastic Block Model (SBM) to measure both runtime efficiency and
balance across real-world network datasets, including LastFM, FacebookNet,
Deezer, and German. We achieve an improvement in computation time that is twice
as fast as the state-of-the-art, and also flexible enough to achieve twice as
much balance.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [49] [Symbolic Regression and Differentiable Fits in Beyond the Standard Model Physics](https://arxiv.org/abs/2510.20453)
*Shehu AbdusSalam,Steven Abel,Deaglan Bartlett,Miguel Crispim Romão*

Main category: hep-ph

TL;DR: Symbolic regression (SR) is used to analyze the Constrained Minimal Supersymmetric Standard Model (CMSSM) by deriving symbolic expressions for key observables (Higgs mass, dark matter relic density, muon magnetic moment) in terms of input parameters, enabling faster analysis and differentiable fitting methods.


<details>
  <summary>Details</summary>
Motivation: To accelerate the analysis of Beyond Standard Model (BSM) physics phenomenology, particularly the CMSSM with its four arbitrary parameters, by using symbolic expressions for observables instead of conventional computational methods.

Method: Applied symbolic regression to derive mathematical expressions connecting CMSSM input parameters to experimental observables (Higgs mass, dark matter relic density, muon anomalous magnetic moment), then used these expressions for global parameter fitting.

Result: SR produced highly accurate symbolic expressions that enabled global fits of CMSSM parameters with posterior probability densities matching conventional methods. SR also allowed differentiable fitting methods and showed better global robustness compared to neural networks.

Conclusion: Symbolic regression is an effective approach for analyzing BSM physics models like CMSSM, providing accurate expressions for observables, enabling faster analysis, differentiable fitting methods, and superior global robustness compared to neural network regression.

Abstract: We demonstrate the efficacy of symbolic regression (SR) to probe models of
particle physics Beyond the Standard Model (BSM), by considering the so-called
Constrained Minimal Supersymmetric Standard Model (CMSSM). Like many
incarnations of BSM physics this model has a number (four) of arbitrary
parameters, which determine the experimental signals, and cosmological
observables such as the dark matter relic density. We show that analysis of the
phenomenology can be greatly accelerated by using symbolic expressions derived
for the observables in terms of the input parameters. Here we focus on the
Higgs mass, the cold dark matter relic density, and the contribution to the
anomalous magnetic moment of the muon. We find that SR can produce remarkably
accurate expressions. Using them we make global fits to derive the posterior
probability densities of the CMSSM input parameters which are in good agreement
with those performed using conventional methods. Moreover, we demonstrate a
major advantage of SR which is the ability to make fits using differentiable
methods rather than sampling methods. We also compare the method with neural
network (NN) regression. SR produces more globally robust results, while NNs
require data that is focussed on the promising regions in order to be equally
performant.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [50] [Two Quantum Algorithms for Nonlinear Reaction-Diffusion Equation using Chebyshev Approximation Method](https://arxiv.org/abs/2510.19855)
*Manish Kumar*

Main category: quant-ph

TL;DR: Two new quantum algorithms for reaction-diffusion equations using truncated Chebyshev polynomial approximation, with different gate complexities and technical contributions in Carleman matrix diagonalization.


<details>
  <summary>Details</summary>
Motivation: To develop efficient quantum algorithms for solving reaction-diffusion equations, leveraging quantum computational advantages over classical methods.

Method: Uses truncated Chebyshev polynomial approximation to solve linearized ODEs. First algorithm employs matrix exponentiation method, second uses quantum spectral method. Main technical contribution is deriving conditions for diagonalizing Carleman embedding matrix.

Result: First algorithm has gate complexity O(d·log(d)+T·polylog(T/ε)), second algorithm scales as O(polylog(d)·T·polylog(T/ε)). Comparable speedup to current best quantum algorithm for this problem.

Conclusion: The approach provides efficient quantum algorithms but has limitations: no upper bound on Carleman matrix condition number and diagonalization success depends on an unproven conjecture, though mitigation strategies are provided.

Abstract: We present two new quantum algorithms for reaction-diffusion equations that
employ the truncated Chebyshev polynomial approximation. This method is
employed to numerically solve the ordinary differential equation emerging from
the linearization of the associated nonlinear differential equation. In the
first algorithm, we use the matrix exponentiation method (Patel et al., 2018),
while in the second algorithm, we repurpose the quantum spectral method (Childs
et al., 2020). Our main technical contribution is to derive the sufficient
conditions for the diagonalization of the Carleman embedding matrix, which is
indispensable for designing both quantum algorithms. We supplement this with an
efficient iterative algorithm to diagonalize the Carleman matrix.
  Our first algorithm has gate complexity of
O(d$\cdot$log(d)+T$\cdot$polylog(T/$\varepsilon$)). Here $d$ is the size of the
Carleman matrix, $T$ is the simulation time, and $\varepsilon$ is the
approximation error. The second algorithm is polynomial in $log(d)$, $T$, and
$log(1/\varepsilon)$ - the gate complexity scales as
O(polylog(d)$\cdot$T$\cdot$polylog(T/$\varepsilon$)). In terms of $T$ and
$\varepsilon$, this is comparable to the speedup gained by the current best
known quantum algorithm for this problem, the truncated Taylor series method
(Costa et.al., 2025).
  Our approach has two shortcomings. First, we have not provided an upper
bound, in terms of d, on the condition number of the Carleman matrix. Second,
the success of the diagonalization is based on a conjecture that a specific
trigonometric equation has no integral solution. However, we provide strategies
to mitigate these shortcomings in most practical cases.

</details>


### [51] [On Encoding Matrices using Quantum Circuits](https://arxiv.org/abs/2510.20030)
*Liron Mor Yosef,Haim Avron*

Main category: quant-ph

TL;DR: This paper systematically studies quantum circuit representations for matrices, focusing on block encodings and state preparation circuits. It provides efficient construction methods from classical matrices and establishes bidirectional conversions between these representations, showing their essential equivalence.


<details>
  <summary>Details</summary>
Motivation: Quantum algorithms like HHL require efficient quantum circuit representations of matrices and vectors. The paper aims to systematically study and improve methods for constructing block encodings and state preparation circuits from classical inputs.

Method: The authors develop: (1) a general method for efficiently constructing block encodings from arbitrary classical matrices, (2) bidirectional conversion algorithms between block encodings and state preparation circuits, using a special constant-depth multiplexer and quantum conversion between standard basis and higher-order Pauli matrix basis expansions.

Result: Key results include: (a) efficient construction of block encodings for arbitrary classical matrices, and (b) low-overhead bidirectional conversions between block encodings and state preparation circuits, demonstrating their essential equivalence.

Conclusion: The paper establishes that block encodings and state preparation circuits are essentially equivalent quantum circuit representations for matrices, with efficient construction methods and conversion algorithms that enable flexible use in quantum linear algebra algorithms.

Abstract: Over a decade ago, it was demonstrated that quantum computing has the
potential to revolutionize numerical linear algebra by enabling algorithms with
complexity superior to what is classically achievable, e.g., the seminal HHL
algorithm for solving linear systems. Efficient execution of such algorithms
critically depends on representing inputs (matrices and vectors) as quantum
circuits that encode or implement these inputs. For that task, two common
circuit representations emerged in the literature: block encodings and state
preparation circuits. In this paper, we systematically study encodings matrices
in the form of block encodings and state preparation circuits. We examine
methods for constructing these representations from matrices given in classical
form, as well as quantum two-way conversions between circuit representations.
Two key results we establish (among others) are: (a) a general method for
efficiently constructing a block encoding of an arbitrary matrix given in
classical form (entries stored in classical random access memory); and (b)
low-overhead, bidirectional conversion algorithms between block encodings and
state preparation circuits, showing that these models are essentially
equivalent. From a technical perspective, two central components of our
constructions are: (i) a special constant-depth multiplexer that simultaneously
multiplexes all higher-order Pauli matrices of a given size, and (ii) an
algorithm for performing a quantum conversion between a matrix's expansion in
the standard basis and its expansion in the basis of higher-order Pauli
matrices.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [52] [Stochastic evolution equations with nonlinear diffusivity, recent progress and critical cases](https://arxiv.org/abs/2510.20471)
*Ioana Ciotir,Dan Goreac,Jonas M. Tölle*

Main category: math.PR

TL;DR: Survey on critical cases of stochastic evolution equations in variational formulation with various noise types, covering solution concepts, convergence, homogenization, and recent progress in regularity, long-time behavior, ergodicity, and numerical analysis.


<details>
  <summary>Details</summary>
Motivation: To summarize recent advances in critical cases of stochastic evolution equations that appear as limit cases in various physical and mathematical models like porous media, diffusion equations, and self-organized criticality.

Method: Survey approach presenting different solution notions, convergence results for parameter-dependent solutions, and homogenization techniques.

Result: Comprehensive overview of the field including various solution concepts, convergence properties, and recent developments in analysis methods.

Conclusion: The survey highlights significant progress in understanding critical stochastic evolution equations and provides references for further research in regularity, long-time behavior, and numerical aspects.

Abstract: This short survey article stems from recent progress on critical cases of
stochastic evolution equations in variational formulation with additive,
multiplicative or gradient noises. Typical examples appear as the limit cases
of the stochastic porous medium equation, stochastic fast- and super
fast-diffusion equations, self-organized criticality, stochastic singular
$p$-Laplace equations, and the stochastic total variation flow, among others.
We present several different notions of solutions, results on convergence of
solutions depending on a parameter, and homogenization. Furthermore, we provide
some references hinting at the recent progress in regularity results, long-time
behavior, ergodicity, and numerical analysis.

</details>


### [53] [On the Structure of Stationary Solutions to McKean-Vlasov Equations with Applications to Noisy Transformers](https://arxiv.org/abs/2510.20094)
*Krishnakumar Balasubramanian,Sayan Banerjee,Philippe Rigollet*

Main category: math.PR

TL;DR: The paper studies stationary solutions of McKean-Vlasov equations on the circle using Fourier analysis, revealing an exact equivalence between solutions and infinite-dimensional quadratic systems, enabling explicit characterization of bifurcations and phase transitions.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive framework for analyzing stationary states of McKean-Vlasov equations, particularly focusing on bifurcations, phase transitions, and the structure of solutions in Fourier space rather than function space.

Method: Establishes an exact equivalence between stationary McKean-Vlasov solutions and infinite-dimensional quadratic systems over Fourier coefficients, enabling explicit characterization in sequence space. Uses Fourier analysis to study local bifurcations, resonance structures, and derives analytic expressions for bifurcation forms.

Result: Successfully characterizes emergence, form, and shape of bifurcations involving multiple Fourier modes, connects them with discontinuous phase transitions, and establishes regularity and concavity properties of free energy landscape. Applied to Noisy Mean-Field Transformer model, showing how temperature parameter affects bifurcation geometry and reveals sharp transition from continuous to discontinuous phase behavior.

Conclusion: The Fourier-based framework provides a powerful approach for analyzing stationary McKean-Vlasov equations, enabling explicit characterization of bifurcations and phase transitions, with applications revealing rich metastable state structures and sharp transitions in physical systems.

Abstract: We study stationary solutions of McKean-Vlasov equations on the circle. Our
main contributions stem from observing an exact equivalence between solutions
of the stationary McKean-Vlasov equation and an infinite-dimensional quadratic
system of equations over Fourier coefficients, which allows explicit
characterization of the stationary states in a sequence space rather than a
function space. This framework provides a transparent description of local
bifurcations, characterizing their periodicity, and resonance structures, while
accommodating singular potentials. We derive analytic expressions that
characterize the emergence, form and shape (supercritical, critical,
subcritical or transcritical) of bifurcations involving possibly multiple
Fourier modes and connect them with discontinuous phase transitions. We also
characterize, under suitable assumptions, the detailed structure of the
stationary bifurcating solutions that are accurate upto an arbitrary number of
Fourier modes. At the global level, we establish regularity and concavity
properties of the free energy landscape, proving existence, compactness, and
coexistence of globally minimizing stationary measures, further identifying
discontinuous phase transitions with points of non-differentiability of the
minimum free energy map. As an application, we specialize the theory to the
Noisy Mean-Field Transformer model, where we show how changing the inverse
temperature parameter $\beta$ affects the geometry of the infinitely many
bifurcations from the uniform measure. We also explain how increasing $\beta$
can lead to a rich class of approximate multi-mode stationary solutions which
can be seen as `metastable states'. Further, a sharp transition from continuous
to discontinuous (first-order) phase behavior is observed as $\beta$ increases.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [54] [Kinetics of Peierls dimerization transition: Machine learning force-field approach](https://arxiv.org/abs/2510.20659)
*Ho Jang,Yang Yang,Gia-Wei Chern*

Main category: cond-mat.stat-mech

TL;DR: A machine learning force-field framework for simulating non-equilibrium dynamics of charge-density-wave order driven by Peierls instability, achieving linear scaling efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To overcome the computational bottleneck in evaluating adiabatic forces during time evolution of charge-density-wave systems, particularly for large systems where Peierls distortion calculations are intensive.

Method: Developed a generalized Behler-Parrinello neural-network architecture to predict forces from local structural environments, leveraging locality of electronic responses for linear scaling efficiency.

Result: Large-scale dynamical simulations revealed two-stage coarsening behavior: early-time power-law growth (L ~ t^0.7) followed by crossover to Allen-Cahn scaling (L ~ sqrt(t)) at late times, attributed to anisotropic domain-wall motion from electron-mediated directional interactions.

Conclusion: Demonstrates the promise of ML-based force fields for multiscale dynamical modeling of condensed-matter lattice models.

Abstract: We present a machine learning (ML) force-field framework for simulating the
non-equilibrium dynamics of charge-density-wave (CDW) order driven by the
Peierls instability. Since the Peierls distortion arises from the coupling
between lattice displacements and itinerant electrons, evaluating the adiabatic
forces during time evolution is computationally intensive, particularly for
large systems. To overcome this bottleneck, we develop a generalized
Behler-Parrinello neural-network architecture -- originally formulated for ab
initio molecular dynamics -- to accurately and efficiently predict forces from
local structural environments. Using the locality of electronic responses, the
resulting ML force field achieves linear scaling efficiency while maintaining
quantitative accuracy. Large-scale dynamical simulations using this framework
uncover a two-stage coarsening behavior of CDW domains: an early-time regime
characterized by a power-law growth $L \sim t^{\alpha}$ with an effective
exponent $\alpha \approx 0.7$, followed by a crossover to the Allen-Cahn
scaling $L \sim \sqrt{t}$ at late times. The enhanced early-time coarsening is
attributed to anisotropic domain-wall motion arising from electron-mediated
directional interactions. This work demonstrates the promise of ML-based force
fields for multiscale dynamical modeling of condensed-matter lattice models.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [55] [Enhanced Cyclic Coordinate Descent Methods for Elastic Net Penalized Linear Models](https://arxiv.org/abs/2510.19999)
*Yixiao Wang,Zishan Shao,Ting Jiang,Aditya Devarakonda*

Main category: stat.ML

TL;DR: Enhanced cyclic coordinate descent (ECCD) framework for generalized linear models with elastic net constraints that achieves 3x speedup over state-of-the-art methods through Taylor expansion approximation and batched computations.


<details>
  <summary>Details</summary>
Motivation: To reduce training time for generalized linear models with elastic net constraints compared to existing methods, while avoiding convergence delay and numerical instability of block coordinate descent.

Method: Redesigned cyclic coordinate descent using Taylor expansion around current iterate to avoid nonlinear gradient operations, unrolling vector recurrences into efficient batched computations with tunable parameter s for performance optimization.

Result: Consistent 3x performance improvements on regularization path variant across diverse benchmark datasets, with s > 1 providing better performance without affecting convergence.

Conclusion: ECCD framework successfully accelerates training for generalized linear models with elastic net constraints while maintaining convergence properties and numerical stability.

Abstract: We present a novel enhanced cyclic coordinate descent (ECCD) framework for
solving generalized linear models with elastic net constraints that reduces
training time in comparison to existing state-of-the-art methods. We redesign
the CD method by performing a Taylor expansion around the current iterate to
avoid nonlinear operations arising in the gradient computation. By introducing
this approximation, we are able to unroll the vector recurrences occurring in
the CD method and reformulate the resulting computations into more efficient
batched computations. We show empirically that the recurrence can be unrolled
by a tunable integer parameter, $s$, such that $s > 1$ yields performance
improvements without affecting convergence, whereas $s = 1$ yields the original
CD method. A key advantage of ECCD is that it avoids the convergence delay and
numerical instability exhibited by block coordinate descent. Finally, we
implement our proposed method in C++ using Eigen to accelerate linear algebra
computations. Comparison of our method against existing state-of-the-art
solvers shows consistent performance improvements of $3\times$ in average for
regularization path variant on diverse benchmark datasets. Our implementation
is available at https://github.com/Yixiao-Wang-Stats/ECCD.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [56] [Magnetic Field-Line Curvature and Its Role in Particle Acceleration by Magnetically Dominated Turbulence](https://arxiv.org/abs/2510.20628)
*Samuel Sebastian,Luca Comisso*

Main category: astro-ph.HE

TL;DR: First-principles kinetic simulations show magnetic field-line curvature drives particle acceleration in turbulent plasmas via curvature-drift motion, with effectiveness increasing with fluctuation-to-mean field ratio.


<details>
  <summary>Details</summary>
Motivation: To understand how magnetic field-line curvature contributes to particle acceleration in magnetically dominated turbulent plasmas, particularly through curvature-drift motion along the motional electric field.

Method: Used first-principles fully kinetic particle-in-cell simulations with varying fluctuation-to-mean magnetic-field ratios, analyzed curvature statistics and guiding-center motion to study acceleration mechanisms.

Result: Curvature probability densities show broad power-law wings with hard high-curvature tails for strong fluctuations. Curvature-drift acceleration accounts for substantial energization and strengthens with increasing fluctuation-to-mean field ratio, typically exceeding other drift mechanisms.

Conclusion: Curvature-drift acceleration is identified as a principal pathway for energy transfer from magnetized turbulence to nonthermal particles in astrophysical plasmas.

Abstract: We employ first-principles, fully kinetic particle-in-cell simulations to
investigate magnetic field-line curvature in magnetically dominated turbulent
plasmas and its role in particle acceleration through curvature-drift motion
along the motional electric field. By varying the fluctuation-to-mean
magnetic-field ratio $\delta B_0/B_0$, we examine curvature $\kappa$ statistics
and their connection to particle acceleration. The curvature probability
densities display broad power-law wings, scaling linearly in $\kappa$ below the
peak and developing hard high-$\kappa$ tails for $\delta B_0/B_0 \gtrsim 1$. As
the mean field strengthens, the high-$\kappa$ tails steepen, and
large-curvature events are suppressed when $\delta B_0/B_0 \ll 1$. The
probability density functions of magnetic field-line contraction, ${\bf v}_E
\cdot {\bf \kappa}$, with ${\bf v}_E$ the field-line velocity, develop
power-law tails well described by a symmetric Pareto distribution,
characteristic of stochastic energy exchanges, with the tails becoming harder
as $\delta B_0/B_0$ increases. Our guiding-center analysis shows that
curvature-drift acceleration accounts for a substantial fraction of the
energization via the motional electric field, and that it strengthens with
increasing $\delta B_0/B_0$. For well-magnetized particles, curvature-drift
acceleration typically exceeds ${\bf\nabla}B$ drift, polarization drift, and
betatron contributions. These results identify curvature-drift acceleration as
a principal pathway through which magnetized turbulence transfers energy to
nonthermal particles in astrophysical plasmas.

</details>


### [57] [The Opacity Project: R-Matrix Calculations for Opacities of High-Energy-Density Astrophysical and Laboratory Plasmas](https://arxiv.org/abs/2510.20775)
*Anil K. Pradhan,Sultana N. Nahar*

Main category: astro-ph.HE

TL;DR: This paper investigates radiative properties in high-energy-density plasmas using R-Matrix atomic data, focusing on opacity calculations for solar and fusion plasma conditions, with emphasis on broadening effects and equation-of-state impacts.


<details>
  <summary>Details</summary>
Motivation: Accurate opacity determination is critical for understanding radiation transport in astrophysical and laboratory plasmas, particularly addressing the solar opacity problem and discrepancies between theory and experiments.

Method: Uses atomic data from R-Matrix calculations to compute Rosseland Mean Opacities across temperature/density ranges, analyzing electron collisional and Stark ion microfield broadening effects on autoionizing resonances, and examining equation-of-state impacts.

Result: Calculated opacity variations under different plasma conditions relevant to solar convective zone base and inertial confinement fusion devices, providing insights into theoretical-experimental discrepancies.

Conclusion: The study contributes to improving opacity models for high-energy-density sources including stellar interiors and laboratory fusion plasma experiments, addressing key challenges in radiation transport modeling.

Abstract: Accurate determination of opacity is critical for understanding radiation
transport in both astrophysical and laboratory plasmas. We employ atomic data
from R-Matrix calculations to investigate radiative properties in
high-energy-density (HED) plasma sources. Specifically, we analyze environments
such as the base of the convective zone (BCZ) of the Sun 2 x 10^6$ K, N_e =
10^{23}/cc and the inertial confinement fusion (ICF) device at the Sandia Z
facility 2.11 x 10^6 K, N_e = 3.16 x 10^{22}/cc. We calculate Rosseland Mean
Opacities (RMO) within a range of temperatures and densities and analyze how
they vary under different plasma conditions. In this study, we specifically
focus on electron collisional and Stark ion microfield broadening effects on
autoionizing resonances in photoabsorption cross sections. Our results are
relevant to astrophysical models, particularly in the context of the solar
opacity problem, and provide insights into discrepancies between theoretical
calculations and experimental measurements. In addition, we investigate the
equation-of-state (EOS) and its impact on opacities. In addition, we examine
the equation-of-state (EOS) and its impact on opacities of the "chemical
picture" Mihalas-Hummer-Dappen EOS with respect to level populations of excited
levels included in the R-matrix calculations. This study should contribute to
improving opacity models of HED sources such as stellar interiors adn
laboratory fusion plasma experiments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [58] [Interpolatory Approximations of PMU Data: Dimension Reduction and Pilot Selection](https://arxiv.org/abs/2510.20116)
*Sean Reiter,Mark Embree,Serkan Gugercin,Vassilis Kekatos*

Main category: eess.SY

TL;DR: This paper proposes interpolatory matrix decompositions (IDs) with DEIM for PMU data compression and fault detection, achieving efficient real-time monitoring with limited measurements.


<details>
  <summary>Details</summary>
Motivation: To enable real-time power system monitoring using fewer PMU measurements while minimizing communication bandwidth requirements.

Method: Uses interpolatory matrix decompositions (IDs) with discrete empirical interpolation method (DEIM) for selecting optimal rows/columns to reconstruct full PMU data matrix.

Result: DEIM shows excellent performance for data compression and provides computable error estimates that serve as effective fault detection tools.

Conclusion: The ID framework with DEIM enables efficient PMU data compression and reliable fault detection while reducing measurement requirements.

Abstract: This work investigates the reduction of phasor measurement unit (PMU) data
through low-rank matrix approximations. To reconstruct a PMU data matrix from
fewer measurements, we propose the framework of interpolatory matrix
decompositions (IDs). In contrast to methods relying on principal component
analysis or singular value decomposition, IDs recover the complete data matrix
using only a few of its rows (PMU datastreams) and/or a few of its columns
(snapshots in time). This compression enables the real-time monitoring of power
transmission systems using a limited number of measurements, thereby minimizing
communication bandwidth. The ID perspective gives a rigorous error bound on the
quality of the data compression. We propose selecting rows and columns used in
an ID via the discrete empirical interpolation method (DEIM), a greedy
algorithm that aims to control the error bound. This bound leads to a
computable estimate for the reconstruction error during online operations. A
violation of this estimate suggests a change in the system's operating
conditions, and thus serves as a tool for fault detection. Numerical tests
using synthetic PMU data illustrate DEIM's excellent performance for data
compression, and validate the proposed DEIM-based fault-detection method.

</details>
