<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 29]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 2]
- [math.PR](#math.PR) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [gr-qc](#gr-qc) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Computing excited states with isometric tensor networks in two-dimensions](https://arxiv.org/abs/2510.20063)
*Alec Dektor,Runze Chi,Roel Van Beeumen,Chao Yang*

Main category: math.NA

TL;DR: A new subspace iteration method using block-isoPEPS ansatz for computing excited states of 2D quantum many-body Hamiltonians, extending block MPS framework to two dimensions with improved computational features.


<details>
  <summary>Details</summary>
Motivation: To develop efficient methods for computing low-lying eigenpairs (excited states) of high-dimensional quantum many-body Hamiltonians on 2D lattices, extending the successful block MPS approach from 1D to 2D systems.

Method: Proposed a block isometric projected entangled pair state (block-isoPEPS) ansatz that generalizes block MPS to 2D, featuring exact block orthogonalization, controlled local truncation via SVD, and efficient observable evaluation. Used inexact subspace iteration with this ansatz.

Result: Successfully computed excitations of 2D transverse-field Ising and Heisenberg models, demonstrating competitive performance compared to existing PEPS methods.

Conclusion: Block isometric tensor networks provide a scalable framework for studying excitations in quantum many-body systems beyond one dimension, with the block-isoPEPS ansatz offering practical advantages for PEPS-based algorithms.

Abstract: We present a new subspace iteration method for computing low-lying eigenpairs
(excited states) of high-dimensional quantum many-body Hamiltonians with
nearest neighbor interactions on two-dimensional lattices. The method is based
on a new block isometric projected entangled pair state (block-isoPEPS) ansatz
that generalizes the block matrix product state (MPS) framework, widely used
for Hamiltonians defined on one-dimensional chains, to two-dimensions. The
proposed block-isoPEPS ansatz offers several attractive features for PEPS-based
algorithms, including exact block orthogonalization, controlled local
truncation via singular value decompositions, and efficient evaluation of
observables. We demonstrate the proposed inexact subspace iteration for
block-isoPEPS by computing excitations of the two-dimensional transverse-field
Ising and Heisenberg models and compare our results with existing PEPS methods.
Our results demonstrate that block isometric tensor networks provide a scalable
framework for studying excitations in quantum many-body systems beyond one
dimension.

</details>


### [2] [Minimizing Residuals in ODE Integration Using Optimal Control](https://arxiv.org/abs/2510.20117)
*Robert M. Corless,C. Yalçın Kaya*

Main category: math.NA

TL;DR: The paper studies fitting curves through ODE solver solution points (skeleton) by minimizing ODE residual norms, reformulating it as a multi-stage optimal control problem and solving analytically for test cases and numerically for Van der Pol equation.


<details>
  <summary>Details</summary>
Motivation: To improve interpolation accuracy between discrete solution points generated by ODE solvers by minimizing the residual error of the original ODE system.

Method: Reformulate interpolation as multi-stage optimal control problem, apply maximum principle for optimality conditions, solve analytically for test problems (Dahlquist, leaky bucket) and numerically for Van der Pol equation using optimization software.

Result: Developed analytical solutions for test problems and numerical solutions for Van der Pol equation, showing improved residual norms compared to MATLAB's deval function.

Conclusion: The proposed approach provides more accurate interpolating curves with minimal ODE residuals than standard interpolation methods, validated through analytical solutions and numerical experiments.

Abstract: Given the set of discrete solution points or nodes, called the skeleton,
generated by an ODE solver, we study the problem of fitting a curve passing
through the nodes in the skeleton minimizing a norm of the residual vector of
the ODE. We reformulate this interpolation problem as a multi-stage optimal
control problem and, for the minimization of two different norms, we apply the
associated maximum principle to obtain the necessary conditions of optimality.
We solve the problem analytically for the Dahlquist test problem and a variant
of the leaky bucket problem, in terms of the given skeleton. We also consider
the Van der Pol equation, for which we obtain interpolating curves with minimal
residual norms by numerically solving a direct discretization of the problem
through optimization software. With the skeletons obtained by various ODE
solvers of MATLAB, we make comparisons between the residuals obtained by our
approach and those obtained by the MATLAB function deval.

</details>


### [3] [Joint Signal Recovery and Uncertainty Quantification via the Residual Prior Transform](https://arxiv.org/abs/2510.20136)
*Yao Xiao,Anne Gelb*

Main category: math.NA

TL;DR: The paper introduces a hierarchical Bayesian framework that reformulates the residual transform operator as a new prior, enabling uncertainty quantification and joint signal recovery from multimodal data without requiring prior knowledge of signal structure.


<details>
  <summary>Details</summary>
Motivation: Conventional signal recovery priors assume fixed signal variability types, which limits performance for complex signals exhibiting different behaviors across domains. The residual transform operator addresses this but lacks uncertainty quantification and multimodal data fusion capabilities.

Method: Reformulates the residual transform operator into a hierarchical Bayesian prior framework, enabling principled uncertainty quantification and joint recovery from multimodal measurements by coherently fusing information from disparate data sources.

Result: Numerical experiments demonstrate that the residual prior yields high-fidelity signal and image recovery from multimodal data while providing robust uncertainty quantification through credible intervals.

Conclusion: The Bayesian residual prior framework successfully addresses limitations of conventional priors by enabling uncertainty quantification and multimodal data fusion without requiring prior structural information about signals.

Abstract: Conventional priors used for signal recovery are often limited by the
assumption that the type of a signal's variability, such as piecewise constant
or linear behavior, is known and fixed. This assumption is problematic for
complex signals that exhibit different behaviors across the domain. The
recently developed {\em residual transform operator} effectively reduces such
variability-dependent error within the LASSO regression framework. Importantly,
it does not require prior information regarding structure of the underlying
signal. This paper reformulates the residual transform operator into a new
prior within a hierarchical Bayesian framework. In so doing, it unlocks two
powerful new capabilities. First, it enables principled uncertainty
quantification, providing robust credible intervals for the recovered signal,
and second, it provides a natural framework for the joint recovery of signals
from multimodal measurements by coherently fusing information from disparate
data sources. Numerical experiments demonstrate that the residual prior yields
high-fidelity signal and image recovery from multimodal data while providing
robust uncertainty quantification.

</details>


### [4] [General transformation neural networks: A class of parametrized functions for high-dimensional function approximation](https://arxiv.org/abs/2510.20142)
*Xiaoyang Wang,Yiqi Gu*

Main category: math.NA

TL;DR: GTNNs are novel neural networks with generalized transformations that improve accuracy for high-dimensional approximation, especially for oscillatory functions, outperforming conventional networks.


<details>
  <summary>Details</summary>
Motivation: Conventional deep neural networks perform poorly on oscillatory functions under gradient descent training, requiring more accurate approximation methods.

Method: Generalize affine transformations to more complex functions (cubic and quadratic transformations) as shape functions with larger capacity, creating CTNNs and QTNNs.

Result: GTNNs achieve universal approximation properties with error bounds for smooth and Barron-type functions, showing better accuracy and robustness in numerical experiments.

Conclusion: GTNNs, particularly CTNNs and QTNNs, provide superior performance over conventional networks for high-dimensional approximation problems.

Abstract: We propose a novel class of neural network-like parametrized functions, i.e.,
general transformation neural networks (GTNNs), for high-dimensional
approximation. Conventional deep neural networks sometimes perform less
accurately in approximation problems under gradient descent training,
especially when the target function is oscillatory. To improve accuracy, we
generalize the affine transformation of the abstract neuron to more general
functions, which act as complex shape functions and have larger capacities.
Specifically, we introduce two types of GTNNs: the cubic and quadratic
transformation neural networks (CTNNs and QTNNs). We perform approximation
error analysis for CTNNs and QTNNs, presenting their universal approximation
properties for continuous functions and error bounds for smooth functions and
Barron-type functions. Several numerical examples of regression problems and
partial differential equations are presented, demonstrating that CTNNs/QTNNs
have advantages in accuracy and robustness over conventional fully connected
neural networks.

</details>


### [5] [IEnSF: Iterative Ensemble Score Filter for Reducing Error in Posterior Score Estimation in Nonlinear Data Assimilation](https://arxiv.org/abs/2510.20159)
*Zezhong Zhang,Feng Bao,Guannan Zhang*

Main category: math.NA

TL;DR: The paper introduces an iterative ensemble score filter (IEnSF) that improves upon the Ensemble Score Filter (EnSF) by using an iterative algorithm to reduce posterior score estimation errors in nonlinear data assimilation problems.


<details>
  <summary>Details</summary>
Motivation: Current EnSF methods use a heuristic weighted sum to combine prior and likelihood scores, which introduces structural errors in nonlinear settings when estimating the posterior score function.

Method: Developed IEnSF that applies an iterative algorithm as an outer loop around the reverse-time stochastic differential equation solver, gradually reducing posterior score estimation error by improving approximation of the conditional expectation of the likelihood score function.

Result: Numerical experiments show IEnSF substantially reduces posterior score estimation error in nonlinear settings and improves accuracy of tracking high-dimensional dynamical systems.

Conclusion: The iterative approach in IEnSF effectively addresses the limitations of heuristic score combination in EnSF, providing more accurate data assimilation for nonlinear high-dimensional systems.

Abstract: The Ensemble Score Filter (EnSF) has emerged as a promising approach to
leverage score-based diffusion models for solving high-dimensional and
nonlinear data assimilation problems. While initial applications of EnSF to the
Lorenz-96 model and the quasi-geostrophic system showed potential, the current
method employs a heuristic weighted sum to combine the prior and the likelihood
score functions. This introduces a structural error into the estimation of the
posterior score function in the nonlinear setting. This work addresses this
challenge by developing an iterative ensemble score filter (IEnSF) that applies
an iterative algorithm as an outer loop around the reverse-time stochastic
differential equation solver. When the state dynamics or the observation
operator is nonlinear, the iterative algorithm can gradually reduce the
posterior score estimation error by improving the accuracy of approximating the
conditional expectation of the likelihood score function. The number of
iterations required depends on the distance between the prior and posterior
distributions and the nonlinearity of the observation operator. Numerical
experiments demonstrate that the IEnSF algorithm substantially reduces the
error in posterior score estimation in the nonlinear setting and thus improves
the accuracy of tracking high-dimensional dynamical systems.

</details>


### [6] [Anderson-type acceleration method for Deep Neural Network optimization](https://arxiv.org/abs/2510.20254)
*Kazufumi Ito,Tiancheng Xue*

Main category: math.NA

TL;DR: Anderson-type acceleration method for stochastic gradient descent improves neural network performance for DNN and CNN, with applications in computer tomography and inverse medium problems.


<details>
  <summary>Details</summary>
Motivation: To enhance neural network optimization by accelerating stochastic gradient descent methods for better performance in deep learning applications.

Method: Developed Anderson-type acceleration method for stochastic gradient descent optimization in neural networks.

Result: Significantly improved neural network performance for both DNN and CNN architectures.

Conclusion: The proposed acceleration method effectively enhances neural network optimization and shows promising applications in computer tomography and inverse medium problems.

Abstract: In this paper we consider the neural network optimization for DNN. We develop
Anderson-type acceleration method for the stochastic gradient decent method and
it improves the network permanence very much. We demonstrate the applicability
of the method for DNN and CNN. We discuss the application of the general class
of the neural network design for computer tomography and inverse medium
problems.

</details>


### [7] [Unique continuation for the wave equation: the stability landscape](https://arxiv.org/abs/2510.20359)
*Erik Burman,Lauri Oksanen,Janosch Preuss,Ziyao Zhao*

Main category: math.NA

TL;DR: Unique continuation for wave equations with volumetric data, showing Hölder stability in subsets and Lipschitz stability with lateral boundary trace knowledge, enabling finite element method design.


<details>
  <summary>Details</summary>
Motivation: To address unique continuation problems for wave equations when data is only available in volumetric subsets rather than on boundaries, which is common in practical applications.

Method: Prove unique continuation with Hölder stability into proper subsets without lateral boundary data, and Lipschitz stability when lateral boundary trace is contained in finite dimensional space.

Result: Established that solution can be continued with Hölder stability into certain subsets, and Lipschitz stability is achievable with knowledge of finite dimensional space containing lateral boundary trace.

Conclusion: The stability results enable design of finite element methods that converge to exact solutions at rates matching continuous problem stability properties.

Abstract: We consider a unique continuation problem for the wave equation given data in
a volumetric subset of the space time domain. In the absence of data on the
lateral boundary of the space-time cylinder we prove that the solution can be
continued with H\"older stability into a certain proper subset of the
space-time domain. Additionally, we show that unique continuation of the
solution to the entire space-time cylinder with Lipschitz stability is possible
given the knowledge of a suitable finite dimensional space in which the trace
of the solution on the lateral boundary is contained. These results allow us to
design a finite element method that provably converges to the exact solution at
a rate that mirrors the stability properties of the continuous problem.

</details>


### [8] [Improving the accuracy of meshless methods via resolving power optimisation using multiple kernels](https://arxiv.org/abs/2510.20365)
*H. Broadley,J. R. C. King,S. J. Lind*

Main category: math.NA

TL;DR: This paper presents a framework for optimizing the resolving power of meshless methods in PDE simulations by exploiting non-unique kernels, improving accuracy for turbulent flows without additional computational cost.


<details>
  <summary>Details</summary>
Motivation: Meshless methods are widely used for turbulent flow simulations with complex geometries, but their accuracy has been assessed mainly through polynomial convergence rather than resolving power, which is crucial for capturing short spatial scales in turbulence.

Method: The authors develop a framework using linear combinations of non-unique kernels to maximize resolving power across wavenumbers, accounting for orientation-dependent resolution characteristics unique to meshless methods.

Result: The optimized approach shows improved accuracy in convergence tests, maintains stability in time-dependent problems, and achieves significant accuracy gains for various PDE systems without extra computational cost per timestep.

Conclusion: The optimization procedure enables accurate simulation of PDE systems with short spatial scales, particularly beneficial for turbulent flow fields, by enhancing resolution characteristics of meshless methods.

Abstract: Meshless methods are commonly used to determine numerical solutions to
partial differential equations (PDEs) for problems involving free surfaces
and/or complex geometries, approximating spatial derivatives at collocation
points via local kernels with a finite size. Despite their common use in
turbulent flow simulations, the accuracy of meshless methods has typically been
assessed using their convergence characteristics resulting from the polynomial
consistency of approximations to operators, with little to no attention paid to
the resolving power of the approximation. Here we provide a framework for the
optimisation of resolving power by exploiting the non-uniqueness of kernels to
provide improvements to numerical approximations of spatial derivatives. We
first demonstrate that, unlike in finite-difference approximations, the
resolving power of meshless methods is dependent not only on the magnitude of
the wavenumber, but also its orientation, before using linear combinations of
kernels to maximise resolving power over a range of wavenumbers. The new
approach shows improved accuracy in convergence tests and has little impact on
stability of time-dependent problems for a range of Eulerian meshless methods.
Solutions to a variety of PDE systems are computed, with significant gains in
accuracy for no extra computational cost per timestep in Eulerian frameworks.
The improved resolution characteristics provided by the optimisation procedure
presented herein enable accurate simulation of systems of PDEs whose solution
contains short spatial scales such as flow fields with homogeneous isotropic
turbulence.

</details>


### [9] [Projecting onto the Unit Dual Quaternion Set](https://arxiv.org/abs/2510.20425)
*Ziyang Li,Chunfeng Cui,Jiaxin Xie*

Main category: math.NA

TL;DR: This paper systematically studies projections onto unit dual quaternion sets under the 2^R-norm, identifying distinct cases based on standard-dual part relationships and validating the proposed algorithm through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: Dual quaternions have wide applications in multi-agent formation control, 3D motion modeling, and robotics, making the study of projections onto unit dual quaternion sets a fundamental research aspect.

Method: The authors systematically study projections under the 2^R-norm, identifying several distinct cases based on the relationship between standard and dual parts in vector form, and propose an algorithm for these projections.

Result: The effectiveness of the proposed projection algorithm is demonstrated through numerical experiments.

Conclusion: The systematic approach to dual quaternion projections under the 2^R-norm provides effective solutions for applications in control systems, motion modeling, and robotics.

Abstract: Dual quaternions have gained significant attention due to their wide
applications in areas such as multi-agent formation control, 3D motion
modeling, and robotics. A fundamental aspect in dual quaternion research
involves the projection onto unit dual quaternion sets. In this paper, we
systematically study such projections under the $2^R$-norm, which is commonly
used in practical applications. We identify several distinct cases based on the
relationship between the standard and dual parts in vector form, and
demonstrate the effectiveness of the proposed algorithm through numerical
experiments.

</details>


### [10] [Well-Posedness and Approximation of Weak Solutions to Time Dependent Maxwell's Equations with $L^2$-Data](https://arxiv.org/abs/2510.20752)
*Harbir Antil*

Main category: math.NA

TL;DR: Analysis of Maxwell's equations in conducting media with rough coefficients and L²-data, proving well-posedness and developing structure-preserving finite element methods.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for Maxwell's equations in conducting media with imperfect material coefficients and boundary conditions, addressing challenges with rough coefficients and L²-data.

Method: Direct proof of well-posedness using interior-in-time mollification for uniqueness and Galerkin method for existence. Structure-preserving semi-discrete finite element method based on Nédélec/Raviart-Thomas de Rham complex with divergence-free initialization.

Result: Proved well-posedness including solution existence, uniqueness, energy identity, and continuous data dependence. Developed stable finite element scheme preserving discrete Gauss law and energy identity, with convergence to weak solution under mesh refinement.

Conclusion: Established comprehensive mathematical framework for Maxwell's equations in conducting media with rough coefficients, providing both theoretical foundations and practical numerical methods with guaranteed convergence properties.

Abstract: We study Maxwell's equations in conducting media with perfectly conducting
boundary conditions on Lipschitz domains, allowing rough material coefficients
and $L^2$-data. Our first contribution is a direct proof of well-posedness of
the first-order weak formulation, including solution existence and uniqueness,
an energy identity, and continuous dependence on the data. The argument uses
interior-in-time mollification to show uniqueness while avoiding reflection
techniques. Existence is via the well-known Galerkin method (cf.~Duvaut and
Lions \cite[Eqns.~(4.31)--(4.32), p.~346; Thm.~4.1]{GDuvaut_JLLions_1976a}).
For completeness, and to make the paper self-contained, a complete proof has
been provided.
  Our second contribution is a structure-preserving semi-discrete finite
element method based on the N\'ed\'elec/Raviart--Thomas de Rham complex. The
scheme preserves a discrete Gauss law for all times and satisfies a
continuous-in-time energy identity with stability for nonnegative conductivity.
With a divergence-free initialization of the magnetic field (via potential
reconstruction or constrained $L^2$ projection), we prove convergence of the
semi-discrete solutions to the unique weak solution as the mesh is refined. The
analysis mostly relies on projector consistency, weak-* compactness in
time-bounded $L^2$ spaces, and identification of time derivatives in dual
spaces.

</details>


### [11] [Preconditioning of a pollution-free discretization of the Helmholtz equation](https://arxiv.org/abs/2510.20564)
*Harald Monsuur*

Main category: math.NA

TL;DR: A pollution-free FOSLS formulation for Helmholtz equation with block preconditioner showing linear iteration growth with wave number.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient iterative solver for Helmholtz equation that avoids pollution effects and works on general domains including scattering problems.

Method: First order system least squares formulation with block preconditioner consisting of Schur complement preconditioner and test space preconditioner using subspace correction techniques.

Result: Numerical experiments show linear dependence of MINRES iterations on wave number κ, and an algebraic error estimation approach prevents unnecessary iterations.

Conclusion: The proposed method is easy to implement, applicable to general domains, and provides efficient solution of Helmholtz problems with controlled iteration growth.

Abstract: We present a pollution-free first order system least squares (FOSLS)
formulation for the Helmholtz equation, solved iteratively using a block
preconditioner. This preconditioner consists of two components: one for the
Schur complement, which corresponds to a preconditioner on $L_2(\Omega)$, and
another defined on the test space, which we ensure remains Hermitian positive
definite using subspace correction techniques. The proposed method is easy to
implement and is directly applicable to general domains, including scattering
problems. Numerical experiments demonstrate a linear dependence of the number
of MINRES iterations on the wave number $\kappa$. We also introduce an approach
to estimate algebraic errors which prevents unnecessary iterations.

</details>


### [12] [Advancing Offshore Renewable Energy: Techno-Economic and Dynamic Performance of Hybrid Wind-Wave Systems](https://arxiv.org/abs/2510.20601)
*Alaa Ahmed,Maha N. Haji*

Main category: math.NA

TL;DR: Hybrid offshore wind-wave energy systems combining floating wind turbines with wave energy converters can reduce power fluctuations by 50% and lower wave energy costs by 15-83% while maintaining wind energy costs.


<details>
  <summary>Details</summary>
Motivation: Offshore wind has matured but wave energy remains costly. Integrating both technologies can enhance power generation, stabilize output, and reduce costs through mutual benefits.

Method: Analyzed 6 configurations combining RM3 wave energy converter with 5MW and 15MW wind turbines on spar and semi-submersible platforms. Examined dynamic response, mooring loads, and power production under varying conditions, including wave converter motion and optional reaction plate.

Result: Reaction plate improved damping for spar platform, enhancing wave energy absorption. Integration reduced wave energy LCOE by 15-83% while wind LCOE remained unaffected. Power fluctuations reduced by 50%. Hybridization reduced LCOE with 5MW turbine but slightly increased with 15MW turbine.

Conclusion: Hybrid systems create mutualistic relationship where wave energy converter benefits substantially while wind turbine experiences slight improvements or negligible effects. Research demonstrates potential to lower costs and support sustainable energy solutions.

Abstract: Offshore wind and wave energy offer high energy density and availability.
While offshore wind has matured significantly, wave energy remains costly and
under development. Integrating both technologies into a hybrid system can
enhance power generation, stabilize output, and reduce costs. This study
explores the benefits of combining an offshore floating wind turbine with the
two-body heaving point absorber wave energy converter, Reference Model 3 (RM3).
Six configurations are analyzed: RM3 integrated with the National Renewable
Energy Laboratory 5 MW and the International Energy Agency 15 MW wind turbines,
each tested on both spar and semi-submersible platforms. The analysis examines
dynamic response, mooring loads, and power production under varying
environmental conditions, considering the influence of the wave energy
converter float motion and an optional reaction plate. Results indicate that
the reaction plate improves damping for the spar platform, enhancing wave
energy absorption and power output. A comparative analysis indicates that
integrating the wave energy converter reduces its levelized cost of energy by
15-83%, while leaving the wind turbine levelized cost of energy unaffected.
Hybridization significantly reduces power fluctuations by 50%, reduces the
levelized cost of energy with the 5 MW wind turbine, and slightly increases it
with the 15 MW wind turbine. The results highlight a mutualistic relationship
between the wave energy converter and the offshore wind turbine, where the
former benefits substantially while the latter experiences slight improvements
or negligible effects. Additional findings quantify hydrodynamic interactions,
mooring performance, and economic feasibility. This research provides insights
into optimizing hybrid offshore renewable systems, demonstrating their
potential to lower costs and support sustainable energy solutions.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [Non-uniqueness and failure of Calderón-Zygmund estimates below the critical exponent for non-monotone PDE with linear growth](https://arxiv.org/abs/2510.20024)
*Akshara Vincent*

Main category: math.AP

TL;DR: Counterexamples to uniqueness and a priori Calderón-Zygmund estimates for non-monotone elliptic equations using convex integration.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that uniqueness and Calderón-Zygmund estimates fail for elliptic equations with smooth, uniformly elliptic operators that have linear growth but lack monotonicity.

Method: Using convex integration arguments to construct counterexamples for equations of the form div(A(∇u)) = 0 in the unit ball.

Result: Provided explicit counterexamples showing non-uniqueness of solutions and failure of Calderón-Zygmund estimates below L^2 regularity.

Conclusion: Monotonicity is essential for uniqueness and Calderón-Zygmund estimates in elliptic equations, even when operators are smooth, uniformly elliptic, and have linear growth.

Abstract: We provide counterexamples to uniqueness of solutions as well as a priori
Calder\'on-Zygmund estimates for solutions below $L^2$ using convex integration
argument for equations of the type $$ \text{div} (A (\nabla u)) = 0 \quad
\text{in } \mathbb{B}^2, $$ where $A: \mathbb{R}^{2} \to \mathbb{R}^2$ is
smooth, uniformly elliptic and has essentially linear growth, but fails to be
monotone.

</details>


### [14] [Well-posedness for a class of parabolic equations with singular-degenerate coefficients](https://arxiv.org/abs/2510.20051)
*Junyuan Fang,Tuoc Phan*

Main category: math.AP

TL;DR: The paper studies linear parabolic equations with measurable coefficients in divergence form, where heat capacity coefficients belong to Muckenhoupt weight classes, allowing for degenerate and singular coefficients. It introduces weighted parabolic cylinders and Sobolev spaces, and proves regularity estimates, existence, and uniqueness of weak solutions under small oscillation assumptions.


<details>
  <summary>Details</summary>
Motivation: To address linear parabolic equations with coefficients that can be degenerate, singular, or both, which arise in various physical applications and require specialized analytical frameworks beyond standard Sobolev spaces.

Method: Introduces weighted parabolic cylinders with non-homogeneous quasi-distance and weighted parabolic Sobolev spaces. Uses level-set method by Caffarelli and Peral, establishes weighted inequalities, and proves a weighted Aubin-Lions compactness theorem for sequences in these spaces.

Result: Proves regularity estimates, existence, and uniqueness of weak solutions in the weighted Sobolev spaces under small mean oscillation assumptions on the coefficients.

Conclusion: The framework of weighted parabolic Sobolev spaces and the analytical tools developed provide a robust foundation for studying parabolic equations with degenerate and singular coefficients, extending classical results to more general coefficient classes.

Abstract: This paper studies a class of linear parabolic equations with measurable
coefficients in divergence form whose volumetric heat capacity coefficients are
assumed to be in some Muckenhoupt class of weights. As such, the coefficients
can be degenerate, singular, or both degenerate and singular. A class of
weighted parabolic cylinders with a non-homogeneous quasi-distance function,
and a class of weighted parabolic Sobolev spaces intrinsically suitable for the
class of equations are introduced. Under some smallness assumptions on the mean
oscillations of the coefficients, regularity estimates, existence, and
uniqueness of weak solutions in the weighted Sobolev spaces are proved. To
achieve the results, we apply the level-set method introduced by Caffarelli and
Peral. Several weighted inequalities and a version of weighted Aubin-Lions
compactness theorem for sequences in weighted parabolic Sobolev spaces are
established.

</details>


### [15] [Time-periodic solutions to the cubic wave equation: an elementary constructive approach](https://arxiv.org/abs/2510.20054)
*Filip Ficek*

Main category: math.AP

TL;DR: Elementary proof of infinite family of time-periodic solutions for 1D cubic wave equation with Dirichlet BCs using perturbative expansion and Banach contraction principle.


<details>
  <summary>Details</summary>
Motivation: To provide explicit information about frequencies and structures of solutions, addressing limitations of previous approaches.

Method: First order perturbative expansion combined with Banach contraction principle to show existence of nearby solutions.

Result: Existence of infinite family of time-periodic solutions with explicit frequency and structure information.

Conclusion: The approach successfully demonstrates existence while providing explicit details about solution properties, improving upon previous methods.

Abstract: We present an elementary proof of existence of infinite family of
time-periodic solutions to the one-dimensional nonlinear cubic wave equation
with Dirichlet boundary conditions. It relies on the first order perturbative
expansion and uses the Banach contraction principle to show existence of nearby
solutions. In contrast to the previous results, this approach provides us
explicit information about the frequencies and structures of the obtained
solutions.

</details>


### [16] [Existence and qualitative properties of ground state solutions for the Schrödinger-Bopp-Podolsky system](https://arxiv.org/abs/2510.20143)
*Sheng Wang,Juan Huang*

Main category: math.AP

TL;DR: The paper proves existence and properties of solutions to the Schrödinger-Bopp-Podolsky system, including nontrivial solutions via mountain-pass lemma, ground state solutions, and their positivity, symmetry, and decay properties.


<details>
  <summary>Details</summary>
Motivation: To study the Schrödinger-Bopp-Podolsky system, which describes a Schrödinger field coupled with its electromagnetic field in Bopp-Podolsky theory under electrostatic conditions.

Method: Applied mountain-pass lemma to obtain nontrivial solutions, estimated ground state energy to prove existence of ground state solutions, analyzed relationship between solutions and critical point paths, and studied properties like positivity, radial symmetry, rotational invariance, and exponential decay.

Result: Proved existence of nontrivial solutions and ground state solutions, demonstrated solutions are ground states of mountain-pass type, established positivity, radial symmetry, rotational invariance, and exponential decay properties, and analyzed asymptotic behavior with respect to parameter a in radial case.

Conclusion: The paper successfully establishes the existence and fundamental properties of solutions to the Schrödinger-Bopp-Podolsky system, providing comprehensive analysis of solution behavior and characteristics.

Abstract: This paper concerns the existence and related properties of solutions to the
Schr\"{o}dinger-Bopp-Podolsky system, which reduces to a nonlinear and nonlocal
partial differential equation describing a Schr\"{o}dinger field coupled with
its electromagnetic field in Bopp-Podolsky theory under purely electrostatic
conditions. Firstly, by applying the mountain-pass lemma, we obtain the
existence of nontrivial solutions. Then, through some estimates of the ground
state energy, we prove the existence of ground state solutions. By exploring
the relationship between solutions and paths associated with critical points,
we further demonstrate that the obtained solutions are ground states of
mountain-pass type. Additionally, the positivity, radial symmetry, rotational
invariance, and exponential decay of the ground state solutions are considered.
Finally, in the radial case, we explore the asymptotic behavior of the obtained
solutions with respect to $a$.

</details>


### [17] [Asymptotic issue for fractional laplacian on long cylinders](https://arxiv.org/abs/2510.20263)
*Tahir Boudjeriou,Prosenjit Roy*

Main category: math.AP

TL;DR: Analysis of weak solutions to fractional p-Laplacian problems in unbounded cylindrical domains, extending local results to nonlocal setting.


<details>
  <summary>Details</summary>
Motivation: To study asymptotic behavior of weak solutions in unbounded cylindrical domains for fractional p-Laplacian equations, addressing technical challenges from nonlocal nature.

Method: Developed a nonlocal abstract framework to analyze elliptic and parabolic problems involving fractional p-Laplacian in domains unbounded in one direction.

Result: Extended and complemented related properties previously established in the local setting, overcoming technical difficulties from nonlocal operators.

Conclusion: Successfully established asymptotic behavior results for fractional p-Laplacian problems in unbounded cylindrical domains using a nonlocal abstract approach.

Abstract: In this paper, we are concerned with the asymptotic behavior of weak
solutions to certain elliptic and parabolic problems involving the fractional
$p$-Laplacian in cylindrical domains that become unbounded in one direction.
The nonlocal nature of the operator describing the equations creates several
technical difficulties in treating problems of this type. The main results,
obtained within a nonlocal abstract framework, extend and complement related
properties established in the local setting.

</details>


### [18] [Qualitative Behavior of Solutions to a Forced Nonlocal Thin-Film Equation](https://arxiv.org/abs/2510.20289)
*Jinhong Zhao,Bin Guo*

Main category: math.AP

TL;DR: Analysis of a 1D nonlocal degenerate fourth-order parabolic equation for hydraulic fracture modeling, establishing global existence and long-time behavior of weak solutions under inhomogeneous forces.


<details>
  <summary>Details</summary>
Motivation: To understand hydraulic fracture modeling through mathematical analysis of relevant degenerate parabolic equations with inhomogeneous forces.

Method: Used regularization scheme, modified energy/entropy methods, and novel differential inequality techniques to analyze weak solutions.

Result: For time-dependent forces, solutions converge to spatial average plus time integral of force. For time-independent forces, difference from linear function remains bounded in H^s space.

Conclusion: Established rigorous mathematical framework for analyzing hydraulic fracture models with inhomogeneous forces, proving global existence and characterizing long-time behavior.

Abstract: We study a one-dimensional nonlocal degenerate fourth-order parabolic
equation with inhomogeneous forces relevant to hydraulic fracture modeling.
Employing a regularization scheme, modified energy/entropy methods, and novel
differential inequality techniques, we establish global existence and long-time
behavior results for weak solutions under both time-dependent and
time-independent inhomogeneous forces. Specifically, for the time-dependent
force $S(t, x)$, we prove that the solution converges in $H^s (\Omega )$ to
$\bar{u}_0+\frac{1}{|\Omega|}\int_0^t \int_\Omega S(r, x)\, dxdr $, where
$\bar{u}_0=\frac{1}{|\Omega|}\int_{\Omega}u_{0}(x)\,dx$ is the spatial average
of the initial data. For the time-independent force $S(x)$, we prove that the
difference between the weak solution and the linear function $\bar{u}_0 +
\frac{t}{|\Omega|}\int_\Omega S(x)\, dx$ remains uniformly bounded in $H^s
(\Omega )$.

</details>


### [19] [On an Analytical Criterion for Detecting Intermittent Turbulent Behaviour of Solutions of Partial Differential Equations](https://arxiv.org/abs/2510.20290)
*Michele V Bartuccelli,Guido Gentile*

Main category: math.AP

TL;DR: The paper develops an analytical criterion using crest factor to distinguish between turbulent and non-turbulent solutions in PDEs, specifically identifying time-intermittent turbulence.


<details>
  <summary>Details</summary>
Motivation: To understand the nature of solutions in partial differential equations and determine whether they exhibit turbulent behavior, particularly focusing on intermittent turbulence.

Method: Proposes an analytical criterion based on the crest factor, which is computed for solutions of classical linear and nonlinear equations to capture essential features of the solutions.

Result: The crest factor criterion successfully distinguishes between solutions showing time-intermittent turbulence behavior and those that are either non-turbulent or exhibit statistically stationary turbulence (like in Kolmogorov's theory).

Conclusion: The crest factor provides an effective analytical tool for identifying and characterizing different types of turbulent behavior in PDE solutions, representing a significant advance in understanding turbulence nature.

Abstract: A main question in the study of partial differential equations is the
following: how do we understand the nature of the solutions and, in particular,
how do we determine if a given solution shows turbulent or non-turbulent
behaviour? Being able to answer such a question would be a major advance in the
comprehension of the nature of turbulence. In this paper we focus on the case
of intermittent turbulence and provide an analytical criterion, based on the
crest factor, which captures the essential feature of the solutions. By
computing the crest factor for the solutions of some classical equations, both
linear and nonlinear, we illustrate the capability of the criterion for
discerning between solutions exhibiting time-intermittent turbulence behaviour
and solutions which either are not turbulent or show statistically stationary
turbulence, like, for example, in the case described by Kolmogorov's theory.

</details>


### [20] [Nonlinear stability of a composite wave to the Cauchy problem of 1-D full compressible Navier-Stokes-Allen-Cahn system](https://arxiv.org/abs/2510.20298)
*Dan Lei,Zhengzheng Chen*

Main category: math.AP

TL;DR: The paper analyzes the large time behavior of solutions to the 1D compressible Navier-Stokes-Allen-Cahn system, proving global existence and convergence to composite rarefaction waves for the Euler system when adiabatic exponent γ is near 1.


<details>
  <summary>Details</summary>
Motivation: To understand the long-term behavior of two-phase compressible fluid mixtures modeled by the Navier-Stokes-Allen-Cahn system, particularly how solutions converge to composite rarefaction wave patterns from the corresponding Euler system.

Method: Using an elementary energy method that accounts for phase field variable effects and nonlinear wave complexity, the authors prove global existence and uniqueness of strong solutions.

Result: Proved that global strong solutions exist uniquely and converge to composite 1-rarefaction and 3-rarefaction waves as time approaches infinity, with large initial perturbations allowed except for temperature.

Conclusion: The compressible Navier-Stokes-Allen-Cahn system exhibits stable convergence to composite rarefaction wave patterns under appropriate conditions, demonstrating robust long-term behavior for two-phase compressible fluid mixtures.

Abstract: The compressible Navier-Stokes-Allen-Cahn system models the motion of a
mixture of two macroscopically immiscible viscous compressible fluids. In this
paper, we are concerned with the large time behavior of solutions to the Cauchy
problem of the one-dimensional full compressible Navier-Stokes-Allen-Cahn
system. If the Riemann problem of the corresponding Euler system admits a
solution which is a linear combination of 1-rarefaction wave and 3-rarefaction
wave, we proved that a global strong solution to the compressible
Navier-Stokes-Allen-Cahn system exists uniquely and converges to the above
composite wave as time goes to infinity, provided that the adiabatic exponent
$\gamma$ is closed to $1$. Here the initial perturbations except for the
temperature function of the fluid, and the strength of rarefaction waves can be
arbitrarily large. The proof is given by an elementary energy method that takes
into account the effect of the phase field variable $\chi(t,x)$ and the
complexity of nonlinear waves.

</details>


### [21] [Continuous data assimilation applied to the Rayleigh-Benard problem for compressible fluid flows](https://arxiv.org/abs/2510.20316)
*Eduard Feireisl,Wladimir Neves*

Main category: math.AP

TL;DR: Continuous data assimilation applied to Navier-Stokes-Fourier system for compressible rotating thermally driven fluids, with rigorous proof of tracking property under low Mach and high Rossby/Froude numbers.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze data assimilation methods for complex fluid systems, specifically addressing the challenges of compressible rotating thermally driven fluids.

Method: Applied continuous data assimilation to the Navier-Stokes-Fourier system, considering large data in the framework of weak solutions.

Result: Provided rigorous proof of the tracking property in the asymptotic regime of low Mach numbers and high Rossby and Froude numbers.

Conclusion: The continuous data assimilation method is effective for tracking solutions in compressible rotating thermally driven fluid systems under specific asymptotic conditions.

Abstract: We apply a continuous data assimilation method to the Navier-Stokes-Fourier
system governing the evolution of a compressible, rotating and thermally driven
fluid. A rigorous proof of the tracking property is given in the asymptotic
regime of low Mach and high Rossby and Froude numbers. Large data in the
framework of weak solutions are considered.

</details>


### [22] [Energy Decay in Measure Time: HUM Observability, Product-Exponential Envelopes, and GCC Calibration](https://arxiv.org/abs/2510.20371)
*Ben F. Tibola*

Main category: math.AP

TL;DR: Proves no uniform exponential energy law exists for impulsive exposure patterns in wall-clock time, introduces measure-valued clock sigma for unified analysis, and establishes sigma-exponential decay under observability conditions.


<details>
  <summary>Details</summary>
Motivation: To address the failure of past time-based unifications of continuous damping with impulses by developing a unified framework that handles both continuous and impulsive energy dissipation.

Method: Replace wall-clock time t with a measure-valued clock sigma that aggregates continuous exposure and atomic doses within a single Lyapunov ledger, applying the Hilbert Uniqueness Method (HUM) framework.

Result: Establishes an observability-dissipation principle: energy decays at product-exponential rate with respect to sigma, with structural constant c_sigma > 0. Framework unifies intermittent regimes and provides robustness under discretizations, variational limits, and stochastic extensions.

Conclusion: Observability implies sigma-exponential decay with sharp constants, providing a qualitative dynamics backbone that successfully unifies continuous and impulsive energy dissipation regimes.

Abstract: We prove that for impulsive exposure patterns there is no uniform exponential
energy law in wall-clock time t, which explains why past t-based unifications
of continuous damping with impulses fail. We therefore replace t by a
measure-valued clock, sigma, that aggregates absolutely continuous exposure and
atomic doses within a single Lyapunov ledger. On this ledger we prove an
observability-dissipation principle in the sense of the Hilbert Uniqueness
Method (HUM): there exists a structural constant c_sigma > 0 such that the
energy decays at least at a product-exponential rate with respect to sigma.
When sigma = t, the statement reduces to classical exponential stabilization
with the same constant. For the damped wave under the Geometric Control
Condition (GCC), the constant is calibrated by the usual observability and
geometric factors. The framework yields a monotonicity principle ("more
sigma-mass implies faster decay") and unifies intermittent regimes where
quiescent intervals are punctuated by impulses. As robustness, secondary to the
main contribution, the same decay law persists under structure-compatible
discretizations and along compact variational limits; a stochastic extension
supplies expectation and pathwise envelopes via the compensator. The
contribution is a qualitative dynamics backbone: observability implies
sigma-exponential decay with sharp constants.

</details>


### [23] [Optimal quantitative stability estimates for Alexandrov's Soap Bubble Theorem via Gagliardo-Nirenberg-type interpolation inequalities](https://arxiv.org/abs/2510.20399)
*João Gonçalves da Silva,Giorgio Poggesi*

Main category: math.AP

TL;DR: Optimal stability estimates for Alexandrov's Soap Bubble Theorem using Gagliardo-Nirenberg inequalities, establishing uniform closeness to a ball for L^r deviations of mean curvature from constant.


<details>
  <summary>Details</summary>
Motivation: To provide quantitative stability estimates for Alexandrov's Soap Bubble Theorem, which characterizes spheres as the only compact surfaces with constant mean curvature, by establishing optimal bounds on how close a domain must be to a ball when its mean curvature is nearly constant.

Method: Leverage Gagliardo-Nirenberg-type interpolation inequalities to derive optimal stability estimates for C^{k,α} domains, analyzing L^r deviations of mean curvature from constant for various r values.

Result: Obtained optimal stability estimates showing linear profile for r > (N-1)/2 and new non-linear profiles for r ≥ (N-1)/2. The non-linear profiles improve with increasing k and become formally linear as k → ∞. All estimates are proven optimal through explicit examples.

Conclusion: The paper establishes complete optimal quantitative stability for Alexandrov's theorem across all relevant parameter ranges, revealing new stability profiles and demonstrating how regularity improves stability bounds.

Abstract: The paper provides optimal quantitative stability estimates for the
celebrated Alexandrov's Soap Bubble Theorem within the class of $C^{k,\alpha}$
domains, for any $k \ge 1$ and $0 < \alpha \le 1$, by leveraging
Gagliardo-Nirenberg-type interpolation inequalities.
  Optimal estimates of uniform closeness to a ball are established for $L^r$
deviations of the mean curvature from being constant, for any $r\ge 2$ (more
generally, for any $r>1$ such that $r\ge (2N-2)/(N+1)$).
  For $r>\frac{N-1}{2}$, the stability profile is linear, thus returning the
existing results established in the literature through computations for nearly
spherical sets. Surprisingly, all the stability estimates for $r\ge
\frac{N-1}{2}$, for which the profile is not linear, are new; even in the
particular case $r=2$ (which has been extensively studied, since it is a case
of interest for several critical applications), the sharp stability profile
that we obtain is new. Interestingly, we also prove that the (non-linear)
profile for $r \ge \frac{N-1}{2}$ improves as $k$ becomes larger to such an
extent that it becomes formally linear as $k$ goes to $\infty$.
  Finally, for any $r$, we show that all our estimates are optimal for any $k
\ge 1$ and $0< \alpha \le 1$, by providing explicit examples.

</details>


### [24] [Hölder regularity for a class of doubly non linear PDEs](https://arxiv.org/abs/2510.20432)
*Filippo Maria Cassanello,Eurica Henriques*

Main category: math.AP

TL;DR: Local Hölder continuity is proven for non-negative, locally bounded weak solutions to doubly nonlinear parabolic equations with p > 2 and 0 < q < p-1.


<details>
  <summary>Details</summary>
Motivation: To establish regularity properties for solutions to doubly nonlinear parabolic equations, which are important in mathematical analysis and applications to physical phenomena.

Method: The proof uses expansion of positivity results, DeGiorgi-type lemmas, an alternative approach, and an exponential shift to handle the intrinsic geometry of the problem.

Result: Local Hölder continuity is established for non-negative, locally bounded weak solutions to the specified class of doubly nonlinear parabolic equations.

Conclusion: The combination of expansion of positivity, DeGiorgi-type techniques, and geometric considerations successfully proves the desired regularity property for solutions to these nonlinear parabolic equations.

Abstract: We prove local H\"older continuity for non negative, locally bounded, local
weak solutions to the class of doubly nonlinear parabolic equations $\partial_t
(u_q) - \text{div} (|Du|^{p-2} Du) = 0$ for $p > 2$, $ 0 < q < p-1$. The proof
relies on expansion of positivity results combined with the study of an
alternative (related to DeGiorgi-type lemmas) and an exponential shift which
allows us to deal with the intrinsic geometry associated to the problem.

</details>


### [25] [Global bifurcation of solutions to elliptic systems with system and domain symmetries](https://arxiv.org/abs/2510.20462)
*Piotr Stefaniak*

Main category: math.AP

TL;DR: Existence of continua of nontrivial solutions bifurcating from constant solutions in parameterized elliptic systems on symmetric domains with system symmetries, using equivariant gradient map degree theory.


<details>
  <summary>Details</summary>
Motivation: To study parameterized elliptic systems on symmetric domains with additional system symmetries and prove existence of bifurcating solutions without requiring nondegeneracy assumptions.

Method: Using the degree for equivariant gradient maps to analyze bifurcation from constant branches determined by critical points of the potential.

Result: Proved existence of continua of nontrivial solutions bifurcating from constant branches. On compact symmetric spaces, bifurcating solutions break symmetry at every nonzero level. Under additional assumptions, the continua are unbounded.

Conclusion: The method provides a framework for establishing bifurcation phenomena in symmetric elliptic systems without nondegeneracy conditions, with symmetry breaking and potential unboundedness of solution branches.

Abstract: We study parameterized elliptic systems on symmetric domains with additional
system symmetries. We prove the existence of continua of nontrivial solutions
bifurcating from the constant branch determined by a critical point of the
potential, without assuming nondegeneracy, via the degree for equivariant
gradient maps. Our assumptions are formulated in terms of the right-hand side.
When the domain is a compact symmetric space, the bifurcating solutions break
symmetry at every nonzero level. Under additional assumptions on the right-hand
side, the continua are unbounded.

</details>


### [26] [On dissipative turbulent solutions to the compressible anisotropic Navier-Stokes equations in unbounded domains](https://arxiv.org/abs/2510.20476)
*Ondřej Kreml,Šárka Nečasová,Tong Tang*

Main category: math.AP

TL;DR: Global existence of dissipative turbulent solutions for compressible Navier-Stokes equations with anisotropic viscous stress tensor on unbounded domains, with relaxed assumptions and weak-strong uniqueness.


<details>
  <summary>Details</summary>
Motivation: To extend the existence results for compressible Navier-Stokes equations with anisotropic viscous stress tensor from bounded domains (like the torus) to unbounded domains that are more relevant for geophysical applications, while relaxing assumptions on coefficients.

Method: Using the concept of dissipative turbulent solutions to prove global existence on unbounded domains, complementing Bresch and Jabin's compactness method approach.

Result: Established global existence of dissipative turbulent solutions on a large class of unbounded domains with relaxed assumptions on anisotropic tensor coefficients and pressure law coefficient, and proved weak-strong uniqueness property.

Conclusion: The work successfully extends the theory of compressible Navier-Stokes equations with anisotropic viscosity to more physically relevant unbounded domains while maintaining solution properties like weak-strong uniqueness.

Abstract: Inspired by Abbatiello, Feireisl and Novotn\'y, we prove the global existence
of dissipative turbulent solution for the compressible Navier-Stokes equations
with anisotropic viscous stress tensor on unbounded domain. Our work
complements the result of Bresch and Jabin, where the authors used the new
compactness method to prove the existence of a weak solution to the same system
in $\mathbb{T}^3$. By virtue of the concept of dissipative turbulent solutions,
we are able to relax assumptions on the anisotropic tensor coefficients and the
pressure law coefficient. We point out that we establish the existence result
on a large class of unbounded domains, which is more conform to geophysical
context. We also prove the weak-strong uniqueness property of acquired
dissipative turbulent solutions.

</details>


### [27] [Non-optimal domains for the helicity maximisation problem](https://arxiv.org/abs/2510.20533)
*Wadim Gerner*

Main category: math.AP

TL;DR: The paper extends previous work on the helicity isoperimetric problem by establishing additional geometric constraints that optimal domains must satisfy, ruling out optimality for a broad class of solid tori.


<details>
  <summary>Details</summary>
Motivation: To extend the results from previous work on the helicity isoperimetric problem and establish additional geometric constraints that optimal domains must satisfy.

Method: The authors build upon existing mathematical framework and establish geometric constraints through theoretical analysis and mathematical proofs.

Result: The work rules out the optimality of a broad class of solid tori by establishing additional geometric constraints that optimal domains must satisfy.

Conclusion: While additional geometric constraints for optimal domains are established, the existence of optimal domains remains an open problem.

Abstract: In [J. Cantarella, D. DeTurck, H. Gluck and M. Teytel, J. Math. Phys. 41:5615
(2000)] the helicity isoperimetric problem which asks to find a smooth domain
of fixed volume which maximises Biot-Savart helicity among all other smooth
domains of fixed volume was initiated. It was shown that if an optimal domain
exists, all of its boundary components must be tori.
  The present work extends these results by establishing additional geometric
constraints which optimal domains, if they exist, must satisfy. This allows to
rule out the optimality of a broad class of solid tori. The existence of
optimal domains remains an open problem.

</details>


### [28] [Homogenization, dimension reduction and linearization of thin elastic plate](https://arxiv.org/abs/2510.20573)
*Amartya Chakrabortty,Georges Griso,Julia Orlik*

Main category: math.AP

TL;DR: This paper studies the homogenization, dimension reduction, and linearization of composite plates in non-linear elasticity, showing that the limit energy remains the same regardless of the order of limits (h→0 followed by ε→0, or simultaneous (ε,h)→(0,0)).


<details>
  <summary>Details</summary>
Motivation: To investigate the relationship between homogenization, dimension reduction, and linearization in composite plates under external loading, and to understand how the order of these limits affects the resulting energy.

Method: The paper uses Γ-convergence technique to analyze the simultaneous limits (ε,h)→(0,0) without coupling assumptions, and performs decomposition of plate deformations and displacements to derive exact forms of limit energies.

Result: The limit energy remains unchanged regardless of whether h→0 first followed by ε→0, or both approach zero simultaneously. The existence of a unique solution for the limit linearized homogenized energy problem is demonstrated.

Conclusion: The results provide rigorous mathematical foundations for linearized elasticity as a limit of non-linear elasticity, and the approach is extended to periodic perforated plates, showing consistency across different limit-taking procedures.

Abstract: This paper investigates the homogenization, dimension reduction, and
linearization of a composite plate subjected to external loading within the
framework of non-linear elasticity problem. The total elastic energy of the
problem is of order $\sim h^2\varepsilon^{2a+3}$, where $a\geq1$. The paper is
divided into two parts: The first part presents the simultaneous
homogenization, dimension reduction and linearization
($(\varepsilon,h)\to(0,0)$) of a composite plate without any coupling
assumption of $\varepsilon$ and $h$. The second part consists of the rigorous
derivation of linearized elasticity as a limit of non-linear elasticity with
small deformation and external loading conditions. The results obtained
demonstrate that the limit energy remains unchanged when the first
linearization ($h\to 0$) is performed, followed by simultaneous homogenization
dimension reduction ($\varepsilon\to0$) and when both limits approach zero
simultaneously, i.e. $(\varepsilon,h)\to (0,0)$. The exact form of the limit
energy(s) is obtained through the decomposition of plate deformations and plate
displacements. By using the $\Gamma$-convergence technique, the existence of a
unique solution for the limit linearized homogenized energy problem is
demonstrated. These results are then extended to certain periodic perforated
plates.

</details>


### [29] [Rothe's method in direct and time-dependent inverse source problems for a semilinear pseudo-parabolic equation](https://arxiv.org/abs/2510.20642)
*Karel Van Bockstal,Khonatbek Khompysh,Arshyn Altybay*

Main category: math.AP

TL;DR: This paper studies the inverse problem of recovering an unknown time-dependent source term in a semilinear pseudo-parabolic equation with variable coefficients using additional measurement data.


<details>
  <summary>Details</summary>
Motivation: To develop methods for determining unknown source terms in complex semilinear pseudo-parabolic equations, which has applications in various physical and engineering problems.

Method: Uses Rothe's time-discretisation method to prove existence and uniqueness of weak solutions, and presents a numerical scheme for computations.

Result: Proves existence and uniqueness of weak solutions under smallness conditions on problem data, and provides a computational framework.

Conclusion: The proposed approach successfully addresses the inverse source problem for semilinear pseudo-parabolic equations with variable coefficients using time-discretisation methods.

Abstract: In this paper, we investigate the inverse problem of determining an unknown
time-dependent source term in a semilinear pseudo-parabolic equation with
variable coefficients and a Dirichlet boundary condition. The unknown source
term is recovered from additional measurement data expressed as a weighted
spatial average of the solution. By employing Rothe's time-discretisation
method, we prove the existence and uniqueness of a weak solution under a
smallness condition on the problem data. We also present a numerical scheme for
computations.

</details>


### [30] [Weak sequential stability of solutions to a nonisothermal kinetic model for incompressible dilute polymeric fluids](https://arxiv.org/abs/2510.20580)
*Miroslav Bulíček,Josef Málek,Endre Süli*

Main category: math.AP

TL;DR: Mathematical analysis of thermodynamically consistent kinetic models for nonisothermal flows of dilute polymeric fluids, showing convergence to global-in-time large-data weak solutions with energy inequality.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze thermodynamically consistent kinetic models for nonisothermal flows of dilute polymeric fluids by identifying energy storage and entropy production mechanisms.

Method: Analysis of nonlinear PDE system coupling temperature-dependent Navier-Stokes equations with temperature-dependent Fokker-Planck equation and temperature evolution equation. Construction of sequences of smooth solutions with uniform bounds.

Result: Sequences of smooth solutions converge to global-in-time large-data weak solutions satisfying energy inequality, with temperature satisfying renormalized variational inequality.

Conclusion: The mathematical model exhibits weak sequential stability and provides a rigorous framework for analyzing nonisothermal polymeric fluid flows with thermodynamic consistency.

Abstract: The paper is concerned with the mathematical analysis of a class of
thermodynamically consistent kinetic models for nonisothermal flows of dilute
polymeric fluids, based on the identification of energy storage mechanisms and
entropy production mechanisms in the fluid under consideration. The model
involves a system of nonlinear partial differential equations coupling the
unsteady incompressible temperature-dependent Navier--Stokes equations to a
temperature-dependent generalization of the classical Fokker--Planck equation
and an evolution equation for the absolute temperature. Sequences of smooth
solutions to the initial-boundary-value problem, satisfying the available
bounds that are uniform with respect to the given data of the model, are shown
to converge to a global-in-time large-data weak solution that satisfies an
energy inequality, where the absolute temperature satisfies a renormalized
variational inequality, implying weak sequential stability of the mathematical
model.

</details>


### [31] [Uniqueness and continuous dependence for a viscoelastic problem with memory in domains with time dependent cracks](https://arxiv.org/abs/2510.20583)
*Federico Cianci,Gianni Dal Maso*

Main category: math.AP

TL;DR: Study of hyperbolic partial integro-differential systems in domains with time-dependent cracks, focusing on uniqueness and continuous dependence on cracks.


<details>
  <summary>Details</summary>
Motivation: To understand how time-dependent cracks affect the behavior of hyperbolic partial integro-differential systems and establish conditions for well-posedness.

Method: Analyze hyperbolic partial integro-differential systems in domains with time-dependent cracks, establish conditions on crack behavior.

Result: Conditions on cracks that ensure uniqueness of solutions with prescribed initial-boundary conditions and continuous dependence on crack variations.

Conclusion: The paper provides mathematical conditions that guarantee well-posedness for hyperbolic systems in cracked domains, establishing both uniqueness and stability with respect to crack variations.

Abstract: We study some hyperbolic partial integro-differential systems in domains with
time dependent cracks. In particular, we give conditions on the cracks which
imply the uniqueness of the solution with prescribed initial-boundary
conditions, and its continuous dependence on the cracks.

</details>


### [32] [Dynamic crack growth in viscoelastic materials with memory](https://arxiv.org/abs/2510.20599)
*Federico Cianci*

Main category: math.AP

TL;DR: A model for dynamic crack growth in viscoelastic materials with history-dependent damping, based on energy dissipation balance and maximal dissipation conditions.


<details>
  <summary>Details</summary>
Motivation: To develop a mathematical model that captures dynamic crack growth in viscoelastic materials where damping depends on deformation history, addressing complex fracture mechanics.

Method: Based on dynamic energy dissipation balance and maximal dissipation condition, with analysis conducted in two dimensions.

Result: Established an existence theorem for the model under certain a priori regularity constraints on the cracks.

Conclusion: Successfully developed a theoretical framework for dynamic crack growth in viscoelastic materials with history-dependent damping, proving existence under specified regularity conditions.

Abstract: In this paper we introduce a model of dynamic crack growth in viscoelastic
material, where the damping term depends on the history of the deformation. The
model is based on a dynamic energy dissipation balance and on a maximal
dissipation condition. Our main result is an existence theorem in dimension two
under some a priori regularity constraints on the cracks.

</details>


### [33] [Analysis of coupled Maxwell-cable problems](https://arxiv.org/abs/2510.20619)
*Timo Reis,Nathanael Skrepek*

Main category: math.AP

TL;DR: Analysis of qualitative properties of a dynamical system modeling electromagnetic field interactions with radiating curved cables, focusing on semigroup generation and well-posedness.


<details>
  <summary>Details</summary>
Motivation: To extend the analysis of a recently published model for electromagnetic field interactions with radiating cables by examining the qualitative properties of the resulting dynamical system.

Method: Analyze the autonomous dynamics (zero input) to show generation of a strongly continuous semigroup and establish sufficient conditions for well-posedness.

Result: The autonomous dynamics generate a strongly continuous semigroup, and sufficient conditions are established for well-posedness, ensuring continuous dependence of state and output trajectories on inputs and initial conditions.

Conclusion: The model for radiating curved cables exhibits well-defined dynamical behavior with continuous dependence on inputs and initial conditions under the established sufficient conditions.

Abstract: Building on the recently published work "Modeling of radiating curved cables
via coupled telegrapher's and Maxwell's equations", which introduces a model
for the interaction between electromagnetic fields and radiating (possibly
curved) cables, we analyze the qualitative properties of the resulting
dynamical system. The model features inputs and outputs given by the currents
and voltages at the cable ends, while the state comprises the corresponding
distributions along the cables and the electromagnetic fields in the
surrounding domain. We show that the autonomous dynamics (i.e., with zero
input) generate a strongly continuous semigroup and establish sufficient
conditions for well-posedness, meaning continuous dependence of the state and
output trajectories on the inputs and initial conditions.

</details>


### [34] [Dimension reduction for time-dependent von Kármán rods](https://arxiv.org/abs/2510.20623)
*Federico Cianci,Bernd Schmidt*

Main category: math.AP

TL;DR: Convergence of 3D nonlinear elastodynamics solutions for thin rods to 1D von Kármán equations as cross-section shrinks, with energy dissipation from torsional vibrations.


<details>
  <summary>Details</summary>
Motivation: To understand how solutions in three-dimensional nonlinear elastodynamics for thin rods converge to dimensionally reduced models as the cross-section shrinks to zero.

Method: Assumes existence of solutions and proper control of torsional velocity, studies convergence as rod cross-section shrinks to zero for displacements comparable to small radius.

Result: Solutions converge to effective 1D von Kármán equations; high-frequency torsional vibrations cause energy dissipation in the limit, adding contributions to limiting equations.

Conclusion: The study successfully demonstrates convergence from 3D elastodynamics to 1D von Kármán equations for thin rods, with identified energy dissipation mechanisms from torsional vibrations.

Abstract: This paper aims to study the convergence of solutions in three-dimensional
nonlinear elastodynamics for a thin rod as its cross section shrinks to zero
for displacements that are comparable to the small radius of the rod. Assuming
the existence of solutions and proper control of the torsional velocity, we
show how these converge to the solutions of an effective dimensionally reduced
model which is a version of the the time dependent von K\'arm\'an equations for
a one-dimensional rod. In the presence of high-frequency torsional vibrations,
energy can dissipate in the limit and we obtain additional contributions in the
limiting equations.

</details>


### [35] [Nonrelativistic limit of bound-state solutions for nonlinear Dirac equation on noncompact quantum graphs](https://arxiv.org/abs/2510.20658)
*Guangze Gu,Michael Ruzhansky,Guoyan Wei,Zhipeng Yang*

Main category: math.AP

TL;DR: This paper studies bound-state solutions of the nonlinear Dirac equation on noncompact quantum graphs, proving their existence and convergence to nonlinear Schrödinger equation solutions in the nonrelativistic limit.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of relativistic quantum systems on complex geometric structures (quantum graphs) and establish connections between Dirac and Schrödinger equations in the nonrelativistic regime.

Method: Mathematical analysis of the nonlinear Dirac equation on noncompact quantum graphs, using techniques to prove existence of bound-state solutions and study their convergence properties as c→∞.

Result: Proved existence of bound-state solutions for NLDE on quantum graphs, established convergence to NLS solutions in nonrelativistic limit, and demonstrated uniform boundedness and exponential decay properties of solutions.

Conclusion: The nonlinear Dirac equation on quantum graphs exhibits well-behaved bound states that smoothly transition to Schrödinger equation solutions in the nonrelativistic limit, with robust decay properties independent of the speed of light parameter.

Abstract: In this paper, we investigate the nonrelativistic limit and qualitative
properties of bound-state solutions for the nonlinear Dirac equation (NLDE)
defined on noncompact quantum graphs: \[ -i c \frac{d}{d x} \sigma_1 \psi+m c^2
\sigma_3 \psi-\omega \psi=g(|\psi|) \psi, \quad \text { in } \mathcal{G} \]
where \( g : \mathbb{R}\rightarrow\mathbb{R} \) is a continuous nonlinear
function, \( c>0 \) represents the speed of light, \( m>0 \) is the particle's
mass, \( \omega\in\mathbb{R} \) is related to the frequency, \( \sigma_1 \) and
\( \sigma_3 \) denote the Pauli matrices, and \(\mathcal{G}\) is a noncompact
quantum graph. We establish the existence of bound-state solutions to the NLDE
on \(\mathcal{G}\), and prove that these solutions converge toward the
corresponding bound-state solutions of a nonlinear Schr\"odinger equation (NLS)
in the nonrelativistic limit (i.e., as the speed of light \( c \to \infty \))
for particles of small mass. Furthermore, we prove uniform boundedness and
exponential decay properties of the NLDE solutions, uniformly in \( c \),
thereby offering insight into their asymptotic behavior.

</details>


### [36] [The Cauchy problem for $p$-evolution equations with variable coefficients in Gelfand-Shilov spaces](https://arxiv.org/abs/2510.20702)
*Marco Cappiello,Eliakim Cleyton Machado*

Main category: math.AP

TL;DR: Well-posedness of linear evolution equations with time-space dependent coefficients in Gelfand-Shilov spaces


<details>
  <summary>Details</summary>
Motivation: To study Cauchy problems for linear evolution equations with coefficients depending on both time and space variables, extending analysis beyond constant coefficients

Method: Impose decay assumptions on lower order coefficients for large |x|, then prove well-posedness using Gelfand-Shilov spaces framework

Result: Established well-posedness result for the Cauchy problem in Gelfand-Shilov spaces under the decay conditions

Conclusion: The decay conditions on coefficients ensure well-posedness for linear evolution equations with time-space dependent coefficients in Gelfand-Shilov spaces

Abstract: We study the Cauchy problem for a class of linear evolution equations of
arbitrary order with coefficients depending both on time and space variables.
Under suitable decay assumptions on the coefficients of the lower order terms
for $|x|$ large, we prove a well-posedness result in Gelfand-Shilov spaces.

</details>


### [37] [Large field problem in coercive singular PDEs](https://arxiv.org/abs/2510.20716)
*Ilya Chevyrev,Massimiliano Gubinelli*

Main category: math.AP

TL;DR: A priori estimates for singular differential equations with irregular distributions, using rough path theory and regularity structures, with local estimates independent of boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To derive estimates for singular differential equations with irregular distributions in subcritical regimes, addressing challenges in handling non-smooth forcing terms.

Method: Use rough path theory for time derivative equations and regularity structures for heat operator equations, with local coercivity assumptions and scaling arguments.

Result: Abstract estimate that converts local coercivity to global coercivity via scaling, reducing a priori estimates to small ξ case.

Conclusion: Developed framework for deriving a priori estimates for singular differential equations with irregular distributions using modern analytical tools.

Abstract: We derive a priori estimates for singular differential equations of the form
\[ \mathcal{L} \phi = P(\phi,\nabla\phi) + f(\phi,\nabla\phi)\xi \] where $P$
is a polynomial, $f$ is a sufficiently well-behaved function, and $\xi$ is an
irregular distribution such that the equation is subcritical. The differential
operator $\mathcal L$ is either a derivative in time, in which case we
interpret the equation using rough path theory, or a heat operator, in which
case we interpret the equation using regularity structures. Our only assumption
on $P$ is that solutions with $\xi=0$ exhibit coercivity. Our estimates are
local in space and time, and independent of boundary conditions.
  One of our main results is an abstract estimate that allows one to pass from
a local coercivity property to a global one using scaling, for a large class of
equations. This allows us to reduce the problem of deriving a priori estimates
to the case when $\xi$ is small.

</details>


### [38] [First Critical Field in the pinned three-dimensional Ginzburg--Landau Model: A matching upper bound](https://arxiv.org/abs/2510.20720)
*Carlos Román*

Main category: math.AP

TL;DR: The paper establishes a matching upper bound for the first critical field H_c1 in extreme type-II superconductors, confirming the sharpness of previous lower bounds and connecting vorticity onset to a weighted isoflux problem.


<details>
  <summary>Details</summary>
Motivation: To complete the characterization of H_c1 by providing an upper bound that matches the previously established lower bound, thereby determining its exact leading-order behavior in extreme type-II superconductors.

Method: Uses the three-dimensional magnetic Ginzburg-Landau functional with pinning term, building on previous lower bound results and employing a Biot-Savart law based construction for the upper bound.

Result: Successfully derived a matching upper bound for H_c1, confirming the sharpness of the previously established lower bound and identifying the leading-order behavior of the first critical field.

Conclusion: The upper bound construction completes the characterization of H_c1, establishes the connection between vorticity onset and a weighted isoflux problem, and validates the sharpness of the previously derived lower bound.

Abstract: We continue our study of the first critical field $H_{c_1}$ for extreme
type-II superconductors governed by the three-dimensional magnetic
Ginzburg--Landau functional with a pinning term $a_\varepsilon$, as introduced
in our previous work [arXiv:2507.10915]. Building upon the lower bound for
$H_{c_1}$ and the characterization of the Meissner solution, we now establish a
matching upper bound for $H_{c_1}$, thereby identifying its leading-order
behavior. This result confirms the sharpness of the previously derived lower
bound and further elucidates the connection between the onset of vorticity and
a weighted variant of the \emph{isoflux problem}. Our argument is prompted by
the upper bound construction we developed in [arXiv:2510.14910], based on the
Biot--Savart law.

</details>


### [39] [Quantitative classification of potential Navier-Stokes singularities beyond the blow-up time](https://arxiv.org/abs/2510.20757)
*Tobias Barker*

Main category: math.AP

TL;DR: This paper provides the first quantitative classification of potentially singular solutions to 3D Navier-Stokes equations for approximately axisymmetric initial data, with bounds that can be numerically tested.


<details>
  <summary>Details</summary>
Motivation: Motivated by investigating the viability of numerical candidates for singular solutions, particularly Hou's numerical candidate for 3D Navier-Stokes singular solutions.

Method: Establishes improved quantitative regions of regularity for axisymmetric data, combines with improved energy estimates, implements physical space analogue of Tao's strategy, and uses recursive quantitative Carleman inequality arguments with careful bookkeeping.

Result: Achieves quantitative classification of potentially singular solutions at any given time in potential blow-up regions, with quantitative bounds that are numerically testable.

Conclusion: The approach successfully provides quantitative lower bounds on solutions near potential blow-up times while avoiding exponential losses through careful iterative application of Carleman inequalities.

Abstract: In \cite{hou}, Hou gave a compelling numerical candidate for a singular
solution of the 3D Navier-Stokes equations. We pioneer classifications of
potentially singular solutions, motivated by the issue of investigating the
viability of numerical candidates.For approximately axisymmetric initial data,
we give the first quantitative classification of potentially singular solutions
at \textit{any} given time in the region of potential blow-up times. Moreover,
the quantitative bounds in the vicinity of any potential blow-up time are in
principle amenable to numerical testing. To achieve this, we establish improved
quantitative regions of regularity for approximately axisymmetric initial data,
which may be of independent interest. Together with improved quantitative
energy estimates from \cite{TB24}, this allows us to get a quantitative lower
bound in the vicinity of a blow-up time by implementing the strategy of
\cite{BP21}, which is a physical space analogue of Tao's strategy \cite{Ta21}
for producing quantitative estimates for critically bounded solutions. To
obtain a quantitative lower bound on the solution at any time in the region of
potential blow-up times, we recursively apply quantitative Carleman inequality
arguments from \cite{Ta21}. This necessitates careful bookkeeping to avoid
exponential losses and to ensure that all forward-in-time iterations of
(localized) vorticity concentration remain within the region of quantitative
regularity of the solution.

</details>


### [40] [Ill-Posedness of the 2D Euler Equations in a Logarithmically Refined Critical Sobolev Space](https://arxiv.org/abs/2510.20773)
*Elaine Cozzi,Nicholas Harrison,Zachary Radke*

Main category: math.AP

TL;DR: The paper extends Bourgain-Li's ill-posedness results for 2D Euler equations to logarithmically regularized spaces that are strictly contained in H² but contain H^s for all s>2.


<details>
  <summary>Details</summary>
Motivation: To generalize the strong ill-posedness results of 2D Euler equations beyond the critical Sobolev space H² to more refined function spaces with logarithmic regularization.

Method: Construct logarithmically regularized spaces using fractional logarithmic derivatives applied to the critical Sobolev norm, and analyze ill-posedness when the logarithmic power α ≤ 1/2.

Result: The 2D Euler equations are strongly ill-posed in these logarithmically regularized spaces when the logarithmic derivative power α ≤ 1/2.

Conclusion: Strong ill-posedness persists in refined function spaces that are strictly smaller than H² but larger than H^s for s>2, extending the Bourgain-Li framework to logarithmic scales.

Abstract: In their seminal work, Bourgain and Li establish strong ill-posedness of the
2D Euler equations for initial velocity in the critical Sobolev space
$H^2(\mathbb{R}^2)$. In this work, we extend those results by demonstrating
strong ill-posedness in logarithmically regularized spaces which are strictly
contained in $H^2(\mathbb{R}^2)$ and which contain $H^s(\mathbb{R}^2)$ for all
$s>2$. These spaces are constructed via application of a fractional logarithmic
derivative to the critical Sobolev norm. We show that if the power $\alpha$ of
the logarithmic derivative satisfies $\alpha\leq 1/2$, then the 2D Euler
equations are strongly ill-posed.

</details>


### [41] [A Weakly Nonlinear Theory for Pattern Formation in Structured Models with Localized Solutions](https://arxiv.org/abs/2510.20781)
*Wesley J. M. Ridgway,Mohit P. Dalwadi,Philip Pearce,S. Jonathan Chapman*

Main category: math.AP

TL;DR: A weakly nonlinear framework for structured PDE models with localized steady states, demonstrated on a bacteria quorum sensing model.


<details>
  <summary>Details</summary>
Motivation: Classical pattern formation tools face challenges with structured PDEs due to sharply peaked or singular steady states.

Method: Uses WKBJ asymptotics and Stokes phenomenon analysis to resolve solution structure in the limit where steady state tends to a Dirac-delta function.

Result: Derived amplitude equation predicting pitchfork bifurcation, with effective parameter determining subcritical vs supercritical behavior.

Conclusion: The framework is broadly applicable to structured PDE models with spatially uniform but exponentially localized base states.

Abstract: Structured models, such as PDEs structured by age or phenotype, provide a
setting to study pattern formation in heterogeneous populations. Classical
tools to quantify the emergence of patterns, such as linear and weakly
nonlinear analyses, pose significant mathematical challenges for these models
due to sharply peaked or singular steady states. Here, we present a weakly
nonlinear framework that extends classical tools to structured PDE models in
settings where the base state is spatially uniform, but exponentially localized
in the structured variable. Our approach utilizes WKBJ asymptotics and an
analysis of the Stokes phenomenon to systematically resolve the solution
structure in the limit where the steady state tends to a Dirac-delta function.
To demonstrate our method, we consider a chemically structured (nonlocal) model
of motile bacteria that interact through quorum sensing. For this example, our
analysis yields an amplitude equation that governs the solution dynamics near a
linear instability, and predicts a pitchfork bifurcation. From the amplitude
equation, we deduce an effective parameter grouping whose sign determines
whether the pitchfork bifurcation is subcritical or supercritical. Although we
demonstrate our framework for a specific example, our techniques are broadly
applicable.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [42] [Investigation of the Mechanical Properties of Three Commercial and Five Variations of IOL Models](https://arxiv.org/abs/2510.20015)
*Taner Karateke,Abdullah MevlÜt Mutluel*

Main category: physics.comp-ph

TL;DR: FEM simulation of 8 haptic IOL models (3 commercial, 5 variations) to assess mechanical stability under compression, showing different models perform better at small vs large forces.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the mechanical stability of different haptic IOL models to help develop more mechanically stable designs.

Method: Used Finite Element Method (FEM) simulation to measure mechanical biomarkers (axial displacement, elasticity modulus, stress) under quasi-static compression.

Result: Commercial IOL model performed better for smaller compression forces, while a variation model performed better for larger compression forces.

Conclusion: Different IOL models have varying mechanical performance depending on compression force magnitude, providing insights for developing more mechanically stable IOL designs.

Abstract: This study aimed to simulate the mechanical stability of eight different
(three commercial and five variations) haptic IOL models using FEM to measure
mechanical biomarkers (axial displacement, elasticity modulus, and stress)
under quasi-static compression. The results revealed that a commercial IOL
model exhibited a better mechanical response for smaller compression forces
than the other models. Conversely, a variation model performed better for
larger compression forces. These findings may help in developing more
mechanically stable IOL models.

</details>


### [43] [ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature](https://arxiv.org/abs/2510.20362)
*Aritra Roy,Enrico Grisan,John Buckeridge,Chiara Gattinoni*

Main category: physics.comp-ph

TL;DR: ComProScanner is an autonomous multi-agent platform that extracts, validates, and visualizes chemical compositions and properties from scientific literature, achieving 0.82 accuracy with DeepSeek-V3-0324 for ceramic piezoelectric materials.


<details>
  <summary>Details</summary>
Motivation: Despite advances in large language models, there's a lack of accessible tools for constructing, validating, and visualizing datasets from scientific literature extraction, particularly for complex chemical compositions and properties.

Method: Developed ComProScanner - an autonomous multi-agent platform that facilitates extraction, validation, classification, and visualization of machine-readable chemical compositions and properties integrated with synthesis data. Evaluated using 100 journal articles against 10 different LLMs.

Result: DeepSeek-V3-0324 outperformed all models with significant overall accuracy of 0.82 for extracting complex compositions and piezoelectric strain coefficients (d33) from ceramic piezoelectric materials literature.

Conclusion: The framework provides a simple, user-friendly package for extracting complex experimental data from literature to build machine learning datasets, addressing the lack of large datasets for specialized materials.

Abstract: Since the advent of various pre-trained large language models, extracting
structured knowledge from scientific text has experienced a revolutionary
change compared with traditional machine learning or natural language
processing techniques. Despite these advances, accessible automated tools that
allow users to construct, validate, and visualise datasets from scientific
literature extraction remain scarce. We therefore developed ComProScanner, an
autonomous multi-agent platform that facilitates the extraction, validation,
classification, and visualisation of machine-readable chemical compositions and
properties, integrated with synthesis data from journal articles for
comprehensive database creation. We evaluated our framework using 100 journal
articles against 10 different LLMs, including both open-source and proprietary
models, to extract highly complex compositions associated with ceramic
piezoelectric materials and corresponding piezoelectric strain coefficients
(d33), motivated by the lack of a large dataset for such materials.
DeepSeek-V3-0324 outperformed all models with a significant overall accuracy of
0.82. This framework provides a simple, user-friendly, readily-usable package
for extracting highly complex experimental data buried in the literature to
build machine learning or deep learning datasets.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [44] [High Gain Fusion Target Design using Generative Artificial Intelligence](https://arxiv.org/abs/2510.20105)
*Michael E. Glinsky*

Main category: physics.plasm-ph

TL;DR: Using generative AI to design optimal topological states for fusion targets, enabling room temperature fusion with high energy yield (10 GJ from 3 MJ input) across various fusion methods.


<details>
  <summary>Details</summary>
Motivation: To improve fusion energy production by leveraging topological principles and AI to create stable, efficient fusion targets that work at room temperature across different fusion approaches.

Method: Generative AI based on Ubuntu concept replaces Deep Convolutional Neural Networks with generating functional formulas for canonical transformations, using renormalization processes and Heisenberg's canonical field theory approach.

Result: Practical room temperature fusion targets that can produce up to 10 GJ of energy from just 3 MJ of absorbed energy input.

Conclusion: Generative AI enables topological characterization and control of complex systems, providing a novel approach to fusion target design that stabilizes entangled topological states for efficient energy production.

Abstract: By returning to the topological basics of fusion target design, Generative
Artificial Intelligence (genAI) is used to specify how to initially configure
and drive the optimally entangled topological state, and stabilize that
topological state from disruption. This can be applied to all methods;
including tokamaks, laser-driven schemes, and pulsed-power driven schemes. The
result is practical, room temperature targets that can yield up to 10 GJ of
energy, driven by as little as 3 MJ of absorbed energy. The genAI is based on
the concept of Ubuntu that replaces the Deep Convolutional Neural Network
approximation of a functional, with the formula for the generating functional
of a canonical transformation from the domain of the canonical field momentums
and fields, to the domain of the canonical momentums and coordinates, that is
the Reduced Order Model. This formula is a logical process of renormalization,
enabling Heisenberg's canonical approach to field theory, via calculation of
the S-matrix, given observation of the fields. This can be viewed as
topological characterization and control of collective, that is complex,
systems.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [45] [Symbolic Regression and Differentiable Fits in Beyond the Standard Model Physics](https://arxiv.org/abs/2510.20453)
*Shehu AbdusSalam,Steven Abel,Deaglan Bartlett,Miguel Crispim Romão*

Main category: hep-ph

TL;DR: Symbolic regression accelerates BSM physics analysis by deriving symbolic expressions for observables from CMSSM parameters, enabling efficient global fits and differentiable optimization.


<details>
  <summary>Details</summary>
Motivation: To speed up analysis of Beyond Standard Model physics phenomenology by replacing computationally expensive conventional methods with symbolic expressions derived from input parameters.

Method: Using symbolic regression to derive mathematical expressions for Higgs mass, dark matter relic density, and muon anomalous magnetic moment in terms of CMSSM input parameters, then performing global fits using both conventional and differentiable methods.

Result: Symbolic regression produces highly accurate expressions that yield posterior probability densities in good agreement with conventional methods, while enabling faster differentiable optimization approaches.

Conclusion: Symbolic regression is an effective tool for BSM physics analysis, producing robust results comparable to neural networks but with better global performance and enabling differentiable fitting methods.

Abstract: We demonstrate the efficacy of symbolic regression (SR) to probe models of
particle physics Beyond the Standard Model (BSM), by considering the so-called
Constrained Minimal Supersymmetric Standard Model (CMSSM). Like many
incarnations of BSM physics this model has a number (four) of arbitrary
parameters, which determine the experimental signals, and cosmological
observables such as the dark matter relic density. We show that analysis of the
phenomenology can be greatly accelerated by using symbolic expressions derived
for the observables in terms of the input parameters. Here we focus on the
Higgs mass, the cold dark matter relic density, and the contribution to the
anomalous magnetic moment of the muon. We find that SR can produce remarkably
accurate expressions. Using them we make global fits to derive the posterior
probability densities of the CMSSM input parameters which are in good agreement
with those performed using conventional methods. Moreover, we demonstrate a
major advantage of SR which is the ability to make fits using differentiable
methods rather than sampling methods. We also compare the method with neural
network (NN) regression. SR produces more globally robust results, while NNs
require data that is focussed on the promising regions in order to be equally
performant.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [46] [Magnetic Field-Line Curvature and Its Role in Particle Acceleration by Magnetically Dominated Turbulence](https://arxiv.org/abs/2510.20628)
*Samuel Sebastian,Luca Comisso*

Main category: astro-ph.HE

TL;DR: First-principles kinetic simulations show magnetic field-line curvature drives particle acceleration in turbulent plasmas via curvature-drift motion, with stronger effects when fluctuation-to-mean magnetic field ratio increases.


<details>
  <summary>Details</summary>
Motivation: To understand how magnetic field-line curvature in turbulent plasmas contributes to particle acceleration through curvature-drift motion along electric fields.

Method: Used first-principles fully kinetic particle-in-cell simulations, varying fluctuation-to-mean magnetic-field ratio, analyzed curvature statistics and guiding-center particle motion.

Result: Curvature probability densities show power-law wings and hard high-curvature tails; curvature-drift acceleration accounts for substantial particle energization and strengthens with increasing fluctuation-to-mean field ratio.

Conclusion: Curvature-drift acceleration is a principal mechanism for energy transfer from magnetized turbulence to nonthermal particles in astrophysical plasmas.

Abstract: We employ first-principles, fully kinetic particle-in-cell simulations to
investigate magnetic field-line curvature in magnetically dominated turbulent
plasmas and its role in particle acceleration through curvature-drift motion
along the motional electric field. By varying the fluctuation-to-mean
magnetic-field ratio $\delta B_0/B_0$, we examine curvature $\kappa$ statistics
and their connection to particle acceleration. The curvature probability
densities display broad power-law wings, scaling linearly in $\kappa$ below the
peak and developing hard high-$\kappa$ tails for $\delta B_0/B_0 \gtrsim 1$. As
the mean field strengthens, the high-$\kappa$ tails steepen, and
large-curvature events are suppressed when $\delta B_0/B_0 \ll 1$. The
probability density functions of magnetic field-line contraction, ${\bf v}_E
\cdot {\bf \kappa}$, with ${\bf v}_E$ the field-line velocity, develop
power-law tails well described by a symmetric Pareto distribution,
characteristic of stochastic energy exchanges, with the tails becoming harder
as $\delta B_0/B_0$ increases. Our guiding-center analysis shows that
curvature-drift acceleration accounts for a substantial fraction of the
energization via the motional electric field, and that it strengthens with
increasing $\delta B_0/B_0$. For well-magnetized particles, curvature-drift
acceleration typically exceeds ${\bf\nabla}B$ drift, polarization drift, and
betatron contributions. These results identify curvature-drift acceleration as
a principal pathway through which magnetized turbulence transfers energy to
nonthermal particles in astrophysical plasmas.

</details>


### [47] [The Opacity Project: R-Matrix Calculations for Opacities of High-Energy-Density Astrophysical and Laboratory Plasmas](https://arxiv.org/abs/2510.20775)
*Anil K. Pradhan,Sultana N. Nahar*

Main category: astro-ph.HE

TL;DR: The paper investigates radiative properties in high-energy-density plasmas using R-Matrix calculations, focusing on opacity determination for astrophysical and laboratory fusion applications.


<details>
  <summary>Details</summary>
Motivation: To address the solar opacity problem and discrepancies between theoretical calculations and experimental measurements in radiation transport for astrophysical and laboratory plasmas.

Method: Uses atomic data from R-Matrix calculations to analyze radiative properties, calculates Rosseland Mean Opacities across temperature/density ranges, and examines electron collisional and Stark ion microfield broadening effects on autoionizing resonances.

Result: Provides opacity calculations relevant to solar convective zone and ICF conditions, investigating how opacities vary under different plasma conditions and the impact of equation-of-state on level populations.

Conclusion: The study contributes to improving opacity models for high-energy-density sources including stellar interiors and laboratory fusion plasma experiments.

Abstract: Accurate determination of opacity is critical for understanding radiation
transport in both astrophysical and laboratory plasmas. We employ atomic data
from R-Matrix calculations to investigate radiative properties in
high-energy-density (HED) plasma sources. Specifically, we analyze environments
such as the base of the convective zone (BCZ) of the Sun 2 x 10^6$ K, N_e =
10^{23}/cc and the inertial confinement fusion (ICF) device at the Sandia Z
facility 2.11 x 10^6 K, N_e = 3.16 x 10^{22}/cc. We calculate Rosseland Mean
Opacities (RMO) within a range of temperatures and densities and analyze how
they vary under different plasma conditions. In this study, we specifically
focus on electron collisional and Stark ion microfield broadening effects on
autoionizing resonances in photoabsorption cross sections. Our results are
relevant to astrophysical models, particularly in the context of the solar
opacity problem, and provide insights into discrepancies between theoretical
calculations and experimental measurements. In addition, we investigate the
equation-of-state (EOS) and its impact on opacities. In addition, we examine
the equation-of-state (EOS) and its impact on opacities of the "chemical
picture" Mihalas-Hummer-Dappen EOS with respect to level populations of excited
levels included in the R-matrix calculations. This study should contribute to
improving opacity models of HED sources such as stellar interiors adn
laboratory fusion plasma experiments.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [48] [Stochastic evolution equations with nonlinear diffusivity, recent progress and critical cases](https://arxiv.org/abs/2510.20471)
*Ioana Ciotir,Dan Goreac,Jonas M. Tölle*

Main category: math.PR

TL;DR: Survey on critical cases of stochastic evolution equations in variational formulation with various noise types, covering solution concepts, convergence, homogenization, and recent progress in regularity, long-time behavior, and numerical analysis.


<details>
  <summary>Details</summary>
Motivation: To summarize recent advances in critical cases of stochastic evolution equations that appear as limit cases in various physical and mathematical models like porous media, diffusion equations, self-organized criticality, and variational flows.

Method: Survey approach presenting different solution notions, convergence results for parameter-dependent solutions, and homogenization techniques for stochastic evolution equations with additive, multiplicative, or gradient noises.

Result: Comprehensive overview of solution concepts and convergence properties for critical stochastic evolution equations, highlighting connections to physical models and mathematical limit cases.

Conclusion: The survey provides a unified framework for understanding critical stochastic evolution equations and points to ongoing research directions in regularity, ergodicity, and numerical methods for these challenging problems.

Abstract: This short survey article stems from recent progress on critical cases of
stochastic evolution equations in variational formulation with additive,
multiplicative or gradient noises. Typical examples appear as the limit cases
of the stochastic porous medium equation, stochastic fast- and super
fast-diffusion equations, self-organized criticality, stochastic singular
$p$-Laplace equations, and the stochastic total variation flow, among others.
We present several different notions of solutions, results on convergence of
solutions depending on a parameter, and homogenization. Furthermore, we provide
some references hinting at the recent progress in regularity results, long-time
behavior, ergodicity, and numerical analysis.

</details>


### [49] [On the Structure of Stationary Solutions to McKean-Vlasov Equations with Applications to Noisy Transformers](https://arxiv.org/abs/2510.20094)
*Krishnakumar Balasubramanian,Sayan Banerjee,Philippe Rigollet*

Main category: math.PR

TL;DR: The paper studies stationary solutions of McKean-Vlasov equations on the circle using Fourier analysis, revealing an exact equivalence between solutions and infinite-dimensional quadratic systems, enabling explicit characterization of bifurcations and phase transitions.


<details>
  <summary>Details</summary>
Motivation: To understand stationary states of McKean-Vlasov equations on the circle and characterize their bifurcation structures, resonance patterns, and phase transitions, particularly for applications like the Noisy Mean-Field Transformer model.

Method: Establishes an exact equivalence between stationary McKean-Vlasov solutions and infinite-dimensional quadratic systems over Fourier coefficients, enabling analysis in sequence space rather than function space. Uses Fourier analysis to characterize bifurcations and derives analytic expressions for bifurcation types.

Result: Provides explicit characterization of local bifurcations (periodicity, resonance structures), analytic expressions for bifurcation emergence and types (supercritical, critical, subcritical, transcritical), and connects them with discontinuous phase transitions. Establishes regularity and concavity properties of free energy landscape, proving existence and coexistence of globally minimizing stationary measures.

Conclusion: The Fourier-based framework offers a powerful approach for analyzing McKean-Vlasov equations, enabling detailed characterization of bifurcations and phase transitions. Applied to the Noisy Mean-Field Transformer model, it reveals how temperature changes affect bifurcation geometry and can lead to metastable states, with a sharp transition from continuous to discontinuous phase behavior as temperature decreases.

Abstract: We study stationary solutions of McKean-Vlasov equations on the circle. Our
main contributions stem from observing an exact equivalence between solutions
of the stationary McKean-Vlasov equation and an infinite-dimensional quadratic
system of equations over Fourier coefficients, which allows explicit
characterization of the stationary states in a sequence space rather than a
function space. This framework provides a transparent description of local
bifurcations, characterizing their periodicity, and resonance structures, while
accommodating singular potentials. We derive analytic expressions that
characterize the emergence, form and shape (supercritical, critical,
subcritical or transcritical) of bifurcations involving possibly multiple
Fourier modes and connect them with discontinuous phase transitions. We also
characterize, under suitable assumptions, the detailed structure of the
stationary bifurcating solutions that are accurate upto an arbitrary number of
Fourier modes. At the global level, we establish regularity and concavity
properties of the free energy landscape, proving existence, compactness, and
coexistence of globally minimizing stationary measures, further identifying
discontinuous phase transitions with points of non-differentiability of the
minimum free energy map. As an application, we specialize the theory to the
Noisy Mean-Field Transformer model, where we show how changing the inverse
temperature parameter $\beta$ affects the geometry of the infinitely many
bifurcations from the uniform measure. We also explain how increasing $\beta$
can lead to a rich class of approximate multi-mode stationary solutions which
can be seen as `metastable states'. Further, a sharp transition from continuous
to discontinuous (first-order) phase behavior is observed as $\beta$ increases.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [50] [Interpolatory Approximations of PMU Data: Dimension Reduction and Pilot Selection](https://arxiv.org/abs/2510.20116)
*Sean Reiter,Mark Embree,Serkan Gugercin,Vassilis Kekatos*

Main category: eess.SY

TL;DR: This paper proposes interpolatory matrix decompositions (IDs) with DEIM for PMU data compression and fault detection, enabling real-time monitoring with limited measurements.


<details>
  <summary>Details</summary>
Motivation: To reduce PMU data transmission needs and enable real-time power system monitoring with minimal communication bandwidth while providing reliable data compression.

Method: Uses interpolatory matrix decompositions (IDs) with discrete empirical interpolation method (DEIM) to reconstruct complete PMU data matrix from selected rows (PMU datastreams) and columns (time snapshots).

Result: DEIM shows excellent performance for data compression and provides computable error estimates that serve as effective fault detection tools when system conditions change.

Conclusion: The ID framework with DEIM offers rigorous error bounds for PMU data compression, enables real-time monitoring with limited measurements, and provides reliable fault detection capabilities.

Abstract: This work investigates the reduction of phasor measurement unit (PMU) data
through low-rank matrix approximations. To reconstruct a PMU data matrix from
fewer measurements, we propose the framework of interpolatory matrix
decompositions (IDs). In contrast to methods relying on principal component
analysis or singular value decomposition, IDs recover the complete data matrix
using only a few of its rows (PMU datastreams) and/or a few of its columns
(snapshots in time). This compression enables the real-time monitoring of power
transmission systems using a limited number of measurements, thereby minimizing
communication bandwidth. The ID perspective gives a rigorous error bound on the
quality of the data compression. We propose selecting rows and columns used in
an ID via the discrete empirical interpolation method (DEIM), a greedy
algorithm that aims to control the error bound. This bound leads to a
computable estimate for the reconstruction error during online operations. A
violation of this estimate suggests a change in the system's operating
conditions, and thus serves as a tool for fault detection. Numerical tests
using synthetic PMU data illustrate DEIM's excellent performance for data
compression, and validate the proposed DEIM-based fault-detection method.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [51] [Kinetics of Peierls dimerization transition: Machine learning force-field approach](https://arxiv.org/abs/2510.20659)
*Ho Jang,Yang Yang,Gia-Wei Chern*

Main category: cond-mat.stat-mech

TL;DR: ML force-field framework for simulating CDW dynamics with linear scaling efficiency, revealing two-stage coarsening behavior with early-time power-law growth and late-time Allen-Cahn scaling.


<details>
  <summary>Details</summary>
Motivation: To overcome computational bottlenecks in simulating non-equilibrium CDW dynamics driven by Peierls instability, which requires intensive force calculations due to electron-lattice coupling.

Method: Developed generalized Behler-Parrinello neural-network architecture to predict forces from local structural environments, leveraging locality of electronic responses for linear scaling.

Result: Large-scale simulations uncovered two-stage CDW domain coarsening: early-time power-law growth (α≈0.7) and late-time Allen-Cahn scaling (L∼√t), attributed to anisotropic domain-wall motion from electron-mediated directional interactions.

Conclusion: Demonstrates promise of ML-based force fields for multiscale dynamical modeling of condensed-matter lattice models with quantitative accuracy and computational efficiency.

Abstract: We present a machine learning (ML) force-field framework for simulating the
non-equilibrium dynamics of charge-density-wave (CDW) order driven by the
Peierls instability. Since the Peierls distortion arises from the coupling
between lattice displacements and itinerant electrons, evaluating the adiabatic
forces during time evolution is computationally intensive, particularly for
large systems. To overcome this bottleneck, we develop a generalized
Behler-Parrinello neural-network architecture -- originally formulated for ab
initio molecular dynamics -- to accurately and efficiently predict forces from
local structural environments. Using the locality of electronic responses, the
resulting ML force field achieves linear scaling efficiency while maintaining
quantitative accuracy. Large-scale dynamical simulations using this framework
uncover a two-stage coarsening behavior of CDW domains: an early-time regime
characterized by a power-law growth $L \sim t^{\alpha}$ with an effective
exponent $\alpha \approx 0.7$, followed by a crossover to the Allen-Cahn
scaling $L \sim \sqrt{t}$ at late times. The enhanced early-time coarsening is
attributed to anisotropic domain-wall motion arising from electron-mediated
directional interactions. This work demonstrates the promise of ML-based force
fields for multiscale dynamical modeling of condensed-matter lattice models.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [52] [Enhanced Cyclic Coordinate Descent Methods for Elastic Net Penalized Linear Models](https://arxiv.org/abs/2510.19999)
*Yixiao Wang,Zishan Shao,Ting Jiang,Aditya Devarakonda*

Main category: stat.ML

TL;DR: Enhanced cyclic coordinate descent (ECCD) framework for generalized linear models with elastic net constraints achieves 3x speedup over state-of-the-art methods through Taylor expansion approximation and batched computations.


<details>
  <summary>Details</summary>
Motivation: To reduce training time for generalized linear models with elastic net constraints compared to existing methods, while avoiding convergence delay and numerical instability of block coordinate descent.

Method: Redesigns cyclic coordinate descent using Taylor expansion around current iterate to avoid nonlinear gradient operations, unrolls vector recurrences into efficient batched computations with tunable parameter s.

Result: Consistent 3x performance improvements on regularization path variant across diverse benchmark datasets, with s > 1 providing better performance without affecting convergence.

Conclusion: ECCD framework successfully accelerates training for generalized linear models with elastic net constraints while maintaining convergence properties and numerical stability.

Abstract: We present a novel enhanced cyclic coordinate descent (ECCD) framework for
solving generalized linear models with elastic net constraints that reduces
training time in comparison to existing state-of-the-art methods. We redesign
the CD method by performing a Taylor expansion around the current iterate to
avoid nonlinear operations arising in the gradient computation. By introducing
this approximation, we are able to unroll the vector recurrences occurring in
the CD method and reformulate the resulting computations into more efficient
batched computations. We show empirically that the recurrence can be unrolled
by a tunable integer parameter, $s$, such that $s > 1$ yields performance
improvements without affecting convergence, whereas $s = 1$ yields the original
CD method. A key advantage of ECCD is that it avoids the convergence delay and
numerical instability exhibited by block coordinate descent. Finally, we
implement our proposed method in C++ using Eigen to accelerate linear algebra
computations. Comparison of our method against existing state-of-the-art
solvers shows consistent performance improvements of $3\times$ in average for
regularization path variant on diverse benchmark datasets. Our implementation
is available at https://github.com/Yixiao-Wang-Stats/ECCD.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [53] [Simultaneously Solving Infinitely Many LQ Mean Field Games In Hilbert Spaces: The Power of Neural Operators](https://arxiv.org/abs/2510.20017)
*Dena Firoozi,Anastasis Kratsios,Xuwei Yang*

Main category: math.OC

TL;DR: Neural operators can learn to solve linear-quadratic mean-field games efficiently, providing statistical guarantees for solving unseen game variants even in infinite-dimensional settings.


<details>
  <summary>Details</summary>
Motivation: Traditional MFG solvers are inefficient for solving many related problems, especially when dealing with perturbations or continuum-parameterized agents, requiring a more scalable approach.

Method: Train neural operators to learn the rules-to-equilibrium map from problem data (dynamics and cost functionals) to equilibrium strategies, with controlled Lipschitz regularity and statistical guarantees.

Result: The approach provides statistical guarantees that neural operators trained on a small number of randomly sampled rules can reliably solve unseen LQ MFG variants, even in infinite-dimensional settings.

Conclusion: Neural operators offer an efficient and scalable solution for solving families of related mean-field games, with provable statistical guarantees and controlled parameter complexity.

Abstract: Traditional mean-field game (MFG) solvers operate on an instance-by-instance
basis, which becomes infeasible when many related problems must be solved
(e.g., for seeking a robust description of the solution under perturbations of
the dynamics or utilities, or in settings involving continuum-parameterized
agents.). We overcome this by training neural operators (NOs) to learn the
rules-to-equilibrium map from the problem data (``rules'': dynamics and cost
functionals) of LQ MFGs defined on separable Hilbert spaces to the
corresponding equilibrium strategy. Our main result is a statistical guarantee:
an NO trained on a small number of randomly sampled rules reliably solves
unseen LQ MFG variants, even in infinite-dimensional settings. The number of NO
parameters needed remains controlled under appropriate rule sampling during
training.
  Our guarantee follows from three results: (i) local-Lipschitz estimates for
the highly nonlinear rules-to-equilibrium map; (ii) a universal approximation
theorem using NOs with a prespecified Lipschitz regularity (unlike traditional
NO results where the NO's Lipschitz constant can diverge as the approximation
error vanishes); and (iii) new sample-complexity bounds for $L$-Lipschitz
learners in infinite dimensions, directly applicable as the Lipschitz constants
of our approximating NOs are controlled in (ii).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals](https://arxiv.org/abs/2510.19917)
*Trajan Murphy,Akshunna S. Dogra,Hanfeng Gu,Caleb Meredith,Mark Kon,Julio Enrique Castrillion-Candas*

Main category: cs.LG

TL;DR: FINDER is a classification framework for noisy datasets that uses stochastic analysis and Hilbert space mappings to create stochastic features, which are decomposed via KLE to enable classification through eigen-decomposition of class-specific operators.


<details>
  <summary>Details</summary>
Motivation: Noisy datasets with low signal-to-noise ratios, small sample sizes, and faulty data collection remain challenging for classification methods, requiring specialized approaches that can handle inherent randomness in empirical data.

Method: FINDER views datasets as realizations from random fields, maps them to Hilbert spaces to create stochastic features, uses Kosambi-Karhunen-Loève expansion for decomposition into irreducible components, and performs classification via eigen-decomposition of associated operators.

Result: FINDER achieved state-of-the-art breakthroughs in Alzheimer's Disease stage classification and remote sensing detection of deforestation, demonstrating superior performance in challenging, data-deficient scientific domains.

Conclusion: FINDER provides a rigorous framework for noisy dataset classification by incorporating stochastic analysis, with specific conditions where it outperforms existing methods, though it has identified failure modes and limitations.

Abstract: ''Noisy'' datasets (regimes with low signal to noise ratios, small sample
sizes, faulty data collection, etc) remain a key research frontier for
classification methods with both theoretical and practical implications. We
introduce FINDER, a rigorous framework for analyzing generic classification
problems, with tailored algorithms for noisy datasets. FINDER incorporates
fundamental stochastic analysis ideas into the feature learning and inference
stages to optimally account for the randomness inherent to all empirical
datasets. We construct ''stochastic features'' by first viewing empirical
datasets as realizations from an underlying random field (without assumptions
on its exact distribution) and then mapping them to appropriate Hilbert spaces.
The Kosambi-Karhunen-Lo\'eve expansion (KLE) breaks these stochastic features
into computable irreducible components, which allow classification over noisy
datasets via an eigen-decomposition: data from different classes resides in
distinct regions, identified by analyzing the spectrum of the associated
operators. We validate FINDER on several challenging, data-deficient scientific
domains, producing state of the art breakthroughs in: (i) Alzheimer's Disease
stage classification, (ii) Remote sensing detection of deforestation. We end
with a discussion on when FINDER is expected to outperform existing methods,
its failure modes, and other limitations.

</details>


### [55] [Alternatives to the Laplacian for Scalable Spectral Clustering with Group Fairness Constraints](https://arxiv.org/abs/2510.20220)
*Iván Ojeda-Ruiz,Young Ju-Lee,Malcolm Dickens,Leonardo Cambisaca*

Main category: cs.LG

TL;DR: The paper presents Fair-SMW, an efficient spectral clustering algorithm that incorporates group fairness constraints using Lagrangian methods and Sherman-Morrison-Woodbury identity to improve computational efficiency while maintaining fairness.


<details>
  <summary>Details</summary>
Motivation: Existing spectral clustering algorithms with fairness constraints suffer from high computational costs, particularly when incorporating group fairness (balance) metrics. The research aims to address this efficiency bottleneck while maintaining equitable outcomes.

Method: Developed Fair-SMW algorithm by reformulating constrained optimization using Lagrangian method and Sherman-Morrison-Woodbury identity. Used three Laplacian matrix alternatives with different spectral gaps to create multiple Fair-SMW variations.

Result: Fair-SMW achieved twice as fast computation time compared to state-of-the-art methods while maintaining comparable balance. Demonstrated flexibility to achieve twice as much balance when needed. Evaluated on real-world datasets including LastFM, FacebookNet, Deezer, and German using Stochastic Block Model.

Conclusion: The Fair-SMW algorithm successfully improves computational efficiency of fair spectral clustering while maintaining or enhancing fairness outcomes, providing a practical solution to the computational challenges of incorporating group fairness constraints.

Abstract: Recent research has focused on mitigating algorithmic bias in clustering by
incorporating fairness constraints into algorithmic design. Notions such as
disparate impact, community cohesion, and cost per population have been
implemented to enforce equitable outcomes. Among these, group fairness
(balance) ensures that each protected group is proportionally represented
within every cluster. However, incorporating balance as a metric of fairness
into spectral clustering algorithms has led to computational times that can be
improved. This study aims to enhance the efficiency of spectral clustering
algorithms by reformulating the constrained optimization problem using a new
formulation derived from the Lagrangian method and the
Sherman-Morrison-Woodbury (SMW) identity, resulting in the Fair-SMW algorithm.
Fair-SMW employs three alternatives to the Laplacian matrix with different
spectral gaps to generate multiple variations of Fair-SMW, achieving clustering
solutions with comparable balance to existing algorithms while offering
improved runtime performance. We present the results of Fair-SMW, evaluated
using the Stochastic Block Model (SBM) to measure both runtime efficiency and
balance across real-world network datasets, including LastFM, FacebookNet,
Deezer, and German. We achieve an improvement in computation time that is twice
as fast as the state-of-the-art, and also flexible enough to achieve twice as
much balance.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [56] [The global nonlinear stability of Minkowski spacetime with self-gravitating massive Dirac fields](https://arxiv.org/abs/2510.20626)
*Philippe G. LeFloch,Yue Ma,Weidong Zhang*

Main category: gr-qc

TL;DR: The paper establishes the gauge-invariant nonlinear stability of massive spinor fields in the Einstein-Dirac system, proving global existence and asymptotic behavior to Minkowski spacetime for initial data close to flat spacetime.


<details>
  <summary>Details</summary>
Motivation: Previous results were limited to the massless case of the Einstein-Dirac system. The authors aim to extend stability analysis to massive spinor fields, which requires new mathematical approaches due to the specific structure of spinor fields and Dirac equations.

Method: Uses the asymptotically hyperboloidal-Euclidean framework with light-bending wave coordinates. Employs Lorentz Clifford algebras, principal fiber bundles for gauge-invariant treatment. Derives L2 estimates, pointwise estimates, and new Sobolev inequalities for spinor fields. Establishes hierarchy of estimates distinguishing translations, rotations, and boosts.

Result: Proves the existence of a globally hyperbolic development that remains asymptotic to Minkowski spacetime in future timelike, null, and spacelike directions for sufficiently small initial data perturbations.

Conclusion: The gauge-invariant nonlinear stability of massive spinor fields in the Einstein-Dirac system is established, extending previous results from massless to massive cases with significant new mathematical developments.

Abstract: We consider the Einstein-Dirac system for a massive field, which describes
the evolution of self-gravitating massive spinor fields, and we investigate the
global evolution problem, when the initial data set is sufficiently close to
data describing a spacelike, asymptotically Euclidean slice of the Minkowski
spacetime. We establish the gauge-invariant nonlinear stability of such fields,
namely the existence of a globally hyperbolic development, which remains
asymptotic to Minkowski spacetime in future timelike, null, and spacelike
directions. Previous results on this problem have been limited to the
Einstein-Dirac system in the massless case. Our analysis follows the
asymptotically hyperboloidal-Euclidean framework introduced by LeFloch and Y.
Ma for the massive Klein-Gordon-Einstein system. The structure specific to
spinor fields and the Dirac equation necessitates significantly new elements in
the proof. In contrast with prior approaches, our treatment of spinor fields
and the Dirac equation is gauge-invariant, relying on the formalism of Lorentz
Clifford algebras, principal fiber bundles, etc. Our analysis is carried out
with the metric expressed in light-bending wave coordinates, as we call them.
This leads us to the study of a global existence problem for a system of wave
equations with constraints and a Klein-Gordon-type equations. We derive L2
estimates for the Dirac equation and its coupling with the Einstein equations,
along with $pointwise estimates. New Sobolev inequalities are proven for spinor
fields in a gauge-invariant manner in the hyperboloidal-Euclidean foliation.
The nonlinear coupling between the massive Dirac equation and the Einstein
equations is investigated, and we establish a hierarchy of estimates, which
distinguish between translations, rotations, and boosts.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [57] [Two Quantum Algorithms for Nonlinear Reaction-Diffusion Equation using Chebyshev Approximation Method](https://arxiv.org/abs/2510.19855)
*Manish Kumar*

Main category: quant-ph

TL;DR: Two new quantum algorithms for reaction-diffusion equations using truncated Chebyshev polynomial approximation, with improved gate complexities compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient quantum algorithms for solving reaction-diffusion equations by leveraging Chebyshev polynomial approximation and addressing the diagonalization of Carleman embedding matrices.

Method: Uses truncated Chebyshev polynomial approximation to solve linearized ODEs. First algorithm employs matrix exponentiation method, second uses quantum spectral method. Key technical contribution is deriving sufficient conditions for diagonalizing Carleman embedding matrix with an efficient iterative algorithm.

Result: First algorithm has gate complexity O(d·log(d)+T·polylog(T/ε)), second algorithm scales as O(polylog(d)·T·polylog(T/ε)), comparable to current best quantum algorithms. Provided efficient diagonalization method for Carleman matrix.

Conclusion: The approach offers significant speedup for quantum simulation of reaction-diffusion equations but has limitations regarding condition number bounds and relies on a conjecture about trigonometric equations, though mitigation strategies are provided.

Abstract: We present two new quantum algorithms for reaction-diffusion equations that
employ the truncated Chebyshev polynomial approximation. This method is
employed to numerically solve the ordinary differential equation emerging from
the linearization of the associated nonlinear differential equation. In the
first algorithm, we use the matrix exponentiation method (Patel et al., 2018),
while in the second algorithm, we repurpose the quantum spectral method (Childs
et al., 2020). Our main technical contribution is to derive the sufficient
conditions for the diagonalization of the Carleman embedding matrix, which is
indispensable for designing both quantum algorithms. We supplement this with an
efficient iterative algorithm to diagonalize the Carleman matrix.
  Our first algorithm has gate complexity of
O(d$\cdot$log(d)+T$\cdot$polylog(T/$\varepsilon$)). Here $d$ is the size of the
Carleman matrix, $T$ is the simulation time, and $\varepsilon$ is the
approximation error. The second algorithm is polynomial in $log(d)$, $T$, and
$log(1/\varepsilon)$ - the gate complexity scales as
O(polylog(d)$\cdot$T$\cdot$polylog(T/$\varepsilon$)). In terms of $T$ and
$\varepsilon$, this is comparable to the speedup gained by the current best
known quantum algorithm for this problem, the truncated Taylor series method
(Costa et.al., 2025).
  Our approach has two shortcomings. First, we have not provided an upper
bound, in terms of d, on the condition number of the Carleman matrix. Second,
the success of the diagonalization is based on a conjecture that a specific
trigonometric equation has no integral solution. However, we provide strategies
to mitigate these shortcomings in most practical cases.

</details>


### [58] [On Encoding Matrices using Quantum Circuits](https://arxiv.org/abs/2510.20030)
*Liron Mor Yosef,Haim Avron*

Main category: quant-ph

TL;DR: This paper systematically studies quantum circuit representations for matrices, focusing on block encodings and state preparation circuits. It provides efficient construction methods and shows bidirectional conversions between these representations, establishing their essential equivalence.


<details>
  <summary>Details</summary>
Motivation: Quantum computing promises superior algorithms for linear algebra problems, but efficient execution depends on proper quantum circuit representations of matrices and vectors. The paper aims to address the need for systematic methods to construct and convert between different quantum circuit representations.

Method: The paper examines methods for constructing block encodings and state preparation circuits from classical matrices. Key technical components include: (1) a special constant-depth multiplexer for higher-order Pauli matrices, and (2) quantum conversion algorithms between standard basis and Pauli basis expansions.

Result: Two main results: (a) an efficient method for constructing block encodings of arbitrary classical matrices, and (b) low-overhead bidirectional conversion algorithms between block encodings and state preparation circuits, demonstrating their essential equivalence.

Conclusion: The paper establishes that block encodings and state preparation circuits are essentially equivalent quantum circuit representations for matrices, with efficient construction and conversion methods that enable practical implementation of quantum linear algebra algorithms.

Abstract: Over a decade ago, it was demonstrated that quantum computing has the
potential to revolutionize numerical linear algebra by enabling algorithms with
complexity superior to what is classically achievable, e.g., the seminal HHL
algorithm for solving linear systems. Efficient execution of such algorithms
critically depends on representing inputs (matrices and vectors) as quantum
circuits that encode or implement these inputs. For that task, two common
circuit representations emerged in the literature: block encodings and state
preparation circuits. In this paper, we systematically study encodings matrices
in the form of block encodings and state preparation circuits. We examine
methods for constructing these representations from matrices given in classical
form, as well as quantum two-way conversions between circuit representations.
Two key results we establish (among others) are: (a) a general method for
efficiently constructing a block encoding of an arbitrary matrix given in
classical form (entries stored in classical random access memory); and (b)
low-overhead, bidirectional conversion algorithms between block encodings and
state preparation circuits, showing that these models are essentially
equivalent. From a technical perspective, two central components of our
constructions are: (i) a special constant-depth multiplexer that simultaneously
multiplexes all higher-order Pauli matrices of a given size, and (ii) an
algorithm for performing a quantum conversion between a matrix's expansion in
the standard basis and its expansion in the basis of higher-order Pauli
matrices.

</details>
