<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 29]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [math.PR](#math.PR) [Total: 2]
- [astro-ph.HE](#astro-ph.HE) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Computing excited states with isometric tensor networks in two-dimensions](https://arxiv.org/abs/2510.20063)
*Alec Dektor,Runze Chi,Roel Van Beeumen,Chao Yang*

Main category: math.NA

TL;DR: A new subspace iteration method using block-isoPEPS ansatz for computing excited states of 2D quantum many-body Hamiltonians, extending block MPS concepts to two dimensions.


<details>
  <summary>Details</summary>
Motivation: To develop efficient methods for computing low-lying eigenpairs (excited states) of high-dimensional quantum many-body Hamiltonians on 2D lattices, extending beyond the limitations of one-dimensional approaches.

Method: Proposes a block isometric projected entangled pair state (block-isoPEPS) ansatz that generalizes block MPS to 2D, featuring exact block orthogonalization, controlled local truncation via SVD, and efficient observable evaluation.

Result: Successfully computed excitations of 2D transverse-field Ising and Heisenberg models, demonstrating competitive performance compared to existing PEPS methods.

Conclusion: Block isometric tensor networks provide a scalable framework for studying excitations in quantum many-body systems beyond one dimension.

Abstract: We present a new subspace iteration method for computing low-lying eigenpairs
(excited states) of high-dimensional quantum many-body Hamiltonians with
nearest neighbor interactions on two-dimensional lattices. The method is based
on a new block isometric projected entangled pair state (block-isoPEPS) ansatz
that generalizes the block matrix product state (MPS) framework, widely used
for Hamiltonians defined on one-dimensional chains, to two-dimensions. The
proposed block-isoPEPS ansatz offers several attractive features for PEPS-based
algorithms, including exact block orthogonalization, controlled local
truncation via singular value decompositions, and efficient evaluation of
observables. We demonstrate the proposed inexact subspace iteration for
block-isoPEPS by computing excitations of the two-dimensional transverse-field
Ising and Heisenberg models and compare our results with existing PEPS methods.
Our results demonstrate that block isometric tensor networks provide a scalable
framework for studying excitations in quantum many-body systems beyond one
dimension.

</details>


### [2] [Minimizing Residuals in ODE Integration Using Optimal Control](https://arxiv.org/abs/2510.20117)
*Robert M. Corless,C. Yalçın Kaya*

Main category: math.NA

TL;DR: The paper studies fitting interpolating curves through ODE solver solution points (skeleton) by minimizing ODE residual norms, formulating it as a multi-stage optimal control problem and solving analytically for test cases and numerically for Van der Pol equation.


<details>
  <summary>Details</summary>
Motivation: To improve interpolation accuracy between discrete solution points generated by ODE solvers by minimizing the residual error of the original differential equation.

Method: Reformulate interpolation as multi-stage optimal control problem, apply maximum principle for optimality conditions, solve analytically for test problems (Dahlquist, leaky bucket) and numerically for Van der Pol equation using optimization software.

Result: Obtained interpolating curves with minimal residual norms, compared favorably against MATLAB's deval function for various ODE solvers.

Conclusion: The proposed optimal control approach provides more accurate interpolation with smaller residual errors compared to standard interpolation methods like MATLAB's deval.

Abstract: Given the set of discrete solution points or nodes, called the skeleton,
generated by an ODE solver, we study the problem of fitting a curve passing
through the nodes in the skeleton minimizing a norm of the residual vector of
the ODE. We reformulate this interpolation problem as a multi-stage optimal
control problem and, for the minimization of two different norms, we apply the
associated maximum principle to obtain the necessary conditions of optimality.
We solve the problem analytically for the Dahlquist test problem and a variant
of the leaky bucket problem, in terms of the given skeleton. We also consider
the Van der Pol equation, for which we obtain interpolating curves with minimal
residual norms by numerically solving a direct discretization of the problem
through optimization software. With the skeletons obtained by various ODE
solvers of MATLAB, we make comparisons between the residuals obtained by our
approach and those obtained by the MATLAB function deval.

</details>


### [3] [Joint Signal Recovery and Uncertainty Quantification via the Residual Prior Transform](https://arxiv.org/abs/2510.20136)
*Yao Xiao,Anne Gelb*

Main category: math.NA

TL;DR: The paper reformulates the residual transform operator into a hierarchical Bayesian prior, enabling uncertainty quantification and joint signal recovery from multimodal measurements without requiring prior structural information.


<details>
  <summary>Details</summary>
Motivation: Conventional signal recovery priors assume fixed signal variability types, which is problematic for complex signals with different behaviors across domains. The residual transform operator reduces variability-dependent error without requiring structural priors.

Method: Reformulates the residual transform operator into a hierarchical Bayesian prior framework, enabling principled uncertainty quantification and joint recovery from multimodal data.

Result: Numerical experiments show the residual prior yields high-fidelity signal and image recovery from multimodal data while providing robust uncertainty quantification.

Conclusion: The Bayesian reformulation of the residual transform operator provides a powerful framework for uncertainty-aware signal recovery that can handle complex signals with varying behaviors and fuse information from multimodal measurements.

Abstract: Conventional priors used for signal recovery are often limited by the
assumption that the type of a signal's variability, such as piecewise constant
or linear behavior, is known and fixed. This assumption is problematic for
complex signals that exhibit different behaviors across the domain. The
recently developed {\em residual transform operator} effectively reduces such
variability-dependent error within the LASSO regression framework. Importantly,
it does not require prior information regarding structure of the underlying
signal. This paper reformulates the residual transform operator into a new
prior within a hierarchical Bayesian framework. In so doing, it unlocks two
powerful new capabilities. First, it enables principled uncertainty
quantification, providing robust credible intervals for the recovered signal,
and second, it provides a natural framework for the joint recovery of signals
from multimodal measurements by coherently fusing information from disparate
data sources. Numerical experiments demonstrate that the residual prior yields
high-fidelity signal and image recovery from multimodal data while providing
robust uncertainty quantification.

</details>


### [4] [General transformation neural networks: A class of parametrized functions for high-dimensional function approximation](https://arxiv.org/abs/2510.20142)
*Xiaoyang Wang,Yiqi Gu*

Main category: math.NA

TL;DR: GTNNs are novel neural networks with generalized transformations (cubic/quadratic) that improve accuracy for oscillatory functions compared to conventional networks.


<details>
  <summary>Details</summary>
Motivation: Conventional neural networks perform poorly on oscillatory functions under gradient descent training, needing better approximation capabilities.

Method: Generalize affine transformations to cubic (CTNNs) and quadratic (QTNNs) transformations as complex shape functions with larger capacity.

Result: GTNNs show universal approximation properties, error bounds for smooth functions, and outperform conventional networks in regression and PDE problems.

Conclusion: CTNNs/QTNNs provide superior accuracy and robustness for high-dimensional approximation of oscillatory functions.

Abstract: We propose a novel class of neural network-like parametrized functions, i.e.,
general transformation neural networks (GTNNs), for high-dimensional
approximation. Conventional deep neural networks sometimes perform less
accurately in approximation problems under gradient descent training,
especially when the target function is oscillatory. To improve accuracy, we
generalize the affine transformation of the abstract neuron to more general
functions, which act as complex shape functions and have larger capacities.
Specifically, we introduce two types of GTNNs: the cubic and quadratic
transformation neural networks (CTNNs and QTNNs). We perform approximation
error analysis for CTNNs and QTNNs, presenting their universal approximation
properties for continuous functions and error bounds for smooth functions and
Barron-type functions. Several numerical examples of regression problems and
partial differential equations are presented, demonstrating that CTNNs/QTNNs
have advantages in accuracy and robustness over conventional fully connected
neural networks.

</details>


### [5] [IEnSF: Iterative Ensemble Score Filter for Reducing Error in Posterior Score Estimation in Nonlinear Data Assimilation](https://arxiv.org/abs/2510.20159)
*Zezhong Zhang,Feng Bao,Guannan Zhang*

Main category: math.NA

TL;DR: The paper introduces an iterative ensemble score filter (IEnSF) that improves upon the Ensemble Score Filter by using an iterative algorithm to reduce posterior score estimation errors in nonlinear data assimilation problems.


<details>
  <summary>Details</summary>
Motivation: The current EnSF method uses a heuristic weighted sum to combine prior and likelihood scores, which introduces structural errors in nonlinear settings. This work aims to address this limitation.

Method: Developed IEnSF that applies an iterative algorithm around the reverse-time SDE solver to gradually reduce posterior score estimation error by improving approximation of conditional expectation of likelihood score function.

Result: Numerical experiments show IEnSF substantially reduces posterior score estimation error in nonlinear settings and improves accuracy of tracking high-dimensional dynamical systems.

Conclusion: The iterative approach in IEnSF effectively addresses the structural error in EnSF, making it more suitable for nonlinear data assimilation problems with high-dimensional systems.

Abstract: The Ensemble Score Filter (EnSF) has emerged as a promising approach to
leverage score-based diffusion models for solving high-dimensional and
nonlinear data assimilation problems. While initial applications of EnSF to the
Lorenz-96 model and the quasi-geostrophic system showed potential, the current
method employs a heuristic weighted sum to combine the prior and the likelihood
score functions. This introduces a structural error into the estimation of the
posterior score function in the nonlinear setting. This work addresses this
challenge by developing an iterative ensemble score filter (IEnSF) that applies
an iterative algorithm as an outer loop around the reverse-time stochastic
differential equation solver. When the state dynamics or the observation
operator is nonlinear, the iterative algorithm can gradually reduce the
posterior score estimation error by improving the accuracy of approximating the
conditional expectation of the likelihood score function. The number of
iterations required depends on the distance between the prior and posterior
distributions and the nonlinearity of the observation operator. Numerical
experiments demonstrate that the IEnSF algorithm substantially reduces the
error in posterior score estimation in the nonlinear setting and thus improves
the accuracy of tracking high-dimensional dynamical systems.

</details>


### [6] [Anderson-type acceleration method for Deep Neural Network optimization](https://arxiv.org/abs/2510.20254)
*Kazufumi Ito,Tiancheng Xue*

Main category: math.NA

TL;DR: Anderson-type acceleration method for stochastic gradient descent improves neural network performance for DNN and CNN, with applications in computer tomography and inverse medium problems.


<details>
  <summary>Details</summary>
Motivation: To enhance neural network optimization for deep neural networks (DNN) and convolutional neural networks (CNN) by improving the stochastic gradient descent method.

Method: Developed an Anderson-type acceleration method for stochastic gradient descent optimization.

Result: The method significantly improves neural network performance for both DNN and CNN architectures.

Conclusion: The Anderson-accelerated stochastic gradient descent method is applicable to neural network design for computer tomography and inverse medium problems.

Abstract: In this paper we consider the neural network optimization for DNN. We develop
Anderson-type acceleration method for the stochastic gradient decent method and
it improves the network permanence very much. We demonstrate the applicability
of the method for DNN and CNN. We discuss the application of the general class
of the neural network design for computer tomography and inverse medium
problems.

</details>


### [7] [Unique continuation for the wave equation: the stability landscape](https://arxiv.org/abs/2510.20359)
*Erik Burman,Lauri Oksanen,Janosch Preuss,Ziyao Zhao*

Main category: math.NA

TL;DR: This paper studies unique continuation for wave equations using volumetric data, proving Hölder stability in subsets and Lipschitz stability with finite-dimensional boundary trace knowledge, leading to a convergent finite element method.


<details>
  <summary>Details</summary>
Motivation: To address unique continuation problems for wave equations when data is only available in volumetric subsets rather than on boundaries, which has practical applications in inverse problems and control theory.

Method: Theoretical analysis of unique continuation properties for wave equations, proving stability results under different data conditions, and designing a finite element method based on these stability properties.

Result: Proved Hölder stability for continuation into proper subsets without lateral boundary data, and Lipschitz stability for full continuation when the lateral boundary trace lies in a finite-dimensional space.

Conclusion: The stability results enable the design of a finite element method that converges to the exact solution with rates matching the continuous problem's stability properties.

Abstract: We consider a unique continuation problem for the wave equation given data in
a volumetric subset of the space time domain. In the absence of data on the
lateral boundary of the space-time cylinder we prove that the solution can be
continued with H\"older stability into a certain proper subset of the
space-time domain. Additionally, we show that unique continuation of the
solution to the entire space-time cylinder with Lipschitz stability is possible
given the knowledge of a suitable finite dimensional space in which the trace
of the solution on the lateral boundary is contained. These results allow us to
design a finite element method that provably converges to the exact solution at
a rate that mirrors the stability properties of the continuous problem.

</details>


### [8] [Improving the accuracy of meshless methods via resolving power optimisation using multiple kernels](https://arxiv.org/abs/2510.20365)
*H. Broadley,J. R. C. King,S. J. Lind*

Main category: math.NA

TL;DR: A framework for optimizing resolving power in meshless methods by using linear combinations of kernels to maximize accuracy over a range of wavenumbers, improving accuracy for turbulent flow simulations without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: Meshless methods are widely used for turbulent flow simulations with complex geometries, but their accuracy has been assessed mainly through polynomial consistency rather than resolving power, which is crucial for capturing short spatial scales in turbulence.

Method: Developed a framework using linear combinations of kernels to maximize resolving power over a range of wavenumbers, addressing the orientation-dependent resolving power characteristic of meshless methods.

Result: The optimized approach shows improved accuracy in convergence tests, maintains stability in time-dependent problems, and provides significant accuracy gains for various PDE systems without increasing computational cost per timestep.

Conclusion: The optimization procedure enables accurate simulation of PDE systems with short spatial scales, particularly beneficial for turbulent flow fields like homogeneous isotropic turbulence.

Abstract: Meshless methods are commonly used to determine numerical solutions to
partial differential equations (PDEs) for problems involving free surfaces
and/or complex geometries, approximating spatial derivatives at collocation
points via local kernels with a finite size. Despite their common use in
turbulent flow simulations, the accuracy of meshless methods has typically been
assessed using their convergence characteristics resulting from the polynomial
consistency of approximations to operators, with little to no attention paid to
the resolving power of the approximation. Here we provide a framework for the
optimisation of resolving power by exploiting the non-uniqueness of kernels to
provide improvements to numerical approximations of spatial derivatives. We
first demonstrate that, unlike in finite-difference approximations, the
resolving power of meshless methods is dependent not only on the magnitude of
the wavenumber, but also its orientation, before using linear combinations of
kernels to maximise resolving power over a range of wavenumbers. The new
approach shows improved accuracy in convergence tests and has little impact on
stability of time-dependent problems for a range of Eulerian meshless methods.
Solutions to a variety of PDE systems are computed, with significant gains in
accuracy for no extra computational cost per timestep in Eulerian frameworks.
The improved resolution characteristics provided by the optimisation procedure
presented herein enable accurate simulation of systems of PDEs whose solution
contains short spatial scales such as flow fields with homogeneous isotropic
turbulence.

</details>


### [9] [Projecting onto the Unit Dual Quaternion Set](https://arxiv.org/abs/2510.20425)
*Ziyang Li,Chunfeng Cui,Jiaxin Xie*

Main category: math.NA

TL;DR: This paper systematically studies projections onto unit dual quaternion sets under the 2^R-norm, identifying distinct cases based on standard-dual part relationships and demonstrating algorithm effectiveness through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: Dual quaternions have wide applications in multi-agent formation control, 3D motion modeling, and robotics, making the fundamental problem of projection onto unit dual quaternion sets important for practical applications.

Method: The study systematically analyzes projections under the 2^R-norm, identifying several distinct cases based on the relationship between standard and dual parts in vector form, and proposes an algorithm for these projections.

Result: Numerical experiments demonstrate the effectiveness of the proposed projection algorithm for unit dual quaternion sets.

Conclusion: The systematic study provides effective projection methods for dual quaternions under the 2^R-norm, with practical applications in robotics and motion modeling.

Abstract: Dual quaternions have gained significant attention due to their wide
applications in areas such as multi-agent formation control, 3D motion
modeling, and robotics. A fundamental aspect in dual quaternion research
involves the projection onto unit dual quaternion sets. In this paper, we
systematically study such projections under the $2^R$-norm, which is commonly
used in practical applications. We identify several distinct cases based on the
relationship between the standard and dual parts in vector form, and
demonstrate the effectiveness of the proposed algorithm through numerical
experiments.

</details>


### [10] [Well-Posedness and Approximation of Weak Solutions to Time Dependent Maxwell's Equations with $L^2$-Data](https://arxiv.org/abs/2510.20752)
*Harbir Antil*

Main category: math.NA

TL;DR: Analysis of Maxwell's equations in conducting media with perfectly conducting boundaries, proving well-posedness and developing a structure-preserving finite element method.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for Maxwell's equations in rough media with Lipschitz domains, addressing challenges with rough coefficients and L²-data.

Method: Direct proof of well-posedness using interior-in-time mollification for uniqueness and Galerkin method for existence. Development of structure-preserving semi-discrete finite element method based on Nédélec/Raviart-Thomas de Rham complex.

Result: Proved well-posedness (existence, uniqueness, energy identity, continuous dependence) and developed stable finite element scheme preserving discrete Gauss law with convergence guarantees.

Conclusion: Provides complete mathematical framework for Maxwell's equations in conducting media with rough coefficients, including both theoretical well-posedness results and practical numerical methods with proven convergence.

Abstract: We study Maxwell's equations in conducting media with perfectly conducting
boundary conditions on Lipschitz domains, allowing rough material coefficients
and $L^2$-data. Our first contribution is a direct proof of well-posedness of
the first-order weak formulation, including solution existence and uniqueness,
an energy identity, and continuous dependence on the data. The argument uses
interior-in-time mollification to show uniqueness while avoiding reflection
techniques. Existence is via the well-known Galerkin method (cf.~Duvaut and
Lions \cite[Eqns.~(4.31)--(4.32), p.~346; Thm.~4.1]{GDuvaut_JLLions_1976a}).
For completeness, and to make the paper self-contained, a complete proof has
been provided.
  Our second contribution is a structure-preserving semi-discrete finite
element method based on the N\'ed\'elec/Raviart--Thomas de Rham complex. The
scheme preserves a discrete Gauss law for all times and satisfies a
continuous-in-time energy identity with stability for nonnegative conductivity.
With a divergence-free initialization of the magnetic field (via potential
reconstruction or constrained $L^2$ projection), we prove convergence of the
semi-discrete solutions to the unique weak solution as the mesh is refined. The
analysis mostly relies on projector consistency, weak-* compactness in
time-bounded $L^2$ spaces, and identification of time derivatives in dual
spaces.

</details>


### [11] [Preconditioning of a pollution-free discretization of the Helmholtz equation](https://arxiv.org/abs/2510.20564)
*Harald Monsuur*

Main category: math.NA

TL;DR: A pollution-free FOSLS formulation for Helmholtz equation with block preconditioner that achieves linear iteration scaling with wave number.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and pollution-free iterative solver for Helmholtz equations that works on general domains and scattering problems.

Method: First order system least squares formulation with block preconditioner using Schur complement preconditioner and Hermitian positive definite test space preconditioner via subspace correction.

Result: Numerical experiments show linear dependence of MINRES iterations on wave number κ, with algebraic error estimation preventing unnecessary iterations.

Conclusion: The method provides an easy-to-implement, pollution-free solver for Helmholtz equations that scales well with wave number and works on general domains.

Abstract: We present a pollution-free first order system least squares (FOSLS)
formulation for the Helmholtz equation, solved iteratively using a block
preconditioner. This preconditioner consists of two components: one for the
Schur complement, which corresponds to a preconditioner on $L_2(\Omega)$, and
another defined on the test space, which we ensure remains Hermitian positive
definite using subspace correction techniques. The proposed method is easy to
implement and is directly applicable to general domains, including scattering
problems. Numerical experiments demonstrate a linear dependence of the number
of MINRES iterations on the wave number $\kappa$. We also introduce an approach
to estimate algebraic errors which prevents unnecessary iterations.

</details>


### [12] [Advancing Offshore Renewable Energy: Techno-Economic and Dynamic Performance of Hybrid Wind-Wave Systems](https://arxiv.org/abs/2510.20601)
*Alaa Ahmed,Maha N. Haji*

Main category: math.NA

TL;DR: Hybrid offshore wind-wave energy systems combining floating wind turbines with wave energy converters can reduce power fluctuations by 50% and lower wave energy costs by 15-83% while maintaining wind energy costs.


<details>
  <summary>Details</summary>
Motivation: Offshore wind has matured but wave energy remains costly; integrating both technologies can enhance power generation, stabilize output, and reduce costs through mutualistic benefits.

Method: Analyzed six configurations combining RM3 wave energy converter with 5MW and 15MW wind turbines on spar and semi-submersible platforms, examining dynamic response, mooring loads, and power production under varying conditions.

Result: Reaction plate improves damping for spar platform, increasing wave energy absorption. Integration reduces wave energy LCOE by 15-83% while wind LCOE unaffected. Power fluctuations reduced by 50%. 5MW wind shows reduced LCOE, 15MW shows slight increase.

Conclusion: Hybrid systems create mutualistic relationship where wave energy benefits substantially while wind experiences slight improvements or negligible effects, demonstrating potential to lower costs and support sustainable energy solutions.

Abstract: Offshore wind and wave energy offer high energy density and availability.
While offshore wind has matured significantly, wave energy remains costly and
under development. Integrating both technologies into a hybrid system can
enhance power generation, stabilize output, and reduce costs. This study
explores the benefits of combining an offshore floating wind turbine with the
two-body heaving point absorber wave energy converter, Reference Model 3 (RM3).
Six configurations are analyzed: RM3 integrated with the National Renewable
Energy Laboratory 5 MW and the International Energy Agency 15 MW wind turbines,
each tested on both spar and semi-submersible platforms. The analysis examines
dynamic response, mooring loads, and power production under varying
environmental conditions, considering the influence of the wave energy
converter float motion and an optional reaction plate. Results indicate that
the reaction plate improves damping for the spar platform, enhancing wave
energy absorption and power output. A comparative analysis indicates that
integrating the wave energy converter reduces its levelized cost of energy by
15-83%, while leaving the wind turbine levelized cost of energy unaffected.
Hybridization significantly reduces power fluctuations by 50%, reduces the
levelized cost of energy with the 5 MW wind turbine, and slightly increases it
with the 15 MW wind turbine. The results highlight a mutualistic relationship
between the wave energy converter and the offshore wind turbine, where the
former benefits substantially while the latter experiences slight improvements
or negligible effects. Additional findings quantify hydrodynamic interactions,
mooring performance, and economic feasibility. This research provides insights
into optimizing hybrid offshore renewable systems, demonstrating their
potential to lower costs and support sustainable energy solutions.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [Non-uniqueness and failure of Calderón-Zygmund estimates below the critical exponent for non-monotone PDE with linear growth](https://arxiv.org/abs/2510.20024)
*Akshara Vincent*

Main category: math.AP

TL;DR: Counterexamples to uniqueness and a priori Calderón-Zygmund estimates for solutions below L^2 using convex integration for div(A(∇u))=0 in 2D balls with smooth, elliptic, linear-growth but non-monotone A.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that without monotonicity, even for smooth uniformly elliptic operators with linear growth, uniqueness of solutions and Calderón-Zygmund estimates can fail for solutions below L^2 regularity.

Method: Uses convex integration arguments to construct counterexamples for the equation div(A(∇u))=0 in 2D balls, where A is smooth, uniformly elliptic, has essentially linear growth but lacks monotonicity.

Result: Provides explicit counterexamples showing non-uniqueness of solutions and failure of a priori Calderón-Zygmund estimates for solutions with regularity below L^2.

Conclusion: Monotonicity is crucial for uniqueness and Calderón-Zygmund estimates in elliptic PDEs; without it, these fundamental properties can fail even for smooth elliptic operators with linear growth.

Abstract: We provide counterexamples to uniqueness of solutions as well as a priori
Calder\'on-Zygmund estimates for solutions below $L^2$ using convex integration
argument for equations of the type $$ \text{div} (A (\nabla u)) = 0 \quad
\text{in } \mathbb{B}^2, $$ where $A: \mathbb{R}^{2} \to \mathbb{R}^2$ is
smooth, uniformly elliptic and has essentially linear growth, but fails to be
monotone.

</details>


### [14] [Well-posedness for a class of parabolic equations with singular-degenerate coefficients](https://arxiv.org/abs/2510.20051)
*Junyuan Fang,Tuoc Phan*

Main category: math.AP

TL;DR: This paper studies linear parabolic equations with measurable coefficients in divergence form, where heat capacity coefficients can be degenerate/singular and belong to Muckenhoupt weight classes. It introduces weighted parabolic cylinders and Sobolev spaces, proving regularity estimates, existence, and uniqueness of weak solutions under small oscillation assumptions.


<details>
  <summary>Details</summary>
Motivation: To address linear parabolic equations with coefficients that can be degenerate, singular, or both, which arise in various physical applications but present analytical challenges due to their irregular nature.

Method: Uses the level-set method by Caffarelli and Peral, introduces weighted parabolic cylinders with non-homogeneous quasi-distance, and develops weighted parabolic Sobolev spaces. Establishes weighted inequalities and a weighted Aubin-Lions compactness theorem.

Result: Proves regularity estimates, existence, and uniqueness of weak solutions in the weighted Sobolev spaces under small mean oscillation assumptions on coefficients.

Conclusion: The framework successfully handles degenerate and singular parabolic equations through weighted analysis, providing existence, uniqueness, and regularity results for weak solutions in appropriately defined weighted Sobolev spaces.

Abstract: This paper studies a class of linear parabolic equations with measurable
coefficients in divergence form whose volumetric heat capacity coefficients are
assumed to be in some Muckenhoupt class of weights. As such, the coefficients
can be degenerate, singular, or both degenerate and singular. A class of
weighted parabolic cylinders with a non-homogeneous quasi-distance function,
and a class of weighted parabolic Sobolev spaces intrinsically suitable for the
class of equations are introduced. Under some smallness assumptions on the mean
oscillations of the coefficients, regularity estimates, existence, and
uniqueness of weak solutions in the weighted Sobolev spaces are proved. To
achieve the results, we apply the level-set method introduced by Caffarelli and
Peral. Several weighted inequalities and a version of weighted Aubin-Lions
compactness theorem for sequences in weighted parabolic Sobolev spaces are
established.

</details>


### [15] [Time-periodic solutions to the cubic wave equation: an elementary constructive approach](https://arxiv.org/abs/2510.20054)
*Filip Ficek*

Main category: math.AP

TL;DR: Elementary proof of infinite family of time-periodic solutions for 1D cubic wave equation with Dirichlet BCs using perturbative expansion and Banach contraction principle.


<details>
  <summary>Details</summary>
Motivation: To provide existence proof for infinite family of time-periodic solutions with explicit frequency and structure information, improving on previous results.

Method: First order perturbative expansion combined with Banach contraction principle to show existence of nearby solutions.

Result: Existence of infinite family of time-periodic solutions with explicit frequency and structural information.

Conclusion: The approach successfully demonstrates existence while providing explicit details about solution frequencies and structures.

Abstract: We present an elementary proof of existence of infinite family of
time-periodic solutions to the one-dimensional nonlinear cubic wave equation
with Dirichlet boundary conditions. It relies on the first order perturbative
expansion and uses the Banach contraction principle to show existence of nearby
solutions. In contrast to the previous results, this approach provides us
explicit information about the frequencies and structures of the obtained
solutions.

</details>


### [16] [Existence and qualitative properties of ground state solutions for the Schrödinger-Bopp-Podolsky system](https://arxiv.org/abs/2510.20143)
*Sheng Wang,Juan Huang*

Main category: math.AP

TL;DR: Existence and properties of solutions to the Schrödinger-Bopp-Podolsky system, including nontrivial solutions via mountain-pass lemma, ground state solutions, and their positivity, symmetry, and decay properties.


<details>
  <summary>Details</summary>
Motivation: Study the Schrödinger field coupled with its electromagnetic field in Bopp-Podolsky theory under electrostatic conditions, addressing the existence and characteristics of solutions to this nonlinear nonlocal PDE system.

Method: Applied mountain-pass lemma to obtain nontrivial solutions, used ground state energy estimates to prove existence of ground state solutions, analyzed relationship between solutions and critical point paths, and examined positivity, radial symmetry, rotational invariance, and exponential decay properties.

Result: Proved existence of nontrivial solutions and ground state solutions, demonstrated these are ground states of mountain-pass type, established positivity, radial symmetry, rotational invariance, and exponential decay of ground state solutions, and analyzed asymptotic behavior with respect to parameter a in radial case.

Conclusion: Successfully established comprehensive existence and qualitative properties of solutions to the Schrödinger-Bopp-Podolsky system, providing a complete characterization including mountain-pass type ground states with specific symmetry and decay properties.

Abstract: This paper concerns the existence and related properties of solutions to the
Schr\"{o}dinger-Bopp-Podolsky system, which reduces to a nonlinear and nonlocal
partial differential equation describing a Schr\"{o}dinger field coupled with
its electromagnetic field in Bopp-Podolsky theory under purely electrostatic
conditions. Firstly, by applying the mountain-pass lemma, we obtain the
existence of nontrivial solutions. Then, through some estimates of the ground
state energy, we prove the existence of ground state solutions. By exploring
the relationship between solutions and paths associated with critical points,
we further demonstrate that the obtained solutions are ground states of
mountain-pass type. Additionally, the positivity, radial symmetry, rotational
invariance, and exponential decay of the ground state solutions are considered.
Finally, in the radial case, we explore the asymptotic behavior of the obtained
solutions with respect to $a$.

</details>


### [17] [Asymptotic issue for fractional laplacian on long cylinders](https://arxiv.org/abs/2510.20263)
*Tahir Boudjeriou,Prosenjit Roy*

Main category: math.AP

TL;DR: Analysis of weak solutions to fractional p-Laplacian problems in unbounded cylindrical domains, extending local results to nonlocal setting.


<details>
  <summary>Details</summary>
Motivation: Study asymptotic behavior of solutions to elliptic and parabolic problems with fractional p-Laplacian in domains unbounded in one direction, addressing technical challenges from nonlocal nature.

Method: Developed a nonlocal abstract framework to analyze weak solutions, overcoming technical difficulties created by the nonlocal operator.

Result: Main results extend and complement related properties previously established in the local setting for similar problems.

Conclusion: The nonlocal abstract framework successfully handles the technical challenges of fractional p-Laplacian problems in unbounded cylindrical domains, providing extensions to local theory.

Abstract: In this paper, we are concerned with the asymptotic behavior of weak
solutions to certain elliptic and parabolic problems involving the fractional
$p$-Laplacian in cylindrical domains that become unbounded in one direction.
The nonlocal nature of the operator describing the equations creates several
technical difficulties in treating problems of this type. The main results,
obtained within a nonlocal abstract framework, extend and complement related
properties established in the local setting.

</details>


### [18] [Qualitative Behavior of Solutions to a Forced Nonlocal Thin-Film Equation](https://arxiv.org/abs/2510.20289)
*Jinhong Zhao,Bin Guo*

Main category: math.AP

TL;DR: Global existence and long-time behavior of weak solutions for a 1D nonlocal degenerate fourth-order parabolic equation with inhomogeneous forces, relevant to hydraulic fracture modeling.


<details>
  <summary>Details</summary>
Motivation: Study hydraulic fracture modeling through a degenerate fourth-order parabolic equation with inhomogeneous forces, addressing both time-dependent and time-independent cases.

Method: Employ regularization scheme, modified energy/entropy methods, and novel differential inequality techniques to analyze weak solutions.

Result: For time-dependent force S(t,x), solution converges in H^s to spatial average plus time integral of force. For time-independent S(x), difference between solution and linear function remains uniformly bounded in H^s.

Conclusion: Established global existence and precise long-time behavior results for weak solutions under both time-dependent and time-independent inhomogeneous forces in hydraulic fracture modeling.

Abstract: We study a one-dimensional nonlocal degenerate fourth-order parabolic
equation with inhomogeneous forces relevant to hydraulic fracture modeling.
Employing a regularization scheme, modified energy/entropy methods, and novel
differential inequality techniques, we establish global existence and long-time
behavior results for weak solutions under both time-dependent and
time-independent inhomogeneous forces. Specifically, for the time-dependent
force $S(t, x)$, we prove that the solution converges in $H^s (\Omega )$ to
$\bar{u}_0+\frac{1}{|\Omega|}\int_0^t \int_\Omega S(r, x)\, dxdr $, where
$\bar{u}_0=\frac{1}{|\Omega|}\int_{\Omega}u_{0}(x)\,dx$ is the spatial average
of the initial data. For the time-independent force $S(x)$, we prove that the
difference between the weak solution and the linear function $\bar{u}_0 +
\frac{t}{|\Omega|}\int_\Omega S(x)\, dx$ remains uniformly bounded in $H^s
(\Omega )$.

</details>


### [19] [On an Analytical Criterion for Detecting Intermittent Turbulent Behaviour of Solutions of Partial Differential Equations](https://arxiv.org/abs/2510.20290)
*Michele V Bartuccelli,Guido Gentile*

Main category: math.AP

TL;DR: The paper provides an analytical criterion based on the crest factor to distinguish between turbulent and non-turbulent solutions in partial differential equations, particularly focusing on intermittent turbulence.


<details>
  <summary>Details</summary>
Motivation: To understand the nature of solutions in PDEs and determine whether they exhibit turbulent or non-turbulent behavior, which would advance the comprehension of turbulence.

Method: Using the crest factor as an analytical criterion to analyze solutions of classical linear and nonlinear equations.

Result: The crest factor criterion successfully distinguishes between solutions showing time-intermittent turbulence and those that are non-turbulent or exhibit statistically stationary turbulence (like in Kolmogorov's theory).

Conclusion: The crest factor provides an effective analytical tool for identifying intermittent turbulent behavior in PDE solutions, contributing to better understanding of turbulence nature.

Abstract: A main question in the study of partial differential equations is the
following: how do we understand the nature of the solutions and, in particular,
how do we determine if a given solution shows turbulent or non-turbulent
behaviour? Being able to answer such a question would be a major advance in the
comprehension of the nature of turbulence. In this paper we focus on the case
of intermittent turbulence and provide an analytical criterion, based on the
crest factor, which captures the essential feature of the solutions. By
computing the crest factor for the solutions of some classical equations, both
linear and nonlinear, we illustrate the capability of the criterion for
discerning between solutions exhibiting time-intermittent turbulence behaviour
and solutions which either are not turbulent or show statistically stationary
turbulence, like, for example, in the case described by Kolmogorov's theory.

</details>


### [20] [Nonlinear stability of a composite wave to the Cauchy problem of 1-D full compressible Navier-Stokes-Allen-Cahn system](https://arxiv.org/abs/2510.20298)
*Dan Lei,Zhengzheng Chen*

Main category: math.AP

TL;DR: The paper studies the large time behavior of solutions to the 1D compressible Navier-Stokes-Allen-Cahn system, proving global existence and convergence to composite rarefaction waves when the corresponding Euler system admits such solutions.


<details>
  <summary>Details</summary>
Motivation: To understand the long-term behavior of mixtures of two immiscible viscous compressible fluids modeled by the compressible Navier-Stokes-Allen-Cahn system.

Method: Using an elementary energy method that accounts for the phase field variable and nonlinear wave complexity, with initial perturbations and rarefaction wave strength allowed to be arbitrarily large.

Result: Proved existence of unique global strong solutions that converge to composite 1-rarefaction and 3-rarefaction waves as time goes to infinity, when the adiabatic exponent γ is close to 1.

Conclusion: The compressible Navier-Stokes-Allen-Cahn system exhibits stable long-term behavior converging to composite rarefaction wave solutions under appropriate conditions, with large initial perturbations allowed.

Abstract: The compressible Navier-Stokes-Allen-Cahn system models the motion of a
mixture of two macroscopically immiscible viscous compressible fluids. In this
paper, we are concerned with the large time behavior of solutions to the Cauchy
problem of the one-dimensional full compressible Navier-Stokes-Allen-Cahn
system. If the Riemann problem of the corresponding Euler system admits a
solution which is a linear combination of 1-rarefaction wave and 3-rarefaction
wave, we proved that a global strong solution to the compressible
Navier-Stokes-Allen-Cahn system exists uniquely and converges to the above
composite wave as time goes to infinity, provided that the adiabatic exponent
$\gamma$ is closed to $1$. Here the initial perturbations except for the
temperature function of the fluid, and the strength of rarefaction waves can be
arbitrarily large. The proof is given by an elementary energy method that takes
into account the effect of the phase field variable $\chi(t,x)$ and the
complexity of nonlinear waves.

</details>


### [21] [Continuous data assimilation applied to the Rayleigh-Benard problem for compressible fluid flows](https://arxiv.org/abs/2510.20316)
*Eduard Feireisl,Wladimir Neves*

Main category: math.AP

TL;DR: Continuous data assimilation applied to Navier-Stokes-Fourier system for compressible rotating thermally driven fluid with rigorous tracking property proof in low Mach/high Rossby-Froude regime.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze continuous data assimilation methods for complex fluid systems, specifically addressing compressible rotating thermally driven fluids under realistic physical conditions.

Method: Applied continuous data assimilation to Navier-Stokes-Fourier system, conducted rigorous mathematical analysis in asymptotic regime of low Mach numbers and high Rossby/Froude numbers.

Result: Successfully proved tracking property for large data within weak solutions framework, demonstrating the method's effectiveness in complex fluid dynamics scenarios.

Conclusion: Continuous data assimilation is viable for tracking compressible rotating thermally driven fluids, with mathematical guarantees established for the tracking property under specified asymptotic conditions.

Abstract: We apply a continuous data assimilation method to the Navier-Stokes-Fourier
system governing the evolution of a compressible, rotating and thermally driven
fluid. A rigorous proof of the tracking property is given in the asymptotic
regime of low Mach and high Rossby and Froude numbers. Large data in the
framework of weak solutions are considered.

</details>


### [22] [Energy Decay in Measure Time: HUM Observability, Product-Exponential Envelopes, and GCC Calibration](https://arxiv.org/abs/2510.20371)
*Ben F. Tibola*

Main category: math.AP

TL;DR: The paper introduces a measure-valued clock (sigma) to unify continuous damping and impulsive exposure patterns, proving that energy decays at a product-exponential rate with respect to sigma rather than wall-clock time t.


<details>
  <summary>Details</summary>
Motivation: Past attempts to unify continuous damping with impulses using wall-clock time t failed because there's no uniform exponential energy law for impulsive exposure patterns in t.

Method: Replace wall-clock time t with a measure-valued clock sigma that aggregates continuous exposure and atomic doses within a single Lyapunov ledger, applying the Hilbert Uniqueness Method (HUM) framework.

Result: Proves an observability-dissipation principle: there exists a structural constant c_sigma > 0 such that energy decays at least at a product-exponential rate with respect to sigma. The framework unifies intermittent regimes and provides monotonicity principles.

Conclusion: The main contribution is a qualitative dynamics backbone showing that observability implies sigma-exponential decay with sharp constants, which persists under discretizations, variational limits, and stochastic extensions.

Abstract: We prove that for impulsive exposure patterns there is no uniform exponential
energy law in wall-clock time t, which explains why past t-based unifications
of continuous damping with impulses fail. We therefore replace t by a
measure-valued clock, sigma, that aggregates absolutely continuous exposure and
atomic doses within a single Lyapunov ledger. On this ledger we prove an
observability-dissipation principle in the sense of the Hilbert Uniqueness
Method (HUM): there exists a structural constant c_sigma > 0 such that the
energy decays at least at a product-exponential rate with respect to sigma.
When sigma = t, the statement reduces to classical exponential stabilization
with the same constant. For the damped wave under the Geometric Control
Condition (GCC), the constant is calibrated by the usual observability and
geometric factors. The framework yields a monotonicity principle ("more
sigma-mass implies faster decay") and unifies intermittent regimes where
quiescent intervals are punctuated by impulses. As robustness, secondary to the
main contribution, the same decay law persists under structure-compatible
discretizations and along compact variational limits; a stochastic extension
supplies expectation and pathwise envelopes via the compensator. The
contribution is a qualitative dynamics backbone: observability implies
sigma-exponential decay with sharp constants.

</details>


### [23] [Optimal quantitative stability estimates for Alexandrov's Soap Bubble Theorem via Gagliardo-Nirenberg-type interpolation inequalities](https://arxiv.org/abs/2510.20399)
*João Gonçalves da Silva,Giorgio Poggesi*

Main category: math.AP

TL;DR: Optimal quantitative stability estimates for Alexandrov's Soap Bubble Theorem using Gagliardo-Nirenberg interpolation inequalities, establishing uniform closeness to a ball for mean curvature deviations.


<details>
  <summary>Details</summary>
Motivation: To provide optimal stability estimates for the Alexandrov's Soap Bubble Theorem across various regularity classes and deviation measures, addressing gaps in existing literature.

Method: Leveraging Gagliardo-Nirenberg-type interpolation inequalities to establish stability estimates for C^{k,α} domains, with analysis of L^r deviations of mean curvature from constant.

Result: Found linear stability profile for r > (N-1)/2, and new non-linear profiles for r ≥ (N-1)/2. Stability improves with increasing k, becoming formally linear as k → ∞. All estimates are proven optimal.

Conclusion: The paper provides complete optimal quantitative stability estimates for Alexandrov's theorem, revealing new stability profiles and demonstrating improvement with domain regularity.

Abstract: The paper provides optimal quantitative stability estimates for the
celebrated Alexandrov's Soap Bubble Theorem within the class of $C^{k,\alpha}$
domains, for any $k \ge 1$ and $0 < \alpha \le 1$, by leveraging
Gagliardo-Nirenberg-type interpolation inequalities.
  Optimal estimates of uniform closeness to a ball are established for $L^r$
deviations of the mean curvature from being constant, for any $r\ge 2$ (more
generally, for any $r>1$ such that $r\ge (2N-2)/(N+1)$).
  For $r>\frac{N-1}{2}$, the stability profile is linear, thus returning the
existing results established in the literature through computations for nearly
spherical sets. Surprisingly, all the stability estimates for $r\ge
\frac{N-1}{2}$, for which the profile is not linear, are new; even in the
particular case $r=2$ (which has been extensively studied, since it is a case
of interest for several critical applications), the sharp stability profile
that we obtain is new. Interestingly, we also prove that the (non-linear)
profile for $r \ge \frac{N-1}{2}$ improves as $k$ becomes larger to such an
extent that it becomes formally linear as $k$ goes to $\infty$.
  Finally, for any $r$, we show that all our estimates are optimal for any $k
\ge 1$ and $0< \alpha \le 1$, by providing explicit examples.

</details>


### [24] [Hölder regularity for a class of doubly non linear PDEs](https://arxiv.org/abs/2510.20432)
*Filippo Maria Cassanello,Eurica Henriques*

Main category: math.AP

TL;DR: Local Hölder continuity is proven for non-negative, locally bounded weak solutions to doubly nonlinear parabolic equations with p > 2 and 0 < q < p-1.


<details>
  <summary>Details</summary>
Motivation: To establish regularity properties for solutions to doubly nonlinear parabolic equations, which are important in various physical and mathematical contexts.

Method: The proof uses expansion of positivity results, DeGiorgi-type lemmas, an alternative approach, and an exponential shift to handle the intrinsic geometry of the problem.

Result: Local Hölder continuity is established for non-negative, locally bounded weak solutions to the given class of doubly nonlinear parabolic equations.

Conclusion: The combination of expansion of positivity, DeGiorgi-type techniques, and geometric considerations successfully proves local Hölder continuity for solutions to these doubly nonlinear parabolic equations.

Abstract: We prove local H\"older continuity for non negative, locally bounded, local
weak solutions to the class of doubly nonlinear parabolic equations $\partial_t
(u_q) - \text{div} (|Du|^{p-2} Du) = 0$ for $p > 2$, $ 0 < q < p-1$. The proof
relies on expansion of positivity results combined with the study of an
alternative (related to DeGiorgi-type lemmas) and an exponential shift which
allows us to deal with the intrinsic geometry associated to the problem.

</details>


### [25] [Global bifurcation of solutions to elliptic systems with system and domain symmetries](https://arxiv.org/abs/2510.20462)
*Piotr Stefaniak*

Main category: math.AP

TL;DR: Existence of continua of nontrivial solutions bifurcating from constant solutions in parameterized elliptic systems on symmetric domains with system symmetries, using equivariant gradient map degree theory.


<details>
  <summary>Details</summary>
Motivation: To study parameterized elliptic systems on symmetric domains with additional system symmetries and prove existence of bifurcating solutions without assuming nondegeneracy conditions.

Method: Using the degree for equivariant gradient maps to prove existence of continua of nontrivial solutions bifurcating from constant branches determined by critical points of the potential.

Result: Proved existence of continua of nontrivial solutions that break symmetry at every nonzero level when the domain is a compact symmetric space. Under additional assumptions, the continua are unbounded.

Conclusion: The approach provides existence results for bifurcating solutions in symmetric elliptic systems without nondegeneracy assumptions, with symmetry breaking behavior and potential unbounded continua.

Abstract: We study parameterized elliptic systems on symmetric domains with additional
system symmetries. We prove the existence of continua of nontrivial solutions
bifurcating from the constant branch determined by a critical point of the
potential, without assuming nondegeneracy, via the degree for equivariant
gradient maps. Our assumptions are formulated in terms of the right-hand side.
When the domain is a compact symmetric space, the bifurcating solutions break
symmetry at every nonzero level. Under additional assumptions on the right-hand
side, the continua are unbounded.

</details>


### [26] [On dissipative turbulent solutions to the compressible anisotropic Navier-Stokes equations in unbounded domains](https://arxiv.org/abs/2510.20476)
*Ondřej Kreml,Šárka Nečasová,Tong Tang*

Main category: math.AP

TL;DR: Global existence of dissipative turbulent solutions for compressible Navier-Stokes equations with anisotropic viscous stress tensor on unbounded domains, with relaxed assumptions and weak-strong uniqueness.


<details>
  <summary>Details</summary>
Motivation: To extend the existence results for compressible Navier-Stokes equations to unbounded domains, relax assumptions on anisotropic tensor coefficients and pressure law, and address geophysical applications.

Method: Using the concept of dissipative turbulent solutions to prove global existence on unbounded domains, complementing Bresch and Jabin's compactness method approach on bounded domains.

Result: Established global existence of dissipative turbulent solutions on a large class of unbounded domains with relaxed coefficient assumptions, and proved weak-strong uniqueness property.

Conclusion: The work provides existence results for anisotropic compressible Navier-Stokes equations on unbounded domains relevant to geophysical contexts, with improved assumptions and uniqueness properties.

Abstract: Inspired by Abbatiello, Feireisl and Novotn\'y, we prove the global existence
of dissipative turbulent solution for the compressible Navier-Stokes equations
with anisotropic viscous stress tensor on unbounded domain. Our work
complements the result of Bresch and Jabin, where the authors used the new
compactness method to prove the existence of a weak solution to the same system
in $\mathbb{T}^3$. By virtue of the concept of dissipative turbulent solutions,
we are able to relax assumptions on the anisotropic tensor coefficients and the
pressure law coefficient. We point out that we establish the existence result
on a large class of unbounded domains, which is more conform to geophysical
context. We also prove the weak-strong uniqueness property of acquired
dissipative turbulent solutions.

</details>


### [27] [Non-optimal domains for the helicity maximisation problem](https://arxiv.org/abs/2510.20533)
*Wadim Gerner*

Main category: math.AP

TL;DR: This paper extends previous work on the helicity isoperimetric problem by establishing additional geometric constraints that optimal domains must satisfy, ruling out optimality for many solid tori.


<details>
  <summary>Details</summary>
Motivation: To further constrain the possible shapes that could maximize Biot-Savart helicity among smooth domains of fixed volume, building on previous work that showed optimal domains must have toroidal boundary components.

Method: The authors establish additional geometric constraints that any optimal domain must satisfy, using mathematical analysis of the helicity isoperimetric problem.

Result: The additional geometric constraints allow ruling out the optimality of a broad class of solid tori, narrowing down the possible candidates for optimal domains.

Conclusion: While significant progress is made in constraining the possible optimal domains, the actual existence of optimal domains remains an open problem in the field.

Abstract: In [J. Cantarella, D. DeTurck, H. Gluck and M. Teytel, J. Math. Phys. 41:5615
(2000)] the helicity isoperimetric problem which asks to find a smooth domain
of fixed volume which maximises Biot-Savart helicity among all other smooth
domains of fixed volume was initiated. It was shown that if an optimal domain
exists, all of its boundary components must be tori.
  The present work extends these results by establishing additional geometric
constraints which optimal domains, if they exist, must satisfy. This allows to
rule out the optimality of a broad class of solid tori. The existence of
optimal domains remains an open problem.

</details>


### [28] [Homogenization, dimension reduction and linearization of thin elastic plate](https://arxiv.org/abs/2510.20573)
*Amartya Chakrabortty,Georges Griso,Julia Orlik*

Main category: math.AP

TL;DR: This paper studies the homogenization, dimension reduction, and linearization of composite plates in non-linear elasticity, showing that the limit energy remains unchanged regardless of the order of taking limits (ε→0 and h→0).


<details>
  <summary>Details</summary>
Motivation: To investigate the simultaneous homogenization, dimension reduction, and linearization of composite plates under external loading in non-linear elasticity framework, without coupling assumptions between the homogenization parameter ε and plate thickness h.

Method: The paper uses Γ-convergence technique to analyze the limit behavior as (ε,h)→(0,0). It decomposes plate deformations and displacements, and extends results to periodic perforated plates. The approach includes two parts: simultaneous limits and rigorous derivation of linearized elasticity from non-linear elasticity.

Result: The limit energy remains unchanged whether taking h→0 first followed by ε→0, or taking both limits simultaneously. The exact form of limit energy is obtained, and existence of unique solution for the limit linearized homogenized energy problem is demonstrated.

Conclusion: The paper successfully establishes that the order of taking limits (ε→0 and h→0) does not affect the final limit energy in composite plate homogenization problems, providing rigorous mathematical foundation for linearized homogenized energy problems in composite plates.

Abstract: This paper investigates the homogenization, dimension reduction, and
linearization of a composite plate subjected to external loading within the
framework of non-linear elasticity problem. The total elastic energy of the
problem is of order $\sim h^2\varepsilon^{2a+3}$, where $a\geq1$. The paper is
divided into two parts: The first part presents the simultaneous
homogenization, dimension reduction and linearization
($(\varepsilon,h)\to(0,0)$) of a composite plate without any coupling
assumption of $\varepsilon$ and $h$. The second part consists of the rigorous
derivation of linearized elasticity as a limit of non-linear elasticity with
small deformation and external loading conditions. The results obtained
demonstrate that the limit energy remains unchanged when the first
linearization ($h\to 0$) is performed, followed by simultaneous homogenization
dimension reduction ($\varepsilon\to0$) and when both limits approach zero
simultaneously, i.e. $(\varepsilon,h)\to (0,0)$. The exact form of the limit
energy(s) is obtained through the decomposition of plate deformations and plate
displacements. By using the $\Gamma$-convergence technique, the existence of a
unique solution for the limit linearized homogenized energy problem is
demonstrated. These results are then extended to certain periodic perforated
plates.

</details>


### [29] [Rothe's method in direct and time-dependent inverse source problems for a semilinear pseudo-parabolic equation](https://arxiv.org/abs/2510.20642)
*Karel Van Bockstal,Khonatbek Khompysh,Arshyn Altybay*

Main category: math.AP

TL;DR: The paper studies how to determine an unknown time-dependent source term in a semilinear pseudo-parabolic equation using additional measurement data.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of recovering unknown time-dependent source terms in semilinear pseudo-parabolic equations with variable coefficients, which has applications in various physical and engineering problems.

Method: Uses Rothe's time-discretisation method to prove existence and uniqueness of weak solutions, and develops a numerical scheme for computations.

Result: Proves existence and uniqueness of weak solutions under smallness conditions on problem data, and provides a computational framework.

Conclusion: The proposed approach successfully addresses the inverse source problem for semilinear pseudo-parabolic equations with rigorous mathematical analysis and practical numerical implementation.

Abstract: In this paper, we investigate the inverse problem of determining an unknown
time-dependent source term in a semilinear pseudo-parabolic equation with
variable coefficients and a Dirichlet boundary condition. The unknown source
term is recovered from additional measurement data expressed as a weighted
spatial average of the solution. By employing Rothe's time-discretisation
method, we prove the existence and uniqueness of a weak solution under a
smallness condition on the problem data. We also present a numerical scheme for
computations.

</details>


### [30] [Weak sequential stability of solutions to a nonisothermal kinetic model for incompressible dilute polymeric fluids](https://arxiv.org/abs/2510.20580)
*Miroslav Bulíček,Josef Málek,Endre Süli*

Main category: math.AP

TL;DR: Mathematical analysis of thermodynamically consistent kinetic models for nonisothermal flows of dilute polymeric fluids, showing convergence to global-in-time large-data weak solutions.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous mathematical analysis of kinetic models for nonisothermal polymeric fluid flows that are thermodynamically consistent, identifying energy storage and entropy production mechanisms.

Method: Analysis of nonlinear PDE system coupling temperature-dependent Navier-Stokes equations with temperature-dependent Fokker-Planck equation and temperature evolution equation. Construction of sequences of smooth solutions satisfying uniform bounds.

Result: Sequences of smooth solutions converge to global-in-time large-data weak solutions satisfying energy inequality, with temperature satisfying renormalized variational inequality, ensuring weak sequential stability.

Conclusion: The mathematical model for nonisothermal polymeric fluid flows is weakly sequentially stable and admits global-in-time large-data weak solutions that satisfy appropriate energy and variational inequalities.

Abstract: The paper is concerned with the mathematical analysis of a class of
thermodynamically consistent kinetic models for nonisothermal flows of dilute
polymeric fluids, based on the identification of energy storage mechanisms and
entropy production mechanisms in the fluid under consideration. The model
involves a system of nonlinear partial differential equations coupling the
unsteady incompressible temperature-dependent Navier--Stokes equations to a
temperature-dependent generalization of the classical Fokker--Planck equation
and an evolution equation for the absolute temperature. Sequences of smooth
solutions to the initial-boundary-value problem, satisfying the available
bounds that are uniform with respect to the given data of the model, are shown
to converge to a global-in-time large-data weak solution that satisfies an
energy inequality, where the absolute temperature satisfies a renormalized
variational inequality, implying weak sequential stability of the mathematical
model.

</details>


### [31] [Uniqueness and continuous dependence for a viscoelastic problem with memory in domains with time dependent cracks](https://arxiv.org/abs/2510.20583)
*Federico Cianci,Gianni Dal Maso*

Main category: math.AP

TL;DR: Analysis of hyperbolic partial integro-differential systems in domains with time-dependent cracks, focusing on uniqueness and continuous dependence conditions.


<details>
  <summary>Details</summary>
Motivation: To understand how time-dependent cracks affect the behavior of hyperbolic partial integro-differential systems and establish conditions for well-posedness.

Method: Studied hyperbolic partial integro-differential systems in domains with time-dependent cracks, developed conditions on crack behavior.

Result: Established conditions that ensure uniqueness of solutions with prescribed initial-boundary conditions and continuous dependence on crack evolution.

Conclusion: The paper provides mathematical conditions that guarantee well-posedness for hyperbolic systems in domains with evolving cracks, addressing both uniqueness and stability.

Abstract: We study some hyperbolic partial integro-differential systems in domains with
time dependent cracks. In particular, we give conditions on the cracks which
imply the uniqueness of the solution with prescribed initial-boundary
conditions, and its continuous dependence on the cracks.

</details>


### [32] [Dynamic crack growth in viscoelastic materials with memory](https://arxiv.org/abs/2510.20599)
*Federico Cianci*

Main category: math.AP

TL;DR: A dynamic crack growth model in viscoelastic materials with history-dependent damping, based on energy dissipation balance and maximal dissipation conditions.


<details>
  <summary>Details</summary>
Motivation: To develop a mathematical model for dynamic crack propagation in viscoelastic materials where damping depends on deformation history, addressing complex fracture mechanics.

Method: Based on dynamic energy dissipation balance and maximal dissipation condition, with existence analysis in two dimensions under regularity constraints on cracks.

Result: Proved an existence theorem for the dynamic crack growth model in dimension two with a priori regularity constraints on cracks.

Conclusion: The paper establishes mathematical existence of solutions for dynamic crack growth in viscoelastic materials with history-dependent damping under specific regularity conditions.

Abstract: In this paper we introduce a model of dynamic crack growth in viscoelastic
material, where the damping term depends on the history of the deformation. The
model is based on a dynamic energy dissipation balance and on a maximal
dissipation condition. Our main result is an existence theorem in dimension two
under some a priori regularity constraints on the cracks.

</details>


### [33] [Analysis of coupled Maxwell-cable problems](https://arxiv.org/abs/2510.20619)
*Timo Reis,Nathanael Skrepek*

Main category: math.AP

TL;DR: Analysis of qualitative properties for a coupled telegrapher's and Maxwell's equations model describing electromagnetic field interactions with radiating curved cables.


<details>
  <summary>Details</summary>
Motivation: To establish mathematical foundations and qualitative properties for a recently developed model of electromagnetic field interactions with radiating curved cables, building on previous work.

Method: Analyzing the dynamical system properties using semigroup theory and establishing sufficient conditions for well-posedness of the coupled telegrapher's and Maxwell's equations model.

Result: Showed that autonomous dynamics generate a strongly continuous semigroup and established sufficient conditions for continuous dependence of state and output trajectories on inputs and initial conditions.

Conclusion: The model exhibits mathematically sound properties with well-posed dynamics, providing rigorous foundation for studying electromagnetic interactions with radiating cables.

Abstract: Building on the recently published work "Modeling of radiating curved cables
via coupled telegrapher's and Maxwell's equations", which introduces a model
for the interaction between electromagnetic fields and radiating (possibly
curved) cables, we analyze the qualitative properties of the resulting
dynamical system. The model features inputs and outputs given by the currents
and voltages at the cable ends, while the state comprises the corresponding
distributions along the cables and the electromagnetic fields in the
surrounding domain. We show that the autonomous dynamics (i.e., with zero
input) generate a strongly continuous semigroup and establish sufficient
conditions for well-posedness, meaning continuous dependence of the state and
output trajectories on the inputs and initial conditions.

</details>


### [34] [Dimension reduction for time-dependent von Kármán rods](https://arxiv.org/abs/2510.20623)
*Federico Cianci,Bernd Schmidt*

Main category: math.AP

TL;DR: Convergence of 3D nonlinear elastodynamics solutions for thin rods to 1D von Kármán equations as cross-section shrinks, with energy dissipation from torsional vibrations.


<details>
  <summary>Details</summary>
Motivation: To understand how 3D nonlinear elasticity solutions for thin rods converge to reduced 1D models when cross-section approaches zero, particularly for displacements comparable to rod radius.

Method: Assume existence of solutions and control torsional velocity, then analyze convergence to dimensionally reduced model (time-dependent von Kármán equations for 1D rod).

Result: Solutions converge to effective 1D von Kármán equations; high-frequency torsional vibrations cause energy dissipation in the limit, adding extra terms to limiting equations.

Conclusion: 3D nonlinear elastodynamics for thin rods converges to 1D von Kármán equations with additional dissipation terms from torsional vibrations when cross-section shrinks to zero.

Abstract: This paper aims to study the convergence of solutions in three-dimensional
nonlinear elastodynamics for a thin rod as its cross section shrinks to zero
for displacements that are comparable to the small radius of the rod. Assuming
the existence of solutions and proper control of the torsional velocity, we
show how these converge to the solutions of an effective dimensionally reduced
model which is a version of the the time dependent von K\'arm\'an equations for
a one-dimensional rod. In the presence of high-frequency torsional vibrations,
energy can dissipate in the limit and we obtain additional contributions in the
limiting equations.

</details>


### [35] [Nonrelativistic limit of bound-state solutions for nonlinear Dirac equation on noncompact quantum graphs](https://arxiv.org/abs/2510.20658)
*Guangze Gu,Michael Ruzhansky,Guoyan Wei,Zhipeng Yang*

Main category: math.AP

TL;DR: This paper studies bound-state solutions of the nonlinear Dirac equation on noncompact quantum graphs, proving their existence, convergence to nonlinear Schrödinger equation solutions in the nonrelativistic limit, and uniform boundedness with exponential decay.


<details>
  <summary>Details</summary>
Motivation: To understand the nonrelativistic limit behavior and qualitative properties of bound-state solutions for the nonlinear Dirac equation on noncompact quantum graphs, bridging relativistic and nonrelativistic quantum mechanics.

Method: Mathematical analysis of the nonlinear Dirac equation on noncompact quantum graphs, establishing existence proofs for bound-state solutions and studying their convergence properties as the speed of light approaches infinity.

Result: Proved existence of bound-state solutions to the nonlinear Dirac equation on noncompact quantum graphs, demonstrated their convergence to nonlinear Schrödinger equation solutions in the nonrelativistic limit, and established uniform boundedness with exponential decay properties.

Conclusion: The nonlinear Dirac equation on noncompact quantum graphs admits bound-state solutions that converge to nonlinear Schrödinger equation solutions in the nonrelativistic limit, with uniform boundedness and exponential decay properties providing insight into their asymptotic behavior.

Abstract: In this paper, we investigate the nonrelativistic limit and qualitative
properties of bound-state solutions for the nonlinear Dirac equation (NLDE)
defined on noncompact quantum graphs: \[ -i c \frac{d}{d x} \sigma_1 \psi+m c^2
\sigma_3 \psi-\omega \psi=g(|\psi|) \psi, \quad \text { in } \mathcal{G} \]
where \( g : \mathbb{R}\rightarrow\mathbb{R} \) is a continuous nonlinear
function, \( c>0 \) represents the speed of light, \( m>0 \) is the particle's
mass, \( \omega\in\mathbb{R} \) is related to the frequency, \( \sigma_1 \) and
\( \sigma_3 \) denote the Pauli matrices, and \(\mathcal{G}\) is a noncompact
quantum graph. We establish the existence of bound-state solutions to the NLDE
on \(\mathcal{G}\), and prove that these solutions converge toward the
corresponding bound-state solutions of a nonlinear Schr\"odinger equation (NLS)
in the nonrelativistic limit (i.e., as the speed of light \( c \to \infty \))
for particles of small mass. Furthermore, we prove uniform boundedness and
exponential decay properties of the NLDE solutions, uniformly in \( c \),
thereby offering insight into their asymptotic behavior.

</details>


### [36] [The Cauchy problem for $p$-evolution equations with variable coefficients in Gelfand-Shilov spaces](https://arxiv.org/abs/2510.20702)
*Marco Cappiello,Eliakim Cleyton Machado*

Main category: math.AP

TL;DR: The paper studies the Cauchy problem for linear evolution equations of arbitrary order with time-space dependent coefficients, proving well-posedness in Gelfand-Shilov spaces under decay conditions on lower-order coefficients.


<details>
  <summary>Details</summary>
Motivation: To establish well-posedness results for linear evolution equations with variable coefficients in Gelfand-Shilov spaces, which are suitable for studying equations with irregular coefficients.

Method: The authors analyze the Cauchy problem for linear evolution equations with time-space dependent coefficients, imposing decay conditions on lower-order terms as |x|→∞.

Result: A well-posedness result is proven in Gelfand-Shilov spaces under suitable decay assumptions on the coefficients.

Conclusion: The paper successfully establishes well-posedness for a broad class of linear evolution equations with variable coefficients in Gelfand-Shilov spaces.

Abstract: We study the Cauchy problem for a class of linear evolution equations of
arbitrary order with coefficients depending both on time and space variables.
Under suitable decay assumptions on the coefficients of the lower order terms
for $|x|$ large, we prove a well-posedness result in Gelfand-Shilov spaces.

</details>


### [37] [Large field problem in coercive singular PDEs](https://arxiv.org/abs/2510.20716)
*Ilya Chevyrev,Massimiliano Gubinelli*

Main category: math.AP

TL;DR: The paper derives a priori estimates for singular differential equations with irregular distributions, using rough path theory for time derivatives and regularity structures for heat operators, with local estimates independent of boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for analyzing singular differential equations with irregular forcing terms, particularly in subcritical regimes where traditional methods fail.

Method: Uses rough path theory for time derivative equations and regularity structures for heat operator equations, with an abstract scaling argument to convert local coercivity to global estimates.

Result: Develops local a priori estimates that are independent of boundary conditions and can handle irregular distributions, with a key result showing how to extend local coercivity to global estimates through scaling.

Conclusion: The framework provides robust estimates for singular differential equations in subcritical regimes, reducing complex problems to cases with small irregular forcing terms through scaling arguments.

Abstract: We derive a priori estimates for singular differential equations of the form
\[ \mathcal{L} \phi = P(\phi,\nabla\phi) + f(\phi,\nabla\phi)\xi \] where $P$
is a polynomial, $f$ is a sufficiently well-behaved function, and $\xi$ is an
irregular distribution such that the equation is subcritical. The differential
operator $\mathcal L$ is either a derivative in time, in which case we
interpret the equation using rough path theory, or a heat operator, in which
case we interpret the equation using regularity structures. Our only assumption
on $P$ is that solutions with $\xi=0$ exhibit coercivity. Our estimates are
local in space and time, and independent of boundary conditions.
  One of our main results is an abstract estimate that allows one to pass from
a local coercivity property to a global one using scaling, for a large class of
equations. This allows us to reduce the problem of deriving a priori estimates
to the case when $\xi$ is small.

</details>


### [38] [First Critical Field in the pinned three-dimensional Ginzburg--Landau Model: A matching upper bound](https://arxiv.org/abs/2510.20720)
*Carlos Román*

Main category: math.AP

TL;DR: The paper establishes a matching upper bound for the first critical field H_c1 in 3D extreme type-II superconductors with pinning, confirming the sharpness of previous lower bounds and connecting vorticity onset to a weighted isoflux problem.


<details>
  <summary>Details</summary>
Motivation: To complete the characterization of H_c1 by providing an upper bound that matches the previously established lower bound, thereby determining its leading-order behavior and confirming the connection between vorticity onset and the isoflux problem.

Method: Uses the Biot-Savart law-based upper bound construction from previous work, applied to the 3D magnetic Ginzburg-Landau functional with pinning term a_ε.

Result: Successfully establishes a matching upper bound for H_c1, identifying its leading-order behavior and confirming the sharpness of previous lower bounds.

Conclusion: The upper bound construction completes the characterization of H_c1, validates the connection between vorticity onset and the weighted isoflux problem, and confirms the sharpness of the previously derived lower bound.

Abstract: We continue our study of the first critical field $H_{c_1}$ for extreme
type-II superconductors governed by the three-dimensional magnetic
Ginzburg--Landau functional with a pinning term $a_\varepsilon$, as introduced
in our previous work [arXiv:2507.10915]. Building upon the lower bound for
$H_{c_1}$ and the characterization of the Meissner solution, we now establish a
matching upper bound for $H_{c_1}$, thereby identifying its leading-order
behavior. This result confirms the sharpness of the previously derived lower
bound and further elucidates the connection between the onset of vorticity and
a weighted variant of the \emph{isoflux problem}. Our argument is prompted by
the upper bound construction we developed in [arXiv:2510.14910], based on the
Biot--Savart law.

</details>


### [39] [Quantitative classification of potential Navier-Stokes singularities beyond the blow-up time](https://arxiv.org/abs/2510.20757)
*Tobias Barker*

Main category: math.AP

TL;DR: This paper provides the first quantitative classification of potentially singular solutions to 3D Navier-Stokes equations for approximately axisymmetric initial data, with bounds that can be numerically tested.


<details>
  <summary>Details</summary>
Motivation: To investigate the viability of numerical candidates for singular solutions of 3D Navier-Stokes equations, particularly motivated by Hou's numerical candidate.

Method: Established improved quantitative regions of regularity for approximately axisymmetric initial data, combined with improved quantitative energy estimates. Used quantitative Carleman inequality arguments recursively with careful bookkeeping to avoid exponential losses.

Result: Achieved quantitative classification of potentially singular solutions at any given time in the region of potential blow-up times, with quantitative bounds amenable to numerical testing.

Conclusion: The approach provides quantitative lower bounds on solutions near potential blow-up times, enabling numerical verification of potential singularities in 3D Navier-Stokes equations.

Abstract: In \cite{hou}, Hou gave a compelling numerical candidate for a singular
solution of the 3D Navier-Stokes equations. We pioneer classifications of
potentially singular solutions, motivated by the issue of investigating the
viability of numerical candidates.For approximately axisymmetric initial data,
we give the first quantitative classification of potentially singular solutions
at \textit{any} given time in the region of potential blow-up times. Moreover,
the quantitative bounds in the vicinity of any potential blow-up time are in
principle amenable to numerical testing. To achieve this, we establish improved
quantitative regions of regularity for approximately axisymmetric initial data,
which may be of independent interest. Together with improved quantitative
energy estimates from \cite{TB24}, this allows us to get a quantitative lower
bound in the vicinity of a blow-up time by implementing the strategy of
\cite{BP21}, which is a physical space analogue of Tao's strategy \cite{Ta21}
for producing quantitative estimates for critically bounded solutions. To
obtain a quantitative lower bound on the solution at any time in the region of
potential blow-up times, we recursively apply quantitative Carleman inequality
arguments from \cite{Ta21}. This necessitates careful bookkeeping to avoid
exponential losses and to ensure that all forward-in-time iterations of
(localized) vorticity concentration remain within the region of quantitative
regularity of the solution.

</details>


### [40] [Ill-Posedness of the 2D Euler Equations in a Logarithmically Refined Critical Sobolev Space](https://arxiv.org/abs/2510.20773)
*Elaine Cozzi,Nicholas Harrison,Zachary Radke*

Main category: math.AP

TL;DR: The paper extends Bourgain-Li's ill-posedness results for 2D Euler equations to logarithmically regularized spaces that are strictly smaller than H² but contain H^s for all s>2, showing strong ill-posedness when the logarithmic derivative power α ≤ 1/2.


<details>
  <summary>Details</summary>
Motivation: To generalize the known ill-posedness results for 2D Euler equations beyond the critical Sobolev space H², exploring finer scales of regularity through logarithmic modifications.

Method: Construct logarithmically regularized spaces using fractional logarithmic derivatives applied to the critical Sobolev norm, then analyze ill-posedness properties.

Result: Proved that 2D Euler equations are strongly ill-posed in these logarithmically regularized spaces when the logarithmic derivative power α ≤ 1/2.

Conclusion: The ill-posedness phenomenon for 2D Euler equations persists in spaces that are strictly contained in H² but contain all H^s spaces for s>2, with the threshold at α=1/2.

Abstract: In their seminal work, Bourgain and Li establish strong ill-posedness of the
2D Euler equations for initial velocity in the critical Sobolev space
$H^2(\mathbb{R}^2)$. In this work, we extend those results by demonstrating
strong ill-posedness in logarithmically regularized spaces which are strictly
contained in $H^2(\mathbb{R}^2)$ and which contain $H^s(\mathbb{R}^2)$ for all
$s>2$. These spaces are constructed via application of a fractional logarithmic
derivative to the critical Sobolev norm. We show that if the power $\alpha$ of
the logarithmic derivative satisfies $\alpha\leq 1/2$, then the 2D Euler
equations are strongly ill-posed.

</details>


### [41] [A Weakly Nonlinear Theory for Pattern Formation in Structured Models with Localized Solutions](https://arxiv.org/abs/2510.20781)
*Wesley J. M. Ridgway,Mohit P. Dalwadi,Philip Pearce,S. Jonathan Chapman*

Main category: math.AP

TL;DR: A weakly nonlinear framework is developed for structured PDE models with exponentially localized steady states, extending classical pattern formation analysis to systems where base states approach Dirac-delta functions.


<details>
  <summary>Details</summary>
Motivation: Structured models like PDEs with age or phenotype structure are important for studying pattern formation in heterogeneous populations, but classical analysis tools face mathematical challenges when steady states are sharply peaked or singular.

Method: The approach uses WKBJ asymptotics and analysis of Stokes phenomenon to systematically resolve solution structure in the limit where steady states tend to Dirac-delta functions. The framework is demonstrated on a chemically structured model of motile bacteria interacting through quorum sensing.

Result: The analysis yields an amplitude equation governing solution dynamics near linear instability and predicts a pitchfork bifurcation. An effective parameter grouping is identified that determines whether the bifurcation is subcritical or supercritical.

Conclusion: Although demonstrated on a specific bacterial model, the techniques are broadly applicable to structured PDE models with localized steady states, providing a systematic framework for pattern formation analysis in such systems.

Abstract: Structured models, such as PDEs structured by age or phenotype, provide a
setting to study pattern formation in heterogeneous populations. Classical
tools to quantify the emergence of patterns, such as linear and weakly
nonlinear analyses, pose significant mathematical challenges for these models
due to sharply peaked or singular steady states. Here, we present a weakly
nonlinear framework that extends classical tools to structured PDE models in
settings where the base state is spatially uniform, but exponentially localized
in the structured variable. Our approach utilizes WKBJ asymptotics and an
analysis of the Stokes phenomenon to systematically resolve the solution
structure in the limit where the steady state tends to a Dirac-delta function.
To demonstrate our method, we consider a chemically structured (nonlocal) model
of motile bacteria that interact through quorum sensing. For this example, our
analysis yields an amplitude equation that governs the solution dynamics near a
linear instability, and predicts a pitchfork bifurcation. From the amplitude
equation, we deduce an effective parameter grouping whose sign determines
whether the pitchfork bifurcation is subcritical or supercritical. Although we
demonstrate our framework for a specific example, our techniques are broadly
applicable.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [42] [Investigation of the Mechanical Properties of Three Commercial and Five Variations of IOL Models](https://arxiv.org/abs/2510.20015)
*Taner Karateke,Abdullah MevlÜt Mutluel*

Main category: physics.comp-ph

TL;DR: FEM simulation of 8 haptic IOL models shows different mechanical stability under compression forces - commercial models better for small forces, variation models better for large forces.


<details>
  <summary>Details</summary>
Motivation: To evaluate mechanical stability of different haptic IOL models and identify optimal designs for different compression scenarios.

Method: Finite Element Method (FEM) simulation of 8 IOL models (3 commercial, 5 variations) under quasi-static compression, measuring axial displacement, elasticity modulus, and stress.

Result: Commercial IOL model performed better for smaller compression forces, while a variation model showed superior performance for larger compression forces.

Conclusion: Findings can guide development of more mechanically stable IOL models by selecting appropriate designs based on expected compression forces.

Abstract: This study aimed to simulate the mechanical stability of eight different
(three commercial and five variations) haptic IOL models using FEM to measure
mechanical biomarkers (axial displacement, elasticity modulus, and stress)
under quasi-static compression. The results revealed that a commercial IOL
model exhibited a better mechanical response for smaller compression forces
than the other models. Conversely, a variation model performed better for
larger compression forces. These findings may help in developing more
mechanically stable IOL models.

</details>


### [43] [ComProScanner: A multi-agent based framework for composition-property structured data extraction from scientific literature](https://arxiv.org/abs/2510.20362)
*Aritra Roy,Enrico Grisan,John Buckeridge,Chiara Gattinoni*

Main category: physics.comp-ph

TL;DR: ComProScanner is an autonomous multi-agent platform that extracts, validates, classifies, and visualizes chemical compositions and properties from scientific literature, achieving 0.82 accuracy with DeepSeek-V3-0324 for piezoelectric materials.


<details>
  <summary>Details</summary>
Motivation: Despite advances in large language models, there's a lack of accessible automated tools for constructing, validating, and visualizing datasets from scientific literature extraction, particularly for complex chemical data.

Method: Developed ComProScanner - an autonomous multi-agent platform that facilitates extraction, validation, classification, and visualization of machine-readable chemical compositions and properties integrated with synthesis data from journal articles.

Result: Evaluated framework using 100 journal articles against 10 different LLMs. DeepSeek-V3-0324 outperformed all models with 0.82 overall accuracy for extracting complex compositions and piezoelectric strain coefficients (d33) of ceramic piezoelectric materials.

Conclusion: The framework provides a simple, user-friendly package for extracting highly complex experimental data from literature to build machine learning or deep learning datasets.

Abstract: Since the advent of various pre-trained large language models, extracting
structured knowledge from scientific text has experienced a revolutionary
change compared with traditional machine learning or natural language
processing techniques. Despite these advances, accessible automated tools that
allow users to construct, validate, and visualise datasets from scientific
literature extraction remain scarce. We therefore developed ComProScanner, an
autonomous multi-agent platform that facilitates the extraction, validation,
classification, and visualisation of machine-readable chemical compositions and
properties, integrated with synthesis data from journal articles for
comprehensive database creation. We evaluated our framework using 100 journal
articles against 10 different LLMs, including both open-source and proprietary
models, to extract highly complex compositions associated with ceramic
piezoelectric materials and corresponding piezoelectric strain coefficients
(d33), motivated by the lack of a large dataset for such materials.
DeepSeek-V3-0324 outperformed all models with a significant overall accuracy of
0.82. This framework provides a simple, user-friendly, readily-usable package
for extracting highly complex experimental data buried in the literature to
build machine learning or deep learning datasets.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [44] [High Gain Fusion Target Design using Generative Artificial Intelligence](https://arxiv.org/abs/2510.20105)
*Michael E. Glinsky*

Main category: physics.plasm-ph

TL;DR: Using generative AI based on Ubuntu concept to design optimally entangled topological states for fusion targets, achieving 10 GJ energy yield from 3 MJ input across various fusion methods.


<details>
  <summary>Details</summary>
Motivation: To develop practical, room temperature fusion targets that can achieve high energy yields with minimal input energy by leveraging topological state optimization.

Method: Generative AI based on Ubuntu concept replaces Deep Convolutional Neural Networks with canonical transformation generating functional, enabling renormalization and topological characterization of collective systems.

Result: Practical fusion targets capable of yielding up to 10 GJ of energy from as little as 3 MJ of absorbed energy, applicable to tokamaks, laser-driven, and pulsed-power schemes.

Conclusion: Generative AI enables optimal topological state configuration and stabilization for fusion targets, providing a novel approach to fusion energy production with high efficiency.

Abstract: By returning to the topological basics of fusion target design, Generative
Artificial Intelligence (genAI) is used to specify how to initially configure
and drive the optimally entangled topological state, and stabilize that
topological state from disruption. This can be applied to all methods;
including tokamaks, laser-driven schemes, and pulsed-power driven schemes. The
result is practical, room temperature targets that can yield up to 10 GJ of
energy, driven by as little as 3 MJ of absorbed energy. The genAI is based on
the concept of Ubuntu that replaces the Deep Convolutional Neural Network
approximation of a functional, with the formula for the generating functional
of a canonical transformation from the domain of the canonical field momentums
and fields, to the domain of the canonical momentums and coordinates, that is
the Reduced Order Model. This formula is a logical process of renormalization,
enabling Heisenberg's canonical approach to field theory, via calculation of
the S-matrix, given observation of the fields. This can be viewed as
topological characterization and control of collective, that is complex,
systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [45] [Simultaneously Solving Infinitely Many LQ Mean Field Games In Hilbert Spaces: The Power of Neural Operators](https://arxiv.org/abs/2510.20017)
*Dena Firoozi,Anastasis Kratsios,Xuwei Yang*

Main category: math.OC

TL;DR: Neural operators can learn to solve mean-field games by mapping problem rules to equilibrium strategies, with statistical guarantees for unseen problems even in infinite dimensions.


<details>
  <summary>Details</summary>
Motivation: Traditional MFG solvers are inefficient for solving many related problems, especially with perturbations or continuum-parameterized agents, requiring a more scalable approach.

Method: Train neural operators to learn the rules-to-equilibrium map from problem data (dynamics and cost functionals) of LQ MFGs on separable Hilbert spaces, with controlled Lipschitz regularity.

Result: NOs trained on a small number of randomly sampled rules reliably solve unseen LQ MFG variants, with controlled parameter count under appropriate rule sampling.

Conclusion: The approach provides statistical guarantees for solving MFGs using neural operators, with controlled Lipschitz constants and sample complexity bounds applicable to infinite-dimensional settings.

Abstract: Traditional mean-field game (MFG) solvers operate on an instance-by-instance
basis, which becomes infeasible when many related problems must be solved
(e.g., for seeking a robust description of the solution under perturbations of
the dynamics or utilities, or in settings involving continuum-parameterized
agents.). We overcome this by training neural operators (NOs) to learn the
rules-to-equilibrium map from the problem data (``rules'': dynamics and cost
functionals) of LQ MFGs defined on separable Hilbert spaces to the
corresponding equilibrium strategy. Our main result is a statistical guarantee:
an NO trained on a small number of randomly sampled rules reliably solves
unseen LQ MFG variants, even in infinite-dimensional settings. The number of NO
parameters needed remains controlled under appropriate rule sampling during
training.
  Our guarantee follows from three results: (i) local-Lipschitz estimates for
the highly nonlinear rules-to-equilibrium map; (ii) a universal approximation
theorem using NOs with a prespecified Lipschitz regularity (unlike traditional
NO results where the NO's Lipschitz constant can diverge as the approximation
error vanishes); and (iii) new sample-complexity bounds for $L$-Lipschitz
learners in infinite dimensions, directly applicable as the Lipschitz constants
of our approximating NOs are controlled in (ii).

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [46] [Kinetics of Peierls dimerization transition: Machine learning force-field approach](https://arxiv.org/abs/2510.20659)
*Ho Jang,Yang Yang,Gia-Wei Chern*

Main category: cond-mat.stat-mech

TL;DR: ML force-field framework for simulating CDW dynamics with linear scaling efficiency, revealing two-stage coarsening behavior with early power-law growth and late-time Allen-Cahn scaling.


<details>
  <summary>Details</summary>
Motivation: Overcome computational bottleneck in simulating non-equilibrium CDW dynamics driven by Peierls instability, where evaluating adiabatic forces from electron-lattice coupling is intensive for large systems.

Method: Developed generalized Behler-Parrinello neural-network architecture to predict forces from local structural environments, leveraging locality of electronic responses for linear scaling efficiency.

Result: Large-scale simulations uncovered two-stage CDW domain coarsening: early-time power-law growth (L ~ t^0.7) followed by crossover to Allen-Cahn scaling (L ~ √t). Enhanced early coarsening attributed to anisotropic domain-wall motion from electron-mediated directional interactions.

Conclusion: Demonstrates promise of ML-based force fields for multiscale dynamical modeling of condensed-matter lattice models, enabling efficient and accurate simulations of complex electron-lattice coupled systems.

Abstract: We present a machine learning (ML) force-field framework for simulating the
non-equilibrium dynamics of charge-density-wave (CDW) order driven by the
Peierls instability. Since the Peierls distortion arises from the coupling
between lattice displacements and itinerant electrons, evaluating the adiabatic
forces during time evolution is computationally intensive, particularly for
large systems. To overcome this bottleneck, we develop a generalized
Behler-Parrinello neural-network architecture -- originally formulated for ab
initio molecular dynamics -- to accurately and efficiently predict forces from
local structural environments. Using the locality of electronic responses, the
resulting ML force field achieves linear scaling efficiency while maintaining
quantitative accuracy. Large-scale dynamical simulations using this framework
uncover a two-stage coarsening behavior of CDW domains: an early-time regime
characterized by a power-law growth $L \sim t^{\alpha}$ with an effective
exponent $\alpha \approx 0.7$, followed by a crossover to the Allen-Cahn
scaling $L \sim \sqrt{t}$ at late times. The enhanced early-time coarsening is
attributed to anisotropic domain-wall motion arising from electron-mediated
directional interactions. This work demonstrates the promise of ML-based force
fields for multiscale dynamical modeling of condensed-matter lattice models.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [47] [Two Quantum Algorithms for Nonlinear Reaction-Diffusion Equation using Chebyshev Approximation Method](https://arxiv.org/abs/2510.19855)
*Manish Kumar*

Main category: quant-ph

TL;DR: Two new quantum algorithms for reaction-diffusion equations using truncated Chebyshev polynomial approximation, with different gate complexities and improved efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient quantum algorithms for solving reaction-diffusion equations, which are important in various scientific fields, by leveraging Chebyshev polynomial approximation and addressing the diagonalization of Carleman embedding matrices.

Method: Used truncated Chebyshev polynomial approximation to solve linearized ODEs. First algorithm employs matrix exponentiation method, second algorithm repurposes quantum spectral method. Key technical contribution is deriving sufficient conditions for diagonalizing Carleman embedding matrix and providing efficient iterative diagonalization algorithm.

Result: First algorithm has gate complexity O(d·log(d)+T·polylog(T/ε)), second algorithm scales as O(polylog(d)·T·polylog(T/ε)), comparable to current best quantum algorithm (truncated Taylor series method).

Conclusion: The approach provides efficient quantum algorithms for reaction-diffusion equations with significant speedup, though limitations exist regarding condition number bounds and reliance on a conjecture about trigonometric equations, with mitigation strategies provided.

Abstract: We present two new quantum algorithms for reaction-diffusion equations that
employ the truncated Chebyshev polynomial approximation. This method is
employed to numerically solve the ordinary differential equation emerging from
the linearization of the associated nonlinear differential equation. In the
first algorithm, we use the matrix exponentiation method (Patel et al., 2018),
while in the second algorithm, we repurpose the quantum spectral method (Childs
et al., 2020). Our main technical contribution is to derive the sufficient
conditions for the diagonalization of the Carleman embedding matrix, which is
indispensable for designing both quantum algorithms. We supplement this with an
efficient iterative algorithm to diagonalize the Carleman matrix.
  Our first algorithm has gate complexity of
O(d$\cdot$log(d)+T$\cdot$polylog(T/$\varepsilon$)). Here $d$ is the size of the
Carleman matrix, $T$ is the simulation time, and $\varepsilon$ is the
approximation error. The second algorithm is polynomial in $log(d)$, $T$, and
$log(1/\varepsilon)$ - the gate complexity scales as
O(polylog(d)$\cdot$T$\cdot$polylog(T/$\varepsilon$)). In terms of $T$ and
$\varepsilon$, this is comparable to the speedup gained by the current best
known quantum algorithm for this problem, the truncated Taylor series method
(Costa et.al., 2025).
  Our approach has two shortcomings. First, we have not provided an upper
bound, in terms of d, on the condition number of the Carleman matrix. Second,
the success of the diagonalization is based on a conjecture that a specific
trigonometric equation has no integral solution. However, we provide strategies
to mitigate these shortcomings in most practical cases.

</details>


### [48] [On Encoding Matrices using Quantum Circuits](https://arxiv.org/abs/2510.20030)
*Liron Mor Yosef,Haim Avron*

Main category: quant-ph

TL;DR: This paper systematically studies quantum circuit representations for matrices, focusing on block encodings and state preparation circuits, and establishes efficient construction methods and bidirectional conversions between these representations.


<details>
  <summary>Details</summary>
Motivation: Quantum computing promises superior algorithms for linear algebra, but efficient execution depends on proper quantum circuit representations of matrices and vectors. The paper aims to address the need for systematic methods to construct and convert between different quantum circuit representations.

Method: The authors examine methods for constructing block encodings and state preparation circuits from classical matrix data, and develop conversion algorithms between these representations. Key technical components include a constant-depth multiplexer for higher-order Pauli matrices and an algorithm for quantum conversion between standard basis and Pauli basis expansions.

Result: The paper establishes two main results: (1) an efficient method for constructing block encodings of arbitrary matrices given in classical form, and (2) low-overhead bidirectional conversion algorithms showing that block encodings and state preparation circuits are essentially equivalent representations.

Conclusion: The study provides systematic approaches for quantum circuit representations of matrices, demonstrating the equivalence between block encodings and state preparation circuits through efficient conversion methods, which advances the practical implementation of quantum linear algebra algorithms.

Abstract: Over a decade ago, it was demonstrated that quantum computing has the
potential to revolutionize numerical linear algebra by enabling algorithms with
complexity superior to what is classically achievable, e.g., the seminal HHL
algorithm for solving linear systems. Efficient execution of such algorithms
critically depends on representing inputs (matrices and vectors) as quantum
circuits that encode or implement these inputs. For that task, two common
circuit representations emerged in the literature: block encodings and state
preparation circuits. In this paper, we systematically study encodings matrices
in the form of block encodings and state preparation circuits. We examine
methods for constructing these representations from matrices given in classical
form, as well as quantum two-way conversions between circuit representations.
Two key results we establish (among others) are: (a) a general method for
efficiently constructing a block encoding of an arbitrary matrix given in
classical form (entries stored in classical random access memory); and (b)
low-overhead, bidirectional conversion algorithms between block encodings and
state preparation circuits, showing that these models are essentially
equivalent. From a technical perspective, two central components of our
constructions are: (i) a special constant-depth multiplexer that simultaneously
multiplexes all higher-order Pauli matrices of a given size, and (ii) an
algorithm for performing a quantum conversion between a matrix's expansion in
the standard basis and its expansion in the basis of higher-order Pauli
matrices.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [49] [Stochastic evolution equations with nonlinear diffusivity, recent progress and critical cases](https://arxiv.org/abs/2510.20471)
*Ioana Ciotir,Dan Goreac,Jonas M. Tölle*

Main category: math.PR

TL;DR: Survey on critical cases of stochastic evolution equations with various noise types, covering solution concepts, convergence, homogenization, and recent advances in regularity, long-time behavior, and numerics.


<details>
  <summary>Details</summary>
Motivation: Address recent progress in stochastic evolution equations at critical cases, including porous media, fast-diffusion, and singular PDEs with additive, multiplicative, or gradient noises.

Method: Survey approach presenting different solution notions, convergence analysis for parameter-dependent solutions, and homogenization techniques.

Result: Comprehensive overview of solution concepts and convergence properties in critical stochastic PDEs, with references to recent developments in regularity, ergodicity, and numerical methods.

Conclusion: Critical stochastic evolution equations represent an active research area with ongoing progress in theory, analysis, and applications across various physical and mathematical contexts.

Abstract: This short survey article stems from recent progress on critical cases of
stochastic evolution equations in variational formulation with additive,
multiplicative or gradient noises. Typical examples appear as the limit cases
of the stochastic porous medium equation, stochastic fast- and super
fast-diffusion equations, self-organized criticality, stochastic singular
$p$-Laplace equations, and the stochastic total variation flow, among others.
We present several different notions of solutions, results on convergence of
solutions depending on a parameter, and homogenization. Furthermore, we provide
some references hinting at the recent progress in regularity results, long-time
behavior, ergodicity, and numerical analysis.

</details>


### [50] [On the Structure of Stationary Solutions to McKean-Vlasov Equations with Applications to Noisy Transformers](https://arxiv.org/abs/2510.20094)
*Krishnakumar Balasubramanian,Sayan Banerjee,Philippe Rigollet*

Main category: math.PR

TL;DR: The paper studies stationary solutions of McKean-Vlasov equations on the circle using Fourier analysis, revealing equivalence between solutions and infinite-dimensional quadratic systems, enabling explicit characterization of bifurcations and phase transitions.


<details>
  <summary>Details</summary>
Motivation: To provide a transparent framework for analyzing stationary states of McKean-Vlasov equations, characterizing bifurcations, phase transitions, and the structure of stationary solutions using Fourier methods.

Method: Establishes exact equivalence between stationary McKean-Vlasov solutions and infinite-dimensional quadratic systems over Fourier coefficients, enabling analysis in sequence space rather than function space. Uses Fourier analysis to characterize bifurcations and stationary solutions.

Result: Derived analytic expressions for bifurcation emergence and types (supercritical, critical, subcritical, transcritical), characterized stationary bifurcating solutions with arbitrary Fourier mode accuracy, established regularity and concavity of free energy landscape, and identified discontinuous phase transitions with non-differentiable points in minimum free energy map.

Conclusion: The Fourier framework provides powerful tools for analyzing McKean-Vlasov equations, enabling detailed characterization of bifurcations and phase transitions. Applied to the Noisy Mean-Field Transformer model, it reveals complex bifurcation geometry and sharp transitions from continuous to discontinuous phase behavior as temperature decreases.

Abstract: We study stationary solutions of McKean-Vlasov equations on the circle. Our
main contributions stem from observing an exact equivalence between solutions
of the stationary McKean-Vlasov equation and an infinite-dimensional quadratic
system of equations over Fourier coefficients, which allows explicit
characterization of the stationary states in a sequence space rather than a
function space. This framework provides a transparent description of local
bifurcations, characterizing their periodicity, and resonance structures, while
accommodating singular potentials. We derive analytic expressions that
characterize the emergence, form and shape (supercritical, critical,
subcritical or transcritical) of bifurcations involving possibly multiple
Fourier modes and connect them with discontinuous phase transitions. We also
characterize, under suitable assumptions, the detailed structure of the
stationary bifurcating solutions that are accurate upto an arbitrary number of
Fourier modes. At the global level, we establish regularity and concavity
properties of the free energy landscape, proving existence, compactness, and
coexistence of globally minimizing stationary measures, further identifying
discontinuous phase transitions with points of non-differentiability of the
minimum free energy map. As an application, we specialize the theory to the
Noisy Mean-Field Transformer model, where we show how changing the inverse
temperature parameter $\beta$ affects the geometry of the infinitely many
bifurcations from the uniform measure. We also explain how increasing $\beta$
can lead to a rich class of approximate multi-mode stationary solutions which
can be seen as `metastable states'. Further, a sharp transition from continuous
to discontinuous (first-order) phase behavior is observed as $\beta$ increases.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [51] [Magnetic Field-Line Curvature and Its Role in Particle Acceleration by Magnetically Dominated Turbulence](https://arxiv.org/abs/2510.20628)
*Samuel Sebastian,Luca Comisso*

Main category: astro-ph.HE

TL;DR: First-principles kinetic simulations show magnetic field-line curvature drives particle acceleration in turbulent plasmas via curvature-drift motion, with acceleration efficiency increasing with fluctuation-to-mean magnetic field ratio.


<details>
  <summary>Details</summary>
Motivation: To understand how magnetic field-line curvature contributes to particle acceleration in magnetically dominated turbulent plasmas, particularly in astrophysical contexts where nonthermal particle energization occurs.

Method: Used first-principles fully kinetic particle-in-cell simulations with varying fluctuation-to-mean magnetic-field ratios (δB₀/B₀), analyzed curvature statistics, probability densities, and guiding-center analysis of curvature-drift acceleration.

Result: Curvature probability densities show broad power-law wings with hard high-κ tails for δB₀/B₀ ≳ 1. Curvature-drift acceleration accounts for substantial energization via motional electric field, strengthening with increasing δB₀/B₀, and typically exceeds other drift mechanisms for magnetized particles.

Conclusion: Curvature-drift acceleration is identified as a principal pathway for energy transfer from magnetized turbulence to nonthermal particles in astrophysical plasmas.

Abstract: We employ first-principles, fully kinetic particle-in-cell simulations to
investigate magnetic field-line curvature in magnetically dominated turbulent
plasmas and its role in particle acceleration through curvature-drift motion
along the motional electric field. By varying the fluctuation-to-mean
magnetic-field ratio $\delta B_0/B_0$, we examine curvature $\kappa$ statistics
and their connection to particle acceleration. The curvature probability
densities display broad power-law wings, scaling linearly in $\kappa$ below the
peak and developing hard high-$\kappa$ tails for $\delta B_0/B_0 \gtrsim 1$. As
the mean field strengthens, the high-$\kappa$ tails steepen, and
large-curvature events are suppressed when $\delta B_0/B_0 \ll 1$. The
probability density functions of magnetic field-line contraction, ${\bf v}_E
\cdot {\bf \kappa}$, with ${\bf v}_E$ the field-line velocity, develop
power-law tails well described by a symmetric Pareto distribution,
characteristic of stochastic energy exchanges, with the tails becoming harder
as $\delta B_0/B_0$ increases. Our guiding-center analysis shows that
curvature-drift acceleration accounts for a substantial fraction of the
energization via the motional electric field, and that it strengthens with
increasing $\delta B_0/B_0$. For well-magnetized particles, curvature-drift
acceleration typically exceeds ${\bf\nabla}B$ drift, polarization drift, and
betatron contributions. These results identify curvature-drift acceleration as
a principal pathway through which magnetized turbulence transfers energy to
nonthermal particles in astrophysical plasmas.

</details>


### [52] [The Opacity Project: R-Matrix Calculations for Opacities of High-Energy-Density Astrophysical and Laboratory Plasmas](https://arxiv.org/abs/2510.20775)
*Anil K. Pradhan,Sultana N. Nahar*

Main category: astro-ph.HE

TL;DR: This paper investigates radiative properties in high-energy-density plasmas using R-Matrix calculations, focusing on opacity determination for astrophysical and laboratory applications including solar opacity problem and inertial confinement fusion.


<details>
  <summary>Details</summary>
Motivation: Accurate opacity determination is critical for understanding radiation transport in astrophysical and laboratory plasmas, addressing discrepancies between theoretical calculations and experimental measurements, particularly the solar opacity problem.

Method: Employ atomic data from R-Matrix calculations to analyze radiative properties, calculate Rosseland Mean Opacities across temperature/density ranges, and study effects of electron collisional and Stark ion microfield broadening on autoionizing resonances in photoabsorption cross sections.

Result: The study provides insights into how opacities vary under different plasma conditions and examines the impact of equation-of-state on level populations and opacities, with specific analysis of environments like the solar base convective zone and Sandia Z facility.

Conclusion: This research contributes to improving opacity models for high-energy-density sources including stellar interiors and laboratory fusion plasma experiments, addressing fundamental challenges in radiation transport physics.

Abstract: Accurate determination of opacity is critical for understanding radiation
transport in both astrophysical and laboratory plasmas. We employ atomic data
from R-Matrix calculations to investigate radiative properties in
high-energy-density (HED) plasma sources. Specifically, we analyze environments
such as the base of the convective zone (BCZ) of the Sun 2 x 10^6$ K, N_e =
10^{23}/cc and the inertial confinement fusion (ICF) device at the Sandia Z
facility 2.11 x 10^6 K, N_e = 3.16 x 10^{22}/cc. We calculate Rosseland Mean
Opacities (RMO) within a range of temperatures and densities and analyze how
they vary under different plasma conditions. In this study, we specifically
focus on electron collisional and Stark ion microfield broadening effects on
autoionizing resonances in photoabsorption cross sections. Our results are
relevant to astrophysical models, particularly in the context of the solar
opacity problem, and provide insights into discrepancies between theoretical
calculations and experimental measurements. In addition, we investigate the
equation-of-state (EOS) and its impact on opacities. In addition, we examine
the equation-of-state (EOS) and its impact on opacities of the "chemical
picture" Mihalas-Hummer-Dappen EOS with respect to level populations of excited
levels included in the R-matrix calculations. This study should contribute to
improving opacity models of HED sources such as stellar interiors adn
laboratory fusion plasma experiments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [53] [Interpolatory Approximations of PMU Data: Dimension Reduction and Pilot Selection](https://arxiv.org/abs/2510.20116)
*Sean Reiter,Mark Embree,Serkan Gugercin,Vassilis Kekatos*

Main category: eess.SY

TL;DR: This paper proposes interpolatory matrix decompositions (IDs) with DEIM for PMU data compression and fault detection, enabling real-time monitoring with limited measurements.


<details>
  <summary>Details</summary>
Motivation: To reduce PMU data transmission requirements and enable real-time power system monitoring with minimal communication bandwidth by compressing data matrices.

Method: Uses interpolatory matrix decompositions (IDs) with discrete empirical interpolation method (DEIM) to reconstruct complete PMU data from selected rows (PMU streams) and columns (time snapshots).

Result: DEIM shows excellent performance for data compression and provides a computable error bound that serves as a fault detection tool when violated.

Conclusion: The ID framework with DEIM effectively compresses PMU data while providing rigorous error bounds for both compression quality assessment and fault detection in power systems.

Abstract: This work investigates the reduction of phasor measurement unit (PMU) data
through low-rank matrix approximations. To reconstruct a PMU data matrix from
fewer measurements, we propose the framework of interpolatory matrix
decompositions (IDs). In contrast to methods relying on principal component
analysis or singular value decomposition, IDs recover the complete data matrix
using only a few of its rows (PMU datastreams) and/or a few of its columns
(snapshots in time). This compression enables the real-time monitoring of power
transmission systems using a limited number of measurements, thereby minimizing
communication bandwidth. The ID perspective gives a rigorous error bound on the
quality of the data compression. We propose selecting rows and columns used in
an ID via the discrete empirical interpolation method (DEIM), a greedy
algorithm that aims to control the error bound. This bound leads to a
computable estimate for the reconstruction error during online operations. A
violation of this estimate suggests a change in the system's operating
conditions, and thus serves as a tool for fault detection. Numerical tests
using synthetic PMU data illustrate DEIM's excellent performance for data
compression, and validate the proposed DEIM-based fault-detection method.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [54] [The global nonlinear stability of Minkowski spacetime with self-gravitating massive Dirac fields](https://arxiv.org/abs/2510.20626)
*Philippe G. LeFloch,Yue Ma,Weidong Zhang*

Main category: gr-qc

TL;DR: The paper proves the nonlinear stability of massive spinor fields in the Einstein-Dirac system, showing that initial data close to Minkowski spacetime leads to globally hyperbolic developments that remain asymptotic to Minkowski spacetime.


<details>
  <summary>Details</summary>
Motivation: Previous results were limited to massless fields, and the authors aim to extend stability analysis to massive spinor fields using a gauge-invariant approach that handles the specific structure of spinor fields and Dirac equations.

Method: Uses asymptotically hyperboloidal-Euclidean framework with light-bending wave coordinates, Lorentz Clifford algebras, principal fiber bundles, and derives L2 and pointwise estimates for the Dirac equation and its coupling with Einstein equations.

Result: Established global existence for the system of wave equations with constraints and Klein-Gordon-type equations, proving nonlinear stability of massive spinor fields in asymptotically Minkowski spacetime.

Conclusion: The massive Einstein-Dirac system is nonlinearly stable for initial data sufficiently close to Minkowski spacetime, with the solution remaining asymptotic to Minkowski in all directions.

Abstract: We consider the Einstein-Dirac system for a massive field, which describes
the evolution of self-gravitating massive spinor fields, and we investigate the
global evolution problem, when the initial data set is sufficiently close to
data describing a spacelike, asymptotically Euclidean slice of the Minkowski
spacetime. We establish the gauge-invariant nonlinear stability of such fields,
namely the existence of a globally hyperbolic development, which remains
asymptotic to Minkowski spacetime in future timelike, null, and spacelike
directions. Previous results on this problem have been limited to the
Einstein-Dirac system in the massless case. Our analysis follows the
asymptotically hyperboloidal-Euclidean framework introduced by LeFloch and Y.
Ma for the massive Klein-Gordon-Einstein system. The structure specific to
spinor fields and the Dirac equation necessitates significantly new elements in
the proof. In contrast with prior approaches, our treatment of spinor fields
and the Dirac equation is gauge-invariant, relying on the formalism of Lorentz
Clifford algebras, principal fiber bundles, etc. Our analysis is carried out
with the metric expressed in light-bending wave coordinates, as we call them.
This leads us to the study of a global existence problem for a system of wave
equations with constraints and a Klein-Gordon-type equations. We derive L2
estimates for the Dirac equation and its coupling with the Einstein equations,
along with $pointwise estimates. New Sobolev inequalities are proven for spinor
fields in a gauge-invariant manner in the hyperboloidal-Euclidean foliation.
The nonlinear coupling between the massive Dirac equation and the Einstein
equations is investigated, and we establish a hierarchy of estimates, which
distinguish between translations, rotations, and boosts.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [55] [Symbolic Regression and Differentiable Fits in Beyond the Standard Model Physics](https://arxiv.org/abs/2510.20453)
*Shehu AbdusSalam,Steven Abel,Deaglan Bartlett,Miguel Crispim Romão*

Main category: hep-ph

TL;DR: Symbolic regression is used to derive analytical expressions for observables in the Constrained Minimal Supersymmetric Standard Model, enabling faster global fits and differentiable optimization methods.


<details>
  <summary>Details</summary>
Motivation: To accelerate analysis of Beyond Standard Model physics phenomenology by deriving symbolic expressions for observables rather than using conventional sampling methods.

Method: Apply symbolic regression to derive analytical expressions for Higgs mass, dark matter relic density, and muon anomalous magnetic moment in terms of CMSSM input parameters.

Result: Symbolic regression produces remarkably accurate expressions that enable global fits with posterior probability densities in good agreement with conventional methods, while allowing differentiable optimization.

Conclusion: Symbolic regression is an effective approach for BSM physics analysis, producing robust results and enabling differentiable fitting methods, outperforming neural networks in global robustness.

Abstract: We demonstrate the efficacy of symbolic regression (SR) to probe models of
particle physics Beyond the Standard Model (BSM), by considering the so-called
Constrained Minimal Supersymmetric Standard Model (CMSSM). Like many
incarnations of BSM physics this model has a number (four) of arbitrary
parameters, which determine the experimental signals, and cosmological
observables such as the dark matter relic density. We show that analysis of the
phenomenology can be greatly accelerated by using symbolic expressions derived
for the observables in terms of the input parameters. Here we focus on the
Higgs mass, the cold dark matter relic density, and the contribution to the
anomalous magnetic moment of the muon. We find that SR can produce remarkably
accurate expressions. Using them we make global fits to derive the posterior
probability densities of the CMSSM input parameters which are in good agreement
with those performed using conventional methods. Moreover, we demonstrate a
major advantage of SR which is the ability to make fits using differentiable
methods rather than sampling methods. We also compare the method with neural
network (NN) regression. SR produces more globally robust results, while NNs
require data that is focussed on the promising regions in order to be equally
performant.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [56] [Enhanced Cyclic Coordinate Descent Methods for Elastic Net Penalized Linear Models](https://arxiv.org/abs/2510.19999)
*Yixiao Wang,Zishan Shao,Ting Jiang,Aditya Devarakonda*

Main category: stat.ML

TL;DR: Enhanced cyclic coordinate descent (ECCD) framework for generalized linear models with elastic net constraints that achieves 3x speedup over state-of-the-art methods through Taylor expansion approximation and batched computations.


<details>
  <summary>Details</summary>
Motivation: To reduce training time for generalized linear models with elastic net constraints compared to existing methods, while avoiding convergence delay and numerical instability issues in block coordinate descent.

Method: Redesigned cyclic coordinate descent using Taylor expansion around current iterate to avoid nonlinear gradient operations, unrolling vector recurrences into efficient batched computations with tunable parameter s for performance optimization.

Result: Empirical results show 3x average performance improvement on regularization path variant across diverse benchmark datasets, with s > 1 providing performance gains without affecting convergence.

Conclusion: ECCD framework successfully accelerates training for generalized linear models with elastic net constraints while maintaining convergence properties and numerical stability, implemented in C++ with Eigen for efficient linear algebra computations.

Abstract: We present a novel enhanced cyclic coordinate descent (ECCD) framework for
solving generalized linear models with elastic net constraints that reduces
training time in comparison to existing state-of-the-art methods. We redesign
the CD method by performing a Taylor expansion around the current iterate to
avoid nonlinear operations arising in the gradient computation. By introducing
this approximation, we are able to unroll the vector recurrences occurring in
the CD method and reformulate the resulting computations into more efficient
batched computations. We show empirically that the recurrence can be unrolled
by a tunable integer parameter, $s$, such that $s > 1$ yields performance
improvements without affecting convergence, whereas $s = 1$ yields the original
CD method. A key advantage of ECCD is that it avoids the convergence delay and
numerical instability exhibited by block coordinate descent. Finally, we
implement our proposed method in C++ using Eigen to accelerate linear algebra
computations. Comparison of our method against existing state-of-the-art
solvers shows consistent performance improvements of $3\times$ in average for
regularization path variant on diverse benchmark datasets. Our implementation
is available at https://github.com/Yixiao-Wang-Stats/ECCD.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals](https://arxiv.org/abs/2510.19917)
*Trajan Murphy,Akshunna S. Dogra,Hanfeng Gu,Caleb Meredith,Mark Kon,Julio Enrique Castrillion-Candas*

Main category: cs.LG

TL;DR: FINDER is a classification framework for noisy datasets that uses stochastic features and KLE to handle data randomness, achieving state-of-the-art results in Alzheimer's disease classification and deforestation detection.


<details>
  <summary>Details</summary>
Motivation: Addressing classification challenges in noisy datasets with low signal-to-noise ratios, small sample sizes, and faulty data collection methods.

Method: Creates stochastic features by treating datasets as random field realizations, maps them to Hilbert spaces, uses Kosambi-Karhunen-Loève expansion for irreducible components, and performs classification via eigen-decomposition of operator spectra.

Result: Achieved state-of-the-art performance in Alzheimer's Disease stage classification and remote sensing detection of deforestation on challenging, data-deficient scientific domains.

Conclusion: FINDER provides a rigorous framework for noisy dataset classification, with discussion on when it outperforms existing methods, failure modes, and limitations.

Abstract: ''Noisy'' datasets (regimes with low signal to noise ratios, small sample
sizes, faulty data collection, etc) remain a key research frontier for
classification methods with both theoretical and practical implications. We
introduce FINDER, a rigorous framework for analyzing generic classification
problems, with tailored algorithms for noisy datasets. FINDER incorporates
fundamental stochastic analysis ideas into the feature learning and inference
stages to optimally account for the randomness inherent to all empirical
datasets. We construct ''stochastic features'' by first viewing empirical
datasets as realizations from an underlying random field (without assumptions
on its exact distribution) and then mapping them to appropriate Hilbert spaces.
The Kosambi-Karhunen-Lo\'eve expansion (KLE) breaks these stochastic features
into computable irreducible components, which allow classification over noisy
datasets via an eigen-decomposition: data from different classes resides in
distinct regions, identified by analyzing the spectrum of the associated
operators. We validate FINDER on several challenging, data-deficient scientific
domains, producing state of the art breakthroughs in: (i) Alzheimer's Disease
stage classification, (ii) Remote sensing detection of deforestation. We end
with a discussion on when FINDER is expected to outperform existing methods,
its failure modes, and other limitations.

</details>


### [58] [Alternatives to the Laplacian for Scalable Spectral Clustering with Group Fairness Constraints](https://arxiv.org/abs/2510.20220)
*Iván Ojeda-Ruiz,Young Ju-Lee,Malcolm Dickens,Leonardo Cambisaca*

Main category: cs.LG

TL;DR: The paper introduces Fair-SMW, an efficient spectral clustering algorithm that incorporates group fairness constraints using Lagrangian methods and SMW identity, achieving twice the speed and balance compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing spectral clustering algorithms with fairness constraints suffer from high computational complexity, creating a need for more efficient methods that maintain fairness while improving runtime performance.

Method: Proposed Fair-SMW algorithm reformulates constrained optimization using Lagrangian method and Sherman-Morrison-Woodbury identity, with three Laplacian matrix alternatives to generate multiple algorithm variations.

Result: Fair-SMW achieves clustering solutions with comparable balance to existing algorithms while offering twice the computational speed improvement over state-of-the-art methods, and can achieve twice as much balance.

Conclusion: The Fair-SMW algorithm successfully enhances efficiency of fair spectral clustering while maintaining or improving fairness metrics, demonstrating practical viability for real-world applications.

Abstract: Recent research has focused on mitigating algorithmic bias in clustering by
incorporating fairness constraints into algorithmic design. Notions such as
disparate impact, community cohesion, and cost per population have been
implemented to enforce equitable outcomes. Among these, group fairness
(balance) ensures that each protected group is proportionally represented
within every cluster. However, incorporating balance as a metric of fairness
into spectral clustering algorithms has led to computational times that can be
improved. This study aims to enhance the efficiency of spectral clustering
algorithms by reformulating the constrained optimization problem using a new
formulation derived from the Lagrangian method and the
Sherman-Morrison-Woodbury (SMW) identity, resulting in the Fair-SMW algorithm.
Fair-SMW employs three alternatives to the Laplacian matrix with different
spectral gaps to generate multiple variations of Fair-SMW, achieving clustering
solutions with comparable balance to existing algorithms while offering
improved runtime performance. We present the results of Fair-SMW, evaluated
using the Stochastic Block Model (SBM) to measure both runtime efficiency and
balance across real-world network datasets, including LastFM, FacebookNet,
Deezer, and German. We achieve an improvement in computation time that is twice
as fast as the state-of-the-art, and also flexible enough to achieve twice as
much balance.

</details>
