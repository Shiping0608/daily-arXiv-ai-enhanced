<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 34]
- [math.AP](#math.AP) [Total: 28]
- [physics.comp-ph](#physics.comp-ph) [Total: 6]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [cs.AI](#cs.AI) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [physics.class-ph](#physics.class-ph) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [math.DS](#math.DS) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 6]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 2]
- [math.OC](#math.OC) [Total: 3]
- [astro-ph.GA](#astro-ph.GA) [Total: 2]
- [math.FA](#math.FA) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Anisotropic mesh adaptation for unsteady two-phase flow simulation with the Cahn-Hilliard Navier-Stokes model](https://arxiv.org/abs/2510.21749)
*Arthur Bawin,Stéphane Étienne,Cédric Béguin*

Main category: math.NA

TL;DR: An anisotropic mesh adaptation method using Riemannian metrics for simulating two-phase incompressible flows with density differences, based on Cahn-Hilliard Navier-Stokes equations with mixed finite elements and implicit time-stepping.


<details>
  <summary>Details</summary>
Motivation: To efficiently simulate two-phase flows with non-matching densities while maintaining spatial accuracy and capturing the fluid-fluid interface with reduced computational cost compared to uniform or isotropic grids.

Method: Uses global transient fixed-point method dividing simulation time into sub-intervals with adapted anisotropic meshes, employing mixed finite elements and implicit time-stepping for CHNS equations discretization.

Result: The method ensures mesh always captures fluid-fluid interface, allows dynamic control of interface thickness at reduced computational cost, and minimizes transfer errors between meshes.

Conclusion: The adaptive procedure is verified with manufactured solutions and the rising bubble benchmark, demonstrating effective simulation of two-phase flows with density differences.

Abstract: We present an anisotropic mesh adaptation procedure based on Riemannian
metrics for the simulation of two-phase incompressible flows with non-matching
densities. The system dynamics are governed by the Cahn-Hilliard Navier-Stokes
(CHNS) equations, discretized with mixed finite elements and implicit
time-stepping. Spatial accuracy is controlled throughout the simulation by the
\emph{global transient fixed-point method} from Alauzet \emph{et al.}, in which
the simulation time is divided into sub-intervals, each associated with an
adapted anisotropic mesh. The simulation is run in a fixed-point loop until
convergence of each mesh--solution pair. Each iteration takes advantage of the
previously computed solution and accurately predicts the flow variations. This
ensures that the mesh always captures the fluid-fluid interface, and allows for
a dynamic control of the interface thickness at a fraction of the computational
cost compared to uniform or isotropic grids. Moreover, using a modest number of
time sub-intervals reduces the transfer error from one mesh to another, which
would otherwise eventually spoil the numerical solution. The overall adaptive
procedure is verified with manufactured solutions and the well-known rising
bubble benchmark.

</details>


### [2] [An Introductory Guide to Koopman Learning](https://arxiv.org/abs/2510.22002)
*Matthew J. Colbrook,Zlatko Drmač,Andrew Horning*

Main category: math.NA

TL;DR: This paper provides an introductory guide to Koopman learning, focusing on data-driven methods for forecasting and spectral analysis of nonlinear dynamical systems using Koopman operators.


<details>
  <summary>Details</summary>
Motivation: Koopman operators offer a linear framework for analyzing nonlinear dynamical systems, but their infinite-dimensional nature presents computational challenges that need to be addressed.

Method: The paper presents a unified approach to error control via residuals in finite- and infinite-dimensional settings, provides an elementary proof of convergence for generalized Laplace analysis, and reviews state-of-the-art methods for computing continuous spectra and spectral measures.

Result: The methods enable reliable data-driven techniques for Koopman spectral analysis, including forecasting and spectral analysis capabilities for systems with continuous spectra and no spectral gaps.

Conclusion: This guide provides both newcomers and experts with a clear, structured overview of convergent data-driven techniques for Koopman spectral analysis, addressing key computational challenges in the field.

Abstract: Koopman operators provide a linear framework for data-driven analyses of
nonlinear dynamical systems, but their infinite-dimensional nature presents
major computational challenges. In this article, we offer an introductory guide
to Koopman learning, emphasizing rigorously convergent data-driven methods for
forecasting and spectral analysis. We provide a unified account of error
control via residuals in both finite- and infinite-dimensional settings, an
elementary proof of convergence for generalized Laplace analysis -- a variant
of filtered power iteration that works for operators with continuous spectra
and no spectral gaps -- and review state-of-the-art approaches for computing
continuous spectra and spectral measures. The goal is to provide both newcomers
and experts with a clear, structured overview of reliable data-driven
techniques for Koopman spectral analysis.

</details>


### [3] [Preconditioning and Reduced-Order Modeling of Navier-Stokes Equations in Complex Porous Microstructures](https://arxiv.org/abs/2510.22077)
*Kangan Li,Yashar Mehmani*

Main category: math.NA

TL;DR: The paper proposes novel algebraic and geometric preconditioners for solving incompressible Navier-Stokes equations in porous materials, with gPLMM showing best performance.


<details>
  <summary>Details</summary>
Motivation: Solving Navier-Stokes equations in porous microstructures requires solving large, ill-conditioned linear systems that demand efficient preconditioning for iterative solvers.

Method: Developed two algebraic preconditioners (aPLMM_NS and aPNM_NS) and two geometric preconditioners (gPLMM and gPNM) based on pore-level multiscale methods and pore network models, with exact zero normal-gradient boundary conditions.

Result: gPLMM is the best-performing preconditioner, followed by aPLMM_S for steady-state flow and aPLMM_NS for transient flow. Also developed a fast coarse-scale solver based on gPLMM.

Conclusion: The proposed preconditioners, particularly gPLMM, significantly improve convergence for Navier-Stokes simulations in porous materials and can be implemented on parallel machines.

Abstract: We aim to solve the incompressible Navier-Stokes equations within the complex
microstructure of a porous material. Discretizing the equations on a fine grid
using a staggered (e.g., marker-and-cell, mixed FEM) scheme results in a
nonlinear residual. Adopting the Newton method, a linear system must be solved
at each iteration, which is large, ill-conditioned, and has a saddle-point
structure. This demands an iterative (e.g., Krylov) solver, that requires
preconditioning to ensure rapid convergence. We propose two monolithic
\textit{algebraic} preconditioners, $a\mathrm{PLMM_{NS}}$ and
$a\mathrm{PNM_{NS}}$, that are generalizations of previously proposed forms by
the authors for the Stokes equations ($a\mathrm{PLMM_{S}}$ and
$a\mathrm{PNM_{S}}$). The former is based on the pore-level multiscale method
(PLMM) and the latter on the pore network model (PNM), both successful
approximate solvers. We also formulate faster-converging geometric
preconditioners $g\mathrm{PLMM}$ and $g\mathrm{PNM}$, which impose
$\partial_n\boldsymbol{u}\!=\!0$ (zero normal-gradient of velocity) exactly at
subdomain interfaces. Finally, we propose an accurate coarse-scale solver for
the steady-state Navier-Stokes equations based on $g\mathrm{PLMM}$, capable of
computing approximate solutions orders of magnitude faster. We benchmark our
preconditioners against state-of-the-art block preconditioners and show
$g\mathrm{PLMM}$ is the best-performing one, followed closely by
$a\mathrm{PLMM_{S}}$ for steady-state flow and $a\mathrm{PLMM_{NS}}$ for
transient flow. All preconditioners can be built and applied on parallel
machines.

</details>


### [4] [Nonconforming Linear Element Method for a Generalized Tensor-Valued Stokes Equation with Application to the Triharmonic Equation](https://arxiv.org/abs/2510.22125)
*Ziwen Gu,Xuehai Huang*

Main category: math.NA

TL;DR: A nonconforming linear element method for 3D generalized tensor-valued Stokes equation using Hessian complex, with discrete Helmholtz decomposition ensuring well-posedness and optimal error estimates. Extended to low-order decoupled FEM for triharmonic equation.


<details>
  <summary>Details</summary>
Motivation: To develop efficient finite element methods for complex 3D tensor-valued Stokes equations and extend to triharmonic equations using Hessian complex framework.

Method: Nonconforming linear element method with discrete Helmholtz decomposition for traceless tensor space; combined Morley-Wang-Xu elements for biharmonic subproblems with proposed method for triharmonic equation.

Result: Well-posedness established, optimal error estimates derived, and numerical experiments confirm theoretical convergence rates.

Conclusion: The proposed methods are effective for 3D tensor-valued Stokes and triharmonic equations, with verified convergence properties.

Abstract: A nonconforming linear element method is developed for a three-dimensional
generalized tensor-valued Stokes equation associated with the Hessian complex
in this paper. A discrete Helmholtz decomposition for the piecewise constant
space of traceless tensors is established, ensuring the well-posedness of the
nonconforming method, and optimal error estimates are derived. Building on
this, a low-order decoupled finite element method for the three-dimensional
triharmonic equation is constructed by combining the Morley-Wang-Xu element
methods for the biharmonic subproblems with the proposed nonconforming linear
element method. Numerical experiments confirm the theoretical convergence
rates.

</details>


### [5] [Stochastic Trace and Diagonal Estimator for Tensors](https://arxiv.org/abs/2510.22157)
*Bhisham Dev Verma,Rameshwar Pratap,Keegan Kang*

Main category: math.NA

TL;DR: Proposes unbiased estimators for trace and diagonal entries of higher-order tensors using tensor-vector queries, generalizing Hutchinson's and Bekas et al.'s matrix methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for estimating trace and diagonal entries only work for matrices (2D), but no analogous methods exist for higher-order tensors (N≥2) using tensor-vector queries.

Method: Develops unbiased estimators that generalize Hutchinson's and Bekas et al.'s matrix estimators to higher-order tensors, using tensor-vector multiplication queries.

Result: The proposed methods provide unbiased estimates of tensor trace and diagonal entries, reducing to known matrix estimators when N=2, with theoretical analysis and simulation support.

Conclusion: Successfully extends trace and diagonal estimation from matrices to higher-order tensors using tensor-vector queries, providing the first such estimators for tensors.

Abstract: We consider the problem of estimating the trace and diagonal entries of an
N-order tensor (where $N \geq 2$) under the framework where the tensor can only
be accessed through tensor-vector multiplication. The aim is to estimate the
tensor's diagonal entries and trace by minimizing the number of tensor-vector
queries. The seminal work of Hutchinson and its extended version due to Bekas
et al. give unbiased estimates of the trace and diagonal elements of a given
matrix, respectively, using matrix-vector queries. However, to the best of our
knowledge, no analogous results are known for estimating the trace and diagonal
entries of higher-order tensors using tensor-vector queries. This paper
addresses this gap and presents unbiased estimators for the trace and diagonal
entries of a tensor under this model. Our proposed methods can be seen as
generalizations of Hutchinson's and Bekas et al.'s estimators and reduce to
their estimators when N = 2. We provide a rigorous theoretical analysis of our
proposals and complement it with supporting simulations.

</details>


### [6] [Stable neural networks and connections to continuous dynamical systems](https://arxiv.org/abs/2510.22299)
*Matthias J. Ehrhardt,Davide Murari,Ferdia Sherry*

Main category: math.NA

TL;DR: This paper provides an overview of neural network stability research using continuous dynamical systems and optimal control theory, with practical implementation guidance and code examples.


<details>
  <summary>Details</summary>
Motivation: To address neural network instabilities like adversarial examples by connecting to continuous dynamical systems and optimal control theory, making the concepts accessible to students.

Method: Uses continuous dynamical systems and optimal control frameworks to design stable neural networks, providing theoretical background and practical implementation with code examples.

Result: Developed a comprehensive framework for understanding and implementing stable neural networks, including working code with interactive examples that can run on standard computers.

Conclusion: The paper successfully bridges theory and practice in neural network stability research, providing accessible educational materials that enable readers to implement and extend stable network designs.

Abstract: The existence of instabilities, for example in the form of adversarial
examples, has given rise to a highly active area of research concerning itself
with understanding and enhancing the stability of neural networks. We focus on
a popular branch within this area which draws on connections to continuous
dynamical systems and optimal control, giving a bird's eye view of this area.
We identify and describe the fundamental concepts that underlie much of the
existing work in this area. Following this, we go into more detail on a
specific approach to designing stable neural networks, developing the
theoretical background and giving a description of how these networks can be
implemented. We provide code that implements the approach that can be adapted
and extended by the reader. The code further includes a notebook with a
fleshed-out toy example on adversarial robustness of image classification that
can be run without heavy requirements on the reader's computer. We finish by
discussing this toy example so that the reader can interactively follow along
on their computer. This work will be included as a chapter of a book on
scientific machine learning, which is currently under revision and aimed at
students.

</details>


### [7] [Davis-Kahan Theorem under a moderate gap condition](https://arxiv.org/abs/2510.22393)
*Phuc Tran,Van Vu*

Main category: math.NA

TL;DR: New bound for eigenspace perturbation under moderate gap conditions using bootstrapping argument


<details>
  <summary>Details</summary>
Motivation: Classical Davis-Kahan theorem requires large eigenvalue gaps, but many practical scenarios have only moderate gaps

Method: Bootstrapping argument to derive new perturbation bound when perturbation matrix is uncorrelated to ground matrix

Result: Obtained efficient bound for moderate gap scenarios, believed to be sharp up to logarithmic term

Conclusion: New bound extends classical Davis-Kahan theorem to handle moderate gap conditions with uncorrelated perturbations

Abstract: The classical Davis-Kahan theorem provides an efficient bound on the
perturbation of eigenspaces of a matrix under a large (eigenvalue) gap
condition. In this paper, we consider the case when the gap is moderate. Using
a bootstrapping argument, we obtain a new bound which is efficient when the
perturbation matrix is uncorrelated to the ground matrix. We believe that this
bound is sharp up to a logarithmic term.

</details>


### [8] [Finite element analysis of density estimation using preintegration for elliptic PDE with random input](https://arxiv.org/abs/2510.22493)
*Alexander D. Gilbert*

Main category: math.NA

TL;DR: This paper analyzes the finite element error component in preintegration methods for uncertainty quantification of elliptic PDEs with random inputs, showing that cdf and pdf approximations achieve the same convergence rate as computing expected values.


<details>
  <summary>Details</summary>
Motivation: To extend previous work on density estimation for UQ problems by analyzing the spatial discretization error from finite element methods, which was not covered in the earlier paper that focused on quadrature aspects.

Method: First proves that finite element approximations satisfy preintegration theory assumptions including monotonicity, then analyzes finite element error, and finally bounds the combined finite element and quasi-Monte Carlo error.

Result: Under similar assumptions, the cumulative distribution function (cdf) and probability density function (pdf) can be approximated with the same rate of convergence as computing expected values for the simpler problem.

Conclusion: The finite element component of the error in preintegration methods for UQ problems with elliptic PDEs can be properly analyzed and bounded, achieving comparable convergence rates to simpler expected value computations.

Abstract: This paper analyses the finite element component of the error when using
preintegration to approximate the cdf and pdf for uncertainty quantification
(UQ) problems involving elliptic PDEs with random inputs. It is a follow up to
Gilbert, Kuo, Srikumar, SIAM J. Numer. Anal. 63 (2025), pp. 1025-1054, which
introduced a method of density estimation for a class of UQ problems, based on
computing the integral formulations of the cdf and pdf by performing an initial
smoothing preintegration step and then applying a quasi-Monte Carlo quadrature
rule to approximate the remaining high-dimensional integral. That paper
focussed on the quadrature aspect of the method, whereas this paper studies the
spatial discretisation of the PDE using finite element methods. First, it is
shown that the finite element approximation satisfies the required assumptions
for the preintegration theory, including the important monotonicity condition.
Then the finite element error is analysed and finally, the combined finite
element and quasi-Monte Carlo error is bounded. It is shown that under similar
assumptions, the cdf and pdf can be approximated with the same rate of
convergence as the much simpler problem of computing expected values.

</details>


### [9] [Multi-Scale Finite Expression Method for PDEs with Oscillatory Solutions on Complex Domains](https://arxiv.org/abs/2510.22497)
*Gareth Hardwick,Haizhao Yang*

Main category: math.NA

TL;DR: Enhanced Finite Expression Method (FEX) for solving oscillatory PDEs on complex domains with improved accuracy, interpretability, and efficiency through symbolic spectral composition, expanded expressivity, and eigenvalue formulation.


<details>
  <summary>Details</summary>
Motivation: Solving PDEs with highly oscillatory solutions on complex domains is challenging due to expensive representations for traditional methods and difficult optimization landscapes for ML approaches.

Method: Three key innovations: symbolic spectral composition for multiscale oscillatory behavior, redesigned linear input layer for expanded expressivity, and eigenvalue formulation for eigenvalue PDEs.

Result: FEX accurately resolves oscillatory PDEs on domains with multiple holes of varying shapes and sizes, achieving substantially higher accuracy than existing neural network-based solvers.

Conclusion: FEX provides a powerful and transparent framework for solving complex PDEs, yielding interpretable closed-form solutions that expose underlying problem structure, advantages absent in conventional methods.

Abstract: Solving partial differential equations (PDEs) with highly oscillatory
solutions on complex domains remains a challenging and important problem.
High-frequency oscillations and intricate geometries often result in
prohibitively expensive representations for traditional numerical methods and
lead to difficult optimization landscapes for machine learning-based
approaches. In this work, we introduce an enhanced Finite Expression Method
(FEX) designed to address these challenges with improved accuracy,
interpretability, and computational efficiency. The proposed framework
incorporates three key innovations: a symbolic spectral composition module that
enables FEX to learn and represent multiscale oscillatory behavior; a
redesigned linear input layer that significantly expands the expressivity of
the model; and an eigenvalue formulation that extends FEX to a new class of
problems involving eigenvalue PDEs. Through extensive numerical experiments, we
demonstrate that FEX accurately resolves oscillatory PDEs on domains containing
multiple holes of varying shapes and sizes. Compared with existing neural
network-based solvers, FEX achieves substantially higher accuracy while
yielding interpretable, closed-form solutions that expose the underlying
structure of the problem. These advantages, often absent in conventional finite
element, finite difference, and black-box neural approaches, highlight FEX as a
powerful and transparent framework for solving complex PDEs.

</details>


### [10] [Accelerated Tensor Completion via Trace-Regularized Fully-Connected Tensor Network](https://arxiv.org/abs/2510.22506)
*Wenchao Xie,Qingsong Wang,Chengcheng Yan,Zheng Peng*

Main category: math.NA

TL;DR: Proposes an efficient tensor completion model that incorporates trace regularization within the FCTN decomposition framework to enhance local detail recovery and continuity in reconstructed tensors.


<details>
  <summary>Details</summary>
Motivation: The fully-connected tensor network (FCTN) decomposition is effective for capturing low-rank characteristics but has limitations in recovering local details in reconstructed tensors.

Method: Uses trace regularization based on mode-k unfolding of FCTN factors combined with periodically modified negative Laplacian, solved via proximal alternating minimization (PAM) with an intermediate tensor reuse mechanism to reduce runtime.

Result: Numerical experiments show the method outperforms existing approaches, with runtime reductions of 10%-30% (more significant for larger-scale data) without affecting image recovery quality.

Conclusion: The proposed trace-regularized FCTN decomposition effectively enhances local recovery performance and continuity while maintaining computational efficiency through the reuse mechanism.

Abstract: The fully-connected tensor network (FCTN) decomposition has gained prominence
in the field of tensor completion owing to its powerful capacity to capture the
low-rank characteristics of tensors. Nevertheless, the recovery of local
details in the reconstructed tensor still leaves scope for enhancement. In this
paper, we propose efficient tensor completion model that incorporates trace
regularization within the FCTN decomposition framework. The trace
regularization is constructed based on the mode-$k$ unfolding of the FCTN
factors combined with periodically modified negative laplacian. The trace
regularization promotes the smoothness of the FCTN factors through discrete
second-order derivative penalties, thereby enhancing the continuity and local
recovery performance of the reconstructed tensor. To solve the proposed model,
we develop an efficient algorithm within the proximal alternating minimization
(PAM) framework and theoretically prove its convergence. To reduce the runtime
of the proposed algorithm, we design an intermediate tensor reuse mechanism
that can decrease runtime by 10\%-30\% without affecting image recovery, with
more significant improvements for larger-scale data. A comprehensive complexity
analysis reveals that the mechanism attains a reduced computational complexity.
Numerical experiments demonstrate that the proposed method outperforms existing
approaches.

</details>


### [11] [Some aspects of neural network parameter optimization for joint inversion of gravitational and magnetic fields](https://arxiv.org/abs/2510.22564)
*Yanfei Wang,Dmitry V. Churbanov,Raul L. Argun,Alexander V. Gorbachev,Alexander S. Leonov,Dmitry V. Lukyanenko*

Main category: math.NA

TL;DR: The paper presents an optimized neural network for joint inversion of 3D gravitational and magnetic fields in mineral exploration, using a two-level algorithm that finds geometrically close source distributions.


<details>
  <summary>Details</summary>
Motivation: To solve ill-posed inverse problems in mineral exploration by developing an efficient neural network approach for joint inversion of gravitational and magnetic fields, improving source distribution accuracy.

Method: A two-level neural network algorithm: lower level uses two separate networks to compute gravitational and magnetic source distributions from field measurements, upper level calculates structural residual between distributions and minimizes it during training to achieve geometrically close source geometries.

Result: Test calculations show high quality joint inversion performance. Real-field data from Jussara region, Brazil demonstrates successful application. The approach also enables determination of both geometric distribution and physical intensities of sources.

Conclusion: The optimized neural network approach effectively solves joint inversion problems for gravitational and magnetic fields, providing accurate source distributions and physical intensities for mineral exploration applications.

Abstract: We consider the optimization of a neural network previously developed by the
authors for the joint inversion of 3D gravitational and magnetic fields in the
context of mineral exploration. The distinctive feature of this neural network
is that it solves ill-posed (ill-conditioned) inverse problems. The neural
network implements a special two-level algorithm. The lower level of the
algorithm uses two neural networks with equivalent architectures. The first of
them computes the gravitational field sources in a given domain from
measurements of this field on a remote surface. The second neural network
processes magnetic field measured on the same surface to find magnetic sources
in the same domain. The found source distributions are used at the upper level
of the algorithm to calculate their structural residual, which determines the
degree of difference (closeness) of their geometries. As a result, minimizing
this residual, when training a neural network at the upper level, implements a
computational algorithm that yields geometrically close source distributions of
different fields. The article examines in detail the possibilities of
optimizing some elements of the neural networks and the algorithms used
(datasets, training process, specific form of loss functions, etc.) Test
calculations for model problem demonstrate high quality of joint inversion by
our optimized neural networks approach. Calculations were also carried out for
the joint processing of real-feald data from gravity and magnetic exploration
in Jussara region, Goias State, Brazil. The article also considers the issue of
determining in joint field inversion not only the geometric distribution of
sources, but also their physical intensities.

</details>


### [12] [Data-driven dimensionally decomposed generalized polynomial chaos expansion for forward uncertainty quantification](https://arxiv.org/abs/2510.22642)
*Hojun Choi,Eunho Heo,Dongjin Lee*

Main category: math.NA

TL;DR: A data-driven DD-GPCE method is introduced that eliminates the need for prior knowledge of input distributions by inferring them directly from sample data using KDE and whitening transformation.


<details>
  <summary>Details</summary>
Motivation: Traditional DD-GPCE requires prior knowledge of input distributions, which is often unavailable in practice, limiting its applicability to uncertainty quantification with high-dimensional inputs.

Method: Input distributions are inferred from sample data using smoothed-bootstrap kernel density estimation (KDE), with DD-GPCE enabling KDE to handle high-dimensional inputs through low-dimensional marginal estimation. A whitening transformation via Monte Carlo Simulation is then used to generate measure-consistent orthonormal basis functions.

Result: The method produces more accurate estimates of output mean and variance compared to conventional data-driven approaches that assume Gaussian input distributions, as demonstrated in mathematical examples and a practical 3D mobility design with 20 random inputs.

Conclusion: The proposed data-driven DD-GPCE method successfully extends the applicability of uncertainty quantification to high-dimensional inputs without requiring prior knowledge of input distributions, outperforming conventional Gaussian assumption approaches.

Abstract: Dimensionally decomposed generalized polynomial chaos expansion (DD-GPCE)
efficiently performs forward uncertainty quantification (UQ) in complex
engineering systems with high-dimensional random inputs of arbitrary
distributions. However, constructing the measure-consistent orthonormal
polynomial bases in DD-GPCE requires prior knowledge of input distributions,
which is often unavailable in practice. This work introduces a data-driven
DD-GPCE method that eliminates the need for such prior knowledge, extending its
applicability to UQ with high-dimensional inputs. Input distributions are
inferred directly from sample data using smoothed-bootstrap kernel density
estimation (KDE), while the DD-GPCE framework enables KDE to handle
high-dimensional inputs through low-dimensional marginal estimation. We then
use the estimated input distributions to perform a whitening transformation via
Monte Carlo Simulation, which enables generation of measure-consistent
orthonormal basis functions. We demonstrate the accuracy of the proposed method
in both mathematical examples and stochastic dynamic analysis for a practical
three-dimensional mobility design involving twenty random inputs. The results
indicate that the proposed method produces more accurate estimates of the
output mean and variance compared to the conventional data-driven approach that
assumes Gaussian input distributions.

</details>


### [13] [Surface layers and linearized water waves: a boundary integral equation framework](https://arxiv.org/abs/2510.22748)
*Travis Askham,Tristan Goodwill,Jeremy G Hoskins,Peter Nekrasov,Manas Rachh*

Main category: math.NA

TL;DR: This paper develops integral equation methods for analyzing surface waves in fluids with floating plates/membranes containing holes, focusing on partial membrane and polynya cases.


<details>
  <summary>Details</summary>
Motivation: Surface wave dynamics change when floating plates/membranes are present, creating mathematical challenges due to jumps in derivative orders across boundaries between covered and uncovered regions.

Method: General integral equation approach for infinite depth linearized surface waves with compact holes in plates/membranes, analyzing boundary integral equations as Fredholm second kind problems.

Result: Developed flexible, fast algorithms for discretizing and solving these equations, demonstrating robustness and scalability in resolving surface wave phenomena through numerical examples.

Conclusion: The integral equation approach provides effective mathematical framework and computational tools for analyzing surface wave problems with floating plates/membranes containing holes.

Abstract: The dynamics of surface waves traveling along the boundary of a liquid medium
are changed by the presence of floating plates and membranes, contributing to a
number of important phenomena in a wide range of applications. Mathematically,
if the fluid is only partly covered by a plate or membrane, the order of
derivatives of the surface-boundary conditions jump between regions of the
surface. In this work, we consider a general class of problems for infinite
depth linearized surface waves in which the plate or membrane has a compact
hole or multiple holes. For this class of problems, we describe a general
integral equation approach, and for two important examples, the partial
membrane and the polynya, we analyze the resulting boundary integral equations.
In particular, we show that they are Fredholm second kind and discuss key
properties of their solutions. We develop flexible and fast algorithms for
discretizing and solving these equations, and demonstrate their robustness and
scalability in resolving surface wave phenomena through several numerical
examples.

</details>


### [14] [HoSGFEM: High-order stable generalized finite element method for elliptic interface problem](https://arxiv.org/abs/2510.22796)
*Bingying Zhao,Yin Song,Quanling Deng,Xin Li*

Main category: math.NA

TL;DR: A new high-order stable Generalized Finite Element Method (HoSGFEM) is proposed for elliptic interface problems, achieving arbitrary high-order accuracy with optimal convergence rates and stable system conditioning (O(h^{-2}) growth).


<details>
  <summary>Details</summary>
Motivation: Existing GFEMs for interface problems struggle with stable high-order implementations - the highest known order was only 2. There's a need for methods that combine arbitrary high-order accuracy with robust system conditioning.

Method: Two key innovations: 1) dimensionality-reduced auxiliary locally supported piecewise polynomials satisfying partition of unity for interface elements, 2) element-based enrichment using d{1,(x-x_c^e),...,(y-y_c^e)^{p-1}} instead of global functions, creating large angle with standard FEM space.

Result: HoSGFEM achieves optimal convergence rates for arbitrary p-th order elements, maintains FEM-comparable system condition number with O(h^{-2}) growth, and shows robustness as element boundaries approach interfaces in both straight and curved interface scenarios.

Conclusion: The proposed construction enables stable arbitrary high-order GFEMs for interface problems, overcoming previous limitations and providing a unified framework for high-order enrichment with guaranteed stability and optimal convergence.

Abstract: The Generalized Finite Element Method (GFEM) is an effective unfitted
numerical method for handling interface problems. By augmenting the standard
FEM space with an appropriate enrichment space, GFEM can accurately capture C^0
solutions across the interfaces. While numerous GFEMs for interface problems
have been studied, establishing a stable high-order GFEM with optimal
convergence rates and robust system conditioning remains a challenge. The
highest known order of two was established by Zhang and Babu\v{s}ka (SGFEM2,
Comput. Methods Appl. Mech. Engrg. 363 (2020), 112889). In this paper, we
propose a unified enrichment space construction and establish arbitrary
high-order stable GFEMs (HoSGFEM) for elliptic interface problems. The main
idea distinguishes itself from Zhang and Babu\v{s}ka's SGFEM2 substantially and
it is twofold: a) we construct dimensionality-reduced auxiliary locally
supported piecewise polynomials that satisfy the partition of unity property
for elements containing interfaces; b) we construct the enrichment scheme based
on d{1,(x-x_c^e),...,(y-y_c^e)^{p-1}} (d is the distance function; (x_c^e,
y_c^e) is the center of the element containing interface, thus element-based)
for arbitrary p-th order elements instead of d, d{1,x,y} or d{1,x,y,x^2,xy,y^2}
(global functions) for p=1,2 in the literature. This idea results in an
enrichment space that has a large angle with the standard FEM space, leading to
the stability of the method with system condition number growing in order
O(h^{-2}). We establish optimal convergence rates for HoSGFEM solutions under
the proposed construction. Various numerical experiments with both straight and
curved interfaces demonstrate the optimal convergence, FEM-comparable system
condition number with O(h^{-2}) growth, and robustness as element boundaries
approach interfaces.

</details>


### [15] [Rational Approximation via p-AAA on Scattered Data Sets](https://arxiv.org/abs/2510.22861)
*Linus Balicki,Serkan Gugercin*

Main category: math.NA

TL;DR: Extension of the parametric AAA algorithm to handle scattered data by reformulating rational least-squares optimization with interpolation constraints, enabling approximation without requiring uniform grid sampling.


<details>
  <summary>Details</summary>
Motivation: The original p-AAA algorithm only works with grid data, requiring function samples at every combination of discrete sampling points. This limitation restricts its applicability to scattered data sets that don't follow a uniform grid structure.

Method: Proposed several formulations for rational least-squares optimization problems that incorporate interpolation conditions as constraints. Used structured matrices whose singular value decompositions provide closed-form solutions to the underlying least-squares problems.

Result: Successfully extended p-AAA to operate on arbitrary sampling points without requiring uniform/grid sampling structure. Several examples demonstrate computational effectiveness of the proposed procedure.

Conclusion: The extended p-AAA algorithm provides an effective approximation method for multivariate rational approximation that works with scattered data, overcoming the grid sampling limitation of the original formulation.

Abstract: Many algorithms for approximating data with rational functions are built on
interpolation or least-squares approximation. Inspired by the adaptive
Antoulas-Anderson (AAA) algorithm for the univariate case, the parametric
adaptive Antoulas-Anderson (p-AAA) algorithm extends this idea to the
multivariate setting, combining least-squares and interpolation formulations
into a single effective approximation procedure. In its original formulation
p-AAA operates on grid data, requiring access to function samples at every
combination of discrete sampling points in each variable. In this work we
extend the p-AAA algorithm to scattered data sets, without requiring
uniform/grid sampling. In other words, our proposed p-AAA formulation operates
on a set of arbitrary sampling points and is not restricted to a grid structure
for the sampled data. Towards this goal, we introduce several formulations for
rational least-squares optimization problems that incorporate interpolation
conditions via constraints. We analyze the structure of the resulting
optimization problems and introduce structured matrices whose singular value
decompositions yield closed-form solutions to the underlying least-squares
problems. Several examples illustrate computational aspects and the
effectiveness of our proposed procedure.

</details>


### [16] [Validity of relaxation models arising from numerical schemes for hyperbolic-parabolic systems](https://arxiv.org/abs/2510.22923)
*Zhiting Ma,Weifeng Zhao*

Main category: math.NA

TL;DR: This paper validates relaxation models for hyperbolic-parabolic systems using convergence criteria theory, demonstrates convergence for five existing models, and proposes a new general multi-dimensional relaxation model that works for general systems.


<details>
  <summary>Details</summary>
Motivation: To justify the convergence of relaxation models used in numerical schemes for hyperbolic-parabolic systems, as existing validation was lacking despite their importance in constructing effective numerical schemes.

Method: Employed recently proposed theory for general hyperbolic relaxation systems to verify convergence criteria, analyzed five representative relaxation models, and proposed a new relaxation model for general multi-dimensional hyperbolic-parabolic systems.

Result: Successfully demonstrated convergence and approximation validity for five existing relaxation models, and showed that the newly proposed model satisfies convergence criteria under mild assumptions.

Conclusion: The work provides solid theoretical foundation for relaxation-based numerical schemes and introduces a more general relaxation model applicable to broader classes of hyperbolic-parabolic systems.

Abstract: This work is concerned with relaxation models arising from numerical schemes
for hyperbolic-parabolic systems. Such models are a hyperbolic system with both
the hyperbolic part and the stiff source term involving a small positive
parameter, and thus are endowed with complicated multiscale properties.
Relaxation models are the basis of constructing corresponding numerical schemes
and a critical issue is the convergence of their solutions to those of the
given target systems, the justification of which is still lacking. In this
work, we employ the recently proposed theory for general hyperbolic relaxation
systems to validate relaxation models in numerical schemes of
hyperbolic-parabolic systems. By verifying the convergence criteria, we
demonstrate the convergence, and thereby the approximation validity, of five
representative relaxation models, providing a solid basis for the effectiveness
of the corresponding numerical schemes. Moreover, we propose a new relaxation
model for the general multi-dimensional hyperbolic-parabolic system. With some
mild assumptions on the system, we show that the proposed model satisfies the
convergence criteria. We remark that the existing relaxation models are
constructed only for a special case of hyperbolic-parabolic system, while our
new relaxation model is valid for general systems.

</details>


### [17] [MultiLevel Variational MultiScale (ML-VMS) framework for large-scale simulation](https://arxiv.org/abs/2510.23004)
*Lei Zhang,Jiachen Guo,Shaoqiang Tang,Thomas J. R. Hughes,Wing Kam Liu*

Main category: math.NA

TL;DR: The ML-VMS method integrates multilevel mesh strategy with VMS framework using C-HiDeNN neural network basis, enabling efficient coupling of multiple mesh resolutions with theoretical guarantees and significant computational speedups.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient computational method that can handle problems with localized high-interest regions while maintaining computational efficiency across different scales.

Method: Combines multilevel mesh strategy with Variational Multiscale framework using C-HiDeNN neural network as approximation basis, with coarse mesh throughout domain and localized fine meshes in high-interest subdomains.

Result: ML-VMS with C-HiDeNN achieves faster computational time than FEM with comparable accuracy. For large-scale LPBF heat transfer problem (10^10 DoFs), 3-level ML-VMS C-HiDeNN-TD achieves ~5,000x speedup over single-level linear FEM-TD ROM on single CPU.

Conclusion: ML-VMS method successfully integrates multilevel mesh strategy with VMS framework, providing efficient computational framework with theoretical error analysis and significant performance improvements for large-scale problems.

Abstract: In this paper, we propose the MultiLevel Variational MultiScale (ML-VMS)
method, a novel approach that seamlessly integrates a multilevel mesh strategy
into the Variational Multiscale (VMS) framework. A key feature of the ML-VMS
method is the use of the Convolutional Hierarchical Deep Neural Network
(C-HiDeNN) as the approximation basis. The framework employs a coarse mesh
throughout the domain, with localized fine meshes placed only in subdomains of
high interest, such as those surrounding a source. Solutions at different
resolutions are robustly coupled through the variational weak form and
interface conditions. Compared to existing multilevel methods, ML-VMS (1) can
couple an arbitrary number of mesh levels across different scales using
variational multiscale framework; (2) allows approximating functions with
arbitrary orders with linear finite element mesh due to the C-HiDeNN basis; (3)
is supported by a rigorous theoretical error analysis; (4) features several
tunable hyperparameters (e.g., order $p$, patch size $s$) with a systematic
guide for their selection. We first show the theoretical error estimates of
ML-VMS. Then through numerical examples, we demonstrate that ML-VMS with the
C-HiDeNN takes less computational time than the FEM basis given comparable
accuracy. Furthermore, we incorporate a space-time reduced-order model (ROM)
based on C-HiDeNN-Tensor Decomposition (TD) into the ML-VMS framework. For a
large-scale single-track laser powder bed fusion (LPBF) transient heat transfer
problem that is equivalent to a full-order finite element model with $10^{10}$
spatial degrees of freedom (DoFs), our 3-level ML-VMS C-HiDeNN-TD achieves an
approximately 5,000x speedup on a single CPU over a single-level linear FEM-TD
ROM.

</details>


### [18] [Effective numerical integration on complex shaped elements by discrete signed measures](https://arxiv.org/abs/2510.23069)
*Laura Rinaldi,Alvise Sommariva,Marco Vianello*

Main category: math.NA

TL;DR: A stable polynomial moment-based compression method for multivariate measures using discrete signed measures, avoiding conditioning issues by not requiring matrix factorization or inversion.


<details>
  <summary>Details</summary>
Motivation: To develop a cheap and stable approach for compressing multivariate measures that avoids the conditioning problems associated with matrix factorization and inversion methods.

Method: Uses an orthonormal basis and low-cardinality algebraic quadrature formula for an auxiliary measure in a bounding set, creating discrete signed measures for polynomial moment-based compression.

Result: Provides bounds for the sum of absolute values of signed measure weights, with applications to efficient quadrature on curved planar elements with spline boundary and compression of QMC integration on 3D complex-shaped elements.

Conclusion: The method offers a stable and computationally efficient alternative for multivariate measure compression that avoids numerical conditioning issues while maintaining practical applicability in high-order FEM/VEM and QMC integration contexts.

Abstract: We discuss a cheap and stable approach to polynomial moment-based compression
of multivariate measures by discrete signed measures. The method is based on
the availability of an orthonormal basis and a low-cardinality algebraic
quadrature formula for an auxiliary measure in a bounding set. Differently from
other approaches, no conditioning issue arises since no matrix factorization or
inversion is needed. We provide bounds for the sum of the absolute values of
the signed measure weights, and we make two examples: efficient quadrature on
curved planar elements with spline boundary (in view of the application to
high-order FEM/VEM), and compression of QMC integration on 3D elements with
complex shape.

</details>


### [19] [Perturbation Function Iteration Method: A New Framework for Solving Periodic Solutions of Non-linear and Non-smooth Systems](https://arxiv.org/abs/2510.23071)
*Limin Cao,Yanmao Chen,Li Wang,Loic Salles,Zechang Zheng*

Main category: math.NA

TL;DR: PFIM is a novel perturbation-based method that computes periodic responses in nonlinear/non-smooth systems more efficiently than traditional methods like HBM and SM, achieving up to 100x speedup for high-dimensional non-smooth problems.


<details>
  <summary>Details</summary>
Motivation: Existing methods (HBM, SM) face limitations in complex, high-dimensional, or non-smooth systems due to unreliable and expensive Jacobian matrix computations, creating a need for more robust and efficient alternatives.

Method: PFIM transforms nonlinear equations into time-varying linear systems using perturbation theory and solves them via piecewise constant approximation, avoiding Fourier truncation errors and simplifying Jacobian computation.

Result: PFIM achieves quadratic convergence in smooth systems and robust linear convergence in non-smooth cases, with computational costs up to two orders of magnitude lower than HBM for comparable accuracy in high-dimensional non-smooth systems.

Conclusion: PFIM provides a robust and efficient alternative for periodic response analysis in complex nonlinear dynamical systems, with strong potential for practical engineering applications.

Abstract: Computing accurate periodic responses in strongly nonlinear or even
non-smooth vibration systems remains a fundamental challenge in nonlinear
dynamics. Existing numerical methods, such as the Harmonic Balance Method (HBM)
and the Shooting Method (SM), have achieved notable success but face intrinsic
limitations when applied to complex, high-dimensional, or non-smooth systems. A
key bottleneck is the construction of Jacobian matrices for the associated
algebraic equations; although numerical approximations can avoid explicit
analytical derivation, they become unreliable and computationally expensive for
large-scale or non-smooth problems. To overcome these challenges, this study
proposes the Perturbation Function Iteration Method (PFIM), a novel framework
built upon perturbation theory. PFIM transforms nonlinear equations into
time-varying linear systems and solves their periodic responses via a piecewise
constant approximation scheme. Unlike HBM, PFIM avoids the trade-off between
Fourier truncation errors and the Gibbs phenomenon in non-smooth problems by
employing a basis-free iterative formulation, while significantly simplifying
the Jacobian computation. Extensive numerical studies, including self-excited
systems, parameter continuation, systems with varying smoothness, and
high-dimensional finite element models, demonstrate that PFIM achieves
quadratic convergence in smooth systems and maintains robust linear convergence
in highly non-smooth cases. Moreover, comparative analyses show that, for
high-dimensional non-smooth systems, PFIM attains solutions of comparable
accuracy with computational costs up to two orders of magnitude lower than HBM.
These results indicate that PFIM provides a robust and efficient alternative
for periodic response analysis in complex nonlinear dynamical systems, with
strong potential for practical engineering applications.

</details>


### [20] [Multiscale modeling for contact problem with high-contrast heterogeneous coefficients with primary-dual formulation](https://arxiv.org/abs/2510.23073)
*Zishang Li,Changqing Ye,Eric T. Chung*

Main category: math.NA

TL;DR: A novel iterative multiscale framework combining CEM-GMsFEM with primal-dual active set strategy for solving high-contrast Signorini contact problems, featuring contrast-robust approximation and iterative basis updates at contact boundaries.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve high-contrast contact problems of Signorini type, which are challenging due to heterogeneous media and complex contact boundaries requiring fine-scale resolution.

Method: Integrates constrained energy minimizing generalized multiscale finite element method (CEM-GMsFEM) with primal-dual active set strategy from semismooth Newton methods. Uses local spectral problems to construct auxiliary multiscale space, derives energy minimizing basis functions on oversampled domains, and iteratively updates multiscale bases only at contact boundaries.

Result: The method provides contrast-robust reduced-order approximation and demonstrates robustness and efficiency in capturing fine-scale features near contact boundaries in numerical experiments with heterogeneous media and high-contrast coefficients.

Conclusion: The proposed iterative multiscale framework successfully addresses high-contrast contact problems by combining multiscale modeling with active set strategies, achieving both theoretical guarantees (error estimates and finite step convergence) and practical effectiveness in capturing fine-scale contact boundary features.

Abstract: In this paper, we propose a novel iterative multiscale framework for solving
high-contrast contact problems of Signorini type. The method integrates the
constrained energy minimizing generalized multiscale finite element method
(CEM-GMsFEM) with a primal-dual active set strategy derived from semismooth
Newton methods. First, local spectral problems are employed to construct an
auxiliary multiscale space, from which energy minimizing multiscale basis
functions are derived on oversampled domains, yielding a contrast-robust
reduced-order approximation of the underlying partial differential equation.
The multiscale bases are updated iteratively, but only at contact boundary,
during the active set evolution process. Rigorous analysis is provided to
establish error estimates and finite step convergence of the iterative scheme.
Numerical experiments on heterogeneous media with high-contrast coefficients
demonstrate that the proposed approach is both robust and efficient in
capturing fine-scale features near contact boundaries.

</details>


### [21] [Numerical Spectrum Linking: Identification of Governing PDE via Koopman-Chebyshev Approximation](https://arxiv.org/abs/2510.23078)
*Phonepaserth Sisaykeo,Shogo Muramatsu*

Main category: math.NA

TL;DR: A numerical framework using Chebyshev polynomial approximation to identify PDEs from observation data, connecting Koopman operators with differential operators for interpretable operator learning.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between data-driven approaches like DMD that approximate Koopman operators without clear connection to differential operators, and the need for interpretable PDE identification from dynamical system data.

Method: Constructs finite-dimensional Koopman matrices by projecting dynamics onto Chebyshev basis, capturing both differential and nonlinear terms to establish numerical link between Koopman and differential operators.

Result: Numerical experiments on benchmark dynamical systems confirm the accuracy and efficiency of the approach.

Conclusion: The framework enables interpretable operator learning and lays foundation for future integration with symbolic regression to construct explicit mathematical models directly from data.

Abstract: A numerical framework is proposed for identifying partial differential
equations (PDEs) governing dynamical systems directly from their observation
data using Chebyshev polynomial approximation. In contrast to data-driven
approaches such as dynamic mode decomposition (DMD), which approximate the
Koopman operator without a clear connection to differential operators, the
proposed method constructs finite-dimensional Koopman matrices by projecting
the dynamics onto a Chebyshev basis, thereby capturing both differential and
nonlinear terms. This establishes a numerical link between the Koopman and
differential operators. Numerical experiments on benchmark dynamical systems
confirm the accuracy and efficiency of the approach, underscoring its potential
for interpretable operator learning. The framework also lays a foundation for
future integration with symbolic regression, enabling the construction of
explicit mathematical models directly from data.

</details>


### [22] [High-order Computation of Floquet Multipliers and Subspaces using Multistep Methods](https://arxiv.org/abs/2510.23082)
*Yehao Zhang,Yuncheng Xu,Yichen Tan,Yangfeng Su*

Main category: math.NA

TL;DR: Multistep methods enable high-order accurate computation of Floquet multipliers for limit cycle analysis, overcoming accuracy limitations of backward Euler and computational costs of collocation methods.


<details>
  <summary>Details</summary>
Motivation: Accurate Floquet multiplier computation is essential for analyzing limit cycles in dynamical systems and periodic steady states in RF simulation, but existing methods face trade-offs between accuracy and computational efficiency.

Method: Apply multistep methods to derive periodic polynomial eigenvalue problem, prove convergence properties of Floquet multipliers and spurious eigenvalues, and develop pTOAR algorithm for efficient computation of dominant eigenpairs.

Result: Multistep methods achieve high-order accuracy with computational and memory costs only slightly higher than backward Euler method, while handling spurious eigenvalues that converge to zero.

Conclusion: Multistep methods provide an effective balance between accuracy and efficiency for large-scale Floquet multiplier computation, with theoretical convergence guarantees and practical performance demonstrated through numerical experiments.

Abstract: Accurate and efficient computation of Floquet multipliers and subspaces is
essential for analyzing limit cycle in dynamical systems and periodic steady
state in Radio Frequency (RF) simulation. This problem is typically addressed
by solving a periodic linear eigenvalue problem, which is discretized from the
linear periodic time-varying system using one-step methods. The backward Euler
method offers a computationally inexpensive overall workflow but has limited
accuracy. In contrast, one-step collocation methods achieve higher accuracy
through over-sampling, explicit matrix construction, and condensation, thus
become costly for large-scale sparse cases. We apply multistep methods to
derive a periodic polynomial eigenvalue problem, which introduces additional
spurious eigenvalues. Under mild smoothness assumptions, we prove that as the
stepsize decreases, the computed Floquet multipliers and their associated
invariant subspace converge with higher order, while the spurious eigenvalues
converge to zero. To efficiently solve large-scale problems, we propose pTOAR,
a memory-efficient iterative algorithm for computing the dominant Floquet
eigenpairs. Numerical experiments demonstrate that multistep methods achieves
high order accuracy, while its computational and memory costs are only
marginally higher than those of the backward Euler method.

</details>


### [23] [Deep Forward-Backward Dynamic Programming Schemes for High-Dimensional Semilinear Nonlocal PDEs and FBSDE with Jumps](https://arxiv.org/abs/2510.23091)
*Wansheng Wang,Jiangtao Pan,Jie Wang,Zaijun Ye*

Main category: math.NA

TL;DR: A new deep learning algorithm for solving high-dimensional parabolic integro-differential equations (PIDEs) and forward-backward stochastic differential equations with jumps (FBSDEJs), extending the DBDP2 scheme and dynamic programming approach.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for solving high-dimensional PIDEs and FBSDEJs, which are challenging problems in mathematical finance and physics.

Method: Simultaneously approximates the solution and integral kernel using deep neural networks, while approximating the gradient through numerical differential techniques. Extends the DBDP2 scheme for semilinear PDEs.

Result: Numerical experiments confirm theoretical results and verify the effectiveness of the proposed methods.

Conclusion: The novel algorithm successfully solves high-dimensional PIDEs and FBSDEJs, with error estimates for integral kernel approximation being crucial for deriving overall error estimates.

Abstract: We propose a new deep learning algorithm for solving high-dimensional
parabolic integro-differential equations (PIDEs) and forward-backward
stochastic differential equations with jumps (FBSDEJs). This novel algorithm
can be viewed as an extension and generalization of the DBDP2 scheme and a
dynamic programming version of the forward-backward algorithm proposed recently
for high-dimensional semilinear PDEs and semilinear PIDEs, respectively.
Different from the DBDP2 scheme for semilinear PDEs, our algorithm approximate
simultaneously the solution and the integral kernel by deep neural networks,
while the gradient of the solution is approximated by numerical differential
techniques. The related error estimates for the integral kernel approximation
play key roles in deriving error estimates for the novel algorithm. Numerical
experiments confirm our theoretical results and verify the effectiveness of the
proposed methods.

</details>


### [24] [Modeling a Smooth Surface by a Constrained Biharmonic Equation with Application in Soil Science](https://arxiv.org/abs/2510.23195)
*Samson Seifu Bekele,Maregnesh Mechal Wolde,Claus Führer,Nils-Otto Kitterød,Anne Kværnø*

Main category: math.NA

TL;DR: A method for mathematical surface modeling using discrete biharmonic equations with inner point and curve data, designed for limited empirical data scenarios like soil thickness mapping.


<details>
  <summary>Details</summary>
Motivation: To address surface construction problems with very limited measured data, particularly for applications like soil thickness modeling where empirical data is sparse.

Method: Solves discrete biharmonic equations over a domain using inner point data (for load vector) and inner curve data (for boundary values), with a special regularization approach for the ill-posed boundary data construction.

Result: The method enables smooth surface generation from sparse empirical data, demonstrated through soil thickness and geological map applications.

Conclusion: The proposed approach provides an effective solution for surface modeling with limited data, particularly useful for geological and soil thickness mapping applications where data scarcity is common.

Abstract: This paper presents a method for mathematical modelling of surfaces
conditioned on empirical data. It is based on solving a discrete biharmonic
equation over a domain with given inner point and inner curve data. The inner
curve data is used to model boundary values while the inner point data is used
for modeling a load vector with the goal to generate a smooth surface. The
construction of boundary data is an ill-posed problem, for which a special
regularization approach is suggested.
  The method is designed for surface construction problems with a very limited
amount of measured data. In the paper we apply the method by using empirical
data of soil thickness and geological maps indicating exposed bedrock regions.

</details>


### [25] [Higher order numerical schemes for SPDEs with additive Noise](https://arxiv.org/abs/2510.23210)
*Abhishek Chaudhary,Andreas Prohl*

Main category: math.NA

TL;DR: High-order numerical schemes for stochastic heat and wave equations with additive noise achieve improved convergence rates (3/2 for heat equation, 2 for wave equation) compared to standard Euler schemes.


<details>
  <summary>Details</summary>
Motivation: Standard Euler schemes for SPDEs have limited convergence rates (1/2 to 1) due to low temporal regularity of noise, motivating the development of higher-order schemes.

Method: Modified Crank-Nicolson scheme with proper numerical quadrature for noise term, reformulating SPDEs as random PDEs.

Result: Achieved strong convergence rate of 3/2 for stochastic heat equation and order 2 for stochastic wave equation.

Conclusion: The proposed high-order schemes significantly improve convergence rates for linear stochastic heat and wave equations with additive noise.

Abstract: We present high-order numerical schemes for linear stochastic heat and wave
equations with Dirichlet boundary conditions, driven by additive noise.
Standard Euler schemes for SPDEs are limited to an order convergence between
1/2 and 1 due to the low temporal regularity of noise. For the stochastic heat
equation, a modified Crank-Nicolson scheme with proper numerical quadrature
rule for the noise term in its reformulation as random PDE achieves a strong
convergence rate of 3/2. For the stochastic wave equation with additive noise a
corresponding approach leads to a scheme which is of order 2.

</details>


### [26] [Noisy nonlinear information and entropy numbers](https://arxiv.org/abs/2510.23213)
*David Krieg,Erich Novak,Leszek Plaskota,Mario Ullrich*

Main category: math.NA

TL;DR: This paper analyzes how deterministic noise affects the quality of optimal discontinuous information for vector recovery, showing that while continuous measurements can provide exponential speed-up over linear ones in noiseless settings, this advantage is limited but still significant in noisy environments.


<details>
  <summary>Details</summary>
Motivation: Recent research showed that continuous adaptive measurements can recover vectors from R^m with only O(log m) measurements, providing exponential speed-up over linear measurements. However, it's unclear how this advantage holds up when measurements are disturbed by deterministic noise.

Method: The authors characterize the quality of optimal discontinuous information disturbed by deterministic noise using entropy numbers analysis.

Result: The analysis reveals that in the presence of noise, the potential gain of continuous over linear measurements is limited but remains significant in some cases.

Conclusion: While deterministic noise constrains the advantage of continuous measurements over linear ones, the benefit of continuous measurements still persists and can be substantial in certain scenarios.

Abstract: It is impossible to recover a vector from $\mathbb{R}^m$ with less than $m$
linear measurements, even if the measurements are chosen adaptively. Recently,
it has been shown that one can recover vectors from $\mathbb{R}^m$ with
arbitrary precision using only $O(\log m)$ continuous (even Lipschitz) adaptive
measurements, resulting in an exponential speed-up of continuous information
compared to linear information for various approximation problems. In this
note, we characterize the quality of optimal (dis-)continuous information that
is disturbed by deterministic noise in terms of entropy numbers. This shows
that in the presence of noise the potential gain of continuous over linear
measurements is limited, but significant in some cases.

</details>


### [27] [Stability analysis of discontinuous Galerkin with a high order embedded boundary treatment for linear hyperbolic equations](https://arxiv.org/abs/2510.23231)
*Mirco Ciallella*

Main category: math.NA

TL;DR: The paper analyzes stability of high-order discontinuous Galerkin methods with embedded boundary treatment using shifted boundary polynomial correction, showing explicit time integration has stability limitations that increase with polynomial order, while implicit treatment achieves unconditional stability.


<details>
  <summary>Details</summary>
Motivation: To address the computational cost of body-fitted meshes by using fixed meshes with embedded boundaries, but the geometrical error from boundary treatment can spoil high-order accuracy, requiring stability analysis.

Method: Stability analysis through eigenvalue spectrum visualization of high-order discretized operators for linear advection equation in 1D, varying polynomial degree and boundary-mesh distance, comparing explicit and implicit time integration.

Result: High-order embedded boundary treatment limits stability region for explicit time integration, with limitations increasing with polynomial order. Implicit boundary condition treatment achieves unconditional stability.

Conclusion: Implicit time integration overcomes stability limitations of explicit methods for high-order embedded boundary treatments, enabling unconditionally stable high-order computations with embedded boundaries.

Abstract: Embedded, or immersed, approaches have the goal of reducing to the minimum
the computational costs associated with the generation of body-fitted meshes by
only employing fixed, possibly Cartesian, meshes over which complex boundaries
can move freely. However, this boundary treatment introduces a geometrical
error of the order of the mesh size that, if not treated properly, can spoil
the global accuracy of a high order discretization, herein based on
discontinuous Galerkin. The shifted boundary polynomial correction was proposed
as a simplified version of the shifted boundary method, which is an embedded
boundary treatment based on Taylor expansions to deal with unfitted boundaries.
It is used to accordingly correct the boundary conditions imposed on a
non-meshed boundary to compensate the aforementioned geometrical error, and
reach high order accuracy. In this paper, the stability analysis of
discontinuous Galerkin methods coupled with the shifted boundary polynomial
correction is conducted in depth for the linear advection equation, by
visualizing the eigenvalue spectrum of the high order discretized operators.
The analysis considers a simplified one-dimensional setting by varying the
degree of the polynomials and the distance between the real boundary and the
closest mesh interface. The main result of the analysis shows that the
considered high order embedded boundary treatment introduces a limitation to
the stability region of high order discontinuous Galerkin methods with explicit
time integration, which becomes more and more important when using higher order
methods. The implicit time integration is also studied, showing that the
implicit treatment of the boundary condition allows one to overcome such
limitation and achieve an unconditionally stable high order embedded boundary
treatment.

</details>


### [28] [An Energy-Stable Discontinuous Galerkin Method for the Compressible Navier--Stokes--Allen--Cahn System](https://arxiv.org/abs/2510.23289)
*Lukas Ostrowski,Christian Rohde*

Main category: math.NA

TL;DR: A fully-discrete discontinuous Galerkin method for compressible two-phase Navier-Stokes-Allen-Cahn system that is mass-conservative, energy-stable, and provides high-order spatial and second-order temporal accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method for compressible two-phase fluid motion that preserves key physical properties like mass conservation and energy stability while achieving high-order accuracy.

Method: Discontinuous Galerkin discretization with Crank-Nicolson time-stepping and special splitting of free energy derivatives, building on previous work in Giesselmann2015a.

Result: The proposed method maintains mass conservation, energy stability, and achieves higher-order spatial accuracy with second-order temporal accuracy, as confirmed by numerical experiments.

Conclusion: The approach successfully provides a robust numerical framework for simulating compressible two-phase fluid systems while preserving essential physical properties and achieving desired accuracy.

Abstract: We consider a Navier--Stokes--Allen--Cahn (NSAC) system that governs the
compressible motion of a viscous, immiscible two-phase fluid at constant
temperature. Weak solutions of the NSAC system dissipate an appropriate energy
functional. Based on an equivalent re-formulation of the NSAC system we propose
a fully-discrete discontinuous Galerkin (dG) discretization that is
mass-conservative, energy-stable, and provides higher-order accuracy in space
and second-order accuracy in time. The approach relies on the approach in
\cite{Giesselmann2015a} and a special splitting discretization of the
derivatives of the free energy function within the Crank-Nicolson
time-stepping. Numerical experiments confirm the analytical statements and show
the applicability of the approach.

</details>


### [29] [A least squares finite element method for backward parabolic problems](https://arxiv.org/abs/2510.23300)
*Harald Monsuur*

Main category: math.NA

TL;DR: A least squares finite element method for backward parabolic equations with conditional stability estimates and a priori error bounds.


<details>
  <summary>Details</summary>
Motivation: Backward parabolic equations are ill-posed problems where solutions may not exist or depend continuously on data, requiring robust numerical methods.

Method: Least squares finite element method with conditional stability estimates for weak formulation, treatment of dual norms via test spaces, and iterative solutions.

Result: Derived conditional stability estimates for inhomogeneous backward parabolic equations with minimal regularity, established a priori error bounds, and validated with numerical experiments.

Conclusion: The proposed least squares finite element method provides a viable approach for numerically approximating solutions to ill-posed backward parabolic equations with theoretical guarantees.

Abstract: Backward parabolic equations, such as the backward heat equation, are
classical examples of ill-posed problems where solutions may not exist or
depend continuously on the data. In this work, we study a least squares finite
element method to numerically approximate solutions to such problems. We derive
conditional stability estimates for the weak formulation of inhomogeneous
backward parabolic equations, assuming minimal regularity of the solution.
These stability results are then used to establish \emph{a priori} error bounds
for our proposed method. We address key computational aspects, including the
treatment of dual norms through the construction of suitable test spaces, and
iterative solutions. Numerical experiments are used to validate our theoretical
findings.

</details>


### [30] [Unified Learning of the Profile Function in Discrete Keller-Segel Models](https://arxiv.org/abs/2510.23381)
*Chi-An Chen,Chun Liu,Ming Zhong*

Main category: math.NA

TL;DR: A unified learning framework for identifying profile functions in discrete Keller-Segel equations using particle methods and SDE approximations, with adaptive learning to handle high-dimensional instability and singular behavior.


<details>
  <summary>Details</summary>
Motivation: To address challenges in identifying profile functions in Keller-Segel chemotaxis models, particularly data instability in high dimensions and accurate capture of singular behavior.

Method: Uses training data from particle methods for stable simulation of high-dimensional Keller-Segel systems and SDE approximations of continuous Keller-Segel PDE, combined with adaptive learning strategy.

Result: Extensive numerical experiments validate the effectiveness of the proposed method.

Conclusion: The framework successfully identifies profile functions in discrete Keller-Segel equations while overcoming key challenges in high-dimensional and singular scenarios.

Abstract: We propose a unified learning framework for identifying the profile function
in discrete Keller-Segel equations, which are widely used mathematical models
for understanding chemotaxis. Training data are obtained via either a
rigorously developed particle method designed for stable simulation of
high-dimensional Keller-Segel systems, or stochastic differential equations
approximating the continuous Keller-Segel PDE. Our approach addresses key
challenges, including data instability in dimensions higher than two and the
accurate capture of singular behavior in the profile function. Additionally, we
introduce an adaptive learning strategy to enhance performance. Extensive
numerical experiments are presented to validate the effectiveness of our
method.

</details>


### [31] [A grad-curl conforming virtual element method for a grad-curl problem linking the 3D quad-curl problem and Stokes system](https://arxiv.org/abs/2510.23425)
*Xiaojing Dong,Yibing Han,Yunqing Huang*

Main category: math.NA

TL;DR: The paper presents a novel H(grad-curl)-conforming virtual element space for solving grad-curl problems, reinterpreted as vector potential formulations of the 3D Stokes system, with exactness in the discrete Stokes complex and pressure-decoupled symmetric positive definite systems.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for grad-curl problems by reinterpreting them as vector potential formulations of the 3D Stokes system, enabling pressure-decoupled symmetric positive definite systems on polyhedral meshes.

Method: Constructed a novel H(grad-curl)-conforming virtual element space with arbitrary approximation order based on the Stokes complex and its dual, with exactness in the associated discrete Stokes complex. The lowest-order case uses three DOFs per vertex and one per edge.

Result: Successfully established interpolation error estimates, stability of discrete bilinear forms, and convergence on polyhedral meshes. The resulting system is pressure-decoupled and symmetric positive definite. Numerical examples verify theoretical results.

Conclusion: The proposed virtual element method provides an effective approach for grad-curl problems with exact discrete Stokes complex structure, enabling pressure-decoupled symmetric positive definite systems that work well on polyhedral meshes.

Abstract: Based on the Stokes complex with vanishing boundary conditions and its dual
complex, we reinterpret a grad-curl problem arising from the quad-curl problem
as a new vector potential formulation of the three-dimensional Stokes system.
By extending the analysis to the corresponding non-homogeneous problems and the
accompanying trace complex, we construct a novel
$\boldsymbol{H}(\operatorname{grad-curl})$-conforming virtual element space
with arbitrary approximation order that satisfies the exactness of the
associated discrete Stokes complex. In the lowest-order case, three degrees of
freedom are assigned to each vertex and one to each edge. For the grad-curl
problem, we rigorously establish the interpolation error estimates, the
stability of discrete bilinear forms, and the convergence of the proposed
element on polyhedral meshes. As a discrete vector potential formulation of the
Stokes problem, the resulting system is pressure-decoupled and symmetric
positive definite. Some numerical examples are presented to verify the
theoretical results.

</details>


### [32] [Solving Biot poroelasticity by coupling OPM Flow with the two-point stress approximation finite volume method](https://arxiv.org/abs/2510.23432)
*Wietse M. Boon,Sarah Gasda,Tor Harald Sandve,Svenn Tveit*

Main category: math.NA

TL;DR: This paper presents a coupling method between the two-point stress approximation finite volume method (TPSA) for elasticity and an established flow simulator (OPM Flow), using a fixed stress coupling scheme and reusing algebraic multi-grid preconditioners.


<details>
  <summary>Details</summary>
Motivation: Finite volume methods are widely used in reservoir simulation for their mass conservation and grid handling capabilities, but a simple and consistent finite volume method for elasticity was lacking until the recent development of TPSA.

Method: The authors couple TPSA to OPM Flow at cell centers without interpolation operators, propose a fixed stress coupling scheme, and reuse algebraic multi-grid preconditioners that are effective for two-point flux finite volume methods.

Result: Numerical examples demonstrate the flexibility of the approach and show how solid mechanics affects compartmentalized flow systems.

Conclusion: The proposed coupling method successfully integrates solid mechanics with flow simulation using finite volume methods, providing a practical approach for reservoir simulation applications.

Abstract: Finite volume methods are prevalent in reservoir simulation due to their mass
conservation properties and their ability to handle complex grids. However, a
simple and consistent finite volume method for elasticity was unavailable until
the recently developed two-point stress approximation finite volume method
(TPSA). In this work, we show how to couple TPSA to an established flow
simulator, using OPM Flow as our primary example. Due to this choice of
numerical methods, the coupling is naturally handled at the cell centers,
without requiring interpolation operators. We propose a fixed stress coupling
scheme and reuse algebraic multi-grid preconditioners, which are known to be
effective for two-point flux finite volume methods. Numerical examples
illustrate the flexibility of the approach and we showcase how the introduction
of solid mechanics impacts the behavior of compartmentalized flow systems.

</details>


### [33] [Tree-Cotree-Based IETI-DP for Eddy Current Problems in Time-Domain](https://arxiv.org/abs/2510.23446)
*Mario Mally,Rafael Vázquez,Sebastian Schöps*

Main category: math.NA

TL;DR: A novel tearing and interconnecting approach for time-domain eddy current problems to improve computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Time-domain simulations for eddy current problems are computationally expensive, especially for capturing transient startup responses and nonlinear behavior.

Method: Proposed a tearing and interconnecting approach specifically designed for eddy current formulations in time-domain.

Result: The method's scalability was investigated, suggesting potential for improved computational performance.

Conclusion: The novel approach offers a promising solution to reduce computational costs in time-domain eddy current simulations.

Abstract: For low-frequency electromagnetic problems, where wave-propagation effects
can be neglected, eddy current formulations are commonly used as a
simplification of the full Maxwell's equations. In this setup, time-domain
simulations, needed to capture transient startup responses or nonlinear
behavior, are often computationally expensive. We propose a novel tearing and
interconnecting approach for eddy currents in time-domain and investigate its
scalability.

</details>


### [34] [A Finite Element framework for bulk-surface coupled PDEs to solve moving boundary problems in biophysics](https://arxiv.org/abs/2510.23459)
*Alessandro Contri,André Massing,Padmini Rangamani*

Main category: math.NA

TL;DR: A new computational framework for moving boundary problems in biophysics that handles bulk-surface PDEs with interpretability, accuracy, and mesh distortion mitigation using ALE framework.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of bulk-surface PDEs in biophysical moving boundary problems while maintaining interpretability and computational efficiency.

Method: Combines structure preservation scheme with Arbitrary Lagrangian Eulerian (ALE) framework to mitigate mesh distortion, uses staggered approach for coupling, and tests on various PDEs including advection-diffusion-reaction equations, Cahn-Hilliard phase-field models, and Helfrich energy gradient flows.

Result: Successfully demonstrated accuracy through convergence studies for all schemes, verified coupling convergence via numerical experiments, and showed broad applicability by simulating state-of-the-art biophysical membrane deformation tests.

Conclusion: The proposed computational framework provides an effective, accurate, and generalizable approach for handling complex moving boundary problems in biophysics with practical applications in membrane deformation modeling.

Abstract: We consider moving boundary problems for biophysics and introduce a new
computational framework to handle the complexity of the bulk-surface PDEs. In
our framework, interpretability is maintained by adapting the fast,
generalizable and accurate structure preservation scheme in [Q. Cheng and J.
Shen, \textit{Computer Methods in Applied Mechanics and Engineering}, 391
(2022)]. We show that mesh distortion is mitigated by adopting the pioneering
work of [B. Duan and B. Li, \textit{SIAM J. Sci. Comput.}, 46 (2024)], which is
tied to an Arbitrary Lagrangian Eulerian (ALE) framework. We test our
algorithms accuracy on moving surfaces with boundary for the following PDEs:
advection-diffusion-reaction equations, phase-field models of Cahn-Hilliard
type, and Helfrich energy gradient flows. We performed convergence studies for
all the schemes introduced to demonstrate accuracy. We use a staggered approach
to achieve coupling and further verify the convergence of this coupling using
numerical experiments. Finally, we demonstrate broad applicability of our work
by simulating state-of-the-art tests of biophysical models that involve
membrane deformation.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [35] [Solvability of the $L^p$ Dirichlet problem for the heat equation implies parabolic uniform rectifiability](https://arxiv.org/abs/2510.22047)
*Simon Bortz,Steven Hofmann,José María Martell,Kaj Nyström*

Main category: math.AP

TL;DR: This paper proves that parabolic uniform rectifiability is necessary for L^p solvability of the Dirichlet problem for the heat equation in space-time domains.


<details>
  <summary>Details</summary>
Motivation: To establish the geometric conditions required for boundary regularity and solvability of parabolic boundary value problems, specifically identifying the correct geometric framework for the heat equation.

Method: The authors work with open sets in space-time with time-symmetrically parabolic Ahlfors-David regular boundaries and interior corkscrew conditions. They prove that if the caloric measure satisfies a weak-A∞ condition with respect to surface measure, then the boundary is parabolically uniformly rectifiable.

Result: The main result shows that solvability of the Dirichlet problem for the heat equation with L^p boundary data implies parabolic uniform rectifiability of the boundary.

Conclusion: Parabolic uniform rectifiability is identified as the correct geometric framework for boundary regularity and L^p solvability in the parabolic setting.

Abstract: Let $\Omega \subset \mathbb{R}^{n+1}$ be an open set in space-time with
boundary $\Sigma = \partial \Omega$. Under minimal and natural background
assumptions - namely, that $\Sigma$ is time-symmetrically parabolic
Ahlfors--David regular and that $\Omega$ satisfies an interior corkscrew
condition - we treat a one-phase parabolic free boundary problem which
establishes the necessity of parabolic uniform rectifiability for
$L^p(d\sigma)$ solvability of the Dirichlet problem for the heat equation. More
precisely, we prove that if the caloric measure associated with $\Omega$
satisfies a weak-$A_\infty$ condition with respect to the surface measure
$\sigma = \mathcal{H}_{\mathrm{par}}^{n+1}\!\lfloor_{\Sigma}$, then $\Sigma$ is
parabolically uniformly rectifiable, hence equivalently, that solvability of
the Dirichlet problem for the heat (or adjoint heat) equation in $\Omega$ with
boundary data in $L^p(d\sigma)$, for some $p \in (1,\infty)$, implies parabolic
uniform rectifiability. Our main theorem thus identifies parabolic uniform
rectifiability as the correct geometric framework for boundary regularity, and
$L^p$ solvability, in the parabolic setting.

</details>


### [36] [Late-time tail for a scalar quasilinear wave equation satisfying the weak null condition](https://arxiv.org/abs/2510.22122)
*Jonathan Luk,Sung-Jin Oh,Dongxiao Yu*

Main category: math.AP

TL;DR: The paper provides an asymptotic formula for global solutions of scalar quasilinear wave equations in 3D satisfying the weak null condition, showing continuous superposition of decay rates rather than uniform decay.


<details>
  <summary>Details</summary>
Motivation: To understand the global asymptotics of solutions to quasilinear wave equations satisfying the weak null condition, particularly how they differ from equations satisfying the standard null condition.

Method: Developed an asymptotic formula in terms of solutions to the linear wave equation and combined this with analysis on linear wave equations to study decay properties.

Result: Proved that late-time asymptotics exhibit continuous superposition of decay rates, unlike equations with null condition. Strengthened rigidity results showing solutions with faster decay away from wave zone must vanish.

Conclusion: Quasilinear wave equations with weak null condition have fundamentally different asymptotic behavior from those with null condition, characterized by continuous decay rate superposition rather than uniform decay.

Abstract: We consider a class of scalar quasilinear wave equations in three spatial
dimensions satisfying the weak null condition. For solutions arising from
small, localized, smooth data, we give an asymptotic formula describing the
global asymptotics towards the future. We prove that the late-time asymptotics
is given by a continuous superposition of decay rates, in stark contrast to
equations satisfying a null condition.
  The asymptotic formula we obtain is given in terms of a solution to the
linear wave equation. Combining this with analysis on the linear wave equation,
we strengthen some rigidity results of the third author, showing in particular
that any solution with a faster time decay than expected away from the wave
zone must vanish identically.

</details>


### [37] [Two dimensional anisotropic mean curvature flow with contact angle condition](https://arxiv.org/abs/2510.22136)
*Can Cui,Nung Kwan Yip*

Main category: math.AP

TL;DR: Establishes prior gradient estimates for anisotropic mean curvature flow with contact angle boundary conditions in convex 2D domains, with extensions to Dirichlet conditions in higher dimensions, proving convergence to time-invariant solutions.


<details>
  <summary>Details</summary>
Motivation: To analyze the behavior of surfaces evolving under anisotropic mean curvature flow with boundary conditions, particularly understanding gradient estimates and long-term convergence properties.

Method: Develops prior gradient estimates for smooth solutions to anisotropic mean curvature flow with contact angle boundary conditions in strictly convex 2D domains, with extension to Dirichlet boundary conditions in higher dimensions.

Result: Proves that solutions converge to translation invariant (time-independent) solutions for both contact angle and Dirichlet boundary value problems.

Conclusion: The analysis provides rigorous mathematical foundations for understanding the asymptotic behavior of anisotropic mean curvature flows with various boundary conditions, establishing convergence to steady states.

Abstract: In this paper, we study surfaces which evolve by anisotropic mean curvature
flow with contact angle boundary condition over a strictly convex domain in
$\mathbb{R}^2$. We establish a prior gradient estimate for smooth solutions to
this boundary value problem. The same approach can also handle Dirichlet
boundary condition in $\mathbb{R}^n$, $n\geq 2$. For both problems, we prove
that the solutions converge to one that is translation invariant in time.

</details>


### [38] [Anisotropic mean curvature flow with contact angle and Neumann boundary conditions in arbitrary dimensions](https://arxiv.org/abs/2510.22146)
*Can Cui,Nung Kwan Yip*

Main category: math.AP

TL;DR: A priori gradient estimate for anisotropic mean curvature flow with prescribed contact angle and Neumann boundary conditions on bounded strictly convex domains.


<details>
  <summary>Details</summary>
Motivation: To establish gradient estimates for anisotropic mean curvature flow problems with boundary conditions, addressing the degeneracy of the anisotropic mean curvature operator.

Method: Careful analysis of the degeneracy property of the anisotropic mean curvature operator to derive a priori gradient estimates.

Result: Successfully established gradient estimates that allow inference of solution convergence to time-translation invariant states.

Conclusion: The solutions converge to translation invariant states in time for both problems studied.

Abstract: Over a bounded strictly convex domain in $\mathbb{R}^n$ with smooth boundary,
we establish a priori gradient estimate for an anisotropic mean curvature flow
with prescribed contact angle and Neumann boundary conditions. The estimates
require careful analysis of the degeneracy property of the anisotropic mean
curvature operator. As a result, for both problems, we can infer that the
solutions converge to one that is translation invariant in time.

</details>


### [39] [Well-posedness and finite-time extinction of a PDE-ODE spatial-network model with anisotropic diffusion](https://arxiv.org/abs/2510.22147)
*Xiao Meng,Kei Fong Lam*

Main category: math.AP

TL;DR: The paper studies a reaction-diffusion system on domains with graph-structured networks, coupling PDE dynamics on edges with ODE dynamics at vertices, applicable to epidemiology and ecology.


<details>
  <summary>Details</summary>
Motivation: To model population movement between cluster centers along road-like structures and into surrounding areas in epidemiological and ecological contexts.

Method: Using a semi-Galerkin approximation to establish well-posedness of weak solutions for the coupled PDE-ODE system with junction conditions and Robin boundary conditions.

Result: Established well-posedness of weak solutions and examined properties including regularity, boundedness, and finite-time extinction.

Conclusion: The framework provides a mathematical foundation for studying complex population dynamics in networked environments with coupled continuum and discrete components.

Abstract: We study a system of reaction-diffusion equations posed on a bounded domain
composed of subdomains separated by a connected network with a metric graph
structure. The reaction-diffusion dynamics with anisotropic diffusion on the
graph edges are coupled to well-mixed ODE dynamics occurring at the vertices by
junction conditions, and to similar PDE dynamics occurring on adjacent
subdomains through Robin-like boundary conditions. The resulting PDE-ODE system
can be used in epidemiological and ecological settings to study population
movement in between cluster centers along road-like structures and into the
surrounding continuum. We employ a semi-Galerkin approximation to establish the
well-posedness of weak solutions to the PDE-ODE system, and examine further
properties such as regularity, boundedness and finite-time extinction.

</details>


### [40] [Normalized solutions to critical Choquard systems with linear and nonlinear couplings](https://arxiv.org/abs/2510.22159)
*Wenliang Pei,Chonghao Deng*

Main category: math.AP

TL;DR: This paper studies a critical Choquard system with linear and nonlinear couplings in R^N (N=3,4). It proves existence of positive normalized ground states for both L^2-subcritical and L^2-supercritical cases using variational methods.


<details>
  <summary>Details</summary>
Motivation: To analyze critical Choquard systems with coupled nonlinearities and establish existence results for normalized ground states under different parameter regimes.

Method: Uses Ekeland's variational principle for L^2-subcritical case (p+q < (2N+2ω+4)/N) and variational methods for L^2-supercritical case (p+q > (2N+2ω+4)/N).

Result: Proves existence of positive normalized ground states: for subcritical case when 0<θ<θ₀, 0<ε<ε*; for supercritical case when θ>θ*, 0<ε<ε̄.

Conclusion: The paper successfully establishes existence of normalized ground states for the coupled Choquard system in both subcritical and supercritical regimes using different variational approaches.

Abstract: We consider the critical Choquard system with both linear and nonlinear
couplings
  $-\Delta v_1 + \mu_1 v_1 = ( I_\omega * |v_1|^{2_\omega^*} )
|v_1|^{2_\omega^* -2} v_1 + \theta p( I_\omega * |v_2|^q)|v_1|^{p-2} v_1 +
\varepsilon v_2, \quad in \,\, \mathbb{R}^N,
  -\Delta v_2 + \mu_2 v_2 = ( I_\omega * |v_2|^{2_\omega^*} ) |v_2|^{2_\omega^*
-2} v_2 + \theta q( I_\omega * |v_1|^p)|v_2|^{q-2} v_2 + \varepsilon v_1 ,
\quad in \,\, \mathbb{R}^N ,
  \int_{\mathbb{R}^N} v_1^2 = \alpha_1^2\, , \int_{\mathbb{R}^N} v_2^2 =
\alpha_2^2,$
  where $N=3\,\, \text{or} \,\, 4$, $\alpha_1,\alpha_2 > 0 $, $\theta > 0 $,
$2_{\omega,*} :=\frac{N+\omega}{N} <p,q<2_\omega^*:=\frac{N+\omega}{N-2}$,
$\varepsilon>0$, $0<\omega<N$, $I_\omega: \mathbb{R}^N \to \mathbb{R}$
represents the Riesz potential. For the $L^2$-subcritical case
$p+q<\frac{2N+2\omega+4}{N}$, we utilize the Ekeland's variational principle to
obtain the existence of a positive normalized ground state for the system as
$0<\theta<\theta_0,\;0<\varepsilon<\varepsilon_*$. For the $L^2$-supercritical
case $p+q>\frac{2N+2\omega+4}{N}$, we apply variational methods to establish
the existence of a positive normalized ground state for the system as
$\theta>\theta_*,\;0<\varepsilon<\overline{\varepsilon}$.

</details>


### [41] [Quantiative Hypocoercivity and Lifting of Classical and Quantum Dynamics](https://arxiv.org/abs/2510.22305)
*Jianfeng Lu*

Main category: math.AP

TL;DR: This paper provides a unified framework for quantitative convergence analysis of hypocoercive dynamics (Langevin and Lindblad equations) using space-time Poincare inequalities and a lifting method to accelerate Markov semigroups.


<details>
  <summary>Details</summary>
Motivation: To develop a unified approach for analyzing convergence rates in both classical and quantum open systems described by hypocoercive dynamics, bridging the gap between classical Langevin and quantum Lindblad equations.

Method: Uses space-time Poincare inequalities for hypocoercivity estimates and introduces a unified lifting framework to accelerate classical and quantum Markov semigroups.

Result: Provides upper and lower bounds for convergence rates in both classical and quantum systems, offering a comprehensive analysis tool for hypocoercive dynamics.

Conclusion: The paper establishes a unified mathematical framework that successfully applies to both classical and quantum hypocoercive systems, enabling systematic convergence analysis across different physical domains.

Abstract: We consider quantitative convergence analysis for hypocoercive dynamics such
as Langevin and Lindblad equations describing classical and quantum open
systems. Our goal is to provide an overview of recent results of hypocoercivity
estimates based on space-time Poincare inequality, providing a unified
treatment for classical and quantum dynamics. Furthermore, we also present a
unified lifting framework for accelerating both classical and quantum Markov
semigroups, which leads to upper and lower bounds of convergence rates.

</details>


### [42] [Traveling waves in nonclassical diffusion equations](https://arxiv.org/abs/2510.22349)
*William Barker,Le Xuan Dong,Vu Trong Luong,Nguyen Duong Toan*

Main category: math.AP

TL;DR: Existence of monotone traveling wave solutions in nonclassical diffusion equations with mixed space-time dispersive terms and nonlinear reactions, proven using upper/lower solutions method and Schauder's fixed point theorem.


<details>
  <summary>Details</summary>
Motivation: To study traveling wave solutions in nonclassical diffusion equations that combine standard diffusion with higher-order mixed space-time dispersive terms under general nonlinear reaction conditions.

Method: Employed method of upper and lower solutions with less smooth super/subsolutions, constructed monotone iterative scheme within convex set, and used Schauder's fixed point theorem for convergence proof.

Result: Successfully constructed explicit super and subsolutions and proved existence of monotone traveling wave solutions for the class of nonclassical diffusion equations.

Conclusion: The approach provides a rigorous framework for establishing existence of monotone traveling waves in nonclassical diffusion equations with mixed dispersive terms and general nonlinear reactions.

Abstract: We study the existence of monotone traveling wave solutions in a class of
nonclassical diffusion equations that include both standard diffusion and a
higher-order mixed space-time dispersive term. The reaction term is nonlinear
and subject to general structural conditions. By employing the method of upper
and lower solutions, using less smooth super and subsolutions, we construct a
monotone iterative scheme within a convex set and prove its convergence using
Schauder's fixed point theorem. Explicit constructions of super and
subsolutions are provided.

</details>


### [43] [Robin harmonic measure with a variable permeability parameter](https://arxiv.org/abs/2510.22354)
*Svitlana Mayboroda,Alberto Pacati*

Main category: math.AP

TL;DR: Study of Robin problem solutions in bounded 1-sided NTA domains with Ahlfors-David regular boundary, extending previous work to non-constant Robin parameters.


<details>
  <summary>Details</summary>
Motivation: To generalize existing results on Robin problems from constant to variable permeability parameters, addressing more realistic boundary conditions.

Method: Analysis of Robin harmonic measure in bounded 1-sided NTA domains with Ahlfors-David regular boundary, using techniques from potential theory and measure theory.

Result: Proof of mutual absolute continuity between Robin harmonic measure and surface measure in the variable permeability setting.

Conclusion: The paper successfully extends previous constant parameter results to the more general case of non-constant Robin parameters, establishing fundamental measure-theoretic properties.

Abstract: In this paper we study the behavior of the solutions to the Robin problem in
bounded $1$-sided NTA domains with Ahlfors-David regular boundary, generalizing
the results of \cite{DavDEMM} to the case of a non constant Robin parameter. In
particular, we will prove the mutual absolute continuity of the Robin harmonic
measure with respect to the surface measure in the setting of variable
permeability.

</details>


### [44] [New cost terms through the homogenization of an optimal control problem under dynamic boundary conditions on the microscopic particles](https://arxiv.org/abs/2510.22357)
*J. I. Díaz,T. A. Shaposhnikova,A. V. Podolskiy*

Main category: math.AP

TL;DR: Study of asymptotic behavior of optimal control problems for periodic heterogeneous structures with microscopic controls on particle subsets, showing emergence of non-local strange terms in critical case.


<details>
  <summary>Details</summary>
Motivation: To understand how microscopic localized controls on periodic heterogeneous structures affect the limit behavior of optimal control problems, particularly when dynamic boundary conditions are applied to subsets of particles.

Method: Analyze asymptotic behavior as structure period parameter e→0 for parabolic optimal control problems with dynamic boundary conditions on particles, focusing on critical case where period, particle diameter, and boundary growth coefficient have specific relation.

Result: In critical case, non-local in time strange terms appear in both the limit parabolic equation and cost functional, which are peculiar to microscopic localized controls and don't occur with full particle controls or Robin boundary conditions.

Conclusion: Microscopic localized controls on periodic heterogeneous structures generate unique non-local terms in the homogenized limit that distinguish them from other control configurations, revealing special mathematical phenomena in the critical scaling regime.

Abstract: Given an optimal control problem on a heterogeneous body with a periodical
structure of particles depending on a small parameter e, we study the
asymptotic behavior, as e converges to zero, of the optimal control functional
and the optimal state when the initial problem is of parabolic type, and when
on the particles' boundary, we assume a dynamic condition and the actuation of
some controls for some subset of the particles. We show, in the so-called
"critical case" (concerning a certain relation between the structure's period,
the diameter of the balls, and the growth coefficient of the particles boundary
condition), the appearance of some new non-local in time "strange terms", not
only in the limit parabolic equation but also in the limit cost functional.
Microscopic localized controls generate peculiar terms in both the limit
equation and the cost functional that do not appear in the case of controls
applied to the entire set of particles or when the boundary condition on the
particles is of Robin type.

</details>


### [45] [Self-Generated Measures and the Centroid Rigidity of Power Laws](https://arxiv.org/abs/2510.22428)
*Vincent E. Coll,Jr. abd Lee B. Whitt*

Main category: math.AP

TL;DR: The paper shows that the geometric scaling property for centroids of functions holds only for power law functions f(x) = A*x^p, with the optimal constant λ = (p+1)/(2(2p+1))*((p+2)/(p+1))^p.


<details>
  <summary>Details</summary>
Motivation: To revisit classical centroid computations and uncover hidden rigidity theorems, specifically examining when the geometric scaling property holds for function centroids.

Method: Uses probabilistic interpretation with self-generated probability measures, differentiation techniques, and variance analysis to prove the rigidity theorem. The approach involves defining centroids x̄(a) and ȳ(a), then showing the geometric scaling property forces the function to be a power law.

Result: Proves that the geometric scaling property ȳ(a) = λ*f(x̄(a)) holds for all a > 0 if and only if f(x) = A*x^p with A > 0, p > 0. The optimal constant λ is explicitly determined.

Conclusion: The geometric scaling property for function centroids exhibits rigidity - it only applies to power law functions, revealing a fundamental constraint in centroid geometry that was hidden in classical calculus computations.

Abstract: We revisit a classical calculus computation: the centroid of the subgraph of
a function on the interval from 0 to a, and show that it hides a rigidity
theorem. Let f be twice continuously differentiable on (0, infinity), take
values in (0, infinity), and satisfy f(0+) = 0. Define xbar(a) as (integral
from 0 to a of x f(x) dx) divided by (integral from 0 to a of f(x) dx), and
define ybar(a) as (1/2) times (integral from 0 to a of f(x)^2 dx) divided by
(integral from 0 to a of f(x) dx). We prove that the Geometric Scaling
Property, namely the identity ybar(a) = lambda * f(xbar(a)) for every a > 0,
holds if and only if f(x) = A * x^p with A > 0 and p > 0. For these power laws
the optimal constant is lambda = (p+1)/(2(2p+1)) * ((p+2)/(p+1))^p. After a
scale-free normalization, the proof is probabilistic: with the self-generated
probability measure on (0, a) having density proportional to f, we have xbar(a)
equal to the expected value of Xa and ybar(a) equal to (1/2) times the expected
value of f(Xa), so the Geometric Scaling Property becomes an equality in
expectation across all truncation scales. Differentiating with respect to a
yields a weighted mean identity for the elasticity E(x) = x f'(x) / f(x); a
second differentiation forces a vanishing variance principle that makes E
constant, hence f a pure power, and the stated value of lambda follows. The
argument uses no asymptotics and extends to f that is once continuously
differentiable on (0, infinity) with locally Lipschitz elasticity.

</details>


### [46] [Ground state solutions to generalized nonlinear wave equations with infinite-dimensional kernel](https://arxiv.org/abs/2510.22544)
*Rainer Mandel,Tobias Weth*

Main category: math.AP

TL;DR: Existence of time-periodic solutions for generalized nonlinear wave equations on closed Riemannian manifolds, focusing on doubly degenerate settings with infinite-dimensional kernel and nonlinearities vanishing on open subsets.


<details>
  <summary>Details</summary>
Motivation: To address the challenging doubly degenerate setting where the wave operator has infinite-dimensional kernel and nonlinearities may vanish on open subsets of the manifold, requiring new mathematical approaches.

Method: Direct variational approach using a new variant of nonlinear saddle point reduction applied to the associated Nehari-Pankov set.

Result: Successfully finds ground state solutions and characterizes the ground state energy through a simple minimax principle.

Conclusion: The proposed variational method effectively handles doubly degenerate wave equations and provides existence results for time-periodic solutions in this challenging setting.

Abstract: The present paper is devoted to existence results for time-periodic solutions
of generalized nonlinear wave equations in a closed Riemannian manifold M. Our
main focus lies on the doubly degenerate setting where the associated
generalized wave operator has an infinite dimensional kernel and the
nonlinearity may vanish on open subsets of M. To deal with this setting, we
apply a direct variational approach based on a new variant of the nonlinear
saddle point reduction to the associated Nehari-Pankov set. This allows us to
find ground state solutions and to characterize the associated ground state
energy by a fairly simple minimax principle.

</details>


### [47] [Diffusion operators on $p$-adic analytic manifolds](https://arxiv.org/abs/2510.22563)
*Patrick Erik Bradley*

Main category: math.AP

TL;DR: Construction of kernel functions for Laplacian integral operators on p-adic analytic manifolds, defining Vladimirov-Taibleson type operators with real parameter s, establishing L^2-spectrum and Feller groups, solving heat equations via Markov processes, and proving existence of heat kernel and Green functions.


<details>
  <summary>Details</summary>
Motivation: To extend the theory of Laplacian integral operators and heat equations from Riemannian geometry to p-adic analytic manifolds, developing analogous analytical tools and spectral theory in the p-adic setting.

Method: Using charts and transition maps similar to Riemannian geometry to construct kernel functions, defining Vladimirov-Taibleson type operators parametrized by real parameter s, analyzing L^2-spectrum, establishing Feller groups, and studying Markov processes for heat equations.

Result: Successfully constructed kernel functions on p-adic manifolds, established L^2-spectrum of the operators, showed they give rise to Feller groups, solved the Cauchy problem for heat equations via Markov transition functions, and proved existence of heat kernel and Green functions for s > 1.

Conclusion: The paper provides a comprehensive framework for Laplacian operators and heat equations on p-adic analytic manifolds, extending classical results from Riemannian geometry to the p-adic context with complete spectral and probabilistic characterizations.

Abstract: Kernel functions for Laplacian integral operators are constructed on $p$-adic
analytic manifolds using charts and transition maps like in Riemannian
geometry. In the compact case, an operator of Vladimirov-Taibleson type
parametrised by a real parameter $s$ is defined. Its $L^2$-spectrum is
established, and it is shown that it gives rise to a Feller group. In this way,
the Cauchy problem for the corresponding heat equation is solved in the
positive by a transition function of a Markov process. Finally the existence of
a heat kernel function and a Green function in the case $s > 1$ is proven.

</details>


### [48] [The spectrum of Dirichlet-to-Neumann maps for radial conductivities](https://arxiv.org/abs/2510.22585)
*Thierry Daudé,Fabricio Macià,Cristóbal Meroño,François Nicoleau*

Main category: math.AP

TL;DR: The paper characterizes Dirichlet-to-Neumann map spectra for rotation-invariant elliptic operators in balls, showing they consist of a universal boundary term plus Hausdorff moments of a Born approximation function.


<details>
  <summary>Details</summary>
Motivation: To solve the long-standing problem of characterizing sequences that arise as spectra of Dirichlet-to-Neumann maps for elliptic operators, specifically in rotation-invariant settings.

Method: Analyze DtN maps for rotation-invariant elliptic operators in Euclidean balls, expressing spectra as universal boundary terms plus Hausdorff moments of a Born approximation function derived from conductivity.

Result: Proved local determination of Born approximation from boundary data, local uniqueness for Calderón problem, and Hölder stability of the mapping from Born approximation to conductivity in Sobolev spaces.

Conclusion: The Born approximation provides a refined characterization of DtN spectra with local uniqueness and stability properties, advancing understanding of inverse problems for elliptic operators.

Abstract: The problem of characterizing sequences of real numbers that arise as spectra
of Dirichlet-to-Neumann (DtN) maps for elliptic operators has attracted
considerable attention over the past fifty years. In this article, we address
this question in the simple setting of DtN maps associated with a
rotation-invariant elliptic operator $\nabla \cdot (\gamma\nabla \centerdot )$
in the ball in Euclidean space. We show that the spectrum of such a DtN
operator can be expressed as a universal term, determined solely by the
boundary values of the conductivity $\gamma$, plus a sequence of Hausdorff
moments of an integrable function, which we call the Born approximation of
$\gamma$. We also show that this object is locally determined from the boundary
by the corresponding values of the conductivity, a property that implies a
local uniqueness result for the Calder\'on Problem in this setting. We also
give a stability result: the functional mapping the Born approximation to its
conductivity is H\"older stable in suitable Sobolev spaces. Finally, in order
to refine the characterization of the Born approximation, we analyze its
regularity properties and their dependence on the conductivity.

</details>


### [49] [Numerical study of transverse (in-)stability of solitary waves in the cubic-quintic nonlinear Schrödinger equation](https://arxiv.org/abs/2510.22735)
*Christian Klein,Christof Sparber*

Main category: math.AP

TL;DR: Numerical study of transverse instability in cubic-quintic nonlinear Schrödinger equation on waveguide domains, finding critical torus length for instability.


<details>
  <summary>Details</summary>
Motivation: To understand the stability properties of line solitary wave solutions in nonlinear Schrödinger equations with competing nonlinearities on waveguide geometries.

Method: Numerical investigation of transverse instability using both spatially localized perturbations and periodic deformations of line solitary waves.

Result: Existence of a critical torus length L_y > 0 above which instability appears in the line solitary wave solutions.

Conclusion: The cubic-quintic nonlinear Schrödinger equation on waveguide domains exhibits a threshold behavior where line solitary waves become unstable when the transverse dimension exceeds a critical size.

Abstract: We study the nonlinear Schr\"odinger equation with a competing cubic-quintic
power law nonlinearity on the waveguide domain $\mathbb R_x \times \mathbb
T_{L_y}$. This model is globally well-posed and admits line solitary wave
solutions, whose transverse (in-)stability is numerically investigated. We
consider both spatially localized perturbations and periodic deformations of
the line solitary wave and numerically confirm that there exists a critical
torus length $L_y>0$ above which instability appears.

</details>


### [50] [Singularities of the Lagrangian mean curvature flow at the critical Lagrangian phase](https://arxiv.org/abs/2510.22741)
*Arunima Bhattacharya,Ravi Shankar,Jeremy Wall,Diego Yepez*

Main category: math.AP

TL;DR: Interior estimates for singularities in Lagrangian mean curvature flow with critical phase, extended to related equations. Shows phase criticality is necessary and introduces new method for C²,α estimates.


<details>
  <summary>Details</summary>
Motivation: To understand singularity formation in Lagrangian mean curvature flow when the Lagrangian phase is critical (|Θ| ≥ (n-2)π/2), and extend analysis to broader class of Lagrangian mean curvature type equations.

Method: Establishes gradient estimates with structural conditions, constructs Cα singular viscosity solutions to prove necessity of phase criticality, and introduces new method using arctangent operator exponentiation for C²,α estimates when |Θ| ≥ (n-2)π/2 and n>2.

Result: Proves interior estimates for singularities, shows critical phase is necessary condition, demonstrates structural conditions cannot be removed in dimension one, and provides new technique for higher regularity estimates.

Conclusion: Criticality of Lagrangian phase is essential for singularity analysis in Lagrangian mean curvature flow, and the new arctangent operator method enables improved regularity estimates in higher dimensions.

Abstract: We establish interior estimates for singularities of the Lagrangian mean
curvature flow when the Lagrangian phase is critical, i.e., $|\Theta|\geq
(n-2)\tfrac{\pi}{2}$, and extend our results to the broader class of Lagrangian
mean curvature type equations. Our gradient estimates require certain
structural conditions, and we construct $C^{\alpha}$ singular viscosity
solutions to show that criticality of the phase is necessary, and that these
conditions cannot be removed in dimension one. We also introduce a new method
for proving $C^{2,\alpha}$ estimates by exponentiating the arctangent operator
into a concave one when $|\Theta|\geq (n-2)\tfrac{\pi}{2}$ and $n>2$.

</details>


### [51] [Propagation of Velocity Moments for the Magnetized Vlasov-Poisson System with Space-Time Dependent Magnetic Fields](https://arxiv.org/abs/2510.22753)
*Immanuel Ben Porat,Antoine Gagnebin,Mikaela Iacobelli,Jonathan Junné*

Main category: math.AP

TL;DR: The paper proves that polynomial velocity moments remain finite for all time in 2D magnetized Vlasov-Poisson and 3D magnetized screened Vlasov-Poisson systems with space-time dependent magnetic fields, leading to global classical solutions and optimal stability estimates.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for magnetized plasma systems by proving long-term behavior of solutions, particularly when external magnetic fields vary in both space and time.

Method: Mathematical analysis of velocity moments in Vlasov-Poisson systems, using techniques to prove boundedness of polynomial velocity moments and deduce regularity propagation.

Result: Polynomial velocity moments remain finite for all times when initially finite, even with space-time dependent magnetic fields. This implies existence of global classical solutions and optimal stability estimates in kinetic-Wasserstein distance.

Conclusion: The analysis provides comprehensive mathematical guarantees for magnetized plasma systems, showing that key physical quantities remain well-behaved over time and establishing stability properties comparable to unmagnetized cases.

Abstract: We prove that polynomial velocity moments of solutions to the 2D magnetized
Vlasov-Poisson system and the 3D magnetized screened Vlasov-Poisson equation
remain finite for all times, provided they are finite initially, even when the
external magnetic field $B=B(t,x)$ is space-time dependent. We deduce
propagation of regularity, thereby implying the existence of global classical
solutions. Moreover, we prove optimal stability estimates in the
kinetic-Wasserstein distance on par with the unmagnetised case.

</details>


### [52] [Optimal Regularity for Hölder continuous Hamiltonian Stationary Lagrangian graphs](https://arxiv.org/abs/2510.22756)
*Arunima Bhattacharya,W. Jacob Ogden*

Main category: math.AP

TL;DR: Optimal regularity for Hölder continuous Hamiltonian stationary Lagrangian graphs in ℜⁿ is established. Graphs are smooth when Hölder exponent > 1/3 with supercritical phase, but singular solutions exist at exponent 1/3 even with hypercritical phase.


<details>
  <summary>Details</summary>
Motivation: To determine the precise regularity threshold for Hamiltonian stationary Lagrangian graphs and understand how this differs from special Lagrangian graphs theory.

Method: Prove regularity results using semi-convexity from supercritical phase, and construct explicit singular solutions at the critical Hölder exponent 1/3.

Result: Graphs are smooth when Hölder exponent > 1/3 with supercritical phase, but singular solutions exist at exactly exponent 1/3 even under hypercritical phase (strongest convexity).

Conclusion: The 1/3 Hölder exponent is optimal and represents a striking departure from special Lagrangian graphs theory, as singularities persist even under maximal convexity assumptions.

Abstract: In this paper, we establish optimal regularity for H\"older continuous
Hamiltonian stationary Lagrangian graphs in $\mathbb{C}^n$. We prove that such
a graph is smooth whenever its H\"older exponent is strictly larger than
$\frac{1}{3}$ and the Lagrangian phase is supercritical, which yields
semi-convexity of the potential. We establish the optimality of our result by
constructing explicit singular solutions to the fourth order Hamiltonian
stationary equation when the H\"older exponent of the graph is $\frac{1}{3}$.
The singular solutions exist even under the strongest convexity assumption on
the Lagrangian phase, namely the hypercritical phase, which enforces convexity
of the potential. This presents a striking departure from the theory of special
Lagrangian graphs.

</details>


### [53] [Doubling and the two-dimensional critical valued Lagrangian phase](https://arxiv.org/abs/2510.22887)
*Arunima Bhattacharya,Ravi Shankar,Jeremy Wall*

Main category: math.AP

TL;DR: Interior Hessian and gradient estimates for 2D Lagrangian mean curvature equation with sign-changing phase, using modified doubling technique for degenerate Jacobi inequalities.


<details>
  <summary>Details</summary>
Motivation: The degeneracy of the Jacobi inequality at critical phase in two dimensions prevents using standard higher-dimensional methods for Hessian estimates.

Method: A modified doubling technique that applies to degenerate Jacobi inequalities.

Result: Successfully established interior Hessian and gradient estimates for the two-dimensional Lagrangian mean curvature equation when phase changes signs.

Conclusion: The proposed modified doubling technique effectively addresses the degeneracy issue and provides interior estimates for this challenging case.

Abstract: In this paper, we establish interior Hessian and gradient estimates for the
two-dimensional Lagrangian mean curvature equation when the phase changes
signs, provided the gradient of the phase vanishes along its zero set. At the
critical phase in two dimensions, the Jacobi inequality degenerates, preventing
the use of higher-dimensional methods to obtain Hessian estimates. To address
this difficulty, we introduce a modified doubling technique that applies to
degenerate Jacobi inequalities and yields interior estimates.

</details>


### [54] [From nonlinear Schrödinger equation to interacting particle system: 1 < p < 2](https://arxiv.org/abs/2510.23128)
*Xin Liao,Juntao Lv*

Main category: math.AP

TL;DR: Analysis of limiting behavior of multi-peak solutions to nonlinear Schrödinger equations as ε→0, focusing on interaction laws between peak points and completing the analysis for 1<p<2.


<details>
  <summary>Details</summary>
Motivation: To understand the interaction dynamics between infinitely many peaks in nonlinear Schrödinger equations as the parameter ε approaches zero, particularly addressing the previously unresolved range 1<p<2.

Method: Derived interaction laws among limiting peak points through mathematical analysis of solutions with infinitely many peaks to the nonlinear Schrödinger equation -ε²Δu_ε + u_ε = u_ε^p.

Result: Successfully completed the analysis for the range 1<p<2, extending previous work by Ao, Lv, and Wang (2025), and established the interaction laws governing the limiting behavior of peak points.

Conclusion: The study provides a comprehensive understanding of multi-peak solution behavior in nonlinear Schrödinger equations across the full Sobolev subcritical range, including the previously unresolved 1<p<2 case.

Abstract: We investigate the limiting behavior of solutions with infinitely many peaks
to nonlinear Schr\"odinger equations [-epsilon^2 Delta u_epsilon + u_epsilon =
u_epsilon^p, u_epsilon > 0 in R^n,] as epsilon -> 0, where p is Sobolev
subcritical. We derive the interaction law among the limiting peak points and
complete the analysis for the previously unresolved range 1 < p < 2, extending
the work of Ao, Lv, and Wang (J. Differential Equations, 2025).

</details>


### [55] [Weighted Sobolev inequalities and superlinear elliptic problems on exterior domains](https://arxiv.org/abs/2510.23157)
*Ardra A,Ameerraja Ansari,Anumol Joseph,Lakshmi Sankar*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Let $B_1 ^c = \{ x\in \mathbb{R}^N: |x|>1 \}, N \geq 2$, and
$\mathcal{D}^{1,N}_0(B^c_1)$, be the Beppo-Levi space. We prove that
$\mathcal{D}^{1,N}_0(B^c_1)$ is compactly embedded into the weighted Lebesgue
space $L^r(B_1^c;K(x))$ for all $r\in[1,\infty)$ for an appropriate class of
weight functions $K$. As an application, we prove the existence of a positive
solution to a superlinear semipositone problem on $B_1 ^c$ in $\mathbb{R}^2$.
We also establish boundedness and regularity of solutions of certain boundary
value problems and derive their Green's function representation.

</details>


### [56] [Classification results for bounded positive solutions to the critical $p$-Laplace equation](https://arxiv.org/abs/2510.23243)
*Giulio Ciraolo,Michele Gatti*

Main category: math.AP

TL;DR: Positive bounded or moderately growing local weak solutions to the critical p-Laplace equation in R^n (n≥3) are shown to be bubbles.


<details>
  <summary>Details</summary>
Motivation: To characterize the structure of solutions to the critical p-Laplace equation by determining when they must be bubble solutions.

Method: Using optimal or nearly optimal integral estimates to analyze local weak solutions.

Result: Every positive, bounded or moderately growing local weak solution to the critical p-Laplace equation in R^n (n≥3) is a bubble.

Conclusion: The paper establishes that under the given growth conditions, all such solutions to the critical p-Laplace equation are necessarily bubble solutions.

Abstract: By providing optimal or nearly optimal integral estimates, we show that every
positive, bounded or moderately growing, local weak solution to the critical
$p$-Laplace equation in $\mathbb{R}^n$, with $n\geq 3$, must be a bubble.

</details>


### [57] [Existence and multiplicity results for the zero mass Schrödinger-Bopp-Podolsky system with critical growth](https://arxiv.org/abs/2510.23266)
*Wentao Huang,Li Wang*

Main category: math.AP

TL;DR: Study of zero mass Schrödinger-Bopp-Podolsky system with critical growth, establishing existence of positive ground state solutions for p∈(3,6) and multiplicity results for p∈(4,6).


<details>
  <summary>Details</summary>
Motivation: To investigate the existence and multiplicity of solutions for a critical growth Schrödinger-Bopp-Podolsky system with zero mass condition, which has important applications in quantum physics.

Method: Introduce new functional framework by Caponio et al., apply abstract critical point theorem by Perera for multiplicity results.

Result: Existence of positive ground state solutions proven for p∈(3,6), multiplicity results obtained for p∈(4,6).

Conclusion: The paper successfully establishes existence and multiplicity of solutions for the critical growth Schrödinger-Bopp-Podolsky system using novel functional frameworks and critical point theory.

Abstract: In this paper we study the following zero mass Schr\"{o}dinger-Bopp-Podolsky
system with critical growth \[ \begin{cases} -\Delta u +q^2\phi
u=\mu|u|^{p-2}u+|u|^4u\\ -\Delta \phi+a^2\Delta^2\phi=4\pi u^2, \end{cases} \]
where $a>0$, $q\neq0$, $\mu>0$ is a parameter and $p\in(3,6)$. By introducing a
new functional framework developed by Caponio et al. \cite{Cd}, we first
establish the existence of positive ground state solutions for the case of
$p\in(3,6)$. Moreover, for the case of $p\in(4,6)$, multiplicity results are
obtained by applying an abstract critical point theorem due to Perera
\cite{Pe}.

</details>


### [58] [Generalized Strichartz estimates for the massive Dirac equation with critical potentials](https://arxiv.org/abs/2510.23283)
*Federico Cacciafesta,Elena Danesi,Eric Séré*

Main category: math.AP

TL;DR: The paper proves generalized Strichartz estimates for the massive Dirac equation with two critical potential perturbations: 2D Aharonov-Bohm magnetic potential and 3D Coulomb potential.


<details>
  <summary>Details</summary>
Motivation: To establish dispersive estimates for the massive Dirac equation with critical potentials, which had not been previously achieved.

Method: Uses the relativistic Hankel transform adapted from massless systems to the massive case, providing explicit solution representations and analyzing generalized eigenfunctions of the operators.

Result: First dispersive estimates for the massive Dirac equation with critical potentials are obtained.

Conclusion: The approach successfully extends previous methods from massless systems to prove Strichartz estimates for massive Dirac equations with critical potentials.

Abstract: In this paper we prove generalized Strichartz estimates for the massive Dirac
equation in the case of two critical potential perturbations, namely the $2d$
Aharonov-Bohm magnetic potential and the $3d$ Coulomb potential. The proof
makes use of the relativistic Hankel transform introduced in previous works of
Cacciafesta, S\'er\'e and Cacciafesta, Fanelli for the massless systems, and
here adapted to the massive case: this allows for an explicit representation of
the solutions, which reduces the analysis to the proof of suitable estimates on
the generalized eigenfunctions of the operators. To the best of our knowledge,
these are the first dispersive estimates for the massive Dirac equation with
critical potentials.

</details>


### [59] [Generalized boundary rigidity and minimal surface transform](https://arxiv.org/abs/2510.23366)
*Leonard Busch,Tony Liimatainen,Mikko Salo,Leo Tzou*

Main category: math.AP

TL;DR: The paper studies whether areas of embedded minimal surfaces can uniquely determine a Riemannian manifold with boundary, proving metric determination under conformal perturbations and establishing Hölder stability.


<details>
  <summary>Details</summary>
Motivation: To investigate the generalized boundary rigidity problem and extend earlier 2+1 dimensional results to higher dimensions, with applications to geometric inverse problems and the AdS/CFT correspondence in physics.

Method: Studies the linearized forward operator giving rise to the minimal surface transform, showing it fits into double fibration transforms framework and satisfies the Bolker condition. Proves invertibility under foliation conditions and analytic wave front set recovery.

Result: For conformal perturbations of analytic metrics in dimension n+1 (n≥2), the metric is determined by minimal surface volumes under ampleness conditions, with Hölder stability established.

Conclusion: The methods provide new tools for generalized boundary rigidity problems and expand applications of double fibration transforms, with potential applications to other geometric inverse problems and physics.

Abstract: We study a generalized boundary rigidity problem, which investigates whether
the areas of embedded minimal surfaces can uniquely determine a Riemannian
manifold with boundary. We prove that for a conformal perturbation of an
analytic metric in dimension $n+1$ ($n \geq 2$), the metric is determined by
these volumes under an ampleness condition. Furthermore, we establish H\"older
stability for this determination. This result extends earlier works in
dimension $2+1$. Instead of relying on reductions to Calder\'on type problems
and complex geometrical optics solutions, we study the linearized forward
operator that gives rise to the minimal surface transform, a generalization of
the X-ray/Radon transform. We demonstrate that this transform fits into the
framework of double fibration transforms and satisfies the Bolker condition in
the sense of Guillemin. Under certain assumptions, including a foliation
condition, we prove invertibility of this transform on an analytic manifold as
well as recovery of the analytic wave front set. The methods developed in this
paper offer new tools for addressing the generalized boundary rigidity problem
and expand the scope of applications of double fibration transforms. We
anticipate that these techniques will also be applicable to other geometric
inverse problems. Beyond mathematics, our results have implications for the
AdS/CFT correspondence in physics.

</details>


### [60] [Full Benjamin-Feir instability of capillary-gravity Stokes waves in finite depth](https://arxiv.org/abs/2510.23456)
*Ting-Yang Hsiao,Alberto Maspero*

Main category: math.AP

TL;DR: Analysis of linear stability/instability of small-amplitude periodic Stokes waves in 2D gravity-capillary water waves under long-wave perturbations, proving complete splitting of four eigenvalues near zero and 'figure 8' spectral pattern in unstable regions.


<details>
  <summary>Details</summary>
Motivation: To rigorously analyze the linear stability properties of periodic Stokes wave solutions in finite-depth fluids under combined gravity and surface tension effects, addressing a long-standing problem in water wave theory.

Method: Using Bloch-Floquet theory to investigate periodic eigenvalue problems, analyzing the linearized operator with periodic coefficients and defective zero eigenvalue of multiplicity four, and studying eigenvalue splitting near zero for small wave amplitudes and Floquet parameters.

Result: Complete splitting of the four eigenvalues near zero is established for all surface tension values and depths. In unstable regions identified by Djordjevic-Redekopp and Ablowitz-Segur, the spectrum near the origin forms a 'figure 8' pattern.

Conclusion: The paper provides rigorous mathematical proof of the spectral structure and instability patterns in gravity-capillary water waves, confirming earlier formal predictions about unstable regions and their characteristic 'figure 8' eigenvalue distribution.

Abstract: We study the two-dimensional gravity-capillary water waves equations for a
fluid of finite depth $\mathtt{h}>0$ under the combined effects of gravity and
surface tension $\kappa \geq 0$. We analyze the linear stability and
instability of small-amplitude, $2\pi$-periodic Stokes wave solutions, under
the effect of longitudinal long-wave perturbations. The corresponding
linearized operator has periodic coefficients and a defective zero eigenvalue
of multiplicity four. Using Bloch-Floquet theory, we investigate the associated
family of periodic eigenvalue problems. For all surface tension values $\kappa
\geq 0$ and depths $\mathtt{h} > 0$, we establish the complete splitting of the
four eigenvalues near zero when both the wave amplitude and the Floquet
parameter are small. Specifically, we rigorously prove that in the regions of
unstable depth and capillarity identified formally by Djordjevic-Redekopp and
Ablowitz-Segur in the 1970's, the spectrum of the linearized operator near the
origin depicts a "figure 8" pattern.

</details>


### [61] [$Γ$-convergence for higher order nonlocal phase transitions](https://arxiv.org/abs/2510.23527)
*Hardy Chan,Serena Dipierro,Mattia Freguglia,Marco Inversi,Enrico Valdinoci*

Main category: math.AP

TL;DR: The paper studies Gamma-convergence of rescaled fractional Allen-Cahn energies, showing convergence to classical perimeter for s≥1/2 and fractional perimeter for s<1/2, contradicting expectations of curvature-dependent terms.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of fractional Allen-Cahn energies and their relationship with different types of perimeters, particularly investigating whether curvature-dependent terms appear in the limit as suggested by previous results in different parameter regimes.

Method: Analysis of the ε-rescaled sum of s-fractional Allen-Cahn energy and squared L²-norm of its first variation, studying the limit as ε→0 using Gamma-convergence techniques.

Result: The contribution of the first variation vanishes as ε→0, leading to Gamma-convergence to classical perimeter for s≥1/2 and to 2s-fractional perimeter for s<1/2. No curvature-dependent terms appear in the limit, contrary to expectations from the regime 3/4≤s<1.

Conclusion: The asymptotic behavior of fractional Allen-Cahn energies depends on the parameter s, with a transition at s=1/2 between classical and fractional perimeter limits, and surprisingly no curvature-dependent terms appear in the limit for 0<s<3/4.

Abstract: For every $0 < s <3/4$, we study the asymptotic behavior of the
$\varepsilon$-rescaled sum of the $s$-fractional Allen-Cahn energy and the
squared $L^2$-norm of its first variation. We prove that the contribution of
the first variation vanishes as $\varepsilon \to 0$. This implies the
Gamma-convergence of the initial sum to either the classical perimeter or to
the $2s$-fractional perimeter, depending on whether $s \ge 1/2$ or not. This
contradicts the expectation of finding curvature-dependent terms in the limit,
as suggested by the regime $3/4 \le s < 1$, and as known to hold in low
dimensions in the local case.

</details>


### [62] [The Benjamin-Ono equation with quasi-periodic data](https://arxiv.org/abs/2510.23599)
*Hagen Papenburg*

Main category: math.AP

TL;DR: Local solutions to the Benjamin-Ono equation constructed for quasi-periodic initial data, with uniqueness among smooth solution limits and continuous dependence on data.


<details>
  <summary>Details</summary>
Motivation: To extend local wellposedness results for the Benjamin-Ono equation to a broader class of quasi-periodic functions than previously covered.

Method: Uses a-priori estimates combining Strichartz estimates for quasi-periodic functions (via decoupling) and a quasi-periodic extension of Tao's gauge transform.

Result: Established local wellposedness for quasi-periodic initial data and new local wellposedness results in certain anisotropic Sobolev spaces.

Conclusion: The method successfully constructs unique local solutions for quasi-periodic data with continuous dependence, extending previous results and providing new wellposedness in anisotropic Sobolev spaces.

Abstract: We construct local solutions to the Benjamin-Ono equation for quasi-periodic
initial data. The solution is unique among limits of smooth solutions and
depends continuously on the data. Our result applies to a richer class of
quasi-periodic functions than previous theorems. Central to the argument is an
a-priori estimate, the proof of which utilizes Strichartz estimates for
quasi-periodic functions obtained recently via decoupling, and a quasi-periodic
extension of Tao's gauge transform. As a byproduct of our method, we also
establish new local wellposedness results in certain anisotropic Sobolev
spaces.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [63] [Utilizing SciPy and other open source packages to provide a powerful API for materials manipulation in the Schrödinger Materials Suite](https://arxiv.org/abs/2510.21756)
*Alexandr Fonari,Farshad Fallah,Michael Rauch*

Main category: physics.comp-ph

TL;DR: The paper discusses integration of open source scientific packages in Schrödinger Materials Science Suite for materials discovery workflows, including recent machine learning implementations.


<details>
  <summary>Details</summary>
Motivation: To demonstrate how open source packages can be effectively incorporated into commercial materials science software to accelerate and improve materials discovery processes.

Method: Integration of multiple open source scientific packages into the Schrödinger Materials Science Suite at various stages of materials discovery workflows, including machine learning implementations.

Result: Successful incorporation of open source packages enabled faster and more efficient materials discovery results through leveraging existing tools and frameworks.

Conclusion: Open source packages provide valuable capabilities that can be effectively integrated into commercial materials science software to enhance workflow efficiency and accelerate discovery.

Abstract: The use of several open source scientific packages in the Schr\"odinger
Materials Science Suite will be discussed. A typical workflow for materials
discovery will be described, discussing how open source packages have been
incorporated at every stage. Some recent implementations of machine learning
for materials discovery will be discussed, as well as how open source packages
were leveraged to achieve results faster and more efficiently.

</details>


### [64] [Quantum-inspired space-time PDE solver and dynamic mode decomposition](https://arxiv.org/abs/2510.21767)
*Raghavendra Dheeraj Peddinti,Stefano Pisoni,Narsimha Rapaka,Mohamed K. Riahi,Egor Tiunov,Leandro Aolita*

Main category: physics.comp-ph

TL;DR: This paper proposes quantum-inspired tensor network methods using matrix product states (MPS) for space-time approaches to solve PDEs and make dynamic mode decomposition (DMD) predictions, achieving efficient computation with logarithmic scaling in resolution.


<details>
  <summary>Details</summary>
Motivation: Standard time-stepping methods for PDEs become costly for long-term dynamics, and space-time methods face the curse of dimensionality. The authors aim to leverage quantum-inspired tensor networks to overcome these limitations.

Method: Developed MPS encoding that treats both spatial and temporal dimensions simultaneously within a single matrix product state. Applied this to both PDE solving and DMD predictions, creating an MPS-DMD algorithm for nonlinear systems.

Result: The MPS ansatz accurately captures spatio-temporal correlations with fewer degrees of freedom. MPS-DMD enables accurate long-term predictions of nonlinear systems with runtime scaling logarithmically in both spatial and temporal resolution.

Conclusion: Tensor networks provide effective and interpretable models that bridge numerical methods and data-driven approaches, demonstrating the potential of quantum-inspired methods for space-time computations.

Abstract: Numerical solutions of partial differential equations (PDEs) are central to
the understanding of dynamical systems. Standard approaches involving
time-stepping schemes compute the solution at each time step, which becomes too
costly when simulating long-term dynamics. Alternatively, space-time methods
that treat the combined space-time domain simultaneously promise better
stability and accuracy. Interestingly, data-driven approaches for learning and
predicting dynamics, such as dynamic mode decomposition (DMD), also employ a
combined space-time representation. However, the curse of dimensionality often
limits the practical benefits of space-time methods. In this work, we
investigate quantum-inspired methods for space-time approaches, both for
solving PDEs and for making DMD predictions. We achieve this goal by treating
both spatial and temporal dimensions within a single matrix product state (MPS)
encoding. First, we benchmark our MPS space-time solver for both linear and
nonlinear PDEs, observing that the MPS ansatz accurately captures the
underlying spatio-temporal correlations while having significantly fewer
degrees of freedom. Second, we develop an MPS-DMD algorithm to make accurate
long-term predictions of nonlinear systems, with runtime scaling
logarithmically in both spatial and temporal resolution. This research
highlights the role of tensor networks in developing effective and
interpretable models, bridging the gap between numerical methods and
data-driven approaches.

</details>


### [65] [A DSMC-PIC coupling method for the Vlasov-Maxwell-Landau system](https://arxiv.org/abs/2510.22226)
*Andrea Medaglia,Lorenzo Pareschi,Mattia Zanella*

Main category: physics.comp-ph

TL;DR: A numerical framework coupling DSMC and PIC methods for simulating collisional plasma dynamics in the Vlasov-Maxwell-Landau system, extending homogeneous Landau equation techniques to inhomogeneous electromagnetic regimes.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and physically consistent method for simulating collisional plasma dynamics that can handle both homogeneous and inhomogeneous electromagnetic regimes while maintaining computational efficiency.

Method: Couples Direct Simulation Monte Carlo (DSMC) for Landau collision operator with Particle-in-Cell (PIC) for Vlasov-Maxwell dynamics using operator splitting. Uses stochastic particle formulation inspired by grazing-collision limit of Boltzmann equation.

Result: The method preserves physical invariants while maintaining computational efficiency and implementation simplicity. Numerical experiments demonstrate accuracy, robustness, and effectiveness across various collisional regimes.

Conclusion: The coupled DSMC-PIC approach provides an accurate, robust, and efficient framework for simulating collisional plasma dynamics in electromagnetic regimes, with flexibility in field discretization and time integration choices.

Abstract: We present a numerical framework for the simulation of collisional plasma
dynamics, based on a coupling between Direct Simulation Monte Carlo (DSMC) and
Particle-in-Cell (PIC) methods for the Vlasov-Maxwell-Landau system. The
approach extends previously developed DSMC techniques for the homogeneous
Landau equation to the fully inhomogeneous, electromagnetic regime. The Landau
collision operator is treated through a stochastic particle formulation
inspired by the grazing-collision limit of the Boltzmann equation, which
enables an efficient and physically consistent representation of Coulomb
interactions without relying on the full Boltzmann structure. The resulting
collisional solver is combined, via operator splitting, with standard PIC
schemes for the Vlasov-Maxwell dynamics, providing flexibility in the choice of
field discretisation and time integration. The overall method preserves the
main physical invariants of the system while maintaining computational
efficiency and simplicity of implementation. Numerical experiments on benchmark
problems demonstrate the accuracy, robustness, and effectiveness of the coupled
DSMC-PIC approach across a wide range of collisional regimes.

</details>


### [66] [Mesoscopic Modeling of High-Density Carbon Nanotube Films for Memristive Device Applications](https://arxiv.org/abs/2510.22623)
*Yvelin Giret,Filippo Federici Canova,Al-Moatasem El-Sayed,Thomas R. Durrant,Rahul Sen,Harry Luan,Gennadi Bersuker,Alexander L. Shluger,David Z. Gao*

Main category: physics.comp-ph

TL;DR: The study uses coarse-grained molecular dynamics to model CNT films and identifies structural parameters that enhance electrical conductivity, particularly high curvature, low bundling, and strong connectivity.


<details>
  <summary>Details</summary>
Motivation: To understand the electrical conduction mechanism in CNT films used in non-volatile memory cells, as current understanding is limited despite their promising applications.

Method: Employed coarse-grained molecular dynamics to construct dense mesoscale CNT film models with varying CNT chiralities and lengths, used structural descriptors to quantify morphology, and applied nodal analysis to compute electrical current.

Result: Electrical transport is enhanced in films with high curvature and buckling, low bundling, strong connectivity, and amorphous carbon inclusions play a configuration-dependent role.

Conclusion: The findings provide a framework for rational design of CNT-based memristor architectures and demonstrate the potential of mesoscale modeling for engineering nanostructured materials.

Abstract: Carbon nanotube (CNTs) materials, which exhibit intrinsically high electrical
conductivity, are promising candidates for energy-efficient electronic devices.
Recently, high-density CNT films have also been successfully employed as
switching elements in non-volatile memory cells. However, the mechanism of
electrical conduction through such complex systems is still poorly understood.
To identify structural parameters that govern the electrical current in CNT
films, we employed coarse-grained molecular dynamics to construct dense
mesoscale CNT film models, where we considered CNTs with different chiralities
and lengths. The effects of CNT geometrical features on the film morphologies
were quantified by devising a set of structural descriptors and analyzing their
mutual correlations. The impact of varying the concentration of amorphous
carbon (aC) inclusions on the film structure was assessed. Finally, we employed
a nodal analysis framework to compute the electrical current across the
networks and correlate the charge transport characteristics to the underlying
structural descriptors. Transport is found to be enhanced in films that exhibit
high curvature and buckling, low bundling, and strong connectivity, with
amorphous carbon components playing a nontrivial configuration-dependent role.
These findings provide a framework for the rational design of CNT-based
memristor architectures and highlight the potential of mesoscale modeling to
guide the engineering of advanced nanostructured materials.

</details>


### [67] [Using "AI Poincare" to analyze non-linear integrable optics](https://arxiv.org/abs/2510.22705)
*Lazare Osmanov,Nilanjan Banerjee*

Main category: physics.comp-ph

TL;DR: This paper evaluates AI Poincaré's performance in discovering conserved quantities in accelerator physics systems, identifies optimal perturbation ranges for manifold extraction, proposes an improved neural network architecture, and successfully applies the method to experimental data from Fermilab's Integrable Optics Test Accelerator.


<details>
  <summary>Details</summary>
Motivation: To explore the applicability of automated discovery of conserved quantities in dynamical systems relevant to accelerator physics, specifically using AI Poincaré to analyze numerical trajectory data from non-linear integrable optics systems.

Method: Comprehensive evaluation using diverse methodologies including analysis of estimated conserved quantities, deviation of interpolated points on inferred manifolds, identification of optimal perturbation distances for manifold extraction, and proposing an improved neural network architecture based on observed results.

Result: The investigation identified an optimal range of perturbation distances where the manifold extraction algorithm exhibits optimal performance. The method was successfully applied to experimental data from Fermilab's Integrable Optics Test Accelerator, inferring conserved quantities even with fast decoherence of measured signals.

Conclusion: AI Poincaré is effective for automated discovery of conserved quantities in accelerator physics systems, with identified optimal operating parameters and demonstrated applicability to real experimental data despite signal decoherence challenges.

Abstract: This study dives into the applicability of using automated discovery of
conserved quantities in dynamical systems relevant to accelerator physics.
Specifically, we explore the performance of AI Poincar\'e in analyzing
numerical trajectory data obtained using the McMillan system of non-linear
integrable optics. A comprehensive evaluation of the algorithm's performance is
conducted through diverse methodologies. These include the analysis of the
estimated number of conserved quantities embedded in a dataset and the
deviation of interpolated points on the inferred manifold with respect to
points in actually in the dataset. the investigation identifies an optimal
range of perturbation distances where the underlying manifold extraction
algorithm inside AI Poincar\'e exhibits optimal performance. Additionally, an
improved neural network architecture is proposed based on the observed results.
Finally, we apply the algorithm to preliminary experimental data from the
Integrable Optics Test Accelerator at Fermilab to successfully infer the number
of conserved quantities even in the presence of fast decoherence of the
measured signal.

</details>


### [68] [Fast integration method for averaging polydisperse bubble population dynamics](https://arxiv.org/abs/2510.22920)
*Spencer H. Bryngelson*

Main category: physics.comp-ph

TL;DR: A Levin collocation method is developed to efficiently compute statistical moments in polydisperse bubbly flow models, reducing computational cost by 100-10,000 times compared to traditional quadrature methods.


<details>
  <summary>Details</summary>
Motivation: Ensemble-averaged bubbly flow models require expensive computation of statistical moments that become highly oscillatory, making traditional quadrature methods computationally prohibitive for large-scale simulations.

Method: The paper formulates a Levin collocation method that approximates the integrand by evaluating its derivative via polynomial collocation, leveraging the differential matrix and amplitude function for efficient numerical differentiation.

Result: For an excited polydisperse bubble population, the method achieves 10^-3 relative error with 100 times fewer quadrature nodes than trapezoidal rule, and 10^4 times fewer points for 10^-8 relative error.

Conclusion: The Levin collocation method maintains constant computational cost as integrands become more oscillatory over time, making it particularly suitable for long-time simulations in bubbly flow applications.

Abstract: Ensemble-averaged polydisperse bubbly flow models require statistical moments
of the evolving bubble size distribution. Under step forcing, these moments
reach statistical equilibrium in finite time. However, the transitional phase
before equilibrium and cases with time-dependent forcing are required to
predict flow in engineering applications. Computing these moments is expensive
because the integrands are highly oscillatory, even when the bubble dynamics
are linear. Ensemble-averaged models compute these moments at each grid point
and time step, making cost reduction important for large-scale bubbly flow
simulations. Traditional methods evaluate the integrals via traditional
quadrature rules. This approach requires a large number of quadrature nodes in
the equilibrium bubble size, each equipped with its own advection partial
differential equation (PDE), resulting in significant computational expense. We
formulate a Levin collocation method to reduce this cost. Given the
differential equation associated with the integrand, or moment, the method
approximates it by evaluating its derivative via polynomial collocation. The
differential matrix and amplitude function are well-suited to numerical
differentiation via collocation, and so the computation is comparatively cheap.
For an example excited polydisperse bubble population, the first moment is
computed with the presented method at $10^{-3}$ relative error with 100 times
fewer quadrature nodes than the trapezoidal rule. The gap increases for smaller
target relative errors: the Levin method requires $10^4$ times fewer points for
a relative error of $10^{-8}$. The formulated method maintains constant cost as
the integrands become more oscillatory with time, making it particularly
attractive for long-time simulations.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [69] [High-Harmonic Optical Vortex Generation from a Plasma Aperture](https://arxiv.org/abs/2510.22091)
*Runze Li,Wenchao Yan,Longqing Yi*

Main category: physics.plasm-ph

TL;DR: Numerical study shows that oblique incidence, thinner targets, and better laser contrast enhance high-order harmonic vortex generation from femtosecond laser pulses on micro-aperture foil targets, with vortex components separable by divergence.


<details>
  <summary>Details</summary>
Motivation: Previous theoretical studies only considered ideal conditions, but practical experiments require understanding non-ideal scenarios like oblique incidence to avoid optics damage from reflected light.

Method: Numerical simulations of femtosecond circularly polarized laser pulses incident on micrometer-scale apertures in solid foil targets under various non-ideal conditions including oblique incidence.

Result: Oblique incidence angle, reduced target thickness, and improved laser contrast enhance harmonic conversion efficiency. Generated harmonic beams contain both vortex (LG) and non-vortex components, which can be separated by their divergence angles.

Conclusion: This study provides practical insights for designing future experiments and shows that high-order harmonic pure LG modes can be filtered out for fundamental and applied physics research applications.

Abstract: When a high-power, femtosecond, circularly polarized (CP) laser pulse is
incident on a micrometer-scale aperture in a solid foil target, it drives
surface plasma oscillation, generating high-order harmonic vortices in the
diffracted light. However, this mechanism has so far only been studied
theoretically under ideal conditions. In this work, we perform numerical
studies on more realistic situations. In particular, we focus on a scenario
where the laser is obliquely incident on the target surface to avoid the
potential damage of the optics by the reflected light. We demonstrate that
increasing oblique incidence angle, reducing target thickness, and improving
laser contrast can enhance the harmonic conversion efficiency. However, the
generated harmonic beams may contain both Laguerre-Gaussian (LG) (vortex) and
non-LG components under non-ideal conditions. We show that they can be
separated by their divergence, as the vortex components has smaller diverging
angle. In addition, we have performed computational analyses on the harmonic
divergence angles and topological charge spectra of vortex high-order harmonics
under different conditions. These high-order harmonic pure LG modes can
potentially be filtered out for wide range of fundamental and applied physics
researches. This study provides valuable insights for the design and
implementation of future experiments.

</details>


### [70] [Machine Learning approach to modeling of neutral particles transport in plasma](https://arxiv.org/abs/2510.23088)
*M. V. Umansky,G. J. Parker,R. D. Smirnov*

Main category: physics.plasm-ph

TL;DR: A neural network-based propagator approach for fast Monte Carlo modeling of neutral particle transport in fusion plasmas, showing promising initial results in 1D.


<details>
  <summary>Details</summary>
Motivation: To develop a fast and accurate method for modeling neutral particle transport in fusion boundary plasmas using Monte Carlo simulations.

Method: Uses a neural network-based model for the propagator (Green function) that depends on plasma profiles, enabling fast computation of neutral distribution functions.

Result: Initial results from 1D test problems are promising, with smooth dependence on plasma parameters enabling Jacobian-based methods for time-integration and root finding.

Conclusion: The approach shows potential but requires further investigation into scaling to larger systems.

Abstract: A propagator-based approach is investigated for Monte-Carlo (MC) modeling of
neutral particles transport in fusion boundary plasmas. The propagator is
essentially a Green function for the neutral kinetic equation, which depends on
the plasma profiles. A Neural Network (NN) based model for the propagator
provides a fast and accurate solution for the neutral distribution function in
plasma. Furthermore, continuous and smooth dependence of NN-based
reconstruction of the propagator on the plasma parameters opens the possibility
for using this approach with Jacobian-based methods for time-integration and
root finding. Initial results from a small 1D test problem look promising;
however, important research questions are concerned with the scaling of the
algorithm to larger systems.

</details>


### [71] [Design of Backscatter Tailored Optical Fibers for distributed magnetic field sensing using Fiber Optic Pulsed Polarimetry](https://arxiv.org/abs/2510.23562)
*Roger J Smith*

Main category: physics.plasm-ph

TL;DR: Fiber optic pulsed polarimetry uses backscatter-tailored optical fiber with Bragg gratings to measure distributed magnetic fields in fusion devices with high precision and speed.


<details>
  <summary>Details</summary>
Motivation: To develop a high-performance distributed magnetic field sensing technique for magnetic fusion energy devices that can operate at high repetition rates with excellent spatial and temporal resolution.

Method: Uses backscatter-tailored optical fiber (BTOF) with uniformly spaced fiber Bragg gratings that exploit the Faraday effect. The Verdet constant determines measurement strength. Algorithms address multipathing contamination from 3rd order reflections.

Result: Achieves high performance: 5 MHz repetition rates, 1-10 cm spatial resolution, <1% B field accuracy, and nanosecond temporal response. Contamination levels are quantified and bracketed for reliable design.

Conclusion: The technique provides robust distributed magnetic field measurements suitable for various applications including magnetic fusion devices, rail guns, and superconducting magnets, with reliable contamination management.

Abstract: Fiber optic pulsed polarimetry is a LIDAR-like fiber sensing technique that
uses a backscatter enhanced single mode backscatter-tailored optical
fiber(BTOF) to measure the distributed B fields on all Magnetic Fusion Energy
devices. The BTOF has a series of wavelength resonant reflection fiber Bragg
gratings written uniformly along its length. The fiber's Verdet constant
determines the strength of the Faraday effect which effectuates the measurement
of local B along the fiber placed intimately next to or within a magnetized
plasma volume. A robust measurement of the field distribution along the fiber
is obtained at high rep rates, 5 MHz, high spatial resolution(1-10cm), high B
field accuracy(<1%) and temporal response (ns). Multipathing in the BTOF
produces 3rd order reflections that contaminate the LIDAR signal. Algorithms
are given for calculating the level of contamination for uniform and flat
reflection designs, in particular and any reflection series in general. The
contamination is bracketed giving confidence in designing and implementing a
BTOF. Applications include magnetic fusion devices, rail guns, high temperature
superconducting magnets and magnetized target fusion research.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [72] [QC Lab: A Python Package for Quantum-Classical Dynamics](https://arxiv.org/abs/2510.22919)
*Alex Krotz,Ethan Byrd,Ken Miyazaki,Roel Tempelaar*

Main category: physics.chem-ph

TL;DR: QC Lab is an open-source Python package for quantum-classical dynamics simulations with modular design for algorithm and model compatibility.


<details>
  <summary>Details</summary>
Motivation: To promote development of QC algorithms and their application to various model problems by providing a reusable framework.

Method: Modular design that decomposes algorithms and models into interchangeable tasks and ingredients, minimizing code redundancy.

Result: First stable release of QC Lab package with cross-compatible architecture.

Conclusion: QC Lab provides a flexible framework that facilitates QC algorithm development and application through its modular design philosophy.

Abstract: QC Lab is an open-source Python package for QC dynamics simulations aimed to
promote the development of QC algorithms, and their application to a wide
variety of relevant model problems. It follows a modular design that
facilitates cross-compatibility between algorithms and models. By decomposing
algorithms and models into a series of tasks and ingredients that can be
substituted and reused, it minimizes development efforts and code redundancy.
In this Paper, we introduce the first stable release of QC Lab, and describe
its design philosophy.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [73] [Adaptive Multilevel Splitting: First Application to Rare-Event Derivative Pricing](https://arxiv.org/abs/2510.23461)
*Riccardo Gozzo*

Main category: q-fin.CP

TL;DR: Adaptive multilevel splitting (AMS) method is applied to price binary options in rare-event settings, achieving up to 200x computational gains over standard Monte Carlo for deep out-of-the-money cases while maintaining unbiasedness.


<details>
  <summary>Details</summary>
Motivation: Standard Monte Carlo is inefficient for pricing deep out-of-the-money binary options due to discontinuous payoffs and low exercise probabilities, requiring very large sample sizes for accurate estimates.

Method: Developed an AMS scheme for binary options under Black-Scholes and Heston dynamics, reformulating the rare-event problem as a sequence of conditional events. Provided open-source Rcpp implementation supporting multiple discretizations and importance functions.

Result: Numerical experiments showed up to 200-fold computational gain for deep out-of-the-money cases compared to Monte Carlo and other techniques (antithetic variables, MLMC), while preserving unbiasedness. No prior applications of AMS to financial derivatives were found.

Conclusion: The AMS approach significantly improves pricing efficiency for rare-event contracts like parametric insurance and catastrophe linked securities, offering substantial computational advantages over traditional methods.

Abstract: This work analyzes the computational burden of pricing binary options in
rare-event settings and introduces an adaptation of the adaptive multilevel
splitting (AMS) method for financial derivatives. Standard Monte Carlo is
inefficient for deep out of the money binaries due to discontinuous payoffs and
low exercise probabilities, requiring very large samples for accurate
estimates. An AMS scheme is developed for binary options under Black-Scholes
and Heston dynamics, reformulating the rare-event problem as a sequence of
conditional events. Numerical experiments compare the method to Monte Carlo and
to other techniques such as antithetic variables and multilevel Monte Carlo
(MLMC) across four contracts: European digital calls and puts, and Asian
digital calls and puts. Results show up to a 200-fold computational gain for
deep out-of-the-money cases while preserving unbiasedness. No evidence is found
of prior applications of AMS to financial derivatives. The approach improves
pricing efficiency for rare-event contracts such as parametric insurance and
catastrophe linked securities. An open-source Rcpp implementation is provided,
supporting multiple discretizations and importance functions.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [74] [Quasi-Biennial Oscillations and Rieger-type Periodicities in a Babcock-Leighton Solar Dynamo](https://arxiv.org/abs/2510.22061)
*Pawan Kumar,Belur Ravindra,Partha Chowdhury,Bidya Binay Karak*

Main category: astro-ph.SR

TL;DR: The study demonstrates that the Babcock-Leighton (BL) dynamo model can produce quasi-biennial oscillations (QBOs) and Rieger-type periods, with enhanced occurrence when considering combined parameter fluctuations rather than individual ones.


<details>
  <summary>Details</summary>
Motivation: To explain the origin of QBOs and Rieger-type periods in the Sun's magnetic field, which lack widespread theoretical acceptance despite several proposed theories.

Method: Used a 3D kinematic BL dynamo model guided by observations, analyzed with Morlet wavelet and global wavelet power spectrum techniques, and investigated individual BL parameter fluctuations (flux, latitude, time delay, tilt scatter).

Result: The model successfully produced QBOs and Rieger-type periods for the first time, with combined parameter fluctuations enhancing their occurrence probability. Higher dynamo supercriticality suppressed Rieger-type periodicity.

Conclusion: The BL dynamo can generate QBOs and Rieger-type periods, supporting that the solar dynamo is not highly supercritical.

Abstract: The Sun's magnetic field shows the 11-year solar cycle and shorter
periodicities, popularly known as the quasi-biennial oscillations (QBOs) and
Rieger-type periods, or ``season of the Sun." Although several theories have
been proposed to explain the origin of QBOs and Rieger-type periods, no single
theory has widespread acceptance. We explore whether the \bl\ dynamo can
produce Rieger-type periodicity and QBOs and investigate their underlying
physical mechanisms. We use the observationally guided three-dimensional
kinematic \bl\ dynamo model, which has emerged as a successful model for
reproducing many characteristic features of the solar cycle. We use Morlet
wavelet and global wavelet power spectrum techniques to analyze the data
obtained from the model. In our model, we report QBOs and Rieger-type periods
for the first time. Further, we investigate the individual \bl\ parameters
(fluctuations in flux, latitude, time delay and tilt scatter) role in the
occurrence of QBOs and Rieger-type periods. We find that while fluctuations in
the individual parameters of the \bl\ process can produce QBOs and Rieger-type
periodicity, their occurrence probability is enhanced when we consider combined
fluctuations of all parameters in the \bl\ process. Finally, we find that with
the increase of dynamo supercriticality, the model tends to suppress the
generation of Rieger-type periodicity. Thus, this result supports earlier
studies that suggest the solar dynamo is not highly supercritical.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [75] [Towards Deep Physics-Informed Kolmogorov-Arnold Networks](https://arxiv.org/abs/2510.23501)
*Spyros Rigas,Fotios Anagnostopoulos,Michalis Papachristou,Georgios Alexandridis*

Main category: cs.LG

TL;DR: The paper proposes a new initialization scheme and RGA KANs architecture to address training instability in deep physics-informed KANs, achieving superior performance and stability on PDE benchmarks.


<details>
  <summary>Details</summary>
Motivation: Deep Chebyshev-based physics-informed KANs (cPIKANs) face training instabilities when scaled to depth, limiting their applicability to PDE problems, similar to issues with multilayer perceptron-based models.

Method: Proposes a basis-agnostic Glorot-like initialization scheme to preserve activation variance, and introduces Residual-Gated Adaptive KANs (RGA KANs) inspired by PirateNet architecture to mitigate divergence in deep networks.

Result: RGA KANs consistently outperform parameter-matched cPIKANs and PirateNets by several orders of magnitude on seven standard forward PDE benchmarks, remaining stable where others diverge, and successfully traverse all training phases unlike baseline cPIKANs.

Conclusion: The proposed initialization scheme and RGA KANs architecture effectively address training instability in deep physics-informed KANs, enabling stable and accurate solutions to PDE problems where previous methods fail.

Abstract: Since their introduction, Kolmogorov-Arnold Networks (KANs) have been
successfully applied across several domains, with physics-informed machine
learning (PIML) emerging as one of the areas where they have thrived. In the
PIML setting, Chebyshev-based physics-informed KANs (cPIKANs) have become the
standard due to their computational efficiency. However, like their multilayer
perceptron-based counterparts, cPIKANs face significant challenges when scaled
to depth, leading to training instabilities that limit their applicability to
several PDE problems. To address this, we propose a basis-agnostic, Glorot-like
initialization scheme that preserves activation variance and yields substantial
improvements in stability and accuracy over the default initialization of
cPIKANs. Inspired by the PirateNet architecture, we further introduce
Residual-Gated Adaptive KANs (RGA KANs), designed to mitigate divergence in
deep cPIKANs where initialization alone is not sufficient. Through empirical
tests and information bottleneck analysis, we show that RGA KANs successfully
traverse all training phases, unlike baseline cPIKANs, which stagnate in the
diffusion phase in specific PDE settings. Evaluations on seven standard forward
PDE benchmarks under a fixed training pipeline with adaptive components
demonstrate that RGA KANs consistently outperform parameter-matched cPIKANs and
PirateNets - often by several orders of magnitude - while remaining stable in
settings where the others diverge.

</details>


### [76] [Numerical Fragility in Transformers: A Layer-wise Theory for Explaining, Forecasting, and Mitigating Instability](https://arxiv.org/abs/2510.21770)
*Jinwoo Baek*

Main category: cs.LG

TL;DR: The paper provides a first-order theory for predicting forward-error amplification in low-precision Transformers, with interpretable diagnostics for self-attention stability and practical mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Transformers trained in low precision suffer from forward-error amplification, but existing methods lack predictive diagnostics to understand when and where errors grow during training.

Method: Developed a module-wise theory with three self-attention diagnostics: score-scale ratio, rowwise softmax sensitivity, and value conditioning. Also introduced residual relaxation inequality and LayerNorm indicator for unified forward-stability bound.

Result: The combined predictor tracks FP32-LP mismatches across configurations, softmax sensitivity acts as early-warning signal (16-24 steps ahead), and LayerNorm tweak provides consistent stabilization with minimal overhead.

Conclusion: The theory provides actionable, unitless diagnostics that explain self-attention fragility, forecast instability, and enable minimally invasive mitigation strategies for low-precision Transformer training.

Abstract: Transformers trained in low precision can suffer forward-error amplification.
We give a first-order, module-wise theory that predicts when and where errors
grow. For self-attention we derive a per-layer bound that factorizes into three
interpretable diagnostics: a score-scale ratio $\kappa_{\rm score}$, a rowwise
softmax sensitivity $\kappa_{\rm softmax}$, and value conditioning $\kappa(V)$.
We prove a residual relaxation inequality showing that residual blocks
attenuate depth-wise accumulation, and we introduce a precision- and
width-aware LayerNorm indicator $\rho_{\rm LN}$ with a matching first-order
bound in the $\epsilon$-dominated regime. These pieces yield a unified
forward-stability bound whose right-hand side is directly estimable during
training.
  On Tiny-ViT/CIFAR-10 we evaluate the bound and components. (1) The combined
predictor $\kappa_{\rm softmax},(1+\kappa_{\rm
score}),\kappa(V),|W_O|2+\kappa{\rm eff}+C_{\rm LN}$ tracks
FP32$\leftrightarrow$LP mismatches across seeds, widths, and precisions;
scaling by $\epsilon_{\rm mach}$ collapses mixed-precision points. (2) The
time-series maximum of $\kappa_{\rm softmax}$ acts as an early-warning signal,
leading error spikes by 16-24 steps (corr. 0.65-0.82; permutation
$p!\approx!10^{-3}$; Precision@K 0.89-1.00). (3) Guided by $\rho_{\rm LN}$, a
small LayerNorm-$\epsilon$ tweak targeting $\rho_\star$ gives consistent
stabilization (mean tail-loss $\downarrow\ \approx0.010$ at $\rho_\star!=!0.6$,
cap$=10^{-2}$) with negligible overhead.
  Overall, our theory supplies actionable, unitless diagnostics that (i)
explain when self-attention is fragile, (ii) forecast instability, and (iii)
motivate a minimally invasive mitigation.

</details>


### [77] [Chebyshev Moment Regularization (CMR): Condition-Number Control with Moment Shaping](https://arxiv.org/abs/2510.21772)
*Jinwoo Baek*

Main category: cs.LG

TL;DR: CMR is a regularization method that directly optimizes layer spectra using Chebyshev moments and condition number control, significantly improving model conditioning and performance in adversarial settings.


<details>
  <summary>Details</summary>
Motivation: To address poor conditioning in deep networks that leads to optimization instability and degraded performance, especially in adversarial scenarios.

Method: Uses Chebyshev Moment Regularization with joint control of spectral edges via log-condition proxy and interior shaping via Chebyshev moments, with decoupled capped mixing to preserve task gradients.

Result: Reduces mean layer condition numbers by ~10^3 (from ~3900 to ~3.4 in 5 epochs), increases gradient magnitude, and restores test accuracy from ~10% to ~86% in adversarial MNIST setting.

Conclusion: Supports optimization-driven spectral preconditioning as an effective approach for steering models toward well-conditioned regimes for stable and accurate learning.

Abstract: We introduce \textbf{Chebyshev Moment Regularization (CMR)}, a simple,
architecture-agnostic loss that directly optimizes layer spectra. CMR jointly
controls spectral edges via a log-condition proxy and shapes the interior via
Chebyshev moments, with a decoupled, capped mixing rule that preserves task
gradients. We prove strictly monotone descent for the condition proxy, bounded
moment gradients, and orthogonal invariance. In an adversarial
``$\kappa$-stress'' setting (MNIST, 15-layer MLP), \emph{compared to vanilla
training}, CMR reduces mean layer condition numbers by $\sim\!10^3$ (from
$\approx3.9\!\times\!10^3$ to $\approx3.4$ in 5 epochs), increases average
gradient magnitude, and restores test accuracy (
$\approx10\%\!\to\!\approx86\%$ ). These results support
\textbf{optimization-driven spectral preconditioning}: directly steering models
toward well-conditioned regimes for stable, accurate learning.

</details>


### [78] [Revisiting Orbital Minimization Method for Neural Operator Decomposition](https://arxiv.org/abs/2510.21952)
*J. Jon Ryu,Samuel Zhou,Gregory W. Wornell*

Main category: cs.LG

TL;DR: The paper revisits the classical orbital minimization method (OMM) from computational physics for spectral decomposition of linear operators, adapts it for neural network training, and demonstrates its advantages across benchmark tasks.


<details>
  <summary>Details</summary>
Motivation: To provide a principled approach for deploying neural networks in numerical simulation and create effective tools for machine learning by revisiting classical numerical methods through modern theory and computation.

Method: Adapt the orbital minimization method (OMM) framework to train neural networks for decomposing positive semidefinite operators, with a linear-algebraic proof of consistency.

Result: Demonstrated practical advantages across a range of benchmark tasks, showing the method's broader applicability in modern learning pipelines.

Conclusion: Revisiting classical numerical methods through modern theory and computation provides both principled approaches for neural networks in numerical simulation and effective, scalable tools for machine learning.

Abstract: Spectral decomposition of linear operators plays a central role in many areas
of machine learning and scientific computing. Recent work has explored training
neural networks to approximate eigenfunctions of such operators, enabling
scalable approaches to representation learning, dynamical systems, and partial
differential equations (PDEs). In this paper, we revisit a classical
optimization framework from the computational physics literature known as the
\emph{orbital minimization method} (OMM), originally proposed in the 1990s for
solving eigenvalue problems in computational chemistry. We provide a simple
linear-algebraic proof of the consistency of the OMM objective, and reveal
connections between this method and several ideas that have appeared
independently across different domains. Our primary goal is to justify its
broader applicability in modern learning pipelines. We adapt this framework to
train neural networks to decompose positive semidefinite operators, and
demonstrate its practical advantages across a range of benchmark tasks. Our
results highlight how revisiting classical numerical methods through the lens
of modern theory and computation can provide not only a principled approach for
deploying neural networks in numerical simulation, but also effective and
scalable tools for machine learning.

</details>


### [79] [Accelerating Eigenvalue Dataset Generation via Chebyshev Subspace Filter](https://arxiv.org/abs/2510.23215)
*Hong Wang,Jie Wang,Jian Luo,huanshuo dong,Yeqiu Chen,Runmin Jiang,Zhen huang*

Main category: cs.LG

TL;DR: SCSF accelerates eigenvalue data generation for neural eigenvalue methods by grouping similar operators and reusing eigenpairs from solved problems, achieving 3.5x speedup over traditional solvers.


<details>
  <summary>Details</summary>
Motivation: Neural eigenvalue methods require large labeled datasets (operators and eigenvalues) for training, but generating this data is computationally expensive with traditional solvers.

Method: Uses truncated fast Fourier transform sorting to group operators with similar eigenvalue distributions, then constructs Chebyshev subspace filter that leverages eigenpairs from previously solved problems to solve new ones efficiently.

Result: Achieves up to 3.5x speedup compared to various numerical solvers for eigenvalue data generation.

Conclusion: SCSF is the first method to accelerate eigenvalue data generation and effectively addresses the data bottleneck in training neural eigenvalue methods.

Abstract: Eigenvalue problems are among the most important topics in many scientific
disciplines. With the recent surge and development of machine learning, neural
eigenvalue methods have attracted significant attention as a forward pass of
inference requires only a tiny fraction of the computation time compared to
traditional solvers. However, a key limitation is the requirement for large
amounts of labeled data in training, including operators and their eigenvalues.
To tackle this limitation, we propose a novel method, named Sorting Chebyshev
Subspace Filter (SCSF), which significantly accelerates eigenvalue data
generation by leveraging similarities between operators -- a factor overlooked
by existing methods. Specifically, SCSF employs truncated fast Fourier
transform sorting to group operators with similar eigenvalue distributions and
constructs a Chebyshev subspace filter that leverages eigenpairs from
previously solved problems to assist in solving subsequent ones, reducing
redundant computations. To the best of our knowledge, SCSF is the first method
to accelerate eigenvalue data generation. Experimental results show that SCSF
achieves up to a $3.5\times$ speedup compared to various numerical solvers.

</details>


### [80] [Grassmanian Interpolation of Low-Pass Graph Filters: Theory and Applications](https://arxiv.org/abs/2510.23235)
*Anton Savostianov,Michael T. Schaub,Benjamin Stamm*

Main category: cs.LG

TL;DR: The paper proposes a novel algorithm for low-pass graph filter interpolation using Riemannian interpolation on the Grassmann manifold to avoid expensive eigenvalue computations for parametric graph families.


<details>
  <summary>Details</summary>
Motivation: Computing low-pass graph filters for parametric graph families is computationally expensive as it requires repeatedly solving eigenvalue problems to find low-frequency subspaces.

Method: Uses Riemannian interpolation in normal coordinates on the Grassmann manifold for low-pass graph filter interpolation, with error bound estimates for subspace interpolation.

Result: Proposes two applications: 1) translating temporal node feature evolution to evolving graph topology via similarity correction, and 2) a dot product graph family induced by static graphs for improved message passing schemes.

Conclusion: The Riemannian interpolation approach enables efficient computation of low-pass graph filters for parametric graph families, with applications in evolving graph analysis and improved graph neural network message passing.

Abstract: Low-pass graph filters are fundamental for signal processing on graphs and
other non-Euclidean domains. However, the computation of such filters for
parametric graph families can be prohibitively expensive as computation of the
corresponding low-frequency subspaces, requires the repeated solution of an
eigenvalue problem. We suggest a novel algorithm of low-pass graph filter
interpolation based on Riemannian interpolation in normal coordinates on the
Grassmann manifold. We derive an error bound estimate for the subspace
interpolation and suggest two possible applications for induced parametric
graph families. First, we argue that the temporal evolution of the node
features may be translated to the evolving graph topology via a similarity
correction to adjust the homophily degree of the network. Second, we suggest a
dot product graph family induced by a given static graph which allows to infer
improved message passing scheme for node classification facilitated by the
filter interpolation.

</details>


### [81] [Mixed Precision Training of Neural ODEs](https://arxiv.org/abs/2510.23498)
*Elena Celledoni,Brynjulf Owren,Lars Ruthotto,Tianjiao Nicole Yang*

Main category: cs.LG

TL;DR: A mixed precision training framework for neural ODEs that uses low-precision computations for velocity evaluation and state storage, with custom adjoint scaling and high-precision accumulation, achieving ~50% memory reduction and 2x speedup while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard mixed precision training schemes are unreliable for continuous-time architectures like neural ODEs, which face computational costs from repeated network evaluations and growing memory requirements with time steps.

Method: Combines explicit ODE solvers with custom backpropagation using low-precision for velocity evaluation and intermediate states, with stability provided by dynamic adjoint scaling and high-precision accumulation of solutions and gradients.

Result: Achieved approximately 50% memory reduction and up to 2x speedup while maintaining comparable accuracy to single-precision training across image classification and generative model applications.

Conclusion: The proposed mixed precision framework effectively addresses computational and memory challenges in neural ODE training while maintaining reliability, with an open-source PyTorch package provided for adoption.

Abstract: Exploiting low-precision computations has become a standard strategy in deep
learning to address the growing computational costs imposed by ever larger
models and datasets. However, naively performing all computations in low
precision can lead to roundoff errors and instabilities. Therefore, mixed
precision training schemes usually store the weights in high precision and use
low-precision computations only for whitelisted operations. Despite their
success, these principles are currently not reliable for training
continuous-time architectures such as neural ordinary differential equations
(Neural ODEs). This paper presents a mixed precision training framework for
neural ODEs, combining explicit ODE solvers with a custom backpropagation
scheme, and demonstrates its effectiveness across a range of learning tasks.
Our scheme uses low-precision computations for evaluating the velocity,
parameterized by the neural network, and for storing intermediate states, while
stability is provided by a custom dynamic adjoint scaling and by accumulating
the solution and gradients in higher precision. These contributions address two
key challenges in training neural ODE: the computational cost of repeated
network evaluations and the growth of memory requirements with the number of
time steps or layers. Along with the paper, we publish our extendable,
open-source PyTorch package rampde, whose syntax resembles that of leading
packages to provide a drop-in replacement in existing codes. We demonstrate the
reliability and effectiveness of our scheme using challenging test cases and on
neural ODE applications in image classification and generative models,
achieving approximately 50% memory reduction and up to 2x speedup while
maintaining accuracy comparable to single-precision training.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [82] [Mirror states enable lower viscosity lattice gases](https://arxiv.org/abs/2510.22059)
*Noah Seekins,Alexander J. Wagner*

Main category: physics.flu-dyn

TL;DR: Developed method to significantly lower viscosity in hydrodynamic lattice gas methods using mirror state derivation.


<details>
  <summary>Details</summary>
Motivation: To overcome viscosity limitations in existing lattice gas methods for hydrodynamic simulations.

Method: Derivation of a mirror state that enables viscosity reduction in lattice gas hydrodynamic methods.

Result: Achieved more than an order of magnitude reduction in viscosity compared to existing lattice gas methods.

Conclusion: The mirror state approach provides substantial improvement in viscosity performance for lattice gas hydrodynamic simulations.

Abstract: We developed a method for significantly lowering the viscosity achievable for
a hydrodynamic lattice gas method. The key advance is the derivation of a
mirror state that allows for a reduction of viscosity by more than an order of
magnitude over existing lattice gas methods.

</details>


### [83] [Aeroelastic Reduced-Order Model Differential Equations in Transonic Buffeting Flow](https://arxiv.org/abs/2510.22216)
*Michael Candon,Pier Marzocca,Earl H. Dowell*

Main category: physics.flu-dyn

TL;DR: This paper presents a novel nonlinear aerodynamic reduced-order model (ROM) that combines nonlinear oscillator dynamics with Volterra theory to efficiently simulate aeroelastic shock buffet phenomena.


<details>
  <summary>Details</summary>
Motivation: Numerical simulation of transonic shock buffet is challenging due to nonlinear and unsteady characteristics, especially in 3D and aeroelastic systems. Current studies are limited to 2D, creating a need for efficient ROMs.

Method: The proposed Integro-Differential Equation ROM (IDE-ROM) integrates nonlinear oscillator dynamics with Volterra theory. Model coefficients are identified using Orthogonal Matching Pursuit (OMP) algorithm.

Result: Applied to an OAT15A airfoil, the IDE-ROM successfully reproduces key nonlinear behaviors including aeroelastic lock-in with high accuracy, while being computationally efficient.

Conclusion: The compact IDE-ROM formulation effectively captures aeroelastic shock buffet phenomena, though limitations and potential extensions are also critically examined.

Abstract: Numerical simulation of the transonic shock buffet phenomenon remains a
formidable challenge due to its inherent nonlinear and unsteady
characteristics. These difficulties are further compounded in three-dimensional
configurations and when aeroelastic coupling is considered. Consequently,
computational studies of aeroelastic shock buffet interactions have largely
been confined to two-dimensional systems. This limitation underscores the need
for reduced-order models (ROMs) capable of efficiently and accurately capturing
the aeroelastic response of structures subjected to shock buffet oscillations.
This paper presents a novel nonlinear unsteady aerodynamic ROM that integrates
nonlinear oscillator dynamics with Volterra theory to model aeroelastic shock
buffet phenomena. The coefficients and terms of the resulting
Integro-Differential Equation ROM (IDE-ROM) are identified using the Orthogonal
Matching Pursuit (OMP) algorithm. Application of the IDE-ROM to an OAT15A
airfoil demonstrates that the compact and computationally efficient formulation
can reproduce key nonlinear behaviors, including aeroelastic lock-in, with a
high degree of accuracy. The limitations and potential extensions of the
proposed approach are also critically examined.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [84] [Accelerating IC Thermal Simulation Data Generation via Block Krylov and Operator Action](https://arxiv.org/abs/2510.23221)
*Hong Wang,Wenkai Yang,Jie Wang,Huanshuo Dong,Zijie Geng,Zhen Huang,Depeng Xie,Zhezheng Hao,Hande Dong*

Main category: cs.AI

TL;DR: BlocKOA is a novel algorithm that accelerates IC thermal simulation data generation while improving precision, achieving 420x speedup and enabling efficient training of neural operators with comparable performance.


<details>
  <summary>Details</summary>
Motivation: Existing neural operators for IC thermal simulations require large amounts of high-fidelity training data, which incurs significant computational costs due to the need for chip parameters and temperature distributions.

Method: BlocKOA uses block Krylov algorithm on heat equation structure to get basic solutions, combines them to generate temperature distributions satisfying physical constraints, then applies heat operators to determine heat source distributions for precise data generation.

Result: BlocKOA achieves 420-fold speedup in generating thermal simulation data for 5000 chips with varying parameters. Data-driven approaches trained on BlocKOA data with just 4% generation time show comparable performance to existing methods.

Conclusion: BlocKOA provides an efficient and precise solution for IC thermal simulation data generation, significantly reducing computational costs while maintaining data quality for training neural operators.

Abstract: Recent advances in data-driven approaches, such as neural operators (NOs),
have shown substantial efficacy in reducing the solution time for integrated
circuit (IC) thermal simulations. However, a limitation of these approaches is
requiring a large amount of high-fidelity training data, such as chip
parameters and temperature distributions, thereby incurring significant
computational costs. To address this challenge, we propose a novel algorithm
for the generation of IC thermal simulation data, named block Krylov and
operator action (BlocKOA), which simultaneously accelerates the data generation
process and enhances the precision of generated data. BlocKOA is specifically
designed for IC applications. Initially, we use the block Krylov algorithm
based on the structure of the heat equation to quickly obtain a few basic
solutions. Then we combine them to get numerous temperature distributions that
satisfy the physical constraints. Finally, we apply heat operators on these
functions to determine the heat source distributions, efficiently generating
precise data points. Theoretical analysis shows that the time complexity of
BlocKOA is one order lower than the existing method. Experimental results
further validate its efficiency, showing that BlocKOA achieves a 420-fold
speedup in generating thermal simulation data for 5000 chips with varying
physical parameters and IC structures. Even with just 4% of the generation
time, data-driven approaches trained on the data generated by BlocKOA exhibits
comparable performance to that using the existing method.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [85] [Study of the Molecular Level Mechanism of Nanoscale Alternating Current Electrohydrodynamic Flow](https://arxiv.org/abs/2510.21754)
*Sobin Alosious,Fiach Antaw,Matt Trau,Shern R. Tee,Debra J. Searles*

Main category: cond-mat.soft

TL;DR: Molecular dynamics study reveals high-frequency AC electrohydrodynamic flow in nanopores is driven by localized heating from water molecule alignment, creating temperature gradients and directional flow in asymmetric electrode structures.


<details>
  <summary>Details</summary>
Motivation: To understand the molecular mechanism of AC electrohydrodynamic flow in nanopores under high-frequency conditions, which could enable precise fluid manipulation in nanoscale systems.

Method: Used molecular dynamics simulations with gold-NaCl systems and symmetric/asymmetric electrode configurations to analyze flow patterns under high-frequency AC potentials.

Result: Found localized heat generation near electrodes creating steep temperature gradients, with heat increasing with AC frequency. Asymmetric electrodes generated net directional flow through force imbalances including buoyancy-driven convection, electrothermal effects, and Maxwell stress.

Conclusion: Identified a new nanoscale AC-EHD flow mechanism operating at higher frequencies than conventional methods, independent of ionic concentration, providing insights for optimizing nanoscale fluid manipulation.

Abstract: This study investigates the molecular-level mechanism of Alternating Current
Electrohydrodynamic (AC-EHD) flow in nanopores under high-frequency conditions,
using molecular dynamics simulations. A gold-NaCl system with symmetric and
asymmetric electrode configurations is used to analyze the flow patterns under
high-frequency AC potentials. Our findings reveal localized heat generation
near the electrode leading to a steep temperature gradient. An order parameter
analysis explained that the heat generation is due to the periodic change in
the alignment of water molecules under AC potential and that at these high
frequencies the influence of Na$^+$ and Cl$^-$ ions are negligible. The heat
generation and temperature gradient are found to increase with the applied AC
frequency. Three different electrode configurations were studied by varying the
size and distance between the electrodes. A net directional flow develops in
the asymmetric electrode structures. A possible mechanism for this is proposed
by analyzing the flow patterns using velocity and temperature profiles, order
parameters, streamline plots and mean square displacements. Different forces
acting on the fluid were identified such as buoyancy-driven convection due to
temperature gradient, electrothermal effects influenced by the
temperature-dependent properties of water, and Maxwell stress due to the
non-uniform electric field. Moreover, the asymmetric electrode structure
created an imbalance in these forces and generated a net directional flow.
These findings suggest the existence of a form of nanoscale AC-EHD flow that
operates in a frequency regime above that of conventional electroosmotic and
electrothermal mechanisms and that, unlike these mechanisms, occurs
independently of ionic concentration. Thereby this work provides insights for
optimizing AC-EHD flow in nanoscale systems where precise fluid manipulation is
critical.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [86] [Discrete Bound States in a Toy Model for Weak Turbulence and Implications for the Invariant Measure](https://arxiv.org/abs/2510.22090)
*Jeremy L. Marzuola,Jonathan C. Mattingly*

Main category: math.CA

TL;DR: Study of frequency cascades in cubic defocusing NLS on torus using Hamiltonian dynamical system approach, focusing on canonical ensemble formulation near energy minimizers.


<details>
  <summary>Details</summary>
Motivation: To understand frequency cascade phenomena in nonlinear Schrödinger equations through Hamiltonian dynamics and statistical mechanics framework.

Method: Derived model Hamiltonian dynamical system, classified energy minimizers for fixed mass, characterized invariant measure near minimizers.

Result: Established framework for canonical ensemble formulation of dynamics in neighborhood of energy minimizers.

Conclusion: Provides systematic approach to study frequency cascades using Hamiltonian dynamics and statistical mechanics principles.

Abstract: A model Hamiltonian dynamical system has been derived to study frequency
cascades in the cubic defocusing nonlinear Schr\"odinger equation on the torus.
Here, we explore the framework for exploring a canonical ensemble formulation
of the dynamics through classification of energy minimizers for fixed mass and
characterizing the invariant measure in a neighborhood of those minimizers.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [87] [The critical case for the concentration of eigenfunctions on singular Riemannian manifolds](https://arxiv.org/abs/2510.23520)
*Charlotte Dietze*

Main category: math.SP

TL;DR: The paper analyzes eigenfunction distribution for Laplace-Beltrami operators on Riemannian manifolds with boundary and critical singular metrics, showing average density spreads across scales λ⁻¹/² to 1 near boundary.


<details>
  <summary>Details</summary>
Motivation: To understand how eigenfunctions of Laplace-Beltrami operators distribute when the underlying Riemannian metric becomes singular at the boundary, particularly in critical cases where the operator resembles Grushin-type operators.

Method: Study compact Riemannian manifolds with boundary having critical singular Riemannian metrics that are singular at the boundary, analyzing the corresponding Laplace-Beltrami operator as a Grushin-type operator plus potential.

Result: In the critical case, the average density of eigenfunctions with eigenvalues below λ>0 is distributed over all length scales between λ⁻¹/² and 1 near the boundary, with precise asymptotic description as λ→∞.

Conclusion: The eigenfunction distribution exhibits scale-invariant behavior near singular boundaries, spreading across multiple length scales in the critical regime, providing insight into spectral properties of singular Riemannian manifolds.

Abstract: We consider a compact Riemannian manifold with boundary with a certain class
of critical singular Riemannian metrics that are singular at the boundary. The
corresponding Laplace-Beltrami operator can be seen as a Grushin-type operator
plus a potential. We show in the critical case that the average density of
eigenfunctions for the Laplace-Beltrami operator with eigenvalues below
$\lambda>0$ is distributed over all length scales between $\lambda^{-1/2}$ and
$1$ near the boundary. We give a precise description of this distribution as
$\lambda\to\infty$.

</details>


<div id='physics.class-ph'></div>

# physics.class-ph [[Back]](#toc)

### [88] [An extensive search for stable periodic orbits of the equal-mass zero angular momentum three-body problem](https://arxiv.org/abs/2510.22802)
*Ivan Hristov,Radoslava Hristova,Kiyotaka Tanikawa*

Main category: physics.class-ph

TL;DR: Analysis of a special 2D domain in the equal-mass zero angular momentum planar three-body problem reveals four stability regions with 971 verified stable periodic orbits, many newly discovered, characterized by specific syzygy sequence patterns.


<details>
  <summary>Details</summary>
Motivation: To deepen understanding of stability regions in a previously studied special domain of the equal-mass zero angular momentum planar three-body problem.

Method: Carefully computed decay times in the domain and established four stability regions through analysis of syzygy sequence patterns.

Result: Found 971 verified initial conditions for linearly stable periodic collisionless orbits, many of which are new discoveries.

Conclusion: The identified orbits are candidates for KAM-stable orbits, with each stability region characterized by distinct syzygy sequence patterns.

Abstract: A special 2D initial conditions' domain of the equal-mass zero angular
momentum planar three-body problem, which has been formerly studied, is
analyzed to deepen the knowledge of the stability regions in it. The decay
times in the domain are carefully computed. Four stability regions are
established. 971 verified initial conditions for linearly stable periodic
collisionless orbits are found. Many of these identified initial conditions are
new ones. The periodic orbits of each stability region are characterized by a
certain pattern in their syzygy sequences. Additional computations show that
the orbits found should be considered as candidates for KAM-stable orbits.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [89] [$L^1$ curvature bounds for Type I Ricci flows](https://arxiv.org/abs/2510.22660)
*Panagiotis Gianniotis,Konstantinos Leskas*

Main category: math.DG

TL;DR: The paper establishes L^1-bounds for the Riemann curvature tensor on closed Ricci flows using a decomposition approach with balls having uniform curvature bounds and (n-2)-content estimates.


<details>
  <summary>Details</summary>
Motivation: To provide quantitative control over the Riemann curvature tensor in Ricci flows, which is important for understanding singularity formation and geometric analysis.

Method: Introduces the concept of 'neck of maximal symmetry' and develops a decomposition result using balls with uniform curvature bounds that satisfy appropriate (n-2)-content estimates.

Result: Establishes L^1-bounds of the Riemann curvature tensor on smooth closed n-dimensional Ricci flows.

Conclusion: The decomposition approach with controlled curvature bounds and content estimates successfully provides L^1-bounds for the Riemann curvature tensor in Ricci flows.

Abstract: We show $L^1$-bounds of the Riemann curvature tensor on a smooth closed
$n$-dimensional Ricci flow. To achieve this we introduce the notion of a neck
of maximal symmetry, similar to the one in Cheeger-Jiang-Naber and Jiang-Naber
and establish a decomposition result by balls with uniform curvature bounds
that satisfy an appropriate $(n-2)$-content estimate.

</details>


### [90] [On the rate of convergence of cylindrical singularity in mean curvature flow](https://arxiv.org/abs/2510.23499)
*Yiqi Huang,Xinrui Zhao*

Main category: math.DG

TL;DR: Global graphs over cylinders in rescaled mean curvature flow with small gradient and super-exponential convergence must be the cylinder itself, but local graphs can have non-unique continuation even with super-exponential convergence.


<details>
  <summary>Details</summary>
Motivation: To understand unique continuation properties in cylindrical settings, which are generic singularity models in mean curvature flow, and contrast global vs local graphical behavior.

Method: Proving rigidity for global graphs with small gradient and super-exponential convergence, while constructing counterexamples of local graphs and non-product flows with prescribed singular sets at arbitrary super-exponential rates.

Result: First unique continuation result for cylindrical setting, but unique continuation fails for local graphical solutions. Constructed infinite-dimensional families of Tikhonov-type examples showing global graphical assumptions are essential for rigidity.

Conclusion: Global graphical assumptions play an essential role in rigidity results for mean curvature flow, with new phenomena absent in compact cases. Unique continuation holds globally but fails locally even with super-exponential convergence.

Abstract: We prove that if a rescaled mean curvature flow is a global graph over the
round cylinder with small gradient and converges super-exponentially fast, then
it must coincide with the cylinder itself. We also show that the result is
sharp with counter-examples of local graphs at arbitrarily super-exponential
convergence rate with the domain expanding arbitrarily fast.
  The first part provides the first unique continuation result in the
cylindrical setting, the generic singularity model in mean curvature flow. In
sharp contrast, in the second part we construct infinite-dimensional families
of Tikhonov-type examples for nonlinear equations, including the rescaled mean
curvature flow, showing that unique continuation fails for local graphical
solutions. These examples demonstrate the essential role of global graphical
assumptions in rigidity and highlight new phenomena absent in the compact case.
We also construct non-product mean curvature flows that develop singular sets
as prescribed lower dimensional Euclidean space at arbitrary super-exponential
rates. Our construction works in great generality for a large class of
non-linear equations.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [91] [Non-local Dirichlet forms, Gibbs measures, and a Hodge theorem for Cantor sets](https://arxiv.org/abs/2510.22742)
*Rodrigo Treviño*

Main category: math.DS

TL;DR: Study of generators of non-local Dirichlet forms on ultrametric spaces from Bratteli diagrams, Gibbs measures, and cohomology with harmonic representatives.


<details>
  <summary>Details</summary>
Motivation: To understand spectral properties of non-local Dirichlet form generators and establish connections between harmonic analysis and cohomology on ultrametric spaces.

Method: Define non-local Dirichlet forms using Gibbs measures from Holder continuous potentials, introduce cohomology dual to Bowen-Franks homology, analyze spectral properties of generators.

Result: For sufficiently large gamma (with sharp bounds depending on diagram structure and measure entropy), there exists a unique harmonic representative for each cohomology class.

Conclusion: Establishes existence of unique harmonic representatives in cohomology classes under specific conditions, connecting harmonic analysis with topological invariants.

Abstract: In this paper I study properties of the generators $\triangle_\gamma$ of
non-local Dirichlet forms $\mathcal{E}^\mu_\gamma$ on ultrametric spaces which
are the path space of simple stationary Bratteli diagrams. The measures used to
define the Dirichlet forms are taken to be the Gibbs measures $\mu_\psi$
associated to H\"older continuous potentials $\psi$ for one-sided shifts. I
also define a cohomology $H_{lc}(X_B)$ for $X_B$ which can be seen as dual to
the homology of Bowen and Franks. Besides studying spectral properties of
$\triangle_\gamma$, I show that for $\gamma$ large enough (with sharp bounds
depending on the diagram and the measure theoretic entropy $h_{\mu_\psi}$ of
$\mu_\psi$) there is a unique harmonic representative of any class $c\in
H_{lc}(X_B)$.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [92] [Magnetic transition in B2 Al-Cr-Co alloys](https://arxiv.org/abs/2510.21982)
*Haireguli Aihemaiti,Esmat Dastanpour,Shashank Chaturvedi,Shuo Huang,Anders Bergman,Levente Vitos*

Main category: cond-mat.mtrl-sci

TL;DR: DFT and MC simulations study magnetic transitions in B2 Al-Cr-Co alloys, showing AFM to FM transition with increasing Co content, with Curie temperatures below 160K and potential spin-glass behavior at high Co concentrations.


<details>
  <summary>Details</summary>
Motivation: To understand how chemistry-driven changes in exchange interactions affect magnetism in B2 Al-Cr-Co alloys, particularly the recently reported magnetic transition.

Method: Used Density Functional Theory (DFT) calculations and Monte-Carlo (MC) simulations to analyze exchange interactions within the Heisenberg Hamiltonian framework, with mean-field approximation for Curie temperature.

Result: Low Co concentrations favor AFM order, while high Co content leads to FM state with TC below ~160K. MC simulations predict spin-glass behavior for alloys with >40 at.% Co on Cr sublattice.

Conclusion: The study provides fundamental understanding of how Co alloying affects magnetic properties in B2 Al-Cr-Co alloys through systematic changes in exchange interactions.

Abstract: Using Density Functional Theory (DFT) calculations and Monte-Carlo (MC)
simulations, we investigate the recently reported magnetic transition in B2
Al-Cr-Co alloys. The Cr sublattice is alloyed with different amounts of Co in
the antiferromagnetic (AFM) B2 AlCr binary alloy and the resulting exchange
interactions are analyzed within the Heisenberg Hamiltonian framework. DFT
results reveal that at low Co concentrations the system favors the AFM order,
while at high Co contents a transition to the ferromagnetic (FM) state is
observed. Within the FM stability field, the Curie temperature (TC), obtained
within the mean-field approximation, is below ~160 K and decreases with Co
concentration. The calculated exchange parameters evolve systematically with Co
content, and the trends are consistent with the DFT total energies. The
magnetic configurations obtained from MC simulations follow the DFT results at
low Cr levels but predict a spin-glass behavior for alloys containing more than
40 at.% Co on Cr sublattice. These findings provide a fundamental understanding
of how the chemistry-driven changes in exchange interactions affect magnetism
in the B2 Al-Cr-Co alloys.

</details>


### [93] [First-principles study of phase stability and magnetic properties of B2 AlCr, AlMn, AlFe, AlCo and AlNi aluminides](https://arxiv.org/abs/2510.22013)
*Haireguli Aihemaiti,Esmat Dastanpour,Anders Bergman,Levente Vitos*

Main category: cond-mat.mtrl-sci

TL;DR: DFT study of Al-3d transition metal alloys shows B2 phase stability, with AlCo being most stable. AlCo and AlNi are non-magnetic, AlFe weakly magnetic, and AlCr antiferromagnetic. Co doping induces ferromagnetism in AlCr.


<details>
  <summary>Details</summary>
Motivation: To understand electronic structure, phase stability, and magnetic properties of equiatomic binary alloys between Al and 3d magnetic transition elements (Cr, Mn, Fe, Co, Ni).

Method: Ab initio Density Functional Theory (DFT) calculations and magnetic simulations based on Heisenberg Hamiltonian.

Result: All five binary aluminides are more stable in ordered B2 phase than disordered bcc. AlCo is strongest B2 former. AlCo/AlNi non-magnetic, AlFe weakly magnetic, AlCr antiferromagnetic. Co doping in AlCr causes antiferromagnetic to ferromagnetic transition.

Conclusion: Findings provide deeper understanding of phase stability and magnetic properties in AlX binary alloys, explaining B2 structure formation mechanisms with 3d magnetic transition metals.

Abstract: Using ab initio Density Functional Theory (DFT) calculations, we investigate
the electronic structure, phase stability, and magnetic properties of
equiatomic binary alloys between Al and 3d magnetic transition elements (Cr,
Mn, Fe, Co, and Ni). Thermodynamically, all five binary aluminides are more
stable in the ordered B2 phase than in the disordered body centered cubic
phase, and Co is found to be the strongest B2 forming element with Al. The AlCo
and AlNi compounds with B2 structure are verified to be non-magnetic, whereas
AlFe turns out to be weakly magnetic, which is consistent with other DFT
calculations employing similar exchange-correlation approximations. Magnetic
simulations based on the Heisenberg Hamiltonian predict an antiferromagnetic
ground state for the hypothetical B2 AlCr, which is also confirmed by direct
DFT calculations. Doping AlCr with Co leads to an antiferromagnetic to
ferromagnetic transition, where ferromagnetism is to a large extent attributed
to Cr atoms. The phase stability and magnetic trends are explained using
electronic structure arguments. The present findings contribute to a deeper
understanding of the phase stability and magnetic properties of AlX binary
alloys, providing insights into the formation mechanisms of the B2 structure
with 3d magnetic transition metals.

</details>


### [94] [Machine-Learning-Guided Insights into Solid-Electrolyte Interphase Conductivity: Are Amorphous Lithium Fluorophosphates the Key?](https://arxiv.org/abs/2510.22912)
*Peichen Zhong,Kristin A. Persson*

Main category: cond-mat.mtrl-sci

TL;DR: The study identifies amorphous lithium difluorophosphate (LiPO2F2) as a promising Li+ conductor in battery SEI layers, showing enhanced conductivity over crystalline forms due to structural disorder and defect formation.


<details>
  <summary>Details</summary>
Motivation: To resolve the identity of the dominant Li+-conducting phase in battery SEI layers, as traditional crystalline phases like LiF/Li2O/Li2CO3 offer limited conductivity, and interfaces/amorphous phases may host the primary fast-ion pathways.

Method: Combined diffusion-based generative structure prediction with machine-learning interatomic potentials (MLIPs) to study lithium difluorophosphate (LiPO2F2), using MLIP-accelerated molecular dynamics to analyze conductivity.

Result: Amorphous LiPO2F2 shows higher conductivity than crystalline form with projected room-temperature σ ≈ 0.18 mS cm⁻¹ and Ea ≈ 0.40 eV, attributed to structural disorder flattening Li site-energy landscape and low formation energy for Li-interstitial defects.

Conclusion: Mixed-anion, amorphous Li-P-O-F phases are promising candidates for the Li+-conducting medium in battery SEI layers, offering a path for engineering improved battery interfaces.

Abstract: Despite decades of study, the identity of the dominant \ce{Li+}-conducting
phase within the inorganic SEI of Li-ion batteries remains unresolved. While
the mosaic model describes LiF/\ce{Li2O}/\ce{Li2CO3} nanocrystallites within a
disordered matrix, these crystalline phases inherently offer limited ionic
conductivity. Growing evidence suggests that interfaces, grain boundaries, and
amorphous phases may instead host the primary fast-ion pathways. Motivated by
mixed-anion electrolyte decomposition products, we combine diffusion-based
generative structure prediction with machine-learning interatomic potentials
(MLIPs) to interrogate lithium difluorophosphate (\ce{LiPO2F2}), a key
decomposition product of phosphorus- and fluorine-containing electrolytes. We
identify a stable crystalline polymorph and, through MLIP-accelerated molecular
dynamics, show that the amorphous counterpart is more conductive, with
projected room-temperature $\sigma$ $\approx$ 0.18 mS cm$^{-1}$ and
$E_\mathrm{a}$ $\approx$ 0.40 eV. This enhancement is attributed to structural
disorder, which flattens the Li site-energy landscape, and to a low formation
energy for Li-interstitial defects, which supplies additional mobile carriers.
We present mixed-anion, amorphous Li--P--O--F phases as promising candidates
for the \ce{Li+}-conducting medium of the inorganic SEI, offering a path
forward for engineering improved battery interfaces.

</details>


### [95] [LightPFP: A Lightweight Route to Ab Initio Accuracy at Scale](https://arxiv.org/abs/2510.23064)
*Wenwen Li,Nontawat Charoenphakdee,Yong-Bin Zhuang,Ryuhei Okuno,Yuta Tsuboi,So Takamoto,Junichi Ishida,Ju Li*

Main category: cond-mat.mtrl-sci

TL;DR: LightPFP is a knowledge distillation framework that generates task-specific MLIPs using universal MLIPs instead of expensive DFT calculations, achieving 1000x faster model development while maintaining accuracy comparable to first-principles methods.


<details>
  <summary>Details</summary>
Motivation: Universal MLIPs have computational overhead limiting large-scale applications, while task-specific MLIPs require prohibitively expensive DFT data generation for each material system.

Method: LightPFP leverages universal MLIPs to generate high-quality training data for specific materials and uses a pre-trained light-weight MLIP to enhance data efficiency, avoiding costly DFT calculations.

Result: LightPFP delivers three orders of magnitude faster model development than conventional DFT-based methods across various materials, with distilled models achieving 1-2 orders of magnitude faster inference than u-MLIPs while maintaining accuracy on par with first-principles predictions.

Conclusion: The u-MLIP-driven distillation approach enables rapid development of high-fidelity, efficient MLIPs for materials science applications, with efficient precision transfer learning using minimal DFT data.

Abstract: Atomistic simulation methods have evolved through successive computational
levels, each building upon more fundamental approaches: from quantum mechanics
to density functional theory (DFT), and subsequently, to machine learning
interatomic potentials (MLIPs). While universal MLIPs (u-MLIPs) offer broad
transferability, their computational overhead limits large-scale applications.
Task-specific MLIPs (ts-MLIPs) achieve superior efficiency but require
prohibitively expensive DFT data generation for each material system. In this
paper, we propose LightPFP, a data-efficient knowledge distillation framework.
Instead of using costly DFT calculations, LightPFP generates a distilled
ts-MLIP by leveraging u-MLIP to generate high-quality training data tailored
for specific materials and utilizing a pre-trained light-weight MLIP to further
enhance data efficiency. Across a broad spectrum of materials, including
solid-state electrolytes, high-entropy alloys, and reactive ionic systems,
LightPFP delivers three orders of magnitude faster model development than
conventional DFT-based methods, while maintaining accuracy on par with
first-principles predictions. Moreover, the distilled ts-MLIPs further sustain
the computational efficiency essential for large-scale molecular dynamics,
achieving 1-2 orders of magnitude faster inference than u-MLIPs. The framework
further enables efficient precision transfer learning, where systematic errors
from the u-MLIP can be corrected using as few as 10 high-accuracy DFT data
points, as demonstrated for MgO melting point prediction. This u-MLIP-driven
distillation approach enables rapid development of high-fidelity, efficient
MLIPs for materials science applications.

</details>


### [96] [All-Altermagnetic Tunnel Junction of RuO2/NiF2/RuO2](https://arxiv.org/abs/2510.23269)
*Long Zhang,Guoying Gao*

Main category: cond-mat.mtrl-sci

TL;DR: Proposes an all-altermagnetic tunnel junction (AAMTJ) using RuO2/NiF2/RuO2 that achieves giant tunneling magnetoresistance (11704%) and high spin filtering (~90%) without ferromagnetic electrodes or stray fields.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of ferromagnetic electrodes (stray fields) and enable tunable magnetoresistance and spin filtering in magnetic tunnel junctions using emerging altermagnets.

Method: Developed an all-altermagnetic tunnel junction architecture composed exclusively of altermagnets (RuO2/NiF2/RuO2), leveraging synergistic and antagonistic alignments of momentum-dependent altermagnetic spin-splitting.

Result: Achieved giant tunneling magnetoresistance of 11704% and high spin filtering of ~90% in both spin channels, with tunable multistate magnetoresistance and spin filtering via magnetization control.

Conclusion: The AAMTJ paradigm combines advantages of both ferromagnetic and antiferromagnetic tunnel junctions, offering a versatile platform for reconfigurable magnetic memory devices with low consumption and no stray fields.

Abstract: Emerging altermagnets offer a promising avenue for spintronics, yet their
integration into magnetic tunnel junctions has been hindered by reliance on
ferromagnetic electrodes (introducing stray fields) or limited functionality
(non-tunable magnetoresistance without spin filtering). Here, we propose an
all-altermagnetic tunnel junction (AAMTJ) paradigm composed exclusively of
altermagnets-exemplified by experiment-feasible RuO2/NiF2/RuO2. Giant tunneling
magnetoresistance of 11704%, and high spin-filtering of ~90% in both spin
channels are achieved. This architecture unlocks tunable multistate
magnetoresistance and spin filtering via magnetization control of electrode and
barrier, stemming from their synergistic and antagonistic alignments of
momentum-dependent altermagnetic spin-splitting. Our AAMTJ inherently exhibits
low consumption and no stray field, with nonrelativistic spin splitting and
zero magnetic moment, combining advantages of both ferromagnetic and
antiferromagnetic tunnel junctions. This AAMTJ paradigm provides a
realistically versatile platform to develop revolutionarily potential of
altermagnets for reconfigurable magnetic memory devices.

</details>


### [97] [Elastic modeling and total energy calculations of the structural characteristics of "free-standing",periodic, pseudomorphic GaN/AlN superlattices](https://arxiv.org/abs/2510.23344)
*Th. Karakostas,Ph. Komninou,V. Pontikis*

Main category: cond-mat.mtrl-sci

TL;DR: Elastic modeling accurately predicts strain states in GaN/AlN superlattices, matching total energy calculations. Interface excess energies can be evaluated through elastic energy analysis.


<details>
  <summary>Details</summary>
Motivation: To validate the accuracy of linear elasticity models for predicting strain states in pseudomorphic superlattices by comparing with total energy calculations.

Method: Applied linear elasticity modeling to GaN/AlN free-standing superlattices and compared predictions with total energy calculations for systems with varying component thicknesses.

Result: Elastic predictions for lattice constants align with total energy calculation values within computational error limits. Elastic energy analysis enables evaluation of interface excess energies.

Conclusion: Linear elasticity provides accurate modeling of strain states in pseudomorphic superlattices, with results consistent with total energy calculations and previous literature findings.

Abstract: The strain states of the components of pseudomorphic superlattices can be
accurately modeled analytically through the application of linear elasticity.
In this particular case of GaN/AlN 'free-standing' superlattices, the
predictions derived from elastic modeling have been compared with total energy
calculations of several systems made of components with varying thicknesses. We
demonstrate that the elastic predictions for the lattice constants of the
components align with the values obtained from their total energy counterparts,
within the limits of computational errors and uncertainties. Furthermore, a
phenomenological analysis of the elastic energy stored in the superlattices
facilitates the evaluation of the excess energies associated with the
interfaces present in these systems. The results mentioned above are briefly
contrasted with findings reported in previous literature.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [98] [Towards Explainable Inverse Design for Photonics via Integrated Gradients](https://arxiv.org/abs/2510.22176)
*Junho Park,Taehan Kim,Sangdae Nam*

Main category: physics.optics

TL;DR: A pipeline combining adjoint-based inverse design with data-driven interpretability: generates wavelength demultiplexers, trains CNN surrogate to predict performance, and uses Integrated Gradients to identify key layout features responsible for optical behavior.


<details>
  <summary>Details</summary>
Motivation: Adjoint-based inverse design produces compact nanophotonic devices but the relationship between pixel-level layouts and optical performance remains difficult to interpret, limiting design intuition.

Method: Four-step pipeline: (1) generate wavelength demultiplexers using SPINS-B, (2) record 2D layouts and spectral metrics, (3) train lightweight CNN surrogate to predict metrics from layouts, (4) apply Integrated Gradients for gradient-based attribution to highlight performance-critical regions.

Result: Integrated Gradients consistently localizes to physically meaningful features like tapers and splitter hubs, providing design intuition that complements adjoint optimization. The method works on a corpus of sampled wavelength demultiplexers.

Conclusion: An end-to-end, data-driven workflow that transforms inverse-designed layouts into interpretable attributions without modifying physics solvers or objectives, reusable for other photonic components.

Abstract: Adjoint-based inverse design yields compact, high-performance nanophotonic
devices, but the mapping from pixel-level layouts to optical figures of merit
remains hard to interpret. We present a simple pipeline that (i) generates a
large set of wavelength demultiplexers (WDMs) with SPINS-B, (ii) records each
final 2D layout and its spectral metrics (e.g., transmitted power at 1310 nm
and 1550 nm), and (iii) trains a lightweight convolutional surrogate to predict
these metrics from layouts, enabling (iv) gradient-based attribution via
Integrated Gradients (IG) to highlight specific regions most responsible for
performance. On a corpus of sampled WDMs, IG saliency consistently localizes to
physically meaningful features (e.g., tapers and splitter hubs), offering
design intuition that complements adjoint optimization. Our contribution is an
end-to-end, data-driven workflow--SPINS-B dataset, CNN surrogate, and IG
analysis--that turns inverse-designed layouts into interpretable attributions
without modifying the physics solver or objective, and that can be reused for
other photonic components.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [99] [Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms](https://arxiv.org/abs/2510.23166)
*Philippe Martin Wyder,Judah Goldfeder,Alexey Yermakov,Yue Zhao,Stefano Riva,Jan P. Williams,David Zoro,Amy Sara Rude,Matteo Tomasetto,Joe Germany,Joseph Bakarji,Georg Maierhofer,Miles Cranmer,J. Nathan Kutz*

Main category: cs.CE

TL;DR: Proposes a Common Task Framework (CTF) for scientific machine learning to address reproducibility issues and inconsistent evaluations through standardized benchmarks with curated datasets and metrics.


<details>
  <summary>Details</summary>
Motivation: Rapid ML development has outpaced standardized benchmarks, leading to weak baselines, reporting bias, and inconsistent evaluations that undermine reproducibility and obscure scientific progress.

Method: Develops a CTF with curated datasets and task-specific metrics spanning forecasting, state reconstruction, and generalization under realistic constraints (noise, limited data). Benchmarks methods on Kuramoto-Sivashinsky and Lorenz systems.

Result: The CTF successfully reveals method strengths, limitations, and suitability for specific problem classes. A competition is launched using a real-world sea surface temperature dataset with hidden test sets.

Conclusion: The framework provides a structured foundation for rigorous algorithm evaluation and aims to replace ad hoc comparisons with standardized evaluations to improve reproducibility in scientific ML.

Abstract: Machine learning (ML) is transforming modeling and control in the physical,
engineering, and biological sciences. However, rapid development has outpaced
the creation of standardized, objective benchmarks - leading to weak baselines,
reporting bias, and inconsistent evaluations across methods. This undermines
reproducibility, misguides resource allocation, and obscures scientific
progress. To address this, we propose a Common Task Framework (CTF) for
scientific machine learning. The CTF features a curated set of datasets and
task-specific metrics spanning forecasting, state reconstruction, and
generalization under realistic constraints, including noise and limited data.
Inspired by the success of CTFs in fields like natural language processing and
computer vision, our framework provides a structured, rigorous foundation for
head-to-head evaluation of diverse algorithms. As a first step, we benchmark
methods on two canonical nonlinear systems: Kuramoto-Sivashinsky and Lorenz.
These results illustrate the utility of the CTF in revealing method strengths,
limitations, and suitability for specific classes of problems and diverse
objectives. Next, we are launching a competition around a global real world sea
surface temperature dataset with a true holdout dataset to foster community
engagement. Our long-term vision is to replace ad hoc comparisons with
standardized evaluations on hidden test sets that raise the bar for rigor and
reproducibility in scientific ML.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [100] [Revealing Liquid-Gas Transitions with Finite-Size Scaling in Confined Systems](https://arxiv.org/abs/2510.22203)
*Chong Zha,Yanshuang Chen,Cheng-Ran Du,Peng Tan,Yuliang Jin*

Main category: cond-mat.stat-mech

TL;DR: The paper presents a finite-size scaling method for density profiles to distinguish liquid-gas phase transitions from single-phase systems in field-confined environments.


<details>
  <summary>Details</summary>
Motivation: External fields make traditional empirical criteria for identifying liquid-gas phase transitions ambiguous, requiring a more definitive method.

Method: Developed a finite-size scaling approach that collapses density profiles onto a master curve for single-phase systems, while causing profile intersections at interfaces for two-phase systems.

Result: Validated the method through experiments and simulations on colloidal suspensions under gravity and 2D complex plasmas confined by central potentials, showing clear distinction between single-phase and two-phase systems.

Conclusion: The finite-size scaling method provides a broadly applicable criterion for detecting liquid-gas phase transitions in laboratory systems with inherent external fields.

Abstract: The application of an external field often renders empirical criteria for
identifying liquid-gas phase transitions ambiguous. Here, we demonstrate that
the finite-size scaling of the density profile provides a definitive criterion
to distinguish liquid-gas coexistence from a single fluid phase in
field-confined systems. Our scaling method collapses the density profiles of
different system sizes onto a single master curve for a one-phase system, while
causing the profiles to intersect at the interface in a two-phase system. We
validate this theoretical proposal through experiments and simulations of two
model systems: colloidal suspensions under gravity and/or two-dimensional
complex plasmas confined by a central potential. Our method is broadly
applicable for detecting liquid-gas phase transitions in laboratory systems
where external fields are inherent.

</details>


### [101] [Dynamic Phase Transitions in Mean-Field Ginzburg-Landau Models: Conjugate Fields and Fourier-Mode Scaling](https://arxiv.org/abs/2510.21803)
*Yelyzaveta Satynska,Daniel T. Robb*

Main category: cond-mat.stat-mech

TL;DR: The paper shows that for mean-field Ginzburg-Landau dynamics at critical period, the correct conjugate field is the even-Fourier component part of the applied field, and the order parameter is z_k = √|m_k² - |m_{k,c}|²|. The study reveals universal 1/2 and 1/3 scaling exponents for Fourier modes.


<details>
  <summary>Details</summary>
Motivation: To understand dynamic phase transitions in periodically forced mean-field ferromagnets and identify the correct conjugate field and order parameter at critical period.

Method: Used high-accuracy limit-cycle integration and Fourier analysis of mean-field Ginzburg-Landau dynamics with energy F(m)=am²+bm⁴-hm, testing Fourier modes up to k≤30.

Result: Found z_k ∝ ε^{1/2} for symmetry-broken branch below P_c, z_k ∝ h_mult^{1/3} at P_c with even perturbations, and mode-resolved deviations follow parity rule: |δm_{2n}| ∝ h_mult^{1/3} and |δm_{2n+1}| ∝ h_mult^{2/3}.

Conclusion: The correct conjugate field is the even-Fourier component part, and universal scaling exponents (1/2 and 1/3) hold across Fourier modes, with findings persisting in modified MFGL models.

Abstract: Dynamic phase transitions of periodically forced mean-field ferromagnets are
often described by a single order parameter and a scalar conjugate field.
Building from previous work, we show that, at the critical period $P_c$ of the
mean-field Ginzburg-Landau (MFGL) dynamics with energy $F(m)=am^2+bm^4-hm$, the
correct conjugate field is the entire even-Fourier component part of the
applied field. The correct order parameter is
$z_k=\sqrt{\bigl|\,m_k^2-|m_{k,c}|^2\,\bigr|}$, where $m_k$ is the $k^{th}$
Fourier component of the magnetization m(t), and $m_{k,c}$ is the $k^{th}$
Fourier component at the critical period. Using high-accuracy limit-cycle
integration and Fourier analysis, we first confirm that, for periodic fields
that contain only odd components, the symmetry-broken branch below $P_c$
exhibits $z_k \propto \varepsilon^{1/2}$ (computationally tested for modes
$k\le30$), where $\varepsilon=(P_c-P)/P_c$. This provides strong evidence that
the 1/2 scaling holds for all Fourier modes. We then find three robust facts:
(1) Exactly at $P_c$, adding a small perturbation composed of even Fourier
components with an overall field multiplier $h_{mult}$ yields $z_k \propto
h_{mult}^{1/3}$ across many $k$. (2) Mode-resolved deviations obey a parity
rule: $|\delta m_{2n}| \propto h_{mult}^{1/3}$ and $|\delta m_{2n+1}| \propto
h_{mult}^{2/3}$. (3) The same findings persist in MFGL models where an $m^6$
replaces the $m^4$ term and come with simple one-period integral criteria to
locate $P_c$.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [102] [Control of neural field equations with step-function inputs](https://arxiv.org/abs/2510.22022)
*Cyprien Tamekue,ShiNung Ching*

Main category: math.OC

TL;DR: This paper develops control synthesis methods for Amari-type neural field models to steer neural activity from initial to target states using piecewise constant inputs, with applications to understanding paradoxical neural representations.


<details>
  <summary>Details</summary>
Motivation: To understand how sensory inputs shape neural activity and predict paradoxical neural representations like visual illusions, by studying controllability properties of Amari-type neural fields.

Method: Two control synthesis approaches: 1) Based on Banach fixed-point theorem for constant-in-time inputs, 2) Generic framework using neural dynamics drift flow for explicit piecewise constant inputs. Both methods tested numerically in 1D and 2D spatial dimensions.

Result: The proposed syntheses effectively achieve control objectives and outperform naive linearization approaches when initial/target states are not equilibria. Numerical results confirm superior performance of the methods.

Conclusion: The work provides a mathematically rigorous framework for controlling nonlinear neural population dynamics, advancing understanding of neural field control with applications in computational neuroscience, psychophysics, and neurostimulation.

Abstract: Wilson-Cowan and Amari-type models capture nonlinear neural population
dynamics, providing a fundamental framework for modeling how sensory and other
exogenous inputs shape activity in neural tissue. We study the controllability
properties of Amari-type neural fields subject to piecewise/constant-in-time
inputs. The model describes the time evolution of the polarization of neural
tissue within a spatial continuum, with synaptic interactions represented by a
convolution kernel. We study the synthesis of piecewise/constant-in-time inputs
to achieve two-point boundary-type control objectives, namely, steering neural
activity from an initial state to a prescribed target state. This approach is
particularly relevant for predicting the emergence of paradoxical neural
representations, such as discordant visual illusions that occur in response to
overt sensory stimuli. We first present a control synthesis based on the Banach
fixed-point theorem, which yields an iterative construction of a
constant-in-time input under minimal regularity assumptions on the kernel and
transfer function; however, it exhibits practical limitations, even in the
linear case. To overcome these challenges, we then develop a generic synthesis
framework based on the flow of neural dynamics drift, enabling explicit
piecewise constant and constant-in-time inputs. Extensive numerical results in
one and two spatial dimensions confirm the effectiveness of the proposed
syntheses and demonstrate their superior performance compared to inputs derived
from naive linearization at the initial or target states when these states are
not equilibria of the drift dynamics. By providing a mathematically rigorous
framework for controlling Amari-type neural fields, this work advances our
understanding of nonlinear neural population control with potential
applications in computational neuroscience, psychophysics, and
neurostimulation.

</details>


### [103] [Feedback approximate controllability of blowup points for the heat equation with anti-interference blowup profile](https://arxiv.org/abs/2510.22770)
*Ping Lin,Hatem Zaag*

Main category: math.OC

TL;DR: Feedback approximate controllability of blowup points for heat equation with bounded feedback operators and stable blowup profiles.


<details>
  <summary>Details</summary>
Motivation: To study feedback controllability of blowup points in heat equations, ensuring system stability against perturbations in initial data.

Method: Developed feedback control strategy with bounded operators before blowup, analyzing blowup profile stability under small perturbations.

Result: System is approximately controllable for blowup points with bounded feedback controls, and blowup profiles remain stable (undergoing only tiny translations) under initial data perturbations.

Conclusion: The feedback control strategy is effective and anti-interference, maintaining controllability and stability of blowup points despite small disturbances.

Abstract: This paper is concerned with a feedback approximate controllability problem
of blowup points for the heat equation. We show that the system is
approximately controllable for blowup points with feedback controls and the
feedback operator is bounded at any time before blowup. It is also proved that
the blowup profile for feedback controllability of blowup points is stable with
respect to initial data. That is, suppose that the initial data has a very
small perturbation, the blowup profiles also have tiny changes. More precisely,
it just undergoes a tiny translation in space and time. This means that our
feedback strategy is anti-interference.

</details>


### [104] [Derivative-Free Sequential Quadratic Programming for Equality-Constrained Stochastic Optimization](https://arxiv.org/abs/2510.22458)
*Sen Na*

Main category: math.OC

TL;DR: Proposes a Derivative-Free Stochastic Sequential Quadratic Programming (DF-SSQP) method for nonlinear optimization with stochastic objective and deterministic constraints, using SPSA for gradient/Hessian estimation and momentum-based debiasing for noise reduction.


<details>
  <summary>Details</summary>
Motivation: Addresses optimization problems where only zero-order information is available for both stochastic objective and deterministic constraints, with random sampling noise, and where derivative information is unavailable.

Method: Uses simultaneous perturbation stochastic approximation (SPSA) to estimate gradients/Hessians with dimension-independent evaluations, plus momentum-style estimators for online debiasing to reduce stochastic noise via moving averaging.

Result: Establishes global almost-sure convergence and local convergence with asymptotic normality, achieving covariance similar to derivative-based methods but larger due to lack of derivatives. Enables online statistical inference.

Conclusion: DF-SSQP effectively solves derivative-free stochastic constrained optimization with theoretical guarantees and practical performance, bridging the gap between derivative-free and derivative-based methods.

Abstract: We consider solving nonlinear optimization problems with a stochastic
objective and deterministic equality constraints, assuming that only zero-order
information is available for both the objective and constraints, and that the
objective is also subject to random sampling noise. Under this setting, we
propose a Derivative-Free Stochastic Sequential Quadratic Programming (DF-SSQP)
method. Due to the lack of derivative information, we adopt a simultaneous
perturbation stochastic approximation (SPSA) technique to randomly estimate the
gradients and Hessians of both the objective and constraints. This approach
requires only a dimension-independent number of zero-order evaluations -- as
few as eight -- at each iteration step. A key distinction between our
derivative-free and existing derivative-based SSQP methods lies in the
intricate random bias introduced into the gradient and Hessian estimates of the
objective and constraints, brought by stochastic zero-order approximations. To
address this issue, we introduce an online debiasing technique based on
momentum-style estimators that properly aggregate past gradient and Hessian
estimates to reduce stochastic noise, while avoiding excessive memory costs via
a moving averaging scheme. Under standard assumptions, we establish the global
almost-sure convergence of the proposed DF-SSQP method. Notably, we further
complement the global analysis with local convergence guarantees by
demonstrating that the rescaled iterates exhibit asymptotic normality, with a
limiting covariance matrix resembling the minimax optimal covariance achieved
by derivative-based methods, albeit larger due to the absence of derivative
information. Our local analysis enables online statistical inference of model
parameters leveraging DF-SSQP. Numerical experiments on benchmark nonlinear
problems demonstrate both the global and local behavior of DF-SSQP.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [105] [Automated Exploration of Radical-Molecule Chemistry: The Case of Oxirane + CH in the ISM](https://arxiv.org/abs/2510.23242)
*Moritz Bensberg,Silvia Alessandrini,Mattia Melosso,Cristina Puzzarini,Markus Reiher*

Main category: astro-ph.GA

TL;DR: Automated workflow explores oxirane + methylidyne reaction potential energy surface, identifying main products and providing kinetic insights for astrochemistry.


<details>
  <summary>Details</summary>
Motivation: To comprehensively map reactive potential energy surfaces for astrochemical reactions, overcoming the challenge of exhaustive exploration of complex reaction networks.

Method: Employed automated workflow for exploring reactive potential energy surfaces, mapping the extensive network with 60 exothermic bimolecular products.

Result: Main product is HCO radical plus ethene; s-trans-propenal (acrolein) and 2H-oxene also form to lesser extent. Higher abundance of s-trans-propenal vs methyl ketene in interstellar medium attributed to gas-phase kinetic effects.

Conclusion: Automated PES exploration successfully mapped complex reaction network, providing insights into product formation kinetics relevant to astrochemical modeling.

Abstract: Quantum chemistry provides accurate and reliable methods to investigate
reaction pathways of reactive molecular systems relevant to the interstellar
medium. However, the exhaustive exploration of a reactive network is often a
daunting task, resulting in unexplored reactive channels that affect kinetic
outcomes and branching ratios. Here, an automated workflow for exploring
reactive potential energy surfaces (PESs) is employed for the first time to
study the oxirane (C$_2$H$_4$O) plus methylidyne ($^.$CH) reaction. The
ultimate goal is to comprehensively map its PES and, subsequently, derive rate
constants for the most important reaction channels. In addition to its
astrochemical relevance, this reaction has been considered because it is a
challenging test case, its network being very extended, with 60 exothermic
bimolecular products lying below the reactant's energy. Kinetic simulations
indicate that the main product of the reaction is the HCO radical plus ethene
(C$_2$H$_4$), while formation of s-trans-propenal (acrolein) and 2H-oxene is
also possible, but to a lesser extent. Based on the present study and other
references in the literature, we suggest that the slightly higher relative
abundance of s-trans-propenal compared to methyl ketene in the interstellar
medium is a gas-phase kinetic effect, s-trans-propenal being a more easily
accessible product on the C$_3$H$_5$O$^.$ PES.

</details>


### [106] [The First Star-by-star $N$-body/Hydrodynamics Simulation of Our Galaxy Coupling with a Surrogate Model](https://arxiv.org/abs/2510.23330)
*Keiya Hirashima,Michiko S. Fujii,Takayuki R. Saitoh,Naoto Harada,Kentaro Nomura,Kohji Yoshikawa,Yutaka Hirai,Tetsuro Asano,Kana Moriwaki,Masaki Iwasawa,Takashi Okamoto,Junichiro Makino*

Main category: astro-ph.GA

TL;DR: The paper presents a novel integration scheme combining N-body/hydrodynamics simulations with machine learning to overcome scalability limitations in Milky Way galaxy simulations, achieving 300 billion particles and enabling the first star-by-star galaxy simulation.


<details>
  <summary>Details</summary>
Motivation: To simulate the Milky Way Galaxy with resolution down to individual stars, overcoming the scaling failures caused by small-scale phenomena like supernova explosions that require prohibitively short timesteps.

Method: Developed a novel integration scheme for N-body/hydrodynamics simulations that uses machine learning and a surrogate model to bypass the short timesteps caused by supernova explosions, improving scalability.

Result: Achieved 300 billion particles using 148,900 nodes (7,147,200 CPU cores), breaking through the billion-particle barrier. This enabled the first star-by-star galaxy simulation resolving individual stars in the Milky Way. Performance scales over 10^4 CPU cores across A64FX, X86-64 processors and NVIDIA CUDA GPUs.

Conclusion: The machine learning-enhanced simulation approach successfully overcomes previous scalability limitations, enabling unprecedented resolution for galaxy simulations and opening new possibilities for computational astrophysics.

Abstract: A major goal of computational astrophysics is to simulate the Milky Way
Galaxy with sufficient resolution down to individual stars. However, the
scaling fails due to some small-scale, short-timescale phenomena, such as
supernova explosions. We have developed a novel integration scheme of
$N$-body/hydrodynamics simulations working with machine learning. This approach
bypasses the short timesteps caused by supernova explosions using a surrogate
model, thereby improving scalability. With this method, we reached 300 billion
particles using 148,900 nodes, equivalent to 7,147,200 CPU cores, breaking
through the billion-particle barrier currently faced by state-of-the-art
simulations. This resolution allows us to perform the first star-by-star galaxy
simulation, which resolves individual stars in the Milky Way Galaxy. The
performance scales over $10^4$ CPU cores, an upper limit in the current
state-of-the-art simulations using both A64FX and X86-64 processors and NVIDIA
CUDA GPUs.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [107] [Sharp angle estimates for second order divergence operators](https://arxiv.org/abs/2510.23450)
*Hannes Meinlschmidt,Joachim Rehberg*

Main category: math.FA

TL;DR: Analysis of sectoriality angles and H∞-angles for elliptic operators in various Banach spaces, focusing on minimal geometric assumptions and explicit estimates.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive understanding of sectoriality properties for elliptic operators with mixed boundary conditions, combining existing results with new explicit estimates.

Method: Uses operator theory in Lebesgue spaces, negative Sobolev spaces, and interpolation scales, relying on Kato square root property and developing new estimates for sectoriality angles.

Result: New explicit estimates for sectoriality angles, including results for operators with complex coefficients without geometric assumptions, and a transfer principle for Crouzeix-Delyon theorem.

Conclusion: Provides a unified framework for sectoriality analysis of elliptic operators with explicit, accessible estimates that work under minimal assumptions.

Abstract: This article is about the (minimal) sector containing the numerical range of
the principal part of a linear second-order elliptic differential operator
defined by a form on closed subspaces V of the first-order Sobolev space
$W^{1,2}(\Omega)$ incorporating mixed boundary conditions. We collect a
comprehensive array of results on the angle of sectoriality and the
$H^\infty$-angle attached to realizations of the elliptic operator. We thereby
consider the operator in several scales of Banach spaces: the Lebesgue space,
the negative Sobolev space, and their interpolation scale. For the latter two
types of spaces, we rely on recent results regarding the Kato square root
property. We focus on minimal assumptions on geometry, and we consider both
real and complex coefficients. Not all results presented are new, but we strive
for a streamlined and comprehensive overall picture from several branches of
operator theory, and we complement the existing results with several new ones,
in particular aiming at explicit estimates built on readily accessible problem
data. This concerns for example a new estimate on the angle of the sector
containing the numerical range of a linear, continuous and coercive Hilbert
space operator, but also an explicit estimate for the angle of sectoriality for
the elliptic operator on $L^p(\Omega)$ with complex coefficients without any
assumptions on geometry and a general transfer principle for the
Crouzeix-Delyon theorem from bounded operators to sectorial ones, keeping the
explicit constant.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [108] [Electric Field-Induced Kerr Rotation on Metallic Surfaces](https://arxiv.org/abs/2510.22486)
*Farzad Mahfouzi,Mark D. Stiles,Paul M. Haney*

Main category: cond-mat.mes-hall

TL;DR: The paper identifies two contributions to electric field-induced Kerr rotation in metallic thin films: orbital Edelstein effect and surface Pockels effect, with comparable contributions in Pt films showing different polarization dependencies.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying mechanisms of electric field-induced Kerr rotation in metallic thin films, particularly identifying previously overlooked contributions beyond the known orbital Edelstein effect.

Method: Combination of density functional theory calculations and optical modeling to analyze Kerr rotation contributions from orbital moment accumulation (orbital Edelstein effect) and surface Pockels effect.

Result: Both orbital Edelstein and surface Pockels effects contribute comparably to Kerr rotation in Pt thin films. The orbital Edelstein effect yields similar Kerr rotation for s and p polarized light, while the surface Pockels effect produces opposing values for different polarizations.

Conclusion: Electric field-induced Kerr rotation in metallic thin films involves two distinct mechanisms with different origins and polarization dependencies, both arising from dual mirror symmetry breaking at the surface and from the applied dc field.

Abstract: We use a combination of density functional theory calculations and optical
modeling to establish that the electric field-induced Kerr rotation in metallic
thin films has contributions from both non-equilibrium orbital moment
accumulation (arising from the orbital Edelstein effect) and a heretofore
overlooked surface Pockels effect. The Kerr rotation associated with orbital
accumulation has been studied in previous works and is largely due to the dc
electric field-induced change of the electron distribution function. In
contrast, the surface Pockels effect is largely due to the dc field-induced
change to the wave functions. Both of these contributions arise from the dual
mirror symmetry breaking from the surface and from the dc applied field. Our
calculations show that the resulting Kerr rotation is due to the dc electric
field modification of the optical conductivity within a couple of nanometers
from the surface, consistent with the dependence on the local mirror symmetry
breaking at the surface. For thin films of Pt, our calculations show that the
relative contributions of the orbital Edelstein and surface Pockels effects are
comparable, and that they have different effects on Kerr rotation of $s$ and
$p$ polarized light, $\theta_K^s$ and $\theta_K^p$. The orbital Edelstein
effect yields similar values of $\theta_K^s$ and $\theta_K^p$, while the
surface Pockels effect leads to opposing values of $\theta_K^s$ and
$\theta_K^p$.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [109] [A Physics-Informed Variational Inference Framework for Identifying Attributions of Extreme Stress Events in Low-Grain Polycrystals](https://arxiv.org/abs/2510.23437)
*Yinling Zhang,Samuel D. Dunham,Curt A. Bronkhorst,Nan Chen*

Main category: stat.AP

TL;DR: A new variational inference framework that integrates physics-informed statistical models with extreme value statistics to identify microstructural features triggering rare material failure events.


<details>
  <summary>Details</summary>
Motivation: Polycrystalline metal failure begins with stress concentration at grain boundaries, but identifying which microstructural features trigger these rare events is challenging due to computational expense and focus on average behavior rather than extremes.

Method: Three key components: 1) Reformulate objective with extreme-value theory to emphasize tail behavior, 2) Constrain inference via physics-informed statistical model for microstructure-stress relationships, 3) Use mixture models in reduced latent space to capture non-Gaussian characteristics of microstructural features.

Result: The framework achieves reliable extreme-event prediction in both controlled and realistic experimental tests for bicrystal configuration, revealing microstructural features associated with material failure.

Conclusion: Provides physical insights for material design with uncertainty quantification by identifying multiple underlying failure mechanisms through the integrated variational inference approach.

Abstract: Polycrystalline metal failure often begins with stress concentration at grain
boundaries. Identifying which microstructural features trigger these events is
important but challenging because these extreme damage events are rare and the
failure mechanisms involve multiple complex processes across scales. Most
existing inference methods focus on average behavior rather than rare events,
whereas standard sample-based methods are computationally expensive for
high-dimensional complex systems. In this paper, we develop a new variational
inference framework that integrates a recently developed computationally
efficient physics-informed statistical model with extreme value statistics to
significantly facilitate the identification of material failure attributions.
First, we reformulate the objective to emphasize observed exceedances by
incorporating extreme-value theory into the likelihood, thereby highlighting
tail behavior. Second, we constrain inference via a physics-informed
statistical model that characterizes microstructure-stress relationships, which
uniquely provides physically consistent predictions for these rare events.
Third, mixture models in a reduced latent space are developed to capture the
non-Gaussian characteristics of microstructural features, allowing the
identification of multiple underlying mechanisms. In both controlled and
realistic experimental tests for the bicrystal configuration, the framework
achieves reliable extreme-event prediction and reveals the microstructural
features associated with material failure, providing physical insights for
material design with uncertainty quantification.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [110] [High-Performance Rotor Cooling with Ducted Liquid in Completely Cold-Formed Modular Motor Shaft](https://arxiv.org/abs/2510.22029)
*Rezvan Alamian,Sören Müller,Uwe Steinmetz,Christian Henrich,Stefan Goetz*

Main category: eess.SY

TL;DR: A novel tooth-guided liquid-cooling shaft design for electric motors that improves cooling efficiency by 110% while maintaining comparable pressure levels through optimized internal channels that reduce vortex formation.


<details>
  <summary>Details</summary>
Motivation: To address the high churning loss and limited heat transfer of conventional cooled rotor shafts in electric motors, particularly for automotive applications where effective cooling is crucial for performance and durability.

Method: Investigated four shaft geometries using computational fluid analysis, focusing on a tooth-guided design with cold-formed internal channels that restrict vortex formation. Evaluated performance metrics including heat transfer rate, outlet temperature, pressure drop, and velocity profiles under varying rotational speeds, inlet flow rates, and coolant temperatures.

Result: The tooth-guided design achieved up to 110% higher cooling efficiency at low rotational speeds compared to conventional hollow shafts, while maintaining comparable pressure levels. It demonstrated superior thermal performance across all tested conditions.

Conclusion: The tooth-guided liquid-cooling shaft provides a practical geometry-driven thermal optimization solution that can significantly improve electric motor performance and durability through enhanced cooling efficiency.

Abstract: This paper suggests a novel rotor-cooling shaft concept for high-performance
electric motors that increases the effectiveness of cooling and is yet simple
and cost-effective to manufacture. We investigate the thermal performance of
four shaft geometries for rotor cooling in automotive applications. The
proposed tooth-guided liquid-cooling shaft design aims to solve the high
churning loss of conventional cooled rotor shafts due to internal vortex
formation and their still limited heat transfer. Therefore, we optimize heat
transfer efficiency and pressure management by incorporating cold-formed
internal channels that restrict vortex formation beyond a degree that improves
heat transfer. We evaluated key performance metrics, including heat transfer
rate, outlet temperature, pressure drop, and velocity profiles, under varying
rotational speeds, inlet flow rates, and coolant temperatures. Computational
fluid analysis demonstrates that the tooth-guided design outperforms
conventional hollow shafts and achieves up to 110% higher cooling efficiency at
low rotational speeds, while it maintains comparable pressure levels. These
findings provide practical insight into geometry-driven thermal optimization
and offer a path toward improving the performance and durability of electric
motors.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [111] [HPC-Driven Modeling with ML-Based Surrogates for Magnon-Photon Dynamics in Hybrid Quantum Systems](https://arxiv.org/abs/2510.22221)
*Jialin Song,Yingheng Tang,Pu Ren,Shintaro Takayoshi,Saurabh Sawant,Yujie Zhu,Jia-Mian Hu,Andy Nonaka,Michael W. Mahoney,Benjamin Erichson,Zhi,Yao*

Main category: quant-ph

TL;DR: A GPU-based simulation framework for hybrid magnonic quantum systems that resolves multiscale dynamics and uses machine learning surrogates to accelerate design workflows.


<details>
  <summary>Details</summary>
Motivation: Simulating hybrid magnonic quantum systems is challenging due to large timescale disparities between magnonic and photonic systems, requiring new computational approaches.

Method: Massively parallel GPU-based simulation framework for fully coupled magnon-photon circuits, combined with physics-informed machine learning surrogates trained on simulation data.

Result: The approach reveals real-time energy exchange dynamics, reproduces anti-crossing behavior, and shows suppression of ferromagnetic resonance under strong electromagnetic fields.

Conclusion: The framework addresses multiscale and multiphysics challenges in magnon-photon modeling, enabling scalable simulation and rapid prototyping of next-generation quantum and spintronic devices.

Abstract: Simulating hybrid magnonic quantum systems remains a challenge due to the
large disparity between the timescales of the two systems. We present a
massively parallel GPU-based simulation framework that enables fully coupled,
large-scale modeling of on-chip magnon-photon circuits. Our approach resolves
the dynamic interaction between ferromagnetic and electromagnetic fields with
high spatiotemporal fidelity. To accelerate design workflows, we develop a
physics-informed machine learning surrogate trained on the simulation data,
reducing computational cost while maintaining accuracy. This combined approach
reveals real-time energy exchange dynamics and reproduces key phenomena such as
anti-crossing behavior and the suppression of ferromagnetic resonance under
strong electromagnetic fields. By addressing the multiscale and multiphysics
challenges in magnon-photon modeling, our framework enables scalable simulation
and rapid prototyping of next-generation quantum and spintronic devices.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [112] [FeaGPT: an End-to-End agentic-AI for Finite Element Analysis](https://arxiv.org/abs/2510.21993)
*Yupeng Qi,Ran Xu,Xu Chu*

Main category: cs.SE

TL;DR: FeaGPT is the first framework that enables complete geometry-mesh-simulation workflows through conversational interfaces, automating the entire FEA pipeline from engineering specifications to validated computational results without manual intervention.


<details>
  <summary>Details</summary>
Motivation: To democratize access to advanced computational engineering tools by enabling natural language control of complex FEA workflows, overcoming the limitations of existing tools that only automate individual components.

Method: Implements a fully integrated Geometry-Mesh-Simulation-Analysis (GMSA) pipeline that interprets engineering intent, generates physics-aware adaptive meshes, configures complete FEA simulations with boundary condition inference, and performs multi-objective analysis through closed-loop iteration.

Result: Successfully transformed natural language specifications into validated CalculiX simulations for industrial turbocharger cases (7-blade compressor and 12-blade turbine at 110,000 rpm), producing physically realistic results. Additional validation with 432 NACA airfoil configurations confirmed scalability for parametric design exploration.

Conclusion: Natural language interfaces can effectively democratize access to advanced computational engineering tools while preserving analytical rigor, enabling complete end-to-end automation of FEA workflows.

Abstract: Large language models (LLMs) are establishing new paradigms for engineering
applications by enabling natural language control of complex computational
workflows. This paper introduces FeaGPT, the first framework to achieve
complete geometry-mesh-simulation workflows through conversational interfaces.
Unlike existing tools that automate individual FEA components, FeaGPT
implements a fully integrated Geometry-Mesh-Simulation-Analysis (GMSA) pipeline
that transforms engineering specifications into validated computational results
without manual intervention. The system interprets engineering intent,
automatically generates physics-aware adaptive meshes, configures complete FEA
simulations with proper boundary condition inference, and performs
multi-objective analysis through closed-loop iteration.
  Experimental validation confirms complete end-to-end automation capability.
Industrial turbocharger cases (7-blade compressor and 12-blade turbine at
\SI{110000}{rpm}) demonstrate the system successfully transforms natural
language specifications into validated CalculiX simulations, producing
physically realistic results for rotating machinery analysis. Additional
validation through 432 NACA airfoil configurations confirms scalability for
parametric design exploration. These results demonstrate that natural language
interfaces can effectively democratize access to advanced computational
engineering tools while preserving analytical rigor.

</details>
