<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 13]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [math.OC](#math.OC) [Total: 2]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [math.MG](#math.MG) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 2]
- [cs.LG](#cs.LG) [Total: 7]
- [nlin.CD](#nlin.CD) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A Semi-Lagrangian Adaptive Rank (SLAR) Method for High-Dimensional Vlasov Dynamics](https://arxiv.org/abs/2510.24861)
*Nanyi Zheng,William A. Sands,Daniel Hayes,Andrew J. Christlieb,Jing-Mei Qiu*

Main category: math.NA

TL;DR: Extension of SLAR integrator to high-order tensor setting for Vlasov-Poisson systems, enabling 6D simulations with linear complexity scaling in grid points.


<details>
  <summary>Details</summary>
Motivation: To overcome the curse of dimensionality in high-dimensional Vlasov models and enable efficient simulation of up to six-dimensional systems while maintaining accuracy and stability.

Method: Third-order high-dimensional polynomial reconstruction, recursive hierarchical adaptive cross approximation in hierarchical Tucker format, and low-complexity Poisson solver leveraging FFT.

Result: Achieves computational complexity of O(d⁴N r^(3+⌈log₂d⌉)) with substantial computational savings in storage and time, capturing complex solution structures.

Conclusion: The proposed algorithm successfully extends semi-Lagrangian methods to high-dimensional tensor settings, providing an efficient framework for high-dimensional Vlasov simulations with linear scaling in grid points.

Abstract: We extend our previous work on a semi-Lagrangian adaptive rank (SLAR)
integrator, in the finite difference framework for nonlinear Vlasov-Poisson
systems, to the general high-order tensor setting. The proposed scheme retains
the high-order accuracy of semi-Lagrangian methods, ensuring stability for
large time steps and avoiding dimensional splitting errors. The primary
contribution of this paper is the novel extension of the algorithm from the
matrix to the high-dimensional tensor setting, which enables the simulation of
Vlasov models in up to six dimensions. The key technical components include (1)
a third-order high-dimensional polynomial reconstruction that scales as
$O(d^2)$, providing a point-wise approximation of the solution at the foot of
characteristics in a semi-Lagrangian scheme; (2) a recursive hierarchical
adaptive cross approximation of high-order tensors in a hierarchical Tucker
format, characterized by a tensor tree; (3) a low-complexity Poisson solver in
the hierarchical Tucker format that leverages the FFT for efficiency. The
computed adaptive rank kinetic solutions exhibit low-rank structures within
branches of the tensor tree resulting in substantial computational savings in
both storage and time. The resulting algorithm achieves a computational
complexity of $O(d^4 N r^{3+\lceil\log_2d\rceil})$, where $N$ is the number of
grid points per dimension, $d$ is the problem dimension, and $r$ is the maximum
rank in the tensor tree, overcoming the curse of dimensionality. Through
extensive numerical tests, we demonstrate the efficiency of the proposed
algorithm and highlight its ability to capture complex solution structures
while maintaining a computational complexity that scales linearly with $N$.

</details>


### [2] [Interpolated Discrepancy Data Assimilation for PDEs with Sparse Observations](https://arxiv.org/abs/2510.24944)
*Tong Wu,Humberto Godinez,Vitaliy Gyrya,James M. Hyman*

Main category: math.NA

TL;DR: IDDA is a new data assimilation method that modifies how observational discrepancies enter governing equations, improving stability and convergence for sparse sensor networks in weather and ocean modeling.


<details>
  <summary>Details</summary>
Motivation: Standard nudging-based data assimilation becomes unstable when sensor networks observe only a small fraction of the system state, particularly when nonlinear effects dominate.

Method: IDDA adjusts both the forcing term and the nonlinear operator using interpolated observational information, suppressing error amplification from nonlinear effects. It uses standard explicit time integration without specialized solvers.

Result: IDDA achieves exponential convergence with error bounds scaling with observation spacing squared, reaches target accuracy faster than standard nudging, remains stable in chaotic regimes, avoids non-monotone transients, and requires minimal parameter tuning.

Conclusion: IDDA provides a practical upgrade for operational systems with sparse sensor coverage, fitting readily into existing simulation pipelines while offering improved stability and convergence properties.

Abstract: Sparse sensor networks in weather and ocean modeling observe only a small
fraction of the system state, which destabilizes standard nudging-based data
assimilation. We introduce Interpolated Discrepancy Data Assimilation (IDDA),
which modifies how discrepancies enter the governing equations. Rather than
adding observations as a forcing term alone, IDDA also adjusts the nonlinear
operator using interpolated observational information. This structural change
suppresses error amplification when nonlinear effects dominate. We prove
exponential convergence under explicit conditions linking error decay to
observation spacing, nudging strength, and diffusion coefficient. The key
requirement establishes bounds on nudging strength relative to observation
spacing and diffusion, giving practitioners a clear operating window. When
observations resolve the relevant scales, error decays at a user-specified
rate. Critically, the error bound scales with the square of observation spacing
rather than through hard-to-estimate nonlinear growth rates. We validate IDDA
on Burgers flow, Kuramoto-Sivashinsky dynamics, and two-dimensional
Navier-Stokes turbulence. Across these tests, IDDA reaches target accuracy
faster than standard interpolated nudging, remains stable in chaotic regimes,
avoids non-monotone transients, and requires minimal parameter tuning. Because
IDDA uses standard explicit time integration, it fits readily into existing
simulation pipelines without specialized solvers. These properties make IDDA a
practical upgrade for operational systems constrained by sparse sensor
coverage.

</details>


### [3] [The B-spline collocation method for solving Cauchy singular integral equations with piecewise Holder continuous coefficients](https://arxiv.org/abs/2510.24984)
*Maria Capcelea,Titu Capcelea*

Main category: math.NA

TL;DR: A numerical method for solving Cauchy singular integral equations on closed smooth contours using B-spline and Heaviside function approximations for piecewise Holder continuous data.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical approach for Cauchy singular integral equations with piecewise Holder continuous coefficients and right-hand sides that may have jump discontinuities, given only at discrete points on the contour.

Method: Proposes a collocation algorithm using linear combinations of B-spline functions and Heaviside step functions to approximate piecewise Holder continuous functions, then applies this to solve the singular integral equation.

Result: Establishes convergence of the constructed approximations to the exact solution in piecewise Holder space norms and derives convergence rate estimates for the method.

Conclusion: The proposed method provides a reliable numerical framework for solving Cauchy singular integral equations with discontinuous coefficients, with proven convergence properties and rate estimates.

Abstract: In this paper, we propose a numerical method for approximating the solution
of a Cauchy singular integral equation defined on a closed, smooth contour in
the complex plane. The coefficients and the right-hand side of the equation are
piecewise Holder continuous functions that may have a finite number of jump
discontinuities, and are given numerically at a finite set of points on the
contour. We introduce an efficient approximation scheme for piecewise Holder
continuous functions based on linear combinations of B-spline functions and
Heaviside step functions, which serves as the foundation for the proposed
collocation algorithm. We then establish the convergence of the sequence of the
constructed approximations to the exact solution of the equation in the norm of
piecewise Holder spaces and derive estimates for the convergence rate of the
method.

</details>


### [4] [Cluster Formation in Diffusive Systems](https://arxiv.org/abs/2510.25034)
*Benedict Leimkuhler,René Lohmann,Grigorios A. Pavliotis,Peter A. Whalley*

Main category: math.NA

TL;DR: This paper studies cluster formation in stochastic interacting particle systems with short-range attractive potentials using kinetic Langevin dynamics in low-friction regimes, bridging overdamped Langevin and Hamiltonian limits.


<details>
  <summary>Details</summary>
Motivation: To understand cluster formation mechanisms in stochastic particle systems with short-range interactions, particularly in the underdamped regime where inertial effects matter, connecting overdamped and Hamiltonian dynamics.

Method: Linear stability analysis of the kinetic McKean-Vlasov equation for low-friction kinetic Langevin dynamics, combined with numerical measurements of cluster formation times.

Result: At low temperatures and for short-range interactions, particles form clusters as metastable states; derived friction and particle-count dependent cluster formation times; numerically measured friction-dependent times to reach stationary single-cluster states.

Conclusion: The work successfully bridges cluster formation studies between overdamped Langevin dynamics and the Hamiltonian limit by providing both theoretical analysis and numerical methods for inertial stochastic systems.

Abstract: In this paper, we study the formation of clusters for stochastic interacting
particle systems (SIPS) that interact through short-range attractive potentials
in a periodic domain. We consider kinetic (underdamped) Langevin dynamics and
focus on the low-friction regime. Employing a linear stability analysis for the
kinetic McKean-Vlasov equation, we show that, at sufficiently low temperatures,
and for sufficiently short-ranged interactions, the particles form clusters
that correspond to metastable states of the mean-field dynamics. We derive the
friction and particle-count dependent cluster-formation time and numerically
measure the friction-dependent times to reach a stationary state (given by a
state in which all particles are bound in a single cluster). By providing both
theory and numerical methods in the inertial stochastic setting, this work acts
as a bridge between cluster formation studies in overdamped Langevin dynamics
and the Hamiltonian (microcanonical) limit.

</details>


### [5] [Learning Hamiltonian flows from numerical integrators and examples](https://arxiv.org/abs/2510.25107)
*Rui Fang,Richard Tsai*

Main category: math.NA

TL;DR: Deep Learning framework learns Hamiltonian system flow maps to accelerate long-time and ensemble simulations, using neural networks with truncated Taylor expansions and achieving substantial speedups while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: Hamiltonian systems with multiple timescales require small time steps for numerical integration, incurring high computational costs, especially in ensemble simulations for uncertainty quantification and sensitivity analysis.

Method: Neural networks trained either without data to approximate flows over large intervals or with data from Hamiltonian Monte Carlo-based generator; architecture uses feedforward networks with truncated Taylor expansions and neural network remainder for unresolved effects.

Result: Applied to non-integrable and non-canonical benchmark systems, the method achieves substantial speedups while preserving accuracy.

Conclusion: Enables scalable simulation of complex Hamiltonian dynamics through learned flow maps that accelerate long-time and ensemble simulations.

Abstract: Hamiltonian systems with multiple timescales arise in molecular dynamics,
classical mechanics, and theoretical physics. Long-time numerical integration
of such systems requires resolving fast dynamics with very small time steps,
which incurs a high computational cost - especially in ensemble simulations for
uncertainty quantification, sensitivity analysis, or varying initial
conditions. We present a Deep Learning framework that learns the flow maps of
Hamiltonian systems to accelerate long-time and ensemble simulations. Neural
networks are trained, according to a chosen numerical scheme, either entirely
without data to approximate flows over large time intervals or with data to
learn flows in intervals far from the initial time. For the latter, we propose
a Hamiltonian Monte Carlo-based data generator. The architecture consists of
simple feedforward networks that incorporate truncated Taylor expansions of the
flow map, with a neural network remainder capturing unresolved effects. Applied
to benchmark non-integrable and non-canonical systems, the method achieves
substantial speedups while preserving accuracy, enabling scalable simulation of
complex Hamiltonian dynamics.

</details>


### [6] [Energy Approach from $\varepsilon$-Graph to Continuum Diffusion Model with Connectivity Functional](https://arxiv.org/abs/2510.25114)
*Yahong Yang,Sun Lee,Jeff Calder,Wenrui Hao*

Main category: math.NA

TL;DR: The paper derives a continuum limit for ε-graphs with general connectivity functionals, proving O(ε) error bounds that remain valid even with strong local fluctuations in connectivity density.


<details>
  <summary>Details</summary>
Motivation: To develop a rigorous mathematical framework connecting discrete graph models to continuum formulations, particularly for applications in brain dynamics where connectivity patterns exhibit spatial variations.

Method: Derive energy-based continuum limit for ε-graphs with connectivity functionals, prove error bounds involving W^{1,1}-norm of connectivity density, and introduce neural-network procedure to reconstruct connectivity density from edge-weight data.

Result: The discrete energy and continuum counterpart differ by at most O(ε), with error bounds valid even for strongly fluctuating connectivity densities. The neural-network procedure successfully embeds continuum models into brain-dynamics frameworks.

Conclusion: The approach enables spatially varying diffusion coefficients in brain dynamics models, producing significantly different dynamics compared to conventional constant-diffusion models, with rigorous mathematical guarantees.

Abstract: We derive an energy-based continuum limit for $\varepsilon$-graphs endowed
with a general connectivity functional. We prove that the discrete energy and
its continuum counterpart differ by at most $O(\varepsilon)$; the prefactor
involves only the $W^{1,1}$-norm of the connectivity density as
$\varepsilon\to0$, so the error bound remains valid even when that density has
strong local fluctuations. As an application, we introduce a neural-network
procedure that reconstructs the connectivity density from edge-weight data and
then embeds the resulting continuum model into a brain-dynamics framework. In
this setting, the usual constant diffusion coefficient is replaced by the
spatially varying coefficient produced by the learned density, yielding
dynamics that differ significantly from those obtained with conventional
constant-diffusion models.

</details>


### [7] [Error Analysis of Third-Order in Time and Fourth-Order Linear Finite Difference Scheme for Landau-Lifshitz-Gilbert Equation under Large Damping Parameters](https://arxiv.org/abs/2510.25172)
*Changjian Xie,Cheng Wang*

Main category: math.NA

TL;DR: A fully discrete numerical scheme for the Landau-Lifshitz-Gilbert equation with fourth-order spatial accuracy and third-order temporal accuracy, preserving the normalization constraint.


<details>
  <summary>Details</summary>
Motivation: To develop a high-order accurate numerical method for the LLG equation that maintains key physical properties while achieving computational efficiency.

Method: Uses long-stencil finite differences for spatial discretization (fourth-order), BDF3 for temporal discretization (third-order), implicit treatment of linear diffusion terms, and explicit extrapolation for nonlinear terms.

Result: The scheme preserves the normalization constraint and achieves optimal convergence rates under ℓ∞([0,T],ℓ²) and ℓ²([0,T],H_h¹) norms.

Conclusion: Numerical experiments validate the theoretical convergence results, showing good agreement between numerical observations and analytical conclusions.

Abstract: This work proposes and analyzes a fully discrete numerical scheme for solving
the Landau-Lifshitz-Gilbert (LLG) equation, which achieves fourth-order spatial
accuracy and third-order temporal accuracy.Spatially, fourth-order accuracy is
attained through the adoption of a long-stencil finite difference method, while
boundary extrapolation is executed by leveraging a higher-order Taylor
expansion to ensure consistency at domain boundaries. Temporally, the scheme is
constructed based on the third-order backward differentiation formula (BDF3),
with implicit discretization applied to the linear diffusion term for numerical
stability and explicit extrapolation employed for nonlinear terms to balance
computational efficiency. Notably, this numerical method inherently preserves
the normalization constraint of the LLG equation, a key physical property of
the system.Theoretical analysis confirms that the proposed scheme exhibits
optimal convergence rates under the \(\ell^{\infty}([0,T],\ell^2)\) and
\(\ell^2([0,T],H_h^1)\) norms. Finally, numerical experiments are conducted to
validate the correctness of the theoretical convergence results, demonstrating
good agreement between numerical observations and analytical conclusions.

</details>


### [8] [Spectral analysis of the stiffness matrix sequence in the approximated Stokes equation](https://arxiv.org/abs/2510.25252)
*Samuele Ferri,Chiara Giraudo,Valerio Loi,Miroslav Kuchta,Stefano Serra-Capizzano*

Main category: math.NA

TL;DR: Spectral analysis of matrix sequences from Taylor-Hood P2-P1 approximation for 2D Stokes problem with variable viscosity, including localization, distribution results, and preliminary preconditioning study.


<details>
  <summary>Details</summary>
Motivation: To understand the spectral properties of matrix sequences arising from finite element discretization of Stokes equations with variable viscosity, which is important for developing efficient numerical solvers.

Method: Detailed spectral analysis of matrix sequences from Taylor-Hood P2-P1 approximation, providing localization and distributional spectral results, supported by numerical tests and visualizations.

Result: Spectral features of the matrix sequences are characterized, with preliminary investigation showing implications for preconditioning strategies.

Conclusion: The work provides spectral analysis results for Stokes discretization with variable viscosity, identifies open problems, and suggests potential applications in preconditioning.

Abstract: In the present paper, we analyze in detail the spectral features of the
matrix sequences arising from the Taylor-Hood $\mathbb{P}_2$-$\mathbb{P}_1$
approximation of variable viscosity for $2d$ Stokes problem under weak
assumptions on the regularity of the diffusion. Localization and distributional
spectral results are provided, accompanied by numerical tests and
visualizations. A preliminary study of the impact of our findings on the
preconditioning problem is also presented. A final section with concluding
remarks and open problems ends the current work.

</details>


### [9] [Identifying Kronecker product factorizations](https://arxiv.org/abs/2510.25292)
*Yannis Voet,Leonardo De Novellis*

Main category: math.NA

TL;DR: This paper develops methods to identify all possible Kronecker factorizations of binary matrices and visualizes them through decomposition graphs, enabling better approximate factorizations and network structure analysis.


<details>
  <summary>Details</summary>
Motivation: The Kronecker product is valuable for data-sparse representations of large networks and matrices, but existing sparsity patterns may already hide Kronecker structures that can be leveraged for better approximations and network analysis.

Method: The authors determine all possible Kronecker factorizations of binary matrices and visualize them using decomposition graphs.

Result: The approach enables identification of hidden Kronecker structures in binary matrices, which can be used for approximate factorizations of real matrices and reveals latent network structures.

Conclusion: The proposed Kronecker factorization method for binary matrices provides a foundation for better approximate factorizations and offers natural visualization capabilities for Kronecker graphs.

Abstract: The Kronecker product is an invaluable tool for data-sparse representations
of large networks and matrices with countless applications in machine learning,
graph theory and numerical linear algebra. In some instances, the sparsity
pattern of large matrices may already hide a Kronecker product. Similarly, a
large network, represented by its adjacency matrix, may sometimes be factorized
as a Kronecker product of smaller adjacency matrices. In this article, we
determine all possible Kronecker factorizations of a binary matrix and
visualize them through its decomposition graph. Such sparsity-informed
factorizations may later enable good (approximate) Kronecker factorizations of
real matrices or reveal the latent structure of a network. The latter also
suggests a natural visualization of Kronecker graphs.

</details>


### [10] [A virtual element approximation for the modified transmission eigenvalues for natural materials](https://arxiv.org/abs/2510.25298)
*Liangkun Xu,Shixi Wang,Hai Bi*

Main category: math.NA

TL;DR: A virtual element method for solving modified transmission eigenvalue problems in inverse scattering, addressing non-coercive sesquilinear forms through T-coercivity analysis.


<details>
  <summary>Details</summary>
Motivation: To develop effective numerical methods for transmission eigenvalue problems in inverse scattering of natural materials, particularly handling the challenge of non-coercive sesquilinear forms due to positive artificial diffusivity parameters.

Method: Virtual element approximation using T-coercivity property to establish well-posedness of discrete source problems, with derivation of a priori error estimates for eigenspaces and eigenvalues.

Result: The method successfully handles non-coercive problems and numerical experiments demonstrate the effectiveness of the proposed virtual element approach.

Conclusion: The virtual element method with T-coercivity analysis provides an effective computational framework for modified transmission eigenvalue problems in inverse scattering applications.

Abstract: In this paper, we discuss a virtual element approximation for the modified
transmission eigenvalue problem in inverse scattering for natural materials. In
this case, due to the positive artificial diffusivity parameter in the
considered problem, the sesquilinear form at the left end of the variational
form is not coercive. We first demonstrate the well-posedness of the discrete
source problem using the $\mathds{T}$-coercivity property, then provide the a
priori error estimates for the approximate eigenspaces and eigenvalues, and
finally reports several numerical examples. The numerical experiments show that
the proposed method is effective

</details>


### [11] [A structure-preserving Lagrangian discontinuous Galerkin method using flux and slope limiting](https://arxiv.org/abs/2510.25395)
*Joshua Vedral,Nathaniel Morgan,Dmitri Kuzmin,Jacob Moore*

Main category: math.NA

TL;DR: A Lagrangian nodal discontinuous Galerkin method with flux-corrected transport that ensures positivity preservation and second-order accuracy for cell averages of specific volume.


<details>
  <summary>Details</summary>
Motivation: To develop a stable and accurate Lagrangian hydrodynamics method that can handle multi-dimensional hyperbolic systems while preserving positivity and capturing shocks effectively.

Method: Combines first-order positivity-preserving scheme with higher-order DG discretization using Zalesak's flux-corrected transport algorithm. Uses discrete geometric conservation law for volume evolution, multidirectional Riemann solver for nodal velocities, and bound-preserving slope limiter.

Result: The method demonstrates stability and excellent shock-capturing capabilities on standard test problems while maintaining global positivity preservation and second-order accuracy for cell averages.

Conclusion: The proposed flux-corrected Lagrangian DG scheme successfully achieves both positivity preservation and high-order accuracy, making it suitable for complex multi-dimensional hydrodynamic simulations.

Abstract: We introduce a Lagrangian nodal discontinuous Galerkin (DG) cell-centered
hydrodynamics method for solving multi-dimensional hyperbolic systems. By
incorporating an adaptation of Zalesak's flux-corrected transport algorithm, we
combine a first-order positivity-preserving scheme with a higher-order target
discretization. This results in a flux-corrected Lagrangian DG scheme that
ensures both global positivity preservation and second-order accuracy for the
cell averages of specific volume. The correction factors for flux limiting are
derived from specific volume and applied to all components of the solution
vector. We algebraically evolve the volumes of mesh cells using a discrete
version of the geometric conservation law (GCL). The application of a limiter
to the GCL fluxes is equivalent to moving the mesh using limited nodal
velocities. Additionally, we equip our method with a locally bound-preserving
slope limiter to effectively suppress spurious oscillations. Nodal velocity and
external forces are computed using a multidirectional approximate Riemann
solver to maintain conservation of momentum and total energy in vertex
neighborhoods. Employing linear finite elements and a second-order accurate
time integrator guarantees GCL consistency. The results for standard test
problems demonstrate the stability and superb shock-capturing capabilities of
our scheme.

</details>


### [12] [Nonparametric estimation of homogenized invariant measures from multiscale data via Hermite expansion](https://arxiv.org/abs/2510.25521)
*Jaroslav I. Borodavka,Max Hirsch,Sebastian Krumscheid,Andrea Zanoni*

Main category: math.NA

TL;DR: A spectral method using Hermite functions for density estimation of homogenized Langevin diffusion from multiscale trajectory data, with proven convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: To learn the invariant measure density of homogenized dynamics from multiscale Langevin diffusion processes where a single-scale surrogate model exists.

Method: Spectral method with truncated Fourier expansion using Hermite functions as orthonormal basis, computing Fourier coefficients directly from data via ergodic theorem.

Result: The density estimator is robust and converges to homogenized invariant density as scale separation vanishes, with proper choice of time horizon and Fourier modes relative to multiscale parameter.

Conclusion: The proposed spectral method provides accurate and reliable density estimation for homogenized dynamics from multiscale data, validated by numerical experiments.

Abstract: We consider the problem of density estimation in the context of multiscale
Langevin diffusion processes, where a single-scale homogenized surrogate model
can be derived. In particular, our aim is to learn the density of the invariant
measure of the homogenized dynamics from a continuous-time trajectory generated
by the full multiscale system. We propose a spectral method based on a
truncated Fourier expansion with Hermite functions as orthonormal basis. The
Fourier coefficients are computed directly from the data owing to the ergodic
theorem. We prove that the resulting density estimator is robust and converges
to the invariant density of the homogenized model as the scale separation
parameter vanishes, provided the time horizon and the number of Fourier modes
are suitably chosen in relation to the multiscale parameter. The accuracy and
reliability of this methodology is further demonstrated through a series of
numerical experiments.

</details>


### [13] [3-Dimensional Adaptive Unstructured Tessellated Look-up Tables for the Approximation of Compton Form Factors](https://arxiv.org/abs/2510.25699)
*Charles Hyde,Mitch Kerver,Christos Tsolakis,Polykarpos Thomadakis,Spiros Tsalikis,Kevin Garner,Angelos Angelopoulos,Wirawan Purwanto,Gagik Gavalian,Christian Weiss,Nikos Chrisochoides*

Main category: math.NA

TL;DR: An iterative algorithm creates unstructured tetrahedral tessellations to approximate functions with 1% mean interpolation error, significantly speeding up Monte Carlo event generation for nuclear femtography.


<details>
  <summary>Details</summary>
Motivation: To enable efficient simulation and analysis of nuclear femtography through high-energy exclusive processes like electron-proton scattering, which requires fast computation of Compton Form Factors for Monte Carlo event generation.

Method: Iterative algorithm that constructs unstructured tessellations of simplices (irregular tetrahedra in 3D) to approximate arbitrary functions via interpolation to desired precision.

Result: Achieved tessellations with only 1% mean interpolation error, reducing computation time for Monte Carlo event generation by ~23 times for 10^7 events and ~955 times for 10^10 events using extrapolation.

Conclusion: The tessellation method provides highly efficient function approximation that dramatically accelerates computational workflows in nuclear femtography simulations while maintaining high accuracy.

Abstract: We describe an iterative algorithm to construct an unstructured tessellation
of simplices (irregular tetrahedra in 3-dimensions) to approximate an arbitrary
function to a desired precision by interpolation. The method is applied to the
generation of Compton Form Factors for simulation and analysis of nuclear
femtography, as enabled by high energy exclusive processes such as
electron-proton scattering producing just an electron, proton, and gamma-ray in
the final state. While producing tessellations with only a 1% mean
interpolation error, our results show that the use of such tessellations can
significantly decrease the computation time for Monte Carlo event generation by
$\sim23$ times for $10^{7}$ events (and using extrapolation, by $\sim955$ times
for $10^{10}$ events).

</details>


### [14] [Meshless solutions of PDE inverse problems on irregular geometries](https://arxiv.org/abs/2510.25752)
*James V. Roggeveen,Michael P. Brenner*

Main category: math.NA

TL;DR: A spectral method for solving PDE inverse and optimization problems on complex domains by parameterizing solutions with spectral bases defined on hyperrectangles and optimizing coefficients using PINN-inspired loss functions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving inverse and optimization problems over nonlinear PDE solutions on complex spatial domains, which has been a long-standing difficulty in the field.

Method: Parameterize PDE solutions using spectral bases on arbitrary domains by defining the basis on a hyperrectangle containing the true domain. Find coefficients by solving an optimization problem where equations, boundary conditions, and optimization targets are enforced through a loss function inspired by PINNs.

Result: The method achieves exponential convergence in solving optimization problems due to the native exponential convergence of the spectral representation. Empirical results show optimization protocols from machine learning find solutions with exponential convergence across various equations.

Conclusion: The approach naturally incorporates data assimilation through additional loss terms and efficiently solves optimization problems over PDE solutions, providing a powerful framework for complex domain problems.

Abstract: Solving inverse and optimization problems over solutions of nonlinear partial
differential equations (PDEs) on complex spatial domains is a long-standing
challenge. Here we introduce a method that parameterizes the solution using
spectral bases on arbitrary spatiotemporal domains, whereby the basis is
defined on a hyperrectangle containing the true domain. We find the
coefficients of the basis expansion by solving an optimization problem whereby
both the equations, the boundary conditions and any optimization targets are
enforced by a loss function, building on a key idea from Physics-Informed
Neural Networks (PINNs). Since the representation of the function natively has
exponential convergence, so does the solution of the optimization problem, as
long as it can be solved efficiently. We find empirically that the optimization
protocols developed for machine learning find solutions with exponential
convergence on a wide range of equations. The method naturally allows for the
incorporation of data assimilation by including additional terms in the loss
function, and for the efficient solution of optimization problems over the PDE
solutions.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Large-Time Analysis of the Langevin Dynamics for Energies Fulfilling Polyak-Łojasiewicz Conditions](https://arxiv.org/abs/2510.24925)
*Massimo Fornasier,Lukang Sun,Rachel Ward*

Main category: math.AP

TL;DR: Analysis of overdamped Langevin dynamics for minimizing objective functions, establishing well-posedness, characterizing large-time behavior, and providing convergence rates under PL conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of overdamped Langevin dynamics for minimizing general objective functions, particularly characterizing convergence properties under minimal assumptions.

Method: Established well-posedness and regularity through novel a priori estimates, analyzed large-time behavior under minimal assumptions, and derived convergence rates under Polyak-Lojasiewicz conditions.

Result: For integrable Gibbs density, law converges to normalized Gibbs measure; for non-integrable case, law diffuses with O(1/t) rate. Under PL conditions, exponential contraction toward minimizers followed by O(1/t) exploration.

Conclusion: First systematic convergence analysis of Langevin dynamics under PL conditions in non-integrable Gibbs settings, revealing two-phase convergence behavior.

Abstract: In this work, we take a step towards understanding overdamped Langevin
dynamics for the minimization of a general class of objective functions
$\mathcal{L}$. We establish well-posedness and regularity of the law $\rho_t$
of the process through novel a priori estimates, and, very importantly, we
characterize the large-time behavior of $\rho_t$ under truly minimal
assumptions on $\mathcal{L}$. In the case of integrable Gibbs density, the law
converges to the normalized Gibbs measure. In the non-integrable case, we prove
that the law diffuses. The rate of convergence is $\mathcal{O}(1/t)$. Under a
Polyak-Lojasiewicz (PL) condition on $\mathcal{L}$, we also derive sharp
exponential contractivity results toward the set of global minimizers.
Combining these results we provide the first systematic convergence analysis of
Langevin dynamics under PL conditions in non-integrable Gibbs settings: a first
phase of exponential in time contraction toward the set of minimizers and then
a large-time exploration over it with rate $\mathcal{O}(1/t)$.

</details>


### [16] [Dynamics of solutions in the 1d bi-harmonic nonlinear Schrödinger equation](https://arxiv.org/abs/2510.24961)
*Christian Klein,Iryna Petrenko,Svetlana Roudenko,Nikola Stoilov*

Main category: math.AP

TL;DR: Analysis of 1D bi-harmonic NLS equation dynamics: ground state solutions form stable/unstable branches dictating long-term behavior. Perturbations lead to dispersion, state jumps, or blow-up depending on dispersion parameter a.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of 4th order nonlinear Schrödinger equation solutions, particularly ground state behavior and stability, and investigate scattering/blow-up dichotomy for different dispersion parameters.

Method: Numerical construction of ground state solutions, stability analysis, and investigation of solution dynamics through perturbations for various nonlinearity powers α and dispersion parameters a.

Result: Ground states form two branches (stable/unstable) that determine long-term behavior. Perturbations on unstable branch lead to dispersion or transition to stable states. Blow-up occurs in critical/supercritical cases with scale-invariant profile.

Conclusion: The bi-harmonic NLS exhibits rich dynamics with scattering/blow-up dichotomy for a≤0 and trichotomy for a>0. Ground state stability branches dictate solution fate, and blow-up follows scale-invariant profile regardless of lower dispersion value.

Abstract: We consider the one dimensional 4th order, or bi-harmonic, nonlinear
Schr\"odinger (NLS) equation, namely, $i u_t - \Delta^2 u - 2a \Delta u +
|u|^{\alpha} u = 0, ~ x,a \in \R$, $\alpha>0$, and investigate the dynamics of
its solutions for various powers of $\alpha$, including the ground state
solutions and their perturbations, leading to scattering or blow-up dichotomy
when $a \leq 0$, or to a trichotomy when $a>0$. Ground state solutions are
numerically constructed, and their stability is studied, finding that the
ground state solutions may form two branches, stable and unstable, which
dictates the long-term behavior of solutions. Perturbations of the ground
states on the unstable branch either lead to dispersion or the jump to a stable
ground state. In the critical and supercritical cases, blow-up in finite time
is also investigated, and it is conjectured that the blow-up happens with a
scale-invariant profile (when $a=0$) regardless of the value of $a$ of the
lower dispersion. The blow-up rate is also explored.

</details>


### [17] [Boundary determination of electromagnetic parameters from local data](https://arxiv.org/abs/2510.25061)
*Chengyu Wu,Jiaqing Yang*

Main category: math.AP

TL;DR: The paper extends and simplifies methods to prove unique boundary determination of electromagnetic parameters from local admittance maps, even with unknown obstacles.


<details>
  <summary>Details</summary>
Motivation: To improve uniqueness results for boundary determination in Maxwell equations, particularly handling unknown obstacles.

Method: Extension and simplification of existing methods, using elaborate singularity analysis of singular solutions to Maxwell equations.

Result: Electromagnetic parameters are uniquely determined to infinite order at the boundary from local admittance map, regardless of unknown obstacles.

Conclusion: The approach successfully establishes unique boundary determination using local Cauchy data of fundamental solutions through singularity analysis.

Abstract: In this paper, we extend and simplify the methods in [13] to improve the
results on uniqueness of the boundary determination for the Maxwell equation.
In particular, we show that the electromagnetic parameters are uniquely
determined to infinite order at the boundary from the local admittance map,
disregarding the presence of an unknown obstacle, where actually only the local
Cauchy data of the fundamental solution are used. The proof mainly relies on an
elaborate singularity analysis on certain singular solutions to the Maxwell
equation.

</details>


### [18] [Self-similar blowup from arbitrary data for supercritical wave maps with additive noise](https://arxiv.org/abs/2510.25326)
*Irfan Glogić,Martina Hofmanová,Eliseo Luongo*

Main category: math.AP

TL;DR: Stochastically perturbed wave maps from R^{1+d} to S^d in energy-supercritical dimensions d≥3 exhibit self-similar blowup with positive probability under corotational non-degenerate Gaussian additive noise for any corotational initial data.


<details>
  <summary>Details</summary>
Motivation: To understand how stochastic perturbations affect the blowup behavior of wave maps in energy-supercritical dimensions, particularly whether noise can induce self-similar blowup that is conjectured but unproven for deterministic large data.

Method: Analysis of stochastically perturbed wave maps using corotational non-degenerate Gaussian additive noise applied to corotational initial data in energy-supercritical dimensions d≥3.

Result: The addition of corotational non-degenerate Gaussian additive noise leads to self-similar blowup with positive probability for any corotational initial data.

Conclusion: Stochastic perturbations can trigger self-similar blowup phenomena in wave maps that are conjectured but not proven in the deterministic case, demonstrating the significant impact of noise on blowup dynamics.

Abstract: We consider stochastically perturbed wave maps from $\mathbb{R}^{1+d}$ into
$\mathbb{S}^d$, in all energy-supercritical dimensions $d \geq 3$. We show that
corotational non-degenerate Gaussian additive noise leads to self-similar
blowup with positive probability for any corotational initial data. The same
result without noise is conjectured, but unknown, for large data.

</details>


### [19] [Steady super-Alfvénic MHD shocks with aligned fields in two-dimensional almost flat nozzles](https://arxiv.org/abs/2510.25374)
*Shangkun Weng,Wengang Yang*

Main category: math.AP

TL;DR: This paper establishes existence of super-Alfvénic transonic shock solutions in steady MHD equations, deriving an admissible condition for shock front locations based on nozzle wall profile and exit total pressure.


<details>
  <summary>Details</summary>
Motivation: The Lorentz force in MHD flow creates anisotropic disturbance propagation, making steady MHD equation types depend on both Mach and Alfvén numbers, requiring new analysis for transonic shocks.

Method: Using deformation-curl decomposition of steady MHD equations to derive admissible conditions for transonic shock locations, starting from initial approximations.

Result: Successfully established nonlinear existence of super-Alfvénic transonic shock solutions with an improved admissible condition that differs from previous work.

Conclusion: The new formulation based on deformation-curl decomposition provides advantages for generalization to three-dimensional cases at the initial approximation level.

Abstract: The Lorentz force induced by the magnetic field in MHD flow introduces a
fundamental difference from pure gas dynamics by facilitating the anisotropic
propagation of small disturbances, thus the type of steady MHD equations
depends on not only the Mach number but also the Alfv\'en number. In the
super-Alfv\'enic case, we derive an admissible condition for the locations of
transonic shock fronts in terms of the nozzle wall profile and the exit total
pressure (the kinetic plus magnetic pressure). Starting from this initial
approximation, a nonlinear existence of super-Alfv\'enic transonic shock
solution to steady MHD equations is established. Our admissible condition is
slightly different from that first introduced by Fang-Xin in [Comm. Pure Appl.
Math., 74 (2021), pp. 1493-1544], and because our formulation is based on the
deformation-curl decomposition of the steady MHD equations, our admissible
condition has the advantage that a direct generalization to three dimensional
case is available at least at the level of the initial approximation of the
shock position.

</details>


### [20] [Solutions to the two-dimensional steady incompressible Euler equations in an annulus](https://arxiv.org/abs/2510.25382)
*Wengang Yang*

Main category: math.AP

TL;DR: Investigation of well-posedness for five classes of boundary value problems for 2D steady incompressible Euler equations in annular domains, with solutions established in C^{1,α} and C^{2,α} spaces.


<details>
  <summary>Details</summary>
Motivation: To analyze the solvability and well-posedness of various boundary value problems for the two-dimensional steady incompressible Euler equations in annular domains, which is fundamental for understanding fluid dynamics in such geometries.

Method: Uses Grad-Shafranov method for three boundary conditions with variational techniques for C^{1,α} solutions, and vorticity transport method for all five classes with perturbation framework for C^{2,α} solutions.

Result: Established well-posedness of solutions in C^{1,α} space via variational methods and proved solvability of all five boundary value problems using vorticity transport method, with further well-posedness of C^{2,α} solutions under perturbation.

Conclusion: All five classes of boundary value problems for 2D steady incompressible Euler equations in annular domains are solvable and well-posed, with solutions existing in both C^{1,α} and C^{2,α} function spaces under appropriate methods.

Abstract: This paper investigates the well-posedness of five classes of boundary value
problems for the two-dimensional steady incompressible Euler equations in an
annular domain. Three of these boundary conditions can be effectively addressed
using the Grad-Shafranov method, and the well-posedness of solutions in the
$C^{1,\al}$ space is established via variational techniques. We demonstrate
that all five classes of boundary value problems are solvable through the
vorticity transport method. Based on this approach, we further prove the
well-posedness of $C^{2,\al}$ solutions under a perturbation framework.

</details>


### [21] [On Type I blowup and $\varepsilon$-regularity criteria of suitable weak solutions to the 3D incompressible MHD equations](https://arxiv.org/abs/2510.25448)
*Wentao Hu,Zhengce Zhang*

Main category: math.AP

TL;DR: New ε-regularity criteria for 3D incompressible MHD equations with flexible smallness assumptions on scaling-invariant quantities of velocity and magnetic fields.


<details>
  <summary>Details</summary>
Motivation: To extend Seregin's Type I criteria from Navier-Stokes equations to MHD system and provide tools for analyzing Type II blowup.

Method: Develop ε-regularity criteria allowing independent smallness/boundedness assumptions on any scaling-invariant quantities of velocity field u and magnetic field b.

Result: Shows that boundedness of any scaling-invariant quantity of u and b (chosen independently) ensures (0,0) is at most a Type I singular point.

Conclusion: Extends Seregin's Type I criteria to MHD equations and provides natural starting point for Type II blowup analysis.

Abstract: In this paper, we study some new $\varepsilon$-regularity criteria related to
the suitable weak solutions to the three-dimensional incompressible MHD
equations. Our criteria allow great flexibility: The smallness and boundedness
assumptions can be imposed on any scaling-invariant quantities of $u$ and $b$,
respectively, which may be chosen independently. As an intermediate step, we
also show that the boundedness of any scaling-invariant quantity of $u$ and
$b$, chosen independently, ensures that $(0,0)$ is at most a Type I singular
point, i.e. $A(u,b;r)+E(u,b;r)+C(u,b;r)+D(p;r)<\infty$. This extends Seregin's
Type I criteria for the Navier--Stokes equations (2006, Zap. Nauchn. Sem. POMI)
\cite{seregin2006Estimates} to the MHD system and provides a natural starting
point for analysing Type II blowup, as in Seregin (2024, Comm. Pure Appl.
Anal.) \cite{seregin2024Remarks}.

</details>


### [22] [Exponential Stability of a Degenerate Euler-Bernoulli Beam with Axial Force and Delayed Boundary Control](https://arxiv.org/abs/2510.25484)
*Ben Bakary Junior Siriki,Adama Coulibaly*

Main category: math.AP

TL;DR: Global exponential stabilization of degenerate Euler-Bernoulli beam with axial loading and time-delay boundary input, addressing degeneracy and delay simultaneously.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of simultaneous degeneracy in flexural rigidity and input delay in beam systems, which complicates stabilization.

Method: Construct non-standard energy space for well-posedness using Lumer-Phillips theorem, and design novel Lyapunov functional with integral terms for delay and weighting functions for degenerate dynamics.

Result: Proved uniform exponential decay for closed-loop system with precise decay rate estimate independent of time delay.

Conclusion: Significant extension to stability theory for complex distributed parameter systems with degenerate and delayed dynamics.

Abstract: We investigate the global exponential stabilization of a degenerate
Euler-Bernoulli beam system subject to axial loading and time-delay boundary
input. The core challenge lies in the simultaneous presence of degeneracy of
flexural rigidity and input delay. We address the well-posedness of the problem
by constructing a non-standard energy space and proving the existence of a
$C_0$-semigroup of contractions using L\"umer-Phillips theorem. For
stabilization, we construct a novel Lyapunov functional incorporating integral
terms specially designed for the delay and weighting functions adapted to the
degenerate dynamics, with which we demonstrate the uniform exponential decay
for the closed-loop system and derive a precise decay rate estimate independent
of the time delay. This work provides a significant extension to the stability
theory for complex distributed parameter systems.

</details>


### [23] [Regularization for the Schrödinger equation with rough potential: one-dimensional case](https://arxiv.org/abs/2510.25540)
*Ruobing Bai,Yajie Lian,Yifei Wu*

Main category: math.AP

TL;DR: This paper provides a complete classification of regularity mechanisms for the 1D Schrödinger equation with rough spatial potentials, establishing sharp regularity results for solutions based on the potential's integrability properties.


<details>
  <summary>Details</summary>
Motivation: To understand the regularization mechanism of the Schrödinger equation when the spatial potential is rough (including delta potentials and |x|^{-γ}-potentials), particularly focusing on how the integrability of the potential affects solution regularity.

Method: The proof mainly uses commutator techniques, local smoothing effects, and normal form methods to analyze the regularity of solutions for different classes of rough potentials.

Result: Established three main results: 1) For η ∈ L¹+L∞, solutions are in H^{3/2-} but not necessarily H^{3/2}; 2) For η ∈ Lʳ+L∞ (1<r≤2), solutions are in H^{5/2-1/r} but not H^{5/2-1/r+}; 3) For η ∈ Lʳ+L∞ (r>2), solutions are in H² but not H²+.

Conclusion: The paper provides a complete classification of regularity mechanisms for the Schrödinger equation with rough potentials, establishing sharp regularity thresholds that depend on the integrability properties of the potential.

Abstract: In this work, we investigate the following Schr\"odinger equation with a
spatial potential
  \begin{align*}
  i\partial_t u+\partial_x^2 u+\eta u=0,
  \end{align*}
  where $\eta$ is a given spatial potential (including the delta potential and
$|x|^{-\gamma}$-potential). Our goal is to provide the regularization mechanism
of this model when the potential $\eta\in L_x^r+L_x^\infty$ is rough. In this
paper, we mainly focus on one-dimensional case and establish the following
results:
  1) When the potential $\eta \in L_x^1+L_x^\infty(\mathbb{R})$, then the
solution is in $H_x^{\frac 32-}(\mathbb{R})$; however, there exists some $\eta
\in L_x^1+L_x^\infty(\mathbb{R})$ such that the solution is not in $H_x^{\frac
32}(\mathbb{R})$;
  2) When the potential $\eta \in L_x^r+L_x^\infty(\mathbb{R})$ for $1<r\leq
2$, then the solution is in $H_x^{\frac 52-\frac 1r}(\mathbb{R})$; however,
there exists some $\eta \in L_x^r+L_x^\infty(\mathbb{R})$ such that the
solution is not in $H_x^{\frac 52-\frac 1r+}(\mathbb{R})$;
  3) When the potential $\eta \in L_x^r+L_x^\infty(\mathbb{R})$ for $r>2$, then
the solution is in $H_x^{2}(\mathbb{R})$; however, there exists some $\eta \in
L_x^r+L_x^\infty(\mathbb{R})$ such that the solution is not in
$H_x^{2+}(\mathbb{R})$.
  Hence, we provide a complete classification of the regularity mechanism. Our
proof is mainly based on the application of the commutator, local smoothing
effect and normal form method. Additionally, we also discuss, without proof,
the influence of the existence of nonlinearity on the regularity of solution.

</details>


### [24] [Regularization for the Schrödinger equation with rough potential: high-dimensional case](https://arxiv.org/abs/2510.25555)
*Ruobing Bai,Yajie Lian,Yifei Wu*

Main category: math.AP

TL;DR: This paper extends the study of Schrödinger equation regularization with rough spatial potentials to high dimensions, establishing sharp ill-posedness and optimal regularity results for different parameter ranges.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity mechanisms of Schrödinger equations with rough spatial potentials in high-dimensional settings, building on previous work in 1D and torus cases.

Method: Combines construction of counterexamples, splitting normal form method, and iterative Duhamel construction to analyze the equation's behavior with potentials in L_x^r + L_x^∞ spaces.

Result: Shows ill-posedness in H_x^γ for any γ when 1≤r<d/2, and establishes optimal regularity H_x^{γ*} with γ*=min{2+d/2-d/r, 2} when d/2≤r≤∞, except for the 2D endpoint case d=2, r=1.

Conclusion: Provides a comprehensive characterization of regularization mechanisms for Schrödinger equations with rough potentials in high dimensions, revealing the critical role of potential regularity and dimension.

Abstract: In this work, we investigate the regularization mechanisms of the
Schr\"odinger equation with a spatial potential
  $$
  i\partial_t u+\Delta u+\eta u =0,
  $$
  where $\eta$ denotes a given spatial potential. The regularity of solutions
constitutes one of the central problems in the theory of dispersive equations.
Recent works \cite{Bai-Lian-Wu-2024, M-Wu-Z24} have established the sharp
regularization mechanisms for this model in the whole space $\mathbb{R}$ and on
the torus $\mathbb{T}$, with $\eta$ being a rough potential.
  The present paper extends the line of research to the high-dimensional
setting with rough potentials $\eta \in L_x^r+L_x^{\infty}$. More precisely, we
first show that when $1\leq r <\frac d2$, there exists some $\eta \in
L_x^r+L_x^{\infty}$ such that the equation is ill-posed in $H_x^{\gamma}$ for
any $\gamma \in \mathbb{R}$. Conversely, when $\frac d2 \leq r \leq \infty$,
the expected optimal regularity is given by $$H_x^{\gamma_*}, \quad
\gamma_*=\mbox{min}\{2+\frac d2-\frac dr, 2\}.$$
  We establish a comprehensive characterization of the regularity, with the
exception of two dimensional endpoint case $d=2, r=1$. Our novel theoretical
framework combines several fundamental ingredients: the construction of
counterexamples, the proposal of splitting normal form method, and the
iterative Duhamel construction. Furthermore, we briefly discuss the effect of
the interaction between rough potentials and nonlinear terms on the regularity
of solutions.

</details>


### [25] [Three solutions with precise sign properties for Gierer-Meinhardt type system](https://arxiv.org/abs/2510.25568)
*Abdelkrim Moussaoui*

Main category: math.AP

TL;DR: Existence of three solutions for sign-coupled Gierer-Meinhardt system with Neumann boundary conditions


<details>
  <summary>Details</summary>
Motivation: To establish the existence of multiple solutions for sign-coupled Gierer-Meinhardt type systems with Neumann boundary conditions

Method: Combines sub-supersolutions method and Leray-Schauder topological degree with perturbation argument

Result: Proved existence of three solutions: two opposite constant-sign solutions and one nodal solution with synchronous sign components

Conclusion: Successfully demonstrated multiple solution existence for the sign-coupled Gierer-Meinhardt system using combined analytical methods

Abstract: We establish the existence of three solutions for sign-coupled
Gierer-Meinhardt type system with Neumann boundary conditions. Two solutions
are of opposite constant-sign while the third solution is nodal with
synchronous sign components. The approach combines sub-supersolutions method
and Leray-Schauder topological degree involving perturbation argument.

</details>


### [26] [Dissipative structure and decay rate for an inviscid non-equilibrium radiation hydrodynamics system](https://arxiv.org/abs/2510.25663)
*Corrado Lattanzio,Ramón G. Plaza,José Manuel Valdovinos*

Main category: math.AP

TL;DR: This paper analyzes a non-equilibrium radiation hydrodynamics model, proving local existence in multiple dimensions, genuine coupling only in 1D, and global existence with decay for 1D perturbations of equilibrium states.


<details>
  <summary>Details</summary>
Motivation: To study the diffusion approximation model of radiation hydrodynamics under non-equilibrium conditions where fluid and radiation temperatures differ, and establish mathematical properties like existence, coupling, and stability.

Method: Mathematical analysis including proving local existence in multiple dimensions, identifying genuine coupling using Kawashima-Shizuta framework, introducing entropy functions for non-conservative systems, symmetrization techniques, and linear decay rate analysis.

Result: Local solutions exist in multiple dimensions; only 1D model is genuinely coupled; identified entropy function works for the system; obtained linear decay rates for 1D problem; global existence and decay of perturbations in 1D.

Conclusion: The non-equilibrium radiation hydrodynamics model has well-defined mathematical properties with local existence in multiple dimensions, but only the one-dimensional case exhibits genuine coupling and global stability with decay for equilibrium perturbations.

Abstract: This paper studies the diffusion approximation, non-equilibrium model of
radiation hydrodynamics derived by Buet and Despr\'es (J. Quant. Spectrosc.
Radiat. Transf. 85 (2004), no. 3-4, 385-418). The latter describes a
non-relativistic inviscid fluid subject to a radiative field under the
non-equilibrium hypothesis, that is, when the temperature of the fluid is
different from the radiation temperature. It is shown that local solutions
exist for the general system in several space dimensions. It is also proved
that only the one-dimensional model is genuinely coupled in the sense of
Kawashima and Shizuta (Hokkaido Math. J. 14 (1985), no. 2, 249-275). A notion
of entropy function for non-conservative parabolic balance laws is also
introduced. It is shown that the entropy identified by Buet and Despr\'es is an
entropy function for the system in the latter sense. This entropy is used to
recast the one-dimensional system in terms of a new set of perturbation
variables and to symmetrize it. With the aid of genuine coupling and
symmetrization, linear decay rates are obtained for the one dimensional
problem. These estimates, combined with the local existence result, yield the
global existence and decay in time of perturbations of constant equilibrium
states in one space dimension.

</details>


### [27] [An almost-almost-Schur lemma on the 3-sphere](https://arxiv.org/abs/2510.25723)
*Tobias König,Jonas W. Peteranderl*

Main category: math.AP

TL;DR: The paper extends a previous stability result for σ₂-curvature minimization from dimensions d>4 to d=3, where the standard metric maximizes rather than minimizes total σ₂-curvature, leading to reverse Sobolev-type inequalities and quantitative versions of interpolation inequalities including the Andrews-De Lellis-Topping inequality.


<details>
  <summary>Details</summary>
Motivation: To extend the quantitative stability result for σ₂-curvature minimization from higher dimensions (d>4) to the 3-dimensional case, where the behavior differs significantly since the standard metric maximizes rather than minimizes total σ₂-curvature.

Method: The authors adapt the approach from the previous work by Frank and the second author to handle the case d=3, where the functional inequality becomes a reverse Sobolev-type inequality due to the maximizing property of the standard metric.

Result: The paper successfully extends the stability result to d=3, obtaining quantitative versions for a family of interpolation inequalities including the Andrews-De Lellis-Topping inequality on the 3-sphere, which is itself a stability result for Schur's lemma.

Conclusion: The extension to d=3 provides an "almost-almost-Schur lemma" - a stability result for the Andrews-De Lellis-Topping inequality, which is already a stability result for Schur's lemma, establishing quantitative control over metrics that nearly maximize the total σ₂-curvature in the conformal class of the standard metric.

Abstract: In a recent preprint, Frank and the second author proved that if a metric on
the sphere of dimension $d>4$ almost minimizes the total $\sigma_2$-curvature
in the conformal class of the standard metric, then it is almost the standard
metric (up to M\"obius transformations). This is achieved quantitatively in
terms of a two-term distance to the set of minimizing conformal factors. We
extend this result to the case $d=3$. While the standard metric still minimizes
the total scalar curvature for $d=3$, it maximizes the total
$\sigma_2$-curvature, which turns the related functional inequality into a
reverse Sobolev-type inequality. As a corollary of our result, we obtain
quantitative versions for a family of interpolation inequalities including the
Andrews--De Lellis--Topping inequality on the $3$-sphere. The latter is itself
a stability result for the well-known Schur lemma and is therefore called
almost-Schur lemma. This makes our stability result an almost-almost-Schur
lemma.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [28] [Discovery of Hyperelastic Constitutive Laws from Experimental Data with EUCLID](https://arxiv.org/abs/2510.24747)
*Arefeh Abbasi,Maurizio Ricci,Pietro Carrara,Moritz Flaschel,Siddhant Kumar,Sonia Marfia,Laura De Lorenzis*

Main category: physics.comp-ph

TL;DR: EUCLID framework for automated constitutive law discovery is evaluated on experimental rubber data, comparing traditional parameter identification with automated model selection and analyzing performance across different data types and geometries.


<details>
  <summary>Details</summary>
Motivation: To assess the performance of EUCLID framework on real experimental data and compare it with conventional constitutive law identification methods using both global and local measurements.

Method: Mechanical tests on natural rubber specimens with varying geometries, collecting force-elongation data and full-field displacement measurements. Comparison between conventional parameter identification in pre-selected models and EUCLID's automated model discovery pipeline.

Result: Analysis of predictive accuracy, generalization to unseen geometries, experimental noise quantification, material state space coverage, and relative performance between different datasets and methods.

Conclusion: Comprehensive evaluation of EUCLID's automated constitutive law discovery capabilities compared to traditional approaches using experimental rubber data.

Abstract: We assess the performance of EUCLID, Efficient Unsupervised Constitutive Law
Identification and Discovery, a recently proposed framework for automated
discovery of constitutive laws, on experimental data. Mechanical tests are
performed on natural rubber specimens spanning simple to complex geometries,
from which we collect both global, force elongation, and local, full-field
displacement, measurements. Using these data, we obtain constitutive laws via
two routes, the conventional identification of unknown parameters in a priori
selected material models, and EUCLID, which automates model selection and
parameter identification within a unified model-discovery pipeline. We compare
the two methodologies using global versus local data, analyze predictive
accuracy, and examine generalization to unseen geometries. Moreover, we
quantify the experimental noise, investigate the coverage of the material state
space achieved by each approach and discuss the relative performance of
different datasets and different a priori chosen models versus EUCLID.

</details>


### [29] [Breaking the Timescale Barrier: Generative Discovery of Conformational Free-Energy Landscapes and Transition Pathways](https://arxiv.org/abs/2510.24979)
*Chenyu Tang,Mayank Prakash Pandey,Cheng Giuseppe Chen,Alberto Megías,François Dehez,Christophe Chipot*

Main category: physics.comp-ph

TL;DR: Gen-COMPAS is a generative committor-guided path sampling framework that efficiently reconstructs molecular transition pathways without predefined variables, using diffusion models and committor-based filtering to rapidly converge transition ensembles.


<details>
  <summary>Details</summary>
Motivation: Molecular transitions like protein folding and allostery are biologically crucial but difficult to simulate due to their rarity, pushing them beyond standard molecular dynamics while enhanced-sampling methods are costly and often biased by arbitrary variables.

Method: Gen-COMPAS couples a generative diffusion model that produces physically realistic intermediates with committor-based filtering to pinpoint transition states, enabling short unbiased simulations from these intermediates to rapidly yield full transition-path ensembles.

Result: The framework converges transition-path ensembles within nanoseconds, orders of magnitude faster than conventional methods, successfully retrieving committors, transition states, and free-energy landscapes for systems ranging from miniproteins to mitochondrial carriers.

Conclusion: Gen-COMPAS unites machine learning and molecular dynamics to provide broad mechanistic and practical insight into molecular transitions, offering an efficient alternative to traditional sampling methods.

Abstract: Molecular transitions -- such as protein folding, allostery, and membrane
transport -- are central to biology yet remain notoriously difficult to
simulate. Their intrinsic rarity pushes them beyond reach of standard molecular
dynamics, while enhanced-sampling methods are costly and often depend on
arbitrary variables that bias outcomes. We introduce Gen-COMPAS, a generative
committor-guided path sampling framework that reconstructs transition pathways
without predefined variables and at a fraction of the cost. Gen-COMPAS couples
a generative diffusion model, which produces physically realistic
intermediates, with committor-based filtering to pinpoint transition states.
Short unbiased simulations from these intermediates rapidly yield full
transition-path ensembles that converge within nanoseconds, where conventional
methods require orders of magnitude more sampling. Applied to systems from a
miniprotein to a ribose-binding protein to a mitochondrial carrier, Gen-COMPAS
retrieves committors, transition states, and free-energy landscapes
efficiently, uniting machine learning and molecular dynamics for broad
mechanistic and practical insight.

</details>


### [30] [Six-Dimensional Movable Antenna Enabled Wideband THz Communications](https://arxiv.org/abs/2510.25088)
*Wencai Yan,Wanming Hao,Yajun Fan,Yabo Guo,Qingqing Wu,Xingwang Li*

Main category: physics.comp-ph

TL;DR: The paper proposes a 6D movable antenna system for wideband terahertz communication that optimizes 3D positions and rotations of antenna surfaces to mitigate beam squint effects, using an alternating optimization algorithm for joint position/rotation and hybrid beamforming optimization.


<details>
  <summary>Details</summary>
Motivation: To address beam squint effects in wideband terahertz communication systems without using costly true-time-delay devices, by leveraging flexible 6D movable antenna surfaces.

Method: An alternating optimization algorithm that decomposes the sum-rate maximization problem into three subproblems: hybrid analog/digital beamforming optimization using semidefinite relaxation-based alternating minimization, and 6DMA surface position/rotation optimization using feasible gradient descent.

Result: Numerical results show superior performance compared to conventional fixed-position antenna architectures, demonstrating effective mitigation of beam squint effects.

Conclusion: The proposed 6D movable antenna system with joint optimization of positions, rotations, and hybrid beamforming provides an effective solution for beam squint mitigation in wideband terahertz communications without expensive hardware.

Abstract: In this paper, we investigate a six-dimensional movable antenna
(6DMA)-enabled wideband terahertz (THz) communication system with sub-connected
hybrid beamforming architecture at the base station (BS). In particular, the
three-dimensional (3D) position and 3D rotation of each 6DMA surface can be
flexibly reconfigured to mitigate the beam squint effects instead of
introducing costly true-time-delay devices. We first analyze the normalized
array gain in the 6DMA-enabled wideband THz systems based on the beam squint
effects. Then, we formulate a sum-rate maximization problem via jointly
optimizing 3D positions, 3D rotations, and hybrid analog/digital beamforming.
To solve the non-convex problem, an alternating optimization algorithm is
developed that decomposes the original problem into three subproblems, which
are solved alternately. Specifically, given the positions and rotations of 6DMA
surfaces, we first reformulate the objective function and design a semidefinite
relaxation-based alternating minimization scheme to obtain the hybrid
analog/digital beamforming. Then, the positions and rotations of the 6DMA
surfaces are further optimized through a feasible gradient descent procedure.
The final solutions are obtained by repeating the above procedure until
convergence. Numerical results demonstrate the superior performance of the
proposed scheme compared with conventional fixed-position antenna
architectures.

</details>


### [31] [Bayesian MINFLUX localization microscopy](https://arxiv.org/abs/2510.25654)
*Steffen Schultze,Helmut Grubmüller*

Main category: physics.comp-ph

TL;DR: A Bayesian approach for MINFLUX microscopy that optimizes scanning patterns to achieve 1 nm resolution with 4x fewer photons than current heuristic methods.


<details>
  <summary>Details</summary>
Motivation: Current MINFLUX microscopy uses heuristic-based scanning patterns that may be suboptimal for achieving maximum resolution with minimal photon detection or exposures.

Method: Developed a rigorous Bayesian framework that provides maximal resolutions from either minimal detected photons or minimal exposures, replacing heuristic scanning patterns.

Result: Simulated localization runs estimate this approach reduces the number of photons required for 1 nm resolution by a factor of about four compared to current methods.

Conclusion: The Bayesian approach significantly improves MINFLUX microscopy efficiency by reducing photon requirements while maintaining nanometer precision.

Abstract: MINFLUX microscopy allows for localization of fluorophores with nanometer
precision using targeted scanning with an illumination profile with a minimum.
However, current scanning patterns and the overall procedure are based on
heuristics, and may therefore be suboptimal. Here we present a rigorous
Bayesian that offers maximal resolutions from either minimal detected photons
or minimal exposures. We estimate using simulated localization runs that this
approach should reduce the number of photons required for 1 nm resolution by a
factor of about four.

</details>


### [32] [Optical excitations in nanographenes from the Bethe-Salpeter equation and time-dependent density functional theory: absorption spectra and spatial descriptors](https://arxiv.org/abs/2510.25658)
*Maximilian Graml,Jan Wilhelm*

Main category: physics.comp-ph

TL;DR: Implementation and validation of GW-BSE method in CP2K code for accurate calculation of excitation energies and optical spectra in molecules and nanostructures, with excellent agreement to reference data and experiments.


<details>
  <summary>Details</summary>
Motivation: To provide an accurate many-body approach for calculating electronic excitations and optical spectra in molecular systems and nanostructures, addressing limitations of time-dependent density functional theory.

Method: Implemented GW-BSE formalism in CP2K code and validated using standard organic molecular test set, then applied to study optical spectra of nanographenes of increasing length.

Result: Excellent agreement with reference data (mean absolute error < 3 meV) and experimental nanographene spectra. Excitation size converges to ~7.6 Å with increasing nanographene length. TD-DFT with various functionals fails to reproduce both excitation size and optical spectra.

Conclusion: GW-BSE provides superior accuracy for describing electronic excitations in nanostructures compared to TD-DFT, highlighting the necessity of many-body methods for accurate optical property calculations.

Abstract: The GW plus Bethe-Salpeter equation (GW-BSE) formalism is a well-established
approach for calculating excitation energies and optical spectra of molecules,
nanostructures, and crystalline materials. We implement GW-BSE in the CP2K code
and validate the implementation for a standard organic molecular test set,
obtaining excellent agreement with reference data, with a mean absolute error
in excitation energies below 3 meV. We then study optical spectra of
nanographenes of increasing length, showing excellent agreement with
experiment. We further compute the size of the excitation of the lowest
optically active excitation which converges to about 7.6 $\r{A}$ with
increasing length. Comparison with time-dependent density functional theory
using functionals of varying exact-exchange fraction shows that none reproduce
both the size of the excitation and optical spectra of GW-BSE, underscoring the
need for many-body methods for accurate description of electronic excitations
in nanostructures.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [33] [Cold atmospheric microplasma jet-water interactions: physicochemical analysis and growth effects in flowering plants](https://arxiv.org/abs/2510.24771)
*Syon Bhattacharjee,Deepika Behmani,Sudeep Bhattacharjee*

Main category: physics.plasm-ph

TL;DR: A helium-air micro-plasma jet generates plasma-activated water (PAW) that significantly enhances Chrysanthemum sapling growth compared to regular water.


<details>
  <summary>Details</summary>
Motivation: To study how plasma-activated water, produced by cold atmospheric pressure plasma jets, can enhance plant growth for sustainable agriculture applications.

Method: Used a helium-air micro-plasma jet to generate PAW, analyzed its physicochemical properties (pH, conductivity, ORP, TDS), and conducted comparative growth experiments on Chrysanthemum saplings over two weeks.

Result: PAW-treated saplings showed significantly greater height (10.2 cm) and soil fertility (2580 μS/cm) compared to DI water or tap water. Optimal PAW was produced with 12 ml DI water treated for 40 minutes.

Conclusion: PAW has strong potential for sustainable enhancement of flowering plant growth, with the helium-air mixture providing more chemically active discharge that favors nitrate generation.

Abstract: Cold atmospheric pressure plasma jets (APPJs) are non-equilibrium plasmas,
that are capable of producing reactive oxygen and nitrogen species (RONS) at
near-room temperature. Their interaction with water leads to the formation of
plasma-activated water (PAW), whose chemical activity depends on discharge
conditions. In this work, a helium-air (14:1) micro-plasma jet operated in a
ring-to-ring electrode configuration is used to generate PAW and study its
influence on the growth of Chrysanthemum saplings. Optical emission
spectroscopy (OES) confirms the presence of $N_2$ bands and He lines, with the
He-air mixture providing more chemically active discharge (in terms of favoring
the generation of nitrates in PAW) as compared to pure helium. The
physicochemical characteristics of PAW such as pH, electrical conductivity
(EC), oxidation-reduction potential (ORP), and total dissolved solids (TDS) are
analyzed as a function of plasma treatment time and water volume. The optimum
condition for PAW generation is found to be 12 ml of de-ionized (DI) water
treated for 40 minutes, which yields the highest ORP and nitrate concentration
with a reduced pH. Comparative growth experiments over two weeks show that
PAW-treated Chrysanthemum saplings exhibit significantly greater height (10.2
cm) and soil fertility (2580 $\mu$S/cm) than those watered with same amount of
DI water or tap water. The results highlight the potential of PAW for
sustainable enhancement of growth of flowering plants.

</details>


### [34] [Magnetic Field Line Chaos, Cantori, and Turnstiles in Toroidal Plasmas](https://arxiv.org/abs/2510.25047)
*Allen H Boozer*

Main category: physics.plasm-ph

TL;DR: The paper introduces magnetic field line chaos, cantori, and turnstiles - concepts important for tokamak disruptions, runaway electron damage, stellarator divertors, and electromagnetic corrections to micro-instabilities.


<details>
  <summary>Details</summary>
Motivation: These concepts are not well known despite being fundamental to important phenomena in fusion physics including tokamak disruptions, runaway electron damage, stellarator non-resonant divertors, and electromagnetic corrections to micro-instabilities.

Method: The paper defines these concepts (magnetic field line chaos, cantori, and turnstiles) and discusses applications that illustrate their importance.

Result: The paper presents definitions and applications of these concepts to demonstrate their significance in fusion physics.

Conclusion: Understanding magnetic field line chaos, cantori, and turnstiles is crucial for addressing key challenges in fusion energy research and plasma physics.

Abstract: Although magnetic field line chaos, cantori, and turnstiles underlie the
physics of tokamak disruptions, runaway electron damage, stellarator
non-resonant divertors, and the most important electromagnetic correction to
what are called electrostatic micro-instabilities, these concepts are not well
known. These concepts will be defined and applications that illustrate their
importance will be discussed.

</details>


### [35] [Robust direct laser acceleration of electrons with flying-focus laser pulses](https://arxiv.org/abs/2510.25376)
*Talia Meir,Kale Weichman,Alexey Arefiev,John P. Palastro,Ishay Pomerantz*

Main category: physics.plasm-ph

TL;DR: Superluminal flying-focus pulses (FFPs) enhance direct laser acceleration by mitigating nonlinear propagation effects, producing 80x more high-energy electrons and tripling x-ray yield compared to Gaussian pulses.


<details>
  <summary>Details</summary>
Motivation: Direct laser acceleration in high-density plasma suffers from nonlinear propagation effects like filamentation that disrupt electron acceleration, limiting electron energy and x-ray generation.

Method: Used 3D particle-in-cell simulations to compare superluminal flying-focus pulses with Gaussian pulses of equal energy (1 J) and intensity (2×10²⁰ W/cm²) in laser-plasma interactions.

Result: FFPs produced 80x more electrons above 100 MeV, increased electron cutoff energy by 20%, tripled high-energy x-ray yield, and improved x-ray collimation compared to Gaussian pulses.

Conclusion: Spatiotemporally structured laser pulses like FFPs provide additional control in highly nonlinear, relativistic laser-plasma interactions, overcoming limitations of conventional pulse shapes.

Abstract: Direct laser acceleration (DLA) offers a compact source of high-charge,
energetic electrons for generating secondary radiation or neutrons. While DLA
in high-density plasma optimizes the energy transfer from a laser pulse to
electrons, it exacerbates nonlinear propagation effects, such as filamentation,
that can disrupt the acceleration process. Here, we show that superluminal
flying-focus pulses (FFPs) mitigate nonlinear propagation, thereby enhancing
the number of high-energy electrons and resulting x-ray yield.
Three-dimensional particle-in-cell simulations show that, compared to a
Gaussian pulse of equal energy (1 J) and intensity (2x10^20 W/cm^2), an FFP
produces 80x more electrons above 100 MeV, increases the electron cutoff energy
by 20%, triples the high-energy x-ray yield, and improves x-ray collimation.
These results illustrate the ability of spatiotemporally structured laser
pulses to provide additional control in the highly nonlinear, relativistic
regime of laser-plasma interactions.

</details>


### [36] [Global Non-Axisymmetric Hall Instabilities in a Rotating Plasma](https://arxiv.org/abs/2510.25532)
*Alexandre Sainterme,Fatima Ebrahimi*

Main category: physics.plasm-ph

TL;DR: Study of non-axisymmetric flow-driven instabilities in Hall-MHD cylindrical plasma shows whistler and ion-cyclotron waves can extract energy from flow shear, creating two global instability branches with whistler modes growing much faster than ideal MHD modes.


<details>
  <summary>Details</summary>
Motivation: To investigate how Hall-MHD effects influence non-axisymmetric instabilities in differentially rotating cylindrical plasmas, particularly examining energy extraction from flow shear by different wave types.

Method: Analysis of non-axisymmetric instabilities in incompressible Hall-MHD model applied to differentially rotating cylindrical plasma, with examination in large-ion-skin-depth electron-MHD limit.

Result: Hall-MHD regime produces two distinct global instability branches from whistler and ion-cyclotron waves extracting flow shear energy; whistler modes grow significantly faster than ideal MHD modes; Hall effects become appreciable when ion skin depth is few percent of annulus width; global modes emerge at stronger magnetic fields than required for MHD modes.

Conclusion: Hall-MHD effects significantly enhance non-axisymmetric instabilities in rotating plasmas, with whistler modes being particularly important due to their rapid growth rates and ability to operate at higher magnetic fields than conventional MHD instabilities.

Abstract: Non-axisymmetric, flow-driven instabilities in the incompressible Hall-MHD
model are studied in a differentially rotating cylindrical plasma. It is found
that in the Hall-MHD regime, both whistler waves and ion-cyclotron waves can
extract energy from the flow shear, resulting in two distinct branches of
global instability. The non-axisymmetric whistler modes grow significantly
faster than non-axisymmetric, ideal MHD modes. A discussion of the whistler
instability mechanism is presented in the large-ion-skin-depth, `electron-MHD'
limit. It is observed that the effect of the Hall term on the non-axisymmetric
modes can be appreciable when $d_i$ is on the order of a few % of the width of
the cylindrical annulus. Distinct global modes emerge in the Hall-MHD regime at
significantly stronger magnetic fields than those required for unstable global
MHD modes.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [37] [Rethinking Pipe Flow Stability: Insights from a Meshless Global Analysis](https://arxiv.org/abs/2510.24915)
*Akash Unnikrishnan,Vinod Narayanan*

Main category: physics.flu-dyn

TL;DR: Global stability analysis using meshless framework reveals new modes in Hagen-Poiseuille flow, distinct from local analysis results, enabling study of transient energy growth without multiple simulations.


<details>
  <summary>Details</summary>
Motivation: Despite experimental evidence of turbulence in Hagen-Poiseuille flow, linear stability analysis has failed to confirm instability due to the 1/r singularity at the pipe center.

Method: Global stability analysis using a meshless framework to avoid the singularity issues of traditional approaches.

Result: Although expected unstable modes were not recovered, a new set of modes with distinct characteristics from local stability analysis were discovered.

Conclusion: The global approach effectively captures localized instabilities and enables analysis of transient energy growth without requiring multiple simulations.

Abstract: Despite extensive experimental evidence of turbulence in Hagen Poiseuille
flow, linear stability analysis has not yet confirmed its instability. One
challenge is the singularity introduced by the term 1/r in the center of the
pipe, which complicates traditional stability approaches. In this study, we
explore a global stability analysis using a meshless framework. Although this
approach did not recover the expected unstable modes, it revealed a new set of
modes with distinct characteristics from those observed in local stability
analysis. We analyze these modes and their impact on transient energy growth,
demonstrating the effectiveness of the global approach in capturing localized
instabilities without requiring multiple simulations.

</details>


### [38] [Simulating Turbulent Wakes without the Upstream Body](https://arxiv.org/abs/2510.24922)
*Zhicheng Wang,Theo Käufer,Khemraj Shukla,Michael Triantafyllou,George Em Karniadakis*

Main category: physics.flu-dyn

TL;DR: A simplified framework for simulating 3D turbulent wakes without resolving the upstream body geometry, using prescribed inflow boundary conditions from experimental data to reconstruct entire wake dynamics.


<details>
  <summary>Details</summary>
Motivation: To reduce computational cost and complexity of turbulent wake simulations by eliminating the need to resolve the bluff body geometry that generates the wake.

Method: Solve incompressible Navier-Stokes equations in rectangular domain with inflow boundary conditions prescribed as phase-averaged or time-averaged velocity profiles from experiments/DNS, using single downstream location inflow data.

Result: Successfully reconstructed entire wake dynamics including vortex shedding and turbulence statistics for Re=500 and 5,000, with good agreement to full-body DNS and experiments, while reducing computational cost by an order of magnitude.

Conclusion: Wake dynamics are induced by near-wake profile instability rather than explicit body presence, enabling body-free simulation paradigm for reduced-complexity modeling and control of turbulent flows.

Abstract: We present a simplified framework for simulating three-dimensional turbulent
wakes without the upstream body that generates them. Instead of resolving the
geometry, the incompressible Navier Stokes equations are solved in a
rectangular domain on which the inflow boundary condition is prescribed as
either phase-averaged or time-averaged velocity profiles obtained from
experimental measurements and direct numerical simulations. Remarkably,
prescribing the inflow at a single downstream location is sufficient to
reconstruct the entire wake, including coherent vortex shedding,
Reynolds-stress distributions, and spectral content, for the two Reynolds
numbers we investigate here: Re=500 and 5,000. Comparisons with corresponding
full-body DNS and experiments show good agreement in mean velocity fields and
turbulence statistics. Our results demonstrate that the essential dynamics of
bluff-body wakes are induced by the instability of the near-wake profile, and
do not require the explicit presence of the bluff body. This body-free
simulation paradigm enables physically interpretable wake reconstruction from
mean profiles that can be easily obtained from measurements or simple 2D
simulations. Our approach reduces the computational cost of DNS by an order of
magnitude, hence offering a new route for reduced-complexity modeling and
control of turbulent separated flows.

</details>


### [39] [Model-Adaptive Simulation of Hierarchical Shallow Water Moment Equations in One Dimension](https://arxiv.org/abs/2510.25351)
*Rik Verbiest,Julian Koellermeier*

Main category: physics.flu-dyn

TL;DR: This paper develops adaptive simulations using Shallow Water Moment Equations, which extend standard Shallow Water Equations to handle vertically changing velocity profiles. The method enables dynamic switching between high-order and low-order models based on model error estimators, achieving up to 60% speedup compared to fixed-complexity models.


<details>
  <summary>Details</summary>
Motivation: Shallow free surface flows have subdomains requiring different modeling complexities that change over time, necessitating space and time adaptivity in simulations.

Method: Proposes two coupling approaches for varying-order shallow water moment equations at boundaries: one with dynamically updated padded state variables (non-conservative) and another with fixed zero-padded variables (conservative). Uses model error estimators derived from high-order model decomposition for switching between model orders.

Result: Numerical tests of dam-break wave collision with smooth wave show accurate results with speedups up to 60% compared to non-adaptive fixed-complexity models.

Conclusion: Shallow water moment models are well-suited for adaptivity due to their hierarchical structure, and the proposed adaptive framework successfully balances accuracy and computational efficiency.

Abstract: Shallow free surface flows are often characterized by both subdomains that
require high modeling complexity and subdomains that can be sufficiently
accurately modeled with low modeling complexity. Moreover, these subdomains may
change in time as the water flows through the domain. This motivates the need
for space and time adaptivity in the simulation of shallow free surface flows.
In this paper, we develop the first adaptive simulations using the recently
developed Shallow Water Moment Equations, which are an extension of the
standard Shallow Water Equations that allow for vertically changing velocity
profiles by including additional variables and equations. The model-specific
modeling complexity of a shallow water moment model is determined by its order.
The higher the order of the model, the more variables and equations are
included in the model. Shallow water moment models are ideally suited for
adaptivity because they are hierarchical such that low-order models and
high-order models share the same structure. To enable adaptive simulations, we
propose two approaches for the coupling of the varying-order shallow water
moment equations at their boundary interfaces. The first approach dynamically
updates padded state variables but cannot be written in conservative form,
while the second approach uses fixed padded state variable values of zero and
reduces to conservative form for conservative moment equations. The switching
procedure between high-order models and low-order models is based on a new set
of model error estimators, originating from a decomposition of the high-order
models. Numerical results of the collision of a dam-break wave with a smooth
wave yield accurate results, while achieving speedups up to 60 percent compared
to a non-adaptive model with fixed modeling complexity.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [40] [Convergence analysis for an implementable scheme to solve the linear-quadratic stochastic optimal control problem with stochastic wave equation](https://arxiv.org/abs/2510.24876)
*Abhishek Chaudhary*

Main category: math.OC

TL;DR: This paper develops a computational method for stochastic linear-quadratic control of wave equations with affine multiplicative noise, using finite elements and implicit time stepping with strong convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: To solve optimal control problems for stochastic wave equations driven by affine multiplicative noise, which requires efficient computational methods with provable convergence properties.

Method: Uses stochastic Pontryagin's maximum principle to derive optimality conditions, then discretizes with conforming finite elements in space and implicit midpoint rule in time. Develops a gradient-descent algorithm with exact conditional expectation computation.

Result: Obtains strong convergence rates for the discrete state-control pair without Malliavin calculus. The gradient-descent method has computational cost proportional to spatial degrees of freedom and preserves established convergence rates.

Conclusion: The proposed method provides an efficient and scalable computational framework for stochastic wave equation control problems, validated by numerical experiments.

Abstract: We study an optimal control problem for the stochastic wave equation driven
by affine multiplicative noise, formulated as a stochastic linear-quadratic
(SLQ) problem. By applying a stochastic Pontryagin's maximum principle, we
characterize the optimal state-control pair via a coupled forward-backward SPDE
system. We propose an implementable discretization using conforming finite
elements in space and an implicit midpoint rule in time. By a new technical
approach we obtain strong convergence rates for the discrete state-control pair
without relying on Malliavin calculus. For the practical computation we develop
a gradient-descent algorithm based on artificial iterates that employs an exact
computation for the arising conditional expectations, thereby eliminating
costly Monte Carlo sampling. Consequently, each iteration has a computational
cost that is proportional to the number of spatial degrees of freedom,
producing a scalable method that preserves the established strong convergence
rates. Numerical results validate its efficiency.

</details>


### [41] [Adaptive Multilevel Newton: A Quadratically Convergent Optimization Method](https://arxiv.org/abs/2510.24967)
*Nick Tsipinakis,Panos Parpas,Matthias Voigt*

Main category: math.OC

TL;DR: An adaptive multilevel Newton-type method that automatically switches to full Newton once quadratic convergence phase is reached, achieving better performance than Newton's method, Gradient Descent, and classical multilevel Newton methods.


<details>
  <summary>Details</summary>
Motivation: Newton's method can be slower than Gradient Descent initially on strongly convex problems, and classical multilevel methods only achieve linear convergence near the minimizer like Gradient Descent.

Method: Introduces an adaptive multilevel Newton-type method with automatic switching to full Newton once the quadratic convergence phase is reached, leveraging both multilevel efficiency and Newton's quadratic convergence.

Result: Establishes local quadratic convergence for strongly convex functions with Lipschitz continuous Hessians and for self-concordant functions, with empirical confirmation. The method consistently outperforms Newton's method, Gradient Descent, and classical multilevel Newton methods.

Conclusion: Second-order methods can outperform first-order methods even when Newton's method is initially slow, demonstrating the effectiveness of the adaptive multilevel approach with automatic switching to full Newton.

Abstract: Newton's method may exhibit slower convergence than vanilla Gradient Descent
in its initial phase on strongly convex problems. Classical Newton-type
multilevel methods mitigate this but, like Gradient Descent, achieve only
linear convergence near the minimizer. We introduce an adaptive multilevel
Newton-type method with a principled automatic switch to full Newton once its
quadratic phase is reached. The local quadratic convergence for strongly convex
functions with Lipschitz continuous Hessians and for self-concordant functions
is established and confirmed empirically. Although per-iteration cost can
exceed that of classical multilevel schemes, the method is efficient and
consistently outperforms Newton's method, Gradient Descent, and the multilevel
Newton method, indicating that second-order methods can outperform first-order
methods even when Newton's method is initially slow.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [42] [Accurate Reporting of Ion Time-of-Flight during HiPIMS with Gated Front-End Mass Spectrometry](https://arxiv.org/abs/2510.24782)
*Nathan Rodkey,Jyotish Patidar,Kerstin Thorwarth,Sebastian Siol*

Main category: physics.ins-det

TL;DR: A practical method to measure ion time-of-flight in HiPIMS using synchronized gating pulses with mass spectrometry, addressing transit time calculation errors that cause nonphysical results.


<details>
  <summary>Details</summary>
Motivation: Precise time-of-flight measurements are crucial for improving film quality in HiPIMS through metal-ion acceleration, but conventional transit time calculations often lead to errors and nonphysical values.

Method: Used bipolar HiPIMS power supply to synchronize gating pulses with HIDEN EQP-300 mass spectrometer, applying +70V bias to repel ions and 5μs 0V pulses to accept them, with grounded shielding to prevent plasma interference.

Result: Successfully measured time-of-flights for Ar+, Al+, Sc+, Y+, and W+ ions, with Al+ and Ar+ measurements matching calculations from mass spectrometry flight tube equations.

Conclusion: The experimental approach provides reliable ion time-of-flight measurements in HiPIMS processes, overcoming limitations of conventional transit time calculations.

Abstract: The quality of high-power impulse magnetron sputtering (HiPIMS) deposited
films can often improve through the effective use of metal-ion acceleration,
requiring precise measurements of time-of-flight (ToF). These measurements are
commonly done using time- and energy-resolved mass spectrometry but require
careful consideration of the transit time of ions inside. The transit time is
typically calculated by considering the travel length in various parts of the
spectrometer (e.g. from orifice to detector), but errors associated with these
estimations can lead to nonphysical values in a HiPIMS process (e.g. negative
ToFs). Here we report a practical approach to determine ion ToF experimentally,
using a bipolar HiPIMS power supply to synchronize a gating pulse to the
front-end of a HIDEN Analytical EQP-300 mass spectrometer, placed at the
working distance. The ToF is measured by applying a +70 V bias to repel ions,
and a 5 us gating pulse of 0 V to accept them. To prevent interference with the
HiPIMS plasma, a grounded shield is placed in front of the mass-spec head with
a variable slit-opening (0.5-3 mm). The effectiveness of the shielding is
verified by Langmuir probe measurements, noting negligible shifts in plasma
potential for a DC sputter discharge. The gate is then synchronized to a HiPIMS
pulse and data collected at 5 us intervals by adjusting the pulse delay.
Measurements of the time-of-flights of Ar+, Al+, Sc+, Y+, and W+ ions are
presented; Al+ and Ar+ ions were also compared to ToF calculated using mass
spectrometry flight tube equations.

</details>


<div id='math.MG'></div>

# math.MG [[Back]](#toc)

### [43] [Minimizing point configurations for tensor product energies on the torus](https://arxiv.org/abs/2510.25442)
*Dmitriy Bilyk,Nicolas Nagel,Ian Ruohoniemi*

Main category: math.MG

TL;DR: Point configurations on the torus with tensor product structure minimize interaction energies for various potentials, particularly permutation sets and Latin hypercube sets with only one distance vector.


<details>
  <summary>Details</summary>
Motivation: To study energy-minimizing point configurations on the torus that arise in discrepancy theory and quasi-Monte Carlo integration, focusing on sets with tensor product structure.

Method: Analyze permutation sets on the 2D torus and Latin hypercube sets in higher dimensions, particularly those with only one distance vector. Characterize lattices and non-lattice sets with this property.

Result: Showed that point configurations with only one distance vector minimize energy for a wide range of potentials, satisfying tensor product universal optimality. Applied to Fibonacci lattices and characterized all lattices with this property.

Conclusion: Point configurations with tensor product structure and single distance vectors are energy minimizers for various potentials, with applications to discrepancy theory and quasi-Monte Carlo integration.

Abstract: We study point configurations on the torus $\mathbb T^d$ that minimize
interaction energies with tensor product structure which arise naturally in the
context of discrepancy theory and quasi-Monte Carlo integration. Permutation
sets on $\mathbb T^2$ and Latin hypercube sets in higher dimensions (i.e. sets
whose projections onto coordinate axes are equispaced points) are natural
candidates to be energy minimizers. We show that such point configurations that
have only one distance in the vector sense minimize the energy for a wide range
of potentials, in other words, such sets satisfy a tensor product version of
universal optimality. This applies, in particular, to three- and five-point
Fibonacci lattices. We also characterize all lattices with this property and
exhibit some non-lattice sets of this type. In addition, we obtain several
further structural results about global and local minimizers of tensor product
energies.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [44] [Stabilisation of hBN/SiC Heterostructures with Vacancies and Transition-Metal Atoms](https://arxiv.org/abs/2510.24952)
*Arsalan Hashemi,Nima Ghafari Cherati,Sadegh Ghaderzadeh,Yanzhou Wang,Mahdi Ghorbani-Asl,Tapio Ala-Nissila*

Main category: cond-mat.mtrl-sci

TL;DR: The paper demonstrates that removing boron atoms from hBN/SiC heterostructures converts weak van der Waals interactions into strong covalent Si-N bonds, enabling controlled single-metal atom isolation for catalytic applications.


<details>
  <summary>Details</summary>
Motivation: To understand how local structural modifications in van der Waals heterostructures can transform interlayer interactions and enable precise control over surface reactivity for single-atom catalysis.

Method: Used quantum mechanical density functional theory calculations and machine-learning-assisted molecular dynamics simulations to study hBN/SiC heterostructures with 18.77% lattice mismatch.

Result: Boron atom removal from specific sites converts vdW coupling to localized Si-N covalent bonding, allowing controlled isolation of transition-metal adatoms with demonstrated dynamical stability at finite temperatures.

Conclusion: hBN/SiC heterostructures serve as versatile platforms for atomically precise transition-metal functionalization with potential for next-generation catalytic energy-conversion technologies.

Abstract: When two-dimensional atomic layers of different materials are brought into
close proximity to form van der Waals (vdW) heterostructures, interactions
between adjacent layers significantly influence their physicochemical
properties. These effects seem particularly pronounced when the interface
exhibits local order and near-perfect structural alignment, leading to the
emergence of Moir\'e patterns. Using quantum mechanical density functional
theory calculations, we propose a prototypical bilayer heterostructure composed
of hexagonal boron nitride (hBN) and silicon carbide (SiC), characterized by a
lattice mismatch of 18.77\% between their primitive unit cells. We find that
the removal of boron atoms from specific lattice sites can convert the
interlayer interaction from weak vdW coupling to robust localized
silicon-nitrogen covalent bonding. Motivated by this, we study the binding of
transition-metal adatoms and formulate design guidelines to enhance surface
reactivity, thereby enabling the controlled isolation of single-metal atoms.
Our machine-learning-assisted molecular dynamics simulations confirm both
dynamical stability and metal anchoring feasibility at finite temperatures. Our
results suggest the hBN/SiC heterostructure as a versatile platform for
atomically precise transition-metal functionalization, having potential for
next-generation catalytic energy-conversion technologies.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [45] [Classically Prepared, Quantumly Evolved: Hybrid Algorithm for Molecular Spectra](https://arxiv.org/abs/2510.24911)
*Alessandro Santini,Stefano Barison,Filippo Vicentini*

Main category: quant-ph

TL;DR: A hybrid classical-quantum algorithm for computing dynamical correlation functions and excitation spectra in quantum systems using classical ground state preparation and short quantum evolution of sampled product states.


<details>
  <summary>Details</summary>
Motivation: To develop efficient methods for computing dynamical correlation functions and excitation spectra in many-body quantum systems, particularly molecular systems, that can leverage near-term quantum hardware capabilities.

Method: Combines classical preparation of perturbed ground state with short-time quantum evolution of product states sampled from it, then projects Hamiltonian onto effective subspace for classical simulation of long-time dynamics.

Result: Achieves high-resolution spectral reconstruction with shallow circuits and few samples, shows excellent agreement with exact diagonalization, and accesses dynamical timescales beyond classical methods.

Conclusion: The method is suitable for near-term and early fault-tolerant quantum hardware, providing an efficient approach for dynamical correlation function computation in quantum systems.

Abstract: We introduce a hybrid classical-quantum algorithm to compute dynamical
correlation functions and excitation spectra in many-body quantum systems, with
a focus on molecular systems. The method combines classical preparation of a
perturbed ground state with short-time quantum evolution of product states
sampled from it. The resulting quantum samples define an effective subspace of
the Hilbert space, onto which the Hamiltonian is projected to enable efficient
classical simulation of long-time dynamics. This subspace-based approach
achieves high-resolution spectral reconstruction using shallow circuits and few
samples. Benchmarks on molecular systems show excellent agreement with exact
diagonalization and demonstrate access to dynamical timescales beyond the reach
of purely classical methods, highlighting its suitability for near-term and
early fault-tolerant quantum hardware.

</details>


### [46] [Variational quantum computing for quantum simulation: principles, implementations, and challenges](https://arxiv.org/abs/2510.25449)
*Lucas Q. Galvão,Anna Beatriz M. de Souza,Marcelo A. Moret,Clebson Cruz*

Main category: quant-ph

TL;DR: This paper provides a comprehensive overview of variational quantum computing's role in quantum simulation, focusing on quantum data in VQAs and QML rather than classical data processing, while addressing challenges in the NISQ era.


<details>
  <summary>Details</summary>
Motivation: To advance quantum simulation by focusing on the critical role of quantum data in Variational Quantum Algorithms and Quantum Machine Learning, distinguishing from classical data processing approaches.

Method: Systematic review and critical examination of variational quantum computing principles within hybrid quantum-classical frameworks, analyzing applications across prototypical quantum simulation problems.

Result: Identifies variational quantum algorithms as promising yet problem-dependent pathways whose practicality depends on overcoming trainability and scalability challenges under noise and barren-plateau constraints.

Conclusion: This review synthesizes recent advancements and provides a focused perspective on persistent challenges and emerging opportunities in variational quantum computing for quantum simulation.

Abstract: This work presents a comprehensive overview of variational quantum computing
and their key role in advancing quantum simulation. This work explores the
simulation of quantum systems and sets itself apart from approaches centered on
classical data processing, by focusing on the critical role of quantum data in
Variational Quantum Algorithms (VQA) and Quantum Machine Learning (QML). We
systematically delineate the foundational principles of variational quantum
computing, establish their motivational and challenges context within the noisy
intermediate-scale quantum (NISQ) era, and critically examine their application
across a range of prototypical quantum simulation problems. Operating within a
hybrid quantum-classical framework, these algorithms represent a promising yet
problem-dependent pathway whose practicality remains contingent on trainability
and scalability under noise and barren-plateau constraints.This review serves
to complement and extend existing literature by synthesizing the most recent
advancements in the field and providing a focused perspective on the persistent
challenges and emerging opportunities that define the current landscape of
variational quantum computing for quantum simulation.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [47] [The Fate of Hydrogen and Helium: From Planetary Embryos to Earth- and Neptune-like Worlds](https://arxiv.org/abs/2510.24956)
*Akash Gupta,Haiyang Luo,Jie Deng,Adam Burrows*

Main category: astro-ph.EP

TL;DR: Hydrogen and helium partitioning between silicate mantles and metallic cores in Earth-to-Neptune-mass planets is quantified using first-principles methods, revealing most volatiles reside in planetary interiors rather than atmospheres.


<details>
  <summary>Details</summary>
Motivation: Chemical interactions between hydrogen, helium, silicates, and iron - key building blocks of rocky and gas-rich planets - remain poorly constrained despite their importance for planetary formation.

Method: First-principles molecular dynamics and thermodynamic integration to quantify hydrogen and helium partitioning between molten silicate mantles and metallic cores across different pressure regimes.

Result: Hydrogen becomes strongly siderophilic above ~25 GPa but weakens beyond ~200 GPa, while helium remains lithophilic but increasingly soluble in metal with pressure. Most hydrogen and helium reside in planetary interiors rather than atmospheres.

Conclusion: Volatile exchange between interiors and atmospheres influences planetary redox states, primordial envelope longevity, CHNOPS abundances, and helium-enriched atmospheres, with observational probes available through spectral lines.

Abstract: Hydrogen, helium, silicates, and iron are key building blocks of rocky and
gas-rich planets, yet their chemical interactions remain poorly constrained.
Using first-principles molecular dynamics and thermodynamic integration, we
quantify hydrogen and helium partitioning between molten silicate mantles and
metallic cores for Earth-to-Neptune-mass planets. Hydrogen becomes strongly
siderophilic above $\sim$25 GPa but weakens beyond $\sim$200 GPa, whereas
helium remains lithophilic yet increasingly soluble in metal with pressure.
Incorporating these trends into coupled structure-chemistry models suggests
that majority of hydrogen and helium reside in planetary interiors, not
atmospheres, with abundances strongly depending on planet mass. Such volatile
exchange may influence the redox states of secondary atmospheres, longevity of
primordial envelopes, predicted CHNOPS abundances, and emergence of
helium-enriched atmospheres, while He 1083 nm and H Lyman-$\alpha$ lines
provide potential probes of atmosphere-interior exchange. These findings link
atomic-scale interactions to planetary-scale observables, providing new
constraints on the origins of Earth-to-Neptune-sized worlds.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [48] [Characteristic Critical Collapse of a Yang-Mills Field With Null Infinity](https://arxiv.org/abs/2510.25534)
*Rita P. Santos,Krinio Marouda,David Hilditch*

Main category: gr-qc

TL;DR: Study of critical gravitational collapse using characteristic evolution in Bondi coordinates to analyze Yang-Mills field collapse, demonstrating universal critical phenomena with specific scaling exponents.


<details>
  <summary>Details</summary>
Motivation: To investigate critical phenomena near black hole formation threshold using global quantities like Bondi mass and news function, building on previous work on gravitational collapse.

Method: Fourth-order accurate characteristic evolution in compactified Bondi coordinates, studying Yang-Mills field collapse and extracting global quantities.

Result: Found local DSS behavior with echoing period Δ≈0.7388, global quantities show same DSS behavior, black hole mass scaling with critical exponent γ≈0.1977±0.0009, results are universal across initial data.

Conclusion: Critical collapse of Yang-Mills field exhibits universal critical phenomena with specific scaling behavior, confirming previous results and extending analysis to global quantities.

Abstract: Solutions to the Einstein equations near the threshold of black hole
formation exhibit remarkable behavior known as critical phenomena gravitational
collapse. In this work we perform characteristic evolution in compactified
Bondi coordinates in order to study the critical collapse of a Yang-Mills
field, allowing for the extraction of global quantities such as the Bondi mass
and news function. Our numerical approach is fourth-order accurate. First, we
demonstrate that the collapsing field exhibits local DSS behavior,
characterized by an echoing period of~$\Delta \simeq 0.7388$, agreeing with
previous works up to the second decimal place. We find that global quantities
such as the Bondi mass and news function display the same DSS behavior. We
furthermore show that the mass of the black holes formed during near-threshold
evolutions scales as a function of the distance to the critical parameter, with
a critical exponent of approximately~$\gamma=0.1977\pm0.0009$. Finally, our
findings indicate that these results are universal, irrespective of the initial
data.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [49] [Observations of the Relationship between Magnetic Anisotropy and Mode Composition in Low-$β$ Solar Wind Turbulence](https://arxiv.org/abs/2510.25636)
*Siqi Zhao,Huirong Yan,Terry Z. Liu,Chuanpeng Hou*

Main category: astro-ph.SR

TL;DR: The paper investigates magnetic anisotropy in compressible MHD turbulence in solar wind, revealing that anisotropy is mode-dependent with quasi-parallel (slab) energy containing both Alfvén and compressible modes, while quasi-perpendicular (2D) energy is almost purely Alfvénic.


<details>
  <summary>Details</summary>
Motivation: To understand the exact form and implications of turbulence anisotropy for magnetic topology and energy transfer, as current understanding remains unclear despite recent progress.

Method: Using Cluster spacecraft measurements to analyze small-amplitude fluctuations in compressible MHD turbulence within low-β solar wind, decomposing fluctuations into Alfvén and compressible modes.

Result: Anisotropy is strongly mode dependent: quasi-parallel (slab) energy contains both Alfvén and compressible modes, whereas quasi-perpendicular (2D) energy is almost purely Alfvénic, linked to collisionless damping of compressible modes.

Conclusion: These findings explain the physical origin of the long-standing 'slab+2D' empirical model and provide new insights into turbulence cascade across full three-dimensional wavevector space.

Abstract: Turbulence is a ubiquitous process that transfers energy across many spatial
and temporal scales, thereby influencing particle transport and heating. Recent
progress has improved our understanding of the anisotropy of turbulence with
respect to the mean magnetic field; however, its exact form and implications
for magnetic topology and energy transfer remain unclear. In this Letter, we
investigate the nature of magnetic anisotropy in compressible
magnetohydrodynamic (MHD) turbulence within low-$\beta$ solar wind using
Cluster spacecraft measurements. By decomposing small-amplitude fluctuations
into Alfv\'en and compressible modes, we reveal that the anisotropy is strongly
mode dependent: quasi-parallel (`slab') energy contains both Alfv\'en and
compressible modes, whereas quasi-perpendicular (`two-dimensional'; 2D) energy
is almost purely Alfv\'enic, a feature closely linked to collisionless damping
of compressible modes. These findings elucidate the physical origin of the
long-standing `slab+2D' empirical model and offer a new perspective on the
turbulence cascade across the full three-dimensional wavevector space.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [50] [Machine Learning the Entropy to Estimate Free Energy Differences without Sampling Transitions](https://arxiv.org/abs/2510.24930)
*Yamin Ben-Shimon,Barak Hirshberg,Yohai Bar-Sinai*

Main category: cond-mat.soft

TL;DR: A deep learning approach to estimate free energy differences between phases using only short simulations of each phase separately, avoiding the need to sample difficult transitions.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for calculating free energy differences require sampling transitions between metastable states, which is computationally challenging for systems with large free energy barriers and requires prior knowledge of slow transition modes.

Method: Uses a deep learning model to estimate entropy and free energy of each metastable state from short simulations of individual phases, without needing to simulate the transition pathway.

Result: Achieved state-of-the-art precision in estimating melting transition temperatures for Na and Al metals, without requiring prior information or simulation of the transition pathway.

Conclusion: The method provides an efficient alternative to traditional enhanced sampling approaches by enabling free energy calculations from short simulations of individual phases using deep learning entropy estimation.

Abstract: Thermodynamic phase transitions, a central concept in physics and chemistry,
are typically controlled by an interplay of enthalpic and entropic
contributions. In most cases, the estimation of the enthalpy in simulations is
straightforward but evaluating the entropy is notoriously hard. As a result, it
is common to induce transitions between the metastable states and estimate
their relative occupancies, from which the free energy difference can be
inferred. However, for systems with large free energy barriers, sampling these
transitions is a significant computational challenge. Dedicated enhanced
sampling algorithms require significant prior knowledge of the slow modes
governing the transition, which is typically unavailable. We present an
alternative approach, which only uses short simulations of each phase
separately. We achieve this by employing a recently developed deep learning
model for estimating the entropy and hence the free energy of each metastable
state. We benchmark our approach calculating the free energies of crystalline
and liquid metals. Our method features state-of-the-art precision in estimating
the melting transition temperature in Na and Al without requiring any prior
information or simulation of the transition pathway itself.

</details>


### [51] [Energy-Conserving Contact Dynamics of Nonspherical Rigid-Body Particles](https://arxiv.org/abs/2510.24945)
*Haoyuan Shi,Christopher J. Mundy,Gregory K. Schenter,Jaehun Chun*

Main category: cond-mat.soft

TL;DR: Energy-conserving contact dynamics framework for arbitrary convex rigid-body particles in 2D and 3D that prevents overlap and enables continuous force evaluation.


<details>
  <summary>Details</summary>
Motivation: Understanding contact dynamics of nonspherical particles is crucial for modeling colloidal and granular systems where shape anisotropy affects structural organization and transport properties.

Method: Introduces vertex-boundary interactions in 2D with vertex-surface and edge-edge detection in 3D, integrating continuous force evaluation while preventing particle overlap and conserving total energy.

Result: Simulations of polygonal and polyhedral particles confirm framework stability and demonstrate capability to capture packing behavior, anisotropic diffusion, and equations of state.

Conclusion: The framework establishes a robust foundation for investigating nonequilibrium dynamics of complex nonspherical particle systems, with applications in colloidal self-assembly, granular flow, and hydrodynamics.

Abstract: Understanding the contact dynamics of nonspherical particles beyond the
microscale is crucial for accurately modeling colloidal and granular systems,
where shape anisotropy dictates structural organization and transport
properties. In this paper, we introduce an energy-conserving contact dynamics
framework for arbitrary convex rigid-body particles, integrating
vertex-boundary interactions in 2D with vertex-surface and edge-edge detection
in 3D. This formulation enables continuous force evaluation and strictly
prevents particle overlap while conserving total energy during translational
and rotational motion. Simulations of polygonal and polyhedral particles
confirm the framework's stability and demonstrate its capability to capture
packing behavior, anisotropic diffusion, and equations of state. The framework
establishes a robust and extensible foundation for investigating the
nonequilibrium dynamics of complex nonspherical particle systems, with
potential applications in colloidal self-assembly, granular flow, and
hydrodynamics.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution Network for an Accurate Ice Sheet Emulator](https://arxiv.org/abs/2510.24926)
*Zesheng Liu,YoungHyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: KAN-GCN is an ice sheet modeling emulator that combines Kolmogorov-Arnold Networks with Graph Convolution Networks, achieving better accuracy and efficiency than pure GCN and MLP-GCN baselines.


<details>
  <summary>Details</summary>
Motivation: To create a fast and accurate emulator for ice sheet modeling that improves feature conditioning and nonlinear encoding without increasing computational complexity.

Method: Places a KAN as a feature-wise calibrator before GCNs, applying learnable one-dimensional warps and linear mixing to improve feature conditioning. Replaces one edge-wise message-passing layer with a node-wise transform.

Result: KAN-GCN matches or exceeds baseline accuracy across 2- to 5-layer architectures. Improves inference throughput on coarser meshes with only modest cost on finest meshes. Tested on 36 melting-rate simulations for Pine Island Glacier.

Conclusion: KAN-first designs offer favorable accuracy vs. efficiency trade-off for large transient scenario sweeps in ice sheet modeling.

Abstract: We introduce KAN-GCN, a fast and accurate emulator for ice sheet modeling
that places a Kolmogorov-Arnold Network (KAN) as a feature-wise calibrator
before graph convolution networks (GCNs). The KAN front end applies learnable
one-dimensional warps and a linear mixing step, improving feature conditioning
and nonlinear encoding without increasing message-passing depth. We employ this
architecture to improve the performance of emulators for numerical ice sheet
models. Our emulator is trained and tested using 36 melting-rate simulations
with 3 mesh-size settings for Pine Island Glacier, Antarctica. Across 2- to
5-layer architectures, KAN-GCN matches or exceeds the accuracy of pure GCN and
MLP-GCN baselines. Despite a small parameter overhead, KAN-GCN improves
inference throughput on coarser meshes by replacing one edge-wise
message-passing layer with a node-wise transform; only the finest mesh shows a
modest cost. Overall, KAN-first designs offer a favorable accuracy vs.
efficiency trade-off for large transient scenario sweeps.

</details>


### [53] [Learning Low Rank Neural Representations of Hyperbolic Wave Dynamics from Data](https://arxiv.org/abs/2510.25123)
*Woojin Cho,Kookjin Lee,Noseong Park,Donsub Rim,Gerrit Welper*

Main category: cs.LG

TL;DR: A data-driven dimensionality reduction method using low rank neural representation (LRNR) for hyperbolic wave propagation, which learns efficient low-dimensional representations from data and enables interpretable physical feature decomposition.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient dimensionality reduction method specifically designed for physics-based data representing hyperbolic wave propagation, motivated by theoretical results proving the existence of efficient representations for this wave class.

Method: Utilizes a specialized neural network architecture called low rank neural representation (LRNR) within a hypernetwork framework, combining deep learning techniques to learn efficient low-dimensional representations directly from data.

Result: The method successfully learns low rank tensor representations that naturally arise in trained LRNRs, revealing a new decomposition of wave propagation where each decomposed mode corresponds to interpretable physical features. It also enables efficient inference via compression.

Conclusion: The LRNR architecture provides an effective data-driven approach for dimensionality reduction in hyperbolic wave propagation problems, offering both interpretable physical insights and efficient computational performance through compression capabilities.

Abstract: We present a data-driven dimensionality reduction method that is well-suited
for physics-based data representing hyperbolic wave propagation. The method
utilizes a specialized neural network architecture called low rank neural
representation (LRNR) inside a hypernetwork framework. The architecture is
motivated by theoretical results that rigorously prove the existence of
efficient representations for this wave class. We illustrate through archetypal
examples that such an efficient low-dimensional representation of propagating
waves can be learned directly from data through a combination of deep learning
techniques. We observe that a low rank tensor representation arises naturally
in the trained LRNRs, and that this reveals a new decomposition of wave
propagation where each decomposed mode corresponds to interpretable physical
features. Furthermore, we demonstrate that the LRNR architecture enables
efficient inference via a compression scheme, which is a potentially important
feature when deploying LRNRs in demanding performance regimes.

</details>


### [54] [A Deep Learning Framework for Multi-Operator Learning: Architectures and Approximation Theory](https://arxiv.org/abs/2510.25379)
*Adrien Weihs,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer*

Main category: cs.LG

TL;DR: This paper studies neural operator learning for approximating mappings between function spaces, distinguishing between multiple operator learning (single network representing continuum of operators) and learning several distinct operators. It introduces new architectures with universal approximation guarantees and scaling laws.


<details>
  <summary>Details</summary>
Motivation: Scientific applications require approximating mappings between function spaces (operators), but most machine learning focuses on finite-dimensional spaces. There's a need for scalable neural operator learning methods that can handle multiple operators efficiently.

Method: Introduced two new architectures (MNO and MONet) for multiple operator learning with universal approximation guarantees. Developed framework for learning several distinct operators with balanced architectural complexity. Provided theoretical analysis in continuous, integrable, and Lipschitz operator settings.

Result: Established universal approximation results and derived explicit scaling laws for network size growth relative to target accuracy. Empirical experiments on parametric PDE benchmarks demonstrated strong expressive power and efficiency of proposed architectures.

Conclusion: This work provides a unified theoretical and practical foundation for scalable neural operator learning across multiple operators, with both theoretical guarantees and empirical validation on scientific applications.

Abstract: While many problems in machine learning focus on learning mappings between
finite-dimensional spaces, scientific applications require approximating
mappings between function spaces, i.e., operators. We study the problem of
learning collections of operators and provide both theoretical and empirical
advances. We distinguish between two regimes: (i) multiple operator learning,
where a single network represents a continuum of operators parameterized by a
parametric function, and (ii) learning several distinct single operators, where
each operator is learned independently. For the multiple operator case, we
introduce two new architectures, $\mathrm{MNO}$ and $\mathrm{MONet}$, and
establish universal approximation results in three settings: continuous,
integrable, or Lipschitz operators. For the latter, we further derive explicit
scaling laws that quantify how the network size must grow to achieve a target
approximation accuracy. For learning several single operators, we develop a
framework for balancing architectural complexity across subnetworks and show
how approximation order determines computational efficiency. Empirical
experiments on parametric PDE benchmarks confirm the strong expressive power
and efficiency of the proposed architectures. Overall, this work establishes a
unified theoretical and practical foundation for scalable neural operator
learning across multiple operators.

</details>


### [55] [Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics](https://arxiv.org/abs/2510.25683)
*Alessandro Lucchetti,Francesco Cadini,Marco Giglio,Luca Lomazzi*

Main category: cs.LG

TL;DR: GNSS is a Graph Neural Network framework for surrogate modeling of dynamic structural problems, achieving accurate physics reproduction and significant speedups over finite element methods.


<details>
  <summary>Details</summary>
Motivation: Address the gap in applying GNNs to structural problems, particularly dynamic cases, where existing GNNs fail to converge or deliver meaningful predictions.

Method: Uses encode-process-decode GNN paradigm with three key features: node kinematics in local frames, sign-aware regression loss, and wavelength-informed connectivity radius.

Result: GNSS accurately reproduces physics over hundreds of timesteps, generalizes to unseen loading conditions, and achieves substantial inference speedups while preserving spatial and temporal fidelity.

Conclusion: Locality-preserving GNNs with physics-consistent update rules are a competitive alternative for dynamic, wave-dominated structural simulations.

Abstract: Graph Neural Networks (GNNs) have recently been explored as surrogate models
for numerical simulations. While their applications in computational fluid
dynamics have been investigated, little attention has been given to structural
problems, especially for dynamic cases. To address this gap, we introduce the
Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate
modeling of dynamic structural problems.
  GNSS follows the encode-process-decode paradigm typical of GNN-based machine
learning models, and its design makes it particularly suited for dynamic
simulations thanks to three key features: (i) expressing node kinematics in
node-fixed local frames, which avoids catastrophic cancellation in
finite-difference velocities; (ii) employing a sign-aware regression loss,
which reduces phase errors in long rollouts; and (iii) using a
wavelength-informed connectivity radius, which optimizes graph construction.
  We evaluate GNSS on a case study involving a beam excited by a 50kHz
Hanning-modulated pulse. The results show that GNSS accurately reproduces the
physics of the problem over hundreds of timesteps and generalizes to unseen
loading conditions, where existing GNNs fail to converge or deliver meaningful
predictions.
  Compared with explicit finite element baselines, GNSS achieves substantial
inference speedups while preserving spatial and temporal fidelity. These
findings demonstrate that locality-preserving GNNs with physics-consistent
update rules are a competitive alternative for dynamic, wave-dominated
structural simulations.

</details>


### [56] [LieSolver: A PDE-constrained solver for IBVPs using Lie symmetries](https://arxiv.org/abs/2510.25731)
*René P. Klausen,Ivan Timofeev,Johannes Frank,Jonas Naujoks,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: LieSolver uses Lie symmetries to enforce PDE constraints exactly, enabling efficient and accurate solutions to initial-boundary value problems with improved convergence and rigorous error estimation.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient and reliable method for solving PDE-constrained problems that improves upon physics-informed neural networks by leveraging physical laws through symmetry transformations.

Method: Uses Lie symmetry transformations to enforce PDE constraints exactly by construction, learns solutions from initial and boundary data, and enables rigorous error estimation for well-posed problems.

Result: LieSolver is faster and more accurate than PINNs for linear homogeneous PDEs with various initial conditions, yielding compact models that facilitate efficient optimization.

Conclusion: The method improves both computational efficiency and prediction reliability for PDE-constrained problems by exactly incorporating physical laws through symmetry transformations.

Abstract: We introduce a method for efficiently solving initial-boundary value problems
(IBVPs) that uses Lie symmetries to enforce the associated partial differential
equation (PDE) exactly by construction. By leveraging symmetry transformations,
the model inherently incorporates the physical laws and learns solutions from
initial and boundary data. As a result, the loss directly measures the model's
accuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our
method enables rigorous error estimation. The approach yields compact models,
facilitating an efficient optimization. We implement LieSolver and demonstrate
its application to linear homogeneous PDEs with a range of initial conditions,
showing that it is faster and more accurate than physics-informed neural
networks (PINNs). Overall, our method improves both computational efficiency
and the reliability of predictions for PDE-constrained problems.

</details>


### [57] [Perturbation Bounds for Low-Rank Inverse Approximations under Noise](https://arxiv.org/abs/2510.25571)
*Phuc Tran,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: This paper provides sharp non-asymptotic perturbation bounds for low-rank pseudoinverses under noise, improving over classical bounds by up to √n factor.


<details>
  <summary>Details</summary>
Motivation: Low-rank pseudoinverses are widely used but their robustness to noise in real-world applications remains poorly understood, especially for spectral-norm error.

Method: The authors use contour integral techniques applied to the non-entire function f(z)=1/z to derive perturbation bounds that account for eigengap, spectral decay, and noise alignment with low-curvature directions.

Result: The derived bounds closely track true perturbation error across various matrices, while classical bounds significantly overpredict. The analysis reveals how error scales with matrix properties and noise characteristics.

Conclusion: The findings provide practical, spectrum-aware guarantees for low-rank inverse approximations in noisy computational environments, with bounds that are up to √n times better than naive adaptations of classical results.

Abstract: Low-rank pseudoinverses are widely used to approximate matrix inverses in
scalable machine learning, optimization, and scientific computing. However,
real-world matrices are often observed with noise, arising from sampling,
sketching, and quantization. The spectral-norm robustness of low-rank inverse
approximations remains poorly understood. We systematically study the
spectral-norm error $\| (\tilde{A}^{-1})_p - A_p^{-1} \|$ for an $n\times n$
symmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\(p\)
approximation of $A^{-1}$, and $\tilde{A} = A + E$ is a noisy observation.
Under mild assumptions on the noise, we derive sharp non-asymptotic
perturbation bounds that reveal how the error scales with the eigengap,
spectral decay, and noise alignment with low-curvature directions of $A$. Our
analysis introduces a novel application of contour integral techniques to the
\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over
naive adaptations of classical full-inverse bounds by up to a factor of
$\sqrt{n}$. Empirically, our bounds closely track the true perturbation error
across a variety of real-world and synthetic matrices, while estimates based on
classical results tend to significantly overpredict. These findings offer
practical, spectrum-aware guarantees for low-rank inverse approximations in
noisy computational environments.

</details>


### [58] [Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy](https://arxiv.org/abs/2510.25670)
*Phuc Tran,Nisheeth K. Vishnoi,Van H. Vu*

Main category: cs.LG

TL;DR: This paper establishes new high-probability spectral-norm perturbation bounds for symmetric matrices, refining classical results and providing improved utility guarantees for differentially private PCA.


<details>
  <summary>Details</summary>
Motivation: To understand how noise affects low-rank approximations in spectral norm, especially for differentially private settings, since prior work using Frobenius norm can misestimate true subspace distortion.

Method: Uses a novel contour bootstrapping method from complex analysis to derive spectral-norm perturbation bounds that capture interactions between matrices and arbitrary symmetric perturbations.

Result: Achieves sharp estimates for spectral norm error with improvements up to a factor of √n, resolving an open problem in differentially private PCA literature.

Conclusion: The new bounds closely track actual spectral error across diverse perturbation regimes and extend to a broad class of spectral functionals including polynomials and matrix exponentials.

Abstract: A central challenge in machine learning is to understand how noise or
measurement errors affect low-rank approximations, particularly in the spectral
norm. This question is especially important in differentially private low-rank
approximation, where one aims to preserve the top-$p$ structure of a
data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius
norm error or changes in reconstruction quality, but these metrics can over- or
under-estimate true subspace distortion. The spectral norm, by contrast,
captures worst-case directional error and provides the strongest utility
guarantees. We establish new high-probability spectral-norm perturbation bounds
for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem
and explicitly capture interactions between a matrix $A \in \mathbb{R}^{n
\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and
norm conditions, our bounds yield sharp estimates for $\|(A + E)_p - A_p\|$,
where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up
to a factor of $\sqrt{n}$. As an application, we derive improved utility
guarantees for differentially private PCA, resolving an open problem in the
literature. Our analysis relies on a novel contour bootstrapping method from
complex analysis and extends it to a broad class of spectral functionals,
including polynomials and matrix exponentials. Empirical results on real-world
datasets confirm that our bounds closely track the actual spectral error under
diverse perturbation regimes.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [59] [Can quantum dynamics emerge from classical chaos?](https://arxiv.org/abs/2510.25307)
*Frédéric Faure*

Main category: nlin.CD

TL;DR: Quantum dynamics emerges from classical correlation functions in Anosov geodesic flows through discrete Pollicott-Ruelle spectrum revealed by microlocal analysis.


<details>
  <summary>Details</summary>
Motivation: To understand how deterministic chaos in Anosov geodesic flows unexpectedly gives rise to quantum dynamics from purely classical correlation functions.

Method: Using microlocal analysis to reveal the discrete Pollicott-Ruelle spectrum of the geodesic flow, which naturally arranges into vertical bands.

Result: When the rightmost band of the spectrum is separated by a gap from the rest, it governs an effective dynamics that mirrors quantum evolution.

Conclusion: Quantum dynamics can emerge from classical systems through the discrete spectrum structure of Anosov geodesic flows, with the rightmost spectral band driving quantum-like behavior when isolated.

Abstract: Anosov geodesic flows are among the simplest mathematical models of
deterministic chaos. In this survey we explain how, quite unexpectedly, quantum
dynamics emerges from purely classical correlation functions. The underlying
mechanism is the discrete Pollicott Ruelle spectrum of the geodesic flow,
revealed through microlocal analysis. This spectrum naturally arranges into
vertical bands; when the rightmost band is separated from the rest by a gap, it
governs an effective dynamics that mirrors quantum evolution.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [60] [On a wave kinetic equation with resonance broadening in oceanography and atmospheric sciences](https://arxiv.org/abs/2510.25031)
*Young Ho Kim,Yuri V. Lvov,Leslie M. Smith,Minh-Binh Tran*

Main category: math-ph

TL;DR: Global existence and uniqueness of strong solutions for a three-wave kinetic equation with modified resonance broadening in stratified ocean flows.


<details>
  <summary>Details</summary>
Motivation: To develop a more suitable model for ocean applications by using a different formulation of resonance broadening compared to previous work.

Method: Employed a modified resonance broadening formulation in the three-wave kinetic equation and established mathematical proofs for solution properties.

Result: Successfully proved the global existence and uniqueness of strong solutions to the new resonance broadening kinetic equation.

Conclusion: The new model with modified resonance broadening is mathematically well-posed and more applicable to ocean flow studies.

Abstract: In this work, we study a three-wave kinetic equation with resonance
broadening arising from the theory of stratified ocean flows. Unlike
Gamba-Smith-Tran(On the wave turbulence theory for stratified flows in the
ocean, Math. Models Methods Appl. Sci. 30 (2020), no.1, 105--137), we employ a
different formulation of the resonance broadening, which makes the present
model more suitable for ocean applications. We establish the global existence
and uniqueness of strong solutions to the new resonance broadening kinetic
equation.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [61] [Action-minimizing periodic orbits of the Lorentz force equation with dominant vector potential](https://arxiv.org/abs/2510.25462)
*Manuel Garzón,Salvador López-Martínez*

Main category: math.DS

TL;DR: Existence of non-constant periodic solutions to Lorentz force equation without scalar potential, using relativistic Lagrangian action minimization.


<details>
  <summary>Details</summary>
Motivation: To establish existence of periodic solutions to Lorentz force equation where electromagnetic field is induced without scalar potential, extending to cases with singular scalar potential where vector potential dominates.

Method: Minimizing action functional of relativistic Lagrangian; proving compactness of minimizing sequences requires negative functional values, achieved through novel ideas exploiting sign-indefinite nature of vector potential term.

Result: Successfully established existence of non-constant periodic solutions to Lorentz force equation in cases without scalar potential and with singular scalar potential where vector potential plays leading role.

Conclusion: The approach using action minimization and exploiting sign-indefinite vector potential terms provides effective method for proving existence of periodic solutions in relativistic electromagnetic systems.

Abstract: We establish the existence of non-constant periodic solutions to the Lorentz
force equation, where no scalar potential is needed to induce the
electromagnetic field. Our results extend to cases where a possibly singular
scalar potential is present, although the vector potential assumes a leading
role. The approach is based on minimizing the action functional associated with
the relativistic Lagrangian. The compactness of the minimizing sequences
requires the existence of negative values for the functional, which is proven
using novel ideas that exploit the sign-indefinite nature of the term involving
the vector potential.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [62] [Constructing entire minimal graphs by evolving planes](https://arxiv.org/abs/2510.24978)
*Chung-Jun Tsai,Mao-Pei Tsui,Jingbo Wan,Mu-Tao Wang*

Main category: math.DG

TL;DR: Introduces an evolving-plane ansatz to construct explicit entire minimal graphs of odd dimension n≥3 and codimension m≥2, reducing the minimal surface system to geodesic equations on Grassmannians.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic method for constructing explicit entire minimal graphs in higher dimensions and codimensions, particularly for odd dimensions.

Method: Uses an evolving-plane ansatz that reduces the minimal surface system to the geodesic equation on the Grassmannian in affine coordinates, describing how (n-1) planes evolve to form minimal graphs.

Result: Produces a rich family of explicit entire minimal graphs for odd dimension n and arbitrary codimension m.

Conclusion: The evolving-plane framework provides an effective geometric approach for constructing explicit minimal graphs in higher dimensions and codimensions.

Abstract: We introduce an evolving-plane ansatz for the explicit construction of entire
minimal graphs of dimension $n$ ($n\geq 3$) and codimension $m$ ($m\geq 2$),
for any odd integer $n$. Under this ansatz, the minimal surface system reduces
to the geodesic equation on the Grassmannian in affine coordinates.
Geometrically, this equation dictates how the slope of an $(n-1)$ plane evolves
as it sweeps out a minimal graph. This framework yields a rich family of
explicit entire minimal graphs of odd dimension $n$ and arbitrary codimension
$m$.

</details>


### [63] [The $p$-th dual Minkowski problem for the $k$-torsional rigidity corresponding to a $k$-Hessian equation](https://arxiv.org/abs/2510.25435)
*Xia Zhao,Peibiao Zhao*

Main category: math.DG

TL;DR: This paper introduces the p-th dual k-torsional rigidity and its associated Minkowski problem, establishing existence of smooth non-even solutions for p<n-2 using curvature flow methods.


<details>
  <summary>Details</summary>
Motivation: Motivated by previous work on dual curvature measures connecting cone-volume measure and Aleksandrov's integral curvature, which created a precedent for dual Brunn-Minkowski theory research.

Method: Introduces p-th dual k-torsional rigidity and establishes Hadamard variational formula. Uses curvature flow method that converges smoothly to the solution of the nonlinear PDE, with a novel approach for uniform lower bound estimation in C^0 estimation using invariant functional Φ(Ω_t).

Result: Confirms existence of smooth non-even solutions to the p-th dual Minkowski problem of k-torsional rigidity for p<n-2. The problem is equivalently converted to a nonlinear partial differential equation in smooth case.

Conclusion: Successfully establishes the p-th dual Minkowski problem framework for k-torsional rigidity and proves existence of solutions using curvature flow techniques with novel estimation methods.

Abstract: The study of the dual curvature measures [Y. Huang, E. Lutwak, D. Yang \& G.
Y. Zhang, Acta. Math. 216 (2016): 325-388], which connects the cone-volume
measure and Aleksandrov's integral curvature, and has created a precedent for
the theoretical research of the dual Brunn-Minkowski theory.
  Motivated by the foregoing groundbreaking works, the present paper introduces
the $p$-th dual $k$-torsional rigidity associated with a $k$-Hessian equation
and establishes its Hadamard variational formula with $1\leq k\leq n-1$, which
induces the $p$-th dual $k$-torsional measure. Further, based on the $p$-th
dual $k$-torsional measure, this article, for the first time, proposes the
$p$-th dual Minkowski problem of the $k$-torsional rigidity which can be
equivalently converted to a nonlinear partial differential equation in smooth
case: \begin{align}\label{eq01} f(x)=\tau(|\nabla
h|^2+h^2)^{\frac{p-n}{2}}h_{\Omega}(x)|Du(\nu^{-1}_\Omega(x))|^{k+1}\sigma_{n-k}(h_{ij}(x)+h_\Omega(x)\delta_{ij}),
\end{align} where $\tau>0$ is a constant, $f$ is a positive smooth function
defined on $S^{n-1}$ and $\sigma_{n-k}$ is the $(n-k)$-th elementary symmetric
function of the principal curvature radii. We confirm the existence of smooth
non-even solution to the $p$-th dual Minkowski problem of the $k$-torsional
rigidity for $p<n-2$ by the method of a curvature flow which converges smoothly
to the solution of equation (\ref{eq01}). Specially, a novel approach for the
uniform lower bound estimation in the $C^0$ estimation for the solution to the
curvature flow is presented with the help of invariant functional
$\Phi(\Omega_t)$.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [64] [Spatiotemporal control of laser intensity using differentiable programming](https://arxiv.org/abs/2510.25673)
*Kyle G Miller,Tomas E Gutierrez,Archis S Joglekar,Amanda Elliott,Dustin H Froula,John P Palastro*

Main category: physics.optics

TL;DR: The paper presents an inverse design approach using automatic differentiation and gradient-based optimization to create structured laser pulses with superior performance for nonlinear optics and plasma applications.


<details>
  <summary>Details</summary>
Motivation: Traditional forward design approaches for structured laser pulses are limited to analyzing known structures, while inverse approaches can discover new structures with better performance for nonlinear optical and plasma-based applications.

Method: Combined an implementation of the unidirectional pulse propagation equation that supports automatic differentiation with gradient-based optimization to design structured pulses.

Result: Successfully designed three types of structured pulses: (1) longitudinally uniform intensity over extended region, (2) superluminal intensity peak with constant properties over many Rayleigh ranges, and (3) laser pulse that creates uniform plasma column. The full spatiotemporal optimization improved performance by 15x compared to optimizing only spatial or temporal structure.

Conclusion: Spatiotemporal control of laser pulses through inverse design enables superior performance in nonlinear optics and plasma applications, with full spatiotemporal optimization providing significant advantages over partial optimization approaches.

Abstract: Optical techniques for spatiotemporal control can produce laser pulses with
custom amplitude, phase, or polarization structure. In nonlinear optics and
plasma physics, the use of structured pulses typically follows a forward design
approach, in which the efficacy of a known structure is analyzed for a
particular application. Inverse approaches, in contrast, enable the discovery
of new structures with the potential for superior performance. Here, an
implementation of the unidirectional pulse propagation equation that supports
automatic differentiation is combined with gradient-based optimization to
design structured pulses with features that are advantageous for a range of
nonlinear optical and plasma-based applications: (1) a longitudinally uniform
intensity over an extended region, (2) a superluminal intensity peak that
travels many Rayleigh ranges with constant duration, spot size, and amplitude,
and (3) a laser pulse that ionizes a gas to form a uniform column of plasma. In
the final case, optimizing the full spatiotemporal structure improves the
performance by a factor of 15 compared to optimizing only spatial or only
temporal structure, highlighting the advantage of spatiotemporal control.

</details>
