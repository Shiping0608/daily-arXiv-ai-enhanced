<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 9]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [math.OC](#math.OC) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A Hybrid Reconstruction Framework for Efficient High-Order Shock-Capturing on Unstructured Meshes](https://arxiv.org/abs/2510.25906)
*Yiren Tong,Panagiotis Tsoutsanis*

Main category: math.NA

TL;DR: A hybrid reconstruction framework for compressible flows that combines linear efficiency with nonlinear robustness, using a priori detection to apply optimal reconstruction methods in different flow regions, achieving up to 2.5x speed-up while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To advance high-resolution schemes by balancing computational efficiency with robustness, minimizing the use of costly nonlinear reconstructions while preserving accuracy and stability in compressible flow simulations.

Method: Combines CWENOZ and MOOD paradigms with a novel a priori detection strategy that classifies flow regions into smooth, weakly non-smooth, and discontinuous zones. Uses high-order linear schemes in smooth areas, CWENOZ in weakly non-smooth zones, and second-order MUSCL near discontinuities.

Result: Achieves designed accuracy in smooth regions while improving robustness in shock-dominated flows. Reduces computational cost by up to 2.5x compared to pure CWENOZ schemes in 3D compressible turbulence, with stable non-oscillatory behavior near shocks.

Conclusion: The hybrid approach successfully balances efficiency, robustness, and reliability, making high-order accuracy more practical for industrial-scale CFD applications through targeted reconstruction allocation.

Abstract: We present a multi-dimensional, arbitrary-order hybrid reconstruction
framework for compressible flows on unstructured meshes. The method advances
high-resolution schemes by combining the efficiency of linear reconstruction
with the robustness of nonlinear formulations, activated only when needed
through a novel a priori detection strategy. This minimizes the use of costly
Compact Weighted Essentially Non-Oscillatory (CWENOZ) or Monotonic
Upstream-centered Scheme for Conservation Laws (MUSCL) reconstructions,
reducing computational cost without compromising accuracy or stability. The
framework merges CWENOZ and the Multi-dimensional Optimal Order Detection
(MOOD) paradigm while introducing a redesigned Numerical Admissibility Detector
(NAD) that classifies the local flow into smooth, weakly non-smooth, and
discontinuous regions in a single step. Each region is then reconstructed using
an optimal method: a high-order linear scheme in smooth areas, CWENOZ in weakly
non-smooth zones, and a second-order MUSCL near discontinuities. This targeted
a priori allocation preserves high-order accuracy where possible and ensures
stable, non-oscillatory behavior near shocks and steep gradients. Implemented
within the open-source unstructured finite-volume solver UCNS3D, the framework
supports arbitrary-order reconstructions on mixed-element meshes. Extensive
two- and three-dimensional benchmarks confirm that it retains the designed
accuracy in smooth regions while greatly improving robustness in
shock-dominated flows. Thanks to the reduced frequency of nonlinear
reconstructions, the method achieves up to 2.5x speed-up over a CWENOZ scheme
of equal order in 3D compressible turbulence. This hybrid approach thus brings
high-order accuracy closer to industrial-scale CFD through its balance of
efficiency, robustness, and reliability.

</details>


### [2] [A fast spectral overlapping domain decomposition method with discretization-independent conditioning bounds](https://arxiv.org/abs/2510.25991)
*Simon Dirckx,Anna Yesypenko,Per-Gunnar Martinsson*

Main category: math.NA

TL;DR: A domain decomposition method for solving variable-coefficient elliptic PDEs using overlapping thin slabs/shells, exploiting H-matrix structure for efficient handling of dense blocks in the reduced linear system.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient solver for general variable-coefficient elliptic PDEs on regular domains that can handle large-scale problems with high degrees of freedom.

Method: Tessellate domain into overlapping thin slabs/shells, form reduced linear system connecting domains, exploit rank-structure (H-matrix) for dense blocks, use black-box randomized compression, and leverage sparse direct solvers on thin sub-domains.

Result: The solver can handle oscillatory 2D and 3D problems with up to 28 million degrees of freedom, with improved conditioning and more data-sparse dense blocks than existing formulations.

Conclusion: The proposed domain decomposition method provides an efficient, well-conditioned approach for solving large-scale variable-coefficient elliptic PDEs by exploiting H-matrix structure and randomized compression techniques.

Abstract: A domain decomposition method for the solution of general
variable-coefficient elliptic partial differential equations on regular domains
is introduced. The method is based on tessellating the domain into overlapping
thin slabs or shells, and then explicitly forming a reduced linear system that
connects the different domains. Rank-structure ('H-matrix structure') is
exploited to handle the large dense blocks that arise in the reduced linear
system. Importantly, the formulation used is well-conditioned, as it converges
to a second kind Fredholm equation as the precision in the local solves is
refined. Moreover, the dense blocks that arise are far more data-sparse than in
existing formulations, leading to faster and more efficient H-matrix
arithmetic. To form the reduced linear system, black-box randomized compression
is used, taking full advantage of the fact that sparse direct solvers are
highly efficient on the thin sub-domains. Numerical experiments demonstrate
that our solver can handle oscillatory 2D and 3D problems with as many as 28
million degrees of freedom.

</details>


### [3] [A two-dimensional fractional-order element-free Galerkin method for nonlocal elasticity and complex domain problems](https://arxiv.org/abs/2510.26161)
*Shubham Desai,Malapeta Hemasundara Rao,Sai Sidhardh*

Main category: math.NA

TL;DR: A meshfree 2D fractional-order Element-Free Galerkin method is developed as an alternative to FEM for solving fractional-order differential equations, handling complex domains better than mesh-based methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of 1D f-EFG and demonstrate the potential of meshfree approaches for solving FDEs in complex 2D geometries, while addressing FEM drawbacks like high computational cost and mesh generation issues.

Method: Uses 2D Moving Least Squares (MLS) approximants within a fractional-order Element-Free Galerkin framework to handle irregular domains and approximate fractional derivatives from nodal values.

Result: The solver successfully handles complex 2D domains (square and circular plates) for nonlocal elasticity problems, validates against benchmark results, and shows effectiveness for both linear and nonlinear fractional PDEs.

Conclusion: The 2D f-EFG method is a viable alternative to FEM for FDEs, with potential for extension to diverse applications including multiscale modeling, multiphysics coupling, anomalous diffusion, and complex material behavior.

Abstract: This study presents a meshfree two-dimensional fractional-order Element-Free
Galerkin (2D f-EFG) method as a viable alternative to conventional mesh-based
FEM for a numerical solution of (spatial) fractional-order differential
equations (FDEs). The previously developed one-dimensional f-EFG solver offers
a limited demonstration of the true efficacy of EFG formulations for FDEs, as
it is restricted to simple 1D line geometries. In contrast, the 2D f-EFG solver
proposed and developed here effectively demonstrates the potential of meshfree
approaches for solving FDEs. The proposed solver can handle complex and
irregular 2D domains that are challenging for mesh-based methods. As an
example, the developed framework is employed to investigate nonlocal elasticity
governed by fractional-order constitutive relations in a square and circular
plate. Furthermore, the proposed approach mitigates key drawbacks of FEM,
including high computational cost, mesh generation, and reduced accuracy in
irregular domains. The 2D f-EFG employs 2D Moving Least Squares (MLS)
approximants, which are particularly effective in approximating fractional
derivatives from nodal values. The 2D f-EFG solver is employed here for the
numerical solution of fractional-order linear and nonlinear partial
differential equations corresponding to the nonlocal elastic response of a
plate. The solver developed here is validated with the benchmark results
available in the literature. While the example chosen here focuses on nonlocal
elasticity, the numerical method can be extended for diverse applications of
fractional-order derivatives in multiscale modeling, multiphysics coupling,
anomalous diffusion, and complex material behavior.

</details>


### [4] [A parallel solver for random input problems via Karhunen-Lo√®ve expansion and diagonalized coarse grid correction](https://arxiv.org/abs/2510.26180)
*Dou Dai,Qiuqi Li,Huailing Song*

Main category: math.NA

TL;DR: Proposes KLE-CGC, a hybrid parallel algorithm combining Karhunen-Lo√®ve expansion and coarse grid correction to improve computational efficiency of parallel-in-time methods for stochastic initial-value problems.


<details>
  <summary>Details</summary>
Motivation: Standard parareal algorithm suffers from slow convergence for stochastic problems due to poor initial guess quality.

Method: Uses KL expansion for low-dimensional parameterization of stochastic fields, constructs gPC spectral surrogate model for rapid solution prediction, and uses this as improved initial value for parareal iterations.

Result: Maintains same convergence order as original algorithm while significantly reducing iteration count and improving parallel scalability.

Conclusion: KLE-CGC framework retains theoretical convergence rate of standard parareal while substantially enhancing computational efficiency for stochastic problems.

Abstract: This paper is dedicated to enhancing the computational efficiency of
traditional parallel-in-time methods for solving stochastic initial-value
problems. The standard parareal algorithm often suffers from slow convergence
when applied to problems with stochastic inputs, primarily due to the poor
quality of the initial guess. To address this issue, we propose a hybrid
parallel algorithm, termed KLE-CGC, which integrates the Karhunen-Lo\`{e}ve
(KL) expansion with the coarse grid correction (CGC). The method first employs
the KL expansion to achieve a low-dimensional parameterization of
high-dimensional stochastic parameter fields. Subsequently, a generalized
Polynomial Chaos (gPC) spectral surrogate model is constructed to enable rapid
prediction of the solution field. Utilizing this prediction as the initial
value significantly improves the initial accuracy for the parareal iterations.
A rigorous convergence analysis is provided, establishing that the proposed
framework retains the same theoretical convergence rate as the standard
parareal algorithm. Numerical experiments demonstrate that KLE-CGC maintains
the same convergence order as the original algorithm while substantially
reducing the number of iterations and improving parallel scalability.

</details>


### [5] [Efficient And Stable Third-order Method for Micromagnetics Simulations](https://arxiv.org/abs/2510.26181)
*Changjian Xie,Cheng Wang*

Main category: math.NA

TL;DR: A third-order accurate numerical scheme for solving the Landau-Lifshitz-Gilbert equation with large damping parameters, featuring linear systems with constant coefficients for efficiency, and demonstrating unconditional stability and improved accuracy in 1D/3D tests.


<details>
  <summary>Details</summary>
Motivation: To address magnetization dynamics in ferromagnetic materials under large damping parameters, building upon existing second-order methods to achieve higher accuracy while maintaining computational efficiency.

Method: Developed a third-order accurate numerical scheme based on a second-order method, using linear systems with constant coefficients to enable fast solvers, achieving third-order temporal and fourth-order spatial accuracy.

Result: Numerical tests in 1D and 3D confirm third-order accuracy and efficiency gains. The method shows unconditional stability for large damping parameters and captures physically plausible structures. For domain wall dynamics, it reproduces linear relationships between wall velocity and damping parameters/external fields, outperforming lower-order methods.

Conclusion: The proposed third-order scheme provides an efficient and accurate solution for magnetization dynamics with large damping parameters, offering significant improvements over existing first and second-order methods while maintaining computational efficiency through linear constant-coefficient systems.

Abstract: To address the magnetization dynamics in ferromagnetic materials described by
the Landau-Lifshitz-Gilbert equation under large damping parameters, a
third-order accurate numerical scheme is developed by building upon a
second-order method \cite{CaiChenWangXie2022} and leveraging its efficiency.
This method boasts two key advantages: first, it only involves solving linear
systems with constant coefficients, enabling the use of fast solvers and thus
significantly enhancing numerical efficiency over existing first or
second-order approaches. Second, it achieves third-order temporal accuracy and
fourth-order spatial accuracy, while being unconditionally stable for large
damping parameters. Numerical tests in 1D and 3D scenarios confirm both its
third-order accuracy and efficiency gains. When large damping parameters are
present, the method demonstrates unconditional stability and reproduces
physically plausible structures. For domain wall dynamics simulations, it
captures the linear relationship between wall velocity and both the damping
parameter and external magnetic field, outperforming lower-order methods in
this regard.

</details>


### [6] [Transcending Sparse Measurement Limits: Operator-Learning-Driven Data Super-Resolution for Inverse Source Problem](https://arxiv.org/abs/2510.26227)
*Guanyu Pan,Jianing Zhou,Xiaotong Liu,Yunqing Huang,Nianyu Yi*

Main category: math.NA

TL;DR: A modular framework combining DeepONet interpolation with Direct Sampling Method significantly improves multi-source localization from extremely sparse single-frequency measurements in narrow apertures.


<details>
  <summary>Details</summary>
Motivation: Inverse source localization from Helmholtz boundary data collected over narrow apertures is highly ill-posed and severely undersampled, undermining classical solvers like the Direct Sampling Method.

Method: Three-step modular framework: 1) Extend uniqueness theorem for inverse source problem under limited viewing apertures; 2) Use DeepONet with branch-trunk architecture to interpolate sparse measurements (6-10 samples) to dense synthetic aperture; 3) Feed super-resolved field into Direct Sampling Method for localization.

Result: For single source, sparse data alone achieves grid-level precision. In 2-3 source trials, DeepONet-reconstructed data reduce localization error by about an order of magnitude compared to raw sparse measurements, remaining effective with apertures as small as œÄ/4.

Conclusion: The decoupled framework allows swapping interpolation and inversion modules with neural operators and classical algorithms, providing a practical and flexible design that improves localization accuracy over standard baselines.

Abstract: Inverse source localization from Helmholtz boundary data collected over a
narrow aperture is highly ill-posed and severely undersampled, undermining
classical solvers (e.g., the Direct Sampling Method). We present a modular
framework that significantly improves multi-source localization from extremely
sparse single-frequency measurements. First, we extend a uniqueness theorem for
the inverse source problem, proving that a unique solution is guaranteed under
limited viewing apertures. Second, we employ a Deep Operator Network (DeepONet)
with a branch-trunk architecture to interpolate the sparse measurements,
lifting six to ten samples within the narrow aperture to a sufficiently dense
synthetic aperture. Third, the super-resolved field is fed into the Direct
Sampling Method (DSM). For a single source, we derive an error estimate showing
that sparse data alone can achieve grid-level precision. In two- and
three-source trials, localization from raw sparse measurements is unreliable,
whereas DeepONet-reconstructed data reduce localization error by about an order
of magnitude and remain effective with apertures as small as $\pi/4$. By
decoupling interpolation from inversion, the framework allows the interpolation
and inversion modules to be swapped with neural operators and classical
algorithms, respectively, providing a practical and flexible design that
improves localization accuracy compared with standard baselines.

</details>


### [7] [Simulation of the magnetic Ginzburg-Landau equation via vortex tracking](https://arxiv.org/abs/2510.26334)
*Thiago Carvalho Corso,Gaspard Kemlin,Christof Melcher,Benjamin Stamm*

Main category: math.NA

TL;DR: A numerical method for simulating 2D magnetic time-dependent Ginzburg-Landau equations in small epsilon regime using reduced ODE system to avoid resolving fine epsilon-scale structures.


<details>
  <summary>Details</summary>
Motivation: To efficiently simulate TDGL equations with small Ginzburg-Landau parameter epsilon, where traditional methods require very fine meshes due to quantized vortices with core size of order epsilon.

Method: Developed a numerical method based on the limiting ODE system for vortex evolution, requiring solution of linear second order PDE at each time step, avoiding direct resolution of epsilon-scale structures.

Result: The method provides accurate simulations of TDGL equations without needing to resolve fine epsilon-scale meshes, with numerical examples demonstrating effectiveness.

Conclusion: The proposed numerical strategy successfully enables efficient simulation of infinite-dimensional TDGL equations in small epsilon regime using finite-dimensional ODE approximation, validated through numerical tests.

Abstract: This paper deals with the numerical simulation of the 2D magnetic
time-dependent Ginzburg-Landau (TDGL) equations in the regime of small but
finite (inverse) Ginzburg-Landau parameter $\epsilon$ and constant (order $1$
in $\epsilon$) applied magnetic field. In this regime, a well-known feature of
the TDGL equation is the appearance of quantized vortices with core size of
order $\epsilon$. Moreover, in the singular limit $\epsilon \searrow 0$, these
vortices evolve according to an explicit ODE system. In this work, we first
introduce a new numerical method for the numerical integration of this limiting
ODE system, which requires to solve a linear second order PDE at each time
step. We also provide a rigorous theoretical justification for this method that
applies to a general class of 2D domains. We then develop and analyze a
numerical strategy based on the finite-dimensional ODE system to efficiently
simulate the infinite-dimensional TDGL equations in the presence of a constant
external magnetic field and for small, but finite, $\epsilon$. This method
allows us to avoid resolving the $\epsilon$-scale when solving the TDGL
equations, where small values of $\epsilon$ typically require very fine meshes
and time steps. We provide numerical examples on a few test cases and justify
the accuracy of the method with numerical investigations.

</details>


### [8] [Incorporating Local H√∂lder Regularity into PINNs for Solving Elliptic PDEs](https://arxiv.org/abs/2510.26365)
*Qirui Zhou,Jiebao Sun,Yi Ran,Boying Wu*

Main category: math.NA

TL;DR: Incorporating local H√∂lder regularization into PINNs for elliptic PDEs improves accuracy and robustness through a modified loss function and variable-distance sampling strategy.


<details>
  <summary>Details</summary>
Motivation: Leveraging interior regularity properties of linear elliptic PDEs to enhance PINN performance through mathematical regularization.

Method: Modified PINN loss function with local H√∂lder regularization term, approximated using variable-distance discrete sampling strategy.

Result: Numerical experiments show notable improvements in prediction accuracy and robustness compared to standard PINNs.

Conclusion: Local H√∂lder regularization effectively enhances PINN performance for elliptic PDEs, with established error estimates supporting generalization capabilities.

Abstract: In this paper, local H\"older regularization is incorporated into a
physics-informed neural networks (PINNs) framework for solving elliptic partial
differential equations (PDEs). Motivated by the interior regularity properties
of linear elliptic PDEs, a modified loss function is constructed by introducing
local H\"older regularization term. To approximate this term effectively, a
variable-distance discrete sampling strategy is developed. Error estimates are
established to assess the generalization performance of the proposed method.
Numerical experiments on a range of elliptic problems demonstrate notable
improvements in both prediction accuracy and robustness compared to standard
physics-informed neural networks.

</details>


### [9] [Asymptotic meshes from $r$-variational adaptation methods for static problems in one dimension](https://arxiv.org/abs/2510.26375)
*Darith Hun,Nicolas Mo√´s,Heiner Olbermann*

Main category: math.NA

TL;DR: The paper analyzes r-adaptive finite element methods for minimizing integral functionals in 1D, showing that optimal grid configurations converge to a well-defined limit as node count increases to infinity.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of optimal grid configurations in r-adaptive finite element approximations of integral functionals.

Method: Using Œì-convergence theory to prove that renormalized energy functionals converge to a limit, treating the grid as a variable in minimization.

Result: Optimal grid configurations have a well-defined limit as the number of nodes goes to infinity, and numerical examples confirm the closeness between asymptotic and finite optimal meshes.

Conclusion: The Œì-convergence framework successfully captures the limiting behavior of optimal adaptive meshes, providing theoretical foundation for r-adaptive finite element methods.

Abstract: We consider the minimization of integral functionals in one dimension and
their approximation by $r$-adaptive finite elements. Including the grid of the
FEM approximation as a variable in the minimization, we are able to show that
the optimal grid configurations have a well-defined limit when the number of
nodes in the grid is being sent to infinity. This is done by showing that the
suitably renormalized energy functionals possess a limit in the sense of
$\Gamma$-convergence. We provide numerical examples showing the closeness of
the optimal asymptotic mesh obtained as a minimizer of the $\Gamma$-limit to
the optimal finite meshes.

</details>


### [10] [Explicit Consistency Error Estimate for Finite Element Solutions of the Poisson Equation on Convex Domains](https://arxiv.org/abs/2510.26404)
*Su Ruibo*

Main category: math.NA

TL;DR: Explicit a priori consistency error estimates for finite element discretization of Poisson equation on convex domains approximated by internal convex polyhedra.


<details>
  <summary>Details</summary>
Motivation: To provide explicit error estimates that depend only on global geometric parameters and are applicable to general convex domains with arbitrary simplicial meshes.

Method: Standard finite element discretization of Poisson equation on convex domains approximated by internal convex polyhedra.

Result: Derived explicit a priori consistency error estimates that are geometric parameter-dependent and broadly applicable.

Conclusion: The paper successfully establishes explicit, geometry-dependent error estimates for finite element methods on convex domains with polyhedral approximations.

Abstract: We derive explicit a priori consistency error estimates for a standard finite
element discretization of the Poisson equation on convex domains, where the
domain is approximated by an internal convex polyhedron. The obtained explicit
estimates depend only on global geometric parameters and are applicable to
general convex domains and arbitrary families of simplicial meshes.

</details>


### [11] [Accelerated decomposition of bistochastic kernel matrices by low rank approximation](https://arxiv.org/abs/2510.26574)
*Chris Vales,Dimitrios Giannakis*

Main category: math.NA

TL;DR: An accelerated algorithm for approximate eigenvalue decomposition of bistochastic normalized kernel matrices using low-rank approximations via pivoted partial Cholesky, achieving linear cost scaling with dataset size.


<details>
  <summary>Details</summary>
Motivation: To efficiently compute eigenvalue decompositions of bistochastic normalized kernel matrices without forming the full matrix, enabling scalable analysis of large datasets.

Method: Constructs low-rank approximation using pivoted partial Cholesky algorithm, then computes approximate decomposition of bistochastic normalization using this approximation.

Result: Algorithm achieves linear cost dependence on dataset size and quadratic dependence on approximation rank, significantly reducing computational cost compared to naive approaches.

Conclusion: The method provides accurate spatiotemporal pattern extraction from chaotic dynamics and outperforms alternative subsampling and Nystr√∂m extension approaches.

Abstract: We develop an accelerated algorithm for computing an approximate eigenvalue
decomposition of bistochastic normalized kernel matrices. Our approach
constructs a low rank approximation of the original kernel matrix by the
pivoted partial Cholesky algorithm and uses it to compute an approximate
decomposition of its bistochastic normalization without requiring the formation
of the full kernel matrix. The cost of the proposed algorithm depends linearly
on the size of the employed training dataset and quadratically on the rank of
the low rank approximation, offering a significant cost reduction compared to
the naive approach. We apply the proposed algorithm to the kernel based
extraction of spatiotemporal patterns from chaotic dynamics, demonstrating its
accuracy while also comparing it with an alternative algorithm consisting of
subsampling and Nystroem extension.

</details>


### [12] [The evolving surface morphochemical reaction-diffusion system for battery modeling](https://arxiv.org/abs/2510.26437)
*Benedetto Bozzini,Massimo Frittelli,Anotida Madzvamuse,Ivonne Sgura*

Main category: math.NA

TL;DR: The paper introduces the Evolving Surface DIB (ESDIB) model, a reaction-diffusion system on dynamically evolving electrode surfaces, to address uncontrolled morphology in electrodeposition for next-generation batteries.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of poorly controllable morphology in electrodeposition, which causes technological issues in electrochemical technology and prevents practical cyclability in high-energy density batteries with metal anodes.

Method: Developed the ESDIB model that couples surface evolution to local concentration of electrochemical species, using an extension of LESFEM for spatial discretization and IMEX Euler scheme for time integration.

Result: The model was validated through six numerical experiments showing accurate capture of branching and dendritic growth, matching laboratory images of electrodeposition.

Conclusion: The ESDIB framework provides a predictive and physically consistent tool for studying metal deposition phenomena in energy storage devices.

Abstract: It is well known that phase formation by electrodeposition yields films of
poorly controllable morphology. This typically leads to a range of
technological issues in many fields of electrochemical technology. Presently, a
particularly relevant case is that of high-energy density next-generation
batteries with metal anodes, that cannot yet reach practical cyclability
targets, owing to uncontrolled elelctrode shape evolution. In this scenario,
mathematical modelling is a key tool to lay the knowledge-base for
materials-science advancements liable to lead to concretely stable battery
material architectures. In this work, we introduce the Evolving Surface DIB
(ESDIB) model, a reaction-diffusion system posed on a dynamically evolving
electrode surface. Unlike previous fixed-surface formulations, the ESDIB model
couples surface evolution to the local concentration of electrochemical
species, allowing the geometry of the electrode itself to adapt in response to
deposition. To handle the challenges related to the coupling between surface
motion and species transport, we numerically solve the system by proposing an
extension of the Lumped Evolving Surface Finite Element Method (LESFEM) for
spatial discretisation, combined with an IMEX Euler scheme for time
integration. The model is validated through six numerical experiments, each
compared with laboratory images of electrodeposition. Results demonstrate that
the ESDIB framework accurately captures branching and dendritic growth,
providing a predictive and physically consistent tool for studying metal
deposition phenomena in energy storage devices.

</details>


### [13] [A GenEO-type coarse space with smaller eigenproblems](https://arxiv.org/abs/2510.26548)
*Peter Bastian,Nils Friess*

Main category: math.NA

TL;DR: A new GenEO coarse space variant that solves eigenproblems only in boundary strips, reducing setup costs while maintaining coefficient robustness.


<details>
  <summary>Details</summary>
Motivation: Traditional adaptive coarse spaces solve expensive local eigenproblems throughout subdomains, creating high setup costs that may exceed iteration costs.

Method: Modified GenEO coarse space that computes eigenproblems only in boundary-connected strips of subdomains rather than entire subdomains.

Result: Significant reduction in setup cost while achieving similar coefficient-robust condition number estimates as the original method.

Conclusion: The boundary-strip approach provides an efficient alternative that maintains robustness while substantially lowering computational overhead.

Abstract: Coarse spaces are essential to ensure robustness w.r.t. the number of
subdomains in two-level overlapping Schwarz methods. Robustness with respect to
the coefficients of the underlying partial differential equation (PDE) can be
achieved by adaptive (or spectral) coarse spaces involving the solution of
local eigenproblems. The solution of these eigenproblems, although scalable,
entails a large setup cost which may exceed the cost for the iteration phase.
In this paper we present and analyse a new variant of the GenEO (Generalised
Eigenproblems in the Overlap) coarse space which involves solving eigenproblems
only in a strip connected to the boundary of the subdomain. This leads to a
significant reduction of the setup cost while the method satisfies a similar
coefficient-robust condition number estimate as the original method, albeit
with a possibly larger coarse space.

</details>


### [14] [Fast tensor-based electrostatic energy calculations in the perspective of protein-ligand docking problem](https://arxiv.org/abs/2510.26611)
*Peter Benner,Boris N. Khoromskij,Venera Khoromskaia,Matthias Stein*

Main category: math.NA

TL;DR: A fast tensor-based approach for calculating electrostatic interaction energy in protein-ligand docking using low-rank range-separated tensor representations on large 3D grids with O(n) complexity.


<details>
  <summary>Details</summary>
Motivation: To enable fast calculation of electrostatic interaction energy for constrained energy minimization in rigid protein-ligand docking, overcoming computational limitations of traditional methods.

Method: Uses low-rank range-separated tensor-based representation of electrostatic potential on large 3D grids, with O(n)-complexity calculations. Employs tensor techniques for collective electrostatic potential and interaction energy computation, combined with van der Waals distance control for pose selection.

Result: Demonstrates proof of concept through numerical tests on synthetic and realistic data, showing the method can handle complex particle configurations and enable usage of large 3D grids (up to ~10^12 points).

Conclusion: The tensor-based approach provides efficient electrostatic energy calculation for protein-ligand docking with mild logarithmic dependence on particle count, and can be integrated with traditional stochastic or deterministic docking techniques.

Abstract: We propose and justify a new approach for fast calculation of the
electrostatic interaction energy of clusters of charged particles in
constrained energy minimization in the framework of rigid protein-ligand
docking. Our ``blind search'' docking technique is based on the low-rank
range-separated (RS) tensor-based representation of the free-space
electrostatic potential of the biomolecule represented on large $n\times
n\times n$ 3D grid. We show that both the collective electrostatic potential of
a complex protein-ligand system and the respective electrostatic interaction
energy can be calculated by tensor techniques in $O(n)$-complexity, such that
the numerical cost for energy calculation only mildly (logarithmically) depends
on the number of particles in the system. Moreover, tensor representation of
the electrostatic potential enables usage of large 3D Cartesian grids (of the
order of $n^3 \sim 10^{12}$), which could allow the accurate modeling of
complexes with several large proteins. In our approach selection of the correct
geometric pose predictions in the localized posing process is based on the
control of van der Waals distance between the target molecular clusters. Here,
we confine ourselves by constrained minimization of the energy functional by
using only fast tensor-based free-space electrostatic energy recalculation for
various rotations and translations of both clusters. Numerical tests of the
electrostatic energy-based ``protein-ligand docking'' algorithm applied to
synthetic and realistic input data present a proof of concept for rather
complex particle configurations. The method may be used in the framework of the
traditional stochastic or deterministic posing/docking techniques.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Solutions to Second-Order Nonlocal Evolution Equations Governed by Non-Autonomous Forms](https://arxiv.org/abs/2510.25881)
*Sajid Ullah,Vittorio Colao*

Main category: math.AP

TL;DR: Proves existence conditions for second-order problems with nonzero nonlocal initial conditions using fundamental solutions and fixed-point techniques, with applications to PDEs modeling vibrating viscoelastic membranes.


<details>
  <summary>Details</summary>
Motivation: To address second-order problems with nonzero nonlocal initial conditions, which are important for modeling physical systems like vibrating viscoelastic membranes with time-dependent properties and memory effects.

Method: Uses fundamental solutions and fixed-point techniques to analyze the existence of solutions.

Result: Establishes sufficient conditions for the existence of solutions to second-order problems with nonzero nonlocal initial conditions.

Conclusion: The theoretical framework successfully applies to partial differential equations modeling complex physical systems with nonlocal memory effects and time-dependent material properties.

Abstract: Our main contributions include proving sufficient conditions for the
existence of solution to a second order problem with nonzero nonlocal initial
conditions, and providing a comprehensive analysis using fundamental solutions
and fixed-point techniques. The theoretical results are illustrated through
applications to partial differential equations, including vibrating
viscoelastic membranes with time-dependent material properties and nonlocal
memory effects.

</details>


### [16] [Bochner-Riesz means on a conical singular manifold](https://arxiv.org/abs/2510.26059)
*Qiuye Jia,Junyong Zhang,Jiqiang Zheng*

Main category: math.AP

TL;DR: Sharp L^p-boundedness criterion for Bochner-Riesz multipliers on flat cones: bounded for 1‚â§p‚â§‚àû, p‚â†2 if and only if Œ¥ > Œ¥_c(p,2) = max{0, 2|1/2 - 1/p| - 1/2}.


<details>
  <summary>Details</summary>
Motivation: To establish precise boundedness conditions for Bochner-Riesz multipliers on flat cones and resolve the critical exponent problem in wedge domains.

Method: Analysis of Bochner-Riesz multipliers S_Œª^Œ¥(Œî_X) on flat cones X=(0,‚àû)√óS_œÉ^1 using L^p-boundedness theory.

Result: Proved sharp criterion: bounded on L^p(X) for 1‚â§p‚â§‚àû, p‚â†2 if and only if Œ¥ > Œ¥_c(p,2) = max{0, 2|1/2 - 1/p| - 1/2}.

Conclusion: The result provides complete characterization of boundedness for Bochner-Riesz multipliers on flat cones and applies to infinite sector domains with boundary conditions.

Abstract: We prove a sharp $L^p$-boundedness criterion for Bochner-Riesz multipliers on
flat cones $X = (0,\infty) \times \mathbb{S}_\sigma^1$. The operator
$S_\lambda^\delta(\Delta_X)$ is bounded on $L^p(X)$ for $1 \leq p \leq \infty$,
$p \neq 2$, if and only if $\delta > \delta_c(p,2) = \max\left\{ 0, 2\left| 1/2
- 1/p \right| - 1/2 \right\}$. This result is also applicable to the infinite
sector domain with Dirichlet or Neumann boundary, resolving the critical
exponent problem in this wedge setting.

</details>


### [17] [A one-dimensional Stefan problem for the heat equation with a nonlinear boundary condition](https://arxiv.org/abs/2510.26088)
*Kensho Araya,Kazuhiro Ishige*

Main category: math.AP

TL;DR: The paper classifies solutions to the 1D Stefan problem with nonlinear boundary conditions into three types based on initial function size: global solutions with exponential decay, global solutions with non-exponential decay, and finite-time blow-up solutions.


<details>
  <summary>Details</summary>
Motivation: To understand the qualitative behavior and classification of solutions to the one-dimensional Stefan problem with nonlinear boundary conditions, particularly how initial conditions determine solution types.

Method: Analysis of the one-dimensional one-phase Stefan problem for the heat equation with nonlinear boundary conditions, studying solution behavior based on initial function size.

Result: All solutions fall into three distinct types: global solutions with exponential decay, global solutions with non-exponential decay, and finite-time blow-up solutions. The classification depends on initial function size, and blow-up behavior is described.

Conclusion: The Stefan problem with nonlinear boundary conditions exhibits a trichotomy of solution behaviors determined by initial conditions, with complete characterization of blow-up behavior.

Abstract: We study the one-dimensional one-phase Stefan problem for the heat equation
with a nonlinear boundary condition. We show that all solutions fall into one
of three distinct types: global-in-time solutions with exponential decay,
global-in-time solutions with non-exponential decay, and finite-time blow-up
solutions. The classification depends on the size of the initial function.
Furthermore, we describe the behavior of solutions at the blow-up time.

</details>


### [18] [Time-periodic boundary effects on the shocks for scalar conservation laws](https://arxiv.org/abs/2510.26153)
*Yuan Yuan*

Main category: math.AP

TL;DR: This paper analyzes asymptotic stabilities of inviscid and viscous shocks for scalar conservation laws on half-line with time-periodic boundary conditions, showing solutions converge to shifted background shocks coupled with boundary-induced periodic solutions.


<details>
  <summary>Details</summary>
Motivation: To understand how time-periodic boundary conditions affect long-time behaviors of Riemann solutions in classical piston problems for fluid mechanics, addressing a gap in understanding shock dynamics under periodic forcing.

Method: Rigorous mathematical analysis of scalar conservation laws on half-line with shock speed s<0, examining both inviscid and viscous cases under time-periodic boundary conditions.

Result: Proved that asymptotic states are governed by shifted background shocks coupled with time-periodic boundary solutions, revealing a propagating "boundary wave" that influences shock dynamics.

Conclusion: Time-periodic boundary conditions significantly affect shock stability by introducing boundary waves that couple with background shocks, providing complete characterization of asymptotic behaviors in both inviscid and viscous cases.

Abstract: This paper is concerned with the asymptotic stabilities of the inviscid and
viscous shocks for the scalar conservation laws on the half-line $(-\infty,0)$
with shock speed $s<0$, subjected to the time-periodic boundary condition,
which arises from the classical piston problems for fluid mechanics. Despite
the importance, how time-periodic boundary conditions affect the long-time
behaviors of Riemann solutions has remained unclear. This work addresses this
gap by rigorously proving that in both inviscid and viscous case, the
asymptotic states of the solutions under the time-periodic boundary conditions
are not only governed by the shifted background (viscous) shocks, but also
coupled with the time-periodic boundary solution induced by the time-periodic
boundary. Our analysis reveals that these effects manifest as a propagating
"boundary wave", which influences the shock dynamics.

</details>


### [19] [Sharp embeddings and existence results for Logarithmic $p$-Laplacian equations with critical growth](https://arxiv.org/abs/2510.26286)
*Rakesh Arora,Jacques Giacomoni,Hichem Hajaiej,Arshi Vaishnavi*

Main category: math.AP

TL;DR: This paper establishes new p-Logarithmic Sobolev inequalities and embeddings for the logarithmic p-Laplacian function space, applies these to prove existence of solutions for critical growth problems using Nehari manifold methods, and analyzes asymptotic behavior of fractional p-Laplacian problems as s‚Üí0‚Å∫.


<details>
  <summary>Details</summary>
Motivation: To extend previous linear results to a broader nonlinear variational framework by developing analytical tools for the logarithmic p-Laplacian operator and studying its applications to critical growth boundary value problems.

Method: Derivation of p-Logarithmic Sobolev inequalities and optimal embeddings into Orlicz-type spaces; application of Nehari manifold method for existence proofs; asymptotic analysis of fractional p-Laplacian problems as s‚Üí0‚Å∫.

Result: Established new p-Logarithmic Sobolev inequality and optimal embeddings; proved existence of nontrivial weak solutions for critical growth problems; demonstrated convergence of least energy solutions to Brezis-Nirenberg or logistic-type problems involving logarithmic p-Laplacian.

Conclusion: The work provides a nonlinear analogue of previous linear results and extends the scope to a broader variational framework, establishing fundamental analytical tools and solution existence for problems involving the logarithmic p-Laplacian.

Abstract: In this paper, we derive a new $p$-Logarithmic Sobolev inequality and optimal
continuous and compact embeddings into Orlicz-type spaces of the function space
associated with the logarithmic $p$-Laplacian. As an application of these
results, we study a class of Dirichlet boundary value problems involving the
logarithmic $p$-Laplacian and critical growth nonlinearities perturbed with
superlinear-subcritical growth terms. By employing the method of the Nehari
manifold, we prove the existence of a nontrivial weak solution.
  Lastly, we conduct an asymptotic analysis of a weighted nonlocal, nonlinear
problem governed by the fractional $p$-Laplacian with superlinear or sublinear
type non-linearity, demonstrating the convergence of least energy solutions to
a non-trivial, non-negative least energy solution of a Brezis-Nirenberg type or
logistic-type problem, respectively, involving the logarithmic $p$-Laplacian as
the fractional parameter $s \to 0^+$.
  The findings in this work serve as a nonlinear analogue of the results
reported in \cite{Angeles-Saldana, Arora-Giacomoni-Vaishnavi,
Santamaria-Saldana}, thereby extending their scope to a broader variational
framework.

</details>


### [20] [Coupling local and nonlocal total variation flow for image despeckling](https://arxiv.org/abs/2510.26296)
*Yi Ran,Zhichang Guo,Kehan Shi,Qirui Zhou,Jingfeng Shao,Martin Burger,Boying Wu*

Main category: math.AP

TL;DR: The paper proposes a coupled local-nonlocal total variation flow for image despeckling that combines the texture preservation of nonlocal equations with the strong denoising capabilities of local equations.


<details>
  <summary>Details</summary>
Motivation: Nonlocal equations preserve textures well but have weak regularization, while local equations offer strong denoising but fail to protect textures. The goal is to integrate the advantages of both approaches.

Method: Developed a coupled local-nonlocal total variation flow model for image despeckling. Established existence and uniqueness of weak solutions, analyzed equivalent forms and asymptotic behavior, and proved convergence to classical total variation flow under kernel rescaling.

Result: Successfully created a unified framework that combines local and nonlocal approaches. Demonstrated mathematical properties including solution existence/uniqueness and convergence behavior.

Conclusion: The coupled local-nonlocal approach effectively integrates the strengths of both methods for image despeckling, with mathematical validation of solution properties and convergence behavior.

Abstract: Nonlocal equations effectively preserve textures but exhibit weak
regularization effects in image denoising, whereas local equations offer strong
denoising capabilities yet fail to protect textures. To integrate the
advantages of both approaches, this paper investigates a coupled local-nonlocal
total variation flow for image despeckling. We establish the existence and
uniqueness of the weak solution for the proposed equation. Several properties,
including the equivalent forms of the weak solution and its asymptotic
behavior, are derived. Furthermore, we demonstrate that the weak solutions of
the proposed equation converge to the weak solution of the classical total
variation flow under kernel rescaling. The importance of coupling is
highlighted through comparisons with local and nonlocal models for image
despeckling.

</details>


### [21] [Complete spectrum of the Robin eigenvalue problem on the ball](https://arxiv.org/abs/2510.26331)
*Cancan Chen,Guowei Dai,Yingxin Sun*

Main category: math.AP

TL;DR: Complete spectral analysis of Robin eigenvalue problem on unit ball, revealing exact formulas for first and second eigenvalues depending on parameter Œ±, with ratio Œº‚ÇÇ/Œº‚ÇÅ varying in sign.


<details>
  <summary>Details</summary>
Motivation: To fully characterize the spectral structure of Robin boundary value problems on the unit ball, which have applications in various physical and mathematical contexts.

Method: Analytical investigation using Bessel functions and their zeros, specifically analyzing the zeros of kJ·µ•‚Çä‚Çó‚Çä‚ÇÅ(k)-(Œ±+l)J·µ•‚Çä‚Çó(k) and Œ±I·µ•(k)+kI·µ•‚Çä‚ÇÅ(k) for different ranges of Œ±.

Result: Obtained exact formulas: for Œ±>0, first eigenvalue is k¬≤·µ•,‚ÇÅ and second is k¬≤·µ•‚Çä‚ÇÅ,‚ÇÅ; for Œ±‚àà(-1,0), first is -kÃÇ¬≤·µ•,‚ÇÅ and second is k¬≤·µ•‚Çä‚ÇÅ,‚ÇÅ; for Œ±=-1, first is -kÃÇ¬≤·µ•,‚ÇÅ and second is 0.

Conclusion: The ratio Œº‚ÇÇ/Œº‚ÇÅ can be positive, negative, or zero depending on Œ±, providing complete spectral characterization of Robin eigenvalue problem on unit ball.

Abstract: We investigate the following Robin eigenvalue problem \begin{equation*}
\left\{ \begin{array}{ll} -\Delta u=\mu u\,\, &\text{in}\,\, B,\\
\partial_\texttt{n} u+\alpha u=0 &\text{on}\,\, \partial B \end{array} \right.
\end{equation*} on the unit ball of $\mathbb{R}^N$. We obtain the complete
spectral structure of this problem. In particular, for $\alpha>0$, we find that
the first eigenvalue is $k_{\nu,1}^2$ and the second eigenvalue is exactly
$k_{\nu+1,1}^2$, where $k_{\nu+l,m}$ is the $m$th positive zero of
$kJ_{\nu+l+1}(k)-(\alpha+l) J_{\nu+l}(k)$. Moreover, when $\alpha\in(-1,0)$,
the first eigenvalue is $-\widehat{k}_{\nu,1}^2$ where $\widehat{k}_{\nu,1}$
denotes the unique zero of $\alpha I_{\nu}(k)+kI_{\nu+1}(k)$, and the second
eigenvalue is exactly $k_{\nu+1,1}^2$. Furthermore, for $\alpha=-1$, the first
eigenvalue is $-\widehat{k}_{\nu,1}^2$ and the second eigenvalue is exactly
$0$. Our conclusions indicate the ratio $\mu_2/\mu_1$ may be positive, negative
or zero according to the suitable ranges of the parameter $\alpha$.

</details>


### [22] [Tangential approach in the Dirichlet problem for elliptic equations](https://arxiv.org/abs/2510.26400)
*Jonathan Bennett,Arnaud Dumont,Andrew J. Morris*

Main category: math.AP

TL;DR: The paper proves that L^p-Dirichlet problem solvability for elliptic equations implies improved tangential convergence for boundary data with Sobolev regularity, with sharp estimates on convergence failure sets.


<details>
  <summary>Details</summary>
Motivation: To extend classical results about harmonic functions to elliptic equations with measurable coefficients, showing that local A‚àû property enables tangential convergence beyond nontangential convergence.

Method: Analyzes the L^p-Dirichlet problem for elliptic equations with real-valued, bounded measurable coefficients on Lipschitz domains, using harmonic measure properties and Sobolev regularity.

Result: Proves that local A‚àû property guarantees tangential convergence for boundary data with Sobolev regularity, with sharp Hausdorff dimension estimates for convergence failure sets.

Conclusion: The results generalize classical harmonic function theory to elliptic equations, showing improved convergence properties under Sobolev regularity conditions.

Abstract: It is well-known that solvability of the $\mathrm{L}^{p}$-Dirichlet problem
for elliptic equations $Lu:=-\mathrm{div}(A\nabla u)=0$ with real-valued,
bounded and measurable coefficients $A$ on Lipschitz domains
$\Omega\subset\mathbb{R}^{1+n}$ is characterised by a quantitative absolute
continuity of the associated $L$-harmonic measure. We prove that this local
$A_{\infty}$ property is sufficient to guarantee that the nontangential
convergence afforded to $\mathrm{L}^{p}$ boundary data actually improves to a
certain \emph{tangential} convergence when the data has additional (Sobolev)
regularity. Moreover, we obtain sharp estimates on the Hausdorff dimension of
the set on which such convergence can fail. This extends results obtained by
Dorronsoro, Nagel, Rudin, Shapiro and Stein for classical harmonic functions in
the upper half-space.

</details>


### [23] [Improved Gevrey Class Regularity of the Kadomtsev Petviashvili Equation](https://arxiv.org/abs/2510.26669)
*Aissa Boukarou,Lamia Seghour*

Main category: math.AP

TL;DR: This paper improves previous results on Gevrey regularity for a fifth-order KP-type equation, establishing sharper time regularity bounds and proving optimality of the regularity exponent.


<details>
  <summary>Details</summary>
Motivation: To extend and improve the Gevrey regularity results obtained by Boukarou et al. for fifth-order KP-type equations, providing sharper bounds and optimality conditions.

Method: Uses the method of majorant series to simultaneously treat all three variables (x, y, t), precisely tracking the influence of higher-order dispersive terms and lower-order terms in the equation.

Result: Proves that if initial data are Gevrey regular of order œÉ ‚â• 1 in spatial variables, then the solution is Gevrey regular of order 5œÉ in time, and this bound is optimal (solution does not belong to G^z for any 1 ‚â§ z < 5œÉ).

Conclusion: The paper establishes optimal Gevrey regularity results for fifth-order KP-type equations, with the regularity exponent in time being exactly 5 times the spatial regularity exponent.

Abstract: In this paper, we improve and extend the results obtained by Boukarou et al.
\cite{boukarou1} on the Gevrey regularity of solutions to a fifth-order
Kadomtsev-Petviashvili (KP)-type equation. We establish Gevrey regularity in
the time variable for solutions in $2+1$ dimensions, providing a sharper result
obtained through a new analytical approach. Assuming that the initial data are
Gevrey regular of order $\sigma \geq 1$ in the spatial variables, we prove that
the corresponding solution is Gevrey regular of order $5 \sigma$ in time.
Moreover, we show that the function $u(x, y, t)$, viewed as a function of $t$,
does not belong to $G^z$ for any $1 \leq z<5 \sigma$. The proof simultaneously
treats all three variables $x, y$, and $t$, and employs the method of majorant
series, precisely tracking the influence of the higher-order dispersive term
$\partial_x^5 u$ together with the lower-order terms $\alpha \partial_x^3 u,
\partial_x^{-1} \partial_y^2 u$, and $u \partial_x u$.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [24] [Equation Discovery, Parametric Simulation, and Optimization Using the Physics-Informed Neural Network (PINN) Method for the Heat Conduction Problem](https://arxiv.org/abs/2510.25925)
*Ehsan Ghaderi,Mohamad Ali Bijarchi,Siamak Kazemzadeh Hannani,Ali Nouri Boroujerdi*

Main category: physics.comp-ph

TL;DR: PINN method applied to heat conduction problems for modeling, simulation, and optimization tasks, showing strength in inverse problems and data-driven applications.


<details>
  <summary>Details</summary>
Motivation: To investigate PINN capabilities for three fundamental engineering tasks in heat conduction: modeling with equation discovery, parametric simulation, and optimization of inverse problems.

Method: Used Physics-Informed Neural Networks for: 1) Equation discovery with fractional-order derivatives to reconstruct heat transfer equations, 2) Parametric simulation treating thermal conductivity as variable parameter, 3) Inverse problem optimization to infer unknown physical properties from data.

Result: PINNs demonstrated strong performance for parametric simulation, optimization, and equation discovery, though not yet outperforming traditional methods in speed and accuracy for forward problems.

Conclusion: PINNs provide a powerful and flexible framework particularly valuable for inverse problems and data-driven modeling applications in heat conduction analysis.

Abstract: In this study, the capabilities of the Physics-Informed Neural Network (PINN)
method are investigated for three major tasks: modeling, simulation, and
optimization in the context of the heat conduction problem. In the modeling
phase, the governing equation of heat transfer by conduction is reconstructed
through equation discovery using fractional-order derivatives, enabling the
identification of the fractional derivative order that best describes the
physical behavior. In the simulation phase, the thermal conductivity is treated
as a physical parameter, and a parametric simulation is performed to analyze
its influence on the temperature field. In the optimization phase, the focus is
placed on the inverse problem, where the goal is to infer unknown physical
properties from observed data. The effectiveness of the PINN approach is
evaluated across these three fundamental engineering problem types and compared
against conventional numerical methods. The results demonstrate that although
PINNs may not yet outperform traditional numerical solvers in terms of speed
and accuracy for forward problems, they offer a powerful and flexible framework
for parametric simulation, optimization, and equation discovery, making them
highly valuable for inverse and data-driven modeling applications.

</details>


### [25] [Generative Artificial Intelligence for Air Shower Simulation](https://arxiv.org/abs/2510.26316)
*C. Bozza,A. Caliv√†,A. De Caro,D. De Gruttola,S. De Pasquale,L. A. Fusco,G. Messuti,C. Poir√®,S. Scarpetta,T. Virgili*

Main category: physics.comp-ph

TL;DR: GAN-based approach accelerates cosmic ray air shower simulations by 10,000x compared to traditional Monte Carlo methods while maintaining accuracy in key particle distributions.


<details>
  <summary>Details</summary>
Motivation: Traditional Monte Carlo simulations of cosmic ray air showers are computationally intensive and consume major resources in astroparticle physics experiments, creating need for faster alternatives.

Method: Developed and trained Generative Adversarial Networks (GANs) on CORSIKA-generated dataset of high-energy proton-induced air showers to learn and reproduce secondary particle distributions.

Result: GAN model reduces generation time per shower by factor of 10^4 compared to full CORSIKA simulation, while accurately reproducing key distributions like muon energy spectra and spatial distributions at ground level.

Conclusion: GAN-based approach provides substantial acceleration in air shower simulations with significant reductions in computational time and energy consumption, enabling more efficient large-scale simulations for astroparticle physics experiments.

Abstract: The detailed simulation of extensive air showers, produced by primary cosmic
rays interacting in the atmosphere, is a task that is traditionally undertaken
by means of Monte Carlo methods. These processes are computationally intensive,
accounting for a major fraction of the computational resources used in the
large-scale simulations required by current and future experiments in the field
of astroparticle physics. In this work, we present a novel approach based on
Generative Adversarial Networks (GANs) to accelerate air shower simulations. We
developed and trained a GAN on a dataset of high-energy proton-induced air
showers generated with \texttt{CORSIKA}; our model reproduces key distributions
of secondary particles, such as energy spectra and spatial distributions at
ground level of muons. Once the model has been trained, which takes
approximately 74 hours, the generation real time per shower is reduced by a
factor of $10^4$ with respect to the full \texttt{CORSIKA} simulation, leading
to a substantial decrease in both computational time and energy consumption.

</details>


### [26] [Patch-MLP-Based Predictive Control: Simulation of Upstream Pointing Stabilization for PHELIX Laser System](https://arxiv.org/abs/2510.26540)
*Jiaying Wang,Jonas Benjamin Ohland,Yen-Yu Chang,Vedhas Pandit,Stefan Bock,Andrew-Hiroaki Okukura,Udo Eisenbarth,Arie Irman,Michael Bussmann,Ulrich Schramm,Jeffrey Kelling*

Main category: physics.comp-ph

TL;DR: Predictive control using Patch-MLP neural network for beam pointing stability in high-energy lasers, reducing jitter by 10-20% compared to traditional PID control.


<details>
  <summary>Details</summary>
Motivation: Traditional PID control is limited by time delays and mechanical inertia in laser systems, requiring improved beam pointing stability for reproducibility in high-energy laser experiments.

Method: Patch-based multilayer perceptron (Patch-MLP) predicts beam pointing errors by capturing local temporal patterns, combined with PID controller for error correction, using feed-forward control to compensate system delays.

Result: Simulations show predictive control reduces residual jitter by 10-20% over conventional PID, maintains stable performance without drift over 10-hour dataset, and improves standard pointing metrics.

Conclusion: The predictive controller operates without drift and may improve reproducibility and operational efficiency in high-energy, low repetition rate laser experiments.

Abstract: High-energy laser facilities such as PHELIX at GSI require excellent beam
pointing stability for reproducibility and relative independence for future
experiments. Beam pointing stability has been traditionally achieved using
simple proportional-integral-derivative (PID) control which removes the problem
of slow drift, but is limited because of the time delay in knowing the
diagnosis and the inertia in the mechanical system associated with mirrors. In
this work, we introduce a predictive control strategy where the forecasting of
beam pointing errors is performed by a patch-based multilayer perceptron
(Patch-MLP) designed to capture local temporal patterns for more robust
short-term jitter prediction. The subsequent conversion of these predicted
errors into correction signals is handled by a PID controller. The neural
network has been trained on diagnostic time-series data to predict beam
pointing error. Using the feed-forward controller compensates for system
delays. Simulations with a correction mirror placed upstream of the PHELIX
pre-amplifier bridge confirm that the predictive control scheme reduces
residual jitter compared to conventional PID control. Over a 10-hour dataset
the controller maintained stable performance without drift, while standard
pointing metrics showed consistent improvements of the order of 10 to 20
percent. The predictive controller operates without drift, and therefore may
improve reproducibility and operational efficiency in high energy, low
repetition rate laser experiment conditions.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [A Self-Consistent Model of Kinetic Alfven Solitons in Pulsar Wind Plasma: Linking Soliton Characteristics to Pulsar Observables](https://arxiv.org/abs/2510.25972)
*Manpreet Singh,Geetika Slathia,N. S. Saini,Siming Liu*

Main category: physics.plasm-ph

TL;DR: A model for kinetic Alfven solitons in pulsar winds, showing how soliton properties depend on pulsar parameters like spin period, plasma composition, and particle distributions.


<details>
  <summary>Details</summary>
Motivation: To understand the formation and propagation of nonlinear wave structures in pulsar magnetospheres and link soliton dynamics to observable pulsar characteristics.

Method: Used reductive perturbation approach to derive a Korteweg-de Vries (KdV) equation governing nonlinear evolution of kinetic Alfven solitons in relativistic electron-positron-ion plasma.

Result: Soliton amplitude and width depend on pulsar spin period, spin-down rate, pair multiplicity, plasma composition, and particle distributions. Heavier ions produce broader solitons, higher pair multiplicity yields smaller solitons, and oblique propagation creates wider but lower-amplitude structures.

Conclusion: The model provides a framework for interpreting pulsar magnetospheric microphysics and emission signatures by linking soliton dynamics to measurable pulsar parameters, with millisecond pulsars hosting the narrowest solitons.

Abstract: We present a self-consistent model for the formation and propagation of
kinetic Alfven (KA) solitons in the pulsar wind zone, where a relativistic,
magnetized electron positron ion plasma flows along open magnetic field lines
beyond the light cylinder. Using a reductive perturbation approach, we derive a
Korteweg de Vries (KdV) equation that governs the nonlinear evolution of KA
solitons in this environment. The soliton amplitude and width are shown to
depend sensitively on key pulsar observables, including spin period, spin-down
rate, and pair multiplicity as well as plasma composition and suprathermal
particle distributions. Our analysis reveals that soliton structures are
strongly influenced by the presence of heavy ions, kappa-distributed pairs, and
oblique propagation angles. Heavier ion species such as Fe26+ produce
significantly broader solitons due to enhanced inertia and dispersion, while
increasing pair multiplicity leads to smaller solitons through stronger
screening. Oblique propagation (larger theta) results in wider but
lower-amplitude solitons, and more thermalized pair plasmas (higher kappa)
support taller and broader structures. A population-level analysis of 1174
pulsars shows a clear positive correlation between soliton width and spin
period, with millisecond pulsars hosting the narrowest solitons. By linking
soliton dynamics to measurable pulsar parameters, this work provides a
framework for interpreting magnetospheric microphysics and its role in shaping
pulsar emission signatures.

</details>


### [28] [Optimization of the Compact Stellarator with Simple Coils at finite-beta](https://arxiv.org/abs/2510.26155)
*Haorong Qiu,Guodong Yu,Peiyou Jiang,Guoyong Fu*

Main category: physics.plasm-ph

TL;DR: Single-stage optimization of coil currents in the CSSC stellarator mitigates finite beta effects on neoclassical confinement.


<details>
  <summary>Details</summary>
Motivation: The CSSC stellarator, optimized in vacuum, suffers from detrimental finite beta effects on confinement, requiring optimization at finite plasma beta.

Method: Simply modifying coil currents of the CSSC through single-stage optimization.

Result: Finite beta effects can be largely mitigated by reducing the coil currents of CSSC.

Conclusion: Optimizing coil currents is an effective approach to improve stellarator performance at finite beta.

Abstract: An optimized stellarator at finite plasma beta is realized by single-stage
optimization of simply modifying the coil currents of the Compact Stellarator
with Simple Coils (CSSC)[Yu et al., J. Plasma Physics 88,905880306 (2022)]. The
CSSC is an optimized stellarator obtained by direct optimization via coil
shapes, with its coil topology similar to that of the Columbia Non-neutral
Torus (CNT) [Pederson et al., Phys. Rev. Lett. 88, 205002 (2002)]. Due to its
vacuum-based optimization, the CSSC exhibits detrimental finite beta effects on
neoclassical confinement. The results of optimization show that the finite beta
effects can be largely mitigated by reducing the coil currents of CSSC.

</details>


### [29] [Design and Implementation of a Fast-Sweeping Langmuir Probe Diagnostic for DC Arc Jet Environments](https://arxiv.org/abs/2510.26162)
*Sebastian V. Colom,Magnus A. Haw,Jocelino Rodrigues*

Main category: physics.plasm-ph

TL;DR: Development of an open-source, low-cost fast-sweeping Langmuir probe system capable of 200 kHz temporal resolution for measuring transient plasma behavior in high-enthalpy environments.


<details>
  <summary>Details</summary>
Motivation: Conventional Langmuir probes lack sufficient temporal resolution to capture transient plasma behavior in dynamic environments, limiting plasma characterization capabilities.

Method: Designed and implemented a fast-sweeping Langmuir probe system with voltage sweeping capability, tested in the 30 kW miniature Arc jet Research Chamber (mARC II) under extreme aerothermal conditions.

Result: The system successfully operated in high-enthalpy conditions, providing time-resolved electron temperature and density measurements along the flow's radial profile at up to 200 kHz resolution.

Conclusion: Established a robust and accessible Langmuir diagnostic solution for researchers studying transient plasma behavior in high-enthalpy environments.

Abstract: Langmuir probe diagnostics are a cornerstone of plasma characterization,
providing critical measurements of electron temperature, electron density, and
plasma potential. However, conventional swept Langmuir probes and other
traditional electrostatic probes often lack the temporal resolution necessary
to capture transient plasma behavior in dynamic environments. This paper
presents the design and implementation of a fast-sweeping Langmuir probe system
that is open-source, low-cost, and adaptable for a wide range of plasma
applications. The probe system incorporates voltage sweeping to resolve rapid
fluctuations in plasma parameters at a temporal resolution of up to 200 kHz. To
validate its performance, the system was implemented in the 30 kW miniature Arc
jet Research Chamber (mARC II), a high-enthalpy DC arc jet facility designed
for prototype testing and development. Experimental results demonstrate the
probe's capability to operate in extreme aerothermal conditions, providing
time-resolved electron temperature and density along the flow's radial profile.
This work establishes a robust and accessible Langmuir diagnostic solution for
researchers studying transient plasma behavior in high-enthalpy environments.

</details>


### [30] [High-order Mie resonance and transient field enhancement in laser-driven plasma nanoshells](https://arxiv.org/abs/2510.26175)
*Xiaohui Gao*

Main category: physics.plasm-ph

TL;DR: Plasma nanoshells achieve significant field enhancement via high-order Mie resonances, with optimal geometries providing ~3x enhancement for 800 nm light before plasma expansion disrupts resonance in tens of femtoseconds.


<details>
  <summary>Details</summary>
Motivation: To optimize laser-plasma interactions for applications like laser-cluster interaction diagnostics and energetic ion production from engineered core-shell targets by understanding field enhancement in plasma nanoshells.

Method: Combined Mie theory and particle-in-cell simulations to analyze field enhancement through high-order Mie resonances in plasma nanoshells with various geometries.

Result: Optimal shell geometries yield approximately threefold electric field enhancement for 800 nm irradiation, with transient buildup times of tens of femtoseconds before plasma expansion disrupts resonance. Few-cycle pulses show reduced enhancement due to insufficient resonance establishment.

Conclusion: Temporal dynamics play a critical role in nanoplasma resonances, and these findings enable optimized laser-plasma interactions for various applications including diagnostics and energetic ion production.

Abstract: We demonstrate substantial field enhancement in plasma nanoshells through
high-order Mie resonances using combined Mie theory and particle-in-cell
simulations. Optimal shell geometries yield approximately threefold electric
field enhancement for 800 nm irradiation, with transient buildup times of tens
of femtoseconds before plasma expansion disrupts resonance. Few-cycle pulses
produce reduced enhancement due to insufficient resonance establishment. These
findings enable optimized laser-plasma interactions for applications including
diagnostics of laser-cluster interaction and energetic ion production from
engineered core-shell targets, highlighting the critical role of temporal
dynamics in nanoplasma resonances.

</details>


### [31] [Nonlocal Model for Electron Heat Flux and Self-generated Magnetic Field](https://arxiv.org/abs/2510.26640)
*Xinyu Zhu,Wenqiang Yuan,Yusen Wang,Zhipeng Zhang,Xianxu Jin,Zhonghai Zhao,Bin Qiao*

Main category: physics.plasm-ph

TL;DR: A new nonlocal model that simultaneously recovers kinetic effects for both electron heat conduction and magnetic field in hydrodynamic scale, addressing limitations of current flux limiters.


<details>
  <summary>Details</summary>
Motivation: Current models use flux limiters for magnetic field kinetic effects rather than nonlocal corrections, and there's a need to self-consistently consider electric field corrections for accurate physical quantities in inertial confinement fusion.

Method: Proposed a new nonlocal model that couples electron heat conduction and magnetic field, incorporating self-consistent electric field corrections to model kinetic effects in hydrodynamic scale.

Result: Nonlocal effects significantly change magnetic field distribution in laser ablation, and the model enables systematic study of transport coefficients in magnetized plasma and magnetic field generation without density gradients.

Conclusion: The nonlocal model reveals that nonlocal effects can potentially influence hydrodynamic instabilities in inertial confinement fusion by altering magnetic field distributions.

Abstract: Coupling of electron heat conduction and magnetic field takes significant
effects in inertial confinement fusion (ICF). As the nonlocal models for
electron heat conduction have been developed for modeling kinetic effects on
heat flux in hydrodynamic scale, modeling kinetic effects on magnetic field are
still restricted to flux limiters instead of nonlocal corrections. We propose a
new nonlocal model which can recover the kinetic effects for heat conduction
and magnetic field in hydrodynamic scale simultaneously. We clarify the
necessity of self-consistently considering the electric field corrections in
nonlocal models to get reasonable physical quantities. Using the new nonlocal
model, the nonlocal corrections of transport coefficients in magnetized plasma
and the magnetic field generation without density gradients are systematically
studied. We find nonlocal effects significantly change the magnetic field
distribution in laser ablation, which potentially influences the hydrodynamic
instabilities in ICF.

</details>


### [32] [Spectral Deconvolution without the Deconvolution: Extracting Temperature from X-ray Thomson Scattering Spectra without the Source-and-Instrument Function](https://arxiv.org/abs/2510.26747)
*Thomas Gawne,Alina Kononov,Andrew Baczewski,Hannah Bellenbaum,Maximilian P B√∂hme,Zhandos Moldabekov,Thomas R Preston,Sebastian Schwalbe,Jan Vorberger,Tobias Dornheim*

Main category: physics.plasm-ph

TL;DR: Proposes a method to extract temperature from X-ray Thomson scattering spectra without needing to know the source-and-instrument function, using ratios of Laplace-transformed spectra from different scattering angles.


<details>
  <summary>Details</summary>
Motivation: The source-and-instrument function (SIF) broadens XRTS spectra and is difficult to measure accurately, making temperature extraction challenging. Existing deconvolution methods depend heavily on SIF shape.

Method: Use ratios of Laplace-transformed XRTS spectra collected at different scattering angles, which effectively performs deconvolution without requiring explicit SIF knowledge.

Result: The method directly extracts temperature from scattering spectra for systems in thermal equilibrium, is robust to spectral noise and spectrometer differences, and can identify non-equilibrium effects through inconsistent temperature readings.

Conclusion: The ratio-based Laplace transform approach provides a reliable way to extract temperature from XRTS spectra without SIF knowledge, with potential for detecting non-equilibrium conditions.

Abstract: X-ray Thomson scattering (XRTS) probes the dynamic structure factor of the
system, but the measured spectrum is broadened by the combined
source-and-instrument function (SIF) of the setup. In order to extract
properties such as temperature from an XRTS spectrum, the broadening by the SIF
needs to be removed. Recent work [Dornheim et al. Nature Commun. 13, 7911
(2022)] has suggested that the SIF may be deconvolved using the two-sided
Laplace transform. However, the extracted information can depend strongly on
the shape of the input SIF, and the SIF is in practice challenging to measure
accurately. Here, we propose an alternative approach: we demonstrate that
considering ratios of Laplace-transformed XRTS spectra collected at different
scattering angles is equivalent to performing the deconvolution, but without
the need for explicit knowledge of the SIF. From these ratios, it is possible
to directly extract the temperature from the scattering spectra, when the
system is in thermal equilibrium. We find the method to be generally robust to
spectral noise and physical differences between the spectrometers, and we
explore situations in which the method breaks down. Furthermore, the fact that
consistent temperatures can be extracted for systems in thermal equilibrium
indicates that non-equilibrium effects could be identified by inconsistent
temperatures of a few eV between the ratios of three or more scattering angles.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [33] [Gradient Flow Sampler-based Distributionally Robust Optimization](https://arxiv.org/abs/2510.25956)
*Zusen Xu,Jia-Jie Zhu*

Main category: math.OC

TL;DR: A PDE gradient flow framework for distributionally robust optimization (DRO) that connects MCMC sampling with gradient flow theory to develop practical algorithms for sampling from worst-case distributions.


<details>
  <summary>Details</summary>
Motivation: To provide a mathematically principled foundation for DRO by leveraging gradient flow theory, offering new insights into existing methods and enabling construction of novel algorithms.

Method: Uses PDE gradient flow framework with Wasserstein Fisher-Rao and Stein variational gradient flows to solve Wasserstein and Sinkhorn DRO problems, connecting MCMC sampling with distributional optimization.

Result: The framework recovers previously proposed DRO methods exactly, provides new theoretical insights into optimization dynamics, and numerical studies with stochastic gradient descent support the theoretical findings.

Conclusion: The gradient flow perspective offers a unified theoretical foundation for DRO that enables both recovery of existing methods and development of new algorithms, with empirical validation supporting the approach.

Abstract: We propose a mathematically principled PDE gradient flow framework for
distributionally robust optimization (DRO). Exploiting the recent advances in
the intersection of Markov Chain Monte Carlo sampling and gradient flow theory,
we show that our theoretical framework can be implemented as practical
algorithms for sampling from worst-case distributions and, consequently, DRO.
While numerous previous works have proposed various reformulation techniques
and iterative algorithms, we contribute a sound gradient flow view of the
distributional optimization that can be used to construct new algorithms. As an
example of applications, we solve a class of Wasserstein and Sinkhorn DRO
problems using the recently-discovered Wasserstein Fisher-Rao and Stein
variational gradient flows. Notably, we also show some simple reductions of our
framework recover exactly previously proposed popular DRO methods, and provide
new insights into their theoretical limit and optimization dynamics. Numerical
studies based on stochastic gradient descent provide empirical backing for our
theoretical findings.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [34] [Exciton dynamics in equilibrium and nonequilibrium regimes](https://arxiv.org/abs/2510.26221)
*Pushpendra Yadav*

Main category: cond-mat.mes-hall

TL;DR: First-principles study of excitons in 2D materials showing density-dependent redshift-blueshift crossover, electron-phonon effects on spectra/lifetimes, and electron-hole liquid formation at high densities and low temperatures.


<details>
  <summary>Details</summary>
Motivation: To understand nonequilibrium exciton behavior at high excitation densities in 2D materials, where phenomena like electron-hole liquids emerge but are less explored theoretically.

Method: Uses GW approximation and Bethe-Salpeter equation to study exciton properties from equilibrium to nonequilibrium conditions in two-dimensional materials.

Result: Shows redshift-blueshift crossover with increasing carrier density, electron-phonon effects on optical spectra and exciton lifetimes, and formation of electron-hole liquid phase above critical density and below critical temperature.

Conclusion: Enhanced Coulomb interactions in 2D materials can stabilize electron-hole liquid phase at significantly higher temperatures, identifying promising material candidates for observing these collective states.

Abstract: The bound electron-hole pairs known as excitons govern the optical properties
of insulating solids. While their behavior in equilibrium is well-understood
theoretically, the nonequilibrium regime at high excitation densities-where
phenomena like electron-hole liquids emerge - is less explored. This thesis
presents a first-principles study of excitons in two-dimensional materials. We
use the GW approximation and the Bethe-Salpeter equation to investigate their
properties from equilibrium to nonequilibrium conditions. We first demonstrate
how increasing photo-excited carrier density leads to a redshift-blueshift
crossover of excitons. We then show that electron-phonon interactions
critically modify optical spectra and exciton lifetimes at finite temperatures.
Finally, we unify these effects to demonstrate the formation of an
electron-hole liquid phase above a critical carrier density and below a
critical temperature. Our work identifies how enhanced Coulomb interactions in
two dimensions can stabilize this phase at significantly higher temperatures,
proposing promising material candidates for observing these collective states.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [35] [Generative sampling with physics-informed kernels](https://arxiv.org/abs/2510.26678)
*Friederike Ihssen,Renzo Kapust,Jan M. Pawlowski*

Main category: hep-lat

TL;DR: A generative network for Monte-Carlo sampling in lattice field theories that uses physics-informed renormalisation group flows to enable independent layerwise learning and optimization.


<details>
  <summary>Details</summary>
Motivation: To address out-of-domain problems in generative models and improve optimization for Monte-Carlo sampling in lattice field theories.

Method: Uses physics-informed renormalisation group flows that transform the generative task into solving independent linear differential equations for transformation kernels, allowing iterative refinement.

Result: Practical feasibility demonstrated through simulations in scalar field theories.

Conclusion: The architecture enables structural handling of out-of-domain problems and opens paths for further optimization in generative models for field theories.

Abstract: We construct a generative network for Monte-Carlo sampling in lattice field
theories and beyond, for which the learning of layerwise propagation is done
and optimised independently on each layer. The architecture uses
physics-informed renormalisation group flows that provide access to the
layerwise propagation step from one layer to the next in terms of a simple
first order partial differential equation for the respective renormalisation
group kernel through a given layer. Thus, it transforms the generative task
into that of solving once the set of independent and linear differential
equations for the kernels of the transformation. As these equations are
analytically known, the kernels can be refined iteratively. This allows us to
structurally tackle out-of-domain problems generally encountered in generative
models and opens the path to further optimisation. We illustrate the practical
feasibility of the architecture within simulations in scalar field theories.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [36] [Geometric and Orbital Control of Correlated States in Small Hubbard Clusters](https://arxiv.org/abs/2510.25919)
*Shivanshu Dwivedi,Kalum Palandage*

Main category: cond-mat.str-el

TL;DR: A framework for engineering local electron pairing in quantum dot arrays using three control levers: lattice geometry, orbital hybridization, and electric fields, with design principles based on coordination number, inter-orbital hopping, and field-induced localization.


<details>
  <summary>Details</summary>
Motivation: To establish a predictive framework for designing correlated quantum matter in semiconductor quantum dot arrays by systematically controlling electron pairing through geometric, orbital, and electric field parameters.

Method: Hartree-Fock simulations on 3D quantum dot clusters (from tetrahedron to FCC lattice) at and near half-filling, analyzing the effects of coordination number, inter-orbital hopping, and electric fields on electron pairing.

Result: Three fundamental design principles: (1) Geometric hierarchy where pairing resilience depends on coordination number, (2) Orbital hybridization enhances double occupancy at moderate Coulomb repulsion, (3) Electric fields robustly induce pairing through charge localization, especially in low-connectivity clusters.

Conclusion: These principles provide a blueprint for deterministic control of charge and spin correlations in quantum-dot-based quantum hardware.

Abstract: Arrays of semiconductor quantum dots provide a powerful platform to design
correlated quantum matter from the bottom up. We establish a predictive
framework for engineering local electron pairing in these artificial molecules
by systematically deploying three control levers: lattice geometry, orbital
hybridization, and external electric fields. Using Hartree-Fock simulations on
canonical 3D clusters from the tetrahedron (Z = 3) to the FCC lattice (Z = 12),
at and near half-filling, we uncover three fundamental design principles. (i)
Geometric Hierarchy: The resilience to Coulomb repulsion U is dictated by the
coordination number Z, which controls kinetic delocalization. (ii) Orbital
Hybridization: Counter-intuitively, inter-orbital hopping t_orb acts not as a
simple suppressor of pairing, but as a sophisticated control knob that enhances
double occupancy at moderate U by engineering the on-site energy landscape.
(iii) Field Squeezing: An electric field robustly induces pairing by forcing
charge localization, an effect most potent in low-connectivity clusters. These
principles form a blueprint for deterministically targeting charge and spin
correlations in quantum-dot-based quantum hardware.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [37] [Temperature dependent ferroelectricity in strained KTaO3 with machine learned force field](https://arxiv.org/abs/2510.26693)
*Yu Zhu,Luigi Ranalli,Taikang Chen,Wei Ren,Cesare Franchini*

Main category: cond-mat.mtrl-sci

TL;DR: Study investigates how 0-1% uniaxial and biaxial strain affects KTaO3's potential for ferroelectricity using advanced computational methods including DFT and machine learning.


<details>
  <summary>Details</summary>
Motivation: To understand how strain engineering can induce ferroelectric order in incipient ferroelectrics like KTaO3, which has fundamental and applied significance.

Method: Combined density functional theory with stochastic self-consistent harmonic approximation and machine learned force fields to study structural and dynamical properties under strain, using Berry phase method for polarization calculations up to 300K.

Result: Found that strain can drive ferroelectric phase transition in KTaO3 by breaking inversion symmetry, providing accurate polarization data across temperature range.

Conclusion: Strain engineering effectively stabilizes ferroelectricity in KTaO3, offering valuable guidance for future experimental and theoretical work on strain-engineered ferroelectric materials.

Abstract: Ferroelectric materials are a class of dielectrics that exhibit spontaneous
polarization which can be reversed under an external electric field. The
emergence of ferroelectric order in incipient ferroelectrics is a topic of
considerable interest from both fundamental and applied perspectives. Among the
various strategies explored, strain engineering has been proven to be a
powerful method for tuning ferroelectric polarization in materials. In the case
of KTaO3, first principles calculations have suggested that strain can drive a
ferroelectric phase transition. In this study, we investigate the impact of
in-plane uniaxial and biaxial strain, ranging from 0% to 1%, on pristine KTaO3
to explore its potential for ferroelectricity induction via inversion symmetry
breaking. By integrating density functional theory calculations with the
stochastic self-consistent harmonic approximation assisted by on the fly
machine learned force field, we obtain accurate structural information and
dynamical properties under varying strain conditions while incorporating
higher-order anharmonic effects. Employing the Berry phase method, we obtained
the ferroelectric polarization of the strained structures over the entire
temperature range up to 300 K. Our findings provide valuable insights into the
role of strain in stabilizing ferroelectricity in KTaO3, offering guidance for
future experimental and theoretical studies on strain-engineered ferroelectric
materials.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [38] [A game-theoretic approach to the parabolic normalized p-Laplacian obstacle problem](https://arxiv.org/abs/2510.25999)
*Hamid El Bahja*

Main category: math.PR

TL;DR: This paper develops a probabilistic representation for the parabolic obstacle problem with normalized p-Laplacian using a zero-sum stochastic tug-of-war game with stopping options.


<details>
  <summary>Details</summary>
Motivation: To establish a game-theoretic interpretation and probabilistic representation for the parabolic obstacle problem associated with the normalized p-Laplacian operator.

Method: Introduces a zero-sum stochastic tug-of-war game with noise in space-time, where one player can stop the game to collect payoff from an obstacle function. Proves existence of value functions and dynamic programming principle.

Result: Value functions exist, satisfy dynamic programming principle, and converge uniformly to the unique viscosity solution of the continuous obstacle problem as step size Œµ‚Üí0.

Conclusion: The stochastic game provides a probabilistic representation for the parabolic obstacle problem with normalized p-Laplacian, with convergence to the viscosity solution.

Abstract: This paper establishes a probabilistic representation for the solution of the
parabolic obstacle problem associated with the normalized $p$-Laplacian. We
introduce a zero-sum stochastic tug-of-war game with noise in a space-time
cylinder, where one player has the option to stop the game at any time to
collect a payoff given by an obstacle function. We prove that the value
functions of this game exist, satisfy a dynamic programming principle, and
converge uniformly to the unique viscosity solution of the continuous obstacle
problem as the step size $\varepsilon$ tends to zero.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [A Practitioner's Guide to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2510.25781)
*Amir Noorizadegan,Sifan Wang,Leevan Ling*

Main category: cs.LG

TL;DR: A comprehensive review of Kolmogorov-Arnold Networks (KANs) covering their theoretical foundations, architectural variants, implementation strategies, and ecosystem mapping.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic overview of the rapidly expanding KAN landscape beyond simple performance comparisons, bridging the conceptual gap between KANs and MLPs.

Method: Collecting and categorizing open-source implementations, analyzing basis function choices (B-splines, polynomials, ReLU, etc.), and surveying techniques for accuracy, efficiency, and regularization.

Result: A structured synthesis of KAN research including theoretical foundations, architectural variants, and practical implementation guidance with an associated GitHub repository.

Conclusion: Provides a practical guide for selecting KAN architectures and identifies current research gaps, serving as a reference for ongoing KAN research.

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising
alternative to traditional Multilayer Perceptrons (MLPs), inspired by the
Kolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed
activation functions on nodes, KANs employ learnable univariate basis functions
on edges, offering enhanced expressivity and interpretability. This review
provides a systematic and comprehensive overview of the rapidly expanding KAN
landscape, moving beyond simple performance comparisons to offer a structured
synthesis of theoretical foundations, architectural variants, and practical
implementation strategies. By collecting and categorizing a vast array of
open-source implementations, we map the vibrant ecosystem supporting KAN
development. We begin by bridging the conceptual gap between KANs and MLPs,
establishing their formal equivalence and highlighting the superior parameter
efficiency of the KAN formulation. A central theme of our review is the
critical role of the basis function; we survey a wide array of choices,
including B-splines, Chebyshev and Jacobi polynomials, ReLU compositions,
Gaussian RBFs, and Fourier series, and analyze their respective trade-offs in
terms of smoothness, locality, and computational cost. We then categorize
recent advancements into a clear roadmap, covering techniques for improving
accuracy, efficiency, and regularization. Key topics include physics-informed
loss design, adaptive sampling, domain decomposition, hybrid architectures, and
specialized methods for handling discontinuities. Finally, we provide a
practical "Choose-Your-KAN" guide to help practitioners select appropriate
architectures, and we conclude by identifying current research gaps. The
associated GitHub repository https://github.com/AmirNoori68/kan-review
complements this paper and serves as a structured reference for ongoing KAN
research.

</details>


### [40] [Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training](https://arxiv.org/abs/2510.25803)
*Hong Wang,Haiyang Xin,Jie Wang,Xuanze Yang,Fei Zha,Huanshuo Dong,Yan Jiang*

Main category: cs.LG

TL;DR: Proposes MoE-POT, a sparse Mixture-of-Experts architecture for PDE neural operators that efficiently scales parameters while controlling inference costs, achieving 40% error reduction with fewer activated parameters.


<details>
  <summary>Details</summary>
Motivation: Address challenges in PDE pre-training: heterogeneity of equation types causing high mixed training errors, and dense models' high inference costs from parameter scaling.

Method: Uses sparse-activated MoE architecture with layer-wise router-gating to dynamically select 4 experts from 16, plus 2 shared experts to capture common PDE properties. Output is weighted average of activated experts.

Result: Pre-trained models from 30M to 0.5B parameters on 6 PDE datasets. 90M activated parameter model achieves 40% zero-shot error reduction vs 120M parameter baselines. Router decisions can infer dataset types.

Conclusion: MoE-POT effectively handles PDE heterogeneity while controlling inference costs, with interpretable expert selection that validates the architecture's effectiveness.

Abstract: Pre-training has proven effective in addressing data scarcity and performance
limitations in solving PDE problems with neural operators. However, challenges
remain due to the heterogeneity of PDE datasets in equation types, which leads
to high errors in mixed training. Additionally, dense pre-training models that
scale parameters by increasing network width or depth incur significant
inference costs. To tackle these challenges, we propose a novel
Mixture-of-Experts Pre-training Operator Transformer (MoE-POT), a
sparse-activated architecture that scales parameters efficiently while
controlling inference costs. Specifically, our model adopts a layer-wise
router-gating network to dynamically select 4 routed experts from 16 expert
networks during inference, enabling the model to focus on equation-specific
features. Meanwhile, we also integrate 2 shared experts, aiming to capture
common properties of PDE and reduce redundancy among routed experts. The final
output is computed as the weighted average of the results from all activated
experts. We pre-train models with parameters from 30M to 0.5B on 6 public PDE
datasets. Our model with 90M activated parameters achieves up to a 40%
reduction in zero-shot error compared with existing models with 120M activated
parameters. Additionally, we conduct interpretability analysis, showing that
dataset types can be inferred from router-gating network decisions, which
validates the rationality and effectiveness of the MoE architecture.

</details>


### [41] [How Regularization Terms Make Invertible Neural Networks Bayesian Point Estimators](https://arxiv.org/abs/2510.26704)
*Nick Heilenk√∂tter*

Main category: cs.LG

TL;DR: The paper shows that specific regularization terms in invertible neural network training can recover Bayesian point estimators like posterior mean and MAP estimators when the network is inverted.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing optimization strategies for invertible neural networks in inverse problems, and to connect network training with classical Bayesian estimators for better interpretability and stability.

Method: Introduce and analyze two regularization terms for invertible neural network training that, upon inversion, recover properties of posterior mean and MAP estimators. Theoretical analysis characterizes how each loss shapes both the learned forward operator and its inverse reconstruction map.

Result: Numerical experiments demonstrate that these loss-term regularizers introduce data-dependence in a stable and interpretable way, supporting the theoretical findings.

Conclusion: Regularization terms in invertible neural network training can effectively recover classical Bayesian point estimators, providing interpretable and stable data-dependent solutions for inverse problems.

Abstract: Can regularization terms in the training of invertible neural networks lead
to known Bayesian point estimators in reconstruction? Invertible networks are
attractive for inverse problems due to their inherent stability and
interpretability. Recently, optimization strategies for invertible neural
networks that approximate either a reconstruction map or the forward operator
have been studied from a Bayesian perspective, but each has limitations. To
address this, we introduce and analyze two regularization terms for the network
training that, upon inversion of the network, recover properties of classical
Bayesian point estimators: while the first can be connected to the posterior
mean, the second resembles the MAP estimator. Our theoretical analysis
characterizes how each loss shapes both the learned forward operator and its
inverse reconstruction map. Numerical experiments support our findings and
demonstrate how these loss-term regularizers introduce data-dependence in a
stable and interpretable way.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [42] [Quantum Stochastic Gradient Descent in its continuous-time limit based on the Wigner formulation of Open Quantum Systems](https://arxiv.org/abs/2510.25910)
*Jose A. Morales Escalante*

Main category: quant-ph

TL;DR: Using Wigner formulation as a bridge between classical and quantum probabilistic algorithms, specifically developing quantum analog of Stochastic Gradient Descent in continuous-time limit.


<details>
  <summary>Details</summary>
Motivation: To create connections between classical and quantum probabilistic algorithms by leveraging the Wigner formulation as an intermediate framework.

Method: Wigner formulation of Open Quantum Systems applied to develop continuous-time limit of quantum analog of Stochastic Gradient Descent.

Result: Research plan presented for bridging classical and quantum probabilistic algorithms through Wigner formulation approach.

Conclusion: Wigner formulation provides a promising bridge for developing quantum analogs of classical probabilistic algorithms like Stochastic Gradient Descent.

Abstract: The main ideas behind a research plan to use the Wigner formulation as a
bridge between classical and quantum probabilistic algorithms are presented,
focusing on a particular case: the Quantum analog of Stochastic Gradient
Descent in its continuous-time limit based on the Wigner formulation of Open
Quantum Systems.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [43] [On a semi-discrete model of Maxwell's equations in three and two dimensions](https://arxiv.org/abs/2510.26427)
*Volodymyr Sushch*

Main category: math-ph

TL;DR: A geometric, structure-preserving semi-discrete formulation of Maxwell's equations using discrete exterior calculus that maintains intrinsic geometric and topological structures.


<details>
  <summary>Details</summary>
Motivation: To develop a spatial discretization of Maxwell's equations that preserves the geometric and topological structures of the continuous theory, providing a consistent framework for numerical analysis.

Method: Using discrete exterior calculus to create semi-discrete formulations of Maxwell's equations in both 3D and 2D settings, with analysis of essential properties and comparison to classical Maxwell's equations.

Result: The model successfully preserves geometric and topological structures, and for the special case of a combinatorial 2D torus, the equations reduce to a system of first-order linear ODEs with an explicit general solution derived.

Conclusion: The discrete exterior calculus approach provides an effective structure-preserving discretization of Maxwell's equations that maintains fundamental geometric properties while enabling analytical solutions in special cases.

Abstract: In this paper, we develop a geometric, structure-preserving semi-discrete
formulation of Maxwell's equations in both three- and two-dimensional settings
within the framework of discrete exterior calculus. This approach preserves the
intrinsic geometric and topological structures of the continuous theory while
providing a consistent spatial discretization. We analyze the essential
properties of the proposed semi-discrete model and compare them with those of
the classical Maxwell's equations. As a special case, the model is illustrated
on a combinatorial two-dimensional torus, where the semi-discrete Maxwell's
equations take the form of a system of first-order linear ordinary differential
equations. An explicit expression for the general solution of this system is
also derived.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [44] [Quantitative Lorentzian isoperimetric inequalities](https://arxiv.org/abs/2510.26755)
*Christian Lange,Jonas W. Peteranderl*

Main category: math.DG

TL;DR: The paper establishes optimal stability estimates for Lorentzian isoperimetric inequalities using Fraenkel asymmetry with universal constants, showing quadratic stability for Bahn-Ehrlich inequality and linear stability for Cavalletti-Mondino inequality.


<details>
  <summary>Details</summary>
Motivation: To provide stability estimates for Lorentzian isoperimetric inequalities with universal dimensional constants, addressing different stability behaviors between existing inequalities.

Method: Using Fraenkel asymmetry as a measure of deviation from optimal sets, establishing stability estimates with universal constants, and refining inequalities through additional geometric terms.

Result: Found quadratic stability for Bahn-Ehrlich inequality and linear stability for Cavalletti-Mondino inequality, with the latter becoming quadratic when refined with additional geometric terms. Also provided simple self-contained proofs.

Conclusion: The paper successfully establishes optimal stability estimates for Lorentzian isoperimetric inequalities, demonstrates different stability behaviors, and shows how refinement can recover quadratic stability patterns.

Abstract: We establish optimal stability estimates in terms of the Fraenkel asymmetry
with universal dimensional constants for a Lorentzian isoperimetric inequality
due to Bahn and Ehrlich and, as a consequence, for a special version of a
Lorentzian isoperimetric inequality due to Cavalletti and Mondino. For the
Bahn--Ehrlich inequality the Fraenkel asymmetry enters the stability result
quadratically like in the Euclidean case while for the Cavalletti--Mondino
inequality the Fraenkel asymmetry enters linearly. As it turns out, refining
the latter inequality through an additional geometric term allows us to recover
the more common quadratic stability behavior. Along the way, we provide simple
self-contained proofs for the above isoperimetric-type inequalities.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [45] [Cosmological Simulations of Weakly Collisional Plasmas with Braginskii Viscosity in Galaxy Clusters](https://arxiv.org/abs/2510.25847)
*Tirso Marin-Gilabert,Ulrich P. Steinwandel,Milena Valentini,John A. ZuHone,Klaus Dolag*

Main category: astro-ph.IM

TL;DR: Implementation of an anisotropic viscosity solver in OpenGadget3's MHD framework with Braginskii formulation and plasma instability limiters, validated against standard tests and applied to galaxy cluster simulations.


<details>
  <summary>Details</summary>
Motivation: To model anisotropic viscous transport along magnetic field lines in magnetized astrophysical plasmas, incorporating physical constraints from plasma microphysics.

Method: Implemented Braginskii formulation of anisotropic viscosity with mirror and firehose instability limiters, integrated within OpenGadget3's adaptive timestepping framework without subcycling.

Result: Excellent agreement with AREPO implementation in validation tests; stable and efficient integration; successful application to cosmological galaxy cluster simulation.

Conclusion: The implementation provides an accurate, robust, and computationally efficient tool for studying anisotropic viscosity in magnetized astrophysical systems.

Abstract: We present the implementation of an anisotropic viscosity solver within the
magnetohydrodynamics (MHD) framework of the TreeSPH code OpenGadget3. The
solver models anisotropic viscous transport along magnetic field lines
following the Braginskii formulation and includes physically motivated limiters
based on the mirror and firehose instability thresholds, which constrain the
viscous stress in weakly collisional plasmas. To validate the implementation,
we performed a suite of standard test problems -- including two variants of the
sound-wave test, circularly and linearly polarized Alfven waves, fast
magnetosonic wave, and the Kelvin-Helmholtz instability -- both with and
without the plasma-instability limiters. The results show excellent agreement
with the AREPO implementation of a similar anisotropic viscosity model (Berlok
et al. 2019), confirming the accuracy and robustness of our method. Our
formulation integrates seamlessly within the individual adaptive timestepping
framework of OpenGadget3, avoiding the need for subcycling. This provides
efficient and stable time integration while maintaining physical consistency.
Finally, we applied the new solver to a cosmological zoom-in simulation of a
galaxy cluster, demonstrating its capability to model anisotropic transport and
plasma microphysics in realistic large-scale environments. Our implementation
offers a versatile and computationally efficient tool for studying anisotropic
viscosity in magnetized astrophysical systems.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [46] [Numerical Investigation of Single-Core to Split-Core Transitions in Nematic Liquid Crystals](https://arxiv.org/abs/2510.26215)
*Daniel Siebel-Cortopassi,Pei Liu*

Main category: cond-mat.soft

TL;DR: Analysis of single-core and split-core defect structures in nematic liquid crystals using Landau-de Gennes framework, revealing temperature-dependent bifurcation behavior.


<details>
  <summary>Details</summary>
Motivation: To understand the stability and transition between different defect structures in nematic liquid crystals, particularly how temperature affects defect configurations.

Method: Study minimizers of the Landau-de Gennes energy functional, analyze bifurcation at critical temperature threshold, and examine Euler-Lagrange equation solutions.

Result: Below critical temperature, both split-core and single-core configurations exist with split-core having lower energy; above threshold, only single-core remains stable. Core sizes depend on temperature and domain size.

Conclusion: The transition between defect types is temperature-dependent with a critical threshold, and split-core defects are energetically favorable below this threshold but vanish above it.

Abstract: We analyze single-core and split-core defect structures in nematic liquid
crystals within the Landau-de Gennes framework by studying minimizers of the
associated energy functional. A bifurcation occurs at a critical temperature
threshold, below which both split-core and single-core configurations are
solutions to the Euler-Lagrange equation, with the split-core defect possessing
lower energy. Above the threshold, the split-core configuration vanishes,
leaving the single-core defect as the only stable solution. We analyze the
dependence of such temperature threshold on the domain size and characterize
the nature of the transition between the two defect types. We carry out a
quantitative study of defect core sizes as functions of temperature and domain
size for both single and split core defects.

</details>
