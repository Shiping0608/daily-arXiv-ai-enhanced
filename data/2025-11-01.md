<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 9]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [math.DG](#math.DG) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A Hybrid Reconstruction Framework for Efficient High-Order Shock-Capturing on Unstructured Meshes](https://arxiv.org/abs/2510.25906)
*Yiren Tong,Panagiotis Tsoutsanis*

Main category: math.NA

TL;DR: A hybrid reconstruction framework for compressible flows that combines linear and nonlinear methods using a priori detection to optimize efficiency while maintaining accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: To advance high-resolution schemes by reducing computational costs of nonlinear reconstructions (CWENOZ/MUSCL) while preserving accuracy and robustness in compressible flow simulations.

Method: Combines CWENOZ and MOOD paradigm with a novel Numerical Admissibility Detector that classifies flow regions into smooth, weakly non-smooth, and discontinuous zones, then applies optimal reconstruction methods for each: high-order linear for smooth areas, CWENOZ for weakly non-smooth, and MUSCL for discontinuities.

Result: Achieves up to 2.5x speed-up over CWENOZ in 3D compressible turbulence while maintaining designed accuracy in smooth regions and improving robustness in shock-dominated flows. Successfully implemented in UCNS3D solver supporting arbitrary-order reconstructions on mixed-element meshes.

Conclusion: The hybrid approach effectively balances efficiency, robustness, and reliability, making high-order accuracy more feasible for industrial-scale CFD applications through targeted a priori allocation of reconstruction methods.

Abstract: We present a multi-dimensional, arbitrary-order hybrid reconstruction
framework for compressible flows on unstructured meshes. The method advances
high-resolution schemes by combining the efficiency of linear reconstruction
with the robustness of nonlinear formulations, activated only when needed
through a novel a priori detection strategy. This minimizes the use of costly
Compact Weighted Essentially Non-Oscillatory (CWENOZ) or Monotonic
Upstream-centered Scheme for Conservation Laws (MUSCL) reconstructions,
reducing computational cost without compromising accuracy or stability. The
framework merges CWENOZ and the Multi-dimensional Optimal Order Detection
(MOOD) paradigm while introducing a redesigned Numerical Admissibility Detector
(NAD) that classifies the local flow into smooth, weakly non-smooth, and
discontinuous regions in a single step. Each region is then reconstructed using
an optimal method: a high-order linear scheme in smooth areas, CWENOZ in weakly
non-smooth zones, and a second-order MUSCL near discontinuities. This targeted
a priori allocation preserves high-order accuracy where possible and ensures
stable, non-oscillatory behavior near shocks and steep gradients. Implemented
within the open-source unstructured finite-volume solver UCNS3D, the framework
supports arbitrary-order reconstructions on mixed-element meshes. Extensive
two- and three-dimensional benchmarks confirm that it retains the designed
accuracy in smooth regions while greatly improving robustness in
shock-dominated flows. Thanks to the reduced frequency of nonlinear
reconstructions, the method achieves up to 2.5x speed-up over a CWENOZ scheme
of equal order in 3D compressible turbulence. This hybrid approach thus brings
high-order accuracy closer to industrial-scale CFD through its balance of
efficiency, robustness, and reliability.

</details>


### [2] [A fast spectral overlapping domain decomposition method with discretization-independent conditioning bounds](https://arxiv.org/abs/2510.25991)
*Simon Dirckx,Anna Yesypenko,Per-Gunnar Martinsson*

Main category: math.NA

TL;DR: A domain decomposition method for solving variable-coefficient elliptic PDEs using overlapping thin slabs/shells, exploiting H-matrix structure for efficient handling of dense blocks in the reduced linear system.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient solver for general variable-coefficient elliptic PDEs on regular domains that can handle large-scale problems with millions of degrees of freedom.

Method: Domain decomposition with overlapping thin slabs/shells, forming a reduced linear system using black-box randomized compression, and exploiting H-matrix structure for efficient dense block handling.

Result: The solver successfully handles oscillatory 2D and 3D problems with up to 28 million degrees of freedom, demonstrating improved efficiency over existing formulations.

Conclusion: The proposed method provides a well-conditioned, efficient approach for large-scale elliptic PDE problems by leveraging domain decomposition and H-matrix arithmetic.

Abstract: A domain decomposition method for the solution of general
variable-coefficient elliptic partial differential equations on regular domains
is introduced. The method is based on tessellating the domain into overlapping
thin slabs or shells, and then explicitly forming a reduced linear system that
connects the different domains. Rank-structure ('H-matrix structure') is
exploited to handle the large dense blocks that arise in the reduced linear
system. Importantly, the formulation used is well-conditioned, as it converges
to a second kind Fredholm equation as the precision in the local solves is
refined. Moreover, the dense blocks that arise are far more data-sparse than in
existing formulations, leading to faster and more efficient H-matrix
arithmetic. To form the reduced linear system, black-box randomized compression
is used, taking full advantage of the fact that sparse direct solvers are
highly efficient on the thin sub-domains. Numerical experiments demonstrate
that our solver can handle oscillatory 2D and 3D problems with as many as 28
million degrees of freedom.

</details>


### [3] [A two-dimensional fractional-order element-free Galerkin method for nonlocal elasticity and complex domain problems](https://arxiv.org/abs/2510.26161)
*Shubham Desai,Malapeta Hemasundara Rao,Sai Sidhardh*

Main category: math.NA

TL;DR: A 2D meshfree fractional-order Element-Free Galerkin method is developed as an alternative to FEM for solving fractional-order differential equations, handling complex domains better than mesh-based methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of 1D f-EFG methods and demonstrate meshfree approaches' potential for FDEs, while addressing FEM drawbacks like high computational cost and mesh generation difficulties in irregular domains.

Method: Uses 2D Moving Least Squares (MLS) approximants within a fractional-order Element-Free Galerkin framework to solve fractional-order linear and nonlinear PDEs, validated against benchmark results.

Result: Successfully solves nonlocal elasticity problems in square and circular plates, demonstrating effectiveness for complex 2D domains where traditional FEM faces challenges.

Conclusion: The 2D f-EFG method is a viable meshfree alternative to FEM for fractional-order differential equations, with potential for broader applications in multiscale modeling, multiphysics coupling, and anomalous diffusion.

Abstract: This study presents a meshfree two-dimensional fractional-order Element-Free
Galerkin (2D f-EFG) method as a viable alternative to conventional mesh-based
FEM for a numerical solution of (spatial) fractional-order differential
equations (FDEs). The previously developed one-dimensional f-EFG solver offers
a limited demonstration of the true efficacy of EFG formulations for FDEs, as
it is restricted to simple 1D line geometries. In contrast, the 2D f-EFG solver
proposed and developed here effectively demonstrates the potential of meshfree
approaches for solving FDEs. The proposed solver can handle complex and
irregular 2D domains that are challenging for mesh-based methods. As an
example, the developed framework is employed to investigate nonlocal elasticity
governed by fractional-order constitutive relations in a square and circular
plate. Furthermore, the proposed approach mitigates key drawbacks of FEM,
including high computational cost, mesh generation, and reduced accuracy in
irregular domains. The 2D f-EFG employs 2D Moving Least Squares (MLS)
approximants, which are particularly effective in approximating fractional
derivatives from nodal values. The 2D f-EFG solver is employed here for the
numerical solution of fractional-order linear and nonlinear partial
differential equations corresponding to the nonlocal elastic response of a
plate. The solver developed here is validated with the benchmark results
available in the literature. While the example chosen here focuses on nonlocal
elasticity, the numerical method can be extended for diverse applications of
fractional-order derivatives in multiscale modeling, multiphysics coupling,
anomalous diffusion, and complex material behavior.

</details>


### [4] [A parallel solver for random input problems via Karhunen-Loève expansion and diagonalized coarse grid correction](https://arxiv.org/abs/2510.26180)
*Dou Dai,Qiuqi Li,Huailing Song*

Main category: math.NA

TL;DR: Proposes KLE-CGC, a hybrid parallel algorithm combining Karhunen-Loève expansion with coarse grid correction to improve computational efficiency of parallel-in-time methods for stochastic initial-value problems.


<details>
  <summary>Details</summary>
Motivation: Standard parareal algorithm suffers from slow convergence for stochastic problems due to poor initial guess quality.

Method: Integrates KL expansion for low-dimensional parameterization of stochastic fields, constructs gPC spectral surrogate model for rapid solution prediction, and uses this as improved initial value for parareal iterations.

Result: Numerical experiments show KLE-CGC maintains same convergence order while substantially reducing iteration count and improving parallel scalability.

Conclusion: The proposed framework retains theoretical convergence rate of standard parareal algorithm while achieving significant computational efficiency gains.

Abstract: This paper is dedicated to enhancing the computational efficiency of
traditional parallel-in-time methods for solving stochastic initial-value
problems. The standard parareal algorithm often suffers from slow convergence
when applied to problems with stochastic inputs, primarily due to the poor
quality of the initial guess. To address this issue, we propose a hybrid
parallel algorithm, termed KLE-CGC, which integrates the Karhunen-Lo\`{e}ve
(KL) expansion with the coarse grid correction (CGC). The method first employs
the KL expansion to achieve a low-dimensional parameterization of
high-dimensional stochastic parameter fields. Subsequently, a generalized
Polynomial Chaos (gPC) spectral surrogate model is constructed to enable rapid
prediction of the solution field. Utilizing this prediction as the initial
value significantly improves the initial accuracy for the parareal iterations.
A rigorous convergence analysis is provided, establishing that the proposed
framework retains the same theoretical convergence rate as the standard
parareal algorithm. Numerical experiments demonstrate that KLE-CGC maintains
the same convergence order as the original algorithm while substantially
reducing the number of iterations and improving parallel scalability.

</details>


### [5] [Efficient And Stable Third-order Method for Micromagnetics Simulations](https://arxiv.org/abs/2510.26181)
*Changjian Xie,Cheng Wang*

Main category: math.NA

TL;DR: A third-order accurate numerical scheme for solving the Landau-Lifshitz-Gilbert equation with large damping parameters, featuring linear systems with constant coefficients for efficiency and unconditional stability.


<details>
  <summary>Details</summary>
Motivation: To address magnetization dynamics in ferromagnetic materials under large damping parameters, improving upon existing lower-order methods for better accuracy and efficiency.

Method: Developed by extending a second-order method, this scheme solves only linear systems with constant coefficients, enabling use of fast solvers. It achieves third-order temporal and fourth-order spatial accuracy while being unconditionally stable for large damping.

Result: Numerical tests in 1D and 3D confirm third-order accuracy and efficiency gains. The method shows unconditional stability with large damping and captures physically plausible structures. For domain wall dynamics, it reproduces linear relationships between wall velocity and damping/external field better than lower-order methods.

Conclusion: The proposed third-order scheme provides superior accuracy, efficiency, and stability for simulating magnetization dynamics with large damping parameters, outperforming existing first and second-order approaches.

Abstract: To address the magnetization dynamics in ferromagnetic materials described by
the Landau-Lifshitz-Gilbert equation under large damping parameters, a
third-order accurate numerical scheme is developed by building upon a
second-order method \cite{CaiChenWangXie2022} and leveraging its efficiency.
This method boasts two key advantages: first, it only involves solving linear
systems with constant coefficients, enabling the use of fast solvers and thus
significantly enhancing numerical efficiency over existing first or
second-order approaches. Second, it achieves third-order temporal accuracy and
fourth-order spatial accuracy, while being unconditionally stable for large
damping parameters. Numerical tests in 1D and 3D scenarios confirm both its
third-order accuracy and efficiency gains. When large damping parameters are
present, the method demonstrates unconditional stability and reproduces
physically plausible structures. For domain wall dynamics simulations, it
captures the linear relationship between wall velocity and both the damping
parameter and external magnetic field, outperforming lower-order methods in
this regard.

</details>


### [6] [Transcending Sparse Measurement Limits: Operator-Learning-Driven Data Super-Resolution for Inverse Source Problem](https://arxiv.org/abs/2510.26227)
*Guanyu Pan,Jianing Zhou,Xiaotong Liu,Yunqing Huang,Nianyu Yi*

Main category: math.NA

TL;DR: A modular framework combining DeepONet interpolation with classical Direct Sampling Method significantly improves multi-source localization from extremely sparse single-frequency measurements in narrow aperture settings.


<details>
  <summary>Details</summary>
Motivation: Inverse source localization from Helmholtz boundary data over narrow apertures is highly ill-posed and severely undersampled, undermining classical solvers like the Direct Sampling Method.

Method: Three-step approach: 1) Extend uniqueness theorem for inverse source problem under limited viewing apertures; 2) Use DeepONet with branch-trunk architecture to interpolate sparse measurements (6-10 samples) to dense synthetic aperture; 3) Feed super-resolved field into Direct Sampling Method for localization.

Result: For single source, sparse data alone achieves grid-level precision. In multi-source trials, DeepONet-reconstructed data reduce localization error by about an order of magnitude and remain effective with apertures as small as π/4.

Conclusion: The modular framework decouples interpolation from inversion, allowing interchangeable neural operators and classical algorithms, providing practical and flexible design that improves localization accuracy compared to standard baselines.

Abstract: Inverse source localization from Helmholtz boundary data collected over a
narrow aperture is highly ill-posed and severely undersampled, undermining
classical solvers (e.g., the Direct Sampling Method). We present a modular
framework that significantly improves multi-source localization from extremely
sparse single-frequency measurements. First, we extend a uniqueness theorem for
the inverse source problem, proving that a unique solution is guaranteed under
limited viewing apertures. Second, we employ a Deep Operator Network (DeepONet)
with a branch-trunk architecture to interpolate the sparse measurements,
lifting six to ten samples within the narrow aperture to a sufficiently dense
synthetic aperture. Third, the super-resolved field is fed into the Direct
Sampling Method (DSM). For a single source, we derive an error estimate showing
that sparse data alone can achieve grid-level precision. In two- and
three-source trials, localization from raw sparse measurements is unreliable,
whereas DeepONet-reconstructed data reduce localization error by about an order
of magnitude and remain effective with apertures as small as $\pi/4$. By
decoupling interpolation from inversion, the framework allows the interpolation
and inversion modules to be swapped with neural operators and classical
algorithms, respectively, providing a practical and flexible design that
improves localization accuracy compared with standard baselines.

</details>


### [7] [Simulation of the magnetic Ginzburg-Landau equation via vortex tracking](https://arxiv.org/abs/2510.26334)
*Thiago Carvalho Corso,Gaspard Kemlin,Christof Melcher,Benjamin Stamm*

Main category: math.NA

TL;DR: A numerical method for simulating 2D magnetic time-dependent Ginzburg-Landau equations in the small epsilon regime, avoiding the need to resolve the epsilon-scale by using a limiting ODE system for vortex dynamics.


<details>
  <summary>Details</summary>
Motivation: To efficiently simulate TDGL equations with small Ginzburg-Landau parameter epsilon, where traditional methods require very fine meshes due to vortex core sizes of order epsilon.

Method: Developed a numerical approach using a limiting ODE system for vortex evolution that requires solving a linear second-order PDE at each time step, with rigorous theoretical justification for general 2D domains.

Result: The method successfully avoids resolving the epsilon-scale in TDGL simulations, enabling efficient computation for small but finite epsilon values with constant external magnetic field.

Conclusion: The proposed numerical strategy provides an accurate and efficient alternative to traditional fine-mesh approaches for TDGL equations in the small epsilon regime, with theoretical backing and numerical validation.

Abstract: This paper deals with the numerical simulation of the 2D magnetic
time-dependent Ginzburg-Landau (TDGL) equations in the regime of small but
finite (inverse) Ginzburg-Landau parameter $\epsilon$ and constant (order $1$
in $\epsilon$) applied magnetic field. In this regime, a well-known feature of
the TDGL equation is the appearance of quantized vortices with core size of
order $\epsilon$. Moreover, in the singular limit $\epsilon \searrow 0$, these
vortices evolve according to an explicit ODE system. In this work, we first
introduce a new numerical method for the numerical integration of this limiting
ODE system, which requires to solve a linear second order PDE at each time
step. We also provide a rigorous theoretical justification for this method that
applies to a general class of 2D domains. We then develop and analyze a
numerical strategy based on the finite-dimensional ODE system to efficiently
simulate the infinite-dimensional TDGL equations in the presence of a constant
external magnetic field and for small, but finite, $\epsilon$. This method
allows us to avoid resolving the $\epsilon$-scale when solving the TDGL
equations, where small values of $\epsilon$ typically require very fine meshes
and time steps. We provide numerical examples on a few test cases and justify
the accuracy of the method with numerical investigations.

</details>


### [8] [Incorporating Local Hölder Regularity into PINNs for Solving Elliptic PDEs](https://arxiv.org/abs/2510.26365)
*Qirui Zhou,Jiebao Sun,Yi Ran,Boying Wu*

Main category: math.NA

TL;DR: Incorporates local Hölder regularization into PINNs for elliptic PDEs, improving accuracy and robustness through a modified loss function and variable-distance sampling strategy.


<details>
  <summary>Details</summary>
Motivation: Leverage interior regularity properties of linear elliptic PDEs to enhance PINN performance by adding local Hölder regularization to the loss function.

Method: Modified PINN framework with local Hölder regularization term in loss function, using variable-distance discrete sampling strategy to approximate the regularization effectively.

Result: Numerical experiments show notable improvements in prediction accuracy and robustness compared to standard PINNs across various elliptic problems.

Conclusion: Local Hölder regularization significantly enhances PINN performance for elliptic PDEs, with established error estimates demonstrating improved generalization.

Abstract: In this paper, local H\"older regularization is incorporated into a
physics-informed neural networks (PINNs) framework for solving elliptic partial
differential equations (PDEs). Motivated by the interior regularity properties
of linear elliptic PDEs, a modified loss function is constructed by introducing
local H\"older regularization term. To approximate this term effectively, a
variable-distance discrete sampling strategy is developed. Error estimates are
established to assess the generalization performance of the proposed method.
Numerical experiments on a range of elliptic problems demonstrate notable
improvements in both prediction accuracy and robustness compared to standard
physics-informed neural networks.

</details>


### [9] [Asymptotic meshes from $r$-variational adaptation methods for static problems in one dimension](https://arxiv.org/abs/2510.26375)
*Darith Hun,Nicolas Moës,Heiner Olbermann*

Main category: math.NA

TL;DR: Analysis of r-adaptive finite element approximation for minimizing integral functionals in 1D, showing optimal grid configurations converge as nodes increase to infinity via Γ-convergence.


<details>
  <summary>Details</summary>
Motivation: To study the behavior of optimal grid configurations in finite element approximations when the number of nodes increases indefinitely, ensuring convergence properties.

Method: Include the FEM grid as a variable in minimization, use Γ-convergence to analyze the limit of renormalized energy functionals as nodes approach infinity.

Result: Optimal grid configurations have a well-defined limit as nodes increase, with numerical examples showing asymptotic mesh minimizers closely match finite optimal meshes.

Conclusion: Γ-convergence provides a rigorous framework for understanding the asymptotic behavior of optimal adaptive meshes in finite element approximations.

Abstract: We consider the minimization of integral functionals in one dimension and
their approximation by $r$-adaptive finite elements. Including the grid of the
FEM approximation as a variable in the minimization, we are able to show that
the optimal grid configurations have a well-defined limit when the number of
nodes in the grid is being sent to infinity. This is done by showing that the
suitably renormalized energy functionals possess a limit in the sense of
$\Gamma$-convergence. We provide numerical examples showing the closeness of
the optimal asymptotic mesh obtained as a minimizer of the $\Gamma$-limit to
the optimal finite meshes.

</details>


### [10] [Explicit Consistency Error Estimate for Finite Element Solutions of the Poisson Equation on Convex Domains](https://arxiv.org/abs/2510.26404)
*Su Ruibo*

Main category: math.NA

TL;DR: Explicit a priori consistency error estimates for FEM discretization of Poisson equation on convex domains approximated by internal convex polyhedra


<details>
  <summary>Details</summary>
Motivation: To provide explicit error bounds for finite element methods applied to Poisson equation on convex domains with polyhedral approximations

Method: Derivation of explicit consistency error estimates using global geometric parameters for general convex domains and arbitrary simplicial meshes

Result: Obtained explicit a priori error estimates that depend only on global geometric parameters

Conclusion: The method provides applicable error estimates for general convex domains and arbitrary mesh families

Abstract: We derive explicit a priori consistency error estimates for a standard finite
element discretization of the Poisson equation on convex domains, where the
domain is approximated by an internal convex polyhedron. The obtained explicit
estimates depend only on global geometric parameters and are applicable to
general convex domains and arbitrary families of simplicial meshes.

</details>


### [11] [Accelerated decomposition of bistochastic kernel matrices by low rank approximation](https://arxiv.org/abs/2510.26574)
*Chris Vales,Dimitrios Giannakis*

Main category: math.NA

TL;DR: An accelerated algorithm for approximate eigenvalue decomposition of bistochastic normalized kernel matrices using low-rank approximation via pivoted partial Cholesky, avoiding full matrix formation.


<details>
  <summary>Details</summary>
Motivation: To efficiently compute eigenvalue decompositions of bistochastic normalized kernel matrices without the computational burden of forming full matrices, especially for large datasets.

Method: Constructs low-rank approximation using pivoted partial Cholesky algorithm, then computes approximate decomposition of bistochastic normalization using this approximation.

Result: Algorithm scales linearly with dataset size and quadratically with approximation rank, significantly reducing computational cost compared to naive approaches.

Conclusion: The method provides accurate spatiotemporal pattern extraction from chaotic dynamics and outperforms subsampling with Nystroem extension.

Abstract: We develop an accelerated algorithm for computing an approximate eigenvalue
decomposition of bistochastic normalized kernel matrices. Our approach
constructs a low rank approximation of the original kernel matrix by the
pivoted partial Cholesky algorithm and uses it to compute an approximate
decomposition of its bistochastic normalization without requiring the formation
of the full kernel matrix. The cost of the proposed algorithm depends linearly
on the size of the employed training dataset and quadratically on the rank of
the low rank approximation, offering a significant cost reduction compared to
the naive approach. We apply the proposed algorithm to the kernel based
extraction of spatiotemporal patterns from chaotic dynamics, demonstrating its
accuracy while also comparing it with an alternative algorithm consisting of
subsampling and Nystroem extension.

</details>


### [12] [The evolving surface morphochemical reaction-diffusion system for battery modeling](https://arxiv.org/abs/2510.26437)
*Benedetto Bozzini,Massimo Frittelli,Anotida Madzvamuse,Ivonne Sgura*

Main category: math.NA

TL;DR: The paper introduces the Evolving Surface DIB (ESDIB) model, a reaction-diffusion system on dynamically evolving electrode surfaces that couples surface evolution to electrochemical species concentration, enabling accurate prediction of branching and dendritic growth in metal deposition.


<details>
  <summary>Details</summary>
Motivation: To address the problem of poorly controllable morphology in electrodeposited films, particularly in high-energy density next-generation batteries with metal anodes, where uncontrolled electrode shape evolution prevents practical cyclability targets.

Method: Developed the ESDIB model that couples surface evolution to local concentration of electrochemical species, and solved it numerically using an extension of the Lumped Evolving Surface Finite Element Method (LESFEM) for spatial discretization combined with an IMEX Euler scheme for time integration.

Result: The model was validated through six numerical experiments compared with laboratory images, demonstrating accurate capture of branching and dendritic growth in electrodeposition.

Conclusion: The ESDIB framework provides a predictive and physically consistent tool for studying metal deposition phenomena in energy storage devices, laying the knowledge-base for materials-science advancements toward stable battery material architectures.

Abstract: It is well known that phase formation by electrodeposition yields films of
poorly controllable morphology. This typically leads to a range of
technological issues in many fields of electrochemical technology. Presently, a
particularly relevant case is that of high-energy density next-generation
batteries with metal anodes, that cannot yet reach practical cyclability
targets, owing to uncontrolled elelctrode shape evolution. In this scenario,
mathematical modelling is a key tool to lay the knowledge-base for
materials-science advancements liable to lead to concretely stable battery
material architectures. In this work, we introduce the Evolving Surface DIB
(ESDIB) model, a reaction-diffusion system posed on a dynamically evolving
electrode surface. Unlike previous fixed-surface formulations, the ESDIB model
couples surface evolution to the local concentration of electrochemical
species, allowing the geometry of the electrode itself to adapt in response to
deposition. To handle the challenges related to the coupling between surface
motion and species transport, we numerically solve the system by proposing an
extension of the Lumped Evolving Surface Finite Element Method (LESFEM) for
spatial discretisation, combined with an IMEX Euler scheme for time
integration. The model is validated through six numerical experiments, each
compared with laboratory images of electrodeposition. Results demonstrate that
the ESDIB framework accurately captures branching and dendritic growth,
providing a predictive and physically consistent tool for studying metal
deposition phenomena in energy storage devices.

</details>


### [13] [A GenEO-type coarse space with smaller eigenproblems](https://arxiv.org/abs/2510.26548)
*Peter Bastian,Nils Friess*

Main category: math.NA

TL;DR: A new GenEO coarse space variant reduces setup costs by solving eigenproblems only in boundary-connected strips, maintaining coefficient robustness with potentially larger coarse spaces.


<details>
  <summary>Details</summary>
Motivation: Traditional adaptive coarse spaces require solving expensive local eigenproblems across entire subdomains, leading to high setup costs that can exceed iteration costs. This paper aims to reduce this setup overhead while preserving robustness.

Method: Proposes a modified GenEO coarse space that solves eigenproblems only in strips connected to subdomain boundaries rather than entire subdomains, significantly reducing computational setup requirements.

Result: The method achieves substantial reduction in setup costs while maintaining similar coefficient-robust condition number estimates as the original GenEO method, though potentially with larger coarse spaces.

Conclusion: The boundary-strip approach provides an effective trade-off, significantly reducing computational setup overhead while preserving the robustness properties essential for scalable domain decomposition methods.

Abstract: Coarse spaces are essential to ensure robustness w.r.t. the number of
subdomains in two-level overlapping Schwarz methods. Robustness with respect to
the coefficients of the underlying partial differential equation (PDE) can be
achieved by adaptive (or spectral) coarse spaces involving the solution of
local eigenproblems. The solution of these eigenproblems, although scalable,
entails a large setup cost which may exceed the cost for the iteration phase.
In this paper we present and analyse a new variant of the GenEO (Generalised
Eigenproblems in the Overlap) coarse space which involves solving eigenproblems
only in a strip connected to the boundary of the subdomain. This leads to a
significant reduction of the setup cost while the method satisfies a similar
coefficient-robust condition number estimate as the original method, albeit
with a possibly larger coarse space.

</details>


### [14] [Fast tensor-based electrostatic energy calculations in the perspective of protein-ligand docking problem](https://arxiv.org/abs/2510.26611)
*Peter Benner,Boris N. Khoromskij,Venera Khoromskaia,Matthias Stein*

Main category: math.NA

TL;DR: Fast electrostatic interaction energy calculation for protein-ligand docking using low-rank tensor representation with O(n) complexity, enabling large 3D grids and efficient pose selection.


<details>
  <summary>Details</summary>
Motivation: Need for efficient calculation of electrostatic interaction energy in rigid protein-ligand docking, particularly for complex systems with many particles and large grid sizes.

Method: Uses low-rank range-separated tensor-based representation of electrostatic potential on 3D grids, with O(n)-complexity energy calculation that depends logarithmically on particle count. Employs constrained energy minimization with tensor-based electrostatic energy recalculation for rotations and translations.

Result: Successfully tested on synthetic and realistic data, providing proof of concept for complex particle configurations. Method enables usage of large 3D Cartesian grids (up to ~10^12 points) for accurate modeling of complexes with multiple large proteins.

Conclusion: The tensor-based approach provides efficient electrostatic energy calculation for protein-ligand docking and can be integrated with traditional stochastic or deterministic posing techniques.

Abstract: We propose and justify a new approach for fast calculation of the
electrostatic interaction energy of clusters of charged particles in
constrained energy minimization in the framework of rigid protein-ligand
docking. Our ``blind search'' docking technique is based on the low-rank
range-separated (RS) tensor-based representation of the free-space
electrostatic potential of the biomolecule represented on large $n\times
n\times n$ 3D grid. We show that both the collective electrostatic potential of
a complex protein-ligand system and the respective electrostatic interaction
energy can be calculated by tensor techniques in $O(n)$-complexity, such that
the numerical cost for energy calculation only mildly (logarithmically) depends
on the number of particles in the system. Moreover, tensor representation of
the electrostatic potential enables usage of large 3D Cartesian grids (of the
order of $n^3 \sim 10^{12}$), which could allow the accurate modeling of
complexes with several large proteins. In our approach selection of the correct
geometric pose predictions in the localized posing process is based on the
control of van der Waals distance between the target molecular clusters. Here,
we confine ourselves by constrained minimization of the energy functional by
using only fast tensor-based free-space electrostatic energy recalculation for
various rotations and translations of both clusters. Numerical tests of the
electrostatic energy-based ``protein-ligand docking'' algorithm applied to
synthetic and realistic input data present a proof of concept for rather
complex particle configurations. The method may be used in the framework of the
traditional stochastic or deterministic posing/docking techniques.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Solutions to Second-Order Nonlocal Evolution Equations Governed by Non-Autonomous Forms](https://arxiv.org/abs/2510.25881)
*Sajid Ullah,Vittorio Colao*

Main category: math.AP

TL;DR: Proves existence conditions for second-order problems with nonzero nonlocal initial conditions using fundamental solutions and fixed-point techniques, with applications to PDEs involving viscoelastic membranes.


<details>
  <summary>Details</summary>
Motivation: To address second-order problems with nonzero nonlocal initial conditions, which arise in applications like vibrating viscoelastic membranes with time-dependent properties and memory effects.

Method: Uses fundamental solutions and fixed-point techniques to analyze the problem and prove existence conditions.

Result: Establishes sufficient conditions for the existence of solutions to second-order problems with nonzero nonlocal initial conditions.

Conclusion: The theoretical framework is successfully applied to partial differential equations modeling physical systems like viscoelastic membranes with nonlocal memory effects.

Abstract: Our main contributions include proving sufficient conditions for the
existence of solution to a second order problem with nonzero nonlocal initial
conditions, and providing a comprehensive analysis using fundamental solutions
and fixed-point techniques. The theoretical results are illustrated through
applications to partial differential equations, including vibrating
viscoelastic membranes with time-dependent material properties and nonlocal
memory effects.

</details>


### [16] [Bochner-Riesz means on a conical singular manifold](https://arxiv.org/abs/2510.26059)
*Qiuye Jia,Junyong Zhang,Jiqiang Zheng*

Main category: math.AP

TL;DR: Sharp L^p-boundedness criterion for Bochner-Riesz multipliers on flat cones, with critical exponent δ_c(p,2) = max{0, 2|1/2 - 1/p| - 1/2} determining boundedness for 1 ≤ p ≤ ∞, p ≠ 2.


<details>
  <summary>Details</summary>
Motivation: To establish precise boundedness conditions for Bochner-Riesz multipliers on flat cones and resolve the critical exponent problem in wedge domains with boundary conditions.

Method: Analysis of Bochner-Riesz multipliers S_λ^δ(Δ_X) on flat cones X = (0,∞) × S_σ^1, examining L^p-boundedness conditions.

Result: S_λ^δ(Δ_X) is bounded on L^p(X) for 1 ≤ p ≤ ∞, p ≠ 2, if and only if δ > δ_c(p,2) = max{0, 2|1/2 - 1/p| - 1/2}.

Conclusion: The paper provides a complete characterization of L^p-boundedness for Bochner-Riesz multipliers on flat cones and wedge domains with Dirichlet/Neumann boundary conditions.

Abstract: We prove a sharp $L^p$-boundedness criterion for Bochner-Riesz multipliers on
flat cones $X = (0,\infty) \times \mathbb{S}_\sigma^1$. The operator
$S_\lambda^\delta(\Delta_X)$ is bounded on $L^p(X)$ for $1 \leq p \leq \infty$,
$p \neq 2$, if and only if $\delta > \delta_c(p,2) = \max\left\{ 0, 2\left| 1/2
- 1/p \right| - 1/2 \right\}$. This result is also applicable to the infinite
sector domain with Dirichlet or Neumann boundary, resolving the critical
exponent problem in this wedge setting.

</details>


### [17] [A one-dimensional Stefan problem for the heat equation with a nonlinear boundary condition](https://arxiv.org/abs/2510.26088)
*Kensho Araya,Kazuhiro Ishige*

Main category: math.AP

TL;DR: The paper classifies solutions to the 1D one-phase Stefan problem with nonlinear boundary conditions into three types based on initial function size: global solutions with exponential decay, global solutions with non-exponential decay, and finite-time blow-up solutions.


<details>
  <summary>Details</summary>
Motivation: To understand the complete classification of solution behaviors for the Stefan problem with nonlinear boundary conditions, which has important applications in phase transition phenomena.

Method: Analysis of the one-dimensional one-phase Stefan problem for the heat equation with nonlinear boundary conditions, studying solution behavior based on initial function size.

Result: All solutions fall into three distinct types: global solutions with exponential decay, global solutions with non-exponential decay, and finite-time blow-up solutions. The classification depends on initial function size, and blow-up behavior is characterized.

Conclusion: The Stefan problem with nonlinear boundary conditions exhibits a complete trichotomy of solution behaviors determined by initial conditions, with detailed characterization of blow-up dynamics.

Abstract: We study the one-dimensional one-phase Stefan problem for the heat equation
with a nonlinear boundary condition. We show that all solutions fall into one
of three distinct types: global-in-time solutions with exponential decay,
global-in-time solutions with non-exponential decay, and finite-time blow-up
solutions. The classification depends on the size of the initial function.
Furthermore, we describe the behavior of solutions at the blow-up time.

</details>


### [18] [Time-periodic boundary effects on the shocks for scalar conservation laws](https://arxiv.org/abs/2510.26153)
*Yuan Yuan*

Main category: math.AP

TL;DR: This paper analyzes how time-periodic boundary conditions affect shock wave stability in scalar conservation laws on a half-line, showing that asymptotic states combine shifted background shocks with boundary-induced periodic solutions.


<details>
  <summary>Details</summary>
Motivation: To understand how time-periodic boundary conditions (arising from piston problems in fluid mechanics) affect the long-time behavior of Riemann solutions, which has remained unclear despite its importance.

Method: Rigorous mathematical analysis of asymptotic stabilities for both inviscid and viscous shocks in scalar conservation laws on half-line (-∞,0) with shock speed s<0, subjected to time-periodic boundary conditions.

Result: Proved that asymptotic states are governed by both shifted background (viscous) shocks and time-periodic boundary solutions, with these effects manifesting as a propagating 'boundary wave' that influences shock dynamics.

Conclusion: Time-periodic boundary conditions significantly affect shock wave behavior, creating composite asymptotic states that combine background shock structures with boundary-induced periodic solutions through propagating boundary waves.

Abstract: This paper is concerned with the asymptotic stabilities of the inviscid and
viscous shocks for the scalar conservation laws on the half-line $(-\infty,0)$
with shock speed $s<0$, subjected to the time-periodic boundary condition,
which arises from the classical piston problems for fluid mechanics. Despite
the importance, how time-periodic boundary conditions affect the long-time
behaviors of Riemann solutions has remained unclear. This work addresses this
gap by rigorously proving that in both inviscid and viscous case, the
asymptotic states of the solutions under the time-periodic boundary conditions
are not only governed by the shifted background (viscous) shocks, but also
coupled with the time-periodic boundary solution induced by the time-periodic
boundary. Our analysis reveals that these effects manifest as a propagating
"boundary wave", which influences the shock dynamics.

</details>


### [19] [Sharp embeddings and existence results for Logarithmic $p$-Laplacian equations with critical growth](https://arxiv.org/abs/2510.26286)
*Rakesh Arora,Jacques Giacomoni,Hichem Hajaiej,Arshi Vaishnavi*

Main category: math.AP

TL;DR: This paper establishes new p-Logarithmic Sobolev inequalities and embeddings for the logarithmic p-Laplacian, applies them to prove existence of solutions for critical growth problems using Nehari manifold methods, and analyzes asymptotic behavior of fractional p-Laplacian problems as the fractional parameter approaches zero.


<details>
  <summary>Details</summary>
Motivation: To extend previous linear results to a broader nonlinear variational framework by developing analytical tools for the logarithmic p-Laplacian operator and studying related boundary value problems with critical growth nonlinearities.

Method: Derives p-Logarithmic Sobolev inequalities and optimal embeddings into Orlicz-type spaces; employs Nehari manifold method for existence proofs; conducts asymptotic analysis of weighted nonlocal problems as fractional parameter s → 0⁺.

Result: Establishes new functional inequalities and embeddings; proves existence of nontrivial weak solutions for critical growth problems; demonstrates convergence of least energy solutions to Brezis-Nirenberg and logistic-type problems involving logarithmic p-Laplacian.

Conclusion: The work provides a nonlinear extension of previous linear results, developing a comprehensive variational framework for the logarithmic p-Laplacian operator with applications to critical growth problems and asymptotic analysis.

Abstract: In this paper, we derive a new $p$-Logarithmic Sobolev inequality and optimal
continuous and compact embeddings into Orlicz-type spaces of the function space
associated with the logarithmic $p$-Laplacian. As an application of these
results, we study a class of Dirichlet boundary value problems involving the
logarithmic $p$-Laplacian and critical growth nonlinearities perturbed with
superlinear-subcritical growth terms. By employing the method of the Nehari
manifold, we prove the existence of a nontrivial weak solution.
  Lastly, we conduct an asymptotic analysis of a weighted nonlocal, nonlinear
problem governed by the fractional $p$-Laplacian with superlinear or sublinear
type non-linearity, demonstrating the convergence of least energy solutions to
a non-trivial, non-negative least energy solution of a Brezis-Nirenberg type or
logistic-type problem, respectively, involving the logarithmic $p$-Laplacian as
the fractional parameter $s \to 0^+$.
  The findings in this work serve as a nonlinear analogue of the results
reported in \cite{Angeles-Saldana, Arora-Giacomoni-Vaishnavi,
Santamaria-Saldana}, thereby extending their scope to a broader variational
framework.

</details>


### [20] [Coupling local and nonlocal total variation flow for image despeckling](https://arxiv.org/abs/2510.26296)
*Yi Ran,Zhichang Guo,Kehan Shi,Qirui Zhou,Jingfeng Shao,Martin Burger,Boying Wu*

Main category: math.AP

TL;DR: A coupled local-nonlocal total variation flow is proposed for image despeckling, combining the texture preservation of nonlocal equations with the strong denoising capabilities of local equations.


<details>
  <summary>Details</summary>
Motivation: Nonlocal equations preserve textures well but have weak regularization, while local equations offer strong denoising but fail to protect textures. The goal is to integrate both advantages.

Method: A coupled local-nonlocal total variation flow for image despeckling is investigated. The existence and uniqueness of weak solutions are established, along with properties like equivalent forms and asymptotic behavior.

Result: The weak solutions of the proposed equation converge to the weak solution of classical total variation flow under kernel rescaling. Coupling shows better performance compared to separate local and nonlocal models.

Conclusion: The coupled approach effectively combines the strengths of both local and nonlocal methods for improved image despeckling, with proven mathematical properties and convergence behavior.

Abstract: Nonlocal equations effectively preserve textures but exhibit weak
regularization effects in image denoising, whereas local equations offer strong
denoising capabilities yet fail to protect textures. To integrate the
advantages of both approaches, this paper investigates a coupled local-nonlocal
total variation flow for image despeckling. We establish the existence and
uniqueness of the weak solution for the proposed equation. Several properties,
including the equivalent forms of the weak solution and its asymptotic
behavior, are derived. Furthermore, we demonstrate that the weak solutions of
the proposed equation converge to the weak solution of the classical total
variation flow under kernel rescaling. The importance of coupling is
highlighted through comparisons with local and nonlocal models for image
despeckling.

</details>


### [21] [Complete spectrum of the Robin eigenvalue problem on the ball](https://arxiv.org/abs/2510.26331)
*Cancan Chen,Guowei Dai,Yingxin Sun*

Main category: math.AP

TL;DR: The paper analyzes the Robin eigenvalue problem on the unit ball, providing complete spectral structure and explicit formulas for eigenvalues in terms of Bessel function zeros.


<details>
  <summary>Details</summary>
Motivation: To understand the complete spectral structure of the Robin eigenvalue problem on the unit ball, particularly how eigenvalues depend on the boundary parameter α.

Method: Mathematical analysis using Bessel functions, specifically studying zeros of kJ_{ν+l+1}(k)-(α+l)J_{ν+l}(k) and αI_ν(k)+kI_{ν+1}(k) for different ranges of α.

Result: Found explicit formulas: for α>0, first eigenvalue is k_{ν,1}^2 and second is k_{ν+1,1}^2; for α∈(-1,0), first is -k̂_{ν,1}^2 and second is k_{ν+1,1}^2; for α=-1, first is -k̂_{ν,1}^2 and second is 0.

Conclusion: The ratio μ₂/μ₁ can be positive, negative, or zero depending on α, revealing the parameter's crucial role in determining the spectral properties.

Abstract: We investigate the following Robin eigenvalue problem \begin{equation*}
\left\{ \begin{array}{ll} -\Delta u=\mu u\,\, &\text{in}\,\, B,\\
\partial_\texttt{n} u+\alpha u=0 &\text{on}\,\, \partial B \end{array} \right.
\end{equation*} on the unit ball of $\mathbb{R}^N$. We obtain the complete
spectral structure of this problem. In particular, for $\alpha>0$, we find that
the first eigenvalue is $k_{\nu,1}^2$ and the second eigenvalue is exactly
$k_{\nu+1,1}^2$, where $k_{\nu+l,m}$ is the $m$th positive zero of
$kJ_{\nu+l+1}(k)-(\alpha+l) J_{\nu+l}(k)$. Moreover, when $\alpha\in(-1,0)$,
the first eigenvalue is $-\widehat{k}_{\nu,1}^2$ where $\widehat{k}_{\nu,1}$
denotes the unique zero of $\alpha I_{\nu}(k)+kI_{\nu+1}(k)$, and the second
eigenvalue is exactly $k_{\nu+1,1}^2$. Furthermore, for $\alpha=-1$, the first
eigenvalue is $-\widehat{k}_{\nu,1}^2$ and the second eigenvalue is exactly
$0$. Our conclusions indicate the ratio $\mu_2/\mu_1$ may be positive, negative
or zero according to the suitable ranges of the parameter $\alpha$.

</details>


### [22] [Tangential approach in the Dirichlet problem for elliptic equations](https://arxiv.org/abs/2510.26400)
*Jonathan Bennett,Arnaud Dumont,Andrew J. Morris*

Main category: math.AP

TL;DR: The paper shows that L^p-Dirichlet problem solvability for elliptic equations implies improved convergence from nontangential to tangential when boundary data has Sobolev regularity, with sharp estimates on convergence failure sets.


<details>
  <summary>Details</summary>
Motivation: To extend classical results for harmonic functions to elliptic equations with measurable coefficients, showing how boundary data regularity improves convergence properties.

Method: Uses quantitative absolute continuity (local A_∞ property) of L-harmonic measure to analyze convergence behavior and estimate Hausdorff dimension of convergence failure sets.

Result: Proves that nontangential convergence improves to tangential convergence for boundary data with Sobolev regularity, with sharp Hausdorff dimension estimates for convergence failure.

Conclusion: The local A_∞ property of harmonic measure guarantees improved convergence behavior for regular boundary data, extending classical harmonic function results to elliptic equations with measurable coefficients.

Abstract: It is well-known that solvability of the $\mathrm{L}^{p}$-Dirichlet problem
for elliptic equations $Lu:=-\mathrm{div}(A\nabla u)=0$ with real-valued,
bounded and measurable coefficients $A$ on Lipschitz domains
$\Omega\subset\mathbb{R}^{1+n}$ is characterised by a quantitative absolute
continuity of the associated $L$-harmonic measure. We prove that this local
$A_{\infty}$ property is sufficient to guarantee that the nontangential
convergence afforded to $\mathrm{L}^{p}$ boundary data actually improves to a
certain \emph{tangential} convergence when the data has additional (Sobolev)
regularity. Moreover, we obtain sharp estimates on the Hausdorff dimension of
the set on which such convergence can fail. This extends results obtained by
Dorronsoro, Nagel, Rudin, Shapiro and Stein for classical harmonic functions in
the upper half-space.

</details>


### [23] [Improved Gevrey Class Regularity of the Kadomtsev Petviashvili Equation](https://arxiv.org/abs/2510.26669)
*Aissa Boukarou,Lamia Seghour*

Main category: math.AP

TL;DR: Improved Gevrey regularity analysis for fifth-order KP-type equations, showing solutions are Gevrey regular of order 5σ in time when initial data are Gevrey regular of order σ in space.


<details>
  <summary>Details</summary>
Motivation: To extend and sharpen previous results by Boukarou et al. on Gevrey regularity of solutions to fifth-order KP-type equations, providing more precise regularity bounds.

Method: Used the method of majorant series to simultaneously treat all three variables (x,y,t), precisely tracking the influence of higher-order dispersive term ∂ₓ⁵u and lower-order terms α∂ₓ³u, ∂ₓ⁻¹∂y²u, and u∂ₓu.

Result: Proved that solutions are Gevrey regular of order 5σ in time when initial data are Gevrey regular of order σ ≥ 1 in spatial variables, and showed u(x,y,t) as function of t does not belong to Gᶻ for any 1 ≤ z < 5σ.

Conclusion: The paper establishes optimal Gevrey regularity bounds for fifth-order KP-type equations, demonstrating a sharp relationship between spatial and temporal regularity through a unified analytical approach.

Abstract: In this paper, we improve and extend the results obtained by Boukarou et al.
\cite{boukarou1} on the Gevrey regularity of solutions to a fifth-order
Kadomtsev-Petviashvili (KP)-type equation. We establish Gevrey regularity in
the time variable for solutions in $2+1$ dimensions, providing a sharper result
obtained through a new analytical approach. Assuming that the initial data are
Gevrey regular of order $\sigma \geq 1$ in the spatial variables, we prove that
the corresponding solution is Gevrey regular of order $5 \sigma$ in time.
Moreover, we show that the function $u(x, y, t)$, viewed as a function of $t$,
does not belong to $G^z$ for any $1 \leq z<5 \sigma$. The proof simultaneously
treats all three variables $x, y$, and $t$, and employs the method of majorant
series, precisely tracking the influence of the higher-order dispersive term
$\partial_x^5 u$ together with the lower-order terms $\alpha \partial_x^3 u,
\partial_x^{-1} \partial_y^2 u$, and $u \partial_x u$.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [24] [Equation Discovery, Parametric Simulation, and Optimization Using the Physics-Informed Neural Network (PINN) Method for the Heat Conduction Problem](https://arxiv.org/abs/2510.25925)
*Ehsan Ghaderi,Mohamad Ali Bijarchi,Siamak Kazemzadeh Hannani,Ali Nouri Boroujerdi*

Main category: physics.comp-ph

TL;DR: PINN method applied to heat conduction for modeling, simulation, and optimization tasks, showing strength in inverse problems and parametric analysis despite not outperforming traditional methods in forward problems.


<details>
  <summary>Details</summary>
Motivation: To investigate PINN capabilities for three major engineering tasks in heat conduction: modeling with fractional derivatives, parametric simulation, and inverse optimization problems.

Method: Used Physics-Informed Neural Networks (PINN) for equation discovery with fractional-order derivatives, parametric simulation of thermal conductivity effects, and inverse problem optimization to infer unknown physical properties.

Result: PINNs demonstrated powerful capabilities for parametric simulation, optimization, and equation discovery, though they don't outperform traditional numerical solvers in speed and accuracy for forward problems.

Conclusion: PINNs provide a flexible framework valuable for inverse and data-driven modeling applications, particularly in parametric analysis and equation discovery tasks.

Abstract: In this study, the capabilities of the Physics-Informed Neural Network (PINN)
method are investigated for three major tasks: modeling, simulation, and
optimization in the context of the heat conduction problem. In the modeling
phase, the governing equation of heat transfer by conduction is reconstructed
through equation discovery using fractional-order derivatives, enabling the
identification of the fractional derivative order that best describes the
physical behavior. In the simulation phase, the thermal conductivity is treated
as a physical parameter, and a parametric simulation is performed to analyze
its influence on the temperature field. In the optimization phase, the focus is
placed on the inverse problem, where the goal is to infer unknown physical
properties from observed data. The effectiveness of the PINN approach is
evaluated across these three fundamental engineering problem types and compared
against conventional numerical methods. The results demonstrate that although
PINNs may not yet outperform traditional numerical solvers in terms of speed
and accuracy for forward problems, they offer a powerful and flexible framework
for parametric simulation, optimization, and equation discovery, making them
highly valuable for inverse and data-driven modeling applications.

</details>


### [25] [Generative Artificial Intelligence for Air Shower Simulation](https://arxiv.org/abs/2510.26316)
*C. Bozza,A. Calivà,A. De Caro,D. De Gruttola,S. De Pasquale,L. A. Fusco,G. Messuti,C. Poirè,S. Scarpetta,T. Virgili*

Main category: physics.comp-ph

TL;DR: GAN-based approach accelerates cosmic ray air shower simulations by 10,000x compared to traditional Monte Carlo methods, reducing computational time and energy consumption.


<details>
  <summary>Details</summary>
Motivation: Traditional Monte Carlo simulations of cosmic ray air showers are computationally intensive and consume major resources in astroparticle physics experiments.

Method: Developed and trained a Generative Adversarial Network (GAN) on CORSIKA-generated proton-induced air shower data to reproduce key particle distributions.

Result: Training took 74 hours, but generation time per shower reduced by factor of 10^4, with accurate reproduction of muon energy spectra and spatial distributions.

Conclusion: GANs provide a highly efficient alternative to traditional Monte Carlo methods for air shower simulations, enabling faster and more energy-efficient computations.

Abstract: The detailed simulation of extensive air showers, produced by primary cosmic
rays interacting in the atmosphere, is a task that is traditionally undertaken
by means of Monte Carlo methods. These processes are computationally intensive,
accounting for a major fraction of the computational resources used in the
large-scale simulations required by current and future experiments in the field
of astroparticle physics. In this work, we present a novel approach based on
Generative Adversarial Networks (GANs) to accelerate air shower simulations. We
developed and trained a GAN on a dataset of high-energy proton-induced air
showers generated with \texttt{CORSIKA}; our model reproduces key distributions
of secondary particles, such as energy spectra and spatial distributions at
ground level of muons. Once the model has been trained, which takes
approximately 74 hours, the generation real time per shower is reduced by a
factor of $10^4$ with respect to the full \texttt{CORSIKA} simulation, leading
to a substantial decrease in both computational time and energy consumption.

</details>


### [26] [Patch-MLP-Based Predictive Control: Simulation of Upstream Pointing Stabilization for PHELIX Laser System](https://arxiv.org/abs/2510.26540)
*Jiaying Wang,Jonas Benjamin Ohland,Yen-Yu Chang,Vedhas Pandit,Stefan Bock,Andrew-Hiroaki Okukura,Udo Eisenbarth,Arie Irman,Michael Bussmann,Ulrich Schramm,Jeffrey Kelling*

Main category: physics.comp-ph

TL;DR: A predictive control strategy using a patch-based MLP for beam pointing error forecasting combined with PID control reduces jitter in high-energy laser facilities by 10-20% compared to conventional PID alone.


<details>
  <summary>Details</summary>
Motivation: Traditional PID control for beam pointing stability in high-energy laser facilities like PHELIX is limited by system delays and mechanical inertia, requiring better methods to handle short-term jitter and improve reproducibility.

Method: A patch-based multilayer perceptron (Patch-MLP) is trained on diagnostic time-series data to predict beam pointing errors, with these predictions fed to a PID controller for correction. This feed-forward approach compensates for system delays.

Result: Simulations show the predictive control scheme reduces residual jitter by 10-20% compared to conventional PID control, maintaining stable performance over 10 hours without drift.

Conclusion: The predictive controller improves beam pointing stability and reproducibility in high-energy, low repetition rate laser experiments by effectively compensating for system delays and reducing jitter.

Abstract: High-energy laser facilities such as PHELIX at GSI require excellent beam
pointing stability for reproducibility and relative independence for future
experiments. Beam pointing stability has been traditionally achieved using
simple proportional-integral-derivative (PID) control which removes the problem
of slow drift, but is limited because of the time delay in knowing the
diagnosis and the inertia in the mechanical system associated with mirrors. In
this work, we introduce a predictive control strategy where the forecasting of
beam pointing errors is performed by a patch-based multilayer perceptron
(Patch-MLP) designed to capture local temporal patterns for more robust
short-term jitter prediction. The subsequent conversion of these predicted
errors into correction signals is handled by a PID controller. The neural
network has been trained on diagnostic time-series data to predict beam
pointing error. Using the feed-forward controller compensates for system
delays. Simulations with a correction mirror placed upstream of the PHELIX
pre-amplifier bridge confirm that the predictive control scheme reduces
residual jitter compared to conventional PID control. Over a 10-hour dataset
the controller maintained stable performance without drift, while standard
pointing metrics showed consistent improvements of the order of 10 to 20
percent. The predictive controller operates without drift, and therefore may
improve reproducibility and operational efficiency in high energy, low
repetition rate laser experiment conditions.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [A Self-Consistent Model of Kinetic Alfven Solitons in Pulsar Wind Plasma: Linking Soliton Characteristics to Pulsar Observables](https://arxiv.org/abs/2510.25972)
*Manpreet Singh,Geetika Slathia,N. S. Saini,Siming Liu*

Main category: physics.plasm-ph

TL;DR: A model for kinetic Alfven soliton formation in pulsar winds shows soliton properties depend on pulsar parameters like spin period, plasma composition, and particle distributions, with heavier ions creating broader solitons and millisecond pulsars hosting the narrowest ones.


<details>
  <summary>Details</summary>
Motivation: To understand how kinetic Alfven solitons form and propagate in pulsar wind zones and how their properties relate to observable pulsar characteristics, providing insights into magnetospheric microphysics.

Method: Using a reductive perturbation approach to derive a Korteweg-de Vries equation that governs nonlinear evolution of kinetic Alfven solitons in relativistic magnetized electron-positron-ion plasma.

Result: Soliton amplitude and width depend on pulsar spin period, spin-down rate, pair multiplicity, plasma composition, and particle distributions. Heavier ions produce broader solitons, higher pair multiplicity leads to smaller solitons, and millisecond pulsars have the narrowest solitons.

Conclusion: This work establishes a framework linking soliton dynamics to measurable pulsar parameters, enabling interpretation of magnetospheric microphysics and its role in shaping pulsar emission signatures.

Abstract: We present a self-consistent model for the formation and propagation of
kinetic Alfven (KA) solitons in the pulsar wind zone, where a relativistic,
magnetized electron positron ion plasma flows along open magnetic field lines
beyond the light cylinder. Using a reductive perturbation approach, we derive a
Korteweg de Vries (KdV) equation that governs the nonlinear evolution of KA
solitons in this environment. The soliton amplitude and width are shown to
depend sensitively on key pulsar observables, including spin period, spin-down
rate, and pair multiplicity as well as plasma composition and suprathermal
particle distributions. Our analysis reveals that soliton structures are
strongly influenced by the presence of heavy ions, kappa-distributed pairs, and
oblique propagation angles. Heavier ion species such as Fe26+ produce
significantly broader solitons due to enhanced inertia and dispersion, while
increasing pair multiplicity leads to smaller solitons through stronger
screening. Oblique propagation (larger theta) results in wider but
lower-amplitude solitons, and more thermalized pair plasmas (higher kappa)
support taller and broader structures. A population-level analysis of 1174
pulsars shows a clear positive correlation between soliton width and spin
period, with millisecond pulsars hosting the narrowest solitons. By linking
soliton dynamics to measurable pulsar parameters, this work provides a
framework for interpreting magnetospheric microphysics and its role in shaping
pulsar emission signatures.

</details>


### [28] [Optimization of the Compact Stellarator with Simple Coils at finite-beta](https://arxiv.org/abs/2510.26155)
*Haorong Qiu,Guodong Yu,Peiyou Jiang,Guoyong Fu*

Main category: physics.plasm-ph

TL;DR: Single-stage optimization of coil currents in the CSSC stellarator mitigates finite beta effects on neoclassical confinement.


<details>
  <summary>Details</summary>
Motivation: The CSSC stellarator, optimized in vacuum, suffers from detrimental finite beta effects on neoclassical confinement.

Method: Simply modifying the coil currents of the CSSC through single-stage optimization.

Result: Finite beta effects can be largely mitigated by reducing the coil currents of CSSC.

Conclusion: Optimizing coil currents is an effective approach to address finite beta effects in stellarators.

Abstract: An optimized stellarator at finite plasma beta is realized by single-stage
optimization of simply modifying the coil currents of the Compact Stellarator
with Simple Coils (CSSC)[Yu et al., J. Plasma Physics 88,905880306 (2022)]. The
CSSC is an optimized stellarator obtained by direct optimization via coil
shapes, with its coil topology similar to that of the Columbia Non-neutral
Torus (CNT) [Pederson et al., Phys. Rev. Lett. 88, 205002 (2002)]. Due to its
vacuum-based optimization, the CSSC exhibits detrimental finite beta effects on
neoclassical confinement. The results of optimization show that the finite beta
effects can be largely mitigated by reducing the coil currents of CSSC.

</details>


### [29] [Design and Implementation of a Fast-Sweeping Langmuir Probe Diagnostic for DC Arc Jet Environments](https://arxiv.org/abs/2510.26162)
*Sebastian V. Colom,Magnus A. Haw,Jocelino Rodrigues*

Main category: physics.plasm-ph

TL;DR: Development of an open-source, low-cost fast-sweeping Langmuir probe system with 200 kHz temporal resolution for measuring transient plasma behavior in high-enthalpy environments.


<details>
  <summary>Details</summary>
Motivation: Conventional Langmuir probes lack sufficient temporal resolution to capture transient plasma behavior in dynamic environments, creating a need for faster diagnostic tools.

Method: Designed and implemented a fast-sweeping Langmuir probe system using voltage sweeping technology, tested in the 30 kW miniature Arc jet Research Chamber (mARC II) under extreme aerothermal conditions.

Result: The system successfully operated in high-enthalpy environments, providing time-resolved electron temperature and density measurements along the flow's radial profile at up to 200 kHz resolution.

Conclusion: Established a robust and accessible Langmuir diagnostic solution for researchers studying transient plasma behavior in high-enthalpy environments.

Abstract: Langmuir probe diagnostics are a cornerstone of plasma characterization,
providing critical measurements of electron temperature, electron density, and
plasma potential. However, conventional swept Langmuir probes and other
traditional electrostatic probes often lack the temporal resolution necessary
to capture transient plasma behavior in dynamic environments. This paper
presents the design and implementation of a fast-sweeping Langmuir probe system
that is open-source, low-cost, and adaptable for a wide range of plasma
applications. The probe system incorporates voltage sweeping to resolve rapid
fluctuations in plasma parameters at a temporal resolution of up to 200 kHz. To
validate its performance, the system was implemented in the 30 kW miniature Arc
jet Research Chamber (mARC II), a high-enthalpy DC arc jet facility designed
for prototype testing and development. Experimental results demonstrate the
probe's capability to operate in extreme aerothermal conditions, providing
time-resolved electron temperature and density along the flow's radial profile.
This work establishes a robust and accessible Langmuir diagnostic solution for
researchers studying transient plasma behavior in high-enthalpy environments.

</details>


### [30] [High-order Mie resonance and transient field enhancement in laser-driven plasma nanoshells](https://arxiv.org/abs/2510.26175)
*Xiaohui Gao*

Main category: physics.plasm-ph

TL;DR: Plasma nanoshells achieve substantial field enhancement via high-order Mie resonances, with optimal geometries providing ~3x enhancement at 800 nm before plasma expansion disrupts resonance in tens of femtoseconds.


<details>
  <summary>Details</summary>
Motivation: To optimize laser-plasma interactions for applications like laser-cluster interaction diagnostics and energetic ion production from engineered core-shell targets.

Method: Combined Mie theory and particle-in-cell simulations to study field enhancement in plasma nanoshells through high-order Mie resonances.

Result: Optimal shell geometries yield approximately threefold electric field enhancement for 800 nm irradiation, with transient buildup times of tens of femtoseconds before plasma expansion disrupts resonance. Few-cycle pulses show reduced enhancement due to insufficient resonance establishment.

Conclusion: Temporal dynamics play a critical role in nanoplasma resonances, enabling optimized laser-plasma interactions for various applications.

Abstract: We demonstrate substantial field enhancement in plasma nanoshells through
high-order Mie resonances using combined Mie theory and particle-in-cell
simulations. Optimal shell geometries yield approximately threefold electric
field enhancement for 800 nm irradiation, with transient buildup times of tens
of femtoseconds before plasma expansion disrupts resonance. Few-cycle pulses
produce reduced enhancement due to insufficient resonance establishment. These
findings enable optimized laser-plasma interactions for applications including
diagnostics of laser-cluster interaction and energetic ion production from
engineered core-shell targets, highlighting the critical role of temporal
dynamics in nanoplasma resonances.

</details>


### [31] [Nonlocal Model for Electron Heat Flux and Self-generated Magnetic Field](https://arxiv.org/abs/2510.26640)
*Xinyu Zhu,Wenqiang Yuan,Yusen Wang,Zhipeng Zhang,Xianxu Jin,Zhonghai Zhao,Bin Qiao*

Main category: physics.plasm-ph

TL;DR: Proposes a new nonlocal model that simultaneously recovers kinetic effects for both electron heat conduction and magnetic field generation in hydrodynamic scale for inertial confinement fusion applications.


<details>
  <summary>Details</summary>
Motivation: Current nonlocal models only address kinetic effects on heat flux but still use flux limiters for magnetic field modeling, lacking self-consistent treatment of both phenomena.

Method: Developed a new nonlocal model that self-consistently considers electric field corrections to recover kinetic effects for both heat conduction and magnetic field simultaneously in hydrodynamic scale.

Result: The model enables systematic study of nonlocal corrections in magnetized plasma and magnetic field generation without density gradients. Nonlocal effects significantly alter magnetic field distribution in laser ablation.

Conclusion: Nonlocal effects substantially change magnetic field distributions in laser ablation processes, which could potentially impact hydrodynamic instabilities in inertial confinement fusion.

Abstract: Coupling of electron heat conduction and magnetic field takes significant
effects in inertial confinement fusion (ICF). As the nonlocal models for
electron heat conduction have been developed for modeling kinetic effects on
heat flux in hydrodynamic scale, modeling kinetic effects on magnetic field are
still restricted to flux limiters instead of nonlocal corrections. We propose a
new nonlocal model which can recover the kinetic effects for heat conduction
and magnetic field in hydrodynamic scale simultaneously. We clarify the
necessity of self-consistently considering the electric field corrections in
nonlocal models to get reasonable physical quantities. Using the new nonlocal
model, the nonlocal corrections of transport coefficients in magnetized plasma
and the magnetic field generation without density gradients are systematically
studied. We find nonlocal effects significantly change the magnetic field
distribution in laser ablation, which potentially influences the hydrodynamic
instabilities in ICF.

</details>


### [32] [Spectral Deconvolution without the Deconvolution: Extracting Temperature from X-ray Thomson Scattering Spectra without the Source-and-Instrument Function](https://arxiv.org/abs/2510.26747)
*Thomas Gawne,Alina Kononov,Andrew Baczewski,Hannah Bellenbaum,Maximilian P Böhme,Zhandos Moldabekov,Thomas R Preston,Sebastian Schwalbe,Jan Vorberger,Tobias Dornheim*

Main category: physics.plasm-ph

TL;DR: Proposes a method to extract temperature from X-ray Thomson scattering spectra without needing to know the source-and-instrument function, by using ratios of Laplace-transformed spectra from different scattering angles.


<details>
  <summary>Details</summary>
Motivation: Current methods for extracting temperature from XRTS spectra require deconvolution of the source-and-instrument function (SIF), which is challenging to measure accurately and can strongly affect results.

Method: Uses ratios of Laplace-transformed XRTS spectra collected at different scattering angles, which effectively performs deconvolution without explicit SIF knowledge.

Result: Method is robust to spectral noise and physical differences between spectrometers, and can identify non-equilibrium effects through inconsistent temperature readings.

Conclusion: This approach enables direct temperature extraction from XRTS spectra without SIF deconvolution, providing a more reliable method for thermal equilibrium systems and detecting non-equilibrium conditions.

Abstract: X-ray Thomson scattering (XRTS) probes the dynamic structure factor of the
system, but the measured spectrum is broadened by the combined
source-and-instrument function (SIF) of the setup. In order to extract
properties such as temperature from an XRTS spectrum, the broadening by the SIF
needs to be removed. Recent work [Dornheim et al. Nature Commun. 13, 7911
(2022)] has suggested that the SIF may be deconvolved using the two-sided
Laplace transform. However, the extracted information can depend strongly on
the shape of the input SIF, and the SIF is in practice challenging to measure
accurately. Here, we propose an alternative approach: we demonstrate that
considering ratios of Laplace-transformed XRTS spectra collected at different
scattering angles is equivalent to performing the deconvolution, but without
the need for explicit knowledge of the SIF. From these ratios, it is possible
to directly extract the temperature from the scattering spectra, when the
system is in thermal equilibrium. We find the method to be generally robust to
spectral noise and physical differences between the spectrometers, and we
explore situations in which the method breaks down. Furthermore, the fact that
consistent temperatures can be extracted for systems in thermal equilibrium
indicates that non-equilibrium effects could be identified by inconsistent
temperatures of a few eV between the ratios of three or more scattering angles.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [33] [Exciton dynamics in equilibrium and nonequilibrium regimes](https://arxiv.org/abs/2510.26221)
*Pushpendra Yadav*

Main category: cond-mat.mes-hall

TL;DR: First-principles study of excitons in 2D materials using GW-BSE approach, showing density-dependent redshift-blueshift crossover, electron-phonon effects on spectra and lifetimes, and formation of electron-hole liquid phase at high densities and low temperatures.


<details>
  <summary>Details</summary>
Motivation: To understand nonequilibrium behavior of excitons at high excitation densities in 2D materials, where phenomena like electron-hole liquids emerge but are less explored theoretically compared to equilibrium conditions.

Method: Uses GW approximation and Bethe-Salpeter equation (GW-BSE approach) to investigate exciton properties from equilibrium to nonequilibrium conditions in two-dimensional materials.

Result: Shows redshift-blueshift crossover with increasing carrier density, electron-phonon interactions modify optical spectra and exciton lifetimes at finite temperatures, and demonstrates formation of electron-hole liquid phase above critical carrier density and below critical temperature.

Conclusion: Enhanced Coulomb interactions in 2D materials can stabilize electron-hole liquid phase at significantly higher temperatures, identifying promising material candidates for observing these collective states.

Abstract: The bound electron-hole pairs known as excitons govern the optical properties
of insulating solids. While their behavior in equilibrium is well-understood
theoretically, the nonequilibrium regime at high excitation densities-where
phenomena like electron-hole liquids emerge - is less explored. This thesis
presents a first-principles study of excitons in two-dimensional materials. We
use the GW approximation and the Bethe-Salpeter equation to investigate their
properties from equilibrium to nonequilibrium conditions. We first demonstrate
how increasing photo-excited carrier density leads to a redshift-blueshift
crossover of excitons. We then show that electron-phonon interactions
critically modify optical spectra and exciton lifetimes at finite temperatures.
Finally, we unify these effects to demonstrate the formation of an
electron-hole liquid phase above a critical carrier density and below a
critical temperature. Our work identifies how enhanced Coulomb interactions in
two dimensions can stabilize this phase at significantly higher temperatures,
proposing promising material candidates for observing these collective states.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [34] [A game-theoretic approach to the parabolic normalized p-Laplacian obstacle problem](https://arxiv.org/abs/2510.25999)
*Hamid El Bahja*

Main category: math.PR

TL;DR: Establishes a probabilistic representation for the parabolic obstacle problem with normalized p-Laplacian using a zero-sum stochastic tug-of-war game with noise and stopping options.


<details>
  <summary>Details</summary>
Motivation: To provide a probabilistic interpretation and game-theoretic approach for solving the parabolic obstacle problem associated with the normalized p-Laplacian operator.

Method: Introduces a zero-sum stochastic tug-of-war game with noise in space-time cylinder, where one player can stop the game to collect payoff from obstacle function. Proves value functions exist, satisfy dynamic programming principle, and converge to viscosity solution.

Result: Value functions of the game exist, satisfy dynamic programming principle, and converge uniformly to the unique viscosity solution of the continuous obstacle problem as step size ε→0.

Conclusion: The stochastic game provides an effective probabilistic representation for the parabolic obstacle problem with normalized p-Laplacian, with convergence to the continuous solution.

Abstract: This paper establishes a probabilistic representation for the solution of the
parabolic obstacle problem associated with the normalized $p$-Laplacian. We
introduce a zero-sum stochastic tug-of-war game with noise in a space-time
cylinder, where one player has the option to stop the game at any time to
collect a payoff given by an obstacle function. We prove that the value
functions of this game exist, satisfy a dynamic programming principle, and
converge uniformly to the unique viscosity solution of the continuous obstacle
problem as the step size $\varepsilon$ tends to zero.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [35] [Numerical Investigation of Single-Core to Split-Core Transitions in Nematic Liquid Crystals](https://arxiv.org/abs/2510.26215)
*Daniel Siebel-Cortopassi,Pei Liu*

Main category: cond-mat.soft

TL;DR: Analysis of single-core and split-core defect structures in nematic liquid crystals using Landau-de Gennes framework, revealing temperature-dependent bifurcation behavior and energy minimization.


<details>
  <summary>Details</summary>
Motivation: To understand the stability and transition between different defect configurations in nematic liquid crystals, particularly how temperature affects defect core structures and their energy landscapes.

Method: Using the Landau-de Gennes framework to study minimizers of the associated energy functional, analyzing bifurcation behavior at critical temperature thresholds, and examining dependence on domain size.

Result: Below critical temperature, both split-core and single-core configurations exist as solutions, with split-core having lower energy. Above threshold, only single-core remains stable. Core sizes depend on temperature and domain size.

Conclusion: The study reveals a temperature-driven bifurcation in nematic liquid crystal defects, with split-core configurations being energetically favorable below a critical temperature threshold that depends on domain size.

Abstract: We analyze single-core and split-core defect structures in nematic liquid
crystals within the Landau-de Gennes framework by studying minimizers of the
associated energy functional. A bifurcation occurs at a critical temperature
threshold, below which both split-core and single-core configurations are
solutions to the Euler-Lagrange equation, with the split-core defect possessing
lower energy. Above the threshold, the split-core configuration vanishes,
leaving the single-core defect as the only stable solution. We analyze the
dependence of such temperature threshold on the domain size and characterize
the nature of the transition between the two defect types. We carry out a
quantitative study of defect core sizes as functions of temperature and domain
size for both single and split core defects.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [36] [Geometric and Orbital Control of Correlated States in Small Hubbard Clusters](https://arxiv.org/abs/2510.25919)
*Shivanshu Dwivedi,Kalum Palandage*

Main category: cond-mat.str-el

TL;DR: A framework for engineering local electron pairing in quantum dot arrays using lattice geometry, orbital hybridization, and electric fields, revealing design principles for controlling charge and spin correlations.


<details>
  <summary>Details</summary>
Motivation: To develop a predictive approach for designing correlated quantum matter in semiconductor quantum dot arrays by systematically controlling key parameters.

Method: Hartree-Fock simulations on 3D quantum dot clusters (from tetrahedron to FCC lattice) at/near half-filling, analyzing three control levers: lattice geometry, orbital hybridization, and electric fields.

Result: Three design principles: (1) Geometric hierarchy where coordination number Z controls resilience to Coulomb repulsion, (2) Orbital hybridization enhances double occupancy at moderate U, (3) Electric field induces pairing through charge localization, especially in low-connectivity clusters.

Conclusion: These principles provide a blueprint for deterministic control of charge and spin correlations in quantum-dot-based quantum hardware.

Abstract: Arrays of semiconductor quantum dots provide a powerful platform to design
correlated quantum matter from the bottom up. We establish a predictive
framework for engineering local electron pairing in these artificial molecules
by systematically deploying three control levers: lattice geometry, orbital
hybridization, and external electric fields. Using Hartree-Fock simulations on
canonical 3D clusters from the tetrahedron (Z = 3) to the FCC lattice (Z = 12),
at and near half-filling, we uncover three fundamental design principles. (i)
Geometric Hierarchy: The resilience to Coulomb repulsion U is dictated by the
coordination number Z, which controls kinetic delocalization. (ii) Orbital
Hybridization: Counter-intuitively, inter-orbital hopping t_orb acts not as a
simple suppressor of pairing, but as a sophisticated control knob that enhances
double occupancy at moderate U by engineering the on-site energy landscape.
(iii) Field Squeezing: An electric field robustly induces pairing by forcing
charge localization, an effect most potent in low-connectivity clusters. These
principles form a blueprint for deterministically targeting charge and spin
correlations in quantum-dot-based quantum hardware.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [37] [Gradient Flow Sampler-based Distributionally Robust Optimization](https://arxiv.org/abs/2510.25956)
*Zusen Xu,Jia-Jie Zhu*

Main category: math.OC

TL;DR: A PDE gradient flow framework for distributionally robust optimization (DRO) that connects Markov Chain Monte Carlo sampling with gradient flow theory to sample from worst-case distributions and solve DRO problems.


<details>
  <summary>Details</summary>
Motivation: To provide a mathematically principled framework for DRO that offers new insights into existing methods and enables construction of new algorithms, addressing limitations of previous reformulation techniques and iterative approaches.

Method: Proposes a gradient flow framework using Wasserstein Fisher-Rao and Stein variational gradient flows to solve Wasserstein and Sinkhorn DRO problems, connecting MCMC sampling with gradient flow theory.

Result: The framework can be implemented as practical algorithms for sampling worst-case distributions and solving DRO problems, with numerical studies using stochastic gradient descent providing empirical validation.

Conclusion: The gradient flow view provides a unified theoretical foundation for DRO that recovers existing methods, offers new insights into their optimization dynamics, and enables development of new algorithms for distributional optimization.

Abstract: We propose a mathematically principled PDE gradient flow framework for
distributionally robust optimization (DRO). Exploiting the recent advances in
the intersection of Markov Chain Monte Carlo sampling and gradient flow theory,
we show that our theoretical framework can be implemented as practical
algorithms for sampling from worst-case distributions and, consequently, DRO.
While numerous previous works have proposed various reformulation techniques
and iterative algorithms, we contribute a sound gradient flow view of the
distributional optimization that can be used to construct new algorithms. As an
example of applications, we solve a class of Wasserstein and Sinkhorn DRO
problems using the recently-discovered Wasserstein Fisher-Rao and Stein
variational gradient flows. Notably, we also show some simple reductions of our
framework recover exactly previously proposed popular DRO methods, and provide
new insights into their theoretical limit and optimization dynamics. Numerical
studies based on stochastic gradient descent provide empirical backing for our
theoretical findings.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [38] [On a semi-discrete model of Maxwell's equations in three and two dimensions](https://arxiv.org/abs/2510.26427)
*Volodymyr Sushch*

Main category: math-ph

TL;DR: A geometric, structure-preserving semi-discrete formulation of Maxwell's equations using discrete exterior calculus that maintains the intrinsic geometric and topological structures of the continuous theory.


<details>
  <summary>Details</summary>
Motivation: To develop a spatial discretization of Maxwell's equations that preserves the fundamental geometric and topological properties of the continuous formulation, addressing the need for consistent discretization methods in computational electromagnetics.

Method: Using discrete exterior calculus framework to create semi-discrete formulations of Maxwell's equations in both 3D and 2D settings, with analysis of essential properties and comparison to classical Maxwell's equations.

Result: Successfully developed a structure-preserving semi-discrete model that maintains geometric and topological properties. Demonstrated the approach on a combinatorial 2D torus where equations become first-order linear ODEs, and derived explicit general solution expressions.

Conclusion: The discrete exterior calculus approach provides a geometrically consistent and structure-preserving method for discretizing Maxwell's equations, offering analytical solutions in special cases like the 2D torus while maintaining fundamental physical properties.

Abstract: In this paper, we develop a geometric, structure-preserving semi-discrete
formulation of Maxwell's equations in both three- and two-dimensional settings
within the framework of discrete exterior calculus. This approach preserves the
intrinsic geometric and topological structures of the continuous theory while
providing a consistent spatial discretization. We analyze the essential
properties of the proposed semi-discrete model and compare them with those of
the classical Maxwell's equations. As a special case, the model is illustrated
on a combinatorial two-dimensional torus, where the semi-discrete Maxwell's
equations take the form of a system of first-order linear ordinary differential
equations. An explicit expression for the general solution of this system is
also derived.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [39] [Generative sampling with physics-informed kernels](https://arxiv.org/abs/2510.26678)
*Friederike Ihssen,Renzo Kapust,Jan M. Pawlowski*

Main category: hep-lat

TL;DR: A generative network for Monte-Carlo sampling in lattice field theories that uses physics-informed renormalization group flows, transforming the generative task into solving independent linear differential equations for transformation kernels.


<details>
  <summary>Details</summary>
Motivation: To address out-of-domain problems in generative models and enable further optimization in Monte-Carlo sampling for lattice field theories.

Method: Uses physics-informed renormalization group flows with layerwise independent learning, where propagation between layers is governed by simple first-order partial differential equations for renormalization group kernels.

Result: The architecture allows iterative refinement of kernels and demonstrates practical feasibility in simulations of scalar field theories.

Conclusion: The approach structurally tackles out-of-domain problems in generative models and provides a path for further optimization in Monte-Carlo sampling applications.

Abstract: We construct a generative network for Monte-Carlo sampling in lattice field
theories and beyond, for which the learning of layerwise propagation is done
and optimised independently on each layer. The architecture uses
physics-informed renormalisation group flows that provide access to the
layerwise propagation step from one layer to the next in terms of a simple
first order partial differential equation for the respective renormalisation
group kernel through a given layer. Thus, it transforms the generative task
into that of solving once the set of independent and linear differential
equations for the kernels of the transformation. As these equations are
analytically known, the kernels can be refined iteratively. This allows us to
structurally tackle out-of-domain problems generally encountered in generative
models and opens the path to further optimisation. We illustrate the practical
feasibility of the architecture within simulations in scalar field theories.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [A Practitioner's Guide to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2510.25781)
*Amir Noorizadegan,Sifan Wang,Leevan Ling*

Main category: cs.LG

TL;DR: A comprehensive review of Kolmogorov-Arnold Networks (KANs) covering their theoretical foundations, architectural variants, implementation strategies, and practical guidance for selection and use.


<details>
  <summary>Details</summary>
Motivation: KANs have emerged as a promising alternative to MLPs with enhanced expressivity and interpretability, requiring a systematic synthesis of the rapidly expanding research landscape to guide practitioners and researchers.

Method: Systematic review and categorization of KAN implementations, analysis of basis function choices (B-splines, polynomials, ReLU, Gaussian RBFs, Fourier series), and development of a structured roadmap covering accuracy improvements, efficiency techniques, and regularization methods.

Result: Created a comprehensive taxonomy of KAN advancements, identified key trade-offs in basis function selection, and provided practical implementation guidance through a 'Choose-Your-KAN' framework with an associated GitHub repository.

Conclusion: KANs offer superior parameter efficiency and interpretability compared to MLPs, with ongoing research addressing current gaps; the review serves as a structured reference for the growing KAN ecosystem.

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising
alternative to traditional Multilayer Perceptrons (MLPs), inspired by the
Kolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed
activation functions on nodes, KANs employ learnable univariate basis functions
on edges, offering enhanced expressivity and interpretability. This review
provides a systematic and comprehensive overview of the rapidly expanding KAN
landscape, moving beyond simple performance comparisons to offer a structured
synthesis of theoretical foundations, architectural variants, and practical
implementation strategies. By collecting and categorizing a vast array of
open-source implementations, we map the vibrant ecosystem supporting KAN
development. We begin by bridging the conceptual gap between KANs and MLPs,
establishing their formal equivalence and highlighting the superior parameter
efficiency of the KAN formulation. A central theme of our review is the
critical role of the basis function; we survey a wide array of choices,
including B-splines, Chebyshev and Jacobi polynomials, ReLU compositions,
Gaussian RBFs, and Fourier series, and analyze their respective trade-offs in
terms of smoothness, locality, and computational cost. We then categorize
recent advancements into a clear roadmap, covering techniques for improving
accuracy, efficiency, and regularization. Key topics include physics-informed
loss design, adaptive sampling, domain decomposition, hybrid architectures, and
specialized methods for handling discontinuities. Finally, we provide a
practical "Choose-Your-KAN" guide to help practitioners select appropriate
architectures, and we conclude by identifying current research gaps. The
associated GitHub repository https://github.com/AmirNoori68/kan-review
complements this paper and serves as a structured reference for ongoing KAN
research.

</details>


### [41] [Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training](https://arxiv.org/abs/2510.25803)
*Hong Wang,Haiyang Xin,Jie Wang,Xuanze Yang,Fei Zha,Huanshuo Dong,Yan Jiang*

Main category: cs.LG

TL;DR: Proposes MoE-POT, a sparse Mixture-of-Experts architecture for PDE neural operators that efficiently scales parameters while controlling inference costs, achieving better zero-shot performance with fewer activated parameters.


<details>
  <summary>Details</summary>
Motivation: Address challenges in PDE neural operator pre-training: heterogeneity of PDE datasets causing high mixed training errors, and dense pre-training models incurring high inference costs.

Method: Uses layer-wise router-gating network to dynamically select 4 routed experts from 16 expert networks during inference, plus 2 shared experts to capture common PDE properties. Output is weighted average of activated experts.

Result: Pre-trained models from 30M to 0.5B parameters on 6 PDE datasets. 90M activated parameter model achieves 40% reduction in zero-shot error compared to 120M parameter existing models. Router decisions can infer dataset types.

Conclusion: MoE-POT effectively addresses PDE dataset heterogeneity and inference cost issues through sparse activation, achieving superior performance with interpretable expert selection.

Abstract: Pre-training has proven effective in addressing data scarcity and performance
limitations in solving PDE problems with neural operators. However, challenges
remain due to the heterogeneity of PDE datasets in equation types, which leads
to high errors in mixed training. Additionally, dense pre-training models that
scale parameters by increasing network width or depth incur significant
inference costs. To tackle these challenges, we propose a novel
Mixture-of-Experts Pre-training Operator Transformer (MoE-POT), a
sparse-activated architecture that scales parameters efficiently while
controlling inference costs. Specifically, our model adopts a layer-wise
router-gating network to dynamically select 4 routed experts from 16 expert
networks during inference, enabling the model to focus on equation-specific
features. Meanwhile, we also integrate 2 shared experts, aiming to capture
common properties of PDE and reduce redundancy among routed experts. The final
output is computed as the weighted average of the results from all activated
experts. We pre-train models with parameters from 30M to 0.5B on 6 public PDE
datasets. Our model with 90M activated parameters achieves up to a 40%
reduction in zero-shot error compared with existing models with 120M activated
parameters. Additionally, we conduct interpretability analysis, showing that
dataset types can be inferred from router-gating network decisions, which
validates the rationality and effectiveness of the MoE architecture.

</details>


### [42] [How Regularization Terms Make Invertible Neural Networks Bayesian Point Estimators](https://arxiv.org/abs/2510.26704)
*Nick Heilenkötter*

Main category: cs.LG

TL;DR: The paper shows that specific regularization terms in invertible neural network training can recover Bayesian point estimators like posterior mean and MAP estimator upon network inversion.


<details>
  <summary>Details</summary>
Motivation: Invertible networks are stable and interpretable for inverse problems, but existing optimization strategies have limitations from a Bayesian perspective. The authors aim to connect network training with classical Bayesian estimators.

Method: Introduce and analyze two regularization terms for invertible neural network training that, when the network is inverted, recover properties of Bayesian point estimators - one connected to posterior mean and another resembling MAP estimator.

Result: Theoretical analysis characterizes how each loss term shapes both the learned forward operator and its inverse reconstruction map. Numerical experiments demonstrate stable and interpretable data-dependence through these loss-term regularizers.

Conclusion: Regularization terms in invertible neural network training can effectively recover classical Bayesian point estimators, providing stable and interpretable data-dependent reconstruction methods.

Abstract: Can regularization terms in the training of invertible neural networks lead
to known Bayesian point estimators in reconstruction? Invertible networks are
attractive for inverse problems due to their inherent stability and
interpretability. Recently, optimization strategies for invertible neural
networks that approximate either a reconstruction map or the forward operator
have been studied from a Bayesian perspective, but each has limitations. To
address this, we introduce and analyze two regularization terms for the network
training that, upon inversion of the network, recover properties of classical
Bayesian point estimators: while the first can be connected to the posterior
mean, the second resembles the MAP estimator. Our theoretical analysis
characterizes how each loss shapes both the learned forward operator and its
inverse reconstruction map. Numerical experiments support our findings and
demonstrate how these loss-term regularizers introduce data-dependence in a
stable and interpretable way.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [43] [Cosmological Simulations of Weakly Collisional Plasmas with Braginskii Viscosity in Galaxy Clusters](https://arxiv.org/abs/2510.25847)
*Tirso Marin-Gilabert,Ulrich P. Steinwandel,Milena Valentini,John A. ZuHone,Klaus Dolag*

Main category: astro-ph.IM

TL;DR: Implementation of an anisotropic viscosity solver in OpenGadget3's MHD framework that models viscous transport along magnetic field lines with physical limiters based on plasma instability thresholds.


<details>
  <summary>Details</summary>
Motivation: To accurately model anisotropic viscous transport in magnetized astrophysical plasmas, particularly along magnetic field lines, which is crucial for understanding plasma behavior in realistic astrophysical environments.

Method: Implemented Braginskii formulation of anisotropic viscosity with mirror and firehose instability limiters in OpenGadget3's TreeSPH code, integrated with individual adaptive timestepping without subcycling.

Result: Excellent agreement with AREPO implementation in standard test problems; successfully applied to cosmological galaxy cluster simulation, demonstrating capability for realistic large-scale modeling.

Conclusion: The implementation provides a versatile and computationally efficient tool for studying anisotropic viscosity in magnetized astrophysical systems with physical consistency and stability.

Abstract: We present the implementation of an anisotropic viscosity solver within the
magnetohydrodynamics (MHD) framework of the TreeSPH code OpenGadget3. The
solver models anisotropic viscous transport along magnetic field lines
following the Braginskii formulation and includes physically motivated limiters
based on the mirror and firehose instability thresholds, which constrain the
viscous stress in weakly collisional plasmas. To validate the implementation,
we performed a suite of standard test problems -- including two variants of the
sound-wave test, circularly and linearly polarized Alfven waves, fast
magnetosonic wave, and the Kelvin-Helmholtz instability -- both with and
without the plasma-instability limiters. The results show excellent agreement
with the AREPO implementation of a similar anisotropic viscosity model (Berlok
et al. 2019), confirming the accuracy and robustness of our method. Our
formulation integrates seamlessly within the individual adaptive timestepping
framework of OpenGadget3, avoiding the need for subcycling. This provides
efficient and stable time integration while maintaining physical consistency.
Finally, we applied the new solver to a cosmological zoom-in simulation of a
galaxy cluster, demonstrating its capability to model anisotropic transport and
plasma microphysics in realistic large-scale environments. Our implementation
offers a versatile and computationally efficient tool for studying anisotropic
viscosity in magnetized astrophysical systems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [44] [Temperature dependent ferroelectricity in strained KTaO3 with machine learned force field](https://arxiv.org/abs/2510.26693)
*Yu Zhu,Luigi Ranalli,Taikang Chen,Wei Ren,Cesare Franchini*

Main category: cond-mat.mtrl-sci

TL;DR: Study investigates how 0-1% uniaxial and biaxial strain induces ferroelectricity in KTaO3 by breaking inversion symmetry, using advanced computational methods to analyze polarization across temperatures up to 300K.


<details>
  <summary>Details</summary>
Motivation: To understand how strain engineering can induce ferroelectric order in incipient ferroelectrics like KTaO3, which has fundamental and applied importance for tuning ferroelectric properties.

Method: Combined density functional theory with stochastic self-consistent harmonic approximation and machine learned force fields to incorporate anharmonic effects, using Berry phase method to calculate polarization under varying strain conditions.

Result: Strain (0-1%) successfully induces ferroelectric polarization in KTaO3 by breaking inversion symmetry, with polarization calculated across temperature range up to 300K.

Conclusion: Strain engineering is an effective approach for stabilizing ferroelectricity in KTaO3, providing valuable insights for future experimental and theoretical work on strain-engineered ferroelectric materials.

Abstract: Ferroelectric materials are a class of dielectrics that exhibit spontaneous
polarization which can be reversed under an external electric field. The
emergence of ferroelectric order in incipient ferroelectrics is a topic of
considerable interest from both fundamental and applied perspectives. Among the
various strategies explored, strain engineering has been proven to be a
powerful method for tuning ferroelectric polarization in materials. In the case
of KTaO3, first principles calculations have suggested that strain can drive a
ferroelectric phase transition. In this study, we investigate the impact of
in-plane uniaxial and biaxial strain, ranging from 0% to 1%, on pristine KTaO3
to explore its potential for ferroelectricity induction via inversion symmetry
breaking. By integrating density functional theory calculations with the
stochastic self-consistent harmonic approximation assisted by on the fly
machine learned force field, we obtain accurate structural information and
dynamical properties under varying strain conditions while incorporating
higher-order anharmonic effects. Employing the Berry phase method, we obtained
the ferroelectric polarization of the strained structures over the entire
temperature range up to 300 K. Our findings provide valuable insights into the
role of strain in stabilizing ferroelectricity in KTaO3, offering guidance for
future experimental and theoretical studies on strain-engineered ferroelectric
materials.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [45] [Quantum Stochastic Gradient Descent in its continuous-time limit based on the Wigner formulation of Open Quantum Systems](https://arxiv.org/abs/2510.25910)
*Jose A. Morales Escalante*

Main category: quant-ph

TL;DR: The paper proposes using the Wigner formulation to bridge classical and quantum probabilistic algorithms, specifically focusing on developing a quantum analog of Stochastic Gradient Descent in continuous-time limit for open quantum systems.


<details>
  <summary>Details</summary>
Motivation: To create a bridge between classical and quantum probabilistic algorithms by leveraging the Wigner formulation, enabling the development of quantum counterparts to classical optimization methods like Stochastic Gradient Descent.

Method: Using the Wigner formulation of Open Quantum Systems to develop a quantum analog of Stochastic Gradient Descent, specifically in its continuous-time limit.

Result: The research plan outlines the main ideas for implementing this quantum-classical bridge through the Wigner formulation approach.

Conclusion: The Wigner formulation provides a promising framework for connecting classical and quantum probabilistic algorithms, particularly for developing quantum versions of classical optimization techniques like Stochastic Gradient Descent.

Abstract: The main ideas behind a research plan to use the Wigner formulation as a
bridge between classical and quantum probabilistic algorithms are presented,
focusing on a particular case: the Quantum analog of Stochastic Gradient
Descent in its continuous-time limit based on the Wigner formulation of Open
Quantum Systems.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [46] [Quantitative Lorentzian isoperimetric inequalities](https://arxiv.org/abs/2510.26755)
*Christian Lange,Jonas W. Peteranderl*

Main category: math.DG

TL;DR: The paper establishes optimal stability estimates for Lorentzian isoperimetric inequalities with universal dimensional constants, focusing on inequalities by Bahn-Ehrlich and Cavalletti-Mondino, showing different dependencies on Fraenkel asymmetry.


<details>
  <summary>Details</summary>
Motivation: To provide stability estimates for Lorentzian isoperimetric inequalities, which are important in geometric analysis and relativity, with precise quantitative bounds on how much sets deviate from optimal shapes.

Method: The authors use geometric analysis techniques to derive stability estimates in terms of Fraenkel asymmetry, providing self-contained proofs for the isoperimetric inequalities and refining them through additional geometric terms.

Result: Found that Bahn-Ehrlich inequality has quadratic dependence on Fraenkel asymmetry (like Euclidean case), while Cavalletti-Mondino inequality has linear dependence, but can be refined to quadratic stability by adding geometric terms.

Conclusion: The paper successfully establishes optimal stability estimates for Lorentzian isoperimetric inequalities with universal constants, showing different stability behaviors that can be unified through geometric refinements.

Abstract: We establish optimal stability estimates in terms of the Fraenkel asymmetry
with universal dimensional constants for a Lorentzian isoperimetric inequality
due to Bahn and Ehrlich and, as a consequence, for a special version of a
Lorentzian isoperimetric inequality due to Cavalletti and Mondino. For the
Bahn--Ehrlich inequality the Fraenkel asymmetry enters the stability result
quadratically like in the Euclidean case while for the Cavalletti--Mondino
inequality the Fraenkel asymmetry enters linearly. As it turns out, refining
the latter inequality through an additional geometric term allows us to recover
the more common quadratic stability behavior. Along the way, we provide simple
self-contained proofs for the above isoperimetric-type inequalities.

</details>
