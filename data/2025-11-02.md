<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 9]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [math-ph](#math-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A Hybrid Reconstruction Framework for Efficient High-Order Shock-Capturing on Unstructured Meshes](https://arxiv.org/abs/2510.25906)
*Yiren Tong,Panagiotis Tsoutsanis*

Main category: math.NA

TL;DR: A hybrid reconstruction framework for compressible flows that combines linear efficiency with nonlinear robustness using a priori detection to minimize costly reconstructions while maintaining accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: To advance high-resolution schemes by balancing computational efficiency with robustness, reducing the need for expensive nonlinear reconstructions while preserving accuracy in compressible flow simulations.

Method: Combines CWENOZ and MOOD paradigms with a novel Numerical Admissibility Detector that classifies flow regions into smooth, weakly non-smooth, and discontinuous zones, then applies optimal reconstruction methods for each: high-order linear for smooth areas, CWENOZ for weakly non-smooth, and MUSCL for discontinuities.

Result: Achieves up to 2.5x speed-up over pure CWENOZ schemes in 3D compressible turbulence while maintaining designed accuracy in smooth regions and improving robustness in shock-dominated flows.

Conclusion: The hybrid approach successfully balances efficiency, robustness, and reliability, making high-order accuracy more feasible for industrial-scale CFD applications through targeted reconstruction allocation.

Abstract: We present a multi-dimensional, arbitrary-order hybrid reconstruction
framework for compressible flows on unstructured meshes. The method advances
high-resolution schemes by combining the efficiency of linear reconstruction
with the robustness of nonlinear formulations, activated only when needed
through a novel a priori detection strategy. This minimizes the use of costly
Compact Weighted Essentially Non-Oscillatory (CWENOZ) or Monotonic
Upstream-centered Scheme for Conservation Laws (MUSCL) reconstructions,
reducing computational cost without compromising accuracy or stability. The
framework merges CWENOZ and the Multi-dimensional Optimal Order Detection
(MOOD) paradigm while introducing a redesigned Numerical Admissibility Detector
(NAD) that classifies the local flow into smooth, weakly non-smooth, and
discontinuous regions in a single step. Each region is then reconstructed using
an optimal method: a high-order linear scheme in smooth areas, CWENOZ in weakly
non-smooth zones, and a second-order MUSCL near discontinuities. This targeted
a priori allocation preserves high-order accuracy where possible and ensures
stable, non-oscillatory behavior near shocks and steep gradients. Implemented
within the open-source unstructured finite-volume solver UCNS3D, the framework
supports arbitrary-order reconstructions on mixed-element meshes. Extensive
two- and three-dimensional benchmarks confirm that it retains the designed
accuracy in smooth regions while greatly improving robustness in
shock-dominated flows. Thanks to the reduced frequency of nonlinear
reconstructions, the method achieves up to 2.5x speed-up over a CWENOZ scheme
of equal order in 3D compressible turbulence. This hybrid approach thus brings
high-order accuracy closer to industrial-scale CFD through its balance of
efficiency, robustness, and reliability.

</details>


### [2] [A fast spectral overlapping domain decomposition method with discretization-independent conditioning bounds](https://arxiv.org/abs/2510.25991)
*Simon Dirckx,Anna Yesypenko,Per-Gunnar Martinsson*

Main category: math.NA

TL;DR: A domain decomposition method for solving variable-coefficient elliptic PDEs using overlapping thin slabs/shells, exploiting rank-structure for efficient handling of dense blocks in the reduced linear system.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient solver for general variable-coefficient elliptic PDEs on regular domains that can handle large-scale problems with millions of degrees of freedom.

Method: Tessellating domain into overlapping thin slabs/shells, forming reduced linear system connecting domains, exploiting H-matrix structure for dense blocks, using black-box randomized compression, and leveraging sparse direct solvers on thin sub-domains.

Result: The solver can handle oscillatory 2D and 3D problems with up to 28 million degrees of freedom, with improved data-sparsity and faster H-matrix arithmetic compared to existing formulations.

Conclusion: The proposed domain decomposition method provides a well-conditioned, efficient approach for solving large-scale variable-coefficient elliptic PDEs, with demonstrated scalability to millions of degrees of freedom.

Abstract: A domain decomposition method for the solution of general
variable-coefficient elliptic partial differential equations on regular domains
is introduced. The method is based on tessellating the domain into overlapping
thin slabs or shells, and then explicitly forming a reduced linear system that
connects the different domains. Rank-structure ('H-matrix structure') is
exploited to handle the large dense blocks that arise in the reduced linear
system. Importantly, the formulation used is well-conditioned, as it converges
to a second kind Fredholm equation as the precision in the local solves is
refined. Moreover, the dense blocks that arise are far more data-sparse than in
existing formulations, leading to faster and more efficient H-matrix
arithmetic. To form the reduced linear system, black-box randomized compression
is used, taking full advantage of the fact that sparse direct solvers are
highly efficient on the thin sub-domains. Numerical experiments demonstrate
that our solver can handle oscillatory 2D and 3D problems with as many as 28
million degrees of freedom.

</details>


### [3] [A two-dimensional fractional-order element-free Galerkin method for nonlocal elasticity and complex domain problems](https://arxiv.org/abs/2510.26161)
*Shubham Desai,Malapeta Hemasundara Rao,Sai Sidhardh*

Main category: math.NA

TL;DR: A 2D meshfree fractional-order Element-Free Galerkin method is developed as an alternative to FEM for solving fractional differential equations, handling complex domains better than mesh-based methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of 1D f-EFG solvers and demonstrate the true potential of meshfree approaches for fractional-order differential equations in complex 2D geometries.

Method: Uses 2D Moving Least Squares approximants within the Element-Free Galerkin framework to handle fractional derivatives from nodal values, avoiding mesh generation.

Result: Successfully validated against benchmark results, showing effectiveness for both linear and nonlinear fractional-order PDEs in complex domains like square and circular plates.

Conclusion: The 2D f-EFG method is a viable alternative to FEM for fractional-order problems, with potential applications in multiscale modeling, multiphysics coupling, and complex material behavior.

Abstract: This study presents a meshfree two-dimensional fractional-order Element-Free
Galerkin (2D f-EFG) method as a viable alternative to conventional mesh-based
FEM for a numerical solution of (spatial) fractional-order differential
equations (FDEs). The previously developed one-dimensional f-EFG solver offers
a limited demonstration of the true efficacy of EFG formulations for FDEs, as
it is restricted to simple 1D line geometries. In contrast, the 2D f-EFG solver
proposed and developed here effectively demonstrates the potential of meshfree
approaches for solving FDEs. The proposed solver can handle complex and
irregular 2D domains that are challenging for mesh-based methods. As an
example, the developed framework is employed to investigate nonlocal elasticity
governed by fractional-order constitutive relations in a square and circular
plate. Furthermore, the proposed approach mitigates key drawbacks of FEM,
including high computational cost, mesh generation, and reduced accuracy in
irregular domains. The 2D f-EFG employs 2D Moving Least Squares (MLS)
approximants, which are particularly effective in approximating fractional
derivatives from nodal values. The 2D f-EFG solver is employed here for the
numerical solution of fractional-order linear and nonlinear partial
differential equations corresponding to the nonlocal elastic response of a
plate. The solver developed here is validated with the benchmark results
available in the literature. While the example chosen here focuses on nonlocal
elasticity, the numerical method can be extended for diverse applications of
fractional-order derivatives in multiscale modeling, multiphysics coupling,
anomalous diffusion, and complex material behavior.

</details>


### [4] [Accelerated decomposition of bistochastic kernel matrices by low rank approximation](https://arxiv.org/abs/2510.26574)
*Chris Vales,Dimitrios Giannakis*

Main category: math.NA

TL;DR: An accelerated algorithm for approximate eigenvalue decomposition of bistochastic normalized kernel matrices using low-rank approximations via pivoted partial Cholesky, avoiding full matrix formation.


<details>
  <summary>Details</summary>
Motivation: To efficiently compute eigenvalue decompositions of bistochastic normalized kernel matrices without forming the full matrix, reducing computational costs for large datasets.

Method: Constructs low-rank approximation using pivoted partial Cholesky algorithm, then computes approximate decomposition of bistochastic normalization from this approximation.

Result: Algorithm achieves linear cost dependence on dataset size and quadratic on approximation rank, significantly reducing computational cost compared to naive approaches.

Conclusion: The method provides accurate spatiotemporal pattern extraction from chaotic dynamics and outperforms subsampling with Nystroem extension in efficiency.

Abstract: We develop an accelerated algorithm for computing an approximate eigenvalue
decomposition of bistochastic normalized kernel matrices. Our approach
constructs a low rank approximation of the original kernel matrix by the
pivoted partial Cholesky algorithm and uses it to compute an approximate
decomposition of its bistochastic normalization without requiring the formation
of the full kernel matrix. The cost of the proposed algorithm depends linearly
on the size of the employed training dataset and quadratically on the rank of
the low rank approximation, offering a significant cost reduction compared to
the naive approach. We apply the proposed algorithm to the kernel based
extraction of spatiotemporal patterns from chaotic dynamics, demonstrating its
accuracy while also comparing it with an alternative algorithm consisting of
subsampling and Nystroem extension.

</details>


### [5] [A parallel solver for random input problems via Karhunen-Loève expansion and diagonalized coarse grid correction](https://arxiv.org/abs/2510.26180)
*Dou Dai,Qiuqi Li,Huailing Song*

Main category: math.NA

TL;DR: Proposes KLE-CGC, a hybrid parallel algorithm combining Karhunen-Loève expansion with coarse grid correction to improve computational efficiency of parallel-in-time methods for stochastic initial-value problems.


<details>
  <summary>Details</summary>
Motivation: Standard parareal algorithm suffers from slow convergence for stochastic problems due to poor initial guess quality, limiting computational efficiency.

Method: Uses KL expansion for low-dimensional parameterization of stochastic fields, constructs gPC spectral surrogate model for rapid solution prediction, and employs this as improved initial value for parareal iterations.

Result: Numerical experiments show KLE-CGC maintains same convergence order as original algorithm while significantly reducing iteration count and improving parallel scalability.

Conclusion: The proposed framework retains theoretical convergence rate of standard parareal algorithm while substantially enhancing computational efficiency for stochastic problems.

Abstract: This paper is dedicated to enhancing the computational efficiency of
traditional parallel-in-time methods for solving stochastic initial-value
problems. The standard parareal algorithm often suffers from slow convergence
when applied to problems with stochastic inputs, primarily due to the poor
quality of the initial guess. To address this issue, we propose a hybrid
parallel algorithm, termed KLE-CGC, which integrates the Karhunen-Lo\`{e}ve
(KL) expansion with the coarse grid correction (CGC). The method first employs
the KL expansion to achieve a low-dimensional parameterization of
high-dimensional stochastic parameter fields. Subsequently, a generalized
Polynomial Chaos (gPC) spectral surrogate model is constructed to enable rapid
prediction of the solution field. Utilizing this prediction as the initial
value significantly improves the initial accuracy for the parareal iterations.
A rigorous convergence analysis is provided, establishing that the proposed
framework retains the same theoretical convergence rate as the standard
parareal algorithm. Numerical experiments demonstrate that KLE-CGC maintains
the same convergence order as the original algorithm while substantially
reducing the number of iterations and improving parallel scalability.

</details>


### [6] [Efficient And Stable Third-order Method for Micromagnetics Simulations](https://arxiv.org/abs/2510.26181)
*Changjian Xie,Cheng Wang*

Main category: math.NA

TL;DR: A third-order accurate numerical scheme for solving the Landau-Lifshitz-Gilbert equation with large damping parameters, offering improved efficiency through linear systems with constant coefficients and unconditional stability.


<details>
  <summary>Details</summary>
Motivation: To address magnetization dynamics in ferromagnetic materials under large damping parameters, improving upon existing lower-order methods for better accuracy and efficiency.

Method: Builds on a second-order method, developing a third-order scheme that solves linear systems with constant coefficients using fast solvers, achieving third-order temporal and fourth-order spatial accuracy.

Result: Numerical tests in 1D and 3D confirm third-order accuracy and efficiency gains. Method shows unconditional stability for large damping and captures physically plausible structures. For domain wall dynamics, it accurately reproduces linear velocity relationships with damping and external field.

Conclusion: The proposed third-order method outperforms lower-order approaches in accuracy, efficiency, and stability for large damping parameters in ferromagnetic material simulations.

Abstract: To address the magnetization dynamics in ferromagnetic materials described by
the Landau-Lifshitz-Gilbert equation under large damping parameters, a
third-order accurate numerical scheme is developed by building upon a
second-order method \cite{CaiChenWangXie2022} and leveraging its efficiency.
This method boasts two key advantages: first, it only involves solving linear
systems with constant coefficients, enabling the use of fast solvers and thus
significantly enhancing numerical efficiency over existing first or
second-order approaches. Second, it achieves third-order temporal accuracy and
fourth-order spatial accuracy, while being unconditionally stable for large
damping parameters. Numerical tests in 1D and 3D scenarios confirm both its
third-order accuracy and efficiency gains. When large damping parameters are
present, the method demonstrates unconditional stability and reproduces
physically plausible structures. For domain wall dynamics simulations, it
captures the linear relationship between wall velocity and both the damping
parameter and external magnetic field, outperforming lower-order methods in
this regard.

</details>


### [7] [Transcending Sparse Measurement Limits: Operator-Learning-Driven Data Super-Resolution for Inverse Source Problem](https://arxiv.org/abs/2510.26227)
*Guanyu Pan,Jianing Zhou,Xiaotong Liu,Yunqing Huang,Nianyu Yi*

Main category: math.NA

TL;DR: A modular framework combining DeepONet interpolation with Direct Sampling Method significantly improves multi-source localization from extremely sparse single-frequency measurements in inverse source problems.


<details>
  <summary>Details</summary>
Motivation: Inverse source localization from Helmholtz boundary data over narrow apertures is highly ill-posed and severely undersampled, undermining classical solvers like the Direct Sampling Method.

Method: Three-step approach: 1) Extend uniqueness theorem for inverse source problem under limited viewing apertures; 2) Use DeepONet with branch-trunk architecture to interpolate sparse measurements (6-10 samples) to dense synthetic aperture; 3) Feed super-resolved field into Direct Sampling Method.

Result: For single source, sparse data alone achieves grid-level precision. In multi-source trials, DeepONet-reconstructed data reduce localization error by about an order of magnitude and remain effective with apertures as small as π/4.

Conclusion: The modular framework decouples interpolation from inversion, allowing flexible combination of neural operators and classical algorithms, providing practical design that improves localization accuracy over standard baselines.

Abstract: Inverse source localization from Helmholtz boundary data collected over a
narrow aperture is highly ill-posed and severely undersampled, undermining
classical solvers (e.g., the Direct Sampling Method). We present a modular
framework that significantly improves multi-source localization from extremely
sparse single-frequency measurements. First, we extend a uniqueness theorem for
the inverse source problem, proving that a unique solution is guaranteed under
limited viewing apertures. Second, we employ a Deep Operator Network (DeepONet)
with a branch-trunk architecture to interpolate the sparse measurements,
lifting six to ten samples within the narrow aperture to a sufficiently dense
synthetic aperture. Third, the super-resolved field is fed into the Direct
Sampling Method (DSM). For a single source, we derive an error estimate showing
that sparse data alone can achieve grid-level precision. In two- and
three-source trials, localization from raw sparse measurements is unreliable,
whereas DeepONet-reconstructed data reduce localization error by about an order
of magnitude and remain effective with apertures as small as $\pi/4$. By
decoupling interpolation from inversion, the framework allows the interpolation
and inversion modules to be swapped with neural operators and classical
algorithms, respectively, providing a practical and flexible design that
improves localization accuracy compared with standard baselines.

</details>


### [8] [Simulation of the magnetic Ginzburg-Landau equation via vortex tracking](https://arxiv.org/abs/2510.26334)
*Thiago Carvalho Corso,Gaspard Kemlin,Christof Melcher,Benjamin Stamm*

Main category: math.NA

TL;DR: A numerical method for simulating 2D magnetic time-dependent Ginzburg-Landau equations in the small epsilon regime, using a reduced ODE system for vortex dynamics to avoid resolving fine epsilon-scale features.


<details>
  <summary>Details</summary>
Motivation: To efficiently simulate TDGL equations with small Ginzburg-Landau parameter epsilon, where traditional methods require very fine meshes and time steps due to the appearance of quantized vortices with core size of order epsilon.

Method: Developed a numerical method based on the limiting ODE system for vortex dynamics in the singular limit epsilon→0, which requires solving a linear second order PDE at each time step. This avoids resolving the epsilon-scale in TDGL simulations.

Result: The method provides efficient simulation of TDGL equations for small but finite epsilon values, with numerical examples demonstrating accuracy and avoiding the need for extremely fine meshes.

Conclusion: The proposed numerical strategy successfully enables efficient simulation of TDGL equations in the presence of constant external magnetic fields for small epsilon values, with rigorous theoretical justification and numerical validation.

Abstract: This paper deals with the numerical simulation of the 2D magnetic
time-dependent Ginzburg-Landau (TDGL) equations in the regime of small but
finite (inverse) Ginzburg-Landau parameter $\epsilon$ and constant (order $1$
in $\epsilon$) applied magnetic field. In this regime, a well-known feature of
the TDGL equation is the appearance of quantized vortices with core size of
order $\epsilon$. Moreover, in the singular limit $\epsilon \searrow 0$, these
vortices evolve according to an explicit ODE system. In this work, we first
introduce a new numerical method for the numerical integration of this limiting
ODE system, which requires to solve a linear second order PDE at each time
step. We also provide a rigorous theoretical justification for this method that
applies to a general class of 2D domains. We then develop and analyze a
numerical strategy based on the finite-dimensional ODE system to efficiently
simulate the infinite-dimensional TDGL equations in the presence of a constant
external magnetic field and for small, but finite, $\epsilon$. This method
allows us to avoid resolving the $\epsilon$-scale when solving the TDGL
equations, where small values of $\epsilon$ typically require very fine meshes
and time steps. We provide numerical examples on a few test cases and justify
the accuracy of the method with numerical investigations.

</details>


### [9] [Incorporating Local Hölder Regularity into PINNs for Solving Elliptic PDEs](https://arxiv.org/abs/2510.26365)
*Qirui Zhou,Jiebao Sun,Yi Ran,Boying Wu*

Main category: math.NA

TL;DR: Incorporating local Hölder regularization into PINNs for elliptic PDEs improves accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Motivated by interior regularity properties of linear elliptic PDEs to enhance PINN performance.

Method: Modified loss function with local Hölder regularization term using variable-distance discrete sampling strategy.

Result: Numerical experiments show notable improvements in prediction accuracy and robustness over standard PINNs.

Conclusion: Local Hölder regularization effectively enhances PINN performance for elliptic PDEs with established error estimates.

Abstract: In this paper, local H\"older regularization is incorporated into a
physics-informed neural networks (PINNs) framework for solving elliptic partial
differential equations (PDEs). Motivated by the interior regularity properties
of linear elliptic PDEs, a modified loss function is constructed by introducing
local H\"older regularization term. To approximate this term effectively, a
variable-distance discrete sampling strategy is developed. Error estimates are
established to assess the generalization performance of the proposed method.
Numerical experiments on a range of elliptic problems demonstrate notable
improvements in both prediction accuracy and robustness compared to standard
physics-informed neural networks.

</details>


### [10] [Asymptotic meshes from $r$-variational adaptation methods for static problems in one dimension](https://arxiv.org/abs/2510.26375)
*Darith Hun,Nicolas Moës,Heiner Olbermann*

Main category: math.NA

TL;DR: The paper analyzes the minimization of integral functionals using r-adaptive finite elements, showing that optimal grid configurations converge to a well-defined limit as the number of nodes increases, proven via Γ-convergence.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of optimal grid configurations in finite element approximations when the number of nodes approaches infinity.

Method: Using r-adaptive finite elements and including the grid as a variable in minimization, the study employs Γ-convergence to analyze the limit of renormalized energy functionals.

Result: Optimal grid configurations converge to a well-defined limit as the number of nodes increases, and numerical examples confirm the closeness of the asymptotic mesh to finite optimal meshes.

Conclusion: The Γ-limit provides a reliable asymptotic description of optimal finite element grids, validated by numerical evidence.

Abstract: We consider the minimization of integral functionals in one dimension and
their approximation by $r$-adaptive finite elements. Including the grid of the
FEM approximation as a variable in the minimization, we are able to show that
the optimal grid configurations have a well-defined limit when the number of
nodes in the grid is being sent to infinity. This is done by showing that the
suitably renormalized energy functionals possess a limit in the sense of
$\Gamma$-convergence. We provide numerical examples showing the closeness of
the optimal asymptotic mesh obtained as a minimizer of the $\Gamma$-limit to
the optimal finite meshes.

</details>


### [11] [Explicit Consistency Error Estimate for Finite Element Solutions of the Poisson Equation on Convex Domains](https://arxiv.org/abs/2510.26404)
*Su Ruibo*

Main category: math.NA

TL;DR: Explicit a priori consistency error estimates for finite element discretization of Poisson equation on convex domains approximated by internal convex polyhedra.


<details>
  <summary>Details</summary>
Motivation: To provide explicit error estimates for finite element methods applied to Poisson equation on convex domains, which depend only on global geometric parameters and work for general convex domains and arbitrary simplicial meshes.

Method: Standard finite element discretization of Poisson equation on convex domains approximated by internal convex polyhedra, with derivation of explicit a priori consistency error estimates.

Result: Obtained explicit error estimates that depend only on global geometric parameters and are applicable to general convex domains and arbitrary families of simplicial meshes.

Conclusion: The paper successfully derives explicit a priori consistency error estimates for finite element discretization of Poisson equation on convex polyhedral approximations, providing generalizable results dependent only on geometric parameters.

Abstract: We derive explicit a priori consistency error estimates for a standard finite
element discretization of the Poisson equation on convex domains, where the
domain is approximated by an internal convex polyhedron. The obtained explicit
estimates depend only on global geometric parameters and are applicable to
general convex domains and arbitrary families of simplicial meshes.

</details>


### [12] [The evolving surface morphochemical reaction-diffusion system for battery modeling](https://arxiv.org/abs/2510.26437)
*Benedetto Bozzini,Massimo Frittelli,Anotida Madzvamuse,Ivonne Sgura*

Main category: math.NA

TL;DR: The ESDIB model is a reaction-diffusion system on evolving electrode surfaces that couples surface evolution with electrochemical species concentration to predict metal deposition morphologies in batteries.


<details>
  <summary>Details</summary>
Motivation: Electrodeposition produces films with uncontrollable morphology, causing issues in electrochemical technologies, particularly in next-generation batteries with metal anodes that fail practical cyclability due to uncontrolled electrode shape evolution.

Method: Proposed ESDIB model - a reaction-diffusion system on dynamically evolving electrode surfaces, numerically solved using extended Lumped Evolving Surface Finite Element Method (LESFEM) for spatial discretization and IMEX Euler scheme for time integration.

Result: Validated through six numerical experiments compared with laboratory images, demonstrating accurate capture of branching and dendritic growth patterns in electrodeposition.

Conclusion: The ESDIB framework provides a predictive and physically consistent tool for studying metal deposition phenomena in energy storage devices, enabling better understanding of electrode shape evolution.

Abstract: It is well known that phase formation by electrodeposition yields films of
poorly controllable morphology. This typically leads to a range of
technological issues in many fields of electrochemical technology. Presently, a
particularly relevant case is that of high-energy density next-generation
batteries with metal anodes, that cannot yet reach practical cyclability
targets, owing to uncontrolled elelctrode shape evolution. In this scenario,
mathematical modelling is a key tool to lay the knowledge-base for
materials-science advancements liable to lead to concretely stable battery
material architectures. In this work, we introduce the Evolving Surface DIB
(ESDIB) model, a reaction-diffusion system posed on a dynamically evolving
electrode surface. Unlike previous fixed-surface formulations, the ESDIB model
couples surface evolution to the local concentration of electrochemical
species, allowing the geometry of the electrode itself to adapt in response to
deposition. To handle the challenges related to the coupling between surface
motion and species transport, we numerically solve the system by proposing an
extension of the Lumped Evolving Surface Finite Element Method (LESFEM) for
spatial discretisation, combined with an IMEX Euler scheme for time
integration. The model is validated through six numerical experiments, each
compared with laboratory images of electrodeposition. Results demonstrate that
the ESDIB framework accurately captures branching and dendritic growth,
providing a predictive and physically consistent tool for studying metal
deposition phenomena in energy storage devices.

</details>


### [13] [A GenEO-type coarse space with smaller eigenproblems](https://arxiv.org/abs/2510.26548)
*Peter Bastian,Nils Friess*

Main category: math.NA

TL;DR: A new GenEO coarse space variant that solves eigenproblems only in boundary strips, reducing setup costs while maintaining coefficient robustness.


<details>
  <summary>Details</summary>
Motivation: Traditional adaptive coarse spaces require solving expensive local eigenproblems across entire subdomains, leading to high setup costs that may exceed iteration costs.

Method: Modified GenEO coarse space that computes eigenproblems only in boundary-connected strips of subdomains instead of entire subdomains.

Result: Significant reduction in setup cost while maintaining similar coefficient-robust condition number estimates as the original method.

Conclusion: The boundary-strip approach provides an efficient alternative to full-domain eigenproblem solving, trading potentially larger coarse space size for substantially reduced computational setup costs.

Abstract: Coarse spaces are essential to ensure robustness w.r.t. the number of
subdomains in two-level overlapping Schwarz methods. Robustness with respect to
the coefficients of the underlying partial differential equation (PDE) can be
achieved by adaptive (or spectral) coarse spaces involving the solution of
local eigenproblems. The solution of these eigenproblems, although scalable,
entails a large setup cost which may exceed the cost for the iteration phase.
In this paper we present and analyse a new variant of the GenEO (Generalised
Eigenproblems in the Overlap) coarse space which involves solving eigenproblems
only in a strip connected to the boundary of the subdomain. This leads to a
significant reduction of the setup cost while the method satisfies a similar
coefficient-robust condition number estimate as the original method, albeit
with a possibly larger coarse space.

</details>


### [14] [Fast tensor-based electrostatic energy calculations in the perspective of protein-ligand docking problem](https://arxiv.org/abs/2510.26611)
*Peter Benner,Boris N. Khoromskij,Venera Khoromskaia,Matthias Stein*

Main category: math.NA

TL;DR: Fast electrostatic interaction energy calculation for protein-ligand docking using low-rank tensor-based representation with O(n) complexity.


<details>
  <summary>Details</summary>
Motivation: Need for efficient calculation of electrostatic interaction energy in rigid protein-ligand docking, especially for large biomolecular complexes.

Method: Uses low-rank range-separated tensor-based representation of electrostatic potential on large 3D grids, enabling O(n)-complexity calculations with logarithmic dependence on particle count.

Result: Demonstrated proof of concept with synthetic and realistic data, showing capability to handle complex particle configurations with large 3D grids (up to ~10^12 grid points).

Conclusion: Tensor-based approach enables efficient electrostatic energy calculation for protein-ligand docking and can be integrated with traditional posing/docking techniques.

Abstract: We propose and justify a new approach for fast calculation of the
electrostatic interaction energy of clusters of charged particles in
constrained energy minimization in the framework of rigid protein-ligand
docking. Our ``blind search'' docking technique is based on the low-rank
range-separated (RS) tensor-based representation of the free-space
electrostatic potential of the biomolecule represented on large $n\times
n\times n$ 3D grid. We show that both the collective electrostatic potential of
a complex protein-ligand system and the respective electrostatic interaction
energy can be calculated by tensor techniques in $O(n)$-complexity, such that
the numerical cost for energy calculation only mildly (logarithmically) depends
on the number of particles in the system. Moreover, tensor representation of
the electrostatic potential enables usage of large 3D Cartesian grids (of the
order of $n^3 \sim 10^{12}$), which could allow the accurate modeling of
complexes with several large proteins. In our approach selection of the correct
geometric pose predictions in the localized posing process is based on the
control of van der Waals distance between the target molecular clusters. Here,
we confine ourselves by constrained minimization of the energy functional by
using only fast tensor-based free-space electrostatic energy recalculation for
various rotations and translations of both clusters. Numerical tests of the
electrostatic energy-based ``protein-ligand docking'' algorithm applied to
synthetic and realistic input data present a proof of concept for rather
complex particle configurations. The method may be used in the framework of the
traditional stochastic or deterministic posing/docking techniques.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Solutions to Second-Order Nonlocal Evolution Equations Governed by Non-Autonomous Forms](https://arxiv.org/abs/2510.25881)
*Sajid Ullah,Vittorio Colao*

Main category: math.AP

TL;DR: Existence conditions and analysis for second order problems with nonzero nonlocal initial conditions using fundamental solutions and fixed-point methods.


<details>
  <summary>Details</summary>
Motivation: To establish sufficient conditions for solution existence in second order problems with nonzero nonlocal initial conditions, which are relevant in physical applications like vibrating viscoelastic membranes.

Method: Using fundamental solutions and fixed-point techniques to analyze the problem theoretically.

Result: Proved sufficient conditions for solution existence and provided comprehensive analysis.

Conclusion: Theoretical results are applicable to partial differential equations modeling physical systems like vibrating viscoelastic membranes with time-dependent properties and nonlocal memory effects.

Abstract: Our main contributions include proving sufficient conditions for the
existence of solution to a second order problem with nonzero nonlocal initial
conditions, and providing a comprehensive analysis using fundamental solutions
and fixed-point techniques. The theoretical results are illustrated through
applications to partial differential equations, including vibrating
viscoelastic membranes with time-dependent material properties and nonlocal
memory effects.

</details>


### [16] [Bochner-Riesz means on a conical singular manifold](https://arxiv.org/abs/2510.26059)
*Qiuye Jia,Junyong Zhang,Jiqiang Zheng*

Main category: math.AP

TL;DR: Sharp L^p-boundedness criterion for Bochner-Riesz multipliers on flat cones is established, with the operator bounded if and only if δ > δ_c(p,2) = max{0, 2|1/2 - 1/p| - 1/2} for 1 ≤ p ≤ ∞, p ≠ 2.


<details>
  <summary>Details</summary>
Motivation: To determine the precise conditions under which Bochner-Riesz multipliers are bounded on L^p spaces for flat cones and infinite sector domains, resolving the critical exponent problem in wedge settings.

Method: Mathematical analysis and proof techniques to establish sharp boundedness criteria for Bochner-Riesz multipliers S_λ^δ(Δ_X) on flat cones X = (0,∞) × S_σ^1.

Result: The operator S_λ^δ(Δ_X) is bounded on L^p(X) for 1 ≤ p ≤ ∞, p ≠ 2, if and only if δ > δ_c(p,2) = max{0, 2|1/2 - 1/p| - 1/2}. This criterion also applies to infinite sector domains with Dirichlet or Neumann boundary.

Conclusion: A complete characterization of L^p-boundedness for Bochner-Riesz multipliers on flat cones is provided, solving the critical exponent problem in wedge geometry settings.

Abstract: We prove a sharp $L^p$-boundedness criterion for Bochner-Riesz multipliers on
flat cones $X = (0,\infty) \times \mathbb{S}_\sigma^1$. The operator
$S_\lambda^\delta(\Delta_X)$ is bounded on $L^p(X)$ for $1 \leq p \leq \infty$,
$p \neq 2$, if and only if $\delta > \delta_c(p,2) = \max\left\{ 0, 2\left| 1/2
- 1/p \right| - 1/2 \right\}$. This result is also applicable to the infinite
sector domain with Dirichlet or Neumann boundary, resolving the critical
exponent problem in this wedge setting.

</details>


### [17] [A one-dimensional Stefan problem for the heat equation with a nonlinear boundary condition](https://arxiv.org/abs/2510.26088)
*Kensho Araya,Kazuhiro Ishige*

Main category: math.AP

TL;DR: The paper classifies solutions to the 1D one-phase Stefan problem with nonlinear boundary conditions into three types based on initial function size: global solutions with exponential decay, global solutions with non-exponential decay, and finite-time blow-up solutions.


<details>
  <summary>Details</summary>
Motivation: To understand the complete classification of solution behaviors for the Stefan problem with nonlinear boundary conditions, particularly how initial conditions determine long-term behavior.

Method: Analysis of the one-dimensional one-phase Stefan problem for the heat equation with nonlinear boundary conditions, examining solution behavior based on initial function size.

Result: All solutions fall into three distinct types: global solutions with exponential decay, global solutions with non-exponential decay, and finite-time blow-up solutions. The behavior at blow-up time is also characterized.

Conclusion: The size of the initial function completely determines the solution type, providing a comprehensive classification of solution behaviors for this Stefan problem with nonlinear boundary conditions.

Abstract: We study the one-dimensional one-phase Stefan problem for the heat equation
with a nonlinear boundary condition. We show that all solutions fall into one
of three distinct types: global-in-time solutions with exponential decay,
global-in-time solutions with non-exponential decay, and finite-time blow-up
solutions. The classification depends on the size of the initial function.
Furthermore, we describe the behavior of solutions at the blow-up time.

</details>


### [18] [Time-periodic boundary effects on the shocks for scalar conservation laws](https://arxiv.org/abs/2510.26153)
*Yuan Yuan*

Main category: math.AP

TL;DR: The paper analyzes asymptotic stabilities of inviscid and viscous shocks for scalar conservation laws on half-line with shock speed s<0 under time-periodic boundary conditions, showing solutions converge to shifted background shocks coupled with boundary-induced periodic solutions.


<details>
  <summary>Details</summary>
Motivation: To understand how time-periodic boundary conditions affect long-time behaviors of Riemann solutions in scalar conservation laws, particularly arising from classical piston problems in fluid mechanics, where this relationship was previously unclear.

Method: Rigorous mathematical analysis proving asymptotic behaviors for both inviscid and viscous cases under time-periodic boundary conditions, examining the coupling between shifted background shocks and boundary-induced periodic solutions.

Result: Proved that asymptotic states are governed by shifted background shocks coupled with time-periodic boundary solutions, revealing a propagating 'boundary wave' that influences shock dynamics.

Conclusion: Time-periodic boundary conditions significantly affect shock dynamics by introducing boundary waves that couple with background shocks, providing complete characterization of asymptotic behaviors in both inviscid and viscous cases.

Abstract: This paper is concerned with the asymptotic stabilities of the inviscid and
viscous shocks for the scalar conservation laws on the half-line $(-\infty,0)$
with shock speed $s<0$, subjected to the time-periodic boundary condition,
which arises from the classical piston problems for fluid mechanics. Despite
the importance, how time-periodic boundary conditions affect the long-time
behaviors of Riemann solutions has remained unclear. This work addresses this
gap by rigorously proving that in both inviscid and viscous case, the
asymptotic states of the solutions under the time-periodic boundary conditions
are not only governed by the shifted background (viscous) shocks, but also
coupled with the time-periodic boundary solution induced by the time-periodic
boundary. Our analysis reveals that these effects manifest as a propagating
"boundary wave", which influences the shock dynamics.

</details>


### [19] [Sharp embeddings and existence results for Logarithmic $p$-Laplacian equations with critical growth](https://arxiv.org/abs/2510.26286)
*Rakesh Arora,Jacques Giacomoni,Hichem Hajaiej,Arshi Vaishnavi*

Main category: math.AP

TL;DR: The paper establishes a new p-Logarithmic Sobolev inequality, studies embeddings into Orlicz-type spaces, analyzes Dirichlet problems with logarithmic p-Laplacian and critical growth, and conducts asymptotic analysis of fractional p-Laplacian problems.


<details>
  <summary>Details</summary>
Motivation: To extend previous results on logarithmic Sobolev inequalities and variational problems to a broader nonlinear framework involving the logarithmic p-Laplacian operator.

Method: Derivation of p-Logarithmic Sobolev inequality, analysis of embeddings into Orlicz spaces, application of Nehari manifold method for existence proofs, and asymptotic analysis of fractional p-Laplacian problems.

Result: Proved existence of nontrivial weak solutions for Dirichlet problems with critical growth, established optimal embeddings, and demonstrated convergence of solutions to Brezis-Nirenberg and logistic-type problems as fractional parameter approaches zero.

Conclusion: The work provides a nonlinear extension of previous results, establishing a comprehensive variational framework for problems involving the logarithmic p-Laplacian with applications to critical growth nonlinearities and asymptotic behavior analysis.

Abstract: In this paper, we derive a new $p$-Logarithmic Sobolev inequality and optimal
continuous and compact embeddings into Orlicz-type spaces of the function space
associated with the logarithmic $p$-Laplacian. As an application of these
results, we study a class of Dirichlet boundary value problems involving the
logarithmic $p$-Laplacian and critical growth nonlinearities perturbed with
superlinear-subcritical growth terms. By employing the method of the Nehari
manifold, we prove the existence of a nontrivial weak solution.
  Lastly, we conduct an asymptotic analysis of a weighted nonlocal, nonlinear
problem governed by the fractional $p$-Laplacian with superlinear or sublinear
type non-linearity, demonstrating the convergence of least energy solutions to
a non-trivial, non-negative least energy solution of a Brezis-Nirenberg type or
logistic-type problem, respectively, involving the logarithmic $p$-Laplacian as
the fractional parameter $s \to 0^+$.
  The findings in this work serve as a nonlinear analogue of the results
reported in \cite{Angeles-Saldana, Arora-Giacomoni-Vaishnavi,
Santamaria-Saldana}, thereby extending their scope to a broader variational
framework.

</details>


### [20] [Coupling local and nonlocal total variation flow for image despeckling](https://arxiv.org/abs/2510.26296)
*Yi Ran,Zhichang Guo,Kehan Shi,Qirui Zhou,Jingfeng Shao,Martin Burger,Boying Wu*

Main category: math.AP

TL;DR: This paper proposes a coupled local-nonlocal total variation flow for image despeckling that combines the texture-preserving benefits of nonlocal equations with the strong denoising capabilities of local equations.


<details>
  <summary>Details</summary>
Motivation: Nonlocal equations preserve textures well but have weak regularization, while local equations offer strong denoising but fail to protect textures. The goal is to integrate the advantages of both approaches.

Method: A coupled local-nonlocal total variation flow is proposed for image despeckling. The paper establishes existence and uniqueness of weak solutions, analyzes equivalent forms and asymptotic behavior, and shows convergence to classical total variation flow under kernel rescaling.

Result: The proposed coupled model successfully integrates the benefits of both local and nonlocal approaches. Mathematical analysis confirms the existence, uniqueness, and convergence properties of the weak solutions.

Conclusion: The coupling of local and nonlocal total variation flows is important for effective image despeckling, as demonstrated through comparisons with standalone local and nonlocal models.

Abstract: Nonlocal equations effectively preserve textures but exhibit weak
regularization effects in image denoising, whereas local equations offer strong
denoising capabilities yet fail to protect textures. To integrate the
advantages of both approaches, this paper investigates a coupled local-nonlocal
total variation flow for image despeckling. We establish the existence and
uniqueness of the weak solution for the proposed equation. Several properties,
including the equivalent forms of the weak solution and its asymptotic
behavior, are derived. Furthermore, we demonstrate that the weak solutions of
the proposed equation converge to the weak solution of the classical total
variation flow under kernel rescaling. The importance of coupling is
highlighted through comparisons with local and nonlocal models for image
despeckling.

</details>


### [21] [Complete spectrum of the Robin eigenvalue problem on the ball](https://arxiv.org/abs/2510.26331)
*Cancan Chen,Guowei Dai,Yingxin Sun*

Main category: math.AP

TL;DR: Complete spectral analysis of Robin eigenvalue problem on unit ball, showing explicit formulas for first and second eigenvalues depending on parameter α, with ratio μ₂/μ₁ varying from positive to negative to zero.


<details>
  <summary>Details</summary>
Motivation: To fully characterize the spectral structure of Robin eigenvalue problems on the unit ball, particularly understanding how the eigenvalues depend on the boundary parameter α.

Method: Mathematical analysis using Bessel functions and their zeros, examining different ranges of the parameter α (positive, negative between -1 and 0, and exactly -1).

Result: Found explicit formulas: for α>0, first eigenvalue is k²_{ν,1} and second is k²_{ν+1,1}; for α∈(-1,0), first is -k̂²_{ν,1} and second is k²_{ν+1,1}; for α=-1, first is -k̂²_{ν,1} and second is 0.

Conclusion: The ratio μ₂/μ₁ can be positive, negative, or zero depending on α, providing complete spectral characterization of the Robin eigenvalue problem on the unit ball.

Abstract: We investigate the following Robin eigenvalue problem \begin{equation*}
\left\{ \begin{array}{ll} -\Delta u=\mu u\,\, &\text{in}\,\, B,\\
\partial_\texttt{n} u+\alpha u=0 &\text{on}\,\, \partial B \end{array} \right.
\end{equation*} on the unit ball of $\mathbb{R}^N$. We obtain the complete
spectral structure of this problem. In particular, for $\alpha>0$, we find that
the first eigenvalue is $k_{\nu,1}^2$ and the second eigenvalue is exactly
$k_{\nu+1,1}^2$, where $k_{\nu+l,m}$ is the $m$th positive zero of
$kJ_{\nu+l+1}(k)-(\alpha+l) J_{\nu+l}(k)$. Moreover, when $\alpha\in(-1,0)$,
the first eigenvalue is $-\widehat{k}_{\nu,1}^2$ where $\widehat{k}_{\nu,1}$
denotes the unique zero of $\alpha I_{\nu}(k)+kI_{\nu+1}(k)$, and the second
eigenvalue is exactly $k_{\nu+1,1}^2$. Furthermore, for $\alpha=-1$, the first
eigenvalue is $-\widehat{k}_{\nu,1}^2$ and the second eigenvalue is exactly
$0$. Our conclusions indicate the ratio $\mu_2/\mu_1$ may be positive, negative
or zero according to the suitable ranges of the parameter $\alpha$.

</details>


### [22] [Tangential approach in the Dirichlet problem for elliptic equations](https://arxiv.org/abs/2510.26400)
*Jonathan Bennett,Arnaud Dumont,Andrew J. Morris*

Main category: math.AP

TL;DR: The paper extends tangential convergence results from classical harmonic functions to elliptic equations with measurable coefficients on Lipschitz domains, showing that L^p solvability implies improved convergence for regular boundary data.


<details>
  <summary>Details</summary>
Motivation: To extend classical results about tangential convergence of harmonic functions to more general elliptic equations with measurable coefficients, and to understand the convergence behavior when boundary data has additional regularity.

Method: The authors use the characterization of L^p-Dirichlet problem solvability through quantitative absolute continuity of L-harmonic measure (local A_∞ property) and analyze nontangential convergence for boundary data with Sobolev regularity.

Result: The local A_∞ property guarantees that nontangential convergence improves to tangential convergence for boundary data with additional Sobolev regularity, with sharp estimates on the Hausdorff dimension of the set where convergence fails.

Conclusion: The results generalize classical harmonic function theory to elliptic equations with measurable coefficients, providing a complete picture of convergence behavior for regular boundary data on Lipschitz domains.

Abstract: It is well-known that solvability of the $\mathrm{L}^{p}$-Dirichlet problem
for elliptic equations $Lu:=-\mathrm{div}(A\nabla u)=0$ with real-valued,
bounded and measurable coefficients $A$ on Lipschitz domains
$\Omega\subset\mathbb{R}^{1+n}$ is characterised by a quantitative absolute
continuity of the associated $L$-harmonic measure. We prove that this local
$A_{\infty}$ property is sufficient to guarantee that the nontangential
convergence afforded to $\mathrm{L}^{p}$ boundary data actually improves to a
certain \emph{tangential} convergence when the data has additional (Sobolev)
regularity. Moreover, we obtain sharp estimates on the Hausdorff dimension of
the set on which such convergence can fail. This extends results obtained by
Dorronsoro, Nagel, Rudin, Shapiro and Stein for classical harmonic functions in
the upper half-space.

</details>


### [23] [Improved Gevrey Class Regularity of the Kadomtsev Petviashvili Equation](https://arxiv.org/abs/2510.26669)
*Aissa Boukarou,Lamia Seghour*

Main category: math.AP

TL;DR: This paper improves previous results on Gevrey regularity for a fifth-order KP-type equation, establishing sharper time regularity bounds using a new analytical approach.


<details>
  <summary>Details</summary>
Motivation: To extend and improve Boukarou et al.'s results on Gevrey regularity for solutions to fifth-order KP-type equations, providing more precise regularity estimates.

Method: Uses the method of majorant series to simultaneously treat all three variables (x, y, t), precisely tracking the influence of higher-order dispersive term ∂ₓ⁵u and lower-order terms.

Result: Proves that if initial data are Gevrey regular of order σ≥1 in spatial variables, the solution is Gevrey regular of order 5σ in time, and u(x,y,t) as a function of t does not belong to Gᶻ for any 1≤z<5σ.

Conclusion: The paper successfully establishes optimal Gevrey regularity bounds for fifth-order KP-type equations, showing the precise relationship between spatial and temporal regularity orders.

Abstract: In this paper, we improve and extend the results obtained by Boukarou et al.
\cite{boukarou1} on the Gevrey regularity of solutions to a fifth-order
Kadomtsev-Petviashvili (KP)-type equation. We establish Gevrey regularity in
the time variable for solutions in $2+1$ dimensions, providing a sharper result
obtained through a new analytical approach. Assuming that the initial data are
Gevrey regular of order $\sigma \geq 1$ in the spatial variables, we prove that
the corresponding solution is Gevrey regular of order $5 \sigma$ in time.
Moreover, we show that the function $u(x, y, t)$, viewed as a function of $t$,
does not belong to $G^z$ for any $1 \leq z<5 \sigma$. The proof simultaneously
treats all three variables $x, y$, and $t$, and employs the method of majorant
series, precisely tracking the influence of the higher-order dispersive term
$\partial_x^5 u$ together with the lower-order terms $\alpha \partial_x^3 u,
\partial_x^{-1} \partial_y^2 u$, and $u \partial_x u$.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [24] [Equation Discovery, Parametric Simulation, and Optimization Using the Physics-Informed Neural Network (PINN) Method for the Heat Conduction Problem](https://arxiv.org/abs/2510.25925)
*Ehsan Ghaderi,Mohamad Ali Bijarchi,Siamak Kazemzadeh Hannani,Ali Nouri Boroujerdi*

Main category: physics.comp-ph

TL;DR: PINN method evaluated for modeling, simulation, and optimization of heat conduction problems using fractional-order derivatives, parametric analysis, and inverse problem solving.


<details>
  <summary>Details</summary>
Motivation: To investigate the capabilities of Physics-Informed Neural Networks (PINNs) for three fundamental engineering tasks in heat conduction problems.

Method: Used PINN approach with fractional-order derivatives for equation discovery, parametric simulation of thermal conductivity, and inverse problem optimization to infer unknown physical properties.

Result: PINNs may not outperform traditional numerical solvers in speed/accuracy for forward problems but offer powerful framework for parametric simulation, optimization, and equation discovery.

Conclusion: PINNs are highly valuable for inverse and data-driven modeling applications despite not being superior to conventional methods for forward problems.

Abstract: In this study, the capabilities of the Physics-Informed Neural Network (PINN)
method are investigated for three major tasks: modeling, simulation, and
optimization in the context of the heat conduction problem. In the modeling
phase, the governing equation of heat transfer by conduction is reconstructed
through equation discovery using fractional-order derivatives, enabling the
identification of the fractional derivative order that best describes the
physical behavior. In the simulation phase, the thermal conductivity is treated
as a physical parameter, and a parametric simulation is performed to analyze
its influence on the temperature field. In the optimization phase, the focus is
placed on the inverse problem, where the goal is to infer unknown physical
properties from observed data. The effectiveness of the PINN approach is
evaluated across these three fundamental engineering problem types and compared
against conventional numerical methods. The results demonstrate that although
PINNs may not yet outperform traditional numerical solvers in terms of speed
and accuracy for forward problems, they offer a powerful and flexible framework
for parametric simulation, optimization, and equation discovery, making them
highly valuable for inverse and data-driven modeling applications.

</details>


### [25] [Generative Artificial Intelligence for Air Shower Simulation](https://arxiv.org/abs/2510.26316)
*C. Bozza,A. Calivà,A. De Caro,D. De Gruttola,S. De Pasquale,L. A. Fusco,G. Messuti,C. Poirè,S. Scarpetta,T. Virgili*

Main category: physics.comp-ph

TL;DR: GAN-based approach accelerates cosmic ray air shower simulations by 10,000x compared to traditional Monte Carlo methods while maintaining accuracy in key particle distributions.


<details>
  <summary>Details</summary>
Motivation: Current Monte Carlo simulations for cosmic ray air showers are computationally intensive and consume major resources in astroparticle physics experiments.

Method: Developed and trained a Generative Adversarial Network (GAN) on CORSIKA-generated proton-induced air shower data to reproduce secondary particle distributions.

Result: Training took 74 hours, but generation time per shower reduced by 10^4 times compared to CORSIKA, with accurate reproduction of muon energy spectra and spatial distributions.

Conclusion: GANs provide a highly efficient alternative to traditional Monte Carlo methods for air shower simulations, significantly reducing computational time and energy consumption.

Abstract: The detailed simulation of extensive air showers, produced by primary cosmic
rays interacting in the atmosphere, is a task that is traditionally undertaken
by means of Monte Carlo methods. These processes are computationally intensive,
accounting for a major fraction of the computational resources used in the
large-scale simulations required by current and future experiments in the field
of astroparticle physics. In this work, we present a novel approach based on
Generative Adversarial Networks (GANs) to accelerate air shower simulations. We
developed and trained a GAN on a dataset of high-energy proton-induced air
showers generated with \texttt{CORSIKA}; our model reproduces key distributions
of secondary particles, such as energy spectra and spatial distributions at
ground level of muons. Once the model has been trained, which takes
approximately 74 hours, the generation real time per shower is reduced by a
factor of $10^4$ with respect to the full \texttt{CORSIKA} simulation, leading
to a substantial decrease in both computational time and energy consumption.

</details>


### [26] [Patch-MLP-Based Predictive Control: Simulation of Upstream Pointing Stabilization for PHELIX Laser System](https://arxiv.org/abs/2510.26540)
*Jiaying Wang,Jonas Benjamin Ohland,Yen-Yu Chang,Vedhas Pandit,Stefan Bock,Andrew-Hiroaki Okukura,Udo Eisenbarth,Arie Irman,Michael Bussmann,Ulrich Schramm,Jeffrey Kelling*

Main category: physics.comp-ph

TL;DR: A predictive control strategy using patch-based MLP for beam pointing error forecasting combined with PID control reduces jitter in high-energy laser facilities like PHELIX, improving stability by 10-20% compared to conventional PID alone.


<details>
  <summary>Details</summary>
Motivation: Traditional PID control in high-energy laser facilities like PHELIX is limited by system delays and mechanical inertia, requiring better beam pointing stability for reproducibility and experimental independence.

Method: Combines patch-based multilayer perceptron (Patch-MLP) for forecasting beam pointing errors by capturing local temporal patterns, with PID controller for converting predictions into correction signals. Uses feed-forward control to compensate system delays.

Result: Simulations show predictive control reduces residual jitter compared to conventional PID, with 10-20% improvement in pointing metrics over 10-hour dataset, maintaining stable performance without drift.

Conclusion: The predictive controller operates without drift and may improve reproducibility and operational efficiency in high-energy, low repetition rate laser experiments.

Abstract: High-energy laser facilities such as PHELIX at GSI require excellent beam
pointing stability for reproducibility and relative independence for future
experiments. Beam pointing stability has been traditionally achieved using
simple proportional-integral-derivative (PID) control which removes the problem
of slow drift, but is limited because of the time delay in knowing the
diagnosis and the inertia in the mechanical system associated with mirrors. In
this work, we introduce a predictive control strategy where the forecasting of
beam pointing errors is performed by a patch-based multilayer perceptron
(Patch-MLP) designed to capture local temporal patterns for more robust
short-term jitter prediction. The subsequent conversion of these predicted
errors into correction signals is handled by a PID controller. The neural
network has been trained on diagnostic time-series data to predict beam
pointing error. Using the feed-forward controller compensates for system
delays. Simulations with a correction mirror placed upstream of the PHELIX
pre-amplifier bridge confirm that the predictive control scheme reduces
residual jitter compared to conventional PID control. Over a 10-hour dataset
the controller maintained stable performance without drift, while standard
pointing metrics showed consistent improvements of the order of 10 to 20
percent. The predictive controller operates without drift, and therefore may
improve reproducibility and operational efficiency in high energy, low
repetition rate laser experiment conditions.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [A Self-Consistent Model of Kinetic Alfven Solitons in Pulsar Wind Plasma: Linking Soliton Characteristics to Pulsar Observables](https://arxiv.org/abs/2510.25972)
*Manpreet Singh,Geetika Slathia,N. S. Saini,Siming Liu*

Main category: physics.plasm-ph

TL;DR: A model for kinetic Alfven soliton formation in pulsar winds using KdV equation, showing soliton properties depend on pulsar parameters and plasma composition.


<details>
  <summary>Details</summary>
Motivation: To understand how kinetic Alfven solitons form and propagate in pulsar wind zones and how their properties relate to observable pulsar characteristics.

Method: Reductive perturbation approach to derive Korteweg-de Vries equation governing nonlinear evolution of kinetic Alfven solitons in relativistic magnetized electron-positron-ion plasma.

Result: Soliton amplitude and width depend on pulsar spin period, spin-down rate, pair multiplicity, plasma composition, and suprathermal distributions. Heavy ions produce broader solitons, higher pair multiplicity yields smaller solitons, oblique propagation creates wider but lower-amplitude solitons.

Conclusion: The model links soliton dynamics to measurable pulsar parameters, providing a framework for interpreting magnetospheric microphysics and pulsar emission signatures.

Abstract: We present a self-consistent model for the formation and propagation of
kinetic Alfven (KA) solitons in the pulsar wind zone, where a relativistic,
magnetized electron positron ion plasma flows along open magnetic field lines
beyond the light cylinder. Using a reductive perturbation approach, we derive a
Korteweg de Vries (KdV) equation that governs the nonlinear evolution of KA
solitons in this environment. The soliton amplitude and width are shown to
depend sensitively on key pulsar observables, including spin period, spin-down
rate, and pair multiplicity as well as plasma composition and suprathermal
particle distributions. Our analysis reveals that soliton structures are
strongly influenced by the presence of heavy ions, kappa-distributed pairs, and
oblique propagation angles. Heavier ion species such as Fe26+ produce
significantly broader solitons due to enhanced inertia and dispersion, while
increasing pair multiplicity leads to smaller solitons through stronger
screening. Oblique propagation (larger theta) results in wider but
lower-amplitude solitons, and more thermalized pair plasmas (higher kappa)
support taller and broader structures. A population-level analysis of 1174
pulsars shows a clear positive correlation between soliton width and spin
period, with millisecond pulsars hosting the narrowest solitons. By linking
soliton dynamics to measurable pulsar parameters, this work provides a
framework for interpreting magnetospheric microphysics and its role in shaping
pulsar emission signatures.

</details>


### [28] [Optimization of the Compact Stellarator with Simple Coils at finite-beta](https://arxiv.org/abs/2510.26155)
*Haorong Qiu,Guodong Yu,Peiyou Jiang,Guoyong Fu*

Main category: physics.plasm-ph

TL;DR: Single-stage optimization of coil currents in the CSSC stellarator mitigates finite beta effects on neoclassical confinement, improving performance at finite plasma beta.


<details>
  <summary>Details</summary>
Motivation: The CSSC stellarator, optimized in vacuum, suffers from detrimental finite beta effects on neoclassical confinement, requiring optimization for finite plasma beta conditions.

Method: Single-stage optimization by simply modifying the coil currents of the Compact Stellarator with Simple Coils (CSSC), which has coil topology similar to Columbia Non-neutral Torus.

Result: Finite beta effects can be largely mitigated by reducing the coil currents of CSSC, achieving optimized stellarator performance at finite plasma beta.

Conclusion: Coil current optimization is an effective approach to address finite beta effects in stellarators originally optimized for vacuum conditions.

Abstract: An optimized stellarator at finite plasma beta is realized by single-stage
optimization of simply modifying the coil currents of the Compact Stellarator
with Simple Coils (CSSC)[Yu et al., J. Plasma Physics 88,905880306 (2022)]. The
CSSC is an optimized stellarator obtained by direct optimization via coil
shapes, with its coil topology similar to that of the Columbia Non-neutral
Torus (CNT) [Pederson et al., Phys. Rev. Lett. 88, 205002 (2002)]. Due to its
vacuum-based optimization, the CSSC exhibits detrimental finite beta effects on
neoclassical confinement. The results of optimization show that the finite beta
effects can be largely mitigated by reducing the coil currents of CSSC.

</details>


### [29] [Design and Implementation of a Fast-Sweeping Langmuir Probe Diagnostic for DC Arc Jet Environments](https://arxiv.org/abs/2510.26162)
*Sebastian V. Colom,Magnus A. Haw,Jocelino Rodrigues*

Main category: physics.plasm-ph

TL;DR: Development of an open-source, low-cost fast-sweeping Langmuir probe system with 200 kHz temporal resolution for measuring transient plasma behavior in high-enthalpy environments.


<details>
  <summary>Details</summary>
Motivation: Conventional Langmuir probes lack sufficient temporal resolution to capture transient plasma behavior in dynamic environments, limiting plasma characterization capabilities.

Method: Designed and implemented a fast-sweeping Langmuir probe system using voltage sweeping techniques, and validated it in the 30 kW miniature Arc jet Research Chamber (mARC II) under extreme aerothermal conditions.

Result: The system successfully operated in high-enthalpy DC arc jet conditions, providing time-resolved electron temperature and density measurements along the flow's radial profile at up to 200 kHz resolution.

Conclusion: Established a robust, accessible Langmuir diagnostic solution for researchers studying transient plasma behavior in high-enthalpy environments.

Abstract: Langmuir probe diagnostics are a cornerstone of plasma characterization,
providing critical measurements of electron temperature, electron density, and
plasma potential. However, conventional swept Langmuir probes and other
traditional electrostatic probes often lack the temporal resolution necessary
to capture transient plasma behavior in dynamic environments. This paper
presents the design and implementation of a fast-sweeping Langmuir probe system
that is open-source, low-cost, and adaptable for a wide range of plasma
applications. The probe system incorporates voltage sweeping to resolve rapid
fluctuations in plasma parameters at a temporal resolution of up to 200 kHz. To
validate its performance, the system was implemented in the 30 kW miniature Arc
jet Research Chamber (mARC II), a high-enthalpy DC arc jet facility designed
for prototype testing and development. Experimental results demonstrate the
probe's capability to operate in extreme aerothermal conditions, providing
time-resolved electron temperature and density along the flow's radial profile.
This work establishes a robust and accessible Langmuir diagnostic solution for
researchers studying transient plasma behavior in high-enthalpy environments.

</details>


### [30] [High-order Mie resonance and transient field enhancement in laser-driven plasma nanoshells](https://arxiv.org/abs/2510.26175)
*Xiaohui Gao*

Main category: physics.plasm-ph

TL;DR: Plasma nanoshells achieve significant field enhancement via high-order Mie resonances, with optimal geometries providing ~3x enhancement at 800 nm, lasting tens of femtoseconds before plasma expansion disrupts resonance.


<details>
  <summary>Details</summary>
Motivation: To optimize laser-plasma interactions for applications like laser-cluster interaction diagnostics and energetic ion production from engineered core-shell targets by understanding field enhancement mechanisms in plasma nanoshells.

Method: Combined Mie theory and particle-in-cell simulations to analyze field enhancement through high-order Mie resonances in plasma nanoshells with various shell geometries.

Result: Optimal shell geometries yield approximately threefold electric field enhancement for 800 nm irradiation, with transient buildup times of tens of femtoseconds before plasma expansion disrupts resonance. Few-cycle pulses show reduced enhancement due to insufficient resonance establishment.

Conclusion: Temporal dynamics play a critical role in nanoplasma resonances, and these findings enable optimized laser-plasma interactions for various applications including diagnostics and energetic ion production.

Abstract: We demonstrate substantial field enhancement in plasma nanoshells through
high-order Mie resonances using combined Mie theory and particle-in-cell
simulations. Optimal shell geometries yield approximately threefold electric
field enhancement for 800 nm irradiation, with transient buildup times of tens
of femtoseconds before plasma expansion disrupts resonance. Few-cycle pulses
produce reduced enhancement due to insufficient resonance establishment. These
findings enable optimized laser-plasma interactions for applications including
diagnostics of laser-cluster interaction and energetic ion production from
engineered core-shell targets, highlighting the critical role of temporal
dynamics in nanoplasma resonances.

</details>


### [31] [Nonlocal Model for Electron Heat Flux and Self-generated Magnetic Field](https://arxiv.org/abs/2510.26640)
*Xinyu Zhu,Wenqiang Yuan,Yusen Wang,Zhipeng Zhang,Xianxu Jin,Zhonghai Zhao,Bin Qiao*

Main category: physics.plasm-ph

TL;DR: A new nonlocal model that simultaneously recovers kinetic effects for both electron heat conduction and magnetic field in hydrodynamic scale, addressing limitations of current flux limiters.


<details>
  <summary>Details</summary>
Motivation: Current nonlocal models only address kinetic effects on heat flux but still use flux limiters for magnetic field modeling, creating inconsistency in inertial confinement fusion simulations.

Method: Proposed a new nonlocal model that self-consistently considers electric field corrections and can handle both heat conduction and magnetic field kinetic effects simultaneously in hydrodynamic scale.

Result: The model enables systematic study of nonlocal corrections in magnetized plasma and magnetic field generation without density gradients. Nonlocal effects significantly alter magnetic field distribution in laser ablation.

Conclusion: Nonlocal effects on magnetic field distribution could potentially influence hydrodynamic instabilities in inertial confinement fusion, highlighting the importance of self-consistent modeling.

Abstract: Coupling of electron heat conduction and magnetic field takes significant
effects in inertial confinement fusion (ICF). As the nonlocal models for
electron heat conduction have been developed for modeling kinetic effects on
heat flux in hydrodynamic scale, modeling kinetic effects on magnetic field are
still restricted to flux limiters instead of nonlocal corrections. We propose a
new nonlocal model which can recover the kinetic effects for heat conduction
and magnetic field in hydrodynamic scale simultaneously. We clarify the
necessity of self-consistently considering the electric field corrections in
nonlocal models to get reasonable physical quantities. Using the new nonlocal
model, the nonlocal corrections of transport coefficients in magnetized plasma
and the magnetic field generation without density gradients are systematically
studied. We find nonlocal effects significantly change the magnetic field
distribution in laser ablation, which potentially influences the hydrodynamic
instabilities in ICF.

</details>


### [32] [Spectral Deconvolution without the Deconvolution: Extracting Temperature from X-ray Thomson Scattering Spectra without the Source-and-Instrument Function](https://arxiv.org/abs/2510.26747)
*Thomas Gawne,Alina Kononov,Andrew Baczewski,Hannah Bellenbaum,Maximilian P Böhme,Zhandos Moldabekov,Thomas R Preston,Sebastian Schwalbe,Jan Vorberger,Tobias Dornheim*

Main category: physics.plasm-ph

TL;DR: Proposes a method to extract temperature from X-ray Thomson scattering spectra without needing to know the source-and-instrument function, by using ratios of Laplace-transformed spectra from different scattering angles.


<details>
  <summary>Details</summary>
Motivation: Current methods for extracting temperature from XRTS spectra require accurate knowledge of the source-and-instrument function (SIF), which is challenging to measure precisely. The SIF broadening affects the extracted properties and depends strongly on the input SIF shape.

Method: Use ratios of Laplace-transformed XRTS spectra collected at different scattering angles, which effectively performs deconvolution without requiring explicit knowledge of the SIF. This allows direct temperature extraction for systems in thermal equilibrium.

Result: The method is robust to spectral noise and physical differences between spectrometers. Consistent temperatures can be extracted for equilibrium systems, and non-equilibrium effects can be identified by inconsistent temperatures between ratios from three or more scattering angles.

Conclusion: The proposed ratio-based Laplace transform approach provides a practical alternative to SIF deconvolution for temperature extraction from XRTS spectra, with the added benefit of detecting non-equilibrium conditions through temperature inconsistencies.

Abstract: X-ray Thomson scattering (XRTS) probes the dynamic structure factor of the
system, but the measured spectrum is broadened by the combined
source-and-instrument function (SIF) of the setup. In order to extract
properties such as temperature from an XRTS spectrum, the broadening by the SIF
needs to be removed. Recent work [Dornheim et al. Nature Commun. 13, 7911
(2022)] has suggested that the SIF may be deconvolved using the two-sided
Laplace transform. However, the extracted information can depend strongly on
the shape of the input SIF, and the SIF is in practice challenging to measure
accurately. Here, we propose an alternative approach: we demonstrate that
considering ratios of Laplace-transformed XRTS spectra collected at different
scattering angles is equivalent to performing the deconvolution, but without
the need for explicit knowledge of the SIF. From these ratios, it is possible
to directly extract the temperature from the scattering spectra, when the
system is in thermal equilibrium. We find the method to be generally robust to
spectral noise and physical differences between the spectrometers, and we
explore situations in which the method breaks down. Furthermore, the fact that
consistent temperatures can be extracted for systems in thermal equilibrium
indicates that non-equilibrium effects could be identified by inconsistent
temperatures of a few eV between the ratios of three or more scattering angles.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [33] [On a semi-discrete model of Maxwell's equations in three and two dimensions](https://arxiv.org/abs/2510.26427)
*Volodymyr Sushch*

Main category: math-ph

TL;DR: A geometric, structure-preserving semi-discrete formulation of Maxwell's equations using discrete exterior calculus, preserving intrinsic geometric and topological structures while providing consistent spatial discretization.


<details>
  <summary>Details</summary>
Motivation: To develop a spatial discretization of Maxwell's equations that preserves the intrinsic geometric and topological structures of the continuous theory, addressing the need for consistent discretization methods.

Method: Using discrete exterior calculus framework to create semi-discrete formulations of Maxwell's equations in both 3D and 2D settings, with analysis of essential properties and comparison to classical Maxwell's equations.

Result: Developed a structure-preserving semi-discrete model that maintains geometric and topological properties. Illustrated on a combinatorial 2D torus where equations become first-order linear ODEs, and derived explicit general solution for this system.

Conclusion: The discrete exterior calculus approach successfully provides a geometric structure-preserving semi-discretization of Maxwell's equations, maintaining essential properties while enabling explicit solutions in special cases like the 2D torus.

Abstract: In this paper, we develop a geometric, structure-preserving semi-discrete
formulation of Maxwell's equations in both three- and two-dimensional settings
within the framework of discrete exterior calculus. This approach preserves the
intrinsic geometric and topological structures of the continuous theory while
providing a consistent spatial discretization. We analyze the essential
properties of the proposed semi-discrete model and compare them with those of
the classical Maxwell's equations. As a special case, the model is illustrated
on a combinatorial two-dimensional torus, where the semi-discrete Maxwell's
equations take the form of a system of first-order linear ordinary differential
equations. An explicit expression for the general solution of this system is
also derived.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [34] [Quantum Stochastic Gradient Descent in its continuous-time limit based on the Wigner formulation of Open Quantum Systems](https://arxiv.org/abs/2510.25910)
*Jose A. Morales Escalante*

Main category: quant-ph

TL;DR: Using Wigner formulation as bridge between classical and quantum probabilistic algorithms, focusing on quantum analog of Stochastic Gradient Descent in continuous-time limit.


<details>
  <summary>Details</summary>
Motivation: To connect classical and quantum probabilistic algorithms through the Wigner formulation, enabling development of quantum analogs of classical optimization methods.

Method: Wigner formulation of Open Quantum Systems applied to develop quantum version of Stochastic Gradient Descent in continuous-time limit.

Result: Research plan presented for bridging classical and quantum probabilistic algorithms using Wigner formulation.

Conclusion: Wigner formulation serves as effective bridge for developing quantum analogs of classical probabilistic algorithms like Stochastic Gradient Descent.

Abstract: The main ideas behind a research plan to use the Wigner formulation as a
bridge between classical and quantum probabilistic algorithms are presented,
focusing on a particular case: the Quantum analog of Stochastic Gradient
Descent in its continuous-time limit based on the Wigner formulation of Open
Quantum Systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [35] [Gradient Flow Sampler-based Distributionally Robust Optimization](https://arxiv.org/abs/2510.25956)
*Zusen Xu,Jia-Jie Zhu*

Main category: math.OC

TL;DR: A PDE gradient flow framework for distributionally robust optimization (DRO) that connects MCMC sampling with gradient flow theory to sample from worst-case distributions and solve DRO problems.


<details>
  <summary>Details</summary>
Motivation: To provide a mathematically principled foundation for DRO using gradient flow theory, offering new insights and unifying existing methods under a single theoretical framework.

Method: Uses PDE gradient flow framework with Wasserstein Fisher-Rao and Stein variational gradient flows to solve Wasserstein and Sinkhorn DRO problems, connecting MCMC sampling with gradient flow theory.

Result: The framework enables practical algorithms for sampling from worst-case distributions and solving DRO problems, recovers existing popular DRO methods as special cases, and provides new theoretical insights.

Conclusion: The gradient flow perspective offers a unified theoretical foundation for DRO that connects with sampling methods and provides new algorithmic approaches, with empirical validation through numerical studies.

Abstract: We propose a mathematically principled PDE gradient flow framework for
distributionally robust optimization (DRO). Exploiting the recent advances in
the intersection of Markov Chain Monte Carlo sampling and gradient flow theory,
we show that our theoretical framework can be implemented as practical
algorithms for sampling from worst-case distributions and, consequently, DRO.
While numerous previous works have proposed various reformulation techniques
and iterative algorithms, we contribute a sound gradient flow view of the
distributional optimization that can be used to construct new algorithms. As an
example of applications, we solve a class of Wasserstein and Sinkhorn DRO
problems using the recently-discovered Wasserstein Fisher-Rao and Stein
variational gradient flows. Notably, we also show some simple reductions of our
framework recover exactly previously proposed popular DRO methods, and provide
new insights into their theoretical limit and optimization dynamics. Numerical
studies based on stochastic gradient descent provide empirical backing for our
theoretical findings.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [36] [Geometric and Orbital Control of Correlated States in Small Hubbard Clusters](https://arxiv.org/abs/2510.25919)
*Shivanshu Dwivedi,Kalum Palandage*

Main category: cond-mat.str-el

TL;DR: A framework for engineering local electron pairing in quantum dot arrays using three control levers: lattice geometry, orbital hybridization, and electric fields, with design principles based on coordination number, orbital hopping, and field-induced localization.


<details>
  <summary>Details</summary>
Motivation: To develop a predictive framework for designing correlated quantum matter in semiconductor quantum dot arrays by systematically controlling key parameters to engineer local electron pairing.

Method: Hartree-Fock simulations on canonical 3D clusters (tetrahedron to FCC lattice) at and near half-filling, analyzing effects of coordination number, inter-orbital hopping, and electric fields.

Result: Three fundamental design principles: geometric hierarchy (coordination number controls resilience to Coulomb repulsion), orbital hybridization (inter-orbital hopping enhances double occupancy at moderate U), and field squeezing (electric fields induce pairing via charge localization).

Conclusion: These principles provide a blueprint for deterministic control of charge and spin correlations in quantum-dot-based quantum hardware.

Abstract: Arrays of semiconductor quantum dots provide a powerful platform to design
correlated quantum matter from the bottom up. We establish a predictive
framework for engineering local electron pairing in these artificial molecules
by systematically deploying three control levers: lattice geometry, orbital
hybridization, and external electric fields. Using Hartree-Fock simulations on
canonical 3D clusters from the tetrahedron (Z = 3) to the FCC lattice (Z = 12),
at and near half-filling, we uncover three fundamental design principles. (i)
Geometric Hierarchy: The resilience to Coulomb repulsion U is dictated by the
coordination number Z, which controls kinetic delocalization. (ii) Orbital
Hybridization: Counter-intuitively, inter-orbital hopping t_orb acts not as a
simple suppressor of pairing, but as a sophisticated control knob that enhances
double occupancy at moderate U by engineering the on-site energy landscape.
(iii) Field Squeezing: An electric field robustly induces pairing by forcing
charge localization, an effect most potent in low-connectivity clusters. These
principles form a blueprint for deterministically targeting charge and spin
correlations in quantum-dot-based quantum hardware.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [37] [Generative sampling with physics-informed kernels](https://arxiv.org/abs/2510.26678)
*Friederike Ihssen,Renzo Kapust,Jan M. Pawlowski*

Main category: hep-lat

TL;DR: A generative network for Monte-Carlo sampling in lattice field theories using physics-informed renormalisation group flows, where layerwise propagation is learned independently through solving linear differential equations for transformation kernels.


<details>
  <summary>Details</summary>
Motivation: To address out-of-domain problems in generative models and enable further optimization in Monte-Carlo sampling for lattice field theories.

Method: Uses physics-informed renormalisation group flows that transform the generative task into solving independent linear differential equations for transformation kernels between layers, allowing iterative refinement.

Result: Demonstrated practical feasibility through simulations in scalar field theories.

Conclusion: The architecture successfully tackles out-of-domain problems in generative models and provides a path for further optimization in lattice field theory simulations.

Abstract: We construct a generative network for Monte-Carlo sampling in lattice field
theories and beyond, for which the learning of layerwise propagation is done
and optimised independently on each layer. The architecture uses
physics-informed renormalisation group flows that provide access to the
layerwise propagation step from one layer to the next in terms of a simple
first order partial differential equation for the respective renormalisation
group kernel through a given layer. Thus, it transforms the generative task
into that of solving once the set of independent and linear differential
equations for the kernels of the transformation. As these equations are
analytically known, the kernels can be refined iteratively. This allows us to
structurally tackle out-of-domain problems generally encountered in generative
models and opens the path to further optimisation. We illustrate the practical
feasibility of the architecture within simulations in scalar field theories.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [38] [A game-theoretic approach to the parabolic normalized p-Laplacian obstacle problem](https://arxiv.org/abs/2510.25999)
*Hamid El Bahja*

Main category: math.PR

TL;DR: The paper establishes a probabilistic representation for the solution of the parabolic obstacle problem with normalized p-Laplacian using a zero-sum stochastic tug-of-war game with noise and stopping options.


<details>
  <summary>Details</summary>
Motivation: To provide a probabilistic interpretation and game-theoretic approach for solving the parabolic obstacle problem associated with the normalized p-Laplacian operator.

Method: Introduces a zero-sum stochastic tug-of-war game with noise in space-time cylinder, where players can stop the game to collect payoff from an obstacle function. Proves existence of value functions, dynamic programming principle, and uniform convergence to viscosity solution.

Result: Value functions exist, satisfy dynamic programming principle, and converge uniformly to the unique viscosity solution of the continuous obstacle problem as step size ε → 0.

Conclusion: The stochastic game provides an effective probabilistic representation for the parabolic obstacle problem with normalized p-Laplacian, connecting game theory with PDE theory through uniform convergence to viscosity solutions.

Abstract: This paper establishes a probabilistic representation for the solution of the
parabolic obstacle problem associated with the normalized $p$-Laplacian. We
introduce a zero-sum stochastic tug-of-war game with noise in a space-time
cylinder, where one player has the option to stop the game at any time to
collect a payoff given by an obstacle function. We prove that the value
functions of this game exist, satisfy a dynamic programming principle, and
converge uniformly to the unique viscosity solution of the continuous obstacle
problem as the step size $\varepsilon$ tends to zero.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [39] [Quantitative Lorentzian isoperimetric inequalities](https://arxiv.org/abs/2510.26755)
*Christian Lange,Jonas W. Peteranderl*

Main category: math.DG

TL;DR: The paper establishes optimal stability estimates for Lorentzian isoperimetric inequalities using Fraenkel asymmetry, with universal dimensional constants. It covers inequalities by Bahn-Ehrlich and Cavalletti-Mondino, showing different dependencies on Fraenkel asymmetry.


<details>
  <summary>Details</summary>
Motivation: To provide stability estimates for Lorentzian isoperimetric inequalities, which are important in geometric analysis and have applications in relativity theory. The goal is to understand how deviations from optimal shapes affect these inequalities.

Method: The authors establish stability estimates using Fraenkel asymmetry as the measure of deviation from optimal shapes. They analyze two specific Lorentzian isoperimetric inequalities and provide self-contained proofs for these inequalities.

Result: For the Bahn-Ehrlich inequality, stability depends quadratically on Fraenkel asymmetry (similar to Euclidean case), while for the Cavalletti-Mondino inequality, it depends linearly. However, refining the latter with an additional geometric term recovers quadratic stability behavior.

Conclusion: The paper provides optimal stability estimates for Lorentzian isoperimetric inequalities with universal constants, showing different stability behaviors for different inequalities and demonstrating how additional geometric terms can improve stability estimates.

Abstract: We establish optimal stability estimates in terms of the Fraenkel asymmetry
with universal dimensional constants for a Lorentzian isoperimetric inequality
due to Bahn and Ehrlich and, as a consequence, for a special version of a
Lorentzian isoperimetric inequality due to Cavalletti and Mondino. For the
Bahn--Ehrlich inequality the Fraenkel asymmetry enters the stability result
quadratically like in the Euclidean case while for the Cavalletti--Mondino
inequality the Fraenkel asymmetry enters linearly. As it turns out, refining
the latter inequality through an additional geometric term allows us to recover
the more common quadratic stability behavior. Along the way, we provide simple
self-contained proofs for the above isoperimetric-type inequalities.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [A Practitioner's Guide to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2510.25781)
*Amir Noorizadegan,Sifan Wang,Leevan Ling*

Main category: cs.LG

TL;DR: A comprehensive review of Kolmogorov-Arnold Networks (KANs) covering theoretical foundations, architectural variants, implementation strategies, and practical guidance for practitioners.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic overview of the rapidly expanding KAN landscape, moving beyond simple performance comparisons to offer structured synthesis of theoretical foundations and practical implementation strategies.

Method: Systematic collection and categorization of open-source implementations, analysis of basis function choices (B-splines, Chebyshev polynomials, ReLU compositions, etc.), and development of a structured roadmap covering accuracy improvement techniques and regularization methods.

Result: Created a comprehensive mapping of the KAN ecosystem, established formal equivalence between KANs and MLPs, highlighted KAN's superior parameter efficiency, and provided practical guidance for architecture selection.

Conclusion: The review serves as a structured reference for ongoing KAN research, identifies current research gaps, and provides practical implementation guidance through an associated GitHub repository.

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising
alternative to traditional Multilayer Perceptrons (MLPs), inspired by the
Kolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed
activation functions on nodes, KANs employ learnable univariate basis functions
on edges, offering enhanced expressivity and interpretability. This review
provides a systematic and comprehensive overview of the rapidly expanding KAN
landscape, moving beyond simple performance comparisons to offer a structured
synthesis of theoretical foundations, architectural variants, and practical
implementation strategies. By collecting and categorizing a vast array of
open-source implementations, we map the vibrant ecosystem supporting KAN
development. We begin by bridging the conceptual gap between KANs and MLPs,
establishing their formal equivalence and highlighting the superior parameter
efficiency of the KAN formulation. A central theme of our review is the
critical role of the basis function; we survey a wide array of choices,
including B-splines, Chebyshev and Jacobi polynomials, ReLU compositions,
Gaussian RBFs, and Fourier series, and analyze their respective trade-offs in
terms of smoothness, locality, and computational cost. We then categorize
recent advancements into a clear roadmap, covering techniques for improving
accuracy, efficiency, and regularization. Key topics include physics-informed
loss design, adaptive sampling, domain decomposition, hybrid architectures, and
specialized methods for handling discontinuities. Finally, we provide a
practical "Choose-Your-KAN" guide to help practitioners select appropriate
architectures, and we conclude by identifying current research gaps. The
associated GitHub repository https://github.com/AmirNoori68/kan-review
complements this paper and serves as a structured reference for ongoing KAN
research.

</details>


### [41] [Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training](https://arxiv.org/abs/2510.25803)
*Hong Wang,Haiyang Xin,Jie Wang,Xuanze Yang,Fei Zha,Huanshuo Dong,Yan Jiang*

Main category: cs.LG

TL;DR: MoE-POT is a sparse-activated Mixture-of-Experts architecture for PDE solving that dynamically selects experts to handle heterogeneous PDE datasets while controlling inference costs, achieving 40% error reduction with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Address challenges in pre-training for PDE problems: heterogeneity of PDE datasets causing high errors in mixed training, and dense pre-training models having high inference costs due to parameter scaling.

Method: Propose Mixture-of-Experts Pre-training Operator Transformer (MoE-POT) with layer-wise router-gating network that dynamically selects 4 routed experts from 16 expert networks during inference, plus 2 shared experts to capture common PDE properties. Final output is weighted average of activated experts.

Result: Pre-trained models from 30M to 0.5B parameters on 6 PDE datasets. Model with 90M activated parameters achieves up to 40% reduction in zero-shot error compared to existing models with 120M activated parameters. Router-gating decisions can infer dataset types, validating MoE effectiveness.

Conclusion: MoE-POT provides an efficient sparse-activated architecture that scales parameters while controlling inference costs, effectively handling heterogeneous PDE datasets through dynamic expert selection and shared expert integration.

Abstract: Pre-training has proven effective in addressing data scarcity and performance
limitations in solving PDE problems with neural operators. However, challenges
remain due to the heterogeneity of PDE datasets in equation types, which leads
to high errors in mixed training. Additionally, dense pre-training models that
scale parameters by increasing network width or depth incur significant
inference costs. To tackle these challenges, we propose a novel
Mixture-of-Experts Pre-training Operator Transformer (MoE-POT), a
sparse-activated architecture that scales parameters efficiently while
controlling inference costs. Specifically, our model adopts a layer-wise
router-gating network to dynamically select 4 routed experts from 16 expert
networks during inference, enabling the model to focus on equation-specific
features. Meanwhile, we also integrate 2 shared experts, aiming to capture
common properties of PDE and reduce redundancy among routed experts. The final
output is computed as the weighted average of the results from all activated
experts. We pre-train models with parameters from 30M to 0.5B on 6 public PDE
datasets. Our model with 90M activated parameters achieves up to a 40%
reduction in zero-shot error compared with existing models with 120M activated
parameters. Additionally, we conduct interpretability analysis, showing that
dataset types can be inferred from router-gating network decisions, which
validates the rationality and effectiveness of the MoE architecture.

</details>


### [42] [How Regularization Terms Make Invertible Neural Networks Bayesian Point Estimators](https://arxiv.org/abs/2510.26704)
*Nick Heilenkötter*

Main category: cs.LG

TL;DR: The paper shows that specific regularization terms in invertible neural network training can recover classical Bayesian point estimators like posterior mean and MAP estimator upon network inversion.


<details>
  <summary>Details</summary>
Motivation: Invertible neural networks are stable and interpretable for inverse problems, but existing optimization strategies have limitations. The authors aim to connect network training with Bayesian estimators.

Method: Introduce and analyze two regularization terms for invertible neural network training that recover Bayesian point estimators when the network is inverted.

Result: Theoretical analysis characterizes how each loss term shapes both the learned forward operator and its inverse reconstruction map. Numerical experiments support the findings.

Conclusion: The proposed loss-term regularizers enable data-dependent reconstruction in a stable and interpretable way, connecting invertible network training to classical Bayesian estimators.

Abstract: Can regularization terms in the training of invertible neural networks lead
to known Bayesian point estimators in reconstruction? Invertible networks are
attractive for inverse problems due to their inherent stability and
interpretability. Recently, optimization strategies for invertible neural
networks that approximate either a reconstruction map or the forward operator
have been studied from a Bayesian perspective, but each has limitations. To
address this, we introduce and analyze two regularization terms for the network
training that, upon inversion of the network, recover properties of classical
Bayesian point estimators: while the first can be connected to the posterior
mean, the second resembles the MAP estimator. Our theoretical analysis
characterizes how each loss shapes both the learned forward operator and its
inverse reconstruction map. Numerical experiments support our findings and
demonstrate how these loss-term regularizers introduce data-dependence in a
stable and interpretable way.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [43] [Exciton dynamics in equilibrium and nonequilibrium regimes](https://arxiv.org/abs/2510.26221)
*Pushpendra Yadav*

Main category: cond-mat.mes-hall

TL;DR: First-principles study of excitons in 2D materials using GW+BSE approach, showing density-dependent redshift-blueshift crossover, electron-phonon effects on spectra/lifetimes, and electron-hole liquid formation at high densities/low temperatures.


<details>
  <summary>Details</summary>
Motivation: To understand nonequilibrium exciton behavior at high excitation densities in 2D materials, particularly phenomena like electron-hole liquids that are less explored compared to equilibrium properties.

Method: Uses GW approximation and Bethe-Salpeter equation (GW+BSE) to study exciton properties from equilibrium to nonequilibrium conditions in two-dimensional materials.

Result: Shows redshift-blueshift crossover with increasing carrier density, electron-phonon interactions modify optical spectra and exciton lifetimes, and demonstrates electron-hole liquid phase formation above critical density and below critical temperature.

Conclusion: Enhanced Coulomb interactions in 2D materials can stabilize electron-hole liquid phase at significantly higher temperatures, identifying promising material candidates for observing these collective states.

Abstract: The bound electron-hole pairs known as excitons govern the optical properties
of insulating solids. While their behavior in equilibrium is well-understood
theoretically, the nonequilibrium regime at high excitation densities-where
phenomena like electron-hole liquids emerge - is less explored. This thesis
presents a first-principles study of excitons in two-dimensional materials. We
use the GW approximation and the Bethe-Salpeter equation to investigate their
properties from equilibrium to nonequilibrium conditions. We first demonstrate
how increasing photo-excited carrier density leads to a redshift-blueshift
crossover of excitons. We then show that electron-phonon interactions
critically modify optical spectra and exciton lifetimes at finite temperatures.
Finally, we unify these effects to demonstrate the formation of an
electron-hole liquid phase above a critical carrier density and below a
critical temperature. Our work identifies how enhanced Coulomb interactions in
two dimensions can stabilize this phase at significantly higher temperatures,
proposing promising material candidates for observing these collective states.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [44] [Numerical Investigation of Single-Core to Split-Core Transitions in Nematic Liquid Crystals](https://arxiv.org/abs/2510.26215)
*Daniel Siebel-Cortopassi,Pei Liu*

Main category: cond-mat.soft

TL;DR: Analysis of single-core and split-core defect structures in nematic liquid crystals using Landau-de Gennes framework, revealing temperature-dependent bifurcation and energy minimization properties.


<details>
  <summary>Details</summary>
Motivation: To understand the temperature-dependent behavior and stability of different defect structures in nematic liquid crystals, particularly the transition between single-core and split-core configurations.

Method: Using the Landau-de Gennes framework to study minimizers of the associated energy functional, analyzing Euler-Lagrange equations, and examining bifurcation at critical temperature thresholds.

Result: Below a critical temperature threshold, both split-core and single-core configurations exist as solutions, with split-core having lower energy. Above this threshold, only single-core defects remain stable. The threshold depends on domain size, and quantitative analysis of defect core sizes was performed.

Conclusion: The study reveals a temperature-dependent bifurcation in nematic liquid crystal defect structures, with split-core configurations being energetically favorable below a critical temperature that depends on domain size, while single-core defects dominate above this threshold.

Abstract: We analyze single-core and split-core defect structures in nematic liquid
crystals within the Landau-de Gennes framework by studying minimizers of the
associated energy functional. A bifurcation occurs at a critical temperature
threshold, below which both split-core and single-core configurations are
solutions to the Euler-Lagrange equation, with the split-core defect possessing
lower energy. Above the threshold, the split-core configuration vanishes,
leaving the single-core defect as the only stable solution. We analyze the
dependence of such temperature threshold on the domain size and characterize
the nature of the transition between the two defect types. We carry out a
quantitative study of defect core sizes as functions of temperature and domain
size for both single and split core defects.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [45] [Temperature dependent ferroelectricity in strained KTaO3 with machine learned force field](https://arxiv.org/abs/2510.26693)
*Yu Zhu,Luigi Ranalli,Taikang Chen,Wei Ren,Cesare Franchini*

Main category: cond-mat.mtrl-sci

TL;DR: The study investigates how uniaxial and biaxial strain (0-1%) induces ferroelectricity in KTaO3 through inversion symmetry breaking using advanced computational methods.


<details>
  <summary>Details</summary>
Motivation: To understand how strain engineering can drive ferroelectric phase transitions in incipient ferroelectrics like KTaO3, which has both fundamental and applied significance.

Method: Combined density functional theory with stochastic self-consistent harmonic approximation and machine-learned force fields to study structural and dynamical properties under strain, using Berry phase method for polarization calculations.

Result: Strain successfully induces ferroelectric polarization in KTaO3 across temperatures up to 300K, with accurate characterization of structural and dynamical properties including anharmonic effects.

Conclusion: Strain engineering effectively stabilizes ferroelectricity in KTaO3, providing valuable insights for future experimental and theoretical work on strain-tuned ferroelectric materials.

Abstract: Ferroelectric materials are a class of dielectrics that exhibit spontaneous
polarization which can be reversed under an external electric field. The
emergence of ferroelectric order in incipient ferroelectrics is a topic of
considerable interest from both fundamental and applied perspectives. Among the
various strategies explored, strain engineering has been proven to be a
powerful method for tuning ferroelectric polarization in materials. In the case
of KTaO3, first principles calculations have suggested that strain can drive a
ferroelectric phase transition. In this study, we investigate the impact of
in-plane uniaxial and biaxial strain, ranging from 0% to 1%, on pristine KTaO3
to explore its potential for ferroelectricity induction via inversion symmetry
breaking. By integrating density functional theory calculations with the
stochastic self-consistent harmonic approximation assisted by on the fly
machine learned force field, we obtain accurate structural information and
dynamical properties under varying strain conditions while incorporating
higher-order anharmonic effects. Employing the Berry phase method, we obtained
the ferroelectric polarization of the strained structures over the entire
temperature range up to 300 K. Our findings provide valuable insights into the
role of strain in stabilizing ferroelectricity in KTaO3, offering guidance for
future experimental and theoretical studies on strain-engineered ferroelectric
materials.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [46] [Cosmological Simulations of Weakly Collisional Plasmas with Braginskii Viscosity in Galaxy Clusters](https://arxiv.org/abs/2510.25847)
*Tirso Marin-Gilabert,Ulrich P. Steinwandel,Milena Valentini,John A. ZuHone,Klaus Dolag*

Main category: astro-ph.IM

TL;DR: Implementation of anisotropic viscosity solver in OpenGadget3 MHD code with Braginskii formulation and plasma instability limiters, validated against standard tests and applied to galaxy cluster simulation.


<details>
  <summary>Details</summary>
Motivation: To model anisotropic viscous transport along magnetic field lines in magnetized astrophysical plasmas, accounting for physical constraints from plasma instabilities in weakly collisional environments.

Method: Implemented anisotropic viscosity solver using Braginskii formulation with mirror and firehose instability limiters, integrated within OpenGadget3's adaptive timestepping framework without subcycling.

Result: Excellent agreement with AREPO implementation in validation tests, successful application to cosmological galaxy cluster simulation demonstrating capability for realistic large-scale modeling.

Conclusion: The implementation provides an efficient, stable, and versatile tool for studying anisotropic viscosity in magnetized astrophysical systems while maintaining physical consistency.

Abstract: We present the implementation of an anisotropic viscosity solver within the
magnetohydrodynamics (MHD) framework of the TreeSPH code OpenGadget3. The
solver models anisotropic viscous transport along magnetic field lines
following the Braginskii formulation and includes physically motivated limiters
based on the mirror and firehose instability thresholds, which constrain the
viscous stress in weakly collisional plasmas. To validate the implementation,
we performed a suite of standard test problems -- including two variants of the
sound-wave test, circularly and linearly polarized Alfven waves, fast
magnetosonic wave, and the Kelvin-Helmholtz instability -- both with and
without the plasma-instability limiters. The results show excellent agreement
with the AREPO implementation of a similar anisotropic viscosity model (Berlok
et al. 2019), confirming the accuracy and robustness of our method. Our
formulation integrates seamlessly within the individual adaptive timestepping
framework of OpenGadget3, avoiding the need for subcycling. This provides
efficient and stable time integration while maintaining physical consistency.
Finally, we applied the new solver to a cosmological zoom-in simulation of a
galaxy cluster, demonstrating its capability to model anisotropic transport and
plasma microphysics in realistic large-scale environments. Our implementation
offers a versatile and computationally efficient tool for studying anisotropic
viscosity in magnetized astrophysical systems.

</details>
