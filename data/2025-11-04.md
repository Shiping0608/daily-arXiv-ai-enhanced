<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 37]
- [math.AP](#math.AP) [Total: 39]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [math.DG](#math.DG) [Total: 3]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [math.CV](#math.CV) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [math.CA](#math.CA) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [math.PR](#math.PR) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [An unconditionally stable numerical approach for solving a nonlinear distributed delay Sobolev model](https://arxiv.org/abs/2511.00003)
*Eric Ngondiep*

Main category: math.NA

TL;DR: An unconditionally stable numerical method for nonlinear Sobolev models with distributed delay using interpolation for time derivatives and finite elements for spatial derivatives.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and stable computational approach for solving nonlinear Sobolev problems with distributed delay, which are challenging due to their complex nature.

Method: Combines interpolation technique for time derivative approximation with finite element method for spatial derivatives. The approach is simple to implement and provides strong norm analysis.

Result: The method is unconditionally stable, achieves spatial fourth-order accuracy, second-order convergence in time, and outperforms existing numerical methods for delay Sobolev problems.

Conclusion: The developed technique is theoretically sound, practically efficient, and validated through numerical examples, making it suitable for solving complex delay Sobolev models.

Abstract: This paper proposes an unconditionally stable numerical method for solving a
nonlinear Sobolev model with distributed delay. The proposed computational
approach approximates the time derivative by interpolation technique whereas
the spatial derivatives are approximated using the finite element
approximation. This combination is simple and easy to implement. Both stability
and error estimates of the constructed method are deeply analyzed in a strong
norm which is equivalent to the $H^{1}$-norm. The theoretical results indicate
that the constructed approach is unconditionally stable, spatial fourth-order
accurate, second-order convergent in time and more efficient than a large class
of numerical methods discussed in the literature for solving a general class of
delay Sobolev problems. Some numerical examples are carried out to confirm the
theory and demonstrate the applicability and validity of the developed
technique.

</details>


### [2] [Uncertainty Quantification in Forward Problems: Balancing Accuracy and Robustness Using CWENO Interpolations](https://arxiv.org/abs/2511.00005)
*Alina Chertock,Arsen S. Iskhakov,Alexander Kurganov*

Main category: math.NA

TL;DR: The paper proposes using CWENO7 scheme in stochastic collocation for uncertainty quantification, achieving non-oscillatory behavior near discontinuities while maintaining high-order accuracy in smooth regions.


<details>
  <summary>Details</summary>
Motivation: Traditional spectral methods like gPC suffer from Gibbs-type oscillations in nonsmooth settings, limiting their effectiveness for uncertainty quantification in problems with discontinuities.

Method: Incorporates seventh-order central weighted essentially non-oscillatory (CWENO7) scheme into stochastic collocation framework, using local stencils to balance accuracy and non-oscillatory behavior.

Result: CWENO7 interpolation provides accurate probability density functions, mean values, and standard deviations, outperforming gPC in nonsmooth regimes while remaining efficient and scalable.

Conclusion: CWENO7 interpolation is a reliable alternative to conventional stochastic collocation techniques for uncertainty quantification, particularly in the presence of discontinuities.

Abstract: In this paper, we study uncertainty quantification (UQ) in forward problems.
Our objective is to construct accurate and robust surrogate models by
incorporating the seventh-order central weighted essentially non-oscillatory
(CWENO7) scheme into the stochastic collocation framework. A key focus is on
mitigating the oscillatory behavior often encountered in traditional spectral
methods while retaining high-order accuracy in smooth regions. We present a
systematic comparison between CWENO7-based and generalized polynomial chaos
(gPC)-based approaches. Although gPC methods achieve spectral convergence, they
are prone to Gibbs-type oscillations in nonsmooth settings. By contrast, CWENO7
utilizes local stencils to achieve a balance: non-oscillatory behavior near
discontinuities and high-order convergence in smooth regions. To validate the
approach, we conduct numerical experiments on a range of one- and
two-dimensional smooth and nonsmooth problems, including shallow water
equations with random inputs. The results demonstrate that CWENO7 interpolation
provides accurate estimates of probability density functions, mean values, and
standard deviations, particularly in regimes where gPC expansions exhibit
strong oscillations. Furthermore, computational tests confirm that CWENO7
interpolation is efficient and scalable, establishing it as a reliable
alternative to conventional stochastic collocation techniques for UQ in the
presence of discontinuities.

</details>


### [3] [Numerical Study of Random Kelvin-Helmholtz Instability](https://arxiv.org/abs/2511.00008)
*Alina Chertock,Michael Herty,Arsen S. Iskhakov,Anna Iskhakova,Alexander Kurganov,Mária Lukáčová-Medvid'ová*

Main category: math.NA

TL;DR: The paper studies random dissipative weak solutions of compressible Euler equations in Kelvin-Helmholtz instability using statistical methods from turbulence theory to identify consistent solution features across different realizations and mesh resolutions.


<details>
  <summary>Details</summary>
Motivation: Weak entropy solutions are not unique and can be viewed as inviscid limits of Navier-Stokes flows, motivating a statistical approach to identify consistent features across different realizations.

Method: Uses stochastic collocation method with fifth-order A-WENO scheme and seventh-order CWENO interpolation, computes Cesàro averages over embedded uniform grids, and analyzes with Reynolds stress, energy defects, probability density functions, and proper orthogonal decomposition.

Result: Numerical experiments show that random KH instabilities can be systematically described using statistical methods, averaging, and reduced-order modeling.

Conclusion: Statistical methods provide a robust methodology for capturing the complex and chaotic dynamics of inviscid compressible flows in Kelvin-Helmholtz instability.

Abstract: In this paper, we study random dissipative weak solutions of the compressible
Euler equations in the Kelvin-Helmholtz (KH) instability. Motivated by the fact
that weak entropy solutions are not unique and can be viewed as inviscid limits
of Navier-Stokes flows, we take a statistical approach following ideas from
turbulence theory. Our aim is to identify solution features that remain
consistent across different realizations and mesh resolutions. For this
purpose, we compute stable numerical solutions using a stochastic collocation
method implemented with the help of a fifth-order alternative weighted
essentially non-oscillatory (A-WENO) scheme and seventh-order central weighted
essentially non-oscillatory (CWENO) interpolation in the random space. The
obtained solutions are averaged over several embedded uniform grids, resulting
in Ces\'aro averages, which are studied using stochastic tools. The analysis
includes Reynolds stress and energy defects, probability density functions of
averaged quantities, and reduced-order representations using proper orthogonal
decomposition. The presented numerical experiments illustrate that random KH
instabilities can be systematically described using statistical methods,
averaging, and reduced-order modeling, providing a robust methodology for
capturing the complex and chaotic dynamics of inviscid compressible flows.

</details>


### [4] [Matrix Phylogeny: Compact Spectral Fingerprints for Trap-Robust Preconditioner Selection](https://arxiv.org/abs/2511.00012)
*Jinwoo Baek*

Main category: math.NA

TL;DR: Matrix Phylogeny introduces compact spectral fingerprints (CSF/ASF) that characterize matrices at the family level using low-dimensional, eigendecomposition-free descriptors built from Chebyshev trace moments.


<details>
  <summary>Details</summary>
Motivation: To create scalable, structure-aware search and recommendation systems for large matrix repositories by developing compact, invariant fingerprints that avoid expensive eigendecompositions.

Method: Uses Chebyshev trace moments estimated by Hutchinson sketches, with affine rescaling to [-1,1] for permutation/similarity invariance and robustness to global scaling. CSF uses fixed dimension (K=3-5), while ASF adapts dimension on demand.

Result: Perfect clustering (ARI=1.0) on synthetic and real tests with only K=3-5 moments. Outperforms alternatives like eigenvalue histograms + Wasserstein while using far fewer features (K≤10 vs 64/9153). Stable to noise and enables near-oracle performance in preconditioner selection.

Conclusion: CSF/ASF deliver compact, fast, invariant fingerprints that enable scalable matrix analysis. Recommend CSF with K=5 by default, and ASF when domain-specific adaptivity is needed.

Abstract: Matrix Phylogeny introduces compact spectral fingerprints (CSF/ASF) that
characterize matrices at the family level. These fingerprints are
low-dimensional, eigendecomposition-free descriptors built from Chebyshev trace
moments estimated by Hutchinson sketches. A simple affine rescaling to [-1,1]
makes them permutation/similarity invariant and robust to global scaling.
  Across synthetic and real tests, we observe phylogenetic compactness: only a
few moments are needed. CSF with K=3-5 already yields perfect clustering
(ARI=1.0; silhouettes ~0.89) on four synthetic families and a five-family set
including BA vs ER, while ASF adapts the dimension on demand (median K*~9). On
a SuiteSparse mini-benchmark (Hutchinson p~100), both CSF-H and ASF-H reach
ARI=1.0. Against strong alternatives (eigenvalue histograms + Wasserstein,
heat-kernel traces, WL-subtree), CSF-K=5 matches or exceeds accuracy while
avoiding eigendecompositions and using far fewer features (K<=10 vs 64/9153).
  The descriptors are stable to noise (log-log slope ~1.03, R^2~0.993) and
support a practical trap->recommend pipeline for automated preconditioner
selection. In an adversarial E6+ setting with a probe-and-switch mechanism, our
physics-guided recommender attains near-oracle iteration counts (p90 regret=0),
whereas a Frobenius 1-NN baseline exhibits large spikes (p90~34-60).
  CSF/ASF deliver compact (K<=10), fast, invariant fingerprints that enable
scalable, structure-aware search and recommendation over large matrix
repositories. We recommend CSF with K=5 by default, and ASF when
domain-specific adaptivity is desired.

</details>


### [5] [Gamma convergence for a phase-field cohesive energy](https://arxiv.org/abs/2511.00016)
*Eleonora Maggiorelli,Matteo Negri,Francesco Vicentini,Laura De Lorenzis*

Main category: math.NA

TL;DR: A new cohesive phase-field fracture model that decouples strength from internal length, treats internal length as variational tool, and enables rigorous variational framework with Gamma-convergence analysis.


<details>
  <summary>Details</summary>
Motivation: Existing phase-field models fail to accurately capture experimentally observed strength surfaces under complex loading conditions, as strength is often misunderstood as a scalar rather than a surface in stress space.

Method: Introduces a new energy functional within cohesive phase-field framework with internal variable for inelastic response, decouples strength from internal length, uses finite element discrete formulation with strong damage localization.

Result: The model achieves Gamma-convergence to sharp cohesive fracture energy in 1D and 2D settings, exhibits coupling of all energy terms unlike classical models, and shows robustness in numerical simulations regarding mesh anisotropy.

Conclusion: The proposed model provides a rigorous variational framework for fracture modeling that accurately captures strength surfaces and offers both theoretical robustness and practical implementation advantages.

Abstract: Reproducing the key features of fracture behavior under multiaxial stress
states is essential for accurate modeling. Experimental evidence indicates that
three intrinsic material properties govern fracture nucleation in elastic
materials: elasticity, strength, and fracture toughness. Among these, strength
remains the most often misunderstood, as it is not a single scalar quantity but
rather a full surface in stress space. The flexibility in defining this
strength envelope in phase-field models poses significant challenges,
especially under complex loading conditions. Existing models in the literature
often fail to capture both the qualitative shape and the quantitative fit of
experimentally observed strength surfaces. To address this limitation, recent
work introduces a new energy functional within a cohesive phase-field
framework, specifically designed to control the shape of elastic domains. This
model introduces an internal variable to describe the inelastic response.
Notably, the strength is decoupled from the internal length, that is not
interpreted as a material length scale, as often done in literature, but rather
as a purely variational tool. The proposed functional allows for a rigorous
variational framework, enabling the use of tools from the calculus of
variations. We investigate the Gamma-convergence of the model to a sharp
cohesive fracture energy in the one- and two-dimensional (anti-plane) setting,
using a finite element discrete formulation and exploiting the strong
localization of the damage variable. Notably, unlike classical models where the
elastic and fracture energies converge independently, this model exhibits a
coupling of all energy terms. We also present numerical simulations exploring
the sensitivity of the model to mesh anisotropy, offering insight into both its
theoretical robustness and its practical implementation.

</details>


### [6] [Two-dimensional Gauss--Jacobi Quadrature for Multiscale Boltzmann Solvers](https://arxiv.org/abs/2511.00017)
*Shanshan Dong,Lu Wang,Xiangxiang Chen,Guanqing Wang*

Main category: math.NA

TL;DR: A new Gaussian quadrature scheme with parameterized weight function and polar coordinate transformation improves velocity space discretization for multiscale Boltzmann solvers, achieving 50x speed-up over conventional methods.


<details>
  <summary>Details</summary>
Motivation: Conventional velocity space discretization methods suffer from uneven node distribution and mismatch issues, limiting the performance of numerical simulations for multiscale Boltzmann solvers.

Method: Proposes a Gaussian quadrature scheme with parameterized weight function combined with polar coordinate transformation for flexible discretization of velocity space.

Result: Significantly improves accuracy while reducing computational cost, achieving up to 50 times speed-up compared to conventional Newton-Cotes quadrature under highly rarefied conditions.

Conclusion: The method offers an efficient tool with broad applicability for numerical simulations of rarefied and multiscale gas flows by effectively mitigating node mismatch problems.

Abstract: The discretization of velocity space plays a crucial role in the accuracy and
efficiency of multiscale Boltzmann solvers. Conventional velocity space
discretization methods suffer from uneven node distribution and mismatch
issues, limiting the performance of numerical simulations. To address this, a
Gaussian quadrature scheme with a parameterized weight function is proposed,
combined with a polar coordinate transformation for flexible discretization of
velocity space. This method effectively mitigates node mismatch problems
encountered in traditional approaches. Numerical results demonstrate that the
proposed scheme significantly improves accuracy while reducing computational
cost. Under highly rarefied conditions, the proposed method achieves a speed-up
of up to 50 times compared to the conventional Newton-Cotes quadrature,
offering an efficient tool with broad applicability for numerical simulations
of rarefied and multiscale gas flows.

</details>


### [7] [Branched Signature Model](https://arxiv.org/abs/2511.00018)
*Munawar Ali,Qi Feng*

Main category: math.NA

TL;DR: The paper introduces the branched signature model as a generalization of geometric rough paths, proves a universal approximation theorem, and shows how to construct extended paths whose geometric signatures equal branched signatures.


<details>
  <summary>Details</summary>
Motivation: To generalize classical geometric rough paths using the branched rough path framework and develop computational methods for branched signatures.

Method: Establishes universal approximation theorem for branched signature model, shows iterative composition of lower-level signatures approximates higher-level ones, and constructs explicit extension map Ψ to embed paths into higher-dimensional spaces.

Result: Demonstrates that branched signatures can be realized as classical geometric signatures of extended paths, providing efficient computational schemes.

Conclusion: The framework enables efficient computation of branched signatures and opens new possibilities for data-driven modeling and applications.

Abstract: In this paper, we introduce the branched signature model, motivated by the
branched rough path framework of [Gubinelli, Journal of Differential Equations,
248(4), 2010], which generalizes the classical geometric rough path. We
establish a universal approximation theorem for the branched signature model
and demonstrate that iterative compositions of lower-level signature maps can
approximate higher-level signatures. Furthermore, building on the existence of
the extension map proposed in [Hairer-Kelly. Annales de l'Institue Henri
Poincar\'e, Probabilit\'es et Statistiques 51, no. 1 (2015)], we show how to
explicitly construct the extension of the original paths into
higher-dimensional spaces via a map $\Psi$, so that the branched signature can
be realized as the classical geometric signature of the extended path. This
framework not only provides an efficient computational scheme for branched
signatures but also opens new avenues for data-driven modeling and
applications.

</details>


### [8] [On the Structure of Floating-Point Noise in Batch-Invariant GPU Matrix Multiplication](https://arxiv.org/abs/2511.00025)
*Tadisetty Sai Yashwanth*

Main category: math.NA

TL;DR: Floating-point non-associativity causes GPU matrix multiplication to be non-deterministic, but the error is structured and correlated rather than independent Gaussian noise as commonly assumed.


<details>
  <summary>Details</summary>
Motivation: To understand the statistical structure of numerical error in deep learning operations on GPUs, challenging the common assumption that floating-point errors behave as independent and identically distributed Gaussian noise.

Method: Compared outputs of single-input and batched matrix multiplications, conducted covariance analysis to examine error structure, and measured prediction flip rates to test the i.i.d. Gaussian noise assumption.

Result: Found 0.00% prediction flip rate (vs. non-zero predicted by i.i.d. model), discovered that nearly 50% of float16 error variance lies in off-diagonal terms, revealing structured and highly correlated error patterns.

Conclusion: Floating-point error behaves as coordinated directional perturbation rather than random static, challenging the stochastic view of numerical noise and providing foundation for analyzing deep learning reliability under hardware non-determinism.

Abstract: Floating-point non-associativity makes fundamental deep learning operations,
such as matrix multiplication (matmul) on GPUs, inherently non-deterministic.
Despite this, the statistical structure of the resulting numerical error
remains poorly understood. A common working assumption is that these errors
behave as independent and identically distributed (i.i.d.) Gaussian noise. In
this paper, we empirically test this assumption and show that it fails to
describe real GPU behavior. By comparing outputs of single-input and batched
matmuls, we find that while the i.i.d. model predicts non-zero output
instability, empirical results show a 0.00% prediction flip rate. Through
covariance analysis, we uncover the cause: the floating-point error is
structured and highly correlated. For float16, nearly 50% of the total error
variance lies in off-diagonal terms, revealing that the noise behaves as a
coordinated, directional perturbation rather than random static. This result
challenges the prevailing stochastic view of numerical noise and provides a
principled foundation for analyzing deep learning reliability under hardware
non-determinism.

</details>


### [9] [Convergence analysis for a tree-based nonlinear reduced basis method](https://arxiv.org/abs/2511.00226)
*Mohamed Barakat,Diane Guignard*

Main category: math.NA

TL;DR: A nonlinear reduced basis method using binary-tree partitioning of parameter domains with local RB spaces, achieving rigorous convergence analysis and outperforming existing methods in numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient nonlinear reduced basis method for parametrized elliptic PDEs that provides geometric control and rigorous convergence guarantees while maintaining low storage requirements.

Method: Binary-tree partition of parameter domain into tensor-product subdomains, each with local RB spaces constructed via greedy algorithm. Splitting strategy along longest edge ensures geometric control.

Result: Established explicit bounds on number of subdomains needed for given tolerance. Numerical experiments for diffusion and convection-diffusion problems confirm theoretical predictions and show improved performance over existing nonlinear RB methods.

Conclusion: The proposed approach achieves expected convergence rates, has low storage requirements, and outperforms existing nonlinear RB methods in several cases while providing rigorous theoretical guarantees.

Abstract: We develop and analyze a nonlinear reduced basis (RB) method for parametrized
elliptic partial differential equations based on a binary-tree partition of the
parameter domain into tensor-product structured subdomains. Each subdomain is
associated with a local RB space of prescribed dimension, constructed via a
greedy algorithm. A splitting strategy along the longest edge of the parameter
subdomains ensures geometric control of the subdomains and enables a rigorous
convergence analysis. Under the assumption that the parameter-to-solution map
admits a holomorphic extension and that the resulting domain partition is
quasi-uniform, we establish explicit bounds on the number of subdomains
required to achieve a given tolerance for arbitrary parameter domain dimension
and RB spaces size. Numerical experiments for diffusion and
convection-diffusion problems confirm the theoretical predictions,
demonstrating that the proposed approach, which has low storage requirements,
achieves the expected convergence rates and in several cases outperforms an
existing nonlinear RB method.

</details>


### [10] [Approximating Young Measures With Deep Neural Networks](https://arxiv.org/abs/2511.00233)
*Rayehe Karimi Mahabadi,Jianfeng Lu,Hossein Salahshoor*

Main category: math.NA

TL;DR: This paper develops a deep neural network framework for approximating Young measures by representing them as push-forwards of Gaussian measures and learning the push-forward maps using neural networks.


<details>
  <summary>Details</summary>
Motivation: Parametrized measures (Young measures) can reformulate non-convex variational problems as convex problems, but require effective approximation methods to benefit from this machinery.

Method: Represent Young measures as push-forwards of Gaussian measures, then approximate the push-forward maps using deep neural networks by encoding the variational problem in the loss function.

Result: The framework is demonstrated through several numerical examples showing successful approximation of Young measures.

Conclusion: This neural network approach provides a pathway for approximating Young measures in various applications including materials microstructure modeling and non-cooperative games.

Abstract: Parametrized measures (or Young measures) enable to reformulate non-convex
variational problems as convex problems at the cost of enlarging the search
space from space of functions to space of measures. To benefit from such
machinery, we need powerful tools for approximating measures. We develop a deep
neural network approximation of Young measures in this paper. The key idea is
to write the Young measure as push-forward of Gaussian measures, and
reformulate the problem of finding Young measures to finding the corresponding
push-forward. We approximate the push-forward map using deep neural networks by
encoding the reformulated variational problem in the loss function. After
developing the framework, we demonstrate the approach in several numerical
examples. We hope this framework and our illustrative computational experiments
provide a pathway for approximating Young measures in their wide range of
applications from modeling complex microstructure in materials to
non-cooperative games.

</details>


### [11] [An introduction to the a posteriori error analysis of parabolic partial differential equations](https://arxiv.org/abs/2511.00245)
*Iain Smears*

Main category: math.NA

TL;DR: Introduction to a posteriori error analysis for parabolic PDEs, focusing on challenges different from steady-state problems, using the heat equation as a model to study error norm choice and solution reconstruction impact on estimator efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the unique challenges in error analysis for time-dependent parabolic PDEs compared to steady-state problems, particularly examining how error norm selection and solution reconstruction affect estimator properties.

Method: Using the heat equation as a model problem to analyze the influence of error norm choice and discrete solution reconstruction on analytical properties of estimators.

Result: The study reveals that both the choice of error norm and the method of reconstructing discrete solutions significantly impact the efficiency and analytical properties of the resulting error estimators.

Conclusion: Proper selection of error norms and solution reconstruction methods is crucial for developing efficient a posteriori error estimators in parabolic PDE analysis, with distinct considerations from steady-state problems.

Abstract: This article provides a brief introduction to the a posteriori error analysis
of parabolic partial differential equations, with an emphasis on challenges
distinct from those of steady-state problems. Using the heat equation as a
model problem, we examine the crucial influence of the choice of error norm, as
well as the choice of notion of reconstruction of the discrete solution, on the
analytical properties of the resulting estimators, especially in terms of the
efficiency of the estimators.

</details>


### [12] [Accuracy and stability of the hyperbolic model time integration scheme revisited](https://arxiv.org/abs/2511.00557)
*Mikhail A. Botchev*

Main category: math.NA

TL;DR: The hyperbolic model (HM) time integration scheme adds a small artificial second-order time derivative term to solve parabolic problems. This paper revisits its accuracy and stability properties, showing that while the stability condition ensures eigenvalues are bounded, the norm of the amplification matrix may exceed one, potentially corrupting convergence.


<details>
  <summary>Details</summary>
Motivation: To revisit and clarify the accuracy and stability properties of the hyperbolic model (HM) time integration scheme, which was originally described by Samarskii and later reappeared as the generalized Du Fort-Frankel scheme.

Method: Analysis of the HM scheme's stability conditions, comparing Samarskii's operator inequality approach with eigenvalue analysis of the amplification matrix, and investigating potential norm growth issues.

Result: The stability condition based on operator inequalities coincides with requiring eigenvalues of the amplification matrix to be smaller than one in absolute value. However, the norm of this matrix may still exceed one under this condition, potentially corrupting convergence.

Conclusion: The paper discusses whether the potential stability deficiency (norm growth despite bounded eigenvalues) can be detected and mitigated in practical applications of the HM scheme.

Abstract: The hyperbolic model (HM) time integration scheme tackles parabolic problems
by adding a small artificial second order time derivative term. Described by
Samarskii in his 1971 book, the scheme reappeared as the generalized Du
Fort-Frankel scheme in a 1976 paper by Gottlieb and Gustafsson. In this note we
revisit accuracy and stability properties of the scheme. In particular, we show
that the stability condition, formulated by Samarskii based on operator
inequalities, coincides with the requirement that the eigenvalues of the
amplification matrix (the stability function operator) are smaller than one in
absolute value. However, under this condition, the norm of this matrix may
exceed one and this, as recently pointed out by Corem and Ditkowski (2012), may
corrupt convergence of the scheme. Hence, we also discuss whether this eventual
stability lack can be detected and mitigated in practice.

</details>


### [13] [Learning and Leveraging Anisotropy Parameters in ANOVA Approximation](https://arxiv.org/abs/2511.00251)
*Felix Bartel,Pascal Schröter*

Main category: math.NA

TL;DR: Fourier-based method for high-dimensional function approximation using truncated ANOVA decomposition to learn anisotropic smoothness and optimize frequency boxes for efficient computation with NFFT.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient approach for high-dimensional function approximation that can handle scattered data and incorporate anisotropic smoothness properties to improve accuracy.

Method: Uses truncated ANOVA decomposition to analyze anisotropic smoothness, then employs least squares approximation with trigonometric polynomials and optimized frequency boxes that enable NFFT acceleration.

Result: The method efficiently optimizes dozens of parameters to achieve high approximation accuracy with minimal computational overhead, as demonstrated in numerical experiments.

Conclusion: The proposed Fourier-based approach with optimized frequency boxes and NFFT acceleration provides an effective and efficient solution for high-dimensional function approximation from scattered data.

Abstract: We present a Fourier-based approach for high-dimensional function
approximation. To this end, we analyze the truncated ANOVA (analysis of
variance) decomposition and learn the anisotropic smoothness properties of the
target function from scattered data. This smoothness information is then
incorporated into our approximation algorithm to improve the accuracy.
Specifically, we employ least squares approximation using trigonometric
polynomials in combination with frequency boxes of optimized aspect ratios.
These frequency boxes allow for the application of the Nonequispaced Fast
Fourier Transform (NFFT), which significantly accelerates the computation of
the method. Our approach enables the efficient optimization of dozens of
parameters to achieve high approximation accuracy with minimal overhead.
Numerical experiments demonstrate the practical effectiveness of the proposed
method.

</details>


### [14] [Numerically stable evaluation of closed-form expressions for eigenvalues of $3 \times 3$ matrices](https://arxiv.org/abs/2511.00292)
*Michal Habera,Andreas Zilian*

Main category: math.NA

TL;DR: Numerically stable closed-form evaluation of eigenvalues for real diagonalizable 3×3 matrices using four invariants (trace, deviatoric invariants, discriminant) with tight error bounds and performance improvements over LAPACK.


<details>
  <summary>Details</summary>
Motivation: Traditional trigonometric formulas for 3×3 matrix eigenvalues based on Cardano's and Viète's work are numerically unstable for matrices with repeated eigenvalues, motivating the need for stable alternatives.

Method: Uses four invariants: trace I₁, deviatoric invariants J₂ and J₃, and discriminant Δ. Proposes algorithms for computing these invariants with proven accuracy, particularly focusing on J₂ computation.

Result: Derived tight forward error bounds for the invariants. Benchmarking shows the new algorithms have errors within forward stability bounds for matrices with well-conditioned eigenbases. The method is approximately 10x faster than LAPACK for challenging cases while maintaining comparable accuracy.

Conclusion: The proposed approach provides numerically stable eigenvalue computation for 3×3 real diagonalizable matrices using invariants, with proven error bounds and significant performance improvements over existing methods.

Abstract: Trigonometric formulas for eigenvalues of $3 \times 3$ matrices that build on
Cardano's and Vi\`ete's work on algebraic solutions of the cubic are
numerically unstable for matrices with repeated eigenvalues. This work presents
numerically stable, closed-form evaluation of eigenvalues of real,
diagonalizable $3 \times 3$ matrices via four invariants: the trace $I_1$, the
deviatoric invariants $J_2$ and $J_3$, and the discriminant $\Delta$. We
analyze the conditioning of these invariants and derive tight forward error
bounds. For $J_2$ we propose an algorithm and prove its accuracy. We benchmark
all invariants and the resulting eigenvalue formulas, relating observed forward
errors to the derived bounds. In particular, we show that, for the special case
of matrices with a well-conditioned eigenbasis, the newly proposed algorithms
have errors within the forward stability bounds. Performance benchmarks show
that the proposed algorithm is approximately ten times faster than the highly
optimized LAPACK library for a challenging test case, while maintaining
comparable accuracy.

</details>


### [15] [A computational inverse random source problem for elastic waves](https://arxiv.org/abs/2511.00367)
*Hao Gu,Tianjiao Wang,Xiang Xu,Yue Zhao*

Main category: math.NA

TL;DR: A non-iterative method for reconstructing random source variance in 3D elastic waves using single-frequency correlation measurements, significantly reducing computational cost compared to multi-frequency approaches.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse random source problem for elastic waves more efficiently by avoiding iterative multi-frequency approaches that are computationally expensive.

Method: Proposes a novel computational method that reconstructs the variance of random sources from correlation boundary measurements using only single-frequency data, making it non-iterative.

Result: The method significantly reduces computational cost while maintaining effectiveness, with rigorous error analysis providing quantitative error estimates. Numerical examples demonstrate its practical effectiveness.

Conclusion: The proposed single-frequency non-iterative method is computationally efficient and effective for reconstructing random source variance in elastic waves, with potential direct application to stochastic Maxwell equations.

Abstract: This paper investigates the inverse random source problem for elastic waves
in three dimensions, where the source is assumed to be driven by an additive
white noise. A novel computational method is proposed for reconstructing the
variance of the random source from the correlation boundary measurement of the
wave field. Compared with existing multi-frequency iterative approaches, our
method is non-iterative and requires data at only a single frequency. As a
result, the computational cost is significantly reduced. Furthermore, rigorous
error analysis is conducted for the proposed method, which gives a quantitative
error estimate. Numerical examples are presented to demonstrate effectiveness
of the proposed method. Moreover, this method can to be directly applied to
stochastic Maxwell equations.

</details>


### [16] [Trust-Region Methods with Low-Fidelity Objective Models](https://arxiv.org/abs/2511.00434)
*Andrea Angino,Matteo Aurina,Alena Kopaničáková,Matthias Voigt,Marco Donatelli,Rolf Krause*

Main category: math.NA

TL;DR: Two multifidelity trust-region methods (STR and SVDTR) are introduced that extend the Magical Trust Region framework by using low-fidelity models to determine secondary directions, improving computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance the classical trust-region optimization framework by incorporating informative secondary directions derived from low-fidelity models, potentially improving computational efficiency.

Method: STR uses sketched matrices to reduce dimensionality of trust-region subproblems, while SVDTR employs truncated singular value decomposition to capture leading variability directions from datasets.

Result: Numerical examples demonstrate potential efficiency gains compared to traditional trust-region methods.

Conclusion: The proposed multifidelity trust-region methods successfully extend the MTR framework and show promise for improved computational efficiency in optimization problems.

Abstract: We introduce two multifidelity trust-region methods based on the Magical
Trust Region (MTR) framework. MTR augments the classical trust-region step with
a secondary, informative direction. In our approaches, the secondary
``magical'' directions are determined by solving coarse trust-region
subproblems based on low-fidelity objective models. The first proposed method,
Sketched Trust-Region (STR), constructs this secondary direction using a
sketched matrix to reduce the dimensionality of the trust-region subproblem.
The second method, SVD Trust-Region (SVDTR), defines the magical direction via
a truncated singular value decomposition of the dataset, capturing the leading
directions of variability. Several numerical examples illustrate the potential
gain in efficiency.

</details>


### [17] [Three-dimensional narrow volume reconstruction method with unconditional stability based on a phase-field Lagrange multiplier approach](https://arxiv.org/abs/2511.00508)
*Renjun Gao,Xiangjie Kong,Dongting Cai,Boyi Fu,Junxiang Yang*

Main category: math.NA

TL;DR: An effective algorithm for 3D object reconstruction from point clouds using an Allen-Cahn-type model with Lagrange multiplier approach, featuring energy stability and unconditional stability.


<details>
  <summary>Details</summary>
Motivation: Reconstruction from point clouds is essential in prosthetics, medical imaging, and computer vision, requiring stable and accurate methods for complex 3D shapes.

Method: Uses Allen-Cahn-type model with Lagrange multiplier technique, Crank-Nicolson time discretization, finite difference spatial approximation, and edge detection function from unsigned distance function for energy stability.

Result: Algorithm demonstrates accuracy, stability, and effectiveness in reconstructing complex 3D volumes including Star Wars characters, with parameter analysis showing control over detail level.

Conclusion: The proposed method provides a stable and effective approach for 3D reconstruction from scattered point data, with computational codes made available for reproducibility.

Abstract: Reconstruction of an object from points cloud is essential in prosthetics,
medical imaging, computer vision, etc. We present an effective algorithm for an
Allen--Cahn-type model of reconstruction, employing the Lagrange multiplier
approach. Utilizing scattered data points from an object, we reconstruct a
narrow shell by solving the governing equation enhanced with an edge detection
function derived from the unsigned distance function. The specifically designed
edge detection function ensures the energy stability. By reformulating the
governing equation through the Lagrange multiplier technique and implementing a
Crank--Nicolson time discretization, we can update the solutions in a stable
and decoupled manner. The spatial operations are approximated using the finite
difference method, and we analytically demonstrate the unconditional stability
of the fully discrete scheme. Comprehensive numerical experiments, including
reconstructions of complex 3D volumes such as characters from \textit{Star
Wars}, validate the algorithm's accuracy, stability, and effectiveness.
Additionally, we analyze how specific parameter selections influence the level
of detail and refinement in the reconstructed volumes. To facilitate the
interested readers to understand our algorithm, we share the computational
codes and data in https://github.com/cfdyang521/C-3PO/tree/main.

</details>


### [18] [Filtered Neural Galerkin model reduction schemes for efficient propagation of initial condition uncertainties in digital twins](https://arxiv.org/abs/2511.00670)
*Zhiyang Ning,Benjamin Peherstorfer*

Main category: math.NA

TL;DR: A filtered Neural Galerkin scheme for efficient uncertainty quantification in digital twins that advances mean and covariance dynamics instead of propagating costly ensembles.


<details>
  <summary>Details</summary>
Motivation: Ensemble-based uncertainty quantification becomes prohibitively expensive in digital twin control and data assimilation loops, even with reduced models.

Method: Moment closure approach using Neural Galerkin schemes on pre-trained neural networks to advance mean and covariance of reduced solution distribution, eliminating ensemble propagation.

Result: Achieves more than one order of magnitude speedup compared to ensemble-based uncertainty propagation in numerical experiments.

Conclusion: Filtered Neural Galerkin schemes provide efficient uncertainty quantification for digital twins by avoiding costly ensemble propagation while maintaining accuracy.

Abstract: Uncertainty quantification in digital twins is critical to enable reliable
and credible predictions beyond available data. A key challenge is that
ensemble-based approaches can become prohibitively expensive when embedded in
control and data assimilation loops in digital twins, even when reduced models
are used. We introduce a reduced modeling approach that advances in time the
mean and covariance of the reduced solution distribution induced by the initial
condition uncertainties, which eliminates the need to maintain and propagate a
costly ensemble of reduced solutions. The mean and covariance dynamics are
obtained as a moment closure from Neural Galerkin schemes on pre-trained neural
networks, which can be interpreted as filtered Neural Galerkin dynamics
analogous to Gaussian filtering and the extended Kalman filter. Numerical
experiments demonstrate that filtered Neural Galerkin schemes achieve more than
one order of magnitude speedup compared to ensemble-based uncertainty
propagation.

</details>


### [19] [Towards a Multigrid Preconditioner Interpretation of Hierarchical Poincaré-Steklov Solvers](https://arxiv.org/abs/2511.00735)
*J. P. Lucero Lorca*

Main category: math.NA

TL;DR: The paper reinterprets the Hierarchical Poincaré-Steklov (HPS) method as a multigrid preconditioner, unifying direct and iterative formulations for elliptic boundary-value problems.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between direct and iterative formulations of HPS, connecting it to multigrid domain decomposition while preserving spectral accuracy.

Method: Reinterpret the hierarchical merge structure of HPS as a natural multigrid preconditioner, building on iterative variants and combining nested dissection with tensor-product spectral element discretizations.

Result: Numerical experiments in 2D demonstrate the performance and convergence behavior of the proposed approach, showing it preserves high accuracy of spectral discretizations.

Conclusion: The unified formulation enables flexible iterative solution strategies while maintaining the accuracy benefits of spectral methods, connecting HPS to multigrid domain decomposition.

Abstract: We revisit the Hierarchical Poincar\'e--Steklov (HPS) method within a
preconditioned iterative framework. Originally introduced as a direct solver
for elliptic boundary-value problems, the HPS method combines nested dissection
with tensor-product spectral element discretizations, even though it has been
shown in other contexts[8]. Building on the iterative variant proposed in[1],
we reinterpret the hierarchical merge structure of HPS as a natural multigrid
preconditioner. This perspective unifies direct and iterative formulations of
HPS connecting it to multigrid domain decomposition. The resulting formulation
preserves the high accuracy of spectral discretizations while enabling flexible
iterative solution strategies. Numerical experiments in two dimensions
demonstrate the performance and convergence behavior of the proposed approach.

</details>


### [20] [Generalized singular value decompositions of dual quaternion matrices and beyond](https://arxiv.org/abs/2511.00761)
*Sitao Ling,Wenxuan Ma,Musheng Wei*

Main category: math.NA

TL;DR: The paper proposes several types of generalized singular value decomposition (GSVD) for dual quaternion matrices, including quotient-type SVD (DQGSVD), canonical correlation decomposition (DQCCD), and product-type SVD (DQPSVD), along with QR decomposition and CS decomposition for dual quaternion matrices.


<details>
  <summary>Details</summary>
Motivation: GSVD of dual quaternion matrix pairs is essential for high-dimensional data processing and dual quaternion statistics, providing elegant problem formulation and numerical implementation tools.

Method: Building on existing SVD of dual quaternion matrices, the authors develop different GSVD types based on matrix dimensions: DQGSVD for matrices with same columns, DQCCD for matrices with same rows, and DQPSVD for multiplication-consistent matrices. Also studies QR decomposition via Householder transformation and CS decomposition for 2x2 blocked unitary matrices.

Result: The paper presents explicit formulations of various GSVD types for dual quaternion matrices and illustrates DQGSVD with three artificial examples. The decompositions are shown to be distinct from those in real, complex, and quaternion fields due to dual part peculiarities.

Conclusion: The proposed GSVD types for dual quaternion matrices represent extensions of classical decompositions to the dual quaternion domain, providing essential tools for dual quaternion-based data analysis and processing.

Abstract: In high-dimensional data processing and data analysis related to dual
quaternion statistics, generalized singular value decomposition (GSVD) of a
dual quaternion matrix pair is an essential numerical linear algebra tool for
an elegant problem formulation and numerical implementation. In this paper,
building upon the existing singular value decomposition (SVD) of a dual
quaternion matrix, we put forward several types of GSVD of dual quaternion data
matrices in accordance with their dimensions. Explicitly, for a given dual
quaternion matrix pair $\{{\boldsymbol A}, {\boldsymbol B}\}$, if ${\boldsymbol
A}$ and ${\boldsymbol B}$ have the same number of columns, we investigate two
forms of their quotient-type SVD (DQGSVD) through different strategies, which
can be selected to use in different scenarios. Three artificial examples are
presented to illustrate the principle of the DQGSVD.
  Alternatively, if ${\boldsymbol A}$ and ${\boldsymbol B}$ have the same
number of rows, we consider their canonical correlation decomposition (DQCCD).
If ${\boldsymbol A}$ and ${\boldsymbol B}$ are consistent for dual quaternion
matrix multiplication, we present their product-type SVD (DQPSVD). As a
preparation, we also study the QR decomposition of a dual quaternion matrix
based on the dual quaternion Householder transformation, and introduce the CS
decomposition of an 2-by-2 blocked unitary dual quaternion matrix. Due to the
peculiarity of containing dual part for dual quaternion matrices, the obtained
series of GSVD of dual quaternion matrices dramatically distinguish from those
in the real number field, the complex number field, and even the quaternion
ring, but can be treated as an extension of them to some extent.

</details>


### [21] [HEATNETs: Explainable Random Feature Neural Networks for High-Dimensional Parabolic PDEs](https://arxiv.org/abs/2511.00886)
*Kyriakos Georgiou,Gianluca Fabiani,Constantinos Siettos,Athanasios N. Yannacopoulos*

Main category: math.NA

TL;DR: HEATNET is a single-hidden layer neural network using randomized heat-kernels as universal approximator for high-dimensional parabolic PDEs, achieving O(N^{-1/2}) convergence rate with remarkable accuracy up to 2,000 dimensions.


<details>
  <summary>Details</summary>
Motivation: To develop an explainable and efficient numerical method for solving high-dimensional parabolic PDEs by leveraging analytical framework of parabolic PDEs and insights from physics-informed neural networks.

Method: Uses random feature neural networks with heat-kernels from fundamental solutions, employs importance Monte Carlo sampling to handle kernel singularities, and scales up through suitable transformations for high-dimensional problems.

Result: Achieves approximation errors of 1.0E-05 to 1.0E-07 for problems up to 500 dimensions, and 1.0E-04 to 1.0E-03 for 1,000-2,000 dimensions with only up to 15,000 features.

Conclusion: HEATNET provides an unbiased universal approximator for high-dimensional parabolic PDEs with explainable structure and remarkable accuracy, making it suitable for practical applications in very high dimensions.

Abstract: We deal with the solution of the forward problem for high-dimensional
parabolic PDEs with random feature (projection) neural networks (RFNNs). We
first prove that there exists a single-hidden layer neural network with
randomized heat-kernels arising from the fundamental solution (Green's
functions) of the heat operator, that we call HEATNET, that provides an
unbiased universal approximator to the solution of parabolic PDEs in arbitrary
(high) dimensions, with the rate of convergence being analogous to the
${O}(N^{-1/2})$, where $N$ is the size of HEATNET. Thus, HEATNETs are
explainable schemes, based on the analytical framework of parabolic PDEs,
exploiting insights from physics-informed neural networks aided by numerical
and functional analysis, and the structure of the corresponding solution
operators. Importantly, we show how HEATNETs can be scaled up for the efficient
numerical solution of arbitrary high-dimensional parabolic PDEs using suitable
transformations and importance Monte Carlo sampling of the integral
representation of the solution, in order to deal with the singularities of the
heat kernel around the collocation points. We evaluate the performance of
HEATNETs through benchmark linear parabolic problems up to 2,000 dimensions. We
show that HEATNETs result in remarkable accuracy with the order of the
approximation error ranging from $1.0E-05$ to $1.0E-07$ for problems up to 500
dimensions, and of the order of $1.0E-04$ to $1.0E-03$ for 1,000 to 2,000
dimensions, with a relatively low number (up to 15,000) of features.

</details>


### [22] [Convergence analysis for a finite volume evolution Galerkin method for multidimensional hyperbolic systems](https://arxiv.org/abs/2511.00957)
*Mária Lukáčová-Medvidová,Zhuyan Tang,Yuhuan Yuan*

Main category: math.NA

TL;DR: Convergence analysis of a finite volume method using bicharacteristics for hyperbolic conservation laws, focusing on linear wave equations and nonlinear Euler equations.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze a numerical method for solving multidimensional hyperbolic conservation laws, which are fundamental in fluid dynamics and wave propagation problems.

Method: Finite volume method based on the method of bicharacteristics, with stability and consistency analysis using the generalized Lax equivalence principle.

Result: Proved stability and consistency of numerical approximations, and demonstrated convergence of numerical solutions to strong solutions on the lifespan.

Conclusion: The proposed finite volume method using bicharacteristics is effective for solving both linear wave equations and nonlinear Euler equations, with proven convergence properties.

Abstract: We study the convergence of a finite volume method based on the method of
bicharacteristics for multidimensional hyperbolic conservation laws. In
particular, we concentrate on the linear wave equation system and nonlinear
Euler equations of gas dynamics. We show the stability and the consistency of
the numerical approximations. By means of the generalized Lax equivalence
principle we prove the convergence of numerical solutions to the strong
solution on the lifespan.

</details>


### [23] [A Stable Loosely-Coupled Dirichlet-Neumann Scheme for Fluid-Structure Interaction with Large Added Mass](https://arxiv.org/abs/2511.01035)
*Francesca Renzi,Christian Vergara*

Main category: math.NA

TL;DR: A new strongly-coupled partitioning strategy for fluid-structure interaction problems with similar densities, which enables a stable loosely-coupled scheme for large added mass regimes.


<details>
  <summary>Details</summary>
Motivation: Solving FSI problems with similar densities (large added mass) is challenging due to stability and convergence issues, particularly limiting the applicability of computationally efficient loosely coupled approaches.

Method: Developed a new strongly-coupled partitioning strategy with Dirichlet and Neumann interface conditions, then derived a stable loosely-coupled scheme by performing a single iteration per time step.

Result: The proposed loosely-coupled scheme is conditionally stable in large added mass regimes under parameter constraints, with numerical experiments confirming theoretical stability.

Conclusion: The new schemes provide effective and applicable solutions for FSI problems in challenging large added mass settings, overcoming stability limitations of traditional methods.

Abstract: Solving fluid-structure interaction (FSI) problems when the densities are
similar (large added mass), such as in hemodynamics, is challenging since the
stability and convergence of the adopted numerical scheme could be compromised.
In particular, while loosely coupled (LC) partitioned approaches are appealing
due to their computational efficiency, the stability issues arising in high
added mass regimes limit their applicability.
  In this work, we present a new strongly-coupled (SC) partitioning strategy
for the solution of the FSI problem, from which we derive a stable LC scheme
based on Dirichlet and Neumann interface conditions. We analyse the convergence
of the new SC scheme on a benchmark problem, demonstrating enhanced behaviour
over the standard DN method for specific ranges of a parameter $\alpha$,
without additional relaxation. Building on this, we introduce a new LC scheme
by performing a single iteration per time step. Stability analysis on a
benchmark problem proves that the proposed LC scheme is conditionally stable in
large added mass regimes, under a constraint on a parameter $\alpha$.
  Numerical experiments in large added mass settings confirm the theoretical
results, demonstrating the effectiveness and applicability of the proposed
schemes.

</details>


### [24] [A parallel-in-time Newton's method-based ODE solver](https://arxiv.org/abs/2511.01465)
*Casian Iacob,Hassan Razavi,Simo Särkkä*

Main category: math.NA

TL;DR: A novel parallel-in-time solver for nonlinear ODEs using Newton's method and parallel prefix sums for logarithmic span complexity.


<details>
  <summary>Details</summary>
Motivation: To improve runtime performance over existing Parareal method for solving nonlinear ordinary differential equations.

Method: Formulate ODE solution as root-finding problem solved with Newton's method, parallelize affine recursive operations using parallel prefix sums (scan operations).

Result: Achieves logarithmic span complexity and demonstrates computational advantage through numerical simulations of various ODE systems.

Conclusion: The proposed method yields improved runtime compared to Parareal method for solving nonlinear ODEs.

Abstract: In this article, we introduce a novel parallel-in-time solver for nonlinear
ordinary differential equations (ODEs). We state the numerical solution of an
ODE as a root-finding problem that we solve using Newton's method. The affine
recursive operations arising in Newton's step are parallelized in time by using
parallel prefix sums, that is, parallel scan operations, which leads to a
logarithmic span complexity. This yields an improved runtime compared to the
previously proposed Parareal method. We demonstrate the computational advantage
through numerical simulations of various systems of ODEs.

</details>


### [25] [SCOUT: Semi-Lagrangian COnservative and Unconditionally sTable schemes for nonlinear advection-diffusion problems](https://arxiv.org/abs/2511.01477)
*Silvia Preda,Walter Boscheri,Matteo Semplice,Maurizio Tavelli*

Main category: math.NA

TL;DR: A new conservative semi-Lagrangian finite difference scheme for nonlinear advection-diffusion problems that integrates equations along characteristic curves, ensuring conservation and unconditional stability even with high CFL numbers.


<details>
  <summary>Details</summary>
Motivation: To develop a fully conservative and unconditionally stable numerical scheme for nonlinear advection-diffusion problems, addressing the fundamental need for physically consistent solutions through proper conservation properties.

Method: Space-time control volume integration along characteristic curves using Gauss theorem, with nonlinear equation solving for characteristic foot location in nonlinear cases. Novel characteristic-based Crank-Nicolson discretization that implicitly evaluates diffusion at the characteristic foot.

Result: The scheme achieves full conservation and unconditional stability, verified with CFL numbers up to 100. Benchmark tests demonstrate accuracy, robustness, and strict conservation properties.

Conclusion: The proposed semi-Lagrangian scheme successfully provides a conservative, unconditionally stable framework for nonlinear advection-diffusion problems, with diffusion terms directly incorporated for the first time in such a conservative framework.

Abstract: In this work, we propose a new semi-Lagrangian (SL) finite difference scheme
for nonlinear advection-diffusion problems. To ensure conservation, which is
fundamental for achieving physically consistent solutions, the governing
equations are integrated over a space-time control volume constructed along the
characteristic curves originating from each computational point. By applying
Gauss theorem, all space-time surface integrals can be evaluated. For nonlinear
problems, a nonlinear equation must be solved to find the foot of the
characteristic, while this is not needed in linear cases. This formulation
yields SL schemes that are fully conservative and unconditionally stable, as
verified by numerical experiments with CFL numbers up to 100. Moreover, the
diffusion terms are, for the first time, directly incorporated within a
conservative semi-Lagrangian framework, leading to the development of a novel
characteristic-based Crank-Nicolson discretization in which the diffusion
contribution is implicitly evaluated at the foot of the characteristic. A broad
set of benchmark tests demonstrates the accuracy, robustness, and strict
conservation property of the proposed method, as well as its unconditional
stability.

</details>


### [26] [Enhancing Non-Terrestrial Network Performance with Free Space Optical Links and Intelligent Reflecting Surfaces](https://arxiv.org/abs/2511.01484)
*Shunyuan Shang,Emna Zedini,Mohamed-Slim Alouini*

Main category: math.NA

TL;DR: Performance analysis of a non-terrestrial network architecture using HAP stations and IRS for optical-to-RF communication with detailed channel modeling and performance metrics.


<details>
  <summary>Details</summary>
Motivation: To address increasing connectivity demands in post-5G era by integrating non-terrestrial networks (NTNs) with HAP stations and intelligent reflecting surfaces (IRS) for enhanced communication infrastructure.

Method: Modeled RF links using Shadowed Rician and generalized Nakagami-m models, FSO link with Gamma-Gamma distribution and generalized pointing errors. Used mixture Gamma model to approximate non-centered chi-square distribution and analyzed performance using bivariate Fox-H function.

Result: Derived analytical expressions for ergodic capacity, average BER, and outage probability. Provided asymptotic expressions for high SNR scenarios. Validation showed exact match between analytical results and Monte-Carlo simulations.

Conclusion: The proposed NTN architecture with HAP relay and IRS nodes provides viable solution for optical-to-RF communication with comprehensive performance analysis validated through simulations.

Abstract: The integration of non-terrestrial networks (NTNs), which include high
altitude platform (HAP) stations and intelligent reflecting surfaces (IRS) into
communication infrastructures has become a crucial area of research to address
the increasing requirements for connectivity and performance in the post-5G
era. This paper presents a comprehensive performance study of a new NTN
architecture, which enables communication from the optical ground station (OGS)
to end users through the utilization of HAP and terrestrial IRS nodes. In this
configuration, the HAP acts as an amplify-and-forward (AF) relay terminal
between the free-space optical (FSO) link and the RF links.
  Specifically, the RF links are modeled using the Shadowed Rician and the
generalized Nakagami-$m$ models, where the FSO link is characterized by the
Gamma-Gamma distribution with generalized pointing errors. The FSO system
operates under either intensity modulation with direct detection or heterodyne
detection. Using the mixture Gamma model, we approximate the non-centered
chi-square distribution that describes the total fading of the RF link, and we
assess the performance of the end-to-end system by analyzing the ergodic
capacity, the average bit-error rate (BER), and the outage probability,
calculated using the bivariate Fox-H function. We also provide simple
asymptotic expressions for the average BER and the outage probability at high
signal-to-noise ratio (SNR). Finally, the proposed analysis is validated with
numerical and Monte-Carlo simulation results, showing an exact match.

</details>


### [27] [Optical Intelligent Reflecting Surfaces Empowering Non-Terrestrial Communications](https://arxiv.org/abs/2511.01488)
*Shunyuan Shang,Emna Zedini,Abla Kammoun,Mohamed-Slim Alouini*

Main category: math.NA

TL;DR: A system combining high-altitude platforms and optical intelligent reflecting surfaces to overcome line-of-sight challenges in urban environments, with analytical models for performance metrics.


<details>
  <summary>Details</summary>
Motivation: To address line-of-sight challenges in urban environments where obstacles like buildings and trees block direct communication paths, improving connectivity for non-line-of-sight users.

Method: A three-hop system using optical ground station, HAP as amplify-and-forward relay, and OIRS to redirect signals. Developed analytical models for channel impairments and derived closed-form expressions for performance metrics.

Result: Derived accurate approximation for Hoyt-distributed geometric and misalignment losses, obtained closed-form expressions for outage probability, average bit error rate, channel capacity, and end-to-end SNR. Also provided asymptotic expressions for high-SNR regimes.

Conclusion: The proposed HAP-OIRS system effectively addresses NLOS challenges in urban environments with comprehensive analytical performance characterization, enabling calculation of diversity order and reliable connectivity.

Abstract: In this work, we propose an innovative system that combines high-altitude
platforms (HAPs) and optical intelligent reflecting surfaces (OIRS) to address
line-of-sight (LOS) challenges in urban environments. Our three-hops system
setup includes an optical ground station (OGS), a HAP, an OIRS, and a user.
Signals are transmitted from the OGS to the HAP via a free space optical (FSO)
link, with the HAP functioning as an amplify-and-forward (AF) relay that
redirects signals through an OIRS, effectively bypassing obstacles such as
buildings and trees to improve connectivity for non-line-of-sight (NLOS) User.
For the OIRS link, we address key channel impairments, including atmospheric
turbulence, pointing errors, attenuation, and geometric and misalignment losses
(GML). An accurate approximation for the Hoyt-distributed GML model is derived,
enabling us to obtain closed-form expressions for outage probability (OP) and
various performance metrics, such as average bit error rate (BER) and channel
capacity of the OIRS-assisted FSO link. Furthermore, we analyze the end-to-end
signal-to-noise ratio (SNR) and derive closed-form expressions for OP and
performance metrics. Asymptotic expressions are provided for high-SNR regimes,
allowing the system's diversity order to be calculated.

</details>


### [28] [On the optimality of dimension truncation error rates for a class of parametric partial differential equations](https://arxiv.org/abs/2511.01492)
*Philipp A. Guth,Vesa Kaarnioja*

Main category: math.NA

TL;DR: Analysis of sharpness in dimension truncation error bounds for parametric PDEs with lognormal random field inputs.


<details>
  <summary>Details</summary>
Motivation: Existing literature lacks analysis on sharpness of dimension truncation error bounds when infinite-dimensional random fields are approximated by finite-dimensional ones in uncertainty quantification.

Method: Investigation of two model problems where existing dimension truncation error rates can be proven to be sharp.

Result: Demonstrates that current dimension truncation error bounds are indeed sharp for the studied model problems.

Conclusion: The work provides rigorous verification that existing error bounds for dimension truncation in parametric PDEs with lognormal fields cannot be improved beyond current rates.

Abstract: In uncertainty quantification for parametric partial differential equations
(PDEs), it is common to model uncertain random field inputs using countably
infinite sequences of independent and identically distributed random variables.
The lognormal random field is a prime example of such a model. While there have
been many studies assessing the error in the PDE response that occurs when an
infinite-dimensional random field input is replaced with a finite-dimensional
random field, there do not seem to be any analyses in the existing literature
discussing the sharpness of these bounds. This work seeks to remedy the
situation. Specifically, we investigate two model problems where the existing
dimension truncation error rates can be shown to be sharp.

</details>


### [29] [Numerically Efficient and Stable Algorithms for Kernel-Based Regularized System Identification Using Givens-Vector Representation](https://arxiv.org/abs/2511.01534)
*Zhuohua Shen,Junpeng Zhang,Martin S. Andersen,Tianshi Chen*

Main category: math.NA

TL;DR: Proposes numerically stable algorithms for kernel-based system identification using Givens-vector representation instead of generator representation to overcome numerical instability issues.


<details>
  <summary>Details</summary>
Motivation: Existing kernel-based regularized system identification algorithms using generator representation are numerically unstable, limiting practical applications.

Method: Derives and exploits Givens-vector representation of kernel matrices to develop new algorithms that maintain computational efficiency while improving numerical stability.

Result: Monte Carlo simulations show proposed algorithms have same computational complexity as state-of-the-art methods but without numerical stability issues, yielding more accurate results.

Conclusion: Givens-vector representation provides a numerically stable alternative to generator representation for kernel-based system identification while maintaining computational efficiency.

Abstract: Numerically efficient and stable algorithms are essential for kernel-based
regularized system identification. The state of art algorithms exploit the
semiseparable structure of the kernel and are based on the generator
representation of the kernel matrix. However, as will be shown from both the
theory and the practice, the algorithms based on the generator representation
are sometimes numerically unstable, which limits their application in practice.
This paper aims to address this issue by deriving and exploiting an alternative
Givens-vector representation of some widely used kernel matrices. Based on the
Givens-vector representation, we derive algorithms that yield more accurate
results than existing algorithms without sacrificing efficiency. We demonstrate
their usage for the kernel-based regularized system identification. Monte Carlo
simulations show that the proposed algorithms admit the same order of
computational complexity as the state-of-the-art ones based on generator
representation, but without issues with numerical stability.

</details>


### [30] [An Adaptive Flux Reconstruction Scheme for Robust Shock Capturing](https://arxiv.org/abs/2511.01564)
*Sai Shruthi Srinivasan,Siva Nadarajah*

Main category: math.NA

TL;DR: An adaptive approach for choosing the lifting operator in nonlinearly-stable flux reconstruction (NSFR) schemes that maintains higher accuracy, allows larger CFL values while retaining entropy stability, and when combined with a positivity-preserving limiter, provides essentially oscillation-free solutions.


<details>
  <summary>Details</summary>
Motivation: High-order methods like DG experience unwanted high-frequency oscillations near shocks, and while shock-capturing methods exist, entropy-stable schemes need to retain provable entropy dissipation guarantees. NSFR schemes mitigate oscillations but sacrifice accuracy.

Method: Proposed an adaptive approach to the choice of lifting operator in NSFR schemes, combined with a positivity-preserving limiter.

Result: The adaptive approach maintains higher accuracy, allows for larger CFL values while retaining entropy stability, and provides essentially oscillation-free solutions when used with a positivity-preserving limiter.

Conclusion: The adaptive NSFR scheme offers improved accuracy and larger CFL values while maintaining entropy stability and providing essentially oscillation-free solutions, though it cannot completely eliminate oscillations like traditional shock-capturing methods.

Abstract: In the case of hyperbolic conservation laws, high-order methods, such as the
classical DG method, experience the phenomenon of unwanted high-frequency
oscillations in the vicinity of a shock. Shock-capturing methods such as
artificial dissipation, solution, flux, or TVD limiting are generally used to
eliminate non-physical oscillations and provide bounds on physical quantities.
For entropy-stable schemes, the additional objective would be to retain
provable entropy dissipation guarantees of the underlying scheme, i.e. subcell
limiting or entropy filtering [1, 2, 3, 4]. The nonlinearly-stable flux
reconstruction (NSFR) semi-discretization given in Eq. 7 with a suitable flux
reconstruction scheme has been demonstrated to mitigate spurious oscillations
in the presence of shock discontinuities and at CFL values substantially larger
than the DG variant of the NSFR scheme whilst retaining the property of entropy
stability [5]. NSFR schemes achieve this by introducing an alternative lifting
operator for surface numerical flux penalization, albeit at the expense of
accuracy. In this technical note, we present an adaptive approach to the choice
of the lifting operator employed, which maintains higher accuracy and allows
for larger CFL values while retaining the underlying provable attributes of the
scheme. While it cannot eliminate oscillations such as the aforementioned
shock-capturing methods, together with a positivity-preserving limiter, the
scheme provides for solutions that are essentially oscillation-free.

</details>


### [31] [Numerical methods for solving PIDEs arising in swing option pricing under a two-factor mean-reverting model with jumps](https://arxiv.org/abs/2511.01587)
*Mustapha Regragui,Karel J. in 't Hout,Michèle Vanmaele,Fred Espen Benth*

Main category: math.NA

TL;DR: Numerical valuation of swing options using second-order methods for 2D PIDEs under a linear two-factor mean-reverting jump model.


<details>
  <summary>Details</summary>
Motivation: Swing options valuation under complex models with jumps, convection dominance, nonlocal integral terms, and nonsmooth initial conditions presents numerical challenges.

Method: Proposed various second-order numerical methods specifically designed to handle convection-dominated 2D PIDEs with jump terms and nonsmooth initial functions.

Result: Theoretical analysis confirms stability and convergence of the methods, and numerical experiments demonstrate second-order convergence behavior.

Conclusion: The developed second-order numerical methods effectively address the challenges in swing option valuation under jump-diffusion models with complex features.

Abstract: This paper concerns the numerical valuation of swing options with discrete
action times under a linear two-factor mean-reverting model with jumps. The
resulting sequence of two-dimensional partial integro-differential equations
(PIDEs) are convection-dominated and possess a nonlocal integral term due to
the presence of jumps. Further, the initial function is nonsmooth. We propose
various second-order numerical methods that can adequately handle these
challenging features. The stability and convergence of these numerical methods
are analysed theoretically. By ample numerical experiments, we confirm their
second-order convergence behaviour.

</details>


### [32] [Analysis of a Schwarz-Fourier domain decomposition method](https://arxiv.org/abs/2511.01616)
*Arnold Reusken*

Main category: math.NA

TL;DR: Analysis of an inexact Schwarz domain decomposition method for Laplace equations on overlapping discs, using Fourier subspace projections for subproblem solutions, with convergence analysis based on maximum principle arguments.


<details>
  <summary>Details</summary>
Motivation: To better understand the ddCOSMO solver used in computational chemistry by analyzing a model problem involving Laplace equations on overlapping domains.

Method: Inexact Schwarz domain decomposition method with Fourier subspace projections for solving subproblems on overlapping discs, analyzed using maximum principle arguments.

Result: Derived a new variant of the maximum principle and obtained contraction number bounds in the maximum norm for the Schwarz-Fourier domain decomposition method.

Conclusion: The analysis provides theoretical foundations for understanding convergence properties of domain decomposition methods with approximate subproblem solutions, particularly relevant for computational chemistry applications like ddCOSMO.

Abstract: The Schwarz domain decomposition method can be used for approximately solving
a Laplace equation on a domain formed by the union of two overlapping discs. We
consider an inexact variant of this method in which the subproblems on the
discs are solved approximately using the projection on a Fourier subspace of
the $L^2$ space on the boundary. This model problem is relevant for better
understanding of the ddCOSMO solver that is used in computational chemistry. We
analyze convergence properties of this Schwarz-Fourier domain decomposition
method. The analysis is based on maximum principle arguments. We derive a new
variant of the maximum principle and contraction number bounds in the maximum
norm.

</details>


### [33] [Sufficient conditions for QMC analysis of finite elements for parametric differential equations](https://arxiv.org/abs/2511.01703)
*Vesa Kaarnioja,Andreas Rupp,Jay Gopalakrishnan*

Main category: math.NA

TL;DR: The paper studies parametric regularity of discretized flux vector fields in balance laws with random parameters, focusing on Gevrey-regular random fields. It derives optimal QMC convergence rates for quantities of interest related to flux, primal variables, or gradients, and verifies assumptions for various finite element discretizations.


<details>
  <summary>Details</summary>
Motivation: To understand how parametric regularity affects quasi-Monte Carlo (QMC) methods for uncertainty quantification in PDEs with random parameters, particularly focusing on flux vector fields in balance laws where the parameter links flux with primal variables.

Method: Model random parameters as Gevrey-regular random fields expressible as functions of infinite sequences of independent random variables. Derive QMC error bounds using parametric regularity analysis. Verify assumptions for conforming finite elements, mixed methods, and hybridizable discontinuous Galerkin schemes.

Result: QMC method converges optimally when the quantity of interest depends continuously on the primal variable, its flux, or its gradient. Numerical experiments confirm analytical findings and highlight the importance of accurate flux approximation in QMC methods.

Conclusion: Parametric regularity enables optimal QMC convergence for flux-related quantities of interest in discretized balance laws with random parameters, with broad applicability across various finite element discretization schemes.

Abstract: Parametric regularity of discretizations of flux vector fields satisfying a
balance law is studied under some assumptions on a random parameter that links
the flux with an unknown primal variable (often through a constitutive law). In
the primary example of the stationary diffusion equation, the parameter
corresponds to the inverse of the diffusivity. The random parameter is modeled
here as a Gevrey-regular random field. Specific focus is on random fields
expressible as functions of countably infinite sequences of independent random
variables, which may be uniformly or normally distributed. Quasi-Monte Carlo
(QMC) error bounds for some quantity of interest that depends on the flux are
then derived using the parametric regularity. It is shown that the QMC method
converges optimally if the quantity of interest depends continuously on the
primal variable, its flux, or its gradient. A series of assumptions are
introduced with the goal of encompassing a broad class of discretizations by
various finite element methods. The assumptions are verified for the diffusion
equation discretized using conforming finite elements, mixed methods, and
hybridizable discontinuous Galerkin schemes. Numerical experiments confirm the
analytical findings, highlighting the role of accurate flux approximation in
QMC methods.

</details>


### [34] [Finite Elements with weighted bases for the fractional Laplacian](https://arxiv.org/abs/2511.01727)
*Félix del Teso,Stefano Fronzoni,David Gómez-Castro*

Main category: math.NA

TL;DR: A novel Finite Element method using non-standard basis functions δ^s × piece-wise linear functions achieves improved convergence rates h^{2-s} for fractional Laplacian problems, overcoming the h^{1/2} limitation of classical approaches.


<details>
  <summary>Details</summary>
Motivation: Classical Finite Element methods with piece-wise linear basis functions only achieve h^{1/2} convergence rates due to limited boundary regularity of solutions to fractional Laplacian problems, which behave like distance^s near boundaries.

Method: Proposed a novel Finite Element basis of the form δ^s × (piece-wise linear functions), where δ approximates the distance to the boundary, exploiting the improved regularity of u/δ^s.

Result: The method achieves h^{2-s} convergence rates on quasi-uniform meshes under standard smoothness assumptions, significantly improving over the h^{1/2} rates of classical approaches.

Conclusion: The proposed non-standard basis approach successfully overcomes boundary regularity limitations in fractional Laplacian problems, achieving higher convergence rates with rigorous theoretical analysis and numerical validation.

Abstract: This work presents a numerical study of the Dirichlet problem for the
fractional Laplacian $(-\Delta)^s$ with $s\in(0,1)$ using Finite Element
methods with non-standard bases. Classical approaches based on piece-wise
linear basis yield $h^{\frac 1 2}$ convergence rates in the Sobolev-Slobodeckij
norm $H^s$ due to the limited boundary regularity of the solution $u(x)$, which
behaves like $\operatorname{dist}(x,\mathbb{R}^d\setminus \Omega)^s$, where $h$
is the diameter of the mesh elements. To overcome this limitation, we propose a
novel Finite Element basis of the form $\delta^s \times ($piece-wise linear
functions$)$, where $\delta$ is any suitably smooth approximation of
$\operatorname{dist}(x,\mathbb{R}^d\setminus \Omega)$. This exploits the
improved regularity of $u/\delta^s$, achieving higher convergence rates. Under
standard smoothness assumptions the method attains an order $h^{2-s}$ on
quasi-uniform meshes, improving the rates with the piece-wise linear basis. We
provide a rigorous theoretical error analysis with explicit rates and validate
it through numerical experiments.

</details>


### [35] [A Low-Rank BUG Method for Sylvester-Type Equations](https://arxiv.org/abs/2511.01735)
*Georgios Vretinaris*

Main category: math.NA

TL;DR: A low-rank algorithm for efficiently solving Sylvester-type equations using BUG integrator principles, exploiting low-rank structure and sparsity to reduce computational complexity.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for solving Sylvester-type equations that can leverage both low-rank solution structure and system sparsity to overcome computational limitations of traditional dense solvers.

Method: Based on the Basis-Update and Galerkin (BUG) integrator, the algorithm constructs reduced Sylvester equations that can be solved using standard dense solvers while maintaining computational efficiency.

Result: The method achieves computational complexity of O(kr(n^2+m^2+mn+r^2)) for approximating solutions X in R^{m×n}, where k is iterations and r is approximation rank, significantly reducing complexity compared to traditional approaches.

Conclusion: The proposed low-rank algorithm provides an efficient framework for solving Sylvester-type equations by exploiting structural properties, offering substantial computational savings while maintaining solution accuracy.

Abstract: We introduce a low-rank algorithm inspired by the Basis-Update and Galerkin
(BUG) integrator to efficiently approximate solutions to Sylvester-type
equations. The algorithm can exploit both the low-rank structure of the
solution as well as any sparsity present to reduce computational complexity.
Even when a standard dense solver, such as the Bartels-Stewart algorithm, is
used for the reduced Sylvester equations generated by our approach, the overall
computational complexity for constructing and solving the associated linear
systems reduces to O(kr(n^2+m^2 +mn + r^2)), for X in R^{m \times n}, where k
is the number of iterations and r the rank of the approximation.

</details>


### [36] [Variational Data-Consistent Assimilation](https://arxiv.org/abs/2511.01759)
*Rylan Spence,Troy Butler,Clint Dawson*

Main category: math.NA

TL;DR: This paper introduces new 4D-Var methods with predictability-aware regularization, showing improved accuracy and robustness in data assimilation.


<details>
  <summary>Details</summary>
Motivation: To enhance classical 4D-Var methods by incorporating predictability-aware regularization for better performance in nonlinear and partially observed dynamical systems.

Method: Developed Data-Consistent 4D-Var (DC-4DVar) and its enhanced version DC-WME 4D-Var with Weighted Mean Error modification, both using predictability-aware regularization terms.

Result: DC-WME 4D-Var consistently outperforms standard 4D-Var in reducing error and bias, maintaining robustness under high observation noise and short assimilation windows in Lorenz-63, Lorenz-96, and shallow water equations.

Conclusion: The new DC-WME 4D-Var method demonstrates practical potential and scalability for high-dimensional data assimilation problems despite modest computational overhead.

Abstract: This work introduces a new class of four-dimensional variational data
assimilation (4D-Var) methods grounded in data-consistent inversion (DCI)
theory. The methods extend classical 4D-Var by incorporating a
predictability-aware regularization term. The first method formulated is
referred to as Data-Consistent 4D-Var (DC-4DVar), which is then enhanced using
a Weighted Mean Error (WME) quantity-of-interest map to construct the DC-WME
4D-Var method. While the DC and DC-WME cost functions both involve a
predictability-aware regularization term, the DC-WME function includes a
modification to the model-data misfit, thereby improving estimation accuracy,
robustness, and theoretical consistency in nonlinear and partially observed
dynamical systems. Proofs are provided that establish the existence and
uniqueness of the minimizer and analyze how a predictability assumption that is
common within the DCI framework helps to promote solution stability. Numerical
experiments are presented on benchmark dynamical systems (Lorenz-63 and
Lorenz-96) as well as for the shallow water equations (SWE). In the benchmark
dynamical systems, the DC-WME 4D-Var formulation is shown to consistently
outperform standard 4D-Var in reducing both error and bias while maintaining
robustness under high observation noise and short assimilation windows. Despite
introducing modest computational overhead, DC-WME 4D-Var delivers improvements
in estimation performance and forecast skill, demonstrating its potential
practicality and scalability for high-dimensional data assimilation problems.

</details>


### [37] [Stochastic Multigrid Method for Blind Ptychographic Phase Retrieval](https://arxiv.org/abs/2511.01793)
*Borong Zhang,Junjing Deng,Yi Jiang,Zichao Wendy Di*

Main category: math.NA

TL;DR: eMAGPIE is a stochastic multigrid method for blind ptychographic phase retrieval that jointly recovers object and probe using geometric-mean phase-aligned updates with guaranteed descent properties.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for blind ptychographic phase retrieval that can jointly recover both the object and probe with improved convergence speed and reconstruction quality.

Method: Uses iterative minimization of quadratic surrogate that majorizes exit-wave misfit, derives closed-form updates combined in geometric-mean phase-aligned joint step, and employs multigrid acceleration.

Result: Achieves lower data misfit and phase error at comparable compute budgets, produces smoother phase reconstructions with reduced artifacts.

Conclusion: eMAGPIE provides an effective multigrid-accelerated approach for blind ptychography with improved convergence and reconstruction quality.

Abstract: We present eMAGPIE (extended Multilevel-Adaptive-Guided Ptychographic
Iterative Engine), a stochastic multigrid method for blind ptychographic phase
retrieval that jointly recovers the object and the probe. We recast the task as
the iterative minimization of a quadratic surrogate that majorizes the
exit-wave misfit. From this surrogate, we derive closed-form updates, combined
in a geometric-mean, phase-aligned joint step, yielding a simultaneous update
of the object and probe with guaranteed descent of the sampled surrogate. This
formulation naturally admits a multigrid acceleration that speeds up
convergence. In experiments, eMAGPIE attains lower data misfit and phase error
at comparable compute budgets and produces smoother, artifact-reduced phase
reconstructions.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [38] [The local existence and uniqueness of strong solutions for Cauchy problem of three-dimensional inhomogeneous incompressible Navier-Stokes-Vlasov equations](https://arxiv.org/abs/2511.00063)
*Binxuan Ru*

Main category: math.AP

TL;DR: This paper studies local existence and uniqueness of strong solutions for 3D inhomogeneous incompressible Navier-Stokes-Vlasov equations using linearization and approximation methods.


<details>
  <summary>Details</summary>
Motivation: To enrich the existence results of strong solutions for Navier-Stokes-Vlasov equations by establishing local well-posedness for the Cauchy problem.

Method: Linearizes the equations, constructs approximate solutions for the linearized equation, obtains consistent estimation, and takes limits of the approximate solutions.

Result: Proves local existence and uniqueness of strong solutions for the Cauchy problem of 3D inhomogeneous incompressible Navier-Stokes-Vlasov equations.

Conclusion: The work successfully establishes local well-posedness and contributes to the broader understanding of strong solution existence for Navier-Stokes-Vlasov equations.

Abstract: In this paper, we study the local existence and uniqueness of strong
solutions for Cauchy problem of three-dimensional inhomogeneous incompressible
Navier-Stokes-Vlasov equations, which are influenced by Young-Pil Choi, Bongsuk
Kwon [London Mathematical Society 28 (2015), pp. 3309-3336]\cite{12L}. As for
the global well-posedness of the solution of the inhomogeneous incompressible
Navier-Stokes-Vlasov equations, this paper first linearizes the inhomogeneous
incompressible Navier-Stokes-Vlasov equations, constructs the approximate
solution of the linearized equation, and obtains the consistent estimation of
the approximate solution. Then, the approximate solution is limited. The local
existence and uniqueness of strong solutions for Cauchy problem of
inhomogeneous incompressible Navier-Stokes-Vlasov equations are obtained, which
further enriches the existence results of strong solutions for
Navier-Stokes-Vlasov equations.

</details>


### [39] [On the well-posedness of the intermediate nonlinear Schrödinger equation on the line](https://arxiv.org/abs/2511.00302)
*Andreia Chapouto,Justin Forlano,Thierry Laurens*

Main category: math.AP

TL;DR: The paper improves local well-posedness for intermediate nonlinear Schrödinger equations (INLS) from s>1/2 to s>1/4 in H^s(R), including continuum Calogero-Moser models, without requiring Hardy space assumptions.


<details>
  <summary>Details</summary>
Motivation: To extend the well-posedness theory for INLS equations, particularly continuum Calogero-Moser models, by removing the restrictive Hardy space requirement and achieving better regularity thresholds.

Method: Uses gauge transformation, exploits nonlinearity structure with bilinear Strichartz estimates to recover derivative loss, refined solution decomposition to observe nonlinear smoothing, and discovers new Lax pair.

Result: Proves local well-posedness in H^s(R) for s>1/4, and global well-posedness for s>1/4 with small L^2-norm using the new Lax pair.

Conclusion: The paper significantly improves well-posedness results for INLS equations, particularly CCM models, by developing new analytical techniques and discovering mathematical structures like the Lax pair.

Abstract: We consider a family of intermediate nonlinear Schr\"{o}dinger equations
(INLS) on the real line, which includes the continuum Calogero-Moser models
(CCM). We prove that INLS is locally well-posed in $H^{s}(\mathbb{R})$ for any
$s>\frac 14$, which improves upon the previous best result of $s>\frac 12$ by
de Moura-Pilod (2008). This result is also new in the special case of CCM, as
the initial condition is not required to lie in any Hardy space. Our approach
is based on a gauge transformation, exploiting the remarkable structure of the
nonlinearity together with bilinear Strichartz estimates, which allows to
recover some of the derivative loss. This turns out to be sufficient to
establish our main results for CCM in the Hardy space. For INLS and CCM outside
of the Hardy space, the main difficulty comes from the lack of the Hardy space
assumption, which we overcome by implementing a refined decomposition of the
solutions, which observes a nonlinear smoothing effect in part of the solution.
We also discover a new Lax pair for INLS and use it to establish global
well-posedness in $H^{s}(\mathbb{R})$ for any $s>\frac 14$ under the additional
assumption of small $L^2$-norm.

</details>


### [40] [Analysis of a nonlinear free-boundary tumor model with three layers](https://arxiv.org/abs/2511.00355)
*Junde Wu,Hao Xu,Yuehong Zhuang*

Main category: math.AP

TL;DR: Analysis of a three-layer tumor growth model with necrotic core, quiescent layer, and proliferating layer, focusing on free boundary problems and evolutionary mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand the complex growth dynamics of spherically symmetric tumors with multiple layers and free boundaries, addressing challenges from nutrient supply and mass conservation.

Method: Developed a nonlinear analysis method to handle free boundaries and discontinuous nutrient-consumption rates, studying mutual relationships between boundaries to reveal tumor evolution mechanisms.

Result: Proved existence and uniqueness of radial stationary solutions, and established globally asymptotic stability towards different dormant tumor states.

Conclusion: The study successfully models tumor growth with three-layer structure, providing mathematical framework for understanding tumor evolution and internal structure transformations.

Abstract: In this paper, we study a nonlinear free boundary problem modeling the growth
of spherically symmetric tumors. The tumor consists of a central necrotic core,
an intermediate annual quiescent-cell layer, and an outer proliferating-cell
layer. The evolution of tumor layers and the movement of the tumor boundary are
totally governed by external nutrient supply and conservation of mass. The
three-layer structure generates three free boundaries with boundary conditions
of different types. We develop a nonlinear analysis method to get over the
great difficulty arising from free boundaries and the discontinuity of the
nutrient-consumption rate function. By carefully studying the mutual
relationships between the free boundaries, we reveal the evolutionary mechanism
in tumor growth and the mutual transformation of its internal structures. The
existence and uniqueness of the radial stationary solution is proved, and its
globally asymptotic stability towards different dormant tumor states is
established.

</details>


### [41] [Sharp Gagliardo-Nirenberg inequality and logarithmic Sobolev inequality on integer lattices](https://arxiv.org/abs/2511.00393)
*Yongjie Shi,Chengjie Yu*

Main category: math.AP

TL;DR: Sharp Gagliardo-Nirenberg inequality on integer lattices with rigidity characterization, leading to sharp logarithmic Sobolev inequalities.


<details>
  <summary>Details</summary>
Motivation: To establish and characterize sharp functional inequalities on discrete integer lattices, extending continuous analysis to discrete settings.

Method: Mathematical analysis and proof techniques to obtain sharp Gagliardo-Nirenberg inequality on integer lattices, then deriving logarithmic Sobolev inequalities as consequences.

Result: Proved sharp Gagliardo-Nirenberg inequality on integer lattices with complete rigidity characterization, and obtained sharp logarithmic Sobolev inequalities as corollaries.

Conclusion: Successfully established sharp functional inequalities on discrete integer lattices, providing discrete analogues of important continuous inequalities with full characterization of extremal cases.

Abstract: In this paper, we obtain a sharp Garliardo-Nirenberg inequality on integer
lattices and characterize its rigidity. Moreover, as a consequence of the sharp
Garliardo-Nirenberg inequality, we obtain sharp logarithmic Sobolev
inequalities on integer lattices.

</details>


### [42] [Monotonicity Conjectures and Sharp Stability for Solitons of the Cubic-Quintic NLS on R^3](https://arxiv.org/abs/2511.00471)
*Jian Zhang,Chenglin Wang,Shihui Zhu*

Main category: math.AP

TL;DR: This paper resolves two monotonicity conjectures about solitons in the cubic-quintic nonlinear Schrödinger equation on R^3, proves uniqueness of energy minimizers, establishes sharp stability of solitons, and provides classification of normalized solutions.


<details>
  <summary>Details</summary>
Motivation: To address and completely resolve two important monotonicity conjectures posed by Killip, Oh, Pocovnicu and Visan regarding solitons in the cubic-quintic nonlinear Schrödinger equation.

Method: Mathematical analysis of the cubic-quintic nonlinear Schrödinger equation on R^3, focusing on proving frequency monotonicity and mass monotonicity properties of solitons.

Result: Successfully resolved both monotonicity conjectures, proved uniqueness of the energy minimizer, established sharp stability of solitons, and provided the first classification of normalized solutions.

Conclusion: The paper provides complete resolution of important conjectures about soliton behavior in the cubic-quintic nonlinear Schrödinger equation, advancing the understanding of soliton stability and classification in this mathematical framework.

Abstract: This paper deals with the cubic-quintic nonlinear Schr\"{o}dinger equation on
R^3. Two monotonicity conjectures for solitons posed by Killip, Oh, Pocovnicu
and Visan are completely resolved: one concerning frequency monotonicity, and
the other concerning mass monotonicity. Uniqueness of the energy minimizer is
proved. Then sharp stability of the solitons is established. And classification
of normalized solutions is first presented.

</details>


### [43] [Sharp Stability of Solitons for the Cubic-Quintic NLS on R^2](https://arxiv.org/abs/2511.00474)
*Yi Jiang,Chenglin Wang,Yibin Xiao,Jian Zhang,Shihui Zhu*

Main category: math.AP

TL;DR: The paper studies the cubic-quintic nonlinear Schrödinger equation in 2D, introducing new variational problems for solitons, proving orbital stability at all frequencies, and classifying normalized ground states.


<details>
  <summary>Details</summary>
Motivation: To address open questions raised by Lewin and Rota Nodari as well as Carles and Sparber regarding soliton stability and classification in the cubic-quintic nonlinear Schrödinger equation on R^2.

Method: Introduces new variational problems related to solitons, applies monotonicity and uniqueness results, and uses Cazenave and Lions' argument to prove orbital stability.

Result: Proves orbital stability of solitons at every frequency, obtains key monotonicity and uniqueness results, and provides first classification of normalized ground states.

Conclusion: The results successfully settle the previously raised questions about soliton stability and classification in the cubic-quintic nonlinear Schrödinger equation on R^2.

Abstract: This paper concerns with the cubic-quintic nonlinear Schr\"{o}dinger equation
on R^2. A family of new variational problems related to the solitons are
introduced and solved. Some key monotonicity and uniqueness results are
obtained. Then the orbital stability of solitons at every frequency are proved
in terms of the Cazenave and Lions' argument. And classification of normalized
ground states is first presented. Our results settle the questions raised by
Lewin and Rota Nodari as well as Carles and Sparber.

</details>


### [44] [Global weak solutions and incompressible limit to the isentropic compressible magnetohydrodynamic equations in 2D bounded domains with ripped density and large initial data](https://arxiv.org/abs/2511.00484)
*Shuai Wang,Guochun Wu,Xin Zhong*

Main category: math.AP

TL;DR: This paper extends previous results on global existence and incompressible limit of weak solutions for isentropic compressible magnetohydrodynamic equations from the whole plane to 2D bounded convex domains with Navier-slip boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To generalize previous results from the whole plane to bounded domains, specifically addressing the initial-boundary value problem for compressible magnetohydrodynamic equations with ripped density and large initial energy.

Method: The authors work with isentropic compressible magnetohydrodynamic equations and establish uniform a priori estimates that are independent of the bulk viscosity coefficient, under Navier-slip boundary conditions on 2D bounded convex domains.

Result: Successfully obtained global existence and incompressible limit of weak solutions for the compressible magnetohydrodynamic equations in bounded domains, with uniform estimates independent of bulk viscosity.

Conclusion: The paper successfully extends the previous whole-plane results to bounded convex domains, providing important theoretical foundations for compressible magnetohydrodynamic flows in confined geometries with appropriate boundary conditions.

Abstract: In our previous work (arXiv:2510.00812), we have shown the global existence
and incompressible limit of weak solutions to the isentropic compressible
magnetohydrodynamic equations involving ripped density and large initial energy
in the whole plane. In this paper we generalize such results to the case of
two-dimensional bounded convex domains under Navier-slip boundary conditions.
When comparing to the known results for global solutions of the
initial-boundary value problem, we obtain uniform a priori estimates
independent of the bulk viscosity coefficient.

</details>


### [45] [Existence results for a biofilm free-boundary problem with dominant detachment](https://arxiv.org/abs/2511.00495)
*Dieudonné Zirhumananana*

Main category: math.AP

TL;DR: The paper extends biofilm modeling by analyzing a free boundary problem with detachment, proving local/global existence, uniqueness, and continuous dependence using fixed point arguments and energy estimates.


<details>
  <summary>Details</summary>
Motivation: To extend existing tumor growth and biofilm modeling approaches by incorporating detachment effects and establishing rigorous mathematical foundations for the resulting free boundary problem.

Method: Used fixed point arguments combined with semigroup theory for local existence, and invariance regions with energy estimates for global existence analysis.

Result: Proved local existence and uniqueness, continuous dependence on initial/boundary data, and global existence for the biofilm free boundary problem with detachment.

Conclusion: Successfully established comprehensive mathematical well-posedness for the biofilm model with detachment, extending previous modeling frameworks with rigorous analytical foundations.

Abstract: Wanner-Gujer free boundary problem modeling biofilms with a prevailing
detachment. This result extends the some works from tumor gowth modeling as
well as those from the biofilm modeling field. Besides the local existence and
uniqueness, the continuous dependence on initial and boundary data and the
global existence is also given. The local existence is obtained by using fixed
point arguments combined with semigroup theory while the global existence is
deduced using invariance regions and energy estimates.

</details>


### [46] [Uniqueness and stability of normalized ground states for Hartree equation with a harmonic potential](https://arxiv.org/abs/2511.00533)
*Yi Jiang,Chenglin Wang,Yibin Xiao,Jian Zhang,Shihui Zhu*

Main category: math.AP

TL;DR: Analysis of normalized ground states for Hartree equation with harmonic potential, confirming existence, uniqueness, and orbital stability.


<details>
  <summary>Details</summary>
Motivation: To study the dynamic properties of normalized ground states in the Hartree equation with harmonic potential, addressing fundamental questions about existence, uniqueness, and stability.

Method: Mass-energy constrained variational approach for existence proof, strictly convex properties of energy functional for uniqueness, and Cazenave and Lions' argument for orbital stability.

Result: Confirmed existence of normalized ground state for any prescribed mass, proved uniqueness, and demonstrated orbital stability of every normalized ground state.

Conclusion: The paper successfully establishes comprehensive dynamic properties of normalized ground states for the Hartree equation with harmonic potential, providing rigorous mathematical foundations for their existence, uniqueness, and stability.

Abstract: The dynamic properties of normalized ground states for the Hartree equation
with a harmonic potential are addressed. The existence of normalized ground
state for any prescribed mass is confirmed according to mass-energy constrained
variational approach. The uniqueness is shown by the strictly convex properties
of the energy functional. Moreover, the orbital stability of every normalized
ground state is proven in terms of the Cazenave and Lions' argument.

</details>


### [47] [Doubly nonlinear Schrödinger normalized ground states on 2D grids: existence results and singular limits](https://arxiv.org/abs/2511.00550)
*Daniele Barbera,Filippo Boni,Simone Dovetta,Lorenzo Tentarelli*

Main category: math.AP

TL;DR: Existence and convergence of normalized ground states for focusing doubly nonlinear Schrödinger equations on 2D square grids with standard and concentrated nonlinearities.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of ground states in doubly nonlinear Schrödinger equations on discrete structures and their continuum limits, particularly when nonlinearities are concentrated at specific vertices.

Method: First, analyze existence/non-existence of ground states based on nonlinearity powers and concentrated nonlinearity locations. Second, study convergence of piecewise-affine extensions to continuum models as grid spacing vanishes.

Result: Established existence criteria and proved strong H¹ convergence to continuum ground states for models with standard nonlinearities only, and models combining standard with line/strip concentrated nonlinearities.

Conclusion: Discrete ground states converge to continuum counterparts, providing rigorous connection between discrete and continuous doubly nonlinear Schrödinger models.

Abstract: We investigate the existence and the singular limit of normalized ground
states for focusing doubly nonlinear Schr\"odinger equations with both standard
and concentrated nonlinearities on two-dimensional square grids. First, we
provide existence and non-existence results for such ground states depending on
the values of the nonlinearity powers and on the structure of the set of
vertices where the concentrated nonlinearities are located. Second, we prove
that suitable piecewise-affine extensions of such states converge strongly in
$H^1(\R^2)$ to ground states of corresponding doubly nonlinear models defined
on the whole plane as the length of the edges in the grid tends to zero. This
convergence is proved both for limit models with standard nonlinearities only
and for models combining standard and singular nonlinearities concentrated on a
line or on a strip.

</details>


### [48] [Linear decay of the beta-plane equation near Couette flow on the plane](https://arxiv.org/abs/2511.00667)
*Jacob Bedrossian,Patrick Flynn,Sameer Iyer*

Main category: math.AP

TL;DR: New time decay estimates for linearized β-plane equation near Couette flow, showing velocity field profiles decay polynomially on compact sets by combining inviscid damping and Rossby wave dispersion.


<details>
  <summary>Details</summary>
Motivation: To understand the combined effects of inviscid damping and Rossby wave dispersion on the time decay behavior of velocity fields in the β-plane equation near Couette flow.

Method: Analysis of oscillatory integrals with homogeneous phase and divergent multipliers using Van der Corput type estimates, with two delicate asymptotic analyses: boundary layer type (sharp gradients across critical ray) and multi-scale type (extracting analytic profile function for phase).

Result: Velocity field profiles decay pointwise on any compact set with polynomial rates, with mixing dominating for O(1) streamwise frequencies and dispersive effects appearing along critical ray for low streamwise frequencies.

Conclusion: The proof successfully combines inviscid damping and dispersion effects to establish polynomial decay rates, requiring sophisticated analysis of singular limits through boundary layer and multi-scale asymptotic methods.

Abstract: We prove new time decay estimates for the linearized $\beta$-plane equation
near the Couette flow on the plane that combine inviscid damping and the
dispersion of Rossby waves. Specifically, we show that the profiles of the
velocity field components (i.e. $u(t,x+ty,y)$) decay pointwise on any compact
set with polynomial rates. While mixing dominates for streamwise frequencies
that are $O(1)$, dispersive effects need to be extracted for low streamwise
frequencies that appear along a critical ray in frequency space. Our proof
entails the analysis of oscillatory integrals with homogeneous phase and
multipliers that diverge in the infinite time limit. To handle this singular
limit, we prove a Van der Corput type estimate, followed by two delicate
asymptotic analyses of the phase and multipliers: one that is of ``boundary
layer" type, featuring sharp gradients that grow in $t$ across the critical
ray, and one that is of ``multi-scale" type, which extracts a governing
analytic profile function for the phase.

</details>


### [49] [Similarity Solutions of Shock Formation for First-order Strictly Hyperbolic Systems](https://arxiv.org/abs/2511.00672)
*Jun Eshima,Luc Deike,Howard A. Stone*

Main category: math.AP

TL;DR: Shock formation in general first-order strictly hyperbolic PDEs is shown to be self-similar and universal, similar to the inviscid Burgers' equation, with an analytical formula derived for the universal solution.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of shock formation beyond the canonical inviscid Burgers' equation to general first-order strictly hyperbolic PDEs in one spatial dimension, investigating whether the local self-similarity and universality observed in Burgers' equation holds more broadly.

Method: Analytical derivation of a formula for the self-similar universal solution, followed by verification of the solution's validity for general first-order strictly hyperbolic PDEs.

Result: Shock formation is indeed self-similar and universal for general first-order strictly hyperbolic PDEs in one spatial dimension, with dynamics equivalent regardless of initial conditions near the shock singularity, and the self-similarity pattern matches that of the inviscid Burgers' equation.

Conclusion: The study establishes that shock formation exhibits universal self-similar behavior across a broad class of first-order strictly hyperbolic PDEs, providing an analytical framework that generalizes the well-known Burgers' equation case.

Abstract: Shocks due to hyperbolic partial differential equations (PDEs) appear
throughout mathematics and science. The canonical example is shock formation in
the inviscid Burgers' equation $\frac{\partial u}{\partial t}+u\frac{\partial
u}{\partial x}=0$. Previous studies have shown that when shocks form for the
inviscid Burgers' equation, for positions and times close to the shock
singularity, the dynamics are locally self-similar and universal, i.e.,
dynamics are equivalent regardless of the initial conditions. In this paper, we
show that, in fact, shock formation is self-similar and universal for general
first-order strictly hyperbolic PDEs in one spatial dimension, and the
self-similarity is like that of the inviscid Burgers' equation. An analytical
formula is derived and verified for the self-similar universal solution.

</details>


### [50] [On taming Moffatt-Kimura vortices of doom in the viscous case](https://arxiv.org/abs/2511.00725)
*Zoran Grujic*

Main category: math.AP

TL;DR: A two-layer viscous mechanism prevents finite-time singularity formation in colliding vortex rings by analyzing sparseness scales and exploiting analytic cancellation properties.


<details>
  <summary>Details</summary>
Motivation: To understand and prevent finite-time singularity formation in the Moffatt-Kimura model of counter-rotating vortex rings colliding at nontrivial angles.

Method: Two-layer approach: first analyzes sparseness scale of intense fluid activity regions, second identifies cancellation properties in vortex-stretching term using compensated compactness in Hardy spaces.

Result: The problem is critical (worst case), and an additional mechanism can drive sparseness into dissipation range to prevent singularity formation.

Conclusion: The proposed two-layer viscous mechanism successfully prevents finite-time singularity formation in colliding vortex rings through sparseness analysis and analytic cancellation properties.

Abstract: In this note we propose a two-layer viscous mechanism for preventing finite
time singularity formation in the Moffatt-Kimura model of two counter-rotating
vortex rings colliding at a nontrivial angle. In the first layer the scenario
is recast within the framework of the study of turbulent dissipation based on a
suitably defined `scale of sparseness' of the regions of intense fluid
activity. Here it is found that the problem is (at worst) critical, i.e., the
upper bound on the scale of sparseness of the vorticity super-level sets is
comparable to the lower bound on the radius of spatial analyticity. In the
second layer, an additional more subtle mechanism is identified, potentially
capable of driving the scale of sparseness into the dissipation range and
preventing the formation of a singularity. The mechanism originates in certain
analytic cancellation properties of the vortex-stretching term in the sense of
compensated compactness in Hardy spaces which then convert information on local
mean oscillations of the vorticity direction (boundedness in certain
log-composite weighted local bmo spaces) into log-composite faster decay of the
vorticity super-level sets.

</details>


### [51] [Stochastic representation of solutions for the parabolic Cauchy problem with variable exponent coefficients](https://arxiv.org/abs/2511.00773)
*Mustafa Avci*

Main category: math.AP

TL;DR: Existence and uniqueness of bounded viscosity solutions for degenerate parabolic equations with variable exponents, solved via stochastic representation and validated numerically.


<details>
  <summary>Details</summary>
Motivation: To extend classical SDEs' expressive power by allowing drift and diffusion coefficients to respond nonlinearly through state-dependent variable exponents, better capturing complex dynamics.

Method: Construct solution directly using stochastic representation, verify it satisfies Cauchy problem. Use corresponding SDE with nonlinear drift/diffusion coefficients. Validate with finite difference (Crank-Nicolson on logarithmic grids) vs Monte Carlo simulations.

Result: Proved existence and uniqueness of bounded viscosity solution. Numerical experiments validate theoretical framework through comparison of finite difference and Monte Carlo methods.

Conclusion: Successfully established theoretical framework for degenerate parabolic equations with variable exponents and validated it numerically, extending classical SDE capabilities for complex dynamics modeling.

Abstract: In this work, we prove existence and uniqueness of a bounded viscosity
solution for the Cauchy problem of degenerate parabolic equations with variable
exponents coefficients. We construct the solution directly using the stochastic
representation, then verify it satisfies the Cauchy problem. The corresponding
SDE, on the other hand, allows the drift and diffusion coefficients to respond
nonlinearly to the current state through the state-dependent variable
exponents, and thus, extends the expressive power of classical SDEs to better
capture complex dynamics. To validate our theoretical framework, we conduct
comprehensive numerical experiments comparing finite difference solutions
(Crank-Nicolson on logarithmic grids) with Monte Carlo simulations of the SDE.

</details>


### [52] [Topological solutions of generalized Chern-Simons equations on discrete lattice graphs](https://arxiv.org/abs/2511.00848)
*Songbo Hou*

Main category: math.AP

TL;DR: Existence of topological solutions for generalized Chern-Simons equations on discrete lattice graphs


<details>
  <summary>Details</summary>
Motivation: To study generalized Chern-Simons equations on discrete lattice graphs and establish the existence of topological solutions

Method: Iterative method starting from trivial initial function using energy functional, constructing monotone decreasing sequence on bounded domains, then passing to limit over expanding domains

Result: Obtained global solution defined on entire graph with topological characteristics

Conclusion: Successfully established existence of topological solutions for generalized Chern-Simons equations on discrete lattice graphs

Abstract: We study a class of generalized Chern-Simons equations on discrete lattice
graphs and establish the existence of topological solutions. Using an iterative
method starting from a trivial initial function and an associated energy
functional, we construct a monotone decreasing sequence that converges to a
solution on bounded domains. By deriving uniform estimates and passing to the
limit over an increasing sequence of expanding domains, we obtain a global
solution defined on the entire graph, which exhibits topological
characteristics.

</details>


### [53] [Fractional Torsional Rigidity of Compact Metric Graphs](https://arxiv.org/abs/2511.00883)
*Sedef Özcan*

Main category: math.AP

TL;DR: This paper extends fractional torsional rigidity to metric graphs, defining it as the L¹-norm of the fractional torsion function solution to (-Δ𝒢)ᵅuᵅ = 1 with zero boundary conditions. The authors establish variational characterization and use surgery principles to derive geometric bounds.


<details>
  <summary>Details</summary>
Motivation: To investigate fractional torsional rigidity on metric graphs as a novel extension of the classical concept to nonlocal operators, exploring geometric dependencies in this nonlocal setting.

Method: Define fractional torsional rigidity via the fractional torsion function, establish variational characterization, and apply surgery principles to derive geometric bounds.

Result: Obtained explicit upper and lower bounds showing the interval serves as an upper comparison case and the flower graph as a lower one among graphs of fixed total length.

Conclusion: The findings mirror classical results but require substantially different methods due to the nonlocal nature of the fractional Laplacian, successfully extending torsional rigidity concepts to fractional settings on metric graphs.

Abstract: This paper investigates fractional torsional rigidity on compact, connected
metric graphs, a novel extension of the classical concept to nonlocal
operators. The fractional torsional rigidity is defined as the $L^1$-norm of
the fractional torsion function, which is the unique solution to the boundary
value problem $(-\Delta_{\mathcal{G}})^\alpha u_\alpha = 1$ on a graph
$\mathcal{G}$ with zero boundary conditions at Dirichlet vertices. We establish
a variational characterization for this quantity, which serves as a powerful
tool to prove a series of results on its geometric dependence. By applying
surgery principles, we derive explicit upper and lower bounds, indicating that
the interval serves as an upper comparison case and the flower graph as a lower
one among graphs of fixed total length. These findings mirror the classical
case, yet the methods required are substantially different due to the nonlocal
nature of the fractional Laplacian.

</details>


### [54] [On a gradient term for a class of second-order PDEs and applications to the infinity Laplace equation](https://arxiv.org/abs/2511.00931)
*José Francisco de Oliveira*

Main category: math.AP

TL;DR: The paper proposes a natural gradient term for second-order PDEs and establishes conditions for transforming equations with quadratic gradient terms into simpler forms without such terms, unifying previous results for various operators.


<details>
  <summary>Details</summary>
Motivation: To unify and extend previous separate findings for Laplacian, m-Laplacian, and k-Hessian operators by providing a general framework that works for broader classes of nonlinear PDEs including infinity-Laplacian, and to handle both C² and viscosity solutions that may change sign.

Method: Proposes a change of variables v = Φ(u) that transforms equations with quadratic gradient terms into simpler forms without such terms, establishing conditions on the class of operators M for this transformation to be valid.

Result: Developed a unified framework that extends previous results to a broader class of nonlinear PDEs, including infinity-Laplacian operator, and handles both C² and viscosity solutions with changing signs. Also obtained Aronsson-type results and investigated viscosity solutions for Dirichlet problems.

Conclusion: The work provides a comprehensive framework that unifies and generalizes previous separate findings, extending applicability to more general nonlinear PDEs and solution types, with practical applications in studying viscosity solutions for Dirichlet problems.

Abstract: We propose a natural gradient term for a class of second-order partial
differential equations of the form \begin{equation}\nonumber
  M(x,Du,D^2u)+g(u)N(x,Du, D^2u)+f(x,u)=0 \;\;\mbox{in}\;\; \Omega,
\end{equation} where $\Omega\subset\mathbb{R}^n$ is an open set, $f\in
C(\Omega\times \mathbb{R}, \mathbb{R})$, $M$ defines the partial differential
operator, $N$ is a quadratic term driven by the gradient $Du$ and $M$ itself,
and $g\in C(\mathbb{R},\mathbb{R})$. We establish conditions on the class of
operators $M$ for the existence of a change of variables $v = \Phi(u)$ that
transforms the previous equation into another one of the form
\begin{equation}\nonumber
  M(x,Dv, D^2v) + h(x,v)=0 \quad \text{in} \;\; \Omega \end{equation} which
does not depend on the quadratic term $N$. The results presented here unify
previous findings for the Laplacian, $m$-Laplacian, and $k$-Hessian operators,
which were derived separately by different authors and are restricted to $C^2$
solutions with fixed sign. Our work provides a more general framework,
extending these findings to a broader class of nonlinear partial differential
equations, including the infinity-Laplacian o\-pe\-ra\-tor. In addition, we
also include both $C^2$ and viscosity solutions that may change sign. As an
application, we also obtain an Aronsson-type result and investigate viscosity
solutions for the Dirichlet problem associated with the infinity Laplace
equation with its natural gradient term.

</details>


### [55] [Large torus limit of global dynamics of the two-dimensional dispersive Anderson model](https://arxiv.org/abs/2511.00971)
*Ruoyuan Liu,Nikolay Tzvetkov*

Main category: math.AP

TL;DR: The paper studies convergence of periodic dynamics to full-space dynamics for the 2D dispersive Anderson model as period increases, with similar analysis for the parabolic Anderson model in appendix.


<details>
  <summary>Details</summary>
Motivation: To establish the connection between periodic and full-space dynamics for the 2D dispersive Anderson model, showing that periodic solutions converge to full-space solutions as the period goes to infinity.

Method: Using suitable initial conditions and periodization procedure of the noise, the authors prove convergence of periodic global dynamics to full-space dynamics in local domains as period increases.

Result: The periodic global dynamics of the dispersive Anderson model converges in spaces of local domains to that of the full-space model as the period goes to infinity.

Conclusion: The study successfully demonstrates the convergence from periodic to full-space dynamics for the 2D dispersive Anderson model, with similar results shown for the parabolic Anderson model in the appendix.

Abstract: We continue the study of the two-dimensional dispersive Anderson model (DAM),
i.e. the nonlinear Schr\"odinger equation with multiplicative spatial white
noise. For this model, global well-posedness on the periodic domain was
established by Visciglia and the second author (2023), and global
well-posedness on the full space was established by Debussche, Visciglia, and
the authors (2024). We show that, under suitable initial conditions and
suitable periodization procedure of the noise, the periodic global dynamics of
the DAM converges in spaces of local domains to that of the DAM on the full
space as the period goes to infinity. In Appendix, we also discuss the same
problem for the parabolic Anderson model.

</details>


### [56] [Regularity for a strongly degenerate equation with explicit $u$-dependence](https://arxiv.org/abs/2511.00976)
*Miriam Piccirillo*

Main category: math.AP

TL;DR: Higher differentiability result for degenerate elliptic PDEs with explicit dependence on solution u in the right-hand side.


<details>
  <summary>Details</summary>
Motivation: To extend previous results on degenerate elliptic PDEs by considering cases where the right-hand side explicitly depends on the solution u, which introduces additional complexity.

Method: Analysis of local weak solutions of widely degenerate elliptic PDEs using higher differentiability techniques for the composition of gradient with a suitable vanishing function.

Result: Established higher differentiability for gradient composition under suitable assumptions on the coefficient a(x) and right-hand side b(x,u).

Conclusion: Successfully extended higher differentiability results to degenerate elliptic PDEs with explicit solution dependence in the right-hand side, overcoming the additional challenges this introduces.

Abstract: We consider local weak solutions of widely degenerate elliptic PDEs of the
type \begin{equation}
  \label{equazione mia}
  \mathrm{div}\Biggl(a(x)(|Du|-1)^{p-1}_+\frac{Du}{|Du|}\Biggr)=b(x,u) \ \
\text{ in }\Omega,
  \end{equation} where $2\leq p<\infty,\textbf{ } \Omega$ is an open subset of
$\mathbb{R}^n,n>2,$ and $( \ \cdot \ )_+$ stands for the positive part. We
establish a higher differentiability result for the composition of the gradient
with a suitable function that vanishes in the unit ball for the gradient, under
suitable assumptions on the datum $b(x,u)$ and the coefficient $a(x).$ The
novelty here with respect to previous papers on the subject is that the right
hand side explicitly depends on the solution $u.$

</details>


### [57] [A Volterra Calculus for Lie Groupoids](https://arxiv.org/abs/2511.00991)
*Karsten Bohlen*

Main category: math.AP

TL;DR: Introduces pseudodifferential Volterra calculus for inverting parabolic differential equations on Lie groupoids, enabling study of heat flows on singular manifolds with corners.


<details>
  <summary>Details</summary>
Motivation: To develop mathematical tools for studying fundamental solutions of heat flows on singular manifolds with corners and other geometric bisection covariant heat flows.

Method: Uses pseudodifferential Volterra calculus on Lie groupoids to invert parabolic differential equations and analyze heat kernel properties.

Result: Establishes short time asymptotic expansion for heat kernel of positive elliptic differential operators on Lie groupoids acting on Sobolev Hilbert modules.

Conclusion: The developed calculus provides a framework for analyzing heat flows on singular geometric structures with non-resonant boundary conditions.

Abstract: A pseudodifferential Volterra calculus for inverting parabolic differential
equations on Lie groupoids is introduced. This enables the study of fundamental
solutions of various cases of heat flows on singular manifolds with corners
with non-resonant boundary indicial symbols, such as the $b$-manifolds, as well
as other geometric bisection covariant heat flows. We also establish the short
time asymptotic expansion for the heat kernel of a positive, elliptic
differential operator on a Lie groupoid that acts on suitable Sobolev Hilbert
modules and is positive definite with respect to the appropriate $L^2$ inner
product.

</details>


### [58] [Potential analysis of multi layers optic fiber models](https://arxiv.org/abs/2511.01036)
*Kateryna Buryachenko,Yuliya Kudrych*

Main category: math.AP

TL;DR: Study of pointwise potential estimates for weak nonnegative solutions to double phase elliptic equations with variable nonlinearity powers p(x) and q(x), with applications to optic fiber and cable technologies.


<details>
  <summary>Details</summary>
Motivation: To develop theoretical foundations for modeling and analyzing modern optic fiber and optic cable technologies through mathematical analysis of double phase elliptic equations with variable exponents.

Method: Analysis of pointwise potential estimates for weak nonnegative solutions of double phase elliptic equations with variable powers p(x) and q(x) of nonlinearity.

Result: Obtained theoretical results on pointwise potential estimates for solutions to these equations with variable exponents.

Conclusion: The developed theoretical framework has practical applications in modeling and analyzing modern optic fiber and optic cable technologies.

Abstract: We study pointwise potential estimates of the weak nonnegative solutions for
the double phase elliptic equations with variables powers of nonlinearity:
p(x), q(x). We discuss also the applications of the obtained theoretical
results for the problem of modeling and analyzing of optic fiber and optic
cable modern technologies.

</details>


### [59] [Non-existence of internal mode for small solitary waves of the 1D Zakharov system](https://arxiv.org/abs/2511.01116)
*Yvan Martel,Guillaume Rialland*

Main category: math.AP

TL;DR: The linearised operator around small solitary waves of the 1D Zakharov system has no internal modes.


<details>
  <summary>Details</summary>
Motivation: To study the asymptotic stability of solitary waves in the one-dimensional Zakharov system.

Method: Mathematical proof showing absence of internal modes in the linearised operator around sufficiently small solitary waves.

Result: No internal modes exist in the linearised operator around small solitary waves.

Conclusion: This spectral result is expected to be important for investigating asymptotic stability of solitary waves.

Abstract: We prove that the linearised operator around any sufficiently small solitary
wave of the one-dimensional Zakharov system has no internal mode. This spectral
result, along with its proof, is expected to play a role in the study of the
asymptotic stability of solitary waves.

</details>


### [60] [The TURBO method for well-posedness of the incompressible Euler equations in Sobolev spaces in any domain](https://arxiv.org/abs/2511.01117)
*I. Kukavica,W. S. Ożański*

Main category: math.AP

TL;DR: New method for constructing local-in-time solutions to incompressible Euler equations in Sobolev spaces on bounded domains using analytic approximation and persistence.


<details>
  <summary>Details</summary>
Motivation: To develop a method for solving incompressible Euler equations in Sobolev spaces on arbitrary bounded domains without modifying or regularizing the equations themselves.

Method: Construct analytic solution in analytically approximated domain, then apply analytic persistence to extend the solution given a priori bounds in Sobolev spaces.

Result: Successfully developed a method for constructing local-in-time solutions to incompressible Euler equations in Sobolev spaces.

Conclusion: The method is applicable to many other PDEs and provides a general approach for solving such equations without equation modification.

Abstract: We introduce a new method for constructing local-in-time solutions the
incompressible Euler equations in Sobolev spaces on an arbitrary Sobolev
bounded domain. The method is based on construction of an analytic solution in
an analytically approximated domain, after which we apply analytic persistence
to extend the analytic solution given a priori bounds in Sobolev spaces. The
method does not introduce any modification or regularization of the equations
themselves and appears applicable to many other PDEs.

</details>


### [61] [Boundary estimates for a fully nonlinear Yamabe problem on Riemannian manifolds](https://arxiv.org/abs/2511.01130)
*Weisong Dong,Yanyan Li,Luc Nguyen*

Main category: math.AP

TL;DR: The paper establishes a priori boundary second derivative estimates for fully nonlinear Yamabe equations on Riemannian manifolds with boundary, leading to existence of smooth solutions under subsolution assumptions.


<details>
  <summary>Details</summary>
Motivation: To address the Dirichlet boundary value problem for fully nonlinear Yamabe equations on Riemannian manifolds with boundary, extending classical results to the fully nonlinear case.

Method: Assuming existence of a subsolution, the authors derive a priori boundary second derivative estimates for fully nonlinear Yamabe equations. They also consider a family of equations interpolating between fully nonlinear and classical semi-linear Yamabe equations.

Result: The paper obtains existence of smooth solutions and shows that the estimates remain uniform across the interpolation family. A counterexample of a C¹ solution that is smooth in the interior but not at the boundary is provided.

Conclusion: The work successfully establishes boundary regularity and existence results for fully nonlinear Yamabe equations, with uniform estimates across interpolating equations, while also demonstrating boundary regularity limitations through a counterexample.

Abstract: In this paper, we consider the Dirichlet boundary value problem for fully
nonlinear Yamabe equations on Riemannian manifolds with boundary. Assuming the
existence of a subsolution, we derive \emph{a priori} boundary second
derivative estimates and consequently obtain the existence of a smooth
solution. Moreover, with respect to a family of equations interpolating the
fully nonlinear Yamabe equation and the classical semi-linear Yamabe equation,
our estimates remain uniform. Finally, an example of a $C^1$ solution which is
smooth in the interior but not smooth at the boundary is also given.

</details>


### [62] [Varifold convergence of free boundary Allen--Cahn equation](https://arxiv.org/abs/2511.01204)
*Jingeon An,Kiichi Tashiro*

Main category: math.AP

TL;DR: This paper establishes the free boundary analogue of Hutchinson-Tonegawa theory, developing varifold convergence for free boundary Allen-Cahn solutions to minimal surfaces, along with Γ-convergence of energy and conservation of local minimization.


<details>
  <summary>Details</summary>
Motivation: The free boundary Allen-Cahn equation retains essential features of classical Allen-Cahn while being more tractable, providing a foundation for studying minimal surfaces with potential applications like proving Yau's conjecture.

Method: Developed varifold convergence framework for solutions of free boundary Allen-Cahn equation to minimal surfaces, established Γ-convergence of energy to area functional, and proved conservation of local minimization property.

Result: Successfully established the free boundary analogue of Hutchinson-Tonegawa theory, providing rigorous mathematical foundation for convergence of free boundary Allen-Cahn solutions to minimal surfaces.

Conclusion: This work provides a foundational framework expected to enable further applications of free boundary Allen-Cahn equation in minimal surface theory, potentially leading to alternative proofs of important results like Yau's conjecture with simpler arguments.

Abstract: The free boundary Allen--Cahn equation $\Delta u=0$ in $\{|u|<1\}$, $|\nabla
u|=1/\varepsilon$ on $\partial\{|u|<1\}$ has recently attracted considerable
attention because it retains the essential features of the classical
Allen--Cahn equation while being significantly more tractable. In this work, we
establish the free boundary analogue of the seminal Hutchinson--Tonegawa
theory, developing the varifold convergence framework for solutions of the free
boundary Allen--Cahn equation to minimal surfaces. In addition, we provide the
$\Gamma$-convergence of the free boundary Allen--Cahn energy to the area
functional, and the conservation of local minimization property. This
foundation is expected to be used in further applications of the free boundary
Allen--Cahn equation in the study of minimal surfaces, such as providing an
alternative proof of celebrated Yau's conjecture, possibly with simpler and
more complete arguments.

</details>


### [63] [Remarks on the maximal regularity for parabolic boundary value problems with inhomogeneous data](https://arxiv.org/abs/2511.01230)
*Hui Chen,Su Liang,Tai-Peng Tsai*

Main category: math.AP

TL;DR: Extends derivative estimates for heat equation solutions in upper half space to any order, including fractional derivatives, building on prior work.


<details>
  <summary>Details</summary>
Motivation: Builds on previous research by Ogawa-Shimizu and Chen-Liang-Tsai that established second and first order derivative estimates for heat equation solutions with boundary data in homogeneous Besov spaces.

Method: Extends the existing derivative estimation framework to cover any order of derivatives, including fractional derivatives, for solutions of the heat equation in the upper half space.

Result: Develops comprehensive derivative estimates that generalize previous results to arbitrary derivative orders and fractional derivatives.

Conclusion: Successfully generalizes derivative estimation theory for heat equation solutions, providing a complete framework that includes both integer and fractional derivatives of any order.

Abstract: Inspired by Ogawa-Shimizu [JEE 2022] and Chen-Liang-Tsai [IMRN 2025] on the
second and first order derivative estimates of solution of heat equation in the
upper half space with boundary data in homogeneous Besov spaces, we extend the
estimates to any order of derivatives, including fractional derivatives.

</details>


### [64] [Global existence of weak solutions to a tissue regeneration model](https://arxiv.org/abs/2511.01335)
*Nishith Mohan,Christina Surulescu*

Main category: math.AP

TL;DR: Global existence of weak solutions for a cross-diffusion model of tissue regeneration involving stem cells, chondrocytes, and differentiation factors.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of tissue regeneration through stem cell-chondrocyte interactions in cartilage formation, with periodic growth factor supply.

Method: Analysis of a simplified cross-diffusion PDE model involving cell migration, differentiation, and extracellular matrix interactions. Mathematical proof of global existence of weak solutions.

Result: Successfully proved global existence of weak solutions for the simplified tissue regeneration model.

Conclusion: The simplified cross-diffusion model for cartilage regeneration is mathematically well-posed, providing a foundation for studying more complex tissue regeneration scenarios.

Abstract: We study a cross-diffusion model for tissue regeneration which involves the
dynamics of human mesenchymal stem cells interacting with chondrocytes in a
medium containing a differentiation factor. The latter acts as a
chemoattractant for the chondrocytes and influences the (de)differentiation of
both cell phenotypes. The stem cells perform haptotaxis towards extracellular
matrix expressed by the chondrocytes and degraded by themselves. Cartilage
production as part of the extracellular matrix is ensured by condrocytes. The
growth factor is provided periodically, to maintain the cell dynamics. We
provide a proof for the global existence of weak solutions to this model, which
is a simplified version of a more complex setting deduced in
\cite{surulescu_AMM}.

</details>


### [65] [Well-posedness of a generalized Stokes operator on smooth bounded domains via layer-potentials](https://arxiv.org/abs/2511.01349)
*Mirela Kohr,Victor Nistor,Wolfgang L. Wendland*

Main category: math.AP

TL;DR: The paper proves invertibility of single and double layer potentials for generalized Stokes operators on bounded domains using algebraic tools for limit and jump relations.


<details>
  <summary>Details</summary>
Motivation: To establish the invertibility of layer potentials associated with generalized Stokes operators, which is important for boundary value problems in fluid dynamics.

Method: Developed an 'algebra tool kit' for limit and jump relations of layer operators, first on R^n for distributions supported on hyperplanes, then generalized to non-compact manifolds. Applied these to study layer potentials of generalized Stokes operators.

Result: Proved the Fredholm property and invertibility of single and double layer potentials for generalized Stokes operators when auxiliary potentials satisfy non-vanishing conditions.

Conclusion: The developed algebraic framework successfully establishes the invertibility of layer potentials for generalized Stokes operators, providing tools for boundary value analysis in fluid dynamics.

Abstract: We prove the invertibility of the relevant single and double layer potentials
associated to some generalizations of the Stokes operator on bounded domains.
In order to do that, we first develop an ``algebra tool kit'' to deal with
limit and jump relations of layer operators. We do that first on
$\mathbb{R}^{n}$ for operators acting on a distribution supported on $\{x_{n} =
0\}$ and then in general on (possibly non-compact manifolds). We use these
results to study the limit and jump relations of the layer potential operators
associated to our generalized Stokes operators. In turn, we then use these
results to prove the Fredholm property of single and double layer potentials of
the generalized Stokes operator and even their invertibility when the auxiliary
potentials satisfy suitable non-vanishing conditions.

</details>


### [66] [Optimizers in Sobolev-curl inequalities](https://arxiv.org/abs/2511.01432)
*Jarosław Mederski,Andrzej Szulkin*

Main category: math.AP

TL;DR: The paper proves existence of minimizers for a Sobolev-type inequality involving the p-curl operator in R³, yielding solutions to p-curl-curl equations in critical cases, with applications to nonlinear Maxwell equations and Dirac equations.


<details>
  <summary>Details</summary>
Motivation: The problem is motivated by nonlinear Maxwell equations and the occurrence of zero modes in three-dimensional Dirac equations, addressing quasilinear strongly indefinite problems.

Method: A new variational approach using direct minimization on a Nehari-type constraint, with consideration of symmetry assumptions for minimizer existence.

Result: Existence of minimizers is proven for the p-curl operator Sobolev inequality, providing solutions to p-curl-curl equations in critical cases.

Conclusion: The approach offers a new proof of compactness for minimizing sequences in critical Sobolev inequalities and successfully treats quasilinear strongly indefinite problems.

Abstract: We study a Sobolev-type inequality involving the $p$-curl operator in
$\mathbb{R}^3$. We prove the existence of a minimizer which yields a solution
to the $p$-curl-curl equation in the critical case. The problem is motivated
both by nonlinear Maxwell equations and by the occurrence of zero modes in
three-dimensional Dirac equations. Moreover, we introduce a new variational
approach that allows to treat quasilinear strongly indefinite problems by
direct minimization on a Nehari-type constraint. We also consider existence of
minimizers under some symmetry assumptions. Finally, our approach offers a new
proof of the compactness of minimizing sequences for the Sobolev inequalities
in the critical case.

</details>


### [67] [Boundary and Interior Control in a Diffusive Lotka-Volterra Model](https://arxiv.org/abs/2511.01453)
*João Carlos Barreira,Maicon Sonego,Enrique Zuazua*

Main category: math.AP

TL;DR: The paper proves controllability of a diffusive Lotka-Volterra model using combined boundary and interior controls, achieving both asymptotic control to single-species states and finite-time control to heterogeneous coexistence.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in previous studies by developing integrated control strategies for ecological competition models with practical constraints on controls and system states.

Method: Analysis of a generalized diffusive Lotka-Volterra competition model with boundary controls and interior multiplicative control, using mathematical proofs and numerical simulations.

Result: Proved asymptotic controllability to single-species survival states under arbitrary parameters, and finite-time controllability to heterogeneous coexistence via a two-phase strategy combining boundary and interior controls.

Conclusion: The synergy between boundary and interior controls is crucial for effective control, with potential for future research extensions.

Abstract: We investigate the controllability of a generalized diffusive Lotka-Volterra
competition model for two species, incorporating boundary controls and an
interior multiplicative control. Considering a smooth, bounded N-dimensional
domain, we analyze ecologically pertinent scenarios characterized by
constraints on both the controls and system states. Our results demonstrate how
integrated control strategies can effectively overcome the limitations
identified in previous studies. We prove two main results: (1) asymptotic
controllability to single-species survival steady states under arbitrary system
parameters, ensured by a combination of boundary and interior controls which
act jointly to stabilize the system; and (2) finite-time controllability to a
specific heterogeneous coexistence steady state via a two-phase strategy -
first steering the system near the target with boundary control, then
activating an interior multiplicative control in a localized region. The strong
synergy between the two control mechanisms is crucial in both cases. We also
analyze extinction dynamics and homogeneous coexistence, and support our
findings with numerical simulations. The work concludes with perspectives for
future research.

</details>


### [68] [Gradient bounds for a widely degenerate orthotropic parabolic equation](https://arxiv.org/abs/2511.01480)
*Pasquale Ambrosio*

Main category: math.AP

TL;DR: The paper proves local Lipschitz continuity of weak solutions for a nonlinear parabolic equation with orthotropic structure and strong degeneracy.


<details>
  <summary>Details</summary>
Motivation: To extend previous elliptic results to the parabolic case and handle more degenerate frameworks than existing literature.

Method: Analysis of weak solutions to a nonlinear parabolic equation combining orthotropic structure with strongly degenerate behavior.

Result: Local weak solutions are shown to be locally Lipschitz continuous in the spatial variable, uniformly in time.

Conclusion: The work successfully establishes regularity results for solutions to degenerate parabolic equations with orthotropic structure, extending previous elliptic results to the parabolic setting.

Abstract: In this paper, we consider the following nonlinear parabolic equation \[
\partial_{t}u\,=\,\sum_{i=1}^{n}\partial_{x_{i}}\left[(\vert
u_{x_{i}}\vert-\delta_{i})_{+}^{p-1}\frac{u_{x_{i}}}{\vert
u_{x_{i}}\vert}\right]\,\,\,\,\,\,\,\,\,\,\mathrm{in}\,\,\,\Omega\times I, \]
where $\Omega$ is a bounded open subset of $\mathbb{R}^{n}$ for $n\geq2$,
$I\subset\mathbb{R}$ is a bounded open interval, $p\geq2$,
$\delta_{1},\ldots,\delta_{n}$ are non-negative numbers and
$\left(\,\cdot\,\right)_{+}$ denotes the positive part. We prove that the local
weak solutions are locally Lipschitz continuous in the spatial variable,
uniformly in time. The main novelty here is that the above equation combines an
orthotropic structure with a strongly degenerate behavior. We emphasize that
our result can be considered, on the one hand, as the parabolic counterpart of
the elliptic result established in [12], and on the other hand as an extension
to a significantly more degenerate framework of the findings contained in [13].

</details>


### [69] [An extension of Cabré-Chanillo theorem to the $p$-laplacian](https://arxiv.org/abs/2511.01542)
*Massimo Grossi,Luigi Montoro,Berardino Sciunzi,Zexi Wang*

Main category: math.AP

TL;DR: The paper studies critical points of stable solutions to p-Laplacian equations in 2D domains with non-negative boundary curvature, proving that stable solutions have only internal maxima and possibly zero-index saddle points as critical points.


<details>
  <summary>Details</summary>
Motivation: To understand the structure of critical points for stable solutions of p-Laplacian equations, particularly in two-dimensional domains with geometric constraints on the boundary.

Method: Using a suitable approximation argument to analyze stable solutions of the p-Laplacian equation with p>2 in smooth bounded domains with non-negative curvature boundaries.

Result: Stable solutions have only internal absolute maxima and possibly saddle points with zero index as critical points. The set of absolute maxima is either a single point or a line segment.

Conclusion: The critical point structure of stable p-Laplacian solutions is highly constrained in 2D domains with non-negative boundary curvature, with maxima being the only possible non-degenerate critical points and the maximum set being geometrically simple.

Abstract: In this paper, we study the critical points of stable solutions for the
following $p$-laplacian equation \begin{equation*} \begin{cases}
-div\big(|\nabla u|^{p-2}\nabla u\big)=f(u)&in\ \Om,\\ u>0&in\ \Om,\\ u=0&on\
\partial\Om, \end{cases} \end{equation*} where $p>2$, $f\in C^1([0,+\infty))$
satisfies $f(t)>0$ for $t>0$, and $\Om\subset\R^2$ is a smooth bounded domain
with non-negative curvature of the boundary. Via a suitable approximation
argument, we prove that, a stable solution $u$ admits, as its only critical
point, the internal absolute maxima and possibly saddle points with zero index.
Moreover, $Argmax(u)$ is a point or segment.

</details>


### [70] [Classical Sobolev approach for a critical fourth-order Leray-Lions type problem: existence and multiplicity of solutions](https://arxiv.org/abs/2511.01578)
*Angelo Guimarães,Edcarlos Domingos da Silva,Eduardo. H. Gomes Tavares,Jin-Yun Yuan*

Main category: math.AP

TL;DR: Existence and multiplicity of weak solutions for a fourth-order elliptic problem with combined nonlinearities and Sobolev-critical growth, using variational methods and critical point theory.


<details>
  <summary>Details</summary>
Motivation: To study fourth-order elliptic problems of Leray-Lions type with combined nonlinearities and Sobolev-critical growth under Navier and Dirichlet boundary conditions.

Method: Variational methods and critical point theory in classical Sobolev spaces. For sublinear perturbations: Krasnosel'skii's genus and Clark's deformation lemma. For superlinear perturbations: Mountain Pass Theorem.

Result: In sublinear case: infinitely many solutions exist. In superlinear case: at least one nontrivial solution exists. Results are applicable to Hamiltonian systems.

Conclusion: The paper successfully establishes existence and multiplicity results for fourth-order elliptic problems with critical growth, using different topological and variational approaches depending on the asymptotic behavior of the perturbation term.

Abstract: A fourth-order elliptic problem of Leray-Lions type is considered for
combined nonlinearities and Sobolev-critical growth with Navier and Dirichlet
boundary conditions. By combining variational methods and critical point
theory, the existence and multiplicity of weak solutions are established in the
setting of classical Sobolev spaces. Two distinct asymptotic regimes are
considered for the perturbation term: sublinear and superlinear. In the
sublinear case, the existence of infinitely many solutions is proved by using
topological tools such as Krasnosel'skii's genus and Clark's deformation lemma.
In the superlinear case, the existence of at least one nontrivial solution is
obtained via the Mountain Pass Theorem. Furthermore, the applicability of the
main results is illustrated in the context of Hamiltonian systems.

</details>


### [71] [Chemotaxis guidance of random walkers modeling self-wiring of neural networks](https://arxiv.org/abs/2511.01653)
*Noah Geltner,Ansgar Jüngel*

Main category: math.AP

TL;DR: A stochastic model for growth cone chemotaxis that couples SDEs for cone motion with reaction-diffusion equations for chemical cues, with existence proofs and numerical analysis.


<details>
  <summary>Details</summary>
Motivation: To mathematically model how growth cones navigate using chemical cues during neural development, accounting for both attractive and repulsive signals from cones and somas.

Method: Proposed a coupled system: stochastic differential equations for growth cone movement and reaction-diffusion equations for chemical concentration dynamics, with existence proofs and numerical experiments.

Result: Proved existence of unique solution to the coupled system, analyzed sensitivity to biological parameters, and studied nonlocal regularization effects in deterministic setting.

Conclusion: The model successfully captures chemotactic guidance mechanisms and provides mathematical foundation for studying growth cone navigation.

Abstract: A stochastic walker model is proposed to describe the chemotactic guidance of
growth cones, i.e. the tips of developing neurites. The model accounts for the
influence of both attractive and repulsive chemical cues, which are emitted by
the growth cones and the somas. The system couples stochastic differential
equations governing the motion of the growth cones with reaction-diffusion
equations that describe the dynamics of the chemical concentrations. The
existence of a unique solution to this coupled system is proved. Numerical
experiments are performed to investigate the sensitivity of the model to key
biological parameters. The impact of the nonlocal regularization of point
sources in the reaction-diffusion equations is analyzed in a simplified
deterministic setting.

</details>


### [72] [Inverse stability for hyperbolic equations with different initial conditions](https://arxiv.org/abs/2511.01688)
*Shiqi Ma*

Main category: math.AP

TL;DR: Lipschitz stability recovery for potential and initial conditions using single boundary measurement in hyperbolic boundary value problem, without time reflection step, applicable to fixed angle inverse scattering with single incident wave.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional B-K method requiring time reflection, enabling application to unresolved fixed angle inverse scattering with single incident wave.

Method: Impose pointwise positivity assumption on initial condition differences, propose initial-potential problem, establish new shorter Carleman estimate proof.

Result: Achieved Lipschitz stability recovery for both potential and initial conditions from single boundary measurement, generalizing previous stability results.

Conclusion: Successfully developed method for stable recovery in hyperbolic problems without time reflection, with potential applications to inverse scattering problems.

Abstract: We establish Lipschitz stability recovery for both the potential and the
initial conditions using a single boundary measurement in the context of a
hyperbolic boundary initial value problem. In our setting, the initial
conditions are allow to differ for different potentials. Compared to the
traditional B-K method, our approach does not require the time reflection step.
This advantage makes it possible to apply our method to the fixed angle inverse
scattering problem with only a single incident wave which remains unresolved.
To achieve our result, we impose certain pointwise positivity assumption on the
difference of initial conditions. The assumption generalizes previous stability
results that usually assume the difference to be zero. We propose the
initial-potential problem and prove a potential inverse stable recovery result
of it. The initial-potential problem plays as an attempt to relate initial
boundary value problem with the scattering problem, and to explore the
possibility to relax the positivity requirement on the initial data. We also
establish a new pointwise Carleman estimate, whose proof is significantly
shorter than traditional ones.

</details>


### [73] [Homogeneous optimal transport maps between oblique cones](https://arxiv.org/abs/2511.01692)
*Tristan C. Collins,Benjy Firester,Freid Tong*

Main category: math.AP

TL;DR: Construction of homogeneous optimal transport maps for quadratic cost between convex cones with homogeneous densities under obliqueness condition.


<details>
  <summary>Details</summary>
Motivation: To establish existence of homogeneous optimal transport maps, which is crucial for boundary regularity theory of optimal transport maps between convex domains and for existence of complete Calabi-Yau metrics on certain quasi-projective varieties.

Method: Construct homogeneous optimal transport maps for quadratic cost between convex cones with homogeneous (possibly degenerate) densities, requiring cones to satisfy an obliqueness condition.

Result: Successfully constructed homogeneous optimal transport maps under the specified conditions.

Conclusion: The constructed maps play a central role in boundary regularity theory for optimal transport and are relevant for existence of complete Calabi-Yau metrics on quasi-projective varieties.

Abstract: We construct homogeneous optimal transport maps for the quadratic cost
between convex cones with homogeneous, possibly degenerate, densities when the
cones satisfy an obliqueness condition. The existence of such maps plays a
central role in the boundary regularity theory for optimal transport maps
between convex domains. Our results are also relevant for the existence of
complete Calabi-Yau metrics on certain quasi-projective varieties.

</details>


### [74] [Constrained hydrodynamic flocking models in the limit of large attraction-repulsion interactions](https://arxiv.org/abs/2511.01721)
*Thierry Goudon,Antoine Mellet*

Main category: math.AP

TL;DR: The paper studies how populations of particles/organisms with attraction-repulsion interactions and external velocity fields concentrate into moving domains, with shape determined by interaction energy minimization and motion driven by external forces.


<details>
  <summary>Details</summary>
Motivation: To understand collective dynamics in biological and physical systems where self-consistent interactions compete with external forces, particularly in the strong interaction limit.

Method: Uses mean-field kinetic models and analyzes the singular limit of strong interaction forces. Employs minimization of interaction energy for domain shape and modulated energy method for internal flow analysis.

Result: Shows population concentrates within moving domains Ω(t)=Ω₀+X(t), where Ω₀ minimizes interaction energy and X(t) follows external forces. Internal flow described by classical hydrodynamic lake equation.

Conclusion: The analysis provides rigorous mathematical framework for understanding how interaction forces and external fields shape collective behavior, with general results for domain formation and more specific conditions for internal flow dynamics.

Abstract: We study the collective dynamics of a population of particles/organisms
subject to self-consistent attraction-repulsion interactions and an external
velocity field. The starting point of our analysis is a mean-field kinetic
model and we investigate the singular limit corresponding to strong interaction
forces. For well-prepared initial data, we show that the population
asymptotically concentrates within a domain $\Omega(t)=\Omega_0+X(t)$ whose
shape $\Omega_0$ is determined by the minimization of the interaction energy
while the evolution of the domain's center of mass $X(t)$ is determined by the
external force field. In addition, we show that the internal flow of organisms
within this moving domain is described by a classical hydrodynamic model (the
lake equation). The first part of our result relies only on the existence and
uniqueness of minimizers for the interaction energy and holds for rather
general interaction kernels. The second part is proved using a modulated energy
method under more restrictive conditions on the nature of the interactions, and
assuming that the limiting lake equation admits strong solutions.

</details>


### [75] [The Regularity of Critical Points to the Dirichlet Energy of the Mean Curvature in Dimension 4](https://arxiv.org/abs/2511.01765)
*Yann Bernard,Tian Lan,Dorian Martino,Tristan Rivière*

Main category: math.AP

TL;DR: Analytic regularity for critical points of Dirichlet Energy of Mean Curvature for 4D manifolds in R^5


<details>
  <summary>Details</summary>
Motivation: Study scaling invariant Lagrangians in 4D curvature energies depending on first and second fundamental forms

Method: Use integrability by compensation theory and interpolation spaces estimates

Result: Prove weak immersions are analytic in local harmonic charts

Conclusion: Critical points exhibit analytic regularity despite critical variational problem

Abstract: We prove that weak immersions of four dimensional manifolds in $\mathbb{R}^5$
which are critical points to the Dirichlet Energy of the Mean Curvature are
analytic in any given local harmonic chart. This variational problem is a model
case for the large family of scaling invariant Lagrangians, hence critical, of
curvature energies in 4 dimensions depending on the first and the second
fundamental form. Because of the criticality of this variational problem, the
regularity result is obtained through an abundant use of methods from
integrability by compensation theory such as interpolation spaces estimates.

</details>


### [76] [Singularity Formation in the Incompressible Porous Medium Equation without Boundary Mass](https://arxiv.org/abs/2511.01827)
*Kevin H. Dembski*

Main category: math.AP

TL;DR: Finite-time singularity formation is proven for Lipschitz continuous solutions of the inviscid porous medium equation that vanish on the domain boundary.


<details>
  <summary>Details</summary>
Motivation: To understand singularity formation in inviscid porous medium equations despite the regularizing effect of transport when density vanishes on boundaries.

Method: Analysis of Lipschitz continuous solutions that vanish on domain boundaries, with solutions smooth away from origin and compactly supported density.

Result: Proved finite-time singularity formation occurs for these solutions, overcoming the full regularizing effect of transport.

Conclusion: Singularities can form in finite time for inviscid porous medium equations even with boundary vanishing conditions and transport regularization.

Abstract: We prove finite-time singularity formation for Lipschitz continuous solutions
of the inviscid porous medium equation which vanish on the boundary of the
domain. As the density vanishes on the boundary of the domain, the full
regularizing effect of transport is present and must be overcome. The solutions
are smooth away from the origin and the density can be made compactly
supported.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [77] [Biomechanical and Mechanobiological Modelling of Functionally Graded Scaffolds for Large Bone Defects](https://arxiv.org/abs/2511.00743)
*Ali Entezari,Vahid Badali,Sara Checa*

Main category: physics.comp-ph

TL;DR: An integrated finite element agent-based modeling framework was developed to evaluate functionally graded scaffolds for bone regeneration, showing axial pore gradients promote bone ingrowth while radial gradients enhance structural strength.


<details>
  <summary>Details</summary>
Motivation: Critical sized bone defects require scaffolds that combine mechanical stability with regenerative capacity, and functionally graded scaffolds inspired by native bone architecture offer a promising solution.

Method: Integrated finite element agent-based modeling (FEA ABM) framework that simulates biomechanics and regeneration under physiological conditions, comparing cylindrical scaffolds with axial or radial pore size gradients against uniform controls.

Result: Axial gradients with larger pores at the host bone interface promoted greater bone ingrowth, while radial gradients with denser peripheral struts substantially reduced peak von Mises stresses, revealing a design trade-off between regeneration and structural competence.

Conclusion: The coupled FEA ABM framework provides a mechanistic platform for rational design of next-generation functionally graded scaffolds, enabling preclinical optimization of implants tailored to specific defect locations and loading environments.

Abstract: Critical sized bone defects remain a major clinical challenge, requiring
scaffolds that combine mechanical stability with regenerative capacity.
Functionally graded (FG) scaffolds, inspired by the graded architecture of
native bone, offer a promising solution by spatially varying porosity to
optimise both load transfer and tissue ingrowth. Here, we present an integrated
finite element agent based modelling (FEA ABM) framework to simultaneously
evaluate the biomechanics and regenerative potential of FG scaffolds under
physiologically relevant conditions. Cylindrical scaffolds with axial or radial
pore size gradients were compared with uniform controls. The finite element
model incorporated poroelastic tissue mechanics and gait related loading to
compute local shear strain and fluid velocity, which guided cellular behaviours
in the agent based model, including progenitor migration, proliferation,
differentiation, and apoptosis. Simulations over 150 days revealed that axial
gradients with larger pores at the host bone interface promoted greater bone
ingrowth, while radial gradients with denser peripheral struts substantially
reduced peak von Mises stresses. These findings highlight a fundamental design
trade off between maximising regenerative performance and enhancing structural
competence. The coupled FEA ABM framework establishes a mechanistic platform
for the rational design of next-generation FG scaffolds, offering a pathway
toward preclinical optimisation of implants tailored to defect location and
loading environment.

</details>


### [78] [Integrated photonic multigrid solver for partial differential equations](https://arxiv.org/abs/2511.01005)
*Timoteo Lee,Frank Brückerhoff-Plückelmann,Jelle Dijkstra,Jan M. Pawlowski,Wolfram Pernice*

Main category: physics.comp-ph

TL;DR: A photonic multigrid solver that offloads computationally intensive smoothening operations to optical domain, achieving up to 97% reduction in digital operations for PDE solving.


<details>
  <summary>Details</summary>
Motivation: Conventional computers struggle with large-scale PDE solving, while photonic processors offer ultrafast hardware acceleration potential.

Method: Mixed-precision photonic multigrid solver that uses low-latency photonic matrix vector multipliers for smoothening operations, tested on integrated photonic accelerator at 2 GSPS.

Result: 80% reduction in digital operations for Poisson and Schrödinger equations, and up to 97% reduction for lattice quantum chromodynamics calculations.

Conclusion: Photonic multigrid solvers enable order-of-magnitude gains in computational speed and efficiency for PDE solving applications.

Abstract: Solving partial differential equations is crucial to analysing and predicting
complex, large-scale physical systems but pushes conventional high-performance
computers to their limits. Application specific photonic processors are an
exciting computing paradigm for building efficient, ultrafast hardware
accelerators. Here, we investigate the synergy between multigrid based partial
differential equations solvers and low latency photonic matrix vector
multipliers. We propose a mixed-precision photonic multigrid solver, that
offloads the computationally demanding smoothening procedure to the optical
domain. We test our approach on an integrated photonic accelerator operating at
2 GSPS solving a Poisson and Schr\"odinger equation. By offloading the
smoothening operation to the photonic system, we can reduce the digital
operation by more than 80%. Finally, we show that the photonic multigrid solver
potentially reduces digital operations by up to 97 % in lattice quantum
chromodynamics (LQCD) calculations, enabling an order-of-magnitude gain in
computational speed and efficiency.

</details>


### [79] [BzScope: an absolute cross section calculator for neutron-phonon scattering](https://arxiv.org/abs/2511.01178)
*Ming Tang,Zi-Yi Pan,Ni Yang,Xiao-Xiao Cai*

Main category: physics.comp-ph

TL;DR: BzScope is a Python package for calculating neutron-phonon inelastic scattering cross sections in crystalline powders, using an adapted integral method to overcome limitations of traditional histogramming techniques and supporting calculations up to high momentum transfers.


<details>
  <summary>Details</summary>
Motivation: Traditional histogramming techniques have limitations in reproducing sharp structures and ensuring convergence in neutron-phonon scattering calculations, particularly for complex crystalline materials.

Method: Uses an adapted integral method for single- and two-phonon scattering functions, with higher-order scatterings calculated via incoherent approximation using NCrystal. Provides numerical robustness up to 100 Ang^-1 momentum transfer and integrates with NCrystal via a plugin for Monte Carlo simulations.

Result: Validation shows good agreement with NCrystal for cubic systems like Ni, improved accuracy for low-symmetry materials like NiP2 by avoiding isotropic displacement approximations, and reliable benchmarks with experimental data for LiH and Be.

Conclusion: BzScope enhances efficiency and accuracy in neutron scattering simulations, advancing condensed matter dynamics research through its integration with NCrystal-enabled Monte Carlo packages.

Abstract: BzScope is a Python package designed for efficiently calculating absolute
cross sections of neutron-phonon inelastic scattering for crystalline powders
in large phase spaces, addressing the limitations of traditional histogramming
techniques in reproducing sharp structures and ensuring convergence. The
package employs an adapted integral method and supports calculations of single-
and two-phonon scattering functions in ideal crystalline powders, with
numerical robustness up to a momentum transfer of 100 Ang^-1. Higher order
scatterings up to several hundred orders are calculated by incoherent
approximation in a well-established thermal neutron scattering physics package,
NCrystal. In addition, a NCrystal plugin is made available for NCrystal-enabled
Monte Carlo packages, facilitating direct comparison between the new physics
and experimental data.
  Validation against NCrystal demonstrates good agreement in incoherent
scattering for cubic systems Ni. In addition, it shows improved accuracy for
low-symmetry materials $NiP_2$ by avoiding the isotropic atomic displacement
approximations in NCrystal. Benchmarks the experimental differential cross
section of LiH and total cross section of Be confirm its reliability.
  BzScope integrates with NCrystal via a plugin and therefore can be directly
used in any NCrystal-enabled Monte Carlo package. This tool enhances the
efficiency and accuracy of neutron scattering simulations, advancing the study
of condensed matter dynamics.

</details>


### [80] [A fast and rigorous numerical tool to measure length-scale artifacts in molecular simulations](https://arxiv.org/abs/2511.01442)
*Benedikt M. Reible,Nils Liebreich,Carsten Hartmann,Luigi Delle Site*

Main category: physics.comp-ph

TL;DR: The paper presents a numerical algorithm for computing a quality factor that quantifies statistical-mechanical consistency based on the two-sided Bogoliubov inequality, which provides rigorous bounds on free-energy cost when partitioning systems.


<details>
  <summary>Details</summary>
Motivation: To develop a practical method for quantifying the degree of statistical-mechanical consistency achieved by simulation box sizes in many-body systems, motivated by the two-sided Bogoliubov inequality theorem.

Method: A numerical algorithm that computes the quality factor by evaluating two six-dimensional integrals, applicable to systems with two-body interactions and known radial distribution functions.

Result: The algorithm demonstrates consistency with results from literature obtained from simulations performed at different box sizes.

Conclusion: The proposed numerical algorithm successfully computes the quality factor and validates its consistency with existing simulation results, providing a practical tool for assessing statistical-mechanical consistency in many-body system simulations.

Abstract: The two-sided Bogoliubov inequality for classical and quantum many-body
systems is a theorem that provides rigorous bounds on the free-energy cost of
partitioning a given system into two or more independent subsystems. This
theorem motivates the definition of a quality factor which directly quantifies
the degree of statistical-mechanical consistency achieved by a given simulation
box size. A major technical merit of the theorem is that, for systems with
two-body interactions and a known radial distribution function, the quality
factor can be computed by evaluating just two six-dimensional integrals. In
this work, we present a numerical algorithm for computing the quality factor
and demonstrate its consistency with respect to results in the literature
obtained from simulations performed at different box sizes.

</details>


### [81] [Simulation of Self-Assembled Monolayers of Polyalanine $α$-Helix Using an Effective Potential](https://arxiv.org/abs/2511.01596)
*Hadis Ghodrati Saeini,Kevin Preis,Thi Ngoc Ha,Christoph Tegenkamp,Sibylle Gemming,Jeffrey Kelling,Florian Günther*

Main category: physics.comp-ph

TL;DR: Self-assembled monolayers of α-polyalanine helices show different structural phases based on chiral composition, with enantiopure systems forming hexagonal lattices and racemic mixtures forming rectangular phases with stripe patterns.


<details>
  <summary>Details</summary>
Motivation: To understand how chiral composition governs supramolecular organization in peptide-based materials and establish structure-property relationships for designing spintronic materials.

Method: Combined scanning tunneling microscopy (STM) with theoretical modeling using SCC-DFTB derived interaction potentials to analyze structural phases and binding interactions.

Result: Opposite-handed helix pairs exhibit stronger binding and closer packing, explaining denser racemic structures. STM contrast arises from anti-parallel alignment of opposite-handed helices rather than physical height variations.

Conclusion: These findings establish fundamental structure-property relationships that can guide the design of peptide-based spintronic materials with controlled chiral-induced spin selectivity.

Abstract: Self-assembled monolayers of $\alpha$-polyalanine helices exhibit distinct
structural phases with implications for chiral-induced spin selectivity. We
combine scanning tunneling microscopy and theoretical modeling to reveal how
chiral composition governs supramolecular organization. Enantiopure systems
form hexagonal lattices, while racemic mixtures organize into rectangular
phases with stripe-like features. Our SCC-DFTB derived interaction potentials
show that opposite-handed helix pairs exhibit stronger binding and closer
packing, explaining the denser racemic structures. Crucially, we demonstrate
that the observed STM contrast arises from anti-parallel alignment of
opposite-handed helices rather than physical height variations. These findings
establish fundamental structure-property relationships for designing
peptide-based spintronic materials.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [82] [Observation of Ion-wave Satellites to Laser Harmonics in Intense Picosecond Laser-Solid Interaction](https://arxiv.org/abs/2511.00174)
*R. S. Marjoribanks,L. Zhao,F. W. Budnik,G. Kulcsar,R. Wagner,D. Umstadter,R. P. Drake,M. C. Downer*

Main category: physics.plasm-ph

TL;DR: Observation of red- and blue-shifted satellites in harmonic spectra from ultra-intense laser-solid interactions, explained by reduced Debye shielding due to intense optical fields.


<details>
  <summary>Details</summary>
Motivation: To understand the origin of regular frequency-shifted satellites observed in harmonic spectra from ultra-intense laser-solid interactions.

Method: Analysis of detailed harmonic spectra from sub-picosecond, high-contrast laser pulses incident on solid targets, comparing observed frequency shifts with theoretical expectations.

Result: First observation of regular red- and blue-shifted satellites with frequency shifts slightly less than expected for pure ion-plasma waves, indicating substantial reduction of Debye shielding.

Conclusion: Intense optical fields reduce Debye shielding through large-amplitude electron oscillations, creating a larger, dynamical and anisotropic Debye length with broad implications for plasma physics in this regime.

Abstract: Detailed spectra of harmonics produced from ultra-intense, sub-picosecond,
high-contrast laser pulses incident on solid targets have shown the first
observation of regular red- and blue-shifted satellites. Their frequency shift
is slightly less than the frequency of a nominal, pure ion-plasma wave
associated with electron critical density, where an ion-acoustic wave would be
expected. We explain this as the result of a substantial reduction of Debye
shielding as the intense optical fields compete to drive electrons in
large-amplitude oscillations. This general effect leads to a larger, dynamical
and anisotropic Debye length, which should have a broad impact on plasma
physics in this regime.

</details>


### [83] [Experimental investigation of plasma-electrode interactions on the ZaP-HD sheared-flow-stabilized Z-pinch device](https://arxiv.org/abs/2511.00354)
*Amierul Aqil Khairi,Uri Shumlak*

Main category: physics.plasm-ph

TL;DR: The ZaP-HD sheared-flow-stabilized Z-pinch device investigates plasma-electrode interactions, showing that carbon electrode erosion occurs primarily through sublimation rather than sputtering, with evidence of electrode recycling and self-healing through redeposition.


<details>
  <summary>Details</summary>
Motivation: To understand plasma-electrode interactions in Z-pinch devices and investigate electrode erosion mechanisms to enable successful operation of solid electrodes in extreme plasma environments.

Method: Used in-situ S/XB spectroscopy to measure carbon erosion flux, analyzed ionization mean free paths, and conducted ex-situ analysis using removable coupons with varying pinch currents and pulse numbers to study surface morphology and mass loss.

Result: Sublimation dominates over physical sputtering for carbon erosion; erosion rates range from 0.01 to 0.1 mg/C; electrode recycling occurs through redeposition; surface shows smoothing except at high fluence with crack formation from thermal cycles.

Conclusion: Electrode erosion in SFS Z-pinch aligns with high-powered arc discharges, providing confidence for managing electrode erosion in this configuration through understanding of sublimation-dominated processes and recycling mechanisms.

Abstract: The ZaP-HD sheared-flow-stabilized (SFS) Z-pinch device is a testbed for
experimental investigation of plasma-electrode interactions. The graphite
electrode is exposed to a high temperature, high density Z-pinch plasma while
supplying large pinch currents. In-situ measurements of the gross carbon
erosion flux obtained with S/XB spectroscopy exceed the expected flux from
physical sputtering, but have reasonable agreement with the expected
sublimation flux. Comparison of the ionization mean free paths of neutrals
produced through both erosion processes shows that sublimated carbon is ionized
within the sheath while sputtered carbon is ionized beyond the sheath. This
suggests a process of electrode recycling and self-healing through
redeposition. The sputtered carbon is primarily responsible for net erosion.
Ex-situ analysis of electrode material is enabled by the design of a removable
coupon. Three different plasma exposure conditions varied the pinch current and
number of pulses. Net mass loss measurements support the physical picture of
electrode recycling. Erosion rates range from 0.01 to 0.1 mg/C, which are
comparable to existing arc discharge devices. Measurements of the microscopic
surface morphology and roughness reveal irregular consolidated structures and
general smoothing except at high particle fluence. Crack formation suggests the
importance of repetitive thermal cycles. Definitive features of sputtering such
as pitting and cratering are absent, although further study is needed to
attribute the observed changes to other processes. These results indicate some
alignment with erosion processes in high-powered arc discharges, which
successfully operate solid electrodes in extreme environments. This provides
confidence in managing electrode erosion in the SFS Z-pinch configuration.

</details>


### [84] [Nonlinear effects in light-ion stopping powers within real-time time-dependent density functional theory](https://arxiv.org/abs/2511.00759)
*Alina Kononov,Thomas W. Hentschel,Stephanie B. Hansen,Andrew D. Baczewski*

Main category: physics.plasm-ph

TL;DR: TDDFT calculations reveal nonlinear effects modify stopping powers in warm dense matter by ~10% for low-Z ions near Bragg peak, challenging linear-response assumptions in fusion energy models.


<details>
  <summary>Details</summary>
Motivation: To investigate limitations of linear-response models for electronic stopping power in inertial fusion energy, which assume quadratic scaling with projectile charge.

Method: Used real-time time-dependent density functional theory (TDDFT) calculations to analyze stopping powers in warm dense matter.

Result: Found nonlinear processes modify stopping powers by about 10% for low-Z ions near and below Bragg peak. Analytic effective charge models capture some qualitative aspects but lack quantitative accuracy.

Conclusion: Nonlinear effects significantly impact stopping power calculations, suggesting need for improved models beyond linear-response assumptions for fusion energy applications.

Abstract: Electronic stopping power models describing fuel heating processes in
inertial fusion energy concepts typically assume linear-response behavior
through quadratic scaling with the projectile charge. We report the results of
real-time time-dependent density functional theory (TDDFT) calculations
indicating that even for low-Z ions, nonlinear processes modify stopping powers
in warm dense matter by about 10% near and below the Bragg peak. By describing
partial neutralization of slow ions, analytic effective charge models capture
some qualitative aspects of the TDDFT results but do not always offer
quantitative accuracy. Cases where the effective charge inferred from TDDFT
exceeds the bare ion charge suggest that more complex nonlinear effects also
contribute. These findings will inform future improvements to more efficient
stopping power models.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [85] [Exploring the limit of the Lattice-Bisognano-Wichmann form describing the Entanglement Hamiltonian: A quantum Monte Carlo study](https://arxiv.org/abs/2511.00950)
*Siyi Yang,Yi-Ming Ding,Zheng Yan*

Main category: cond-mat.str-el

TL;DR: A general scheme using lattice-Bisognano-Wichmann ansatz and quantum Monte Carlo methods to reconstruct entanglement Hamiltonians in 2D systems, extending beyond Lorentz-invariant cases to systems without translational invariance.


<details>
  <summary>Details</summary>
Motivation: The entanglement Hamiltonian is crucial for understanding quantum entanglement properties, but its general analytical form remains largely unknown, especially for lattice systems where Lorentz invariance is absent and the Bisognano-Wichmann theorem has limited applicability.

Method: Combined lattice-Bisognano-Wichmann ansatz with multi-replica-trick quantum Monte Carlo methods to numerically reconstruct entanglement Hamiltonians in two-dimensional systems, systematically exploring applicability to systems without translational invariance.

Result: Successfully reconstructed entanglement Hamiltonians across various quantum phases including gapped/gapless phases, critical points, and phases with discrete/continuous symmetry breaking. Found that LBW ansatz provides accurate approximation beyond Lorentz-invariant cases when entanglement boundary is ordinary (free from surface anomalies).

Conclusion: Established a general framework for investigating the analytical structure of entanglement in complex quantum many-body systems, demonstrating versatility across diverse quantum phases and extending applicability beyond original Bisognano-Wichmann theorem scope.

Abstract: The entanglement Hamiltonian (EH) encapsulates the essential entanglement
properties of a quantum many-body system and serves as a powerful theoretical
construct. From the EH, one can extract a variety of entanglement quantities,
such as entanglement entropies, negativity, and the entanglement spectrum.
However, its general analytical form remains largely unknown. While the
Bisognano-Wichmann theorem gives an exact EH form for Lorentz-invariant field
theories, its validity on lattice systems is limited, especially when Lorentz
invariance is absent. In this work, we propose a general scheme based on the
lattice-Bisognano-Wichmann (LBW) ansatz and multi-replica-trick quantum Monte
Carlo methods to numerically reconstruct the entanglement Hamiltonian in
two-dimensional systems and systematically explore its applicability to systems
without translational invariance, going beyond the original scope of the
primordial Bisognano-Wichmann theorem. Various quantum phases--including gapped
and gapless phases, critical points, and phases with either discrete or
continuous symmetry breaking--are investigated, demonstrating the versatility
of our method in reconstructing entanglement Hamiltonians. Furthermore, we find
that when the entanglement boundary of a system is ordinary (i.e., free from
surface anomalies), the LBW ansatz provides an accurate approximation well
beyond Lorentz-invariant cases. Our work thus establishes a general framework
for investigating the analytical structure of entanglement in complex quantum
many-body systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [86] [Closed-loop calculations of electronic structure on a quantum processor and a classical supercomputer at full scale](https://arxiv.org/abs/2511.00224)
*Tomonori Shirakawa,Javier Robledo-Moreno,Toshinari Itoko,Vinay Tripathi,Kento Ueda,Yukio Kawashima,Lukas Broers,William Kirby,Himadri Pathak,Hanhee Paik,Miwako Tsuji,Yuetsu Kodama,Mitsuhisa Sato,Constantinos Evangelinos,Seetharami Seelam,Robert Walkup,Seiji Yunoki,Mario Motta,Petar Jurcevic,Hiroshi Horii,Antonio Mezzacapo*

Main category: quant-ph

TL;DR: Largest quantum-classical hybrid computation using Heron quantum processor and Fugaku supercomputer to calculate electronic structure beyond exact diagonalization limits


<details>
  <summary>Details</summary>
Motivation: To understand quantum-classical computing interactions and characterize scalability/efficiency of hybrid workflows, pushing beyond current scale limitations

Method: Closed-loop workflow between quantum processors and 152,064 classical nodes of Fugaku supercomputer to approximate electronic structure of chemistry models

Result: Achieved electronic structure calculations beyond exact diagonalization reach with accuracy comparable to classical approximation methods

Conclusion: Demonstrates largest-scale integration of quantum and classical high-performance computing, showcasing unprecedented computational resource orchestration

Abstract: Quantum computers must operate in concert with classical computers to deliver
on the promise of quantum advantage for practical problems. To achieve that, it
is important to understand how quantum and classical computing can interact
together, and how one can characterize the scalability and efficiency of hybrid
quantum-classical workflows. So far, early experiments with quantum-centric
supercomputing workflows have been limited in scale and complexity. Here, we
use a Heron quantum processor deployed on premises with the entire
supercomputer Fugaku to perform the largest computation of electronic structure
involving quantum and classical high-performance computing. We design a
closed-loop workflow between the quantum processors and 152,064 classical nodes
of Fugaku, to approximate the electronic structure of chemistry models beyond
the reach of exact diagonalization, with accuracy comparable to some
all-classical approximation methods. Our work pushes the limits of the
integration of quantum and classical high-performance computing, showcasing
computational resource orchestration at the largest scale possible for current
classical supercomputers.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [87] [Recovering functions via doubly homogeneous nonlocal gradients](https://arxiv.org/abs/2511.01606)
*Stefano Buccheri,Augusto C. Ponce*

Main category: math.FA

TL;DR: Analysis of nonlocal gradients with different homogeneities at zero and infinity, including representation formulas and Sobolev-type inequalities.


<details>
  <summary>Details</summary>
Motivation: To study nonlocal operators with distinct scaling behaviors at different scales and establish fundamental mathematical properties.

Method: Developed representation formulas for doubly homogeneous operators and derived associated Sobolev-type inequalities through mathematical analysis.

Result: Established representation formulas for nonlocal gradients with distinct homogeneities and proved corresponding Sobolev-type inequalities.

Conclusion: The work provides mathematical foundations for doubly homogeneous nonlocal operators and identifies open questions for future research inspired by Haim Brezis.

Abstract: We investigate a class of nonlocal gradients featuring distinct homogeneities
at zero and infinity. We establish a representation formula for such doubly
homogeneous operators and derive associated Sobolev-type inequalities. We also
propose open questions linked to our results, suggesting directions for future
research inspired by the work of Haim Brezis.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [88] [Elastic and Strain--Tunable Electronic and Optical Properties of La2AlGaO6 Hybrid Perovskite: A First-Principles Study](https://arxiv.org/abs/2511.00430)
*Chaithanya Purushottam Bhat,Joyti Dagar,Ashwin K. Godbole,Debashis Bandyopadhyay*

Main category: cond-mat.mtrl-sci

TL;DR: DFT study of orthorhombic La2AlGaO6 perovskite reveals mechanical stability, elastic properties, and strain-tunable electronic/optical characteristics.


<details>
  <summary>Details</summary>
Motivation: To investigate the elastic, mechanical, electronic, and optical properties of La2AlGaO6 perovskite for advanced electronic and optoelectronic applications.

Method: First-principles DFT calculations using LDA and GGA approximations, structural optimization, Born-Huang criteria analysis, and biaxial strain engineering.

Result: Confirmed mechanical stability, derived elastic constants and mechanical parameters, and demonstrated significant band gap modulation under compressive/tensile strain affecting optical properties.

Conclusion: LAGO shows potential for tunable device applications where mechanical stimuli can tailor electronic functionality through elastic-electronic coupling.

Abstract: Perovskite materials, known for their structural versatility and
multifunctional properties, continue to draw interest for advanced electronic
and optoelectronic applications. In this study, we investigate the elastic and
strain--engineered mechanical, electronic properties and optical properties of
the orthorhombic La2AlGaO6 (LAGO) hybrid perovskite using first--principles
quantum mechanical calculations based on density functional theory (DFT).
Structural optimizations were performed using both the local density
approximation (LDA) and the generalized gradient approximation (GGA). The
mechanical stability of LAGO was confirmed through the Born--Huang criteria,
and key elastic constants (C11, C12, C33, C44, and C66) were evaluated. These
constants were further used to derive mechanical parameters such as Young's
modulus, bulk modulus, shear modulus, Poisson's ratio, Cauchy's pressure, and
anisotropic factor, offering insights into the material's ductility, hardness,
and elastic anisotropy. Crucially, we explored the influence of biaxial strain
on the electronic band structure, DOS/PDOS, and Fermi energy, revealing
significant band gap modulation under compressive and tensile strain, and
hence, varying the optical properties. The coupling between elastic response
and electronic structure highlights LAGO's potential for tunable device
applications, where mechanical stimuli can be employed to tailor its electronic
functionality.

</details>


### [89] [Intertwined Hyperferroelectricity, Tunable Multiple Topological Phases and Giant Rashba Effect in Wurtzite LiZnAs](https://arxiv.org/abs/2511.01370)
*Saurav Patel,Paras Patel,Shaohui Qiu,Prafulla K. Jha*

Main category: cond-mat.mtrl-sci

TL;DR: The paper presents a unified framework for achieving intertwined hyperferroelectricity, topological phases, and Rashba spin-splitting in LiZnAs compound through crystalline symmetries and spin-orbit coupling, enabling reversible spin texture control.


<details>
  <summary>Details</summary>
Motivation: To uncover complex interrelations between distinct quantum phenomena and develop advanced materials for nonvolatile and spintronics applications by synergizing hyperferroelectricity, topological phases, and Rashba effects.

Method: First-principles calculations using VASP and WIEN2k to analyze LiZnAs compound, examining unstable phonon modes, effective charges, dielectric constants, and biaxial strain effects on topological phase transitions.

Result: Stable hyperferroelectricity with polarization of 0.282 C/m², giant Rashba coefficients (5.91 eV·Å and 2.42 eV·Å), strain-induced topological phase transitions to Weyl semimetal and topological insulator phases, and reversible spin texture switching via polarization control.

Conclusion: LiZnAs demonstrates a robust synergy of hyperferroelectricity, topological phases, and Rashba effects, providing a mechanism to control spin degrees of freedom for advanced quantum material applications.

Abstract: Composite quantum compounds offer a fertile ground for uncovering the complex
interrelations between seemingly distinct phenomena in condensed matter physics
for advanced nonvolatile and spintronics applications. Beyond topological
superconductors and axion insulators, the idea of intertwined
Hyperferroelectricity (HyFE), multiple topological phases and Rashba
spin-splitting with reversible spin textures represents the local, global and
symmetry-driven characteristics of quantum materials, respectively, offering
unique pathways for enhanced functionalities. We unveiled a unified framework
to achieve this synergy through the presence of crystalline symmetries and
spin-orbit coupling in LiZnAs compound using first-principles calculations.
HyFE exhibits ability to maintain spontaneous polarization under open-circuit
boundary conditions, even with existence of depolarization field while Rashba
effect exhibits paradigmatic spin texture in momentum space with tangential
vector field. The presence of unstable $A_{2u}(LO)$ mode leads to free energy
minimum with significant well depth and polarization of -66 meV and $P_{HyFE} =
0.282~C/m^2$, respectively indicating stable HyFE. The robust HyFE stem from
mode-specific effective charges and larger high-frequency dielectric constants.
This study also addresses the subtle question of whether critical point of
topological phase transition shifts in response to drastically different Rashba
spin-splitting values obtained from VASP and WIEN2k. Moreover, biaxial strain
(BAS) induced Weyl semimetal (at 3.4% BAS) and topological insulating phase
(after 3.4% BAS) is observed with giant Rashba coefficient of 5.91 eV {\AA} and
2.42 eV {\AA}, respectively. Furthermore, switching of bulk polarization leads
to spin texture reversal, providing a robust mechanism to leverage spin degrees
of freedom in these Hyperferroelectric Rashba topological materials.

</details>


### [90] [Thermal tuning of dynamic response in Ag-based nanowire networks](https://arxiv.org/abs/2511.01792)
*J. I. Diaz Schneider,C. Gomez,C. Acha,P. E. Levy,E. D. Martínez,C. P. Quinteros*

Main category: cond-mat.mtrl-sci

TL;DR: Study of electrical transport in self-assembled silver nanowire networks with insulating coatings, showing how annealing and AC stimulation transform resistive networks into capacitive systems suitable for neuromorphic applications.


<details>
  <summary>Details</summary>
Motivation: To explore metallic nanowire networks as platforms for neuromorphic computing by understanding how their electrical properties can be dynamically modified through thermal and electrical treatments.

Method: Used DC and AC electrical stimuli on dense Ag nanowire networks with thin insulating coatings, combined with low-temperature annealing to modify junction properties and humidity content at intersections.

Result: Pristine networks showed frequency-independent resistive behavior, but annealing and AC stimulation transformed them into capacitive systems by depopulating metallic junctions, enabling multiple switching schemes.

Conclusion: Thermal treatment combined with AC stimulation effectively modifies nanowire network properties, creating tunable capacitive responses suitable for brain-like processing through controlled junction transformations.

Abstract: Self-assembled networks of metallic nanowires (NWs) are being intensively
explored as test benches for neuromorphic proposals. In this work, we study the
electric transport properties of dense self-assembled networks of Ag-based NWs
(AgNWNs) coated with a thin insulating layer, using DC and AC stimuli. The
building blocks of this network are the metallic NWs and the NW-NW junctions,
either metallic or memristive. In the pristine state, frequency independence of
the impedance reveals an over-percolated purely resistive network. A
combination of low-temperature annealing and AC stimulus is shown to
drastically affect the resistivity of the sample (interpreted as a depopulation
of purely metallic junctions), unveiling a rich dynamic response. This
procedure triggers the achievement of a capacitive response, which is
successfully rationalized using a previously introduced 'two junction model'.
Thermal treatment appears to be an indirect strategy to effectively modify the
humidity content at the NW-NW intersections and, consequently, enable multiple
switching schemes suitable for brain-like processing alternatives.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [91] [Design, Assessment, and Application of Machine Learning Potential Energy Surfaces](https://arxiv.org/abs/2511.00951)
*Valerii Andreichev,Sena Aydin,Kai Töpfer,Markus Meuwly,Luis Itza Vazquez-Salazar*

Main category: physics.chem-ph

TL;DR: This paper provides a comprehensive overview of Machine Learned Potential Energy Surfaces (ML-PESs), covering concepts, methodologies, and practical recommendations for constructing and using them in chemical and biological systems.


<details>
  <summary>Details</summary>
Motivation: Potential Energy Surfaces (PESs) are essential tools for investigating chemical and biological systems, and recent advances in Machine Learning have enabled the development of ML-PESs that are now widely used for simulations.

Method: The paper discusses concepts, methodologies, and practical recommendations for constructing ML-PESs, with applications demonstrated through two biomolecular systems: Alanine-Lysine-Alanine tripeptide dynamics and double proton transfer reactions in DNA base pairs.

Result: The principles discussed are successfully illustrated through practical applications, showing the utility of ML-PESs for simulating non-reactive dynamics in peptides and proton transfer reactions in DNA base pairs.

Conclusion: The work provides valuable guidance and practical recommendations for researchers developing and using Machine Learned Potential Energy Surfaces for chemical and biological simulations.

Abstract: Potential Energy Surfaces (PESs) are an indispensable tool to investigate,
characterise and understand chemical and biological systems in the gas and
condensed phases. Advances in Machine Learning (ML) methodologies have led to
the development of Machine Learned Potential Energy Surfaces (ML-PES) which are
now widely used to simulate such systems. The present work provides an overview
of concepts, methodologies and recommendations for constructing and using
ML-PESs. The choice of topics is focused on practical and recurrent issues to
conceive and use such model. Application of the principles discussed are
illustrated through two different systems of biomolecular importance: the
non-reactive dynamics of the Alanine-Lysine-Alanine tripeptide in gas and
solution phases, and double proton transfer reactions in DNA base pairs.

</details>


### [92] [Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions](https://arxiv.org/abs/2511.01464)
*Sander Hummerich,Tristan Bereau,Ullrich Köthe*

Main category: physics.chem-ph

TL;DR: Split-flows is a novel flow-based method for backmapping coarse-grained molecular models to atomistic detail, enabling expressive conditional sampling and tractable computation of mapping entropies.


<details>
  <summary>Details</summary>
Motivation: Coarse-grained models accelerate molecular simulations but lose microscopic information. Recovering fine-grained detail is essential for atomistic accuracy, making backmapping a central challenge in molecular modeling.

Method: Split-flows reinterpret backmapping as continuous-time measure transport across resolutions, establishing a direct probabilistic link between coarse-grained and atomistic structures using flow-based approach.

Result: The method enables expressive conditional sampling of atomistic structures and provides the first tractable route to computing mapping entropies. Demonstrated on chignolin, lipid bilayer, and alanine dipeptide systems.

Conclusion: Split-flows provide a principled framework for accurate backmapping and systematic evaluation of coarse-grained models through information-theoretic measures.

Abstract: By reducing resolution, coarse-grained models greatly accelerate molecular
simulations, unlocking access to long-timescale phenomena, though at the
expense of microscopic information. Recovering this fine-grained detail is
essential for tasks that depend on atomistic accuracy, making backmapping a
central challenge in molecular modeling. We introduce split-flows, a novel
flow-based approach that reinterprets backmapping as a continuous-time measure
transport across resolutions. Unlike existing generative strategies,
split-flows establish a direct probabilistic link between resolutions, enabling
expressive conditional sampling of atomistic structures and -- for the first
time -- a tractable route to computing mapping entropies, an
information-theoretic measure of the irreducible detail lost in
coarse-graining. We demonstrate these capabilities on diverse molecular
systems, including chignolin, a lipid bilayer, and alanine dipeptide,
highlighting split-flows as a principled framework for accurate backmapping and
systematic evaluation of coarse-grained models.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [93] [Shortest Geodesic Loops, Sectional Curvature, and Injectivity Radius of the Stiefel Manifold](https://arxiv.org/abs/2511.01563)
*Jakob Stoye,Simon Mataigne,P. -A. Absil,Ralf Zimmermann*

Main category: math.DG

TL;DR: This paper determines the exact injectivity radius of Stiefel manifolds under a one-parameter family of Riemannian metrics by analyzing geodesic loop lengths and sectional curvature bounds.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric properties of Stiefel manifolds under various Riemannian metrics, particularly the injectivity radius which is crucial for understanding the manifold's local geometry and has applications in optimization algorithms on manifolds.

Method: Combined existing and new bounds on sectional curvature to analyze geodesic loops, focusing on the one-parameter family of metrics introduced by Hüper et al. (2021) that includes canonical and Euclidean metrics.

Result: Determined the exact value of the injectivity radius for Stiefel manifolds under a wide range of metrics in the parameter family, providing precise geometric characterization.

Conclusion: Successfully established exact injectivity radii for Stiefel manifolds across various metrics, advancing the understanding of their geometric structure and providing tools for applications in manifold optimization.

Abstract: We determine the length of the shortest nontrivial geodesic loops on the
Stiefel manifold endowed with any member of the one-parameter family of
Riemannian metrics introduced by H\"uper et al. (2021). This family includes,
in particular, the canonical and Euclidean metrics. By combining existing and
new bounds on the sectional curvature, we determine the exact value of the
injectivity radius of the Stiefel manifold under a wide range of members of the
metric family.

</details>


### [94] [Stability of volume and area preserving mean curvature flow in asymptotic Schwarzschild space](https://arxiv.org/abs/2511.00435)
*Yaoting Gui,Yuqiao Li,Jun Sun*

Main category: math.DG

TL;DR: The paper analyzes stability of volume/area preserving mean curvature flows in Schwarzschild space, showing global existence and convergence to constant mean curvature hypersurfaces for initial surfaces close to coordinate spheres.


<details>
  <summary>Details</summary>
Motivation: To understand the stability properties and long-time behavior of volume preserving mean curvature flow (VPMCF) and area preserving mean curvature flow (APMCF) in Schwarzschild and asymptotically Schwarzschild spaces.

Method: Using perturbation analysis for initial surfaces close to coordinate spheres, and combining center manifold analysis with similar methods for asymptotically Schwarzschild spaces with pinched curvature outside large compact sets.

Result: Both VPMCF and APMCF exist globally and converge smoothly to constant mean curvature hypersurfaces (coordinate spheres) when initial hypersurfaces are sufficiently close to coordinate spheres. In asymptotically Schwarzschild space, flows exist for long time and converge exponentially fast to CMC hypersurfaces.

Conclusion: The analysis provides existence results for constant mean curvature hypersurfaces in asymptotically flat spaces through the study of volume/area preserving mean curvature flows.

Abstract: In this paper, we investigate the stability of the volume preserving mean
curvature flow (VPMCF) and area preserving mean curvature flow (APMCF) in the
Schwarzschild space. We show that if the initial hypersurface is sufficiently
close to a coordinate sphere, these flows exist globally in time and converge
smoothly to a constant mean curvature (CMC) hypersurface, namely a coordinate
sphere. For asymptotically Schwarzschild space, if the initial hypersurface has
pinched curvature outside of some large compact set, we will similar method
combined with the center manifold analysis to see that the flow still exists
for long time and converges to CMC hypersurface exponentially fast. This in
particular gives an existence result for a CMC hypersurface in asymptotically
flat space.

</details>


### [95] [The Analysis of Willmore Surfaces and its Generalizations in Higher Dimensions](https://arxiv.org/abs/2511.01777)
*Tian Lan,Dorian Martino,Tristan Rivière*

Main category: math.DG

TL;DR: Review of recent progress in analyzing Lagrangians on immersions into ℝ^d that depend on first and second fundamental forms and their covariant derivatives.


<details>
  <summary>Details</summary>
Motivation: To survey and consolidate recent developments in the mathematical analysis of geometric Lagrangians defined on immersed surfaces, particularly those involving fundamental forms and their derivatives.

Method: Comprehensive review and analysis of existing literature and research papers on geometric Lagrangians, focusing on immersions into Euclidean space and their associated fundamental forms.

Result: Synthesis of current state-of-the-art understanding of how Lagrangians behave when defined on immersed surfaces, with emphasis on dependencies on first and second fundamental forms and their covariant derivatives.

Conclusion: The review provides a consolidated overview of recent advances in this specialized area of geometric analysis, highlighting key developments and potentially identifying directions for future research.

Abstract: We review recent progress concerning the analysis of Lagrangians on
immersions into $\mathbb{R}^d$ depending on the first and second fundamental
forms and their covariant derivatives.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [96] [Diversity in emergent cell locomotion from the coupling cytosolic and cortical Marangoni flows with reaction-diffusion dynamics](https://arxiv.org/abs/2511.00558)
*Blaž Ivšić,Igor Weber,Piotr Nowakowski,Ana-Sunčana Smith*

Main category: physics.bio-ph

TL;DR: A cross-scale model integrating reaction-diffusion signaling with hydrodynamics explains how Rho GTPase dynamics and mechanical feedback generate diverse cell migration patterns through self-organized limit cycles.


<details>
  <summary>Details</summary>
Motivation: To understand how biochemical signaling (Rho GTPases) and mechanical feedback coordinate to produce the fundamental process of cell migration in eukaryotic cells.

Method: Developed a mean-field framework that couples reaction-diffusion dynamics of Rho GTPases with cytosolic and cortical hydrodynamics to model emergent cellular locomotion.

Result: The model reproduces diverse experimentally observed shape and motility phenotypes with small parameter changes, showing these behaviors correspond to self-organized limit cycles.

Conclusion: Coupling to both cytosolic flow and spatially varying surface tension is essential for recovering the full spectrum of motility modes, providing a theoretical foundation for understanding amoeboid migration.

Abstract: Cell migration is a fundamental process underlying the survival and function
of both unicellular and multicellular organisms. Crawling motility in
eukaryotic cells arises from cyclic protrusion and retraction driven by the
cytoskeleton, whose organization is regulated by reaction-diffusion (RD)
dynamics of Rho GTPases between the cytosol and the cortex. These dynamics
generate spatial membrane patterning and establish front-rear polarity through
the coupling of biochemical signalling and mechanical feedback. We develop a
cross-scale mean-field framework that integrates RD signalling with cytosolic
and cortical hydrodynamics to capture emergent cellular locomotion. Our model
reproduces diverse experimentally observed shape and motility phenotypes with
small parameter changes, indicating that these behaviours correspond to
self-organized limit cycles. Phase-space analysis reveals that coupling to both
cytosolic flow and spatially varying surface tension is essential to recover
the full spectrum of motility modes, providing a theoretical foundation for
understanding amoeboid migration.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [97] [Constraint Penalization Method in the Lattice Boltzmann Method (LBM) for Fluid-Structure Interaction](https://arxiv.org/abs/2511.01629)
*Tristan Millet,Erwan Liberge*

Main category: physics.flu-dyn

TL;DR: A constraint penalization method for fluid-structure interactions in Lattice Boltzmann framework that enforces rigid-body motion through velocity field penalization.


<details>
  <summary>Details</summary>
Motivation: To model fluid-structure interactions involving rigid bodies while preserving the locality and simplicity of LBM algorithm, eliminating the need for explicit Lagrange multipliers or interface force computation.

Method: Extends fictitious domain concept by enforcing rigid-body motion through a penalization term directly applied to the fluid velocity field, ensuring implicit coupling between fluid and solid regions.

Result: Numerical experiments demonstrate accurate reproduction of rigid-body motion and hydrodynamic interactions with minimal additional computational cost.

Conclusion: The proposed constraint penalization method provides an efficient and accurate approach for modeling fluid-structure interactions in LBM framework, successfully applied to particle sedimentation cases of varying complexity.

Abstract: A constraint penalization method is introduced within the Lattice Boltzmann
(LBM) framework to model fluid-structure interactions involving rigid bodies.
The proposed approach extends the fictitious domain concept by enforcing the
rigid-body motion through a penalization term directly applied to the fluid
velocity field, eliminating the need for explicit Lagrange multipliers or
interface force computation. This formulation preserves the locality and
simplicity of the LBM algorithm while ensuring an implicit coupling between the
fluid and solid regions. Numerical experiments demonstrate that the method
accurately reproduces rigid-body motion and hydrodynamic interactions with
minimal additional computational cost. The method is applied to particle
sedimentation, starting with a simple example and progressing to increasingly
complex cases.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [98] [T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression](https://arxiv.org/abs/2511.01079)
*Nikolay I. Kalmykov,Razan Dibo,Kaiyu Shen,Xu Zhonghan,Anh-Huy Phan,Yipeng Liu,Ivan Oseledets*

Main category: cs.CV

TL;DR: T-MLA is the first targeted multiscale log-exponential attack framework that crafts adversarial perturbations in the wavelet domain to compromise neural image compression systems while maintaining visual stealth.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks on neural image compression (NIC) are naive adaptations of pixel-space methods, overlooking the unique structured nature of compression pipelines. There's a need to understand the advanced security vulnerabilities in NIC systems.

Method: Proposed T-MLA framework crafts adversarial perturbations in the wavelet domain by directly targeting reconstruction quality. Perturbations are strategically confined to specific wavelet subbands to maximize distortion while ensuring perceptual stealth through an offline attack approach.

Result: Extensive evaluation across multiple state-of-the-art NIC architectures shows large drops in reconstruction quality while perturbations remain visually imperceptible, revealing critical security flaws.

Conclusion: The findings expose a critical security vulnerability at the core of generative and content delivery pipelines that use neural image compression systems.

Abstract: Neural image compression (NIC) has become the state-of-the-art for
rate-distortion performance, yet its security vulnerabilities remain
significantly less understood than those of classifiers. Existing adversarial
attacks on NICs are often naive adaptations of pixel-space methods, overlooking
the unique, structured nature of the compression pipeline. In this work, we
propose a more advanced class of vulnerabilities by introducing T-MLA, the
first targeted multiscale log--exponential attack framework. Our approach
crafts adversarial perturbations in the wavelet domain by directly targeting
the quality of the attacked and reconstructed images. This allows for a
principled, offline attack where perturbations are strategically confined to
specific wavelet subbands, maximizing distortion while ensuring perceptual
stealth. Extensive evaluation across multiple state-of-the-art NIC
architectures on standard image compression benchmarks reveals a large drop in
reconstruction quality while the perturbations remain visually imperceptible.
Our findings reveal a critical security flaw at the core of generative and
content delivery pipelines.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [99] [Polar sets for $m$-subharmonic functions on compact Hermitian manifolds](https://arxiv.org/abs/2511.01159)
*Slawomir Kolodziej,Ngoc Cuong Nguyen*

Main category: math.CV

TL;DR: Sharp decay of capacity for sublevel sets of (ω,m)-subharmonic functions on compact Hermitian manifolds, with characterizations of polar sets.


<details>
  <summary>Details</summary>
Motivation: Generalize capacity decay results from the case m=n and from compact Kähler manifolds to the broader setting of compact Hermitian manifolds for (ω,m)-subharmonic functions.

Method: Mathematical analysis and capacity theory applied to (ω,m)-subharmonic functions on n-dimensional compact Hermitian manifolds.

Result: Proved sharp decay of capacity for sublevel sets, and obtained complete characterizations of polar sets in terms of local/global capacities and extremal functions.

Conclusion: The results provide comprehensive understanding of capacity behavior and polar set characterizations for (ω,m)-subharmonic functions on compact Hermitian manifolds, extending previous work.

Abstract: We prove a sharp decay of capacity of sublevel sets of a
$(\omega,m)$-subharmonic functions on a $n$-dimensional compact Hermitian
manifold $(X,\omega)$ which generalizes the case $m=n$ as well as the case
$1\leq m\leq n$ on a compact K\"ahler manifold. We also obtain the full
characterizations of polar sets of such functions in terms of the corresponding
local and global capacities, and of the extremal functions.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [100] [Trade Execution Flow as the Underlying Source of Market Dynamics](https://arxiv.org/abs/2511.01471)
*Mikhail Gennadievich Belov,Victor Victorovich Dubov,Vadim Konstantinovich Ivanov,Alexander Yurievich Maslov,Olga Vladimirovna Proshina,Vladislav Gennadievich Malyshkin*

Main category: q-fin.CP

TL;DR: Execution flow (I = dV/dt) is identified as the fundamental driver of market dynamics, with a numerical framework developed using Radon-Nikodym derivative to calculate it from sampled moments, automatically determining actionable thresholds and characteristic time scales.


<details>
  <summary>Details</summary>
Motivation: To establish execution flow as the fundamental driving force in market dynamics and develop practical methods to quantify and utilize it for market analysis.

Method: Developed a numerical framework using Radon-Nikodym derivative to calculate execution flow from sampled moments, with automatic threshold determination and characteristic time scale identification. Also introduced a Christoffel function spectrum framework as an alternative to PCA with broader invariance properties.

Result: The methodology was validated on actual market data, supporting the finding that execution flow drives market dynamics. The framework successfully determines actionable thresholds and characteristic time scales directly from data.

Conclusion: Execution flow is experimentally demonstrated to be the fundamental driver of market dynamics, with the proposed frameworks providing practical tools for market analysis that overcome limitations of traditional methods like PCA.

Abstract: In this work, we demonstrate experimentally that the execution flow, $I =
dV/dt$, is the fundamental driving force of market dynamics. We develop a
numerical framework to calculate execution flow from sampled moments using the
Radon-Nikodym derivative. A notable feature of this approach is its ability to
automatically determine thresholds that can serve as actionable triggers. The
technique also determines the characteristic time scale directly from the
corresponding eigenproblem. The methodology has been validated on actual market
data to support these findings. Additionally, we introduce a framework based on
the Christoffel function spectrum, which is invariant under arbitrary
non-degenerate linear transformations of input attributes and offers an
alternative to traditional principal component analysis (PCA), which is limited
to unitary invariance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [101] [Training with Fewer Bits: Unlocking Edge LLMs Training with Stochastic Rounding](https://arxiv.org/abs/2511.00874)
*Taowen Liu,Marta Andronic,Deniz Gündüz,George A. Constantinides*

Main category: cs.LG

TL;DR: Increased batch sizes can compensate for quantization noise in LLM training when using Stochastic Rounding, with different effects on gradient variance from weight vs activation quantization.


<details>
  <summary>Details</summary>
Motivation: Quantized training improves efficiency but introduces quantization noise that degrades accuracy. Stochastic Rounding offers unbiased gradients but its interaction with batch size is under-explored.

Method: Theoretical and empirical study of mini-batch SGD with Stochastic Rounding, analyzing how batch size affects quantization noise compensation and gradient variance from weight/activation quantization.

Result: Experiments validate that larger batch sizes can compensate for reduced precision during back-propagation, and quantizing weights vs activations affects gradient variance differently.

Conclusion: Batch size is a key factor in mitigating quantization noise in LLM training with Stochastic Rounding, with distinct variance effects from weight and activation quantization.

Abstract: LLM training is resource-intensive. Quantized training improves computational
and memory efficiency but introduces quantization noise, which can hinder
convergence and degrade model accuracy. Stochastic Rounding (SR) has emerged as
a theoretically attractive alternative to deterministic rounding, offering
unbiased gradient estimates. However, its interaction with other training
factors -- especially batch size -- remains under explored. In this paper, we
present a theoretical and empirical study of mini-batch stochastic gradient
descent (SGD) with SR, showing that increased batch sizes can compensate for
reduced precision during back-propagation. Furthermore, we show that quantizing
weights and activations impacts gradient variance in distinct ways. Our
experiments validate these theoretical insights.

</details>


### [102] [One model to solve them all: 2BSDE families via neural operators](https://arxiv.org/abs/2511.01125)
*Takashi Furuya,Anastasis Kratsios,Dylan Possamaï,Bogdan Raonić*

Main category: cs.LG

TL;DR: The paper introduces a generative neural operator model using Kolmogorov-Arnold networks to solve families of 2BSDEs with random terminal time, showing polynomial parameter requirements for structured subclasses.


<details>
  <summary>Details</summary>
Motivation: To develop efficient neural operator models for solving infinite families of second-order backward stochastic differential equations (2BSDEs) with random terminal time on bounded domains.

Method: Uses Kolmogorov-Arnold networks within a generative neural operator framework to approximate solution operators for 2BSDE families.

Result: Shows neural operators can approximate 2BSDE solution operators and identifies structured subclasses requiring only polynomial parameters in approximation rate, avoiding exponential worst-case requirements.

Conclusion: The proposed neural operator approach provides efficient approximation for 2BSDE families, with polynomial parameter complexity for structured cases.

Abstract: We introduce a mild generative variant of the classical neural operator
model, which leverages Kolmogorov--Arnold networks to solve infinite families
of second-order backward stochastic differential equations ($2$BSDEs) on
regular bounded Euclidean domains with random terminal time. Our first main
result shows that the solution operator associated with a broad range of
$2$BSDE families is approximable by appropriate neural operator models. We then
identify a structured subclass of (infinite) families of $2$BSDEs whose neural
operator approximation requires only a polynomial number of parameters in the
reciprocal approximation rate, as opposed to the exponential requirement in
general worst-case neural operator guarantees.

</details>


### [103] [Stochastic Regret Guarantees for Online Zeroth- and First-Order Bilevel Optimization](https://arxiv.org/abs/2511.01126)
*Parvin Nazari,Bojian Hou,Davoud Ataee Tarzanagh,Li Shen,George Michailidis*

Main category: cs.LG

TL;DR: This paper introduces a novel search direction for online bilevel optimization that achieves sublinear stochastic bilevel regret without window smoothing, improving efficiency through reduced oracle dependence and simultaneous updates.


<details>
  <summary>Details</summary>
Motivation: Current online bilevel optimization approaches rely on window-smoothed regret minimization, which may not accurately reflect system performance when functions change rapidly. The authors aim to develop methods that work without this smoothing requirement.

Method: The authors introduce a novel search direction and develop both first- and zeroth-order stochastic algorithms that leverage this direction. The framework reduces oracle dependence in hypergradient estimation, updates inner and outer variables simultaneously with linear system solutions, and uses zeroth-order estimation of Hessians, Jacobians, and gradients.

Result: The proposed algorithms achieve sublinear stochastic bilevel regret without requiring window smoothing. Experiments validate the approach on online parametric loss tuning and black-box adversarial attacks.

Conclusion: The introduced search direction and framework provide efficient online bilevel optimization methods that work without window smoothing while maintaining theoretical guarantees and practical effectiveness across various applications.

Abstract: Online bilevel optimization (OBO) is a powerful framework for machine
learning problems where both outer and inner objectives evolve over time,
requiring dynamic updates. Current OBO approaches rely on deterministic
\textit{window-smoothed} regret minimization, which may not accurately reflect
system performance when functions change rapidly. In this work, we introduce a
novel search direction and show that both first- and zeroth-order (ZO)
stochastic OBO algorithms leveraging this direction achieve sublinear
{stochastic bilevel regret without window smoothing}. Beyond these guarantees,
our framework enhances efficiency by: (i) reducing oracle dependence in
hypergradient estimation, (ii) updating inner and outer variables alongside the
linear system solution, and (iii) employing ZO-based estimation of Hessians,
Jacobians, and gradients. Experiments on online parametric loss tuning and
black-box adversarial attacks validate our approach.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [104] [Identities and inequalities for integral transforms involving squares of the Bessel functions](https://arxiv.org/abs/2511.00137)
*Soichiro Suzuki*

Main category: math.CA

TL;DR: This paper extends an integral transform identity involving Bessel functions from integer to non-integer indices and derives related inequalities.


<details>
  <summary>Details</summary>
Motivation: The integral transform T_ν plays a crucial role in studying optimal constants for smoothing estimates of free Schrödinger equations. Previous work by Bez et al. provided an alternative expression for this transform using Fourier transforms, but only for integer indices.

Method: The authors extend the identity from Bez et al. for non-integer indices and derive several inequalities from this generalized identity.

Result: The paper successfully generalizes the integral transform identity to non-integer indices and obtains new inequalities related to this transform.

Conclusion: The extension to non-integer indices provides a more comprehensive understanding of the integral transform and its applications in smoothing estimates for Schrödinger equations.

Abstract: We consider an integral transform given by $T_{\nu} f(s) := \pi \int_0^\infty
rs J_{\nu}(r s)^2 f(r) \, dr$, where $J_{\nu}$ denotes the Bessel function of
the first kind of order $\nu$. As shown by Walther (2002,
doi:10.1006/jfan.2001.3863), this transform plays an essential role in the
study of optimal constants of smoothing estimates for the free Schr\"{o}dinger
equations on $\mathbb{R}^d$. On the other hand, Bez et al. (2015,
doi:10.1016/j.aim.2015.08.025) studied these optimal constants using a
different method, and obtained a certain alternative expression for $T_{\nu} f$
involving the $d$-dimensional Fourier transform of $x \mapsto f(\lvert x
\rvert)$ when $\nu = k + d/2 - 1$ for $k \in \mathbb{N}$. The aims of this
paper are to extend their identity for non-integer indices and to derive
several inequalities from it.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [105] [Projected Subgradient Ascent for Convex Maximization](https://arxiv.org/abs/2511.00741)
*Pedro Felzenszwalb,Heon Lee*

Main category: math.OC

TL;DR: Single orthogonal projection suffices for approximate linear maximization over convex sets. For general convex functions, projected subgradient ascent converges to stationary points with large step sizes, connecting to conditional gradient methods.


<details>
  <summary>Details</summary>
Motivation: To develop efficient methods for maximizing convex functions over convex sets, particularly exploring the sufficiency of single projections for linear cases and convergence properties with large step sizes.

Method: For linear maximization: single orthogonal projection. For general convex functions: projected subgradient ascent with arbitrarily large step sizes, connecting to conditional gradient algorithm and iterated linear optimization.

Result: Single projection provides approximate solutions for linear optimization. Projected subgradient ascent converges to first-order stationary points even with infinite step sizes, recovering conditional gradient methods.

Conclusion: Simple projection-based methods are effective for convex maximization problems, with connections established between different optimization approaches through step size analysis.

Abstract: We consider the problem of maximizing a convex function over a closed convex
set. Classical methods solve such problems using iterative schemes that
repeatedly improve a solution. For linear maximization, we show that a single
orthogonal projection suffices to obtain an approximate solution. For general
convex functions over convex sets, we show that projected subgradient ascent
converges to a first-order stationary point when using arbitrarily large step
sizes. Taking the step size to infinity leads to the conditional gradient
algorithm, and iterated linear optimization as a special case. We illustrate
numerical experiments using a single projection for linear optimization in the
elliptope, reducing the problem to the computation of a nearest correlation
matrix.

</details>


### [106] [SHAP values through General Fourier Representations: Theory and Applications](https://arxiv.org/abs/2511.00185)
*Roberto Morales*

Main category: math.OC

TL;DR: This paper establishes a rigorous spectral framework for analyzing SHAP values using Fourier expansions, showing SHAP attributions can be represented as linear functionals of model Fourier coefficients. It studies deterministic and probabilistic regimes with stability estimates and convergence proofs.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous mathematical foundation for SHAP value analysis, addressing the need for theoretical guarantees and stability properties in explainable AI methods.

Method: Develops a spectral framework using generalized Fourier expansions with orthonormal tensor-product basis under product probability measures. Studies two regimes: deterministic (stability estimates) and probabilistic (neural networks in infinite-width limit).

Result: SHAP values can be represented as linear functionals of Fourier coefficients. In deterministic regime: attribution map is Lipschitz continuous. In probabilistic regime: SHAP values converge to Gaussian process prior values with explicit error bounds. Numerical experiments validate findings.

Conclusion: The spectral framework provides rigorous mathematical foundations for SHAP value analysis, with proven stability and convergence properties that enhance the theoretical understanding and reliability of SHAP-based explanations.

Abstract: This article establishes a rigorous spectral framework for the mathematical
analysis of SHAP values. We show that any predictive model defined on a
discrete or multi-valued input space admits a generalized Fourier expansion
with respect to an orthonormalisation tensor-product basis constructed under a
product probability measure. Within this setting, each SHAP attribution can be
represented as a linear functional of the model's Fourier coefficients.
  Two complementary regimes are studied. In the deterministic regime, we derive
quantitative stability estimates for SHAP values under Fourier truncation,
showing that the attribution map is Lipschitz continuous with respect to the
distance between predictors. In the probabilistic regime, we consider neural
networks in their infinite-width limit and prove convergence of SHAP values
toward those induced by the corresponding Gaussian process prior, with explicit
error bounds in expectation and with high probability based on concentration
inequalities.
  We also provide a numerical experiment on a clinical unbalanced dataset to
validate the theoretical findings.

</details>


### [107] [Mutual Consensus and its Application in Minimum Cost Consensus Models](https://arxiv.org/abs/2511.01614)
*Diego García-Zamora,Bapi Dutta,Luis Martínez*

Main category: math.OC

TL;DR: This paper introduces mutual consensus as a non-compensatory measure for robust consensus evaluation, develops new Minimum Cost Consensus models based on this concept, and applies them to OWA-MCC models with practical solution approaches.


<details>
  <summary>Details</summary>
Motivation: To develop a more robust consensus evaluation method that accounts for maximum opinion disparities rather than relying on compensatory measures, improving consensus modeling in group decision-making.

Method: Proposed mutual consensus concept and several new Minimum Cost Consensus (MCC) models, analyzed their properties, and applied them to OWA-MCC models with linearized formulations and approximate solution methods.

Result: Developed mutual consensus-based MCC models that can handle non-convex feasible regions and provide approximate solutions for OWA-MCC models, demonstrating practical effectiveness.

Conclusion: Mutual consensus advances both theoretical and applied dimensions of consensus modeling by providing robust non-compensatory measures and practical solution approaches for complex group decision-making problems.

Abstract: This paper introduces the concept of {mutual consensus} as a novel
non-compensatory consensus measure that accounts for the maximum disparity
among opinions to ensure robust consensus evaluation. Incorporating this
concept, several new Minimum Cost Consensus (MCC) models are proposed, and
their properties are analyzed. To show their applicability, these mutual
consensus-based MCC models are then considered in the context of the {OWA-MCC}
model, which employs Ordered Weighted Averaging (OWA) operators for preference
aggregation. Concretely, we include a linearized formulation under symmetry
conditions as well as examples of the non-convexity of the feasible region in
the general case. Finally, mutual consensus is utilized to obtain approximate
solutions for the OWA-MCC model, demonstrating its practical effectiveness and
advancing the theoretical and applied dimensions of consensus modeling in group
decision-making.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [108] [Elastic Brownian motion with random jumps from the boundary](https://arxiv.org/abs/2511.01455)
*Fausto Colantoni,Mirko D'Ovidio*

Main category: math.PR

TL;DR: Study of elastic Brownian motion on a C² domain where the process restarts from random positions inside the domain instead of being killed at boundaries.


<details>
  <summary>Details</summary>
Motivation: To understand Brownian motion behavior with elastic boundary conditions where particles restart from random positions rather than being absorbed.

Method: Characterization through stochastic differential equations, generator analysis, path descriptions, invariant probability measures, and spectral representations.

Result: Derived complete characterization of the process including its SDE, generator, path behavior, invariant measure, and spectral properties.

Conclusion: Successfully characterized elastic Brownian motion with random restarting and studied harmonic functions on upper half-space for trace process analysis.

Abstract: In this paper, we study elastic Brownian motion on a \(C^2\) domain. Instead
of being killed at the boundary, the process restarts from a random position
inside the domain. We characterize this process through its stochastic
differential equation (SDE), its generator, and a description of the paths. We
also derive the invariant probability measure and the spectral representation.
At the end, we focus on the harmonic functions on the upper half-space to study
the trace process.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [109] [CaloClouds3: Ultra-Fast Geometry-Independent Highly-Granular Calorimeter Simulation](https://arxiv.org/abs/2511.01460)
*Thorsten Buss,Henry Day-Hall,Frank Gaede,Gregor Kasieczka,Katja Krüger,Anatolii Korol,Thomas Madlener,Peter McKeown,Martina Mozzanica,Lorenzo Valente*

Main category: physics.ins-det

TL;DR: CaloClouds3 is a pointcloud model for fast photon shower simulation in detector barrels, using angular conditioning to handle all incident angles and achieving 100x speedup over Geant4.


<details>
  <summary>Details</summary>
Motivation: To enable fast simulation of photon showers across entire detector barrels with position-agnostic training and angular flexibility for full simulation chains.

Method: Uses pointcloud model with angular conditioning and position-agnostic training data, with aggressive optimization of pre-processing and hyperparameters.

Result: Achieved two orders of magnitude speedup over Geant4 at inference while maintaining physics performance across all incident angles.

Conclusion: CaloClouds3 provides a fast, flexible photon shower simulation model usable in full detector simulation and reconstruction workflows.

Abstract: We present CaloClouds3, a model for the fast simulation of photon showers in
the barrel of a high granularity detector. This iteration demonstrates for the
first time how a pointcloud model can employ angular conditioning to replicate
photons at all incident angles. Showers produced by this model can be used
across the whole detector barrel, due to specially produced position agnostic
training data. With this flexibility, the model is usable in a full simulation
and reconstruction chain, which offers a further handle for evaluating physics
performance of the model. As inference time is a crucial consideration for a
generative model, the pre-processing and hyperparameters are aggressively
optimised, achieving a speed up factor of two orders of magnitude over Geant4
at inference.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [110] [Dragging of electric current by hydrodynamic flow at charge neutrality](https://arxiv.org/abs/2511.00221)
*Dmitry Zverevich,Alex Levchenko,A. V. Andreev*

Main category: cond-mat.mes-hall

TL;DR: Theory of drag in graphene double layers near charge neutrality using electron hydrodynamics, accounting for interlayer correlations of charge puddle disorder.


<details>
  <summary>Details</summary>
Motivation: To understand drag resistivity in graphene double layers and how interlayer correlations of charge puddle disorder affect it near charge neutrality.

Method: Develop theory using electron hydrodynamics regime, expressing drag resistivity in terms of viscosity, intrinsic conductivity, and puddle disorder correlation function.

Result: Drag resistivity shows nonmonotonic dependence on carrier density, changes sign for layer-symmetric doping, and transconductivity saturates at short interlayer separations.

Conclusion: Provides quantitative estimates for Dirac electron liquids in monolayer and bilayer graphene double-layer devices, revealing opposite contributions from momentum and energy transfer to drag.

Abstract: We develop a theory of drag in graphene double layers near charge neutrality.
We work in the regime of electron hydrodynamics and account for interlayer
correlations of charge puddle disorder. The drag resistivity is expressed in
terms of the viscosity, intrinsic conductivity of the electron liquid, and the
correlation function of the puddle disorder. The contributions of the
interlayer transfer of momentum and energy to drag have opposite signs. This
leads to a nonmonotonic dependence of the drag resistivity on the carrier
density. For layer-symmetric doping, the drag resistivity changes sign as a
function of the carrier density. At interlayer separations shorter than the
disorder correlation length, the transconductivity saturates to the
disorder-induced enhancement of the intralayer conductivity. We provide
quantitative estimates of the effect for Dirac electron liquids in monolayer
graphene and bilayer graphene double-layer devices.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [111] [Jacobi's solution for geodesics on a triaxial ellipsoid](https://arxiv.org/abs/2511.01621)
*Charles F. F. Karney*

Main category: physics.geo-ph

TL;DR: Numerical implementation of Jacobi's 1838 solution for geodesics on triaxial ellipsoids using integral evaluation and coupled equations.


<details>
  <summary>Details</summary>
Motivation: To provide a practical computational method for solving geodesic problems on triaxial ellipsoids, building on Jacobi's theoretical solution.

Method: Accurately evaluating one-dimensional integrals and solving coupled system of equations derived from Jacobi's solution; using similar approach as biaxial ellipsoids for inverse problems.

Result: Successfully implemented numerical solution for geodesic paths and distances on triaxial ellipsoids.

Conclusion: Jacobi's theoretical solution can be effectively implemented numerically, enabling practical geodesic calculations on triaxial ellipsoids.

Abstract: On Boxing Day, 1838, Jacobi found a solution to the problem of geodesics on a
triaxial ellipsoid, with the course of the geodesic and the distance along it
given in terms of one-dimensional integrals. Here, a numerical implementation
of this solution is described. This entails accurately evaluating the integrals
and solving the resulting coupled system of equations. The inverse problem,
finding the shortest path between two points on the ellipsoid, can then be
solved using a similar method as for biaxial ellipsoids.

</details>
