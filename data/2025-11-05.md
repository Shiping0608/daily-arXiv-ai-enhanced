<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 16]
- [math.AP](#math.AP) [Total: 18]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 5]
- [stat.ML](#stat.ML) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [cs.CV](#cs.CV) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.LG](#cs.LG) [Total: 5]
- [math.DG](#math.DG) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Explicit invariant-preserving integration of differential equations using homogeneous projection](https://arxiv.org/abs/2511.02131)
*Benjamin Kwanen Tapley*

Main category: math.NA

TL;DR: A framework for solving differential equations that preserves invariants by projecting base integrators onto invariant-preserving manifolds using homogeneous symmetries, enabling exact closed-form projections.


<details>
  <summary>Details</summary>
Motivation: To develop high-order numerical methods for differential equations that preserve invariants with minimal computational overhead, improving accuracy and efficiency over standard approaches.

Method: Project arbitrary base integrators onto invariant-preserving manifolds using homogeneous symmetries to evaluate projections exactly and in closed form, yielding explicit invariant-preserving integrators.

Result: The methods are high-order, introduce negligible computational overhead, and can be incorporated into adaptive solvers like Dormand-Prince 8(5,3) for error-controlled, invariant-preserving time-stepping.

Conclusion: Numerical experiments on various ODEs and PDEs demonstrate substantial improvements in both accuracy and efficiency compared to standard approaches.

Abstract: We develop a general framework for numerically solving differential equations
while preserving invariants. As in standard projection methods, we project an
arbitrary base integrator onto an invariant-preserving manifold, however, our
method exploits homogeneous symmetries to evaluate the projection exactly and
in closed form. This yields explicit invariant-preserving integrators for a
broad class of nonlinear systems, as well as pseudo-invariant-preserving
schemes capable of preserving multiple invariants to arbitrarily high
precision. The resulting methods are high-order and introduce negligible
computational overhead relative to the base solver. When incorporated into
adaptive solvers such as Dormand-Prince 8(5,3), they provide error-controlled,
invariant-preserving, high-order time-stepping schemes. Numerical experiments
on double-pendulum and Kepler ODEs as well as semidiscretised KdV and
Camassa-Holm PDEs demonstrate substantial improvements in both accuracy and
efficiency over standard approaches.

</details>


### [2] [A Joint Variational Framework for Multimodal X-ray Ptychography and Fluorescence Reconstruction](https://arxiv.org/abs/2511.02153)
*Eric Zou,Elle Buser,Zichao Wendy Di,Yuanzhe Xi*

Main category: math.NA

TL;DR: Joint variational framework combining ptychography and X-ray fluorescence for improved multimodal X-ray imaging reconstruction.


<details>
  <summary>Details</summary>
Motivation: To address the ill-posed inverse problems in coherent X-ray measurements by integrating structural (ptychography) and compositional (fluorescence) information for better conditioning and stability.

Method: Formulated a joint variational framework as a single nonlinear least-squares problem with shared spatial variables, enforcing cross-modal consistency between structural and compositional estimates.

Result: Numerical experiments showed faster convergence, sharper reconstructions, lower relative error, and more quantitative results compared to separate inversions.

Conclusion: Multimodal variational formulations enhance stability, resolution, and interpretability in computational X-ray imaging by coupling complementary contrast mechanisms.

Abstract: Recovering high-resolution structural and compositional information from
coherent X-ray measurements involves solving coupled, nonlinear, and ill-posed
inverse problems. Ptychography reconstructs a complex transmission function
from overlapping diffraction patterns, while X-ray fluorescence provides
quantitative, element-specific contrast at lower spatial resolution. We
formulate a joint variational framework that integrates these two modalities
into a single nonlinear least-squares problem with shared spatial variables.
This formulation enforces cross-modal consistency between structural and
compositional estimates, improving conditioning and promoting stable
convergence. The resulting optimization couples complementary contrast
mechanisms (i.e., phase and absorption from ptychography, elemental composition
from fluorescence) within a unified inverse model. Numerical experiments on
simulated data demonstrate that the joint reconstruction achieves faster
convergence, sharper and more quantitative reconstructions, and lower relative
error compared with separate inversions. The proposed approach illustrates how
multimodal variational formulations can enhance stability, resolution, and
interpretability in computational X-ray imaging.

</details>


### [3] [On Eigenvector Computation and Eigenvalue Reordering for the Non-Hermitian Quaternion Eigenvalue Problem](https://arxiv.org/abs/2511.02232)
*Zhigang Jia,Meiyue Shao,Yanjun Shao*

Main category: math.NA

TL;DR: Enhanced quaternion QR algorithm with eigenvector computation, eigenvalue reordering, and aggressive early deflation (AED) techniques.


<details>
  <summary>Details</summary>
Motivation: To improve the convergence and efficiency of the quaternion QR algorithm for solving quaternion eigenvalue problems.

Method: Developed algorithms for eigenvector computation and eigenvalue reordering, and successfully applied aggressive early deflation (AED) technique to quaternion eigenvalue problems.

Result: Numerical experiments demonstrate the efficiency and effectiveness of the proposed algorithms.

Conclusion: The additions significantly enhance the quaternion QR algorithm's performance, particularly through the successful application of AED technique.

Abstract: In this paper we present several additions to the quaternion QR algorithm,
including algorithms for eigenvector computation and eigenvalue reordering. A
key outcome of the eigenvalue reordering algorithm is that the aggressive early
deflation (AED) technique, which significantly enhances the convergence of the
QR algorithm, is successfully applied to the quaternion eigenvalue problem. We
conduct numerical experiments to demonstrate the efficiency and effectiveness
of the proposed algorithms.

</details>


### [4] [Convergence analysis of positivity-preserving finite difference scheme for the Flory-Huggins-Cahn-Hilliard equation with dynamical boundary condition](https://arxiv.org/abs/2511.02298)
*Yunzhuo Guo,Cheng Wang,Zhengru Zhang*

Main category: math.NA

TL;DR: A convex-splitting numerical method for the Cahn-Hilliard equation with dynamical boundary conditions that preserves positivity, energy dissipation, and achieves first-order temporal and second-order spatial accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical approach for the Cahn-Hilliard equation with dynamical boundary conditions that maintains physical properties like positivity preservation and energy dissipation while ensuring convergence.

Method: Convex-splitting numerical approach combined with finite difference spatial approximation, using Fourier projection for boundary mass conservation and trigonometric auxiliary function for bulk mass conservation.

Result: Theoretical establishment of ℓ∞(0,T;H_h^{-1}) ∩ ℓ²(0,T;H_h¹) convergence analysis with first-order temporal and second-order spatial accuracy, while maintaining mass conservation properties.

Conclusion: The proposed method successfully achieves convergence with the desired accuracy while preserving key physical properties like positivity, energy dissipation, and mass conservation in both bulk and boundary domains.

Abstract: The Cahn-Hilliard equation has a wide range of applications in many areas of
physics and chemistry. To describe the short-range interaction between the
solution and the boundary, scientists have constructed dynamical boundary
conditions by introducing boundary energy. In this work, the dynamical boundary
condition is located on two opposite edges of a square domain and is connected
with bulk by a normal derivative. A convex-splitting numerical approach is
proposed to enforce the positivity-preservation and energy dissipation,
combined with the finite difference spatial approximation. The
$\ell^\infty(0,T;H_h^{-1}) \cap \ell^2(0,T;H_h^1)$ convergence analysis and
error estimate is theoretically established, with the first order accuracy in
time and second order accuracy in space. The bulk and surface discrete mass
conservation of the exact solution is required to reach the mean-zero property
of the error function, so that the associated discrete $H_h^{-1}$ norm is
well-defined. The mass conservation on the physical boundary is maintained by
the classic Fourier projection. In terms of the mass conservation in bulk, we
introduce a trigonometric auxiliary function based on the truncation error
expansion, so that the bulk mass conservation is achieved, and it has no effect
on the boundary. The smoothness of trigonometric function makes the Taylor
expansion valid and maintains the convergence order of truncation error as
well. As a result, the convergence analysis could be derived with a careful
nonlinear error estimate.

</details>


### [5] [About subspaces the most deviating from the coordinate ones](https://arxiv.org/abs/2511.02387)
*Yuri Nesterenko*

Main category: math.NA

TL;DR: This paper identifies extremal subspaces in R^n that maximize the distance to coordinate subspaces, achieving the theoretical upper bound of arccos(1/√n) proposed in previous work.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by a long-standing hypothesis from GTZ1997 that the distance between any nontrivial linear subspace and the closest coordinate subspace in R^n cannot exceed arccos(1/√n). This work aims to find and characterize subspaces that achieve this theoretical maximum.

Method: The authors describe subspaces that deviate from all coordinate subspaces by exactly arccos(1/√n). These subspaces are realized as the star spaces of all nontrivial 2-connected series-parallel graphs with specific edge weights and arbitrary edge directions.

Result: The paper successfully identifies and characterizes the class of extremal subspaces that achieve the maximum possible distance of arccos(1/√n) from all coordinate subspaces, thus proving the existence of subspaces that reach the theoretical upper bound.

Conclusion: The identified subspaces serve as extremal examples that validate the original hypothesis, and their construction using graph theory suggests potential applications beyond numerical linear algebra, particularly in fields involving series-parallel graphs and their associated spaces.

Abstract: Taking the largest principal angle as the distance function between same
dimensional nontrivial linear subspaces in $\mathds{R}^n$, we describe the
class of subspaces deviating from all the coordinate ones by at least
$\arccos(1 / \sqrt{n})$. This study compliments and is motivated by the
long-standing hypothesis put forward in \cite{GTZ1997} and essentially stating
that so-defined distance to the closest coordinate subspace cannot exceed
$\arccos(1 / \sqrt{n})$. In this context, the subspaces presented here claim to
be the extremal ones.
  Realized as the star spaces of all nontrivial 2-connected series-parallel
graphs with certain edge weights and arbitrary edge directions, the given
subspaces may be of interest beyond numerical linear algebra within which the
original problem was formulated.

</details>


### [6] [Sparse Source Identification in Transient Advection-Diffusion Problems with a Primal-Dual-Active-Point Strategy](https://arxiv.org/abs/2511.02552)
*Marco Mattuschka,Daniel Walter,Max von Danwitz,Alexander Popp*

Main category: math.NA

TL;DR: Mathematical model for rapid airborne contaminant prediction using sparse sensor data, with applications in critical infrastructure protection and evacuation planning.


<details>
  <summary>Details</summary>
Motivation: Enable timely decision-making in critical infrastructure protection scenarios with limited observation data, particularly for evacuation planning after contaminant release.

Method: Formulates inverse problem using advection-diffusion equation, employs variational regularization, models contaminant sources as spatial distributions, and uses Primal-Dual-Active-Point algorithm for efficient sparse minimization.

Result: Outperforms state-of-the-art techniques with L²-regularization in 2D/3D test cases with instantaneous and continuous sources, effective in complex real-world building geometries from OpenStreetMap.

Conclusion: The proposed method provides efficient and reliable contaminant source identification and transport prediction for critical infrastructure protection applications.

Abstract: This work presents a mathematical model to enable rapid prediction of
airborne contaminant transport based on scarce sensor measurements. The method
is designed for applications in critical infrastructure protection (CIP), such
as evacuation planning following contaminant release. In such scenarios, timely
and reliable decision-making is essential, despite limited observation data. To
identify contaminant sources, we formulate an inverse problem governed by an
advection-diffusion equation. Given the problem's underdetermined nature, we
further employ a variational regularization ansatz and model the unknown
contaminant sources as distribution over the spatial domain. To efficiently
solve the arising inverse problem, we employ a problem-specific variant of the
Primal-Dual-Active-Point (PDAP) algorithm which efficiently approximates sparse
minimizers of the inverse problem by alternating between greedy location
updates and source intensity optimization. The approach is demonstrated on two-
and three-dimensional test cases involving both instantaneous and continuous
contaminant sources and outperforms state-of-the-art techniques with
$L^2$-regularization. Its effectiveness is further illustrated in complex
domains with real-world building geometries imported from OpenStreetMap.

</details>


### [7] [A Block-Shifted Cyclic Reduction Algorithm for Solving a Class of Quadratic Matrix Equations](https://arxiv.org/abs/2511.02598)
*Xu Li,Beatrice Meini*

Main category: math.NA

TL;DR: The Block-Shifted CR algorithm improves cyclic reduction for solving quadratic matrix equations in QBD processes by using SVD and block shift-and-deflate techniques, extending applicability to cases with multiple eigenvalues on the unit circle.


<details>
  <summary>Details</summary>
Motivation: The standard cyclic reduction algorithm fails to converge when the matrix polynomial has more than one eigenvalue on the unit circle, limiting its applicability for certain quadratic matrix equations in quasi-birth-death processes.

Method: Proposed Block-Shifted CR algorithm that combines singular value decomposition with block shift-and-deflate techniques to enhance the standard cyclic reduction method.

Result: Numerical experiments show the method is effective and robust, successfully solving quadratic matrix equations that standard cyclic reduction cannot handle.

Conclusion: The Block-Shifted CR algorithm extends the applicability of existing solvers to a broader class of quadratic matrix equations by overcoming convergence limitations of traditional cyclic reduction.

Abstract: The cyclic reduction (CR) algorithm is an efficient method for solving
quadratic matrix equations that arise in quasi-birth-death (QBD) stochastic
processes. However, its convergence is not guaranteed when the associated
matrix polynomial has more than one eigenvalue on the unit circle. To address
this limitation, we introduce a novel iteration method, referred to as the
Block-Shifted CR algorithm, that improves the CR algorithm by utilizing
singular value decomposition (SVD) and block shift-and-deflate techniques. This
new approach extends the applicability of existing solvers to a broader class
of quadratic matrix equations. Numerical experiments demonstrate the
effectiveness and robustness of the proposed method.

</details>


### [8] [The stability of shallow neural networks on spheres: A sharp spectral analysis](https://arxiv.org/abs/2511.02625)
*Xinliang Liu,Tong Mao,Jinchao Xu*

Main category: math.NA

TL;DR: Sharp asymptotic estimates for condition numbers of mass and stiffness matrices from shallow ReLU^k neural networks on the unit sphere, showing optimal numerical stability when nodes are antipodally quasi-uniform.


<details>
  <summary>Details</summary>
Motivation: To establish the relationship between approximation power and numerical stability in neural networks by analyzing the spectral properties of key matrices.

Method: Analyzed the full spectrum of eigenvalues and eigenspace structure of mass and stiffness matrices from shallow ReLU^k networks on the unit sphere with antipodally quasi-uniform nodes.

Result: Found sharp condition numbers and characterized eigenspaces: smallest eigenvalues correspond to low-degree polynomials, largest eigenvalues to high-degree polynomials.

Conclusion: Established precise correspondence between network approximation power and numerical stability through spectral analysis.

Abstract: We present an estimation of the condition numbers of the \emph{mass} and
\emph{stiffness} matrices arising from shallow ReLU$^k$ neural networks defined
on the unit sphere~$\mathbb{S}^d$. In particular, when $\{\theta_j^*\}_{j=1}^n
\subset \mathbb{S}^d$ is \emph{antipodally quasi-uniform}, the condition number
is sharp. Indeed, in this case, we obtain sharp asymptotic estimates for the
full spectrum of eigenvalues and characterize the structure of the
corresponding eigenspaces, showing that the smallest eigenvalues are associated
with an eigenbasis of low-degree polynomials while the largest eigenvalues are
linked to high-degree polynomials. This spectral analysis establishes a precise
correspondence between the approximation power of the network and its numerical
stability.

</details>


### [9] [Error Estimates of Generic Discretisation of Reaction-Diffusion System with Constraints](https://arxiv.org/abs/2511.02654)
*Yahya Alnashri*

Main category: math.NA

TL;DR: This paper establishes general convergence rates for numerical schemes approximating a parabolic reaction-diffusion system modeling biofilm growth, covering both conforming and non-conforming discretization methods.


<details>
  <summary>Details</summary>
Motivation: To provide a unified framework for analyzing numerical schemes for biofilm growth models and derive the first general convergence rates for such approximations.

Method: Developed a unified framework encompassing multiple numerical schemes, proved existence and uniqueness of discrete solutions under standard time discretization assumptions, and conducted numerical experiments using a mixed finite volume scheme.

Result: Established general convergence rates for both conforming and non-conforming discretization methods, with numerical experiments confirming theoretical convergence rates using a test case with analytical solution.

Conclusion: The proposed unified framework successfully provides general convergence rates for numerical approximations of biofilm growth models, validated through both theoretical analysis and numerical experiments.

Abstract: In this paper, we study a parabolic reaction diffusion system with
constraints that model biofilm growth. Within a unified framework encompassing
multiple numerical schemes, we derive the first general convergence rates for
approximating this model using both conforming and non conforming
discretisation methods. Under standard assumptions on the time discretisation,
we establish the existence and uniqueness of the discrete solution. Numerical
experiments are conducted using a mixed finite volume scheme that fits within
the proposed unified framework. A test case with an analytical solution is
designed to confirm our theoretical convergence rates.

</details>


### [10] [Joint transfer pricing decision on tangible and intangible assets for multinational firms](https://arxiv.org/abs/2511.02658)
*Yaling Kang,Zujun Ma,Xin Tian,Zhiqiao Wu*

Main category: math.NA

TL;DR: Study compares tax avoidance strategies of multinational firms: conventional firms use markup on tangible assets, while high-tech firms use royalty fees on intangible assets. Analyzes effects under commissionaire (C) and limited-risk (R) structures.


<details>
  <summary>Details</summary>
Motivation: To understand how different tax avoidance strategies (markup vs royalty) affect multinational firms' decision-making and profits under different operational structures, especially given the different approaches used by conventional vs high-tech firms.

Method: Mathematical modeling comparing effects of tax differences, markups, and royalties on decision-making under two operational structures: commissionaire structure (C) with complete information and limited-risk structure (R) in principal-agent setting.

Result: Tax difference always improves profits under C structure but shows non-monotonic behavior under R structure. When order quantity is small, markup improves profits faster than royalty; when quantity is large, royalty improves profits faster than markup.

Conclusion: The effectiveness of tax avoidance strategies depends on operational structure and order quantity, with markup being more effective for small quantities and royalty for large quantities, while tax differences have different effects under complete vs principal-agent settings.

Abstract: While conventional multinational firms (MNFs) often avoid taxes by
transferring their profits to low-tax regions through markup on tangible asset
costs, high-tech MNFs may avoid taxes by transferring royalty fees to
intangible assets (i.e., royalty-based transfer prices). This study
investigates the effects of tax differences, markups, and royalties on
decision-making. We also compare the different effects of markups and royalties
on the improvement of MNFs' after-tax profit under two main business
structures: the commissionaire operational structure (C) with complete
information, and the limited-risk operational structure (R) in the
principal-agent setting. We find that the tax difference always improves MNFs'
profits under the C structure, whereas non-monotonic behavior exists under the
R structure. More interestingly, when the order quantity is relatively small,
the markup improves MNFs' profits faster than the royalty; conversely, the
royalty improves MNFs' profits faster than the markup.

</details>


### [11] [Discretization and convergence of the ballistic Benamou-Brenier formulation of the porous medium and Burgers equations](https://arxiv.org/abs/2511.02662)
*Mirebeau Jean-Marie,Stampfli Erwan*

Main category: math.NA

TL;DR: The paper presents discretization, convergence analysis, and numerical implementation of reformulations of quadratic porous medium and Burgers' equations as forward-time variants of the Benamou-Brenier optimal transport formulation.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical schemes for evolution problems by reformulating them as global optimization problems in time and space using the Benamou-Brenier optimal transport framework.

Method: Introduces discretization with harmonic interpolation of densities, proves unconditional stability, establishes quadratic convergence rate for dual PDE solution, and implements using proximal splitting method with global space-time fast Fourier transform.

Result: The resulting schemes are unconditionally stable with respect to space and time steps, achieve quadratic convergence rate under suitable assumptions, and can be efficiently solved numerically.

Conclusion: The proposed approach successfully transforms evolution problems into global optimization problems with stable, convergent schemes that are computationally efficient using modern optimization techniques.

Abstract: We study the discretization, convergence, and numerical implementation of
recent reformulations of the quadratic porous medium equation (multidimensional
and anisotropic) and Burgers' equation (one-dimensional, with optional
viscosity), as forward in time variants of the Benamou-Brenier formulation of
optimal transport. This approach turns those evolution problems into global
optimization problems in time and space, of which we introduce a
discretization, one of whose originalities lies in the harmonic interpolation
of the densities involved. We prove that the resulting schemes are
unconditionally stable w.r.t. the space and time steps, and we establish a
quadratic convergence rate for the dual PDE solution, under suitable
assumptions. We also show that the schemes can be efficiently solved
numerically using a proximal splitting method and a global space-time fast
Fourier transform, and we illustrate our results with numerical experiments.

</details>


### [12] [Numerical valuation of European options under two-asset infinite-activity exponential Lévy models](https://arxiv.org/abs/2511.02700)
*Massimiliano Moda,Karel J. in 't Hout,Michèle Vanmaele,Fred Espen Benth*

Main category: math.NA

TL;DR: A numerical method for valuing European options under two-asset exponential Lévy models, extending 1D methods to 2D with efficient FFT-based discretization and second-order convergence.


<details>
  <summary>Details</summary>
Motivation: To develop efficient valuation methods for European options under two-asset infinite-activity exponential Lévy models, extending existing 1D approaches to handle the more complex 2D case.

Method: Extends Wang-Wan-Forsyth 1D approach to 2D setting with tailored discretization of non-local integral term using FFT, and semi-Lagrangian theta-method with splitting (implicit diffusion, explicit integral via fixed-point iteration).

Result: Method achieves favorable second-order convergence for put-on-the-average options under Normal Tempered Stable dynamics, particularly when Lévy process has finite-variation.

Conclusion: The proposed method successfully extends 1D valuation techniques to 2D exponential Lévy models with efficient implementation and good convergence properties.

Abstract: We propose a numerical method for the valuation of European-style options
under two-asset infinite-activity exponential L\'evy models. Our method extends
the effective approach developed by Wang, Wan & Forsyth (2007) for the
1-dimensional case to the 2-dimensional setting and is applicable for general
L\'evy measures under mild assumptions. A tailored discretization of the
non-local integral term is developed, which can be efficiently evaluated by
means of the fast Fourier transform. For the temporal discretization, the
semi-Lagrangian theta-method is employed in a convenient splitting fashion,
where the diffusion term is treated implicitly and the integral term is handled
explicitly by a fixed-point iteration. Numerical experiments for
put-on-the-average options under Normal Tempered Stable dynamics reveal
favourable second-order convergence of our method whenever the exponential
L\'evy process has finite-variation.

</details>


### [13] [Many (most?) column subset selection criteria are NP hard](https://arxiv.org/abs/2511.02740)
*Ilse C. F. Ipsen,Arvind K. Saibaba*

Main category: math.NA

TL;DR: Analysis of NP-hard subset selection criteria for choosing k representative columns from matrices, including volume/S-optimality maximization, norm/condition minimization, and stable rank maximization.


<details>
  <summary>Details</summary>
Motivation: To understand the computational complexity and develop theoretical foundations for various criteria used in selecting representative columns from matrices, which has applications in data analysis, dimensionality reduction, and numerical linear algebra.

Method: Formulate optimization problems as decision problems, derive optimal values for subset selection criteria, and develop expressions for partitioned pseudo-inverses to analyze computational complexity.

Result: Proved that all considered subset selection criteria are NP-hard and do not admit polynomial time approximation schemes (PTAS), establishing strong computational barriers.

Conclusion: The various criteria for selecting representative columns from matrices are computationally intractable, requiring alternative approaches like approximation algorithms or heuristics for practical applications.

Abstract: We consider a variety of criteria for selecting k representative columns from
a real matrix A with rank(A)>=k. The criteria include the following
optimization problems: absolute volume and S-optimality maximization; norm and
condition minimization in the two-norm, Frobenius norm and Schatten p-norms for
p>2; stable rank maximization; and the new criterion of relative volume
maximization. We show that these criteria are NP hard and do not admit
polynomial time approximation schemes (PTAS). To formulate the optimization
problems as decision problems, we derive optimal values for the subset
selection criteria, as well as expressions for partitioned pseudo-inverses.

</details>


### [14] [Approximation by Certain Complex Nevai Operators : Theory and Applications](https://arxiv.org/abs/2511.02750)
*Priyanka Majethiya,Shivam Bajpeyi*

Main category: math.NA

TL;DR: The paper proposes a family of complex Nevai interpolation operators for approximating complex-valued functions, including generalized, Kantorovich, and Hermite types, with applications in image processing.


<details>
  <summary>Details</summary>
Motivation: To extend classical approximation theory to complex domains for handling amplitude and phase-dependent phenomena, building on Nevai's mathematical framework.

Method: Developed three types of complex Nevai operators: generalized (using Chebyshev polynomials), Kantorovich (for integrable functions), and Hermite (preserving higher derivatives), with theoretical analysis using modulus of continuity.

Result: Established approximation results, boundedness, and convergence properties for all three operator types, validated through numerical illustrations.

Conclusion: The proposed family of complex Nevai operators provides effective tools for approximating various classes of complex-valued functions, with practical applications in areas like image processing.

Abstract: The approximation of complex-valued functions is of fundamental importance as
it generalizes classical approximation theory to the complex domain, providing
a rigorous framework for amplitude and phase-dependent phenomena. In this
paper, we study the Nevai operator, a concept formulated by the distinguished
mathematician Paul G. Nevai. We propose a family of complex Nevai interpolation
operators to approximate analytic as well as non-analytic complex-valued
functions along with real-life application in image processing. In this
direction, the first operator is constructed using Chebyshev polynomials of the
first kind, namely complex generalized Nevai operators for approximating
complex-valued continuous functions. We establish the approximation results for
the proposed operators utilizing the notion of a modulus of continuity. To
approximate not necessary continuous but integrable function, we define complex
Kantorovich type Nevai operators and establish their boundedness and
convergence. Furthermore, in order to approximate functions preserving higher
derivatives, we introduce complex Hermite type Nevai operators and study their
approximation capabilities using higher order of modulus of continuity. To
validate the theoretical results, we provide numerical illustrations of
approximation abilities of proposed family of complex Nevai operators.

</details>


### [15] [Finite element analysis for a Herrmann pressure formulation of the elastoacoustic problem with variable coefficients](https://arxiv.org/abs/2511.02782)
*Arbaz Khan,Felipe Lepe,David Mora,Ricardo Ruíz-Baier,Jesus Vellojin*

Main category: math.NA

TL;DR: Numerical analysis of fluid-structure eigenproblem for sloshing and elasto-acoustic vibration using displacement-Herrmann pressure formulation for solid and pure displacement for fluid, with non-conforming locking-free finite element method.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate numerical method for analyzing natural frequencies in coupled fluid-structure systems involving sloshing and elasto-acoustic vibration problems.

Method: Non-conforming locking-free finite element method using displacement-Herrmann pressure formulation for solid and pure displacement formulation for fluid, with convergence proofs using non-compact operator theory and a posteriori error estimator.

Result: Proven convergence and error estimates, efficient and reliable a posteriori error estimator, validated through 2D and 3D numerical tests.

Conclusion: The proposed method successfully approximates natural frequencies of coupled fluid-structure systems with proven mathematical properties and practical validation.

Abstract: In two and three dimensions, this study is focused on the numerical analysis
of an eigenproblem associated with a fluid-structure model for sloshing and
elasto-acoustic vibration. We use a displacement-Herrmann pressure formulation
for the solid, while for the fluid, a pure displacement formulation is
considered. Under this approach we propose a non conforming locking-free method
based on classic finite elements to approximate the natural frequencies (of the
eigenmodes) of the coupled system. Employing the theory for non-compact
operators we prove convergence and error estimates. Also we propose an a
posteriori error estimator for this coupled problem which is shown to be
efficient and reliable. All the presented theory is contrasted with a set of
numerical tests in 2D and 3D.

</details>


### [16] [A computationally efficient fractional predictor corrector approach involving the Mittag Leffler kernel](https://arxiv.org/abs/2511.02822)
*Sami Aljhani*

Main category: math.NA

TL;DR: A predictor-corrector numerical scheme using Newton interpolation to solve fractional differential equations with Atangana-Baleanu derivatives, featuring improved accuracy through auxiliary midpoints and piecewise quadratic interpolation.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate numerical method for solving fractional differential equations involving Mittag-Leffler function and Atangana-Baleanu fractional derivatives, which are challenging to handle numerically.

Method: Predictor-corrector scheme using Newton interpolation with auxiliary midpoints in each sub-interval. Piecewise quadratic Newton interpolation for corrector scheme, and piecewise linear Newton interpolation for midpoint and predictor derivation.

Result: Numerical experiments demonstrate the scheme is effective for handling fractional differential equations with nonlinear terms involving Atangana-Baleanu operators, showing significantly improved accuracy compared to other methods.

Conclusion: The proposed predictor-corrector method with Newton interpolation and auxiliary midpoints provides a powerful and accurate technique for solving fractional differential equations with Atangana-Baleanu derivatives.

Abstract: In this paper, based on Newton interpolation we have proposed a numerical
scheme of predictor-corrector type in order to solve fractional differential
equations with the fractional derivative involving the Mittag-Leffler function.
We have added an auxiliary midpoint in each sub-interval, this allows us to use
a piecewise quadratic Newton interpolation to derive the corrector scheme. The
derivation of the schemes for the midpoint and the predictor is done by means
of a piecewise linear Newton interpolation. We present some illustrative
examples for initial value problems that involve fractional derivatives in the
sense of Atangana-Baleanu. The results of numerical experiments show that the
proposed scheme is a powerful technique to handle fractional differential
equations with nonlinear terms that involve operators of Atangana-Baleanu type.
Moreover, the proposed method significantly improves the numerical accuracy in
comparison with other methods.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [17] [Magnetically Insulated Diode: Existence of Solutions and Complex Bifurcation. I](https://arxiv.org/abs/2511.01901)
*Denis Sidorov,Alexander Sinitsyn,Omar Toledo Leguizamón,Liguo Wang*

Main category: math.AP

TL;DR: Analysis of electron motion in vacuum diodes using Vlasov-Maxwell system to find optimal trajectories and study complex bifurcations in magnetic insulation regimes.


<details>
  <summary>Details</summary>
Motivation: To avoid electron oscillation at the cathode and enhance vacuum diode work efficiency by analyzing electron motion trajectories and magnetic insulation effects.

Method: Reduces Vlasov-Maxwell system to nonlinear singular ODEs for field potentials, then to effective potential equation. Uses coupled nonlinear Fredholm integral equations and fixed point analysis for existence proofs, plus numerical analysis of complex bifurcations.

Result: Established existence of non-negative solutions on [0,x*) interval, analyzed previously unexplored θ(x)<0 regime, constructed bifurcation diagrams showing solution dependence on free boundary x*, and determined insulated diode spacing.

Conclusion: The approach successfully analyzes vacuum diode electron motion, identifies optimal trajectories, and provides comprehensive understanding of complex bifurcations in magnetic insulation regimes, enabling enhanced diode efficiency.

Abstract: In order to avoid the electron oscillation of the cathode and enhance the
work efficiency of a vacuum diode, an approach for analyzing the solutions and
complex bifurcation has been proposed and used to determine the optimal
trajectory of electron motion of the vacuum diode. This work is focusing on the
stationary self-consistent problem of magnetic insulation in a
space-charge-limited vacuum diode, modeled by a singularly perturbed
1.5-dimensional Vlasov-Maxwell system. We focus on the insulated regime,
characterized by the reflection of electrons back toward the cathode at a point
$x^{*}.$ The analysis proceeds in two primary stages. First, the original
Vlasov-Maxwell system is reduced to a nonlinear singular system of ordinary
differential equations governing the electric and magnetic field potentials.
Subsequently, this system is further reduced to a novel nonlinear singular ODE
for an effective potential $\theta(x).$ The existence of non-negative solutions
to this final equation is established on the interval $[0, x^{*})$, where
$\theta(x)>0$. This is achieved by reformulating the associated initial value
problem into a system of coupled nonlinear Fredholm integral equations and
proving the existence of fixed points for the corresponding operators. The most
significant and previously unexplored case occurs when $\theta(x)<0$ on the
interval $(x^{*}, 1]$, which corresponds to the fully insulated diode. For this
regime, we present a novel numerical analysis of complex solution bifurcations,
examining their dependence on system parameters and boundary conditions.
Bifurcation diagrams illustrating the solution $\theta(x)$ as a function of the
free boundary $x^{*}$ is constructed, and the insulated diode spacing is
determined.

</details>


### [18] [On the existence of solutions to some singular parabolic free boundary problems](https://arxiv.org/abs/2511.01987)
*Alessandro Audrito,Tomás Sanz-Perela*

Main category: math.AP

TL;DR: The paper constructs nonnegative weak solutions to a singular parabolic free boundary problem involving a non-smooth nonlinearity, establishes uniform regularity and growth estimates, and provides examples of self-similar and traveling wave solutions.


<details>
  <summary>Details</summary>
Motivation: To study singular parabolic free boundary problems with non-smooth nonlinearities, particularly the case where the right-hand side involves the derivative of u_+^γ with γ ∈ (0,1], which presents analytical challenges due to the singularity and lack of smoothness.

Method: Use an approximation procedure to construct weak solutions as limits, establish uniform optimal regularity, growth and nondegeneracy estimates, prove a Weiss-type monotonicity formula for approximating problems, and pass these uniform estimates to the limit.

Result: Existence of a class of weak solutions that is closed under blow-up and encodes the sharp free boundary condition, along with construction of explicit self-similar and traveling wave solutions.

Conclusion: The developed framework successfully handles the singular parabolic free boundary problem with non-smooth nonlinearity, providing a comprehensive analysis including existence, regularity, and explicit solution constructions.

Abstract: We construct nonnegative weak solutions to the singular parabolic free
boundary problem \[ \partial_t u - \Delta u = - \frac{\mathrm{d}}{\mathrm{d} u}
u_+^\gamma , \] where $\gamma \in (0,1]$, $u_+ := \max\{u,0\}$, and the term in
the right-hand side denotes the formal derivative of the non-smooth function $u
\mapsto u_+^\gamma$. Weak solutions are obtained as limits of a suitable
approximation procedure. We show uniform optimal regularity, optimal growth and
nondegeneracy estimates, and a Weiss-type monotonicity formula for solutions to
the approximating problem. Such uniform estimates are then passed to limit: we
prove the existence of a class of weak solutions to the free boundary problem
which is closed under blow-up and whose weak formulation encodes the sharp free
boundary condition. Finally, we construct several examples of weak solutions
with self-similar and traveling wave form.

</details>


### [19] [Gradient bounds for viscosity solutions to certain elliptic equations](https://arxiv.org/abs/2511.02073)
*Thalia Jeffres,Xiaolong Li*

Main category: math.AP

TL;DR: Analysis of modulus of continuity for solutions to degenerate elliptic equations with specific second-order structure, extending parabolic case results to derive gradient bounds.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of solutions to degenerate elliptic equations by studying their modulus of continuity, building on previous work in the parabolic case.

Method: Identify a one-dimensional equation where the modulus of continuity serves as a subsolution, using the specific structure of second-order terms involving symmetric positive semi-definite matrices.

Result: Successfully derived a method to obtain gradient bounds and other conclusions about solution behavior using the one-dimensional operator approach.

Conclusion: The modulus of continuity analysis provides an effective tool for studying regularity and gradient bounds in degenerate elliptic equations with the specified structural properties.

Abstract: Our principal object of study is the modulus of continuity of a periodic or
uniformly vanishing function \( u: \mathbb{R} ^{n} \rightarrow \mathbb{R} \)
which satisfies a degenerate elliptic equation \( F(x, u, \nabla u, D^{2} u) =
0 \) in the viscosity sense. The equations under consideration here have
second-order terms of the form \( -{\rm Trace} \, (\mathcal{A} (\|\nabla u \|)
\cdot D^{2} u) , \) where \( \mathcal{A} \) is an \( n\times n\) matrix which
is symmetric and positive semi-definite. Following earlier work, \cite{Li21},
of the second author, which addressed the parabolic case, we identify a
one-dimensional equation for which the modulus of continuity is a subsolution.
In favorable cases, this one-dimensional operator can be used to derive a
gradient bound on $u$ or to draw other conclusions about the nature of the
solution.

</details>


### [20] [Limited-Range Multilinear Off-Diagonal Extrapolation and Weighted Transference Principle](https://arxiv.org/abs/2511.02139)
*Jonas Sauer*

Main category: math.AP

TL;DR: Multilinear L^p extrapolation in limited-range, off-diagonal setting for mixed-norm Lebesgue spaces, covering full range (0,∞] for integrability exponents with detachable weight classes.


<details>
  <summary>Details</summary>
Motivation: To establish extrapolation results that completely detach weight class exponents from initial/target space exponents, enabling coverage of full integrability range and providing new insights into weight characteristic dependencies.

Method: Multilinear L^p extrapolation in limited-range, off-diagonal setting for mixed-norm Lebesgue spaces over σ-finite measure spaces, with weighted transference principle for compact abelian groups.

Result: Established extrapolation results covering full range (0,∞] for integrability exponents, with certain endpoint results being new even for ℝ^d, and dependency insights on weight characteristic.

Conclusion: The approach enables complete detachment of weight class exponents from space exponents, providing comprehensive extrapolation coverage and new insights into weight characteristic effects, with applications including weighted transference in compact abelian groups.

Abstract: Multilinear $L^p$ extrapolation results are established in a limited-range,
multilinear, and off-diagonal setting for mixed-norm Lebesgue spaces over
$\sigma$-finite measure spaces. Integrability exponents are allowed in the full
range $(0,\infty]$. We detach the exponents for the weight classes completely
from the exponents for the initial and target spaces for the extrapolation
except for the basic consistency condition. This enables to cover the full
range $(0,\infty]$ for all integrability exponents and provides new insights
into the dependency of the extrapolated bounds on the weight characteristic.
Certain endpoint results are new even for $\mathbb{R}^d$. Additionally, in the
setting of compact abelian groups, a weighted transference principle is
established.

</details>


### [21] [A new approach for the analysis of evolution partial differential equations on a finite interval](https://arxiv.org/abs/2511.02145)
*Türker Özsarı,Dionyssios Mantzavinos,Konstantinos Kalimeris*

Main category: math.AP

TL;DR: A method to reconstruct solutions of evolution PDEs on finite intervals using solutions from associated half-line problems, formulated via the Fokas unified transform and solved with fixed point arguments.


<details>
  <summary>Details</summary>
Motivation: To develop a unified approach for solving evolution PDEs on finite intervals by leveraging solutions from simpler half-line problems, enabling direct derivation of regularity estimates.

Method: Use the Fokas unified transform to formulate inverse problems for half-line data, solve via fixed point arguments in L^2-based Sobolev spaces with interpolation techniques, applied to heat and KdV equations.

Result: Successfully reconstructed finite interval solutions from half-line problems, demonstrated with heat and KdV equations, extended to time-dependent coefficient PDEs, enabling direct derivation of spatial/temporal regularity estimates.

Conclusion: The approach provides a powerful framework for analyzing evolution PDEs on finite intervals, with linear estimates applicable to establishing local well-posedness for nonlinear problems.

Abstract: We show that, for certain evolution partial differential equations, the
solution on a finite interval $(0,\ell)$ can be reconstructed as a
superposition of restrictions to $(0,\ell)$ of solutions to two associated
partial differential equations posed on the half-lines $(0,\infty)$ and
$(-\infty,\ell)$. Determining the appropriate data for these half-line problems
amounts to solving an inverse problem, which we formulate via the unified
transform of Fokas (also known as the Fokas method) and address via a fixed
point argument in $L^2$-based Sobolev spaces, including fractional ones through
interpolation techniques. We illustrate our approach through two canonical
examples, the heat equation and the Korteweg-de Vries (KdV) equation, and
provide numerical simulations for the former example. We further demonstrate
that the new approach extends to more general evolution partial differential
equations, including those with time-dependent coefficients. A key outcome of
this work is that spatial and temporal regularity estimates for problems on a
finite interval can be directly derived from the corresponding estimates on the
half-line. These results can, in turn, be used to establish local
well-posedness for related nonlinear problems, as the essential ingredients are
the linear estimates within nonlinear frameworks.

</details>


### [22] [On the Boltzmann-Fermi-Dirac Equation for Hard Potential: Global Existence and Uniqueness, Gaussian Lower Bound, and Moment Estimates](https://arxiv.org/abs/2511.02273)
*Gayoung An,Sungbin Park*

Main category: math.AP

TL;DR: Global existence, uniqueness, Gaussian bounds, and moment estimates for the spatially homogeneous Boltzmann-Fermi-Dirac equation with hard potentials and angular cutoff.


<details>
  <summary>Details</summary>
Motivation: Extend classical Boltzmann equation results to the Fermi-Dirac particle setting, addressing quantum statistical effects in kinetic theory.

Method: Mathematical analysis of the Boltzmann-Fermi-Dirac equation using techniques from kinetic theory and functional analysis for hard potentials (0≤γ≤2) with angular cutoff b.

Result: Proved: (1) global existence, uniqueness, and L¹₂ stability; (2) Gaussian lower bound creation for non-saturated solutions; (3) L¹ moment creation/propagation; (4) L∞ Gaussian/polynomial upper bound propagation for constant b and 0<γ≤1.

Conclusion: Successfully extended classical Boltzmann equation theory to the quantum Fermi-Dirac case, establishing fundamental mathematical properties including existence, uniqueness, and various moment bounds.

Abstract: In this paper, we study the global existence and uniqueness, Gaussian lower
bound, and moment estimates in the spatially homogeneous Boltzmann equation for
Fermi-Dirac particles for hard potential ($0\leq \gamma\leq 2$) with angular
cutoff $b$. Our results extend classical results to the Boltzmann-Fermi-Dirac
setting. In detail, (1) we show existence, uniqueness, and $L^1_2$ stability of
global-in-time solutions of the Boltzmann-Fermi-Dirac equation. (2) Assuming
the solution is not a saturated equilibrium, we prove creation of a Gaussian
lower bound for the solution. (3) We prove creation and propagation of $L^1$
polynomial and exponential moments of the solution under additional assumptions
on the angular kernel $b$ and $0<\gamma\leq 2$. (4) Finally, we show
propagation of $L^\infty$ Gaussian and polynomial upper bounds when $b$ is
constant and $0<\gamma\leq 1$.

</details>


### [23] [Uniform stability and optimal time decay rates of the compressible pressureless Navier-Stokes system in the critical regularity framework](https://arxiv.org/abs/2511.02321)
*Fucai Li,Jinkai Ni,Zhipeng Zhang*

Main category: math.AP

TL;DR: Global well-posedness and decay estimates for compressible pressureless Navier-Stokes system in critical Besov spaces, showing uniform bounded density behavior different from standard compressible models.


<details>
  <summary>Details</summary>
Motivation: To analyze the Cauchy problem for compressible pressureless Navier-Stokes system where density lacks dissipative mechanism, leading to strong coupling effects from nonlinear terms in momentum equations.

Method: Prove global well-posedness and uniform stability in critical Besov space, then establish optimal decay estimates for velocity under additional assumptions on initial density and velocity in Besov spaces.

Result: Global well-posedness achieved, optimal decay estimates for velocity established, and density shown to remain uniformly bounded in time - a new asymptotic behavior contrasting with dissipative density decay in standard compressible Navier-Stokes.

Conclusion: The pressureless model exhibits fundamentally different asymptotic behavior with uniformly bounded density, overcoming derivative loss challenges from nonlinearity through careful Besov space analysis.

Abstract: This paper investigates the Cauchy problem for the compressible pressureless
Navier-Stokes system in $\mathbb{R}^d$ with $d \geq 2$. Unlike the standard
isentropic compressible Navier-Stokes system, the density in the pressureless
model lacks a dissipative mechanism, leading to significant coupling effects
from nonlinear terms in the momentum equations. We first prove the global
well-posedness and uniform stability of strong solutions to the compressible
pressureless Navier-Stokes system in the critical Besov space
$\dot{B}_{2,1}^{\frac{d}{2}} \times \dot{B}_{2,1}^{\frac{d}{2}-1}$. Then, under
the additional assumption that the low-frequency component of the initial
density belongs to $\dot{B}_{2,\infty}^{\sigma_0+1}$ and that the initial
velocity is sufficiently small in $\dot{B}_{2,\infty}^{\sigma_0}$ with
$\sigma_0 \in (-\frac{d}{2}, \frac{d}{2}-1]$, we overcome the challenge of
derivative loss caused by nonlinearity and establish optimal decay estimates
for $u$ in $\dot{B}_{2,1}^{\sigma}$ with $\sigma \in (\sigma_0,
\frac{d}{2}+1]$. In particular, it is shown that the density remains uniformly
bounded in time which reveals a new asymptotic behavior in contrast to the
isentropic compressible Navier-Stokes system where the density exhibits a
dissipative structure and decays over time.

</details>


### [24] [Global well-posedness for generalized fractional Hartree equations with rough initial data in all dimensions](https://arxiv.org/abs/2511.02327)
*Yufeng Lu*

Main category: math.AP

TL;DR: Global existence of solutions for fractional Hartree equations is proven in real interpolation spaces between L² and new function spaces defined by fractional Schrödinger semigroup, implying global well-posedness in modulation spaces M_{p,p'}^{s_p} for p near 2 without small initial data.


<details>
  <summary>Details</summary>
Motivation: To establish global well-posedness for fractional Hartree equations in modulation spaces without requiring small initial data conditions, extending previous results.

Method: Adapts a splitting method from Hyakuna-Tsutsumi and Chaichenets et al. to modulation spaces, exploiting polynomial growth properties of fractional Schrödinger semigroup on these spaces with regularity loss s_p.

Result: Proves global existence of solutions for fractional Hartree equations in specified interpolation spaces and modulation spaces M_{p,p'}^{s_p} for p close to 2.

Conclusion: The approach successfully establishes global well-posedness for fractional Hartree equations in modulation spaces without smallness conditions on initial data, leveraging semigroup properties and splitting methods.

Abstract: We prove the global existence of the solution for fractional Hartree
equations with initial data in certain real interpolation spaces between
$L^{2}$ and some kinds of new function spaces defined by fractional
Schr\"odinger semigroup, which could imply the global well-posedness of the
equation in modulation spaces $M_{p,p'}^{s_{p}}$ for $p$ close to 2 with no
smallness condition on initial data, where $s_{p}=(m-2)(1/2-1/p)$. The proof
adapts a splitting method inspired by the work of Hyakuna-Tsutsumi, Chaichenets
et al. to the modulation spaces and exploits polynomial growth of the
fractional Schr\"odinger semi-group on modulation spaces $M_{p,p'}$ with loss
of regularity $s_{p}$.

</details>


### [25] [Global Well-Posedness for the 2D and 3D Prandtl-Shercliff Model](https://arxiv.org/abs/2511.02338)
*Wei-Xi Li,Zhan Xu,Anita Yang*

Main category: math.AP

TL;DR: Global well-posedness and analytic regularization for 2D/3D Prandtl-Shercliff model with Shercliff boundary layer effects.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for the Prandtl-Shercliff model, which describes boundary layer phenomena in magnetohydrodynamics, particularly addressing the challenging well-posedness issues in both 2D and 3D settings.

Method: Utilized Sobolev space analysis and exploited the intrinsic non-local diffusion properties induced by the Shercliff boundary layer. For 2D case: global well-posedness without structural assumptions; for 3D case: studied linearized version with analyticity in one tangential direction.

Result: 2D: Global well-posedness established with analytic regularization in all variables globally in time and space. 3D: Global well-posedness proved for linearized model with initial data analytic in one tangential direction.

Conclusion: The Shercliff boundary layer's non-local diffusion mechanism plays a crucial role in establishing global well-posedness and regularization properties for the Prandtl-Shercliff model across different dimensions.

Abstract: We investigate the Prandtl-Shercliff model in both two and three dimensions.
For the two-dimensional case, we establish global-in-time well-posedness in
Sobolev spaces without any structural assumptions on the initial data.
Furthermore, we show that the solution exhibits an analytic regularization
effect in all variables, which holds globally in time and in space up to the
boundary. For the three-dimensional case, we study a linearized version of the
model and prove its global-in-time well-posedness for initial data that are
analytic in only one tangential direction. The proofs rely crucially on the
intrinsic non-local diffusion induced by the Shercliff boundary layer.

</details>


### [26] [Local asymptotics for the nonlocal Swift-Hohenberg equation](https://arxiv.org/abs/2511.02341)
*Elisa Davoli,Christian Kuehn,Luca Scarpa,Lara Trussardi*

Main category: math.AP

TL;DR: Analysis of nonlocal-to-local asymptotics for Swift-Hohenberg equations, proving well-posedness and studying convergence under Neumann boundary conditions using energy estimates.


<details>
  <summary>Details</summary>
Motivation: Nonlocal-to-local asymptotics is crucial in PDE theory, functional analysis, and phase-separation models. Swift-Hohenberg equations are fundamental benchmark models in pattern formation and amplitude equations.

Method: Prove well-posedness of nonlocal Swift-Hohenberg equation and study nonlocal-to-local asymptotics with one and two nonlocal contributions using energy estimates on nonlocal problems under homogeneous Neumann boundary conditions.

Result: Established well-posedness results and analyzed convergence behavior from nonlocal to local formulations.

Conclusion: The study provides rigorous mathematical foundation for nonlocal-to-local transitions in Swift-Hohenberg equations, which are important for understanding pattern formation phenomena.

Abstract: The nonlocal-to-local asymptotics investigation for evolutionary problems is
a central topic both in the theory of PDEs and in functional analysis. More
recently, it became the main core of the mathematical analysis of
phase-separation models. In this paper we focus on the Swift-Hohenberg
equations which are key benchmark models in pattern formation problems and
amplitude equations. We prove well-posedness of the nonlocal Swift-Hohenberg
equation, and study the nonlocal-to-local asymptotics with one and two nonlocal
contributions under homogeneous Neumann boundary conditions using suitable
energy estimates on the nonlocal problems.

</details>


### [27] [Two-way Coupling of Fluid--Structure Interaction for Elastic Magneto-Swimmers:A Finite Element ALE Approach](https://arxiv.org/abs/2511.02402)
*Christophe Prud'Homme,Vincent Chabannes,Laëtitia Giraldi,Agathe Chouippe,Céline Van Landeghem*

Main category: math.AP

TL;DR: A finite element framework for simulating deformable magnetic micro-swimmers in confined fluids, validated against experimental data with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Magnetic micro-swimmers show promise for biomedical applications but are hard to control due to complex nonlinear dynamics involving magnetic actuation, elasticity, and fluid interactions in confined spaces.

Method: Developed a comprehensive finite element framework using Arbitrary Lagrangian-Eulerian formulation to simulate deformable elastic micro-swimmers in confined fluid domains, resolving full fluid dynamics and swimmer deformation on conforming meshes.

Result: Numerical experiments using Feel++ library showed excellent agreement with experimental data from literature. Validation benchmarks in 2D and 3D confirmed accuracy, robustness, and computational efficiency.

Conclusion: The framework represents a foundational step toward developing digital twins of magneto-swimmers for biomedical applications.

Abstract: Artificial micro-swimmers actuated by external magnetic fields hold
significant promise for targeted biomedical applications, including drug
delivery and micro-robot-assisted therapy. However, their dynamics remain
challenging to control due to the complex nonlinear coupling between magnetic
actuation, elastic deformations, and fluid interactions in confined biological
environments. Numerical modeling is therefore essential to better understand,
predict, and optimize their behavior for practical applications. In this work,
we present a comprehensive finite element framework based on the Arbitrary
Lagrangian--Eulerian formulation to simulate deformable elastic micro-swimmers
in confined fluid domains. The method employs a full-order model that resolves
the complete fluid dynamics while simultaneously tracking swimmer deformation
and global displacement on conforming meshes. Numerical experiments are
performed with the open-source finite element library Feel++, demonstrating
excellent agreement with experimental data from the literature. The validation
benchmarks in both two and three dimensions confirm the accuracy, robustness,
and computational efficiency of the proposed framework, representing a
foundational step toward developing digital twins of magneto-swimmers for
biomedical applications.

</details>


### [28] [Anisotropic Calderón problem for a logarithmic Schrödinger operator of order $2+$ on closed Riemannian manifolds](https://arxiv.org/abs/2511.02409)
*Saumyajit Das,Tuhin Ghosh,Susovan Pramanik*

Main category: math.AP

TL;DR: The paper studies anisotropic Calderón problems for non-local logarithmic Schrödinger operators on Riemannian manifolds, showing that both the metric and potential can be recovered from Cauchy data.


<details>
  <summary>Details</summary>
Motivation: To investigate whether the Riemannian metric and potential can be uniquely determined from boundary measurements (Cauchy data) for non-local logarithmic Schrödinger operators on manifolds.

Method: Analysis of the operator $(-\Delta_g+m)\log{(-\Delta_g+m)}+V$ with $m>1$ on closed, connected Riemannian manifolds of dimension $n\geq2$, using Cauchy data measurements.

Result: For common underlying manifolds with varying metrics, recovery of both metric and potential from Cauchy data is unconditional. For setwise distinct manifolds, Cauchy data uniquely determines the manifold up to isometry and potential up to gauge transformation.

Conclusion: The non-local logarithmic Schrödinger operator enables unique recovery of geometric and potential information from boundary measurements, with unconditional results in certain cases and requiring geometric assumptions in more general settings.

Abstract: In this article, we study the anisotropic Calder\'on problems for the non
local logarithimic Schr\"odinger operators $(-\Delta_g+m)\log{(-\Delta_g+m)}+V$
with $m>1$ on a closed, connected, smooth Riemannian manifold of dimension
$n\geq2$. We will show that, for the operator
$(-\Delta_g+m)\log{(-\Delta_g+m)}+V$, the recovery of both the Riemannian
metric and the potential is possible from the Cauchy data, in the setting of a
common underlying manifold with varying metrics. This result is unconditional.
The last result can be extended to the case of setwise distinct manifolds also.
In particular, we demonstrate that for setwise distinct manifolds, the Cauchy
data associated with the operator $(-\Delta_g+m)\log{(-\Delta_g+m)}+V$,
measured on a suitable non-empty open subset, uniquely determines the
Riemannian manifold up to isometry and the potential up to an appropriate gauge
transformation. This particular result is unconditional when the potential is
supported entirely within the observation set. In the more general
setting-where the potential may take nonzero values outside the observation
set-specific geometric assumptions are required on both the observation set and
the unknown region of the manifold.

</details>


### [29] [Self-similar blow-up solutions for the supercritical parabolic Hardy-Hénon equation](https://arxiv.org/abs/2511.02511)
*Razvan Gabriel Iagar,Ana I. Muñoz,Ariel Sánchez*

Main category: math.AP

TL;DR: This paper classifies self-similar blow-up solutions for the parabolic Hardy-Hénon equation and establishes existence results for various parameter ranges, contrasting with standard reaction-diffusion equations.


<details>
  <summary>Details</summary>
Motivation: To understand the existence and classification of self-similar finite-time blow-up solutions for the parabolic Hardy-Hénon equation, particularly contrasting with the non-existence results for standard reaction-diffusion equations.

Method: Mathematical analysis of self-similar solutions, derivation of generalized Lepin exponents, and numerical verification of optimality.

Result: Established existence of self-similar blow-up solutions for p > p_S(σ) when σ ≥ 2, and showed that for σ ≥ 4k-2, there are at least k different self-similar blow-up solutions. For σ ∈ (-2,2), existence is proven for p between p_S(σ) and the generalized Lepin exponents.

Conclusion: The Hardy-Hénon equation exhibits rich blow-up behavior with multiple self-similar solutions, fundamentally different from standard reaction-diffusion equations where such solutions don't exist beyond the Lepin exponent.

Abstract: We classify the self-similar solutions presenting finite time blow-up to the
parabolic Hardy-H\'enon equation $$ \partial_tu=\Delta u+|x|^{\sigma}u^p, \quad
(x,t)\in\mathbb{R}^N\times(0,\infty), $$ in dimension $N\geq3$ and the range of
exponents $$ \sigma\in(-2,\infty), \quad
p>p_S(\sigma):=\frac{N+2\sigma+2}{N-2}. $$ We establish the \emph{existence of
self-similar blow-up solutions for any $p>p_S(\sigma)$}, provided
$\sigma\geq2$. Moreover, we prove that, if $k$ is any natural number and
$\sigma\geq 4k-2$, the parabolic Hardy-H\'enon equation has at least $k$
different self-similar blow-up solutions for any $p>p_S(\sigma)$. These results
are in a stark contrast with the standard reaction-diffusion equation $$
\partial_tu=\Delta u+u^p, \quad (x,t)\in\mathbb{R}^N\times(0,\infty), $$ for
which non-existence of any self-similar solution has been established, provided
$p$ overpasses the Lepin exponent $p_L:=1+\frac{6}{N-10}$, $N\geq11$.
  For $\sigma\in(-2,2)$, we derive the expression of generalized Lepin
exponents $p_L(\sigma)$ for $\sigma\in(0,2)$, respectively
$\overline{p_L}(\sigma)$ for $\sigma\in(-2,0)$, and prove existence of
self-similar solutions with finite time blow-up for
$p\in(p_S(\sigma),p_L(\sigma))$, respectively
$p\in(p_S(\sigma),\overline{p_L}(\sigma))$. Numerical evidence of the
optimality of these exponents is also included.

</details>


### [30] [Global higher integrability for systems with $p$-growth structure in noncylindrical domains](https://arxiv.org/abs/2511.02553)
*Kristian Moring,Christoph Scheven,Leah Schätzler*

Main category: math.AP

TL;DR: The paper proves global higher integrability of the spatial gradient Du for p-growth parabolic systems in bounded noncylindrical domains, with p > 2(n+1)/(n+2).


<details>
  <summary>Details</summary>
Motivation: To extend higher integrability results for parabolic systems with p-growth structure from cylindrical domains to more general noncylindrical domains that satisfy certain regularity conditions.

Method: The authors consider the Cauchy-Dirichlet problem for p-growth parabolic systems in bounded noncylindrical domains, using analytical techniques to prove global higher integrability under suitable domain regularity assumptions.

Result: Global higher integrability of Du is established for p > 2(n+1)/(n+2) in domains that are sufficiently regular and do not grow or shrink too rapidly.

Conclusion: The work provides new higher integrability results for parabolic systems in noncylindrical domains, with the case p=2 being particularly novel and significant.

Abstract: We consider the Cauchy-Dirichlet problem to systems with $p$-growth structure
with $1 < p < \infty$, whose prototype is \begin{equation*}
  \partial_t u- \operatorname{div} \big( |Du|^{p-2} Du \big) =
\operatorname{div} \left( |F|^{p-2} F \right), \end{equation*} in a bounded
noncylindrical domain $E \subset \mathbb{R}^{n+1}$. For $p> \frac{2(n+1)}{n+2}$
and domains $E$ that satisfy suitable regularity assumptions and do not grow or
shrink too fast, we prove global higher integrability of $Du$. The result is
already new in the case $p=2$.

</details>


### [31] [A Monotonicity formula for almost self-similar suitable weak solutions to the stationary Navier-Stokes equations in $\mathbb R^5$](https://arxiv.org/abs/2511.02579)
*Yucong Huang,Aram Karakhanyan*

Main category: math.AP

TL;DR: A suitable weak solution to the stationary Navier-Stokes system in R^5 cannot behave like a self-similar function of degree -1 if the lower limit of the local Reynolds number is finite.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of weak solutions to the stationary Navier-Stokes equations in higher dimensions (R^5) and determine constraints on self-similar behavior.

Method: Developed a method combining monotonicity formula approach, classification of homogeneous solutions to incompressible Euler equations in R^5, and a projection theorem.

Result: Proved that weak solutions cannot exhibit self-similar behavior of degree -1 when the local Reynolds number has finite lower limit.

Conclusion: The paper establishes important constraints on the possible asymptotic behavior of solutions to the stationary Navier-Stokes system in five-dimensional space.

Abstract: In this paper we show that a suitable weak solution to the stationary
Navier-Stokes system in $\mathbb R^5$, cannot behave like a self-similar
function of degree negative one if the lower limit of the local Reynolds number
is finite.
  To prove the result we develop a method that uses a monotonicity formula
approach, classification of homogenous solutions to the incompressible Euler
equations in $\mathbb R^5$, and a projection theorem.

</details>


### [32] [Numbers and numerosities](https://arxiv.org/abs/2511.02639)
*Vieri Benci*

Main category: math.AP

TL;DR: This paper explores connections between numerosity theory and various number systems (ordinal, cardinal, hyperreal, surreal numbers), and proposes a unified definition of the Euclidean line incorporating these infinite number sets.


<details>
  <summary>Details</summary>
Motivation: To establish relationships between numerosity theory and different mathematical number systems, and to create a comprehensive definition of the Euclidean line that encompasses various types of infinite numbers.

Method: The authors develop new aspects of numerosity theory by combining it with the concept of continuum, analyzing its connections with ordinal numbers, cardinal numbers, hyperreal numbers, and surreal numbers.

Result: A unified definition of the Euclidean line is obtained that includes all the mentioned sets of infinite numbers, integrating numerosity theory with continuum mathematics.

Conclusion: The paper successfully bridges numerosity theory with various infinite number systems and provides an expanded definition of the Euclidean line that incorporates ordinal, cardinal, hyperreal, and surreal numbers.

Abstract: We develop new aspects of the the of numerosity theory; more exactly, we
emphasize its relation with the ordinal numbers, cardinal numbers, hyperreal
numbers and surreal numbers. In particular, we combine the notion of numerosity
with the idea of continuum and we get a definition of Euclidean line which
includes all the sets of infinite numbers mentioned above.

</details>


### [33] [Revisited for existence proof of optimal solution in Bernoulli free boundary problem using an energy-gap cost functional](https://arxiv.org/abs/2511.02702)
*Shiouhe Wang,Fang Shen,Yi Yang,Xueshang Feng*

Main category: math.AP

TL;DR: This paper corrects a proof error in a previous work on Bernoulli free boundary problems solved via shape optimization, specifically fixing an incorrect application of the Cauchy-Schwarz inequality in bounding solution sequences.


<details>
  <summary>Details</summary>
Motivation: To rectify a mathematical error in a previous paper's proof regarding the boundedness of solution sequences for state problems in shape optimization of Bernoulli free boundary problems.

Method: The authors use the Poincaré-Friedrichs inequality to properly establish the boundedness of solution sequences with respect to domain variations, replacing the incorrect Cauchy-Schwarz inequality approach from the original paper.

Result: Successfully corrected the proof gap in the original paper's Eq.(48), providing a mathematically sound justification for the continuity of state problems with respect to domain variations.

Conclusion: The corrigendum properly addresses the mathematical flaw in the original proof and establishes the correct foundation for proving existence of optimal solutions in the Bernoulli free boundary shape optimization framework.

Abstract: Bernoulli free boundary problem is numerically solved via shape optimization
that minimizes a cost functional subject to state problems constraints. In
\cite{1}, an energy-gap cost functional was formulated based on two auxiliary
state problems, with existence of optimal solution attempted through continuity
of state problems with respect to the domain. Nevertheless, there exists a
corrigendum in Eq.(48) in \cite{1}, where the boundedness of solution sequences
for state problems with respect to the domain cannot be directly estimated via
the Cauchy-Schwarz inequality as \textbf{Claimed}. In this comment, we rectify
this proof by Poincar\'e-Friedrichs inequality.

</details>


### [34] [Global well-posedness of the 2D primitive equations with fractional horizontal dissipation](https://arxiv.org/abs/2511.02723)
*Changhui Tan,Zhuan Ye*

Main category: math.AP

TL;DR: Global well-posedness of 2D incompressible primitive equations with fractional horizontal dissipation is established for large initial data when α≥α₀≈1.1108, and for small initial data when α∈[1,α₀).


<details>
  <summary>Details</summary>
Motivation: To understand the global existence and uniqueness of strong solutions for 2D primitive equations with fractional dissipation, particularly determining the critical dissipation exponent.

Method: Analysis of two-dimensional incompressible primitive equations with fractional horizontal dissipation, using mathematical techniques to establish well-posedness conditions.

Result: Proved global well-posedness for arbitrarily large initial data when α≥α₀≈1.1108, and for small initial data (with smallness only on L∞ norm of initial vorticity) when α∈[1,α₀).

Conclusion: The critical dissipation exponent for global well-posedness of 2D primitive equations is approximately α₀≈1.1108, with different regimes for large vs small initial data.

Abstract: In this paper, we investigate the two-dimensional incompressible primitive
equations with fractional horizontal dissipation. Specifically, we establish
global well-posedness of strong solutions for arbitrarily large initial data
when the dissipation exponent satisfies $\alpha\geq\alpha_{0}\approx1.1108$. In
addition, we prove global well-posedness of strong solutions for small initial
data when $\alpha \in [1, \alpha_0)$. Notably, the smallness assumption is
imposed only on the $L^\infty$ norm of the initial vorticity.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [35] [Making PLUMED fly: a tutorial on optimizing performance](https://arxiv.org/abs/2511.02699)
*Daniele Rapetti,Massimiliano Bonomi,Carlo Camilloni,Giovanni Bussi,Gareth A. Tribello*

Main category: physics.comp-ph

TL;DR: A tutorial on optimizing PLUMED performance using vector-based commands and algorithmic tricks for computationally demanding molecular dynamics analysis tasks.


<details>
  <summary>Details</summary>
Motivation: PLUMED is increasingly used for computationally intensive tasks where performance optimization becomes critical, despite typically having negligible computational costs compared to molecular dynamics force evaluation.

Method: Implemented a performance measurement tool and used it to benchmark optimization strategies, including using vector-based commands instead of individual scalar operations for distance/angle/torsion calculations, and optimizing atomic order parameters and secondary structure variables.

Result: Generated detailed performance benchmarks showing significant improvements when using vector-based commands for large-number calculations, and provided optimization strategies for various computational tasks.

Conclusion: The tutorial provides practical prescriptions for performance optimization in PLUMED that can also be applied to custom code development, using algorithmic tricks and vector-based operations to enhance computational efficiency.

Abstract: PLUMED is an open-source software package that is widely used for analyzing
and enhancing molecular dynamics simulations that works in conjunction with
most available molecular dynamics softwares. While the computational cost of
PLUMED calculations is typically negligible compared to the molecular dynamics
code's force evaluation, the software is increasingly being employed for more
computationally demanding tasks where performance optimization becomes
critical. In this tutorial, we describe a recently implemented tool that can be
used to reliably measure code performance. We then use this tool to generate
detailed performance benchmarks that show how calculations of large-numbers of
distances, angles or torsions can be optimized by using vector-based commands
rather than individual scalar operations. We then present benchmarks that
illustrate how to optimize calculations of atomic order parameters and
secondary structure variables. Throughout the tutorial and in our
implementations we endeavor to explain the algorithmic tricks that are being
used to optimize the calculations so others can make use of these prescriptions
both when they are using PLUMED and when they are writing their own codes.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [36] [Unified Model of Heated Plasma Expansion](https://arxiv.org/abs/2511.02028)
*Ritwik Sain,Lance Labun,Ou Z. Labun,Bjorn Manuel Hegelich*

Main category: physics.plasm-ph

TL;DR: A fluid model for plasma expansion into vacuum with external heating is developed, featuring a three-parameter family of self-similar solutions that classify plasma dynamics into five regimes based on length scale ratios.


<details>
  <summary>Details</summary>
Motivation: To predict plasma density and temperature distributions in early stages of high-intensity laser-plasma interactions, particularly for understanding prepulse-target interactions.

Method: Developed a fluid model of plasma expansion incorporating external heating, with a new three-parameter family of self-similar solutions modeling electron temperature variations.

Result: Identified five dynamical regimes based on length scale ratios (λ_s/λ_D and L/λ_s), ranging from quasineutral expansion to Coulomb explosion of bare ion slabs, with detailed analysis of asymptotic limits.

Conclusion: The self-similar framework provides scaling relations for optimizing laser-plasma schemes and guiding experimental designs in high-intensity laser experiments.

Abstract: Motivated by the need to predict plasma density and temperature distributions
created in the early stages of high-intensity laser-plasma interactions, we
develop a fluid model of plasma expansion into vacuum that incorporates
external heating. We propose a new three-parameter family of self-similar
solutions for plasma expansion that models a wide range of spatiotemporal
variations of the electron temperature. Depending on the relative scales of the
heated plasma domain $L$, the Debye length $\lambda_D$ and an emergent
ion-acoustic correlation length $\lambda_s$, characterized by the parameters
$\lambda_s/\lambda_D$ and $L/\lambda_s$, a spectrum of dynamical behaviors for
the expanding plasma are identified. The behavior is classified into five
dynamical regimes, ranging from nearly quasineutral expansion to the formation
of bare ion slabs susceptible to Coulomb explosion. The limiting self-similar
solutions are analyzed, and the dynamics in the five asymptotic limits in the
parameter space are detailed. Scaling relations for the length scales and
energies of the expanding plasma are proposed. The self-similar framework is
applied to laser-plasma interactions, specifically addressing the plasma
dynamics at a target surface during prepulse-target interactions. The results
offer insights into the expansion behavior based on the laser-plasma
parameters, and scaling relations for optimizing laser-plasma schemes and
guiding experimental designs in high-intensity laser experiments.

</details>


### [37] [Integrated Observation of Isotope-Dependent Turbulence, Zonal Flow, and Turbulence-Driven Transport](https://arxiv.org/abs/2511.02070)
*Shinsuke Ohshima,Hiroyuki Okada,Shinji Kobayashi,Shinichiro Kado,Takashi Minami,Fumiyoshi Kin,Shigeru Inagaki,Shigeru Konoshima,Tohru Mizuuchi,Kazunobu Nagasaki*

Main category: physics.plasm-ph

TL;DR: Study shows deuterium plasmas enhance zonal flow activity and reduce turbulence-induced transport through improved nonlinear coupling between zonal flows and turbulence, explaining the isotope effect in plasma confinement.


<details>
  <summary>Details</summary>
Motivation: To address the unresolved issue of "isotope effect" in plasma transport by investigating hydrogen/deuterium isotope dependence in nonlinear turbulence systems.

Method: Analysis of zonal flow activity, turbulence properties, and their nonlinear interaction in torus plasma using low-density electron cyclotron heating plasmas in Heliotron J with varying D gas fractions (10-80%).

Result: Deuterium plasmas showed enhanced zonal flow activity, larger turbulence scale size, reduced fluctuations, and significant reduction of turbulence-induced transport due to improved nonlinear coupling between zonal flows and turbulence.

Conclusion: The isotope dependence on turbulence systems is essential for explaining the isotope effect on confinement improvement and vital for predicting future fusion reactor performance.

Abstract: To address the long-standing unresolved issue of "isotope effect" in plasma
transport, this study investigates the hydrogen/deuterium (H/D) isotope
dependence of a nonlinear turbulence system. The analysis focuses on the zonal
flow (ZF) activity, turbulence properties, their nonlinear interaction, and
resulting turbulent transport in a torus plasma. ZF activity, observed in
low-density electron cyclotron heating plasmas in Heliotron J, is enhanced with
increasing D gas fraction from 10 to 80 percent. While the turbulence scale
size in the edge region (rho approx 0.8) is larger in D plasmas, the reduction
and decoupling of fluctuations, associated with an enhanced ZF, results in
beneficial impacts on turbulent transport, driven by an enhanced nonlinear
coupling between the ZF and turbulence in D plasmas. These differences in the
turbulence nature lead to the significant reduction of turbulence-induced
transport observed in the D plasma. These comprehensive observations suggest
that the isotope dependence on the turbulence system is essential for
explaining the isotope effect on confinement improvement and is vital in
predicting the performance of future fusion reactors.

</details>


### [38] [Detecting Shearless Phase-Space Transport Barriers in Global Gyrokinetic Turbulence Simulations with Test Particle Map Models](https://arxiv.org/abs/2511.02075)
*Norman M. Cao,Hongxuan Zhu,Gabriel C. Grime,Timothy Stoltzfus-Dueck*

Main category: physics.plasm-ph

TL;DR: Zonal jets with non-zero flow curvature in fusion plasmas act as robust barriers to particle transport and turbulence spreading, forming shearless invariant tori that create partial phase-space transport barriers.


<details>
  <summary>Details</summary>
Motivation: To understand the role of weak shear regions in non-monotonic radial electric field profiles in fusion plasmas, particularly how these regions affect turbulent transport suppression beyond the well-understood zonal E×B flow shear layers.

Method: Used global total-f gyrokinetic particle-in-cell code XGC for electrostatic simulations, isolated quasi-coherent fluctuations to construct a map model for Lagrangian dynamics of gyrokinetic test particles, and identified shearless invariant tori.

Result: Shearless regions with non-zero flow curvature form zonal jets that act as robust barriers to particle transport and turbulence spreading. Shearless invariant tori were identified as partial phase-space transport barriers, and avalanches cause reconnection events forming 'cold/warm core ring' structures similar to oceanic jets.

Conclusion: Shearless tori may generically arise from tertiary instabilities or other discrete eigenmodes, suggesting their potential relevance to broader classes of turbulent fluctuations in fusion plasmas.

Abstract: In magnetically confined fusion plasmas, the role played by zonal E$\times$B
flow shear layers in the suppression of turbulent transport is relatively
well-understood. However, less is understood about the role played by the weak
shear regions that arise in the non-monotonic radial electric field profiles
often associated with these shear layers. In electrostatic simulations from the
global total-f gyrokinetic particle-in-cell code XGC, we demonstrate how
shearless regions with non-zero flow curvature form zonal "jets" that, in
conjunction with neighboring regions of shear, can act as robust barriers to
particle transport and turbulence spreading. By isolating quasi-coherent
fluctuations radially localized to the zonal jets, we construct a map model for
the Lagrangian dynamics of gyrokinetic test particles in the presence of drift
waves. We identify the presence of shearless invariant tori in this model and
verify that these tori act as partial phase-space transport barriers in the
simulations. We also demonstrate how avalanches impinging on these shearless
tori cause reconnection events that form "cold/warm core ring" structures
analogous to those found in oceanic jets, facilitating transport across the
barriers without destroying them completely. We discuss how shearless tori may
generically arise from tertiary instabilities or other types of discrete
eigenmodes, suggesting their potential relevance to broader classes of
turbulent fluctuations.

</details>


### [39] [Laser diagnostics for negative ion source optimization: insights from SPIDER at the ITER Neutral Beam Test Facility](https://arxiv.org/abs/2511.02381)
*R. Agnello,M. Barbisan,R. Pasqualotto,B. Pouradier-Duteil,E. Sartori,A. Tiso,B. Zaniol*

Main category: physics.plasm-ph

TL;DR: SPIDER negative ion beam source uses advanced laser diagnostics (CRDS and LAS) to monitor H-/D- ion densities and caesium distribution for ITER-relevant plasma source optimization.


<details>
  <summary>Details</summary>
Motivation: ITER Heating Neutral Beams require high-energy H/D atom beams, and SPIDER serves as a full-scale prototype to validate design and verify performance for meeting ITER source targets.

Method: Uses Cavity Ring-Down Spectroscopy (CRDS) for H-/D- ion density measurements and Laser Absorption Spectroscopy (LAS) for caesium neutral density tracking with four lines of sight.

Result: CRDS demonstrated sensitivity to alignment due to long optical cavity, requiring structural improvements. LAS effectively monitors caesium conditioning status and distribution within the source.

Conclusion: Laser diagnostics play an essential role in developing ITER-relevant plasma sources and ongoing efforts focus on improving measurement accuracy in challenging beam source environments.

Abstract: The ITER Heating Neutral Beams (HNBs) require large, high-energy H/D atom
beams (285/330 A/m^2 extracted current density, and 1/0.87 MeV acceleration
energy, respectively for H and D). To address the associated challenges, the
SPIDER negative ion RF beam source at the Neutral Beam Test Facility (NBTF) in
Padova (Italy) serves as a full-scale source prototype with a 100 kV triode
accelerator, for design validation and performance verification. SPIDER is
equipped with two advanced laser diagnostics to monitor key plasma parameters;
Cavity Ring-Down Spectroscopy (CRDS) is used to measure H$^-$\slash D$^-$ ion
densities, while Laser Absorption Spectroscopy (LAS) tracks caesium neutral
density in the source. These measurements are essential for optimizing negative
ion production and meeting ITER source targets. We present diagnostic upgrade
details, recent experimental results, and correlations with other machine
parameters. Since CRDS relies on a single 4.637-meter-long optical cavity, the
longest used in such sources, it has demonstrated sensitivity to alignment.
Based on recent experimental experience, structural improvements are being
implemented to enhance both stability and measurement reliability. LAS has
mainly been employed as a tool to monitor the caesium conditioning status of
SPIDER. Additionally, due to a distributed measurement over four lines of
sight, LAS has proven effective in monitoring the caesium distribution within
the source. This work demonstrates the essential role of laser diagnostics in
developing ITER-relevant plasma sources and informs ongoing efforts to improve
measurement accuracy in challenging environments.

</details>


### [40] [Simulation of multi-species kinetic instabilities with the Numerical Flow Iteration](https://arxiv.org/abs/2511.02405)
*Rostislav-Paul Wilhelm,Manuel Torrilhon*

Main category: physics.plasm-ph

TL;DR: Extends Numerical Flow Iteration (NuFI) method to handle non-periodic boundary conditions and reduces computational complexity for longer kinetic instability simulations.


<details>
  <summary>Details</summary>
Motivation: Kinetic instabilities in plasma physics require high-dimensional distribution functions that are computationally expensive, and plasma dynamics involves multi-scale phenomena with scale separation between ions and electrons.

Method: Extends the Numerical Flow Iteration (NuFI) method to support non-periodic boundary conditions and implements computational complexity reduction techniques.

Result: Successfully demonstrated that NuFI can handle non-periodic boundaries and achieve reduced computational complexity, enabling longer simulation periods.

Conclusion: The extended NuFI method provides a viable high-fidelity alternative for simulating complex kinetic instabilities with improved computational efficiency and boundary condition handling.

Abstract: Kinetic instabilities are one of the most challenging aspects in
computational plasma physics. Accurately capturing their onset and evolution
requires fine resolution of the high-dimensional distribution functions of each
relevant species, which quickly becomes computationally prohibitively
expensive. Additionally, plasma dynamics is an inherently multi-scale
phenomenon due to the vast separation of scales between heavy ions and
light-weight electrons. In previous work the Numerical Flow Iteration (NuFI)
was suggested as a high-fidelity alternative with reduced memory complexity
making it an interesting candidate to simulate complicated kinetic
instabilities. In this work we extend NuFI to non-periodic boundary conditions
and demonstrate how it is possible to reduce the computational complexity to
allow for longer simulation periods.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [41] [Data-driven Learning of Interaction Laws in Multispecies Particle Systems with Gaussian Processes: Convergence Theory and Applications](https://arxiv.org/abs/2511.02053)
*Jinchao Feng,Charles Kulick,Sui Tang*

Main category: stat.ML

TL;DR: Gaussian process framework for learning interaction kernels in multi-species particle systems from trajectory data, extending previous single-species methods to handle heterogeneous populations and asymmetric interactions.


<details>
  <summary>Details</summary>
Motivation: Multi-species systems present new challenges including heterogeneous populations, multiple unknown kernels, and asymmetric interactions like predator-prey dynamics, requiring extension of previous single-species approaches.

Method: Nonparametric Bayesian formulation using Gaussian processes to learn interaction kernels from trajectory data, with rigorous statistical analysis including recoverability proofs and error bounds.

Result: Established recoverability of interaction kernels, provided quantitative error bounds, proved statistical optimality of posterior estimators, and demonstrated effectiveness through numerical experiments.

Conclusion: Provides a complete statistical framework for data-driven inference in multi-species systems, advancing multiscale modeling by connecting microscopic dynamics with emergent macroscopic behavior.

Abstract: We develop a Gaussian process framework for learning interaction kernels in
multi-species interacting particle systems from trajectory data. Such systems
provide a canonical setting for multiscale modeling, where simple microscopic
interaction rules generate complex macroscopic behaviors. While our earlier
work established a Gaussian process approach and convergence theory for
single-species systems, and later extended to second-order models with
alignment and energy-type interactions, the multi-species setting introduces
new challenges: heterogeneous populations interact both within and across
species, the number of unknown kernels grows, and asymmetric interactions such
as predator-prey dynamics must be accommodated. We formulate the learning
problem in a nonparametric Bayesian setting and establish rigorous statistical
guarantees. Our analysis shows recoverability of the interaction kernels,
provides quantitative error bounds, and proves statistical optimality of
posterior estimators, thereby unifying and generalizing previous single-species
theory. Numerical experiments confirm the theoretical predictions and
demonstrate the effectiveness of the proposed approach, highlighting its
advantages over existing kernel-based methods. This work contributes a complete
statistical framework for data-driven inference of interaction laws in
multi-species systems, advancing the broader multiscale modeling program of
connecting microscopic particle dynamics with emergent macroscopic behavior.

</details>


### [42] [Optimizing Kernel Discrepancies via Subset Selection](https://arxiv.org/abs/2511.02706)
*Deyao Chen,François Clément,Carola Doerr,Nathan Kirk*

Main category: stat.ML

TL;DR: A novel subset selection algorithm for kernel discrepancies that efficiently generates low-discrepancy samples from uniform distributions and more general distributions using kernel Stein discrepancy.


<details>
  <summary>Details</summary>
Motivation: To extend subset selection problems to kernel discrepancies for analyzing worst-case errors in quasi-Monte Carlo methods, enabling efficient generation of low-discrepancy samples from both uniform and general distributions.

Method: Introduce a subset selection algorithm applicable to general kernel discrepancies, using kernel Stein discrepancy for general distributions with known density functions, and explore relationships between classical L2 and L∞ star discrepancies.

Result: The algorithm efficiently selects m-element subsets from large populations (n ≫ m) to generate low-discrepancy samples for both traditional QMC settings and more general distributions.

Conclusion: The proposed method provides a powerful framework for subset selection in kernel discrepancy settings, bridging classical QMC approaches with modern kernel methods for improved sample generation across various distributions.

Abstract: Kernel discrepancies are a powerful tool for analyzing worst-case errors in
quasi-Monte Carlo (QMC) methods. Building on recent advances in optimizing such
discrepancy measures, we extend the subset selection problem to the setting of
kernel discrepancies, selecting an m-element subset from a large population of
size $n \gg m$. We introduce a novel subset selection algorithm applicable to
general kernel discrepancies to efficiently generate low-discrepancy samples
from both the uniform distribution on the unit hypercube, the traditional
setting of classical QMC, and from more general distributions $F$ with known
density functions by employing the kernel Stein discrepancy. We also explore
the relationship between the classical $L_2$ star discrepancy and its
$L_\infty$ counterpart.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [43] [Delta-learned force fields for nonbonded interactions: Addressing the strength mismatch between covalent-nonbonded interaction for global models](https://arxiv.org/abs/2511.01913)
*Leonardo Cázares-Trejo,Marco Loreto-Silva,Huziel E. Sauceda*

Main category: physics.chem-ph

TL;DR: Δ-sGDML is a scale-aware ML force field that decouples intra- and intermolecular physics by training fragment-specific models alongside a binding model, improving force accuracy for noncovalent interactions without sacrificing energy accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurately learning noncovalent interactions (vdW dispersion, hydrogen/halogen bonding, ion-π, π-stacking) alongside covalent forces remains challenging for ML force fields, especially with Coulomb-matrix descriptors that overrepresent intermolecular features.

Method: Δ-sGDML framework explicitly decouples intra- and intermolecular physics by training fragment-specific models and a dedicated binding model, then composing them at inference time.

Result: Across benzene dimers, host-guest complexes, benzene-water, and benzene-Na+, Δ-sGDML achieves up to 75% force-error reduction over single global models while maintaining energy accuracy. MD simulations show stable trajectories across 10-400K temperature range.

Conclusion: The method provides a practical approach to homogenize per-fragment errors and recover reliable noncovalent physics in global ML force fields, addressing the mismatch between covalent force labels and intermolecular feature representation.

Abstract: Noncovalent interactions--vdW dispersion, hydrogen/halogen bonding,
ion-$\pi$, and $\pi$-stacking--govern structure, dynamics, and emergent
phenomena in materials and molecular systems, yet accurately learning them
alongside covalent forces remains a core challenge for machine-learned force
fields (MLFFs). This challenge is acute for global models that use
Coulomb-matrix (CM) descriptors compared under Euclidean/Frobenius metrics in
multifragment settings. We show that the mismatch between predominantly
covalent force labels and the CM's overrepresentation of intermolecular
features biases single-model training and degrades force-field fidelity. To
address this, we introduce \textit{$\Delta$-sGDML}, a scale-aware formulation
within the sGDML framework that explicitly decouples intra- and intermolecular
physics by training fragment-specific models alongside a dedicated binding
model, then composing them at inference. Across benzene dimers, host-guest
complexes (C$_{60}$@buckycatcher, NO$_3^-$@i-corona[6]arene), benzene-water,
and benzene-Na$^+$, \mbox{$\Delta$-sGDML} delivers consistent gains over a
single global model, with fragment-resolved force-error reductions up to
\textbf{75\%}, without loss of energy accuracy. Furthermore, molecular-dynamics
simulations further confirm that the $\Delta$-model yields a reliable force
field for C$_{60}$@buckycatcher, producing stable trajectories across a wide
range of temperatures (10-400~K), unlike the single global model, which loses
stability above $\sim$200~K. The method offers a practical route to homogenize
per-fragment errors and recover reliable noncovalent physics in global MLFFs.

</details>


### [44] [From Densities to Potentials: Benchmarking Local Exchange-Correlation Approximations](https://arxiv.org/abs/2511.02744)
*Visagan Ravindran,Clio Johnson,Neil D. Drummond,Stewart J. Clark,Nikitas. I. Gidopoulos*

Main category: physics.chem-ph

TL;DR: The paper benchmarks Kohn-Sham potentials from QMC calculations against popular density functional approximations, finding that different DFAs produce similar densities despite differing potentials, and QMC-KS gaps are larger than most DFA gaps except Hartree-Fock.


<details>
  <summary>Details</summary>
Motivation: To provide benchmark QMC-KS potentials for insulators and semiconductors and compare them with popular density functional approximations to understand differences in their performance.

Method: Using Kohn-Sham inversion method on densities from variational and diffusion quantum Monte Carlo calculations to obtain QMC-KS potentials, then comparing with KS potentials from various DFAs.

Result: Different DFAs yield similar electron densities despite differences in their KS potentials; QMC-KS gaps are larger than most DFA gaps (except Hartree-Fock); KS gap is sensitive to semicore states in pseudopotentials.

Conclusion: Comparison of KS gaps with experiment should be done cautiously due to sensitivity to semicore states, and QMC provides valuable benchmarks for evaluating density functional approximations.

Abstract: Using the Kohn-Sham (KS) inversion method of Hollins et al. [J. Phys.:
Condens. Matter 29, 04LT01 (2017)], we invert densities from variational and
diffusion quantum Monte Carlo (QMC) calculations to obtain benchmark QMC-KS
potentials for a range of insulators and semiconductors, which we then compare
to the KS potentials of popular density functional approximations (DFAs). Our
results show that different DFAs yield similar electron densities, despite
differences in their KS potentials, which originate primarily from the exchange
and correlation contribution. We also find that the KS gap from the QMC density
is typically larger than the KS gaps of most DFAs, with the exception of
Hartree-Fock. Finally, the KS gap is sensitive to the inclusion of semicore
states in the pseudopotentials, such that comparison with experiment should be
done with caution.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [45] [Locally-Supervised Global Image Restoration](https://arxiv.org/abs/2511.01998)
*Benjamin Walder,Daniel Toader,Robert Nuster,Günther Paltauf,Peter Burgholzer,Gregor Langer,Lukas Krainer,Markus Haltmeier*

Main category: cs.CV

TL;DR: A learning-based method for image reconstruction from incomplete measurements using multiple invariances to achieve fully supervised performance with less ground truth data.


<details>
  <summary>Details</summary>
Motivation: To address image reconstruction from fixed, deterministic sampling patterns with incomplete coverage, overcoming limitations of conventional supervised and self-supervised methods.

Method: Exploits multiple invariances of the underlying image distribution to enable reconstruction from incomplete measurements without requiring full ground truth coverage.

Result: Validated on optical-resolution image upsampling in photoacoustic microscopy, achieving competitive or superior results with substantially less ground truth data.

Conclusion: The approach enables high-quality image reconstruction from incomplete measurements by leveraging distributional invariances, reducing ground truth data requirements.

Abstract: We address the problem of image reconstruction from incomplete measurements,
encompassing both upsampling and inpainting, within a learning-based framework.
Conventional supervised approaches require fully sampled ground truth data,
while self-supervised methods allow incomplete ground truth but typically rely
on random sampling that, in expectation, covers the entire image. In contrast,
we consider fixed, deterministic sampling patterns with inherently incomplete
coverage, even in expectation. To overcome this limitation, we exploit multiple
invariances of the underlying image distribution, which theoretically allows us
to achieve the same reconstruction performance as fully supervised approaches.
We validate our method on optical-resolution image upsampling in photoacoustic
microscopy (PAM), demonstrating competitive or superior results while requiring
substantially less ground truth data.

</details>


### [46] [Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization](https://arxiv.org/abs/2511.02329)
*Shaohan Li,Yunpeng Shi,Gilad Lerman*

Main category: cs.CV

TL;DR: Cycle-Sync is a robust global framework for camera pose estimation that uses modified message-passing least squares with cycle consistency and robust loss functions, achieving state-of-the-art performance without bundle adjustment.


<details>
  <summary>Details</summary>
Motivation: To develop a robust and global camera pose estimation framework that avoids the need for bundle adjustment while achieving strong theoretical guarantees and practical performance.

Method: Adapts message-passing least squares (MPLS) for camera location estimation, emphasizes cycle-consistent information, redefines cycle consistencies using estimated distances, incorporates Welsch-type robust loss, and adds outlier rejection via robust subspace recovery.

Result: Establishes strongest known deterministic exact-recovery guarantee for camera location estimation, shows cycle consistency alone achieves lowest known sample complexity, and outperforms leading pose estimators including full structure-from-motion pipelines with bundle adjustment.

Conclusion: Cycle-Sync provides a robust, globally optimal camera pose estimation framework that eliminates the need for bundle adjustment while delivering superior performance through cycle consistency and robust optimization techniques.

Abstract: We introduce Cycle-Sync, a robust and global framework for estimating camera
poses (both rotations and locations). Our core innovation is a location solver
that adapts message-passing least squares (MPLS) -- originally developed for
group synchronization -- to camera location estimation. We modify MPLS to
emphasize cycle-consistent information, redefine cycle consistencies using
estimated distances from previous iterations, and incorporate a Welsch-type
robust loss. We establish the strongest known deterministic exact-recovery
guarantee for camera location estimation, showing that cycle consistency alone
-- without access to inter-camera distances -- suffices to achieve the lowest
sample complexity currently known. To further enhance robustness, we introduce
a plug-and-play outlier rejection module inspired by robust subspace recovery,
and we fully integrate cycle consistency into MPLS for rotation
synchronization. Our global approach avoids the need for bundle adjustment.
Experiments on synthetic and real datasets show that Cycle-Sync consistently
outperforms leading pose estimators, including full structure-from-motion
pipelines with bundle adjustment.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [47] [Machine Learning for RNA Secondary Structure Prediction: a review of current methods and challenges](https://arxiv.org/abs/2511.02622)
*Giuseppe Sacco,Giovanni Bussi,Guido Sanguinetti*

Main category: q-bio.BM

TL;DR: This review surveys modern machine learning approaches for RNA secondary structure prediction, highlighting the field's evolution from thermodynamic methods to data-driven models, current challenges with generalization, and future directions including foundation models and complex motif prediction.


<details>
  <summary>Details</summary>
Motivation: RNA secondary structure prediction is crucial for understanding molecular function and designing therapeutics, with the field transitioning from limited thermodynamic approaches to more powerful data-driven machine learning methods.

Method: The review covers single-sequence models, evolutionary-based approaches, and hybrid models that combine machine learning with biophysics, with emphasis on RNA foundation models trained on large unlabeled sequence datasets.

Result: Modern machine learning models have achieved significant performance gains but face a "generalization crisis" where they fail on new RNA families, prompting stricter benchmarking standards and the development of foundation models to address data scarcity.

Conclusion: Future challenges include predicting complex motifs like pseudoknots, scaling to long transcripts, incorporating modified nucleotides, predicting dynamic ensembles rather than static structures, and establishing standardized prospective benchmarking for unbiased validation.

Abstract: Predicting the secondary structure of RNA is a core challenge in
computational biology, essential for understanding molecular function and
designing novel therapeutics. The field has evolved from foundational but
accuracy-limited thermodynamic approaches to a new data-driven paradigm
dominated by machine learning and deep learning. These models learn folding
patterns directly from data, leading to significant performance gains. This
review surveys the modern landscape of these methods, covering single-sequence,
evolutionary-based, and hybrid models that blend machine learning with
biophysics. A central theme is the field's "generalization crisis," where
powerful models were found to fail on new RNA families, prompting a
community-wide shift to stricter, homology-aware benchmarking. In response to
the underlying challenge of data scarcity, RNA foundation models have emerged,
learning from massive, unlabeled sequence corpora to improve generalization.
Finally, we look ahead to the next set of major hurdles-including the accurate
prediction of complex motifs like pseudoknots, scaling to kilobase-length
transcripts, incorporating the chemical diversity of modified nucleotides, and
shifting the prediction target from static structures to the dynamic ensembles
that better capture biological function. We also highlight the need for a
standardized, prospective benchmarking system to ensure unbiased validation and
accelerate progress.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [48] [Redundancy Maximization as a Principle of Associative Memory Learning](https://arxiv.org/abs/2511.02584)
*Mark Blümel,Andreas C. Schneider,Valentin Neuhaus,David A. Ehrlich,Marcel Graetz,Michael Wibral,Abdullah Makkeh,Viola Priesemann*

Main category: cs.IT

TL;DR: The paper uses Partial Information Decomposition to analyze Hopfield networks, finding that redundancy between external and internal inputs characterizes memory function. By optimizing for redundancy as a learning goal, they achieve a 10x improvement in memory capacity to 1.59, outperforming state-of-the-art implementations.


<details>
  <summary>Details</summary>
Motivation: To understand the local computational principles that enable associative memory in Hopfield networks, which remain incompletely understood despite their traditional use for pattern retrieval from noisy cues.

Method: Applied Partial Information Decomposition (PID) framework to individual neurons in classical Hopfield networks, then used redundancy maximization as an information-theoretic learning goal that is directly optimized for each neuron.

Result: Achieved dramatic increase in memory capacity to 1.59 (compared to classical 0.14), representing more than tenfold improvement and outperforming recent state-of-the-art Hopfield network implementations.

Conclusion: Establishes redundancy maximization as a new design principle for associative memories and opens pathways for new associative memory models based on information-theoretic goals.

Abstract: Associative memory, traditionally modeled by Hopfield networks, enables the
retrieval of previously stored patterns from partial or noisy cues. Yet, the
local computational principles which are required to enable this function
remain incompletely understood. To formally characterize the local information
processing in such systems, we employ a recent extension of information theory
- Partial Information Decomposition (PID). PID decomposes the contribution of
different inputs to an output into unique information from each input,
redundant information across inputs, and synergistic information that emerges
from combining different inputs. Applying this framework to individual neurons
in classical Hopfield networks we find that below the memory capacity, the
information in a neuron's activity is characterized by high redundancy between
the external pattern input and the internal recurrent input, while synergy and
unique information are close to zero until the memory capacity is surpassed and
performance drops steeply. Inspired by this observation, we use redundancy as
an information-theoretic learning goal, which is directly optimized for each
neuron, dramatically increasing the network's memory capacity to 1.59, a more
than tenfold improvement over the 0.14 capacity of classical Hopfield networks
and even outperforming recent state-of-the-art implementations of Hopfield
networks. Ultimately, this work establishes redundancy maximization as a new
design principle for associative memories and opens pathways for new
associative memory models based on information-theoretic goals.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [49] [On the existence of solutions of fractional differential equations in Banach spaces](https://arxiv.org/abs/2511.01944)
*Dušan Oberta*

Main category: math.FA

TL;DR: The paper establishes sufficient conditions for the existence of local solutions to fractional differential equations in Banach spaces using measures of non-compactness and Kamke functions.


<details>
  <summary>Details</summary>
Motivation: To address the solvability problem of fractional differential equations in Banach spaces, particularly for systems arising from semi-discretization of fractional PDEs with p-Laplacian.

Method: Utilizes measures of non-compactness and Kamke functions of order α to develop existence criteria for fractional differential equations.

Result: Provides sufficient conditions ensuring the existence of local solutions to fractional differential equations in Banach spaces.

Conclusion: The main existence theorem is successfully applied to countable systems of fractional differential equations derived from semi-discretization of fractional PDEs with p-Laplacian.

Abstract: Utilising the notion of measures of non-compactness and Kamke function of
order $\alpha$, we address the question of solvability of fractional
differential equations in Banach spaces. In particular, we provide sufficient
conditions ensuring the existence of a local solution. Our main existence
theorem is then applied on countable systems of fractional differential
equations arising from semi-discretisation of fractional PDEs with
$p$-Laplacian.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [50] [Simulation of a Non-Newtonian drop impact on a rigid surface: A mess-free approach](https://arxiv.org/abs/2511.02308)
*Tapan Jana,Amit Shaw,L. S. Ramachandra*

Main category: physics.flu-dyn

TL;DR: Mesh-free SPH simulation of non-Newtonian fluid drop impact using Oldroyd-B model to study viscoelastic effects on deformation, spreading, and recoil.


<details>
  <summary>Details</summary>
Motivation: To understand complex viscous-elastic interactions during drop impact and develop computational framework for viscoelastic fluid dynamics problems.

Method: Smoothed Particle Hydrodynamics (SPH) with Oldroyd-B viscoelastic model and suitable fluid-solid boundary conditions.

Result: Validated computational framework reproduces drop deformation, spreading, and recoil behaviors observed in literature.

Conclusion: The study provides insights into viscoelasticity's influence on drop impact behavior and enables future research in fluid dynamics.

Abstract: The present study explores the impact of a non-Newtonian fluid drop on a
rigid surface using a mesh-free approach, Smoothed Particle Hydrodynam-ics. The
complex interaction between viscous and elastic forces during drop impact may
be reproduced by integrating the Oldroyd-B model, which de-scribes viscoelastic
fluids, into the developed computational framework. A suit-able boundary
condition is assumed for interaction between fluid and solid sur-faces. The
ongoing study examines how a droplet deforms, spreads, and recoils after an
impact due to its viscoelasticity. The developed computational frame-work is
validated by comparing results from existing literature, providing an
understanding of how viscoelasticity influences drop-impact behaviour and
opening up new avenues for subsequent research in fluid dynamic problems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [51] [Ultrafast magnetic moment transfer and bandgap renormalization in monolayer \ce{FeCl2}](https://arxiv.org/abs/2511.02461)
*Yu-Hui Song,Huan-Cheng Yang,Kai Liu,Zhong-Yi Lu*

Main category: cond-mat.mtrl-sci

TL;DR: Femtosecond laser pulses induce ultrafast magnetic moment transfer from Fe to Cl atoms in ferromagnetic FeCl2 monolayer, with demagnetization showing non-monotonic dependence on laser photon energy and reaching maximum at resonant excitation.


<details>
  <summary>Details</summary>
Motivation: To understand the microscopic origin of laser-induced ultrafast demagnetization and the role of non-thermal electronic distribution in ferromagnetic materials.

Method: Real-time time-dependent density functional theory (rt-TDDFT) with self-consistent Hubbard U correction, analyzing orbital-resolved electronic structure and dynamical evolution of band structure.

Result: Laser pulses cause ultrafast magnetic moment transfer from Fe to Cl atoms, with Fe demagnetization reaching maximum at resonant excitation. Bandgap reduction up to 41% occurs within tens of femtoseconds under resonant excitation.

Conclusion: The study provides fundamental insights into ultrafast spin control and suggests a strategy for optically engineering magnetism in 2D magnetic materials through specific charge transfer pathways driven by non-thermal excitations.

Abstract: The microscopic origin of laser-induced ultrafast demagnetization remains an
open question, to which the non-thermal electronic distribution plays a vital
role at the initial stage. Herein, we investigate the connection between the
non-thermal electronic distribution and the ultrafast spin dynamics as well as
the electronic structure evolution in ferromagnetic \ce{FeCl2} monolayer using
real-time time-dependent density functional theory (rt-TDDFT) with
self-consistent Hubbard $U$ correction. Our simulations reveal that femtosecond
laser pulses induce ultrafast magnetic moment transfer from Fe to Cl atoms.
More importantly, through a comprehensive analysis of orbital-resolved
electronic structure, we elucidate the microscopic origin of this transfer,
attributing it to specific intra-atomic and inter-atomic charge transfer
pathways driven by non-thermal excitations. The extent of demagnetization of Fe
atoms exhibits a non-monotonic dependence on the laser photon energy, reaching
a maximum at the resonant excitation. In addition, the dynamical evolution of
the band structure was studied based on the eigenstates of the instantaneous
Hamiltonian. Under resonant excitation, the bandgap reduction reaches up to
$41\%$ within tens of fs. These findings provide fundamental insights into
ultrafast spin control and suggest a strategy to optically engineer the
magnetism in two-dimensional magnetic materials.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [52] [Comparative Analysis of Discrete and Continuous Action Spaces in Reservoir Management and Inventory Control Problems](https://arxiv.org/abs/2511.02093)
*Sravani Boddepalli,Prathamesh Kothavale*

Main category: math.OC

TL;DR: Comparative analysis of discrete vs continuous action spaces in reservoir management and inventory control, showing trade-offs between computational costs and performance.


<details>
  <summary>Details</summary>
Motivation: To understand the computational trade-offs between discrete action discretizations and continuous action settings in complex decision problems like reservoir management and inventory control.

Method: Comparative analysis of discrete and continuous action spaces, evaluation of discretization levels in reservoir management, investigation of deterministic/stochastic demand scenarios in inventory control, and introduction of symbolic approach using XADD data structure for hybrid MDPs.

Result: Finer discretizations approach continuous action performance but with increased computational costs; exponential growth in time/space with increasing discrete actions and inventory items; symbolic approach enables solving continuous problems in hybrid MDPs.

Conclusion: Highlights scaling challenges and provides insights for efficient handling of discrete/continuous action spaces; suggests future research on heuristic search methods and improved approximations.

Abstract: This paper presents a comparative analysis of discrete and continuous action
spaces within the contexts of reservoir management and inventory control
problems. We explore the computational trade-offs between discrete action
discretizations and continuous action settings, focusing on their effects on
time complexity and space requirements across different horizons. Our analysis
includes a detailed evaluation of discretization levels in reservoir
management, highlighting that finer discretizations approach the performance of
continuous actions but at increased computational costs. For inventory control,
we investigate deterministic and stochastic demand scenarios, demonstrating the
exponential growth in time and space with increasing discrete actions and
inventory items. We also introduce a novel symbolic approach for solving
continuous problems in hybrid MDPs (H-MDPs), utilizing a new XADD data
structure to manage piecewise symbolic value functions. Our results underscore
the challenges of scaling solutions and provide insights into efficient
handling of discrete and continuous action spaces in complex decision problems.
Future research directions include exploring heuristic search methods and
improved approximations for enhancing the practicality of exact solutions.

</details>


### [53] [An accelerated primal-dual gradient flow for linearly constrained multiobjective optimization](https://arxiv.org/abs/2511.02751)
*Hao Luo,Qiaoyuan Shu,Xinmin Yang*

Main category: math.OC

TL;DR: Proposes a continuous-time primal-dual approach for multiobjective optimization with exponential convergence rates and numerical validation.


<details>
  <summary>Details</summary>
Motivation: Extend accelerated primal-dual methods from single to multiobjective optimization to handle multiple conflicting objectives efficiently.

Method: Develops an accelerated multiobjective primal-dual flow with second-order primal and first-order dual equations, using Lyapunov analysis and implicit-explicit discretization.

Result: Establishes exponential decay in continuous time and sublinear convergence rates for feasibility violation and objective gap in discrete implementation.

Conclusion: The method effectively solves multiobjective problems with theoretical guarantees and practical performance demonstrated through numerical experiments.

Abstract: In this paper, we propose a continuous-time primal-dual approach for linearly
constrained multiobjective optimization problems. A novel dynamical model,
called accelerated multiobjective primal-dual flow, is presented with a
second-order equation for the primal variable and a first-order equation for
the dual variable. It can be viewed as an extension of the accelerated
primal-dual flow by Luo [arXiv:2109.12604, 2021] for the single objective case.
To facilitate the convergence rate analysis, we introduce a new merit function,
which motivates the use of the feasibility violation and the objective gap to
measure the weakly Pareto optimality. By using a proper Lyapunov function, we
establish the exponential decay rate in the continuous level. After that, we
consider an implicit-explicit scheme, which yields an accelerated
multiobjective primal-dual method with a quadratic subproblem, and prove the
sublinear rates of the feasibility violation and the objective gap, under the
convex case and the strongly convex case, respectively. Numerical results are
provided to demonstrate the performance of the proposed method.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [Energy Loss Functions for Physical Systems](https://arxiv.org/abs/2511.02087)
*Sékou-Oumar Kaba,Kusha Sareen,Daniel Levy,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: A framework for incorporating physical knowledge directly into loss functions for ML in scientific domains, using energy-based formulations derived from thermal equilibrium assumptions.


<details>
  <summary>Details</summary>
Motivation: Previous approaches focused on architectural modifications to incorporate physical insights, but this work aims to leverage physical information directly in the loss function for better alignment with system physics.

Method: Derive energy loss functions assuming data samples are in thermal equilibrium with an approximate energy landscape, using reverse KL divergence with Boltzmann distribution around data to obtain energy differences between data and model predictions.

Result: Significant improvements over baselines demonstrated on molecular generation and spin ground-state prediction tasks.

Conclusion: The proposed framework yields physically grounded loss functions that respect physical symmetries, provide better gradient alignment with valid configurations, and are architecture-agnostic while being computationally efficient.

Abstract: Effectively leveraging prior knowledge of a system's physics is crucial for
applications of machine learning to scientific domains. Previous approaches
mostly focused on incorporating physical insights at the architectural level.
In this paper, we propose a framework to leverage physical information directly
into the loss function for prediction and generative modeling tasks on systems
like molecules and spins. We derive energy loss functions assuming that each
data sample is in thermal equilibrium with respect to an approximate energy
landscape. By using the reverse KL divergence with a Boltzmann distribution
around the data, we obtain the loss as an energy difference between the data
and the model predictions. This perspective also recasts traditional objectives
like MSE as energy-based, but with a physically meaningless energy. In
contrast, our formulation yields physically grounded loss functions with
gradients that better align with valid configurations, while being
architecture-agnostic and computationally efficient. The energy loss functions
also inherently respect physical symmetries. We demonstrate our approach on
molecular generation and spin ground-state prediction and report significant
improvements over baselines.

</details>


### [55] [Variational Geometry-aware Neural Network based Method for Solving High-dimensional Diffeomorphic Mapping Problems](https://arxiv.org/abs/2511.01911)
*Zhiwen Li,Cheuk Hin Ho,Lok Ming Lui*

Main category: cs.LG

TL;DR: A mesh-free learning framework for high-dimensional diffeomorphic mapping that combines variational principles with quasi-conformal theory to ensure accurate, bijective mappings while controlling deformation quality.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for high-dimensional diffeomorphic mapping struggle with the curse of dimensionality, limiting their effectiveness in complex registration scenarios.

Method: Proposes a mesh-free learning framework that seamlessly combines variational principles with quasi-conformal theory, regulating conformality distortion and volume distortion to ensure accurate, bijective mappings.

Result: Numerical experiments on synthetic and real-world medical image data validate the accuracy, robustness, and effectiveness of the proposed method in complex registration scenarios.

Conclusion: The framework is inherently compatible with gradient-based optimization and neural network architectures, making it highly flexible and scalable to higher-dimensional settings for complex mapping problems.

Abstract: Traditional methods for high-dimensional diffeomorphic mapping often struggle
with the curse of dimensionality. We propose a mesh-free learning framework
designed for $n$-dimensional mapping problems, seamlessly combining variational
principles with quasi-conformal theory. Our approach ensures accurate,
bijective mappings by regulating conformality distortion and volume distortion,
enabling robust control over deformation quality. The framework is inherently
compatible with gradient-based optimization and neural network architectures,
making it highly flexible and scalable to higher-dimensional settings.
Numerical experiments on both synthetic and real-world medical image data
validate the accuracy, robustness, and effectiveness of the proposed method in
complex registration scenarios.

</details>


### [56] [DeepContour: A Hybrid Deep Learning Framework for Accelerating Generalized Eigenvalue Problem Solving via Efficient Contour Design](https://arxiv.org/abs/2511.01927)
*Yeqiu Chen,Ziyan Liu,Hong Wang*

Main category: cs.LG

TL;DR: DeepContour is a hybrid framework that combines deep learning with contour integral methods to efficiently solve large-scale Generalized Eigenvalue Problems by automatically optimizing integration contours.


<details>
  <summary>Details</summary>
Motivation: Contour integral methods for solving Generalized Eigenvalue Problems are efficient but critically depend on proper contour selection, which is challenging without prior knowledge of eigenvalue distribution.

Method: Uses Fourier Neural Operator to predict spectral distribution, applies Kernel Density Estimation to automatically determine optimal integration contours, then guides contour integral solvers with these optimized contours.

Result: Achieves up to 5.63× speedup in solving Generalized Eigenvalue Problems across multiple datasets compared to traditional methods.

Conclusion: DeepContour pioneers an efficient and robust paradigm combining deep learning prediction with classical numerical solvers for high-dimensional eigenvalue problems.

Abstract: Solving large-scale Generalized Eigenvalue Problems (GEPs) is a fundamental
yet computationally prohibitive task in science and engineering. As a promising
direction, contour integral (CI) methods, such as the CIRR algorithm, offer an
efficient and parallelizable framework. However, their performance is
critically dependent on the selection of integration contours -- improper
selection without reliable prior knowledge of eigenvalue distribution can incur
significant computational overhead and compromise numerical accuracy. To
address this challenge, we propose DeepContour, a novel hybrid framework that
integrates a deep learning-based spectral predictor with Kernel Density
Estimation for principled contour design. Specifically, DeepContour first
employs a Fourier Neural Operator (FNO) to rapidly predict the spectral
distribution of a given GEP. Subsequently, Kernel Density Estimation (KDE) is
applied to the predicted spectrum to automatically and systematically determine
proper integration contours. Finally, these optimized contours guide the CI
solver to efficiently find the desired eigenvalues. We demonstrate the
effectiveness of our method on diverse challenging scientific problems. In our
main experiments, DeepContour accelerates GEP solving across multiple datasets,
achieving up to a 5.63$\times$ speedup. By combining the predictive power of
deep learning with the numerical rigor of classical solvers, this work pioneers
an efficient and robust paradigm for tackling difficult generalized eigenvalue
involving matrices of high dimension.

</details>


### [57] [Geometric Data Valuation via Leverage Scores](https://arxiv.org/abs/2511.02100)
*Rodrigo Mendoza-Smith*

Main category: cs.LG

TL;DR: The paper proposes geometric leverage scores as an efficient alternative to computationally expensive Shapley data valuation, showing they satisfy key axioms and provide theoretical guarantees for model quality.


<details>
  <summary>Details</summary>
Motivation: Shapley data valuation is computationally infeasible at scale due to its combinatorial nature, requiring evaluation of all data subsets.

Method: Propose geometric leverage scores that quantify each datapoint's structural influence in representation space by measuring how much it extends dataset span and contributes to effective dimensionality.

Result: Leverage scores satisfy dummy, efficiency, and symmetry axioms of Shapley valuation. Ridge leverage scores provide strictly positive marginal gains and connect to classical optimal design criteria. Training on leverage-sampled subsets produces models within O(ε) of full-data optimum.

Conclusion: Leverage scores provide computationally efficient data valuation with theoretical guarantees, and empirical results show ridge-leverage sampling outperforms standard baselines in active learning without requiring gradients.

Abstract: Shapley data valuation provides a principled, axiomatic framework for
assigning importance to individual datapoints, and has gained traction in
dataset curation, pruning, and pricing. However, it is a combinatorial measure
that requires evaluating marginal utility across all subsets of the data,
making it computationally infeasible at scale. We propose a geometric
alternative based on statistical leverage scores, which quantify each
datapoint's structural influence in the representation space by measuring how
much it extends the span of the dataset and contributes to the effective
dimensionality of the training problem. We show that our scores satisfy the
dummy, efficiency, and symmetry axioms of Shapley valuation and that extending
them to \emph{ridge leverage scores} yields strictly positive marginal gains
that connect naturally to classical A- and D-optimal design criteria. We
further show that training on a leverage-sampled subset produces a model whose
parameters and predictive risk are within $O(\varepsilon)$ of the full-data
optimum, thereby providing a rigorous link between data valuation and
downstream decision quality. Finally, we conduct an active learning experiment
in which we empirically demonstrate that ridge-leverage sampling outperforms
standard baselines without requiring access gradients or backward passes.

</details>


### [58] [In Situ Training of Implicit Neural Compressors for Scientific Simulations via Sketch-Based Regularization](https://arxiv.org/abs/2511.02659)
*Cooper Simpson,Stephen Becker,Alireza Doostan*

Main category: cs.LG

TL;DR: A novel in situ training protocol using implicit neural representations with memory buffers of full and sketched data samples to prevent catastrophic forgetting, applied to neural compression with strong performance at high compression rates.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in continual learning scenarios, particularly for in situ neural compression using implicit neural representation-based hypernetworks, by leveraging sketching as a regularizer.

Method: In situ training protocol with limited memory buffers containing both full and sketched data samples, using sketching as a regularizer motivated by Johnson-Lindenstrauss theory, applied to implicit neural representations for compression tasks.

Result: Strong reconstruction performance at high compression rates on complex simulation data in 2D/3D over long time horizons, across unstructured grids and non-Cartesian geometries. Sketching enables in situ performance to approximately match equivalent offline methods.

Conclusion: Sketching effectively prevents catastrophic forgetting in continual learning scenarios, enabling in situ training to achieve performance comparable to offline methods for neural compression tasks using implicit neural representations.

Abstract: Focusing on implicit neural representations, we present a novel in situ
training protocol that employs limited memory buffers of full and sketched data
samples, where the sketched data are leveraged to prevent catastrophic
forgetting. The theoretical motivation for our use of sketching as a
regularizer is presented via a simple Johnson-Lindenstrauss-informed result.
While our methods may be of wider interest in the field of continual learning,
we specifically target in situ neural compression using implicit neural
representation-based hypernetworks. We evaluate our method on a variety of
complex simulation data in two and three dimensions, over long time horizons,
and across unstructured grids and non-Cartesian geometries. On these tasks, we
show strong reconstruction performance at high compression rates. Most
importantly, we demonstrate that sketching enables the presented in situ scheme
to approximately match the performance of the equivalent offline method.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [59] [Spectral projection estimates restricted to uniformly embedded submanifolds](https://arxiv.org/abs/2511.02012)
*Zhexing Zhang*

Main category: math.DG

TL;DR: The paper generalizes spectral projection estimates to noncompact manifolds with nonpositive curvature and bounded geometry, and proves sharp estimates for spectral windows on nontrapped geodesics in asymptotically hyperbolic surfaces.


<details>
  <summary>Details</summary>
Motivation: To extend previous spectral projection results by X. Chen from compact to noncompact settings, and to establish sharp estimates for spectral windows in asymptotically hyperbolic geometries.

Method: Uses geometric analysis techniques on manifolds with nonpositive sectional curvature and bounded geometry, analyzing spectral projection operators and their L^2 to L^q norms on uniformly embedded submanifolds.

Result: Obtained estimates for L^2(M)→L^q(Σ) norms of log-scale spectral projection operators, and proved sharp spectral projection estimates for spectral windows of any small size on nontrapped geodesics in asymptotically hyperbolic surfaces.

Conclusion: Successfully generalized previous results to noncompact cases and established sharp spectral projection bounds in asymptotically hyperbolic geometries with curvature constraints.

Abstract: Let $M$ be a manifold with nonpositive sectional curvature and bounded
geometry, and let $\Sigma$ be a uniformly embedded submanifold of $M.$ We
estimate the $L^2(M)\to L^q(\Sigma)$ norm of a $\log$-scale spectral projection
operator. It is a generalization of result of X. Chen to noncompact cases. We
also prove sharp spectral projection estimates of spectral windows of any small
size restricted to nontrapped geodesics on even asymptotically hyperbolic
surfaces with bounded geometry and curvature pinched below 0.

</details>
