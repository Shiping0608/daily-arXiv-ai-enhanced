<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 10]
- [math.AP](#math.AP) [Total: 7]
- [physics.comp-ph](#physics.comp-ph) [Total: 6]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 7]
- [stat.ML](#stat.ML) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Relative entropy estimate and geometric ergodicity for implicit Langevin Monte Carlo](https://arxiv.org/abs/2511.04041)
*Lei Li,Jian-Guo Liu,Yuliang Wang*

Main category: math.NA

TL;DR: The paper analyzes the implicit Langevin Monte Carlo (iLMC) method, proving its convergence and geometric ergodicity for non-Lipschitz drift fields using PDE techniques and coupling methods.


<details>
  <summary>Details</summary>
Motivation: Explicit schemes like LMC may fail when the drift field is not globally Lipschitz, while iLMC can handle one-sided Lipschitz drifts, making it more robust for practical applications.

Method: Uses continuous-time interpolation, PDE techniques (Bernstein method) for gradient estimates, and reflection-type continuous-discrete coupling to prove geometric ergodicity under Wasserstein-1 distance.

Result: Proves time-discretization error bound under relative entropy, geometric ergodicity of iLMC, and extends to uniform-in-time error bounds by combining entropy error and ergodicity.

Conclusion: The proof technique is universal and applicable to other implicit or splitting schemes for simulating SDEs with non-Lipschitz drifts, establishing iLMC's theoretical guarantees.

Abstract: We study the implicit Langevin Monte Carlo (iLMC) method, which simulates the
overdamped Langevin equation via an implicit iteration rule. In many
applications, iLMC is favored over other explicit schemes such as the
(explicit) Langevin Monte Carlo (LMC). LMC may blow up when the drift field
$\nabla U$ is not globally Lipschitz, while iLMC has convergence guarantee when
the drift is only one-sided Lipschitz. Starting from an adapted continuous-time
interpolation, we prove a time-discretization error bound under the relative
entropy (or the Kullback-Leibler divergence), where a crucial gradient estimate
for the logarithm numerical density is obtained via a sequence of PDE
techniques, including Bernstein method. Based on a reflection-type
continuous-discrete coupling method, we prove the geometric ergodicity of iLMC
under the Wasserstein-1 distance. Moreover, we extend the error bound to a
uniform-in-time one by combining the relative entropy error bound and the
ergodicity. Our proof technique is universal and can be applied to other
implicit or splitting schemes for simulating stochastic differential equations
with non-Lipschitz drifts.

</details>


### [2] [Numerical boundary flux functions that give provable bounds for nonlinear initial boundary value problems with open boundaries](https://arxiv.org/abs/2511.04197)
*Andrew R. Winters,David A. Kopriva,Jan Nordström*

Main category: math.NA

TL;DR: A strategy for interpreting nonlinear characteristic-type penalty terms as numerical boundary flux functions that provide provable bounds for solutions to nonlinear hyperbolic initial boundary value problems with open boundaries.


<details>
  <summary>Details</summary>
Motivation: To develop provable bounds for solutions to nonlinear hyperbolic initial boundary value problems with open boundaries, addressing limitations of standard boundary treatments based on linear analysis.

Method: Uses a matrix formulation to express entropy flux as a quadratic form, enabling systematic design of characteristic-based penalty terms. Requires special decomposition of boundary matrix to define characteristic-type variables. Compatible with high-order accurate split form discontinuous Galerkin spectral element methods.

Result: New boundary fluxes guarantee entropy stability and solution bounded solely by external data. Derived inflow-outflow boundary fluxes for Burgers equation and 2D shallow water equations that are also energy stable.

Conclusion: The nonlinear boundary fluxes successfully handle situations where standard linear analysis-based boundary treatments fail, providing provable stability and boundedness for nonlinear hyperbolic problems.

Abstract: We present a strategy for interpreting nonlinear, characteristic-type penalty
terms as numerical boundary flux functions that provide provable bounds for
solutions to nonlinear hyperbolic initial boundary value problems with open
boundaries. This approach is enabled by recent work that found how to express
the entropy flux as a quadratic form defined by a symmetric boundary matrix.
The matrix formulation provides additional information for how to
systematically design characteristic-based penalty terms for the weak
enforcement of boundary conditions. A special decomposition of the boundary
matrix is required to define an appropriate set of characteristic-type
variables. The new boundary fluxes are directly compatible with high-order
accurate split form discontinuous Galerkin spectral element and similar methods
and guarantee that the solution is entropy stable and bounded solely by
external data. We derive inflow-outflow boundary fluxes specifically for the
Burgers equation and the two-dimensional shallow water equations, which are
also energy stable. Numerical experiments demonstrate that the new nonlinear
fluxes do not fail in situations where standard boundary treatments based on
linear analysis do.

</details>


### [3] [A space-time adaptive boundary element method for the wave equation](https://arxiv.org/abs/2511.04265)
*Alessandra Aimi,Giulia Di Credico,Heiko Gimperlein,Chiara Guardasoni*

Main category: math.NA

TL;DR: This paper introduces space-time adaptive mesh refinement for boundary element methods applied to wave equations, focusing on acoustic soft-scattering problems with local tensor-product refinements.


<details>
  <summary>Details</summary>
Motivation: To develop efficient adaptive boundary element procedures for time-dependent wave equations that can handle various types of singularities (spatial, temporal, traveling) through space-time mesh refinements.

Method: Formulated an adaptive boundary element procedure based on residual-type error indicators with local tensor-product refinements of the space-time mesh for acoustic soft-scattering problems.

Result: The method demonstrates improved convergence rates in energy norm for problems with different singularity types, and both rigorous and heuristic a posteriori error indicators show efficiency.

Conclusion: Space-time adaptive mesh refinements effectively handle various singularity types in wave equation formulations, with the proposed error indicators proving efficient for boundary element methods.

Abstract: This article initiates the study of space-time adaptive mesh refinements for
time-dependent boundary element formulations of wave equations. Based on error
indicators of residual type, we formulate an adaptive boundary element
procedure for acoustic soft-scattering problems with local tensor-product
refinements of the space-time mesh. We discuss the algorithmic challenges and
investigate the proposed method in numerical experiments. In particular, we
study the performance and improved convergence rates with respect to the energy
norm for problems dominated by spatial, temporal or traveling singularities of
the solution. The efficiency of the considered rigorous and heuristic a
posteriori error indicators is discussed.

</details>


### [4] [DeepPAAC: A New Deep Galerkin Method for Principal-Agent Problems](https://arxiv.org/abs/2511.04309)
*Michael Ludkovski,Changgen Xie,Zimu Zhu*

Main category: math.NA

TL;DR: Developed DeepPAAC, a deep learning method for solving principal-agent problems in continuous time with multi-dimensional states and controls.


<details>
  <summary>Details</summary>
Motivation: To address the numerical resolution of principal-agent problems with continuous and lump payments and multi-dimensional agent strategies.

Method: Deep Principal-Agent Actor Critic (DeepPAAC) algorithm using deep learning to solve Hamilton-Jacobi-Bellman equations with implicit Hamiltonians.

Result: Successfully handles multi-dimensional states, controls, and constraints; investigated neural network architecture, training designs, and loss functions across five case studies.

Conclusion: DeepPAAC provides an effective deep learning approach for solving complex principal-agent problems in continuous time settings.

Abstract: We consider numerical resolution of principal-agent (PA) problems in
continuous time. We formulate a generic PA model with continuous and lump
payments and a multi-dimensional strategy of the agent. To tackle the resulting
Hamilton-Jacobi-Bellman equation with an implicit Hamiltonian we develop a
novel deep learning method: the Deep Principal-Agent Actor Critic (DeepPAAC)
Actor-Critic algorithm. DeepPAAC is able to handle multi-dimensional states and
controls, as well as constraints. We investigate the role of the neural network
architecture, training designs, loss functions, etc. on the convergence of the
solver, presenting five different case studies.

</details>


### [5] [Normalized tensor train decomposition](https://arxiv.org/abs/2511.04369)
*Renfeng Peng,Chengkai Zhu,Bin Gao,Xin Wang,Ya-xiang Yuan*

Main category: math.NA

TL;DR: The paper introduces normalized tensor train (NTT) decomposition to approximate unit-norm tensors while preserving the tensor train format's low-rank structure, enabling efficient geometric methods for various applications.


<details>
  <summary>Details</summary>
Motivation: Tensors with unit Frobenius norm are fundamental in scientific computing and quantum physics, but existing tensor train decomposition doesn't enforce unit-norm constraints, limiting its applicability to normalized problems.

Method: Developed normalized tensor train (NTT) decomposition that represents unit-norm tensors in tensor train format, proved it forms a smooth manifold, derived Riemannian geometry, and proposed geometric optimization methods.

Result: The NTT decomposition successfully preserves unit-norm structure while maintaining low-rank efficiency, enabling applications in tensor recovery, eigenvalue problems, quantum state analysis, and quantum channel entropy calculations.

Conclusion: NTT-based methods demonstrate superior efficiency and scalability compared to traditional approaches, providing a principled geometric framework for handling unit-norm tensor problems across scientific computing and quantum physics domains.

Abstract: Tensors with unit Frobenius norm are fundamental objects in many fields,
including scientific computing and quantum physics, which are able to represent
normalized eigenvectors and pure quantum states. While the tensor train
decomposition provides a powerful low-rank format for tackling high-dimensional
problems, it does not intrinsically enforce the unit-norm constraint. To
address this, we introduce the normalized tensor train (NTT) decomposition,
which aims to approximate a tensor by unit-norm tensors in tensor train format.
The low-rank structure of NTT decomposition not only saves storage and
computational cost but also preserves the underlying unit-norm structure. We
prove that the set of fixed-rank NTT tensors forms a smooth manifold, and the
corresponding Riemannian geometry is derived, paving the way for geometric
methods. We propose NTT-based methods for low-rank tensor recovery,
high-dimensional eigenvalue problem, estimation of stabilizer rank, and
calculation of the minimum output R\'enyi 2-entropy of quantum channels.
Numerical experiments demonstrate the superior efficiency and scalability of
the proposed NTT-based methods.

</details>


### [6] [The Loewner framework applied to Zolotarev sign and ratio problems](https://arxiv.org/abs/2511.04404)
*Athanasios C. Antoulas,Ion Victor Gosea,Charles Poussot-Vassal*

Main category: math.NA

TL;DR: Comparison of approximation methods for Zolotarev problems shows Loewner framework outperforms AAA variants in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare different numerical methods for approximating functions in the 3rd and 4th Zolotarev problems, particularly focusing on computational efficiency and accuracy.

Method: Numerical study comparing Loewner framework, standard AAA algorithm, and AAA extensions (sign and Lawson variants) for function approximation.

Result: Loewner framework is fast, reliable, and provides highly accurate approximants, often outperforming AAA-Lawson for higher-degree approximations with constant running time.

Conclusion: The Loewner framework is superior to AAA variants for Zolotarev problems, offering better accuracy, faster computation, and constant running time regardless of approximant degree.

Abstract: In this work, we propose a numerical study concerning the approximation of
functions associated with the 3rd and 4th Zolotarev problems. We compare
various methods, in particular the Loewner framework, the standard AAA
algorithm, and recently-proposed extensions of AAA (namely, the sign and Lawson
variants). We show that the Loewner framework is fast and reliable, and
provides approximants with a high level of accuracy. When the approximants are
of a higher degree, Loewner approximants are often more accurate than
near-optimal ones computed with AAA-Lawson. Last but not least, the Loewner
framework is a direct method for which the running time is typically lower than
that of the iterative AAA-Lawson variants. Moreover, for the latter, the
running time increases substantially with the degree of the approximant,
whereas for the Loewner method, it remains constant. These claims are supported
by an extensive numerical treatment.

</details>


### [7] [Mean square error analysis of stochastic gradient and variance-reduced sampling algorithms](https://arxiv.org/abs/2511.04413)
*Jianfeng Lu,Xuda Ye,Zhennan Zhou*

Main category: math.NA

TL;DR: MSE analysis of stochastic gradient sampling algorithms for underdamped Langevin dynamics, showing first-order convergence for SG-UBU and phase transition to second-order convergence for variance-reduced methods.


<details>
  <summary>Details</summary>
Motivation: To analyze mean square error for stochastic gradient sampling algorithms in underdamped Langevin dynamics under convexity assumptions, providing theoretical guarantees and practical selection criteria.

Method: Developed a novel discrete Poisson equation framework to bound time-averaged sampling error, analyzed SG-UBU sampler and extended to variance-reduced algorithms (SVRG-UBU and SAGA-UBU).

Result: SG-UBU exhibits first-order convergence with step size, variance-reduced methods show phase transition from first to second-order convergence below critical step size threshold, validated by numerical experiments.

Conclusion: The analysis provides theoretical guarantees for stochastic gradient samplers and practical criteria for selecting optimal algorithms (mini-batch SG-UBU vs SVRG-UBU) based on computational efficiency.

Abstract: This paper considers mean square error (MSE) analysis for stochastic gradient
sampling algorithms applied to underdamped Langevin dynamics under a global
convexity assumption. A novel discrete Poisson equation framework is developed
to bound the time-averaged sampling error. For the Stochastic Gradient UBU
(SG-UBU) sampler, we derive an explicit MSE bound and establish that the
numerical bias exhibits first-order convergence with respect to the step size
$h$, with the leading error coefficient proportional to the variance of the
stochastic gradient. The analysis is further extended to variance-reduced
algorithms for finite-sum potentials, specifically the SVRG-UBU and SAGA-UBU
methods. For these algorithms, we identify a phase transition phenomenon
whereby the convergence rate of the numerical bias shifts from first to second
order as the step size decreases below a critical threshold. Theoretical
findings are validated by numerical experiments. In addition, the analysis
provides a practical empirical criterion for selecting between the mini-batch
SG-UBU and SVRG-UBU samplers to achieve optimal computational efficiency.

</details>


### [8] [An efficient boundary integral equation solution technique for solving aperiodic scattering problems from two-dimensional, periodic boundaries](https://arxiv.org/abs/2511.04424)
*Riley Fisher,Fruzsina Agocs,Adrianna Gillman*

Main category: math.NA

TL;DR: Efficient boundary integral method for 2D Helmholtz problems in half-plane with periodic boundary and aperiodic source, using Floquet-Bloch transform and periodizing scheme to avoid quasiperiodic Green's function evaluation.


<details>
  <summary>Details</summary>
Motivation: To solve Helmholtz problems with periodic boundaries more efficiently, avoiding the computational burden of evaluating quasiperiodic Green's functions.

Method: Uses Floquet-Bloch transform to convert problem into contour integral, employs periodizing scheme by Cho and Barnett variant, accelerated with low rank linear algebra and reusable precomputation.

Result: Method achieves 20-30 times faster computation compared to techniques using quasiperiodic Green's function for stair-like geometry.

Conclusion: The presented boundary integral technique is highly efficient for solving 2D Helmholtz problems with periodic boundaries and aperiodic sources.

Abstract: This manuscript presents an efficient boundary integral equation technique
for solving two-dimensional Helmholtz problems defined in the half-plane
bounded by an infinite, periodic curve with Neumann boundary conditions and an
aperiodic point source. The technique is designed for boundaries where one
period does not require a large number of discretization points to achieve high
accuracy. The Floquet--Bloch transform turns the problem into evaluating a
contour integral where the integrand is the solution of quasiperiodic boundary
value problems. To approximate the integral, one must solve a collection of
these problems. This manuscript uses a variant of the periodizing scheme by Cho
and Barnett which alleviates the need for evaluating the quasiperiodic Green's
function and is amenable to a large amount of precomputation that can be reused
for all of the necessary solves. The solution technique is accelerated by the
use of low rank linear algebra. The numerical results illustrate that the
presented method is 20-30 faster than the technique utilizing the quasiperiodic
Green's function for a stair-like geometry.

</details>


### [9] [A Two-stage Adaptive Lifting PINN Framework for Solving Viscous Approximations to Hyperbolic Conservation Laws](https://arxiv.org/abs/2511.04490)
*Yameng Zhu,Weibing Deng,Ran Bi*

Main category: math.NA

TL;DR: Proposes a two-stage adaptive lifting PINN framework to handle training difficulties in physics informed neural networks for hyperbolic conservation laws near inviscid limits, using learned auxiliary fields to improve conditioning and convergence.


<details>
  <summary>Details</summary>
Motivation: Training PINNs for hyperbolic conservation laws near inviscid limits is challenging due to ill-posed residuals at shock discontinuities and spectral bias from small viscosity regularization.

Method: Two-stage adaptive lifting PINN framework that augments physical coordinates with learned auxiliary fields through r-adaptive coordinate transformations, supported by theoretical error analysis and NTK/gradient flow analysis.

Result: Numerical experiments show accelerated and more stable convergence, with accurate reconstructions near discontinuities, demonstrating variance reduction similar to importance sampling.

Conclusion: The proposed lifting framework effectively mitigates training difficulties in PINNs for hyperbolic conservation laws without requiring a priori knowledge of interface geometry, improving conditioning and accelerating residual decay.

Abstract: Training physics informed neural networks PINNs for hyperbolic conservation
laws near the inviscid limit presents considerable difficulties because strong
form residuals become ill posed at shock discontinuities, while small viscosity
regularization introduces narrow boundary layers that exacerbate spectral bias.
To address these issues this paper proposes a novel two stage adaptive lifting
PINN, a lifting based framework designed to mitigate such challenges without
requiring a priori knowledge of the interface geometry. The key idea is to
augment the physical coordinates by introducing a learned auxiliary field
generated through r adaptive coordinate transformations. Theoretically we first
derive an a posteriori L2 error estimate to quantify how training difficulty
depends on viscosity. Secondly we provide a statistical interpretation
revealing that embedded sampling induces variance reduction analogous to
importance sampling. Finally we perform an NTK and gradient flow analysis,
demonstrating that input augmentation improves conditioning and accelerates
residual decay. Supported by these insights our numerical experiments show
accelerated and more stable convergence as well as accurate reconstructions
near discontinuities.

</details>


### [10] [Spurious resonances for substructured FEM-BEM coupling](https://arxiv.org/abs/2511.04501)
*Antonin Boisneault,Marcella Bonazzoli,Pierre Marchand,Xavier Claeys*

Main category: math.NA

TL;DR: The paper analyzes spurious resonances in Generalized Optimized Schwarz Method (GOSM) for FEM-BEM coupling in acoustic scattering problems, showing it shares the same ill-posedness issues as classical couplings.


<details>
  <summary>Details</summary>
Motivation: To address the well-known problem of spurious resonances in classical FEM-BEM couplings for acoustic scattering, even when the original boundary value problem is well-posed.

Method: Analysis of GOSM formulation derived from Johnson-Nédélec and Costabel couplings, with explicit characterization of the kernel of the local interface operator.

Result: GOSM is not immune to spurious resonances - the kernel of its local interface operator is non-trivial and coincides with that of classical FEM-BEM couplings.

Conclusion: The new substructured FEM-BEM formulation (GOSM) inherits the same ill-posedness issues as classical couplings for certain wavenumbers, requiring further investigation to overcome spurious resonances.

Abstract: We are interested in time-harmonic acoustic scattering by an impenetrable
obstacle in a medium where the wavenumber is constant in an exterior unbounded
subdomain and is possibly heterogeneous in a bounded subdomain. The associated
Helmholtz boundary value problem can be solved by coupling the Finite Element
Method (FEM) in the heterogeneous subdomain with the Boundary Element Method
(BEM) in the homogeneous subdomain. Recently, we designed and analyzed a new
substructured FEM-BEM formulation, called Generalized Optimized Schwarz Method
(GOSM). Unfortunately, it is well known that, even when the initial boundary
value problem is well-posed, the variational formulation of classical FEM-BEM
couplings can be ill-posed for certain wavenumbers, called spurious resonances.
In this paper, we focus on the Johnson-N\'ed\'elec and Costabel couplings and
show that the GOSM derived from both is not immune to that issue. In
particular, we give an explicit expression of the kernel of the local operator
associated with the interface between the FEM and BEM subdomains. That kernel
and the one of classical FEM-BEM couplings are simultaneously non-trivial.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [11] [On blow-ups of sets with finite fractional variation](https://arxiv.org/abs/2511.03854)
*Giorgio Stefani*

Main category: math.AP

TL;DR: For sets with finite fractional α-variation, almost every point with a non-trivial tangent set having finite integer perimeter also has a tangent half-space oriented by the fractional normal.


<details>
  <summary>Details</summary>
Motivation: To establish connections between geometric properties of sets with fractional variation and their tangent structures, particularly relating non-trivial tangent sets to half-space approximations.

Method: Analysis of sets with locally finite fractional α-variation, examining tangent sets and their properties at points where the α-variation measure is concentrated.

Result: Proves that almost every point (with respect to α-variation measure) with a non-trivial tangent set having locally finite integer perimeter admits a tangent half-space oriented by the fractional unit normal.

Conclusion: There is a strong geometric relationship between the existence of non-trivial tangent sets and half-space approximations for sets with fractional variation, with the fractional normal playing a key orientational role.

Abstract: Given $\alpha \in (0,1)$ and a set $E \subset \mathbb R^N$ with locally
finite fractional $\alpha$-variation, we show that for almost every $x \in
\mathbb R^N$ with respect to the $\alpha$-variation measure of $\mathbf 1_E$,
if $E$ admits a non-trivial tangent set at $x$ with locally finite integer
perimeter, then $E$ also admits a tangent half-space oriented by the fractional
unit normal of $E$ at $x$.

</details>


### [12] [Bifurcation analysis of Stokes waves with piecewise smooth vorticity in deep water](https://arxiv.org/abs/2511.03973)
*Changfeng Gui,Jun Wang,Wen Yang,Yong Zhang*

Main category: math.AP

TL;DR: Existence of Stokes waves with piecewise smooth vorticity in 2D deep water, using hodograph transformation and singular bifurcation approach.


<details>
  <summary>Details</summary>
Motivation: To study traveling water waves over sheared currents with discontinuous vorticity in unbounded domains, addressing novel challenges like internal interfaces and non-Fredholm operators.

Method: Hodograph transformation to reformulate as elliptic boundary value problem, height function formulation as transmission problem, and singular bifurcation combining global bifurcation theory with Whyburns lemma.

Result: Established existence of such waves, showing along bifurcation branch waves either achieve arbitrarily large wave speed or approach horizontal stagnation.

Conclusion: Successfully proved existence of Stokes waves with piecewise smooth vorticity in deep water, overcoming challenges of internal interfaces and unbounded domains.

Abstract: In this paper, we establish the existence of Stokes waves with piecewise
smooth vorticity in a two-dimensional, infinitely deep fluid domain. These
waves represent traveling water waves propagating over sheared currents in a
semi-infinite cylinder, where the vorticity may exhibit discontinuities. The
analysis is carried out by applying a hodograph transformation, which
reformulates the original free boundary problem into an abstract elliptic
boundary value problem. Compared to previously studied steady water waves, the
present setting introduces several novel features: the presence of an internal
interface, an unbounded spatial domain, and a non-Fredholm linearized operator.
To address these difficulties, we introduce a height function formulation,
casting the problem as a transmission problem with suitable transmission
conditions. A singular bifurcation approach is then employed, combining global
bifurcation theory with Whyburns topological lemma. Along the global
bifurcation branch, we show that the resulting wave profiles either attain
arbitrarily large wave speed or approach horizontal stagnation.

</details>


### [13] [Isocapacitary constants associated with $p$-Laplacian on graphs](https://arxiv.org/abs/2511.04039)
*Bobo Hua,Lili Wang*

Main category: math.AP

TL;DR: Introduction of isocapacitary constants for p-Laplacian on graphs to estimate first eigenvalues of Dirichlet, Neumann, and p-Steklov problems.


<details>
  <summary>Details</summary>
Motivation: To develop analytical tools for estimating eigenvalues of various p-Laplacian operators on graphs, extending classical spectral theory to discrete settings.

Method: Define isocapacitary constants specifically for p-Laplacian operators on graphs and apply them to derive eigenvalue bounds.

Result: Obtained estimates for the first eigenvalues of Dirichlet p-Laplacian, Neumann p-Laplacian, and p-Steklov problem using the introduced isocapacitary constants.

Conclusion: Isocapacitary constants provide effective tools for eigenvalue estimation in graph-based p-Laplacian problems, bridging continuous and discrete spectral analysis.

Abstract: In this paper, we introduce isocapacitary constants for the $p$-Laplacian on
graphs and apply them to derive estimates for the first eigenvalues of the
Dirichlet $p$-Laplacian, the Neumann $p$-Laplacian, and the $p$-Steklov
problem.

</details>


### [14] [The Kato problem and extensions for degenerate elliptic operators of higher order in weighted spaces](https://arxiv.org/abs/2511.04046)
*Guoming Zhang*

Main category: math.AP

TL;DR: The paper extends the Kato problem to degenerate elliptic operators of arbitrary order with measurable complex coefficients satisfying a Gårding inequality with respect to A₂-weights, and applies this to solve L^p boundary value problems near p=2.


<details>
  <summary>Details</summary>
Motivation: To generalize previous work by Cruz-Uribe, Martell and Rios (2018) on the Kato problem to higher-order degenerate elliptic operators with complex coefficients and A₂-weights.

Method: Extends the Kato problem framework to degenerate elliptic operators of order 2m (m≥1) with measurable complex coefficients satisfying Gårding inequality with Muckenhoupt A₂-weights.

Result: Successfully solves the unweighted L^p-Dirichlet, regularity and Neumann boundary value problems for such operators when p is sufficiently close to 2.

Conclusion: The extension of the Kato problem to higher-order degenerate elliptic operators with complex coefficients and A₂-weights enables solving boundary value problems in L^p spaces near p=2.

Abstract: We consider the Kato problem and extensions for degenerate elliptic operators
of arbitrary order $2m$ ($m\geq 1$), whose coefficients are measurable,
complex-valued and satisfy the G$\mathring{a}$rding inequality with respect to
a Muckenhoupt $A_{2}$-weight; this generalizes the work of [Cruz-Uribe, Martell
and Rios 2018]. As an application, the unweighted $L^{p}$-Dirichlet, regularity
and Neumann boundary value problems associated to such an operator are solved
when $p$ is sufficiently close to $2.$

</details>


### [15] [A variational Lippmann-Schwinger-type approach for the Helmholtz impedance problem on bounded domains](https://arxiv.org/abs/2511.04056)
*Andreas Tataris,Alexander V. Mamonov*

Main category: math.AP

TL;DR: This paper develops a Lippmann-Schwinger type equation for Helmholtz boundary value problems with variable refractive index and impedance boundary conditions, establishing analytical properties and proving weak-to-strong continuity for solving inverse boundary value problems.


<details>
  <summary>Details</summary>
Motivation: To address the loss of structural properties in boundary value formulations compared to scattering problems in unbounded domains, particularly the unavailability of the classical Lippmann-Schwinger integral equation.

Method: Derived a variational Lippmann-Schwinger type equation from the boundary value problem's variational formulation, established analytical properties of the operator, and proved weak-to-strong sequential continuity of the parameter-to-state map.

Result: Showed that the parameter-to-state map maps weakly convergent sequences to strongly convergent ones for refractive indices in Lebesgue spaces with exponent greater than 2, enabling existence proofs for minimizers in optimization methods.

Conclusion: The developed Lippmann-Schwinger type framework provides a foundation for proving existence of minimizers in reduced order model based optimization and conventional waveform inversion methods for solving inverse boundary value problems.

Abstract: Recently, reduced order modeling methods have been applied to solving inverse
boundary value problems arising in frequency domain scattering theory. A key
step in projection-based reduced order model methods is the use of a
sesquilinear form associated with the forward boundary value problem. However,
in contrast to scattering problems posed in $\mathbb{R}^d$, boundary value
formulations lose certain structural properties, most notably the classical
Lippmann-Schwinger integral equation is no longer available. In this paper we
derive a Lippmann-Schwinger type equation aimed at studying the solution of a
Helmholtz boundary value problem with a variable refractive index and impedance
boundary conditions. In particular, we start from the variational formulation
of the boundary value problem and we obtain an equivalent operator equation
which can be viewed as a bounded domain analogue of the classical
Lippmann-Schwinger equation. We first establish analytical properties of our
variational Lippmann-Schwinger type operator. Based on these results, we then
show that the parameter-to-state map, which maps a refractive index to the
corresponding wavefield, maps weakly convergent sequences to strongly
convergent ones when restricted to refractive indices in Lebesgue spaces with
exponent greater than 2. Finally, we use the derived weak to strong sequential
continuity to show existence of minimizers for a reduced order model based
optimization methods aimed at solving the inverse boundary value problem as
well as for a conventional data misfit based waveform inversion method.

</details>


### [16] [Fujita exponent for heat equation with Hörmander vector fields](https://arxiv.org/abs/2511.04196)
*Marianna Chatzakou,Aidyn Kassymov,Michael Ruzhansky*

Main category: math.AP

TL;DR: Global existence and non-existence results for heat equations with vector fields satisfying Hörmander's condition, including critical Fujita exponent calculation for power nonlinearities.


<details>
  <summary>Details</summary>
Motivation: To study the behavior of solutions to heat equations with nonlinear terms involving squares of vector fields, particularly determining conditions for global existence versus blow-up.

Method: Analysis of heat equations with nonlinearities f(u) and φ(t)f(u), using vector fields satisfying Hörmander's rank condition, with focus on power nonlinearities u^p.

Result: Established critical Fujita exponent for power nonlinearities and provided necessary conditions for blow-up/sufficient conditions for global positive solutions for time-dependent nonlinearities.

Conclusion: The paper determines precise conditions under which solutions exist globally or blow up, extending Fujita-type results to heat equations with vector fields satisfying Hörmander's condition.

Abstract: In this paper, we show global existence and non-existence results for the
heat equation with some of the squares of smooth vector fields on $\Rn$
satisfying H\"{o}rmander's rank condition with a non-linearity of the form
$f(u)$, where $f$ is a suitable function and $u$ is the solution. In
particular, when $f(u)=u^p$, we calculate the critical Fujita exponent. We also
give necessary conditions for blow-up or, alternatively, a sufficient condition
for the existence of positive global solutions for time-dependent
nonlinearities of the type $\varphi(t)f(u)$.

</details>


### [17] [NLS with mass-subcritical combined nonlinearities: small mass $L^2$-scattering](https://arxiv.org/abs/2511.04340)
*Jacopo Bellazzini,Luigi Forcella,Vladimir Georgiev*

Main category: math.AP

TL;DR: Small data scattering for mass-subcritical NLS with double nonlinearities (focusing leading term + defocusing perturbation) using pseudo-conformal transformation and variational arguments.


<details>
  <summary>Details</summary>
Motivation: To establish scattering results for nonlinear Schrödinger equations with competing nonlinearities, where a focusing leading term is balanced by a defocusing lower-order perturbation.

Method: Uses pseudo-conformal transformation combined with general variational arguments to ensure positivity of modified energies. The approach requires smallness only on the initial mass, not the full Σ-norm.

Result: Proves small data scattering in the mass-subcritical regime for NLS with double nonlinearities.

Conclusion: The combination of pseudo-conformal transformation and variational methods successfully establishes scattering for this class of equations with reduced smallness assumptions.

Abstract: We prove small data scattering in the mass-subcritical regime for the NLS
equation with double nonlinearities, where a focusing leading term is perturbed
by a lower order defocusing nonlinear term. Our proof relies on the
pseudo-conformal transformation in conjunction with a general variational
argument used to obtain the positivity of certain modified energies. Moreover,
the smallness assumption is only on the mass of the initial data, and not on
the whole $\Sigma$-norm.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [18] [Robust Subgroup Method Using DE Algorithm for Resonance Self-Shielding Calculation](https://arxiv.org/abs/2511.04062)
*Beichen Zheng,Ying Chen,Lili Wen,Xiaofei Wu*

Main category: physics.comp-ph

TL;DR: Enhanced subgroup method combining Robust Estimation and Differential Evolution to remove systematic absorption bias in resonance self-shielding treatment, improving predictive accuracy in transport simulations.


<details>
  <summary>Details</summary>
Motivation: To address systematic absorption bias in conventional subgroup methods that depresses reactivity, particularly in benchmarks sensitive to U-238, caused by threshold-like conditioning failures from strong self-shielding leverage and dilution-induced multicollinearity.

Method: Robust subgroup method integrating Robust Estimation (RE) to handle model misspecification and data contamination, with Differential Evolution (DE) algorithm as optimization tool within RE framework to obtain constrained solutions.

Result: Numerical validation shows the method removes systematic absorption bias that appears only in U-238 sensitive benchmarks, improving predictive fidelity by bounding influence and enforcing feasibility.

Conclusion: The RE-DE framework enables subgroup parameters to track underlying physics more faithfully, addressing conditioning failures seeded by narrow, sparse resonance structures in fertile even-even nuclides at low energies.

Abstract: This paper presents an enhanced version of the subgroup method for resonance
self-shielding treatment, termed the robust subgroup method, which integrates
Robust Estimation (RE) with a Differential Evolution (DE) algorithm. The RE
approach is employed to handle model misspecification and data contamination,
while the DE algorithm serves as an optimization tool within the RE framework
to obtain constrained solutions. Numerical validation against experimental
benchmarks shows that the proposed method removes a systematic absorption bias
in conventional subgroup fits that would otherwise depress reactivity. This
bias appears only in benchmarks sensitive to U-238. Mechanistically, it
reflects a threshold-like conditioning failure: strong self-shielding leverage
dominates the loss and is magnified by dilution-induced multicollinearity. This
adverse conditioning appears to be seeded by a narrow, sparse resonance
structure at low energies in fertile even-even nuclides, thereby causing rapid
self-shielding response saturation and a weak Doppler broadening. By bounding
influence and enforcing feasibility within an RE-DE framework, the inferred
subgroup parameters track the underlying physics more faithfully, improving the
predictive fidelity of subsequent transport simulations.

</details>


### [19] [Novel Numerical Methods for Accurate Space Thermal Analysis: Enforcing View Factors and Modeling Diffuse Reflectivity](https://arxiv.org/abs/2511.04277)
*Bernat Frangi*

Main category: physics.comp-ph

TL;DR: This research improves space thermal modeling by developing novel methods to incorporate diffuse reflectivity and correct view factor errors in radiative heat transfer simulations, enabling more accurate spacecraft thermal predictions.


<details>
  <summary>Details</summary>
Motivation: Accurate thermal analysis is crucial for modern spacecraft, but existing correction schemes cannot simultaneously enforce closure and reciprocity for open systems in radiative heat transfer modeling.

Method: Proposes two novel enforcement methods: (1) least-squares optimization with non-negativity rectification and small positive value avoidance, and (2) an iterative enforcement algorithm. Also introduces multi-node surface model relations to formalize connections between different discretization levels.

Result: Substantial reduction in mean absolute error: least-squares method achieves 81% MAE reduction, iterative method offers 56% MAE reduction with best computational efficiency. Including diffuse reflections decreases steady-state temperature of a plate by 4°C.

Conclusion: The work introduces and validates computationally efficient methods for integrating diffuse reflectivity into space thermal analyses and consistently coupling multi-node surface radiative models, enabling more accurate and robust thermal predictions for spacecraft systems.

Abstract: Accurate thermal analysis is crucial for modern spacecraft, driving demand
for reliable modeling tools. This research advances space thermal modeling by
improving the simulation accuracy and efficiency of radiative heat transfer,
the dominant mode of heat exchange in space. To this end, we incorporate
diffuse reflectivity using the Gebhart method, which computes radiative
exchange factors (REFs) from geometric view factors. The view factors, obtained
via Monte Carlo ray tracing (MCRT), require post-processing to mitigate
statistical errors. Critically, existing correction schemes cannot
simultaneously enforce closure and reciprocity for open systems. This research
addresses this gap by proposing two novel enforcement methods: (i) a
least-squares optimization with non-negativity rectification (NNR) and small
positive value avoidance (SPVA), and (ii) an iterative enforcement algorithm.
To ensure consistency across different discretization levels, this work also
introduces the multi-node surface model relations to formalize the connection
between sub-face, face, and node representations of view factors and REFs. A
simple case study demonstrates a substantial reduction in mean absolute error
(MAE): the least-squares method achieves an 81% MAE reduction, while the
iterative method offers the best balance of accuracy (56% MAE reduction) and
computational efficiency. A second case study shows that including diffuse
reflections decreases the steady-state temperature of a plate by $4^{\circ}C$,
reinforcing that reflected radiation reduces net absorption. This work
introduces and validates computationally efficient methods for integrating
diffuse reflectivity into space thermal analyses and for consistently coupling
multi-node surface radiative models. The results enable more accurate and
robust thermal predictions for spacecraft systems.

</details>


### [20] [Unveiling the Adsorption and Electronic Interactions of Drugs on 2D Graphsene: Insights from DFT and Machine Learning Approach](https://arxiv.org/abs/2511.04483)
*Chaithanya Purushottam Bhat,Pranav Suryawanshi,Aditya Guneja,Debashis Bandyopadhyay*

Main category: physics.comp-ph

TL;DR: Synergistic DFT-ML framework predicts drug adsorption on novel 2D graphene allotrope (Graphsene) with high accuracy, enabling efficient screening of drug candidates for nanomaterial-based delivery systems.


<details>
  <summary>Details</summary>
Motivation: Efficient identification of promising drug candidates for nanomaterial-based delivery systems is essential for advancing next-generation therapeutics.

Method: Combined density functional theory (DFT) and machine learning (ML) approach using dataset of 67 drugs on 2D substrates; DFT analyses include adsorption energetics, PDOS, and Bader charge calculations.

Result: ML model achieved 0.075 eV mean absolute error in DFT validation; DFT revealed pronounced charge transfer and electronic coupling between drugs and Graphsene surface.

Conclusion: Integrated DFT-ML strategy offers rapid, cost-efficient approach for screening drug-nanomaterial interactions, enabling data-driven design of advanced drug delivery systems.

Abstract: Efficient identification of promising drug candidates for nanomaterial-based
delivery systems is essential for advancing next-generation therapeutics. In
this work, we present a synergistic framework combining density functional
theory (DFT) and machine learning (ML) to explore the adsorption behavior and
electronic interactions of drugs on a novel 2D graphene allotrope, termed
Graphsene (GrS). Graphsene, characterized by its porous ring topology and large
surface area, offers an excellent platform for efficient adsorption and strong
electronic coupling with drug molecules. A dataset comprising 67 drugs adsorbed
on various 2D substrates was employed to train the ML model, which was
subsequently applied to predict suitable drug candidates for GrS based on
molecular size and adsorption energy criteria (database link provided in a
later section). The ML model exhibited robust predictive accuracy, achieving a
mean absolute error of 0.075 eV upon DFT validation, though its sensitivity to
initialization highlighted the need for larger and more diverse datasets.
DFT-based analyses, including adsorption energetics, projected density of
states (PDOS), and Bader charge calculations, revealed pronounced charge
transfer and electronic coupling between the drug molecules and the GrS
surface, elucidating the fundamental nature of drug-substrate interactions. The
study reveals that the integrated DFT-ML strategy offers a rapid,
cost-efficient approach for screening and understanding drug-nanomaterial
interactions, paving the way for data-driven design of advanced
nanomaterial-enabled drug delivery systems.

</details>


### [21] [Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear Fusion](https://arxiv.org/abs/2511.04489)
*Oskar Lappi,Huw Leggate,Yannick Marandet,Jan Åström,Keijo Heljanko,Dmitriy V. Borodin*

Main category: physics.comp-ph

TL;DR: Eiron implements domain-decomposed Monte Carlo (DDMC) algorithm that outperforms existing parallel algorithms in EIRENE, enabling simulations on grids too large for single nodes.


<details>
  <summary>Details</summary>
Motivation: EIRENE lacks domain decomposition, preventing simulations where grid data doesn't fit on one compute node due to memory constraints.

Method: Implemented DDMC algorithm in new open-source code Eiron, compared with two existing EIRENE parallel algorithms through strong scaling tests.

Result: DDMC performed better in nearly all cases, achieving superlinear strong scaling on Mahti supercomputer for large grids, and scaled to 16384 cores with 45% weak scaling efficiency in high-collisional cases.

Conclusion: Implementing DDMC in EIRENE would improve performance and enable currently impossible simulations due to memory limitations.

Abstract: EIRENE [1] is a Monte Carlo neutral transport solver heavily used in the
fusion community. EIRENE does not implement domain decomposition, making it
impossible to use for simulations where the grid data does not fit on one
compute node (see e.g. [2]). This paper presents a domain-decomposed Monte
Carlo (DDMC) algorithm implemented in a new open source Monte Carlo code,
Eiron. Two parallel algorithms currently used in EIRENE are also implemented in
Eiron, and the three algorithms are compared by running strong scaling tests,
with DDMC performing better than the other two algorithms in nearly all cases.
On the supercomputer Mahti [3], DDMC strong scaling is superlinear for grids
that do not fit into an L3 cache slice (4 MiB). The DDMC algorithm is also
scaled up to 16384 cores in weak scaling tests, with a weak scaling efficiency
of 45% in a high-collisional (heavier compute load) case, and 26% in a
low-collisional (lighter compute load) case. We conclude that implementing this
domain decomposition algorithm in EIRENE would improve performance and enable
simulations that are currently impossible due to memory constraints.

</details>


### [22] [Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI](https://arxiv.org/abs/2511.04564)
*Yoh-ichi Mototake,Makoto Sasaki*

Main category: physics.comp-ph

TL;DR: A framework for quantifying uncertainties in physics-informed machine learning (PIML) when estimating coefficient functions from data, addressing the challenge of selecting physically meaningful solutions beyond just predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: Physics relies on more than just prediction accuracy to evaluate models (e.g., Kepler's heliocentric model was preferred despite similar accuracy to geocentric model). PIML faces inherent uncertainties in data-driven model inference, highlighting the need to quantify uncertainties and select physically meaningful solutions.

Method: Proposed a framework to quantify and analyze uncertainties in the estimation of coefficient functions in PIML. Applied the framework to a reduced model of magnetohydrodynamics and incorporated geometric constraints for unique identification.

Result: The framework revealed uncertainties in coefficient function estimation, and showed that unique identification is possible when incorporating geometric constraints.

Conclusion: The proposed framework successfully quantifies uncertainties in PIML and enables unique estimation of reduced models by incorporating appropriate physical constraints.

Abstract: Physics-informed machine learning (PIML) integrates partial differential
equations (PDEs) into machine learning models to solve inverse problems, such
as estimating coefficient functions (e.g., the Hamiltonian function) that
characterize physical systems. This framework enables data-driven understanding
and prediction of complex physical phenomena. While coefficient functions in
PIML are typically estimated on the basis of predictive performance, physics as
a discipline does not rely solely on prediction accuracy to evaluate models.
For example, Kepler's heliocentric model was favored owing to small
discrepancies in planetary motion, despite its similar predictive accuracy to
the geocentric model. This highlights the inherent uncertainties in data-driven
model inference and the scientific importance of selecting physically
meaningful solutions. In this paper, we propose a framework to quantify and
analyze such uncertainties in the estimation of coefficient functions in PIML.
We apply our framework to reduced model of magnetohydrodynamics and our
framework shows that there are uncertainties, and unique identification is
possible with geometric constraints. Finally, we confirm that we can estimate
the reduced model uniquely by incorporating these constraints.

</details>


### [23] [Combining Harmonic Sampling with the Worm Algorithm to Improve the Efficiency of Path Integral Monte Carlo](https://arxiv.org/abs/2511.04597)
*Sourav Karmakar,Sutirtha Paul,Adrian Del Maestro,Barak Hirshberg*

Main category: physics.comp-ph

TL;DR: Improved PIMC algorithms (H-PIMC and M-PIMC) that separate harmonic and anharmonic potential contributions to enhance sampling efficiency for quantum solids and dense confined liquids.


<details>
  <summary>Details</summary>
Motivation: Standard PIMC suffers from low acceptance ratios and high autocorrelation times when studying quantum condensed phases like solids and dense confined liquids.

Method: Developed two sampling schemes: H-PIMC generates exact harmonic potential paths and accepts/rejects based on anharmonic part; M-PIMC restricts harmonic sampling near local minima and uses standard PIMC elsewhere.

Result: For weakly to moderately anharmonic systems at βℏω=16: 6-16x improvement in acceptance ratio, 7-30x reduction in autocorrelation time, and 2-4x acceleration from fewer imaginary time slices. M-PIMC works well for strongly anharmonic systems.

Conclusion: The proposed H-PIMC and M-PIMC methods significantly improve sampling efficiency for quantum condensed phases and can be combined with worm algorithm for indistinguishable particles.

Abstract: We propose an improved Path Integral Monte Carlo (PIMC) algorithm called
Harmonic PIMC (H-PIMC) and its generalization, Mixed PIMC (M-PIMC). PIMC is a
powerful tool for studying quantum condensed phases. However, it often suffers
from a low acceptance ratio for solids and dense confined liquids. We develop
two sampling schemes especially suited for such problems by dividing the
potential into its harmonic and anharmonic contributions. In H-PIMC, we
generate the imaginary time paths for the harmonic part of the potential
exactly and accept or reject it based on the anharmonic part. In M-PIMC, we
restrict the harmonic sampling to the vicinity of local minimum and use
standard PIMC otherwise, to optimize efficiency. We benchmark H-PIMC on systems
with increasing anharmonicity, improving the acceptance ratio and lowering the
auto-correlation time. For weakly to moderately anharmonic systems, at $\beta
\hbar \omega=16$, H-PIMC improves the acceptance ratio by a factor of 6-16 and
reduces the autocorrelation time by a factor of 7-30. We also find that the
method requires a smaller number of imaginary time slices for convergence,
which leads to another two- to four-fold acceleration. For strongly anharmonic
systems, M-PIMC converges with a similar number of imaginary time slices as
standard PIMC, but allows the optimization of the auto-correlation time. We
extend M-PIMC to periodic systems and apply it to a sinusoidal potential.
Finally, we combine H- and M-PIMC with the worm algorithm, allowing us to
obtain similar efficiency gains for systems of indistinguishable particles.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [24] [Modeling of Injected Current Stream-Induced 3D Perturbations in Local Helicity Injection Plasmas](https://arxiv.org/abs/2511.03930)
*C. E. Schaefer,A. C. Sontag,N. M. Ferraro,J. D. Weberski,S. J. Diem*

Main category: physics.plasm-ph

TL;DR: Local helicity injection (LHI) for solenoid-free tokamak startup causes substantial magnetic topology degradation in Pegasus-III plasmas, with rotation and two-fluid physics playing key roles in screening perturbations.


<details>
  <summary>Details</summary>
Motivation: Solenoid-free startup techniques like LHI are essential for spherical tokamaks to reduce costs and simplify fusion energy system designs.

Method: Used helical filament model for injected current, calculated linear plasma response with M3D-C1, performed Poincaré mapping, and compared single-fluid vs two-fluid models with/without rotation.

Result: LHI causes flux surface degradation starting at Ψ_N ≈ 0.37, with rotation providing partial screening of n=1 perturbations. Two-fluid models show stronger edge suppression. Magnetic probe measurements match better with spatially spread stream models.

Conclusion: Rotation and two-fluid physics are crucial for screening LHI perturbations. Future work should focus on plasma flow measurements and refined stream models for improved predictive capability.

Abstract: Solenoid-free tokamak startup techniques are essential for spherical tokamaks
and offer a pathway to cost reduction and design simplification in fusion
energy systems. Local helicity injection (LHI) is one such approach, employing
compact edge current sources to drive open field line current that initiates
and sustains tokamak plasmas. The recently commissioned Pegasus-III spherical
tokamak provides a platform for advancing this and other solenoid-free startup
methods. This study investigates the effect of LHI on magnetic topology in
Pegasus-III plasmas. A helical filament model represents the injected current,
and the linear plasma response to its 3D field is calculated with M3D-C1.
Poincar\'e mapping reveals substantial flux surface degradation in all modeled
cases. The onset of overlapping magnetic structures and large-scale surface
deformation begins at $\Psi_{N} \approx 0.37$, indicating a broad region of
perturbed topology extending toward the edge. In rotating plasmas, both
single-fluid and two-fluid models exhibit partial screening of the $n = 1$
perturbation, with two-fluid calculations showing stronger suppression near the
edge. In contrast, the absence of rotation leads to strong resonant field
amplification in the single-fluid case, while the two-fluid case with zero
electron rotation mitigates this amplification and preserves edge screening.
Magnetic probe measurements indicate that modeling the stream with spatial
spreading$-$representing distributed current and/or oscillatory motion$-$better
reproduces measured magnetic power profiles than a rigid filament model. The
results underscore the role of rotation and two-fluid physics in screening
stream perturbations and point to plasma flow measurements and refined stream
models as key steps toward improving predictive fidelity.

</details>


### [25] [Electromagnetic turbulence in EAST plasmas with internal transport barrier](https://arxiv.org/abs/2511.04044)
*Yuehao Ma,Pengfei Liu,Jian Bao,Zhihong Lin,Huishan Cai*

Main category: physics.plasm-ph

TL;DR: Global nonlinear electromagnetic gyrokinetic simulations show electromagnetic effects suppress ITG turbulence in EAST tokamak ITB regions, reducing thermal ion heat conductivity by at least 4x through enhanced zonal flow shearing.


<details>
  <summary>Details</summary>
Motivation: To investigate turbulence in the Internal Transport Barrier (ITB) region of EAST tokamak with weakly reversed magnetic shear, particularly understanding the role of electromagnetic effects on ion temperature gradient (ITG) modes.

Method: Global nonlinear electromagnetic gyrokinetic simulations, including linear analysis of ITG modes at different radial positions (q=1 and q_min surfaces), studying finite β effects, and examining nonlinear electromagnetic regime with zonal flows.

Result: Finite β effects suppress higher frequency ITG modes at q=1 surface; when β_i exceeds 0.5%, the ITG mode near q_min becomes dominant. Nonlinear electromagnetic effects reduce thermal ion heat conductivity by at least factor of 4 through enhanced zonal flow shearing. Energetic particles provide slight stabilizing effect.

Conclusion: Electromagnetic effects play crucial role in stabilizing ITG modes and causing transitions between dominant modes. Including electromagnetic effects is essential for accurate transport coefficient calculations in ITB regions with weak magnetic shear.

Abstract: In this study, global nonlinear electromagnetic gyrokinetic simulations are
conducted to investigate turbulence in the Internal transport barrier (ITB)
region of the EAST tokamak discharge with weakly reversed magnetic shear.
Linear simulations reveal two dominant ion temperature gradient (ITG) modes: a
higher frequency mode at the $q=1$ surface, which dominates in the
electrostatic limit, and a lower frequency mode near the $q_{\min}$ surface,
which prevails under the experimental $\beta$ (the ratio of plasma pressure to
magnetic pressure). Finite $\beta$ effects effectively suppress higher
frequency ITG modes, and once $\beta_i$ on axis exceeds 0.5\%, this ITG mode is
no longer dominant, and the ITG mode near $q_{\min}$ surface becomes the
primary instability. Therefore, electromagnetic effects play a crucial role in
stabilizing ITG modes, and in causing the transition between the most unstable
mode at different radial positions. The linear growth rate of the unstable mode
in the electrostatic limit is approximately 1.25 times higher than that of the
dominant mode in the electromagnetic case. However, in the electromagnetic
nonlinear regime, the thermal ion heat conductivity is reduced by at least a
factor of 4. This reduction primarily results from nonlinear electromagnetic
effects enhancing the shearing effect of zonal flows, thereby further
suppressing microturbulence. Finally, energetic particles exert a slight
stabilizing effect on ITG turbulence due to dilution and finite $\beta$
contributions. It is emphasized that the electromagnetic effect on ITG with
weak magnetic shear should be included to accurately calculate the transport
coefficients.

</details>


### [26] [Cross-scale Interaction between Microturbulence and Fishbone in Fusion Plasmas](https://arxiv.org/abs/2511.04051)
*Yuehao Ma,Bin Zhang,Pengfei Liu,Jian Bao,Zhihong Lin,Huishan Cai,Liutian Gao,AhDi Liu,Hailin Zhao,Tao Zhang*

Main category: physics.plasm-ph

TL;DR: Global gyrokinetic simulations reveal that fishbone instability-driven zonal radial electric fields significantly suppress electromagnetic ITG turbulence, reducing ion thermal transport to neoclassical levels.


<details>
  <summary>Details</summary>
Motivation: To investigate cross-scale interactions between electromagnetic ion temperature gradient (ITG) turbulence and fishbone instability in tokamak plasmas, as these multiscale interactions are crucial for understanding thermal confinement in fusion plasmas.

Method: Performed global gyrokinetic simulations for the first time to study the interaction between electromagnetic ITG turbulence and fishbone instability in tokamak plasmas.

Result: Fishbone-driven zonal radial electric fields at nonlinear saturation significantly suppress electromagnetic ITG turbulence, reducing ion thermal transport close to the neoclassical level. The simulation results agree well with experimental observations of turbulence suppression during fishbone bursts.

Conclusion: These findings advance understanding of multiscale interactions that enhance thermal confinement in fusion plasmas, demonstrating that fishbone instability can play a beneficial role in turbulence suppression.

Abstract: Global gyrokinetic simulations are performed for the first time to
investigate cross-scale interactions between electromagnetic ion temperature
gradient (ITG) turbulence and fishbone instability in tokamak plasmas. The
investigation of fluctuation response in the multiscale simulation including
both instabilities indicates a strong impact of fishbone on ITG turbulence.
Detailed analysis reveals that fishbone-driven zonal radial electric fields at
nonlinear saturation significantly suppress electromagnetic ITG turbulence,
reducing ion thermal transport close to the neoclassical level. The simulation
results agree well with experimental observations that turbulence suppression
during fishbone bursts. These findings advance understanding of multiscale
interactions that enhance thermal confinement in fusion plasmas.

</details>


### [27] [Stochastic simulation of partial discharge inception](https://arxiv.org/abs/2511.04356)
*Jannis Teunissen,Yuting Gao*

Main category: physics.plasm-ph

TL;DR: A Monte Carlo method for simulating electric discharge inception in gases using electrostatic field data on unstructured grids, estimating discharge probability and time lag per initial electron position.


<details>
  <summary>Details</summary>
Motivation: To develop a computational model that can predict electric discharge inception probabilities and timing across various electrode geometries, including regions with sub-critical electric fields.

Method: Simulates electron avalanches along field lines with photon and ion feedback mechanisms, using statistical distributions for avalanche size validated against particle simulations.

Result: Successfully demonstrates discharge inception simulations in 2D Cartesian, 2D axisymmetric, and 3D electrode geometries with validated statistical distributions.

Conclusion: The method provides reliable estimates of discharge inception probability and timing across different geometries, including challenging cases with electron attachment.

Abstract: We present a Monte Carlo method for simulating the inception of electric
discharges in gases. The input consists of an unstructured grid containing the
electrostatic field. The output of the model is the estimated probability of
discharge inception per initial electron position, as well as the estimated
time lag between the appearance of the initial electron and discharge
inception. To obtain these quantities electron avalanches are simulated for
initial electron positions throughout the whole domain, also including regions
below the critical electric field. Avalanches are assumed to propagate along
field lines, and they can produce additional avalanches due to photon and ion
feedback. If the number of avalanches keeps increasing over time we assume that
an electric discharge will eventually form. A statistical distribution for the
electron avalanche size is used, which is also valid for gases with strong
electron attachment. We compare this distribution against the results of
particle simulations. Furthermore, we demonstrate examples of inception
simulations in 2D Cartesian, 2D axisymmetric and 3D electrode geometries.

</details>


### [28] [Lightning-Induced Faults in Low-Voltage Distribution Networks via Hybrid VTS-PEEC Method](https://arxiv.org/abs/2511.04441)
*Xiaobing Xiao,Xipeng Chen,Lei Jia,Huaifei Chen,Lu Qu,Chakhung Yeung*

Main category: physics.plasm-ph

TL;DR: Analysis of lightning-induced faults in low-voltage distribution networks using a hybrid VTS-PEEC method, focusing on how lightning stroke location affects overvoltage and fault risk.


<details>
  <summary>Details</summary>
Motivation: Low-voltage distribution networks are critical for grid stability but face significant threats from lightning-induced faults, which transient simulations can study more economically than experiments.

Method: Uses a hybrid Variable Time Step (VTS)-Partial Element Equivalent Circuit (PEEC) method validated in previous studies for Lightning-induced Electromagnetic Pulse (LEMP) simulation and fault analysis in extended unequal-length double-circuit low-voltage distribution networks.

Result: Ground strokes in front of circuit centers produce similar three-phase negative and bipolar oscillatory waveforms linked to fault initiation. Closer strokes generate bipolar waveforms with negative main peaks, higher overvoltages, and increased fault risk.

Conclusion: The findings provide essential insights into lightning-induced fault mechanisms, forming a foundation for developing more targeted and effective lightning protection measures.

Abstract: As a critical component of power supply systems, low-voltage distribution
net-works directly affect grid stability and user power supply reliability, yet
they face significant threats from lightning-induced faults. Transient
simulations are more economical and adaptable for investigating
lightning-induced faults in low-voltage distribution networks than experiments.
A hybrid Variable Time Step (VTS)-Partial Element Equivalent Circuit (PEEC)
method, has been validat-ed in previous study, is used for Lightning-induced
Electromagnetic Pulse (LEMP) simulation and fault analysis. The
lightning-induced faults in ex-tended unequal-length double-circuit low-voltage
distribution networks are ana-lyzed in this paper. The impact of lightning
stroke location on overvoltage and fault risk is the primary focus of this
study. Key findings indicate that, for ground strokes in front of the center of
one double circuit, similar three-phase negative and bipolar oscillatory
waveforms that are linked to fault initiation emerge. Closer strokes promote
bipolar waveforms with the main peak negative as well as higher overvoltages
and fault risk. These results provide essential insights for under-standing
lightning-induced fault mechanisms, thereby laying a foundation for formulating
more targeted and effective lightning protection measures.

</details>


### [29] [Approaching the thermodynamic limit of a bounded one-component plasma](https://arxiv.org/abs/2511.04516)
*D. I. Zhukhovitskii,E. E. Perevoshchikov*

Main category: physics.plasm-ph

TL;DR: Molecular dynamics study of spherical bounded one-component plasma establishes thermodynamic limit properties and improves simulation accuracy for Coulomb coupling parameters from 0.03 to 1000.


<details>
  <summary>Details</summary>
Motivation: To accurately determine the thermodynamic properties of bounded one-component plasma systems and improve simulation methods by establishing size dependencies and extrapolating to infinite system limits.

Method: Used molecular dynamics simulations of spherical bounded one-component plasma systems, established size dependencies, extrapolated to thermodynamic limit, and introduced new converging characteristic energies for better analysis.

Result: Obtained total electrostatic energy per ion with 0.1% relative error across wide Γ range (0.03-1000), showing 0.5% lower values than Monte Carlo at Γ<30 and good agreement at Γ>175. Derived ionic equation of state and improved cutoff radius for LAMMPS simulations.

Conclusion: The study provides accurate thermodynamic properties for bounded one-component plasma, enables calculation of previously inaccessible compressibility factors, and demonstrates that fluid-solid phase transition locations depend sensitively on simulation cutoff radii.

Abstract: The classical one-component plasma (OCP) bounded by a spherical surface
reflecting ions (BOCP) is studied using molecular dynamics (MD). Simulations
performed for a series of sufficiently large BOCP's make it possible to
establish the size dependencies for the investigated quantities and extrapolate
them to the thermodynamic limit. In particular, the total electrostatic energy
per ion is estimated in the limit of infinite BOCP in a wide range of the
Coulomb coupling parameter $\Gamma$ from 0.03 to 1000 with the relative error
of the order 0.1%. Calculated energies are by about 0.5% lower as compared to
the modern Monte Carlo (MC) simulation data obtained by different authors at
$\Gamma<30$ and almost coincide with the MC results at $\Gamma>175$. We
introduce two more converging characteristic energies, the excess interatomic
electrostatic energy and the excess ion-background electrostatic energy, which
enable us to calculate the ionic compressibility factor inaccessible in
conventional MC and MD simulation of the OCP with periodic boundary conditions.
The derived wide-range ionic equation of state can be recommended for testing
OCP simulations with various effective interaction potentials. Based on this
equation, we propose an improved cutoff radius for the interionic forces
implemented in LAMMPS and perform MD simulation of the OCP to demonstrate that
location of the metastable region of the fluid-solid phase transition depends
sensitively on this radius.

</details>


### [30] [Electromagnetic plasma wave modes propagating along light-cone coordinates](https://arxiv.org/abs/2511.04554)
*Felipe A. Asenjo,Swadesh M. Mahajan*

Main category: physics.plasm-ph

TL;DR: The paper presents new electromagnetic plasma modes that propagate in 1+1 dimensions using light-cone coordinates, enabling wavepacket solutions with unique properties like defined wavefronts and faster-than-light velocity.


<details>
  <summary>Details</summary>
Motivation: To develop novel electromagnetic wave solutions that differ from conventional plane waves by using light-cone coordinates, allowing for more complex wavepacket structures with enhanced functional properties.

Method: Construct wavepacket solutions using light-cone coordinates instead of separation of variables, employing special functions like Airy functions, Parabolic cylinder functions, Mathieu functions, and Bessel functions to create structured electromagnetic modes.

Result: Successfully derived several new wavepacket solutions, including a double Airy solution with defined wavefront and velocity faster than electromagnetic plane waves, demonstrating novel electromagnetic properties.

Conclusion: The approach using light-cone coordinates enables construction of structured electromagnetic wavepackets with unique properties, and more general wavepackets can be built from these foundational solutions.

Abstract: We present new electromagnetic plasma modes that propagates in one time and
one space coordinates. Differently to the usual plane wave solution, which is
written in terms of separation of variables, all our solutions are along the
light-cone coordinates. This allow us to find several new wavepacket solutions
whose functionality properties rely on the conditions imposed on the choice for
their light-cone coordinates dependence. The presented wavepacket solutions are
constructed in terms of multiplications of Airy functions, Parabolic cylinder
functions, Mathieu functions, or Bessel functions. We thoroughly analyze the
case of a double Airy solution, which have new electromagnetic properties, as a
defined wavefront, and velocity faster than the electromagnetic plane wave
counterpart solution. It is also mentioned how more general structured
wavepackets can be constructed from these new solutions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [31] [Friction on Demand: A Generative Framework for the Inverse Design of Metainterfaces](https://arxiv.org/abs/2511.03735)
*Valentin Mouton,Adrien Mélot*

Main category: stat.ML

TL;DR: A generative modeling framework using VAEs to infer surface topographies from target friction laws, enabling efficient simulation-free generation of frictional interfaces.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches rely on heuristic search over low-dimensional parameterizations, limiting applicability to complex friction laws. The inverse design problem is challenging due to non-uniqueness of solutions and computational cost of contact simulations.

Method: Use Variational Autoencoders (VAEs) trained on a synthetic dataset of 200 million samples from a parameterized contact mechanics model to generate candidate surface topographies from target friction laws.

Result: The method enables efficient, simulation-free generation of candidate topographies. The study examines trade-offs between accuracy, throughput, and diversity in generated solutions.

Conclusion: This approach paves the way for near-real-time control of frictional behavior through tailored surface topographies, highlighting practical considerations for balancing design objectives.

Abstract: Designing frictional interfaces to exhibit prescribed macroscopic behavior is
a challenging inverse problem, made difficult by the non-uniqueness of
solutions and the computational cost of contact simulations. Traditional
approaches rely on heuristic search over low-dimensional parameterizations,
which limits their applicability to more complex or nonlinear friction laws. We
introduce a generative modeling framework using Variational Autoencoders (VAEs)
to infer surface topographies from target friction laws. Trained on a synthetic
dataset composed of 200 million samples constructed from a parameterized
contact mechanics model, the proposed method enables efficient, simulation-free
generation of candidate topographies. We examine the potential and limitations
of generative modeling for this inverse design task, focusing on balancing
accuracy, throughput, and diversity in the generated solutions. Our results
highlight trade-offs and outline practical considerations when balancing these
objectives. This approach paves the way for near-real-time control of
frictional behavior through tailored surface topographies.

</details>
