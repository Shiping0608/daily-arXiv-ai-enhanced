<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 13]
- [physics.comp-ph](#physics.comp-ph) [Total: 6]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 8]
- [cs.LG](#cs.LG) [Total: 2]
- [math.PR](#math.PR) [Total: 1]
- [nlin.SI](#nlin.SI) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Relative entropy estimate and geometric ergodicity for implicit Langevin Monte Carlo](https://arxiv.org/abs/2511.04041)
*Lei Li,Jian-Guo Liu,Yuliang Wang*

Main category: math.NA

TL;DR: The paper analyzes the implicit Langevin Monte Carlo (iLMC) method, proving its convergence and geometric ergodicity for non-Lipschitz drift fields using PDE techniques and coupling methods.


<details>
  <summary>Details</summary>
Motivation: Explicit schemes like LMC may fail when the drift field is not globally Lipschitz, while iLMC can handle one-sided Lipschitz conditions, making it more robust for practical applications.

Method: Uses continuous-time interpolation, PDE techniques including Bernstein method, and reflection-type continuous-discrete coupling to analyze iLMC's convergence and ergodicity.

Result: Proves time-discretization error bound under relative entropy, geometric ergodicity under Wasserstein-1 distance, and extends to uniform-in-time error bound.

Conclusion: iLMC provides reliable convergence guarantees for non-Lipschitz drifts, and the proof technique is universal for other implicit or splitting schemes in stochastic differential equations.

Abstract: We study the implicit Langevin Monte Carlo (iLMC) method, which simulates the
overdamped Langevin equation via an implicit iteration rule. In many
applications, iLMC is favored over other explicit schemes such as the
(explicit) Langevin Monte Carlo (LMC). LMC may blow up when the drift field
$\nabla U$ is not globally Lipschitz, while iLMC has convergence guarantee when
the drift is only one-sided Lipschitz. Starting from an adapted continuous-time
interpolation, we prove a time-discretization error bound under the relative
entropy (or the Kullback-Leibler divergence), where a crucial gradient estimate
for the logarithm numerical density is obtained via a sequence of PDE
techniques, including Bernstein method. Based on a reflection-type
continuous-discrete coupling method, we prove the geometric ergodicity of iLMC
under the Wasserstein-1 distance. Moreover, we extend the error bound to a
uniform-in-time one by combining the relative entropy error bound and the
ergodicity. Our proof technique is universal and can be applied to other
implicit or splitting schemes for simulating stochastic differential equations
with non-Lipschitz drifts.

</details>


### [2] [Numerical boundary flux functions that give provable bounds for nonlinear initial boundary value problems with open boundaries](https://arxiv.org/abs/2511.04197)
*Andrew R. Winters,David A. Kopriva,Jan Nordström*

Main category: math.NA

TL;DR: This paper presents a strategy for designing nonlinear characteristic-type penalty terms as numerical boundary flux functions that provide provable bounds for solutions to nonlinear hyperbolic initial boundary value problems with open boundaries.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop boundary treatments that guarantee entropy stability and solution bounds for nonlinear hyperbolic problems, addressing limitations of standard boundary treatments based on linear analysis that can fail in certain situations.

Method: The method uses a matrix formulation to express entropy flux as a quadratic form, enabling systematic design of characteristic-based penalty terms. A special decomposition of the boundary matrix defines appropriate characteristic-type variables, and the approach is compatible with high-order accurate split form discontinuous Galerkin spectral element methods.

Result: The authors derive inflow-outflow boundary fluxes specifically for the Burgers equation and two-dimensional shallow water equations, which are both entropy stable and energy stable. Numerical experiments show the new nonlinear fluxes work in situations where standard boundary treatments fail.

Conclusion: The proposed nonlinear boundary flux strategy provides provable bounds for solutions to nonlinear hyperbolic problems, ensuring entropy stability and boundedness solely by external data, overcoming limitations of traditional linear analysis-based boundary treatments.

Abstract: We present a strategy for interpreting nonlinear, characteristic-type penalty
terms as numerical boundary flux functions that provide provable bounds for
solutions to nonlinear hyperbolic initial boundary value problems with open
boundaries. This approach is enabled by recent work that found how to express
the entropy flux as a quadratic form defined by a symmetric boundary matrix.
The matrix formulation provides additional information for how to
systematically design characteristic-based penalty terms for the weak
enforcement of boundary conditions. A special decomposition of the boundary
matrix is required to define an appropriate set of characteristic-type
variables. The new boundary fluxes are directly compatible with high-order
accurate split form discontinuous Galerkin spectral element and similar methods
and guarantee that the solution is entropy stable and bounded solely by
external data. We derive inflow-outflow boundary fluxes specifically for the
Burgers equation and the two-dimensional shallow water equations, which are
also energy stable. Numerical experiments demonstrate that the new nonlinear
fluxes do not fail in situations where standard boundary treatments based on
linear analysis do.

</details>


### [3] [A space-time adaptive boundary element method for the wave equation](https://arxiv.org/abs/2511.04265)
*Alessandra Aimi,Giulia Di Credico,Heiko Gimperlein,Chiara Guardasoni*

Main category: math.NA

TL;DR: Study of space-time adaptive mesh refinements for wave equation boundary element methods using residual-based error indicators.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency and accuracy for time-dependent boundary element formulations of wave equations, particularly for acoustic soft-scattering problems with various types of solution singularities.

Method: Formulated adaptive boundary element procedure with local tensor-product refinements of space-time mesh based on residual-type error indicators.

Result: Demonstrated improved convergence rates in energy norm for problems with spatial, temporal, or traveling singularities; evaluated performance of rigorous and heuristic a posteriori error indicators.

Conclusion: The proposed adaptive method effectively handles different types of solution singularities and shows efficiency in space-time boundary element computations for wave equations.

Abstract: This article initiates the study of space-time adaptive mesh refinements for
time-dependent boundary element formulations of wave equations. Based on error
indicators of residual type, we formulate an adaptive boundary element
procedure for acoustic soft-scattering problems with local tensor-product
refinements of the space-time mesh. We discuss the algorithmic challenges and
investigate the proposed method in numerical experiments. In particular, we
study the performance and improved convergence rates with respect to the energy
norm for problems dominated by spatial, temporal or traveling singularities of
the solution. The efficiency of the considered rigorous and heuristic a
posteriori error indicators is discussed.

</details>


### [4] [DeepPAAC: A New Deep Galerkin Method for Principal-Agent Problems](https://arxiv.org/abs/2511.04309)
*Michael Ludkovski,Changgen Xie,Zimu Zhu*

Main category: math.NA

TL;DR: DeepPAAC: A novel deep learning method for solving continuous-time principal-agent problems with multi-dimensional states/controls and constraints.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for solving complex principal-agent problems in continuous time, which involve Hamilton-Jacobi-Bellman equations with implicit Hamiltonians.

Method: Deep Principal-Agent Actor Critic (DeepPAAC) algorithm using deep learning to handle multi-dimensional states, controls, and constraints.

Result: Successfully developed DeepPAAC method and investigated neural network architecture, training designs, and loss functions through five case studies.

Conclusion: DeepPAAC provides an effective deep learning approach for solving challenging principal-agent problems in continuous time with complex constraints.

Abstract: We consider numerical resolution of principal-agent (PA) problems in
continuous time. We formulate a generic PA model with continuous and lump
payments and a multi-dimensional strategy of the agent. To tackle the resulting
Hamilton-Jacobi-Bellman equation with an implicit Hamiltonian we develop a
novel deep learning method: the Deep Principal-Agent Actor Critic (DeepPAAC)
Actor-Critic algorithm. DeepPAAC is able to handle multi-dimensional states and
controls, as well as constraints. We investigate the role of the neural network
architecture, training designs, loss functions, etc. on the convergence of the
solver, presenting five different case studies.

</details>


### [5] [Normalized tensor train decomposition](https://arxiv.org/abs/2511.04369)
*Renfeng Peng,Chengkai Zhu,Bin Gao,Xin Wang,Ya-xiang Yuan*

Main category: math.NA

TL;DR: The paper introduces normalized tensor train (NTT) decomposition to approximate unit-norm tensors while preserving the tensor train format's low-rank structure, with applications in quantum physics and scientific computing.


<details>
  <summary>Details</summary>
Motivation: Tensors with unit Frobenius norm are fundamental in scientific computing and quantum physics, but standard tensor train decomposition doesn't enforce the unit-norm constraint needed for normalized eigenvectors and pure quantum states.

Method: Proposed normalized tensor train (NTT) decomposition that approximates tensors by unit-norm tensors in tensor train format, proved the set forms a smooth manifold with Riemannian geometry, and developed geometric methods for various applications.

Result: NTT decomposition preserves unit-norm structure while maintaining low-rank efficiency, enabling applications in tensor recovery, eigenvalue problems, stabilizer rank estimation, and quantum channel entropy calculations with superior efficiency and scalability.

Conclusion: The NTT decomposition provides an effective framework for handling unit-norm tensors in tensor train format, with proven geometric properties and demonstrated practical advantages in computational efficiency and scalability across multiple applications.

Abstract: Tensors with unit Frobenius norm are fundamental objects in many fields,
including scientific computing and quantum physics, which are able to represent
normalized eigenvectors and pure quantum states. While the tensor train
decomposition provides a powerful low-rank format for tackling high-dimensional
problems, it does not intrinsically enforce the unit-norm constraint. To
address this, we introduce the normalized tensor train (NTT) decomposition,
which aims to approximate a tensor by unit-norm tensors in tensor train format.
The low-rank structure of NTT decomposition not only saves storage and
computational cost but also preserves the underlying unit-norm structure. We
prove that the set of fixed-rank NTT tensors forms a smooth manifold, and the
corresponding Riemannian geometry is derived, paving the way for geometric
methods. We propose NTT-based methods for low-rank tensor recovery,
high-dimensional eigenvalue problem, estimation of stabilizer rank, and
calculation of the minimum output R\'enyi 2-entropy of quantum channels.
Numerical experiments demonstrate the superior efficiency and scalability of
the proposed NTT-based methods.

</details>


### [6] [The Loewner framework applied to Zolotarev sign and ratio problems](https://arxiv.org/abs/2511.04404)
*Athanasios C. Antoulas,Ion Victor Gosea,Charles Poussot-Vassal*

Main category: math.NA

TL;DR: Comparison of approximation methods for Zolotarev problems shows Loewner framework outperforms AAA variants in speed and accuracy for higher-degree approximants.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare different numerical approximation methods for solving the 3rd and 4th Zolotarev problems, particularly assessing the performance of the Loewner framework against AAA algorithm variants.

Method: Numerical study comparing Loewner framework, standard AAA algorithm, and AAA extensions (sign and Lawson variants) for approximating functions in Zolotarev problems.

Result: Loewner framework is faster, more reliable, and provides higher accuracy approximants. For higher-degree approximants, Loewner outperforms AAA-Lawson in accuracy and maintains constant running time while AAA-Lawson's time increases with degree.

Conclusion: The Loewner framework is superior to AAA variants for Zolotarev problems, offering better speed, reliability, and accuracy, especially for higher-degree approximations.

Abstract: In this work, we propose a numerical study concerning the approximation of
functions associated with the 3rd and 4th Zolotarev problems. We compare
various methods, in particular the Loewner framework, the standard AAA
algorithm, and recently-proposed extensions of AAA (namely, the sign and Lawson
variants). We show that the Loewner framework is fast and reliable, and
provides approximants with a high level of accuracy. When the approximants are
of a higher degree, Loewner approximants are often more accurate than
near-optimal ones computed with AAA-Lawson. Last but not least, the Loewner
framework is a direct method for which the running time is typically lower than
that of the iterative AAA-Lawson variants. Moreover, for the latter, the
running time increases substantially with the degree of the approximant,
whereas for the Loewner method, it remains constant. These claims are supported
by an extensive numerical treatment.

</details>


### [7] [Mean square error analysis of stochastic gradient and variance-reduced sampling algorithms](https://arxiv.org/abs/2511.04413)
*Jianfeng Lu,Xuda Ye,Zhennan Zhou*

Main category: math.NA

TL;DR: MSE analysis for stochastic gradient sampling algorithms in underdamped Langevin dynamics, showing first-order convergence for SG-UBU and phase transition to second-order convergence for variance-reduced methods.


<details>
  <summary>Details</summary>
Motivation: To analyze mean square error for stochastic gradient sampling algorithms applied to underdamped Langevin dynamics under convexity assumptions, and understand convergence behavior of different samplers.

Method: Developed a novel discrete Poisson equation framework to bound time-averaged sampling error. Analyzed SG-UBU, SVRG-UBU, and SAGA-UBU methods for finite-sum potentials.

Result: SG-UBU exhibits first-order convergence with step size, with error coefficient proportional to stochastic gradient variance. Variance-reduced methods show phase transition from first to second-order convergence when step size decreases below critical threshold.

Conclusion: Theoretical analysis validated by experiments provides practical criterion for selecting between mini-batch SG-UBU and SVRG-UBU samplers to achieve optimal computational efficiency.

Abstract: This paper considers mean square error (MSE) analysis for stochastic gradient
sampling algorithms applied to underdamped Langevin dynamics under a global
convexity assumption. A novel discrete Poisson equation framework is developed
to bound the time-averaged sampling error. For the Stochastic Gradient UBU
(SG-UBU) sampler, we derive an explicit MSE bound and establish that the
numerical bias exhibits first-order convergence with respect to the step size
$h$, with the leading error coefficient proportional to the variance of the
stochastic gradient. The analysis is further extended to variance-reduced
algorithms for finite-sum potentials, specifically the SVRG-UBU and SAGA-UBU
methods. For these algorithms, we identify a phase transition phenomenon
whereby the convergence rate of the numerical bias shifts from first to second
order as the step size decreases below a critical threshold. Theoretical
findings are validated by numerical experiments. In addition, the analysis
provides a practical empirical criterion for selecting between the mini-batch
SG-UBU and SVRG-UBU samplers to achieve optimal computational efficiency.

</details>


### [8] [An efficient boundary integral equation solution technique for solving aperiodic scattering problems from two-dimensional, periodic boundaries](https://arxiv.org/abs/2511.04424)
*Riley Fisher,Fruzsina Agocs,Adrianna Gillman*

Main category: math.NA

TL;DR: Efficient boundary integral method for 2D Helmholtz problems in half-plane with periodic boundaries and Neumann conditions, using Floquet-Bloch transform and periodizing scheme to avoid quasiperiodic Green's function evaluation.


<details>
  <summary>Details</summary>
Motivation: To develop a faster computational method for solving Helmholtz problems with periodic boundaries and aperiodic sources, avoiding the computational challenges of quasiperiodic Green's function evaluation.

Method: Uses Floquet-Bloch transform to convert problem into contour integral, employs Cho and Barnett's periodizing scheme variant to avoid quasiperiodic Green's function, and accelerates with low rank linear algebra.

Result: Method achieves 20-30 times speedup compared to techniques using quasiperiodic Green's function for stair-like geometries.

Conclusion: The presented boundary integral technique provides significant computational efficiency improvements for Helmholtz problems with periodic boundaries through innovative periodizing and acceleration methods.

Abstract: This manuscript presents an efficient boundary integral equation technique
for solving two-dimensional Helmholtz problems defined in the half-plane
bounded by an infinite, periodic curve with Neumann boundary conditions and an
aperiodic point source. The technique is designed for boundaries where one
period does not require a large number of discretization points to achieve high
accuracy. The Floquet--Bloch transform turns the problem into evaluating a
contour integral where the integrand is the solution of quasiperiodic boundary
value problems. To approximate the integral, one must solve a collection of
these problems. This manuscript uses a variant of the periodizing scheme by Cho
and Barnett which alleviates the need for evaluating the quasiperiodic Green's
function and is amenable to a large amount of precomputation that can be reused
for all of the necessary solves. The solution technique is accelerated by the
use of low rank linear algebra. The numerical results illustrate that the
presented method is 20-30 faster than the technique utilizing the quasiperiodic
Green's function for a stair-like geometry.

</details>


### [9] [A Two-stage Adaptive Lifting PINN Framework for Solving Viscous Approximations to Hyperbolic Conservation Laws](https://arxiv.org/abs/2511.04490)
*Yameng Zhu,Weibing Deng,Ran Bi*

Main category: math.NA

TL;DR: Proposes a two-stage adaptive lifting PINN framework to handle hyperbolic conservation laws near inviscid limits, using learned auxiliary fields to improve training stability and accuracy near discontinuities.


<details>
  <summary>Details</summary>
Motivation: Training PINNs for hyperbolic conservation laws near inviscid limits is challenging due to ill-posed residuals at shock discontinuities and spectral bias from small viscosity regularization.

Method: Two-stage adaptive lifting PINN with learned auxiliary fields through r-adaptive coordinate transformations, supported by theoretical error estimates, statistical interpretation, and NTK analysis.

Result: Numerical experiments show accelerated and more stable convergence with accurate reconstructions near discontinuities.

Conclusion: The proposed lifting framework effectively mitigates training difficulties for PINNs in hyperbolic conservation laws near inviscid limits without requiring prior knowledge of interface geometry.

Abstract: Training physics informed neural networks PINNs for hyperbolic conservation
laws near the inviscid limit presents considerable difficulties because strong
form residuals become ill posed at shock discontinuities, while small viscosity
regularization introduces narrow boundary layers that exacerbate spectral bias.
To address these issues this paper proposes a novel two stage adaptive lifting
PINN, a lifting based framework designed to mitigate such challenges without
requiring a priori knowledge of the interface geometry. The key idea is to
augment the physical coordinates by introducing a learned auxiliary field
generated through r adaptive coordinate transformations. Theoretically we first
derive an a posteriori L2 error estimate to quantify how training difficulty
depends on viscosity. Secondly we provide a statistical interpretation
revealing that embedded sampling induces variance reduction analogous to
importance sampling. Finally we perform an NTK and gradient flow analysis,
demonstrating that input augmentation improves conditioning and accelerates
residual decay. Supported by these insights our numerical experiments show
accelerated and more stable convergence as well as accurate reconstructions
near discontinuities.

</details>


### [10] [Spurious resonances for substructured FEM-BEM coupling](https://arxiv.org/abs/2511.04501)
*Antonin Boisneault,Marcella Bonazzoli,Pierre Marchand,Xavier Claeys*

Main category: math.NA

TL;DR: The paper analyzes spurious resonances in Generalized Optimized Schwarz Method (GOSM) for FEM-BEM coupling in acoustic scattering problems, showing it shares the same ill-posedness issues as classical couplings.


<details>
  <summary>Details</summary>
Motivation: To address the well-known problem of spurious resonances in classical FEM-BEM couplings for Helmholtz boundary value problems, even when the original problem is well-posed.

Method: Analysis of the Generalized Optimized Schwarz Method (GOSM) derived from Johnson-Nédélec and Costabel couplings, examining the kernel of the local operator at the FEM-BEM interface.

Result: GOSM is not immune to spurious resonances - the kernel of its local interface operator becomes non-trivial simultaneously with classical FEM-BEM couplings.

Conclusion: The new substructured FEM-BEM formulation (GOSM) inherits the ill-posedness issues of classical couplings for certain wavenumbers, requiring further investigation to overcome spurious resonances.

Abstract: We are interested in time-harmonic acoustic scattering by an impenetrable
obstacle in a medium where the wavenumber is constant in an exterior unbounded
subdomain and is possibly heterogeneous in a bounded subdomain. The associated
Helmholtz boundary value problem can be solved by coupling the Finite Element
Method (FEM) in the heterogeneous subdomain with the Boundary Element Method
(BEM) in the homogeneous subdomain. Recently, we designed and analyzed a new
substructured FEM-BEM formulation, called Generalized Optimized Schwarz Method
(GOSM). Unfortunately, it is well known that, even when the initial boundary
value problem is well-posed, the variational formulation of classical FEM-BEM
couplings can be ill-posed for certain wavenumbers, called spurious resonances.
In this paper, we focus on the Johnson-N\'ed\'elec and Costabel couplings and
show that the GOSM derived from both is not immune to that issue. In
particular, we give an explicit expression of the kernel of the local operator
associated with the interface between the FEM and BEM subdomains. That kernel
and the one of classical FEM-BEM couplings are simultaneously non-trivial.

</details>


### [11] [Preconditioning of GMRES for Helmholtz problems with quasimodes](https://arxiv.org/abs/2511.04512)
*Victorita Dolean,Pierre Marchand,Axel Modave,Timothée Raynaud*

Main category: math.NA

TL;DR: GMRES convergence analysis for Helmholtz problems shows how small eigenvalues from quasimodes hinder convergence, and domain decomposition with deflation techniques can improve performance.


<details>
  <summary>Details</summary>
Motivation: Finite element methods for Helmholtz problems create large, indefinite linear systems that are challenging for iterative solvers, especially at high wave numbers or near resonance.

Method: Derived GMRES convergence bound incorporating nonlinear residual behavior and harmonic Ritz values, combined domain decomposition with deflation using approximate eigenvectors for resonant regimes.

Result: Analysis reveals how small eigenvalues from quasimodes affect convergence, and numerical experiments show domain decomposition with tailored deflation improves GMRES performance.

Conclusion: Understanding the relationship between quasimodes, eigenvalues, and GMRES convergence enables effective domain decomposition strategies with deflation to solve challenging Helmholtz problems.

Abstract: Finite element methods are effective for Helmholtz problems involving complex
geometries and heterogeneous media. However, the resulting linear systems are
often large, indefinite, and challenging for iterative solvers, particularly at
high wave numbers or near resonant conditions. We derive a GMRES convergence
bound that incorporates the nonlinear behavior of the relative residual and
relates convergence to harmonic Ritz values. This perspective reveals how small
eigenvalues associated with quasimodes can hinder convergence, and when they
cease to have an effect. These phenomena occur in domain decomposition, and we
illustrate them through numerical experiments. We also combine domain
decomposition methods with deflation techniques using (approximate)
eigenvectors tailored to resonant regimes. Their impact on GMRES performance is
evaluated.

</details>


### [12] [Mixed precision multigrid with smoothing based on incomplete Cholesky factorization](https://arxiv.org/abs/2511.04566)
*Petr Vacek,Hartwig Anzt,Erin Carson,Nils Kohl,Ulrich Rüde,Yu-Hsiang Tsai*

Main category: math.NA

TL;DR: Mixed precision multigrid V-cycle with analysis of finite precision errors, showing that incomplete Cholesky smoothing can use lower precision than other components, achieving up to 1.43x speedup and 71% energy savings.


<details>
  <summary>Details</summary>
Motivation: Multigrid methods are widely used for large-scale linear systems, but their performance can be limited by precision requirements. The paper aims to develop mixed precision formulations to reduce computational costs while maintaining accuracy.

Method: Derived theoretical bounds on finite precision errors in multigrid V-cycle, analyzed incomplete Cholesky smoothing in mixed precision, and conducted numerical experiments using MATLAB Advanpix toolbox and GPU experiments with Ginkgo library.

Result: Theoretical analysis shows IC smoothing can use significantly lower precision than other components. Experiments demonstrate up to 1.43x speedup and 71% energy savings compared to uniform double precision, while maintaining solution accuracy.

Conclusion: Mixed precision multigrid methods with reduced precision for IC smoothing are effective, providing substantial performance and energy benefits without compromising solution quality in appropriate settings.

Abstract: Multigrid methods are popular iterative methods for solving large-scale
sparse systems of linear equations. We present a mixed precision formulation of
the multigrid V-cycle with general assumptions on the finite precision errors
coming from the application of coarsest-level solver and smoothing. Inspired by
existing analysis, we derive a bound on the relative finite precision error of
the V-cycle which gives insight into how the finite precision errors from the
individual components of the method may affect the overall finite precision
error. We use the result to study V-cycle methods with smoothing based on
incomplete Cholesky factorization. The results imply that in certain settings
the precisions used for applying the IC smoothing can be significantly lower
than the precision used for computing the residual, restriction, prolongation
and correction on the concrete level. We perform numerical experiments using
simulated floating point arithmetic with the MATLAB Advanpix toolbox as well as
experiments computed on GPUs using the Ginkgo library. The experiments
illustrate the theoretical findings and show that in the considered settings
the IC smoothing can be applied in relatively low precisions, resulting in
significant speedups (up to 1.43x) and energy savings (down to 71%) in
comparison with the uniform double precision variant.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [On blow-ups of sets with finite fractional variation](https://arxiv.org/abs/2511.03854)
*Giorgio Stefani*

Main category: math.AP

TL;DR: The paper shows that for sets with finite fractional α-variation, if a non-trivial tangent set with integer perimeter exists at almost every point, then a tangent half-space oriented by the fractional unit normal also exists.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric structure of sets with fractional α-variation and establish connections between tangent sets and tangent half-spaces in fractional calculus.

Method: Using measure theory and geometric analysis of sets with locally finite fractional α-variation, analyzing tangent sets and their properties.

Result: Proves that existence of non-trivial tangent sets with integer perimeter implies existence of tangent half-spaces oriented by fractional unit normals for almost every point.

Conclusion: Sets with finite fractional α-variation exhibit regular geometric behavior where tangent structure implies half-space structure at most points.

Abstract: Given $\alpha \in (0,1)$ and a set $E \subset \mathbb R^N$ with locally
finite fractional $\alpha$-variation, we show that for almost every $x \in
\mathbb R^N$ with respect to the $\alpha$-variation measure of $\mathbf 1_E$,
if $E$ admits a non-trivial tangent set at $x$ with locally finite integer
perimeter, then $E$ also admits a tangent half-space oriented by the fractional
unit normal of $E$ at $x$.

</details>


### [14] [Bifurcation analysis of Stokes waves with piecewise smooth vorticity in deep water](https://arxiv.org/abs/2511.03973)
*Changfeng Gui,Jun Wang,Wen Yang,Yong Zhang*

Main category: math.AP

TL;DR: Existence of Stokes waves with piecewise smooth vorticity in 2D infinitely deep fluids, propagating over sheared currents with possible vorticity discontinuities.


<details>
  <summary>Details</summary>
Motivation: To study traveling water waves in semi-infinite cylinders with sheared currents, where vorticity may be discontinuous, extending previous steady water wave analyses.

Method: Hodograph transformation to reformulate free boundary problem into elliptic boundary value problem, height function formulation for transmission problem, and singular bifurcation approach combining global bifurcation theory with Whyburns topological lemma.

Result: Established existence of such waves along global bifurcation branch, showing wave profiles either attain arbitrarily large wave speed or approach horizontal stagnation.

Conclusion: Successfully proved existence of Stokes waves with piecewise smooth vorticity in unbounded domains, addressing novel challenges like internal interfaces and non-Fredholm operators.

Abstract: In this paper, we establish the existence of Stokes waves with piecewise
smooth vorticity in a two-dimensional, infinitely deep fluid domain. These
waves represent traveling water waves propagating over sheared currents in a
semi-infinite cylinder, where the vorticity may exhibit discontinuities. The
analysis is carried out by applying a hodograph transformation, which
reformulates the original free boundary problem into an abstract elliptic
boundary value problem. Compared to previously studied steady water waves, the
present setting introduces several novel features: the presence of an internal
interface, an unbounded spatial domain, and a non-Fredholm linearized operator.
To address these difficulties, we introduce a height function formulation,
casting the problem as a transmission problem with suitable transmission
conditions. A singular bifurcation approach is then employed, combining global
bifurcation theory with Whyburns topological lemma. Along the global
bifurcation branch, we show that the resulting wave profiles either attain
arbitrarily large wave speed or approach horizontal stagnation.

</details>


### [15] [Isocapacitary constants associated with $p$-Laplacian on graphs](https://arxiv.org/abs/2511.04039)
*Bobo Hua,Lili Wang*

Main category: math.AP

TL;DR: Introduces isocapacitary constants for p-Laplacian on graphs to estimate first eigenvalues of Dirichlet p-Laplacian, Neumann p-Laplacian, and p-Steklov problem.


<details>
  <summary>Details</summary>
Motivation: To develop new tools for eigenvalue estimation in graph theory by extending isocapacitary concepts from continuous settings to discrete graphs for p-Laplacian operators.

Method: Define isocapacitary constants specifically for p-Laplacian operators on graphs and apply them to derive analytical estimates.

Result: Obtains eigenvalue estimates for three different p-Laplacian problems: Dirichlet, Neumann, and Steklov boundary conditions on graphs.

Conclusion: Isocapacitary constants provide effective tools for bounding eigenvalues of p-Laplacian operators in various boundary value problems on graphs.

Abstract: In this paper, we introduce isocapacitary constants for the $p$-Laplacian on
graphs and apply them to derive estimates for the first eigenvalues of the
Dirichlet $p$-Laplacian, the Neumann $p$-Laplacian, and the $p$-Steklov
problem.

</details>


### [16] [The Kato problem and extensions for degenerate elliptic operators of higher order in weighted spaces](https://arxiv.org/abs/2511.04046)
*Guoming Zhang*

Main category: math.AP

TL;DR: The paper extends the Kato problem to degenerate elliptic operators of arbitrary order with measurable complex coefficients satisfying Gårding inequality with A₂-weights, generalizing previous work and solving unweighted L^p boundary value problems near p=2.


<details>
  <summary>Details</summary>
Motivation: To generalize the Kato problem and boundary value problem solutions to higher-order degenerate elliptic operators with complex coefficients and A₂-weights, extending previous limited-order results.

Method: Extends the Kato problem framework to arbitrary order 2m degenerate elliptic operators with measurable complex coefficients satisfying Gårding inequality with Muckenhoupt A₂-weights.

Result: Successfully solves the unweighted L^p-Dirichlet, regularity and Neumann boundary value problems for such operators when p is sufficiently close to 2.

Conclusion: The work provides a comprehensive extension of the Kato problem to arbitrary-order degenerate elliptic operators with complex coefficients and A₂-weights, with applications to boundary value problems.

Abstract: We consider the Kato problem and extensions for degenerate elliptic operators
of arbitrary order $2m$ ($m\geq 1$), whose coefficients are measurable,
complex-valued and satisfy the G$\mathring{a}$rding inequality with respect to
a Muckenhoupt $A_{2}$-weight; this generalizes the work of [Cruz-Uribe, Martell
and Rios 2018]. As an application, the unweighted $L^{p}$-Dirichlet, regularity
and Neumann boundary value problems associated to such an operator are solved
when $p$ is sufficiently close to $2.$

</details>


### [17] [A variational Lippmann-Schwinger-type approach for the Helmholtz impedance problem on bounded domains](https://arxiv.org/abs/2511.04056)
*Andreas Tataris,Alexander V. Mamonov*

Main category: math.AP

TL;DR: This paper develops a Lippmann-Schwinger type equation for Helmholtz boundary value problems with variable refractive index and impedance boundary conditions, establishing analytical properties and using it to prove existence of minimizers for inverse problem optimization methods.


<details>
  <summary>Details</summary>
Motivation: To address the loss of structural properties in boundary value formulations compared to scattering problems in unbounded domains, particularly the unavailability of the classical Lippmann-Schwinger integral equation.

Method: Derived a variational Lippmann-Schwinger type equation from the variational formulation of the boundary value problem, establishing analytical properties of the operator and proving weak-to-strong sequential continuity of the parameter-to-state map.

Result: Established that the parameter-to-state map maps weakly convergent sequences to strongly convergent ones for refractive indices in Lebesgue spaces with exponent greater than 2, enabling existence proofs for optimization methods.

Conclusion: The derived Lippmann-Schwinger type equation provides a foundation for proving existence of minimizers in reduced order model based optimization and conventional waveform inversion methods for solving inverse boundary value problems.

Abstract: Recently, reduced order modeling methods have been applied to solving inverse
boundary value problems arising in frequency domain scattering theory. A key
step in projection-based reduced order model methods is the use of a
sesquilinear form associated with the forward boundary value problem. However,
in contrast to scattering problems posed in $\mathbb{R}^d$, boundary value
formulations lose certain structural properties, most notably the classical
Lippmann-Schwinger integral equation is no longer available. In this paper we
derive a Lippmann-Schwinger type equation aimed at studying the solution of a
Helmholtz boundary value problem with a variable refractive index and impedance
boundary conditions. In particular, we start from the variational formulation
of the boundary value problem and we obtain an equivalent operator equation
which can be viewed as a bounded domain analogue of the classical
Lippmann-Schwinger equation. We first establish analytical properties of our
variational Lippmann-Schwinger type operator. Based on these results, we then
show that the parameter-to-state map, which maps a refractive index to the
corresponding wavefield, maps weakly convergent sequences to strongly
convergent ones when restricted to refractive indices in Lebesgue spaces with
exponent greater than 2. Finally, we use the derived weak to strong sequential
continuity to show existence of minimizers for a reduced order model based
optimization methods aimed at solving the inverse boundary value problem as
well as for a conventional data misfit based waveform inversion method.

</details>


### [18] [Fujita exponent for heat equation with Hörmander vector fields](https://arxiv.org/abs/2511.04196)
*Marianna Chatzakou,Aidyn Kassymov,Michael Ruzhansky*

Main category: math.AP

TL;DR: Global existence and non-existence results for heat equations with vector fields satisfying Hörmander's condition and nonlinearities f(u), including critical Fujita exponent calculation for f(u)=u^p.


<details>
  <summary>Details</summary>
Motivation: To study the behavior of solutions to heat equations with nonlinearities and vector fields satisfying Hörmander's condition, determining when solutions exist globally or blow up.

Method: Mathematical analysis of heat equations with squares of smooth vector fields satisfying Hörmander's rank condition, considering nonlinearities f(u) and time-dependent nonlinearities φ(t)f(u).

Result: Established global existence and non-existence results, calculated critical Fujita exponent for f(u)=u^p, and provided necessary conditions for blow-up/sufficient conditions for positive global solutions.

Conclusion: The paper provides comprehensive criteria for determining when solutions to these nonlinear heat equations exist globally or blow up, with specific focus on critical exponents and time-dependent nonlinearities.

Abstract: In this paper, we show global existence and non-existence results for the
heat equation with some of the squares of smooth vector fields on $\Rn$
satisfying H\"{o}rmander's rank condition with a non-linearity of the form
$f(u)$, where $f$ is a suitable function and $u$ is the solution. In
particular, when $f(u)=u^p$, we calculate the critical Fujita exponent. We also
give necessary conditions for blow-up or, alternatively, a sufficient condition
for the existence of positive global solutions for time-dependent
nonlinearities of the type $\varphi(t)f(u)$.

</details>


### [19] [NLS with mass-subcritical combined nonlinearities: small mass $L^2$-scattering](https://arxiv.org/abs/2511.04340)
*Jacopo Bellazzini,Luigi Forcella,Vladimir Georgiev*

Main category: math.AP

TL;DR: Proves small data scattering for mass-subcritical NLS with double nonlinearities (focusing leading term + defocusing lower order term) using pseudo-conformal transformation and variational methods.


<details>
  <summary>Details</summary>
Motivation: To establish scattering results for nonlinear Schrödinger equations with competing nonlinearities, where the focusing term dominates but is balanced by a defocusing perturbation.

Method: Uses pseudo-conformal transformation combined with variational arguments to ensure positivity of modified energies. Only requires smallness in mass norm, not full Σ-norm.

Result: Achieves small data scattering in the mass-subcritical regime for this class of equations with competing nonlinearities.

Conclusion: The approach successfully handles the challenging case of focusing-leading nonlinearities by leveraging the defocusing perturbation and variational techniques.

Abstract: We prove small data scattering in the mass-subcritical regime for the NLS
equation with double nonlinearities, where a focusing leading term is perturbed
by a lower order defocusing nonlinear term. Our proof relies on the
pseudo-conformal transformation in conjunction with a general variational
argument used to obtain the positivity of certain modified energies. Moreover,
the smallness assumption is only on the mass of the initial data, and not on
the whole $\Sigma$-norm.

</details>


### [20] [Modified scattering dynamics in the Vlasov-Poisson equation near an attractive point mass](https://arxiv.org/abs/2511.04363)
*Bernhard Kepka,Klaus Widmayer*

Main category: math.AP

TL;DR: Global unique Lagrangian solutions for Vlasov-Poisson equation with attractive point mass and small particle distribution on hyperbolic Kepler trajectories, showing modified scattering dynamics asymptotically.


<details>
  <summary>Details</summary>
Motivation: To understand long-time behavior of radially symmetric solutions to Vlasov-Poisson equation with attractive point mass and small particle distributions, particularly when particles follow hyperbolic trajectories.

Method: Study radially symmetric solutions with initially localized particle distributions on hyperbolic trajectories for Kepler problem, using low regularity regime without derivative control.

Result: Obtain global in time, unique Lagrangian solutions that asymptotically undergo modified scattering dynamics in the sense of distributions.

Conclusion: The low regularity approach works effectively and can be upgraded to strong solutions with strong convergence through propagation of regularity.

Abstract: We study the long-time behavior of radially symmetric solutions to the
Vlasov-Poisson equation consisting of an attractive point mass and a small,
suitably localized and absolutely continuous distribution of particles: if the
latter is initially localized on hyperbolic trajectories for the associated
Kepler problem, we obtain global in time, unique Lagrangian solutions that
asymptotically undergo a modified scattering dynamics (in the sense of
distributions). A key feature of this result is its low regularity regime,
which does not make use of derivative control, but can be upgraded to strong
solutions and strong convergence by propagation of regularity.

</details>


### [21] [2D Navier-Stokes with Navier Slip: Strong Vorticity Convergence and Strong Solutions for Unbounded Vorticity](https://arxiv.org/abs/2511.04368)
*Josef Demmel,Emil Wiedemann*

Main category: math.AP

TL;DR: The paper shows strong convergence of vorticity in the vanishing viscosity limit for 2D incompressible Navier-Stokes equations with Navier boundary conditions, using an interior framework to upgrade local to global convergence.


<details>
  <summary>Details</summary>
Motivation: To analyze the behavior of 2D Navier-Stokes equations with Navier boundary conditions and establish strong convergence results for vorticity in the vanishing viscosity limit.

Method: Utilizes a purely interior framework originally developed for no-slip conditions, studies the Laplacian with Navier boundary conditions, and proves ellipticity in the Agmon-Douglis-Nirenberg sense.

Result: Shows strong convergence of vorticity from initial L^p data (p>2) and proves that velocity becomes a strong solution satisfying Navier slip conditions for any positive time.

Conclusion: The key insight is establishing the ellipticity of the boundary-value problem with Navier conditions, enabling the upgrade from local to global convergence results.

Abstract: We analyze the two-dimensional incompressible Navier-Stokes equations on a
smooth, bounded domain with Navier boundary conditions. Starting from an
initial vorticity in $L^p$ with $p>2$, we show strong convergence of the
vorticity in the vanishing viscosity limit. We utilize a purely interior
framework from Seis, Wiedemann, and Wo\'{z}nicki, originally derived for
no-slip, and upgrade local to global convergence. Under the same assumptions,
we also show that the velocity is in fact a strong solution and satisfies the
Navier slip conditions for any positive time. The key idea is to study the
Laplacian subject to Navier boundary conditions and prove that this
boundary-value problem is elliptic in the sense of Agmon-Douglis-Nirenberg.

</details>


### [22] [Lecture notes on Quantum Diffusion and Random Matrix Theory](https://arxiv.org/abs/2511.04380)
*Felipe Hernández*

Main category: math.AP

TL;DR: New approach using random matrix theory to understand diffusive limit of random Schrödinger equation, presented in simplified lecture notes.


<details>
  <summary>Details</summary>
Motivation: To develop a novel methodology for analyzing the diffusive limit of the random Schrödinger equation by leveraging concepts from random matrix theory.

Method: Application of random matrix theory ideas to study the diffusive behavior in random Schrödinger equations.

Result: Development of a new analytical framework that provides insights into the diffusive limit behavior.

Conclusion: The approach offers a fresh perspective on understanding diffusion in random Schrödinger systems through random matrix theory techniques.

Abstract: In joint work with Adam Black and Reuben Drogin, we develop a new approach to
understanding the diffusive limit of the random Schrodinger equation based on
ideas taken from random matrix theory. These lecture notes present the main
ideas from this work in a self-contained and simplified presentation. The
lectures were given at the summer school "PDE and Probability" at Sorbonne
Universit\'e from June 16-20, 2025.

</details>


### [23] [Regularizing effect of the interplay between coefficients in linear and semilinear $X$-elliptic equations](https://arxiv.org/abs/2511.04447)
*Paolo Malanchini,Giovanni Molica Bisci,Simone Secchi*

Main category: math.AP

TL;DR: The paper studies the regularizing effect in elliptic PDEs with zero-order terms, proving existence and boundedness of solutions under specific conditions between the coefficient and data.


<details>
  <summary>Details</summary>
Motivation: To understand how the interaction between the zero-order coefficient a(x) and the forcing term f(x) can regularize solutions in degenerate elliptic equations, extending previous results to X-elliptic operators.

Method: Analysis of degenerate elliptic PDEs using X-elliptic operators framework, applying the Q-condition introduced by Arcoya and Boccardo to prove existence and boundedness.

Result: Proved that the Q-condition is sufficient for existence and boundedness of solutions when f ∈ L¹(Ω), and established existence of bounded solutions for linear problems under more general conditions.

Conclusion: The interaction between the zero-order coefficient and forcing term provides a regularizing effect that ensures bounded solutions even for L¹ data, extending classical results to degenerate elliptic operators.

Abstract: We study the regularizing effect arising from the interaction between the
coefficient \(a\) of the zero order term and the datum \(f\) in the problem $$
\left\lbrace \begin{array}{ll}
  -\mathcal{L}u + a(x) g(u) = f(x) \quad &\mbox{in} \;\; \Omega,
  u = 0 \quad &\mbox{on} \;\; \partial\Omega, \end{array} \right. $$ where
$\Omega\subseteq\mathbb{R}^N$ is a bounded domain and $\mathcal{L}$ is an
$X$-elliptic operator introduced by Lanconelli and Kogoj. If $f \in
L^1(\Omega)$, we prove that the \(Q\)-condition introduced by Arcoya and
Boccardo is sufficient to ensure the existence and boundedness of solutions in
the framework of $X$-elliptic operators as well. Finally, we prove the
existence of a bounded solution for linear problems under a more general
condition between $f$ and $a$.

</details>


### [24] [Finite time blow-up for a multi-dimensional model of the Kiselev-Sarsam equation](https://arxiv.org/abs/2511.04660)
*Wanwan Zhang*

Main category: math.AP

TL;DR: The paper proposes a multi-dimensional nonlocal active scalar equation that generalizes the Kiselev-Sasarm equation, proving local well-posedness and finite-time gradient blow-up for certain initial data.


<details>
  <summary>Details</summary>
Motivation: To develop a natural multi-dimensional generalization of the Kiselev-Sasarm equation, which serves as a one-dimensional model for the two-dimensional incompressible porous media equation.

Method: The authors study a multi-dimensional nonlocal active scalar equation with a specific transform operator ℐ_a, analyzing its mathematical properties through theoretical analysis.

Result: The paper establishes local well-posedness for the multi-dimensional model and demonstrates that gradient blow-up occurs in finite time for a class of initial data.

Conclusion: The proposed multi-dimensional nonlocal active scalar equation successfully generalizes the Kiselev-Sasarm model while exhibiting both well-posedness and finite-time singularity formation properties.

Abstract: In this paper, we propose and study a multi-dimensional nonlocal active
scalar equation of the form \begin{eqnarray*}
\partial_t\rho+g\mathcal{R}_a\rho\cdot \nabla\rho= 0,~\rho(\cdot,0)=\rho_{0},
\end{eqnarray*} where the transform $\mathcal{R}_a$ is defined by
\begin{eqnarray*}
\mathcal{R}_af(x)=\frac{\Gamma(\frac{n+1}{2})}{\pi^{\frac{n+1}{2}}}P.V.\int\limits_{\mathbb{R}^n}\Big(\frac{x-y}{|x-y|^{n+1}}-\frac{x-y}{(|x-y|^2+a^2)^{\frac{n+1}{2}}}\Big)f(y)dy.
\end{eqnarray*} This model can be viewed as a natural generalization of the
well-known Kiselev-Sasarm equation, which was introduced in [14] as a
one-dimensional model for the two-dimensional incompressible porous media
equation. We show the local well-posedness for this multi-dimensional model as
well as the gradient blow-up in finite time for a class of initial data.

</details>


### [25] [A priori estimates and $η-$compactness for anisotropic Ginzburg-Landau minimizers with tangential anchoring](https://arxiv.org/abs/2511.04672)
*Lia Bronsard,Andrew Colinet,Dominik Stantejsky,Lee van Brussel*

Main category: math.AP

TL;DR: Analysis of Ginzburg-Landau energy minimizers with divergence/curl penalization on 2D domains, showing uniform L∞ bounds, Lipschitz blowup, and convergence to S¹-valued maps with specific defect patterns.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of Ginzburg-Landau energy minimizers under quadratic divergence or curl penalization with strong tangential boundary anchoring, particularly focusing on defect formation and boundary behavior.

Method: Proving a priori estimates in L∞ uniform in ε, analyzing Lipschitz constant blowup (∼ε⁻¹), and studying compactness properties for subsequences converging to S¹-valued maps.

Result: Minimizers converge to S¹-valued maps with either one interior point defect or two boundary half-defects. No boundary vortices occur in the divergence penalized case.

Conclusion: The penalization type determines defect patterns: divergence penalization prevents boundary vortices, while both cases lead to specific defect configurations (interior point or boundary half-defects) in the limit.

Abstract: We consider minimizers $u_\varepsilon$ of the Ginzburg-Landau energy with
quadratic divergence or curl penalization on a simply-connected two-dimensional
domain $\Omega$. On the boundary, strong tangential anchoring is imposed. We
prove a priori estimates for $u_\varepsilon$ in $L^\infty$ uniform in
$\varepsilon$ and that the Lipschitz constant of $u_\varepsilon$ blows up like
$\varepsilon^{-1}$. We then deduce compactness for a subsequence that converges
to an $\mathbb{S}^1-$valued map with either one interior point defect or two
boundary half-defects. We conclude our study with a proof that no boundary
vortices can occur in the divergence penalized case.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [26] [Robust Subgroup Method Using DE Algorithm for Resonance Self-Shielding Calculation](https://arxiv.org/abs/2511.04062)
*Beichen Zheng,Ying Chen,Lili Wen,Xiaofei Wu*

Main category: physics.comp-ph

TL;DR: Enhanced subgroup method combining Robust Estimation and Differential Evolution to remove systematic absorption bias in resonance self-shielding, improving predictive accuracy in transport simulations.


<details>
  <summary>Details</summary>
Motivation: To address systematic absorption bias in conventional subgroup methods that depresses reactivity, particularly in benchmarks sensitive to U-238, caused by threshold-like conditioning failures and dilution-induced multicollinearity.

Method: Integrates Robust Estimation (RE) to handle model misspecification and data contamination, with Differential Evolution (DE) algorithm as optimization tool within RE framework to obtain constrained solutions.

Result: Removes systematic absorption bias that appears only in U-238 sensitive benchmarks, improves parameter inference to better track underlying physics, and enhances predictive fidelity of transport simulations.

Conclusion: The robust subgroup method successfully addresses conditioning failures in conventional approaches by bounding influence and enforcing feasibility, leading to more accurate physics modeling and improved simulation predictions.

Abstract: This paper presents an enhanced version of the subgroup method for resonance
self-shielding treatment, termed the robust subgroup method, which integrates
Robust Estimation (RE) with a Differential Evolution (DE) algorithm. The RE
approach is employed to handle model misspecification and data contamination,
while the DE algorithm serves as an optimization tool within the RE framework
to obtain constrained solutions. Numerical validation against experimental
benchmarks shows that the proposed method removes a systematic absorption bias
in conventional subgroup fits that would otherwise depress reactivity. This
bias appears only in benchmarks sensitive to U-238. Mechanistically, it
reflects a threshold-like conditioning failure: strong self-shielding leverage
dominates the loss and is magnified by dilution-induced multicollinearity. This
adverse conditioning appears to be seeded by a narrow, sparse resonance
structure at low energies in fertile even-even nuclides, thereby causing rapid
self-shielding response saturation and a weak Doppler broadening. By bounding
influence and enforcing feasibility within an RE-DE framework, the inferred
subgroup parameters track the underlying physics more faithfully, improving the
predictive fidelity of subsequent transport simulations.

</details>


### [27] [Novel Numerical Methods for Accurate Space Thermal Analysis: Enforcing View Factors and Modeling Diffuse Reflectivity](https://arxiv.org/abs/2511.04277)
*Bernat Frangi*

Main category: physics.comp-ph

TL;DR: This research improves space thermal modeling by developing novel methods to enforce closure and reciprocity for radiative heat transfer in open systems, incorporating diffuse reflectivity using the Gebhart method, and introducing multi-node surface model relations for consistent discretization.


<details>
  <summary>Details</summary>
Motivation: Accurate thermal analysis is crucial for modern spacecraft, but existing correction schemes cannot simultaneously enforce closure and reciprocity for open systems in radiative heat transfer modeling, creating a gap in reliable thermal modeling tools.

Method: Proposed two novel enforcement methods: (1) least-squares optimization with non-negativity rectification and small positive value avoidance, and (2) an iterative enforcement algorithm. Also introduced multi-node surface model relations to formalize connections between different discretization levels of view factors and radiative exchange factors.

Result: Case studies showed substantial error reduction: least-squares method achieved 81% MAE reduction, iterative method offered best balance with 56% MAE reduction and computational efficiency. Including diffuse reflections decreased steady-state temperature by 4°C, showing reduced net absorption.

Conclusion: This work validates computationally efficient methods for integrating diffuse reflectivity into space thermal analyses and consistently coupling multi-node surface radiative models, enabling more accurate and robust thermal predictions for spacecraft systems.

Abstract: Accurate thermal analysis is crucial for modern spacecraft, driving demand
for reliable modeling tools. This research advances space thermal modeling by
improving the simulation accuracy and efficiency of radiative heat transfer,
the dominant mode of heat exchange in space. To this end, we incorporate
diffuse reflectivity using the Gebhart method, which computes radiative
exchange factors (REFs) from geometric view factors. The view factors, obtained
via Monte Carlo ray tracing (MCRT), require post-processing to mitigate
statistical errors. Critically, existing correction schemes cannot
simultaneously enforce closure and reciprocity for open systems. This research
addresses this gap by proposing two novel enforcement methods: (i) a
least-squares optimization with non-negativity rectification (NNR) and small
positive value avoidance (SPVA), and (ii) an iterative enforcement algorithm.
To ensure consistency across different discretization levels, this work also
introduces the multi-node surface model relations to formalize the connection
between sub-face, face, and node representations of view factors and REFs. A
simple case study demonstrates a substantial reduction in mean absolute error
(MAE): the least-squares method achieves an 81% MAE reduction, while the
iterative method offers the best balance of accuracy (56% MAE reduction) and
computational efficiency. A second case study shows that including diffuse
reflections decreases the steady-state temperature of a plate by $4^{\circ}C$,
reinforcing that reflected radiation reduces net absorption. This work
introduces and validates computationally efficient methods for integrating
diffuse reflectivity into space thermal analyses and for consistently coupling
multi-node surface radiative models. The results enable more accurate and
robust thermal predictions for spacecraft systems.

</details>


### [28] [Unveiling the Adsorption and Electronic Interactions of Drugs on 2D Graphsene: Insights from DFT and Machine Learning Approach](https://arxiv.org/abs/2511.04483)
*Chaithanya Purushottam Bhat,Pranav Suryawanshi,Aditya Guneja,Debashis Bandyopadhyay*

Main category: physics.comp-ph

TL;DR: A synergistic DFT-ML framework for predicting drug adsorption on Graphsene, achieving high accuracy in identifying promising drug candidates for nanomaterial-based delivery systems.


<details>
  <summary>Details</summary>
Motivation: Efficient identification of promising drug candidates for nanomaterial-based delivery systems is essential for advancing next-generation therapeutics.

Method: Combined density functional theory (DFT) and machine learning (ML) using a dataset of 67 drugs on 2D substrates; DFT analyses included adsorption energetics, PDOS, and Bader charge calculations.

Result: ML model achieved mean absolute error of 0.075 eV upon DFT validation; DFT revealed pronounced charge transfer and electronic coupling between drugs and Graphsene surface.

Conclusion: The integrated DFT-ML strategy offers a rapid, cost-efficient approach for screening drug-nanomaterial interactions, enabling data-driven design of advanced drug delivery systems.

Abstract: Efficient identification of promising drug candidates for nanomaterial-based
delivery systems is essential for advancing next-generation therapeutics. In
this work, we present a synergistic framework combining density functional
theory (DFT) and machine learning (ML) to explore the adsorption behavior and
electronic interactions of drugs on a novel 2D graphene allotrope, termed
Graphsene (GrS). Graphsene, characterized by its porous ring topology and large
surface area, offers an excellent platform for efficient adsorption and strong
electronic coupling with drug molecules. A dataset comprising 67 drugs adsorbed
on various 2D substrates was employed to train the ML model, which was
subsequently applied to predict suitable drug candidates for GrS based on
molecular size and adsorption energy criteria (database link provided in a
later section). The ML model exhibited robust predictive accuracy, achieving a
mean absolute error of 0.075 eV upon DFT validation, though its sensitivity to
initialization highlighted the need for larger and more diverse datasets.
DFT-based analyses, including adsorption energetics, projected density of
states (PDOS), and Bader charge calculations, revealed pronounced charge
transfer and electronic coupling between the drug molecules and the GrS
surface, elucidating the fundamental nature of drug-substrate interactions. The
study reveals that the integrated DFT-ML strategy offers a rapid,
cost-efficient approach for screening and understanding drug-nanomaterial
interactions, paving the way for data-driven design of advanced
nanomaterial-enabled drug delivery systems.

</details>


### [29] [Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear Fusion](https://arxiv.org/abs/2511.04489)
*Oskar Lappi,Huw Leggate,Yannick Marandet,Jan Åström,Keijo Heljanko,Dmitriy V. Borodin*

Main category: physics.comp-ph

TL;DR: Eiron implements domain decomposition for Monte Carlo neutral transport, enabling simulations that exceed single-node memory limits and showing superior scaling compared to existing parallel algorithms.


<details>
  <summary>Details</summary>
Motivation: EIRENE lacks domain decomposition, preventing simulations where grid data doesn't fit on one compute node, limiting its applicability for large-scale fusion simulations.

Method: Implemented a domain-decomposed Monte Carlo (DDMC) algorithm in new code Eiron, compared with two existing EIRENE parallel algorithms through strong scaling tests on supercomputer Mahti.

Result: DDMC outperformed other algorithms in most cases, showed superlinear strong scaling for grids >4 MiB, and achieved 45% weak scaling efficiency on 16384 cores for high-collisional cases (26% for low-collisional).

Conclusion: Implementing domain decomposition in EIRENE would improve performance and enable currently impossible simulations due to memory constraints.

Abstract: EIRENE [1] is a Monte Carlo neutral transport solver heavily used in the
fusion community. EIRENE does not implement domain decomposition, making it
impossible to use for simulations where the grid data does not fit on one
compute node (see e.g. [2]). This paper presents a domain-decomposed Monte
Carlo (DDMC) algorithm implemented in a new open source Monte Carlo code,
Eiron. Two parallel algorithms currently used in EIRENE are also implemented in
Eiron, and the three algorithms are compared by running strong scaling tests,
with DDMC performing better than the other two algorithms in nearly all cases.
On the supercomputer Mahti [3], DDMC strong scaling is superlinear for grids
that do not fit into an L3 cache slice (4 MiB). The DDMC algorithm is also
scaled up to 16384 cores in weak scaling tests, with a weak scaling efficiency
of 45% in a high-collisional (heavier compute load) case, and 26% in a
low-collisional (lighter compute load) case. We conclude that implementing this
domain decomposition algorithm in EIRENE would improve performance and enable
simulations that are currently impossible due to memory constraints.

</details>


### [30] [Uncertainties in Physics-informed Inverse Problems: The Hidden Risk in Scientific AI](https://arxiv.org/abs/2511.04564)
*Yoh-ichi Mototake,Makoto Sasaki*

Main category: physics.comp-ph

TL;DR: A framework for quantifying uncertainties in physics-informed machine learning (PIML) when estimating coefficient functions, addressing the issue that predictive accuracy alone is insufficient for selecting physically meaningful solutions.


<details>
  <summary>Details</summary>
Motivation: Physics doesn't rely solely on prediction accuracy for model evaluation (as shown by Kepler's heliocentric vs geocentric models), highlighting uncertainties in data-driven model inference and the need to select physically meaningful solutions.

Method: Proposed framework to quantify and analyze uncertainties in coefficient function estimation in PIML, incorporating geometric constraints to enable unique identification.

Result: Applied to reduced magnetohydrodynamics model, showing uncertainties exist but unique identification is possible with geometric constraints.

Conclusion: Confirmed that reduced models can be uniquely estimated by incorporating geometric constraints, addressing uncertainty in physics-informed machine learning.

Abstract: Physics-informed machine learning (PIML) integrates partial differential
equations (PDEs) into machine learning models to solve inverse problems, such
as estimating coefficient functions (e.g., the Hamiltonian function) that
characterize physical systems. This framework enables data-driven understanding
and prediction of complex physical phenomena. While coefficient functions in
PIML are typically estimated on the basis of predictive performance, physics as
a discipline does not rely solely on prediction accuracy to evaluate models.
For example, Kepler's heliocentric model was favored owing to small
discrepancies in planetary motion, despite its similar predictive accuracy to
the geocentric model. This highlights the inherent uncertainties in data-driven
model inference and the scientific importance of selecting physically
meaningful solutions. In this paper, we propose a framework to quantify and
analyze such uncertainties in the estimation of coefficient functions in PIML.
We apply our framework to reduced model of magnetohydrodynamics and our
framework shows that there are uncertainties, and unique identification is
possible with geometric constraints. Finally, we confirm that we can estimate
the reduced model uniquely by incorporating these constraints.

</details>


### [31] [Combining Harmonic Sampling with the Worm Algorithm to Improve the Efficiency of Path Integral Monte Carlo](https://arxiv.org/abs/2511.04597)
*Sourav Karmakar,Sutirtha Paul,Adrian Del Maestro,Barak Hirshberg*

Main category: physics.comp-ph

TL;DR: Proposed improved Path Integral Monte Carlo algorithms (H-PIMC and M-PIMC) that separate harmonic and anharmonic potential contributions to overcome low acceptance ratios in quantum solids and dense liquids.


<details>
  <summary>Details</summary>
Motivation: Standard PIMC suffers from low acceptance ratios for solids and dense confined liquids, limiting its effectiveness for quantum condensed phases.

Method: Developed two sampling schemes: H-PIMC generates paths exactly for harmonic potential and accepts/rejects based on anharmonic part; M-PIMC restricts harmonic sampling near local minima and uses standard PIMC elsewhere. Combined with worm algorithm for indistinguishable particles.

Result: For weakly to moderately anharmonic systems at βℏω=16: 6-16x improvement in acceptance ratio, 7-30x reduction in autocorrelation time, and 2-4x acceleration from fewer imaginary time slices needed. M-PIMC works well for strongly anharmonic systems and periodic systems.

Conclusion: The proposed H-PIMC and M-PIMC methods significantly improve efficiency for quantum condensed phase simulations, particularly for systems with harmonic character, while maintaining effectiveness for strongly anharmonic systems.

Abstract: We propose an improved Path Integral Monte Carlo (PIMC) algorithm called
Harmonic PIMC (H-PIMC) and its generalization, Mixed PIMC (M-PIMC). PIMC is a
powerful tool for studying quantum condensed phases. However, it often suffers
from a low acceptance ratio for solids and dense confined liquids. We develop
two sampling schemes especially suited for such problems by dividing the
potential into its harmonic and anharmonic contributions. In H-PIMC, we
generate the imaginary time paths for the harmonic part of the potential
exactly and accept or reject it based on the anharmonic part. In M-PIMC, we
restrict the harmonic sampling to the vicinity of local minimum and use
standard PIMC otherwise, to optimize efficiency. We benchmark H-PIMC on systems
with increasing anharmonicity, improving the acceptance ratio and lowering the
auto-correlation time. For weakly to moderately anharmonic systems, at $\beta
\hbar \omega=16$, H-PIMC improves the acceptance ratio by a factor of 6-16 and
reduces the autocorrelation time by a factor of 7-30. We also find that the
method requires a smaller number of imaginary time slices for convergence,
which leads to another two- to four-fold acceleration. For strongly anharmonic
systems, M-PIMC converges with a similar number of imaginary time slices as
standard PIMC, but allows the optimization of the auto-correlation time. We
extend M-PIMC to periodic systems and apply it to a sinusoidal potential.
Finally, we combine H- and M-PIMC with the worm algorithm, allowing us to
obtain similar efficiency gains for systems of indistinguishable particles.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [32] [Modeling of Injected Current Stream-Induced 3D Perturbations in Local Helicity Injection Plasmas](https://arxiv.org/abs/2511.03930)
*C. E. Schaefer,A. C. Sontag,N. M. Ferraro,J. D. Weberski,S. J. Diem*

Main category: physics.plasm-ph

TL;DR: Local helicity injection (LHI) for solenoid-free tokamak startup causes magnetic topology degradation in Pegasus-III plasmas, with rotation and two-fluid physics playing crucial roles in screening perturbations.


<details>
  <summary>Details</summary>
Motivation: Solenoid-free startup techniques like LHI are essential for spherical tokamaks to reduce costs and simplify fusion energy system designs, but their effects on magnetic topology need investigation.

Method: Used helical filament model for injected current, calculated linear plasma response with M3D-C1, performed Poincaré mapping, and compared single-fluid vs two-fluid models with and without rotation.

Result: LHI causes substantial flux surface degradation starting at Ψ_N ≈ 0.37, with rotation providing partial screening of n=1 perturbations. Two-fluid models show stronger edge suppression, while absence of rotation leads to resonant field amplification.

Conclusion: Rotation and two-fluid physics are critical for screening LHI perturbations. Future work requires plasma flow measurements and refined stream models to improve predictive capability.

Abstract: Solenoid-free tokamak startup techniques are essential for spherical tokamaks
and offer a pathway to cost reduction and design simplification in fusion
energy systems. Local helicity injection (LHI) is one such approach, employing
compact edge current sources to drive open field line current that initiates
and sustains tokamak plasmas. The recently commissioned Pegasus-III spherical
tokamak provides a platform for advancing this and other solenoid-free startup
methods. This study investigates the effect of LHI on magnetic topology in
Pegasus-III plasmas. A helical filament model represents the injected current,
and the linear plasma response to its 3D field is calculated with M3D-C1.
Poincar\'e mapping reveals substantial flux surface degradation in all modeled
cases. The onset of overlapping magnetic structures and large-scale surface
deformation begins at $\Psi_{N} \approx 0.37$, indicating a broad region of
perturbed topology extending toward the edge. In rotating plasmas, both
single-fluid and two-fluid models exhibit partial screening of the $n = 1$
perturbation, with two-fluid calculations showing stronger suppression near the
edge. In contrast, the absence of rotation leads to strong resonant field
amplification in the single-fluid case, while the two-fluid case with zero
electron rotation mitigates this amplification and preserves edge screening.
Magnetic probe measurements indicate that modeling the stream with spatial
spreading$-$representing distributed current and/or oscillatory motion$-$better
reproduces measured magnetic power profiles than a rigid filament model. The
results underscore the role of rotation and two-fluid physics in screening
stream perturbations and point to plasma flow measurements and refined stream
models as key steps toward improving predictive fidelity.

</details>


### [33] [Electromagnetic turbulence in EAST plasmas with internal transport barrier](https://arxiv.org/abs/2511.04044)
*Yuehao Ma,Pengfei Liu,Jian Bao,Zhihong Lin,Huishan Cai*

Main category: physics.plasm-ph

TL;DR: Global gyrokinetic simulations show electromagnetic effects suppress ITG turbulence in EAST tokamak ITB region, reducing heat transport by factor of 4 through enhanced zonal flow shearing.


<details>
  <summary>Details</summary>
Motivation: To investigate turbulence in Internal Transport Barrier (ITB) region of EAST tokamak with weakly reversed magnetic shear, particularly the role of electromagnetic effects on ion temperature gradient (ITG) modes.

Method: Global nonlinear electromagnetic gyrokinetic simulations with linear analysis to identify dominant ITG modes at different radial positions under electrostatic and electromagnetic conditions.

Result: Finite β effects suppress higher frequency ITG modes at q=1 surface; electromagnetic nonlinear regime reduces thermal ion heat conductivity by factor of 4; energetic particles provide slight stabilizing effect.

Conclusion: Electromagnetic effects are crucial for stabilizing ITG modes and accurately calculating transport coefficients in weakly reversed magnetic shear configurations.

Abstract: In this study, global nonlinear electromagnetic gyrokinetic simulations are
conducted to investigate turbulence in the Internal transport barrier (ITB)
region of the EAST tokamak discharge with weakly reversed magnetic shear.
Linear simulations reveal two dominant ion temperature gradient (ITG) modes: a
higher frequency mode at the $q=1$ surface, which dominates in the
electrostatic limit, and a lower frequency mode near the $q_{\min}$ surface,
which prevails under the experimental $\beta$ (the ratio of plasma pressure to
magnetic pressure). Finite $\beta$ effects effectively suppress higher
frequency ITG modes, and once $\beta_i$ on axis exceeds 0.5\%, this ITG mode is
no longer dominant, and the ITG mode near $q_{\min}$ surface becomes the
primary instability. Therefore, electromagnetic effects play a crucial role in
stabilizing ITG modes, and in causing the transition between the most unstable
mode at different radial positions. The linear growth rate of the unstable mode
in the electrostatic limit is approximately 1.25 times higher than that of the
dominant mode in the electromagnetic case. However, in the electromagnetic
nonlinear regime, the thermal ion heat conductivity is reduced by at least a
factor of 4. This reduction primarily results from nonlinear electromagnetic
effects enhancing the shearing effect of zonal flows, thereby further
suppressing microturbulence. Finally, energetic particles exert a slight
stabilizing effect on ITG turbulence due to dilution and finite $\beta$
contributions. It is emphasized that the electromagnetic effect on ITG with
weak magnetic shear should be included to accurately calculate the transport
coefficients.

</details>


### [34] [Cross-scale Interaction between Microturbulence and Fishbone in Fusion Plasmas](https://arxiv.org/abs/2511.04051)
*Yuehao Ma,Bin Zhang,Pengfei Liu,Jian Bao,Zhihong Lin,Huishan Cai,Liutian Gao,AhDi Liu,Hailin Zhao,Tao Zhang*

Main category: physics.plasm-ph

TL;DR: Global gyrokinetic simulations reveal fishbone instability suppresses electromagnetic ITG turbulence via zonal radial electric fields, reducing ion thermal transport to neoclassical levels.


<details>
  <summary>Details</summary>
Motivation: To investigate cross-scale interactions between electromagnetic ion temperature gradient (ITG) turbulence and fishbone instability in tokamak plasmas, as understanding these multiscale interactions can enhance thermal confinement in fusion plasmas.

Method: Performed global gyrokinetic simulations for the first time to study cross-scale interactions between electromagnetic ITG turbulence and fishbone instability in tokamak plasmas.

Result: Fishbone-driven zonal radial electric fields at nonlinear saturation significantly suppress electromagnetic ITG turbulence, reducing ion thermal transport close to the neoclassical level. Results agree with experimental observations of turbulence suppression during fishbone bursts.

Conclusion: These findings advance understanding of multiscale interactions that enhance thermal confinement in fusion plasmas, showing fishbone instability can beneficially suppress turbulent transport.

Abstract: Global gyrokinetic simulations are performed for the first time to
investigate cross-scale interactions between electromagnetic ion temperature
gradient (ITG) turbulence and fishbone instability in tokamak plasmas. The
investigation of fluctuation response in the multiscale simulation including
both instabilities indicates a strong impact of fishbone on ITG turbulence.
Detailed analysis reveals that fishbone-driven zonal radial electric fields at
nonlinear saturation significantly suppress electromagnetic ITG turbulence,
reducing ion thermal transport close to the neoclassical level. The simulation
results agree well with experimental observations that turbulence suppression
during fishbone bursts. These findings advance understanding of multiscale
interactions that enhance thermal confinement in fusion plasmas.

</details>


### [35] [Stochastic simulation of partial discharge inception](https://arxiv.org/abs/2511.04356)
*Jannis Teunissen,Yuting Gao*

Main category: physics.plasm-ph

TL;DR: A Monte Carlo method for simulating electric discharge inception in gases using electrostatic field data on unstructured grids, estimating discharge probability and time lag.


<details>
  <summary>Details</summary>
Motivation: To develop a computational model that can predict electric discharge inception probabilities and timing across various electrode geometries, including regions with sub-critical electric fields.

Method: Simulates electron avalanches propagating along field lines with photon and ion feedback mechanisms, using statistical distributions for avalanche size that account for electron attachment effects.

Result: The model successfully estimates discharge inception probabilities and time lags, validated against particle simulations, and demonstrates applicability in 2D Cartesian, 2D axisymmetric, and 3D electrode configurations.

Conclusion: The presented Monte Carlo method provides an effective computational approach for predicting electric discharge inception across diverse geometries, with validation against particle simulations confirming its accuracy.

Abstract: We present a Monte Carlo method for simulating the inception of electric
discharges in gases. The input consists of an unstructured grid containing the
electrostatic field. The output of the model is the estimated probability of
discharge inception per initial electron position, as well as the estimated
time lag between the appearance of the initial electron and discharge
inception. To obtain these quantities electron avalanches are simulated for
initial electron positions throughout the whole domain, also including regions
below the critical electric field. Avalanches are assumed to propagate along
field lines, and they can produce additional avalanches due to photon and ion
feedback. If the number of avalanches keeps increasing over time we assume that
an electric discharge will eventually form. A statistical distribution for the
electron avalanche size is used, which is also valid for gases with strong
electron attachment. We compare this distribution against the results of
particle simulations. Furthermore, we demonstrate examples of inception
simulations in 2D Cartesian, 2D axisymmetric and 3D electrode geometries.

</details>


### [36] [Lightning-Induced Faults in Low-Voltage Distribution Networks via Hybrid VTS-PEEC Method](https://arxiv.org/abs/2511.04441)
*Xiaobing Xiao,Xipeng Chen,Lei Jia,Huaifei Chen,Lu Qu,Chakhung Yeung*

Main category: physics.plasm-ph

TL;DR: Analysis of lightning-induced faults in low-voltage distribution networks using hybrid VTS-PEEC method, focusing on how lightning stroke location affects overvoltage and fault risk.


<details>
  <summary>Details</summary>
Motivation: Low-voltage distribution networks are critical for grid stability but face significant threats from lightning-induced faults, requiring economical simulation methods for investigation.

Method: Hybrid Variable Time Step (VTS)-Partial Element Equivalent Circuit (PEEC) method for Lightning-induced Electromagnetic Pulse (LEMP) simulation and fault analysis in extended unequal-length double-circuit networks.

Result: Ground strokes near circuit centers produce similar three-phase negative and bipolar oscillatory waveforms linked to fault initiation. Closer strokes generate bipolar waveforms with negative main peaks, higher overvoltages, and increased fault risk.

Conclusion: The study provides essential insights into lightning-induced fault mechanisms, forming a foundation for developing more targeted and effective lightning protection measures.

Abstract: As a critical component of power supply systems, low-voltage distribution
net-works directly affect grid stability and user power supply reliability, yet
they face significant threats from lightning-induced faults. Transient
simulations are more economical and adaptable for investigating
lightning-induced faults in low-voltage distribution networks than experiments.
A hybrid Variable Time Step (VTS)-Partial Element Equivalent Circuit (PEEC)
method, has been validat-ed in previous study, is used for Lightning-induced
Electromagnetic Pulse (LEMP) simulation and fault analysis. The
lightning-induced faults in ex-tended unequal-length double-circuit low-voltage
distribution networks are ana-lyzed in this paper. The impact of lightning
stroke location on overvoltage and fault risk is the primary focus of this
study. Key findings indicate that, for ground strokes in front of the center of
one double circuit, similar three-phase negative and bipolar oscillatory
waveforms that are linked to fault initiation emerge. Closer strokes promote
bipolar waveforms with the main peak negative as well as higher overvoltages
and fault risk. These results provide essential insights for under-standing
lightning-induced fault mechanisms, thereby laying a foundation for formulating
more targeted and effective lightning protection measures.

</details>


### [37] [Approaching the thermodynamic limit of a bounded one-component plasma](https://arxiv.org/abs/2511.04516)
*D. I. Zhukhovitskii,E. E. Perevoshchikov*

Main category: physics.plasm-ph

TL;DR: Molecular dynamics study of spherical one-component plasma establishes accurate thermodynamic limit energies and proposes improved force cutoff for better phase transition modeling.


<details>
  <summary>Details</summary>
Motivation: To accurately determine thermodynamic properties of bounded one-component plasma and improve simulation methods by establishing size dependencies and better force cutoffs.

Method: Used molecular dynamics simulations of spherical bounded one-component plasma with size extrapolation to thermodynamic limit, and introduced new characteristic energies for compressibility calculation.

Result: Obtained total electrostatic energy per ion with 0.1% relative error across wide coupling range (Γ=0.03-1000), showing 0.5% lower values than MC at Γ<30 and agreement at Γ>175. Derived ionic equation of state and improved force cutoff.

Conclusion: The study provides accurate thermodynamic limit data for OCP, enables calculation of previously inaccessible compressibility, and demonstrates that phase transition location depends sensitively on force cutoff radius.

Abstract: The classical one-component plasma (OCP) bounded by a spherical surface
reflecting ions (BOCP) is studied using molecular dynamics (MD). Simulations
performed for a series of sufficiently large BOCP's make it possible to
establish the size dependencies for the investigated quantities and extrapolate
them to the thermodynamic limit. In particular, the total electrostatic energy
per ion is estimated in the limit of infinite BOCP in a wide range of the
Coulomb coupling parameter $\Gamma$ from 0.03 to 1000 with the relative error
of the order 0.1%. Calculated energies are by about 0.5% lower as compared to
the modern Monte Carlo (MC) simulation data obtained by different authors at
$\Gamma<30$ and almost coincide with the MC results at $\Gamma>175$. We
introduce two more converging characteristic energies, the excess interatomic
electrostatic energy and the excess ion-background electrostatic energy, which
enable us to calculate the ionic compressibility factor inaccessible in
conventional MC and MD simulation of the OCP with periodic boundary conditions.
The derived wide-range ionic equation of state can be recommended for testing
OCP simulations with various effective interaction potentials. Based on this
equation, we propose an improved cutoff radius for the interionic forces
implemented in LAMMPS and perform MD simulation of the OCP to demonstrate that
location of the metastable region of the fluid-solid phase transition depends
sensitively on this radius.

</details>


### [38] [Electromagnetic plasma wave modes propagating along light-cone coordinates](https://arxiv.org/abs/2511.04554)
*Felipe A. Asenjo,Swadesh M. Mahajan*

Main category: physics.plasm-ph

TL;DR: The paper presents new electromagnetic plasma modes that propagate in one time and one space coordinate using light-cone coordinates, enabling novel wavepacket solutions with unique properties including faster-than-light wavefronts.


<details>
  <summary>Details</summary>
Motivation: To develop electromagnetic plasma modes that differ from conventional plane wave solutions by using light-cone coordinates instead of separation of variables, allowing for more flexible wavepacket constructions.

Method: Constructing wavepacket solutions using light-cone coordinates and special functions including Airy functions, Parabolic cylinder functions, Mathieu functions, and Bessel functions, with detailed analysis of double Airy solutions.

Result: Discovery of new electromagnetic wavepackets with defined wavefronts and velocities faster than conventional electromagnetic plane waves, particularly demonstrated through the double Airy solution.

Conclusion: The approach enables construction of structured wavepackets with novel electromagnetic properties that surpass conventional solutions, opening possibilities for more general wavepacket designs.

Abstract: We present new electromagnetic plasma modes that propagates in one time and
one space coordinates. Differently to the usual plane wave solution, which is
written in terms of separation of variables, all our solutions are along the
light-cone coordinates. This allow us to find several new wavepacket solutions
whose functionality properties rely on the conditions imposed on the choice for
their light-cone coordinates dependence. The presented wavepacket solutions are
constructed in terms of multiplications of Airy functions, Parabolic cylinder
functions, Mathieu functions, or Bessel functions. We thoroughly analyze the
case of a double Airy solution, which have new electromagnetic properties, as a
defined wavefront, and velocity faster than the electromagnetic plane wave
counterpart solution. It is also mentioned how more general structured
wavepackets can be constructed from these new solutions.

</details>


### [39] [Machine Learning for Electron-Scale Turbulence Modeling in W7-X](https://arxiv.org/abs/2511.04567)
*Ionut-Gabriel Farcas,Don Lawrence Carl Agapito Fernando,Alejandro Banon Navarro,Gabriele Merlo,Frank Jenko*

Main category: physics.plasm-ph

TL;DR: Machine learning models for predicting electron temperature gradient turbulence heat flux in W7-X stellarator using active learning and sparse-grid training.


<details>
  <summary>Details</summary>
Motivation: Need reduced models for turbulent transport to accelerate profile predictions and enable uncertainty quantification, parameter scans, and design optimization in fusion devices.

Method: Active machine learning with sparse-grid initialization and iterative refinement using most informative points from simulation database; models predict ETG heat flux from three plasma parameters at multiple radial locations.

Result: Models show robust performance with predictive accuracy comparable to original simulations, even when applied beyond training domain; evaluated on out-of-sample datasets with 393+ points per location.

Conclusion: Machine-learning-driven reduced models successfully predict ETG turbulence heat flux in W7-X, enabling efficient plasma transport modeling for fusion research applications.

Abstract: Constructing reduced models for turbulent transport is essential for
accelerating profile predictions and enabling many-query tasks such as
uncertainty quantification, parameter scans, and design optimization. This
paper presents machine-learning-driven reduced models for Electron Temperature
Gradient (ETG) turbulence in the Wendelstein 7-X (W7-X) stellarator. Each model
predicts the ETG heat flux as a function of three plasma parameters: the
normalized electron temperature radial gradient ($\omega_{T_e}$), the ratio of
normalized electron temperature and density radial gradients ($\eta_e$), and
the electron-to-ion temperature ratio ($\tau$). We first construct models
across seven radial locations using regression and an active
machine-learning-based procedure. This process initializes models using
low-cardinality sparse-grid training data and then iteratively refines their
training sets by selecting the most informative points from a pre-existing
simulation database. We evaluate the prediction capabilities of our models
using out-of-sample datasets with over $393$ points per location, and $95\%$
prediction intervals are estimated via bootstrapping to assess prediction
uncertainty. We then investigate the construction of generalized reduced
models, including a generic, position-independent model, and assess their heat
flux prediction capabilities at three additional locations. Our models
demonstrate robust performance and predictive accuracy comparable to the
original reference simulations, even when applied beyond the training domain.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [Comparing EPGP Surrogates and Finite Elements Under Degree-of-Freedom Parity](https://arxiv.org/abs/2511.04518)
*Obed Amo,Samit Ghosh,Markus Lange-Hegermann,Bogdan Raiţă,Michael Pokojovy*

Main category: cs.LG

TL;DR: Benchmarking study comparing boundary-constrained Ehrenpreis-Palamodov Gaussian Process (B-EPGP) surrogate with classical finite element method (CN-FEM) for 2D wave equation, showing B-EPGP achieves ~100x better accuracy under matched degrees of freedom.


<details>
  <summary>Details</summary>
Motivation: To compare the performance of a novel B-EPGP surrogate method against traditional CN-FEM for solving the 2D wave equation with homogeneous Dirichlet boundary conditions.

Method: B-EPGP uses exponential-polynomial bases from characteristic variety to exactly enforce PDE and boundary conditions, with penalized least squares for coefficient estimation. A degrees-of-freedom matching protocol ensures fair comparison with CN-FEM.

Result: Under matched DoF, B-EPGP consistently achieves lower space-time L²-error and maximum-in-time L²-error in space than CN-FEM, improving accuracy by roughly two orders of magnitude.

Conclusion: The B-EPGP surrogate method significantly outperforms classical CN-FEM for solving the 2D wave equation, demonstrating superior accuracy with exact enforcement of PDE and boundary conditions.

Abstract: We present a new benchmarking study comparing a boundary-constrained
Ehrenpreis--Palamodov Gaussian Process (B-EPGP) surrogate with a classical
finite element method combined with Crank--Nicolson time stepping (CN-FEM) for
solving the two-dimensional wave equation with homogeneous Dirichlet boundary
conditions. The B-EPGP construction leverages exponential-polynomial bases
derived from the characteristic variety to enforce the PDE and boundary
conditions exactly and employs penalized least squares to estimate the
coefficients. To ensure fairness across paradigms, we introduce a
degrees-of-freedom (DoF) matching protocol. Under matched DoF, B-EPGP
consistently attains lower space-time $L^2$-error and maximum-in-time
$L^{2}$-error in space than CN-FEM, improving accuracy by roughly two orders of
magnitude.

</details>


### [41] [Uncertainty Quantification for Reduced-Order Surrogate Models Applied to Cloud Microphysics](https://arxiv.org/abs/2511.04534)
*Jonas E. Katona,Emily K. de Jong,Nipun Gunawardena*

Main category: cs.LG

TL;DR: A model-agnostic framework for uncertainty quantification in reduced-order models using conformal prediction to estimate prediction intervals for latent dynamics, reconstruction, and end-to-end predictions.


<details>
  <summary>Details</summary>
Motivation: Existing uncertainty quantification methods for reduced-order models are architecture- or training-specific, limiting flexibility and generalization.

Method: Post hoc framework using conformal prediction that requires no modification to the underlying ROM architecture or training procedure.

Result: Successfully demonstrated on a latent space dynamical model for cloud microphysics, accurately predicting droplet-size distribution evolution and quantifying uncertainty across the ROM pipeline.

Conclusion: The proposed framework provides robust, model-agnostic uncertainty quantification for reduced-order models without requiring architectural or training modifications.

Abstract: Reduced-order models (ROMs) can efficiently simulate high-dimensional
physical systems, but lack robust uncertainty quantification methods. Existing
approaches are frequently architecture- or training-specific, which limits
flexibility and generalization. We introduce a post hoc, model-agnostic
framework for predictive uncertainty quantification in latent space ROMs that
requires no modification to the underlying architecture or training procedure.
Using conformal prediction, our approach estimates statistical prediction
intervals for multiple components of the ROM pipeline: latent dynamics,
reconstruction, and end-to-end predictions. We demonstrate the method on a
latent space dynamical model for cloud microphysics, where it accurately
predicts the evolution of droplet-size distributions and quantifies uncertainty
across the ROM pipeline.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [42] [The Navier-Stokes equations with transport noise in critical $H^{1/2}$ space](https://arxiv.org/abs/2511.04138)
*Mustafa Sencer Aydın,Fanhui Xu*

Main category: math.PR

TL;DR: Local existence and uniqueness of probabilistically strong solutions for Navier-Stokes equations with transport noise in critical spaces, with global existence probability approaching 1 for small initial data.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for Navier-Stokes equations with transport noise in critical function spaces, extending classical results to stochastic settings.

Method: Probabilistic analysis in critical function spaces, working with initial data in H^{1/2} almost surely, independent of spatial domain compactness.

Result: Existence and uniqueness of local-in-time probabilistically strong solutions; global existence probability can be made arbitrarily close to 1 for sufficiently small initial data; solution norm remains small for all time.

Conclusion: The results apply to both three-dimensional torus and whole space, providing a unified framework for Navier-Stokes equations with transport noise in critical spaces.

Abstract: We study the Navier-Stokes equations with transport noise in critical
function spaces. Assuming the initial data belongs to $H^{1/2}$ almost surely,
we establish the existence and uniqueness of a local-in-time probabilistically
strong solution. Moreover, we show that the probability of global existence can
be made arbitrarily close to $1$ by choosing the initial data norm sufficiently
small, and that the solution norm remains small for all time. Our analysis is
independent of the compactness of the spatial domain, and consequently, the
results apply both to the three-dimensional torus and to the whole space.

</details>


<div id='nlin.SI'></div>

# nlin.SI [[Back]](#toc)

### [43] [A superintegrable quantum field theory](https://arxiv.org/abs/2511.03373)
*Marine De Clerck,Oleg Evnin*

Main category: nlin.SI

TL;DR: The paper studies the quantum version of the cubic Szegő equation, a remarkable integrable classical field theory with turbulent energy transfer, focusing on its integer spectra and structure beyond ordinary quantum integrability.


<details>
  <summary>Details</summary>
Motivation: To systematically study the quantum system derived from the cubic Szegő equation, which exhibits purely integer spectra in both its Hamiltonian and conserved hierarchies, indicating a structure beyond ordinary quantum integrability.

Method: A mixture of analytic results and empirical observations on the structure of eigenvalues, eigenvectors, conservation laws, ladder operators, and other properties of the quantum system.

Result: The quantum system displays purely integer spectra for both the Hamiltonian and its associated conserved hierarchies, revealing a structure that goes beyond ordinary quantum integrability.

Conclusion: The quantum cubic Szegő system possesses remarkable mathematical properties with integer spectra and conservation laws, warranting further systematic investigation into its structure.

Abstract: G\'erard and Grellier proposed, under the name of the cubic Szeg\H{o}
equation, a remarkable classical field theory on a circle with a quartic
Hamiltonian. The Lax integrability structure that emerges from their definition
is so constraining that it allows for writing down an explicit general solution
for prescribed initial data, and at the same time, the dynamics is highly
nontrivial and involves turbulent energy transfer to arbitrarily short
wavelengths. The quantum version of the same Hamiltonian is even more striking:
not only the Hamiltonian itself, but also its associated conserved hierarchies
display purely integer spectra, indicating a structure beyond ordinary quantum
integrability. Here, we initiate a systematic study of this quantum system by
presenting a mixture of analytic results and empirical observations on the
structure of its eigenvalues and eigenvectors, conservation laws, ladder
operators, etc.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [44] [Twist and higher modes of a complex scalar field at the threshold of collapse](https://arxiv.org/abs/2511.04649)
*Krinio Marouda,Daniela Cors,Hannes R. Rüter,Alex Vaño-Viñuales,David Hilditch*

Main category: gr-qc

TL;DR: Study of critical collapse in axisymmetric spacetimes for massless complex scalar fields with angular momentum, showing universality within fixed m-symmetry classes but with critical exponents depending on angular mode m.


<details>
  <summary>Details</summary>
Motivation: To investigate how angular momentum and higher angular modes affect critical gravitational collapse, particularly whether universality and discrete self-similarity persist in systems with nonzero twist and angular momentum.

Method: Used pseudospectral code bamps with new m-cartoon symmetry reduction method and generalized twist-compatible apparent horizon finder to evolve near-critical initial data for m=1 and m=2 angular modes in axisymmetric spacetimes.

Result: Found discrete self-similarity with echoing period Δ≈0.42 and scaling exponent γ≈0.11 for m=1, and Δ≈0.09, γ≈0.035 for m=2. Angular momentum effect is minimal at threshold with χ_AH = J_AH/M_AH² → 0, excluding extremal black holes.

Conclusion: Universality and DSS hold within each m-sector but critical values vary with m. No extremality or bifurcation occurs in complex scalar field model for considered families.

Abstract: We investigate the threshold of collapse of a massless complex scalar field
in axisymmetric spacetimes under the ansatz of Choptuik et al. 2004, in which a
symmetry depending on the azimuthal parameter $m$ is imposed on the scalar
field. This allows for both non-vanishing twist and angular momentum. We extend
earlier work to include higher angular modes. Using the pseudospectral code
bamps with a new adapted symmetry reduction method, which we call $m$-cartoon,
and a generalized twist-compatible apparent horizon finder, we evolve
near-critical initial data to the verge of black hole formation for the lowest
nontrivial modes, $m=1$ and $m=2$. For $m=1$ we recover discrete
self-similarity with echoing period $\Delta\simeq0.42$ and power-law scaling
with exponent $\gamma\simeq0.11$, consistent with earlier work. For $m=2$ we
find that universality is maintained within this nonzero fixed-$m$ symmetry
class but with smaller period and critical exponents, $\Delta\simeq0.09$ and
$\gamma\simeq0.035$, establishing an explicit dependence of the critical
solution on the angular mode. Analysis of the relation between the angular
momentum and the mass of apparent horizons at the instant of formation,
$J_{\mathrm{AH}}{-}M_{\mathrm{AH}}$, shows that the effect of angular momentum
is minimal at the threshold, with
$\chi_{\mathrm{AH}}=J_{\mathrm{AH}}/M_{\mathrm{AH}}^2\to0$, and, therefore,
excludes extremal black holes for the families under consideration. Our results
demonstrate that while universality and DSS hold within each $m$-sector, the
critical universal values vary with $m$, and neither extremality nor
bifurcation occur in the complex scalar field model within the families
considered here.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [45] [Generic regularity of isoperimetric regions in dimension eight](https://arxiv.org/abs/2511.03795)
*Kobe Marshall-Stevens,Gongping Niu,Davide Parise*

Main category: math.DG

TL;DR: Generic regularity results for isoperimetric regions in 8-dimensional closed Riemannian manifolds, showing smooth nondegenerate boundaries for generic metrics and volumes.


<details>
  <summary>Details</summary>
Motivation: To establish regularity properties of isoperimetric regions in higher-dimensional Riemannian manifolds, particularly in dimension eight where such results are not well-established.

Method: Mathematical analysis using geometric measure theory and perturbation techniques to prove generic regularity for isoperimetric regions.

Result: Proved that every isoperimetric region has a smooth nondegenerate boundary for generic smooth metrics and enclosed volumes in 8-dimensional closed Riemannian manifolds.

Conclusion: The paper establishes strong regularity results for isoperimetric regions in dimension eight, showing they behave well generically in terms of metric and volume choices.

Abstract: We establish generic regularity results for isoperimetric regions in closed
Riemannian manifolds of dimension eight. In particular, we show that every
isoperimetric region has a smooth nondegenerate boundary for a generic choice
of smooth metric and enclosed volume, or for a fixed enclosed volume and a
generic choice of smooth metric.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [46] [Signature-Based Universal Bilinear Approximations for Nonlinear Systems and Model Order Reduction](https://arxiv.org/abs/2511.04303)
*Martin Redmann,Justus Werner*

Main category: math.OC

TL;DR: This paper presents a method for approximating non-Lipschitz nonlinear systems using signature-based bilinearization and model order reduction, enabling efficient modeling and data fitting without requiring explicit knowledge of the original system.


<details>
  <summary>Details</summary>
Motivation: To develop a feasible approach for approximating non-Lipschitz nonlinear systems that remains practical in large-scale settings, overcoming limitations of traditional linearization techniques.

Method: Approximate nonlinear systems using linear maps of signatures (iterated integrals from rough path theory), identify a universal bilinear system solved by signatures, and apply model order reduction specifically designed for signature processes.

Result: The method yields potentially low-dimensional bilinear models that can approximate original nonlinear dynamics arbitrarily well, with dimension growing only with the number of inputs rather than system complexity.

Conclusion: The signature-based approach provides an effective framework for both efficient data fitting using small-scale bilinear systems and model order reduction for nonlinear systems, as demonstrated through numerical experiments.

Abstract: This paper deals with non-Lipschitz nonlinear systems. Such systems can be
approximated by a linear map of so-called signatures, which play a crucial role
in the theory of rough paths and can be interpreted as collections of iterated
integrals involving the control process. As a consequence, we identify a
universal bilinear system, solved by the signature, that can approximate the
state or output of the original nonlinear dynamics arbitrarily well. In
contrast to other (bi)linearization techniques, the signature approach remains
feasible in large-scale settings, as the dimension of the associated bilinear
system grows only with the number of inputs. However, the signature model is
typically of high order, requiring an optimization process based on model order
reduction (MOR). We derive an MOR method for unstable bilinear systems with
non-zero initial states and apply it to the signature, yielding a potentially
low-dimensional bilinear model. An advantage of our method is that the original
nonlinear system need not be known explicitly, since only data are required to
learn the linear map of the signature. The subsequent MOR procedure is
model-oriented and specifically designed for the signature process.
Consequently, this work has two main applications: (1) efficient modeling/data
fitting using small-scale bilinear systems, and (2) MOR for nonlinear systems.
We illustrate the effectiveness of our approach in the second application
through numerical experiments.

</details>


### [47] [Computational Modeling and Learning-Based Adaptive Control of Solid-Fuel Ramjets](https://arxiv.org/abs/2511.04580)
*Gohar T. Khokhar,Kyle Hanquist,Parham Oveissi,Alex Dorsey,Ankit Goel*

Main category: math.OC

TL;DR: A computational and control framework combining CFD modeling with learning-based adaptive control for solid-fuel ramjet thrust regulation, achieving robust performance despite strong nonlinearities and complex physics.


<details>
  <summary>Details</summary>
Motivation: Solid-fuel ramjets offer compact, energy-dense propulsion for long-range high-speed flight but face significant thrust regulation challenges due to strong nonlinearities, limited actuation authority, and complex multi-physics coupling between fuel regression, combustion, and compressible flow.

Method: Developed a CFD model with heat addition to characterize thrust response and identify operational limits, then applied an adaptive PI controller updated online using the retrospective cost adaptive control (RCAC) algorithm for thrust regulation.

Result: Closed-loop simulations demonstrated accurate thrust regulation under both static and dynamic operating conditions, with robustness to variations in commands, hyperparameters, and inlet states.

Conclusion: RCAC is well-suited for SFRJ control where accurate reduced-order models are challenging to obtain, and learning-based adaptive control shows potential for enabling robust and reliable SFRJ operation in future air-breathing propulsion applications.

Abstract: Solid-fuel ramjets offer a compact, energy-dense propulsion option for
long-range, high-speed flight but pose significant challenges for thrust
regulation due to strong nonlinearities, limited actuation authority, and
complex multi-physics coupling between fuel regression, combustion, and
compressible flow. This paper presents a computational and control framework
that combines a computational fluid dynamics model of an SFRJ with a
learning-based adaptive control approach. A CFD model incorporating heat
addition was developed to characterize thrust response, establish the
operational envelope, and identify the onset of inlet unstart. An adaptive
proportional-integral controller, updated online using the retrospective cost
adaptive control (RCAC) algorithm, was then applied to regulate thrust.
Closed-loop simulations demonstrate that the RCAC-based controller achieves
accurate thrust regulation under both static and dynamic operating conditions,
while remaining robust to variations in commands, hyperparameters, and inlet
states. The results highlight the suitability of RCAC for SFRJ control, where
accurate reduced-order models are challenging to obtain, and underscore the
potential of learning-based adaptive control to enable robust and reliable
operation of SFRJs in future air-breathing propulsion applications.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [48] [Super amplification of lunar response to gravitational waves driven by thick crust](https://arxiv.org/abs/2511.04067)
*Lei Zhang,Jinhai Zhang,Han Yan,Xian Chen*

Main category: astro-ph.EP

TL;DR: The Moon amplifies gravitational waves (GWs) with most regions showing over 2x amplification, and highlands around the South Pole-Aitken basin reaching tens of times amplification at resonant frequency (~0.015 Hz) due to thick crust.


<details>
  <summary>Details</summary>
Motivation: To understand the spatial variation of GW amplification capacity on the Moon, which has been regarded as a natural GW resonator since 1960 but with unclear regional differences.

Method: Numerical simulation of lunar response to GWs considering fluctuant topography and laterally heterogeneous interior structures.

Result: Most lunar regions amplify GWs with ratio over 2, significantly higher than previous estimates. Highlands around South Pole-Aitken basin show amplification factors of tens at resonant frequency (~0.015 Hz) where crust is thickest.

Conclusion: Thick-crust regions are critical zones for GW amplification, essential for future landing site selection and instrument placement for GW detection on the Moon.

Abstract: The Moon has been long regarded as a natural resonator of gravitational waves
(GWs) since 1960, showing great potential to fill the frequency gap left behind
GW detections by ground- or space-based laser interferometry. However, the
spatial variation of this amplification capacity on the Moon remains unclear.
Here, we numerically simulate the lunar response to GWs by fully considering
the fluctuant topography and laterally heterogeneous interior structures. Our
results show that most regions on the Moon can amplify GWs with a ratio over 2,
a finding significantly higher than previous estimations. Particularly, the
amplification ratio can even reach factors of tens at the resonant frequency of
~0.015 Hz on the highlands surrounding the South Pole-Aitken (SPA) basin, where
the regional crust is the thickest. Our findings establish the thick-crust
regions as critical zones of GW amplification, which is essential for future
landing site selection and instrumental setting for GW detection on the Moon.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [49] [The Mean-Field Ott-Antonsen Manifold is an Unstable Manifold in the Continuum Limit](https://arxiv.org/abs/2511.03833)
*Christian Kuehn,Giacomo Landi*

Main category: math.DS

TL;DR: The paper studies the relationship between continuum limit (CL) and mean-field limit (MFL) PDEs for Kuramoto-type particle systems, showing how to generate CL solutions from MFL solutions and proving that the unstable manifold of CL's homogeneous steady state corresponds to the Ott-Antonsen manifold in MFL.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamical relationship between two different PDEs (continuum limit and mean-field limit) that arise from the same interacting particle system but have different mathematical structures.

Method: Analyzing the relation between solutions of CL and MFL PDEs, and providing an explicit proof connecting invariant manifolds - specifically showing the correspondence between CL's unstable manifold and MFL's Ott-Antonsen manifold.

Result: Demonstrated how to generate CL solutions from MFL solutions, and proved that the unstable manifold of the homogeneous steady state in CL is the direct dynamical analogue of the Ott-Antonsen manifold in MFL.

Conclusion: The paper establishes a fundamental connection between two important mathematical frameworks for studying interacting particle systems, revealing that key invariant manifolds in continuum and mean-field limits are dynamically equivalent.

Abstract: We study interacting particle systems of Kuramoto-type. Our focus is on the
dynamical relation between the partial differential equation (PDE) arising in
the continuum limit (CL) and the one obtained in the mean-field limit (MFL).
Both equations arise when we are considering the limit of infinitely many
interacting particles but the classes of PDEs are structurally different. The
CL tracks particles effectively pointwise, while the MFL is an evolution for a
typical particle. First, we briefly discuss the relation between solutions of
the CL and the MFL showing how to generate solutions of the CL starting from
solutions of the MFL. Our main result concerns a dynamical relation between
important invariant manifolds of the CFL and the MFL. In particular, we give an
explicit proof that the unstable manifold of the homogeneous steady state of
the CL is the direct dynamical analogue of the famous Ott-Antonsen manifold for
the MFL.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [50] [Energy transport and chaos in a one-dimensional disordered nonlinear stub lattice](https://arxiv.org/abs/2511.04159)
*Su Ho Cheong,Arnold Ngapasare,Vassos Achilleos,Georgios Theocharis,Olivier Richoux,Charalampos Skokos*

Main category: nlin.CD

TL;DR: Study of energy propagation in 1D stub lattice with disorder and nonlinearity, identifying three dynamical regimes (weak chaos, strong chaos, self-trapping) with subdiffusive spreading patterns.


<details>
  <summary>Details</summary>
Motivation: To investigate how disorder and nonlinearity affect energy propagation in flat-band systems like the stub lattice, which hosts dispersive bands separated by a flat band.

Method: Numerical simulations of initially localized wave packets, mapping the 2D parameter space of disorder and nonlinearity, analyzing frequency gaps and chaotic behavior.

Result: Found three distinct regimes with subdiffusive spreading: weak chaos (m₂ ∝ t^0.33), strong chaos (m₂ ∝ t^0.5), and self-trapping. Lyapunov exponents decay as Λ ∝ t^-0.25 and Λ ∝ t^-0.3. Strong disorder closes band gaps, resembling 1D disordered nonlinear systems.

Conclusion: The characterization of nonlinear disordered lattices extends to flat-band systems like stub lattices, with disorder closing frequency gaps and leading to similar spreading behavior as in standard 1D disordered systems.

Abstract: We investigate energy propagation in a one-dimensional stub lattice in the
presence of both disorder and nonlinearity. In the periodic case, the stub
lattice hosts two dispersive bands separated by a flat band; however, we show
that sufficiently strong disorder fills all intermediate band gaps. By mapping
the two-dimensional parameter space of disorder and nonlinearity, we identify
three distinct dynamical regimes (weak chaos, strong chaos, and self-trapping)
through numerical simulations of initially localized wave packets. When
disorder is strong enough to close the frequency gaps, the results closely
resemble those obtained in the one-dimensional disordered discrete nonlinear
Schr\"{o}dinger equation and Klein-Gordon lattice model. In particular,
subdiffusive spreading is observed in both the weak and strong chaos regimes,
with the second moment $m_2$ of the norm distribution scaling as $m_2 \propto
t^{0.33}$ and $m_2 \propto t^{0.5}$, respectively. The system's chaotic
behavior follows a similar trend, with the finite-time maximum Lyapunov
exponent $\Lambda$ decaying as $\Lambda \propto t^{-0.25}$ and $\Lambda \propto
t^{-0.3}$. For moderate disorder strengths, i.e., near the point of gap
closing, we find that the presence of small frequency gaps does not exert any
noticeable influence on the spreading behavior. Our findings extend the
characterization of nonlinear disordered lattices in both weak and strong chaos
regimes to other network geometries, such as the stub lattice, which serves as
a representative flat-band system.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [51] [N-Mode Quantized Anharmonic Vibronic Hamiltonians for Matrix Product State Dynamics](https://arxiv.org/abs/2511.03936)
*Valentin Barandun,Nina Glaser,Markus Reiher*

Main category: physics.chem-ph

TL;DR: The paper presents a second-quantized framework using n-mode quantization of vibronic Hamiltonian terms for accurate quantum dynamics calculations, demonstrated on maleimide's excited state dynamics.


<details>
  <summary>Details</summary>
Motivation: Theoretical predictions of photochemical processes require precise modeling of anharmonic effects and nonadiabatic couplings in vibronic systems for reliable quantum dynamics calculations.

Method: n-mode quantization of all vibronic Hamiltonian terms using general high-dimensional model representations, implemented with density matrix renormalization group algorithm.

Result: The approach enables accurate calculations of complex photochemical dynamics, as demonstrated on maleimide's excited state quantum dynamics with analyzed convergence and parameter choices.

Conclusion: The n-mode vibronic Hamiltonian framework provides accurate and reliable quantum dynamics calculations for complex photochemical systems.

Abstract: Theoretical predictions of photochemical processes are essential for
interpreting and understanding spectral features. Reliable quantum dynamics
calculations of vibronic systems require precise modeling of anharmonic effects
in the potential energy surfaces and off-diagonal nonadiabatic coupling terms.
In this work, we present the n-mode quantization of all vibronic Hamiltonian
terms comprised of general high-dimensional model representations. This results
in a second-quantized framework for accurate vibronic calculations employing
the density matrix renormalization group algorithm. We demonstrate the accuracy
and reliability of this approach by calculating the excited state quantum
dynamics of maleimide. We analyze convergence and the choice of parameters of
the underlying time-dependent density matrix renormalization group algorithm
for the n-mode vibronic Hamiltonian, demonstrating that it enables accurate
calculations of complex photochemical dynamics.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [52] [Friction on Demand: A Generative Framework for the Inverse Design of Metainterfaces](https://arxiv.org/abs/2511.03735)
*Valentin Mouton,Adrien Mélot*

Main category: stat.ML

TL;DR: A generative modeling framework using VAEs to infer surface topographies from target friction laws, enabling efficient generation of candidate solutions for frictional interface design.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches for designing frictional interfaces rely on heuristic search over low-dimensional parameterizations, which limits applicability to complex friction laws and is computationally expensive.

Method: Used Variational Autoencoders trained on a synthetic dataset of 200 million samples from a parameterized contact mechanics model to enable simulation-free generation of surface topographies.

Result: The method enables efficient generation of candidate topographies and examines trade-offs between accuracy, throughput, and diversity in generated solutions.

Conclusion: This approach paves the way for near-real-time control of frictional behavior through tailored surface topographies, highlighting practical considerations for balancing design objectives.

Abstract: Designing frictional interfaces to exhibit prescribed macroscopic behavior is
a challenging inverse problem, made difficult by the non-uniqueness of
solutions and the computational cost of contact simulations. Traditional
approaches rely on heuristic search over low-dimensional parameterizations,
which limits their applicability to more complex or nonlinear friction laws. We
introduce a generative modeling framework using Variational Autoencoders (VAEs)
to infer surface topographies from target friction laws. Trained on a synthetic
dataset composed of 200 million samples constructed from a parameterized
contact mechanics model, the proposed method enables efficient, simulation-free
generation of candidate topographies. We examine the potential and limitations
of generative modeling for this inverse design task, focusing on balancing
accuracy, throughput, and diversity in the generated solutions. Our results
highlight trade-offs and outline practical considerations when balancing these
objectives. This approach paves the way for near-real-time control of
frictional behavior through tailored surface topographies.

</details>


### [53] [Robustness of Minimum-Volume Nonnegative Matrix Factorization under an Expanded Sufficiently Scattered Condition](https://arxiv.org/abs/2511.04291)
*Giovanni Barbarino,Nicolas Gillis,Subhayan Saha*

Main category: stat.ML

TL;DR: Min-vol NMF identifies groundtruth factors under noise when data points are sufficiently scattered in the latent simplex.


<details>
  <summary>Details</summary>
Motivation: Robustness of minimum-volume NMF to noise has been an open problem despite its successful applications in various fields.

Method: Proving theoretical guarantees for min-vol NMF under the expanded sufficiently scattered condition.

Result: Min-vol NMF can identify groundtruth factors in the presence of noise when data points are sufficiently scattered.

Conclusion: The paper provides theoretical justification for min-vol NMF's robustness to noise under specific scattering conditions.

Abstract: Minimum-volume nonnegative matrix factorization (min-vol NMF) has been used
successfully in many applications, such as hyperspectral imaging, chemical
kinetics, spectroscopy, topic modeling, and audio source separation. However,
its robustness to noise has been a long-standing open problem. In this paper,
we prove that min-vol NMF identifies the groundtruth factors in the presence of
noise under a condition referred to as the expanded sufficiently scattered
condition which requires the data points to be sufficiently well scattered in
the latent simplex generated by the basis vectors.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [54] [Mixed-State Measurement-Induced Phase Transitions in Imaginary-Time Dynamics](https://arxiv.org/abs/2511.04402)
*Yi-Ming Ding,Zenan Liu,Xu Tian,Zhe Wang,Yanzhang Zhu,Zheng Yan*

Main category: quant-ph

TL;DR: Introduces measurement-dressed imaginary-time evolution (MDITE) as a framework to study mixed-state quantum phases and decoherence-driven criticality through alternating imaginary-time evolution and projective measurements.


<details>
  <summary>Details</summary>
Motivation: To explore mixed-state phase transitions as a new frontier in nonequilibrium quantum matter and quantum information, focusing on the interplay between coherence-restoring dynamics and decoherence-inducing events.

Method: Measurement-dressed imaginary-time evolution (MDITE) combining alternating imaginary-time evolution and projective measurements, with numerical simulations of 1D transverse-field Ising model and 2D dimerized Heisenberg model, plus diagrammatic representation for efficient quantum Monte Carlo studies.

Result: MDITE gives rise to a new class of mixed-state phase transitions and enables efficient investigation of mixed-state phase transitions in large-scale and higher-dimensional Hamiltonians.

Conclusion: MDITE serves as a powerful paradigm for investigating non-unitary dynamics and the fundamental role of decoherence in many-body quantum systems.

Abstract: Mixed-state phase transitions have recently attracted growing attention as a
new frontier in nonequilibrium quantum matter and quantum information. In this
work, we introduce the measurement-dressed imaginary-time evolution (MDITE) as
a novel framework to explore mixed-state quantum phases and decoherence-driven
criticality. In this setup, alternating imaginary-time evolution and projective
measurements generate a competition between coherence-restoring dynamics and
decoherence-inducing events. While reminiscent of monitored unitary circuits,
MDITE fundamentally differs in that the physics is encoded in decoherent mixed
states rather than in quantum trajectories. We demonstrate that this interplay
gives rise to a new class of mixed-state phase transitions, using numerical
simulations of the one-dimensional transverse-field Ising model and the
two-dimensional dimerized Heisenberg model. Furthermore, we provide a
diagrammatic representation of the evolving state, which naturally enables
efficient studies of MDITE with quantum Monte Carlo and other many-body
numerical methods, thereby extending investigations of mixed-state phase
transitions to large-scale and higher-dimensional Hamiltonians. Our results
highlight MDITE as a powerful paradigm for investigating non-unitary dynamics
and the fundamental role of decoherence in many-body quantum systems.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [55] [Navier-Stokes Equations on Quantum Euclidean Spaces](https://arxiv.org/abs/2511.04318)
*Deyu Chen,Guixiang Hong,Liang Wang,Wenhua Wang*

Main category: math.FA

TL;DR: The paper establishes global well-posedness for 2D quantum Navier-Stokes equations and local well-posedness for higher dimensions on quantum Euclidean spaces, developing harmonic analysis tools and applying noncommutative Lp-space techniques.


<details>
  <summary>Details</summary>
Motivation: To extend classical Navier-Stokes theory to quantum Euclidean spaces as a standard example of non-compact noncommutative manifolds, and systematically apply real analysis techniques from noncommutative Lp-spaces to quantum PDEs.

Method: Developed theory of harmonic analysis and function spaces on quantum Euclidean spaces, applied sharp estimates from noncommutative Lp-spaces to quantum Navier-Stokes equations, using techniques independent of the deformed parameter θ.

Result: Proved global well-posedness in 2D case and local well-posedness with solution in L_d(R^d) in higher dimensions for quantum Navier-Stokes equations, with results on semiclassical limits.

Conclusion: This represents the first systematic application of noncommutative Lp-space techniques to quantum PDEs, opening possibilities for numerous similar applications in the future, analogous to classical case developments.

Abstract: We investigate in the present paper the Navier-Stokes equations on quantum
Euclidean spaces $\mathbb{R}^d_{\theta}$ with $\theta$ being a $d\times d$
antisymmetric matrix, which is a standard example of non-compact noncommutative
manifolds. The quantum analogues of Ladyzhenskaya and Kato's results are
established, that is, we obtain the global well-posedness in the 2D case and
the local well-posedness with solution in $L_d(\mathbb{R}^d)$ in higher
dimensions. To achieve these optimal results, we develop the related theory of
harmonic analysis and function spaces on $\mathbb{R}^d_{\theta}$, and apply the
sharp estimates around noncommutative $L_p$-spaces to quantum Navier-Stokes
equations. Moreover, our techniques, which are independent of the deformed
parameter $\theta$, allow us to conclude some results on the semiclassical
limits. This is the first instance of systematical applications to the theory
of quantum partial differential equations of the powerful real analysis
techniques around noncommutative $L_p$-spaces, which date back to the seminal
work \cite{PiXu97} in 1997 on noncommutative martingale inequalities. As in
classical case, one may expect numerous similar applications in the future.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [56] [Machine learning-driven elasticity prediction in advanced inorganic materials via convolutional neural networks](https://arxiv.org/abs/2511.04468)
*Yujie Liu,Zhenyu Wang,Hang Lei,Guoyu Zhang,Jiawei Xian,Zhibin Gao,Jun Sun,Haifeng Song,Xiangdong Ding*

Main category: cond-mat.mtrl-sci

TL;DR: This study used crystal graph convolutional neural networks (CGCNNs) to predict shear and bulk modulus for 80,664 inorganic crystals, achieving high accuracy and providing an open dataset for material design.


<details>
  <summary>Details</summary>
Motivation: Traditional experimental measurement of elastic properties is costly and inefficient, while machine learning methods like CGCNNs offer effective alternatives for predicting material properties.

Method: Trained two CGCNN models using shear and bulk modulus data from 10,987 materials in Matbench v0.1 dataset, screened materials with band gaps 0.1-3.0 eV and excluded radioactive compounds.

Result: Models achieved high accuracy (MAE <13, R-squared close to 1) and good generalization. Final dataset includes 54,359 Materials Project structures and 26,305 structures from Merchant et al. (2023).

Conclusion: Successfully predicted elastic properties for 80,664 inorganic crystals, enriching material data resources and supporting material design with openly available data.

Abstract: Inorganic crystal materials have broad application potential due to excellent
physical and chemical properties, with elastic properties (shear modulus, bulk
modulus) crucial for predicting materials' electrical conductivity, thermal
conductivity and mechanical properties. Traditional experimental measurement
suffers from high cost and low efficiency, while theoretical simulation and
graph neural network-based machine learning methods--especially crystal graph
convolutional neural networks (CGCNNs)--have become effective alternatives,
achieving remarkable results in predicting material elastic properties. This
study trained two CGCNN models using shear modulus and bulk modulus data of
10987 materials from the Matbench v0.1 dataset, which exhibit high accuracy
(mean absolute error <13, coefficient of determination R-squared close to 1)
and good generalization ability. Materials were screened to retain those with
band gaps between 0.1-3.0 eV and exclude radioactive element-containing
compounds. The final predicted dataset comprises two parts: 54359 crystal
structures from the Materials Project database and 26305 crystal structures
discovered by Merchant et al. (2023 Nature 624 80). Ultimately, this study
completed the prediction of shear modulus and bulk modulus for 80664 inorganic
crystals. This work enriches existing material elastic data resources and
provides robust support for material design, with all data openly available at
https://doi.org/10.57760/sciencedb.j00213.00104.

</details>


### [57] [The phase-field model of fracture incorporating Mohr-Coulomb, Mogi-Coulomb, and Hoek-Brown strength surfaces](https://arxiv.org/abs/2511.04627)
*S Chockalingam,Adrian Buganza Tepole,Aditya Kumar*

Main category: cond-mat.mtrl-sci

TL;DR: This paper implements a general driving-force expression for phase-field fracture models that incorporates arbitrary strength surfaces, enabling both toughness-controlled crack propagation and strength-governed fracture nucleation.


<details>
  <summary>Details</summary>
Motivation: Classical phase-field theories only capture toughness-controlled crack propagation but fail to account for material strength surfaces that govern fracture nucleation in the absence of pre-existing cracks.

Method: Implementation of a general driving-force expression within a finite-element framework, incorporating representative strength surfaces (Mohr-Coulomb, 3D Hoek-Brown, and Mogi-Coulomb) and validating through simulations of canonical fracture problems.

Result: The formulation successfully captures three fracture regimes: nucleation under uniform stress, crack growth from large pre-existing flaws, and fracture governed jointly by strength and toughness.

Conclusion: The proposed driving-force construction demonstrates generality and robustness for materials governed by arbitrary strength surfaces, spanning a broad range of brittle materials.

Abstract: Classical phase-field theories of brittle fracture capture
toughness-controlled crack propagation but do not account for the material's
strength surface, which governs fracture nucleation in the absence of cracks.
The phase-field formulation of Kumar et al. (2020) proposed a blueprint for
incorporating the strength surface while preserving toughness-controlled
propagation by introducing a nucleation driving force and presented results for
the Drucker--Prager surface. Following this blueprint, Chockalingam (2025)
recently derived a general driving-force expression that incorporates arbitrary
strength surfaces. The present work implements this driving force within a
finite-element framework and incorporates representative strength surfaces that
span diverse mathematical and physical characteristics -- the Mohr--Coulomb, 3D
Hoek--Brown, and Mogi--Coulomb surfaces. Through simulations of canonical
fracture problems, the formulation is comprehensively validated across fracture
regimes, capturing (i) nucleation under uniform stress, (ii) crack growth from
large pre-existing flaws, and (iii) fracture governed jointly by strength and
toughness. While the strength surfaces examined here already encompass a broad
range of brittle materials, the results demonstrate the generality and
robustness of the proposed driving-force construction for materials governed by
arbitrary strength surfaces.

</details>
