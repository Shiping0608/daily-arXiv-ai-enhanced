<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 23]
- [math.AP](#math.AP) [Total: 41]
- [physics.comp-ph](#physics.comp-ph) [Total: 6]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [math.CV](#math.CV) [Total: 1]
- [math.KT](#math.KT) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [physics.atom-ph](#physics.atom-ph) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 2]
- [math.OC](#math.OC) [Total: 2]
- [physics.class-ph](#physics.class-ph) [Total: 1]
- [physics.hist-ph](#physics.hist-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [math-ph](#math-ph) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A Spectral LOD Method for Multiscale Problems with High Contrast](https://arxiv.org/abs/2511.05776)
*Susanne C. Brenner,José C. Garay,Li-yeng Sung*

Main category: math.NA

TL;DR: A multiscale finite element method for diffusion problems with rough, high contrast coefficients using localized orthogonal decomposition and local eigenvalue problems.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient finite element method that can handle diffusion problems with challenging coefficients (rough and high contrast) while maintaining performance comparable to standard methods for simpler problems.

Method: Construction of multiscale finite element space using localized orthogonal decomposition methodology with local finite element eigenvalue problems.

Result: The method achieves performance similar to standard finite element methods for homogeneous Dirichlet boundary value problems for Poisson equation on smooth/convex domains.

Conclusion: Simple explicit error estimates are established under verifiable computational conditions, providing reliable performance guarantees.

Abstract: We present a multiscale finite element method for a diffusion problem with rough and high contrast coefficients. The construction of the multiscale finite element space is based on the localized orthogonal decomposition methodology and it involves solutions of local finite element eigenvalue problems. We show that the performance of the multiscale finite element method is similar to the performance of standard finite element methods for the homogeneous Dirichlet boundary value problem for the Poisson equation on smooth or convex domains.} Simple explicit error estimates are established under conditions that can be verified from the outputs of the computation.

</details>


### [2] [Learning solutions of parameterized stiff ODEs using Gaussian processes](https://arxiv.org/abs/2511.05990)
*Idoia Cortes Garcia,P. Förster,W. Schilders,S. Schöps*

Main category: math.NA

TL;DR: This paper proposes a reparameterization method to improve Gaussian process (GP) surrogate modeling for stiff ODE solutions, which are typically nonstationary and challenging for standard GPs.


<details>
  <summary>Details</summary>
Motivation: Stiff ODE solutions have mixed regions of rapid and slow variation, making them nonstationary and difficult for standard GPs to approximate effectively. Direct parameter dependence studies become computationally expensive, requiring efficient surrogate models.

Method: The authors propose a data-driven reparameterization approach that transforms stiff ODE solutions to appear more stationary before applying GP modeling. This is implemented as a separate preprocessing step with minimal computational overhead and no changes to the GP implementation.

Result: The method successfully improves GP performance for approximating stiff ODE solutions by making them appear more stationary through reparameterization, as demonstrated through multiple examples.

Conclusion: The proposed reparameterization approach effectively addresses the nonstationarity challenge in stiff ODE solutions, enabling better GP surrogate modeling performance with minimal computational overhead and implementation complexity.

Abstract: Stiff ordinary differential equations (ODEs) play an important role in many scientific and engineering applications. Often, the dependence of the solution of the ODE on additional parameters is of interest, e.g.\ when dealing with uncertainty quantification or design optimization. Directly studying this dependence can quickly become too computationally expensive, such that cheaper surrogate models approximating the solution are of interest. One popular class of surrogate models are Gaussian processes (GPs). They perform well when approximating stationary functions, functions which have a similar level of variation along any given parameter direction, however solutions to stiff ODEs are often characterized by a mixture of regions of rapid and slow variation along the time axis and when dealing with such nonstationary functions, GP performance frequently degrades drastically. We therefore aim to reparameterize stiff ODE solutions based on the available data, to make them appear more stationary and hence recover good GP performance. This approach comes with minimal computational overhead and requires no internal changes to the GP implementation, as it can be seen as a separate preprocessing step. We illustrate the achieved benefits using multiple examples.

</details>


### [3] [Variable-order fractional wave equation: Analysis, numerical approximation, and fast algorithm](https://arxiv.org/abs/2511.06014)
*Jinhong Jia,Chuanting Jiang,Yiqun Li,Mengmeng Liu,Wenlin Qiu*

Main category: math.NA

TL;DR: Analysis of a variable-order fractional wave equation with fast computational algorithm


<details>
  <summary>Details</summary>
Motivation: To model diffusive wave propagation in viscoelastic media with evolving physical properties and develop efficient computational methods

Method: Ritz-Volterra finite element projection with fast divide-and-conquer algorithm exploiting translational invariance of convolution coefficients

Result: Proved well-posedness and regularity, derived rigorous error estimates, reduced computational complexity from O(MN²) to O(MNlog²N)

Conclusion: The proposed method is theoretically sound and computationally efficient for solving variable-order fractional wave equations

Abstract: We investigate a local modification of a variable-order fractional wave equation, which describes the propagation of diffusive wave in viscoelastic media with evolving physical property. We incorporate an equivalent formulation to prove the well-posedness of the model as well as its high order regularity estimates. To accommodate the convolution term in the reformulated model, we adopt the Ritz-Volterra finite element projection and then derive the rigorous error estimate for the fully-discretized finite element scheme. To circumvent the high computational cost from the temporal integral term, we exploit the translational invariance of the discrete coefficients associated with the convolution structure and construct a fast divide-and-conquer algorithm which reduces the computational complexity from $O(MN^2)$ to $O(MN\log^2 N)$. Numerical experiments are provided to verify the theoretical results and to demonstrate the accuracy and efficiency of the proposed method.

</details>


### [4] [Stability estimates for Interior Penalty D.G. Methods for the Nonlinear Dynamics of the complex Ginzburg Landau equation](https://arxiv.org/abs/2511.06158)
*Dimitrios Kostas*

Main category: math.NA

TL;DR: This paper analyzes three Discontinuous Galerkin (DG) schemes (SIPG, NIPG, IIPG) for solving the complex Landau equation, finding SIPG to be the most robust with bounded norms under nonlinear terms, while IIPG shows superior stability over NIPG.


<details>
  <summary>Details</summary>
Motivation: The complex Landau equation presents significant computational challenges due to its nonlinear imaginary component and rich dynamics, requiring effective numerical methods for applications in nonlinear optics and fluid dynamics.

Method: Used three distinct Discontinuous Galerkin finite element methods (SIPG, NIPG, IIPG) with rigorous stability analysis and comparative study, examining stability and computational efficiency.

Result: All three DG schemes are stable, but SIPG is most robust with bounded norms even with nonlinear terms. IIPG has superior stability properties compared to NIPG. For high penalty parameters, all methods show similar stability behavior.

Conclusion: DG methods are suitable for complex nonlinear reaction-diffusion systems, with SIPG recommended as the most robust scheme, providing a practical framework for selecting efficient methods for specific problems.

Abstract: This study investigates the complex Landau equation, a reaction diffusion system with applications in nonlinear optics and fluid dynamics. The equation's nonlinear imaginary component introduces rich dynamics and significant computational challenges. We address these challenges using Discontinuous Galerkin (DG) finite element methods. A rigorous stability analysis and a comparative study are performed on three distinct DG schemes : Symmetric Interior Penalty Galerkin (SIPG), Nonsymmetric Interior Penalty Galerkin (NIPG), and Incomplete Interior Penalty Galerkin (IIPG). These methods are compared in terms of their stability and computational efficiency. Our numerical analysis and computational results demonstrate that all three discontinuous Galerkin (DG) schemes are stable. However, the Symmetric Interior Penalty Galerkin (SIPG) scheme proves to be the most robust, as its norm remains bounded even in the presence of nonlinear terms a property not shared by the others. A comparison between the Incomplete Interior Penalty Galerkin (IIPG) and Nonsymmetric Interior Penalty Galerkin (NIPG) schemes shows that IIPG has superior stability properties. For high values of the penalty parameter, all methods exhibit similar stability behavior. Our results highlight the suitability of DG methods for simulating complex nonlinear reaction-diffusion systems and provide a practical framework for selecting the most efficient scheme for a given problem.

</details>


### [5] [Quasi-Monte Carlo time-splitting methods for Schrödinger equation with Gaussian random potential](https://arxiv.org/abs/2511.06236)
*Zhizhang Wu,Zhiwen Zhang,Xiaofei Zhao*

Main category: math.NA

TL;DR: Developed an efficient QMC time-splitting method for solving Schrödinger equation with Gaussian random potential, achieving dimension-independent convergence rates.


<details>
  <summary>Details</summary>
Motivation: The unboundedness of Gaussian random variables in Schrödinger equation with Gaussian random potential poses significant difficulties in sampling and error analysis, requiring efficient numerical methods.

Method: Time-splitting discretizations combined with non-standard weighted Sobolev spaces and randomly shifted lattice-based quasi-Monte Carlo quadrature for efficient sampling.

Result: Proved that the QMC time-splitting method achieves dimension-independent convergence rate that is almost linear with respect to the number of QMC samples.

Conclusion: The proposed QMC-TS scheme effectively handles the challenges of Gaussian random potential in Schrödinger equation, with numerical experiments confirming the sharpness of error estimates.

Abstract: In this paper, we study the Schrödinger equation with a Gaussian random potential (SE-GP) and develop an efficient numerical method to approximate the expectation of physical observables. The unboundedness of Gaussian random variables poses significant difficulties in both sampling and error analysis. Under time-splitting discretizations of SE-GP, we establish the regularity of the semi-discrete solution in the random space. Then, we introduce a non-standard weighted Sobolev space with properly chosen weight functions, and obtain a randomly shifted lattice-based quasi-Monte Carlo (QMC) quadrature rule for efficient sampling. This approach leads to a QMC time-splitting (QMC-TS) scheme for solving the SE-GP. We prove that the proposed QMC-TS method achieves a dimension-independent convergence rate that is almost linear with respect to the number of QMC samples. Numerical experiments illustrate the sharpness of the error estimate.

</details>


### [6] [GLT matrix-sequences and few emblematic applications](https://arxiv.org/abs/2511.06312)
*Muhammad Faisal Khan*

Main category: math.NA

TL;DR: This paper proves that the geometric mean of Hermitian positive definite GLT sequences has a GLT symbol equal to the geometric mean of their symbols when the symbols commute, settling a conjecture for certain cases. It also extends results to Karcher means and applies GLT theory to quantum spin systems.


<details>
  <summary>Details</summary>
Motivation: To advance spectral theory of structured matrix-sequences in GLT algebras, specifically addressing the behavior of geometric means of Hermitian positive definite sequences and their applications in mathematical physics, particularly quantum spin systems.

Method: Using Generalized Locally Toeplitz (GLT) *-algebra framework, the authors prove spectral properties of geometric means of HPD GLT sequences, analyze degenerate cases, provide numerical evidence for non-commuting symbols, extend to Karcher means for multiple sequences, and apply to quantum Curie-Weiss model.

Result: When symbols commute, geometric mean sequence is GLT with symbol equal to geometric mean of individual symbols. For degenerate cases, conditions ensure GLT behavior. Numerical evidence shows spectral symbols exist even for non-commuting cases. Extension to Karcher means works similarly. Quantum spin system matrices form GLT sequences with computable spectral distributions.

Conclusion: The GLT framework successfully characterizes spectral behavior of geometric means of matrix sequences, with commuting symbols yielding exact results and applications extending to quantum physics models, demonstrating the power of GLT theory in analyzing structured matrix sequences.

Abstract: This thesis advances the spectral theory of structured matrix-sequences within the framework of Generalized Locally Toeplitz (GLT) $*$-algebras, focusing on the geometric mean of Hermitian positive definite (HPD) GLT sequences and its applications in mathematical physics. For two HPD sequences $\{A_n\}_n \sim_{\mathrm{GLT}} κ$ and $\{B_n\}_n \sim_{\mathrm{GLT}} ξ$ in the same $d$-level, $r$-block GLT $*$-algebra, we prove that when $κ$ and $ξ$ commute, the geometric mean sequence $\{G(A_n,B_n)\}_n$ is GLT with symbol $(κξ)^{1/2}$, without requiring invertibility of either symbol, settling \cite[Conjecture 10.1]{garoni2017} for $r=1$, $d\ge1$. In degenerate cases, we identify conditions ensuring $\{G(A_n,B_n)\}_n \sim_{\mathrm{GLT}} G(κ,ξ)$. For $r>1$ and non-commuting symbols, numerical evidence shows the sequence still admits a spectral symbol, indicating maximality of the commuting result. Numerical experiments in scalar and block settings confirm the theory and illustrate spectral behaviour. We also sketch the extension to $k\ge2$ sequences via the Karcher mean, obtaining $\{G(A_n^{(1)},\ldots,A_n^{(k)})\}_n \sim_{\mathrm{GLT}} G(κ_1,\ldots,κ_k)$. Finally, we apply the GLT framework to mean-field quantum spin systems, showing that matrices from the quantum Curie--Weiss model form GLT sequences with explicitly computable spectral distributions.

</details>


### [7] [A framework of discontinuous Galerkin neural networks for iteratively approximating residuals](https://arxiv.org/abs/2511.06349)
*Long Yuan,Hongxing Rui*

Main category: math.NA

TL;DR: Proposes a discontinuous Galerkin neural network (DGNN) framework for least-squares methods using neural networks as feasible solutions, with improved accuracy over PINN methods.


<details>
  <summary>Details</summary>
Motivation: To develop a more accurate and computationally efficient neural network-based method for solving PDEs compared to existing Physics-Informed Neural Networks (PINNs), addressing issues with convergence and parameter boundedness assumptions.

Method: Defines quadratic loss functional with h-refinement, uses element-wise neural network functions, recursively supplements solutions through quasi-minimization problems, and proposes discontinuous Galerkin Trefftz neural network (DGTNN) with single hidden layer to reduce costs.

Result: Numerical experiments show DGNN with 1-2 hidden layers improves relative L² error by at least one order of magnitude compared to PINN algorithms at low computational costs.

Conclusion: The proposed DGNN framework provides an effective approach for neural network-based PDE solutions with significantly improved accuracy and computational efficiency over existing PINN methods.

Abstract: We propose an abstract discontinuous Galerkin neural network (DGNN) framework for analyzing the convergence of least-squares methods based on the residual minimization when feasible solutions are neural networks. Within this framework, we define a quadratic loss functional as in the least square method with $h-$refinement and introduce new discretization sets spanned by element-wise neural network functions. The desired neural network approximate solution is recursively supplemented by solving a sequence of quasi-minimization problems associated with the underlying loss functionals and the adaptively augmented discontinuous neural network sets without the assumption on the boundedness of the neural network parameters. We further propose a discontinuous Galerkin Trefftz neural network discretization (DGTNN) only with a single hidden layer to reduce the computational costs. Moreover, we design a template based on the considered models for initializing nonlinear weights. Numerical experiments confirm that compared to existing PINN algorithms, the proposed DGNN method with one or two hidden layers is able to improve the relative $L^2$ error by at least one order of magnitude at low computational costs.

</details>


### [8] [On a spectral solver for highly oscillatory and non-smooth solutions of a class of linear fractional differential systems](https://arxiv.org/abs/2511.06410)
*Amin Faghih*

Main category: math.NA

TL;DR: A spectral Galerkin method using Müntz-Jacobi functions is developed for solving fractional differential equations with non-constant coefficients, particularly effective for highly oscillatory and non-smooth problems.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of solving linear systems of fractional differential equations with non-constant coefficients that exhibit highly oscillatory and non-smooth behavior, which traditional methods struggle with.

Method: Developed a spectral Galerkin method based on Müntz-Jacobi functions that can handle non-smooth and highly oscillatory solutions efficiently through recurrence relations, avoiding complex algebraic systems.

Result: The method remains stable at higher approximation degrees and achieves exponential accuracy in the L²-norm, effectively capturing highly oscillatory solutions with high accuracy.

Conclusion: The proposed spectral Galerkin method is validated through numerical examples and theoretical analysis, demonstrating efficiency and accuracy for solving challenging fractional differential equation problems.

Abstract: This study discusses a class of linear systems of fractional differential equations with non-constant coefficients, with a particular focus on problems exhibiting highly oscillatory and non-smooth behavior. We first establish the regularity properties of the solutions under specific conditions on the input data. A spectral Galerkin method based on Müntz-Jacobi functions is developed that efficiently handle the non-smooth and highly oscillatory solutions. A key advantage of the proposed approach is the ability to compute the approximate solution via recurrence relations, avoiding the need to solve complex algebraic systems. Moreover, the method remains stable even at higher approximation degrees, effectively capturing highly oscillatory solutions with high accuracy. The well-known exponential accuracy is established in the $L^2$-norm, and some numerical examples are provided to demonstrate both the validity of the theoretical analysis and the efficiency of the proposed algorithm.

</details>


### [9] [A generalization bound for exit wave reconstruction via deep unfolding](https://arxiv.org/abs/2511.06413)
*Moussa Atwi,Benjamin Berkels*

Main category: math.NA

TL;DR: The paper proposes an unrolled neural network approach for electron microscopy exit wave reconstruction, analyzing parameter perturbation effects and establishing generalization bounds.


<details>
  <summary>Details</summary>
Motivation: Transmission Electron Microscopy produces difficult-to-interpret images, requiring exit wave reconstruction from intensity-only measurements to recover complex-valued electron waves.

Method: Proximal gradient algorithm (PGA) is unfolded into a neural network where each layer corresponds to one PGA step with learnable parameters, enabling improved reconstruction and implicit dictionary learning.

Result: Analysis shows parameter perturbations accumulate exponentially with network layers, and generalization error bounds of order O(√L) are established for the nonlinear phase retrieval problem.

Conclusion: The unrolling approach provides interpretable neural networks for electron microscopy reconstruction with theoretical guarantees on generalization performance.

Abstract: Transmission Electron Microscopy enables high-resolution imaging of materials, but the resulting images are difficult to interpret directly. One way to address this is exit wave reconstruction, i.e., the recovery of the complex-valued electron wave at the specimen's exit plane from intensity-only measurements. This is an inverse problem with a nonlinear forward model. We consider a simplified forward model, making the problem equivalent to phase retrieval, and propose a discretized regularized variational formulation. To solve the resulting non-convex problem, we employ the proximal gradient algorithm (PGA) and unfold its iterations into a neural network, where each layer corresponds to one PGA step with learnable parameters. This unrolling approach, inspired by LISTA, enables improved reconstruction quality, interpretability, and implicit dictionary learning from data. We analyze the effect of parameter perturbations and show that they can accumulate exponentially with the number of layers $L$. Building on proof techniques of Behboodi et al., originally developed for LISTA, i.e., for a linear forward model, we extend the analysis to our nonlinear setting and establish generalization error bounds of order $\mathcal{O}(\sqrt{L})$. Numerical experiments support the exponential growth of parameter perturbations.

</details>


### [10] [A novel phase-field model for $N$-phase problems: modeling, asymptotic analysis and numerical simulations](https://arxiv.org/abs/2511.06462)
*Lun Zhang,Chenxi Wang,Nan Lu,Zhen Zhang*

Main category: math.NA

TL;DR: A novel phase-field modeling framework without simplex constraint for multiphase problems using dichotomic representation and interpolation from two-phase Ginzburg-Landau free energies.


<details>
  <summary>Details</summary>
Motivation: Classical phase-field modeling for multiphase problems requires simplex constraints and faces difficulties in constructing nonlinear potentials, posing challenges for many-phase problems.

Method: Dichotomic approach to represent multiphase, interpolation from classical two-phase Ginzburg-Landau free energies with established interpolation rules and explicit construction of interpolation functions. Uses mobility operator splitting technique for linear, decoupled, energy stable numerical scheme.

Result: Model enjoys energy dissipation property, asymptotically consistent with sharp interface limit, recovers Neumann triangle condition at triple junctions. Validated through numerical examples showing stability, accuracy, and consistency. Successfully simulates liquid lenses, double emulsions, and Janus emulsions with experimental agreement.

Conclusion: The proposed framework overcomes limitations of classical phase-field modeling by eliminating simplex constraints while maintaining consistency properties, enabling efficient simulation of complex multiphase phenomena.

Abstract: The classical phase-field modeling approaches for multiphase problems represent each phase using a regularized characteristic function, which necessarily introduces a simplex constraint for the phase-field variables. Additionally, the consistency requirement for phase-field modeling brings difficulties to the construction of nonlinear potentials in the energy functionals, posing significant challenges for classical phase-field modeling and its numerical methods for problems involving many phases. In this work, by adopting a dichotomic approach to represent multiphase, we propose a novel phase-field modeling framework without simplex constraint,in which the free energy is interpolated from the classical two-phase Ginzburg-Landau free energies. We systematically establish the interpolation rules and explicitly construct the interpolation functions, rendering the consistency properties of the model. The proposed model enjoys an energy dissipation property and is shown to be asymptotically consistent with its sharp interface limit, with the Neumann triangle condition recovered at the triple junction.Based on a mobility operator splitting technique, we develop a linear, decoupled, and energy stable scheme for efficiently solving the system of phase-field equations. The numerical stability and accuracy, as well as the consistency properties of the model, are validated through a large number of numerical examples. In particular, the model demonstrates its success in several benchmark simulations for multiphase problems, such as the formation of liquid lenses between two stratified fluids, the generation of double emulsions and Janus emulsions, showing good agreement with experimental observations.

</details>


### [11] [A B-spline-Heaviside collocation method for solving Fredholm integral equations with piecewise Holder-continuous right-hand sides](https://arxiv.org/abs/2511.06590)
*Maria Capcelea,Titu Capcelea*

Main category: math.NA

TL;DR: A collocation method using B-splines and Heaviside functions to solve linear Fredholm integral equations on complex plane contours with discontinuous right-hand sides.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate numerical method for solving Fredholm integral equations defined on closed contours in the complex plane, particularly when the right-hand side has jump discontinuities.

Method: Combines B-spline functions and Heaviside step functions in a collocation approach to handle discontinuities while maintaining smooth approximation elsewhere on the contour.

Result: Established convergence in piecewise Holder spaces with explicit error estimates, and numerical experiments demonstrate the method's effectiveness and convergence rate.

Conclusion: The proposed hybrid approach successfully handles discontinuous functions in Fredholm integral equations on complex contours, providing both theoretical guarantees and practical numerical performance.

Abstract: This work presents a collocation method for solving linear Fredholm integral equations of the second kind defined on a closed contour in the complex plane. The right-hand side of the equation is a piecewise continuous function that may have a finite number of jump discontinuities and is known numerically at discrete points on the contour. The proposed approach employs a combination of B-spline functions and Heaviside step functions to ensure accurate approximation near discontinuity points and smooth behavior elsewhere on the contour. Convergence in the norm of piecewise Holder spaces is established, together with explicit error estimates. Numerical results illustrate the effectiveness and convergence rate of the method.

</details>


### [12] [Some p-robust a posteriori error estimates based on auxiliary spaces](https://arxiv.org/abs/2511.06603)
*Yuwen Li*

Main category: math.NA

TL;DR: Develops p-robust equilibrated a posteriori error estimates for finite element methods using H^1 auxiliary space decomposition and fictitious space lemma.


<details>
  <summary>Details</summary>
Motivation: To create polynomial-degree-robust error estimators that work effectively across different polynomial degrees in finite element methods.

Method: Uses H^1 auxiliary space decomposition, fictitious space lemma for preconditioning, and H^1 regular decomposition to decompose finite element residuals into H^{-1} residuals controlled by p-robust equilibrated analysis.

Result: Obtained novel p-robust a posteriori error estimates for H(curl) and H(div) conforming methods and mixed method for biharmonic equation, with guaranteed upper error bounds under certain conditions.

Conclusion: The proposed error estimators are effective and p-robust, as demonstrated by numerical experiments for H(curl) conforming and Hellan-Herrmann-Johnson methods.

Abstract: This work develops polynomial-degree-robust (p-robust) equilibrated a posteriori error estimates of finite element methods, based on $H^1$ auxiliary space decomposition. The proposed framework employs the fictitious space lemma for preconditioning and $H^1$ regular decomposition to decompose the finite element residual into $H^{-1}$ residuals that are further controlled by p-robust equilibrated a posteriori error analysis. As a result, we obtain novel p-robust a posteriori error estimates of conforming methods for the $H(\rm curl)$ and $H(\rm div)$ problems and the mixed method for the biharmonic equation. We also prove guaranteed a posteriori upper error bounds under convex domains or certain boundary conditions. Numerical experiments demonstrate the effectiveness and p-robustness of the proposed error estimators for the $H(\rm curl)$ conforming and the Hellan--Herrmann--Johnson methods.

</details>


### [13] [A kernel method for the learning of Wasserstein geometric flows](https://arxiv.org/abs/2511.06655)
*Jianyu Hu,Juan-Pablo Ortega,Daiying Yin*

Main category: math.NA

TL;DR: This paper addresses the inverse problem of recovering potential functions and interaction kernels from discretized observations of Wasserstein gradient and Hamiltonian flows using an optimization framework with RKHS-based approach.


<details>
  <summary>Details</summary>
Motivation: Wasserstein gradient and Hamiltonian flows are important in modeling complex dynamics but the inverse identification of their underlying potential functions and interaction kernels remains relatively unexplored.

Method: Formulates the problem as an optimization task with a specially designed loss function that enforces variational structure, using a kernel-based operator approach in Reproducing Kernel Hilbert Space (RKHS) for closed-form representation.

Result: Provides comprehensive error analysis with convergence rates under adaptive regularization parameters as temporal and spatial discretization mesh sizes approach zero, and presents stability analysis connecting discrete trajectory data to continuous-time flow dynamics.

Conclusion: The framework successfully addresses the inverse problem for Wasserstein Hamiltonian flows, bridging discrete observations with continuous dynamics through rigorous mathematical analysis.

Abstract: Wasserstein gradient and Hamiltonian flows have emerged as essential tools for modeling complex dynamics in the natural sciences, with applications ranging from partial differential equations (PDEs) and optimal transport to quantum mechanics and information geometry. Despite their significance, the inverse identification of potential functions and interaction kernels underlying these flows remains relatively unexplored. In this work, we tackle this challenge by addressing the inverse problem of simultaneously recovering the potential function and interaction kernel from discretized observations of the density flow. We formulate the problem as an optimization task that minimizes a loss function specifically designed to enforce the underlying variational structure of Wasserstein flows, ensuring consistency with the geometric properties of the density manifold. Our framework employs a kernel-based operator approach using the associated Reproducing Kernel Hilbert Space (RKHS), which provides a closed-form representation of the unknown components. Furthermore, a comprehensive error analysis is conducted, providing convergence rates under adaptive regularization parameters as the temporal and spatial discretization mesh sizes tend to zero. Finally, a stability analysis is presented to bridge the gap between discrete trajectory data and continuous-time flow dynamics for the Wasserstein Hamiltonian flow.

</details>


### [14] [An orthogonality-preserving approach for eigenvalue problems](https://arxiv.org/abs/2511.06788)
*Tianyang Chu,Xiaoying Dai,Shengyue Wang,Aihui Zhou*

Main category: math.NA

TL;DR: An intrinsic orthogonality-preserving model and numerical method for large-scale eigenvalue problems that automatically maintains orthogonality and exhibits energy dissipation without CFL condition restrictions.


<details>
  <summary>Details</summary>
Motivation: Solving large-scale eigenvalue problems is challenging due to computational complexity and parallel scalability limitations of orthogonalization operations when many eigenpairs are needed.

Method: Proposed an intrinsic orthogonality-preserving model formulated as an evolution equation and corresponding numerical method that automatically preserves orthogonality and exhibits energy dissipation during time evolution and iterations.

Result: The method provides accurate and efficient approximation for large-scale eigenvalue problems with orthogonality constraints, with rigorous convergence proof without CFL time step restrictions.

Conclusion: Numerical experiments validate theoretical analyses and demonstrate remarkably high efficiency of the algorithm for large-scale eigenvalue problems.

Abstract: Solving large-scale eigenvalue problems poses a significant challenge due to the computational complexity and limitations on the parallel scalability of the orthogonalization operation, when many eigenpairs are required. In this paper, we propose an intrinsic orthogonality-preserving model, formulated as an evolution equation, and a corresponding numerical method for eigenvalue problems. The proposed approach automatically preserves orthogonality and exhibits energy dissipation during both time evolution and numerical iterations, provided that the initial data are orthogonal, thus offering an accurate and efficient approximation for the large-scale eigenvalue problems with orthogonality constraints. Furthermore, we rigorously prove the convergence of the scheme without the time step size restrictions imposed by the CFL conditions. Numerical experiments not only corroborate the validity of our theoretical analyses but also demonstrate the remarkably high efficiency of the algorithm.

</details>


### [15] [Pointwise A Posteriori Error Estimators for Elliptic Eigenvalue Problems](https://arxiv.org/abs/2511.06815)
*Zhenglei Li,Qigang Liang,Xuejun Xu*

Main category: math.NA

TL;DR: Proposes a pointwise a posteriori error estimator for elliptic eigenvalue problems with AFEMs, proving reliability and efficiency in L∞-norm with logarithmic factor.


<details>
  <summary>Details</summary>
Motivation: To develop reliable and efficient error estimators for adaptive finite element methods applied to elliptic eigenvalue problems, enabling accurate error control.

Method: Uses residual-type a posteriori error estimator, analyzes relationship between computable and theoretical estimators, employs estimates for regularized derivative Green's functions, extends to nonconforming finite elements.

Result: Proves reliability and efficiency of the estimator in L∞-norm up to logarithmic factor, validates with numerical experiments.

Conclusion: The proposed error estimator is effective for elliptic eigenvalue problems and can be extended to nonconforming finite element approximations.

Abstract: In this work, we propose and analyze a pointwise a posteriori error estimator for simple eigenvalues of elliptic eigenvalue problems with adaptive finite element methods (AFEMs). We prove the reliability and efficiency of the residual-type a posteriori error estimator in the sense of $L^{\infty}$-norm, up to a logarithmic factor of the mesh size. For theoretical analysis, we also propose a theoretical and non-computable estimator, and then analyze the relationship between computable estimator and theoretical estimator. A key ingredient in the a posteriori error analysis is some new estimates for regularized derivative Green's functions. This methodology is also extended to the nonconforming finite element approximations. Some numerical experiments verify our theoretical results.

</details>


### [16] [Multipoint stress mixed finite element methods for the linear Cosserat equations](https://arxiv.org/abs/2511.06861)
*Wietse M. Boon,Alessio Fumagalli,Jan M. Nordbotten,Ivan Yotov*

Main category: math.NA

TL;DR: Mixed finite element methods for Cosserat materials using quadrature rules to eliminate stress variables locally, resulting in reduced system with only displacement and rotation variables.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for Cosserat materials by reducing computational complexity through local elimination of stress variables.

Method: Four variants of mixed finite element methods using suitable quadrature rules to eliminate Cauchy and coupled stress variables locally, maintaining only displacement and rotation variables.

Result: Stability and convergence proven using a priori estimates, with numerical experiments verifying theoretical findings and showing higher order convergence in some variables.

Conclusion: The proposed methods successfully reduce system complexity while maintaining stability and convergence properties, with potential for higher order convergence in certain applications.

Abstract: We propose mixed finite element methods for Cosserat materials that use suitable quadrature rules to eliminate the Cauchy and coupled stress variables locally. The reduced system consists of only the displacement and rotation variables. Four variants are proposed for which we show stability and convergence using a priori estimates. Numerical experiments verify the theoretical findings and higher order convergence is observed in some variables.

</details>


### [17] [A Convergent Algorithm Based on Deterministic Approximation for a Large Class of Regime-Switching Generalized Stochastic Game-Theoretic Riccati Differential Equations](https://arxiv.org/abs/2511.06920)
*Yiyuan Wang*

Main category: math.NA

TL;DR: Novel iterative algorithm for computing stabilizing solutions of regime-switching stochastic game-theoretic Riccati differential equations with periodic coefficients.


<details>
  <summary>Details</summary>
Motivation: Need for computational methods to solve complex regime-switching stochastic game-theoretic Riccati differential equations, which previously lacked general solution approaches.

Method: Decomposes stochastic problem into deterministic subproblems, sequentially solves for minimal Riccati solutions, constructs iterative sequence, and proves convergence using comparison theorem.

Result: Algorithm successfully computes stabilizing solutions, with numerical experiments verifying effectiveness and stability.

Conclusion: First general computational approach developed for regime-switching stochastic game-theoretic Riccati differential equations with proven convergence properties.

Abstract: This paper proposes a novel iterative algorithm to compute the stabilizing solution of regime-switching stochastic game-theoretic Riccati differential equations with periodic coefficients. The method decomposes the original complex stochastic problem into a sequence of deterministic subproblems. By sequentially solving for the minimal solutions of the Riccati differential equations in each subproblem, a sequence of matrix-valued functions is constructed. Leveraging the comparison theorem, the monotonicity, boundedness, and convergence of the iterative sequence are rigorously proven. Numerical experiments verifies algorithm effectiveness and stability. To the best of our knowledge, this is the first general computational approach developed for this class of problems.

</details>


### [18] [A Provably-Correct and Robust Convex Model for Smooth Separable NMF](https://arxiv.org/abs/2511.07109)
*Junjun Pan,Valentin Leplat,Michael Ng,Nicolas Gillis*

Main category: math.NA

TL;DR: The paper introduces smooth separable NMF (SSNMF), a variant of NMF that assumes each basis vector is close to multiple data points, addressing limitations of standard separable NMF in noisy real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Standard separable NMF (SNMF) assumes basis vectors exactly match input columns, but this fails in noisy environments where multiple data points cluster near basis vectors. SSNMF addresses this by leveraging the proximity of multiple data points to each basis vector.

Method: Proposes a convex model for SSNMF that provably recovers factors even with noise, and adapts an existing fast gradient method to solve the convex optimization problem efficiently.

Result: The convex SSNMF model successfully recovers the true factors in both synthetic and real hyperspectral datasets, outperforming state-of-the-art methods and demonstrating robustness to noise.

Conclusion: SSNMF provides a more realistic and robust alternative to SNMF for real-world applications with noise, offering theoretical guarantees and practical performance improvements through a convex formulation and efficient optimization approach.

Abstract: Nonnegative matrix factorization (NMF) is a linear dimensionality reduction technique for nonnegative data, with applications such as hyperspectral unmixing and topic modeling. NMF is a difficult problem in general (NP-hard), and its solutions are typically not unique. To address these two issues, additional constraints or assumptions are often used. In particular, separability assumes that the basis vectors in the NMF are equal to some columns of the input matrix. In that case, the problem is referred to as separable NMF (SNMF) and can be solved in polynomial-time with robustness guarantees, while identifying a unique solution. However, in real-world scenarios, due to noise or variability, multiple data points may lie near the basis vectors, which SNMF does not leverage. In this work, we rely on the smooth separability assumption, which assumes that each basis vector is close to multiple data points. We explore the properties of the corresponding problem, referred to as smooth SNMF (SSNMF), and examine how it relates to SNMF and orthogonal NMF. We then propose a convex model for SSNMF and show that it provably recovers the sought-after factors, even in the presence of noise. We finally adapt an existing fast gradient method to solve this convex model for SSNMF, and show that it compares favorably with state-of-the-art methods on both synthetic and hyperspectral datasets.

</details>


### [19] [Towards a parallel Schwarz solver framework for virtual elements using GDSW coarse spaces](https://arxiv.org/abs/2511.07144)
*Tommaso Bevilacqua,Axel Klawonn,Martin Lanser,Adam Wasiak*

Main category: math.NA

TL;DR: Parallel implementation of two-level overlapping Schwarz preconditioners with GDSW coarse spaces for Virtual Element Method discretizations of Poisson problems on polygonal meshes.


<details>
  <summary>Details</summary>
Motivation: To develop robust and scalable parallel solvers for Virtual Element Method discretizations on complex polygonal meshes, addressing the need for efficient solution of the resulting symmetric positive definite linear systems.

Method: Use overlapping Schwarz domain decomposition preconditioners with GDSW coarse space variants, implemented in PETSc combined with Vem++ library for parallel computation.

Result: Numerical experiments in 2D and 3D demonstrate scalability up to 1,000 parallel cores for VEM discretizations of degrees k=1,2.

Conclusion: The proposed two-level GDSW preconditioners provide effective parallel scalability for VEM discretizations, representing the first successful parallel application of these preconditioners to VEM.

Abstract: The Virtual Element Method (VEM) is used to perform the discretization of the Poisson problem on polygonal and polyhedral meshes. This results in a symmetric positive definite linear system, which is solved iteratively using overlapping Schwarz domain decomposition preconditioners, where to ensure robustness and parallel scalability a second level has to be employed. The construction and numerical study of two-level overlapping Schwarz preconditioners with variants of the GDSW (Generalized Dryja-Smith-Widlund) coarse space are presented here. Our PETSc-based parallel implementation of GDSW and variants, combined with the Vem++ library, represent the first parallel application of these GDSW preconditioners to VEM. Numerical experiments in 2D and 3D demonstrate scalability of our preconditioners up to 1 000 parallel cores for VEM discretizations of degrees k=1,2.

</details>


### [20] [Multicentric representation of piecewise constant holomorphic functions and Hermite interpolation](https://arxiv.org/abs/2511.07174)
*Olavi Nevanlinna,Tiina Vesanen*

Main category: math.NA

TL;DR: This paper analyzes multicentric representations of piecewise holomorphic functions using Lagrange interpolation and truncated power series, showing that Hermite interpolation polynomials derived from this approach have better numerical stability with bounded errors as truncation increases.


<details>
  <summary>Details</summary>
Motivation: To develop stable numerical methods for representing piecewise holomorphic functions by combining Lagrange interpolation with convergent power series, addressing the numerical error accumulation problem in traditional Hermite interpolation approaches.

Method: Combines Lagrange interpolation at polynomial roots with truncated power series of the polynomial as coefficients for Lagrange basis polynomials, formally obtaining Hermite interpolation polynomials of degree d(n+1)-1 where d is polynomial degree and n is truncation power.

Result: Multicentric representation maintains bounded numerical errors as truncation power n grows, unlike traditional Hermite interpolation where errors accumulate. Even for simple cases (function equals 1 in one component and 0 elsewhere), the truncated multicentric representation performs favorably.

Conclusion: The multicentric representation with truncated power series provides numerically stable Hermite interpolation with bounded error accumulation, making it superior to traditional approaches that require linear combinations of d(n+1) basis polynomials.

Abstract: In multicentric representation of piecewise holomorphic functions one combines Lagrange interpolation at roots of a polynomial $p$ with convergent power series of $p$ as the "coefficients" multiplying the Lagrange basis polynomials. When these power series are truncated one obtains Hermite interpolation polynomials. In this paper we first review different approaches to obtain multicentric representations with emphasis in piecewise constant holomorphic functions. When the polynomial is of degree $d$ and all power series are truncated after $n^{th}$ power, we formally arrive into a Hermite interpolation polynomial of degree $d(n+1)-1$. The natural way to represent Hermite interpolation is to have for each interpolation condition a basis polynomial which in this case leads to $d(n+1)$ basis polynomials. We then consider the numerical accumulation of errors in the different ways to represent and evaluate the Hermite interpolation. In the multicentric representation due to the convergence of the power series, numerical errors stay bounded as $n$ grows. When we assume that the piecewise constant holomorphic function takes the value $1$ in one of the components and vanishes in the other so that the Hermite interpolation agrees with just one basis polynomial, even then the truncated multicentric representation is favorable. In the general case one would take a linear combination of all $d(n+1)$ basis polynomials.

</details>


### [21] [A General Probability Density Framework for Local Histopolation and Weighted Function Reconstruction from Mesh Line Integrals](https://arxiv.org/abs/2511.07259)
*Francesco Dell'Accio,Allal Guessab,Mohammed Kbiri Alaoui,Federico Nudo*

Main category: math.NA

TL;DR: The paper presents new quadratic reconstruction methods for bivariate functions from weighted edge integrals using generalized truncated normal distributions, improving accuracy while maintaining locality and simplicity.


<details>
  <summary>Details</summary>
Motivation: To address the important problem in tomography, computer vision, and numerical approximation of reconstructing bivariate functions from weighted integrals along triangular mesh edges, with improved accuracy over standard linear methods.

Method: Uses local histopolation with unisolvent triples and introduces two new families of generalized truncated normal distributions to create quadratic reconstruction operators that generalize linear histopolation.

Result: Theoretical foundations established (unisolvency, explicit basis functions), numerical tests show improved accuracy, and an algorithm for optimal parameter selection ensures robustness and adaptivity.

Conclusion: The framework extends to any bivariate function with valid probability densities on edges, demonstrating broad applicability and generality beyond the specific distributions introduced.

Abstract: In this paper, we study the reconstruction of a bivariate function from weighted integrals along the edges of a triangular mesh, a problem of central importance in tomography, computer vision, and numerical approximation. Our approach relies on local histopolation methods defined through unisolvent triples, where the edge weights are induced by suitable probability densities. In particular, we introduce two new two-parameter families of generalized truncated normal distributions, which extend classical exponential-type laws and provide additional flexibility in capturing local features of the target function. These distributions give rise to new quadratic reconstruction operators that generalize the standard linear histopolation scheme, while retaining its simplicity and locality. We establish their theoretical foundations, proving unisolvency and deriving explicit basis functions, and we demonstrate their improved accuracy through extensive numerical tests. Moreover, we design an algorithm for the optimal selection of the distribution parameters, ensuring robustness and adaptivity of the reconstruction. Finally, we show that the proposed framework naturally extends to any bivariate function whose restriction to the edges defines a valid probability density, thus highlighting its generality and broad applicability.

</details>


### [22] [High-dimensional Bayesian filtering through deep density approximation](https://arxiv.org/abs/2511.07261)
*Kasper Bågmark,Filip Rydin*

Main category: math.NA

TL;DR: Benchmarking of two deep density methods (deep splitting filter and deep BSDE filter) for nonlinear filtering, showing superior performance in high-dimensional settings compared to particle filters and ensemble Kalman filters.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional particle filters and ensemble Kalman filters in high-dimensional nonlinear filtering problems, particularly when dealing with discretely observed stochastic differential equations.

Method: Two deep density filters based on Feynman-Kac formulas, Euler-Maruyama discretizations, and neural networks, extended with logarithmic formulations for robustness in high dimensions.

Result: In low dimensions, particle filters work well, but in a 100-dimensional Lorenz-96 model, particle-based methods fail while the logarithmic deep density method succeeds. Deep density methods reduce inference time by 2-5 orders of magnitude.

Conclusion: Deep density methods provide computationally efficient and robust solutions for high-dimensional nonlinear filtering problems where traditional particle-based methods fail.

Abstract: In this work, we benchmark two recently developed deep density methods for nonlinear filtering. Starting from the Fokker--Planck equation with Bayes updates, we model the filtering density of a discretely observed SDE. The two filters: the deep splitting filter and the deep BSDE filter, are both based on Feynman--Kac formulas, Euler--Maruyama discretizations and neural networks. The two methods are extended to logarithmic formulations providing sound and robust implementations in increasing state dimension. Comparing to the classical particle filters and ensemble Kalman filters, we benchmark the methods on numerous examples. In the low-dimensional examples the particle filters work well, but when we scale up to a partially observed 100-dimensional Lorenz-96 model the particle-based methods fail and the logarithmic deep density method prevails. In terms of computational efficiency, the deep density methods reduce inference time by roughly two to five orders of magnitude relative to the particle-based filters.

</details>


### [23] [Quadratic Weighted Histopolation on Tetrahedral Meshes with Probabilistic Degrees of Freedom](https://arxiv.org/abs/2511.07271)
*Allal Guessab,Federico Nudo*

Main category: math.NA

TL;DR: Three 3D weighted quadratic enrichment strategies for improving local histopolation accuracy on tetrahedral meshes, using different combinations of face, volume, and edge moments with probability densities.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of local histopolation on tetrahedral meshes beyond classical linear approaches.

Method: Three complementary strategies: 1) face-volume using face and interior weighted moments, 2) purely volumetric using only volumetric quadratic moments, 3) edge-face using edge-supported probabilistic moments. All use integral functionals with probability densities and orthogonal polynomials in quadratic trial spaces.

Result: Comprehensive analysis establishes unisolvence and necessary/sufficient conditions for well-posedness. Representative density families examined, adaptive algorithm selects optimal parameters. Numerical experiments show substantial accuracy improvements over classical linear histopolation.

Conclusion: The proposed weighted quadratic enrichment strategies significantly enhance histopolation accuracy on tetrahedral meshes compared to traditional linear methods.

Abstract: In this paper we introduce three complementary three-dimensional weighted quadratic enrichment strategies to improve the accuracy of local histopolation on tetrahedral meshes. The first combines face and interior weighted moments (face-volume strategy), the second uses only volumetric quadratic moments (purely volumetric strategy), and the third enriches the quadratic space through edge-supported probabilistic moments (edge-face strategy). All constructions are based on integral functionals defined by suitable probability densities and orthogonal polynomials within quadratic trial spaces. We provide a comprehensive analysis that establishes unisolvence and derives necessary and sufficient conditions on the densities to guarantee well-posedness. Representative density families, including two-parameter symmetric Dirichlet laws and convexly blended volumetric families, are examined in detail, and a general procedure for constructing the associated quadratic basis functions is outlined. For all admissible densities, an adaptive algorithm automatically selects optimal parameters. Extensive numerical experiments confirm that the proposed strategies yield substantial accuracy improvements over the classical linear histopolation scheme.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [24] [Molecular Seeds of Shear: An operator-level necessity result for first-order Chapman-Enskog deviatoric stress](https://arxiv.org/abs/2511.05514)
*Tristan Barkman*

Main category: math.AP

TL;DR: Anisotropic stress in kinetic systems appears only when the first Chapman-Enskog correction is nonzero, proved under specific operator assumptions.


<details>
  <summary>Details</summary>
Motivation: To rigorously establish the precise conditions under which anisotropic stress emerges in kinetic systems and understand how microscopic fluctuations project into macroscopic amplification channels.

Method: Functional-analytic formulation with Chapman-Enskog scaling, using nullspace, coercivity, and Fredholm solvability assumptions on the linearized collision operator, with uniform remainder control.

Result: Vanishing first Chapman-Enskog correction implies absence of O(ε) deviatoric stress in the hydrodynamic limit, with BGK example and remainder estimates provided.

Conclusion: The result clarifies the mathematical conditions for anisotropic stress emergence and has implications for understanding transition and turbulence mechanisms from microscopic seeds.

Abstract: We provide an explicit functional-analytic formulation and rigorous derivation showing that, in closed and unforced kinetic systems under Chapman-Enskog scaling, anisotropic (deviatoric) stress appears only when the first Chapman-Enskog kinetic correction $f^{(1)}$ is nonzero. Under precise nullspace, coercivity, and Fredholm solvability assumptions on the linearized collision operator, and with uniform remainder control, we prove that a vanishing first correction implies the absence of $O(\varepsilon)$ deviatoric stress in the hydrodynamic limit. A fully worked BGK example and remainder estimates are included. We discuss implications for how microscopic seeds (deterministic departures or finite-$N$ fluctuations) project into macroscopic amplification channels for transition and turbulence. The result is proved under explicit nullspace, coercivity (or hypocoercivity) and Fredholm solvability hypotheses for the linearized collision operator, together with uniform remainder bounds; these assumptions are stated precisely in Section 4 and Appendix A and restrict the class of admissible interaction kernels to those listed in A1.

</details>


### [25] [On semilinear damped wave equations with initial data in homogeneous sobolev spaces](https://arxiv.org/abs/2511.05670)
*Mitsuhiro Matsunaga*

Main category: math.AP

TL;DR: Study of semilinear damped wave equations with initial data in negative Sobolev spaces, extending previous work to the case γ ≥ n/2.


<details>
  <summary>Details</summary>
Motivation: Previous research by Chen-Reissig covered 0<γ<n/2 and identified critical exponent p_crit=1+4/(n+2γ). This paper aims to extend the analysis to the case γ ≥ n/2.

Method: Analysis of semilinear damped equations u_tt + u_t - Δu = |u|^p with initial data in (Ḣ^{-γ}∩H^s)×(Ḣ^{-γ}∩L^2) spaces.

Result: The paper discusses the critical exponent p_crit that distinguishes between global existence and blow-up of solutions for the case γ ≥ n/2.

Conclusion: Extends the understanding of critical exponents in semilinear damped wave equations to larger values of γ, completing the analysis started by previous research.

Abstract: In this paper, we study semilinear damped equations $u_{tt}+u_t-Δu=|u|^p$ with the initial data in $({\dot{H}^{-γ}}\cap H^s)\times({\dot{H}^{-γ}}\cap L^2)$. Chen-Reissig \cite{chenreissig2023} studied the case $0<γ<\frac{n}{2}$ and showed that the exponent $p_{\mathrm{crit}}=1+\frac{4}{n+2γ}$ of $p$ distinguishes the time global existence and the blow-up of solution. In this paper, we discuss the case $γ\ge\frac{n}{2}$.

</details>


### [26] [Electron beams: partially flat solutions of a nonlinear elliptic equation with a singular absorption term](https://arxiv.org/abs/2511.05677)
*Jesús Ildefonso Díaz*

Main category: math.AP

TL;DR: Rigorous mathematical treatment of Child-Langmuir law for electron beams between electrodes, addressing edge effects when electrode separation exceeds area dimensions and resolving open questions about current singularities.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous mathematical foundation for previous studies on Child-Langmuir law that left open questions about edge effects and current singularities near cathode edges when electrode separation D > √A.

Method: Mathematical analysis of singular semilinear equations with boundary conditions, examining the need for current singularities j(x) near cathode edges to obtain partially flat solutions u(x,y) with u = ∂u/∂n = 0 on part of boundary.

Result: Established rigorous treatment confirming that current singularity j(x) is required near cathode edges to achieve partially flat solutions when edge effects become significant (D > √A).

Conclusion: The study provides mathematical validation for previous conjectures about Child-Langmuir law behavior under edge effects, resolving open questions about current singularities and boundary conditions in the regime where electrode separation exceeds characteristic area dimensions.

Abstract: In the so-called Child-Langmuir law, established since 1911, an electron beam is formed linking two electrodes, which are assumed to be two parallel plates of area $A$, separated to a finite distance $D.$ When $% D\ll \sqrt{A},$ \textquotedblleft edge effects\textquotedblright\ are negligible and the modelling is reduced to a nonlinear boundary problem for a singular ordinary differential equation\ in which a constant coefficient (the generated electric current $j$) must be found in order to get simultaneously Dirichlet and Neumann homogeneous boundary conditions in one of the extremes. If $D>\sqrt{A},$ then the problem becomes much more difficult since the \textquotedblleft edge effects\textquotedblright\ arise in the plane $(x,y)$ and the electric current (now $j(x)$ due to the presence of a very large perpendicular magnetic field) must be determined in order to get solutions $u(x,y)$ of a singular semilinear equation which are partially flat ($u=\frac{\partial u}{\partial n}=0$ on a part of the boundary). In this paper, we offer a rigorous mathematical treatment of some former studies (Joel Lebowitz and Alexander Rokhenko (2003) and Alexander Rokhenko (2006)), where several open questions were left open: for instance, the need for a singularity of $j(x)$ near the cathode edge to get such partially flat solutions.

</details>


### [27] [On the asymptotically linear problem for an elliptic equation with an indefinite nonlinearity](https://arxiv.org/abs/2511.05679)
*Mónica Clapp,Cristian Morales-Encinos,Alberto Saldaña,Mayra Soares*

Main category: math.AP

TL;DR: The paper studies a semilinear elliptic problem with sign-changing weight arising in optical waveguides, proving existence and uniqueness of positive solutions near the linear case, along with spectral analysis and geometric inequalities.


<details>
  <summary>Details</summary>
Motivation: This equation models optical waveguides and features indefinite nonlinearity due to the sign-changing weight Q_Ω, which presents mathematical challenges requiring new analytical approaches.

Method: Combines detailed eigenvalue analysis with variational methods and blow-up techniques in the asymptotically linear regime, plus comprehensive spectral analysis of the linear problem.

Result: For p > 2 sufficiently close to 2, the problem admits a unique positive solution that is nondegenerate. Also establishes spectral properties, decay estimates, symmetry results, and analogues of Faber-Krahn and Hong-Krahn-Szegö inequalities.

Conclusion: The study provides complete understanding of the semilinear elliptic problem with sign-changing weight, establishing existence, uniqueness, and spectral properties while extending classical geometric inequalities to this non-standard setting.

Abstract: We study the semilinear elliptic problem \[ -Δu = Q_Ω |u|^{p-2}u \quad \text{in } \mathbb{R}^N, \] where \( Q_Ω = χ_Ω - χ_{\mathbb{R}^N \setminus Ω} \) for a bounded smooth domain \( Ω\subset \mathbb{R}^N \), \( N \ge 3 \), and \( 1 < p < 2^{*} \). This equation arises in the study of optical waveguides and exhibits indefinite nonlinearity due to the sign-changing weight \( Q_Ω \). We prove that, for \( p > 2 \) sufficiently close to \( 2 \), the problem admits a unique positive solution, which is nondegenerate. Our approach combines a detailed analysis of an associated eigenvalue problem involving \( Q_Ω \) with variational methods and blow-up techniques in the asymptotically linear regime. We also provide a comprehensive study of the spectral properties of the corresponding linear problem, including the existence and qualitative behavior of eigenfunctions, sharp decay estimates, and symmetry results. In particular, we establish analogues of the Faber--Krahn and Hong--Krahn--Szeg{ö} inequalities in this non-standard setting.

</details>


### [28] [Gradient regularity for strongly singular or degenerate elliptic and parabolic equations](https://arxiv.org/abs/2511.05692)
*Pasquale Ambrosio*

Main category: math.AP

TL;DR: Advances in regularity theory for weak solutions to elliptic and parabolic equations with singular/degenerate structure, focusing on Besov and Sobolev regularity outside origin.


<details>
  <summary>Details</summary>
Motivation: To extend regularity theory for equations that only satisfy standard p-growth and p-ellipticity conditions away from the origin, addressing both singular and degenerate cases.

Method: Analyze weak solutions using nonlinear functions of gradients, covering subquadratic (1<p<2) and superquadratic (p≥2) regimes in elliptic setting, with analogous parabolic framework.

Result: Obtained Besov and Sobolev regularity results for gradient functions in elliptic case, and higher spatial/temporal differentiability in parabolic case under appropriate data assumptions.

Conclusion: Successfully developed comprehensive regularity theory for singular/degenerate elliptic and parabolic equations, extending classical results to non-standard growth conditions.

Abstract: We present recent advances in the regularity theory for weak solutions to some classes of elliptic and parabolic equations with strongly singular or degenerate structure. The equations under consideration satisfy standard $p$-growth and $p$-ellipticity conditions only outside a ball centered at the origin. In the elliptic setting, we describe Besov and Sobolev regularity results for suitable nonlinear functions of the gradient of the weak solutions, covering both the subquadratic ($1<p<2$) and superquadratic ($p\geq2$) regimes. Analogous results are obtained in the corresponding parabolic framework, where we address the higher spatial and temporal differentiability of the solutions under appropriate assumptions on the data.

</details>


### [29] [Factorization method for the biharmonic scattering problem for an absorbing penetrable obstacle](https://arxiv.org/abs/2511.05711)
*Rafael Ceja Ayala,Isaac Harris,General Ozochiawaeze*

Main category: math.AP

TL;DR: Extends factorization method to reconstruct absorbing penetrable obstacles in thin elastic plates using biharmonic wave equation modeling and far-field spectral data.


<details>
  <summary>Details</summary>
Motivation: To develop an inverse scattering method for locating and characterizing absorbing penetrable obstacles in thin elastic plates, which has applications in non-destructive testing and structural health monitoring.

Method: Uses the factorization method with rigorous justification, employing the two-dimensional biharmonic wave equation in frequency domain and analyzing spectral data of the far-field operator to distinguish interior vs exterior sampling points.

Result: Provides a binary criterion for obstacle localization using far-field operator spectral data, and numerically analyzes Born approximation accuracy for weak scatterers by computing relative error against exact far-field data.

Conclusion: The factorization method is successfully extended to biharmonic scattering problems, providing effective obstacle reconstruction, and the Born approximation is validated as a useful though limited approximation for weak scatterers in this context.

Abstract: This work extends the factorization method to the inverse scattering problem of reconstructing the shape and location of an absorbing penetrable obstacle embedded in a thin infinite elastic (Kirchhoff--Love) plate. With the assumption that the plate thickness is small compared to the wavelength of the incident wave, the propagation of flexural perturbations is modeled by the two--dimensional biharmonic wave equation in the frequency domain. Within this setting, we provide a rigorous justification of the factorization method and demonstrate that it yields a binary criterion for distinguishing whether a sampling point lies inside or outside the scattering obstacle, using only the spectral data of the far--field operator. In addition, we numerically analyze the Born approximation for weak scatterers in this biharmonic scattering context and compute the relative error against exact far--field data for sample weak scatterers, thereby quantifying its validity as a limited but useful approximation.

</details>


### [30] [Plankton-Oxygen Dynamics in the Context of Climate Change: A Fractional Model with A Probability Density Function Approach](https://arxiv.org/abs/2511.05742)
*Mahmoud M. El-Borai,Wagdy G. El-Sayed,Mahmoud A. Habib*

Main category: math.AP

TL;DR: The paper develops a fractional-order nonlinear model for plankton-oxygen dynamics to study climate change impacts on marine oxygen production, proving the model's well-posedness through rigorous mathematical analysis.


<details>
  <summary>Details</summary>
Motivation: To capture memory effects in marine ecosystems that classical integer-order models miss, and provide a sound mathematical foundation for analyzing climate change impacts on marine oxygen production.

Method: Formulated a 3D fractional-order nonlinear system using a probability density function kernel, derived Lipschitz bounds for the nonlinear term, and proved existence, uniqueness, and continuous dependence using generalized Grönwall inequality.

Result: Established computable Lipschitz bounds ensuring existence and uniqueness of solutions, and proved continuous dependence on initial conditions, validating the model's well-posedness.

Conclusion: The fractional framework successfully captures memory effects in plankton-oxygen dynamics and provides rigorous mathematical justification for numerical simulations and policy analysis of marine ecosystem management under climate change.

Abstract: We analyze how climate change affects marine oxygen production by modeling plankton--oxygen dynamics with a fractional-order nonlinear system and establishing rigorous conditions for the model's well-posedness. We formulate a three-dimensional system $d^αx(t)/dt^α= A x(t) + f(x(t))$, where $A$ is a diagonal $3\times 3$ matrix and $f$ is nonlinear. We (i) rigorously state the model, (ii) derive a Lipschitz constant for $f$ under suitable assumptions, and (iii) prove existence, uniqueness, and continuous dependence on initial data using a fractional formula with a probability density kernel and a generalized Gr"onwall inequality. Under the stated conditions, $f$ satisfies a computable Lipschitz bound that yields existence and uniqueness of solutions for the fractional system, and the solutions depend continuously on initial conditions, establishing the well-posedness of the plankton--oxygen model. We introduce a fractional, PDF-kernel--based framework for plankton--oxygen dynamics and provide general proofs of well-posedness via a generalized Gr"onwall approach, capturing memory effects that classical integer-order models miss. These results justify numerical simulations and sensitivity analyses of fractional marine-ecosystem models, providing a sound basis for testing mitigation and management strategies affecting oxygen dynamics and supporting evidence-based policies for protecting marine ecosystems under global warming.

</details>


### [31] [Stability of variable-coefficient parabolic equations in non-cylindrical domains](https://arxiv.org/abs/2511.05783)
*Lingyang Liu*

Main category: math.AP

TL;DR: Analyzes stability of parabolic equations with space-time-dependent coefficients in non-cylindrical domains, establishing L∞ and L² norm estimates and investigating stability for specific boundary curves.


<details>
  <summary>Details</summary>
Motivation: To study stability of parabolic equations in non-cylindrical domains where solutions may not exhibit exponential decay unlike in cylindrical domains.

Method: Uses weighted multiplier method and comparison principle to establish norm estimates.

Result: Establishes L∞- and L²-norm estimates for solutions and investigates stability for specific boundary curves.

Conclusion: Demonstrates that in non-cylindrical domains, energy of solutions may not decay exponentially, unlike in cylindrical domains.

Abstract: This paper addresses the stability of parabolic equations with space-time-dependent coefficients in non-cylindrical domains. We establish $L^\infty$- and $L^2$-norm estimates for solutions of the system. Based on these estimates, we investigate the stability of solutions for specific boundary curves. Unlike in cylindrical domains, the energy of solutions to such equations may not exhibit exponential decay. The techniques used in the proof rely on the weighted multiplier method and the comparison principle.

</details>


### [32] [On the exchange of stability for the subcritical laminar flow](https://arxiv.org/abs/2511.05942)
*Vladimir Kozlov,Oleg Motygin*

Main category: math.AP

TL;DR: Analysis of steady water waves with constant vorticity, focusing on the second eigenvalue of the Fréchet derivative near laminar flow and its dependence on depth and vorticity.


<details>
  <summary>Details</summary>
Motivation: To understand the stability properties and subharmonic bifurcations of rotational water waves with constant vorticity, particularly how the second eigenvalue's sign affects exchange of stability principle.

Method: Analytic branch approach starting from subcritical laminar flow, using period as bifurcation parameter. Study of Fréchet derivative eigenvalues and their dependence on Bernoulli constant (depth) and vorticity.

Result: For each vorticity a, there exists critical depth d₀(a) where second eigenvalue changes sign. Positive sign for d<d₀(a), negative for d>d₀(a). Relationship between d₀(a) and stagnation depth d_s(a) depends on vorticity sign.

Conclusion: The second eigenvalue's sign determines period behavior along bifurcation curve and formal stability domains. Numerical results confirm theoretical predictions about stability exchange principle.

Abstract: We consider steady water waves in two-dimensional channel bounded below by a flat, rigid bottom and above by a free surface. The surface tension is neglected and the water motion is rotational with a constant vorticity $a$. We consider an analytic branch of Stokes waves started from a subcritical laminar flow, where the period is considered as the bifurcation parameter. The first eigenvalue of the Fréchet derivative on this branch is always negative. The main object of our study is the second eigenvalue of the Fréchet derivative at this branch in a neighborhood of the laminar flow. This is a small eigenvalue and the positive sign corresponds to the confirmation of the principle of exchange of stability and the negative sign to its violation. We consider the dependence of the sign on the Bernoulli constant $R$ (or the depth $d$ of the laminar flow) and the value of the constant vorticity $a$. We show that for each $a$ there exist a depth $d_0(a)$ such that the sign of the second eigenvalue is positive for $d<d_0(a)$ and negative for $d>d_0(a)$. If $d_s(a)$ is the depth where a stagnation point appears on the corresponding laminar flow then $d_0(a)<d_s(a)$ for positive $a$ and $d_0(a)>d_s(a)$ for negative $a<-1.01803$. The sign of the second eigenvalue is important in study of subharmonic bifurcations. Another observed property consists of the following: if the sign of the second eigenvalue is positive then the period is increasing along the bifurcation curve in a neighborhood of the bifurcation point, if the sign is negative then the period is decreasing there. We also verify the property of formal stability by a description of the domain in $(a,d)$ variables, where this property holds. Numerical illustrations of these properties are presented in the paper.

</details>


### [33] [Co-rotating nearly parallel helical vortices with small cross-section in 3D incompressible Euler equations](https://arxiv.org/abs/2511.05956)
*Daomin Cao,Jie Wan*

Main category: math.AP

TL;DR: The paper studies clustered solutions to a semilinear elliptic equation and applies the results to prove existence of helical vortex filaments in 3D Euler equations.


<details>
  <summary>Details</summary>
Motivation: To understand clustered solutions in elliptic equations and justify/existential results for helical vortex filaments in fluid dynamics, particularly validating previous work by Klein, Majda and Damodaran.

Method: Uses Green's function of the elliptic operator and finite-dimensional reduction method to prove existence of clustered solutions with specific cluster distance and structure.

Result: Proves existence of clustered solutions with cluster point 0 and cluster distance |lnε|^(-1/2), and existence of traveling-rotating helical vorticity fields in 3D Euler equations with various asymmetric configurations.

Conclusion: The analysis provides rigorous mathematical justification for helical vortex filament models in fluid dynamics and extends previous results by considering various asymmetric co-rotating helical filament configurations.

Abstract: In this article, we consider clustered solutions to a semilinear elliptic equation in divergence form \begin{equation*} \begin{cases} -\varepsilon^2\text{div}(K(x)\nabla u)= (u-q|\ln\varepsilon|)^{p}_+,\ \ &x\in Ω,\\ u=0,\ \ &x\in\partial Ω\end{cases} \end{equation*} for small values of $ \varepsilon $. Using Green's function of the elliptic operator $ -\text{div}(K(x)\nabla) $ and finite-dimensional reduction method, we prove that there exist clustered solutions with cluster point $ 0 $ and cluster distance $ |\ln\varepsilon| ^{-\frac{1}{2}} $ whose small-structure is governed by some functional $ H_N $ determined by $ K $ and $ q $. As an application, we prove the existence of traveling-rotating helical vorticity fields to 3D incompressible Euler equations in infinite cylinders, whose support sets consist of helical tubes with small cross-section of radius $ \varepsilon $ and arbitrary circulation $ κ$ and concentrates near ``$ 2N $'' and ``$ 2N+1 $'' type of co-rotating helical solutions of nearly parallel vortex filaments model as $ \varepsilon\to0 $, which justifies the result in Klein, Majda and Damodaran [1995, JFM] and generalizes results in Guerra and Musso [arxiv: 2502.01470]. Several kinds of solutions such as ``2 asymmetric'', ``$ 2\times2 $ asymmetric'' and ``$ 2\times2+1 $ asymmetric'' type of co-rotating helical filaments are also considered.

</details>


### [34] [The Kawahara equation on star graphs](https://arxiv.org/abs/2511.06007)
*Márcio Cavalcante,Chulkwang Kwak,José Marques*

Main category: math.AP

TL;DR: Local well-posedness established for Kawahara equation on metric star graphs using boundary conditions, forcing operator method, and Fourier restriction techniques.


<details>
  <summary>Details</summary>
Motivation: To extend the analysis of fifth-order nonlinear dispersive equations to star graph geometries, building on previous work for half-line cases.

Method: Identified suitable boundary conditions for linear dynamics, applied forcing operator method (from Cavalcante and Kwak) and Fourier restriction method (from Bourgain) to derive integral formula.

Result: Successfully established local well-posedness for the Cauchy problem of Kawahara equation on general metric star graphs.

Conclusion: The approach provides a framework that can potentially be extended to other fifth-order nonlinear dispersive equations on star graphs.

Abstract: In this paper, we establish local well-posedness for the Cauchy problem associated with the Kawahara equation on a general metric star graph. Initially, we identify suitable boundary conditions that produce a well-behaved dynamics for the linear equation. Subsequently, we derive the integral formula using the forcing operator method, previously applied to the Kawahara equation on the half-line by Cavalcante and Kwak (GAFA 1993), and the Fourier restriction method of Bourgain (NoDEA 2020). This work has the potential to be extended to other fifth-order nonlinear dispersive equations on star graphs.

</details>


### [35] [Asymptotic behavior of solutions to a space fractional diffusion equation](https://arxiv.org/abs/2511.06030)
*Barbara Łupińska,Piotr Rybka*

Main category: math.AP

TL;DR: Improved time decay estimates for 1D fractional diffusion equation with Caputo derivative on half-line, showing convergence to self-similar solutions or decay to zero with specified rates.


<details>
  <summary>Details</summary>
Motivation: To enhance understanding of long-term behavior and decay properties of solutions to fractional diffusion equations, particularly for boundary value problems on half-lines.

Method: Analysis of one-dimensional fractional diffusion equation involving Caputo derivative on half-line, examining different boundary conditions.

Result: Solutions converge in L^p (p>1) to multiples of self-similar solutions or decay to zero, with explicit convergence rates provided.

Conclusion: The paper establishes precise decay estimates and convergence behavior for fractional diffusion equations, advancing the mathematical understanding of their long-term dynamics.

Abstract: We improve the time decay estimates of solutions to the one-dimensional fractional diffusion equation involving the Caputo derivative. The equation is considered on the half-line. Depending on the boundary condition, we show that solutions converge in $L^p$, $p>1$ to a multiple of the self-similar solutions or decay to zero. The convergence rate is provided.

</details>


### [36] [Radial symmetry of positive solutions to quasilinear Hardy-Sobolev doubly critical systems](https://arxiv.org/abs/2511.06071)
*Laura Baldelli,Francesco Esposito,Rafael Lopez-Soriano,Berardino Sciunzi*

Main category: math.AP

TL;DR: This paper proves radial symmetry for positive weak solutions with finite energy to a quasilinear doubly critical system involving p-Laplacian operators with Hardy potential and critical Sobolev exponents.


<details>
  <summary>Details</summary>
Motivation: To establish symmetry properties for solutions to quasilinear systems with critical nonlinearities, which are important in understanding the structure and behavior of solutions to such challenging PDE systems.

Method: The authors analyze positive weak solutions with finite energy to a quasilinear system involving p-Laplacian operators, Hardy potential terms, and critical Sobolev exponents, using techniques from symmetry analysis and variational methods.

Result: The paper proves radial symmetry results for positive weak solutions with finite energy to the given quasilinear doubly critical system under the specified parameter conditions.

Conclusion: The established radial symmetry results provide important structural information about solutions to quasilinear systems with critical nonlinearities, contributing to the understanding of symmetry properties in such challenging PDE settings.

Abstract: The aim of this paper is to prove radial symmetry results for positive weak solutions with finite energy to the following quasilinear doubly critical system \begin{equation} \begin{cases} -Δ_p u\,=γ\frac{u^{p-1}}{|x|^p} + u^{p^*-1}+ ναu^{α-1} v^β& \text{in}\quad \mathbb{R}^n \\ -Δ_p v\,=γ\frac{v^{p-1}}{|x|^p} + v^{p^*-1}+ νβu^αv^{β-1} & \text{in}\quad\mathbb{R}^n, \end{cases} \end{equation} where $1<p<n$, $γ\in [0, Λ_{n,p})$ with $Λ_{n,p} = \left[(n-p)/p\right]^p$, $α, β> 1$ such that $α+ β= p^*=np/(n-p)$ and $ν>0$.

</details>


### [37] [Non-Radial Solution Structures for Quasilinear Hamilton--Jacobi--Bellman Equations in Bounded Settings](https://arxiv.org/abs/2511.06277)
*Dragos-Patru Covei*

Main category: math.AP

TL;DR: Analysis of quasilinear Hamilton-Jacobi-Bellman equations on bounded convex domains, showing their connection to stochastic optimal control with exit-time costs, with existence/uniqueness results for classical solutions under sub-quadratic growth conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the natural connection between quasilinear Hamilton-Jacobi-Bellman equations and stochastic optimal control problems with exit-time costs, providing both probabilistic interpretation and rigorous derivation.

Method: Dynamic programming applied to controlled Itô diffusions, using monotone iteration and barrier techniques for constructive proofs with rigorous estimates.

Result: Established existence and uniqueness of positive classical solutions under sub-quadratic growth conditions on the source term, with framework for algorithmic implementation.

Conclusion: Successfully connected quasilinear Hamilton-Jacobi-Bellman equations to stochastic control theory through dynamic programming principle, with applications in production planning and image restoration.

Abstract: We study quasilinear Hamilton--Jacobi--Bellman equations on bounded smooth convex domains. We show that the quasilinear Hamilton--Jacobi--Bellman equations arise naturally from stochastic optimal control problems with exit-time costs. The PDE is obtained via dynamic programming applied to controlled Itô diffusions, providing both a probabilistic interpretation and a rigorous derivation. Our result establishes existence and uniqueness of positive classical solutions under sub-quadratic growth conditions on the source term. The constructive proofs, based on monotone iteration and barrier techniques, also provide a framework for algorithmic implementation with applications in production planning and image restoration. We provide complete detailed proofs with rigorous estimates and establish the connection to stochastic control theory through the dynamic programming principle.

</details>


### [38] [The emergence of nonlinear Jeans-type instabilities for quasilinear wave equations. II: Generalizations](https://arxiv.org/abs/2511.06289)
*Chao Liu,Yiqing Shi*

Main category: math.AP

TL;DR: This paper extends previous work on quasilinear wave equations, showing that self-increasing blowup solutions exist for a broad parameter range (1<a≤30, 1/3≤b≤2/3), with singularities forming at future endpoints of null geodesics under small perturbations.


<details>
  <summary>Details</summary>
Motivation: To analyze the long-term behavior of solutions to a broader class of quasilinear wave equations and extend previous results on self-increasing blowup phenomena to a wider parameter range.

Method: Extends previous mathematical analysis of quasilinear wave equations with specific parameter constraints, examining solutions to equations with nonlinear damping and source terms in the given parameter ranges.

Result: Demonstrates existence of self-increasing blowup solutions for the extended parameter family, with singularities emerging at future endpoints of null geodesics when data perturbations are sufficiently small.

Conclusion: The analysis confirms that self-increasing blowup behavior persists across a broad parameter space for quasilinear wave equations, establishing robustness of this singularity formation mechanism.

Abstract: This work extends the previous work by the first author [arXiv:2409.02516] and [Math. Ann. 393 (2025), 317-363], analyzing the long-term behavior of solutions to a broader class of quasilinear wave equations with parameter $1<\mathsf{a}\leq30$ and $\frac{1}{3}\leq\mathsf{b}\leq\frac{2}{3}$: \begin{equation*}
  \partial^2_t \varrho- \biggl( \frac{ \mathsf{m}^2 (\partial_{t}\varrho )^2}{(1+\varrho )^2} + 4(\mathsf{k}-\mathsf{m}^2)(1+\varrho )\biggr) Δ\varrho = F(t,\varrho,\partial_μ \varrho) \end{equation*} where $F$ is given by \begin{equation*}
  F(t,\varrho,\partial_μ \varrho):=
  \mathsf{b} \varrho (1+ \varrho ) -(\mathsf{a}-1) \partial_{t}\varrho
  + \frac{4}{3} \frac{(\partial_{t}\varrho )^2}{1+\varrho } + \biggl(\mathsf{m}^2 \frac{ (\partial_{t}\varrho )^2}{(1+\varrho )^2} + 4(\mathsf{k}-\mathsf{m}^2) (1+\varrho ) \biggr) q^i \partial_{i}\varrho - \mathtt{K}^{ij} \partial_{i}\varrho\partial_{j}\varrho . \end{equation*} The results demonstrate that for this extensive family of quasilinear wave equations satisfying $1<\mathsf{a}\leq30$ and $\frac{1}{3}\leq\mathsf{b}\leq\frac{2}{3}$, self-increasing blowup solutions also exist, and self-increasing singularities emerge at certain future endpoints of null geodesics provided the inhomogeneous perturbations of data are sufficiently small.

</details>


### [39] [Liouville results for supersolutions of fractional $p$-Laplacian equations with gradient nonlinearities](https://arxiv.org/abs/2511.06334)
*Mousomi Bhakta,Anup Biswas,Aniket Sen*

Main category: math.AP

TL;DR: Nonnegative viscosity solutions of a fractional p-Laplacian inequality must be constant under specific parameter conditions.


<details>
  <summary>Details</summary>
Motivation: To establish Liouville-type results for fractional p-Laplacian inequalities with gradient terms, extending previous results for classical cases.

Method: Using viscosity solution theory and careful analysis of the fractional p-Laplacian operator with gradient-dependent nonlinearities.

Result: Proved that any nonnegative viscosity solution of the given inequality must be constant under the stated parameter conditions.

Conclusion: The paper provides sharp conditions for Liouville-type theorems in fractional p-Laplacian equations with gradient terms.

Abstract: We prove that any nonnegative viscosity solution of the inequality $$(-Δ_p)^s u(x) \geq u^{t} |\nabla u|^{m}\quad \text{ in }\; \mathbb{R}^N,\; N\geq 2,$$ must be constant. This result holds for parameters $p\in (1, \infty), s\in (0, 1)$, $t, m\geq 0$, satisfying $$t (N-sp) + m(N-(sp-p+1)) < N(p-1),$$ with the additional condition that either $m\leq p-1$ if $p-1<sp$, or $m<sp$ if $p-1\geq sp$.

</details>


### [40] [Critical threshold for a two-species chemotaxis system with the energy critical exponent](https://arxiv.org/abs/2511.06343)
*Shen Bian*

Main category: math.AP

TL;DR: This paper analyzes a two-species chemotaxis model with nonlinear diffusion and nonlocal interactions, establishing sharp conditions for global existence versus finite-time blow-up based on comparison with stationary solutions.


<details>
  <summary>Details</summary>
Motivation: To understand the delicate balance between nonlinear diffusion and attractive forces in chemotaxis systems, particularly how this balance determines whether solutions exist globally or blow up in finite time.

Method: The authors use a conformal invariant free energy framework and analyze the system through comparison with radially symmetric stationary solutions. They establish critical thresholds based on L^m norms of initial data relative to stationary solutions.

Result: Proved that solutions exist globally if initial data norms are strictly smaller than stationary solution norms, while blow-up occurs when initial data norms exceed stationary solution norms.

Conclusion: The paper provides a complete classification of global existence and finite-time blow-up for the two-species chemotaxis model, with stationary solutions serving as sharp thresholds separating these behaviors.

Abstract: We consider a two-species chemotaxis model in $\R^d(d \ge 3)$ featuring nonlinear porous medium-type diffusion and nonlocal attractive power-law interaction. Here, the nonlinear diffusion is chosen to be $1/m_1+1/m_2=(d+2)/d$ in such a way that the associated free energy is conformal invariant, and there are radially symmetric, non-increasing and non-compactly supported stationary solutions $(U_s(x),V_s(x))$. We analyze the conditions on initial data $(u_0,v_0)$ under which attractive forces dominate over diffusion, and further classify the global existence and finite time blow-up of dynamical solutions by virtue of these stationary solutions. Specifically, the solution $(u,v)(x,t)$ exists globally in time if the initial data satisfy $\|u_0\|_{L^{m_1}(\R^d)}<\|U_s\|_{L^{m_1}(\R^d)}$ and $\|v_0\|_{L^{m_2}(\R^d)}<\|V_s\|_{L^{m_2}(\R^d)}$. In contrast, there are blowing-up solutions when $\|u_0\|_{L^{m_1}(\R^d)}>\|U_s\|_{L^{m_1}(\R^d)}$ and $\|v_0\|_{L^{m_2}(\R^d)}>\|V_s\|_{L^{m_2}(\R^d)}$.

</details>


### [41] [Scattering of the defocusing Calogero--Moser derivative nonlinear Schrödinger equation](https://arxiv.org/abs/2511.06432)
*Xi Chen*

Main category: math.AP

TL;DR: Scattering results for defocusing Calogero-Moser derivative nonlinear Schrödinger equation with initial data in weighted Sobolev spaces using Gérard-type explicit formula and distorted Fourier transform.


<details>
  <summary>Details</summary>
Motivation: To study long-time behavior and scattering of solutions to the defocusing Calogero-Moser derivative nonlinear Schrödinger equation for a broad class of initial data beyond previously studied rational cases.

Method: Using Gérard-type explicit formula and distorted Fourier transform associated with the Lax operator to analyze scattering behavior.

Result: Proved scattering result for solutions with initial data in H_{+}^{1,α}(ℝ) for α>1/4, characterizing scattering using distorted Fourier transform.

Conclusion: Successfully applied Gérard-type explicit formula to study long-time behavior of integrable equations for broad initial data classes, extending beyond rational cases.

Abstract: In this paper, we study the defocusing Calogero--Moser derivative nonlinear Schrödinger equation. Using the Gérard-type explicit formula, we prove the scattering result of solutions to this equation with initial data in $H_{+}^{1,α}(\mathbb{R}) := \{u\in H_{+}^1(\mathbb{R}) : |x|^αu\in L^2(\mathbb{R})\}$ for $α>1/4$. We also characterize the scattering using the distorted Fourier transform associated with the Lax operator. This is one of the first works that apply the Gérard-type explicit formula to study the long-time behavior of an integrable equation for a broad class of initial data, beyond the previously studied rational cases.

</details>


### [42] [Improved equilibration rates to self-similarity for strong solutions of a thin-film and related evolution equations](https://arxiv.org/abs/2511.06553)
*Mario Bukal*

Main category: math.AP

TL;DR: Analysis of strong solutions to nonlinear fourth-order evolution equations, focusing on thin-film equation, using time-dependent rescaling to establish sharp convergence rates toward steady state via relative Rényi entropy.


<details>
  <summary>Details</summary>
Motivation: To investigate asymptotic behavior and establish improved convergence rates for strong solutions of nonlinear fourth-order evolution equations, particularly the thin-film equation, compared to classical entropy dissipation methods.

Method: Uses time-dependent rescaling that preserves second moment, building on Carrillo-Toscani framework for second-order equations, applied to fourth-order equations including thin-film and DLSS equations at formal level, rigorously justified for thin-film equation.

Result: Obtains sharp convergence rates toward steady state in terms of relative Rényi entropy, showing improved estimates at early and intermediate times compared to classical relative entropy dissipation, leading to sharper L^1-norm convergence.

Conclusion: The method provides enhanced convergence analysis for fourth-order evolution equations, with rigorous justification for thin-film equation, demonstrating superiority of Rényi entropy approach over classical entropy methods for early and intermediate time behavior.

Abstract: This paper investigates the asymptotic behavior of strong solutions to a family of nonlinear fourth-order evolution equations on the real line, with particular focus on the thin-film equation $\partial_tu = -(uu_{xxx})_x$. The method builds on the framework introduced by Carrillo and Toscani (Nonlinearity 27 (2014), 3159) for second-order nonlinear diffusion equations - by introducing a time-dependent rescaling that preserves the second moment, we establish sharp convergence rates toward the steady state in terms of the relative Rényi entropy. Compared to rates derived from the dissipation of the classical relative entropy, this approach yields improved estimates at early and intermediate times, and consequently a sharper convergence in the $L^1$-norm. The method is developed at a formal level for the family of fourth-order equations, including the well-known Derrida-Lebowitz-Speer-Spohn (DLSS) equation, but can be rigorously justified for strong solutions of the thin-film equation.

</details>


### [43] [On the well-posedness of a nonlocal kinetic model for dilute polymers with anomalous diffusion](https://arxiv.org/abs/2511.06570)
*Marvin Fritz,Endre Süli,Barbara Wohlmuth*

Main category: math.AP

TL;DR: Existence of global-in-time large-data weak solutions for nonlocal kinetic models of polymeric fluids, with uniqueness under sufficient regularity.


<details>
  <summary>Details</summary>
Motivation: To study nonlocal-in-time kinetic models of incompressible dilute polymeric fluids that capture subdiffusive and memory-type phenomena.

Method: Energy estimate using a suitable relative entropy to handle the critical non-corotational drag term; Galerkin approximations with novel compactness techniques for nonlocal PDEs.

Result: Proved global existence of weak solutions, nonnegativity of probability density function, and uniqueness with sufficient regularity.

Conclusion: The nonlocal system admits global weak solutions, and uniqueness holds under appropriate regularity conditions.

Abstract: In this work, we study a class of nonlocal-in-time kinetic models of incompressible dilute polymeric fluids. The system couples a macroscopic balance of linear momentum equation with a mezoscopic subdiffusive Fokker-Planck equation governing the evolution of the probability density function of polymer configurations. The model incorporates nonlocal features to capture subdiffusive and memory-type phenomena. Our main result asserts the existence of global-in-time large-data weak solutions to this nonlocal system. The proof relies on an energy estimate involving a suitable relative entropy, which enables us to handle the critical general non-corotational drag term that couples the two equations. As a side result, we prove nonnegativity of the probability density function. A crucial step in our analysis is to establish strong convergence of the sequence of Galerkin approximations by a combination of techniques, involving a novel compactness result for nonlocal PDEs. Lastly, we prove the uniqueness of weak solutions with sufficient regularity.

</details>


### [44] [Global existence for the relativistic Vlasov-Poisson system in a two-dimensional bounded domain](https://arxiv.org/abs/2511.06595)
*Yanmin Mu,Dehua Wang*

Main category: math.AP

TL;DR: Global existence of solutions for relativistic Vlasov-Poisson system in 2D convex bounded domains with specular reflection boundary conditions for distribution density, and both Neumann and Dirichlet boundary conditions for electric potential.


<details>
  <summary>Details</summary>
Motivation: To establish the global existence of solutions for the relativistic Vlasov-Poisson system in bounded domains, which is important for understanding plasma physics and kinetic theory in confined geometries.

Method: Constructing velocity lemmas and applying geometric techniques, specifically using arc length parameterization and Frenet-Serret formulas to describe the distribution density equation near boundaries in 2D.

Result: Proved global existence of solutions for general initial data in two-dimensional convex bounded domains with the specified boundary conditions.

Conclusion: The geometric approach using arc length parameterization and Frenet-Serret formulas provides an effective framework for analyzing the relativistic Vlasov-Poisson system in bounded domains, establishing crucial connections in the geometric representation.

Abstract: In this paper, we prove the global existence of solutions to the relativistic Vlasov-Poisson system for general initial data in convex bounded domains of two space dimensions, assuming the specular reflection boundary conditions for the distribution density. The boundary conditions for the electric potential are considered in two cases: Neumann boundary conditions and homogeneous Dirichlet boundary conditions. The core ideas involve constructing suitable velocity lemmas and applying geometric techniques. In the two-dimensional case, it is crucial to select the arc length as the parameter of the curve and to further combine this with the Frenet-Serret formulas, enabling us to effectively describe the distribution density equation near the boundary and thus establishing a vital connection in the geometric representation.

</details>


### [45] [A Revisiting of the Pressure Elimination for a Fluid-Structure PDE Interaction and its Implications](https://arxiv.org/abs/2511.06615)
*George Avalos,Yuhao Mu*

Main category: math.AP

TL;DR: New technique for pressure elimination/recovery in fluid-structure interaction models valid in general bounded Lipschitz domains without geometric restrictions.


<details>
  <summary>Details</summary>
Motivation: To establish well-posedness of coupled Stokes-elasticity FSI models in general geometries without requiring convexity or other geometric conditions.

Method: Developed a pressure elimination technique that yields explicit C₀-semigroup generator representation in the energy space of fluid-structure initial data.

Result: First analytic proof of well-posedness for continuous PDE in general Lipschitz domains, with applications to FEM convergence estimates in polygonal domains.

Conclusion: The new pressure elimination technique successfully establishes well-posedness for FSI models in general geometries and has broader implications for numerical methods.

Abstract: In this paper we establish, for the first time, a new technique for eliminating and recovering the pressure for a fluid-structure interaction model that is valid in general bounded Lipschitz domains, without additional geometric conditions such as convexity of angles. The specific fluid-structure interaction (FSI) that we consider is a well-known model of coupled Stokes flow with linear elasticity, which constitutes a coupled parabolic-hyperbolic system. The coupling between these two distinct PDE dynamics occurs across a boundary interface, with each of the components evolving on its own distinct geometry, with the boundaries concerned being Lipschitz. For simplicity, we consider the linear version of this FSI system with Stokes flow. Our new pressure elimination technique admits of an explicit $C_{0}$-semigroup generator representation $\mathcal{A} : D(\mathcal{A}) \subset \mathbf{H} \to \mathbf{H}$, where $\mathbf{H}$ is the associated energy space of fluid-structure initial data. This leads to an analytic proof for the first time of the well-posedness of the continuous PDE in such general geometries. We illustrate some automatic consequences of our results to other fields, such as numerical approximations where it provides FEM convergence estimates over polygonal domains.

</details>


### [46] [Stability of Solitary Capillary-Gravity Water Waves in Three Dimensions](https://arxiv.org/abs/2511.06629)
*Changfeng Gui,Shanfa Lai,Yong Liu,Juncheng Wei,Wen Yang*

Main category: math.AP

TL;DR: Proves conditional orbital stability of 3D capillary-gravity solitary water waves in finite depth under strong surface tension using adapted GSS framework.


<details>
  <summary>Details</summary>
Motivation: The constructed solitary waves are not energy minimizers, requiring direct stability analysis rather than variational methods.

Method: Adapts Grillakis-Shatah-Strauss framework within Mielke's approach to handle mismatch between well-posedness and energy spaces; uses spectral analysis of linearized dynamics and careful treatment of Hamiltonian structure.

Result: Establishes conditional orbital stability of fully localized 3D capillary-gravity solitary waves.

Conclusion: Successfully proves stability of non-variational solitary water waves through adapted mathematical framework and spectral analysis.

Abstract: This paper establishes the conditional orbital stability of fully localized solitary waves for the three-dimensional capillary-gravity water wave problem in finite depth under strong surface tension. The waves, constructed via a non-variational Lyapunov-Schmidt reduction in [26], are not energy minimizers and thus require a direct stability analysis. We adapt the Grillakis-Shatah-Strauss framework within Mielke's approach to handle the mismatch between well-posedness and energy spaces. The proof relies on spectral analysis of the linearized dynamics and careful treatment of the Hamiltonian structure defined by the energy and momentum functionals.

</details>


### [47] [Modified Scattering for Nonlocal Nonlinear Schrödinger Equations](https://arxiv.org/abs/2511.06637)
*Tim Van Hoose*

Main category: math.AP

TL;DR: Modified scattering and sharp L∞ decay for Hartree and Schrödinger-Bopp-Podolsky equations in 2D and 3D using wavepackets approach, achieving lower regularity than previous results.


<details>
  <summary>Details</summary>
Motivation: To improve upon previous scattering results by Hayashi-Naumkin and Kato-Pusateri by achieving modified scattering and sharp decay at lower regularity levels.

Method: Using the testing by wavepackets approach developed by Ifrim and Tataru.

Result: Proved modified scattering and sharp L∞ decay for Hartree and Schrödinger-Bopp-Podolsky equations in dimensions 2 and 3 at much lower regularity than previous results.

Conclusion: The approach enables minimal regularity assumptions for scattering-critical NLS results, extending Hayashi-Naumkin's work under more general conditions.

Abstract: We prove a modified scattering and sharp $L^\infty$ decay result for both the Hartree and Schrödinger-Bopp-Podolsky equations in dimensions $2$ and $3$ using the testing by wavepackets approach due to Ifrim and Tataru. We show that modified scattering and sharp pointwise decay occur for these equations at a regularity much lower than previous results due to Hayashi-Naumkin and Kato-Pusateri, and as a corollary also show that the results on power-type scattering-critical NLS due to Hayashi-Naumkin can be proven under minimal regularity assumptions.

</details>


### [48] [Segregated solutions for nonlinear Schrödinger systems with sublinear coupling terms](https://arxiv.org/abs/2511.06671)
*Qing Guo,Chengxiang Zhang*

Main category: math.AP

TL;DR: The paper establishes infinitely many nonnegative segregated solutions for sublinearly coupled Schrödinger systems with critical sublinear coupling exponents, overcoming challenges from nonsmooth nonlinearities through an enhanced Lyapunov-Schmidt reduction framework.


<details>
  <summary>Details</summary>
Motivation: Sublinear coupling exponents in Schrödinger systems introduce fundamental challenges due to nonsmooth nonlinearities and singularities that classical reduction methods cannot handle, requiring new mathematical frameworks.

Method: Developed an enhanced Lyapunov-Schmidt reduction framework by recasting the problem in a specially constructed metric space of local minimizers for an outer boundary value problem, enabling sharp a priori estimates and contraction mapping arguments.

Result: Proved existence of infinitely many nonnegative segregated solutions and discovered a novel 'dead core' phenomenon where solutions exhibit topological segregation with non-strict positivity and specific support properties.

Conclusion: The methodology provides a versatile framework for handling nonsmooth nonlinearities in reduction techniques, overcoming limitations of classical methods for sublinear couplings in Schrödinger systems.

Abstract: We establish the existence of infinitely many nonnegative, segregated solutions for the sublinearly coupled Schrödinger system \[ \begin{cases} -Δu + K_1(x)u = μu^{p-1} + βu^{σ_1}v^{σ_2+1} \\ -Δv + K_2(x)v = νv^{p-1} + βu^{σ_1+1}v^{σ_2} \end{cases} \quad x \in \mathbb{R}^N \] where $N \geq 2$, $p \in (2,2^*)$, $2^* = 2N/(N-2)$ ($2^* = \infty$ if $N=2$), $K_j$ are radial potentials, $μ, ν> 0$, $β\in \mathbb{R}$, and critically $σ_j \in (0,1)$. The sublinear coupling exponents $σ_j$ introduce fundamental challenges due to nonsmooth nonlinearities and singularities in standard reduction methods. To overcome this, we develop an enhanced Lyapunov-Schmidt reduction framework. By recasting the problem within a specially constructed metric space of local minimizers for an outer boundary value problem, we derive sharp a priori estimates enabling contraction mapping arguments. This approach circumvents the limitations of classical methods for sublinear couplings. We further uncover a novel "dead core" phenomenon: solutions $(u_\ell, v_\ell)$ exhibit non-strict positivity with topological segregation. Specially, for $N=2$ and large integers $\ell$, there exist radii $0 < R_1 < R_2$ such that $\text{supp } u_\ell \subset B_{R_2}(0)$, $\text{supp } v_\ell \subset \mathbb{R}^N \setminus B_{R_1}(0)$, and $u_\ell + v_\ell \to 0$ uniformly in $B_{R_2}(0) \setminus B_{R_1}(0)$ as $\ell \to \infty$. Our methodology provides a versatile framework for handling nonsmooth nonlinearities in reduction techniques.

</details>


### [49] [Hydrodynamic limit for compressible Navier-Stokes-Vlasov-Poisson equations with local alignment force](https://arxiv.org/abs/2511.06792)
*Yunfei Su,Lei Yao*

Main category: math.AP

TL;DR: The paper studies the hydrodynamic limit of compressible Navier-Stokes-Vlasov-Poisson equations with local alignment force in 3D torus, showing convergence to a two-phase fluid model.


<details>
  <summary>Details</summary>
Motivation: To investigate the hydrodynamic limit of weak solutions to coupled fluid-particle systems, addressing the challenge posed by the absence of dissipation terms in the particle equation.

Method: Uses the relative entropy method to analyze global weak solutions of compressible Navier-Stokes-Vlasov-Poisson equations with local alignment force in three-dimensional torus domain.

Result: The distribution function converges to a Dirac distribution in velocity, and the fluid density and velocity converge to their respective limiting values in the two-phase fluid model.

Conclusion: The global weak solutions of the compressible Navier-Stokes-Vlasov-Poisson equations converge to smooth solutions of the limiting two-phase fluid model.

Abstract: We investigate the hydrodynamic limit of weak solutions to compressible Navier-Stokes-Vlasov-Poisson equations with local alignment force in three-dimensional torus domain. Due to the absence of dissipation terms in particle equation, it is difficult to study this problem. Based on the relative entropy method, it is shown that the global weak solutions of the compressible Navier-Stokes-Vlasov-Poisson equations converge to the smooth solutions of the limiting two-phase fluid model.We obtained that the distribution function $f^ε$ converges to a Dirac distribution in velocity, the fluid density $ρ^ε$ and velocity $u^ε$ converge to $ρ$ and $u$, respectively.

</details>


### [50] [A PDE perspective on the flat chain conjecture](https://arxiv.org/abs/2511.06822)
*Andrea Marchese*

Main category: math.AP

TL;DR: Recent progress on the flat chain conjecture showing equivalence between metric currents and flat chains with finite mass in Euclidean space, with focus on connection to Lipschitz regularity estimates for PDEs.


<details>
  <summary>Details</summary>
Motivation: To investigate and summarize recent developments on the flat chain conjecture, which concerns the relationship between metric currents and flat chains in Euclidean space.

Method: Survey approach summarizing recent work, particularly focusing on the equivalence between the conjecture and Lipschitz regularity estimates for certain PDEs.

Result: The paper establishes that the flat chain conjecture is equivalent to Lipschitz regularity estimates for a specific PDE, providing new insights into the conjecture's validity.

Conclusion: Recent work has revealed a fundamental connection between the flat chain conjecture and PDE regularity theory, advancing understanding of this important mathematical problem.

Abstract: This survey summarizes recent progress on the flat chain conjecture, which asserts the equivalence between metric currents and flat chains with finite mass in the Euclidean space. In particular, we focus on recent work showing that the conjecture is equivalent to a Lipschitz regularity estimate for a certain PDE.

</details>


### [51] [Global well-posedness of strong solutions to a bulk-surface Navier-Stokes-Cahn-Hilliard model with non-degenerate mobilities in two dimensions](https://arxiv.org/abs/2511.06847)
*Jonas Stange*

Main category: math.AP

TL;DR: Global well-posedness of strong solutions for a thermodynamically consistent diffuse interface model of bulk-surface viscous fluid mixtures in 2D, including non-degenerate mobility functions.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for thermodynamically consistent diffuse interface models that couple bulk and surface fluid dynamics with phase separation.

Method: Developed new well-posedness and regularity theory for convective bulk-surface Cahn-Hilliard equations with non-degenerate mobilities and bulk-surface Stokes equations with non-constant coefficients.

Result: Proved global existence, uniqueness, and continuous dependence on initial data for strong solutions in the two-dimensional setting.

Conclusion: The model exhibits mathematically sound behavior with well-posed strong solutions, providing a solid theoretical foundation for studying bulk-surface fluid mixture dynamics.

Abstract: We examine a thermodynamically consistent diffuse interface model for bulk-surface viscous fluid mixtures. This model consists of a Navier--Stokes--Cahn--Hilliard model in the bulk coupled to a surface Navier--Stokes--Cahn--Hilliard system on the boundary. In this paper, we address the global well-posedness of strong solutions in the two-dimensional setting, also covering the physically meaningful case of non-degenerate mobility functions. Lastly, we prove the uniqueness of the corresponding strong solutions and their continuous dependence on the initial data. Our approach hinges upon new well-posedness and regularity theory for a convective bulk-surface Cahn--Hilliard equation with non-degenerate mobilities, as well as a bulk-surface Stokes equation with non-constant coefficients.

</details>


### [52] [Asymptotics and periodic dynamics in a negative chemotaxis system with cell lethality](https://arxiv.org/abs/2511.06889)
*Federico Herrero-Hervás,Mihaela Negreanu*

Main category: math.AP

TL;DR: This paper analyzes a parabolic PDE system modeling negative chemotaxis between a biological species and a lethal chemical substance, showing that solutions converge to those of an associated ODE system when the external chemical supply function becomes spatially homogeneous.


<details>
  <summary>Details</summary>
Motivation: To understand the long-term behavior of biological systems with negative chemotaxis interactions, particularly how spatial dynamics converge to simpler temporal dynamics when external forcing becomes spatially uniform.

Method: Analysis of a coupled parabolic PDE system describing chemotaxis with logistic growth and chemical degradation, comparing it to an associated ODE system through convergence proofs and asymptotic analysis.

Result: Proves that when the external chemical supply function f converges to a spatially homogeneous function, the PDE solution converges to the ODE solution in L^2 norm as time goes to infinity.

Conclusion: The spatial dynamics of the chemotaxis system simplify to temporal dynamics when external forcing becomes spatially uniform, with additional insights provided for periodic forcing scenarios.

Abstract: This work studies the following system of parabolic partial differential equations \begin{equation*} \begin{cases} \displaystyle \frac{\partial u}{\partial t} = DΔu + χ\nabla \cdot(u \nabla v) + ru(1-u) - u v, \quad & x \in Ω, ~t > 0, \\ \displaystyle \frac{\partial v}{\partial t} = Δv + a u -v+ f(x,t), \quad & x \in Ω, ~t > 0, \end{cases} \end{equation*} modeling the negative chemotaxis interactions between a biological species and a lethal chemical substance that is supplied according to the known function $f(x,t)$. \\\\ It is shown that if $f$ converges to a spatially homogeneous function $\tilde{f}$ in a certain sense, then the solution $(u,v)$ satisfies $$ ||u-\tilde{u}||_{L^2(Ω)} + ||v-\tilde{v}||_{L^2(Ω)} \to 0 \quad \text{as } t \to \infty, $$ where $(\tilde{u},\tilde{v})$ is the solution to the associated ODE system \begin{equation*} \begin{cases} \displaystyle \frac{d \tilde{u}}{dt~} = r \tilde{u} (1 - \tilde{u}) - \tilde{u}\tilde{v}, \quad & t>0,\\ \displaystyle \frac{d \tilde{v}}{dt~} = a\tilde{u} - \tilde{v} + \tilde{f},\quad & t>0. \end{cases} \end{equation*} Some final remarks are given for the case in which $\tilde{f}$ is a time periodic function, and under which hypotheses do $(\tilde{u},\tilde{v})$ inherit this periodicity.

</details>


### [53] [Profile of a Touch-Down solution to a nonlocal MEMS model with critical parameters](https://arxiv.org/abs/2511.06910)
*Maissâ Boughrara*

Main category: math.AP

TL;DR: This paper constructs a solution for a MEMS device model that quenches (blows up) at a single interior point in finite time, with detailed asymptotic profile analysis.


<details>
  <summary>Details</summary>
Motivation: To study finite-time quenching behavior in MEMS devices described by a parabolic equation with nonlocal integral term, extending blow-up analysis techniques to this physical system.

Method: Reformulate quenching as blow-up problem, apply techniques from Merle-Zaag (1997) and subsequent works, use reduction to finite-dimensional dynamical system and topological index theory.

Result: Successfully constructed solution that quenches at exactly one interior point in finite time T, with complete asymptotic description of the quenching profile.

Conclusion: The nonlocal integral term introduces additional gradient terms in blow-up framework, but the methodology successfully handles this challenge to characterize quenching behavior.

Abstract: This work investigates a mathematical model arising in the study of MEMS devices, described by the following parabolic equation on $[0,T)\timesΩ$: $$\partial_t v = Δv + \fracλ{(1-v)^2\left( 1 + γ\int_Ω \frac{1}{1-v}\, dx \right)^{2}} ,
\qquad 0 \leq v \leq 1,$$ where $Ω\subset \mathbb{R}^N$ is a bounded domain and $λ, γ> 0$. We construct a solution with a prescribed profile, which quenches in finite time $T$ at exactly one interior point $a \in Ω$. Moreover, we are able to provide an asymptotic description of the quenching profile.
  We reformulate the problem as a blow-up problem to utilize the techniques employed in Merle, Zaag in 1997, Duong, Zaag in 2019 and Duong, Ghoul, Kavallaris, Zaag 2022. The proof proceeds through two principal steps: a reduction to a finite-dimensional dynamical system and a classical topological argument employing index theory. The main challenge lies in managing the nonlocal integral term, which generates an additional gradient term when the problem is transformed into the blow-up framework.

</details>


### [54] [Vortex Solutions for A Mixed Boundary-Value Problem in the Abelian-Higgs Model with A Neutral Scalar Field](https://arxiv.org/abs/2511.06931)
*Guange Su,Xiaosen Han*

Main category: math.AP

TL;DR: Establishes existence of vortex solutions in Abelian-Higgs model with neutral scalar field, provides analytical criteria for Abelian/non-Abelian vortex phase boundary, and confirms numerical observations.


<details>
  <summary>Details</summary>
Motivation: Vortices are important topological solitons in physics, and understanding their phase transitions in gauge theories with complex scalar fields is significant for various physical domains.

Method: Combines shooting method with Schauder fixed-point theorem to derive analytical criteria and rigorously analyze vortex profile functions.

Result: Derived sharp analytical criteria for Abelian vortex phase boundary, established monotonicity, boundedness, and asymptotic behavior of vortex profiles, confirming numerical observations.

Conclusion: Provides rigorous mathematical foundation for vortex phase transitions in Abelian-Higgs model with neutral scalar field, validating previous numerical findings.

Abstract: Vortices represent a class of topological solitons arising in gauge theories coupled with complex scalar fields, holding significant importance across various domains of modern physics. In this paper we establish the existence of vortex solutions for a mixed boundary-value problem derived from the Abelian-Higgs model incorporating a neutral scalar field, a system recently investigated by Eto, Peterson et al. [7]. By synergistically combining the shooting method with the Schauder fixed-point theorem, we derive sharp analytical criteria that delineate the Abelian vortex phase from the non-Abelian one. We also rigorously establish the monotonicity, uniform boundedness, and precise asymptotic behavior of the vortex profile functions. Our results provide rigorous confirmation of numerical observations regarding the phase boundary between these distinct vortex types.

</details>


### [55] [On the Propagation of Regularity of Solutions to the KdV Equation on the positive Half-line](https://arxiv.org/abs/2511.06951)
*Márcio Cavalcante,Aílton C. Nascimento*

Main category: math.AP

TL;DR: The paper studies regularity properties of Korteweg-de Vries equation solutions on the positive half-line, showing infinite speed of regularity propagation leftward until a boundary-dependent stopping time, with trace derivative regularity gains.


<details>
  <summary>Details</summary>
Motivation: To understand how boundary conditions affect the propagation of regularity in KdV equations on half-lines, particularly the interaction between initial data regularity and boundary data effects.

Method: Analysis of initial-boundary value problem for KdV equations on positive half-line with initial data in H^{3/4+} and boundary data in H^{3/2+}, studying regularity propagation properties.

Result: Proved that solution regularity propagates with infinite speed to the left until a boundary-dependent stopping time T*, and established gains in trace derivative regularity.

Conclusion: Boundary functions create a stopping time for infinite regularity propagation, and the analysis reveals improved regularity for trace derivatives of KdV solutions on half-lines.

Abstract: We study special regularity properties of solutions to the initial-boundary value problem associated with the Korteweg-de Vries equations posed on the positive half-line. In particular, for initial data $u_0 \in H^{\frac{3}{4}^{+}}(\mathbb{R}^+)$ and boundary data $f\in H^{\frac32^+}(\R^+)$, where the restriction of $u_0$ to some subset of $(b,\infty)$ has an extra regularity for any $b>0$, we prove that the regularity of solutions $u$ moves with infinite speed to its left as time evolves until a certain time $T^*$. The existence of a stopping time $T^{*}$ appears because of the effect of the boundary function $f$. Also, as a consequence of our proof, we prove a gain in the regularity of the trace derivatives of the solutions for the Korteweg-de Vries on the half-line.

</details>


### [56] [Global dynamics of chemotaxis-consumption systems with oppositely acting nonlocal terms](https://arxiv.org/abs/2511.06974)
*Rafael Diaz Fuentes,Fatma Gamze Duzgun,Silvia Frassu,Giuseppe Viglialoro*

Main category: math.AP

TL;DR: Analysis of chemotaxis system with external source terms combining local/nonlocal growth and dampening, focusing on conditions for global existence and boundedness of solutions.


<details>
  <summary>Details</summary>
Motivation: To understand how different types of source terms affect long-term behavior and prevent cell aggregation in chemotaxis systems within confined habitats.

Method: Study two types of source terms: one ensuring total cell mass control, and another without guaranteed mass control, analyzing their effects on solution existence and boundedness.

Result: Identified conditions under which solutions exist globally and remain uniformly bounded, showing how source term variations influence system dynamics and prevent cell aggregation.

Conclusion: The structure of source terms significantly impacts long-term behavior, with mass-controlling terms ensuring bounded solutions while non-mass-controlling terms can lead to different dynamic behaviors.

Abstract: This paper studies a chemotaxis system where cells move in response to a chemical signal within a confined habitat. The model includes external source terms that combine local and nonlocal growth with dampening effects. The main focus is on conditions under which solutions exist for all time and remain uniformly bounded, preventing cell aggregation.
  Two types of source terms are considered. In the first case, the structure of the source term ensures that the total cell mass remains controlled over time. In the second case, this mass control is not guaranteed, which can lead to different dynamic behaviors. The results extend previous studies that examined similar systems but with more specific source terms and slightly different chemical dynamics. This work highlights how variations in the reaction terms influence the long-term behavior of the system.

</details>


### [57] [A note on the fourth-order Schrodinger equation with spatially growing inhomogeneous source term](https://arxiv.org/abs/2511.06985)
*Alaa Mohammed Alqaied,Tarek Saanouni*

Main category: math.AP

TL;DR: Study of non-linear biharmonic Schrödinger equation with unbounded inhomogeneous term, developing local and global theories in energy space and lower regularity Sobolev spaces.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of dealing with an inhomogeneous unbounded term that breaks space translation invariance in the biharmonic Schrödinger equation.

Method: Use Strauss type estimates requiring spherically symmetric assumption to handle the inhomogeneous term.

Result: Development of local theory and global theory for small data in energy space, plus local theory in lower regularity Sobolev spaces.

Conclusion: Successfully established mathematical theories for the non-linear biharmonic Schrödinger equation despite the challenging unbounded inhomogeneous term.

Abstract: This paper studies a non-linear biharmonic Schödinger equation with an unbounded inhomogeneous term. The main goal is to develop a local theory but also a global theory for small data, in the energy space. Moreover, we develop a local theory in Sobolev spaces with lower regularity. The challenge is to deal with the inhomogeneous unbounded term, which broke down the space translation invariance. In order to handle the inhomogenous term, we use some Strauss type estimates, which require a spherically symmetric assumption.

</details>


### [58] [Well-posedness of the focusing stochastic nonlinear Schrödinger equation: $L^2$-critical and supercritical cases](https://arxiv.org/abs/2511.07072)
*Annie Millet,Svetlana Roudenko*

Main category: math.AP

TL;DR: Analysis of global behavior and blow-up criteria for stochastic nonlinear Schrödinger equations with additive/multiplicative noise in L^2-critical and supercritical regimes.


<details>
  <summary>Details</summary>
Motivation: To understand how stochastic perturbations affect the global well-posedness and finite-time blow-up behavior of nonlinear Schrödinger equations, extending deterministic results to stochastic settings.

Method: Study solutions in H^1 space with deterministic or random initial data, establish quantitative bounds on well-posedness time and solution behavior, analyze probability of blow-up.

Result: Established criteria for finite time blow-up with positive probability for H^1-valued initial data with positive energy in both additive and multiplicative noise cases.

Conclusion: Stochastic noise affects the blow-up dynamics of nonlinear Schrödinger equations, with positive probability blow-up occurring under specific energy conditions in both noise types.

Abstract: We study the focusing $L^2$-critical and supercritical stochastic nonlinear Schrödinger equation subject to additive or multiplicative noise. We investigate global or long time behavior of solutions in $H^1$, which would correspond to global well-posedness in the deterministic case, with either deterministic or random initial data, and establish quantitative information about the well-posedness time, its probability and bounds on the solution in both cases. We then give criteria for finite time blow-up with positive probability for an $H^1$-valued initial data with positive energy in both cases.

</details>


### [59] [Properties of Solutions to the Full Fractional Heat Operator Equation](https://arxiv.org/abs/2511.07153)
*Lu Haipeng,Yu Mei*

Main category: math.AP

TL;DR: No positive bounded solutions exist for an indefinite fully fractional heat equation with master operator under certain weight and nonlinearity conditions, using moving planes method.


<details>
  <summary>Details</summary>
Motivation: To study the existence of positive bounded solutions for indefinite fully fractional heat equations with master operator and understand how different weight conditions affect solution behavior.

Method: Employed method of moving planes to prove monotonicity along first direction, and introduced mathematical tools to handle difficulties from the fractional operator.

Result: No positive bounded solutions exist under certain weight and nonlinearity assumptions; different conclusions reached for other weight conditions depending on nonlinearity behavior at infinity.

Conclusion: The study provides insights into solution existence for fractional heat equations and introduces useful mathematical tools for analyzing problems with fractional operators and nonlinearities.

Abstract: In this paper, we consider the following indefinite fully fractional heat equation involving the master operator . Under certain assumptions of the indefinite nonlinearity and its weight, we prove that there is no positive bounded solution, which is based on the monotonicity of the solution along the first direction that is proved by employing the method of moving planes. Besides, if the weight satisfy other conditions, we come to different conclusions according to the behavior of the nonlinearity at infinity.
  To overcome the difficulties caused by the operator, we lead in some mathematics tools that, as we believe, will be useful in studying problems involving other fractional operators or nonlinearities.

</details>


### [60] [Numerical simulations of the Gatenby-Gawlinski model with heterogeneous acid diffusion in one space dimension](https://arxiv.org/abs/2511.07187)
*Chiara Simeoni,Elisa Scanu,Donato Pera,Corrado Mascia*

Main category: math.AP

TL;DR: A modified Gatenby-Gawlinski model for acid-mediated tumor invasion with heterogeneous lactic acid diffusion in 1D, showing invasion fronts and interstitial gaps driven by pH lowering.


<details>
  <summary>Details</summary>
Motivation: To study acid-mediated tumor invasion accounting for heterogeneous diffusion of lactic acid across healthy tissues, which affects the invasive process.

Method: Numerical simulations using finite volume schemes on staggered cartesian grids with explicit time discretization of the modified tumor invasion model.

Result: Reproduced biologically relevant invasion fronts and interstitial gaps between normal and cancerous cells, showing dependence on non-homogeneous diffusion rates and emergence of homogenization for certain parameter values.

Conclusion: The approach effectively models acid-mediated tumor invasion, revealing homogenization phenomena that enable applications to interface diffusion problems in invasive processes.

Abstract: In this work, we introduce a variant of the Gatenby-Gawlinski model for acid-mediated tumor invasion in the one-dimensional experimental setting, accounting for heterogeneous diffusion of the lactic acid across the surrounding healthy tissues. Numerical simulations are performed by employing finite volume schemes on staggered cartesian grids, together with explicit time discretization. The effectiveness of such approach is proven by reproducing biologically relevant results like the formation of propagating invasion fronts and the emergence of an interstitial gap between normal and cancerous cells, which is driven by the pH lowering strategy and depends significantly on the non-homogenous diffusion rates. By means of a comparison analysis, we infer that a homogenization phenomenon arises in the long run for appropriate values of the physical parameters, thus paving the way for complex applications to interface diffusion problems of invasive processes.

</details>


### [61] [Diffusion-Reaction Epidemic Model with a Free Boundary](https://arxiv.org/abs/2511.07291)
*Aesol Jeon,Ki-Ahm Lee*

Main category: math.AP

TL;DR: Analysis of an SEIS PDE model with free boundary for epidemic transmission, establishing local/global solution existence and uniqueness, defining basic reproductive number R0 for disease spread conditions, and analyzing convergence speed.


<details>
  <summary>Details</summary>
Motivation: To model epidemic transmission dynamics (like COVID-19) using an SEIS PDE system with free boundary in rotationally symmetric domains, providing mathematical framework for understanding disease spread and control.

Method: Using parabolic PDE system with free boundary in rotationally symmetric domain, applying straightening lemma for local solution existence/uniqueness, analyzing diffusion coefficients for global solutions, and employing nonlinear elliptic eigenvalue techniques for convergence analysis.

Result: Established existence/uniqueness of local and global solutions under specific diffusion conditions. Defined R0 where R0<1 ensures global stability of disease-free equilibrium and disease vanishes, while R0>1 makes DFE unstable and disease spreads.

Conclusion: The SEIS PDE model with free boundary provides rigorous mathematical framework for epidemic analysis, with R0 serving as critical threshold for disease control, and nonlinear eigenvalue techniques enable convergence speed analysis of solutions.

Abstract: This study investigates an SEIS PDE model with a free boundary, which captures the dynamics of epidemic transmission, including diseases like COVID-19. This parabolic PDE system is analyzed in a rotationally symmetric domain, and the existence and uniqueness of the local solution are established through the straightening lemma. Furthermore, the existence and uniqueness of the global solution are established under specific conditions on the diffusion coefficients. Then the model introduces the basic reproductive number, $R_0$, which provides sufficient conditions for determining whether the disease will vanish or spread. Notably, when $R_0<1$, the disease-free equilibrium(DFE) is shown to be globally stable, and when $R_0>1$, the DFE is unstable. Lastly, we investigate the convergence speed of solutions by applying nonlinear elliptic eigenvalue techniques to the associated parabolic PDE system.

</details>


### [62] [Identification of Source Terms in the Ginzburg-Landau Equation from Final Data](https://arxiv.org/abs/2511.07345)
*Roberto Morales,Javier-Ramírez-Ganga*

Main category: math.AP

TL;DR: Identification of space-time dependent source term in Ginzburg-Landau equation from final-time observations using Tikhonov regularization and adjoint methods.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of identifying unknown space-time dependent source terms in the Ginzburg-Landau equation using only final-time measurement data.

Method: Weak-solution framework with Tikhonov regularization, derivation of explicit gradient formula via adjoint system, and iterative numerical methods for implementation.

Result: Established existence and uniqueness of quasi-solutions, proved Lipschitz continuity of the gradient, and validated approach with numerical experiments.

Conclusion: The proposed framework successfully addresses the inverse source identification problem in Ginzburg-Landau equations with rigorous mathematical foundation and practical numerical implementation.

Abstract: In this article, we study an inverse problem consisting in the identification of a space-time dependent source term in the Ginzburg-Landau equation from final-time observations. We adopt a weak-solution framework and analyze Tikhonov's functional, deriving an explicit gradient formula via an adjoint system and proving its Lipschitz continuity. We then establish existence and uniqueness results for quasi-solutions, and validate the theory with numerical experiments based on iterative methods.

</details>


### [63] [Regularity for the normalized p-Laplacian equation with an arbitrary degeneracy law](https://arxiv.org/abs/2511.07356)
*Claudemir Alcantara,Makson Santos*

Main category: math.AP

TL;DR: The paper proves C^1 regularity for solutions to degenerate normalized p-Laplace equations under Dini continuity conditions on the degeneracy modulus.


<details>
  <summary>Details</summary>
Motivation: To establish interior regularity for solutions of degenerate normalized p-Laplace equations, extending classical regularity theory to cases with controlled degeneracy.

Method: Approximation of solutions by sequences of hyperplanes to establish C^1 regularity under general degeneracy assumptions.

Result: Solutions belong to the C^1 class when the inverse of the degeneracy modulus satisfies a Dini continuity condition.

Conclusion: The paper successfully establishes C^1 regularity for degenerate normalized p-Laplace equations under minimal degeneracy assumptions via hyperplane approximation.

Abstract: We examine the interior regularity of solutions to a degenerate normalized $p$-Laplace equation, where the degeneracy is governed by a modulus of continuity whose inverse satisfies a Dini continuity condition. We prove that under very general assumptions on the degeneracy law, solutions belong to the $C^1$ class. We argue by approximating the solutions by a sequence of hyperplanes, which allows us to prove the desired regularity.

</details>


### [64] [Global Well-posedness and Scattering for Stochastic generalized KdV Equations with additive noise](https://arxiv.org/abs/2511.07386)
*Engin Başakoğlu,Faruk Temur,Oğuz Yılmaz*

Main category: math.AP

TL;DR: Local and global well-posedness results for defocusing stochastic generalized KdV equations with additive noise, focusing on mass-critical and supercritical cases.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for stochastic versions of generalized KdV equations, particularly addressing the challenges of mass-critical and supercritical nonlinearities with additive noise.

Method: Developed oscillatory integral estimates for general dispersion relations, used to bound the tail of stochastic convolution for sgKdV. Applied probabilistic methods to establish local well-posedness almost surely at scaling critical regularity.

Result: Proved local well-posedness almost surely for k≥4 up to scaling critical regularity. Established global well-posedness and scattering in L² for mass-critical case with small data, and in H¹ for mass supercritical case.

Conclusion: The oscillatory integral estimates developed are of independent interest and crucial for obtaining scattering results. The work provides comprehensive well-posedness theory for stochastic generalized KdV equations in mass-critical and supercritical regimes.

Abstract: We study the defocusing stochastic generalized Korteweg-de Vries equations (sgKdV) driven by additive noise, with a focus on mass-critical and supercritical nonlinearities. For integers $k \geq 4$, we establish local well-posedness almost surely up to scaling critical regularity. We also prove global well-posedness and scattering in $L^{2}_{x}(\mathbb{R})$ for the mass-critical equation with small initial data; also in $H^{1}_{x}(\mathbb{R})$ for the mass supercritical equation. In particular, we prove oscillatory integral estimates associated with more general dispersion relations, which are of independent interest; and we make use of a special case of these estimates as a main ingredient for the necessary bounds on the tail of the stochastic convolution for sgKdV, which is crucial to conclude scattering results.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [65] [Coupled Aerodynamic-Electromagnetic Modeling for RCS Estimation of Million-Scale Chaff Clouds with Arbitrarily Curved 3D Geometries](https://arxiv.org/abs/2511.05539)
*Chung Hyun Lee,Bowoo Jang,Kyoungil Kwon,Kyung-Tae Kim,Dong-Yeop Na*

Main category: physics.comp-ph

TL;DR: Extended 6-DoF aerodynamic framework for 3D curved chaff geometries, coupled with fast electromagnetic solver for real-time RCS prediction of million-scale chaff clouds.


<details>
  <summary>Details</summary>
Motivation: Accurate RCS prediction requires modeling aerodynamic effects on chaff element orientation and distribution, which conventional straight/2D bent geometry models cannot capture.

Method: Extended 6-DoF formulations to handle arbitrary 3D curved chaff geometries, coupled with fast method-of-moments solver for electromagnetic analysis of large-scale chaff clouds.

Result: Simulations show monostatic RCS is strongly influenced by aerodynamic effects, with flattened and helical motions critically determining scattering response. Framework enables real-time RCS prediction.

Conclusion: The coupled multiphysics framework provides physically grounded, computationally efficient approach for predicting RCS of large-scale chaff clouds and can be extended to radar signal processing applications.

Abstract: Accurate prediction of the radar cross section (RCS) of chaff clouds requires careful consideration of aerodynamic effects, as the orientation and spatial distribution of individual chaff elements evolve significantly after deployment. Building upon conventional six-degree-of-freedom (6-DoF) formulations for chaff aerodynamic analysis-which assumed straight or two-dimensionally bent geometries-we extend the framework to incorporate arbitrarily curved three-dimensional chaff geometries. This extension enables accurate modeling of both flattened and helical dynamics induced by aerodynamic moments acting along the roll, pitch, and yaw directions, thereby providing a more comprehensive and realistic description of chaff motion. We then finally develop a coupled aerodynamic-electromagnetic framework that integrates the extended aerodynamic model with our recently developed fast method-of-moments solver, which is optimized for efficiently estimating the RCS of million-scale chaff clouds. The proposed multiphysics coupled framework allows real-time, first-principles prediction of the monostatic and bistatic RCS of large-scale chaff clouds with arbitrary geometries, orientations, and lengths, accurately incorporating their time-varying aerodynamic evolution. Simulation results confirm that the monostatic RCS is strongly influenced by aerodynamic effects, with the coexistence of flattened and helical motions playing a critical role in determining the overall scattering response. The proposed framework thus provides a physically grounded and computationally efficient approach for predicting the RCS of large-scale chaff clouds. Furthermore, it can be directly extended to radar signal processing applications by utilizing multi-frequency complex-valued far-field responses, thereby enabling the reconstruction of Range-Doppler, Range-Angle, and Doppler-Angle maps.

</details>


### [66] [Towards Unified AI-Driven Fracture Mechanics: The Extended Deep Energy Method (XDEM)](https://arxiv.org/abs/2511.05888)
*Yizheng Wang,Yuzhou Lin,Somdatta Goswami,Luyang Zhao,Huadong Zhang,Jinshuai Bai,Cosmin Anitescu,Mohammad Sadegh Eshaghi,Xiaoying Zhuang,Timon Rabczuk,Yinghua Liu*

Main category: physics.comp-ph

TL;DR: XDEM is a unified deep learning framework that integrates discrete and continuous fracture models, enabling accurate fracture predictions with sparse collocation points and outperforming standard DEM in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing DEM approaches that require dense collocation near cracks, face stability challenges, and treat discrete and continuous fracture models separately.

Method: Extended Deep Energy Method (XDEM) incorporates displacement discontinuities and crack-tip asymptotics in discrete setting, while flexibly coupling displacement and phase fields in continuous setting.

Result: XDEM consistently outperforms standard DEM in accuracy and efficiency across benchmark problems including stress intensity factor evaluation, straight and kinked crack growth, and complex crack initiation.

Conclusion: XDEM establishes a robust foundation for applying AI to fracture mechanics and opens new avenues for predictive modeling in engineering and materials science by bridging discrete and phase-field models.

Abstract: Physics-Informed Neural Networks (PINNs) have recently emerged as powerful tools for solving partial differential equations (PDEs), with the Deep Energy Method (DEM) proving especially effective in fracture mechanics due to its energy-based formulation. Despite these advances, existing DEM approaches require dense collocation near cracks, face stability challenges, and typically treat discrete and continuous fracture models separately. To overcome these limitations, we introduce the Extended Deep Energy Method (XDEM), a unified deep learning framework that incorporates both displacement discontinuities and crack-tip asymptotics in the discrete setting, while flexibly coupling displacement and phase fields in the continuous setting. This integration enables accurate fracture predictions using uniformly distributed, relatively sparse collocation points. Validation across benchmark problems including stress intensity factor evaluation, straight and kinked crack growth, and complex crack initiation demonstrates that XDEM consistently outperforms standard DEM in accuracy and efficiency. By bridging discrete and phase-field models within a single framework, XDEM establishes a robust foundation for applying AI to fracture mechanics and opens new avenues for predictive modeling in engineering and materials science.

</details>


### [67] [NeuroPINNs: Neuroscience Inspired Physics Informed Neural Networks](https://arxiv.org/abs/2511.06081)
*Shailesh Garg,Souvik Chakraborty*

Main category: physics.comp-ph

TL;DR: NeuroPINNs extend PINNs with spiking neuron models for energy-efficient PDE solving, using Variable Spiking Neurons to enable sparse communication suitable for neuromorphic hardware.


<details>
  <summary>Details</summary>
Motivation: To reduce computational and energy costs of conventional PINNs by leveraging biologically inspired spiking neurons, making them suitable for resource-constrained environments like embedded and edge devices.

Method: Uses Variable Spiking Neurons (VSNs) with stochastic projection to handle discontinuous spiking dynamics while maintaining compatibility with gradient-based optimization, employing surrogate backpropagation for parameter updates.

Result: NeuroPINNs achieve high accuracy on four PDE problems across regular and irregular domains, including 3D linear elastic micromechanics, while substantially reducing communication and energy demands.

Conclusion: NeuroPINNs represent a step toward scalable, neuromorphic-ready scientific machine learning by combining energy efficiency with accurate PDE solving capabilities.

Abstract: We introduce NeuroPINNs, a neuroscience-inspired extension of Physics-Informed Neural Networks (PINNs) that incorporates biologically motivated spiking neuron models to achieve energy-efficient PDE solving. Unlike conventional PINNs, which rely on continuously firing activations and therefore incur high computational and energy costs, NeuroPINNs leverage Variable Spiking Neurons (VSNs) to enable sparse, event-driven communication. This makes them particularly well-suited for deployment on neuromorphic hardware and for scenarios with constrained computational resources, such as embedded and edge devices. A central challenge, however, lies in reconciling the discontinuous dynamics of spiking neurons with the smooth residual-based loss formulation required in PINNs. Direct smoothing introduces systematic biases, leading to inaccurate PDE learning. To overcome this, we employ a novel stochastic projection method inspired from upscaled theory that faithfully captures spiking behavior while maintaining compatibility with gradient-based optimization. Standard surrogate backpropagation is used for parameter updates, ensuring computational tractability. We demonstrate the effectiveness of NeuroPINNs on four representative PDE problems across both regular and irregular domains. Furthermore, application of NeuroPINN for linear elastic micromechnics in three dimensions was also explored. Results show that NeuroPINNs achieve high accuracy while substantially reducing communication and energy demands, marking a step toward scalable, neuromorphic-ready scientific machine learning.

</details>


### [68] [Estimation of Dielectric Parameters from Ultrasound Waves in Quantitative Thermoacoustic Tomography](https://arxiv.org/abs/2511.06278)
*Teemu Sahlström,Tanja Tarvainen*

Main category: physics.comp-ph

TL;DR: Proposes a Bayesian approach for quantitative thermoacoustic tomography to simultaneously estimate electrical conductivity and permittivity from ultrasound measurements.


<details>
  <summary>Details</summary>
Motivation: Extend thermoacoustic tomography to quantitative imaging by estimating dielectric parameters (conductivity and permittivity) rather than just initial pressure distribution.

Method: Bayesian inverse problem framework combining Maxwell's equations for electromagnetic wave propagation and acoustic wave equation for ultrasound propagation.

Result: Dielectric parameters can be accurately estimated, but accuracy depends on ultrasound sensor geometry and number of electromagnetic pulses used.

Conclusion: The proposed Bayesian approach successfully enables simultaneous estimation of electrical conductivity and permittivity in quantitative thermoacoustic tomography.

Abstract: Thermoacoustic tomography (TAT) is an imaging modality based on the thermoacoustic effect. In TAT, a short microwave or radio wave pulse is directed to the imaged target. The energy of the electromagnetic pulse is absorbed depending on the target's dielectric parameters, resulting in a spatially varying pressure distribution via the thermoacoustic effect. This pressure, known as the initial pressure distribution, relaxes as broadband ultrasound waves that are measured on the boundary of the target. In the inverse problem of TAT, the initial pressure is estimated from the measured ultrasound waves. TAT can further be extended to quantitative TAT (QTAT), where the aim is to estimate the dielectric parameters of the imaged target by utilizing a model for electromagnetic wave propagation. In this work, we consider the QTAT problem and propose an approach for simultaneous estimation of electrical conductivity and permittivity from the ultrasound waves. The problem is approached in the framework of Bayesian inverse problems. The forward model describing electromagnetic and acoustic wave propagation is based on Maxwell's equations and the acoustic wave equation, respectively. The approach is evaluated with numerical simulations. The results show that the dielectric parameters can be accurately estimated using the proposed approach. However, the ultrasound sensor geometry and the number of electromagnetic pulses have a significant effect on the accuracy of the estimated parameters.

</details>


### [69] [The noiseless limit and improved-prior limit of the maximum entropy method and their implications for the analytic continuation problem](https://arxiv.org/abs/2511.06915)
*Thomas Chuna,Nicholas Barnfield,Paul Hamann,Sebastian Schwalbe,Michael P. Friedlander,Tobias Dornheim*

Main category: physics.comp-ph

TL;DR: The paper investigates when maximum entropy method and Bryan's algorithm are appropriate for analytic continuation in quantum Monte Carlo simulations, showing they work best when the Bayesian prior is close to the true solution or when noise is near zero.


<details>
  <summary>Details</summary>
Motivation: Quantum Monte Carlo methods can't directly extract dynamic properties without solving the analytic continuation problem, and there's no universally accepted algorithm for this across different fields.

Method: The authors analyze when entropy maximization and Bryan's algorithm are appropriate by examining their behavior under different conditions, comparing Bryan's algorithm with a newly formulated dual approach.

Result: Stochastic sampling reduces to entropy maximization when Bayesian prior is near true solution. Bryan's algorithm works when noise is near zero or prior is near true solution, with better error scaling when prior is near solution.

Conclusion: Maximum entropy methods are most effective when Bayesian priors are well-chosen and close to the true solution, pointing to the advantage of improved data-driven priors that leverage this property.

Abstract: Quantum Monte Carlo (QMC) methods are uniquely capable of providing exact simulations of quantum many-body systems. Unfortunately, the applications of a QMC simulation are limited because extracting dynamic properties requires solving the analytic continuation (AC) problem. Across the many fields that use QMC methods, there is no universally accepted analytic continuation algorithm for extracting dynamic properties, but many publications compare to the maximum entropy method. We investigate when entropy maximization is an acceptable approach. We show that stochastic sampling algorithms reduce to entropy maximization when the Bayesian prior is near to the true solution. We investigate when is Bryan's controversial optimization algorithm [Bryan, Eur. Biophys. J. 18, 165-174 (1990)] for entropy maximization (sometimes known as the maximum entropy method) appropriate to use. We show that Bryan's algorithm is appropriate when the noise is near zero or when the Bayesian prior is near to the true solution. We also investigate the mean squared error, finding a better scaling when the Bayesian prior is near the true solution than when the noise is near zero. We point to examples of improved data-driven Bayesian priors that have already leveraged this advantage. We support these results by solving the double Gaussian problem using both Bryan's algorithm and the newly formulated dual approach to entropy maximization [Chuna et al., J. Phys. A: Math. Theor. 58, 335203 (2025)].

</details>


### [70] [Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients](https://arxiv.org/abs/2511.07347)
*Giorrgio M. Cavallazzi,Miguel Perex Cuadrado,Alfredo Pinelli*

Main category: physics.comp-ph

TL;DR: WHNO uses Walsh-Hadamard transforms for better handling of PDEs with discontinuous coefficients, outperforming Fourier methods and achieving even better results when combined with FNO in ensembles.


<details>
  <summary>Details</summary>
Motivation: Standard Fourier-based neural operators struggle with discontinuous coefficients due to Gibbs phenomenon and poor representation of sharp interfaces in PDEs.

Method: Introduces Walsh-Hadamard Neural Operator (WHNO) using Walsh-Hadamard transforms with learnable spectral weights to handle piecewise constant fields and sharp interfaces efficiently.

Result: WHNO outperforms FNO in preserving sharp solution features at material interfaces. Ensemble combinations of WHNO and FNO reduce mean squared error by 35-40% and maximum error by up to 25% compared to individual models.

Conclusion: Walsh-Hadamard and Fourier representations capture complementary aspects of discontinuous PDE solutions, with WHNO excelling at sharp interfaces while FNO captures smooth features effectively.

Abstract: Neural operators have emerged as powerful tools for learning solution operators of partial differential equations (PDEs). However, standard spectral methods based on Fourier transforms struggle with problems involving discontinuous coefficients due to the Gibbs phenomenon and poor representation of sharp interfaces. We introduce the Walsh-Hadamard Neural Operator (WHNO), which leverages Walsh-Hadamard transforms-a spectral basis of rectangular wave functions naturally suited for piecewise constant fields-combined with learnable spectral weights that transform low-sequency Walsh coefficients to capture global dependencies efficiently. We validate WHNO on three problems: steady-state Darcy flow (preliminary validation), heat conduction with discontinuous thermal conductivity, and the 2D Burgers equation with discontinuous initial conditions. In controlled comparisons with Fourier Neural Operators (FNO) under identical conditions, WHNO demonstrates superior accuracy with better preservation of sharp solution features at material interfaces. Critically, we discover that weighted ensemble combinations of WHNO and FNO achieve substantial improvements over either model alone: for both heat conduction and Burgers equation, optimal ensembles reduce mean squared error by 35-40 percent and maximum error by up to 25 percent compared to individual models. This demonstrates that Walsh-Hadamard and Fourier representations capture complementary aspects of discontinuous PDE solutions, with WHNO excelling at sharp interfaces while FNO captures smooth features effectively.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [71] [Fusion alpha particle momentum deposition in thermonuclear burn dynamics](https://arxiv.org/abs/2511.06154)
*A. J. Crilly,B. D. Appelbe,E. A. Ferdinandi,S. T. O'Neill,H. Biragnet,N. Chaturvedi,J. P. Chittenden,B. Duhig,P. W. Moloney*

Main category: physics.plasm-ph

TL;DR: Alpha particle momentum deposition in inertial confinement fusion reduces yield by accelerating the shell, decreasing hotspot compression and increasing disassembly rate.


<details>
  <summary>Details</summary>
Motivation: Current models of thermonuclear burn neglect the momentum carried by DT fusion alpha particles, which could significantly impact ignition and burn propagation in central hotspot ignition schemes.

Method: Used radiation hydrodynamics simulations with Monte Carlo alpha particle transport model to investigate alpha momentum deposition across sub-ignition to robust ignition regimes, scaling current NIF designs hydrodynamically.

Result: Alpha particle ram pressure accelerates the shell during burn, reducing hotspot compression and increasing disassembly rate, causing ~30% yield reduction at current NIF scale and ~10% penalty at larger scales.

Conclusion: Alpha momentum deposition is a significant effect for present ignition-scale implosions that must be included in ignition criteria, burn models, and high-gain ICF designs.

Abstract: In inertial confinement fusion, the DT fusion alpha particles carry not only energy but also appreciable momentum that is typically neglected in models of thermonuclear burn. In the central hotspot ignition scheme, the hotspot must self-heat and propagate thermonuclear burn before disassembly. Using radiation hydrodynamics simulations with a Monte Carlo alpha particle transport model, we investigate the effect of alpha momentum deposition across sub-ignition to robustly igniting regimes by hydrodynamic scaling of current central hotspot ignition designs from the National Ignition Facility (NIF). We find that the effective alpha particle ram pressure accelerates the shell at burn, reducing hotspot compression, increasing the rate of disassembly and decreasing yield. This causes a notable (~ 30%) reduction in yield at current NIF scale, with a persistent (~ 10%) penalty at larger hydrodynamic scales. These results demonstrate that alpha momentum deposition is a significant effect for present ignition-scale implosions, necessitating its inclusion in ignition criteria, burn models, and designs for high-gain inertial confinement fusion.

</details>


### [72] [Beam-tracing and profile evolution for localised beams in inhomogeneous plasmas](https://arxiv.org/abs/2511.06255)
*Lewin Basil Slader Marsh*

Main category: physics.plasm-ph

TL;DR: Derives beam tracing and profile evolution for localized beams in inhomogeneous cold plasma, showing Hermite modes evolve into mode superpositions during propagation.


<details>
  <summary>Details</summary>
Motivation: To understand how arbitrary beam profiles evolve during propagation through inhomogeneous plasmas, addressing limitations in prior work that assumed mode preservation.

Method: Developed beam tracing equations with additional PDE for profile evolution, used ladder operators to solve for generic solution families, constructed orthogonal inner product for solutions.

Result: Obtained exact expression for beam profile showing Hermite modes evolve into superpositions of different modes in inhomogeneous plasmas, contrary to previous assumptions.

Conclusion: The approach provides a framework for analyzing beam evolution in inhomogeneous plasmas with orthogonal solutions useful for diagnostic signal analysis.

Abstract: We derive the beam tracing and profile evolution for the propagation of any localised beam with arbitrary profile through an inhomogeneous cold plasma. We recover standard Gaussian beam-tracing, with an additional PDE describing the evolution of the beam's profile as it propagates through the plasma. We then solve for generic families of solutions to the PDE using ladder operators, which can be chosen to reduce to Gauss-Hermite beams in homogeneous media. We importantly obtain an exact expression for the resulting beam profile, demonstrating that Hermite modes will generally evolve into a superposition of different modes during propagation through inhomogeneous plasmas, contrary to prior work on the subject. Importantly, this approach allows us to construct an inner product with orthogonality between solutions for the beam evolution, a useful feature for future analysis of the diagnostic signal received from arbitrary beams.

</details>


### [73] [Conceptual design of Thomson scattering system with high wavelength resolution in magnetically confined plasmas for electron phase-space measurements](https://arxiv.org/abs/2511.06330)
*Kentaro Sakai,Kentaro Tomita,Takeo Hoshi,Akito Nakano,Motoshi Goto,Kenichi Nagaoka,Ryo Yasuhara*

Main category: physics.plasm-ph

TL;DR: Design of a high-resolution Thomson scattering system with 2560 wavelength channels to measure electron velocity distribution functions in magnetically confined plasmas at CHD.


<details>
  <summary>Details</summary>
Motivation: To enable direct observation of electron velocity distribution function shapes in magnetically confined plasmas, which is crucial for understanding plasma behavior and detecting non-Maxwellian distributions.

Method: Developed a spatially-resolved spectrometer with 2560 wavelength channels, calculated synthetic spectra with various plasma parameters and distribution functions, and analyzed signal-to-noise ratio and parameter estimation accuracy.

Result: Estimated scattered photons per channel significantly exceed unity, enabling direct observation of scattered spectra shapes. System can accurately estimate plasma parameters and shows feasibility for detecting non-Maxwellian distributions.

Conclusion: The high-wavelength resolution Thomson scattering system is feasible for measuring electron velocity distribution functions and can potentially detect non-Maxwellian distributions in magnetically confined plasmas.

Abstract: We discuss the conceptual design of a spatially-resolved spectroscopy system of Thomson scattering with high wavelength resolution capable of measuring the shape of electron velocity distribution functions in magnetically confined plasmas. We design a spatially-resolved spectrometer with 2560 wavelength channels. The estimated number of scattered photons in a single spectrometer channel is much larger than unity under the setup and plasma parameters at the Compact Helical Device (CHD), indicating that the shape of scattered spectra can be directly observed. We calculate the synthetic spectra with various plasma parameters and electron velocity distribution functions. The signal-to-noise ratio and accuracy of the estimated parameter are examined using the synthetic spectra assuming Maxwellian electron velocity distribution functions. We also discuss the feasibility of detecting non-Maxwellian electron velocity distribution functions with the high wavelength resolution spectrometer.

</details>


### [74] [The Non-thermal Energy Window for Laser-Driven Nuclear Reactions](https://arxiv.org/abs/2511.06657)
*Eunseok Hwang,Heamin Ko,Myung-Ki Cheoun,Dukjae Jang*

Main category: physics.plasm-ph

TL;DR: Analytical framework for nuclear reaction rates in laser-plasma interactions, identifying new effective energy window distinct from conventional Gamow window.


<details>
  <summary>Details</summary>
Motivation: Conventional Gamow window formulation is inadequate for non-thermal ion distributions in ultra-intense laser-plasma interactions used in laboratory astrophysics.

Method: Analytical framework based on Target Normal Sheath Acceleration (TNSA) mechanism to evaluate nuclear reaction rates under non-equilibrium conditions.

Result: Identified new effective energy window and analytical expression of fusion reactivity different from conventional Gamow window.

Conclusion: Provides predictive tool for laboratory astrophysics experiments replicating astrophysical nuclear processes using laser-driven nuclear reactions.

Abstract: Astrophysical nuclear reaction rates in stellar environments are governed by the Gamow window, where Maxwell-Boltzmann distributions and quantum tunneling probabilities combine to produce effective reactivity. However, this conventional formulation is inadequate for the non-thermal ion distributions generated in ultra-intense laser-plasma interactions. Here, we introduce an analytical framework, based on a Target Normal Sheath Acceleration (TNSA) mechanism, to evaluate nuclear reaction rates under these non-equilibrium conditions. We identify a new effective energy window and analytical expression of the fusion reactivity distinct from the conventional Gamow window, providing a predictive tool for laboratory astrophysics experiments designed to replicate astrophysical nuclear processes using laser-driven nuclear reactions.

</details>


### [75] [Reduced kinetic model for ion temperature gradient instability in tokamaks with reversed magnetic shear](https://arxiv.org/abs/2511.06959)
*B. Jia,Q. Zhong,Y. Li,Y. Xiao*

Main category: physics.plasm-ph

TL;DR: The paper develops a Schrödinger-type model for ITG modes in tokamaks using magnetic drift and FLR expansion, extending it to reversed magnetic shear configurations. The model reveals a double-well potential structure that causes mode degeneracy and shows ITG instability resonates with magnetic drift frequency.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between simplified analytic models and global simulations for ion temperature gradient modes in tokamak plasmas, particularly under reversed magnetic shear configurations where traditional models may not apply.

Method: Using averaged magnetic drift model and first-order finite Larmor radius expansion to reduce ITG eigenvalue equation to Schrödinger-type differential equation, then extending to reversed magnetic shear via generalized translational invariance. Benchmarking against global gyrokinetic simulations from GTC.

Result: Good quantitative agreement with GTC simulations. Discovery of characteristic double-well potential in RMS profiles causing degeneracy between even and odd eigenmodes. ITG instability resonates with magnetic drift frequency, with maximum growth when rational surfaces are slightly separated.

Conclusion: The model provides new physical insights into ITG behavior under reversed magnetic shear and offers a compact, accurate theoretical framework that successfully bridges analytic models and global simulations.

Abstract: Using the averaged magnetic drift model and a first-order finite Larmor radius (FLR) expansion, the eigenvalue equation for the ion temperature gradient (ITG) mode in tokamak plasmas is reduced to a Schrödinger-type differential equation. By invoking generalized translational invariance, the model is extended to reversed magnetic shear (RMS) configurations and benchmarked against global gyrokinetic simulations from GTC, showing good quantitative agreement. The analysis reveals a characteristic double-well potential unique to RMS profiles, which gives rise to the degeneracy between the lowest-order even and first-order odd eigenmodes when the two potential wells are sufficiently separated radially. The ITG instability is also found to resonate with the magnetic drift frequency, and its maximum growth occurs when the two rational surfaces are slightly separated. These results provide new physical insight into ITG mode behavior under reversed magnetic shear and offer a compact, accurate theoretical framework that bridges simplified analytic models and global simulations.

</details>


### [76] [Design and operation of APEX-LD: a compact levitated dipole for the confinement of electron-positron pair plasmas](https://arxiv.org/abs/2511.07191)
*A. Card,M. R. Stoneking,A. Deller,E. V. Stenson*

Main category: physics.plasm-ph

TL;DR: The APEX project built a levitated dipole trap (APEX-LD) using a high-temperature superconducting coil to confine electron-positron pair plasmas, achieving stable levitation for over 3 hours and persistent magnetic fields of ~0.5 T.


<details>
  <summary>Details</summary>
Motivation: To magnetically confine and study electron-positron pair plasmas, which requires stable magnetic confinement systems.

Method: Constructed a levitated dipole trap with a compact (7.5-cm radius) No-Insulation ReBCO superconducting coil, solder-potted in gold-plated-copper case, using helium gas cooling and inductive charging.

Result: Achieved persistent currents of ~60 kA-turns, axial magnetic flux density of B_0 ~ 0.5 T, levitation times exceeding 3 hours with vertical stability sigma_z < 20 um, and demonstrated coil robustness despite quenches and shocks.

Conclusion: The APEX-LD successfully demonstrates stable magnetic confinement capabilities and prepares for future positron injection experiments.

Abstract: The objective of the APEX (A Positron-Electron eXperiment) project is to magnetically confine and study electron--positron pair plasmas. For this purpose, a levitated dipole trap (APEX-LD) has been constructed. The magnetically levitated, compact (7.5-cm radius), closed-loop, high-temperature superconducting (HTS) floating (F-)coil consists exclusively of a No-Insulation (NI) Rare-earth Barium Copper Oxide (ReBCO) winding pack, solder-potted in a gold-plated-copper case. A resealable in-vacuum cryostat facilitates cooling (via helium gas) and inductive charging of the F-coil. The 70-minute preparation cycle reliably generates persistent currents of ~60 kA-turns and an axial magnetic flux density of B_0 ~ 0.5 T. We demonstrate levitation times in excess of three hours with a vertical stability of sigma_z < 20 um. Despite being subjected to routine quenches (and occasional mechanical shocks), the F-coil has proven remarkably robust. We present the results of preliminary experiments with electrons, and outline the next steps for injecting positron bunches into the device.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [77] [Particle loads for cosmological simulations with equal-mass dark matter and baryonic particles](https://arxiv.org/abs/2511.06210)
*Shihong Liao,Yizhou Liu,Haonan Zheng,Ming Li,Jie Wang,Liang Gao,Bingqing Sun,Shi Shao*

Main category: astro-ph.CO

TL;DR: Proposes a new method using the glass approach to generate two-component particle loads with general dark matter to gas particle number ratios, avoiding spurious collisional heating in cosmological simulations.


<details>
  <summary>Details</summary>
Motivation: Traditional simulations with unequal-mass dark matter and baryonic particles cause spurious collisional heating due to energy equipartition. Equal-mass particle setups are preferred but previous grid-based methods can only achieve specific particle number ratios due to symmetry constraints.

Method: Developed a glass-based approach that simultaneously relaxes two Poisson particle distributions by introducing additional repulsive forces between particles of the same component, enabling generation of particle loads with general N_DM:N_gas ratios.

Result: The generated particle load follows the expected minimal power spectrum (P(k) ∝ k^4), exhibits good homogeneity and isotropy, remains stable under gravity, and both components show uniform and isotropic distributions. Equal-mass setups effectively mitigate spurious collisional heating.

Conclusion: The proposed glass-based method successfully generates two-component uniform and isotropic particle distributions with flexible particle number ratios, solving the collisional heating problem in cosmological simulations. The method can be extended to multi-component systems.

Abstract: Traditional cosmological hydrodynamical simulations usually assume equal-numbered but unequal-mass dark matter and baryonic particles, which can lead to spurious collisional heating due to energy equipartition. To avoid such a numerical heating effect, a simulation setup with equal-mass dark matter and baryonic particles, which corresponds to a particle number ratio of $N_{\rm DM}:N_{\rm gas} = Ω_{\rm cdm} / Ω_{\rm b}$, is preferred. However, previous studies have typically used grid-based particle loads to prepare such initial conditions, which can only reach specific values for $N_{\rm DM}:N_{\rm gas}$ due to symmetry requirements. In this study, we propose a method based on the glass approach that can generate two-component particle loads with more general $N_{\rm DM}:N_{\rm gas}$ ratios. The method simultaneously relaxes two Poisson particle distributions by introducing an additional repulsive force between particles of the same component. We show that the final particle load closely follows the expected minimal power spectrum, $P(k) \propto k^{4}$, exhibits good homogeneity and isotropy properties, and remains sufficiently stable under gravitational interactions. Both the dark matter and gas components individually also exhibit uniform and isotropic distributions. We apply our method to two-component cosmological simulations and demonstrate that an equal-mass particle setup effectively mitigates the spurious collisional heating that arises in unequal-mass simulations. Our method can be extended to generate multi-component uniform and isotropic distributions. Our code based on Gadget-2 is available at https://github.com/liaoshong/gadget-2glass .

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [78] [A general comparison principle for the pluripotential complex Monge-Ampère flow](https://arxiv.org/abs/2511.06412)
*Bowoo Kang*

Main category: math.CV

TL;DR: Proves comparison principle for pluripotential complex Monge-Ampère flows with specific right-hand side form, leading to uniqueness of weak solutions and analysis of long-term behavior.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for pluripotential complex Monge-Ampère flows, particularly addressing uniqueness of solutions and their asymptotic properties.

Method: Proves comparison principle for flows with right-hand side dt∧dμ where dμ is dominated by Monge-Ampère measure of bounded plurisubharmonic function.

Result: Obtains uniqueness of weak solution to pluripotential Cauchy-Dirichlet problem and studies long-term behavior under certain assumptions.

Conclusion: Establishes fundamental comparison principle enabling uniqueness results and provides insights into asymptotic behavior of solutions in pluripotential theory.

Abstract: We prove a comparison principle for the pluripotential complex Monge-Ampère flows for the right-hand side of the form $dt \wedge dμ$ where $dμ$ is dominated by a Monge-Ampère measure of a bounded plurisubharmonic function. As a consequence, we obtain the uniqueness of the weak solution to the pluripotential Cauchy-Dirichlet problem. We also study the long-term behavior of the solution under some assumption.

</details>


<div id='math.KT'></div>

# math.KT [[Back]](#toc)

### [79] [The relative index theorem and a characterization of Fredholm operators](https://arxiv.org/abs/2511.06375)
*Magnus Fries*

Main category: math.KT

TL;DR: Extension of relative index theorem to hypoelliptic operators on non-compact manifolds, showing local index calculation and necessary/sufficient Fredholm conditions.


<details>
  <summary>Details</summary>
Motivation: To generalize index theory to non-compact manifolds and hypoelliptic operators, providing better tools for calculating indices locally and characterizing Fredholm operators geometrically.

Method: Extend relative index theorem framework to arbitrary-order hypoelliptic differential operators, analyze invertibility at infinity conditions, and connect to unbounded KK-theory models.

Result: Proved that local index changes can be calculated locally, and that invertibility at infinity is both necessary and sufficient for Fredholmness, providing geometric characterization of Fredholm operators.

Conclusion: The work provides a general geometric characterization of Fredholmness and convenient tools for index theory on non-compact spaces, connecting to unbounded KK-theory with Fredholm assumptions rather than compact resolvent requirements.

Abstract: We extend the relative index theorem on non-compact manifolds to encompass a wide variety of hypoelliptic differential operators of arbitrary order, demonstrating that the change in index when changing a differential operator locally can be calculated locally. We also show that the notion of invertibility at infinity (and coercive at infinity) is not only sufficient condition for an operator to be Fredholm but also necessary, resulting in a general geometric characterization of Fredholmness. This characterization connects to a model for unbounded \(KK\)-theory which assumes the operator to be Fredholm instead of having (locally) compact resolvent, and thus provides a convenient tool for index theory on non-compact spaces.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [80] [Sparsity via Hyperpriors: A Theoretical and Algorithmic Study under Empirical Bayes Framework](https://arxiv.org/abs/2511.06235)
*Zhitao Li,Yiqiu Dong,Xueying Zeng*

Main category: stat.ML

TL;DR: Analysis of hyperparameter estimation in empirical Bayes framework for sparse learning, showing certain hyperpriors promote sparsity and stability, with PALM algorithm implementation and validation on image deblurring.


<details>
  <summary>Details</summary>
Motivation: To understand how hyperpriors influence sparsity and local optimality in empirical Bayes framework for sparse learning problems.

Method: Theoretical analysis of hyperpriors' effects, then implementation of proximal alternating linearized minimization (PALM) algorithm with convergence guarantees for both convex and concave hyperpriors.

Result: Strictly increasing hyperpriors like half-Laplace and half-generalized Gaussian effectively promote sparsity and improve solution stability against noise. Extensive tests on 2D image deblurring show significant sparsity promotion and restoration accuracy enhancement.

Conclusion: Appropriate hyperpriors significantly improve sparsity and accuracy in empirical Bayes framework, with performance influenced by noise level and problem ill-posedness.

Abstract: This paper presents a comprehensive analysis of hyperparameter estimation within the empirical Bayes framework (EBF) for sparse learning. By studying the influence of hyperpriors on the solution of EBF, we establish a theoretical connection between the choice of the hyperprior and the sparsity as well as the local optimality of the resulting solutions. We show that some strictly increasing hyperpriors, such as half-Laplace and half-generalized Gaussian with the power in $(0,1)$, effectively promote sparsity and improve solution stability with respect to measurement noise. Based on this analysis, we adopt a proximal alternating linearized minimization (PALM) algorithm with convergence guaranties for both convex and concave hyperpriors. Extensive numerical tests on two-dimensional image deblurring problems demonstrate that introducing appropriate hyperpriors significantly promotes the sparsity of the solution and enhances restoration accuracy. Furthermore, we illustrate the influence of the noise level and the ill-posedness of inverse problems to EBF solutions.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [81] [Lecture notes on flow equation approach to singular stochastic PDEs](https://arxiv.org/abs/2511.07120)
*Paweł Duch*

Main category: math.PR

TL;DR: The flow equation approach provides a robust framework for solving singular SPDEs with fractional Laplacians across the subcritical regime, using Wilson's renormalization group to study coarse-grained processes.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic method for handling singular stochastic partial differential equations (SPDEs) with fractional Laplacians throughout the subcritical regime, inspired by renormalization group theory.

Method: Uses flow equations to study coarse-grained processes across spatial scales, with the flow equation describing how nonlinear terms evolve with coarse-graining scale, analogous to Polchinski equation in QFT.

Result: The approach enables solving renormalization problems inductively by imposing appropriate boundary conditions on the flow equation.

Conclusion: The flow equation framework offers a robust and systematic approach for renormalization in singular SPDEs with fractional Laplacians throughout the subcritical regime.

Abstract: The flow equation approach is a robust framework applicable to a broad class of singular SPDEs, including those with fractional Laplacians, throughout the entire subcritical regime. Inspired by Wilson's renormalization group, this method studies the coarse-grained process, which captures the behaviour of solutions across spatial scales. The corresponding flow equation describes how the nonlinear terms in the effective dynamics evolve with the coarse-graining scale, playing a role analogous to the Polchinski equation in quantum field theory. The renormalization problem is then solved inductively by imposing appropriate boundary conditions on the flow equation.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [82] [Resonating valence bond pairing energy in graphene by quantum Monte Carlo](https://arxiv.org/abs/2511.06506)
*S. Azadi,A. Principi,T. D. Kühne,M. S. Bahramy*

Main category: cond-mat.str-el

TL;DR: The study investigates RVB states in graphene using quantum Monte Carlo methods, finding that electron pairing is stabilized by finite energy gaps near the Fermi level, which depend on system geometry.


<details>
  <summary>Details</summary>
Motivation: To understand the resonating-valence-bond state in graphene and how system geometry affects electron pairing stability.

Method: Used variational and diffusion quantum Monte Carlo with Jastrow-Slater-determinant and Jastrow-antisymmetrized-geminal-power ansatze on rectangular graphene samples.

Result: Found no stable RVB pairing in zero-gap systems, but finite gaps near Fermi level stabilize pairing with energy of ~0.48(1) mHa/atom at thermodynamic limit.

Conclusion: Reveals a geometry-driven electron pairing mechanism in confined graphene nanostructures where finite energy gaps enable stable RVB states.

Abstract: We determine the resonating-valence-bond (RVB) state in graphene using real-space quantum Monte Carlo with correlated variational wave functions. Variational and diffusion quantum Monte Carlo (DMC) calculations with Jastrow-Slater-determinant and Jastrow-antisymmetrized-geminal-power ansatze are employed to evaluate the RVB pairing energy. Using a rectangular graphene sample that lacks $π/3$ rotational symmetry, we found that the single-particle energy gap near the Fermi level depends on the system size along the $x$-direction. The gap vanishes when the length satisfies $L_x=3n\sqrt{3}d$, where $n$ is an integer and $d$ is the carbon-carbon bond length, otherwise, the system, exhibits a finite gap. Our DMC results show no stable RVB pairing in the zero-gap case, whereas the opening of a finite gap near the Fermi level stabilizes the electron pairing. The DMC predicted absolute value of pairing energy at the thermodynamic limit for a finite-gap system is $\sim 0.48(1)$ mHa/atom. Our results reveal a feometry-driven electron pairing mechanism in the confined graphene nanostructure.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [83] [A Field Free Line 3D Reconstruction Model for Magnetic Particle Imaging for Improved Sensitivity, Resolution, and High Dynamic Range Imaging](https://arxiv.org/abs/2511.06627)
*Toby Sanders,Hayden Carlton,Preethi Korangath,Olivia C. Sehl,Robert Ivkov,Patrick W. Goodwill*

Main category: physics.med-ph

TL;DR: New 3D reconstruction framework for magnetic particle imaging (MPI) using multi-angle field-free line scans improves resolution, quantitative accuracy, and dynamic range over conventional methods.


<details>
  <summary>Details</summary>
Motivation: MPI is used for cancer cell tracking, lymph node mapping, and cell therapy monitoring, but conventional reconstruction pipelines have limitations in spatial resolution and quantitative accuracy.

Method: Combines physics-based FFL signal model with tomographic projection operators to create efficient 3D forward operator, enabling joint reconstruction of full dataset with harmonic-domain compression for reduced memory usage.

Result: Substantially reduced background haze, improved visualization of low-intensity regions, ~11× improvement in iron detection sensitivity compared to conventional X-space CT approach, with volumetric reconstructions on standard GPU hardware in minutes.

Conclusion: The framework enhances MPI image quality and quantitative reliability, supporting broader use in preclinical and future clinical imaging applications.

Abstract: Magnetic particle imaging (MPI) is a tracer-based imaging modality that detects superparamagnetic iron oxide nanoparticles in vivo, with applications in cancer cell tracking, lymph node mapping, and cell therapy monitoring. We introduce a new 3D image reconstruction framework for MPI data acquired using multi-angle field-free line (FFL) scans, demonstrating improvements in spatial resolution, quantitative accuracy, and high dynamic range performance over conventional sequential reconstruction pipelines. The framework is built by combining a physics-based FFL signal model with tomographic projection operators to form an efficient 3D forward operator, enabling the full dataset to be reconstructed jointly rather than as a series of independent 2D projections. A harmonic-domain compression step is incorporated naturally within this operator formulation, reducing memory overhead by over two orders of magnitude while preserving the structure and fidelity of the model, enabling volumetric reconstructions on standard desktop GPU hardware in only minutes. Phantom and in vivo results demonstrate substantially reduced background haze and improved visualization of low-intensity regions adjacent to bright structures, with an estimated $\sim$11$\times$ improvement in iron detection sensitivity relative to the conventional X-space CT approach. These advances enhance MPI image quality and quantitative reliability, supporting broader use of MPI in preclinical and future clinical imaging.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [84] [Rigidity of Gradient Shrinking Ricci Solitons with Bach-like Flatness and Related Variational Formulas](https://arxiv.org/abs/2511.05774)
*James Siene*

Main category: math.DG

TL;DR: The paper analyzes Bach-like tensors in four dimensions, proving that their vanishing forces gradient-shrinking Ricci solitons to be Einstein or Gaussian, and shows these tensors arise from quadratic curvature functionals.


<details>
  <summary>Details</summary>
Motivation: To extend classification results for gradient-shrinking Ricci solitons beyond the classical Bach tensor case by considering general Bach-like tensors and their geometric implications.

Method: Define Bach-like tensors as linear combinations of two symmetric divergence-free curvature tensors U and V, prove classification theorems for solitons where these vanish, and derive them as Euler-Lagrange equations from quadratic curvature functionals.

Result: Except along the Bach line, vanishing of any Bach-like tensor forces four-dimensional gradient-shrinking Ricci solitons to be either Einstein or isometric to the Gaussian soliton.

Conclusion: Bach-like tensors provide a natural generalization of the Bach tensor with similar classification power for Ricci solitons, and they arise from a two-parameter family of quadratic curvature functionals with computable variational formulas.

Abstract: The classical Bach tensor in four dimensions can be expressed as a linear combination of two independent, symmetric, divergence-free, quadratic in curvature tensors U and V. Several classification results for gradient-shrinking Ricci solitons have been obtained under the assumption that the Bach tensor vanishes. We define a Bach-like tensor to be any other linear combination of U and V. We prove that, except along the Bach line, the vanishing of a Bach-like tensor forces a four-dimensional complete gradient-shrinking Ricci soliton to be either Einstein or isometric to the Gaussian soliton. Finally, we show that Bach-like tensors arise as Euler-Lagrange equations of a two-parameter family of quadratic curvature functionals and compute the corresponding first and second variation formulas.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [85] [Simulating Clifford Circuits with Gaussian Elimination](https://arxiv.org/abs/2511.06127)
*Yuchen Pang,Edgar Solomonik*

Main category: quant-ph

TL;DR: Efficient algorithm for simulating Clifford quantum circuits using Gaussian elimination on modified adjacency matrices derived from circuit structure, with improved performance for strong simulation via fast matrix multiplication.


<details>
  <summary>Details</summary>
Motivation: Clifford circuits are important quantum circuits that can be simulated efficiently classically while still exhibiting quantum effects like entanglement, but existing simulation methods have limitations in computational complexity.

Method: Uses ZX-calculus tensor network representation to reduce Clifford circuits to graph states, then applies Gaussian elimination on modified adjacency matrices and LDL decomposition over GF(2) with tree-decomposition-based fast algorithms.

Result: Achieves state-of-the-art complexity for weak graph state simulation and improves strong graph state simulation using Strassen-like fast matrix multiplication. Efficient for computing multiple amplitudes or samples.

Conclusion: Provides efficient simulation algorithms for Clifford circuits with improved strong simulation performance, plus new characterization of locally Clifford equivalent graph states and learning protocol for low-rank adjacency matrices.

Abstract: Quantum circuits are considered more powerful than classical circuits and require exponential resources to simulate classically. Clifford circuits are a special class of quantum circuits that can be simulated in polynomial time but still show important quantum effects such as entanglement. In this work, we present an algorithm that simulates Clifford circuits by performing Gaussian elimination on a modified adjacency matrix derived from the circuit structure. Our work builds on an ZX-calculus tensor network representation of Clifford circuits that reduces to quantum graph states. We give a concise formula of amplitudes of graph states based on the LDL decomposition of matrices over GF(2), and use it to get efficient algorithms for strong and weak simulation of Clifford circuits using tree-decomposition-based fast LDL algorithm. The complexity of our algorithm matches the state of art for weak graph state simulation and improves the state of art for strong graph state simulation by taking advantage of Strassen-like fast matrix multiplication. Our algorithm is also efficient when computing many amplitudes or samples of a Clifford circuit. Further, our amplitudes formula provides a new characterization of locally Clifford equivalent graph states as well as an efficient protocol to learn graph states with low-rank adjacency matrices.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [86] [Challenges in predicting positron annihilation lifetimes in lead halide perovskites: correlation functionals and polymorphism](https://arxiv.org/abs/2511.06926)
*Kajal Madaan,Guido Roma,Jasurbek Gulomov,Pascal Pochet,Catherine Corbel,Ilja Makkonen*

Main category: cond-mat.mtrl-sci

TL;DR: This study examines positron annihilation spectroscopy in halide perovskites, focusing on how different electron-positron correlation functionals affect vacancy detection, particularly for cation vacancies.


<details>
  <summary>Details</summary>
Motivation: Previous studies on positron annihilation in APbX$_3$ halide perovskites show inconsistent theoretical predictions and suggest that A-site vacancies are not detected, highlighting the need to understand the role of electron-positron correlation functionals.

Method: Theoretical study comparing various electron-positron correlation approximations (including semi-local and weighted density approximation) applied to MAPbI$_3$, CsPbI$_3$, and CsPbBr$_3$ perovskites with/without vacancies, analyzing void sizes via Voronoi volumes and investigating polymorphism effects.

Result: The choice of electron-positron correlation functional has crucial influence on positron lifetime calculations in halide perovskites, especially for cation vacancies - much stronger than in metals, alloys and conventional semiconductors.

Conclusion: The strong dependence on correlation functionals may require reconsideration of experimental lifetime interpretations in halide perovskites, particularly for detecting cation vacancies.

Abstract: Halide perovskites have emerged in the last decade as a new important class of semiconductors for a variety of optoelectronic applications. A lot of previous studies were thus devoted to the characterisation of their point defects. Positron annihilation spectroscopy is a well recognized tool for probing vacancies in materials. Recent applications of this technique to APbX$_3$ halide perovskites are sparse, and the rare theoretical predictions of positron lifetimes in these materials, published in association with experiments, do not fully agree with each other. These works suggest that vacancies on the A site are not detected.
  In our theoretical study we focus on the role of the electron-positron correlation functional. We thoroughly revisit and compare several approximations when applied to methylammonium lead iodide (MAPbI$_3$) with or without vacancies, as well as inorganic perovskites CsPbI$_3$ and CsPbBr$_3$, in various phases. We show also the relationship between the size of the voids, through Voronoi volumes, and the calculated lifetimes. For the cubic phases we investigate in detail the role of polymorphism, including the distribution of vacancy formation energies and positron annihilation lifetimes.
  In our lifetimes calculations, apart from older and more recent semi-local approximations for the electron-positron correlation potential, we also consider the weighted density approximation (WDA), which is truly non-local and should better describe positron annihilation in regions with strong electronic density variations. We show that for this class of materials, and especially for cations vacancies, the influence of the chosen approximation is crucial, much stronger than in metals, alloys and conventional semiconductors. This influence may induce to reconsider the interpretation of experimentally determined lifetimes.

</details>


### [87] [Effect of Misfit and Threading Dislocations on Surface Energies of PbTe-PbSe Interfaces](https://arxiv.org/abs/2511.07182)
*Emir Bilgili,Nicholas Taormina,Yang Li,Adrian Diaz,Simon R. Phillpot,Youping Chen*

Main category: cond-mat.mtrl-sci

TL;DR: Simulated PbTe/PbSe heterostructures show that manufacturing methods affect interface structure and surface energy calculations. Wafer bonding creates 2D misfit dislocations while heteroepitaxial growth forms complex 3D networks, leading to significantly different surface energies compared to coherent interface models.


<details>
  <summary>Details</summary>
Motivation: To understand how different manufacturing processes (wafer bonding vs heteroepitaxial growth) affect the interface structure and properties of heterostructures, particularly surface energy calculations that are often based on simplified coherent interface models.

Method: Simulated PbTe and PbSe heterostructures manufactured via direct wafer bonding and heteroepitaxial growth, then computed surface energy using interaction energy measurements across surfaces with a verified code. Compared results with hypothetical coherent interfaces used in traditional slab-based methods.

Result: Semi-coherent interfaces from actual manufacturing processes exhibit up to ~27% lower surface energies than coherent models, while coherent models overestimate surface energies by up to ~50% relative to epitaxial interfaces. Wafer bonding creates interfaces with 2D misfit dislocation networks, while heteroepitaxial growth produces complex 3D networks with both misfit and threading dislocations.

Conclusion: Manufacturing processes significantly impact interface structure and surface energy calculations. Using simplified coherent interface models can lead to substantial errors (up to 50% overestimation) and potentially conflicting predictions of physical phenomena like fracture toughness or growth mode.

Abstract: The manufacturing processes of heterostructures determine the structure and properties of their interfaces. In this work, we simulate PbTe and PbSe heterostructures manufactured via (1) direct wave bonding and (2) heteroepitaxial growth. The former contains interfaces with 2D misfit dislocation networks while the latter contains complex 3D networks with both misfit and threading dislocations. To compute the surface energy of interfaces, we measure the interaction energy across surfaces using a well-verified code. Compared with hypothetical interfaces modeled to be coherent, a typical assumption in traditional slab-based methods, the surface energy of wafer bonded and epitaxially grown interfaces are significantly different. Semi-coherent interfaces exhibit up to ~27% lower surface energies than coherent ones, while coherent models overestimate surface energies by up to ~50% relative to epitaxial interfaces. The consequence of such differences can lead to conflicting predictions of physical phenomena such as fracture toughness or growth mode.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [88] [Learning Biomolecular Motion: The Physics-Informed Machine Learning Paradigm](https://arxiv.org/abs/2511.06585)
*Aaryesh Deshpande*

Main category: q-bio.BM

TL;DR: Physics-informed machine learning (PIML) integrates data-driven methods with physical constraints to create accurate, generalizable models for biomolecular systems, addressing limitations of classical force fields while maintaining thermodynamic consistency.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of classical force fields in modeling biomolecular systems by integrating statistical learning with molecular physics, enabling more accurate modeling of long-timescale kinetics, rare events, and free-energy estimation.

Method: Survey of physics-informed neural networks, operator learning, differentiable molecular simulation, and hybrid physics-ML potentials, framed as solutions to the 'biomolecular closure problem' to recover unresolved interactions beyond classical force fields.

Result: Development of systematic frameworks that produce models preserving thermodynamic consistency and mechanistic interpretability while being able to extrapolate beyond observed domains.

Conclusion: Future advancements in biomolecular simulation will depend on mechanistic inductive biases and integrated differentiable physical learning frameworks, bridging machine learning, statistical physics, and computational chemistry.

Abstract: The convergence of statistical learning and molecular physics is transforming our approach to modeling biomolecular systems. Physics-informed machine learning (PIML) offers a systematic framework that integrates data-driven inference with physical constraints, resulting in models that are accurate, mechanistic, generalizable, and able to extrapolate beyond observed domains. This review surveys recent advances in physics-informed neural networks and operator learning, differentiable molecular simulation, and hybrid physics-ML potentials, with emphasis on long-timescale kinetics, rare events, and free-energy estimation. We frame these approaches as solutions to the "biomolecular closure problem", recovering unresolved interactions beyond classical force fields while preserving thermodynamic consistency and mechanistic interpretability. We examine theoretical foundations, tools and frameworks, computational trade-offs, and unresolved issues, including model expressiveness and stability. We outline prospective research avenues at the intersection of machine learning, statistical physics, and computational chemistry, contending that future advancements will depend on mechanistic inductive biases, and integrated differentiable physical learning frameworks for biomolecular simulation and discovery.

</details>


<div id='physics.atom-ph'></div>

# physics.atom-ph [[Back]](#toc)

### [89] [Optimizing Antihydrogen Production via Slow Plasma Merging](https://arxiv.org/abs/2511.06883)
*E D Hunter,M Bumbar,C Amsler,M Bayo,H Breuker,M Cerwenka,G Costantini,R Ferragut,M Giammarchi,A Gligorova,G Gosta,M Hori,C Killian,V Kraxberger,N Kuroda,A Lanz,M Leali,G Maero,C Malbrunot,V Mascagna,Y Matsuda,S Migliorati,D J Murtagh,M Romé,R E Sheldon,M C Simon,M Tajima,V Toso,S Ulmer,L Venturelli,A Weiser,E Widmann*

Main category: physics.atom-ph

TL;DR: Researchers optimized antihydrogen production in a Penning-Malmberg trap by controlling positron temperature and antiproton entry radius, achieving a 20x improvement over previous methods with 2.3 million atoms per 15-minute run.


<details>
  <summary>Details</summary>
Motivation: To improve antihydrogen production efficiency for fundamental physics research, as previous methods had limited yields of only 31,000 atoms in 4 minutes.

Method: Used a nested Penning-Malmberg trap to slowly combine antiprotons and positrons while controlling the electrostatic barrier lowering rate and positron heating to optimize temperature and density distribution.

Result: Achieved production of 2.3×10^6 antihydrogen atoms per 15-minute run, which is 20 times more efficient than the previous state-of-the-art method (3.1×10^4 atoms in 4 minutes).

Conclusion: Optimal antihydrogen production occurs when positron temperature is lowest and antiprotons enter the positron plasma at the smallest radius, enabling significantly higher yields for fundamental physics experiments.

Abstract: We measure the time-dependent temperature and density distribution of antiprotons and positrons while slowly combining them to make antihydrogen atoms in a nested Penning-Malmberg trap. The total antihydrogen yield and the number of atoms escaping the trap as a beam are greatest when the positron temperature is lowest and when antiprotons enter the positron plasma at the smallest radius. We control these parameters by changing the rate at which we lower the electrostatic barrier between the antiproton and positron plasmas and by heating the positrons. With the optimal settings, we produce $2.3\times 10^6$ antihydrogen atoms per $15$-minute run, surpassing the previous state of the art -- $3.1\times 10^4$ atoms in $4$ minutes -- by a factor of $20$.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [90] [Cluster percolation in the three-dimensional $\pm J$ random-bond Ising model](https://arxiv.org/abs/2511.05748)
*Lambert Münster,Martin Weigel*

Main category: cond-mat.dis-nn

TL;DR: Study shows cluster percolation in 3D random-bond Ising model reveals dual percolating clusters above thermodynamic ordering points, with density divergence marking actual phase transitions.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between cluster percolation and equilibrium ordering phenomena in disordered magnetic systems, particularly how percolation transitions relate to thermodynamic phase transitions.

Method: Used parallel-tempering Monte Carlo simulations on 3D ±J random-bond Ising model with varying antiferromagnetic bond fractions, analyzing cluster definitions in overlap space between independent real replicas.

Result: Found percolation transition occurs above thermodynamic ordering point in disordered ferromagnet and spin-glass phases, characterized by appearance of two percolating clusters with equal density that diverge at actual phase transitions.

Conclusion: Cluster percolation provides signatures of thermodynamic transitions, with dual percolating clusters appearing above ordering points and their density divergence marking the actual ferromagnetic and spin-glass phase transitions.

Abstract: Based on extensive parallel-tempering Monte Carlo simulations, we investigate the relationship between cluster percolation and equilibrium ordering phenomena in the three-dimensional $\pm J$ random-bond Ising model as one varies the fraction of antiferromagnetic bonds. We consider a range of cluster definitions, most of which are constructed in the space of overlaps between two independent real replicas of the system. In the pure ferromagnet that is contained as a limiting case in the class of problems considered, the relevant percolation point coincides with the thermodynamic ordering transition. For the disordered ferromagnet encountered first on introducing antiferromagnetic bonds and the adjacent spin-glass phase of strong disorder this connection is altered, and one finds a percolation transition above the thermodynamic ordering point that is accompanied by the appearance of /two/ percolating clusters of equal density. Only at the lower (disordered) ferromagnetic or spin-glass transition points the densities of these two clusters start to diverge, thus providing a percolation signature of these thermodynamic transitions. We compare the scaling behavior at this secondary percolation transition with the thermodynamic behavior at the corresponding ferromagnetic and spin-glass phase transitions.

</details>


### [91] [Comprehensive Validation of Replica Symmetry Breaking via Quantum Annealing: From Ground States to Topological Collapse](https://arxiv.org/abs/2511.06403)
*Kumar Ghosh*

Main category: cond-mat.dis-nn

TL;DR: Quantum annealing validates Parisi's spin glass solution for 4000 spins, confirming replica symmetry breaking predictions and revealing hierarchical complexity persists under 36% network dilution before collapsing abruptly at a critical threshold.


<details>
  <summary>Details</summary>
Motivation: To systematically validate Giorgio Parisi's Nobel Prize-winning solution of the Sherrington-Kirkpatrick spin glass model beyond computational limits and explore the topological boundaries of hierarchical complexity in disordered systems.

Method: Leveraged quantum annealing to compute ground states for up to 4000 spins, measured three independent RSB predictions (ground-state energies, chaos exponent, state-space overlap distribution), and introduced controlled network dilution via Blume-Capel model to test RSB robustness.

Result: Confirmed Parisi's predictions: energy convergence to -0.7633/N with N^{-2/3} corrections, chaos exponent θ=0.51±0.02, broad overlap distribution (σ_q=0.19). Hierarchical complexity remains invariant under 36% dilution, but collapses discontinuously beyond critical threshold 0.8<D_c<0.9 via avalanche-driven transition.

Conclusion: Quantum annealing provides computational advantage for validating fundamental statistical mechanics, establishing RSB as topological property of network connectivity rather than spin density, with implications for neural networks, optimization, and materials science.

Abstract: Giorgio Parisi's exact solution of the Sherrington-Kirkpatrick spin glass, recognized with the 2021 Nobel Prize in Physics, revealed revolutionary hierarchical organization in disordered systems, yet systematic validation has remained computationally intractable beyond $N \sim 100$ spins, and the topological limits of this complexity remain unexplored. Here we leverage quantum annealing to extend ground-state computations to 4000 spins and systematically probe both the emergence and breakdown of replica symmetry breaking. Three independent measurements validate core RSB predictions: ground-state energies converge to Parisi's value $E_\infty/N = -0.7633$ with predicted $N^{-2/3}$ finite-size corrections; chaos exponent $θ= 0.51 \pm 0.02$ confirms mean-field square-root scaling ($R^2 = 0.989$); and state-space overlap distribution exhibits broad continuous structure ($σ_q = 0.19$) characteristic of hierarchical landscape organization. We then investigate RSB robustness by introducing controlled network dilution via the Blume-Capel model with vacancy formation. Remarkably, hierarchical complexity remains invariant under 36\% dilution, proving RSB is a topological property of network connectivity rather than spin density. Beyond a critical threshold in the range $0.8 < D_c < 0.9$, the hierarchy collapses discontinuously as the system undergoes complete conversion to the all-vacancy state within a narrow parameter window an abrupt avalanche-driven transition where independent-vacancy mean-field theory correctly predicts the energy scale but fails to capture the cooperative dynamics. This comprehensive validation across thermodynamics, universality, landscape geometry, and topological limits establishes quantum advantage for probing fundamental statistical mechanics in complex systems relevant to neural networks, optimization, and materials science.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [92] [Solving bilevel optimization via sequential minimax optimization](https://arxiv.org/abs/2511.07398)
*Zhaosong Lu,Sanyou Mei*

Main category: math.OC

TL;DR: Proposes a sequential minimax optimization (SMO) method for constrained bilevel optimization problems with nonsmooth convex lower-level and possibly nonconvex upper-level objectives, achieving improved computational complexity.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for solving constrained bilevel optimization problems where traditional approaches face computational challenges, particularly when dealing with nonsmooth convex lower-level problems and nonconvex upper-level objectives.

Method: SMO applies first-order methods to solve a sequence of minimax subproblems using a hybrid of modified augmented Lagrangian and penalty schemes on the bilevel optimization problems.

Result: Establishes operation complexities of O(ε⁻⁷logε⁻¹) for merely convex lower-level objectives and O(ε⁻⁶logε⁻¹) for strongly convex lower-level objectives, improving previous best-known complexity by a factor of ε⁻¹.

Conclusion: SMO demonstrates superior computational performance compared to existing first-order penalty methods, providing an efficient approach for constrained bilevel optimization with improved complexity bounds.

Abstract: In this paper we propose a sequential minimax optimization (SMO) method for solving a class of constrained bilevel optimization problems in which the lower-level part is a possibly nonsmooth convex optimization problem, while the upper-level part is a possibly nonconvex optimization problem. Specifically, SMO applies a first-order method to solve a sequence of minimax subproblems, which are obtained by employing a hybrid of modified augmented Lagrangian and penalty schemes on the bilevel optimization problems. Under suitable assumptions, we establish an operation complexity of $O(\varepsilon^{-7}\log\varepsilon^{-1})$ and $O(\varepsilon^{-6}\log\varepsilon^{-1})$, measured in terms of fundamental operations, for SMO in finding an $\varepsilon$-KKT solution of the bilevel optimization problems with merely convex and strongly convex lower-level objective functions, respectively. The latter result improves the previous best-known operation complexity by a factor of $\varepsilon^{-1}$. Preliminary numerical results demonstrate significantly superior computational performance compared to the recently developed first-order penalty method.

</details>


### [93] [A PDE Perspective on Generative Diffusion Models](https://arxiv.org/abs/2511.05940)
*Kang Liu,Enrique Zuazua*

Main category: math.OC

TL;DR: Rigorous PDE framework for score-based diffusion models, proving well-posedness, stability estimates, and data manifold concentration with theoretical guarantees for generative fidelity.


<details>
  <summary>Details</summary>
Motivation: Despite empirical success of score-based diffusion models, their mathematical foundations regarding stability and consistency of governing equations remain only partially understood.

Method: Developed PDE framework using Li-Yau differential inequality for heat flow, proving well-posedness and L^p-stability estimates for score-based Fokker-Planck dynamics through entropy stability methods.

Result: Proved reverse-time dynamics concentrate on data manifold for compactly supported distributions with √t concentration rate, providing theoretical guarantee that diffusion trajectories return to data manifold under exact score guidance.

Conclusion: Framework provides quantitative understanding of trade-off between generative capacity and imitation fidelity, with practical insights for score-function construction, loss formulation, and stopping-time selection.

Abstract: Score-based diffusion models have emerged as a powerful class of generative methods, achieving state-of-the-art performance across diverse domains. Despite their empirical success, the mathematical foundations of those models remain only partially understood, particularly regarding the stability and consistency of the underlying stochastic and partial differential equations governing their dynamics.
  In this work, we develop a rigorous partial differential equation (PDE) framework for score-based diffusion processes. Building on the Li--Yau differential inequality for the heat flow, we prove well-posedness and derive sharp $L^p$-stability estimates for the associated score-based Fokker--Planck dynamics, providing a mathematically consistent description of their temporal evolution. Through entropy stability methods, we further show that the reverse-time dynamics of diffusion models concentrate on the data manifold for compactly supported data distributions and a broad class of initialization schemes, with a concentration rate of order $\sqrt{t}$ as $t \to 0$.
  These results yield a theoretical guarantee that, under exact score guidance, diffusion trajectories return to the data manifold while preserving imitation fidelity. Our findings also provide practical insights for designing diffusion models, including principled criteria for score-function construction, loss formulation, and stopping-time selection. Altogether, this framework provides a quantitative understanding of the trade-off between generative capacity and imitation fidelity, bridging rigorous analysis and model design within a unified mathematical perspective.

</details>


<div id='physics.class-ph'></div>

# physics.class-ph [[Back]](#toc)

### [94] [Do Discrete Fine-Scale Mechanical Models with Rotational Degrees of Freedom Homogenize Into a Cosserat or a Cauchy Continuum?](https://arxiv.org/abs/2511.06279)
*Jan Eliáš,Gianluca Cusatis*

Main category: physics.class-ph

TL;DR: Homogenization of discrete mechanical models with rotational degrees of freedom produces either Cauchy or Cosserat continuum models depending on local bending stiffness magnitude, with Cosserat behavior occurring only at non-physically high fine-scale stiffness values.


<details>
  <summary>Details</summary>
Motivation: To determine whether homogenization of discrete fine-scale mechanical models (particle/lattice models) with rotational degrees of freedom produces equivalent Cauchy-type or Cosserat-type continua.

Method: Uses asymptotic expansion homogenization to analyze discrete mechanical models with rotational degrees of freedom, valid for both stationary and transient conditions and arbitrary nonlinear, inelastic fine-scale constitutive equations.

Result: The unit cell problem is always stationary, with inertia appearing only in coarse-scale linear momentum balance. Two limiting conditions emerge based on local bending stiffness: Cauchy continuum (low bending stiffness) and Cosserat continuum (high bending stiffness). A heuristic combination provides accurate results in transition regions.

Conclusion: Cases where Cosserat character is significant correspond to non-physically high fine-scale bending stiffness, making them of no practical interest in real-world applications.

Abstract: This article answers the question of whether homogenization of discrete fine-scale mechanical models, such as particle or lattice models, gives rise to an equivalent continuum that is of Cauchy-type or Cosserat-type. The study employs the machinery of asymptotic expansion homogenization to analyze discrete mechanical models with rotational degrees of freedom commonly used to simulate the mechanical behavior of heterogeneous solids. The proposed derivation has general validity in both stationary (steady-state) and transient conditions (assuming wavelength much larger that particle size) and for arbitrary nonlinear, inelastic fine-scale constitutive equations. The results show that the unit cell problem is always stationary, and the only inertia term appears in the linear momentum balance equation at the coarse scale. Depending on the magnitude of the local bending stiffness, mathematical homogenization rigorously identifies two limiting conditions that correspond to the Cauchy continuum and the Cosserat continuum. A heuristic combination of these two limiting conditions provides very accurate results also in the transition from one limiting case to the other. Finally, the study demonstrates that cases for which the Cosserat character of the homogenized response is significant are associated with non-physically high fine-scale bending stiffness and, as such, are of no interest in practice.

</details>


<div id='physics.hist-ph'></div>

# physics.hist-ph [[Back]](#toc)

### [95] [A computational framework for evaluating an edge-integrated, multi-ramp construction model of the Great Pyramid of Giza](https://arxiv.org/abs/2511.06112)
*Vicente Luis Rosell Roig*

Main category: physics.hist-ph

TL;DR: The paper presents an Integrated Edge-Ramp (IER) model for Khufu's pyramid construction using a helical ramp system, showing it can achieve 4-6 minute block dispatches and complete construction in 20-27 years with acceptable structural stresses.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a quantitative framework evaluating construction throughput, geometric control, and zero external footprint for Khufu's pyramid.

Method: Used a unified pipeline combining parametric geometry, discrete-event logistics, and staged finite-element analysis (FEA) to test the helical IER model with adaptive multiramp strategy.

Result: The model achieves 4-6 minute block dispatches, 13.8-20.6 year on-site construction (20-27 years total), acceptable structural stresses, and consistency with muon-imaged internal voids.

Conclusion: The IER model reconciles construction requirements and provides falsifiable predictions, offering a transferable framework for testing ancient megastructure construction hypotheses.

Abstract: Despite decades of study, a quantitative, integrated framework to evaluate minutescale throughput, geometric control, and a zero external footprint for Khufu's pyramid has been lacking. We test the Integrated Edge-Ramp (IER) model-a helical path formed by omitting and backfilling perimeter courses-using a unified, end-to-end pipeline coupling parametric geometry, discrete-event logistics, and staged finite-element analysis (FEA). An adaptive multiramp strategy can sustain 4-6-minute dispatches and yields a median on-site duration of 13.8-20.6 years (95% CI); including quarrying, river transport, and seasonal pauses gives 20-27 years. FEA indicates that stresses and settlements remain within plausible limits for Old Kingdom limestone under self-weight. The model's geometry is also consistent with internal voids identified by muon imaging (a hypothesis-generating result). The IER helps reconcile throughput, survey access, and zero-footprint closure, and produces falsifiable predictions (edge-fill signatures, corner wear). Our study provides a transferable, open-data/code framework for testing construction hypotheses for ancient megastructures.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [96] [Mathematical Modeling and Error Estimation for the Thermal Dunking Problem: A Hierarchical Approach](https://arxiv.org/abs/2511.07138)
*Theron Guo,Kento Kaneko,Claude Le Bris,Anthony T. Patera*

Main category: cs.CE

TL;DR: The paper analyzes thermal dunking problems with small Biot numbers, developing simplified models from conjugate heat transfer equations and providing error bounds for practical applications.


<details>
  <summary>Details</summary>
Motivation: Full conjugate heat transfer simulations are computationally expensive, so simplified models are needed for practical thermal dunking analysis, especially in the small-Biot-number regime.

Method: Systematically reduces conjugate heat transfer formulation to lumped-capacitance model using time scale separation and uniform solid temperature assumptions. Derives asymptotic error bounds and proposes data-driven framework for extending empirical correlations.

Result: Developed computable upper bounds for modeling errors, validated by direct numerical simulations up to Reynolds numbers of 10,000. Showed that large time scale separation yields small homogenization errors.

Conclusion: The proposed framework provides practical tools for thermal dunking analysis with quantified error bounds, extending empirical correlations to broader geometries through learned characteristic length scales.

Abstract: We consider the thermal dunking problem, in which a solid body is suddenly immersed in a fluid of different temperature, and study both the temporal evolution of the solid and the associated Biot number -- a non-dimensional heat transfer coefficient characterizing heat exchange across the solid-fluid interface. We focus on the small-Biot-number regime. The problem is accurately described by the conjugate heat transfer (CHT) formulation, which couples the Navier-Stokes and energy equations in the fluid with the heat equation in the solid through interfacial continuity conditions. Because full CHT simulations are computationally expensive, simplified models are often used in practice. Starting from the coupled equations, we systematically reduce the formulation to the lumped-capacitance model, a single ordinary differential equation with a closed-form solution, based on two assumptions: time scale separation and a spatially uniform solid temperature. The total modeling error is decomposed into time homogenization and lumping contributions. We derive an asymptotic error bound for the lumping error, valid for general heterogeneous solids and spatially varying heat transfer coefficients. Building on this theoretical result, we introduce a computable upper bound expressed in measurable quantities for practical evaluation. Time scale separation is analyzed theoretically and supported by physical arguments and simulations, showing that large separation yields small time homogenization errors. In practice, the Biot number must be estimated from so-called empirical correlations, which are typically limited to specific canonical geometries. We propose a data-driven framework that extends empirical correlations to a broader range of geometries through learned characteristic length scales. All results are validated by direct numerical simulations up to Reynolds numbers of 10,000.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [97] [AutoHood3D: A Multi-Modal Benchmark for Automotive Hood Design and Fluid-Structure Interaction](https://arxiv.org/abs/2511.05596)
*Vansh Sharma,Harish Jai Ganesh,Maryam Akram,Wanjiao Liu,Venkat Raman*

Main category: cs.LG

TL;DR: AutoHood3D is a high-fidelity multi-modal dataset with 16,000+ automotive hood variants for ML applications in engineering design and multiphysics surrogates, addressing hood deformation during rotary-dip painting.


<details>
  <summary>Details</summary>
Motivation: Existing datasets are limited to 2D cases, have few geometric variations, and lack multi-modal annotations and data structures needed for advanced ML applications in engineering.

Method: Created dataset using coupled Large-Eddy Simulation (LES)-Finite Element Analysis (FEA) with 1.2M cells for accuracy, providing time-resolved physical fields, STL meshes, and natural language prompts for text-to-geometry synthesis.

Result: Validated numerical methodology and established quantitative baselines across five neural architectures, demonstrating systematic surrogate errors in displacement and force predictions.

Conclusion: The dataset enables physics-aware ML development, accelerates generative-design iteration, and facilitates new FSI benchmarks, motivating novel approaches with multiphysics loss functions that enforce fluid-solid coupling.

Abstract: This study presents a new high-fidelity multi-modal dataset containing 16000+ geometric variants of automotive hoods useful for machine learning (ML) applications such as engineering component design and process optimization, and multiphysics system surrogates. The dataset is centered on a practical multiphysics problem-hood deformation from fluid entrapment and inertial loading during rotary-dip painting. Each hood is numerically modeled with a coupled Large-Eddy Simulation (LES)-Finite Element Analysis (FEA), using 1.2M cells in total to ensure spatial and temporal accuracy. The dataset provides time-resolved physical fields, along with STL meshes and structured natural language prompts for text-to-geometry synthesis. Existing datasets are either confined to 2D cases, exhibit limited geometric variations, or lack the multi-modal annotations and data structures - shortcomings we address with AutoHood3D. We validate our numerical methodology, establish quantitative baselines across five neural architectures, and demonstrate systematic surrogate errors in displacement and force predictions. These findings motivate the design of novel approaches and multiphysics loss functions that enforce fluid-solid coupling during model training. By providing fully reproducible workflows, AutoHood3D enables physics-aware ML development, accelerates generative-design iteration, and facilitates the creation of new FSI benchmarks. Dataset and code URLs in Appendix.

</details>


### [98] [An MLCommons Scientific Benchmarks Ontology](https://arxiv.org/abs/2511.05614)
*Ben Hawks,Gregor von Laszewski,Matthew D. Sinclair,Marco Colombo,Shivaram Venkataraman,Rutwik Jain,Yiwei Jiang,Nhan Tran,Geoffrey Fox*

Main category: cs.LG

TL;DR: This paper introduces MLCommons Science Benchmarks Ontology - a unified, community-driven framework for standardizing scientific machine learning benchmarks across physics, chemistry, materials science, biology, climate science, and other domains.


<details>
  <summary>Details</summary>
Motivation: Existing scientific ML benchmarks are siloed and lack standardization, making novel applications fragmented and unclear in their impact pathways. There's a need for a unified benchmarking framework to enable reproducible, cross-domain evaluation.

Method: Developed through community-driven effort extending MLCommons ecosystem. Consolidates disparate benchmarks into a single taxonomy with six-category rating rubric. Uses open submission workflow coordinated by MLCommons Science Working Group.

Result: Created an extensible ontology that supports future scientific and AI/ML motifs. Provides standardized foundation for reproducible benchmarking across scientific domains. Includes methods for identifying emerging computing patterns for unique scientific workloads.

Conclusion: The MLCommons Science Benchmarks Ontology offers a scalable, standardized framework that enables stakeholders to select appropriate benchmarks and promotes high-quality benchmarking practices in scientific machine learning.

Abstract: Scientific machine learning research spans diverse domains and data modalities, yet existing benchmark efforts remain siloed and lack standardization. This makes novel and transformative applications of machine learning to critical scientific use-cases more fragmented and less clear in pathways to impact. This paper introduces an ontology for scientific benchmarking developed through a unified, community-driven effort that extends the MLCommons ecosystem to cover physics, chemistry, materials science, biology, climate science, and more. Building on prior initiatives such as XAI-BENCH, FastML Science Benchmarks, PDEBench, and the SciMLBench framework, our effort consolidates a large set of disparate benchmarks and frameworks into a single taxonomy of scientific, application, and system-level benchmarks. New benchmarks can be added through an open submission workflow coordinated by the MLCommons Science Working Group and evaluated against a six-category rating rubric that promotes and identifies high-quality benchmarks, enabling stakeholders to select benchmarks that meet their specific needs. The architecture is extensible, supporting future scientific and AI/ML motifs, and we discuss methods for identifying emerging computing patterns for unique scientific workloads. The MLCommons Science Benchmarks Ontology provides a standardized, scalable foundation for reproducible, cross-domain benchmarking in scientific machine learning. A companion webpage for this work has also been developed as the effort evolves: https://mlcommons-science.github.io/benchmark/

</details>


### [99] [Guiding Generative Models to Uncover Diverse and Novel Crystals via Reinforcement Learning](https://arxiv.org/abs/2511.07158)
*Hyunsoo Park,Aron Walsh*

Main category: cs.LG

TL;DR: A reinforcement learning framework that guides diffusion models to generate diverse, novel, and thermodynamically viable crystalline compounds, addressing the misalignment between likelihood-based sampling and targeted exploration of underexplored regions.


<details>
  <summary>Details</summary>
Motivation: To overcome the fundamental challenge of objective misalignment between generative modeling's likelihood-based sampling and the need to discover novel compounds in underexplored regions of the design space.

Method: Integrates group relative policy optimization with verifiable multi-objective rewards that balance creativity, stability, and diversity in latent denoising diffusion models.

Result: Enables generation of diverse and novel crystalline compounds while maintaining thermodynamic viability, and demonstrates enhanced property-guided design that preserves chemical validity.

Conclusion: Establishes a modular foundation for controllable AI-driven inverse design that addresses the novelty-validity trade-off in scientific discovery applications of generative models.

Abstract: Discovering functional crystalline materials entails navigating an immense combinatorial design space. While recent advances in generative artificial intelligence have enabled the sampling of chemically plausible compositions and structures, a fundamental challenge remains: the objective misalignment between likelihood-based sampling in generative modelling and targeted focus on underexplored regions where novel compounds reside. Here, we introduce a reinforcement learning framework that guides latent denoising diffusion models toward diverse and novel, yet thermodynamically viable crystalline compounds. Our approach integrates group relative policy optimisation with verifiable, multi-objective rewards that jointly balance creativity, stability, and diversity. Beyond de novo generation, we demonstrate enhanced property-guided design that preserves chemical validity, while targeting desired functional properties. This approach establishes a modular foundation for controllable AI-driven inverse design that addresses the novelty-validity trade-off across scientific discovery applications of generative models.

</details>


### [100] [Explainable Deep Learning-based Classification of Wolff-Parkinson-White Electrocardiographic Signals](https://arxiv.org/abs/2511.05973)
*Alice Ragonesi,Stefania Fresca,Karli Gillette,Stefan Kurath-Koller,Gernot Plank,Elena Zappon*

Main category: cs.LG

TL;DR: A deep learning model using synthetic ECG data from virtual heart models achieves >95% accuracy in localizing Wolff-Parkinson-White accessory pathways across 24 cardiac regions, with explainable AI methods providing physiological validation and identifying key ECG leads.


<details>
  <summary>Details</summary>
Motivation: Current methods for accessory pathway localization in WPW syndrome have limitations including poor anatomical resolution, limited interpretability, and small clinical datasets, creating barriers to clinical adoption.

Method: Developed a deep learning model trained on a large database of synthetic ECGs generated using personalized virtual heart models, integrated with explainable AI methods (Guided Backpropagation, Grad-CAM, Guided Grad-CAM) for interpretation.

Result: Achieved 95% localization accuracy with 94.32% sensitivity and 99.78% specificity. XAI outputs were physiologically validated, and lead V2 was identified as most critical for localization, followed by aVF, V1, and aVL.

Conclusion: Combining cardiac digital twins with explainable deep learning enables accurate, transparent, and non-invasive accessory pathway localization, demonstrating potential for clinical translation.

Abstract: Wolff-Parkinson-White (WPW) syndrome is a cardiac electrophysiology (EP) disorder caused by the presence of an accessory pathway (AP) that bypasses the atrioventricular node, faster ventricular activation rate, and provides a substrate for atrio-ventricular reentrant tachycardia (AVRT). Accurate localization of the AP is critical for planning and guiding catheter ablation procedures. While traditional diagnostic tree (DT) methods and more recent machine learning (ML) approaches have been proposed to predict AP location from surface electrocardiogram (ECG), they are often constrained by limited anatomical localization resolution, poor interpretability, and the use of small clinical datasets. In this study, we present a Deep Learning (DL) model for the localization of single manifest APs across 24 cardiac regions, trained on a large, physiologically realistic database of synthetic ECGs generated using a personalized virtual heart model. We also integrate eXplainable Artificial Intelligence (XAI) methods, Guided Backpropagation, Grad-CAM, and Guided Grad-CAM, into the pipeline. This enables interpretation of DL decision-making and addresses one of the main barriers to clinical adoption: lack of transparency in ML predictions. Our model achieves localization accuracy above 95%, with a sensitivity of 94.32% and specificity of 99.78%. XAI outputs are physiologically validated against known depolarization patterns, and a novel index is introduced to identify the most informative ECG leads for AP localization. Results highlight lead V2 as the most critical, followed by aVF, V1, and aVL. This work demonstrates the potential of combining cardiac digital twins with explainable DL to enable accurate, transparent, and non-invasive AP localization.

</details>


### [101] [DyKAF: Dynamical Kronecker Approximation of the Fisher Information Matrix for Gradient Preconditioning](https://arxiv.org/abs/2511.06477)
*Nikolay Yudin,Ekaterina Grishina,Andrey Veprikov,Alexandr Beznosikov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: DyKAF optimizer uses projector-splitting integrators to create better Kronecker-factorized Fisher matrix preconditioners, outperforming existing methods in LLM training.


<details>
  <summary>Details</summary>
Motivation: Existing matrix-based optimizers use heuristic Kronecker approximations of the Fisher matrix due to computational constraints, but these lack optimality and accuracy.

Method: Leverages projector-splitting integrators to construct effective preconditioners, improving the Fisher matrix approximation quality through dynamical Kronecker approximation.

Result: DyKAF consistently improves Fisher matrix approximation quality and outperforms existing optimizers in large language model pre-training and fine-tuning across multiple metrics.

Conclusion: The DyKAF optimizer provides a novel and effective approach for constructing accurate Kronecker-factorized Fisher matrix preconditioners, demonstrating superior performance in practical LLM training scenarios.

Abstract: Recently, optimizers that explicitly treat weights as matrices, rather than flattened vectors, have demonstrated their effectiveness. This perspective naturally leads to structured approximations of the Fisher matrix as preconditioners, where the matrix view induces a Kronecker-factorized form that enables memory-efficient representation. However, constructing such approximations both efficiently and accurately remains an open challenge, since obtaining the optimal factorization is resource-intensive and practical methods therefore rely on heuristic design choices. In this work, we introduce a novel approach that leverages projector-splitting integrators to construct effective preconditioners. Our optimizer, DyKAF (Dynamical Kronecker Approximation of the Fisher Matrix), consistently improves the Fisher matrix approximation quality. Experiments on large language model pre-training and fine-tuning demonstrate that DyKAF outperforms existing optimizers across a range of evaluation metrics.

</details>


### [102] [Fast Bayesian Updates via Harmonic Representations](https://arxiv.org/abs/2511.06978)
*Di Zhang*

Main category: cs.LG

TL;DR: A novel framework using harmonic analysis transforms Bayesian updates into spectral convolution, enabling fast O(N log N) computation via FFT instead of traditional O(N^2) methods.


<details>
  <summary>Details</summary>
Motivation: Bayesian inference is computationally intractable due to challenging evidence integrals, with conventional methods like MCMC and VI facing scalability and efficiency limitations.

Method: Represent prior and likelihood in orthogonal basis to transform Bayesian update into spectral convolution, then use spectral truncation and FFT for efficient computation.

Result: The method achieves O(N log N) complexity - substantial improvement over O(N^2) naive methods, with rigorous mathematical criteria established for applicability.

Conclusion: This work offers a paradigm shift connecting Bayesian computation to signal processing, enabling real-time sequential inference for smooth distributions with spectral decay.

Abstract: Bayesian inference, while foundational to probabilistic reasoning, is often hampered by the computational intractability of posterior distributions, particularly through the challenging evidence integral. Conventional approaches like Markov Chain Monte Carlo (MCMC) and Variational Inference (VI) face significant scalability and efficiency limitations. This paper introduces a novel, unifying framework for fast Bayesian updates by leveraging harmonic analysis. We demonstrate that representing the prior and likelihood in a suitable orthogonal basis transforms the Bayesian update rule into a spectral convolution. Specifically, the Fourier coefficients of the posterior are shown to be the normalized convolution of the prior and likelihood coefficients. To achieve computational feasibility, we introduce a spectral truncation scheme, which, for smooth functions, yields an exceptionally accurate finite-dimensional approximation and reduces the update to a circular convolution. This formulation allows us to exploit the Fast Fourier Transform (FFT), resulting in a deterministic algorithm with O(N log N) complexity -- a substantial improvement over the O(N^2) cost of naive methods. We establish rigorous mathematical criteria for the applicability of our method, linking its efficiency to the smoothness and spectral decay of the involved distributions. The presented work offers a paradigm shift, connecting Bayesian computation to signal processing and opening avenues for real-time, sequential inference in a wide class of problems.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [103] [Magnetic Pseudo-differential Operators with Hörmander Symbols Dominated by Tempered Weights](https://arxiv.org/abs/2511.07184)
*Mikkel Hviid Thorn*

Main category: math-ph

TL;DR: Extension of matrix representation for magnetic pseudo-differential operators in tight Gabor frames to asymmetrical quantizations and smooth symbols with tempered weight dominance.


<details>
  <summary>Details</summary>
Motivation: To generalize previous work on magnetic pseudo-differential operators by allowing asymmetrical quantizations and broader symbol classes beyond momentum variable decay/growth properties.

Method: Extends matrix representation framework from prior papers to handle asymmetrical quantizations and symbols dominated by tempered weights rather than just momentum variable properties.

Result: Achieves new results in symbol calculus for these operators and establishes Schatten-class properties for the extended operator class.

Conclusion: Successfully generalizes the theory of magnetic pseudo-differential operators to more flexible quantizations and symbol classes, yielding new mathematical insights.

Abstract: We extend the matrix representation of magnetic pseudo-differential operators in a tight Gabor frame from [arXiv:1804.05220, arXiv:2212.12229] to asymmetrical quantizations and smooth symbols dominated by a tempered weight (and not just decay/growth properties in the momentum variables). This leads to new results regarding the symbol calculus of such operators and their Schatten-class properties.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [104] [Kerr Black Hole Shadows in Dispersive Plasma: Frequency-Dependent Geodesics and Shadow Distortions](https://arxiv.org/abs/2511.06466)
*Sai Karan Mukthapuram,Sandeep Kumar Kataria*

Main category: astro-ph.HE

TL;DR: This paper extends black hole shadow theory to include plasma effects, deriving analytical solutions for photon trajectories in Kerr spacetime with specific plasma distributions and showing how plasma alters shadow size and shape.


<details>
  <summary>Details</summary>
Motivation: Real astrophysical black holes are surrounded by plasma, which can modify photon trajectories through dispersion effects, unlike vacuum models used in current black hole shadow theory.

Method: Built on Hamilton-Jacobi equation framework to systematically study light propagation in Kerr spacetime with pressureless, non-magnetized cold plasma, deriving separability conditions and identifying plasma densities that permit generalized Carter constant.

Result: Computed photon regions and shadow boundaries, characterized frequency-dependent deviations from vacuum case, and determined critical plasma frequency beyond which shadow disappears.

Conclusion: Provides analytical benchmarks for plasma-induced shadow distortions and links observable shadow features to ambient plasma properties, establishing foundation for studying more complex plasma distributions.

Abstract: The black hole shadow, a direct probe of the event horizon's gravitational influence, has been observationally confirmed by the Event Horizon Telescope (EHT). While theoretical models of shadows in vacuum are mature, real astrophysical black holes like M87* and Sgr A* are enveloped in plasma, which can alter photon trajectories through dispersion. Current understanding, based on foundational work, indicates that only specific plasma distributions allow for an analytical treatment via the separation of the Hamilton-Jacobi equation. In this work, we build upon this framework to systematically investigate the propagation of light rays in Kerr spacetime surrounded by a pressureless, non-magnetized cold plasma. We explicitly derive the separability condition, identifying the exact class of plasma densities that permit a generalized Carter constant. For these models, we compute the photon regions and shadow boundaries, characterizing how the shadow's size and shape deviate from the vacuum case in a frequency-dependent manner. Our results provide analytical benchmarks for the distortion of shadows in dispersive media and determine the critical plasma frequency beyond which the shadow is erased, offering a direct link between observable shadow features and the properties of the ambient plasma environment and providing a foundation for studying more dynamic, non-separable plasma distributions.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [105] [From LIF to QIF: Toward Differentiable Spiking Neurons for Scientific Machine Learning](https://arxiv.org/abs/2511.06614)
*Ruyin Wan,George Em Karniadakis,Panos Stinis*

Main category: cs.NE

TL;DR: QIF neurons outperform LIF neurons in SNNs for scientific machine learning tasks, providing smoother and more accurate predictions in function approximation, operator learning, and PDE solving.


<details>
  <summary>Details</summary>
Motivation: Spiking neural networks (SNNs) are underexplored for continuous regression tasks in scientific machine learning, and conventional LIF neurons have limitations like discontinuous responses and jagged activation surfaces.

Method: Systematically evaluate Quadratic Integrate-and-Fire (QIF) neurons as an alternative to Leaky Integrate-and-Fire (LIF) in both directly trained SNNs and ANN-to-SNN conversion frameworks, using architectures like MLPs, DeepONets, and PINNs.

Result: QIF-based networks yield smoother, more accurate, and more stable predictions across benchmarks on function approximation, operator learning, and PDE solving compared to LIF counterparts.

Conclusion: QIF neurons serve as a computational bridge between spiking and continuous-valued deep learning, advancing the integration of neuroscience-inspired dynamics into physics-informed and operator-learning frameworks.

Abstract: Spiking neural networks (SNNs) offer biologically inspired computation but remain underexplored for continuous regression tasks in scientific machine learning. In this work, we introduce and systematically evaluate Quadratic Integrate-and-Fire (QIF) neurons as an alternative to the conventional Leaky Integrate-and-Fire (LIF) model in both directly trained SNNs and ANN-to-SNN conversion frameworks. The QIF neuron exhibits smooth and differentiable spiking dynamics, enabling gradient-based training and stable optimization within architectures such as multilayer perceptrons (MLPs), Deep Operator Networks (DeepONets), and Physics-Informed Neural Networks (PINNs). Across benchmarks on function approximation, operator learning, and partial differential equation (PDE) solving, QIF-based networks yield smoother, more accurate, and more stable predictions than their LIF counterparts, which suffer from discontinuous time-step responses and jagged activation surfaces. These results position the QIF neuron as a computational bridge between spiking and continuous-valued deep learning, advancing the integration of neuroscience-inspired dynamics into physics-informed and operator-learning frameworks.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [106] [Metabolic quantum limit and holographic bound to the information capacity of magnetoencephalography](https://arxiv.org/abs/2511.06401)
*E. Gkoudinakis,S. Li,I. K. Kominis*

Main category: physics.bio-ph

TL;DR: A fundamental bound on brain activity information rate from magnetic field measurements is derived, limited by quantum sensing energy resolution and brain metabolism, yielding 2.6 Mbit/s for human brain.


<details>
  <summary>Details</summary>
Motivation: To establish a universal, technology-independent limit on how much information about brain activity can be extracted from magnetic field measurements, considering quantum sensing constraints and biological energy constraints.

Method: Using energy resolution limit of magnetic sensing combined with brain's metabolic power to derive a universal expression for maximum information rate that depends only on geometry, metabolism, and Planck's constant.

Result: Derived fundamental bound of 2.6 Mbit/s for human brain information rate from magnetic field measurements, with geometric bound reaching 6.6 Gbit/s at high bandwidth limit.

Conclusion: Established a biophysical holographic bound for metabolically powered information conveyed by magnetic fields, demonstrating universal limits on brain activity measurement regardless of sensor technology.

Abstract: Magnetoencephalography, the noninvasive measurement of magnetic fields produced by brain activity, utilizes quantum sensors like superconducting quantum interference devices or atomic magnetometers. Here we derive a fundamental, technology-independent bound on the information that such measurements can convey. Using the energy resolution limit of magnetic sensing together with the brain's metabolic power, we obtain a universal expression for the maximum information rate, which depends only on geometry, metabolism, and Planck's constant, and the numerical value of which is 2.6 Mbit/s. At the high bandwidth limit we arrive at a bound scaling linearly with the area of the current source boundary. We thus demonstrate a biophysical holographic bound for metabolically powered information conveyed by the magnetic field. For the geometry and metabolic power of the human brain the geometric bound is 6.6 Gbit/s.

</details>
