<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 17]
- [math.AP](#math.AP) [Total: 16]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [math-ph](#math-ph) [Total: 2]
- [cond-mat.quant-gas](#cond-mat.quant-gas) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A neural-network based nonlinear non-intrusive reduced basis method with online adaptation for parametrized partial differential equations](https://arxiv.org/abs/2511.07684)
*Jingye Li,Alex Bespalov,Jinglai Li*

Main category: math.NA

TL;DR: A nonlinear, non-intrusive reduced basis method with online adaptation using neural networks for efficient approximation of parametrized PDEs.


<details>
  <summary>Details</summary>
Motivation: To enhance accuracy and efficiency in approximating parametrized partial differential equations, especially for complex scenarios or with limited snapshot data.

Method: Combines neural networks with reduced-order modeling and physics-informed training. Offline stage: nonlinear dimension reduction for basis functions and neural surrogate training. Online stage: lightweight physics-informed neural network refinement.

Result: Enables more accurate representation of complex solution structures than linear mappings through nonlinear reconstruction.

Conclusion: The proposed offline-online framework demonstrates effective performance for accurate prediction in parametrized PDE problems.

Abstract: We propose a nonlinear, non-intrusive reduced basis method with online adaptation for efficient approximation of parametrized partial differential equations. The method combines neural networks with reduced-order modeling and physics-informed training to enhance both accuracy and efficiency. In the offline stage, reduced basis functions are obtained via nonlinear dimension reduction, and a neural surrogate is trained to map parameters to approximate solutions. The surrogate employs a nonlinear reconstruction of the solution from the basis functions, enabling more accurate representation of complex solution structures than linear mappings. The model is further refined during the online stage using lightweight physics-informed neural network training. This offline-online framework enables accurate prediction especially in complex scenarios or with limited snapshot data. We demonstrate the performance and effectiveness of the proposed method through numerical experiments.

</details>


### [2] [A New Initial Approximation Bound in the Durand Kerner Algorithm for Finding Polynomial Zeros](https://arxiv.org/abs/2511.07728)
*B. A. Sanjoyo,M. Yunus,N. Hidayat*

Main category: math.NA

TL;DR: Two new initial value bounds for the Durand-Kerner algorithm improve convergence stability and speed.


<details>
  <summary>Details</summary>
Motivation: The Durand-Kerner algorithm's convergence depends heavily on initial approximations, requiring better methods to ensure stability and faster convergence.

Method: Introduces two novel approaches: New bound 1 and the lambda maximal bound, with theoretical analysis and numerical experiments to evaluate their effectiveness.

Result: The lambda maximal bound consistently ensures all roots lie within the complex circle, leading to faster and more stable convergence. New bound 1 guarantees convergence but yields excessively large radii.

Conclusion: The lambda maximal bound is superior for improving the Durand-Kerner algorithm's performance, providing better convergence characteristics than New bound 1.

Abstract: The Durand-Kerner algorithm is a widely used iterative technique for simultaneously finding all the roots of a polynomial. However, its convergence heavily depends on the choice of initial approximations. This paper introduces two novel approaches for determining the initial values: New bound 1 and the lambda maximal bound, aimed at improving the stability and convergence speed of the algorithm. Theoretical analysis and numerical experiments were conducted to evaluate the effectiveness of these bounds. The lambda maximal bound consistently ensures that all the roots lie within the complex circle, leading to faster and more stable convergence. Comparative results demonstrate that while New bound 1 guarantees convergence, but it yields excessively large radii.

</details>


### [3] [Structure-Preserving Transfer of Grad-Shafranov Equilibria to Magnetohydrodynamic Solvers](https://arxiv.org/abs/2511.07763)
*Rushan Zhang,Golo Wimmer,Qi Tang*

Main category: math.NA

TL;DR: Analysis of errors in transferring Grad-Shafranov equilibria to MHD solvers, identifying three main error sources: improper finite element space choices, mesh misalignment, and under-resolved gradients near separatrix.


<details>
  <summary>Details</summary>
Motivation: To study and reduce errors introduced when transferring magnetic confinement fusion equilibria from Grad-Shafranov solvers to MHD solvers, which cause unwanted perturbations in the loaded equilibria.

Method: Identified three error sources and studied impact of different finite element spaces (including compatible finite elements), mesh alignment, and refinement near separatrix through numerical experiments.

Result: Force balance is best preserved when using structure-preserving finite element spaces with aligned and refined meshes. Projecting magnetic field into divergence-conforming spaces is optimal for force balance, while curl-conforming spaces weakly preserve divergence-free property.

Conclusion: Proper choice of finite element spaces and mesh alignment/refinement are crucial for minimizing errors when transferring Grad-Shafranov equilibria to MHD solvers in magnetic confinement fusion studies.

Abstract: Magnetohydrodynamic (MHD) solvers used to study dynamic plasmas for magnetic confinement fusion typically rely on initial conditions that describe force balance, which are provided by an equilibrium solver based on the Grad-Shafranov (GS) equation. Transferring such equilibria from the GS discretization to the MHD discretization often introduces errors that lead to unwanted perturbations to the equilibria on the level of the MHD discretization. In this work, we identify and analyze sources of such errors in the context of finite element methods, with a focus on the force balance and divergence-free properties of the loaded equilibria. In particular, we reveal three main sources of errors: (1) the improper choice of finite element spaces in the MHD scheme relative to the poloidal flux and toroidal field function spaces in the GS scheme, (2) the misalignment of the meshes from two solvers, and (3) possibly under-resolved strong gradients near the separatrix. With this in mind, we study the impact of different choices of finite element spaces, including those based on compatible finite elements. In addition, we also investigate the impact of mesh misalignment and propose to conduct mesh refinement to resolve the strong gradients near the separatrix. Numerical experiments are conducted to demonstrate equilibria errors arising in the transferred initial conditions. Results show that force balance is best preserved when structure-preserving finite element spaces are used and when the MHD and GS meshes are both aligned and refined. Given that the poloidal flux is often computed in continuous Galerkin spaces, we further demonstrate that projecting the magnetic field into divergence-conforming spaces is optimal for preserving force balance, while projection into curl-conforming spaces, although less optimal for force balance, weakly preserves the divergence-free property.

</details>


### [4] [Fast Direct Solvers](https://arxiv.org/abs/2511.07773)
*Per-Gunnar Martinsson,Michael O'Neil*

Main category: math.NA

TL;DR: Survey of fast direct solvers that build approximate inverses or factorizations of linear systems from PDE discretizations, achieving linear or near-linear time complexity by exploiting data-sparse matrix structures.


<details>
  <summary>Details</summary>
Motivation: Traditional iterative solvers have been industry standard for large-scale PDE problems, but recent developments enable direct solvers with near-linear complexity by leveraging matrix structure.

Method: Builds approximations to matrix inverses or easily invertible factorizations (LU/Cholesky) by exploiting that A⁻¹ is data-sparse with low-rank blocks, enabling linear-time algorithms.

Result: Provides unified framework for both sparse and dense fast direct solvers, with practical guidance for method selection based on application requirements.

Conclusion: Fast direct solvers offer efficient alternatives to iterative methods for PDE discretizations, with linear-time complexity achieved through data-sparse matrix representations.

Abstract: This survey describes a class of methods known as "fast direct solvers". These algorithms address the problem of solving a system of linear equations $\boldsymbol{Ax}=\boldsymbol{b}$ arising from the discretization of either an elliptic PDE or of an associated integral equation. The matrix $\boldsymbol{A}$ will be sparse when the PDE is discretized directly, and dense when an integral equation formulation is used. In either case, industry practice for large scale problems has for decades been to use iterative solvers such as multigrid, GMRES, or conjugate gradients. A direct solver, in contrast, builds an approximation to the inverse of $\boldsymbol{A}$, or alternatively, an easily invertible factorization (e.g. LU or Cholesky). A major development in numerical analysis in the last couple of decades has been the emergence of algorithms for constructing such factorizations or performing such inversions in linear or close to linear time. Such methods must necessarily exploit that the matrix $\boldsymbol{A}^{-1}$ is "data-sparse", typically in the sense that it can be tessellated into blocks that have low numerical rank. This survey provides a unifying context to both sparse and dense fast direct solvers, introduces key concepts with a minimum of notational overhead, and provides guidance to help a user determine the best method to use for a given application.

</details>


### [5] [Hyperellipsoid Density Sampling: Exploitative Sequences to Accelerate High-Dimensional Optimization](https://arxiv.org/abs/2511.07836)
*Julian Soltes*

Main category: math.NA

TL;DR: Hyperellipsoid Density Sampling (HDS) is an adaptive sampling method that outperforms traditional quasi-Monte Carlo methods in high-dimensional optimization by using unsupervised learning to generate intelligent, non-uniform samples.


<details>
  <summary>Details</summary>
Motivation: To address the curse of dimensionality in optimization problems where traditional algorithms become inefficient due to exponential search space expansion.

Method: HDS defines multiple hyperellipsoids throughout the search space using three types of unsupervised learning algorithms to avoid high-dimensional geometric calculations, with optional Gaussian weights to focus sampling on regions of interest.

Result: HDS showed statistically significant improvements over Sobol QMC method on CEC2017 benchmark functions, with performance gains ranging from 3% in 30D to 37% in 10D.

Conclusion: HDS is an effective alternative to QMC sampling for high-dimensional optimization, providing robust performance improvements through intelligent, adaptive sampling.

Abstract: The curse of dimensionality presents a pervasive challenge in optimization problems, with exponential expansion of the search space rapidly causing traditional algorithms to become inefficient or infeasible. An adaptive sampling strategy is presented to accelerate optimization in this domain as an alternative to uniform quasi-Monte Carlo (QMC) methods.
  This method, referred to as Hyperellipsoid Density Sampling (HDS), generates its sequences by defining multiple hyperellipsoids throughout the search space. HDS uses three types of unsupervised learning algorithms to circumvent high-dimensional geometric calculations, producing an intelligent, non-uniform sample sequence that exploits statistically promising regions of the parameter space and improves final solution quality in high-dimensional optimization problems.
  A key feature of the method is optional Gaussian weights, which may be provided to influence the sample distribution towards known locations of interest. This capability makes HDS versatile for applications beyond optimization, providing a focused, denser sample distribution where models need to concentrate their efforts on specific, non-uniform regions of the parameter space.
  The method was evaluated against Sobol, a standard QMC method, using differential evolution (DE) on the 29 CEC2017 benchmark test functions. The results show statistically significant improvements in solution geometric mean error (p < 0.05), with average performance gains ranging from 3% in 30D to 37% in 10D. This paper demonstrates the efficacy of HDS as a robust alternative to QMC sampling for high-dimensional optimization.

</details>


### [6] [Derivation of resonance-based schemes via normal forms](https://arxiv.org/abs/2511.07838)
*Yvain Bruned*

Main category: math.NA

TL;DR: Systematic derivation of resonance-based schemes using normal forms, arborification maps, and Fourier operator decompositions to create low regularity schemes with explicit coefficients and local error formulas.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic approach for deriving resonance-based numerical schemes that can handle low regularity problems with explicit error analysis.

Method: Uses arborification maps on decorated trees, Butcher-Connes-Kreimer type coproduct, and lower-dominant parts decompositions of the Fourier operator from nonlinear interactions.

Result: Produces a new family of low regularity schemes with explicit formulae for coefficients and local error, potentially matching the local error of existing low regularity schemes.

Conclusion: The proposed method provides a systematic framework for deriving resonance-based schemes with explicit error control, offering potential improvements for low regularity problems.

Abstract: In this work, we propose a systematic derivation of resonance-based schemes via normal forms. The main idea is to use an arborification map on decorated trees together with a Butcher-Connes-Kreimer type coproduct and lower-dominant parts decompositions of the Fourier operator coming from the nonlinear interactions. This new family of low regularity schemes has explicit formulae for its coefficients and its local error. Under a mild assumption, one could expect these schemes to have a similar local error as the low regularity schemes proposed in arXiv:2005.01649.

</details>


### [7] [A Novel Block-Alternating Iterative Algorithm for Retrieving Top-$k$ Elements from Factorized Tensors](https://arxiv.org/abs/2511.07898)
*Chuanfu Xiao,Jiaxin Zeng*

Main category: math.NA

TL;DR: The paper proposes a new algorithm for retrieving top-k elements from low-rank tensors by modeling it as a constrained optimization problem and solving it with a block-alternating iterative approach.


<details>
  <summary>Details</summary>
Motivation: High-dimensional tensors are stored in low-rank formats to save memory, but in practice only a small fraction of elements (like top-k largest/smallest) are needed. Efficient retrieval of these elements from compressed formats is crucial for many applications.

Method: Model the top-k retrieval as a continuous constrained optimization problem, then develop a block-alternating iterative algorithm that decomposes it into small subproblems. Use a heuristic algorithm with separable summation structure to solve subproblems alternately.

Result: Numerical experiments on synthetic and real-world tensors show the proposed algorithm achieves better accuracy and stability compared to existing methods.

Conclusion: The optimization-based approach with block-alternating iterations provides an effective solution for top-k element retrieval from low-rank tensors, outperforming previous techniques.

Abstract: Tensors, especially higher-order tensors, are typically represented in low-rank formats to preserve the main information of the high-dimensional data while saving memory space. In practice, only a small fraction elements in high-dimensional data are of interest, such as the $k$ largest or smallest elements. Thus, retrieving the $k$ largest/smallest elements from a low-rank tensor is a fundamental and important task in a wide variety of applications. In this paper, we first model the top-$k$ elements retrieval problem to a continuous constrained optimization problem. To address the equivalent optimization problem, we develop a block-alternating iterative algorithm that decomposes the original problem into a sequence of small-scale subproblems. Leveraging the separable summation structure of the objective function, a heuristic algorithm is proposed to solve these subproblems in an alternating manner. Numerical experiments with tensors from synthetic and real-world applications demonstrate that the proposed algorithm outperforms existing methods in terms of accuracy and stability.

</details>


### [8] [Constructive quasi-uniform sequences over triangles](https://arxiv.org/abs/2511.07909)
*Hengjun Xu,Takashi Goda*

Main category: math.NA

TL;DR: The paper presents a Voronoi-guided greedy packing algorithm for generating quasi-uniform point sets on triangular domains with optimal mesh ratio of 2, analyzes existing triangular low-discrepancy point sets, and demonstrates practical efficiency through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To develop efficient methods for generating high-quality, quasi-uniform point sets over arbitrary two-dimensional triangular domains, which is important for various computational applications requiring uniform sampling on triangular regions.

Method: Voronoi-guided greedy packing algorithm that iteratively selects the point farthest from the current set among candidates determined by the Voronoi diagram of the triangle.

Result: The algorithm achieves optimal mesh ratio of at most 2 after finite iterations, and existing triangular low-discrepancy point sets are proven to have uniformly bounded mesh ratios, establishing their quasi-uniformity.

Conclusion: The proposed method provides an efficient and practical strategy for generating high-quality point sets on individual triangles, with theoretical guarantees of optimal mesh ratio.

Abstract: In this paper, we develop constructive algorithms for generating quasi-uniform point sets and sequences over arbitrary two-dimensional triangular domains. Our proposed method, called the \emph{Voronoi-guided greedy packing} algorithm, iteratively selects the point farthest from the current set among a finite candidate set determined by the Voronoi diagram of the triangle. Our main theoretical result shows that, after a finite number of iterations, the mesh ratio of the generated point set is at most~2, which is known to be optimal. We further analyze two existing triangular low-discrepancy point sets and prove that their mesh ratios are uniformly bounded, thereby establishing their quasi-uniformity. Finally, through a series of numerical experiments, we demonstrate that the proposed method provides an efficient and practical strategy for generating high-quality point sets on individual triangles.

</details>


### [9] [Standard versus Asymptotic Preserving Time Discretizations for the Poisson-Nernst-Planck System in the Quasi-Neutral Limit](https://arxiv.org/abs/2511.07964)
*Clarissa Astuto*

Main category: math.NA

TL;DR: Comparison of time discretization methods for Poisson-Nernst-Planck systems, validating an Asymptotic-Preserving scheme that remains stable for all Debye lengths without initial condition restrictions.


<details>
  <summary>Details</summary>
Motivation: Standard numerical methods struggle with small but non-negligible Debye lengths due to severe stability restrictions and difficulties handling initial conditions, while IMEX schemes offer better asymptotic stability.

Method: Comparison of different time discretization methods, focusing on an Asymptotic-Preserving (AP) IMEX scheme that preserves stability across all Debye length scales without requiring assumptions on initial conditions.

Result: The AP scheme demonstrates asymptotic stability for all Debye lengths and handles initial conditions without restrictions, overcoming limitations of standard methods that suffer from stability issues at small Debye lengths.

Conclusion: Asymptotic-Preserving IMEX schemes provide robust numerical solutions for PNP systems across all Debye length scales, making them superior to standard methods for problems involving small but non-negligible Debye lengths.

Abstract: In this paper, we investigate the correlated diffusion of two ion species governed by a Poisson-Nernst-Planck (PNP) system. Here we further validate the numerical scheme recently proposed in \cite{astuto2025asymptotic}, where a time discretization method was shown to be Asymptotic-Preserving (AP) with respect to the Debye length. For vanishingly Debye lengths, the so called Quasi-Neutral limit can be adopted, reducing the system to a single diffusion equation with an effective diffusion coefficient \cite{CiCP-31-707}. Choosing small, but not negligible, Debye lengths, standard numerical methods suffer from severe stability restrictions and difficulties in handling initial conditions. IMEX schemes, on the other hand, are proved to be asymptotically stable for all Debye lengths, and do not require any assumption on the initial conditions. In this work, we compare different time discretizations to show their asymptotic behaviors.

</details>


### [10] [Generalized Probability Density Approach to Histopolation Schemes of Arbitrary Order](https://arxiv.org/abs/2511.07972)
*Gradimir V. Milovanovic,Federico Nudo*

Main category: math.NA

TL;DR: The paper presents a framework for reconstructing bivariate functions from weighted edge integrals using local histopolation methods with probability densities, extending to arbitrary polynomial orders with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the important problem in tomography, computer vision, and numerical approximation of reconstructing bivariate functions from weighted edge integrals on triangular meshes.

Method: Uses local histopolation methods with unisolvent triples, where edge weights are induced by probability densities. Introduces Jacobi-type and Gegenbauer distributions, employs adaptive parameter selection to minimize reconstruction error, and provides explicit basis functions.

Result: Developed new quadratic reconstruction operators that generalize standard linear histopolation while preserving simplicity and locality. Numerical experiments show superior accuracy for both smooth and oscillatory functions.

Conclusion: The framework provides a flexible, broadly applicable tool for weighted function reconstruction that accommodates any admissible edge density, with enhanced robustness and adaptivity across different function classes and mesh resolutions.

Abstract: In this paper, we investigate the reconstruction of a bivariate function from weighted edge integrals on a triangular mesh, a problem of central importance in tomography, computer vision, and numerical approximation. Our approach is based on local histopolation methods defined through unisolvent triples, where the edge weights are induced by probability densities. We present a general strategy that applies to arbitrary polynomial order~$k$, in which edge moments are taken against orthogonal polynomials associated with the chosen densities. This yields a systematic framework for weighted reconstructions of any degree, with theoretical guarantees of unisolvency and fully explicit basis functions. As a concrete and flexible instance, we introduce a two-parameter family of Jacobi-type distributions on $[-1,1]$, together with its symmetric Gegenbauer subclass, and show how these densities generate new quadratic reconstruction operators that generalize the standard linear histopolation scheme while preserving its simplicity and locality. We employ an adaptive parameter selection algorithm for Jacobi densities, which automatically tunes the distribution parameters to minimize the global reconstruction error. This strategy enhances robustness and adaptivity across different function classes and mesh resolutions. The effectiveness of the proposed operators is demonstrated through extensive numerical experiments, which confirm their superior accuracy in approximating both smooth and highly oscillatory functions. Finally, the framework is sufficiently general to accommodate any admissible edge density, thus providing a flexible and broadly applicable tool for weighted function reconstruction.

</details>


### [11] [In-Memory Load Balancing for Discontinuous Galerkin Methods on Polytopal Meshes](https://arxiv.org/abs/2511.08020)
*Patrick Kopper,Anna Schwarz,Jens Keim,Andrea Beck*

Main category: math.NA

TL;DR: A lightweight load balancing strategy for high-order DG solvers on heterogeneous meshes that uses runtime measurements and space-filling curves to redistribute elements, recovering efficiency while maintaining scaling.


<details>
  <summary>Details</summary>
Motivation: High-order DG methods on general polytopal elements enable complex geometry simulations but create workload imbalance due to modal degrees of freedom evolution and transformations in heterogeneous meshes.

Method: System-agnostic in-memory load balancing using high-precision runtime measurements and efficient data redistribution along space-filling curves to dynamically reassign mesh elements.

Result: The strategy recovers significant lost efficiency on heterogeneous meshes while maintaining excellent strong and weak scaling, demonstrated through Taylor-Green vortex simulations and large-scale parallel runs on MareNostrum 5.

Conclusion: The proposed lightweight load balancing approach effectively addresses workload imbalance in high-order DG solvers on heterogeneous meshes without compromising scalability.

Abstract: High-order accurate discontinuous Galerkin (DG) methods have emerged as powerful tools for solving partial differential equations such as the compressible Navier-Stokes equations due to their excellent dispersion-dissipation properties and scalability on modern hardware. The open-source DG framework FLEXI has recently been extended to support DG schemes on general polytopal elements including tetrahedra, prisms, and pyramids. This advancement enables simulations on complex geometries where purely hexahedral meshes are difficult or impossible to generate. However, the use of meshes with heterogeneous element types introduces a workload imbalance, a consequence of the temporal evolution of modal rather than nodal degrees of freedom and the accompanying transformations. In this work, we present a lightweight, system-agnostic in-memory load balancing strategy designed for high-order DG solvers. The method employs high-precision runtime measurements and efficient data redistribution to dynamically reassign mesh elements along a space-filling curve. We demonstrate the effectiveness of the approach through simulations of the Taylor-Green vortex and large-scale parallel runs on the EuroHPC pre-exascale system MareNostrum 5. Results show that the proposed strategy recovers a significant fraction of the lost efficiency on heterogeneous meshes while retaining excellent strong and weak scaling.

</details>


### [12] [A simple predictor-corrector scheme without order reduction for advection-diffusion-reaction problems](https://arxiv.org/abs/2511.08164)
*Thi Tam Dang,Lukas Einkemmer,Alexander Ostermann*

Main category: math.NA

TL;DR: A predictor-corrector scheme for semilinear advection-diffusion-reaction equations that avoids order reduction by treating diffusion and advection/reaction separately, with rigorous second-order convergence.


<details>
  <summary>Details</summary>
Motivation: Existing splitting approaches for semilinear advection-diffusion-reaction equations suffer from order reduction, especially with inhomogeneous Dirichlet boundary conditions.

Method: Extended approach using modified splitting method with predictor-corrector scheme: predictor solves linear diffusion equation, corrector uses explicit Euler step for advection-reaction equation.

Result: The scheme avoids order reduction and significantly improves accuracy, with rigorous second-order convergence established under appropriate regularity assumptions.

Conclusion: Numerical experiments confirm the theoretical second-order convergence results for this predictor-corrector scheme.

Abstract: Treating diffusion and advection/reaction separately is an effective strategy for solving semilinear advection-diffusion-reaction equations. However, such an approach is prone to suffer from order reduction, especially in the presence of inhomogeneous Dirichlet boundary conditions. In this paper, we extend an approach of Einkemmer and Ostermann [SIAM J. Sci. Comput. 37, A1577-A1592, 2015] to advection-diffusion-reaction problems, where the advection and reaction terms depend nonlinearly on both the solution and its gradient. Starting from a modified splitting method, we construct a predictor-corrector scheme that avoids order reduction and significantly improves accuracy. The predictor only requires the solution of a linear diffusion equation, while the corrector is simply an explicit Euler step of an advection-reaction equation. Under appropriate regularity assumptions on the exact solution, we rigorously establish second-order convergence for this scheme. Numerical experiments are presented to confirm the theoretical results.

</details>


### [13] [A Stable Iterative Direct Sampling Method for Elliptic Inverse Problems with Partial Cauchy Data](https://arxiv.org/abs/2511.08171)
*Bangti Jin,Fengru Wang,Jun Zou*

Main category: math.NA

TL;DR: A novel iterative direct sampling method (IDSM) for solving elliptic inverse problems with partial Cauchy data, featuring data completion, heterogeneously regularized Dirichlet-to-Neumann mapping, and stabilization-correction for robust, flexible, and high-resolution reconstruction of inhomogeneities.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in solving linear or nonlinear elliptic inverse problems with partial Cauchy data, particularly the need for robustness to measurement noise, flexibility in measurement configurations, and enhanced resolution for recovering inhomogeneities.

Method: Integrates three innovations: data completion scheme to reconstruct missing boundary information, heterogeneously regularized Dirichlet-to-Neumann map to improve probing function orthogonality, and stabilization-correction strategy for numerical stability.

Result: The method demonstrates remarkable robustness to measurement noise, flexibility with measurement configurations, provable stability guarantee, and enhanced resolution for recovering inhomogeneities. Numerical experiments in electrical impedance tomography, diffuse optical tomography, and cardiac electrophysiology show accurate reconstruction of locations and geometries of inhomogeneities.

Conclusion: The proposed iterative direct sampling method (IDSM) is an effective approach for solving elliptic inverse problems with partial Cauchy data, achieving robust, flexible, and high-resolution reconstruction of inhomogeneities across various application domains.

Abstract: We develop a novel iterative direct sampling method (IDSM) for solving linear or nonlinear elliptic inverse problems with partial Cauchy data. It integrates three innovations: a data completion scheme to reconstruct missing boundary information, a heterogeneously regularized Dirichlet-to-Neumann map to enhance the near-orthogonality of probing functions, and a stabilization-correction strategy to ensure the numerical stability. The resulting method is remarkably robust with respect to measurement noise, is flexible with the measurement configuration, enjoys provable stability guarantee, and achieves enhanced resolution for recovering inhomogeneities. Numerical experiments in electrical impedance tomography, diffuse optical tomography, and cardiac electrophysiology show its effectiveness in accurately reconstructing the locations and geometries of inhomogeneities.

</details>


### [14] [An Iterative Direct Sampling Method for Reconstructing Moving Inhomogeneities in Parabolic Problems](https://arxiv.org/abs/2511.08197)
*Bangti Jin,Fengru Wang,Jun Zou*

Main category: math.NA

TL;DR: A novel iterative direct sampling method for imaging moving inhomogeneities in parabolic problems using boundary measurements, capable of identifying locations and shapes with limited data and showing numerical stability against noise.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for imaging moving inhomogeneities in parabolic problems when only limited boundary measurement data is available, addressing challenges with noisy data and extended time horizons.

Method: An iterative direct sampling method formulated in an abstract framework that works with linear and nonlinear parabolic problems, including various types of inhomogeneities (linear, nonlinear, mixed-type).

Result: The method effectively identifies locations and shapes of moving inhomogeneities even with only one pair of lateral Cauchy data, demonstrating remarkable numerical stability for noisy data over extended time periods.

Conclusion: Numerical experiments across diverse scenarios confirm the method's effectiveness and robustness against data noise, making it suitable for practical applications with limited measurement data.

Abstract: We propose in this work a novel iterative direct sampling method for imaging moving inhomogeneities in parabolic problems using boundary measurements. It can efficiently identify the locations and shapes of moving inhomogeneities when very limited data are available, even with only one pair of lateral Cauchy data, and enjoys remarkable numerical stability for noisy data and over an extended time horizon. The method is formulated in an abstract framework, and is applicable to linear and nonlinear parabolic problems, including linear, nonlinear, and mixed-type inhomogeneities. Numerical experiments across diverse scenarios show its effectiveness and robustness against the data noise.

</details>


### [15] [Pointwise A Posteriori Error Estimators for Multiple and Clustered Eigenvalue Computations](https://arxiv.org/abs/2511.08259)
*Zhenglei Li,Qigang Liang,Xuejun Xu*

Main category: math.NA

TL;DR: Proposes a pointwise a posteriori error estimator for finite element approximations of eigenfunctions with multiple/clustered eigenvalues, proven reliable and efficient with mesh-independent constants.


<details>
  <summary>Details</summary>
Motivation: Need accurate error estimation for eigenfunction approximations, especially for multiple and clustered eigenvalues where traditional methods may fail.

Method: Develops pointwise a posteriori error estimator using regularized derivative Green's functions and weighted Sobolev stability of L²-projection.

Result: Proves estimator is reliable and efficient (up to logarithmic factors), with constants independent of eigenvalue gaps and mesh parameters. Edge residuals dominate error for linear elements.

Conclusion: The proposed estimator provides robust error control for eigenfunction approximations, validated by numerical experiments.

Abstract: In this work, we propose an a pointwise a posteriori error estimator for conforming finite element approximations of eigenfunctions corresponding to multiple and clustered eigenvalues of elliptic operators. It is proven that the pointwise a posteriori error estimator is reliable and efficient, up to some logarithmic factors of the mesh size. The constants involved in the reliability and efficiency are independent of the gaps among the targeted eigenvalues, the mesh size and the number of mesh level. Specially, we obtain a by-product that edge residuals dominate the a posteriori error in the sense of $L^{\infty}$-norm when the linear element is used. With the aid of the weighted Sobolev stability of the $L^2$-projection, we also propose a new method to prove the reliability of the a posteriori error estimator for higher order finite elements. A key ingredient in the a posteriori error analysis is some new estimates for regularized derivative Green's functions. Some numerical experiments verify our theoretical results.

</details>


### [16] [Numerical Approaches for Identifying the Time-Dependent Potential Coefficient in the Diffusion Equation](https://arxiv.org/abs/2511.08302)
*Arshyn Altybay,Michael Ruzhansky*

Main category: math.NA

TL;DR: This paper solves the inverse problem of identifying a time-dependent potential coefficient in a 1D diffusion equation using nonlocal integral measurements, and compares three numerical methods for computation.


<details>
  <summary>Details</summary>
Motivation: To address the challenging inverse problem of recovering time-dependent coefficients in diffusion equations from spatially averaged measurements, which has applications in parameter identification from experimental data.

Method: Three numerical approaches: integration-based scheme, Newton-Raphson iterative solver, and physics-informed neural network (PINN), applied after establishing theoretical well-posedness and uniqueness.

Result: All three methods demonstrate accuracy, robustness, and efficiency in numerical experiments with both exact and noisy data, successfully recovering the unknown time-dependent coefficient.

Conclusion: The proposed methods provide effective computational tools for solving inverse coefficient problems in diffusion equations, with PINN offering a promising machine learning alternative to traditional numerical schemes.

Abstract: We address the inverse problem of identifying a time-dependent potential coefficient in a one-dimensional diffusion equation subject to Dirichlet boundary conditions and a nonlocal integral overdetermination constraint reflecting spatially averaged measurements. After establishing well-posedness for the forward problem and deriving an a priori estimate that ensures uniqueness and continuous dependence on the data, we prove existence and uniqueness for the inverse problem. To compute numerically the unknown coefficient, we propose and compare three numerical methods: an integration-based scheme, a Newton-Raphson iterative solver, and a physics-informed neural network (PINN). Numerical experiments on both exact and noisy data demonstrate the accuracy, robustness, and efficiency of each approach.

</details>


### [17] [Fast integral methods for the Neumann Green's function: applications to capture and signaling problems in two dimensions](https://arxiv.org/abs/2511.08458)
*Sanchita Chakraborty,Jeremy Hoskins,Alan E. Lindsay*

Main category: math.NA

TL;DR: High-order numerical method for computing Neumann Green's functions in 2D, handling both interior and exterior domains with sources in bulk or on surface, with applications to Brownian particle capture optimization.


<details>
  <summary>Details</summary>
Motivation: Need for accurate computation of Neumann Green's functions for general closed planar curves to solve problems in Brownian particle capture and optimize trap configurations.

Method: Decomposition of singular and regular components, fast integral method for regular part, high-order discretization for general domains, exact prescription of integral constraint for interior functions.

Result: Accurate resolution of four distinct Green's functions (interior/exterior × bulk/surface sources), validated against closed-form solutions for simple geometries like disks and ellipses.

Conclusion: Method provides efficient computation of Neumann Green's functions and enables solving open problems in Brownian particle capture optimization, particularly for maximizing capture rates through optimal trap configurations.

Abstract: We present a high order numerical method for the solution of the Neumann Green's function in two dimensions. For a general closed planar curve, our computational method resolves both the interior and exterior Green's functions with the source placed either in the bulk or on the surface -- yielding four distinct functions. Our method exactly represents the singular nature of the Green's function by decomposing the singular and regular components. In the case of the interior function, we exactly prescribe an integral constraint which is necessary to obtain a unique solution given the arbitrary constant solution associated with Neumann boundary conditions. Our implementation is based on a fast integral method for the regular part of the Green's function which allows for a rapid and high order discretization for general domains. We demonstrate the accuracy of our method for simple geometries such as disks and ellipses where closed form solutions are available. To exhibit the usefulness of these new routines, we demonstrate several applications to open problems in the capture of Brownian particles, specifically, how the small traps or boundary windows should be configured to maximize the capture rate of Brownian particles.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [18] [A quasilinear wave with a supersonic shock in a weak solution interrupting the classical development](https://arxiv.org/abs/2511.07594)
*Leonardo Abbrescia,Pieter Blue,Jan Sbierski,Jared Speck*

Main category: math.AP

TL;DR: Analysis of shock-forming solutions to a 1+1D quasilinear wave equation, showing both classical blow-up solutions and global weak entropy solutions with shocks.


<details>
  <summary>Details</summary>
Motivation: To study shock phenomena in quasilinear hyperbolic PDEs using a simple model that allows elementary analysis while capturing geometric and analytic significance of shocks.

Method: Using a convenient choice of C∞ initial data to solve a model quasilinear wave equation, analyzing both classical solutions that blow up and weak entropy solutions with shocks.

Result: First example of provably unique maximal globally hyperbolic development for shock-forming solutions; classical solution blows up with bounded first derivatives but unbounded second derivatives; global weak entropy solution exists with shock curve separating smooth regions.

Conclusion: Classical and weak solutions agree before shock formation but differ in regions after the first singularity, providing insights into shock behavior in quasilinear hyperbolic systems.

Abstract: We study the Cauchy problem for classical and weak shock-forming solutions to a model quasilinear wave equation in $1+1$ dimensions arising from a convenient choice of $C^{\infty}$ initial data, which allows us to solve the equation using elementary arguments. The simplicity of our model allows us to succinctly illustrate various phenomena of geometric and analytic significance tied to shocks, which we view as a prototype for phenomena that can occur in more general quasilinear hyperbolic PDE solutions.
  Our Cauchy problem admits a classical solution that blows up in finite time. The classical solution is defined in a largest possible globally hyperbolic region called a maximal globally hyperbolic development (MGHD), and its properties are tied to the intrinsic Lorentzian geometry of the equation and solution. The boundary of the MGHD contains an initial singularity, a singular boundary along which the solution's second derivatives blow up (the solution and its first derivatives remain bounded), and a Cauchy horizon. Our main results provide the first example of a provably unique MGHD for a shock-forming quasilinear wave equation solution; it is provably unique because its boundary has a favorable global structure that we precisely describe.
  We also prove that for the same $C^{\infty}$ initial data, the Cauchy problem admits a second kind of solution: a unique global weak entropy solution that has a shock curve separating two smooth regions. Of particular interest is our proof that the classical and weak solutions agree before the shock but differ in a region to the future of the first singularity where both solutions are defined.

</details>


### [19] [On the proximal dynamics between integrable and non-integrable members of a generalized Korteweg-de Vries family of equations](https://arxiv.org/abs/2511.07609)
*Nikos I. Karachalios,Dionyssios Mantzavinos,Jeffrey Oregero*

Main category: math.AP

TL;DR: Estimates distance between integrable KdV and non-integrable gKdV solutions in Sobolev spaces, showing close dynamics for various initial amplitudes and nonlinearity parameters.


<details>
  <summary>Details</summary>
Motivation: To understand how integrable dynamics persist in non-integrable systems and quantify the deviation between KdV and generalized KdV equations over time.

Method: Proves size estimates for local gKdV solutions linear in initial data norm, establishes distance estimates for arbitrary initial data and nonlinearity parameters, and validates with numerical simulations using one-soliton and two-soliton initial data.

Result: Distance estimates show close dynamics for initial amplitudes near unity, explicit deviation rates for larger amplitudes, and numerical simulations confirm theoretical predictions. Rescaling KdV with rotation effects can drastically reduce deviation for large solitonic data.

Conclusion: Integrable dynamics from rescaled KdV can persist remarkably long in gKdV equations, providing insights into persistence of integrable behavior in non-integrable systems.

Abstract: The distance between the solutions to the integrable Korteweg-de Vries (KdV) equation and a broad class of non-integrable generalized KdV (gKdV) equations is estimated in appropriate Sobolev spaces. Special cases of the latter family of equations include the standard gKdV equation with a power nonlinearity, as well as weakly nonlinear perturbations of the KdV equation. The distance estimates are established for initial data and nonlinearity parameters of arbitrary size. A crucial step for their derivation is the proof of a size estimate for local solutions to the gKdV family of equations, which is linear in the norm of the initial data. As a result, the distance estimates themselves predict that the dynamics between the gKdV and KdV equations remains close over long times for initial amplitudes even close to unity, while for larger amplitudes they predict an explicit rate of deviation of the dynamics between the compared equations. These theoretical results are illustrated via numerical simulations in the case of one-soliton and two-soliton initial data, which are in an excellent agreement with the theoretical predictions. Importantly, in the case of a power nonlinearity and large solitonic initial data, the deviation between the integrable and non-integrable dynamics can be drastically reduced upon incorporating suitable rotation effects by means of a rescaled KdV equation. As a result, the integrable dynamics stemming from the rescaled KdV equation may persist in the gKdV family of equations over remarkably long timescales.

</details>


### [20] [Hamilton-Jacobi-Bellman equations on graphs](https://arxiv.org/abs/2511.07653)
*Nicolò Forcillo,Jun Kitagawa,Russell W. Schwab*

Main category: math.AP

TL;DR: Study of Hamilton-Jacobi-Bellman equations on graphs as analogs of continuum PDEs, with existence/uniqueness results and examples showing common monotone structure.


<details>
  <summary>Details</summary>
Motivation: To complement earlier Hamilton-Jacobi work, import ideas from nonlocal elliptic equations, and demonstrate common structural properties across this equation family.

Method: Analyze Hamilton-Jacobi-Bellman equations on graphs, establish conditions for existence/uniqueness, examine examples, and identify monotone structure with Bellman-Isaacs representations.

Result: Established existence and uniqueness conditions for solutions, identified common monotone structure across examples, and showed most operators have Bellman-Isaacs representation as min-max of linear operators with graph Laplacian structure.

Conclusion: Hamilton-Jacobi-Bellman equations on graphs share a unified structure as monotone functions of differences plus lower-order terms, with most admitting Bellman-Isaacs representations, connecting discrete and continuum equation theories.

Abstract: Here, we study Hamilton-Jacobi-Bellman equations on graphs. These are meant to be the analog of any of the following types of equations in the continuum setting of partial differential and nonlocal integro-differential equations: Hamilton-Jacobi (typically first order and local), Hamilton-Jacobi-Bellmann-Isaacs (first, second, or fractional order), and elliptic integro-differential equations (nonlocal equations). We give conditions for the existence and uniqueness of solutions of these equations, and work through a long list of examples in which these assumptions are satisfied. This work is meant to accomplish three goals: complement and unite earlier assumptions and arguments focused more on the Hamilton-Jacobi type structure; import ideas from nonlocal elliptic integro-differential equations; and argue that nearly all of the operators in this family enjoy a common structure of being a monotone function of the differences of the unknown, plus ``lower order'' terms. This last goal is tied to the fact that most of the examples in this family can be proven to have a Bellman-Isaacs representation as a min-max of linear operators with a graph Laplacian structure.

</details>


### [21] [Asymptotic analysis of transmission problems with parameter-dependent Robin conditions](https://arxiv.org/abs/2511.07704)
*Takeshi Fukao*

Main category: math.AP

TL;DR: Asymptotic analysis of Neumann-Robin transmission problem with parameter α, studying limits α→0 (decoupling) and α→∞ (unification), with applications to biological gap junctions and connection to Mosco convergence.


<details>
  <summary>Details</summary>
Motivation: Study transmission problems modeling biological gap junctions between cells, where parameter α represents permeability. Understanding asymptotic behavior helps model scenarios from closed junctions (α→0) to unified structures (α→∞).

Method: Mathematical analysis of Neumann-Robin transmission problem with parameter α, performing asymptotic analysis for α→0 and α→∞ regimes, establishing convergence rates, and connecting to Mosco convergence of convex functionals.

Result: Obtained convergence rates for both α→0 (decoupling) and α→∞ (unification) limits. Showed solution can be extended beyond finite-time blow-up of α under regularity assumptions, remaining in unified structure regime.

Conclusion: The analysis provides mathematical framework for understanding biological gap junction dynamics, connecting asymptotic parameter behavior to physical interpretations of cell coupling and establishing robustness of unified structure regime.

Abstract: We study a transmission problem of {N}eumann--{R}obin type involving a parameter $α$ and perform an asymptotic analysis with respect to $α$. The limits $α\to 0$ and $α\to +\infty$ correspond respectively to complete decoupling and full unification of the problem, and we obtain rates of convergence for both regimes. Biologically, the model describes two cells connected by a gap junction with permeability $α$: the case $α\to 0$ corresponds to a situation where the gap junction is closed, leaving only tight junctions between the cells so that no substance exchange occurs, while $α\to +\infty$ corresponds to a situation that can be interpreted as the cells forming a single structure. We also clarify the relationship between the asymptotic analysis with respect to the parameter $α$ and the asymptotics of the system in connection with the convergence of convex functionals known as {M}osco convergence. Finally, we consider time-dependent permeability and analyze the case where $α$ blows up in finite time. Under suitable regularity assumptions, we show that the solution can be extended beyond the blow-up time, remaining in the single structure regime.

</details>


### [22] [Doubling Argument of the Hessian Estimate for the Special Lagrangian Equation on General Phases with Constraints](https://arxiv.org/abs/2511.07757)
*Cheuk Yan Fung*

Main category: math.AP

TL;DR: A doubling argument is developed to obtain Hessian estimates for the special Lagrangian equation under general phase constraints, without relying on the Michael-Simon mean value inequality.


<details>
  <summary>Details</summary>
Motivation: To establish Hessian estimates for the special Lagrangian equation under general phase constraints using a method that avoids dependence on the Michael-Simon mean value inequality.

Method: Developed a doubling argument approach and established Alexandrov-type theorems as an intermediate step.

Result: Successfully obtained Hessian estimates for the special Lagrangian equation under general phase constraints without using the Michael-Simon mean value inequality.

Conclusion: The doubling argument provides an alternative approach for Hessian estimates in special Lagrangian equations, and the Alexandrov-type theorems developed may have independent mathematical interest.

Abstract: In this paper, we establish a doubling argument to obtain Hessian estimates for the special Lagrangian equation under general phase constraints. In particular, our approach does not rely on the Michael-Simon mean value inequality. As an intermediate step, we also establish Alexandrov-type theorems, which may be of independent interest.

</details>


### [23] [Asymptotic stability of planar viscous shock wave to three-dimensional relaxed compressible Navier-Stokes equations](https://arxiv.org/abs/2511.07799)
*Renyong Guan,Yuxi Hu*

Main category: math.AP

TL;DR: This paper proves nonlinear stability of planar viscous shock waves in 3D relaxed compressible Navier-Stokes equations using a modified Maxwell model, with convergence to classical Navier-Stokes as relaxation parameter approaches zero.


<details>
  <summary>Details</summary>
Motivation: To establish nonlinear time-asymptotic stability of shifted planar viscous shock waves for three-dimensional relaxed compressible Navier-Stokes equations, which use a modified Maxwell-type model instead of classical Newtonian constitutive relation.

Method: Uses relative entropy method with a-contraction framework with shifts to derive energy estimates for weighted relative entropy of perturbations, followed by high-order and dissipation estimates via direct energy arguments, combined with local existence result.

Result: Proves that planar viscous shock waves are nonlinearly stable under small shock strength and initial perturbations, and shows global asymptotic stability of shifted planar viscous shock waves.

Conclusion: Establishes global asymptotic stability of shifted planar viscous shock waves and demonstrates that solutions of relaxed system converge to classical Navier-Stokes system as relaxation parameter tends to zero.

Abstract: This paper establishes the nonlinear time-asymptotic stability of shifted planar viscous shock waves for the three-dimensional relaxed compressible Navier-Stokes equations, in which a modified Maxwell-type model replaces the classical Newtonian constitutive relation. Under the assumptions of sufficiently small shock strength and initial perturbations, we prove that planar viscous shock waves are nonlinearly stable. The main steps of our analysis are as follows. First, using the relative entropy method together with the framework of $a$-contraction with shifts, we derive energy estimates for the weighted relative entropy of perturbations. We then successively obtain high-order and dissipation estimates via direct energy arguments, which provide the required a priori bounds. Combining these estimates with a local existence result, we establish the global asymptotic stability of the shifted planar viscous shock wave. Finally, we show that as the relaxation parameter tends to zero, solutions of the relaxed system converge globally in time to those of the classical Navier-Stokes system.

</details>


### [24] [Global branching for semilinear fractional Laplace with sublinear nonlinearity](https://arxiv.org/abs/2511.08037)
*Jefferson Abrantes,Rohit Kumar,Abhishek Sarkar*

Main category: math.AP

TL;DR: Study of positive solutions for sublinear fractional elliptic problems, establishing existence, nonexistence, and multiplicity results using a priori estimates and critical parameter analysis.


<details>
  <summary>Details</summary>
Motivation: Extend previous results on elliptic problems to the nonlocal fractional setting (s ∈ (0,1)), addressing technical challenges like the lack of standard comparison principles in ℝᴺ.

Method: Use a priori estimates, sub/supersolution method, relation between local minimizers in different function spaces, and Classical Linking Theorem to prove solution multiplicity.

Result: Identified critical threshold for parameter λ determining solution existence/nonexistence. Proved existence of at least two distinct positive weak solutions to the fractional elliptic problem.

Conclusion: Successfully extended classical elliptic problem results to fractional setting, overcoming technical obstacles and establishing robust existence and multiplicity theory for sublinear fractional elliptic equations.

Abstract: This article investigates the existence, nonexistence, and multiplicity of positive solutions to the sublinear fractional elliptic problem $(P_λ^s)$. We begin by establishing several a priori estimates that provide regularity results and describe the qualitative behavior of solutions. A critical threshold level for the parameter $λ$ is identified, which plays a crucial role in determining the existence or nonexistence of solutions. The sub and supersolution method is employed to obtain a weak solution. Furthermore, we establish a relation between the local minimizers of $\mathcal{D}^{s,2}(\mathbb{R}^N)$ versus $C(\mathbb{R}^N; 1+|x|^{N-2s})$. Combining these results with the Classical Linking Theorem, we demonstrate the existence of at least two distinct positive weak solutions to $(P_λ^s)$. This work extends the results of Yang, Abrantes, Ubilla, and Zhou (J. Differential Equations, 416:159-189, 2025) to the nonlocal setting, i.e., when $s \in (0,1)$. Several technical challenges arise in this framework, such as the lack of a standard comparison principle in $\mathbb{R}^N$ in the fractional setting.

</details>


### [25] [Critical curve for weakly coupled system of semilinear Euler-Poisson-Darboux-Tricomi equations](https://arxiv.org/abs/2511.08084)
*Yuequn Li,Fei Guo*

Main category: math.AP

TL;DR: This paper studies a weakly coupled system of semilinear Euler-Poisson-Darboux-Tricomi equations with power-type nonlinearities, determining the critical curve that separates global existence from blow-up behavior.


<details>
  <summary>Details</summary>
Motivation: To understand the threshold behavior between global existence and finite-time blow-up for weakly coupled semilinear Euler-Poisson-Darboux-Tricomi equations with power-type nonlinear terms.

Method: Constructed new test functions to address blow-up when Γ_m ≥ 0, and used (L^1∩L^2)-L^2 estimates from previous work to prove global existence when Γ_m < 0 with small initial data.

Result: Found that the critical curve Γ_m(n,p,q,β_1,β_2)=0 delineates the threshold between global existence and blow-up, with blow-up occurring when Γ_m ≥ 0 and global existence when Γ_m < 0 (with damping dominating mass).

Conclusion: The paper establishes a complete characterization of the critical behavior for the EPDTS system, providing both blow-up and global existence results based on the sign of Γ_m.

Abstract: This paper investigates a weakly coupled system of semilinear Euler-Poisson-Darboux-Tricomi equations (EPDTS) with power-type nonlinear terms. More precisely, in the case where the damping terms dominate over the mass terms, the critical curve in the $p-q$ plane that delineates the threshold between global existence and blow-up for the EPDTS is given by \begin{equation*} Γ_m(n,p,q,β_1,β_2)=0, \end{equation*} where $Γ_m$ is defined by (\ref{gammam}). Through the construction of new test functions, the blow-up problem is addressed when $Γ_m(n,p,q,β_1,β_2)\geq0$. Based on the $(L^1\cap L^2)-L^2$ estimates of the solution to the corresponding linear equation established in our previous work \cite{LiGuo2025}, we derive the global existence of solutions with small initial data when $Γ_m(n,p,q,β_1,β_2)<0$, provided that the damping terms prevail over the mass terms.

</details>


### [26] [Positive solutions to semipositone problems on Heisenberg group](https://arxiv.org/abs/2511.08104)
*Vikram Naik,Rohit Kumar*

Main category: math.AP

TL;DR: Existence and positivity of weak solutions for semipositone problems on the Heisenberg group with sign-changing nonlinearities.


<details>
  <summary>Details</summary>
Motivation: To study semipositone problems on the Heisenberg group, where nonlinearities become negative in parts of the domain, making positivity analysis challenging due to inability to directly apply maximum principles.

Method: Use mountain pass technique to prove existence of weak solutions, establish regularity properties, prove L∞-norm convergence of solution sequence as parameter a→0, and apply Riesz-representation formula under additional hypotheses.

Result: Proved existence of weak solutions via mountain pass, established regularity, showed convergence to positive function as a→0, and obtained positivity under extra conditions on f₀ and g.

Conclusion: First work addressing semipositone problems in Heisenberg group setting, successfully establishing positive weak solutions despite sign-changing nonlinearities through combined analytical techniques.

Abstract: This article focuses on establishing a positive weak solution to a class of semipositone problems over the Heisenberg group $\mathbb{H}^N$. In particular, we are interested in the positive weak solution to the following problem: \begin{equation}\label{p1}
  -Δ_{\mathbb{H}}u= g(ξ)f_a(u) \text{ in } \mathbb{H}^N \tag{$P_a$}, \end{equation} where $a>0$ is a real parameter and $g$ is a positive function. The function $f_a: \mathbb{R} \rightarrow \mathbb{R}$ is continuous and of semipositone type which means it becomes negative on some parts of the domain. Due to this sign-changing nonlinearity, we can not directly apply the maximum principle to obtain the positivity of the solution to \eqref{p1}. For that purpose, we need some regularity results for our solutions. In this direction, we first prove the existence of weak solutions to \eqref{p1} via the mountain pass technique. Further, we establish some regularity properties of our solutions and using that we prove the $L^\infty$-norm convergence of the sequence of solutions $\{u_a\}$ to a positive function $u$ as $a \rightarrow 0$, which yields $u_a \geq 0$ for $a$ sufficiently small. Finally, we use the Riesz-representation formula to obtain the positivity of solutions under some extra hypothesis on $f_0$ and $g$. To the best of our knowledge, there is no article dealing with semipositone problems in Heisenberg group set up.

</details>


### [27] [Classification of unstable travelling wave solutions to KdV type equations](https://arxiv.org/abs/2511.08211)
*Kaito Kokubu*

Main category: math.AP

TL;DR: Classification of travelling wave solutions in KdV-type equations with double power nonlinearities and fractional dispersion, focusing on unstable solutions based on nonlinearity signatures and index parities.


<details>
  <summary>Details</summary>
Motivation: To understand how travelling wave solutions depend on the signatures of nonlinearities and parity combinations of indices in KdV-type equations with double power nonlinearities.

Method: Analysis of travelling wave solutions to Korteweg-de Vries type equations with double power nonlinearities (like Gardner equation) and fractional dispersion, examining signatures and parity combinations of indices.

Result: The existence of ground state solutions depends on nonlinearity signatures and parity combinations of indices, with classification focusing on unstable travelling wave solutions.

Conclusion: The study provides a classification framework for travelling wave solution phenomena based on nonlinearity signatures and index parities, particularly for unstable solutions.

Abstract: We study travelling wave solutions to Korteweg--de Vries type equations which have double power nonlinearities with integer indices, such as the Gardner equation, and fractional dispersion. Whether these equations have ground state solutions depends on signatures of nonlinearities and parity combinations of the two indices. The aim of this study is to give the classification of phenomena of travelling wave solutions from the perspective of the signatures and parities of the indices. In this paper, we focus on unstable travelling wave solutions.

</details>


### [28] [A unified clasification of Liouville properties and nontrivial solution for fractional elliptic equations with general Hénon-type superquadratic and gradient growth](https://arxiv.org/abs/2511.08286)
*Hoang-Hung Vo*

Main category: math.AP

TL;DR: This paper studies nonlinear nonlocal elliptic equations with integro-differential operators, establishing critical thresholds for Liouville-type results, existence, uniqueness, and symmetry of solutions.


<details>
  <summary>Details</summary>
Motivation: To develop a unified analytical framework for understanding the behavior of solutions to nonlinear nonlocal elliptic equations, particularly identifying the critical balance between diffusion and nonlinear terms that determines solution properties.

Method: Developed new nonlocal analytical techniques including quantitative cutoff estimates for integro-differential kernels, fractional Bernstein-type transforms for gradient control, and moving plane/sliding methods in integral form for symmetry and uniqueness analysis.

Result: Identified critical threshold γ+p=2s separating three regimes: supercritical (γ+p>2s) where all subcritical solutions are constant; critical (γ+p=2s) where all bounded positive solutions are constant; subcritical (γ+p<2s) where unique positive radially symmetric monotone entire solutions with algebraic decay exist.

Conclusion: The paper provides a comprehensive framework for understanding nonlocal elliptic equations, establishing precise conditions for Liouville properties and constructing explicit solutions, unifying and extending previous results in the field.

Abstract: We investigate Liouville-type results, existence, uniqueness and symmetry to the solution of nonlinear nonlocal elliptic equations of the form \[ Lu = |x|^γ\,H(u)\,G(\nabla u), \qquad x\in\R^n, \] where $L$ is a symmetric, translation-invariant, uniformly elliptic integro--differential operator of order $2s\in(0,2)$, and $H,G$ satisfy general structural and growth conditions. A unified analytical framework is developed to identify the precise critical balance $γ+p=2s$, which separates the supercritical, critical, and subcritical situations. In the supercritical case $γ+p>2s$, the diffusion dominates the nonlinear term and every globally defined solution with subcritical growth must be constant; in the critical case $γ+p=2s$, all bounded positive solutions are constant, showing that the nonlocal diffusion prevents the formation of nontrivial equilibria; in the subcritical case $γ+p<2s$, we are able construct a unique, positive, radially symmetric, and monotone entire solution with explicit algebraic decay \[ u(x)\sim (1+|x|^2)^{-β}, \qquad β=\frac{2s+γ-p}{1-p}>0. \] The proofs rely on new nonlocal analytical techniques, including quantitative cutoff estimates for general integro--differential kernels, a fractional Bernstein-type transform providing pointwise gradient control, and moving plane and sliding methods formulated in integral form to establish symmetry and uniqueness. The current investigation provides an equivalent and unifying contribution to Liouville properties and related existence results, comparable to the recent deep studies of Chen--Dai--Qin~\cite{Chen2023} and Biswas--Quaas--Topp~\cite{Biswas2025} on this direction.

</details>


### [29] [On the asymptotic properties of solutions to one-phase free boundary problems](https://arxiv.org/abs/2511.08393)
*Max Engelstein,Daniel Restrepo,Zihui Zhao*

Main category: math.AP

TL;DR: Study of solutions to the one-phase Bernoulli problem, proving uniqueness of blowups under symmetry conditions and rigidity at infinity under additional constraints on linearized operators.


<details>
  <summary>Details</summary>
Motivation: To understand the structure of solutions modeled by one-homogeneous solutions with isolated singularities, particularly for non-minimizing solutions where such results were previously unavailable.

Method: Prove uniqueness of blowups under natural symmetry conditions (Allard-Almgren style) and rigidity at infinity (Simon-Solomon style) with additional constraints on linearized operators around one-homogeneous solutions.

Result: First uniqueness of blow-up/blow-down results at singular points for non-minimizing solutions to the one-phase problem.

Conclusion: The paper establishes foundational uniqueness and rigidity results for non-minimizing solutions in the one-phase Bernoulli problem, filling a gap in the existing literature.

Abstract: In this article we study the structure of solutions to the one-phase Bernoulli problem that are modeled either infinitesimally or at infinity by one-homogeneous solutions with an isolated singularity. In particular, we prove a uniqueness of blowups result under a natural symmetry condition on the one-homogeneous solution (à la Allard--Almgren) and we prove a rigidity result at infinity (à la Simon--Solomon) under additional constraints on the linearized operator around the one-homogeneous solution (which are satisfied by the only known examples of minimizing one-homogeneous solutions). We believe these are the first uniqueness of blow-up/blow-down results at singular points for non-minimizing solutions to the one-phase problem.

</details>


### [30] [A note on the a.e. second-order differentiability of rank-one convex functions](https://arxiv.org/abs/2511.08397)
*Jonas Hirsch*

Main category: math.AP

TL;DR: Extends Alexandrov's theorem from convex functions to rank-one convex functions using viscosity techniques from fully nonlinear elliptic equations.


<details>
  <summary>Details</summary>
Motivation: To generalize the classical Alexandrov theorem about twice differentiability of convex functions to the broader class of rank-one convex functions.

Method: Uses viscosity techniques developed for fully nonlinear elliptic equations, providing a novel approach that reduces the original Alexandrov theorem to the almost everywhere differentiability of one-dimensional monotone functions.

Result: Successfully proves that rank-one convex functions are twice differentiable almost everywhere, extending the classical result.

Conclusion: The extension of Alexandrov's theorem to rank-one convex functions is achieved through innovative viscosity methods, offering a simplified perspective on the original theorem.

Abstract: In the Euclidean setting, the well-known Alexandrov theorem states that convex functions are twice differentiable almost everywhere. In this note, we extend this theorem to rank-one convex functions. Our approach is novel in that it draws more from viscosity techniques developed in the context of fully nonlinear elliptic equations. As a byproduct, the original Alexandrov theorem can essentially be reduced to the a.e. differentiability of one-dimensional monotone functions, as presented in the appendix.

</details>


### [31] [Total variation flow of curves in Riemannian manifolds](https://arxiv.org/abs/2511.08459)
*Lorenzo Giacomelli,Michał Łasica,Salvador Moll*

Main category: math.AP

TL;DR: Global existence and finite-time convergence of strong solutions to the L²-gradient flow of total variation for maps from an interval into Riemannian submanifolds.


<details>
  <summary>Details</summary>
Motivation: To study the gradient flow of total variation functional for maps into Riemannian manifolds, which has applications in image processing and geometric analysis.

Method: Define strong solutions for the L²-gradient flow system, prove global existence for BV initial data, establish variational equality, and analyze convergence properties.

Result: Global existence of strong solutions for BV initial data, uniqueness under non-positive sectional curvature, and finite-time convergence to constant maps.

Conclusion: The total variation gradient flow exhibits well-posedness and finite-time convergence behavior for maps into Riemannian submanifolds, with curvature conditions affecting uniqueness.

Abstract: We consider the functional of total variation of maps from an interval into a Riemannian submanifold of $\mathbb R^N$. We define a notion of strong solution to the system of equations corresponding to the $L^2$-gradient flow of this functional. We prove global existence of strong solutions for initial data of bounded variation. We show that the solutions satisfy a variational equality, and deduce uniqueness in the case of non-positive sectional curvature. We prove convergence of strong solutions to a constant map in finite time.

</details>


### [32] [Inverse Source Problem for a Nonlinear Parabolic Equation via Paralinearization](https://arxiv.org/abs/2511.08460)
*Hu Xirui*

Main category: math.AP

TL;DR: Inverse source problem for semilinear parabolic equations


<details>
  <summary>Details</summary>
Motivation: To study the identification of unknown sources in semilinear parabolic equations from boundary measurements

Method: Mathematical analysis of inverse source problems using semilinear parabolic equation theory

Result: Established theoretical framework for source identification in semilinear parabolic systems

Conclusion: The approach provides a foundation for solving inverse source problems in semilinear parabolic equations

Abstract: We consider an inverse source problem for a semilinear parabolic equation.

</details>


### [33] [Oil displacement by slug injection: a rigorous justification for the Jouguet principle heuristic](https://arxiv.org/abs/2511.08533)
*Sergey Matveenko,Nikita Rastegaev*

Main category: math.AP

TL;DR: Improved semi-analytical method for polymer flooding in enhanced oil recovery using Lagrange coordinates and characteristic methods, with rigorous justification for Jouguet principle to close characteristic gaps.


<details>
  <summary>Details</summary>
Motivation: To address gaps in characteristics near chemical shock fronts in existing polymer flood models and provide rigorous mathematical justification for the Jouguet heuristic approach.

Method: Uses transformation into Lagrange coordinates to split equations, solves chromatographic one-phase problem separately, then applies method of characteristics to hyperbolic conservation law. Develops second splitting technique within Lagrange coordinates.

Result: Successfully analyzed conditions for characteristic gaps, provided rigorous argumentation for Jouguet principle based on Kružkov-type uniqueness theorem, and developed simplified analysis through additional splitting technique.

Conclusion: The improved method provides mathematically rigorous foundation for polymer flood modeling with closed characteristic gaps, enhancing reliability of semi-analytical solutions for enhanced oil recovery applications.

Abstract: In this paper we discuss a one-dimensional model for two-phase Enhanced Oil Recovery (EOR) floods, primarily for the polymer flood. We improve upon the method for the construction of semi-analytical solutions for the oil displacement by a water slug containing dissolved chemicals given in (Pires, Bedrikovetsky and Shapiro, 2006) and later generalized in (Apolinário, de Paula and Pires, 2020), (Apolinário and Pires, 2021). This method utilizes a transformation into the Lagrange coordinates that splits the equations and allows one to solve the chromatographic one-phase problem separately. The solution is then substituted into a scalar hyperbolic conservation law, which is solved using the method of characteristics. However, there is often a gap in the characteristics near the chemical shock front. It was posited to the authors that the Jouguet principle could be used to close that gap. However, no rigorous justification was given for this approach, and as such it remained a heuristic. We analyze the conditions for the appearance of the gap and its properties, and give a proper argumentation for the Jouguet heuristic and its applicability based on the Kružkov-type uniqueness theorem for the conservation law system. Additionally, a second splitting technique within the Lagrange coordinates is developed that simplifies this analysis and the construction of characteristics.
  Keywords: Enhanced oil recovery, Polymer flooding, Slug injection, Conservation laws, Hyperbolic systems of partial differential equations

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [34] [An Improved High-order Adaptive Mesh Refinement Framework for Shock-turbulence Interaction Problems based on cell-centered finite difference schemes](https://arxiv.org/abs/2511.08335)
*Yuqi Wang,Yadong Zeng,Ralf Deiterding,Jinhui Yang,Jianhan Liang*

Main category: physics.comp-ph

TL;DR: A high-order finite-difference AMR framework using staggered grids for shock-turbulence interaction problems, featuring hybrid interpolation to handle discontinuities and smooth regions separately.


<details>
  <summary>Details</summary>
Motivation: To address boundary conservation issues in previous AMR studies and enable robust simulation of complex shock-turbulence interaction problems that are challenging for existing methods.

Method: Staggered-grid arrangement with cell-center storage, high-order nonlinear interpolation for prolongation, newly developed high-order restriction method, and hybrid interpolation strategy combining WENO for smooth regions and conservative interpolation for shocks.

Result: The method accurately resolves complex shock-turbulence interaction problems and significantly mitigates numerical instabilities from non-conservative interpolation.

Conclusion: The proposed framework successfully handles intricate shock-turbulence interactions that previous approaches struggled with, demonstrating robust performance through canonical tests.

Abstract: This work presents a high-order finite-difference adaptive mesh refinement (AMR) framework for robust simulation of shock-turbulence interaction problems. A staggered-grid arrangement, in which solution points are stored at cell centers instead of at the vertices, is presented to address the boundary conservation issues encountered in previous studies. The key ingredient in the AMR framework, i.e., the high-order nonlinear interpolation method applied in the prolongation step together with the determination of fine-grid boundary conditions, are re-derived for staggered grids following the procedures in prior work [1] and are thus used here. Meanwhile, a high-order restriction method is developed in the present study as the coarse and fine grid solutions are non-collocated in this configuration. To avoid non-conservative interpolation at discontinuous cells that could incur instabilities, a hybrid interpolation strategy is proposed in this work for the first time, where the non-conservative WENO interpolation is applied in smooth regions whereas the second-order conservative interpolation is applied at shocks. This significantly mitigates the numerical instabilities introduced by non-conservative interpolation and pointwise replacement. The two interpolation approaches are seamlessly coupled through a troubled-cell detector achieved by a scale-irrelevant Riemann solver in a robust way. The present work is developed on a publicly available block-structured adaptive mesh refinement framework AMReX [2]. The canonical tests demonstrate that the proposed method is capable of accurately resolving a wide range of complex shock-turbulence interaction problems that have been proven intricate for existing approaches

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [35] [Theory of a dynamic plasma flow pressure sensor](https://arxiv.org/abs/2511.07446)
*Evgeny Kolesnikov,Igor Kotelnikov,Vadim Prikhodko*

Main category: physics.plasm-ph

TL;DR: Solved inverse problem to reconstruct plasma jet dynamic pressure from rod displacement measurements, enabling smaller sensor design.


<details>
  <summary>Details</summary>
Motivation: To develop a more compact dynamic pressure sensor for plasma jets by solving the inverse problem of pressure reconstruction from displacement data.

Method: Solved the inverse problem of determining time-dependent dynamic pressure from measured displacement at the opposite end of a solid rod.

Result: Successfully reconstructed plasma jet dynamic pressure time dependence, allowing for sensor size reduction.

Conclusion: The solution enables development of smaller dynamic pressure sensors for plasma jet applications as proposed in previous works.

Abstract: The problem of reconstructing the time dependence of the dynamic pressure of a plasma jet impinging on one end of a solid rod based on the measured displacement of the opposite end has been solved. This solution allows for a reduction in the size of the dynamic pressure sensor proposed and later improved in the works [1, 2].

</details>


### [36] [Response of a magnetically diverted tokamak plasma to a resonant magnetic perturbation](https://arxiv.org/abs/2511.07666)
*R. Fitzpatrick*

Main category: physics.plasm-ph

TL;DR: The safety-factor profile diverges near the separatrix in diverted tokamaks, but finite resistivity limits the number of rational surfaces needed for RMP response calculations to those within Psi_c<1.


<details>
  <summary>Details</summary>
Motivation: To understand how many rational magnetic flux-surfaces need to be considered when calculating plasma response to resonant magnetic perturbations in diverted tokamaks, given the logarithmic divergence of safety-factor profiles near the separatrix.

Method: Analysis of the safety-factor profile behavior near the magnetic separatrix and consideration of finite plasma resistivity effects on rational surface inclusion criteria.

Result: Only rational surfaces in the region 0<Psi<Psi_c need to be included, where Psi_c depends on edge plasma parameters (Psi_c=0.9985 for n=1 RMP, Psi_c=0.9952 for n=4 RMP in typical JET H-mode plasma).

Conclusion: Finite plasma resistivity eliminates the need to include an infinite number of rational surfaces near the separatrix when calculating RMP response, significantly simplifying the computational requirements.

Abstract: The safety-factor profile of a magnetically diverted tokamak plasma diverges logarithmically as the magnetic separatrix (a.k.a. the last closed magnetic flux-surface) is approached. At first sight, this suggests that, when determining the response of such a plasma to a static, externally generated, resonant magnetic perturbation (RMP), it is necessary to include an infinite number of rational magnetic flux-surfaces in the calculation, the majority of which lie very close to the separatrix. In fact, when finite plasma resistivity is taken into account, this turns out not to be the case. Instead, it is only necessary to include rational surfaces that lie in the region 0<Psi<Psi_c, where Psi is the normalized poloidal magnetic flux, and Psi_c<1 can be calculated from the edge plasma parameters. It is estimated that Psi_c= 0.9985 for an n=1 RMP, and Psi_c=0.9952 for an n=4 RMP, in a typical JET H-mode plasma.

</details>


### [37] [On How Avalanches Penetrate the SOL and Broaden Heat Loads](https://arxiv.org/abs/2511.07733)
*Y. Kosuga,R. Matsui,P. H. Diamond*

Main category: physics.plasm-ph

TL;DR: Derives threshold criterion for heat avalanche penetration into SOL based on temperature gradient steepening and shock formation at separatrix.


<details>
  <summary>Details</summary>
Motivation: Recent experiments show correlation between power law core temperature spectra and Dα emission, suggesting heat avalanches penetrate the SOL.

Method: Uses reduced model to derive threshold criterion for avalanche penetration based on temperature gradient steepening and shock formation at separatrix.

Result: Avalanches with RMS temperature gradient > critical gradient at separatrix penetrate SOL, broadening heat load distribution. Critical gradient ~1/τ∥ (parallel heat flow time). Penetration depth exceeds heuristic drift limit when shocks form.

Conclusion: Provides theoretical framework for understanding heat avalanche penetration into SOL, with implications for numerical and physical experiments.

Abstract: Recent experiments reported a correlation between power law core temperature spectra and $D_α$ emission, suggesting that heat avalanches penetrate the SOL. This paper derives a threshold criterion for avalanche penetration using a reduced model. Avalanches with $(\nabla\tilde T)_{rms}>\nabla\tilde T_{crit}$ at the separatrix are predicted to penetrate, and so broaden the SOL and heat load distribution. $\nabla\tilde T_{crit}$ is $\sim 1/τ_\parallel$, where $τ_\parallel$ is the parallel heat flow time through the SOL. Penetration occurs when avalanches are strong enough to steepen sufficiently to shock at the separatrix. A positive correlation is found between the nonlinear drive for steepening and the penetration depth. In particular, penetration depth exceeds that of the heuristic drift limit when shocks form. Implications for numerical and physical experiments are also discussed.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [38] [Benchmarking Simulacra AI's Quantum Accurate Synthetic Data Generation for Chemical Sciences](https://arxiv.org/abs/2511.07433)
*Fabio Falcioni,Elena Orlova,Timothy Heightman,Philip Mantrov,Aleksei Ustimenko*

Main category: physics.chem-ph

TL;DR: Simulacra's LWM pipeline with VMC sampling reduces data generation costs by 15-50x while maintaining energy accuracy, enabling affordable large-scale ab-initio datasets for AI-driven optimization.


<details>
  <summary>Details</summary>
Motivation: To create affordable, large-scale ab-initio datasets for accelerating AI-driven optimization and discovery in pharmaceutical and other industries by reducing computational costs of quantum chemistry calculations.

Method: Benchmarked Simulacra's synthetic data generation pipeline against Microsoft's state-of-the-art pipeline using energy quality, autocorrelation times, and effective sample size metrics. Used Large Wavefunction Models (LWM) pipeline with Variational Monte Carlo (VMC) sampling algorithms and novel RELAX sampling scheme.

Result: Simulacra's pipeline reduces data generation costs by 15-50x compared to Microsoft's pipeline, maintains parity in energy accuracy, and achieves 2-3x improvement over traditional CCSD methods for amino acid-scale systems.

Conclusion: The approach enables creation of affordable, large-scale ab-initio datasets, accelerating AI-driven optimization and discovery in pharmaceutical industry and beyond through significant cost reductions while maintaining accuracy.

Abstract: In this work, we benchmark \simulacra's synthetic data generation pipeline against a state-of-the-art Microsoft pipeline on a dataset of small to large systems. By analyzing the energy quality, autocorrelation times, and effective sample size, our findings show that Simulacra's Large Wavefunction Models (LWM) pipeline, paired with state-of-the-art Variational Monte Carlo (VMC) sampling algorithms, reduces data generation costs by 15-50x, while maintaining parity in energy accuracy, and 2-3x compared to traditional CCSD methods on the scale of amino acids. This enables the creation of affordable, large-scale \textit{ab-initio} datasets, accelerating AI-driven optimization and discovery in the pharmaceutical industry and beyond. Our improvements are based on a novel and proprietary sampling scheme called Replica Exchange with Langevin Adaptive eXploration (RELAX).

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [39] [A control variate method based on polynomial approximation of Brownian path](https://arxiv.org/abs/2511.08021)
*Josselin Garnier,Laurent Mertz*

Main category: math.PR

TL;DR: Novel control variate technique using dual time-step discretizations with piecewise parabolic Brownian motion approximation for efficient Monte Carlo estimation of SDE expectations.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency of Monte Carlo estimation for stochastic differential equations by reducing variance through control variates.

Method: Combines fine-time-step SDE discretization with control variate from coarse-time-step discretization using piecewise parabolic Brownian motion approximation, with independent MC for control variate expectation.

Result: Characterized minimized quadratic error decay as function of computational budget and discretization orders, demonstrated effectiveness through numerical experiments.

Conclusion: Proposed control variate technique successfully enhances Monte Carlo estimation efficiency for SDE expectations through strong coupling between estimators.

Abstract: We present a novel control variate technique for enhancing the efficiency of Monte Carlo (MC) estimation of expectations involving solutions to stochastic differential equations (SDEs). Our method integrates a primary fine-time-step discretization of the SDE with a control variate derived from a secondary coarse-time-step discretization driven by a piecewise parabolic approximation of Brownian motion. This approximation is conditioned on the same fine-scale Brownian increments, enabling strong coupling between the estimators. The expectation of the control variate is computed via an independent MC simulation using the coarse approximation. We characterize the minimized quadratic error decay as a function of the computational budget and the weak and strong orders of the primary and secondary discretization schemes. We demonstrate the method's effectiveness through numerical experiments on representative SDEs.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [40] [A Fast and Accurate Approach for Covariance Matrix Construction](https://arxiv.org/abs/2511.08223)
*Felix Reichel*

Main category: stat.CO

TL;DR: The paper extends the Bariance concept to covariance matrices, showing an algebraic identity between pairwise-difference and matrix product forms that avoids explicit centering and enables faster computation.


<details>
  <summary>Details</summary>
Motivation: To develop a more computationally efficient method for calculating covariance matrices by extending Reichel's Bariance concept from vectors to matrices.

Method: Prove algebraic identity between pairwise-difference covariance form and matrix product formulation using X^⊤X and outer product of column sums, eliminating explicit centering step.

Result: Empirical benchmarks in Python show runtime gains over numpy.cov, with further improvements using faster Gram matrix routines like RXTX.

Conclusion: The proposed covariance formulation provides computational advantages over traditional methods, especially in non-BLAS-optimized environments, with potential for additional speedups using optimized matrix multiplication algorithms.

Abstract: Reichel (2025) defined the Bariance as $\mathrm{Bariance}(x)=\frac{1}{n(n-1)}\sum_{i<j}(x_i-x_j)^2$, which admits an $O(n)$ reformulation using scalar sums. We extend this to the covariance matrix by showing that $\mathrm{Cov}(X)=\frac{1}{n-1}\!\left(X^\top X-\frac{1}{n}\,s\,s^\top\right)$ with $s=X^\top \mathbf{1}_n$ is algebraically identical to the pairwise-difference form yet avoids explicit centering. Computation reduces to a single $p\times p$ outer matrix product and one subtraction. Empirical benchmarks in Python show clear runtime gains over numpy.cov in non-BLAS-tuned settings. Faster Gram routines such as RXTX (Rybin et. al) for $XX^\top$ further reduce total cost.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [41] [Quantum annealing for lattice models with competing long-range interactions](https://arxiv.org/abs/2511.08336)
*Jan Alexander Koziol,Kai Phillip Schmidt*

Main category: quant-ph

TL;DR: Using quantum annealing devices to find ground states of Ising models with long-range interactions via unit-cell optimization, applied to triangular and Kagome lattice problems relevant for quantum simulation and materials science.


<details>
  <summary>Details</summary>
Motivation: To demonstrate practical applications of existing quantum annealing technology for solving complex lattice problems with long-range interactions that are relevant for quantum simulators, artificial spin ice, and frustrated magnetic materials.

Method: Unit-cell-based optimization scheme where finite optimizations on each unit cell are performed using commercial quantum annealing hardware to determine ground states of Ising models with algebraically decaying long-range interactions.

Result: Successfully calculated devil's staircases of magnetization plateaux on triangular lattice, evaluated ground states on Kagome lattice, and studied models with additional nearest-neighbor interactions relevant for frustrated Ising compounds.

Conclusion: The approach provides a useful and realistic application of existing quantum annealing technology that is applicable across many research areas dealing with lattice problems involving resummable long-range interactions.

Abstract: We use superconducting qubit quantum annealing devices to determine the ground state of Ising models with algebraically decaying competing long-range interactions in the thermodynamic limit. This is enabled by a unit-cell-based optimization scheme, in which the finite optimizations on each unit cell are performed using commercial quantum annealing hardware. To demonstrate the capabilities of the approach, we choose three exemplary problems relevant for other quantum simulation platforms and material science: (i) the calculation of devil's staircases of magnetization plateaux of the long-range Ising model in a longitudinal field on the triangular lattice, motivated by atomic and molecular quantum simulators; (ii) the evaluation of the ground state of the same model on the Kagome lattice in the absence of a field, motivated by artificial spin ice metamaterials; (iii) the study of models with additional few-nearest-neighbor interactions relevant for frustrated Ising compounds with potential long-range interactions. The approach discussed in this work provides a useful and realistic application of existing quantum annealing technology, applicable across many research areas in which lattice problems with resummable long-range interactions are relevant.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [42] [Multiscale Dynamics of Roughness-Driven Flow in Soft Interfaces](https://arxiv.org/abs/2511.08457)
*Qian Wang,Suhaib Ardah,Tom Reddyhoff,Daniele Dini*

Main category: cond-mat.soft

TL;DR: A computational framework for simulating soft lubricated contacts that captures the interplay between multiscale surface roughness and fluid-solid interactions across various lubrication regimes.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately modeling the coupled effects of surface roughness and non-linear fluid-solid interactions in soft lubricated contacts, which span multiple spatiotemporal scales.

Method: Developed a modular computational framework using Persson's statistical theory and CG-FFT approach for surface roughness, and introduced the Reduced Stiffness Method (RSM) to model pressure-induced surface responses beyond classical half-space limitations.

Result: The framework successfully captures the full evolution of frictional behavior and was validated against experiments on rough elastomer-glass interfaces, revealing how surface roughness and material compliance drive transitions between solid contact and fluid-mediated sliding.

Conclusion: The approach provides a robust and versatile simulation tool for analyzing soft interfacial systems influenced by fluid-solid interactions, with broad applications in biomechanics, soft robotics, and microfluidic systems.

Abstract: Soft lubricated contacts exhibit complex interfacial behaviours governed by the coupled effects of multiscale surface roughness and non-linear fluid-solid interactions. Accurately capturing this interplay across thin-film flows is challenging due to the strong synergy between contact mechanics and hydrodynamic flow, spanning over various spatiotemporal scales. Here, we develop a rigorous computational framework to simulate the frictional behaviour of soft lubricated interfaces; its modularity and the use of optimal solvers provides solutions for realistic configurations in lubrication regimes ranging from direct solid contact to complete fluid separation. Surface roughness is described via Persson's statistical theory as well as a deterministic Conjugate Gradient with Fast Fourier Transform (CG-FFT) approach, while limitations associated with classical half-space models are addressed by developing the Reduced Stiffness Method (RSM) to rigorously model pressure-induced surface responses. The integrated framework captures the full evolution of frictional behaviour, validated against experiments on rough elastomer-glass interfaces, revealing how surface roughness and material compliance together drive the transition from solid contact to fluid-mediated sliding. The developed approach establishes a robust and versatile simulation tool for analysing a plethora of soft interfacial systems shaped by fluid-solid interactions, with potential applications including but not limited to biomechanics, soft robotics and microfluidic systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [Two Datasets Are Better Than One: Method of Double Moments for 3-D Reconstruction in Cryo-EM](https://arxiv.org/abs/2511.07438)
*Joe Kileel,Oscar Mickelin,Amit Singer,Sheng Xu*

Main category: cs.CV

TL;DR: MoDM framework uses second-order moments from cryo-EM images with different orientation distributions to uniquely determine molecular structures through convex optimization.


<details>
  <summary>Details</summary>
Motivation: To leverage dataset diversity in cryo-EM by using multiple datasets under different experimental conditions to improve reconstruction quality.

Method: Method of double moments (MoDM) that fuses second-order moments from projection images with uniform and unknown non-uniform orientation distributions using convex-relaxation-based algorithm.

Result: The method generically uniquely determines molecular structures (up to global rotation/reflection) and achieves accurate recovery using only second-order statistics.

Conclusion: Collecting and modeling multiple datasets under different experimental conditions can substantially enhance reconstruction quality in computational imaging tasks.

Abstract: Cryo-electron microscopy (cryo-EM) is a powerful imaging technique for reconstructing three-dimensional molecular structures from noisy tomographic projection images of randomly oriented particles. We introduce a new data fusion framework, termed the method of double moments (MoDM), which reconstructs molecular structures from two instances of the second-order moment of projection images obtained under distinct orientation distributions--one uniform, the other non-uniform and unknown. We prove that these moments generically uniquely determine the underlying structure, up to a global rotation and reflection, and we develop a convex-relaxation-based algorithm that achieves accurate recovery using only second-order statistics. Our results demonstrate the advantage of collecting and modeling multiple datasets under different experimental conditions, illustrating that leveraging dataset diversity can substantially enhance reconstruction quality in computational imaging tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Intelligent Optimization of Multi-Parameter Micromixers Using a Scientific Machine Learning Framework](https://arxiv.org/abs/2511.07702)
*Meraj Hassanzadeh,Ehsan Ghaderi,Mohamad Ali Bijarchi,Siamak Kazemzadeh Hannani*

Main category: cs.LG

TL;DR: A Sci-ML framework using DRL and PINNs for instant multidimensional optimization, demonstrated on micromixer design with 32% efficiency improvement.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of traditional simulation-based optimization methods that are slow, single-problem focused, and computationally expensive.

Method: Deep Reinforcement Learning agent interacts with Physics-Informed Neural Network environment to explore parameter relationships and optimize geometric/physical parameters for micromixer efficiency.

Result: Achieved consistent efficiency improvements across Schmidt number spectrum, with maximum 32% improvement at Schmidt number 13.3, outperforming baseline and Genetic Algorithm.

Conclusion: The proposed Sci-ML framework enables rapid, multi-problem optimization with significant performance gains over traditional methods.

Abstract: Multidimensional optimization has consistently been a critical challenge in engineering. However, traditional simulation-based optimization methods have long been plagued by significant limitations: they are typically capable of optimizing only a single problem at a time and require substantial computational time for meshing and numerical simulation. This paper introduces a novel framework leveraging cutting-edge Scientific Machine Learning (Sci-ML) methodologies to overcome these inherent drawbacks of conventional approaches. The proposed method provides instantaneous solutions to a spectrum of complex, multidimensional optimization problems. A micromixer case study is employed to demonstrate this methodology. An agent, operating on a Deep Reinforcement Learning (DRL) architecture, serves as the optimizer to explore the relationships between key problem parameters. This optimizer interacts with an environment constituted by a parametric Physics-Informed Neural Network (PINN), which responds to the agent's actions at a significantly higher speed than traditional numerical methods. The agent's objective, conditioned on the Schmidt number is to discover the optimal geometric and physical parameters that maximize the micromixer's efficiency. After training the agent across a wide range of Schmidt numbers, we analyzed the resulting optimal designs. Across this entire spectrum, the achieved efficiency was consistently greater than the baseline, normalized value. The maximum efficiency occurred at a Schmidt number of 13.3, demonstrating an improvement of approximately 32%. Finally, a comparative analysis with a Genetic Algorithm was conducted under equivalent conditions to underscore the advantages of the proposed method.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [45] [Mechanistic multiphysics modeling reveals how blood pulsation drives CSF flow, pressure, and brain deformation under physiological and injection conditions](https://arxiv.org/abs/2511.07705)
*Zhuogen Li,Keyu Feng,Hector Gomez*

Main category: physics.med-ph

TL;DR: A comprehensive multiphysics computational model of cerebrospinal fluid dynamics in the central nervous system that includes tissue mechanics and can predict CSF flow across different physiological states using only cardiac blood pulsation data.


<details>
  <summary>Details</summary>
Motivation: Existing CSF dynamics models neglect tissue mechanics, focus on partial geometries, or rely on specific measured flow rates, leaving full-CNS CSF flow predictions underexplored. Better models are needed to optimize intrathecal drug delivery.

Method: Proposed a multiphysics computational model with: (1) fully closed CNS geometry, (2) interaction between CSF and poroelastic tissue plus compliant spinal dura mater, (3) predictive capability using only cardiac blood pulsation data.

Result: Model accurately reconstructs CSF pulsation, captures craniocaudal attenuation and phase shift of CSF flow along spinal SAS. Successfully captures ICP elevation during IT injection and subsequent recovery.

Conclusion: The multiphysics model provides a unified, extensible framework for parametric studies of CSF dynamics and optimization of IT injections, serving as foundation for integrating additional physiological mechanisms.

Abstract: Intrathecal (IT) injection is an effective way to deliver drugs to the brain bypassing the blood-brain barrier. To evaluate and optimize IT drug delivery, it is necessary to understand the cerebrospinal fluid (CSF) dynamics in the central nervous system (CNS). In combination with experimental measurements, computational modeling plays an important role in reconstructing CSF flow in the CNS. Existing models have provided valuable insights into the CSF dynamics; however, most neglect the effects of tissue mechanics, focus on partial geometries, or rely on measured CSF flow rates under specific conditions, leaving full-CNS CSF flow field predictions across different physiological states underexplored. Here, we propose a comprehensive multiphysics computational model of the CNS with three key features: (1) it is implemented on a fully closed geometry of CNS; (2) it includes the interaction between CSF and poroelastic tissue as well as the compliant spinal dura mater; (3) it has potential for predictive simulations because it only needs data on cardiac blood pulsation into the brain. Our simulations under physiological conditions demonstrate that our model accurately reconstructs the CSF pulsation and captures both the craniocaudal attenuation and phase shift of CSF flow along the spinal subarachnoid space (SAS). When applied to the simulation of IT drug delivery, our model successfully captures the intracranial pressure (ICP) elevation during injection and subsequent recovery after injections. The proposed multiphysics model provides a unified and extensible framework that allows parametric studies of CSF flow dynamics and optimization of IT injections, serving as a strong foundation for integration of additional physiological mechanisms.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [46] [A ballistic upper bound on the accumulation of bosonic on-site energies](https://arxiv.org/abs/2511.07558)
*Tomotaka Kuwahara,Marius Lemm,Carla Rubiliani*

Main category: math-ph

TL;DR: Improved bound on local repulsive energy growth in Bose-Hubbard systems from t^{2d} to t^d, showing ballistic accumulation of bosonic on-site energies using novel ASTLOs.


<details>
  <summary>Details</summary>
Motivation: To better understand transport properties and improve bounds on energy accumulation in translation-invariant Bose-Hubbard systems, which could lead to stronger bosonic Lieb-Robinson bounds.

Method: Developed novel adiabatic space-time localization observables (ASTLOs) to track growth of local boson-boson correlations, moving beyond previous particle transport approaches.

Result: Proved that ⟨n²_x⟩_t ≲ t^d for translation-invariant initial states, improving previous bound of t^{2d} and showing bosonic on-site energies accumulate at most ballistically.

Conclusion: The improved bound demonstrates ballistic energy accumulation and the ASTLO method shows promise for extending to higher moments, potentially enabling stronger bosonic Lieb-Robinson bounds.

Abstract: In this note, we study transport properties of the dynamics generated by translation-invariant and possibly long-ranged Hamiltonians of Bose-Hubbard type. For translation-invariant initial states with controlled boson density, we improve the known bound on the local repulsive energy at time $t$ from $\langle n^2_x\rangle_t\lesssim t^{2d}$ to $\langle n^2_x\rangle_t\lesssim t^d$. This shows that bosonic on-site energies accumulate at most ballistically. Extending the result to higher moments would have powerful implications for bosonic Lieb-Robinson bounds. While previous approaches focused on controlling particle transport, our proof develops novel ASTLOs (adiabatic space-time localization observables) that are able to track the growth of local boson-boson correlations.

</details>


### [47] [Elementary commutator method for the Dirac equation with long-range perturbations](https://arxiv.org/abs/2511.08209)
*Shinichi Arita,Kenichi Ito*

Main category: math-ph

TL;DR: Direct commutator techniques for Dirac equation with long-range perturbations, showing absence of generalized eigenfunctions and resolvent estimates in optimal Besov spaces, plus radiation conditions in massless case.


<details>
  <summary>Details</summary>
Motivation: To develop foundation for stationary scattering theory of Dirac operator using elementary methods without advanced functional analysis or pseudodifferential calculus.

Method: Direct commutator techniques using generator of radial translations as conjugate operator with various weight functions, following Ito-Skibsted scheme.

Result: Absence of generalized eigenfunctions, locally uniform resolvent estimates in optimal Besov-type spaces, and algebraic radiation condition of projection type for massless case.

Conclusion: The results provide a foundation for stationary scattering theory of the Dirac operator using elementary commutator methods.

Abstract: We present direct and elementary commutator techniques for the Dirac equation with long-range electric and mass perturbations. The main results are absence of generalized eigenfunctions and locally uniform resolvent estimates, both in terms of the optimal Besov-type spaces. With an additional massless assumption, we also obtain an algebraic radiation condition of projection type. For their proofs, following the scheme of Ito-Skibsted, we adopt, along with various weight functions, the generator of radial translations as conjugate operator, and avoid any of advanced functional analysis, pseudodifferential calculus, or even reduction to the Schrödinger equation. The results of the paper would serve as a foundation for the stationary scattering theory of the Dirac operator.

</details>


<div id='cond-mat.quant-gas'></div>

# cond-mat.quant-gas [[Back]](#toc)

### [48] [Toward fast, accurate and robust AI prediction of ground states in rotating BEC](https://arxiv.org/abs/2511.07489)
*Zhizhong Kong,Jerry Zhijian Yang,Cheng Yuan,Xiaofei Zhao*

Main category: cond-mat.quant-gas

TL;DR: Unsupervised deep learning approach for computing ground states of rotating Bose-Einstein condensates with novel normalization and training strategies to handle mass constraints and avoid local minima.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient computational method for predicting ground states of rotating Bose-Einstein condensates across various physical conditions, overcoming challenges like mass constraints and local minima in optimization.

Method: Uses unsupervised deep learning with two key innovations: normalized loss function for exact mass constraint enforcement, and virtual rotation acceleration training strategy to avoid local minima and guide learning to correct quantized vortex phase.

Result: Extensive numerical experiments show the approach is effective and accurate for predicting ground states from slow to fast rotation and isotropic to anisotropic confinement. Further distillation creates a unified operator network for efficient generalization across physical parameters.

Conclusion: The method enables rapid ground state predictions while correctly capturing phase transitions and can be applied to inverse problems, establishing a unified framework for Bose-Einstein condensate ground state computation.

Abstract: We propose an unsupervised deep learning approach for computing the ground state (GS) of rotating Bose-Einstein condensation. To minimize the energy under a mass constraint, our approach introduces two key and novel ingredients: a normalized loss function that exactly enforces the mass constraint, and a training strategy named virtual rotation acceleration that is essential for avoiding local minima and guiding the learning process to the correct quantized vortex phase. Extensive numerical experiments demonstrate the proposed approach as an effective and accurate method to predict GS across physical conditions--from slow to fast rotation and from isotropic to anisotropic confinement. Through further distillation, we establish a unified operator network capable of efficiently generalizing physical parameters across different phases. It enables rapid GS predictions while correctly capturing phase transitions and is applied for inverse problems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [49] [Machine-learning interatomic potential for AlN for epitaxial simulation](https://arxiv.org/abs/2511.08330)
*Nicholas Taormina,Emir Bilgili,Jason Gibson,Richard Hennig,Simon Phillpot,Youping Chen*

Main category: cond-mat.mtrl-sci

TL;DR: Development of a machine learned interatomic potential for AlN using UF3 methodology that accurately predicts structural/mechanical properties and reproduces experimental observations of dislocation structures and wurtzite crystal formation during epitaxial growth.


<details>
  <summary>Details</summary>
Motivation: To create a computationally efficient yet accurate interatomic potential for AlN that can enable large-scale atomistic simulations of epitaxial growth processes, overcoming limitations of prior potentials.

Method: Used ultra-fast force field (UF3) methodology to develop a machine learned interatomic potential for AlN, validated against density functional theory calculations and experimental observations.

Result: Potential achieved strong agreement with DFT for structural/mechanical properties, accurately reproduced experimental edge dislocation structures and wurtzite crystal formation during homoepitaxial growth, and demonstrated layer-by-layer growth mode.

Conclusion: The UF3-based potential provides the accuracy, transferability, and computational speed needed for feasible large-scale atomistic simulations of AlN epitaxial growth.

Abstract: A machine learned interatomic potential for AlN was developed using the ultra-fast force field (UF3) methodology. A strong agreement with density functional theory calculations in predicting key structural and mechanical properties, including lattice constants, elastic constants, cohesive energy, and surface energies has been demonstrated. The potential was also shown to accurately reproduce the experimentally observed atomic core structure of edge dislocations. Most significantly, it reproduced the experimentally observed wurtzite crystal structure in the overlayer during homoepitaxial growth of AlN on wurtzite AlN, something that prior potentials failed to achieve. Additionally, the potential reproduced the experimentally observed layer-by-layer growth mode in the epilayer. The combination of accuracy, transferability, and computational speed afforded by the UF3 framework thus makes large-scale, atomistic simulations of epitaxial growth of AlN feasible.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [50] [Fill the gaps: continuous in time interpolation of fluid dynamical simulations](https://arxiv.org/abs/2511.08239)
*Jonas Pronk,Oliver Porth,Jordy Davelaar*

Main category: astro-ph.HE

TL;DR: The paper presents PITT FNO, a machine learning model combining physics-informed token transformer with Fourier neural operator for accurate interpolation of multi-channel data governed by Euler equations, achieving 6-10x data efficiency over linear interpolation.


<details>
  <summary>Details</summary>
Motivation: To develop flexible and accurate interpolation schemes for numerical simulations and post-processing tasks like temporal upsampling and storage reduction using machine learning.

Method: Adapted physics-informed token transformer (PITT) for multi-channel data and coupled it with Fourier neural operator (FNO), trained on Euler equations dataset for interpolation tasks.

Result: PITT FNO requires 6-10 times less data than linear interpolation to achieve same mean square error, exhibits excellent mass and energy conservation due to physics-informed nature, and shows limitations in recovering fine details related to decreasing correlation time with increasing Fourier modes.

Conclusion: The PITT FNO network provides efficient interpolation with good conservation properties, though fine detail recovery is limited by correlation time constraints that cannot be resolved by simply increasing Fourier mode truncation.

Abstract: Flexible and accurate interpolation schemes using machine learning could be of great benefit for many use-cases in numerical simulations and post-processing, such as temporal upsampling or storage reduction. In this work, we adapt the physics-informed token transformer (PITT) network for multi-channel data and couple it with Fourier neural operator (FNO). The resulting PITT FNO network is trained for interpolation tasks on a dataset governed by the Euler equations. We compare the performance of our machine learning model with a linear interpolation baseline and show that it requires $\sim6-10$ times less data to achieve the same mean square error of the interpolated quantities. Additionally, PITT FNO has excellent mass and energy conservation as a result of its physics-informed nature. We further discuss the ability of the network to recover fine detail using a spectral analysis. Our results suggest that loss of fine details is related to the decreasing correlation time of the data with increasing Fourier mode which cannot be resolved by simply increasing Fourier mode truncation in FNO.

</details>
