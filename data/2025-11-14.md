<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 16]
- [math.AP](#math.AP) [Total: 14]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 8]
- [cs.MS](#cs.MS) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [math.DG](#math.DG) [Total: 3]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Convergence analysis of a third order semi-implicit projection method for Landau-Lifshitz-Gilbert equation](https://arxiv.org/abs/2511.09589)
*Changjian Xie,Cheng Wang*

Main category: math.NA

TL;DR: Third-order convergence analysis for Landau-Lifshitz-Gilbert equation with non-convex constraint using semi-implicit BDF3 method with projection.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze a high-order numerical scheme for the highly nonlinear Landau-Lifshitz-Gilbert equation while preserving the magnetization length constraint.

Method: Fully discrete semi-implicit method using third-order backward differentiation formula (BDF3) with one-sided extrapolation and projection step to maintain unit magnetization length.

Result: Achieves third-order accuracy in time and fourth-order in space when spatial step-size equals temporal step-size with damping parameter α > √2/2. Unique solvability proven without step-size restrictions.

Conclusion: The proposed method provides rigorous convergence analysis and unique solvability, validated by numerical experiments in 1D and 3D spaces.

Abstract: The convergence analysis of a third-order scheme for the highly nonlinear Landau-Lifshitz-Gilbert equation with a non-convex constraint is considered. In this paper, we first present a fully discrete semi-implicit method for solving the Landau-Lifshitz-Gilbert equation based on the third-order backward differentiation formula and the one-sided extrapolation (using previous time-step numerical values). A projection step is further used to preserve the length of the magnetization. We provide a rigorous convergence analysis for the fully discrete numerical solution by the introduction of two sets of approximated solutions where one set of solutions solves the Landau-Lifshitz-Gilbert equation and the other is projected onto the unit sphere. Third-order accuracy in time and fourth order accuracy in space is obtained provided that the spatial step-size is the same order as the temporal step-size and slightly large damping parameter $α$ (greater than $\sqrt{2}/2$). And also, the unique solvability of the numerical solution without any assumption for the step-size in both time and space is theoretically justified, using a monotonicity analysis. All these theoretical results are guaranteed by numerical examples in both 1D and 3D spaces.

</details>


### [2] [SLIPT for Underwater IoT: System Modeling and Performance Analysis](https://arxiv.org/abs/2511.09680)
*Shunyuan Shang,Ziyuan Shi,Mohamed-Slim Alouini*

Main category: math.NA

TL;DR: Unified analytical framework for two-phase underwater wireless optical communication with SLIPT using PV panel receivers, enabling self-powered underwater sensor nodes.


<details>
  <summary>Details</summary>
Motivation: To enable self-powered underwater sensor nodes by integrating simultaneous lightwave information and power transfer using cost-effective photovoltaic panels for concurrent optical signal detection and energy harvesting.

Method: Developed composite statistical channel model combining distance-dependent absorption, turbulence-induced fading (mixture EGG distribution), and beam misalignment. Derived closed-form expressions for key performance metrics using Meijer G and Fox H functions.

Result: Successfully derived analytical expressions for probability density function, cumulative distribution function, outage probability, average bit error rate, ergodic capacity, and harvested power.

Conclusion: Provides a practical analytical framework that offers clear guidance for design, optimization, and operation of SLIPT-based underwater wireless optical communication systems.

Abstract: This paper presents a unified analytical framework for a two phase underwater wireless optical communication (UWOC) system that integrates Simultaneous Lightwave Information and Power Transfer (SLIPT) using a photovoltaic (PV) panel receiver. The proposed architecture enables self powered underwater sensor nodes by leveraging wide area and low cost PV panels for concurrent optical signal detection and energy harvesting. We develop a composite statistical channel that combines distance dependent absorption, turbulence induced fading characterized by the mixture Exponential Generalized Gamma (EGG )distribution, and beam misalignment due to pointing errors. Based on this model we derive closed form expressions for the probability density function, the cumulative distribution function, the outage probability (OP), the average bit error rate, the ergodic capacity, and the harvested power using Meijer G and Fox H functions. Overall, the paper introduces a practical analytical framework that provides clear guidance for design, optimization, and operation of SLIPT based UWOC systems.

</details>


### [3] [Regularity and error estimates in physics-informed neural networks for the Kuramoto-Sivashinsky equation](https://arxiv.org/abs/2511.09728)
*Mohammad Mahabubur Rahman,Deepanshu Verma*

Main category: math.NA

TL;DR: This paper establishes the first rigorous error estimates for Physics-Informed Neural Networks (PINNs) applied to the challenging Kuramoto-Sivashinsky equation, overcoming difficulties from nonlinearity, bi-harmonic dissipation, and backward heat-like terms.


<details>
  <summary>Details</summary>
Motivation: The Kuramoto-Sivashinsky equation presents significant challenges due to its nonlinearity, bi-harmonic dissipation, and backward heat-like term without divergence-free conditions. Despite extensive PINN applications, no previous study provided rigorous error estimates for this equation.

Method: The approach combines classical mathematical analysis with numerical approximation through PINNs framework. It establishes global regularity criteria based on space-time integrability conditions in Besov spaces and derives rigorous error estimates.

Result: The paper successfully overcomes inherent challenges and establishes several global regularity criteria. It provides the first rigorous error estimates for PINNs approximation of the Kuramoto-Sivashinsky equation, with theoretical bounds validated through numerical simulations.

Conclusion: This work bridges the gap between classical analysis and numerical approximation for challenging PDEs like the Kuramoto-Sivashinsky equation, providing foundational error estimates that enable reliable PINN applications to complex nonlinear systems.

Abstract: Due to its nonlinearity, bi-harmonic dissipation, and backward heat-like term in the absence of a divergence-free condition, the $2$-D/$3$-D Kuramoto-Sivashinsky equation poses significant challenges for both mathematical analysis and numerical approximation. These difficulties motivate the development of methods that blend classical analysis with numerical approximation approaches embodied in the framework of the physics-informed neural networks (PINNs). In addition, despite the extensive use of PINN frameworks for various linear and nonlinear PDEs, no study had previously established rigorous error estimates for the Kuramoto-Sivashinsky equation within a PINN setting. In this work, we overcome the inherent challenges, and establish several global regularity criteria based on space-time integrability conditions in Besov spaces. We then derive the first rigorous error estimates for the PINNs approximation of the Kuramoto-Sivashinsky equation and validate our theoretical error bounds through numerical simulations.

</details>


### [4] [Global iterative methods for sparse approximate inverses of symmetric positive-definite matrices](https://arxiv.org/abs/2511.09753)
*Nicolas Venkovic,Hartwig Anzt*

Main category: math.NA

TL;DR: The paper introduces and analyzes N(P)CG and LO(P)MR methods for computing sparse approximate inverses of SPD matrices, comparing them with existing methods like (P)CG, (P)MR, and (P)SD.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient iterative methods for computing sparse approximate inverses that accelerate convergence compared to existing global iteration methods like (P)MR and (P)SD.

Method: N(P)CG uses one-dimensional projection with residual orthogonality, while LO(P)MR employs two-dimensional projection to enrich (P)MR iterates. Both methods are implemented with practical dropping strategies to control sparsity.

Result: N(P)CG provides slight improvement over (P)SD but is less effective than (P)MR. LO(P)MR consistently outperforms all other methods, converging faster to better and often sparser approximations.

Conclusion: LO(P)MR is the most robust and effective method for computing sparse approximate inverses, offering faster convergence and better sparsity control compared to other approaches.

Abstract: The nonlinear (preconditioned) conjugate gradient N(P)CG method and the locally optimal (preconditioned) minimal residual LO(P)MR method, both of which are used for the iterative computation of sparse approximate inverses (SPAIs) of symmetric positive-definite (SPD) matrices, are introduced and analyzed. The (preconditioned) conjugate gradient (P)CG method is also employed and presented for comparison. The N(P)CG method is defined as a one-dimensional projection with residuals made orthogonal to the current search direction, itself made $A$-orthogonal to the last search direction. The residual orthogonality, expressed via Frobenius inner product, actually holds against all previous search directions, making each iterate globally optimal, that is, that minimizes the Frobenius A-norm of the error over the affine Krylov subspace of $A^2$ generated by the initial gradient. The LO(P)MR method is a two-dimensional projection method that enriches iterates produced by the (preconditioned) minimal residual (P)MR method. These approaches differ from existing descent methods and aim to accelerate convergence compared to other global iteration methods, including (P)MR and (preconditioned) steepest descent (P)SD, previously used for SPAI computation. The methods are implemented with practical dropping strategies to control the growth of nonzero components in the approximate inverse. Numerical experiments are performed in which approximate inverses of several sparse SPD matrices are computed. N(P)CG provides a slight improvement over (P)SD, but remains generally less effective than (P)MR. On the other hand, while (P)CG does improve (P)MR, its convergence is more affected by the dropping of nonzero components, ill-conditioning, and small eigenvalues. LO(P)MR is more robust than (P)MR and (P)CG, consistently outperforms other methods, converging faster to better and often sparser approximations.

</details>


### [5] [A model-free method for discovering symmetry in differential equations](https://arxiv.org/abs/2511.09779)
*Max Kreider,John Harlim,Daning Huang*

Main category: math.NA

TL;DR: A numerical method for discovering Lie symmetries from scattered data without knowing the governing equations, using manifold learning to approximate infinitesimal generators.


<details>
  <summary>Details</summary>
Motivation: Identifying Lie symmetries directly from scattered data without explicit knowledge of governing equations is challenging but crucial for reducing model complexity in dynamical systems.

Method: Uses Generalized Moving Least Squares manifold learning to prolongate data, then constructs a linear system whose null space encodes infinitesimal generators representing symmetries.

Result: The method demonstrates accuracy, robustness, and convergence in numerical experiments with ordinary and partial differential equations, with derived convergence bounds.

Conclusion: The approach enables data-driven discovery of continuous symmetries in dynamical systems without requiring analytical forms of differential equations.

Abstract: Symmetry in differential equations reveals invariances and offers a powerful means to reduce model complexity. Lie group analysis characterizes these symmetries through infinitesimal generators, which provide a local, linear criterion for invariance. However, identifying Lie symmetries directly from scattered data, without explicit knowledge of the governing equations, remains a significant challenge. This work introduces a numerical scheme that approximates infinitesimal generators from data sampled on an unknown smooth manifold, enabling the recovery of continuous symmetries without requiring the analytical form of the differential equations. We employ a manifold learning technique, Generalized Moving Least Squares, to prolongate the data, from which a linear system is constructed whose null space encodes the infinitesimal generators representing the symmetries. Convergence bounds for the proposed approach are derived. Several numerical experiments, including ordinary and partial differential equations, demonstrate the method's accuracy, robustness, and convergence, highlighting its potential for data-driven discovery of symmetries in dynamical systems.

</details>


### [6] [Efficient Krylov-Regularization Solvers for Multiquadric RBF Discretizations of the 3D Helmholtz Equation](https://arxiv.org/abs/2511.09798)
*Mohamed El Guide,Khalide Jbilou,Kamal Lachhab,Driss Ouazar*

Main category: math.NA

TL;DR: Three regularization methods (TSVD, Tikhonov, and hybrid Krylov-Tikhonov) are developed to stabilize meshless collocation with multiquadric radial basis functions for 3D Helmholtz equations, overcoming ill-conditioning while maintaining accuracy at reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: Meshless collocation with MQ-RBFs provides high accuracy for 3D Helmholtz equations but produces dense, severely ill-conditioned linear systems that are unstable at scale.

Method: Three complementary regularization methods: (i) inexpensive TSVD using Golub-Kahan bidiagonalization, (ii) classical Tikhonov regularization with GCV/L-curve parameter selection, and (iii) hybrid Krylov-Tikhonov scheme that projects first then regularizes.

Result: HKT consistently matches or surpasses full TSVD/Tikhonov accuracy at fraction of runtime and memory. Inexpensive TSVD provides fastest reconstructions when only leading modes are needed.

Conclusion: Coupling Krylov projection with TSVD/Tikhonov regularization provides robust, scalable pathway for MQ-RBF Helmholtz methods in complex 3D settings.

Abstract: Meshless collocation with multiquadric radial basis functions (MQ-RBFs) delivers high accuracy for the three-dimensional Helmholtz equation but produces dense, severely ill-conditioned linear systems. We develop and evaluate three complementary methods that embed regularization in Krylov projections to overcome this instability at scale: (i) an inexpensive TSVD that replaces the full SVD by a short Golub-Kahan bidiagonalization and a small projected SVD, retaining the dominant spectral content at greatly reduced cost; (ii) classical Tikhonov regularization with principled parameter choice (GCV/L-curve), expressed in SVD form for transparent filtering; and (iii) a hybrid Krylov-Tikhonov (HKT) scheme that first projects with Golub-Kahan and then selects the regularization parameter on the reduced problem, yielding stable solutions in few iterations. Extensive tests on canonical domains (cube and sphere) and a realistic industrial pump-casing geometry demonstrate that HKT consistently matches or surpasses the accuracy of full TSVD/Tikhonov at a fraction of the runtime and memory, while inexpensive TSVD provides the fastest viable reconstructions when only the leading modes are needed. These results show that coupling Krylov projection with TSVD/Tikhonov regularization provides a robust, scalable pathway for MQ-RBF Helmholtz methods in complex three-dimensional settings.

</details>


### [7] [Randomized batch-sampling Kaczmarz methods for general linear systems](https://arxiv.org/abs/2511.09872)
*Dong-Yue Xie,Xi Yang*

Main category: math.NA

TL;DR: Unified randomized batch-sampling Kaczmarz framework with low per-iteration costs achieves expected linear convergence with scale-invariant bounds that are tighter than existing ones.


<details>
  <summary>Details</summary>
Motivation: To conduct deeper investigation of randomized solvers for general linear systems and develop more accurate convergence analysis for block Kaczmarz methods.

Method: Adopt unified randomized batch-sampling Kaczmarz framework with concentration inequalities to derive new convergence rate bounds for static stochastic samplings.

Result: New scale-invariant convergence bounds eliminate dependence on data matrix magnitude, are significantly tighter than existing bounds, and better reflect empirical convergence behavior.

Conclusion: The batch-sampling distribution as a learnable parameter enables efficient performance in specific applications, warranting further investigation.

Abstract: To conduct a more in-depth investigation of randomized solvers for general linear systems, we adopt a unified randomized batch-sampling Kaczmarz framework with per-iteration costs as low as cyclic block methods, and develop a general analysis technique to establish its convergence guarantee. With concentration inequalities, we derive new expected linear convergence rate bounds. The analysis applies to any randomized non-extended block Kaczmarz methods with static stochastic samplings. In addition, the new rate bounds are scale-invariant which eliminate the dependence on the magnitude of the data matrix. In most experiments, the new bounds are significantly tighter than existing ones and better reflect the empirical convergence behavior of block methods. Within this new framework, the batch-sampling distribution, as a learnable parameter, provides the possibility for block methods to achieve efficient performance in specific application scenarios, which deserves further investigation.

</details>


### [8] [Implicit Multiple Tensor Decomposition](https://arxiv.org/abs/2511.09916)
*Kunjing Yang,Libin Zheng,Minru Bai*

Main category: math.NA

TL;DR: Proposes Multiple decomposition as a generalization of triple decomposition for arbitrary-order tensors with flexible dimensions, combined with implicit neural representations for non-grid data, and provides convergence analysis without KL property.


<details>
  <summary>Details</summary>
Motivation: Triple decomposition is limited to third-order tensors and requires uniform lower dimensions across factor tensors, restricting flexibility and applicability to higher-order tensors and non-uniform structures.

Method: Introduces Multiple decomposition framework for arbitrary-order tensors with varying short dimensions, uses implicit neural representations (INR) for continuous factor tensor representation (IMTD), and applies Proximal Alternating Least Squares (PALS) algorithm with KL-free convergence analysis.

Result: Extensive numerical experiments validate the effectiveness of the proposed method, demonstrating successful tensor decomposition and reconstruction across various scenarios.

Conclusion: The proposed Multiple decomposition framework successfully generalizes triple decomposition to arbitrary-order tensors with flexible dimensions, and the IMTD approach enables effective handling of non-grid data with proven convergence properties.

Abstract: Recently, triple decomposition has attracted increasing attention for decomposing third-order tensors into three factor tensors. However, this approach is limited to third-order tensors and enforces uniformity in the lower dimensions across all factor tensors, which restricts its flexibility and applicability. To address these issues, we propose the Multiple decomposition, a novel framework that generalizes triple decomposition to arbitrary order tensors and allows the short dimensions of the factor tensors to differ. We establish its connections with other classical tensor decompositions. Furthermore, implicit neural representation (INR) is employed to continuously represent the factor tensors in Multiple decomposition, enabling the method to generalize to non-grid data. We refer to this INR-based Multiple decomposition as Implicit Multiple Tensor Decomposition (IMTD). Then, the Proximal Alternating Least Squares (PALS) algorithm is utilized to solve the IMTD-based tensor reconstruction models. Since the objective function in IMTD-based models often lacks the Kurdyka-Lojasiewicz (KL) property, we establish a KL-free convergence analysis for the algorithm. Finally, extensive numerical experiments further validate the effectiveness of the proposed method.

</details>


### [9] [Asymptotic-preserving and energy-conserving methods for a hyperbolic approximation of the BBM equation](https://arxiv.org/abs/2511.10044)
*Sebastian Bleecke,Abhijit Biswas,David I. Ketcheson,Hendrik Ranocha,Jochen Schutz*

Main category: math.NA

TL;DR: Development of asymptotic-preserving numerical methods for hyperbolic approximation of BBM equation using implicit-explicit Runge-Kutta methods with entropy relaxation for energy preservation.


<details>
  <summary>Details</summary>
Motivation: To address the hyperbolic approximation of the Benjamin-Bona-Mahony (BBM) equation proposed by Gavrilyuk and Shyue (2022) and develop numerical methods that preserve important invariants.

Method: Implicit-explicit (additive) Runge-Kutta methods that are implicit in the stiff linear part, with entropy relaxation approach to make fully discrete schemes energy-preserving.

Result: The new discretization conserves important invariants converging to invariants of the BBM equation, and numerical experiments demonstrate effectiveness.

Conclusion: The developed asymptotic-preserving numerical methods successfully preserve invariants and energy while effectively approximating the hyperbolic BBM equation.

Abstract: We study the hyperbolic approximation of the Benjamin-Bona-Mahony (BBM) equation proposed recently by Gavrilyuk and Shyue (2022). We develop asymptotic-preserving numerical methods using implicit-explicit (additive) Runge-Kutta methods that are implicit in the stiff linear part. The new discretization of the hyperbolization conserves important invariants converging to invariants of the BBM equation. We use the entropy relaxation approach to make the fully discrete schemes energy-preserving. Numerical experiments demonstrate the effectiveness of these discretizations.

</details>


### [10] [A Third-order Conservative Semi-Lagrangian Discontinuous Galerkin Scheme For the Transport Equation on Curvilinear Unstructured Meshes](https://arxiv.org/abs/2511.10100)
*Xiaofeng Cai,Yibing Chen,Kunkai Fu,Liujun Pan*

Main category: math.NA

TL;DR: A third-order conservative semi-Lagrangian discontinuous Galerkin scheme for linear transport equations on curvilinear unstructured triangular meshes, featuring mass conservation, large time stepping, and oscillation suppression.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient high-order numerical scheme for solving linear transport equations on complex geometries using curvilinear unstructured meshes while ensuring mass conservation and handling discontinuities.

Method: Developed a conservative intersection-based remapping algorithm for curvilinear meshes, combined with a non-splitting SLDG method using WENO and positivity-preserving limiters for oscillation control.

Result: Achieves third-order accuracy in both space and time, validated through rigorous numerical analysis and benchmarks including rigid body rotation and swirling deformation flows with smooth/discontinuous initial conditions.

Conclusion: The scheme demonstrates high accuracy, stability, and robustness for linear transport problems on complex geometries while maintaining mass conservation and handling discontinuities effectively.

Abstract: We develop a third-order conservative semi-Lagrangian discontinuous Galerkin (SLDG) scheme for solving linear transport equations on curvilinear unstructured triangular meshes, tailored for complex geometries. To ensure third-order spatial accuracy while strictly preserving mass, we develop a high-order conservative intersection-based remapping algorithm for curvilinear unstructured meshes, which enables accurate and conservative data transfer between distinct curvilinear meshes. Incorporating this algorithm, we construct a non-splitting high-order SLDG method equipped with weighted essentially non-oscillatory and positivity-preserving limiters to effectively suppress numerical oscillations and maintain solution positivity. For the linear problem, the semi-Lagrangian update enables large time stepping, yielding an explicit and efficient implementation. Rigorous numerical analysis confirms that our scheme achieves third-order accuracy in both space and time, as validated by consistent error analysis in terms of $L^1$ and $L^2$-norms. Numerical benchmarks, including rigid body rotation and swirling deformation flows with smooth and discontinuous initial conditions, validate the scheme's accuracy, stability, and robustness.

</details>


### [11] [Accelerating the Serviceability-Based Design of Reinforced Concrete Rail Bridges under Geometric Uncertainties induced by unforeseen events: A Surrogate Modeling approach](https://arxiv.org/abs/2511.10129)
*Mouhammed Achhab,Pierre Jehel,Fabrice Gatuingt*

Main category: math.NA

TL;DR: Surrogate modeling is used as an efficient probabilistic design approach for reinforced concrete rail bridges to handle uncertainties from construction constraints and reduce reliance on time-consuming finite element simulations.


<details>
  <summary>Details</summary>
Motivation: To address uncertainties from unforeseen construction constraints (e.g., pier repositioning, geometric changes) that cause repeated redesigns, added costs, and project delays in traditional bridge design processes.

Method: Multi-fiber finite element modeling in Cast3M software to generate design experiments, with comparative assessment of Kriging surrogate against polynomial chaos expansion and support vector regression methods.

Result: The methodology enables efficient exploration of numerous design scenarios, representation of bridge performance functions as variable design parameters, and classification of designs into failure and safe scenarios.

Conclusion: Surrogate modeling supports early-stage uncertainty-informed design, enhancing robustness and adaptability of reinforced concrete rail bridges against practical constraints and changing site conditions.

Abstract: Reinforced concrete rail bridges are essential components of railway infrastructure, where reliability, durability, and adaptability are key design priorities. However, the design process is often complicated by uncertainties stemming from unforeseen construction constraints, such as the need to reposition piers or alter geometric characteristics. These design adaptations can lead to repeated redesigns, added costs, and project delays if not anticipated in the early design stages, as well as significant computational overhead when using traditional finite element (FE) simulations. To address this and anticipate such unexpected events, this study adopts surrogate modeling as an efficient probabilistic design approach. This methodology integrates key geometric parameters as random variables, capturing the uncertainties that may arise during the design and construction phases and propagating them on the bridge's performance functions. By doing so, we aim to enable the efficient exploration of a large number of design scenarios with minimal reliance on time-consuming finite element (FE) simulations, represent the performance functions of a reinforced concrete bridge as a function of our variable design parameters, and classify the overall design scenarios into failure and safe scenarios In this study, a four-span reinforced concrete bridge deck is modeled using a multi-fiber finite element approach in Cast3M software. This FE model is used to generate the required design of experiments to train the surrogate models. Within this framework, a comparative performance assessment is conducted to evaluate the performance of the Kriging surrogate against alternative methods, including polynomial chaos expansion (implemented in UQLab) and support vector regression (SVR). This methodology supports early-stage uncertainty-informed design, enhancing the robustness and adaptability of reinforced concrete rail bridges in the face of practical constraints and changing site conditions.

</details>


### [12] [Control strategies for magnetized plasma: a polar coordinates framework](https://arxiv.org/abs/2511.10214)
*Federica Ferrarese*

Main category: math.NA

TL;DR: Overview of magnetic field control strategies for plasma steering using Vlasov equation modeling in 2D polar coordinates with feedback mechanisms.


<details>
  <summary>Details</summary>
Motivation: To develop effective control strategies for steering plasma toward desired configurations in toroidal devices like Tokamaks and Stellarators using external magnetic fields.

Method: Modeling using Vlasov equation in 2D bounded domain with self-induced electric field and strong external magnetic field, presented in polar coordinates with feedback control based on instantaneous prediction of discretized system.

Result: Numerical experiments in 2D polar coordinate setting demonstrate the effectiveness of the proposed control approaches.

Conclusion: The feedback-based control strategies using magnetic fields are effective for plasma steering in toroidal confinement devices.

Abstract: In this work, we provide an overview of various control strategies aimed at steering plasma toward desired configurations using an external magnetic field. From a modeling perspective, we focus on the Vlasov equation in a two-dimensional bounded domain, accounting for both a self-induced electric field and a strong external magnetic field. The results are presented in a polar coordinate framework, which is particularly well-suited for simulating toroidal devices such as Tokamaks and Stellarators. A key feature of the proposed control strategies is their feedback mechanism, which is based on an instantaneous prediction of the discretized system. Finally, different numerical experiments in the two-dimensional polar coordinate setting demonstrate the effectiveness of the approaches.

</details>


### [13] [A Stabilized Unfitted Space-time Finite Element Method for Parabolic Problems on Moving Domains](https://arxiv.org/abs/2511.10242)
*Ruizhi Wang,Weibing Deng*

Main category: math.NA

TL;DR: A space-time FEM using unfitted meshes for parabolic problems on moving domains, featuring SUPG stabilization for time-advection and ghost penalty for small cut issues, with optimal convergence rates proven.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for solving parabolic problems on moving domains using unfitted meshes, avoiding the limitations of traditional time-stepping approaches like DG methods.

Method: Fully coupled space-time discretization with SUPG stabilization for time-advection and ghost penalty stabilization to handle small cut elements, ensuring well-conditioned stiffness matrices.

Result: The method achieves optimal convergence rates with respect to mesh size, supported by a priori error estimates and space-time Poincare-Friedrichs inequality for condition number analysis.

Conclusion: The proposed unfitted space-time FEM is effective for parabolic problems on moving domains, validated by numerical examples and theoretical analysis showing optimal convergence and stability.

Abstract: This paper presents a space-time finite element method (FEM) based on an unfitted mesh for solving parabolic problems on moving domains. Unlike other unfitted space-time finite element approaches that commonly employ the discontinuous Galerkin (DG) method for time-stepping, the proposed method employs a fully coupled space-time discretization. To stabilize the time-advection term, the streamline upwind Petrov-Galerkin (SUPG) scheme is applied in the temporal direction. A ghost penalty stabilization term is further incorporated to mitigate the small cut issue, thereby ensuring the well-conditioning of the stiffness matrix. Moreover, an a priori error estimate is derived in a discrete energy norm, which achieves an optimal convergence rate with respect to the mesh size. In particular, a space-time Poincare-Friedrichs inequality is established to support the condition number analysis. Several numerical examples are provided to validate the theoretical findings.

</details>


### [14] [A novel mathematical and computational framework of amyloid-beta triggered seizure dynamics in Alzheimer's disease](https://arxiv.org/abs/2511.10369)
*Caterina B. Leimer Saglio,Mattia Corti,Stefano Pagani,Paola F. Antonietti*

Main category: math.NA

TL;DR: A multiscale mathematical model shows how amyloid-β accumulation in Alzheimer's disease leads to calcium dysregulation, neuronal hyperexcitability, and epileptiform activity through multiple molecular mechanisms.


<details>
  <summary>Details</summary>
Motivation: To quantitatively describe how amyloid-β accumulation in Alzheimer's disease affects neuronal excitability and leads to epileptic activity, bridging molecular alterations with electrophysiological dynamics.

Method: Extended Barreto-Cressman ionic model incorporating amyloid-β-induced calcium dysregulation mechanisms, coupled with monodomain equation and discretized using p-adaptive discontinuous Galerkin method on polytopal meshes.

Result: Progressive amyloid-β accumulation causes severe calcium homeostasis alterations, increased neuronal hyperexcitability, pathological seizure propagation, secondary epileptogenic sources, and spatially heterogeneous wavefronts.

Conclusion: Multiscale modeling provides mechanistic insights into the interplay between neurodegeneration and epilepsy in Alzheimer's disease, showing biochemical inhomogeneities critically shape seizure dynamics.

Abstract: The association of epileptic activity and Alzheimer's disease (AD) has been increasingly reported in both clinical and experimental studies, suggesting that amyloid-$β$ accumulation may directly affect neuronal excitability. Capturing these interactions requires a quantitative description that bridges the molecular alterations of AD with the fast electrophysiological dynamics of epilepsy. We introduce a novel mathematical model that extends the Barreto-Cressman ionic formulation by incorporating multiple mechanisms of calcium dysregulation induced by amyloid-$β$, including formation of $\mathrm{Ca}^{2+}$-permeable pores, overactivation of voltage-gated $\mathrm{Ca}^{2+}$ channels, and suppression of $\mathrm{Ca}^{2+}$-sensitive potassium currents. The resulting ionic model is coupled with the monodomain equation and discretized using a $p$-adaptive discontinuous Galerkin method on polytopal meshes, providing an effective balance between efficiency and accuracy in capturing the sharp spatiotemporal electrical wavefronts associated with epileptiform discharges. Numerical simulations performed on idealized and realistic brain geometries demonstrate that progressive amyloid-\textbeta{} accumulation leads to severe alterations in calcium homeostasis, increased neuronal hyperexcitability, and pathological seizure propagation. Specifically, high amyloid-$β$ concentrations produce secondary epileptogenic sources and spatially heterogeneous wavefronts, indicating that biochemical inhomogeneities play a critical role in shaping seizure dynamics. These results illustrate how multiscale modeling provides new mechanistic insights into the interplay between neurodegeneration and epilepsy in Alzheimer's disease.

</details>


### [15] [Learning parameter-dependent shear viscosity from data, with application to sea and land ice](https://arxiv.org/abs/2511.10452)
*Gonzalo G. de Diego,Georg Stadler*

Main category: math.NA

TL;DR: A framework for inferring non-Newtonian fluid rheology models using neural networks that satisfy physical constraints like frame-indifference and convex dissipation potential.


<details>
  <summary>Details</summary>
Motivation: To develop data-driven methods for learning rheological models of complex fluids that automatically satisfy key physical and mathematical properties.

Method: Two approaches: 1) standard regression fitting stress data, and 2) PDE-constrained optimization using velocity data, combining finite element and machine learning libraries with neural networks parameterized by tensor invariants.

Result: Successfully inferred temperature-dependent Glen's law for land ice and concentration-dependent shear component for sea ice, and discovered an unknown concentration-dependent rheology that generalizes well and exhibits shear-thickening/thinning behaviors.

Conclusion: The proposed framework enables robust inference of physically consistent rheological models from data, handling external parameter dependencies and demonstrating good generalization beyond training data.

Abstract: Complex physical systems which exhibit fluid-like behavior are often modeled as non-Newtonian fluids. A crucial element of a non-Newtonian model is the rheology, which relates inner stresses with strain-rates. We propose a framework for inferring rheological models from data that represents the fluid's effective viscosity with a neural network. By writing the rheological law in terms of tensor invariants and tailoring the network's properties, the inferred model satisfies key physical and mathematical properties, such as isotropic frame-indifference and existence of a convex potential of dissipation. Within this framework, we propose two approaches to learning a fluid's rheology: 1) a standard regression that fits the rheological model to stress data and 2) a PDE-constrained optimization method that infers rheological models from velocity data. For the latter approach, we combine finite element and machine learning libraries. We demonstrate the accuracy and robustness of our method on land and sea ice rheologies which also depend on external parameters. For land ice, we infer the temperature-dependent Glen's law and, for sea ice, the concentration-dependent shear component of the viscous-plastic model. For these two models, we explore the effects of large data errors. Finally, we infer an unknown concentration-dependent model that reproduces Lagrangian ice floe simulation data. Our method discovers a rheology that generalizes well outside of the training dataset and exhibits both shear-thickening and thinning behaviors depending on the concentrations.

</details>


### [16] [The $L_p$-error rate for randomized quasi-Monte Carlo self-normalized importance sampling of unbounded integrands](https://arxiv.org/abs/2511.10599)
*Jiarui Du,Zhijian He*

Main category: math.NA

TL;DR: Derives L_p-error rates for RQMC-based self-normalized importance sampling with unbounded integrands on unbounded domains, establishing improved convergence rates under mild boundary growth conditions.


<details>
  <summary>Details</summary>
Motivation: Existing L1 and L2 error estimates for SNIS are limited to bounded integrands, leaving a gap in understanding performance with unbounded integrands, especially under RQMC sampling.

Method: First establish L_p-error rates for plain RQMC integration, then extend to RQMC-SNIS estimators with unbounded integrands, allowing broader class of transport maps for sample generation.

Result: Achieved L_p-error rate of O(N^{-β+ε}) for RQMC-SNIS estimators, where β depends on boundary growth rate and ε>0 is arbitrarily small, with numerical validation.

Conclusion: The work provides theoretical foundation for RQMC-SNIS with unbounded integrands, demonstrating improved convergence rates under appropriate boundary conditions.

Abstract: Self-normalized importance sampling (SNIS) is a fundamental tool in Bayesian inference when the posterior distribution involves an unknown normalizing constant. Although $L_1$-error (bias) and $L_2$-error (root mean square error) estimates of SNIS are well established for bounded integrands, results for unbounded integrands remain limited, especially under randomized quasi-Monte Carlo (RQMC) sampling. In this work, we derive $L_p$-error rate $(p\ge1)$ for RQMC-based SNIS (RQMC-SNIS) estimators with unbounded integrands on unbounded domains. A key step in our analysis is to first establish the $L_p$-error rate for plain RQMC integration. Our results allow for a broader class of transport maps used to generate samples from RQMC points. Under mild function boundary growth conditions, we further establish \(L_p\)-error rate of order \(\mathcal{O}(N^{-β+ ε})\) for RQMC-SNIS estimators, where $ε>0$ is arbitrarily small, $N$ is the sample size, and \(β\in (0,1]\) depends on the boundary growth rate of the resulting integrand. Numerical experiments validate the theoretical results.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [17] [Euler equation on a fast rotating ellipsoid](https://arxiv.org/abs/2511.09721)
*Haoran Wu*

Main category: math.AP

TL;DR: The paper extends the study of incompressible Euler equations from spheres to biaxial ellipsoids, proving that rapid rotation leads to zonal flows that are independent of longitude.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between spherical and ellipsoidal theories of fast rotating Euler dynamics by extending the zonalization phenomenon discovered on spheres to more realistic ellipsoidal geometries.

Method: Adapting the framework from Cheng-Mahalov (fast rotating spheres) and Xu (Rossby-Haurwitz solutions on ellipsoids) to establish parallel results for Euler flows on rotating ellipsoidal surfaces in the rapid rotation regime.

Result: In rapid rotation, the time-averaged velocity field remains uniformly bounded in Sobolev norms independent of rotation rate and converges to a longitude-independent zonal flow depending only on latitude.

Conclusion: The zonalization phenomenon discovered by Cheng and Mahalov on spheres persists on biaxial ellipsoids, successfully extending the theory from spherical to ellipsoidal geometries.

Abstract: This paper extends the analytical study of the incompressible Euler equations from the classical spherical setting to the more realistic geometry of a biaxial ellipsoid. Motivated by the work of Cheng and Mahalov on fast rotating spheres and Xu on Rossby-Haurwitz solutions on ellipsoids, we adapt their framework to establish a parallel result for Euler flows on a rotating ellipsoidal surface. In the regime of rapid rotation, we prove that the time-averaged velocity field remains uniformly bounded in Sobolev norms independent of the rotation rate and converges to a longitude-independent zonal flow depending only on latitude. This shows that the zonalization phenomenon discovered by Cheng and Mahalov on the sphere persists on biaxial ellipsoids, thereby bridging the gap between spherical and ellipsoidal theories of fast rotating Euler dynamics.

</details>


### [18] [Stability of the Rankine Vortex and Perimeter Growth in Vortex Patches](https://arxiv.org/abs/2511.09772)
*John Brownfield*

Main category: math.AP

TL;DR: The paper proves bounds on L^1 deviation from Rankine vortex using pseudo-energy deviation and angular momentum, with simplified results for m-fold symmetric cases, and confirms linear perimeter growth for perturbations.


<details>
  <summary>Details</summary>
Motivation: To mathematically verify prior simulation results about vortex dynamics and provide rigorous bounds for deviations from the Rankine vortex model.

Method: Mathematical analysis using vorticity functions, establishing bounds on L^1 deviation from Rankine vortex in terms of pseudo-energy deviation and angular momentum, with special treatment for m-fold symmetric cases.

Result: Proved that L^1 deviation from Rankine patch can be bounded by pseudo-energy deviation and angular momentum, with angular momentum dependence removable for m-fold symmetric cases. Confirmed linear in time perimeter growth for simply connected perturbations.

Conclusion: The analysis provides mathematical validation of prior simulation results and establishes fundamental bounds for vortex dynamics, particularly for perturbations of the Rankine vortex.

Abstract: We prove that for $ω: \mathbb{R}^2 \to [0,1]$ sharing the same total vorticity and center of vorticity as the Rankine vortex, the $L^1$ deviation from the Rankine patch can be bounded by a function of the pseudo-energy deviation and the angular momentum of $ω$. In the case of $m-$fold symmetry, the dependence on the angular momentum can be dropped. Using this, we affirm the results of prior simulations by demonstrating linear in time perimeter growth for a simply connected perturbation of the Rankine vortex.

</details>


### [19] [Analysis of the adhesion model and the reconstruction problem in cosmology](https://arxiv.org/abs/2511.09800)
*Jian-Guo Liu,Robert L. Pego*

Main category: math.AP

TL;DR: Analysis of mass flow in cosmology's adhesion model shows unique Lagrangian semi-flow exists in zero-viscosity limit, but mass measure singularities differ from Monge-Ampère measures when flows merge, preventing exact reconstruction of inverse Lagrangian maps near singular structures.


<details>
  <summary>Details</summary>
Motivation: To understand mass concentration in cosmological structures using the adhesion model, which suppresses multi-streaming by introducing viscosity, and to study the zero-viscosity limit behavior.

Method: Analysis of Lagrangian advection in the adhesion model's zero-viscosity limit, using differential inclusions, Monge-Ampère measures, and optimal transport theory to characterize particle paths and mass flow.

Result: A unique limiting Lagrangian semi-flow exists where particle paths stick together after collision. The absolutely continuous mass measure matches Monge-Ampère measures from convexification, but singular parts differ when flows merge, preventing exact reconstruction of inverse Lagrangian maps near singularities.

Conclusion: While the adhesion model provides insights into mass flow and structure formation, exact reconstruction of inverse Lagrangian maps using Monge-Ampère measures and optimal transport is impossible near merging singular structures, even away from the singularities themselves.

Abstract: In cosmology, a basic explanation of the observed concentration of mass in singular structures is provided by the Zeldovich approximation, which takes the form of free-streaming flow for perturbations of a uniform Einstein-de Sitter universe in co-moving coordinates. The adhesion model suppresses multi-streaming by introducing viscosity. We study mass flow in this model by analysis of Lagrangian advection in the zero-viscosity limit. Under mild conditions, we show that a unique limiting Lagrangian semi-flow exists. Limiting particle paths stick together after collision and are characterized uniquely by a differential inclusion. The absolutely continuous part of the mass measure agrees with that of a Monge-Ampère measure arising by convexification of the free-streaming velocity potential. But the singular parts of these measures can differ when flows along singular structures merge, as shown by analysis of a 2D Riemann problem. The use of Monge-Ampère measures and optimal transport theory for the reconstruction of inverse Lagrangian maps in cosmology was introduced in work of Brenier & Frisch et al. (Month. Not. Roy. Ast. Soc. 346, 2003). In a neighborhood of merging singular structures in our examples, however, we show that reconstruction yielding a monotone Lagrangian map cannot be exact a.e., even off of the singularities themselves.

</details>


### [20] [Uniqueness results for positive harmonic functions on manifolds with nonnegative Ricci curvature and strictly convex boundary](https://arxiv.org/abs/2511.09994)
*Xiaohan Cai*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We prove some Liouville-type theorems for positive harmonic functions on compact Riemannian manifolds with nonnegative Ricci curvature and strictly convex boundary, thereby confirming some cases of Wang's conjecture (J. Geom. Anal. 31, 2021).
  We further investigate Wang's conjecture on warped product manifolds and provide a partial verification of this conjecture, which also yields an alternative proof of Gu-Li's resolution of the conjecture in the $\mathbb{B}^n$ case (Math. Ann. 391, 2025). Our approach is based on a general principle of employing the P-function method to such Liouville-type results, with particular emphasis on the role of a closed conformal vector field inherent to such manifolds.

</details>


### [21] [Locally uniform ellipticity of the fractional Hessian operators](https://arxiv.org/abs/2511.10034)
*Ziyu Gan,Heming Jiao*

Main category: math.AP

TL;DR: This paper introduces fractional analogues of general Hessian operators, proves their stability, and establishes strict ellipticity for convex solutions across all k values from 2 to n.


<details>
  <summary>Details</summary>
Motivation: To extend previous work on fractional Monge-Ampère operators and fractional k-Hessian operators by developing a comprehensive theory for general fractional Hessian operators and addressing ellipticity properties.

Method: Introduces fractional analogues of general Hessian operators, proves stability properties, and establishes strict ellipticity conditions for convex solutions.

Result: Shows that fractional k-Hessian operators are strictly elliptic for convex solutions when 2 ≤ k ≤ n, and provides a new proof for k=2 that doesn't require convexity.

Conclusion: The paper successfully generalizes fractional Hessian operator theory, establishes key ellipticity properties, and provides improved proofs that relax convexity requirements in certain cases.

Abstract: In [1], Caffarelli-Charro introduced a fractional Monge-Ampère operator. Later, Wu [17] generalized it to a fractional analogue of $k$-Hessian operators and proved the strict ellipticity for $k=2$. In this paper, we introduce a fractional analogue of general Hessian operators and prove the stability. We also show that the fractional analogue $k$-Hessian operators defined in [17] are strictly elliptic with respect to convex solutions for all $2 \leq k \leq n$. Furthermore, we provide a new proof for the case $k=2$ without the convexity condition.

</details>


### [22] [A unified approach to Hardy-type inequalities with Bessel pairs](https://arxiv.org/abs/2511.10039)
*Lucrezia Cossetti,Lorenzo D'Arca*

Main category: math.AP

TL;DR: Characterizes Bessel pairs for weighted Hardy inequalities, extends beyond Euclidean settings to general L^p framework, provides explicit maximizing functions and sharp constants.


<details>
  <summary>Details</summary>
Motivation: To establish weighted Hardy-type inequalities in a unified framework that goes beyond classical Euclidean settings and improves existing literature.

Method: Abstract approach using Bessel pairs characterization, working in general L^p framework with explicit maximizing functions.

Result: Valid weighted Hardy-type inequalities with explicit maximizing functions and sharp constants in specific situations.

Conclusion: The approach unifies, generalizes, and improves several existing results in the literature for weighted Hardy inequalities.

Abstract: In this paper, we provide suitable characterisations of pairs of weights $(V,W),$ known as Bessel pairs, that ensure the validity of weighted Hardy-type inequalities. The abstract approach adopted here makes it possible to establish such inequalities also going beyond the classical Euclidean setting and also within a more general $L^p$ framework. As a byproduct of our method, we obtain explicit expressions for the maximizing functions and, in certain specific situations, we show that the associated constants are sharp. We emphasise that our approach unifies, generalises and improves several existing results in the literature.

</details>


### [23] [On Uniqueness For The Three-Dimensional Vlasov-Navier-Stokes System](https://arxiv.org/abs/2511.10099)
*D Han-Kwan,É Miot,A Moussa,I Moyano*

Main category: math.AP

TL;DR: Uniqueness of Leray solutions for 3D Vlasov-Navier-Stokes system established beyond Osgood class using Cannone-Meyer-Planchon framework, with stability estimate.


<details>
  <summary>Details</summary>
Motivation: To extend uniqueness results for Leray solutions beyond the classical Osgood uniqueness class in the three-dimensional Vlasov-Navier-Stokes system.

Method: Employ the Cannone-Meyer-Planchon class framework for fluid velocity fields, which allows analysis beyond the Osgood uniqueness class.

Result: Successfully established uniqueness of Leray solutions when fluid velocity field belongs to the Cannone-Meyer-Planchon class, and provided a stability estimate.

Conclusion: The Cannone-Meyer-Planchon framework enables proving uniqueness for Leray solutions in 3D Vlasov-Navier-Stokes system beyond traditional Osgood class limitations.

Abstract: We study the problem of uniqueness of Leray solutions to the three-dimensional Vlasov-Navier-Stokes system. We establish uniqueness whenever the fluid velocity field belongs to the Cannone-Meyer-Planchon class, which allows to go beyond the Osgood uniqueness class. A stability estimate in this setting is also provided.

</details>


### [24] [Explicit pulsating fronts and minimal speeds in periodic Fisher-KPP equations](https://arxiv.org/abs/2511.10104)
*Lionel Roques*

Main category: math.AP

TL;DR: Exact pulsating traveling front solution found for Fisher-KPP equation with periodic diffusion and reaction terms through nonlinear transformation.


<details>
  <summary>Details</summary>
Motivation: To study Fisher-KPP equations with spatially periodic diffusion and reaction terms and identify conditions for explicit solutions.

Method: Used nonlinear change of variables to reduce problem to homogeneous Fisher-KPP equation, enabling construction of exact pulsating traveling front.

Result: Derived explicit expression for asymptotic spreading speed and established new asymptotic and comparison results.

Conclusion: Identified class of periodic media allowing closed-form solutions and exact pulsating traveling fronts connecting periodic stationary states.

Abstract: We study a Fisher-KPP equation with spatially periodic diffusion and reaction terms. We identify a class of periodic media for which the equation admits an explicit, closed-form solution. Through a nonlinear change of variables, the problem is reduced to the homogeneous Fisher-KPP equation, allowing us to construct an exact pulsating traveling front that connects the positive periodic stationary state to 0. We also derive an explicit expression for the asymptotic spreading speed and establish new asymptotic and comparison results.

</details>


### [25] [On Rayleigh quotients connected to $p$-Laplace equations with polynomial nonlinearities](https://arxiv.org/abs/2511.10199)
*Vladimir Bobkov,Mieko Tanaka*

Main category: math.AP

TL;DR: The paper establishes a bijection between solutions of a p-Laplacian equation and critical points of a 0-homogeneous Rayleigh quotient. This connection is used to analyze degenerate solutions, ground states, and sign-changing solutions for various parameter regimes.


<details>
  <summary>Details</summary>
Motivation: To understand the structure of solutions to p-Laplacian equations with competing nonlinearities by establishing a fundamental connection between these solutions and critical points of a normalized Rayleigh quotient.

Method: The authors introduce a 0-homogeneous Rayleigh quotient R_α(u) and prove a bijection between W_0^{1,p}(Ω)-solutions of the p-Laplacian equation and properly normalized critical points of R_α. They analyze this bijection for different parameter relationships between p, q, r.

Result: For the convex-concave case (q<p<r), all degenerate solutions correspond to critical points of R_α with α=(r-p)/(r-q). In the subhomogeneous case (q<r≤p), ground states are simple and isolated, and minimizers exhaust sign-constant solutions. In the superhomogeneous case (p<q<r), no sign-changing critical points exist near ground states.

Conclusion: The bijection between p-Laplacian solutions and critical points of the Rayleigh quotient provides a powerful framework for systematically analyzing solution structures across different nonlinear regimes, enabling characterization of degenerate solutions, ground states, and sign-changing behavior.

Abstract: Let $Ω$ be a bounded open set and $p,q,r>1$. The main observation of the present work is the following: $W_0^{1,p}(Ω)$-solutions of the equation $-Δ_p u = μ|u|^{q-2}u + |u|^{r-2}u$ parameterized by $μ$ are in bijection with properly normalized critical points of the $0$-homogeneous Rayleigh type quotient $R_α(u)=\|\nabla u\|_p^p/ (\|u\|_q^{αp} \|u\|_r^{p-αp})$ parameterized by $α$. We study this bijection and properties of $R_α$ for various relations between $p,q,r$. In particular, for the generalized convex-concave problem (the case $q<p<r$) the bijection allows to provide the existence and characterization of all degenerate solutions corresponding to the inflection point of the fibred energy functional: they are critical points of $R_α$ exclusively with $α= (r-p)/(r-q)$. In the subhomogeneous case $q<r \leq p$ and under additional assumptions on $Ω$, the ground state level of $R_α$ is simple and isolated, and minimizers of $R_α$ exhaust the whole set of sign-constant solutions of the corresponding equation. In the superhomogeneous case $p < q<r$, there are no sign-changing critical points in a vicinity of the ground state level of $R_α$.

</details>


### [26] [Observable sets for free Schrödinger equation on combinatorial graphs](https://arxiv.org/abs/2511.10358)
*Zhiqiang Wan,Heng Zhang*

Main category: math.AP

TL;DR: The paper studies observability for the free Schrödinger equation on combinatorial graphs, establishing sharp thresholds for thick sets on 1D lattices, showing finite complements are observable on higher-dimensional lattices, and characterizing observability on finite graphs.


<details>
  <summary>Details</summary>
Motivation: To understand how observability properties of the Schrödinger equation differ between discrete graphs and continuous spaces, particularly identifying critical thresholds where discrete systems behave differently from their continuous counterparts.

Method: Mathematical analysis using spectral theory and combinatorial graph theory, studying the observability inequality for the free Schrödinger equation on various graph structures including 1D lattices, higher-dimensional lattices, and finite graphs.

Result: Found a sharp threshold of γ=1/2 for thick sets on 1D lattice Z, showed complements of finite sets are observable on Z^d, and characterized observability on finite graphs via Laplacian eigenfunctions. Constructed unobservable sets of large density on discrete tori.

Conclusion: Discrete graphs exhibit fundamentally different observability properties from continuous spaces, with critical thresholds that must be attained in discrete settings but not in continuous ones, highlighting the structural differences between discrete and continuous Schrödinger dynamics.

Abstract: We study observability for the free Schrödinger equation $\partial_t u = iΔu$ on combinatorial graphs $G=(\mathcal{V},\mathcal{E})$. A subset $E\subset\mathcal{V}$ is observable at time $T>0$ if there exists $C(T,E)>0$ such that for all $u_0\in l^2(\mathcal{V})$, $$ \|u_0\|_{l^2(\mathcal{V})}^2 \le C(T,E)\int_0^T \|e^{itΔ}u_0\|_{l^2(E)}^2\,dt. $$
  On the one-dimensional lattice $\mathbb{Z}$ we obtain a sharp threshold for thick sets: if $E\subset\mathbb{Z}$ is $γ$-thick with $γ\geq1/2$, then $E$ is observable at some time; conversely, for every $γ<1/2$ there exists a $γ$-thick set that is not observable at any time. This critical threshold marks the exact point where the discrete lattice departs from the real line: on the lattice it must be attained, whereas on $\R$ any $γ$-thick set with $γ>0$ already suffices.
  On $\mathbb{Z}^d$ we show that complements of finite sets are observable at any time $T>0$. This is a similar result to Euclidean space $\R^d$: any set that contains the exterior of a finite ball is observable at any time, in analogy with the free Schrödinger flow on $\mathbb{R}^d$.
  For finite graphs we give an equivalent characterization of observability in terms of the zero sets of Laplacian eigenfunctions. As an application, we construct unobservable sets of large density on discrete tori, in contrast with the continuous torus $\mathbb{T}^d$, where every nonempty open set is observable.

</details>


### [27] [An initial-boundary value problem describing moisture transport in porous media: existence of strong solutions and an error estimate for a finite volume scheme](https://arxiv.org/abs/2511.10378)
*Akiko Morimura,Toyohiko Aiki*

Main category: math.AP

TL;DR: Analysis of moisture transport in porous media using finite volume method with existence proofs and error estimates.


<details>
  <summary>Details</summary>
Motivation: Study mathematical models of moisture transport in porous media to understand fluid dynamics in materials like soils and rocks.

Method: Finite volume method for approximate solutions, using Gagliardo-Nirenberg inequality for error analysis.

Result: Established existence of strong solutions and provided error estimates for the numerical approximations.

Conclusion: The finite volume method is effective for solving moisture transport problems with rigorous error bounds.

Abstract: We consider an initial-boundary value problem motivated by a mathematical model of moisture transport in porous media. We establish the existence of strong solutions and provide an error estimate for the approximate solutions constructed by the finite volume method. In the proof of the error estimate, the Gagliardo--Nirenberg type inequality for the difference between a continuous function and a piecewise constant function plays an important role.

</details>


### [28] [Restriction estimates for 2D surfaces of finite type 3 and applications to dispersive equations](https://arxiv.org/abs/2511.10538)
*Jiajun Wang*

Main category: math.AP

TL;DR: Proves restriction estimates for 2D surfaces (xi1, xi2, xi1^3 ± xi2^3) using Guth's paraboloid and Buschenhenke-Muller-Vargas's hyperboloid results, with applications to discrete nonlinear Schrödinger equations.


<details>
  <summary>Details</summary>
Motivation: To establish restriction estimates for specific 2D surfaces that combine cubic terms, which have applications in analyzing discrete nonlinear Schrödinger equations.

Method: Uses rescaling technique from previous work (LMZ21a) and reduces the problem to known results on perturbed paraboloid (Guth) and perturbed hyperboloid (Buschenhenke-Muller-Vargas).

Result: Successfully proves restriction estimates for the specified 2D surfaces with cubic terms.

Conclusion: The restriction estimates obtained can be applied to provide better analysis for discrete nonlinear Schrödinger equations.

Abstract: In this paper, we prove the restriction estimates for 2D surfaces S := {(xi1, xi2, xi1^3 +/- xi2^3) : (xi1, xi2) in [0,1]^2} by reducing to Guth's result on the perturbed paraboloid and Buschenhenke-Muller-Vargas's result on the perturbed hyperboloid. The method is based on the rescaling technique, developed in [LMZ21a]. Besides, we will use the estimates to give a better analysis for discrete nonlinear Schrodinger equations.

</details>


### [29] [Thin shell limit and the derivation of the viscosity operator on the ellipsoid](https://arxiv.org/abs/2511.10579)
*Chi Hin Chan,Magdalena Czubak,Padi Fuster Aguilera*

Main category: math.AP

TL;DR: Derived four new intrinsic viscosity operators for ellipsoids using thin shell limit heuristic, showing dependence on averaging methods and considering both Navier and Hodge boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To develop new intrinsic viscosity operators for ellipsoids by applying the thin shell limit approach along ellipsoid scaling directions, exploring how different averaging methods affect the results.

Method: Used thin shell limit heuristic along ellipsoid scaling directions, performed asymptotic expansion analysis, considered both homogeneous Navier and Hodge boundary conditions, and derived geometric representations of boundary conditions.

Result: Successfully derived four new candidates for intrinsic viscosity operators on ellipsoids, demonstrated that thin shell limit method depends on the averaging approach used, and obtained geometric interpretations of both boundary conditions.

Conclusion: The thin shell limit approach provides multiple viable candidates for intrinsic viscosity operators on ellipsoids, with results being sensitive to the choice of averaging method, and both Navier and Hodge boundary conditions can be geometrically represented.

Abstract: In this paper we derive four new candidates for an intrinsic viscosity operator on an ellipsoid by using the heuristic of the thin shell limit along the scaling direction of the ellipsoid. We show that the general method of the thin shell limit through the asymptotic expansion depends on the averaging method used. We consider both the homogeneous Navier and Hodge boundary conditions. We also obtain a geometric representation of these two boundary conditions.

</details>


### [30] [Convergence of approximate solutions constructed by the finite volume method for the moisture transport model in porous media](https://arxiv.org/abs/2505.09763)
*Akiko Morimura,Toyohiko Aiki*

Main category: math.AP

TL;DR: Analysis of uniqueness and convergence for nonlinear parabolic equations modeling moisture transport in porous media


<details>
  <summary>Details</summary>
Motivation: The study is motivated by mathematical modeling of moisture transport in porous media, which requires understanding solution properties for practical applications

Method: Uses dual equation method for uniqueness proofs and finite volume method for constructing approximate solutions

Result: Established uniqueness of weak solutions and proved convergence of finite volume approximations

Conclusion: The paper provides rigorous mathematical foundations for moisture transport modeling with proven solution uniqueness and numerical method convergence

Abstract: We consider the initial-boundary value problem for a nonlinear parabolic equation in the one-dimensional interval. This problem is motivated by a mathematical model for moisture transport in porous media. We establish the uniqueness of weak solutions to the problem by using the dual equation method. Moreover, we prove the convergence of approximate solutions constructed with the finite volume method.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [31] [Parallel and GPU accelerated code for phase-field and reaction-diffusion simulations](https://arxiv.org/abs/2511.10508)
*Steven A. Silber,Mikko Karttunen*

Main category: physics.comp-ph

TL;DR: SymPhas 2.0 is a major update to a symbolic algebra simulation framework for phase-field and reaction-diffusion models, featuring compile-time functional differentiation, directional derivatives, tensor expressions, and GPU acceleration with CUDA, achieving up to 1000x speedup over the previous version.


<details>
  <summary>Details</summary>
Motivation: To create a more efficient and scalable framework for phase-field and reaction-diffusion simulations by enabling direct model definition from free-energy functionals and leveraging GPU computing for large-scale simulations.

Method: Uses compile-time symbolic algebra with functional differentiation, introduces directional derivatives, symbolic summation, tensor expressions, and compile-time derived finite difference stencils. Parallelized with MPI for CPUs and CUDA for GPU computing, where symbolic expressions are compiled into optimized CUDA kernels.

Result: Achieved speedups up to ~1000x compared to the first version of SymPhas for large systems (32,768² in 2D and 1,024³ in 3D with double precision) using GPU execution versus multi-threaded CPU execution on a single system.

Conclusion: SymPhas 2.0 establishes itself as a flexible and scalable framework for efficient implementation of phase-field and reaction-diffusion models on GPU-based high-performance computing platforms.

Abstract: We present SymPhas 2.0, a major update of the compile-time symbolic algebra simulation framework SymPhas for phase-field and reaction-diffusion models. This release introduces significant expansions and enhancements that enable the definition of a phase-field model directly from the free-energy functional via compile-time evaluated functional differentiation. It also introduces directional derivatives, symbolic summation, tensor-valued expressions, and compile-time derived finite difference stencils of arbitrary order and accuracy. Furthermore, the code has been parallelized for CPUs with MPI, and GPU computing has been added using CUDA (Compute Unified Device Architecture). For the latter, symbolic expressions are compiled into optimized CUDA kernels, allowing large-scale simulations to execute entirely on the GPU. For large systems ($32,768^2$ in 2D and $1,024^3$ in 3D with double precision), speedups up to $\sim \!\!1,000 \times$ were obtained compared to the first version of SymPhas using multi-threaded CPU execution on a single system. These developments establish SymPhas 2.0 as a flexible and scalable framework for efficient implementation of phase-field and reaction-diffusion models on GPU-based high-performance computing platforms.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [32] [The Data Fusion Labeler (dFL): Challenges and Solutions to Data Harmonization, Labeling, and Provenance in Fusion Energy](https://arxiv.org/abs/2511.09725)
*Craig Michoski,Matthew Waller,Brian Sammuli,Zeyu Li,Tapan Ganatma Nakkina,Raffi Nazikian,Sterling Smith,David Orozco,Dongyang Kuang,Martin Foltin,Erik Olofsson,Mike Fredrickson,Jerry Louis-Jeune,David R. Hatch,Todd A. Oliver,Mitchell Clark,Steph-Yves Louis*

Main category: physics.plasm-ph

TL;DR: The Data Fusion Labeler (dFL) is a unified workflow tool that enables uncertainty-aware data harmonization, schema-compliant fusion, and provenance-rich labeling at scale, reducing analysis time by 50X and improving label quality for fusion energy research.


<details>
  <summary>Details</summary>
Motivation: Fusion energy research faces challenges with integrating heterogeneous, multimodal datasets from diagnostics, control systems, and simulations, requiring new tools to systematically harmonize and extract knowledge across diverse modalities.

Method: dFL performs uncertainty-aware data harmonization, schema-compliant data fusion, and provenance-rich manual/automated labeling within a reproducible, operator-order-aware framework.

Result: Reduces time-to-analysis by greater than 50X (enabling >200 shots/hour vs. handful per day), enhances label quality, enables cross-device comparability, and successfully applied to automated ELM detection and confinement regime classification at DIII-D.

Conclusion: dFL has potential as a core component for data-driven discovery, model validation, and real-time control in future burning plasma devices.

Abstract: Fusion energy research increasingly depends on the ability to integrate heterogeneous, multimodal datasets from high-resolution diagnostics, control systems, and multiscale simulations. The sheer volume and complexity of these datasets demand the development of new tools capable of systematically harmonizing and extracting knowledge across diverse modalities. The Data Fusion Labeler (dFL) is introduced as a unified workflow instrument that performs uncertainty-aware data harmonization, schema-compliant data fusion, and provenance-rich manual and automated labeling at scale. By embedding alignment, normalization, and labeling within a reproducible, operator-order-aware framework, dFL reduces time-to-analysis by greater than 50X (e.g., enabling >200 shots/hour to be consistently labeled rather than a handful per day), enhances label (and subsequently training) quality, and enables cross-device comparability. Case studies from DIII-D demonstrate its application to automated ELM detection and confinement regime classification, illustrating its potential as a core component of data-driven discovery, model validation, and real-time control in future burning plasma devices.

</details>


### [33] [Plasma hydrodynamics from mean force kinetic theory](https://arxiv.org/abs/2511.09786)
*Jarett LeVan,Scott D. Baalrud*

Main category: physics.plasm-ph

TL;DR: Mean force kinetic theory extends hydrodynamics to dense plasmas, accurately predicting transport coefficients up to high Coulomb coupling strengths (Γ≈20), outperforming traditional theories.


<details>
  <summary>Details</summary>
Motivation: To develop a theoretical framework that can accurately predict transport properties in dense plasmas beyond the limitations of traditional kinetic theories, which break down at low coupling strengths.

Method: Used mean force kinetic theory to calculate transport coefficients (electrical conductivity, thermal conductivity, etc.) and validated results against molecular dynamics simulations with repulsive Coulomb interactions.

Result: Excellent agreement found between mean force kinetic theory predictions and molecular dynamics simulations for all transport coefficients up to Coulomb coupling strength Γ≈20, which is over 100 times higher than traditional theory limits.

Conclusion: Mean force kinetic theory successfully extends hydrodynamic descriptions to dense plasma regimes, providing accurate transport coefficient predictions at coupling strengths where conventional theories fail.

Abstract: Mean force kinetic theory is used to evaluate the electrical conductivity, thermal conductivity, electrothermal coefficient, thermoelectric coefficient, and shear viscosity of a two-component (ion-electron) plasma. Results are compared with molecular dynamics simulations. These simulations are made possible by assuming a repulsive Coulomb force for all interactions. Good agreement is found for all coefficients up to a Coulomb coupling strength of $Γ\approx 20$. This is over 100-times larger than the coupling strength at which traditional theories break down. It is concluded that mean force kinetic theory provides a means to extend hydrodynamics to dense plasmas.

</details>


### [34] [Molecular Dynamics Simulation of Hydrodynamic Transport Coefficients in Plasmas](https://arxiv.org/abs/2511.09787)
*Briggs Damman,Jarett LeVan,Scott Baalrud*

Main category: physics.plasm-ph

TL;DR: MD simulations calculate transport coefficients in a two-component plasma using Green-Kubo formalism across coupling strengths Γ=0.01-140. Results validate Chapman-Enskog theory for weak coupling (Γ<0.1) but reveal divergence issues in strong coupling regime.


<details>
  <summary>Details</summary>
Motivation: To compute various transport coefficients (thermal conductivity, electrical conductivity, etc.) in plasmas across different coupling strengths and test theoretical models like Chapman-Enskog against first-principles simulations.

Method: Molecular dynamics simulations using Green-Kubo formalism to calculate transport coefficients over a broad range of Coulomb coupling strength (0.01 ≤ Γ ≤ 140).

Result: Good agreement with Chapman-Enskog theory for weak coupling (Γ ≲ 0.1), but only when careful attention is paid to linear constitutive relations. In strong coupling (Γ ≫ 1), potential and virial components diverge, making only kinetic components meaningful.

Conclusion: Standard Green-Kubo expressions require careful interpretation as they combine multiple transport coefficients. For strongly coupled plasmas, only kinetic components of transport coefficients remain physically meaningful due to divergence of potential/virial terms.

Abstract: Molecular dynamics (MD) simulations are used to calculate transport coefficients in a two-component plasma interacting through a repulsive Coulomb potential. The thermal conductivity, electrical conductivity, electrothermal coefficient, thermoelectric coefficient, and shear viscosity are computed using the Green-Kubo formalism over a broad range of Coulomb coupling strength, $0.01 \leq Γ\leq 140$. Emphasis is placed on testing standard results of the Chapman-Enskog solution in the weakly coupled regime ($Γ\ll 1$) using these first-principles simulations. As expected, the results show good agreement for $Γ\lesssim 0.1$. However, this agreement is only possible if careful attention is paid to the definitions of linear constitutive relations in each of the theoretical models, a point that is often overlooked. For example, the standard Green-Kubo expression for thermal conductivity is a linear combination of thermal conductivity, electrothermal and thermoelectric coefficients computed in the Chapman-Enskog formalism. Meaningful results for electrical conductivity are obtained over the full range of coupling strengths explored, but it is shown that potential and virial components of the other transport coefficients diverge in the strongly coupled regime ($Γ\gg 1$). In this regime, only the kinetic components of the transport coefficients are meaningful for a classical plasma.

</details>


### [35] [$π$-PIC: a framework for modular particle-in-cell developments and simulations](https://arxiv.org/abs/2511.09950)
*Frida Brogren,Christoffer Olofsson,Joel Magnusson,Arkady Gonoskov*

Main category: physics.plasm-ph

TL;DR: A Python-controlled framework for particle-in-cell (PIC) methods that unifies interfaces for testing and comparing PIC solvers and extensions, enabling easier adoption of advanced PIC developments.


<details>
  <summary>Details</summary>
Motivation: To address limitations in standard PIC methods and facilitate the dissemination of next-generation PIC developments that preserve conserved quantities and enable more efficient simulations.

Method: Developed a unified Python-controlled framework with interfaces for accommodating, cross-testing, and comparing PIC solvers and extensions written in Python, C++, or Fortran.

Result: Successfully implemented and tested several PIC solvers and extensions capable of managing QED processes, moving-window techniques, and tight focusing of laser pulses.

Conclusion: The proposed framework demonstrates flexibility in integrating various PIC developments and can stimulate wider adoption of advanced PIC methods across different computing environments.

Abstract: Recently proposed modifications of the standard particle-in-cell (PIC) method resolve long-standing limitations such as exact preservation of physically conserved quantities and unbiased ensemble down-sampling. Such advances pave the way for next-generation PIC codes capable of using lower resolution and fewer particles per cell, enabling interactive studies on personal computers and facilitating large-scale parameter scans on supercomputers. Here, we present a Python-controlled framework designed to stimulate the dissemination and adoption of novel PIC developments by providing unified interfaces for accommodation, cross-testing, and comparison of PIC solvers and extensions written in Python or low-level languages like C++ and Fortran. To demonstrate flexibility of proposed interfaces we present and test implementations of several PIC solvers, as well as of extensions that is capable of managing QED processes, moving-window, and tight focusing of laser pulses.

</details>


### [36] [Tailored Three Dimensional Betatron Dynamics in UltraStable Hybrid Laser Plasma RF Accelerators](https://arxiv.org/abs/2511.10096)
*A. A. Molavi Choobini,M. Shahmansouri*

Main category: physics.plasm-ph

TL;DR: Hybrid laser plasma RF accelerators enable precise control over electron beam dynamics through RF field manipulation, achieving enhanced focusing, reduced emittance, and controlled polarization states.


<details>
  <summary>Details</summary>
Motivation: To understand and control transverse beam dynamics, betatron polarization, and radiation reaction in ultra-relativistic electron bunches within hybrid laser plasma RF accelerators for improved beam quality and stability.

Method: Combined analytical models of plasma wakefield modulation, RF-driven oscillations, and quantum-corrected radiation reaction with fully self-consistent 3D particle-in-cell simulations using EPOCH code.

Result: RF parameters enable precise control over transverse focusing, betatron amplitudes, and polarization. Resonant alignment amplifies transverse excursions while damping parasitic oscillations, reducing emittance and energy losses. Strong phase sensitivity governs beam evolution.

Conclusion: Hybrid laser plasma RF systems provide comprehensive control over nonlinear, resonant, and damping phenomena in ultra-relativistic electron beams, enabling manipulation of transverse, longitudinal, and polarization dynamics.

Abstract: The detailed theoretical and numerical investigation of hybrid laser plasma RF accelerators, elucidating the mechanisms governing transverse beam dynamics, betatron polarization, and radiation reaction in ultra-relativistic electron bunches is presented. This framework combines analytical models of spatiotemporal plasma wakefield modulation, phase-dependent RF-driven oscillations, and quantum-corrected Landau Lifshitz radiation reaction with fully self-consistent 3D particle in cell simulations using EPOCH. The results demonstrate that RF amplitude, frequency, and phase enable precise control over transverse focusing strengths, betatron oscillation amplitudes, and polarization states. Resonant alignment between RF fields and natural betatron frequencies amplifies transverse excursions while damping parasitic oscillations through enhanced focusing gradients and radiation reaction, yielding reductions in emittance and mitigation of synchrotron-like energy losses. Stability maps and 3D force landscapes reveal strong phase sensitivity, where initial conditions and RF component ratios govern the temporal evolution of betatron amplitudes, and longitudinal field gradients modulate γ growth rates. These findings provide a comprehensive picture of nonlinear, resonant, and damping phenomena in hybrid laser plasma RF systems, highlighting the full spectrum of controllable transverse, longitudinal, and polarization dynamics in ultra relativistic electron beams.

</details>


### [37] [Microscopy X-ray Imaging enriched with Small Angle X-ray Scattering for few nanometer resolution reveals shock waves and compression in intense short pulse laser irradiation of solids](https://arxiv.org/abs/2511.10127)
*Thomas Kluge,Arthur Hirsch-Passicos,Jannis Schulz,Mungo Frost,Eric Galtier,Maxence Gauthier,Jörg Grenzer,Christian Gutt,Lingen Huang,Uwe Hübner,Megan Ikeya,Hae Ja Lee,Dimitri Khaghani,Willow Moon Martin,Brian Edward Marré,Motoaki Nakatsutsumi,Paweł Ordyna,Franziska-Luise Paschke-Brühl,Alexander Pelka,Lisa Randolph,Hans-Peter Schlenvoigt,Christopher Schoenwaelder,Michal Šmíd,Long Yang,Ulrich Schramm,Thomas E. Cowan*

Main category: physics.plasm-ph

TL;DR: Combined X-ray imaging and small-angle X-ray scattering method enables simultaneous observation of macroscopic geometry and nanometer-scale structure in laser-driven shock compression of copper wires.


<details>
  <summary>Details</summary>
Motivation: To understand how laser pulses compress solids into high-energy-density states, requiring diagnostics that bridge the gap between macroscopic geometry and nanometer-scale structure.

Method: Used X-ray radiography (XRM) and small-angle X-ray scattering (SAXS) at LCLS to irradiate 25-micrometer copper wires with 45-fs laser pulses while probing with 8.2-keV XFEL pulses.

Result: Revealed that an initially smooth compression steepens into a nanometer-sharp shock front after ~18 ps, reaching 25 km/s velocity and lateral width of tens of micrometers.

Conclusion: The integrated XRM-SAXS method establishes a quantitative, multiscale diagnostic for laser-driven shocks in dense plasmas relevant to fusion, warm dense matter, and planetary physics.

Abstract: Understanding how laser pulses compress solids into high-energy-density states requires diagnostics that simultaneously resolve macroscopic geometry and nanometer-scale structure. Here we present a combined X-ray imaging (XRM) and small-angle X-ray scattering (SAXS) approach that bridges this diagnostic gap. Using the Matter in Extreme Conditions end station at LCLS, we irradiated 25-micrometer copper wires with 45-fs, 0.9-J, 800-nm pulses at 3.5e19 W/cm2 while probing with 8.2-keV XFEL pulses. XRM visualizes the evolution of ablation, compression, and inward-propagating fronts with about 200-nm resolution, while SAXS quantifies their nanometer-scale sharpness through the time-resolved evolution of scattering streaks. The joint analysis reveals that an initially smooth compression steepens into a nanometer-sharp shock front after roughly 18 ps, consistent with an analytical steepening model and hydrodynamic simulations. The front reaches a velocity of about 25 km/s and a lateral width of several tens of micrometers, demonstrating for the first time the direct observation of shock formation and decay at solid density with few-nanometer precision. This integrated XRM-SAXS method establishes a quantitative, multiscale diagnostic of laser-driven shocks in dense plasmas relevant to inertial confinement fusion, warm dense matter, and planetary physics.

</details>


### [38] [Data-driven multi-species heat flux closures for two-stream-unstable plasmas with nonlinear sparse regression](https://arxiv.org/abs/2511.10147)
*Emil R. Ingelsten,Madox C. McGrae-Menge,E. Paulo Alves,Istvan Pusztai*

Main category: physics.plasm-ph

TL;DR: This paper extends a previously discovered single-species heat flux closure model to multi-species plasma modeling using data-driven methods, achieving 80-90% accuracy in heat flux predictions.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency in plasma physics while maintaining accuracy, particularly for multi-scale collisionless processes where traditional fluid models struggle due to inadequate closures.

Method: Used sparse regression and neural networks on OSIRIS particle-in-cell simulation data to generalize a six-term closure model from single- to multi-species, estimating coefficients from box-averaged fluid quantities.

Result: The models predict species-specific heat flux with 80-90% accuracy and account for 85-95% of pressure rate changes, with comparison to multi-species linear collisionless theory.

Conclusion: Data-driven methods successfully generalize heat flux closures to multi-species plasma modeling, providing accurate predictions while maintaining computational efficiency.

Abstract: The dual aims of accuracy and computational efficiency in computational plasma physics lend themselves well to the use of fluid models. The first of these goals, however, is only satisfied for such models insofar as the utilized closure can capture the neglected kinetic physics -- something which has proven challenging for multi-scale collisionless processes. In a recent article [E. R. Ingelsten et al. (2025) J. Plasma Phys. 91 E64], we used the data-driven method of sparse regression to discover a novel heat flux closure for electrostatic phenomena. Here, we generalize the six-term closure model found in that work from single- to multi-species modeling. Using data from OSIRIS particle-in-cell simulations over a range of initial conditions, we then demonstrate how the unknown coefficients in front of the three most important terms in the closure can be estimated from box-averaged fluid quantities. Both neural networks and a newly developed framework for nonlinear sparse regression are showcased. The resulting models predict the heat flux for each species with a typical accuracy of 80-90 % and regularly account for 85-95 % of the rate of change in the pressure. The models are also compared with results from multi-species linear collisionless theory.

</details>


### [39] [Self-organisation through layering of $β$-plane like turbulence in plasmas and geophysical fluids](https://arxiv.org/abs/2511.10438)
*P. L. Guillon,G. Dif-Pradalier,Y. Sarazin,D. W. Hughes,Ö. D. Gürcan*

Main category: physics.plasm-ph

TL;DR: This paper studies staircase formation and layering in plasma and geophysical fluid models, comparing turbulent self-organization with different energy production mechanisms (forcing vs. linear instability) and zonal flow responses.


<details>
  <summary>Details</summary>
Motivation: To understand how different mechanisms of free energy production and zonal flow responses affect turbulent self-organization and nonlinear saturation in plasmas and geophysical fluids.

Method: Analyzed staircase formation in standard and modified Charney-Hasegawa-Mima equations with stochastic forcing, plus two instability-driven models (plasma and geophysical contexts) using potential vorticity conserving models.

Result: β-plane turbulence with standard zonal response gradually forms large-scale elliptic zonal structures that merge progressively. Plasma systems with modified zonal response rapidly form straight, stationary jets of well-defined size. Instability-driven plasma systems show phase transitions between zonal flow and eddy dominated states.

Conclusion: Different zonal flow responses lead to distinct staircase formation patterns: gradual merging of elliptic structures in geophysical systems vs. rapid formation of straight jets in plasma systems, with plasma systems exhibiting phase transitions between flow regimes.

Abstract: Staircase formation and layering is studied in simplified, potential vorticity conserving models of plasmas and geophysical fluids, by investigating turbulent self-organisation and nonlinear saturation with different mechanisms of free energy production -- forcing or linear instability -- and with standard or modified zonal flow responses. To this end, staircase formation in both the standard and modified Charney-Hasegawa-Mima equations with stochastic forcing, along with two different simple instability driven models -- one from a plasma and from a geophysical context -- are studied and compared. In these studies, it is observed that $β$-plane turbulence that does not distinguish between zonal and non-zonal perturbations (i.e., standard zonal response) gradually forms large-scale, elliptic zonal structures that merge progressively, regardless of whether it is driven by forcing (though it should be slow enough to allow wave couplings) or by the baroclinic instability, using for example a two-layer model. Conversely, the plasma system, with its modified zonal response, can rapidly form straight, stationary jets of well-defined size, again regardless of the way it is driven: by stochastic forcing or by the dissipative drift instability. Furthermore, the instability-driven plasma system exhibits a phase transition between a zonal flow dominated state and an eddy dominated state. In both states, saturation is possible without large-scale friction.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [40] [SeQuant Framework for Symbolic and Numerical Tensor Algebra. I. Core Capabilities](https://arxiv.org/abs/2511.09943)
*Bimal Gaudel,Robert G. Adam,Ajay Melekamburath,Conner Masteran,Nakul Teke,Azam Besharatnik,Andreas Köhn,Edward F. Valeev*

Main category: cs.MS

TL;DR: SeQuant is an open-source library for symbolic tensor algebra that uses graph-theoretic tensor network canonicalization to handle symmetries efficiently, supporting both commutative and non-commutative rings.


<details>
  <summary>Details</summary>
Motivation: To provide efficient symbolic algebra for tensors with symmetries, handling both conventional tensor expressions and complex scenarios like noncovariant networks and nested tensors that arise in quantum simulation and data science.

Method: Uses a graph-theoretic tensor network canonicalizer that is faster than group-theoretic approaches, supports Wick's theorem optimization, and includes compiler-like components for numerical evaluation with external tensor algebra frameworks.

Result: Developed a functional library capable of handling complex tensor networks with symmetries, supporting both symbolic manipulation and numerical evaluation through integration with external numerical frameworks.

Conclusion: SeQuant successfully bridges symbolic manipulation and numerical evaluation for tensor algebra, providing efficient handling of tensor networks with symmetries and supporting modern applications in quantum simulation and data science.

Abstract: SeQuant is an open-source library for symbolic algebra of tensors over commutative (scalar) and non-commutative (operator) rings. The key innovation supporting most of its functionality is a graph-theoretic tensor network (TN) canonicalizer that can handle tensor networks with symmetries faster than their standard group-theoretic counterparts. The TN canonicalizer is used for routine simplification of conventional tensor expressions, for optimizing application of Wick's theorem (used to canonicalize products of tensors over operator fields), and for manipulation of the intermediate representation leading to the numerical evaluation. Notable features of SeQuant include support for noncovariant tensor networks (which often arise from tensor decompositions) and for tensors with modes that depend parametrically on indices of other tensor modes (such dependencies between degrees of freedom are naturally viewed as nesting of tensors, "tensors of tensors" arising in block-wise data compressions in data science and modern quantum simulation). SeQuant blurs the line between pure symbolic manipulation/code generation and numerical evaluation by including compiler-like components to optimize and directly interpret tensor expressions using external numerical tensor algebra frameworks. The SeQuant source code is available at https://github.com/ValeevGroup/SeQuant.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [41] [Learning of Statistical Field Theories](https://arxiv.org/abs/2511.09859)
*Shreya Shukla,Abhijith Jayakumar,Andrey Y. Lokhov*

Main category: cond-mat.stat-mech

TL;DR: Proposes an inverse approach to recover microscopic couplings from data for statistical field theories, accommodating discrete, continuous, and hybrid variables, with applications to various benchmark systems and non-perturbative renormalization-group flows.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem in statistical field theories by recovering microscopic couplings directly from data, complementing the traditional forward approach that is often computationally intractable.

Method: An approach that uniformly accommodates systems with discrete, continuous, and hybrid variables, demonstrated on benchmark systems and iterated under coarse-graining to reconstruct renormalization-group flows.

Result: Accurate parameter recovery in several benchmark systems including Wegner's Ising gauge theory, φ⁴ theory, Schwinger and Sine-Gordon models, and mixed spin-gauge systems. Also addresses realistic settings where full gauge configurations are unavailable.

Conclusion: The methodology provides direct access to phase boundaries, fixed points, and emergent interactions without relying on perturbation theory, and is anticipated to be widely useful for practical learning of field theories in strongly coupled regimes.

Abstract: Recovering microscopic couplings directly from data provides a route to solving the inverse problem in statistical field theories, one that complements the traditional-often computationally intractable-forward approach of predicting observables from an action or Hamiltonian. Here, we propose an approach for the inverse problem that uniformly accommodates systems with discrete, continuous, and hybrid variables. We demonstrate accurate parameter recovery in several benchmark systems-including Wegner's Ising gauge theory, $φ^4$ theory, Schwinger and Sine-Gordon models, and mixed spin-gauge systems, and show how iterating the procedure under coarse-graining reconstructs full non-perturbative renormalization-group flows. This gives direct access to phase boundaries, fixed points, and emergent interactions without relying on perturbation theory. We also address a realistic setting where full gauge configurations may be unavailable, and reformulate learning algorithms for multiple field theories so that they are recovered directly using observables such as correlations from scattering data or quantum simulators. We anticipate that our methodology will find widespread use in practical learning of field theories in strongly coupled regimes where analytical tools might fail.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [42] [Optimal Interpolation of Entanglement Purification Protocols](https://arxiv.org/abs/2511.09657)
*Matthew Barber,Stefano Pirandola*

Main category: quant-ph

TL;DR: The paper presents a method to optimize entanglement purification by randomly selecting protocols from a distribution, achieving better rate-fidelity trade-offs than individual protocols alone.


<details>
  <summary>Details</summary>
Motivation: To overcome the trade-off between rate and fidelity in entanglement purification protocols by combining multiple protocols probabilistically.

Method: Randomly choosing purification protocols according to an optimized probability distribution for each entangled pair production, considering both asymptotic and finite-size regimes.

Result: The approach achieves rates and fidelities not possible with any single protocol, with optimization methods developed for both infinite and finite input pair scenarios.

Conclusion: Probabilistic protocol selection enables superior entanglement purification performance by breaking the conventional rate-fidelity trade-off limitations.

Abstract: Bipartite entanglement purification is the conversion of copies of weakly entangled pairs shared between two separated parties into a smaller number of strongly entangled shared pairs using only local operations and classical communication. Choosing between different entanglement purification protocols generally involves weighing up a trade-off between the ratio of strongly entangled pairs produced to weakly entangled pairs consumed, which we call the rate of the protocol, and the degree of the entanglement of the strongly entangled pairs, typically measured by the fidelity of those pairs to maximally entangled states. By randomly choosing a protocol according to a probability distribution over a list of protocols for each pair we want to produce, we can achieve rates and fidelities not achieved by any of the original protocols. Here, we show how to choose this distribution to maximise the rate at which we produce qubit pairs with a given fidelity to a Bell state or, equivalently, to maximise the fidelity to a Bell state of the qubit pairs produced at a given rate. We investigate both the asymptotic case, where the number of initial pairs goes to infinity, and the finite-size regime, where protocols are restricted to a finite number of weakly entangled pairs.

</details>


### [43] [Constrained Shadow Tomography for Molecular Simulation on Quantum Devices](https://arxiv.org/abs/2511.09717)
*Irma Avdic,Yuchen Wang,Michael Rose,Lillian I. Payne Torres,Anna O. Schouten,Kevin J. Sung,David A. Mazziotti*

Main category: quant-ph

TL;DR: A bi-objective semidefinite programming approach for constrained shadow tomography that reconstructs the 2-RDM from noisy shadow data using N-representability constraints and nuclear-norm regularization.


<details>
  <summary>Details</summary>
Motivation: Quantum state tomography has exponential measurement and computational demands that limit scalability, motivating efficient alternatives like classical shadows for predicting observables from randomized measurements.

Method: Bi-objective semidefinite programming integrating N-representability constraints and nuclear-norm regularization to reconstruct 2-RDM from noisy shadow data, balancing fidelity to measurements with energy minimization.

Result: Numerical and hardware results show significant improvements in accuracy, noise resilience, and scalability for fermionic state reconstruction in quantum simulations.

Conclusion: The approach provides a robust foundation for physically consistent fermionic state reconstruction in realistic quantum simulations by mitigating noise and sampling errors while enforcing physical consistency.

Abstract: Quantum state tomography is a fundamental task in quantum information science, enabling detailed characterization of correlations, entanglement, and electronic structure in quantum systems. However, its exponential measurement and computational demands limit scalability, motivating efficient alternatives such as classical shadows, which enable accurate prediction of many observables from randomized measurements. In this work, we introduce a bi-objective semidefinite programming approach for constrained shadow tomography, designed to reconstruct the two-particle reduced density matrix (2-RDM) from noisy or incomplete shadow data. By integrating $N$-representability constraints and nuclear-norm regularization into the optimization, the method builds an $N$-representable 2-RDM that balances fidelity to the shadow measurements with energy minimization. This unified framework mitigates noise and sampling errors while enforcing physical consistency in the reconstructed states. Numerical and hardware results demonstrate that the approach significantly improves accuracy, noise resilience, and scalability, providing a robust foundation for physically consistent fermionic state reconstruction in realistic quantum simulations.

</details>


### [44] [Quantum Period-Finding using One-Qubit Reduced Density Matrices](https://arxiv.org/abs/2511.09896)
*Marco Bernardi*

Main category: quant-ph

TL;DR: The paper explores using single-qubit reduced density matrices (1-RDMs) instead of full quantum circuit measurements to extract periods in quantum period-finding algorithms, showing that O(n) one-qubit marginals contain sufficient information typically obtained from O(2^n) bit strings.


<details>
  <summary>Details</summary>
Motivation: To develop an alternative approach to quantum period-finding that uses local quantum information (one-qubit marginals) rather than global measurements of the entire quantum circuit output, potentially enabling more efficient period extraction.

Method: Used state-vector simulations to compute 1-RDMs of QPF circuits for generic periodic functions, analyzed patterns in these 1-RDMs as functions of period, and employed numerical root-finding to extract periods from the 1-RDM data.

Result: Demonstrated that distinctive patterns in 1-RDMs allow reconstruction of unknown periods, showing that O(n) one-qubit marginals contain enough information typically obtained from sampling O(2^n) bit strings.

Conclusion: The approach represents a "compression" of QPF information and motivates development of approximate simulations of reduced density matrices for novel period-finding algorithms.

Abstract: The quantum period-finding (QPF) algorithm can compute the period of a function exponentially faster than the best-known classical algorithm. In standard QPF, the output state has a primary contribution from $r$ high-probability bit strings, where $r$ is the period. Measurement of this state, combined with continued fraction analysis, reveals the unknown period. Here, we explore a different approach to QPF, where the period is obtained from single-qubit quantities $-$ specifically, the set of one-qubit reduced density matrices (1-RDMs) $-$ rather than the output bit strings of the entire quantum circuit. Using state-vector simulations, we compute the 1-RDMs of the QPF circuit for a generic periodic function. Analysis of these 1-RDMs as a function of period reveals distinctive patterns, which allows us to obtain the unknown period from the 1-RDMs using a numerical root-finding approach. Our results show that the 1-RDMs $-$ a set of $O(n)$ one-qubit marginals $-$ contain enough information to reconstruct the period, which is typically obtained by sampling the space of $O(2^n)$ bit strings. Conceptually, this can be viewed as a "compression" of the information in the QPF algorithm, which enables period-finding from $n$ one-qubit marginals. Our results motivate the development of approximate simulations of reduced density matrices to design novel period-finding algorithms.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [45] [Collisional and magnetic effects on the polarization of the solar oxygen infrared triplet](https://arxiv.org/abs/2511.10476)
*Moncef Derouich,Saleh Qutub*

Main category: astro-ph.SR

TL;DR: The paper investigates how collisions and magnetic fields affect polarization in the O I IR triplet, finding that atomic alignment is suppressed in the photosphere but persists in the chromosphere.


<details>
  <summary>Details</summary>
Motivation: To understand the combined effects of collisional and magnetic depolarization on the alignment of O I levels and polarization of the O I IR triplet, which is important for diagnosing solar atmospheric conditions.

Method: First comprehensive calculation of collisional depolarization and polarization transfer rates for O I energy levels, incorporated into a multi-level atomic model with statistical equilibrium equations.

Result: Elastic collisions with hydrogen and magnetic fields stronger than 20 G efficiently suppress atomic alignment in deep photosphere (n_H > 10^16 cm^-3), but polarization persists in chromosphere due to lower hydrogen density.

Conclusion: Results support chromospheric origin for O I IR triplet polarization signals; future studies need accurate non-LTE radiative transfer with reliable collisional rates for consistent modeling.

Abstract: Context: The scattering polarization of the infrared (IR) triplet of neutral oxygen (O\,\textsc{i}) near 777\,nm provides a powerful diagnostic of solar atmospheric conditions. However, interpreting such polarization requires a rigorous treatment of isotropic depolarizing collisions between O\,\textsc{i} atoms and neutral hydrogen.
  Aims: We aim to investigate the combined effects of collisional and magnetic depolarization in shaping the alignment of O\,\textsc{i} levels (and thus the polarization of the O\,\textsc{i} IR triplet).
  Methods: We compute, for the first time, a comprehensive set of collisional depolarization and polarization transfer rates for the relevant O\,\textsc{i} energy levels. These rates are incorporated into a multi-level atomic model, and the statistical equilibrium equations (SEE) are solved to quantify the impact of collisions and magnetic fields on atomic alignment.
  Results: Our calculations indicate that elastic collisions with neutral hydrogen, together with the Hanle effect of turbulent magnetic fields stronger than about 20 G, efficiently suppress the bulk of the atomic alignment in deep photospheric conditions where hydrogen densities exceed $n_{\mathrm{H}} \sim 10^{16}$ cm$^{-3}$. In the chromosphere, however, the lower hydrogen density weakens collisional depolarization, allowing polarization to persist.
  Conclusions: Our results are consistent with a chromospheric origin for the linear polarization signals of the O I IR triplet. Future studies should combine accurate non-LTE radiative transfer with reliable collisional rates in order to achieve fully consistent modeling.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [46] [Effect of Concentration Fluctuations on Material Properties of Disordered Alloys](https://arxiv.org/abs/2511.10259)
*Han-Pu Liang,Chuan-Nan Li,Xin-Ru Tang,Xun Xu,Chen Qiu,Qiu-Shi Huang,Su-Huai Wei*

Main category: cond-mat.mtrl-sci

TL;DR: Standard SQS calculations underestimate alloy bandgaps due to rare defect-like configurations. A density-of-states fitting method is proposed to extract bandgaps from majority configurations, resolving discrepancies with experiments.


<details>
  <summary>Details</summary>
Motivation: To address the underestimation of semiconductor bandgaps in disordered alloy calculations using SQS methods, where rare local concentration fluctuations create defect-like configurations that dominate conventional bandgap definitions.

Method: Proposed a density-of-states fitting (DOSF) method to extract alloy bandgaps from majority configurations rather than using conventional energy difference between occupied and unoccupied states, which is affected by localized wavefunctions from rare motifs.

Result: The DOSF approach successfully resolves the underestimation issue in alloy bandgap calculations and provides results consistent with experimental measurements, overcoming limitations of standard SQS methods.

Conclusion: The DOSF method provides a reliable approach for calculating electronic properties of disordered semiconductor alloys, and similar approaches should be developed for other material properties affected by localized alloy wavefunctions.

Abstract: Alloying compound AX with another compound BX is widely used to tune material properties. For disordered alloys, due to the lack of periodicity, it has been challenging to calculate and study their material properties. Special quasi-random structure (SQS) method has been developed and widely used to treat this issue by matching averaged atomic correlation functions to those of ideal random alloys, enabling accurate predictions of macroscopic material properties such as total energy and volume. However, in AxB1-x alloys, statistically allowed local concentration fluctuations can give rise to defect-like minority configurations, such as bulk-like AX or BX regions in the extreme, which could strongly affect calculation of some of the material properties such as semiconductor bandgap, if it is not defined properly, leading to significant discrepancies between theory and experiment. In this work, taking the bandgap as an example, we demonstrate that the calculated alloy bandgap can be significantly underestimated in standard SQS calculations when the SQS cell size is increased to improve the structural model and the bandgap is defined conventionally as the energy difference between the lowest unoccupied state and the highest occupied state, because the rare event motifs can lead to wavefunction localization and become the dominant factor in determining the "bandgap", contrary to experiment. To be consistent with experiment, we show that the bandgap of the alloy should be extracted from the majority configurations using a density-of-states fitting (DOSF) method. This DOSF approach resolves the long-standing issue of calculating electronic structure of disordered semiconductor alloys. Similar approaches should also be developed to treat material properties that depends on localized alloy wavefunctions.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [47] [Kinetic Theory with Fluctuations: Strong Well-Posedness of the Vlasov-Fokker-Planck-Dean-Kawasaki System](https://arxiv.org/abs/2511.10194)
*Zimo Hao,Zhengyan Wu,Johannes Zimmer*

Main category: math.PR

TL;DR: Existence and uniqueness of renormalized kinetic solutions for the Vlasov-Fokker-Planck-Dean-Kawasaki equation with correlated noise in bounded nonlocal interactions and square-root diffusion.


<details>
  <summary>Details</summary>
Motivation: To establish well-posedness for the VFPDK equation, which emerges as a fluctuating mean-field limit of second-order Newtonian particle systems with correlated noise.

Method: Combination of kinetic semigroup estimates with the framework of renormalized kinetic solutions.

Result: Proved existence and uniqueness of renormalized kinetic solutions for the VFPDK equation with bounded nonlocal interactions and square-root diffusion coefficient.

Conclusion: The paper successfully establishes well-posedness for the VFPDK equation with correlated noise through a novel analytical approach.

Abstract: We establish the well-posedness of the Vlasov-Fokker-Planck-Dean-Kawasaki (VFPDK) equation with correlated noise, which arises as a fluctuating mean-field limit of second-order Newtonian particle systems. We focus on the case of bounded nonlocal interactions and a diffusion coefficient of square-root type. In this setting, we prove existence and uniqueness of renormalized kinetic solutions. The proof relies on a novel combination of kinetic semigroup estimates with the framework of renormalized kinetic solutions.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [48] [Dynamical functionals on ancient ARF Ricci flows](https://arxiv.org/abs/2511.10460)
*Isaac M. Lopez,Rio Schillmoeller*

Main category: math.DG

TL;DR: The paper introduces a dynamical energy functional for compact ancient asymptotically Ricci-flat Ricci flows using conjugate heat flows, establishes breather-type rigidity, and provides local eigenvalue estimates.


<details>
  <summary>Details</summary>
Motivation: To develop analytical tools for studying ancient Ricci flows with asymptotically Ricci-flat behavior, motivated by previous work of Colding and Minicozzi on geometric flows.

Method: Uses limits of conjugate heat flows to construct a dynamical energy functional, analyzes its properties including steady Ricci breather-type rigidity, and derives local eigenvalue estimates for normalized Ricci flows coupled with conjugate heat flows.

Result: The introduced functional bounds the ordinary λ-functional while preserving its key properties, and local eigenvalue estimates are established for the coupled system.

Conclusion: The dynamical energy functional provides a useful analytical framework for studying ancient Ricci flows with specific asymptotic behavior, offering new rigidity results and eigenvalue estimates.

Abstract: We introduce a dynamical energy functional on compact ancient asymptotically Ricci-flat Ricci flows with modest decay using limits of conjugate heat flows. This functional satisfies a steady Ricci breather-type rigidity and provides an upper bound for the ordinary $λ$-functional while retaining many of its properties. In addition, motivated by work of Colding and Minicozzi, we derive local eigenvalue estimates for normalized Ricci flows coupled with conjugate heat flows.

</details>


### [49] [On smooth approximation of integral cycles mod 2](https://arxiv.org/abs/2511.10545)
*Gianmarco Caldini*

Main category: math.DG

TL;DR: This paper proves that mod 2 integral cycles in Riemannian manifolds can be approximated by smooth submanifolds with nearly the same area, with singularities only in codimension 3 or better depending on the cycle's codimension.


<details>
  <summary>Details</summary>
Motivation: To establish a smooth approximation theorem for unoriented (mod 2) integral cycles, extending previous results for oriented cycles and providing better control over singular sets.

Method: The authors develop mathematical techniques to approximate mod 2 integral cycles with smooth submanifolds, analyzing the structure of singular sets and refining estimates based on codimension.

Result: Every mod 2 integral cycle can be approximated by a smooth submanifold with nearly identical area, with singularities confined to codimension 3 or higher, and can be made completely smooth if the homology class admits a smooth embedded representative.

Conclusion: This work completes the smooth approximation theory for integral cycles by providing the unoriented version, with improved singularity control that depends on the cycle's codimension.

Abstract: We prove that every mod 2 integral cycle $T$ in a Riemannian manifold $\mathcal{M}$ can be approximated in flat norm by a cycle which is a smooth submanifold $Σ$ of nearly the same area, up to a singular set of codimension 3; in addition, this estimate on the singular set can be refined depending on the codimension of the cycle. Moreover, if the mod 2 homology class $τ$ admits a smooth embedded representative, then $Σ$ can be chosen free of singularities. This article provides the unoriented version of the smooth approximation theorem for integral cycles.

</details>


### [50] [Sign-changing solutions to the Escobar problem on manifolds with boundary](https://arxiv.org/abs/2511.10553)
*Mónica Clapp,Benedetta Pellacci,Angela Pistoia*

Main category: math.DG

TL;DR: Existence of least-energy sign-changing solutions to Escobar's conformal problem in two cases: scalar-flat problem and minimal boundary problem, particularly for manifolds with nonumbilic boundary points when n≥7.


<details>
  <summary>Details</summary>
Motivation: While positive solutions to Escobar's conformal problem are well understood, the existence of sign-changing (nodal) solutions remains largely open and unexplored.

Method: Variational approach using analysis of conformal invariants and sharp energy estimates derived from Escobar's work.

Result: Proved existence of least-energy nodal solutions for both scalar-flat and minimal boundary problems when n≥7 and manifold has nonumbilic boundary point. Also showed minimal boundary problem has infinitely many sign-changing solutions on unit ball when n≥5.

Conclusion: The study successfully establishes existence of sign-changing solutions in previously unexplored cases of Escobar's conformal problem, opening new directions in the analysis of nodal solutions for geometric boundary value problems.

Abstract: Let $(M, g)$ be a $n-$dimensional compact Riemannian manifold with boundary. The Escobar problem concerning the existence of a metric conformally equivalent to $g$ having constant scalar curvature on $M$ and constant mean curvature on its boundary is equivalent, in analytic terms, to finding a positive solution to a nonlinear boundary-value problem with critical growth. While the existence of positive solutions to this problem is by now well understood, the existence of sign-changing (nodal) solutions remains largely open. In this work we establish the existence of least-energy sign-changing solutions in two particular cases: the scalar-flat problem, where the scalar curvature on $M$ is zero and the mean curvature of its boundary is constant, and the minimal boundary problem, where the mean curvature of the boundary vanishes and the scalar curvature of $M$ is constant. More precisely, we prove that if $n\ge7$ and $M$ has a nonumbilic boundary point, then both problems admit least-energy nodal solutions. In addition, we show that when $n\ge5$, the minimal boundary problem possesses infinitely many sign-changing solutions on the unit ball. Our approach is variational and relies on the analysis of suitable conformal invariants and sharp energy estimates derived from Escobar's work.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [51] [Mitigating numerical dissipation in simulations of subsonic turbulent flows](https://arxiv.org/abs/2511.09806)
*James Watt,Christoph Federrath,Claudius Birke,Christian Klingenberg*

Main category: physics.flu-dyn

TL;DR: Conventional MHD schemes are too dissipative for low-Mach turbulence. New USM-BK scheme reduces dissipation, maintains magnetic divergence, and resolves smaller scales better than other schemes.


<details>
  <summary>Details</summary>
Motivation: Subsonic MHD turbulence is important for various astrophysical and geophysical processes, but existing numerical schemes are excessively dissipative in the low-Mach regime.

Method: Developed new USM-BK scheme in FLASH MHD code and compared it against multiple conventional schemes (Split-Roe, Split-Bouchut, USM-Roe, USM-HLLC, USM-HLLD) using vortex tests and turbulent dynamo simulations.

Result: USM-BK reduces kinetic/magnetic energy dissipation, maintains magnetic divergence near machine precision, resolves smaller-scale structures, provides highest numerical Reynolds number, and affects dynamo growth/saturation.

Conclusion: USM-BK is the most accurate scheme for low-Mach turbulent MHD flows among those compared, offering superior performance in reducing numerical dissipation.

Abstract: Magnetohydrodynamic (MHD) simulations of subsonic (Mach number~$<1$) turbulence are crucial to our understanding of several processes including oceanic and atmospheric flows, the amplification of magnetic fields in the early universe, accretion discs, and stratified flows in stars. In this work, we demonstrate that conventional numerical schemes are excessively dissipative in this low-Mach regime. We demonstrate that a new numerical scheme (termed `USM-BK' and implemented in the FLASH MHD code) reduces the dissipation of kinetic and magnetic energy, constrains the divergence of magnetic field to zero close to machine precision, and resolves smaller-scale structure than other, more conventional schemes, and hence, is the most accurate for simulations of low-Mach turbulent flows among the schemes compared in this work. We first compare several numerical schemes/solvers, including Split-Roe, Split-Bouchut, USM-Roe, USM-HLLC, USM-HLLD, and the new USM-BK, on a simple vortex problem. We then compare the schemes/solvers in simulations of the turbulent dynamo and show that the choice of scheme affects the growth rate, saturation level, and viscous and resistive dissipation scale of the dynamo. We also measure the numerical kinematic Reynolds number (Re) and magnetic Reynolds number (Rm) of our otherwise ideal MHD flows, and show that the new USM-BK scheme provides the highest Re and comparable Rm amongst all the schemes compared.

</details>


### [52] [Data-driven modeling of multiscale phenomena with applications to fluid turbulence](https://arxiv.org/abs/2511.09847)
*Brandon Choi,Matteo Ugliotti,Mateo Reynoso,Daniel R. Gurevich,Roman O. Grigoriev*

Main category: physics.flu-dyn

TL;DR: A data-driven framework for building equivariant multiscale models without physics assumptions, demonstrated on 2D turbulence to capture backscatter effects.


<details>
  <summary>Details</summary>
Motivation: To create accurate multiscale models that don't rely on specific physics assumptions, addressing the challenge of modeling backscatter (energy transfer from small to large scales) in turbulence.

Method: Used direct numerical simulations of 2D decaying turbulence to infer an effective field theory with explicit evolution equations for both resolved large scales and modeled small scales.

Result: Developed a closed system of equations that accurately describes small-scale effects including backscatter, which is particularly pronounced in 2D turbulence.

Conclusion: The framework successfully addresses the challenge of modeling backscatter in multiscale phenomena, providing interpretable evolution equations without relying on specific physical assumptions.

Abstract: This letter introduces a novel data driven framework for constructing accurate and general equivariant models of multiscale phenomena which does not rely on specific assumptions about the underlying physics. This framework is illustrated using incompressible fluid turbulence as an example that is representative, practically important, reasonably simple, and exceedingly well studied. We use direct numerical simulations of freely decaying turbulence in two spatial dimensions to infer an effective field theory comprising explicit, interpretable evolution equations for both the large (resolved) and small (modeled) scales. The resulting closed system of equations is capable of accurately describing the effect of small scales, including backscatter -- the flow of energy from small to large scales, which is particularly pronounced in two dimensions -- which is an outstanding challenge that, to our knowledge, no existing alternative successfully tackles.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [53] [Sample Complexity of Quadratically Regularized Optimal Transport](https://arxiv.org/abs/2511.09807)
*Alberto González-Sanz,Eustasio del Barrio,Marcel Nutz*

Main category: math.ST

TL;DR: Quadratically regularized optimal transport (QOT) has parametric sample complexity despite lacking smooth potentials and strong concavity, avoiding the curse of dimensionality through analysis of optimal coupling support regularity.


<details>
  <summary>Details</summary>
Motivation: To address limitations of entropically regularized OT (overspreading, computational instability) and challenge the assumption that QOT suffers from curse of dimensionality due to non-smooth potentials and lack of strong concavity.

Method: Novel analysis focusing on regularity of optimal QOT coupling support, establishing Lipschitz property of its sections and leveraging VC theory to bound statistical complexity, plus gradient estimates including C¹,¹ regularity of population potentials.

Result: Established central limit theorems for QOT's dual potentials, optimal couplings, and optimal costs, proving parametric sample complexity despite previous assumptions.

Conclusion: QOT avoids the curse of dimensionality and has parametric sample complexity, making it a viable alternative to EOT with benefits of sparse approximations and computational stability.

Abstract: It is well known that optimal transport suffers from the curse of dimensionality: when the prescribed marginals are approximated by i.i.d. samples, the convergence of the empirical optimal transport problem to the population counterpart slows exponentially with increasing dimension. Entropically regularized optimal transport (EOT) has become the standard bearer in many statistical applications as it avoids this curse. Indeed, EOT has parametric sample complexity, as has been shown in a series of works based on the smoothness of the EOT potentials or the strong concavity of the dual EOT problem. However, EOT produces full-support approximations to the (sparse) OT problem, leading to overspreading in applications, and is computationally unstable for small regularization parameters. The most popular alternative is quadratically regularized optimal transport (QOT), which penalizes couplings by $L^2$ norm instead of relative entropy. QOT produces sparse approximations of OT and is computationally stable. However, its potentials are not smooth (do not belong to a Donsker class) and its dual problem is not strongly concave, hence QOT is often assumed to suffer from the curse of dimensionality. In this paper, we show that QOT nevertheless has parametric sample complexity. More precisely, we establish central limit theorems for its dual potentials, optimal couplings, and optimal costs. Our analysis is based on novel arguments that focus on the regularity of the support of the optimal QOT coupling. Specifically, we establish a Lipschitz property of its sections and leverage VC theory to bound its statistical complexity. Our analysis also leads to gradient estimates of independent interest, including $C^{1,1}$ regularity of the population potentials.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [54] [Electron Heat Flux and Whistler Instability in the Earth's Magnetosheath](https://arxiv.org/abs/2511.10275)
*Ida Svenningsson,Emiliya Yordanova,Yuri V. Khotyaintsev,Mats André,Giulia Cozzani,Alexandros Chasapis,Steven J. Schwartz*

Main category: physics.space-ph

TL;DR: Analysis of electron heat flux in Earth's magnetosheath using MMS measurements, showing it's shaped by magnetic field draping and limited by whistler instability thresholds.


<details>
  <summary>Details</summary>
Motivation: Heat flux regulates energy conversion in collisionless plasmas but is scarcely explored in the magnetosheath downstream of Earth's bow shock.

Method: Used MMS in situ measurements to quantify and characterize electron heat flux in the magnetosheath.

Result: Heat flux is shaped by magnetosheath magnetic field draping around magnetosphere, affected by solar wind upstream conditions and increases with magnetic field strength, but not substantially changed by local magnetosheath processes.

Conclusion: Electron heat flux in the magnetosheath is limited by whistler instability thresholds.

Abstract: Despite heat flux's role in regulating energy conversion in collisionless plasmas, its properties and evolution in the magnetosheath downstream of the Earth's bow shock are scarcely explored. We use MMS in situ measurements to quantify and characterize the electron heat flux in the magnetosheath. We find that the heat flux is shaped by the magnetosheath magnetic field as it drapes around the magnetosphere. While it is affected by solar wind upstream conditions and increases with magnetic field strength, it is not substantially changed by local magnetosheath processes. Also, the heat flux is limited by whistler instability thresholds.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [55] [The Age-Structured Chemostat with Substrate Dynamics as a Control System](https://arxiv.org/abs/2511.09963)
*Iasson Karafyllis,Dionysis Theodosis,Miroslav Krstic*

Main category: math.OC

TL;DR: Analysis of an age-structured chemostat model with renewal boundary conditions and coupled substrate dynamics, proving global existence and uniqueness of solutions for all admissible controls.


<details>
  <summary>Details</summary>
Motivation: To study a nonlinear age-structured chemostat model with renewal boundary conditions and coupled substrate equations, establishing a rigorous mathematical framework for control system analysis.

Method: Combination of Banach's fixed-point theorem with implicit solution formulas and solution estimates, defining appropriate state and input spaces under a weak solution framework.

Result: Proved global existence and uniqueness of solutions for all admissible initial conditions and allowable control inputs, establishing the model as a well-defined control system on a metric space.

Conclusion: The age-structured chemostat model with renewal boundary conditions and coupled substrate dynamics forms a mathematically rigorous control system with guaranteed solution existence and uniqueness.

Abstract: In this work we study an age-structured chemostat model with a renewal boundary condition and a coupled substrate equation. The model is nonlinear and consists of a hyperbolic partial differential equation and an ordinary differential equation with nonlinear, nonlocal terms appearing both in the ordinary differential equation and the boundary condition. Both differential equations contain a non-negative control input, while the states of the model are required to be positive. Under an appropriate weak solution framework, we determine the state space and the input space for this model. We prove global existence and uniqueness of solutions for all admissible initial conditions and all allowable control inputs. To this purpose we employ a combination of Banach's fixed-point theorem with implicit solution formulas and useful solution estimates. Finally, we show that the age-structured chemostat model gives a well-defined control system on a metric space.

</details>


### [56] [An inexact semismooth Newton-Krylov method for semilinear elliptic optimal control problem](https://arxiv.org/abs/2511.10058)
*Shiqi Chen,Xuesong Chen*

Main category: math.OC

TL;DR: An inexact semismooth Newton method with GMRES and nonmonotonic line search for solving semi-linear elliptic optimal control problems, achieving superlinear convergence.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for solving semi-linear elliptic optimal control problems that reduces computational overhead while ensuring global convergence.

Method: Reformulates the problem into nonlinear equations using variational inequality principles, discretizes with second-order finite difference scheme, uses semismooth Newton directions with GMRES for inexact solving, and employs dynamic nonmonotonic line search for step size adjustment.

Result: The algorithm achieves superlinear convergence near optimal solutions when residual control parameter approaches 0, with numerical experiments validating accuracy and efficiency.

Conclusion: The proposed inexact semismooth Newton method with GMRES and nonmonotonic line search is effective for solving semi-linear elliptic optimal control problems, combining computational efficiency with theoretical convergence guarantees.

Abstract: An inexact semismooth Newton method has been proposed for solving semi-linear elliptic optimal control problems in this paper. This method incorporates the generalized minimal residual (GMRES) method, a type of Krylov subspace method, to solve the Newton equations and utilizes nonmonotonic line search to adjust the iteration step size. The original problem is reformulated into a nonlinear equation through variational inequality principles and discretized using a second-order finite difference scheme. By leveraging slanting differentiability, the algorithm constructs semismooth Newton directions and employs GMRES method to inexactly solve the Newton equations, significantly reducing computational overhead. A dynamic nonmonotonic line search strategy is introduced to adjust stepsizes adaptively, ensuring global convergence while overcoming local stagnation. Theoretical analysis demonstrates that the algorithm achieves superlinear convergence near optimal solutions when the residual control parameter $η_k$ approaches to 0. Numerical experiments validate the method's accuracy and efficiency in solving semilinear elliptic optimal control problems, corroborating theoretical insights.

</details>


### [57] [Low-Discrepancy Set Post-Processing via Gradient Descent](https://arxiv.org/abs/2511.10496)
*François Clément,Linhang Huang,Woorim Lee,Cole Smidt,Braeden Sodt,Xuan Zhang*

Main category: math.OC

TL;DR: Simple gradient descent techniques can achieve comparable results to expensive optimization methods for constructing low-discrepancy sets, making the process more efficient and accessible.


<details>
  <summary>Details</summary>
Motivation: Current methods for constructing low-discrepancy sets using optimization and machine learning are computationally expensive, requiring days of computation or GPU clusters.

Method: Using simple gradient descent-based techniques starting with reasonably uniform point sets, applied as post-processing to any low-discrepancy set generation method.

Result: Achieves comparable results to expensive optimization methods while being much more efficient and accessible.

Conclusion: Gradient descent provides an efficient alternative to computationally intensive methods for improving low-discrepancy sets across various standard discrepancy measures.

Abstract: The construction of low-discrepancy sets, used for uniform sampling and numerical integration, has recently seen great improvements based on optimization and machine learning techniques. However, these methods are computationally expensive, often requiring days of computation or access to GPU clusters. We show that simple gradient descent-based techniques allow for comparable results when starting with a reasonably uniform point set. Not only is this method much more efficient and accessible, but it can be applied as post-processing to any low-discrepancy set generation method for a variety of standard discrepancy measures.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [58] [Regret, Uncertainty, and Bounded Rationality in Norm-Driven Decisions](https://arxiv.org/abs/2511.10342)
*Christos Charalambous*

Main category: physics.soc-ph

TL;DR: An agent-based model shows how regret, uncertainty, and social norms interact to shape vaccination behavior during epidemics, with optimal outcomes occurring at intermediate rationality levels.


<details>
  <summary>Details</summary>
Motivation: To understand how psychological factors like regret, uncertainty, and social norms influence vaccination decisions during epidemics, integrating behavioral mechanisms from psychology and economics.

Method: Developed an agent-based model integrating anticipated regret, evolving norms, and uncertainty-dependent trust within a unified learning framework, simulating SIR epidemic processes.

Result: Collective outcomes are best with intermediate rationality; moderate regret promotes adaptation while excessive regret destabilizes choices; moderate uncertainty encourages caution but too much disrupts coordination; social norms restore cooperation.

Conclusion: The model provides a psychologically grounded computational account of how emotion, cognition, and social norms govern preventive behavior during epidemics, with different norm types serving complementary roles under varying information conditions.

Abstract: This study introduces an agent-based model that explains how regret, uncertainty, and social norms interact to shape vaccination behavior during epidemics. The model integrates three behavioral mechanisms, anticipated regret, evolving norms, and uncertainty-dependent trust, within a unified learning framework. Grounded in psychology and behavioral economics, it captures how individuals make probabilistic choices influenced by material payoffs, fear, trust, and social approval. Simulations of the Susceptible-Infected-Recovered process show that collective outcomes are best when agents display an intermediate level of rationality; they deliberate enough to respond to risk but remain flexible enough to adapt, avoiding the instability of both random and overly rigid decision-making. Regret exerts a dual influence; moderate levels encourage adaptive self-correction, while excessive regret or greed destabilize choices. Uncertainty has a similarly non-linear effect; moderate ambiguity promotes caution, but too much uncertainty disrupts coordination. Social norms restore cooperation by compensating for incomplete information. Among them, personal norms drive behavior when individuals have clear information and moral confidence; injunctive norms, reflecting perceived approval, gain influence under uncertainty; and descriptive norms, based on observing others' actions, serve as informational cues that guide behavior when direct knowledge is limited. Overall, the model provides a psychologically grounded, computationally explicit account of how emotion, cognition, and social norms govern preventive behavior during epidemics.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [59] [The Role of Deep Mesoscale Eddies in Ensemble Forecast Performance](https://arxiv.org/abs/2511.09747)
*Justin Cooke,Kathleen Donohue,Clark D Rowley,Prasad G Thoppil,D Randolph Watts*

Main category: physics.ao-ph

TL;DR: Deep ocean initialization significantly impacts surface forecast accuracy in ocean modeling, with better deep field initialization reducing upper ocean uncertainty during forecasts.


<details>
  <summary>Details</summary>
Motivation: Current forecasting only assimilates upper ocean data (<1000m), but deep ocean dynamics critically influence surface circulation development, particularly in complex systems like the Loop Current.

Method: Analyzed two 92-day forecasts during Loop Current Eddy Thor separation, comparing best/worst ensemble members by assessing surface variables against analysis/satellite data, and examining deep cyclonic/anticyclonic features against observations.

Result: Deep initial conditions that better match observations lead to lower upper ocean uncertainty. Subtle differences in deep eddy locations significantly impact surface forecast performance.

Conclusion: Deep circulation is crucial for Loop Current dynamics, motivating assimilation of deep observations to better constrain initial fields and improve surface predictions.

Abstract: Present forecasting efforts rely on assimilation of observational data captured in the upper ocean (< 1000 m depth). These observations constrain the upper ocean and minimally influence the deep ocean. Nevertheless, development of the full water column circulation critically depends upon the dynamical interactions between upper and deep fields. Forecasts demonstrate that the initialization of the deep field is influential for the development and evolution of the surface in the forecast. Deep initial conditions that better agree with observations have lower upper ocean uncertainty as the forecast progresses. Here, best and worst ensemble members in two 92-day forecasts are identified and contrasted in order to determine how the deep ocean differs between these groups. The forecasts cover the duration of the Loop Current Eddy Thor separation event, which coincides with available deep observations in the Gulf. Model member performance is assessed by comparing surface variables against verifying analysis and satellite altimeter data during the forecast time-period. Deep cyclonic and anticyclonic features are reviewed, and compared against deep observations, indicating subtle differences in locations of deep eddies at relevant times. These results highlight both the importance of deep circulation in the dynamics of the Loop Current system and more broadly motivate efforts to assimilate deep observations to better constrain the deep initial fields and improve surface predictions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [60] [HeatGen: A Guided Diffusion Framework for Multiphysics Heat Sink Design Optimization](https://arxiv.org/abs/2511.09578)
*Hadi Keramati,Morteza Sadeghi,Rajeev K. Jaiman*

Main category: cs.LG

TL;DR: A generative optimization framework using guided denoising diffusion probabilistic model (DDPM) with surrogate gradients to create heat sink designs that minimize pressure drop while maintaining temperature constraints, outperforming traditional optimization methods.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable generative model for electronics cooling that can efficiently generate optimized heat sink designs under new constraints without retraining, overcoming limitations of traditional black-box optimization and topology optimization methods.

Method: Uses boundary representations of multiple fins with multi-fidelity training data, trains DDPM for geometry generation, employs two residual neural networks as surrogate models for pressure drop and temperature prediction, and uses gradient guidance during inference to satisfy constraints.

Result: Generated heat sink designs achieve pressure drops up to 10% lower than traditional black-box optimization methods like CMA-ES, while maintaining surface temperatures below specified thresholds.

Conclusion: The framework represents progress toward building foundational generative models for electronics cooling, offering computationally efficient inference under new constraints without retraining requirements.

Abstract: This study presents a generative optimization framework based on a guided denoising diffusion probabilistic model (DDPM) that leverages surrogate gradients to generate heat sink designs minimizing pressure drop while maintaining surface temperatures below a specified threshold. Geometries are represented using boundary representations of multiple fins, and a multi-fidelity approach is employed to generate training data. Using this dataset, along with vectors representing the boundary representation geometries, we train a denoising diffusion probabilistic model to generate heat sinks with characteristics consistent with those observed in the data. We train two different residual neural networks to predict the pressure drop and surface temperature for each geometry. We use the gradients of these surrogate models with respect to the design variables to guide the geometry generation process toward satisfying the low-pressure and surface temperature constraints. This inference-time guidance directs the generative process toward heat sink designs that not only prevent overheating but also achieve lower pressure drops compared to traditional optimization methods such as CMA-ES. In contrast to traditional black-box optimization approaches, our method is scalable, provided sufficient training data is available. Unlike traditional topology optimization methods, once the model is trained and the heat sink world model is saved, inference under new constraints (e.g., temperature) is computationally inexpensive and does not require retraining. Samples generated using the guided diffusion model achieve pressure drops up to 10 percent lower than the limits obtained by traditional black-box optimization methods. This work represents a step toward building a foundational generative model for electronics cooling.

</details>


### [61] [Incremental Generation is Necessity and Sufficient for Universality in Flow-Based Modelling](https://arxiv.org/abs/2511.09902)
*Hossein Rouhvarzi,Anastasis Kratsios*

Main category: cs.LG

TL;DR: Incremental flow-based denoising models require multiple steps for universal approximation of orientation-preserving homeomorphisms on [0,1]^d, as single-step flows are insufficient but compositions of multiple flows can achieve approximation.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous theoretical foundation for incremental flow-based generative models by determining when incremental generation is necessary and sufficient for universal approximation.

Method: Using topological-dynamical arguments and algebraic properties of autonomous flows to prove impossibility theorems for single-step flows and construct multi-step flow compositions that achieve approximation.

Result: Single-step flows are meagre and cannot universally approximate orientation-preserving homeomorphisms, but compositions of multiple flows can achieve approximation rates of O(n^{-1/d}) for Lipschitz homeomorphisms, with dimension-free rates under smoothness assumptions.

Conclusion: Incremental generation is both necessary and sufficient for universal flow-based generation on orientation-preserving homeomorphisms of [0,1]^d, providing theoretical justification for multi-step denoising models.

Abstract: Incremental flow-based denoising models have reshaped generative modelling, but their empirical advantage still lacks a rigorous approximation-theoretic foundation. We show that incremental generation is necessary and sufficient for universal flow-based generation on the largest natural class of self-maps of $[0,1]^d$ compatible with denoising pipelines, namely the orientation-preserving homeomorphisms of $[0,1]^d$. All our guarantees are uniform on the underlying maps and hence imply approximation both samplewise and in distribution.
  Using a new topological-dynamical argument, we first prove an impossibility theorem: the class of all single-step autonomous flows, independently of the architecture, width, depth, or Lipschitz activation of the underlying neural network, is meagre and therefore not universal in the space of orientation-preserving homeomorphisms of $[0,1]^d$. By exploiting algebraic properties of autonomous flows, we conversely show that every orientation-preserving Lipschitz homeomorphism on $[0,1]^d$ can be approximated at rate $\mathcal{O}(n^{-1/d})$ by a composition of at most $K_d$ such flows, where $K_d$ depends only on the dimension. Under additional smoothness assumptions, the approximation rate can be made dimension-free, and $K_d$ can be chosen uniformly over the class being approximated. Finally, by linearly lifting the domain into one higher dimension, we obtain structured universal approximation results for continuous functions and for probability measures on $[0,1]^d$, the latter realized as pushforwards of empirical measures with vanishing $1$-Wasserstein error.

</details>
