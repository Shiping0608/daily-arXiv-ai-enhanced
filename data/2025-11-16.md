<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 16]
- [math.AP](#math.AP) [Total: 14]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 8]
- [math.DG](#math.DG) [Total: 3]
- [math.ST](#math.ST) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.MS](#cs.MS) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Convergence analysis of a third order semi-implicit projection method for Landau-Lifshitz-Gilbert equation](https://arxiv.org/abs/2511.09589)
*Changjian Xie,Cheng Wang*

Main category: math.NA

TL;DR: A third-order semi-implicit method for the Landau-Lifshitz-Gilbert equation with non-convex constraint, using BDF3 and projection to preserve magnetization length, achieving third-order temporal and fourth-order spatial accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze a high-order numerical scheme for the highly nonlinear Landau-Lifshitz-Gilbert equation with non-convex constraint, addressing convergence and unique solvability.

Method: Fully discrete semi-implicit method using third-order backward differentiation formula (BDF3) and one-sided extrapolation with projection step to preserve magnetization length on unit sphere.

Result: Achieved third-order accuracy in time and fourth-order accuracy in space when spatial step-size matches temporal step-size and damping parameter α > √2/2. Unique solvability proven without step-size assumptions.

Conclusion: The proposed third-order scheme is theoretically justified and numerically validated in 1D and 3D spaces, providing reliable convergence for the Landau-Lifshitz-Gilbert equation with non-convex constraint.

Abstract: The convergence analysis of a third-order scheme for the highly nonlinear Landau-Lifshitz-Gilbert equation with a non-convex constraint is considered. In this paper, we first present a fully discrete semi-implicit method for solving the Landau-Lifshitz-Gilbert equation based on the third-order backward differentiation formula and the one-sided extrapolation (using previous time-step numerical values). A projection step is further used to preserve the length of the magnetization. We provide a rigorous convergence analysis for the fully discrete numerical solution by the introduction of two sets of approximated solutions where one set of solutions solves the Landau-Lifshitz-Gilbert equation and the other is projected onto the unit sphere. Third-order accuracy in time and fourth order accuracy in space is obtained provided that the spatial step-size is the same order as the temporal step-size and slightly large damping parameter $α$ (greater than $\sqrt{2}/2$). And also, the unique solvability of the numerical solution without any assumption for the step-size in both time and space is theoretically justified, using a monotonicity analysis. All these theoretical results are guaranteed by numerical examples in both 1D and 3D spaces.

</details>


### [2] [SLIPT for Underwater IoT: System Modeling and Performance Analysis](https://arxiv.org/abs/2511.09680)
*Shunyuan Shang,Ziyuan Shi,Mohamed-Slim Alouini*

Main category: math.NA

TL;DR: A unified analytical framework for underwater wireless optical communication systems integrating simultaneous lightwave information and power transfer using photovoltaic panel receivers.


<details>
  <summary>Details</summary>
Motivation: To enable self-powered underwater sensor nodes by leveraging wide-area, low-cost PV panels for concurrent optical signal detection and energy harvesting in underwater environments.

Method: Developed a composite statistical channel model combining distance-dependent absorption, turbulence-induced fading using mixture Exponential Generalized Gamma distribution, and beam misalignment from pointing errors. Derived closed-form expressions for key performance metrics using Meijer G and Fox H functions.

Result: Successfully derived analytical expressions for probability density function, cumulative distribution function, outage probability, average bit error rate, ergodic capacity, and harvested power.

Conclusion: The paper provides a practical analytical framework that offers clear guidance for the design, optimization, and operation of SLIPT-based underwater wireless optical communication systems.

Abstract: This paper presents a unified analytical framework for a two phase underwater wireless optical communication (UWOC) system that integrates Simultaneous Lightwave Information and Power Transfer (SLIPT) using a photovoltaic (PV) panel receiver. The proposed architecture enables self powered underwater sensor nodes by leveraging wide area and low cost PV panels for concurrent optical signal detection and energy harvesting. We develop a composite statistical channel that combines distance dependent absorption, turbulence induced fading characterized by the mixture Exponential Generalized Gamma (EGG )distribution, and beam misalignment due to pointing errors. Based on this model we derive closed form expressions for the probability density function, the cumulative distribution function, the outage probability (OP), the average bit error rate, the ergodic capacity, and the harvested power using Meijer G and Fox H functions. Overall, the paper introduces a practical analytical framework that provides clear guidance for design, optimization, and operation of SLIPT based UWOC systems.

</details>


### [3] [Regularity and error estimates in physics-informed neural networks for the Kuramoto-Sivashinsky equation](https://arxiv.org/abs/2511.09728)
*Mohammad Mahabubur Rahman,Deepanshu Verma*

Main category: math.NA

TL;DR: This paper establishes the first rigorous error estimates for Physics-Informed Neural Networks (PINNs) applied to the challenging 2D/3D Kuramoto-Sivashinsky equation, overcoming its nonlinearity, bi-harmonic dissipation, and backward heat-like term difficulties.


<details>
  <summary>Details</summary>
Motivation: The Kuramoto-Sivashinsky equation poses significant challenges due to its nonlinearity, bi-harmonic dissipation, and backward heat-like term without divergence-free condition. Despite extensive PINN use for various PDEs, no previous study had established rigorous error estimates for this equation within a PINN framework.

Method: Developed methods blending classical analysis with numerical approximation through PINNs framework. Established global regularity criteria based on space-time integrability conditions in Besov spaces, then derived rigorous error estimates for PINNs approximation.

Result: Successfully established several global regularity criteria and derived the first rigorous error estimates for PINNs approximation of the Kuramoto-Sivashinsky equation. Theoretical error bounds were validated through numerical simulations.

Conclusion: The work overcomes inherent challenges of the Kuramoto-Sivashinsky equation by establishing rigorous error estimates for PINNs approximation, providing theoretical foundation and validation through numerical experiments.

Abstract: Due to its nonlinearity, bi-harmonic dissipation, and backward heat-like term in the absence of a divergence-free condition, the $2$-D/$3$-D Kuramoto-Sivashinsky equation poses significant challenges for both mathematical analysis and numerical approximation. These difficulties motivate the development of methods that blend classical analysis with numerical approximation approaches embodied in the framework of the physics-informed neural networks (PINNs). In addition, despite the extensive use of PINN frameworks for various linear and nonlinear PDEs, no study had previously established rigorous error estimates for the Kuramoto-Sivashinsky equation within a PINN setting. In this work, we overcome the inherent challenges, and establish several global regularity criteria based on space-time integrability conditions in Besov spaces. We then derive the first rigorous error estimates for the PINNs approximation of the Kuramoto-Sivashinsky equation and validate our theoretical error bounds through numerical simulations.

</details>


### [4] [Global iterative methods for sparse approximate inverses of symmetric positive-definite matrices](https://arxiv.org/abs/2511.09753)
*Nicolas Venkovic,Hartwig Anzt*

Main category: math.NA

TL;DR: The paper introduces and analyzes N(P)CG and LO(P)MR methods for computing sparse approximate inverses (SPAIs) of SPD matrices, comparing them with existing methods like (P)CG, (P)MR, and (P)SD.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient iterative methods for computing SPAIs that accelerate convergence compared to existing global iteration methods like (P)MR and (P)SD.

Method: N(P)CG is a one-dimensional projection method with residual orthogonality, while LO(P)MR is a two-dimensional projection method that enriches (P)MR iterates. Both use practical dropping strategies to control nonzero growth.

Result: N(P)CG slightly improves over (P)SD but is less effective than (P)MR. LO(P)MR consistently outperforms other methods, converging faster to better and often sparser approximations, showing greater robustness.

Conclusion: LO(P)MR is the most robust and effective method for SPAI computation, providing faster convergence and better approximations than existing methods, while N(P)CG offers limited improvements.

Abstract: The nonlinear (preconditioned) conjugate gradient N(P)CG method and the locally optimal (preconditioned) minimal residual LO(P)MR method, both of which are used for the iterative computation of sparse approximate inverses (SPAIs) of symmetric positive-definite (SPD) matrices, are introduced and analyzed. The (preconditioned) conjugate gradient (P)CG method is also employed and presented for comparison. The N(P)CG method is defined as a one-dimensional projection with residuals made orthogonal to the current search direction, itself made $A$-orthogonal to the last search direction. The residual orthogonality, expressed via Frobenius inner product, actually holds against all previous search directions, making each iterate globally optimal, that is, that minimizes the Frobenius A-norm of the error over the affine Krylov subspace of $A^2$ generated by the initial gradient. The LO(P)MR method is a two-dimensional projection method that enriches iterates produced by the (preconditioned) minimal residual (P)MR method. These approaches differ from existing descent methods and aim to accelerate convergence compared to other global iteration methods, including (P)MR and (preconditioned) steepest descent (P)SD, previously used for SPAI computation. The methods are implemented with practical dropping strategies to control the growth of nonzero components in the approximate inverse. Numerical experiments are performed in which approximate inverses of several sparse SPD matrices are computed. N(P)CG provides a slight improvement over (P)SD, but remains generally less effective than (P)MR. On the other hand, while (P)CG does improve (P)MR, its convergence is more affected by the dropping of nonzero components, ill-conditioning, and small eigenvalues. LO(P)MR is more robust than (P)MR and (P)CG, consistently outperforms other methods, converging faster to better and often sparser approximations.

</details>


### [5] [A model-free method for discovering symmetry in differential equations](https://arxiv.org/abs/2511.09779)
*Max Kreider,John Harlim,Daning Huang*

Main category: math.NA

TL;DR: A numerical method to discover Lie symmetries from scattered data without knowing the governing equations, using manifold learning and linear algebra to approximate infinitesimal generators.


<details>
  <summary>Details</summary>
Motivation: Identifying Lie symmetries directly from scattered data is challenging when governing equations are unknown, but symmetries can reduce model complexity and reveal system invariances.

Method: Uses Generalized Moving Least Squares for manifold learning to prolongate data, then constructs a linear system whose null space encodes infinitesimal generators representing symmetries.

Result: Convergence bounds are derived, and numerical experiments on ODEs and PDEs demonstrate the method's accuracy, robustness, and convergence.

Conclusion: The approach enables data-driven discovery of continuous symmetries in dynamical systems without requiring analytical forms of differential equations.

Abstract: Symmetry in differential equations reveals invariances and offers a powerful means to reduce model complexity. Lie group analysis characterizes these symmetries through infinitesimal generators, which provide a local, linear criterion for invariance. However, identifying Lie symmetries directly from scattered data, without explicit knowledge of the governing equations, remains a significant challenge. This work introduces a numerical scheme that approximates infinitesimal generators from data sampled on an unknown smooth manifold, enabling the recovery of continuous symmetries without requiring the analytical form of the differential equations. We employ a manifold learning technique, Generalized Moving Least Squares, to prolongate the data, from which a linear system is constructed whose null space encodes the infinitesimal generators representing the symmetries. Convergence bounds for the proposed approach are derived. Several numerical experiments, including ordinary and partial differential equations, demonstrate the method's accuracy, robustness, and convergence, highlighting its potential for data-driven discovery of symmetries in dynamical systems.

</details>


### [6] [Efficient Krylov-Regularization Solvers for Multiquadric RBF Discretizations of the 3D Helmholtz Equation](https://arxiv.org/abs/2511.09798)
*Mohamed El Guide,Khalide Jbilou,Kamal Lachhab,Driss Ouazar*

Main category: math.NA

TL;DR: Three regularization methods (TSVD, Tikhonov, and hybrid Krylov-Tikhonov) are developed for meshless collocation with multiquadric radial basis functions to solve the 3D Helmholtz equation, overcoming ill-conditioning while maintaining accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Meshless collocation with MQ-RBFs provides high accuracy for 3D Helmholtz problems but produces dense, severely ill-conditioned linear systems that require regularization for stable solutions at scale.

Method: Three complementary regularization methods: (1) inexpensive TSVD using Golub-Kahan bidiagonalization, (2) classical Tikhonov regularization with GCV/L-curve parameter selection, and (3) hybrid Krylov-Tikhonov scheme that projects first then regularizes.

Result: HKT consistently matches or surpasses full TSVD/Tikhonov accuracy at much lower runtime and memory costs. Inexpensive TSVD provides fastest reconstructions when only leading modes are needed.

Conclusion: Coupling Krylov projection with TSVD/Tikhonov regularization provides a robust, scalable pathway for MQ-RBF Helmholtz methods in complex 3D settings.

Abstract: Meshless collocation with multiquadric radial basis functions (MQ-RBFs) delivers high accuracy for the three-dimensional Helmholtz equation but produces dense, severely ill-conditioned linear systems. We develop and evaluate three complementary methods that embed regularization in Krylov projections to overcome this instability at scale: (i) an inexpensive TSVD that replaces the full SVD by a short Golub-Kahan bidiagonalization and a small projected SVD, retaining the dominant spectral content at greatly reduced cost; (ii) classical Tikhonov regularization with principled parameter choice (GCV/L-curve), expressed in SVD form for transparent filtering; and (iii) a hybrid Krylov-Tikhonov (HKT) scheme that first projects with Golub-Kahan and then selects the regularization parameter on the reduced problem, yielding stable solutions in few iterations. Extensive tests on canonical domains (cube and sphere) and a realistic industrial pump-casing geometry demonstrate that HKT consistently matches or surpasses the accuracy of full TSVD/Tikhonov at a fraction of the runtime and memory, while inexpensive TSVD provides the fastest viable reconstructions when only the leading modes are needed. These results show that coupling Krylov projection with TSVD/Tikhonov regularization provides a robust, scalable pathway for MQ-RBF Helmholtz methods in complex three-dimensional settings.

</details>


### [7] [Randomized batch-sampling Kaczmarz methods for general linear systems](https://arxiv.org/abs/2511.09872)
*Dong-Yue Xie,Xi Yang*

Main category: math.NA

TL;DR: A unified randomized batch-sampling Kaczmarz framework for linear systems with low per-iteration costs and new convergence analysis using concentration inequalities.


<details>
  <summary>Details</summary>
Motivation: To conduct deeper investigation of randomized solvers for general linear systems and develop more accurate convergence analysis for block Kaczmarz methods.

Method: Adopt a unified randomized batch-sampling Kaczmarz framework with concentration inequalities to derive new expected linear convergence rate bounds for static stochastic samplings.

Result: New scale-invariant convergence rate bounds that are significantly tighter than existing ones and better reflect empirical convergence behavior in most experiments.

Conclusion: The batch-sampling distribution as a learnable parameter enables efficient performance in specific applications and deserves further investigation.

Abstract: To conduct a more in-depth investigation of randomized solvers for general linear systems, we adopt a unified randomized batch-sampling Kaczmarz framework with per-iteration costs as low as cyclic block methods, and develop a general analysis technique to establish its convergence guarantee. With concentration inequalities, we derive new expected linear convergence rate bounds. The analysis applies to any randomized non-extended block Kaczmarz methods with static stochastic samplings. In addition, the new rate bounds are scale-invariant which eliminate the dependence on the magnitude of the data matrix. In most experiments, the new bounds are significantly tighter than existing ones and better reflect the empirical convergence behavior of block methods. Within this new framework, the batch-sampling distribution, as a learnable parameter, provides the possibility for block methods to achieve efficient performance in specific application scenarios, which deserves further investigation.

</details>


### [8] [Implicit Multiple Tensor Decomposition](https://arxiv.org/abs/2511.09916)
*Kunjing Yang,Libin Zheng,Minru Bai*

Main category: math.NA

TL;DR: Proposes Multiple decomposition as a generalization of triple decomposition for arbitrary-order tensors with flexible dimensions, combined with implicit neural representations for non-grid data, and provides convergence analysis for the optimization algorithm.


<details>
  <summary>Details</summary>
Motivation: Triple decomposition is limited to third-order tensors and requires uniform lower dimensions across factor tensors, restricting flexibility and applicability to various tensor structures.

Method: Develops Multiple decomposition framework for arbitrary-order tensors with flexible dimensions, uses implicit neural representations (INR) for continuous factor tensor representation (IMTD), and employs Proximal Alternating Least Squares (PALS) with KL-free convergence analysis.

Result: Extensive numerical experiments validate the effectiveness of the proposed method, demonstrating successful tensor decomposition and reconstruction across various scenarios.

Conclusion: The proposed Multiple decomposition framework successfully overcomes limitations of triple decomposition, enabling flexible tensor decomposition for arbitrary orders and non-grid data with proven convergence guarantees.

Abstract: Recently, triple decomposition has attracted increasing attention for decomposing third-order tensors into three factor tensors. However, this approach is limited to third-order tensors and enforces uniformity in the lower dimensions across all factor tensors, which restricts its flexibility and applicability. To address these issues, we propose the Multiple decomposition, a novel framework that generalizes triple decomposition to arbitrary order tensors and allows the short dimensions of the factor tensors to differ. We establish its connections with other classical tensor decompositions. Furthermore, implicit neural representation (INR) is employed to continuously represent the factor tensors in Multiple decomposition, enabling the method to generalize to non-grid data. We refer to this INR-based Multiple decomposition as Implicit Multiple Tensor Decomposition (IMTD). Then, the Proximal Alternating Least Squares (PALS) algorithm is utilized to solve the IMTD-based tensor reconstruction models. Since the objective function in IMTD-based models often lacks the Kurdyka-Lojasiewicz (KL) property, we establish a KL-free convergence analysis for the algorithm. Finally, extensive numerical experiments further validate the effectiveness of the proposed method.

</details>


### [9] [Asymptotic-preserving and energy-conserving methods for a hyperbolic approximation of the BBM equation](https://arxiv.org/abs/2511.10044)
*Sebastian Bleecke,Abhijit Biswas,David I. Ketcheson,Hendrik Ranocha,Jochen Schutz*

Main category: math.NA

TL;DR: Development of asymptotic-preserving numerical methods for hyperbolic approximation of BBM equation using implicit-explicit Runge-Kutta methods with entropy relaxation.


<details>
  <summary>Details</summary>
Motivation: To create numerical methods that preserve important invariants and energy properties when approximating the Benjamin-Bona-Mahony equation through hyperbolic approximation.

Method: Implicit-explicit Runge-Kutta methods with implicit treatment of stiff linear part, using entropy relaxation approach to ensure energy preservation in fully discrete schemes.

Result: The new discretization conserves important invariants that converge to BBM equation invariants, and numerical experiments demonstrate effectiveness.

Conclusion: The developed asymptotic-preserving methods successfully preserve energy and invariants while effectively approximating the BBM equation through hyperbolic discretization.

Abstract: We study the hyperbolic approximation of the Benjamin-Bona-Mahony (BBM) equation proposed recently by Gavrilyuk and Shyue (2022). We develop asymptotic-preserving numerical methods using implicit-explicit (additive) Runge-Kutta methods that are implicit in the stiff linear part. The new discretization of the hyperbolization conserves important invariants converging to invariants of the BBM equation. We use the entropy relaxation approach to make the fully discrete schemes energy-preserving. Numerical experiments demonstrate the effectiveness of these discretizations.

</details>


### [10] [A Third-order Conservative Semi-Lagrangian Discontinuous Galerkin Scheme For the Transport Equation on Curvilinear Unstructured Meshes](https://arxiv.org/abs/2511.10100)
*Xiaofeng Cai,Yibing Chen,Kunkai Fu,Liujun Pan*

Main category: math.NA

TL;DR: A third-order conservative semi-Lagrangian discontinuous Galerkin scheme for linear transport equations on curvilinear unstructured triangular meshes, featuring mass conservation, large time stepping, and effective oscillation suppression.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and accurate numerical scheme for solving linear transport equations on complex geometries using curvilinear unstructured meshes, while maintaining strict mass conservation and handling both smooth and discontinuous solutions.

Method: Third-order conservative semi-Lagrangian discontinuous Galerkin method with intersection-based remapping algorithm for curvilinear meshes, incorporating WENO and positivity-preserving limiters for oscillation control.

Result: The scheme achieves third-order accuracy in both space and time, validated through rigorous numerical analysis and benchmarks including rigid body rotation and swirling deformation flows with various initial conditions.

Conclusion: The developed SLDG scheme provides an accurate, stable, and robust solution for linear transport problems on complex geometries, with large time stepping capabilities and effective handling of both smooth and discontinuous solutions.

Abstract: We develop a third-order conservative semi-Lagrangian discontinuous Galerkin (SLDG) scheme for solving linear transport equations on curvilinear unstructured triangular meshes, tailored for complex geometries. To ensure third-order spatial accuracy while strictly preserving mass, we develop a high-order conservative intersection-based remapping algorithm for curvilinear unstructured meshes, which enables accurate and conservative data transfer between distinct curvilinear meshes. Incorporating this algorithm, we construct a non-splitting high-order SLDG method equipped with weighted essentially non-oscillatory and positivity-preserving limiters to effectively suppress numerical oscillations and maintain solution positivity. For the linear problem, the semi-Lagrangian update enables large time stepping, yielding an explicit and efficient implementation. Rigorous numerical analysis confirms that our scheme achieves third-order accuracy in both space and time, as validated by consistent error analysis in terms of $L^1$ and $L^2$-norms. Numerical benchmarks, including rigid body rotation and swirling deformation flows with smooth and discontinuous initial conditions, validate the scheme's accuracy, stability, and robustness.

</details>


### [11] [Accelerating the Serviceability-Based Design of Reinforced Concrete Rail Bridges under Geometric Uncertainties induced by unforeseen events: A Surrogate Modeling approach](https://arxiv.org/abs/2511.10129)
*Mouhammed Achhab,Pierre Jehel,Fabrice Gatuingt*

Main category: math.NA

TL;DR: Surrogate modeling enables efficient probabilistic design of reinforced concrete rail bridges by replacing time-consuming finite element simulations, allowing rapid exploration of design scenarios under uncertainty.


<details>
  <summary>Details</summary>
Motivation: Address uncertainties from unforeseen construction constraints (e.g., pier repositioning) that cause repeated redesigns, added costs, and project delays in traditional bridge design processes.

Method: Multi-fiber finite element modeling in Cast3M software to generate training data, followed by comparative assessment of Kriging surrogate against polynomial chaos expansion and support vector regression.

Result: Surrogate models efficiently represent bridge performance functions as functions of variable design parameters and classify design scenarios into failure/safe categories with minimal computational overhead.

Conclusion: Surrogate modeling supports early-stage uncertainty-informed design, enhancing robustness and adaptability of reinforced concrete rail bridges against practical constraints and changing site conditions.

Abstract: Reinforced concrete rail bridges are essential components of railway infrastructure, where reliability, durability, and adaptability are key design priorities. However, the design process is often complicated by uncertainties stemming from unforeseen construction constraints, such as the need to reposition piers or alter geometric characteristics. These design adaptations can lead to repeated redesigns, added costs, and project delays if not anticipated in the early design stages, as well as significant computational overhead when using traditional finite element (FE) simulations. To address this and anticipate such unexpected events, this study adopts surrogate modeling as an efficient probabilistic design approach. This methodology integrates key geometric parameters as random variables, capturing the uncertainties that may arise during the design and construction phases and propagating them on the bridge's performance functions. By doing so, we aim to enable the efficient exploration of a large number of design scenarios with minimal reliance on time-consuming finite element (FE) simulations, represent the performance functions of a reinforced concrete bridge as a function of our variable design parameters, and classify the overall design scenarios into failure and safe scenarios In this study, a four-span reinforced concrete bridge deck is modeled using a multi-fiber finite element approach in Cast3M software. This FE model is used to generate the required design of experiments to train the surrogate models. Within this framework, a comparative performance assessment is conducted to evaluate the performance of the Kriging surrogate against alternative methods, including polynomial chaos expansion (implemented in UQLab) and support vector regression (SVR). This methodology supports early-stage uncertainty-informed design, enhancing the robustness and adaptability of reinforced concrete rail bridges in the face of practical constraints and changing site conditions.

</details>


### [12] [Control strategies for magnetized plasma: a polar coordinates framework](https://arxiv.org/abs/2511.10214)
*Federica Ferrarese*

Main category: math.NA

TL;DR: Overview of magnetic field control strategies for plasma steering using Vlasov equation modeling in 2D polar coordinates, with feedback control based on system predictions.


<details>
  <summary>Details</summary>
Motivation: To develop effective control strategies for steering plasma toward desired configurations in toroidal devices like Tokamaks and Stellarators using external magnetic fields.

Method: Modeling using Vlasov equation in 2D bounded domain with self-induced electric field and strong external magnetic field, presented in polar coordinate framework with feedback control based on instantaneous prediction of discretized system.

Result: Numerical experiments in 2D polar coordinate setting demonstrate the effectiveness of the proposed control approaches.

Conclusion: The feedback control strategies based on system predictions are effective for plasma steering in toroidal magnetic confinement devices.

Abstract: In this work, we provide an overview of various control strategies aimed at steering plasma toward desired configurations using an external magnetic field. From a modeling perspective, we focus on the Vlasov equation in a two-dimensional bounded domain, accounting for both a self-induced electric field and a strong external magnetic field. The results are presented in a polar coordinate framework, which is particularly well-suited for simulating toroidal devices such as Tokamaks and Stellarators. A key feature of the proposed control strategies is their feedback mechanism, which is based on an instantaneous prediction of the discretized system. Finally, different numerical experiments in the two-dimensional polar coordinate setting demonstrate the effectiveness of the approaches.

</details>


### [13] [A Stabilized Unfitted Space-time Finite Element Method for Parabolic Problems on Moving Domains](https://arxiv.org/abs/2511.10242)
*Ruizhi Wang,Weibing Deng*

Main category: math.NA

TL;DR: A space-time FEM using unfitted meshes for parabolic problems on moving domains, featuring SUPG stabilization for time-advection and ghost penalty for small cut issues, with optimal convergence rates proven.


<details>
  <summary>Details</summary>
Motivation: To solve parabolic problems on moving domains using unfitted meshes without the limitations of DG time-stepping methods, enabling fully coupled space-time discretization.

Method: Unfitted space-time FEM with SUPG stabilization for temporal advection, ghost penalty stabilization for small cut problems, and fully coupled discretization.

Result: Derived a priori error estimate with optimal convergence rate in discrete energy norm, established space-time Poincare-Friedrichs inequality, and validated with numerical examples.

Conclusion: The method effectively handles moving domain problems with unfitted meshes, provides theoretical guarantees for convergence and conditioning, and demonstrates practical performance through numerical validation.

Abstract: This paper presents a space-time finite element method (FEM) based on an unfitted mesh for solving parabolic problems on moving domains. Unlike other unfitted space-time finite element approaches that commonly employ the discontinuous Galerkin (DG) method for time-stepping, the proposed method employs a fully coupled space-time discretization. To stabilize the time-advection term, the streamline upwind Petrov-Galerkin (SUPG) scheme is applied in the temporal direction. A ghost penalty stabilization term is further incorporated to mitigate the small cut issue, thereby ensuring the well-conditioning of the stiffness matrix. Moreover, an a priori error estimate is derived in a discrete energy norm, which achieves an optimal convergence rate with respect to the mesh size. In particular, a space-time Poincare-Friedrichs inequality is established to support the condition number analysis. Several numerical examples are provided to validate the theoretical findings.

</details>


### [14] [A novel mathematical and computational framework of amyloid-beta triggered seizure dynamics in Alzheimer's disease](https://arxiv.org/abs/2511.10369)
*Caterina B. Leimer Saglio,Mattia Corti,Stefano Pagani,Paola F. Antonietti*

Main category: math.NA

TL;DR: A multiscale mathematical model shows how amyloid-β accumulation in Alzheimer's disease causes calcium dysregulation, leading to neuronal hyperexcitability and epileptic seizure propagation.


<details>
  <summary>Details</summary>
Motivation: To quantitatively describe the link between Alzheimer's molecular pathology (amyloid-β accumulation) and epilepsy's electrophysiological dynamics, as clinical evidence increasingly connects these conditions.

Method: Extended Barreto-Cressman ionic model incorporating amyloid-β-induced calcium dysregulation mechanisms, coupled with monodomain equation and solved using p-adaptive discontinuous Galerkin method on polytopal meshes.

Result: Progressive amyloid-β accumulation causes severe calcium homeostasis alterations, increased neuronal hyperexcitability, pathological seizure propagation, secondary epileptogenic sources, and spatially heterogeneous wavefronts.

Conclusion: Multiscale modeling reveals how biochemical inhomogeneities from amyloid-β accumulation critically shape seizure dynamics, providing mechanistic insights into the neurodegeneration-epilepsy interplay in Alzheimer's disease.

Abstract: The association of epileptic activity and Alzheimer's disease (AD) has been increasingly reported in both clinical and experimental studies, suggesting that amyloid-$β$ accumulation may directly affect neuronal excitability. Capturing these interactions requires a quantitative description that bridges the molecular alterations of AD with the fast electrophysiological dynamics of epilepsy. We introduce a novel mathematical model that extends the Barreto-Cressman ionic formulation by incorporating multiple mechanisms of calcium dysregulation induced by amyloid-$β$, including formation of $\mathrm{Ca}^{2+}$-permeable pores, overactivation of voltage-gated $\mathrm{Ca}^{2+}$ channels, and suppression of $\mathrm{Ca}^{2+}$-sensitive potassium currents. The resulting ionic model is coupled with the monodomain equation and discretized using a $p$-adaptive discontinuous Galerkin method on polytopal meshes, providing an effective balance between efficiency and accuracy in capturing the sharp spatiotemporal electrical wavefronts associated with epileptiform discharges. Numerical simulations performed on idealized and realistic brain geometries demonstrate that progressive amyloid-\textbeta{} accumulation leads to severe alterations in calcium homeostasis, increased neuronal hyperexcitability, and pathological seizure propagation. Specifically, high amyloid-$β$ concentrations produce secondary epileptogenic sources and spatially heterogeneous wavefronts, indicating that biochemical inhomogeneities play a critical role in shaping seizure dynamics. These results illustrate how multiscale modeling provides new mechanistic insights into the interplay between neurodegeneration and epilepsy in Alzheimer's disease.

</details>


### [15] [Learning parameter-dependent shear viscosity from data, with application to sea and land ice](https://arxiv.org/abs/2511.10452)
*Gonzalo G. de Diego,Georg Stadler*

Main category: math.NA

TL;DR: A framework for inferring non-Newtonian fluid rheology models using neural networks that satisfy physical constraints like frame-indifference and convex dissipation potential.


<details>
  <summary>Details</summary>
Motivation: To develop data-driven rheological models for complex fluids that automatically satisfy key physical principles, addressing limitations of traditional empirical models.

Method: Two approaches: 1) regression fitting to stress data, and 2) PDE-constrained optimization using velocity data, combining finite element and machine learning libraries with neural networks tailored for tensor invariants.

Result: Successfully inferred temperature-dependent Glen's law for land ice and concentration-dependent viscous-plastic model for sea ice, with robustness to large data errors. Discovered a new concentration-dependent rheology that generalizes well and shows shear-thickening/thinning behaviors.

Conclusion: The framework enables discovery of physically consistent rheological models from data, demonstrating accuracy, robustness, and ability to capture complex fluid behaviors like shear-thickening/thinning.

Abstract: Complex physical systems which exhibit fluid-like behavior are often modeled as non-Newtonian fluids. A crucial element of a non-Newtonian model is the rheology, which relates inner stresses with strain-rates. We propose a framework for inferring rheological models from data that represents the fluid's effective viscosity with a neural network. By writing the rheological law in terms of tensor invariants and tailoring the network's properties, the inferred model satisfies key physical and mathematical properties, such as isotropic frame-indifference and existence of a convex potential of dissipation. Within this framework, we propose two approaches to learning a fluid's rheology: 1) a standard regression that fits the rheological model to stress data and 2) a PDE-constrained optimization method that infers rheological models from velocity data. For the latter approach, we combine finite element and machine learning libraries. We demonstrate the accuracy and robustness of our method on land and sea ice rheologies which also depend on external parameters. For land ice, we infer the temperature-dependent Glen's law and, for sea ice, the concentration-dependent shear component of the viscous-plastic model. For these two models, we explore the effects of large data errors. Finally, we infer an unknown concentration-dependent model that reproduces Lagrangian ice floe simulation data. Our method discovers a rheology that generalizes well outside of the training dataset and exhibits both shear-thickening and thinning behaviors depending on the concentrations.

</details>


### [16] [The $L_p$-error rate for randomized quasi-Monte Carlo self-normalized importance sampling of unbounded integrands](https://arxiv.org/abs/2511.10599)
*Jiarui Du,Zhijian He*

Main category: math.NA

TL;DR: Derives L_p-error rates for RQMC-based self-normalized importance sampling with unbounded integrands, establishing convergence rates under mild boundary growth conditions.


<details>
  <summary>Details</summary>
Motivation: Existing L_1 and L_2 error estimates for SNIS are limited to bounded integrands, leaving a gap for unbounded integrands especially under RQMC sampling.

Method: First establish L_p-error rates for plain RQMC integration, then extend to RQMC-SNIS estimators with broader class of transport maps for generating samples from RQMC points.

Result: Established L_p-error rate of order O(N^{-β+ε}) for RQMC-SNIS estimators, where β∈(0,1] depends on boundary growth rate and ε>0 is arbitrarily small.

Conclusion: Theoretical results are validated through numerical experiments, providing error bounds for RQMC-SNIS with unbounded integrands on unbounded domains.

Abstract: Self-normalized importance sampling (SNIS) is a fundamental tool in Bayesian inference when the posterior distribution involves an unknown normalizing constant. Although $L_1$-error (bias) and $L_2$-error (root mean square error) estimates of SNIS are well established for bounded integrands, results for unbounded integrands remain limited, especially under randomized quasi-Monte Carlo (RQMC) sampling. In this work, we derive $L_p$-error rate $(p\ge1)$ for RQMC-based SNIS (RQMC-SNIS) estimators with unbounded integrands on unbounded domains. A key step in our analysis is to first establish the $L_p$-error rate for plain RQMC integration. Our results allow for a broader class of transport maps used to generate samples from RQMC points. Under mild function boundary growth conditions, we further establish \(L_p\)-error rate of order \(\mathcal{O}(N^{-β+ ε})\) for RQMC-SNIS estimators, where $ε>0$ is arbitrarily small, $N$ is the sample size, and \(β\in (0,1]\) depends on the boundary growth rate of the resulting integrand. Numerical experiments validate the theoretical results.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [17] [Euler equation on a fast rotating ellipsoid](https://arxiv.org/abs/2511.09721)
*Haoran Wu*

Main category: math.AP

TL;DR: The paper extends the study of incompressible Euler equations from spheres to biaxial ellipsoids, showing that rapid rotation leads to zonal flows independent of longitude.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between spherical and ellipsoidal theories of fast rotating Euler dynamics by extending Cheng and Mahalov's work on spheres to more realistic ellipsoidal geometries.

Method: Adapting the framework from Cheng and Mahalov on fast rotating spheres and Xu on Rossby-Haurwitz solutions on ellipsoids to establish parallel results for Euler flows on rotating ellipsoidal surfaces.

Result: In rapid rotation regime, the time-averaged velocity field remains uniformly bounded in Sobolev norms and converges to a longitude-independent zonal flow depending only on latitude.

Conclusion: The zonalization phenomenon discovered on spheres persists on biaxial ellipsoids, confirming the robustness of this behavior across different geometries.

Abstract: This paper extends the analytical study of the incompressible Euler equations from the classical spherical setting to the more realistic geometry of a biaxial ellipsoid. Motivated by the work of Cheng and Mahalov on fast rotating spheres and Xu on Rossby-Haurwitz solutions on ellipsoids, we adapt their framework to establish a parallel result for Euler flows on a rotating ellipsoidal surface. In the regime of rapid rotation, we prove that the time-averaged velocity field remains uniformly bounded in Sobolev norms independent of the rotation rate and converges to a longitude-independent zonal flow depending only on latitude. This shows that the zonalization phenomenon discovered by Cheng and Mahalov on the sphere persists on biaxial ellipsoids, thereby bridging the gap between spherical and ellipsoidal theories of fast rotating Euler dynamics.

</details>


### [18] [Stability of the Rankine Vortex and Perimeter Growth in Vortex Patches](https://arxiv.org/abs/2511.09772)
*John Brownfield*

Main category: math.AP

TL;DR: The paper proves that the L^1 deviation from the Rankine vortex can be bounded by pseudo-energy deviation and angular momentum, with angular momentum dependence removable for m-fold symmetric cases. It confirms prior simulation results showing linear perimeter growth for perturbed Rankine vortices.


<details>
  <summary>Details</summary>
Motivation: To mathematically validate prior simulation results about vortex dynamics and provide rigorous bounds for deviations from the Rankine vortex model, particularly addressing perimeter growth in perturbed vortices.

Method: Mathematical proof establishing bounds for L^1 deviation from Rankine vortex using pseudo-energy deviation and angular momentum, with special treatment for m-fold symmetric cases.

Result: Demonstrated that L^1 deviation can be bounded by pseudo-energy deviation and angular momentum, and confirmed linear in time perimeter growth for simply connected perturbations of Rankine vortex.

Conclusion: The paper provides rigorous mathematical validation of prior simulation results and establishes important bounds for vortex dynamics, particularly confirming the linear perimeter growth phenomenon in perturbed Rankine vortices.

Abstract: We prove that for $ω: \mathbb{R}^2 \to [0,1]$ sharing the same total vorticity and center of vorticity as the Rankine vortex, the $L^1$ deviation from the Rankine patch can be bounded by a function of the pseudo-energy deviation and the angular momentum of $ω$. In the case of $m-$fold symmetry, the dependence on the angular momentum can be dropped. Using this, we affirm the results of prior simulations by demonstrating linear in time perimeter growth for a simply connected perturbation of the Rankine vortex.

</details>


### [19] [Analysis of the adhesion model and the reconstruction problem in cosmology](https://arxiv.org/abs/2511.09800)
*Jian-Guo Liu,Robert L. Pego*

Main category: math.AP

TL;DR: Analysis of mass flow in cosmology's adhesion model, showing unique Lagrangian semi-flow exists in zero-viscosity limit, with mass measure agreement in continuous parts but differences in singular parts during flow mergers.


<details>
  <summary>Details</summary>
Motivation: To understand mass concentration in cosmological structures using the adhesion model, which suppresses multi-streaming by introducing viscosity, and to study the zero-viscosity limit behavior.

Method: Analysis of Lagrangian advection in the zero-viscosity limit of the adhesion model, using differential inclusions and Monge-Ampère measures from optimal transport theory.

Result: A unique limiting Lagrangian semi-flow exists where particle paths stick after collision; continuous mass measures agree with Monge-Ampère measures but singular parts differ during flow mergers.

Conclusion: In regions of merging singular structures, exact reconstruction of monotone Lagrangian maps cannot be achieved even away from singularities, highlighting limitations in cosmological reconstruction methods.

Abstract: In cosmology, a basic explanation of the observed concentration of mass in singular structures is provided by the Zeldovich approximation, which takes the form of free-streaming flow for perturbations of a uniform Einstein-de Sitter universe in co-moving coordinates. The adhesion model suppresses multi-streaming by introducing viscosity. We study mass flow in this model by analysis of Lagrangian advection in the zero-viscosity limit. Under mild conditions, we show that a unique limiting Lagrangian semi-flow exists. Limiting particle paths stick together after collision and are characterized uniquely by a differential inclusion. The absolutely continuous part of the mass measure agrees with that of a Monge-Ampère measure arising by convexification of the free-streaming velocity potential. But the singular parts of these measures can differ when flows along singular structures merge, as shown by analysis of a 2D Riemann problem. The use of Monge-Ampère measures and optimal transport theory for the reconstruction of inverse Lagrangian maps in cosmology was introduced in work of Brenier & Frisch et al. (Month. Not. Roy. Ast. Soc. 346, 2003). In a neighborhood of merging singular structures in our examples, however, we show that reconstruction yielding a monotone Lagrangian map cannot be exact a.e., even off of the singularities themselves.

</details>


### [20] [Uniqueness results for positive harmonic functions on manifolds with nonnegative Ricci curvature and strictly convex boundary](https://arxiv.org/abs/2511.09994)
*Xiaohan Cai*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We prove some Liouville-type theorems for positive harmonic functions on compact Riemannian manifolds with nonnegative Ricci curvature and strictly convex boundary, thereby confirming some cases of Wang's conjecture (J. Geom. Anal. 31, 2021).
  We further investigate Wang's conjecture on warped product manifolds and provide a partial verification of this conjecture, which also yields an alternative proof of Gu-Li's resolution of the conjecture in the $\mathbb{B}^n$ case (Math. Ann. 391, 2025). Our approach is based on a general principle of employing the P-function method to such Liouville-type results, with particular emphasis on the role of a closed conformal vector field inherent to such manifolds.

</details>


### [21] [Locally uniform ellipticity of the fractional Hessian operators](https://arxiv.org/abs/2511.10034)
*Ziyu Gan,Heming Jiao*

Main category: math.AP

TL;DR: This paper introduces fractional analogues of general Hessian operators, proves their stability, and shows strict ellipticity for fractional k-Hessian operators with convex solutions for all 2≤k≤n.


<details>
  <summary>Details</summary>
Motivation: To extend previous work on fractional Monge-Ampère operators and fractional k-Hessian operators by introducing a more general fractional Hessian operator framework and addressing ellipticity properties.

Method: Developed fractional analogues of general Hessian operators, established stability results, and provided new proofs for ellipticity conditions including cases without convexity assumptions.

Result: Proved that fractional k-Hessian operators are strictly elliptic for convex solutions when 2≤k≤n, and provided a new proof for k=2 case without requiring convexity.

Conclusion: The paper successfully generalizes fractional Hessian operator theory, establishes key stability and ellipticity properties, and extends previous results to broader settings.

Abstract: In [1], Caffarelli-Charro introduced a fractional Monge-Ampère operator. Later, Wu [17] generalized it to a fractional analogue of $k$-Hessian operators and proved the strict ellipticity for $k=2$. In this paper, we introduce a fractional analogue of general Hessian operators and prove the stability. We also show that the fractional analogue $k$-Hessian operators defined in [17] are strictly elliptic with respect to convex solutions for all $2 \leq k \leq n$. Furthermore, we provide a new proof for the case $k=2$ without the convexity condition.

</details>


### [22] [A unified approach to Hardy-type inequalities with Bessel pairs](https://arxiv.org/abs/2511.10039)
*Lucrezia Cossetti,Lorenzo D'Arca*

Main category: math.AP

TL;DR: The paper characterizes Bessel pairs of weights (V,W) that ensure weighted Hardy-type inequalities, extending beyond Euclidean settings to general L^p frameworks with sharp constants.


<details>
  <summary>Details</summary>
Motivation: To establish a unified framework for weighted Hardy-type inequalities that generalizes and improves existing results, extending beyond classical Euclidean settings.

Method: Using an abstract approach to characterize Bessel pairs of weights (V,W) that guarantee weighted Hardy-type inequalities, with explicit expressions for maximizing functions.

Result: Obtained suitable characterizations of Bessel pairs ensuring weighted Hardy inequalities, with explicit maximizing functions and sharp constants in specific situations.

Conclusion: The approach unifies, generalizes, and improves several existing results in the literature on weighted Hardy-type inequalities.

Abstract: In this paper, we provide suitable characterisations of pairs of weights $(V,W),$ known as Bessel pairs, that ensure the validity of weighted Hardy-type inequalities. The abstract approach adopted here makes it possible to establish such inequalities also going beyond the classical Euclidean setting and also within a more general $L^p$ framework. As a byproduct of our method, we obtain explicit expressions for the maximizing functions and, in certain specific situations, we show that the associated constants are sharp. We emphasise that our approach unifies, generalises and improves several existing results in the literature.

</details>


### [23] [On Uniqueness For The Three-Dimensional Vlasov-Navier-Stokes System](https://arxiv.org/abs/2511.10099)
*D Han-Kwan,É Miot,A Moussa,I Moyano*

Main category: math.AP

TL;DR: Uniqueness of Leray solutions for 3D Vlasov-Navier-Stokes system is established beyond Osgood uniqueness class using Cannone-Meyer-Planchon class for fluid velocity.


<details>
  <summary>Details</summary>
Motivation: To study uniqueness of solutions to the three-dimensional Vlasov-Navier-Stokes system, going beyond the conventional Osgood uniqueness class.

Method: Use the Cannone-Meyer-Planchon class for the fluid velocity field to establish uniqueness.

Result: Uniqueness of Leray solutions is established, and a stability estimate is provided in this setting.

Conclusion: The Cannone-Meyer-Planchon class enables proving uniqueness for 3D Vlasov-Navier-Stokes system beyond the Osgood class, with stability guarantees.

Abstract: We study the problem of uniqueness of Leray solutions to the three-dimensional Vlasov-Navier-Stokes system. We establish uniqueness whenever the fluid velocity field belongs to the Cannone-Meyer-Planchon class, which allows to go beyond the Osgood uniqueness class. A stability estimate in this setting is also provided.

</details>


### [24] [Explicit pulsating fronts and minimal speeds in periodic Fisher-KPP equations](https://arxiv.org/abs/2511.10104)
*Lionel Roques*

Main category: math.AP

TL;DR: Exact pulsating traveling front solution found for Fisher-KPP equation with periodic diffusion and reaction terms through nonlinear transformation.


<details>
  <summary>Details</summary>
Motivation: To study Fisher-KPP equations with spatially periodic diffusion and reaction terms and identify conditions for explicit solutions.

Method: Nonlinear change of variables reduces the periodic problem to the homogeneous Fisher-KPP equation, enabling construction of exact pulsating traveling fronts.

Result: Derived explicit closed-form solution, asymptotic spreading speed expression, and established new asymptotic and comparison results.

Conclusion: Identified a class of periodic media allowing exact solutions and provided explicit expressions for traveling fronts and spreading speeds.

Abstract: We study a Fisher-KPP equation with spatially periodic diffusion and reaction terms. We identify a class of periodic media for which the equation admits an explicit, closed-form solution. Through a nonlinear change of variables, the problem is reduced to the homogeneous Fisher-KPP equation, allowing us to construct an exact pulsating traveling front that connects the positive periodic stationary state to 0. We also derive an explicit expression for the asymptotic spreading speed and establish new asymptotic and comparison results.

</details>


### [25] [On Rayleigh quotients connected to $p$-Laplace equations with polynomial nonlinearities](https://arxiv.org/abs/2511.10199)
*Vladimir Bobkov,Mieko Tanaka*

Main category: math.AP

TL;DR: The paper establishes a bijection between solutions of a p-Laplacian equation with two power nonlinearities and critical points of a 0-homogeneous Rayleigh quotient. This connection enables analysis of degenerate solutions, ground states, and sign-changing solutions across different parameter regimes.


<details>
  <summary>Details</summary>
Motivation: To understand the structure of solutions to p-Laplacian equations with competing nonlinearities by establishing a fundamental connection with a normalized Rayleigh quotient, allowing unified treatment of various parameter regimes including convex-concave, subhomogeneous, and superhomogeneous cases.

Method: Introduces a 0-homogeneous Rayleigh quotient R_α(u) and proves a bijection between solutions of the p-Laplacian equation and properly normalized critical points of R_α. Analyzes properties of this bijection and studies R_α for different parameter relationships between p, q, r.

Result: For convex-concave case (q<p<r), identifies all degenerate solutions as critical points of R_α with specific α=(r-p)/(r-q). In subhomogeneous case (q<r≤p), ground state level is simple and isolated, with minimizers exhausting sign-constant solutions. In superhomogeneous case (p<q<r), no sign-changing critical points exist near ground state level.

Conclusion: The bijection between p-Laplacian solutions and critical points of the normalized Rayleigh quotient provides a powerful framework for unified analysis of solution structure across different nonlinear regimes, enabling characterization of degenerate solutions, ground states, and sign-changing behavior.

Abstract: Let $Ω$ be a bounded open set and $p,q,r>1$. The main observation of the present work is the following: $W_0^{1,p}(Ω)$-solutions of the equation $-Δ_p u = μ|u|^{q-2}u + |u|^{r-2}u$ parameterized by $μ$ are in bijection with properly normalized critical points of the $0$-homogeneous Rayleigh type quotient $R_α(u)=\|\nabla u\|_p^p/ (\|u\|_q^{αp} \|u\|_r^{p-αp})$ parameterized by $α$. We study this bijection and properties of $R_α$ for various relations between $p,q,r$. In particular, for the generalized convex-concave problem (the case $q<p<r$) the bijection allows to provide the existence and characterization of all degenerate solutions corresponding to the inflection point of the fibred energy functional: they are critical points of $R_α$ exclusively with $α= (r-p)/(r-q)$. In the subhomogeneous case $q<r \leq p$ and under additional assumptions on $Ω$, the ground state level of $R_α$ is simple and isolated, and minimizers of $R_α$ exhaust the whole set of sign-constant solutions of the corresponding equation. In the superhomogeneous case $p < q<r$, there are no sign-changing critical points in a vicinity of the ground state level of $R_α$.

</details>


### [26] [Convergence of approximate solutions constructed by the finite volume method for the moisture transport model in porous media](https://arxiv.org/abs/2505.09763)
*Akiko Morimura,Toyohiko Aiki*

Main category: math.AP

TL;DR: Analysis of weak solution uniqueness and finite volume method convergence for a 1D nonlinear parabolic equation modeling moisture transport in porous media.


<details>
  <summary>Details</summary>
Motivation: Study motivated by mathematical modeling of moisture transport in porous media, addressing the initial-boundary value problem for a nonlinear parabolic equation.

Method: Used dual equation method to prove uniqueness of weak solutions and analyzed convergence of approximate solutions constructed with finite volume method.

Result: Established uniqueness of weak solutions and proved convergence of finite volume method approximations for the nonlinear parabolic equation.

Conclusion: Successfully demonstrated both uniqueness of solutions and convergence of numerical approximations for the moisture transport model in porous media.

Abstract: We consider the initial-boundary value problem for a nonlinear parabolic equation in the one-dimensional interval. This problem is motivated by a mathematical model for moisture transport in porous media. We establish the uniqueness of weak solutions to the problem by using the dual equation method. Moreover, we prove the convergence of approximate solutions constructed with the finite volume method.

</details>


### [27] [Observable sets for free Schrödinger equation on combinatorial graphs](https://arxiv.org/abs/2511.10358)
*Zhiqiang Wan,Heng Zhang*

Main category: math.AP

TL;DR: Study of observability for the free Schrödinger equation on combinatorial graphs, establishing sharp thresholds for thick sets on 1D lattice, showing finite complement observability on higher-dimensional lattices, and characterizing observability on finite graphs.


<details>
  <summary>Details</summary>
Motivation: To understand how the discrete structure of combinatorial graphs affects the observability properties of the Schrödinger equation, particularly comparing with continuous Euclidean space results.

Method: Mathematical analysis using spectral theory and graph Laplacian properties, studying thick sets and their observability thresholds on different graph structures including 1D lattice, higher-dimensional lattices, and finite graphs.

Result: Found sharp threshold γ=1/2 for thick sets on 1D lattice; showed complements of finite sets are observable on higher-dimensional lattices; characterized observability on finite graphs via Laplacian eigenfunctions; constructed unobservable dense sets on discrete tori.

Conclusion: Discrete graphs exhibit fundamentally different observability properties than continuous spaces, with exact thresholds and counterintuitive phenomena like unobservable dense sets on discrete tori.

Abstract: We study observability for the free Schrödinger equation $\partial_t u = iΔu$ on combinatorial graphs $G=(\mathcal{V},\mathcal{E})$. A subset $E\subset\mathcal{V}$ is observable at time $T>0$ if there exists $C(T,E)>0$ such that for all $u_0\in l^2(\mathcal{V})$, $$ \|u_0\|_{l^2(\mathcal{V})}^2 \le C(T,E)\int_0^T \|e^{itΔ}u_0\|_{l^2(E)}^2\,dt. $$
  On the one-dimensional lattice $\mathbb{Z}$ we obtain a sharp threshold for thick sets: if $E\subset\mathbb{Z}$ is $γ$-thick with $γ\geq1/2$, then $E$ is observable at some time; conversely, for every $γ<1/2$ there exists a $γ$-thick set that is not observable at any time. This critical threshold marks the exact point where the discrete lattice departs from the real line: on the lattice it must be attained, whereas on $\R$ any $γ$-thick set with $γ>0$ already suffices.
  On $\mathbb{Z}^d$ we show that complements of finite sets are observable at any time $T>0$. This is a similar result to Euclidean space $\R^d$: any set that contains the exterior of a finite ball is observable at any time, in analogy with the free Schrödinger flow on $\mathbb{R}^d$.
  For finite graphs we give an equivalent characterization of observability in terms of the zero sets of Laplacian eigenfunctions. As an application, we construct unobservable sets of large density on discrete tori, in contrast with the continuous torus $\mathbb{T}^d$, where every nonempty open set is observable.

</details>


### [28] [An initial-boundary value problem describing moisture transport in porous media: existence of strong solutions and an error estimate for a finite volume scheme](https://arxiv.org/abs/2511.10378)
*Akiko Morimura,Toyohiko Aiki*

Main category: math.AP

TL;DR: Existence of strong solutions and error estimates for finite volume method in moisture transport problems in porous media, using Gagliardo-Nirenberg inequalities.


<details>
  <summary>Details</summary>
Motivation: To analyze moisture transport in porous media through mathematical modeling, focusing on solution existence and numerical approximation accuracy.

Method: Finite volume method for constructing approximate solutions, with proof techniques relying on Gagliardo-Nirenberg type inequalities for error analysis.

Result: Established existence of strong solutions and provided error estimates for the finite volume approximations.

Conclusion: The finite volume method is effective for solving moisture transport problems, with rigorous error bounds supported by functional inequalities.

Abstract: We consider an initial-boundary value problem motivated by a mathematical model of moisture transport in porous media. We establish the existence of strong solutions and provide an error estimate for the approximate solutions constructed by the finite volume method. In the proof of the error estimate, the Gagliardo--Nirenberg type inequality for the difference between a continuous function and a piecewise constant function plays an important role.

</details>


### [29] [Restriction estimates for 2D surfaces of finite type 3 and applications to dispersive equations](https://arxiv.org/abs/2511.10538)
*Jiajun Wang*

Main category: math.AP

TL;DR: The paper proves restriction estimates for 2D surfaces defined by (ξ₁, ξ₂, ξ₁³ ± ξ₂³) by leveraging existing results on perturbed paraboloids and hyperboloids, and applies these estimates to improve analysis of discrete nonlinear Schrödinger equations.


<details>
  <summary>Details</summary>
Motivation: To establish restriction estimates for specific 2D surfaces that combine cubic terms, which are important for understanding wave propagation and have applications in analyzing discrete nonlinear Schrödinger equations.

Method: Uses rescaling technique from previous work [LMZ21a] and reduces the problem to Guth's result on perturbed paraboloid and Buschenhenke-Muller-Vargas's result on perturbed hyperboloid.

Result: Successfully proves restriction estimates for the surfaces S = {(ξ₁, ξ₂, ξ₁³ ± ξ₂³) : (ξ₁, ξ₂) ∈ [0,1]²}.

Conclusion: The established restriction estimates provide a foundation for improved analysis of discrete nonlinear Schrödinger equations, demonstrating the utility of the rescaling technique and connections to existing results on perturbed surfaces.

Abstract: In this paper, we prove the restriction estimates for 2D surfaces S := {(xi1, xi2, xi1^3 +/- xi2^3) : (xi1, xi2) in [0,1]^2} by reducing to Guth's result on the perturbed paraboloid and Buschenhenke-Muller-Vargas's result on the perturbed hyperboloid. The method is based on the rescaling technique, developed in [LMZ21a]. Besides, we will use the estimates to give a better analysis for discrete nonlinear Schrodinger equations.

</details>


### [30] [Thin shell limit and the derivation of the viscosity operator on the ellipsoid](https://arxiv.org/abs/2511.10579)
*Chi Hin Chan,Magdalena Czubak,Padi Fuster Aguilera*

Main category: math.AP

TL;DR: Derived four new intrinsic viscosity operators for ellipsoids using thin shell limit heuristic along scaling direction, showing dependence on averaging method and considering both Navier and Hodge boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To develop improved viscosity operators for ellipsoidal geometries by leveraging the thin shell limit approach and understanding how different averaging methods affect the results.

Method: Used thin shell limit heuristic along ellipsoid scaling direction, performed asymptotic expansion analysis, considered both homogeneous Navier and Hodge boundary conditions, and obtained geometric representations of boundary conditions.

Result: Successfully derived four new candidate operators for intrinsic viscosity on ellipsoids and demonstrated that the thin shell limit method's outcome depends on the specific averaging approach used.

Conclusion: The thin shell limit provides a viable heuristic for deriving viscosity operators on ellipsoids, but the results are sensitive to the choice of averaging method and boundary conditions, with both Navier and Hodge conditions yielding meaningful geometric representations.

Abstract: In this paper we derive four new candidates for an intrinsic viscosity operator on an ellipsoid by using the heuristic of the thin shell limit along the scaling direction of the ellipsoid. We show that the general method of the thin shell limit through the asymptotic expansion depends on the averaging method used. We consider both the homogeneous Navier and Hodge boundary conditions. We also obtain a geometric representation of these two boundary conditions.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [31] [Parallel and GPU accelerated code for phase-field and reaction-diffusion simulations](https://arxiv.org/abs/2511.10508)
*Steven A. Silber,Mikko Karttunen*

Main category: physics.comp-ph

TL;DR: SymPhas 2.0 is a major update to a symbolic algebra simulation framework for phase-field and reaction-diffusion models, featuring compile-time functional differentiation, tensor expressions, arbitrary-order finite differences, and GPU acceleration with CUDA, achieving up to 1000x speedup.


<details>
  <summary>Details</summary>
Motivation: To create a more efficient and scalable framework for phase-field and reaction-diffusion simulations by enabling direct model definition from free-energy functionals and leveraging GPU computing for large-scale simulations.

Method: Introduces compile-time symbolic algebra with functional differentiation, directional derivatives, symbolic summation, tensor expressions, and derived finite difference stencils. Parallelized for CPUs with MPI and added GPU computing via CUDA kernel compilation.

Result: Achieved speedups up to ~1000x compared to previous version for large systems (32,768² in 2D and 1,024³ in 3D with double precision). Enables efficient execution of large-scale simulations entirely on GPU.

Conclusion: SymPhas 2.0 establishes itself as a flexible and scalable framework for efficient implementation of phase-field and reaction-diffusion models on GPU-based high-performance computing platforms.

Abstract: We present SymPhas 2.0, a major update of the compile-time symbolic algebra simulation framework SymPhas for phase-field and reaction-diffusion models. This release introduces significant expansions and enhancements that enable the definition of a phase-field model directly from the free-energy functional via compile-time evaluated functional differentiation. It also introduces directional derivatives, symbolic summation, tensor-valued expressions, and compile-time derived finite difference stencils of arbitrary order and accuracy. Furthermore, the code has been parallelized for CPUs with MPI, and GPU computing has been added using CUDA (Compute Unified Device Architecture). For the latter, symbolic expressions are compiled into optimized CUDA kernels, allowing large-scale simulations to execute entirely on the GPU. For large systems ($32,768^2$ in 2D and $1,024^3$ in 3D with double precision), speedups up to $\sim \!\!1,000 \times$ were obtained compared to the first version of SymPhas using multi-threaded CPU execution on a single system. These developments establish SymPhas 2.0 as a flexible and scalable framework for efficient implementation of phase-field and reaction-diffusion models on GPU-based high-performance computing platforms.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [32] [The Data Fusion Labeler (dFL): Challenges and Solutions to Data Harmonization, Labeling, and Provenance in Fusion Energy](https://arxiv.org/abs/2511.09725)
*Craig Michoski,Matthew Waller,Brian Sammuli,Zeyu Li,Tapan Ganatma Nakkina,Raffi Nazikian,Sterling Smith,David Orozco,Dongyang Kuang,Martin Foltin,Erik Olofsson,Mike Fredrickson,Jerry Louis-Jeune,David R. Hatch,Todd A. Oliver,Mitchell Clark,Steph-Yves Louis*

Main category: physics.plasm-ph

TL;DR: The Data Fusion Labeler (dFL) is a unified workflow tool that enables uncertainty-aware data harmonization, schema-compliant data fusion, and provenance-rich labeling for fusion energy research, reducing analysis time by over 50x.


<details>
  <summary>Details</summary>
Motivation: Fusion energy research faces challenges integrating heterogeneous, multimodal datasets from diagnostics, control systems, and simulations, requiring new tools for systematic knowledge extraction across diverse data modalities.

Method: dFL performs uncertainty-aware data harmonization, schema-compliant data fusion, and provenance-rich manual/automated labeling within a reproducible, operator-order-aware framework.

Result: dFL reduces time-to-analysis by >50x (enabling >200 shots/hour vs. handful per day), enhances label/training quality, and enables cross-device comparability. Case studies show successful ELM detection and confinement regime classification.

Conclusion: dFL demonstrates potential as a core component for data-driven discovery, model validation, and real-time control in future burning plasma devices.

Abstract: Fusion energy research increasingly depends on the ability to integrate heterogeneous, multimodal datasets from high-resolution diagnostics, control systems, and multiscale simulations. The sheer volume and complexity of these datasets demand the development of new tools capable of systematically harmonizing and extracting knowledge across diverse modalities. The Data Fusion Labeler (dFL) is introduced as a unified workflow instrument that performs uncertainty-aware data harmonization, schema-compliant data fusion, and provenance-rich manual and automated labeling at scale. By embedding alignment, normalization, and labeling within a reproducible, operator-order-aware framework, dFL reduces time-to-analysis by greater than 50X (e.g., enabling >200 shots/hour to be consistently labeled rather than a handful per day), enhances label (and subsequently training) quality, and enables cross-device comparability. Case studies from DIII-D demonstrate its application to automated ELM detection and confinement regime classification, illustrating its potential as a core component of data-driven discovery, model validation, and real-time control in future burning plasma devices.

</details>


### [33] [Plasma hydrodynamics from mean force kinetic theory](https://arxiv.org/abs/2511.09786)
*Jarett LeVan,Scott D. Baalrud*

Main category: physics.plasm-ph

TL;DR: Mean force kinetic theory extends hydrodynamics to dense plasmas, accurately predicting transport coefficients up to high Coulomb coupling strengths (Γ≈20), validated by molecular dynamics simulations.


<details>
  <summary>Details</summary>
Motivation: Traditional theories for plasma transport coefficients break down at low Coulomb coupling strengths, limiting their applicability to dense plasmas where strong interactions dominate.

Method: Used mean force kinetic theory to compute transport coefficients and compared results with molecular dynamics simulations using repulsive Coulomb force interactions.

Result: Excellent agreement found between mean force kinetic theory and molecular dynamics simulations for all transport coefficients up to Γ≈20, which is 100x higher than traditional theory limits.

Conclusion: Mean force kinetic theory successfully extends hydrodynamic descriptions to dense plasmas, providing accurate transport coefficients at high coupling strengths where conventional approaches fail.

Abstract: Mean force kinetic theory is used to evaluate the electrical conductivity, thermal conductivity, electrothermal coefficient, thermoelectric coefficient, and shear viscosity of a two-component (ion-electron) plasma. Results are compared with molecular dynamics simulations. These simulations are made possible by assuming a repulsive Coulomb force for all interactions. Good agreement is found for all coefficients up to a Coulomb coupling strength of $Γ\approx 20$. This is over 100-times larger than the coupling strength at which traditional theories break down. It is concluded that mean force kinetic theory provides a means to extend hydrodynamics to dense plasmas.

</details>


### [34] [Molecular Dynamics Simulation of Hydrodynamic Transport Coefficients in Plasmas](https://arxiv.org/abs/2511.09787)
*Briggs Damman,Jarett LeVan,Scott Baalrud*

Main category: physics.plasm-ph

TL;DR: MD simulations compute transport coefficients in two-component plasmas using Green-Kubo formalism across coupling strengths 0.01≤Γ≤140, validating Chapman-Enskog theory in weak coupling and revealing limitations in strong coupling.


<details>
  <summary>Details</summary>
Motivation: To test standard Chapman-Enskog theory results in weakly coupled plasmas using first-principles MD simulations and investigate transport coefficient behavior across the full coupling strength range.

Method: Molecular dynamics simulations with Green-Kubo formalism to calculate thermal conductivity, electrical conductivity, electrothermal coefficient, thermoelectric coefficient, and shear viscosity for repulsive Coulomb potential plasmas.

Result: Good agreement with Chapman-Enskog theory for Γ≲0.1 only with careful attention to linear constitutive relation definitions. Electrical conductivity works across all coupling strengths, but other coefficients' potential and virial components diverge in strong coupling (Γ≫1).

Conclusion: Only kinetic components of transport coefficients remain meaningful for classical plasmas in the strongly coupled regime, while careful definition of constitutive relations is crucial for meaningful comparisons in weak coupling.

Abstract: Molecular dynamics (MD) simulations are used to calculate transport coefficients in a two-component plasma interacting through a repulsive Coulomb potential. The thermal conductivity, electrical conductivity, electrothermal coefficient, thermoelectric coefficient, and shear viscosity are computed using the Green-Kubo formalism over a broad range of Coulomb coupling strength, $0.01 \leq Γ\leq 140$. Emphasis is placed on testing standard results of the Chapman-Enskog solution in the weakly coupled regime ($Γ\ll 1$) using these first-principles simulations. As expected, the results show good agreement for $Γ\lesssim 0.1$. However, this agreement is only possible if careful attention is paid to the definitions of linear constitutive relations in each of the theoretical models, a point that is often overlooked. For example, the standard Green-Kubo expression for thermal conductivity is a linear combination of thermal conductivity, electrothermal and thermoelectric coefficients computed in the Chapman-Enskog formalism. Meaningful results for electrical conductivity are obtained over the full range of coupling strengths explored, but it is shown that potential and virial components of the other transport coefficients diverge in the strongly coupled regime ($Γ\gg 1$). In this regime, only the kinetic components of the transport coefficients are meaningful for a classical plasma.

</details>


### [35] [$π$-PIC: a framework for modular particle-in-cell developments and simulations](https://arxiv.org/abs/2511.09950)
*Frida Brogren,Christoffer Olofsson,Joel Magnusson,Arkady Gonoskov*

Main category: physics.plasm-ph

TL;DR: A Python-controlled framework for particle-in-cell (PIC) methods that enables unified testing and comparison of novel PIC solvers and extensions across different programming languages.


<details>
  <summary>Details</summary>
Motivation: To facilitate dissemination and adoption of recent PIC method advancements that overcome limitations like exact conservation of physical quantities and unbiased ensemble down-sampling, enabling interactive studies on personal computers and large-scale parameter scans.

Method: Developed a Python-controlled framework with unified interfaces for accommodating, cross-testing, and comparing PIC solvers and extensions written in Python, C++, or Fortran.

Result: Successfully implemented and tested several PIC solvers and extensions capable of managing QED processes, moving-window techniques, and tight focusing of laser pulses, demonstrating framework flexibility.

Conclusion: The proposed framework provides a flexible platform for advancing PIC method development and adoption, supporting next-generation PIC codes with improved computational efficiency.

Abstract: Recently proposed modifications of the standard particle-in-cell (PIC) method resolve long-standing limitations such as exact preservation of physically conserved quantities and unbiased ensemble down-sampling. Such advances pave the way for next-generation PIC codes capable of using lower resolution and fewer particles per cell, enabling interactive studies on personal computers and facilitating large-scale parameter scans on supercomputers. Here, we present a Python-controlled framework designed to stimulate the dissemination and adoption of novel PIC developments by providing unified interfaces for accommodation, cross-testing, and comparison of PIC solvers and extensions written in Python or low-level languages like C++ and Fortran. To demonstrate flexibility of proposed interfaces we present and test implementations of several PIC solvers, as well as of extensions that is capable of managing QED processes, moving-window, and tight focusing of laser pulses.

</details>


### [36] [Tailored Three Dimensional Betatron Dynamics in UltraStable Hybrid Laser Plasma RF Accelerators](https://arxiv.org/abs/2511.10096)
*A. A. Molavi Choobini,M. Shahmansouri*

Main category: physics.plasm-ph

TL;DR: Hybrid laser plasma RF accelerators enable precise control over transverse beam dynamics, betatron polarization, and radiation reaction through RF field manipulation and resonant alignment.


<details>
  <summary>Details</summary>
Motivation: To understand and control the complex dynamics of ultra-relativistic electron bunches in hybrid laser plasma RF accelerators, including transverse focusing, betatron oscillations, and radiation effects.

Method: Combines analytical models of plasma wakefield modulation, RF-driven oscillations, and quantum-corrected radiation reaction with 3D particle-in-cell simulations using EPOCH code.

Result: RF parameters enable precise control over transverse focusing, betatron amplitudes, and polarization states. Resonant alignment amplifies transverse excursions while damping parasitic oscillations, reducing emittance and energy losses.

Conclusion: The study provides comprehensive understanding of nonlinear, resonant, and damping phenomena in hybrid systems, demonstrating full control over transverse, longitudinal, and polarization dynamics in ultra-relativistic electron beams.

Abstract: The detailed theoretical and numerical investigation of hybrid laser plasma RF accelerators, elucidating the mechanisms governing transverse beam dynamics, betatron polarization, and radiation reaction in ultra-relativistic electron bunches is presented. This framework combines analytical models of spatiotemporal plasma wakefield modulation, phase-dependent RF-driven oscillations, and quantum-corrected Landau Lifshitz radiation reaction with fully self-consistent 3D particle in cell simulations using EPOCH. The results demonstrate that RF amplitude, frequency, and phase enable precise control over transverse focusing strengths, betatron oscillation amplitudes, and polarization states. Resonant alignment between RF fields and natural betatron frequencies amplifies transverse excursions while damping parasitic oscillations through enhanced focusing gradients and radiation reaction, yielding reductions in emittance and mitigation of synchrotron-like energy losses. Stability maps and 3D force landscapes reveal strong phase sensitivity, where initial conditions and RF component ratios govern the temporal evolution of betatron amplitudes, and longitudinal field gradients modulate γ growth rates. These findings provide a comprehensive picture of nonlinear, resonant, and damping phenomena in hybrid laser plasma RF systems, highlighting the full spectrum of controllable transverse, longitudinal, and polarization dynamics in ultra relativistic electron beams.

</details>


### [37] [Microscopy X-ray Imaging enriched with Small Angle X-ray Scattering for few nanometer resolution reveals shock waves and compression in intense short pulse laser irradiation of solids](https://arxiv.org/abs/2511.10127)
*Thomas Kluge,Arthur Hirsch-Passicos,Jannis Schulz,Mungo Frost,Eric Galtier,Maxence Gauthier,Jörg Grenzer,Christian Gutt,Lingen Huang,Uwe Hübner,Megan Ikeya,Hae Ja Lee,Dimitri Khaghani,Willow Moon Martin,Brian Edward Marré,Motoaki Nakatsutsumi,Paweł Ordyna,Franziska-Luise Paschke-Brühl,Alexander Pelka,Lisa Randolph,Hans-Peter Schlenvoigt,Christopher Schoenwaelder,Michal Šmíd,Long Yang,Ulrich Schramm,Thomas E. Cowan*

Main category: physics.plasm-ph

TL;DR: Combined X-ray imaging and small-angle X-ray scattering enables multiscale observation of laser-driven shock formation in copper with nanometer precision, revealing compression steepening into sharp shock fronts.


<details>
  <summary>Details</summary>
Motivation: To understand laser pulse compression of solids into high-energy-density states requires diagnostics that can resolve both macroscopic geometry and nanometer-scale structure simultaneously.

Method: Used combined X-ray imaging (XRM) and small-angle X-ray scattering (SAXS) at LCLS to probe 25-micrometer copper wires irradiated with 45-fs laser pulses, visualizing ablation and compression with 200-nm resolution while quantifying nanometer-scale sharpness.

Result: Revealed that initially smooth compression steepens into nanometer-sharp shock front after ~18 ps, reaching ~25 km/s velocity and lateral width of tens of micrometers - first direct observation of shock formation at solid density with few-nanometer precision.

Conclusion: The integrated XRM-SAXS method establishes quantitative multiscale diagnostic for laser-driven shocks in dense plasmas, relevant to inertial confinement fusion, warm dense matter, and planetary physics.

Abstract: Understanding how laser pulses compress solids into high-energy-density states requires diagnostics that simultaneously resolve macroscopic geometry and nanometer-scale structure. Here we present a combined X-ray imaging (XRM) and small-angle X-ray scattering (SAXS) approach that bridges this diagnostic gap. Using the Matter in Extreme Conditions end station at LCLS, we irradiated 25-micrometer copper wires with 45-fs, 0.9-J, 800-nm pulses at 3.5e19 W/cm2 while probing with 8.2-keV XFEL pulses. XRM visualizes the evolution of ablation, compression, and inward-propagating fronts with about 200-nm resolution, while SAXS quantifies their nanometer-scale sharpness through the time-resolved evolution of scattering streaks. The joint analysis reveals that an initially smooth compression steepens into a nanometer-sharp shock front after roughly 18 ps, consistent with an analytical steepening model and hydrodynamic simulations. The front reaches a velocity of about 25 km/s and a lateral width of several tens of micrometers, demonstrating for the first time the direct observation of shock formation and decay at solid density with few-nanometer precision. This integrated XRM-SAXS method establishes a quantitative, multiscale diagnostic of laser-driven shocks in dense plasmas relevant to inertial confinement fusion, warm dense matter, and planetary physics.

</details>


### [38] [Data-driven multi-species heat flux closures for two-stream-unstable plasmas with nonlinear sparse regression](https://arxiv.org/abs/2511.10147)
*Emil R. Ingelsten,Madox C. McGrae-Menge,E. Paulo Alves,Istvan Pusztai*

Main category: physics.plasm-ph

TL;DR: This paper extends a previously developed data-driven heat flux closure model from single-species to multi-species plasma modeling, using neural networks and nonlinear sparse regression to achieve 80-90% accuracy in heat flux predictions.


<details>
  <summary>Details</summary>
Motivation: To improve computational plasma physics by developing accurate yet computationally efficient fluid models that can capture kinetic physics through better closures, particularly for multi-scale collisionless processes.

Method: Generalized a six-term closure model from single- to multi-species modeling using OSIRIS particle-in-cell simulation data. Employed neural networks and nonlinear sparse regression to estimate unknown coefficients from box-averaged fluid quantities.

Result: The resulting models predict heat flux for each species with 80-90% accuracy and account for 85-95% of the pressure rate of change. Models were validated against multi-species linear collisionless theory.

Conclusion: The data-driven approach successfully extends heat flux closure modeling to multi-species plasmas, achieving high accuracy while maintaining computational efficiency, with good agreement with theoretical predictions.

Abstract: The dual aims of accuracy and computational efficiency in computational plasma physics lend themselves well to the use of fluid models. The first of these goals, however, is only satisfied for such models insofar as the utilized closure can capture the neglected kinetic physics -- something which has proven challenging for multi-scale collisionless processes. In a recent article [E. R. Ingelsten et al. (2025) J. Plasma Phys. 91 E64], we used the data-driven method of sparse regression to discover a novel heat flux closure for electrostatic phenomena. Here, we generalize the six-term closure model found in that work from single- to multi-species modeling. Using data from OSIRIS particle-in-cell simulations over a range of initial conditions, we then demonstrate how the unknown coefficients in front of the three most important terms in the closure can be estimated from box-averaged fluid quantities. Both neural networks and a newly developed framework for nonlinear sparse regression are showcased. The resulting models predict the heat flux for each species with a typical accuracy of 80-90 % and regularly account for 85-95 % of the rate of change in the pressure. The models are also compared with results from multi-species linear collisionless theory.

</details>


### [39] [Self-organisation through layering of $β$-plane like turbulence in plasmas and geophysical fluids](https://arxiv.org/abs/2511.10438)
*P. L. Guillon,G. Dif-Pradalier,Y. Sarazin,D. W. Hughes,Ö. D. Gürcan*

Main category: physics.plasm-ph

TL;DR: This paper studies staircase formation and layering in plasma and geophysical fluid models, comparing turbulent self-organization with different energy production mechanisms (forcing vs instability) and zonal flow responses.


<details>
  <summary>Details</summary>
Motivation: To understand how different mechanisms of free energy production and zonal flow responses affect turbulent self-organization and staircase formation in plasmas and geophysical fluids.

Method: Study staircase formation in standard and modified Charney-Hasegawa-Mima equations with stochastic forcing, and compare with two instability-driven models (plasma and geophysical contexts) using potential vorticity conserving models.

Result: β-plane turbulence with standard zonal response gradually forms large-scale elliptic zonal structures that merge progressively. Plasma systems with modified zonal response rapidly form straight, stationary jets of defined size. Instability-driven plasma systems show phase transitions between zonal flow and eddy dominated states.

Conclusion: Different zonal flow responses lead to distinct staircase formation patterns: gradual merging of elliptic structures in geophysical systems vs rapid formation of straight jets in plasma systems, with plasma systems exhibiting phase transitions between flow regimes.

Abstract: Staircase formation and layering is studied in simplified, potential vorticity conserving models of plasmas and geophysical fluids, by investigating turbulent self-organisation and nonlinear saturation with different mechanisms of free energy production -- forcing or linear instability -- and with standard or modified zonal flow responses. To this end, staircase formation in both the standard and modified Charney-Hasegawa-Mima equations with stochastic forcing, along with two different simple instability driven models -- one from a plasma and from a geophysical context -- are studied and compared. In these studies, it is observed that $β$-plane turbulence that does not distinguish between zonal and non-zonal perturbations (i.e., standard zonal response) gradually forms large-scale, elliptic zonal structures that merge progressively, regardless of whether it is driven by forcing (though it should be slow enough to allow wave couplings) or by the baroclinic instability, using for example a two-layer model. Conversely, the plasma system, with its modified zonal response, can rapidly form straight, stationary jets of well-defined size, again regardless of the way it is driven: by stochastic forcing or by the dissipative drift instability. Furthermore, the instability-driven plasma system exhibits a phase transition between a zonal flow dominated state and an eddy dominated state. In both states, saturation is possible without large-scale friction.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [40] [Dynamical functionals on ancient ARF Ricci flows](https://arxiv.org/abs/2511.10460)
*Isaac M. Lopez,Rio Schillmoeller*

Main category: math.DG

TL;DR: Introduces a dynamical energy functional on compact ancient asymptotically Ricci-flat Ricci flows using conjugate heat flows, with steady Ricci breather rigidity and bounds on λ-functional.


<details>
  <summary>Details</summary>
Motivation: To develop analytical tools for studying Ricci flows, particularly ancient asymptotically Ricci-flat flows, by leveraging conjugate heat flows and extending work by Colding and Minicozzi.

Method: Uses limits of conjugate heat flows to define a dynamical energy functional, analyzes its properties including steady Ricci breather rigidity, and derives local eigenvalue estimates for normalized Ricci flows coupled with conjugate heat flows.

Result: The dynamical energy functional satisfies steady Ricci breather-type rigidity, provides an upper bound for the ordinary λ-functional while preserving its properties, and yields local eigenvalue estimates.

Conclusion: The introduced dynamical energy functional and derived eigenvalue estimates provide new analytical tools for studying Ricci flows, particularly in the context of ancient asymptotically Ricci-flat flows and their geometric properties.

Abstract: We introduce a dynamical energy functional on compact ancient asymptotically Ricci-flat Ricci flows with modest decay using limits of conjugate heat flows. This functional satisfies a steady Ricci breather-type rigidity and provides an upper bound for the ordinary $λ$-functional while retaining many of its properties. In addition, motivated by work of Colding and Minicozzi, we derive local eigenvalue estimates for normalized Ricci flows coupled with conjugate heat flows.

</details>


### [41] [On smooth approximation of integral cycles mod 2](https://arxiv.org/abs/2511.10545)
*Gianmarco Caldini*

Main category: math.DG

TL;DR: Unoriented smooth approximation theorem for mod 2 integral cycles in Riemannian manifolds, showing cycles can be approximated by smooth submanifolds with controlled singular sets.


<details>
  <summary>Details</summary>
Motivation: To establish a smooth approximation result for mod 2 integral cycles that parallels existing oriented versions, providing geometric representations with controlled singularities.

Method: Using flat norm approximation techniques and geometric measure theory to construct smooth submanifolds that approximate given mod 2 integral cycles, with careful control over singular sets based on codimension.

Result: Every mod 2 integral cycle can be approximated by a smooth submanifold with singular set of codimension 3, and if a smooth embedded representative exists, the approximation can be made completely smooth.

Conclusion: This completes the unoriented counterpart to smooth approximation theorems for integral cycles, providing geometric representations with optimal singularity control in the mod 2 setting.

Abstract: We prove that every mod 2 integral cycle $T$ in a Riemannian manifold $\mathcal{M}$ can be approximated in flat norm by a cycle which is a smooth submanifold $Σ$ of nearly the same area, up to a singular set of codimension 3; in addition, this estimate on the singular set can be refined depending on the codimension of the cycle. Moreover, if the mod 2 homology class $τ$ admits a smooth embedded representative, then $Σ$ can be chosen free of singularities. This article provides the unoriented version of the smooth approximation theorem for integral cycles.

</details>


### [42] [Sign-changing solutions to the Escobar problem on manifolds with boundary](https://arxiv.org/abs/2511.10553)
*Mónica Clapp,Benedetta Pellacci,Angela Pistoia*

Main category: math.DG

TL;DR: Existence of least-energy sign-changing solutions for Escobar's conformal problem in two cases: scalar-flat problem and minimal boundary problem, particularly when n≥7 with nonumbilic boundary or n≥5 on the unit ball.


<details>
  <summary>Details</summary>
Motivation: While existence of positive solutions to Escobar's conformal problem is well understood, the existence of sign-changing (nodal) solutions remains largely open and unexplored.

Method: Variational approach analyzing suitable conformal invariants and sharp energy estimates derived from Escobar's work.

Result: Proved existence of least-energy nodal solutions for both scalar-flat and minimal boundary problems when n≥7 with nonumbilic boundary, and infinite sign-changing solutions for minimal boundary problem on unit ball when n≥5.

Conclusion: The work successfully establishes existence of sign-changing solutions in important special cases of Escobar's conformal problem, advancing understanding beyond positive solutions.

Abstract: Let $(M, g)$ be a $n-$dimensional compact Riemannian manifold with boundary. The Escobar problem concerning the existence of a metric conformally equivalent to $g$ having constant scalar curvature on $M$ and constant mean curvature on its boundary is equivalent, in analytic terms, to finding a positive solution to a nonlinear boundary-value problem with critical growth. While the existence of positive solutions to this problem is by now well understood, the existence of sign-changing (nodal) solutions remains largely open. In this work we establish the existence of least-energy sign-changing solutions in two particular cases: the scalar-flat problem, where the scalar curvature on $M$ is zero and the mean curvature of its boundary is constant, and the minimal boundary problem, where the mean curvature of the boundary vanishes and the scalar curvature of $M$ is constant. More precisely, we prove that if $n\ge7$ and $M$ has a nonumbilic boundary point, then both problems admit least-energy nodal solutions. In addition, we show that when $n\ge5$, the minimal boundary problem possesses infinitely many sign-changing solutions on the unit ball. Our approach is variational and relies on the analysis of suitable conformal invariants and sharp energy estimates derived from Escobar's work.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [43] [Sample Complexity of Quadratically Regularized Optimal Transport](https://arxiv.org/abs/2511.09807)
*Alberto González-Sanz,Eustasio del Barrio,Marcel Nutz*

Main category: math.ST

TL;DR: Quadratically regularized optimal transport (QOT) has parametric sample complexity despite lacking smooth potentials and strong concavity, avoiding the curse of dimensionality through analysis of optimal coupling support regularity.


<details>
  <summary>Details</summary>
Motivation: EOT avoids curse of dimensionality but produces overspread approximations and is computationally unstable for small regularization. QOT produces sparse approximations and is stable, but was assumed to suffer from curse of dimensionality due to non-smooth potentials and lack of strong concavity.

Method: Analyze QOT sample complexity using novel arguments focusing on regularity of optimal coupling support, establishing Lipschitz property of its sections and leveraging VC theory to bound statistical complexity.

Result: QOT has parametric sample complexity, with central limit theorems established for dual potentials, optimal couplings, and optimal costs. Gradient estimates show C^{1,1} regularity of population potentials.

Conclusion: QOT avoids the curse of dimensionality despite lacking the smoothness properties of EOT, making it a viable alternative with sparse approximations and computational stability.

Abstract: It is well known that optimal transport suffers from the curse of dimensionality: when the prescribed marginals are approximated by i.i.d. samples, the convergence of the empirical optimal transport problem to the population counterpart slows exponentially with increasing dimension. Entropically regularized optimal transport (EOT) has become the standard bearer in many statistical applications as it avoids this curse. Indeed, EOT has parametric sample complexity, as has been shown in a series of works based on the smoothness of the EOT potentials or the strong concavity of the dual EOT problem. However, EOT produces full-support approximations to the (sparse) OT problem, leading to overspreading in applications, and is computationally unstable for small regularization parameters. The most popular alternative is quadratically regularized optimal transport (QOT), which penalizes couplings by $L^2$ norm instead of relative entropy. QOT produces sparse approximations of OT and is computationally stable. However, its potentials are not smooth (do not belong to a Donsker class) and its dual problem is not strongly concave, hence QOT is often assumed to suffer from the curse of dimensionality. In this paper, we show that QOT nevertheless has parametric sample complexity. More precisely, we establish central limit theorems for its dual potentials, optimal couplings, and optimal costs. Our analysis is based on novel arguments that focus on the regularity of the support of the optimal QOT coupling. Specifically, we establish a Lipschitz property of its sections and leverage VC theory to bound its statistical complexity. Our analysis also leads to gradient estimates of independent interest, including $C^{1,1}$ regularity of the population potentials.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [44] [Learning of Statistical Field Theories](https://arxiv.org/abs/2511.09859)
*Shreya Shukla,Abhijith Jayakumar,Andrey Y. Lokhov*

Main category: cond-mat.stat-mech

TL;DR: An inverse approach for recovering microscopic couplings from data in statistical field theories, applicable to discrete, continuous, and hybrid variables, enabling accurate parameter recovery and reconstruction of renormalization-group flows.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem in statistical field theories by directly recovering microscopic couplings from data, complementing the computationally intractable forward approach of predicting observables from actions or Hamiltonians.

Method: Proposes an inverse approach that uniformly accommodates systems with discrete, continuous, and hybrid variables, and can be iterated under coarse-graining to reconstruct full non-perturbative renormalization-group flows.

Result: Demonstrates accurate parameter recovery in several benchmark systems including Wegner's Ising gauge theory, φ⁴ theory, Schwinger and Sine-Gordon models, and mixed spin-gauge systems. Also addresses realistic settings where full gauge configurations are unavailable.

Conclusion: The methodology provides direct access to phase boundaries, fixed points, and emergent interactions without relying on perturbation theory, and is anticipated to find widespread use in practical learning of field theories in strongly coupled regimes where analytical tools might fail.

Abstract: Recovering microscopic couplings directly from data provides a route to solving the inverse problem in statistical field theories, one that complements the traditional-often computationally intractable-forward approach of predicting observables from an action or Hamiltonian. Here, we propose an approach for the inverse problem that uniformly accommodates systems with discrete, continuous, and hybrid variables. We demonstrate accurate parameter recovery in several benchmark systems-including Wegner's Ising gauge theory, $φ^4$ theory, Schwinger and Sine-Gordon models, and mixed spin-gauge systems, and show how iterating the procedure under coarse-graining reconstructs full non-perturbative renormalization-group flows. This gives direct access to phase boundaries, fixed points, and emergent interactions without relying on perturbation theory. We also address a realistic setting where full gauge configurations may be unavailable, and reformulate learning algorithms for multiple field theories so that they are recovered directly using observables such as correlations from scattering data or quantum simulators. We anticipate that our methodology will find widespread use in practical learning of field theories in strongly coupled regimes where analytical tools might fail.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [45] [Regret, Uncertainty, and Bounded Rationality in Norm-Driven Decisions](https://arxiv.org/abs/2511.10342)
*Christos Charalambous*

Main category: physics.soc-ph

TL;DR: An agent-based model shows how regret, uncertainty, and social norms interact to shape vaccination behavior during epidemics, with optimal outcomes occurring at intermediate levels of rationality and moderate levels of regret and uncertainty.


<details>
  <summary>Details</summary>
Motivation: To understand how psychological factors (regret, uncertainty) and social norms interact to influence vaccination decisions during epidemics, providing a unified framework that bridges behavioral economics and psychology.

Method: Developed an agent-based model integrating three behavioral mechanisms: anticipated regret, evolving norms, and uncertainty-dependent trust within a unified learning framework. Simulated the Susceptible-Infected-Recovered process with agents making probabilistic choices influenced by material payoffs, fear, trust, and social approval.

Result: Collective outcomes are best when agents display intermediate rationality - enough deliberation to respond to risk but flexible enough to adapt. Moderate regret encourages adaptive self-correction, while excessive regret or greed destabilizes choices. Moderate uncertainty promotes caution, but too much disrupts coordination. Social norms restore cooperation by compensating for incomplete information.

Conclusion: The model provides a psychologically grounded, computationally explicit account of how emotion, cognition, and social norms govern preventive behavior during epidemics, with different types of norms (personal, injunctive, descriptive) playing complementary roles depending on information availability and uncertainty.

Abstract: This study introduces an agent-based model that explains how regret, uncertainty, and social norms interact to shape vaccination behavior during epidemics. The model integrates three behavioral mechanisms, anticipated regret, evolving norms, and uncertainty-dependent trust, within a unified learning framework. Grounded in psychology and behavioral economics, it captures how individuals make probabilistic choices influenced by material payoffs, fear, trust, and social approval. Simulations of the Susceptible-Infected-Recovered process show that collective outcomes are best when agents display an intermediate level of rationality; they deliberate enough to respond to risk but remain flexible enough to adapt, avoiding the instability of both random and overly rigid decision-making. Regret exerts a dual influence; moderate levels encourage adaptive self-correction, while excessive regret or greed destabilize choices. Uncertainty has a similarly non-linear effect; moderate ambiguity promotes caution, but too much uncertainty disrupts coordination. Social norms restore cooperation by compensating for incomplete information. Among them, personal norms drive behavior when individuals have clear information and moral confidence; injunctive norms, reflecting perceived approval, gain influence under uncertainty; and descriptive norms, based on observing others' actions, serve as informational cues that guide behavior when direct knowledge is limited. Overall, the model provides a psychologically grounded, computationally explicit account of how emotion, cognition, and social norms govern preventive behavior during epidemics.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [46] [The Role of Deep Mesoscale Eddies in Ensemble Forecast Performance](https://arxiv.org/abs/2511.09747)
*Justin Cooke,Kathleen Donohue,Clark D Rowley,Prasad G Thoppil,D Randolph Watts*

Main category: physics.ao-ph

TL;DR: Deep ocean initialization significantly impacts surface forecast accuracy in ocean modeling, with better deep field initialization reducing upper ocean uncertainty during forecasts.


<details>
  <summary>Details</summary>
Motivation: Current forecasting relies on upper ocean data assimilation, but deep ocean dynamics critically influence surface circulation development and evolution.

Method: Analyzed 92-day forecasts during Loop Current Eddy Thor separation, comparing best and worst ensemble members by assessing surface variables against analysis and satellite data, and examining deep cyclonic/anticyclonic features.

Result: Deep initial conditions that better match observations lead to lower upper ocean uncertainty; subtle differences in deep eddy locations significantly affect surface predictions.

Conclusion: Deep circulation is crucial for Loop Current system dynamics, motivating assimilation of deep observations to improve deep field initialization and surface forecasts.

Abstract: Present forecasting efforts rely on assimilation of observational data captured in the upper ocean (< 1000 m depth). These observations constrain the upper ocean and minimally influence the deep ocean. Nevertheless, development of the full water column circulation critically depends upon the dynamical interactions between upper and deep fields. Forecasts demonstrate that the initialization of the deep field is influential for the development and evolution of the surface in the forecast. Deep initial conditions that better agree with observations have lower upper ocean uncertainty as the forecast progresses. Here, best and worst ensemble members in two 92-day forecasts are identified and contrasted in order to determine how the deep ocean differs between these groups. The forecasts cover the duration of the Loop Current Eddy Thor separation event, which coincides with available deep observations in the Gulf. Model member performance is assessed by comparing surface variables against verifying analysis and satellite altimeter data during the forecast time-period. Deep cyclonic and anticyclonic features are reviewed, and compared against deep observations, indicating subtle differences in locations of deep eddies at relevant times. These results highlight both the importance of deep circulation in the dynamics of the Loop Current system and more broadly motivate efforts to assimilate deep observations to better constrain the deep initial fields and improve surface predictions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [HeatGen: A Guided Diffusion Framework for Multiphysics Heat Sink Design Optimization](https://arxiv.org/abs/2511.09578)
*Hadi Keramati,Morteza Sadeghi,Rajeev K. Jaiman*

Main category: cs.LG

TL;DR: A generative optimization framework using guided DDPM with surrogate gradients to create heat sink designs that minimize pressure drop while keeping surface temperatures below specified limits, outperforming traditional optimization methods.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable generative approach for heat sink design that can efficiently produce optimized geometries under new constraints without retraining, addressing limitations of traditional black-box optimization and topology optimization methods.

Method: Uses boundary representations of multiple fins, trains a denoising diffusion probabilistic model on multi-fidelity data, and employs two residual neural networks as surrogate models for pressure drop and temperature prediction. Uses surrogate gradients to guide the generation process toward constraint satisfaction.

Result: Generated heat sink designs achieve pressure drops up to 10% lower than traditional black-box optimization methods like CMA-ES, while maintaining surface temperatures below specified thresholds.

Conclusion: The framework represents progress toward building foundational generative models for electronics cooling, offering computationally efficient inference under new constraints without retraining requirements.

Abstract: This study presents a generative optimization framework based on a guided denoising diffusion probabilistic model (DDPM) that leverages surrogate gradients to generate heat sink designs minimizing pressure drop while maintaining surface temperatures below a specified threshold. Geometries are represented using boundary representations of multiple fins, and a multi-fidelity approach is employed to generate training data. Using this dataset, along with vectors representing the boundary representation geometries, we train a denoising diffusion probabilistic model to generate heat sinks with characteristics consistent with those observed in the data. We train two different residual neural networks to predict the pressure drop and surface temperature for each geometry. We use the gradients of these surrogate models with respect to the design variables to guide the geometry generation process toward satisfying the low-pressure and surface temperature constraints. This inference-time guidance directs the generative process toward heat sink designs that not only prevent overheating but also achieve lower pressure drops compared to traditional optimization methods such as CMA-ES. In contrast to traditional black-box optimization approaches, our method is scalable, provided sufficient training data is available. Unlike traditional topology optimization methods, once the model is trained and the heat sink world model is saved, inference under new constraints (e.g., temperature) is computationally inexpensive and does not require retraining. Samples generated using the guided diffusion model achieve pressure drops up to 10 percent lower than the limits obtained by traditional black-box optimization methods. This work represents a step toward building a foundational generative model for electronics cooling.

</details>


### [48] [Incremental Generation is Necessity and Sufficient for Universality in Flow-Based Modelling](https://arxiv.org/abs/2511.09902)
*Hossein Rouhvarzi,Anastasis Kratsios*

Main category: cs.LG

TL;DR: Incremental flow-based denoising models require multiple steps for universal approximation of orientation-preserving homeomorphisms on [0,1]^d, as single-step flows are insufficient but compositions of multiple flows can achieve approximation.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous approximation-theoretic foundation for incremental flow-based denoising models and understand why incremental generation is necessary for universal approximation.

Method: Used topological-dynamical arguments to prove impossibility of single-step autonomous flows, then exploited algebraic properties to show compositions of multiple flows can approximate orientation-preserving homeomorphisms with dimension-dependent rates.

Result: Single-step flows are meagre and not universal, but compositions of K_d flows can approximate Lipschitz homeomorphisms at rate O(n^{-1/d}). With smoothness assumptions, dimension-free rates are possible. Linear lifting enables universal approximation of continuous functions and probability measures.

Conclusion: Incremental generation is both necessary and sufficient for universal flow-based generation on orientation-preserving homeomorphisms of [0,1]^d, providing theoretical justification for multi-step denoising models.

Abstract: Incremental flow-based denoising models have reshaped generative modelling, but their empirical advantage still lacks a rigorous approximation-theoretic foundation. We show that incremental generation is necessary and sufficient for universal flow-based generation on the largest natural class of self-maps of $[0,1]^d$ compatible with denoising pipelines, namely the orientation-preserving homeomorphisms of $[0,1]^d$. All our guarantees are uniform on the underlying maps and hence imply approximation both samplewise and in distribution.
  Using a new topological-dynamical argument, we first prove an impossibility theorem: the class of all single-step autonomous flows, independently of the architecture, width, depth, or Lipschitz activation of the underlying neural network, is meagre and therefore not universal in the space of orientation-preserving homeomorphisms of $[0,1]^d$. By exploiting algebraic properties of autonomous flows, we conversely show that every orientation-preserving Lipschitz homeomorphism on $[0,1]^d$ can be approximated at rate $\mathcal{O}(n^{-1/d})$ by a composition of at most $K_d$ such flows, where $K_d$ depends only on the dimension. Under additional smoothness assumptions, the approximation rate can be made dimension-free, and $K_d$ can be chosen uniformly over the class being approximated. Finally, by linearly lifting the domain into one higher dimension, we obtain structured universal approximation results for continuous functions and for probability measures on $[0,1]^d$, the latter realized as pushforwards of empirical measures with vanishing $1$-Wasserstein error.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [49] [Electron Heat Flux and Whistler Instability in the Earth's Magnetosheath](https://arxiv.org/abs/2511.10275)
*Ida Svenningsson,Emiliya Yordanova,Yuri V. Khotyaintsev,Mats André,Giulia Cozzani,Alexandros Chasapis,Steven J. Schwartz*

Main category: physics.space-ph

TL;DR: Analysis of electron heat flux in Earth's magnetosheath using MMS measurements, showing it's shaped by magnetic field draping around magnetosphere and limited by whistler instability thresholds.


<details>
  <summary>Details</summary>
Motivation: Heat flux regulates energy conversion in collisionless plasmas but its properties in the magnetosheath downstream of Earth's bow shock are poorly understood.

Method: Used MMS in situ measurements to quantify and characterize electron heat flux in the magnetosheath.

Result: Heat flux is shaped by magnetosheath magnetic field draping, affected by solar wind upstream conditions and increases with magnetic field strength, but not substantially changed by local magnetosheath processes. Heat flux is limited by whistler instability thresholds.

Conclusion: Electron heat flux in magnetosheath is primarily controlled by upstream solar wind conditions and magnetic field draping, with whistler instabilities serving as limiting factors.

Abstract: Despite heat flux's role in regulating energy conversion in collisionless plasmas, its properties and evolution in the magnetosheath downstream of the Earth's bow shock are scarcely explored. We use MMS in situ measurements to quantify and characterize the electron heat flux in the magnetosheath. We find that the heat flux is shaped by the magnetosheath magnetic field as it drapes around the magnetosphere. While it is affected by solar wind upstream conditions and increases with magnetic field strength, it is not substantially changed by local magnetosheath processes. Also, the heat flux is limited by whistler instability thresholds.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [50] [Collisional and magnetic effects on the polarization of the solar oxygen infrared triplet](https://arxiv.org/abs/2511.10476)
*Moncef Derouich,Saleh Qutub*

Main category: astro-ph.SR

TL;DR: The study investigates how collisions and magnetic fields affect polarization in the O I IR triplet, finding that collisional depolarization with hydrogen and magnetic fields suppress alignment in the photosphere but polarization persists in the chromosphere.


<details>
  <summary>Details</summary>
Motivation: To understand the combined effects of collisional and magnetic depolarization on the alignment of O I levels and polarization of the O I IR triplet, which serves as a diagnostic for solar atmospheric conditions.

Method: First computation of comprehensive collisional depolarization and polarization transfer rates for O I energy levels, incorporated into a multi-level atomic model with statistical equilibrium equations to quantify collision and magnetic field impacts.

Result: Elastic collisions with neutral hydrogen and magnetic fields stronger than 20 G efficiently suppress atomic alignment in deep photosphere (n_H > 10^16 cm^-3), while lower hydrogen density in chromosphere allows polarization to persist.

Conclusion: Results support chromospheric origin for O I IR triplet linear polarization signals, and future studies should combine accurate non-LTE radiative transfer with reliable collisional rates for fully consistent modeling.

Abstract: Context: The scattering polarization of the infrared (IR) triplet of neutral oxygen (O\,\textsc{i}) near 777\,nm provides a powerful diagnostic of solar atmospheric conditions. However, interpreting such polarization requires a rigorous treatment of isotropic depolarizing collisions between O\,\textsc{i} atoms and neutral hydrogen.
  Aims: We aim to investigate the combined effects of collisional and magnetic depolarization in shaping the alignment of O\,\textsc{i} levels (and thus the polarization of the O\,\textsc{i} IR triplet).
  Methods: We compute, for the first time, a comprehensive set of collisional depolarization and polarization transfer rates for the relevant O\,\textsc{i} energy levels. These rates are incorporated into a multi-level atomic model, and the statistical equilibrium equations (SEE) are solved to quantify the impact of collisions and magnetic fields on atomic alignment.
  Results: Our calculations indicate that elastic collisions with neutral hydrogen, together with the Hanle effect of turbulent magnetic fields stronger than about 20 G, efficiently suppress the bulk of the atomic alignment in deep photospheric conditions where hydrogen densities exceed $n_{\mathrm{H}} \sim 10^{16}$ cm$^{-3}$. In the chromosphere, however, the lower hydrogen density weakens collisional depolarization, allowing polarization to persist.
  Conclusions: Our results are consistent with a chromospheric origin for the linear polarization signals of the O I IR triplet. Future studies should combine accurate non-LTE radiative transfer with reliable collisional rates in order to achieve fully consistent modeling.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [51] [An inexact semismooth Newton-Krylov method for semilinear elliptic optimal control problem](https://arxiv.org/abs/2511.10058)
*Shiqi Chen,Xuesong Chen*

Main category: math.OC

TL;DR: An inexact semismooth Newton method with GMRES and nonmonotonic line search for solving semi-linear elliptic optimal control problems, achieving superlinear convergence near optimal solutions.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for solving semi-linear elliptic optimal control problems that reduces computational overhead while ensuring global convergence.

Method: Reformulates the problem into a nonlinear equation using variational inequality principles, discretizes with second-order finite difference, uses semismooth Newton directions with GMRES for inexact solving, and employs dynamic nonmonotonic line search.

Result: The algorithm achieves superlinear convergence near optimal solutions when residual control parameter approaches 0, with numerical experiments validating accuracy and efficiency.

Conclusion: The proposed method effectively solves semilinear elliptic optimal control problems with reduced computational cost and proven convergence properties.

Abstract: An inexact semismooth Newton method has been proposed for solving semi-linear elliptic optimal control problems in this paper. This method incorporates the generalized minimal residual (GMRES) method, a type of Krylov subspace method, to solve the Newton equations and utilizes nonmonotonic line search to adjust the iteration step size. The original problem is reformulated into a nonlinear equation through variational inequality principles and discretized using a second-order finite difference scheme. By leveraging slanting differentiability, the algorithm constructs semismooth Newton directions and employs GMRES method to inexactly solve the Newton equations, significantly reducing computational overhead. A dynamic nonmonotonic line search strategy is introduced to adjust stepsizes adaptively, ensuring global convergence while overcoming local stagnation. Theoretical analysis demonstrates that the algorithm achieves superlinear convergence near optimal solutions when the residual control parameter $η_k$ approaches to 0. Numerical experiments validate the method's accuracy and efficiency in solving semilinear elliptic optimal control problems, corroborating theoretical insights.

</details>


### [52] [Low-Discrepancy Set Post-Processing via Gradient Descent](https://arxiv.org/abs/2511.10496)
*François Clément,Linhang Huang,Woorim Lee,Cole Smidt,Braeden Sodt,Xuan Zhang*

Main category: math.OC

TL;DR: Simple gradient descent techniques can achieve comparable low-discrepancy set construction results to complex ML methods, but are much more efficient and can be applied as post-processing.


<details>
  <summary>Details</summary>
Motivation: Current optimization and ML methods for constructing low-discrepancy sets are computationally expensive, requiring days of computation or GPU clusters.

Method: Using simple gradient descent-based techniques starting with reasonably uniform point sets, which can be applied as post-processing to any existing low-discrepancy set generation method.

Result: Achieves comparable results to complex ML-based methods but with much greater efficiency and accessibility.

Conclusion: Gradient descent provides an efficient, accessible alternative to expensive ML methods for low-discrepancy set construction and can enhance existing methods through post-processing.

Abstract: The construction of low-discrepancy sets, used for uniform sampling and numerical integration, has recently seen great improvements based on optimization and machine learning techniques. However, these methods are computationally expensive, often requiring days of computation or access to GPU clusters. We show that simple gradient descent-based techniques allow for comparable results when starting with a reasonably uniform point set. Not only is this method much more efficient and accessible, but it can be applied as post-processing to any low-discrepancy set generation method for a variety of standard discrepancy measures.

</details>


### [53] [The Age-Structured Chemostat with Substrate Dynamics as a Control System](https://arxiv.org/abs/2511.09963)
*Iasson Karafyllis,Dionysis Theodosis,Miroslav Krstic*

Main category: math.OC

TL;DR: Analysis of an age-structured chemostat model with renewal boundary conditions and coupled substrate dynamics, proving global existence and uniqueness of solutions for all admissible controls.


<details>
  <summary>Details</summary>
Motivation: To study a nonlinear age-structured chemostat model with coupled substrate equations and establish its well-posedness as a control system.

Method: Combined Banach's fixed-point theorem with implicit solution formulas and solution estimates to prove global existence and uniqueness of solutions.

Result: Proved global existence and uniqueness of solutions for all admissible initial conditions and allowable control inputs, establishing the model as a well-defined control system on a metric space.

Conclusion: The age-structured chemostat model with renewal boundary conditions and coupled substrate dynamics is mathematically well-posed and forms a proper control system framework.

Abstract: In this work we study an age-structured chemostat model with a renewal boundary condition and a coupled substrate equation. The model is nonlinear and consists of a hyperbolic partial differential equation and an ordinary differential equation with nonlinear, nonlocal terms appearing both in the ordinary differential equation and the boundary condition. Both differential equations contain a non-negative control input, while the states of the model are required to be positive. Under an appropriate weak solution framework, we determine the state space and the input space for this model. We prove global existence and uniqueness of solutions for all admissible initial conditions and all allowable control inputs. To this purpose we employ a combination of Banach's fixed-point theorem with implicit solution formulas and useful solution estimates. Finally, we show that the age-structured chemostat model gives a well-defined control system on a metric space.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [54] [Effect of Concentration Fluctuations on Material Properties of Disordered Alloys](https://arxiv.org/abs/2511.10259)
*Han-Pu Liang,Chuan-Nan Li,Xin-Ru Tang,Xun Xu,Chen Qiu,Qiu-Shi Huang,Su-Huai Wei*

Main category: cond-mat.mtrl-sci

TL;DR: Standard SQS calculations underestimate alloy bandgaps due to rare defect-like configurations. A DOSF method is proposed to extract bandgaps from majority configurations, resolving discrepancies with experiments.


<details>
  <summary>Details</summary>
Motivation: Disordered alloys present challenges for property calculations due to lack of periodicity. SQS methods work for macroscopic properties but fail for semiconductor bandgaps due to rare minority configurations that dominate electronic structure calculations.

Method: Proposed density-of-states fitting (DOSF) method to extract bandgaps from majority configurations rather than conventional energy difference between occupied and unoccupied states, avoiding influence from rare defect-like motifs.

Result: DOSF approach successfully resolves long-standing discrepancies between calculated and experimental bandgaps in disordered semiconductor alloys by focusing on majority configurations.

Conclusion: The DOSF method provides accurate bandgap calculations for disordered alloys and similar approaches should be developed for other properties affected by localized wavefunctions.

Abstract: Alloying compound AX with another compound BX is widely used to tune material properties. For disordered alloys, due to the lack of periodicity, it has been challenging to calculate and study their material properties. Special quasi-random structure (SQS) method has been developed and widely used to treat this issue by matching averaged atomic correlation functions to those of ideal random alloys, enabling accurate predictions of macroscopic material properties such as total energy and volume. However, in AxB1-x alloys, statistically allowed local concentration fluctuations can give rise to defect-like minority configurations, such as bulk-like AX or BX regions in the extreme, which could strongly affect calculation of some of the material properties such as semiconductor bandgap, if it is not defined properly, leading to significant discrepancies between theory and experiment. In this work, taking the bandgap as an example, we demonstrate that the calculated alloy bandgap can be significantly underestimated in standard SQS calculations when the SQS cell size is increased to improve the structural model and the bandgap is defined conventionally as the energy difference between the lowest unoccupied state and the highest occupied state, because the rare event motifs can lead to wavefunction localization and become the dominant factor in determining the "bandgap", contrary to experiment. To be consistent with experiment, we show that the bandgap of the alloy should be extracted from the majority configurations using a density-of-states fitting (DOSF) method. This DOSF approach resolves the long-standing issue of calculating electronic structure of disordered semiconductor alloys. Similar approaches should also be developed to treat material properties that depends on localized alloy wavefunctions.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [55] [Kinetic Theory with Fluctuations: Strong Well-Posedness of the Vlasov-Fokker-Planck-Dean-Kawasaki System](https://arxiv.org/abs/2511.10194)
*Zimo Hao,Zhengyan Wu,Johannes Zimmer*

Main category: math.PR

TL;DR: The paper establishes well-posedness for the Vlasov-Fokker-Planck-Dean-Kawasaki equation with correlated noise, proving existence and uniqueness of renormalized kinetic solutions.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous mathematical foundation for the VFPDK equation, which arises from fluctuating mean-field limits of second-order Newtonian particle systems with correlated noise.

Method: Uses a novel combination of kinetic semigroup estimates with the framework of renormalized kinetic solutions, focusing on bounded nonlocal interactions and square-root type diffusion coefficients.

Result: Proves existence and uniqueness of renormalized kinetic solutions for the VFPDK equation with correlated noise.

Conclusion: The VFPDK equation with correlated noise is well-posed, with rigorous existence and uniqueness results established through renormalized kinetic solution framework.

Abstract: We establish the well-posedness of the Vlasov-Fokker-Planck-Dean-Kawasaki (VFPDK) equation with correlated noise, which arises as a fluctuating mean-field limit of second-order Newtonian particle systems. We focus on the case of bounded nonlocal interactions and a diffusion coefficient of square-root type. In this setting, we prove existence and uniqueness of renormalized kinetic solutions. The proof relies on a novel combination of kinetic semigroup estimates with the framework of renormalized kinetic solutions.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [56] [Mitigating numerical dissipation in simulations of subsonic turbulent flows](https://arxiv.org/abs/2511.09806)
*James Watt,Christoph Federrath,Claudius Birke,Christian Klingenberg*

Main category: physics.flu-dyn

TL;DR: A new numerical scheme (USM-BK) is developed for low-Mach MHD turbulence simulations, showing superior accuracy with reduced dissipation and better magnetic field divergence control compared to conventional schemes.


<details>
  <summary>Details</summary>
Motivation: Conventional numerical schemes are excessively dissipative in low-Mach number turbulent flows, which affects simulations of important astrophysical and geophysical processes like dynamo action, accretion discs, and stellar flows.

Method: Developed and implemented the USM-BK scheme in FLASH MHD code, comparing it against several conventional schemes (Split-Roe, Split-Bouchut, USM-Roe, USM-HLLC, USM-HLLD) on vortex problems and turbulent dynamo simulations.

Result: USM-BK reduces kinetic/magnetic energy dissipation, maintains near-machine-precision magnetic field divergence, resolves smaller-scale structures, provides highest numerical Reynolds number, and significantly affects dynamo growth rates and saturation levels.

Conclusion: USM-BK is the most accurate scheme for low-Mach turbulent MHD flows among those compared, offering improved performance for simulations of astrophysical and geophysical turbulent processes.

Abstract: Magnetohydrodynamic (MHD) simulations of subsonic (Mach number~$<1$) turbulence are crucial to our understanding of several processes including oceanic and atmospheric flows, the amplification of magnetic fields in the early universe, accretion discs, and stratified flows in stars. In this work, we demonstrate that conventional numerical schemes are excessively dissipative in this low-Mach regime. We demonstrate that a new numerical scheme (termed `USM-BK' and implemented in the FLASH MHD code) reduces the dissipation of kinetic and magnetic energy, constrains the divergence of magnetic field to zero close to machine precision, and resolves smaller-scale structure than other, more conventional schemes, and hence, is the most accurate for simulations of low-Mach turbulent flows among the schemes compared in this work. We first compare several numerical schemes/solvers, including Split-Roe, Split-Bouchut, USM-Roe, USM-HLLC, USM-HLLD, and the new USM-BK, on a simple vortex problem. We then compare the schemes/solvers in simulations of the turbulent dynamo and show that the choice of scheme affects the growth rate, saturation level, and viscous and resistive dissipation scale of the dynamo. We also measure the numerical kinematic Reynolds number (Re) and magnetic Reynolds number (Rm) of our otherwise ideal MHD flows, and show that the new USM-BK scheme provides the highest Re and comparable Rm amongst all the schemes compared.

</details>


### [57] [Data-driven modeling of multiscale phenomena with applications to fluid turbulence](https://arxiv.org/abs/2511.09847)
*Brandon Choi,Matteo Ugliotti,Mateo Reynoso,Daniel R. Gurevich,Roman O. Grigoriev*

Main category: physics.flu-dyn

TL;DR: A data-driven framework for building equivariant multiscale models without physics assumptions, demonstrated on 2D fluid turbulence to capture backscatter effects.


<details>
  <summary>Details</summary>
Motivation: To create accurate multiscale models that can handle phenomena like backscatter (energy flow from small to large scales) in turbulence, which existing methods fail to address.

Method: Used direct numerical simulations of 2D freely decaying turbulence to infer an effective field theory with explicit evolution equations for both resolved large scales and modeled small scales.

Result: Developed a closed system of equations that accurately describes small-scale effects including backscatter, particularly pronounced in 2D turbulence.

Conclusion: The framework successfully addresses the challenge of modeling backscatter in multiscale phenomena, providing interpretable evolution equations without relying on specific physics assumptions.

Abstract: This letter introduces a novel data driven framework for constructing accurate and general equivariant models of multiscale phenomena which does not rely on specific assumptions about the underlying physics. This framework is illustrated using incompressible fluid turbulence as an example that is representative, practically important, reasonably simple, and exceedingly well studied. We use direct numerical simulations of freely decaying turbulence in two spatial dimensions to infer an effective field theory comprising explicit, interpretable evolution equations for both the large (resolved) and small (modeled) scales. The resulting closed system of equations is capable of accurately describing the effect of small scales, including backscatter -- the flow of energy from small to large scales, which is particularly pronounced in two dimensions -- which is an outstanding challenge that, to our knowledge, no existing alternative successfully tackles.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [58] [Optimal Interpolation of Entanglement Purification Protocols](https://arxiv.org/abs/2511.09657)
*Matthew Barber,Stefano Pirandola*

Main category: quant-ph

TL;DR: The paper presents a method to optimize entanglement purification protocols by randomly selecting protocols from a distribution, achieving better trade-offs between rate and fidelity than individual protocols.


<details>
  <summary>Details</summary>
Motivation: To overcome the trade-off between rate and fidelity in entanglement purification protocols by combining multiple protocols probabilistically.

Method: Random selection of purification protocols according to an optimized probability distribution for each entangled pair production, analyzed in both asymptotic and finite-size regimes.

Result: The approach achieves rates and fidelities not possible with individual protocols, with optimized distributions for maximizing rate at given fidelity or fidelity at given rate.

Conclusion: Probabilistic protocol selection enables superior entanglement purification performance, providing optimal trade-offs between rate and fidelity in both asymptotic and finite scenarios.

Abstract: Bipartite entanglement purification is the conversion of copies of weakly entangled pairs shared between two separated parties into a smaller number of strongly entangled shared pairs using only local operations and classical communication. Choosing between different entanglement purification protocols generally involves weighing up a trade-off between the ratio of strongly entangled pairs produced to weakly entangled pairs consumed, which we call the rate of the protocol, and the degree of the entanglement of the strongly entangled pairs, typically measured by the fidelity of those pairs to maximally entangled states. By randomly choosing a protocol according to a probability distribution over a list of protocols for each pair we want to produce, we can achieve rates and fidelities not achieved by any of the original protocols. Here, we show how to choose this distribution to maximise the rate at which we produce qubit pairs with a given fidelity to a Bell state or, equivalently, to maximise the fidelity to a Bell state of the qubit pairs produced at a given rate. We investigate both the asymptotic case, where the number of initial pairs goes to infinity, and the finite-size regime, where protocols are restricted to a finite number of weakly entangled pairs.

</details>


### [59] [Constrained Shadow Tomography for Molecular Simulation on Quantum Devices](https://arxiv.org/abs/2511.09717)
*Irma Avdic,Yuchen Wang,Michael Rose,Lillian I. Payne Torres,Anna O. Schouten,Kevin J. Sung,David A. Mazziotti*

Main category: quant-ph

TL;DR: A bi-objective semidefinite programming method for constrained shadow tomography that reconstructs the 2-particle reduced density matrix from noisy shadow data using N-representability constraints and nuclear-norm regularization.


<details>
  <summary>Details</summary>
Motivation: Quantum state tomography has exponential measurement and computational demands, limiting scalability. Classical shadows offer efficient alternatives but need methods to handle noisy or incomplete data while maintaining physical consistency.

Method: Bi-objective semidefinite programming approach that integrates N-representability constraints and nuclear-norm regularization to reconstruct 2-RDM from shadow data, balancing measurement fidelity with energy minimization.

Result: The method significantly improves accuracy, noise resilience, and scalability, providing robust physically consistent fermionic state reconstruction in quantum simulations.

Conclusion: This unified framework enables accurate and physically consistent reconstruction of quantum states from noisy shadow measurements, advancing scalable quantum simulation capabilities.

Abstract: Quantum state tomography is a fundamental task in quantum information science, enabling detailed characterization of correlations, entanglement, and electronic structure in quantum systems. However, its exponential measurement and computational demands limit scalability, motivating efficient alternatives such as classical shadows, which enable accurate prediction of many observables from randomized measurements. In this work, we introduce a bi-objective semidefinite programming approach for constrained shadow tomography, designed to reconstruct the two-particle reduced density matrix (2-RDM) from noisy or incomplete shadow data. By integrating $N$-representability constraints and nuclear-norm regularization into the optimization, the method builds an $N$-representable 2-RDM that balances fidelity to the shadow measurements with energy minimization. This unified framework mitigates noise and sampling errors while enforcing physical consistency in the reconstructed states. Numerical and hardware results demonstrate that the approach significantly improves accuracy, noise resilience, and scalability, providing a robust foundation for physically consistent fermionic state reconstruction in realistic quantum simulations.

</details>


### [60] [Quantum Period-Finding using One-Qubit Reduced Density Matrices](https://arxiv.org/abs/2511.09896)
*Marco Bernardi*

Main category: quant-ph

TL;DR: The paper proposes a novel approach to quantum period-finding using single-qubit reduced density matrices (1-RDMs) instead of measuring output bit strings, showing that period information can be compressed into O(n) one-qubit marginals rather than sampling O(2^n) bit strings.


<details>
  <summary>Details</summary>
Motivation: To explore whether the period in quantum period-finding algorithms can be obtained from simpler single-qubit quantities (1-RDMs) rather than requiring measurement of the full output state and continued fraction analysis.

Method: Used state-vector simulations to compute 1-RDMs of QPF circuits for generic periodic functions, analyzed patterns in these 1-RDMs as functions of period, and developed a numerical root-finding approach to extract the period from the 1-RDMs.

Result: The 1-RDMs contain distinctive patterns that enable reconstruction of the unknown period, demonstrating that O(n) one-qubit marginals suffice to obtain period information typically requiring sampling of O(2^n) bit strings.

Conclusion: The information in QPF algorithms can be compressed into one-qubit marginals, motivating development of approximate simulations of reduced density matrices for novel period-finding algorithm design.

Abstract: The quantum period-finding (QPF) algorithm can compute the period of a function exponentially faster than the best-known classical algorithm. In standard QPF, the output state has a primary contribution from $r$ high-probability bit strings, where $r$ is the period. Measurement of this state, combined with continued fraction analysis, reveals the unknown period. Here, we explore a different approach to QPF, where the period is obtained from single-qubit quantities $-$ specifically, the set of one-qubit reduced density matrices (1-RDMs) $-$ rather than the output bit strings of the entire quantum circuit. Using state-vector simulations, we compute the 1-RDMs of the QPF circuit for a generic periodic function. Analysis of these 1-RDMs as a function of period reveals distinctive patterns, which allows us to obtain the unknown period from the 1-RDMs using a numerical root-finding approach. Our results show that the 1-RDMs $-$ a set of $O(n)$ one-qubit marginals $-$ contain enough information to reconstruct the period, which is typically obtained by sampling the space of $O(2^n)$ bit strings. Conceptually, this can be viewed as a "compression" of the information in the QPF algorithm, which enables period-finding from $n$ one-qubit marginals. Our results motivate the development of approximate simulations of reduced density matrices to design novel period-finding algorithms.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [61] [SeQuant Framework for Symbolic and Numerical Tensor Algebra. I. Core Capabilities](https://arxiv.org/abs/2511.09943)
*Bimal Gaudel,Robert G. Adam,Ajay Melekamburath,Conner Masteran,Nakul Teke,Azam Besharatnik,Andreas Köhn,Edward F. Valeev*

Main category: cs.MS

TL;DR: SeQuant is an open-source library for symbolic tensor algebra that introduces a graph-theoretic tensor network canonicalizer for faster handling of tensor networks with symmetries, supporting both commutative and non-commutative rings.


<details>
  <summary>Details</summary>
Motivation: To provide efficient symbolic algebra capabilities for tensor networks with symmetries, bridging the gap between symbolic manipulation and numerical evaluation in tensor computations.

Method: Uses a graph-theoretic tensor network canonicalizer that outperforms standard group-theoretic approaches, supports noncovariant tensor networks and nested tensors, and includes compiler-like components for optimization and interpretation.

Result: Developed a functional library capable of simplifying tensor expressions, optimizing Wick's theorem applications, and manipulating intermediate representations for numerical evaluation.

Conclusion: SeQuant successfully blurs the line between symbolic manipulation and numerical evaluation, providing an efficient open-source solution for tensor algebra with applications in data science and quantum simulation.

Abstract: SeQuant is an open-source library for symbolic algebra of tensors over commutative (scalar) and non-commutative (operator) rings. The key innovation supporting most of its functionality is a graph-theoretic tensor network (TN) canonicalizer that can handle tensor networks with symmetries faster than their standard group-theoretic counterparts. The TN canonicalizer is used for routine simplification of conventional tensor expressions, for optimizing application of Wick's theorem (used to canonicalize products of tensors over operator fields), and for manipulation of the intermediate representation leading to the numerical evaluation. Notable features of SeQuant include support for noncovariant tensor networks (which often arise from tensor decompositions) and for tensors with modes that depend parametrically on indices of other tensor modes (such dependencies between degrees of freedom are naturally viewed as nesting of tensors, "tensors of tensors" arising in block-wise data compressions in data science and modern quantum simulation). SeQuant blurs the line between pure symbolic manipulation/code generation and numerical evaluation by including compiler-like components to optimize and directly interpret tensor expressions using external numerical tensor algebra frameworks. The SeQuant source code is available at https://github.com/ValeevGroup/SeQuant.

</details>
