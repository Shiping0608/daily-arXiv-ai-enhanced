<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 8]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [cs.DC](#cs.DC) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.atm-clus](#physics.atm-clus) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cond-mat.supr-con](#cond-mat.supr-con) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [math-ph](#math-ph) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Tracking EEG Thalamic and Cortical Focal Brain Activity using Standardized Kalman Filtering with Kinematics Modeling](https://arxiv.org/abs/2511.10877)
*Veikka Piispa,Dilshanie Prasikala,Joonas Lahtinen,Alexandra Koulouri,Sampsa Pursiainen*

Main category: math.NA

TL;DR: Proposes improved Kalman filtering for EEG brain activity estimation using kinematic evolution models with velocity/acceleration states, yielding smoother estimates and enhanced computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve brain activity estimation from EEG recordings by addressing depth bias and providing more physically plausible estimates, especially for deep sources like thalamic activity.

Method: Extended Kalman filtering with first and second-order kinematic evolution models that include source velocity and acceleration in state-space vector, plus tunable power parameter for computational efficiency.

Result: Simulation study shows accurate estimation and tracking of both superficial (cortical) and deep (thalamic) brain activity in somatosensory region, with smoother estimates even under high measurement noise.

Conclusion: The proposed kinematic models enable feasible and accurate tracking of both superficial and deep brain activity, overcoming depth bias and providing more physically plausible EEG source localization.

Abstract: Kalman filtering has proven to be effective for estimating brain activity using EEG recordings. In particular, the introduced post hoc standardization step of the algorithm, inspired by the sLORETA time-invariant method, reduces the depth bias and thus allows the estimation to appear at the correct depth from the electrode surface. In the current work, we propose first and second-order kinematic evolution models, where the state-space vector includes not only the dipolar source activity but also its velocity and acceleration. Compared to our previous study, this motion model yields smoother and more physically plausible estimates of brain activity even when the measurement noise is high, for both superficial and deep sources. In addition, we introduce a tunable power parameter that enhances the computational efficiency of the algorithm. Our simulation study, which involves thalamic and cortical activity in the somatosensory region, demonstrates that accurate estimation and tracking of both superficial and deep brain activity are feasible.

</details>


### [2] [Two Generalized Derivative-free Methods to Solve Large Scale Nonlinear Equations with Convex Constraints](https://arxiv.org/abs/2511.10928)
*Kabenge Hamiss,Mohammed M. Alshahrani,Mujahid N. Syed*

Main category: math.NA

TL;DR: Two derivative-free methods for large-scale nonlinear equations with convex constraints, featuring sufficient descent conditions and global convergence properties.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving large-scale nonlinear equations with convex constraints using derivative-free approaches that maintain convergence guarantees.

Method: Proposed two algorithms: one generalizing Modified Optimal Perry conjugate gradient method and conjugate gradient projection method, and another generalizing Spectral Modified Optimal Perry conjugate gradient method and Spectral Conjugate Gradient Projection method.

Result: The first algorithm achieves global convergence independent of Lipschitz continuity of G, while the second requires Lipschitz continuity. Numerical results demonstrate algorithm efficiency.

Conclusion: The proposed derivative-free methods effectively solve large-scale nonlinear equations with convex constraints, with different convergence requirements and demonstrated numerical efficiency.

Abstract: In this work, we propose two derivative-free methods to address the problem of large-scale nonlinear equations with convex constraints. These algorithms satisfy the sufficient descent condition. The search directions can be considered generalizations of the Modified Optimal Perry conjugate gradient method and the conjugate gradient projection method or the Spectral Modified Optimal Perry conjugate gradient method and the Spectral Conjugate Gradient Projection method. The global convergence of the former does not depend on the Lipschitz continuity of G. In contrast, the latter's global convergence depends on the Lipschitz continuity of G. The numerical results show the efficiency of the algorithms.

</details>


### [3] [Bifurcations in Interior Transmission Eigenvalues: Theory and Computation](https://arxiv.org/abs/2511.11016)
*Davide Pradovera,Alessandro Borghi,Lukas Pieronek,Andreas Kleefeld*

Main category: math.NA

TL;DR: Analysis of non-smooth spectral behavior in interior transmission eigenvalue problems, with theoretical framework for general domains and specialized analysis for radial geometries, plus computational tracking of eigenvalue trajectories.


<details>
  <summary>Details</summary>
Motivation: The interior transmission eigenvalue problem is crucial in inverse scattering theory and spectral analysis of inhomogeneous media, but exhibits non-smooth or bifurcating behavior despite smooth PDE dependence on refractive index.

Method: Developed theoretical framework for non-smooth spectral behavior in general domains, specialized analysis for radially symmetric geometries, and used match-based adaptive contour eigensolver to track eigenvalue trajectories computationally.

Result: Numerical experiments confirmed theoretical predictions and revealed novel non-smooth spectral effects in the interior transmission eigenvalue problem.

Conclusion: The study provides both theoretical understanding and computational tools for analyzing non-smooth spectral behavior in interior transmission eigenvalue problems across various domain geometries.

Abstract: The interior transmission eigenvalue problem (ITP) plays a central role in inverse scattering theory and in the spectral analysis of inhomogeneous media. Despite its smooth dependence on the refractive index at the PDE level, the corresponding spectral map from material parameters to eigenpairs may exhibit non-smooth or bifurcating behavior. In this work, we develop a theoretical framework identifying sufficient conditions for such non-smooth spectral behavior in the ITP on general domains. We further specialize our analysis to some radially symmetric geometries, enabling a more precise characterization of bifurcations in the spectrum. Computationally, we formulate the ITP as a parametric, discrete, nonlinear eigenproblem and use a match-based adaptive contour eigensolver to accurately and efficiently track eigenvalue trajectories under parameter variation. Numerical experiments confirm the theoretical predictions and reveal novel non-smooth spectral effects.

</details>


### [4] [Numerical approximation of Caputo-type advection-diffusion equations in one and multiple spatial dimensions via shifted Chebyshev polynomials](https://arxiv.org/abs/2511.11082)
*Francisco de la Hoz,Peru Muniain*

Main category: math.NA

TL;DR: Developed stable operational matrices using shifted Chebyshev polynomials and variable precision arithmetic to approximate Caputo fractional derivatives and Riemann-Liouville fractional integrals, applied to solve Caputo-type advection-diffusion equations via Sylvester equations.


<details>
  <summary>Details</summary>
Motivation: To develop stable numerical methods for approximating fractional derivatives and integrals, and apply them to solve Caputo-type advection-diffusion equations in multiple spatial dimensions.

Method: Pseudospectral approach using shifted Chebyshev polynomials to create operational matrices, employing variable precision arithmetic for stability, and transforming discretization into Sylvester (tensor) equations for solving advection-diffusion equations.

Result: Successfully developed stable operational matrices and applied them to solve Caputo-type advection-diffusion equations, with numerical experiments confirming effectiveness even for highly oscillatory functions in time.

Conclusion: The approach using shifted Chebyshev polynomials with variable precision arithmetic provides an effective and stable method for numerical approximation of fractional derivatives and solving fractional advection-diffusion equations.

Abstract: In this paper, using a pseudospectral approach, we develop operational matrices based on the shifted Chebyshev polynomials to approximate numerically Caputo fractional derivatives and Riemann-Liouville fractional integrals. In order to make the generation of these matrices stable, we use variable precision arithmetic. Then, we apply the Caputo differentiation matrices to solve numerically Caputo-type advection-diffusion equations in one and multiple spatial dimensions, which involves transforming the discretization of the concerning equation into a Sylvester (tensor) equation. We provide complete Matlab codes, whose implementation is carefully explained. The numerical experiments involving highly oscillatory functions in time confirm the effectiveness of this approach.

</details>


### [5] [Asymptotic models for time-domain scattering by small particles of arbitrary shapes](https://arxiv.org/abs/2511.11103)
*Maryna Kachanovska,Adrian Savchuk*

Main category: math.NA

TL;DR: Asymptotic modeling of time-dependent wave scattering by multiple small particles using boundary integral formulation with Galerkin discretization, including simplified models for computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop efficient computational methods for wave scattering by multiple small particles of arbitrary shape, addressing the computational challenges when particle size is small and number of obstacles is large.

Method: Boundary integral formulation semi-discretized in space using Galerkin approach with specialized basis functions, plus derivation of simplified models and Born approximation for improved computational efficiency.

Result: Derived asymptotic model valid as particle size tends to zero, with error analysis provided for high-order models and validation through numerical experiments.

Conclusion: The developed asymptotic models and computational approaches effectively address wave scattering problems involving multiple small particles, with validated accuracy and improved computational efficiency.

Abstract: In this work, we investigate time-dependent wave scattering by multiple small particles of arbitrary shape. To approximate the solution of the associated boundary-value problem, we derive an asymptotic model that is valid in the limit as the particle size tends to zero. Our method relies on a boundary integral formulation, semi-discretized in space using a Galerkin approach with appropriately chosen basis functions, s.t. convergence is achieved as the particle size vanishes rather than by increasing the number of basis functions. Since the computation of the Galerkin matrix involves double integration over particles, the method can become computationally demanding when the number of obstacles is large. To address this, we also derive a simplified model and consider the Born approximation to improve computational efficiency. For the high-order models, we provide an error analysis, supported and validated by numerical experiments.

</details>


### [6] [Extended-Krylov-subspace methods for trust-region and norm-regularization subproblems](https://arxiv.org/abs/2511.11135)
*Hussam Al Daas,Nicholas I. M. Gould*

Main category: math.NA

TL;DR: The paper introduces TREK/NREK, an efficient method for solving trust-region and norm-regularization subproblems by exploiting their low-rank manifold structure and using extended-Krylov-subspace iterations with a single matrix factorization.


<details>
  <summary>Details</summary>
Motivation: Trust-region and norm-regularization problems are common subproblems in optimization that typically require multiple matrix factorizations or standard Krylov subspace methods, which can be computationally expensive.

Method: The method identifies that solutions lie on a low-rank manifold, builds a basis using an efficient extended-Krylov-subspace iteration with only one matrix factorization, and solves subspace problems using high-order root-finding methods.

Result: Numerical results demonstrate the effectiveness of the TREK/NREK approach, providing an alternative to methods requiring multiple factorizations or standard Krylov subspaces.

Conclusion: The proposed method offers an efficient solution for trust-region and norm-regularization subproblems by leveraging their low-rank manifold structure and using extended-Krylov subspaces with minimal computational overhead.

Abstract: We consider an effective new method for solving trust-region and norm-regularization problems that arise as subproblems in many optimization applications. We show that the solutions to such subproblems lie on a manifold of approximately very low rank as a function of their controlling parameters (trust-region radius or regularization weight). Based on this, we build a basis for this manifold using an efficient extended-Krylov-subspace iteration that involves a single matrix factorization. The problems within the subspace using such a basis may be solved at very low cost using effective high-order root-finding methods. This then provides an alternative to common methods using multiple factorizations or standard Krylov subspaces. We provide numerical results to illustrate the effectiveness of our {\tt TREK}/{\tt NREK} approach.

</details>


### [7] [One-Shot Transfer Learning for Nonlinear PDEs with Perturbative PINNs](https://arxiv.org/abs/2511.11137)
*Samuel Auroy,Pavlos Protopapas*

Main category: math.NA

TL;DR: A framework combining perturbation theory with one-shot transfer learning in PINNs to solve nonlinear PDEs by decomposing them into linear subproblems, enabling fast adaptation to new instances without retraining.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve nonlinear PDEs with varying parameters, perturbations, or boundary conditions by leveraging transfer learning to avoid expensive retraining for each new problem instance.

Method: Decompose nonlinear PDEs with polynomial terms into linear subproblems using perturbation theory, solve with Multi-Head PINN to learn latent representations, then obtain closed-form solutions for new instances.

Result: Achieved errors around 1e-3 on KPP-Fisher and wave equations, adapting to new instances in under 0.2 seconds with comparable accuracy to classical solvers but faster transfer.

Conclusion: Successfully extended one-shot transfer learning from ODEs to PDEs, derived closed-form adaptation solutions, and demonstrated efficient accuracy on canonical nonlinear PDEs, with potential extensions to derivative-dependent nonlinearities and higher dimensions.

Abstract: We propose a framework for solving nonlinear partial differential equations (PDEs) by combining perturbation theory with one-shot transfer learning in Physics-Informed Neural Networks (PINNs). Nonlinear PDEs with polynomial terms are decomposed into a sequence of linear subproblems, which are efficiently solved using a Multi-Head PINN. Once the latent representation of the linear operator is learned, solutions to new PDE instances with varying perturbations, forcing terms, or boundary/initial conditions can be obtained in closed form without retraining.
  We validate the method on KPP-Fisher and wave equations, achieving errors on the order of 1e-3 while adapting to new problem instances in under 0.2 seconds; comparable accuracy to classical solvers but with faster transfer. Sensitivity analyses show predictable error growth with epsilon and polynomial degree, clarifying the method's effective regime.
  Our contributions are: (i) extending one-shot transfer learning from nonlinear ODEs to PDEs, (ii) deriving a closed-form solution for adapting to new PDE instances, and (iii) demonstrating accuracy and efficiency on canonical nonlinear PDEs. We conclude by outlining extensions to derivative-dependent nonlinearities and higher-dimensional PDEs.

</details>


### [8] [Inverse modeling of porous flow through deep neural networks: the case of coffee percolation](https://arxiv.org/abs/2511.11194)
*Antoniorenee Barletta,Salvatore Cuomo,Nadaniela Egidi,Josephin Giacomini,Pierluigi Maponi*

Main category: math.NA

TL;DR: This paper solves the inverse problem of espresso coffee extraction by developing a framework to reconstruct brewing conditions from desired chemical profiles, combining multiphysics modeling with data-driven methods.


<details>
  <summary>Details</summary>
Motivation: To enable personalized brewing, recipe optimization, and smart coffee-machine integration by solving the inverse problem of determining brewing parameters that produce specific chemical profiles in espresso.

Method: Derived a reduced forward operator from a high-fidelity multiphysics percolation model, then established mathematical conditions for local solvability using the Constant Rank Theorem and developed data-driven inverse mapping.

Result: The learned inverse map accurately reconstructs brewing temperature, grind size, and powder composition, validated through extensive experiments including off-grid testing.

Conclusion: The framework successfully combines rigorous analytical guarantees with modern data-driven methods to provide a principled and computationally efficient solution for inverse espresso extraction problems.

Abstract: This work addresses the inverse problem of espresso coffee extraction, in which one aims to reconstruct the brewing conditions that generate a desired chemical profile in the final beverage. Starting from a high-fidelity multiphysics percolation model, describing fluid flow, solute transport, solid, liquid reactions, and heat exchange within the coffee bed, we derive a reduced forward operator mapping controllable brewing parameters to the concentrations of the main chemical species in the cup. From a mathematical standpoint, we formalize the structural requirements for the local solvability of inverse problems, providing a minimal analytical condition for the existence of a (local) inverse map: continuous differentiability of the forward operator and a locally constant, nondegenerate Jacobian rank. Under these assumptions, the Constant Rank Theorem ensures that the image of the forward operator is a smooth embedded manifold on which well-defined local right-inverses exist.
  Extensive experiments, including off-grid validation, show that the learned inverse map accurately reconstructs brewing temperature, grind size, and powder composition. The resulting framework combines rigorous analytical guarantees with modern data-driven methods, providing a principled and computationally efficient solution to the inverse extraction problem and enabling personalised brewing, recipe optimisation, and integration into smart coffee-machine systems.

</details>


### [9] [The RBF Collocation Method to Design a Digital Twin for Coffee Percolation](https://arxiv.org/abs/2511.11202)
*Nadaniela Egidi,Lauro Fioretti,Josephin Giacomini,Pierluigi Maponi,Gianluca Pacini*

Main category: math.NA

TL;DR: Numerical solution for espresso coffee extraction using meshless collocation method with radial basis functions


<details>
  <summary>Details</summary>
Motivation: Espresso coffee extraction is a complex physico-chemical process that can be modeled with coupled partial differential equations, requiring accurate numerical solutions

Method: Meshless Collocation Method using Radial Basis Functions and Kansa's approach

Result: The method proves accurate and robust compared to reference numerical solutions from established simulation software

Conclusion: The proposed meshless method provides an effective numerical solution for modeling espresso coffee extraction processes

Abstract: Espresso coffee extraction is a complex physico-chemical process and can be modeled through a system of coupled partial differential equations. We present a numerical solution based on a meshless Collocation Method using Radial Basis Functions and Kansa's approach, which reveals to be accurate and robust in comparison to a reference numerical solution provided by a well-known simulation software.

</details>


### [10] [The modified Physics-Informed Hybrid Parallel Kolmogorov--Arnold and Multilayer Perceptron Architecture with domain decomposition](https://arxiv.org/abs/2511.11228)
*Qiumei Huang,Xu Wang,Yu Zhao*

Main category: math.NA

TL;DR: Proposed hybrid KAN-MLP PINN with trainable weighting and domain decomposition for high-frequency multiscale problems


<details>
  <summary>Details</summary>
Motivation: To overcome high-frequency and multiscale challenges in Physics-Informed Neural Networks

Method: Modified hybrid KAN-MLP PINN with trainable weighting parameter and overlapping domain decomposition

Result: Reduced training costs and improved computational efficiency compared to manual hyperparameter tuning

Conclusion: The proposed hybrid model effectively addresses high-frequency multiscale challenges in PINNs

Abstract: In this work, we propose a modified Hybrid Parallel Kolmogorov--Arnold Network and Multilayer Perceptron Physics-Informed Neural Network to overcome the high-frequency and multiscale challenges inherent in Physics-Informed Neural Networks. This proposed model features a trainable weighting parameter to optimize the convex combination of outputs from the Kolmogorov--Arnold Network and the Multilayer Perceptron, thus maximizing the networks' capabilities to capture different frequency components. Furthermore, we adopt an overlapping domain decomposition technique to decompose complex problems into subproblems, which alleviates the challenge of global optimization. Benchmark results demonstrate that our method reduces training costs and improves computational efficiency compared with manual hyperparameter tuning in solving high-frequency multiscale problems.

</details>


### [11] [Analyzing Smoothness and Dynamics in an SEIR$^{\text{T}}$R$^{\text{P}}$D Endemic Model with Distributed Delays](https://arxiv.org/abs/2511.11246)
*Tin Nwe Aye,Linus Carlsson*

Main category: math.NA

TL;DR: Analysis of an SEIR^T R^P D endemic model with distributed delays for latency and temporary immunity, showing solution smoothness, boundedness, and numerical approximation properties.


<details>
  <summary>Details</summary>
Motivation: To understand the variability of latent periods and immunity durations across diseases using continuous integral kernels with compact support in delay-differential equations.

Method: Developed an SEIR^T R^P D endemic model expressed through delay-differential equations with distributed delays, using continuous integral kernels with compact support to define delay classes.

Result: Proved solution smoothness under mild parameter conditions, established boundedness and non-negativity, and showed through numerical simulations that continuous models can be approximated by discrete lag endemic models.

Conclusion: The study enhances understanding of infectious disease dynamics and provides insights for numerical approximation of exact solutions across different delay scenarios.

Abstract: This article explores the properties of an SEIR$^{\text{T}}$R$^{\text{P}}$D endemic model expressed through delay-differential equations with distributed delays for latency and temporary immunity. Our research delves into the variability of latent periods and immunity durations across diseases, in particular, we introduce a class of delays defined by continuous integral kernels with compact support. The main result of the paper is a kind of smoothening property which the solution function posesses under mild conditions of the system parameter functions. Also, boundedness and non-negativity is proved. Numerical simulations indicates that the continuous model can be approximated with a discrete lag endemic models. The study contributes to understanding infectious disease dynamics and provides insights into the numerical approximation of exact solution for different delay scenarios.

</details>


### [12] [L^1 data fitting for Inverse Problems yields optimal rates of convergence in case of discretized white Gaussian noise](https://arxiv.org/abs/2511.11321)
*Kristina Bätz,Frank Werner*

Main category: math.NA

TL;DR: Analysis of L^1 data fitting in Tikhonov regularization for inverse problems, showing improved robustness and order-optimal convergence rates even with regular data.


<details>
  <summary>Details</summary>
Motivation: To investigate whether L^1 data fitting maintains good performance with regular data without outliers, addressing uncertainty about its effectiveness beyond outlier scenarios.

Method: Generalized Tikhonov regularization with L^1 data fidelity for inverse problems F(u) = g, considering general measurement errors and errors in the forward operator, applied to discretized Gaussian white noise.

Result: Derived error bounds demonstrate order-optimal rates of convergence for L^1 data fitting in Tikhonov regularization.

Conclusion: L^1 data fitting provides both improved robustness and maintains optimal performance with regular data, achieving order-optimal convergence rates as validated by numerical simulations.

Abstract: It is well-known in practice, that L^1 data fitting leads to improved robustness compared to standard L^2 data fitting. However, it is unclear whether resulting algorithms will perform as well in case of regular data without outliers. In this paper, we therefore analyze generalized Tikhonov regularization with L^1 data fidelity for Inverse Problems F(u) = g in a general setting, including general measurement errors and errors in the forward operator. The derived results are then applied to the situation of discretized Gaussian white noise, and we show that the resulting error bounds allow for order-optimal rates of convergence. These findings are also investigated in numerical simulations.

</details>


### [13] [A pressure-robust and parameter-free enriched Galerkin method for the Navier-Stokes equations of rotational form](https://arxiv.org/abs/2511.11330)
*Shuai Su,Xiurong Yan,Qian Zhang*

Main category: math.NA

TL;DR: Developed a novel enriched Galerkin method for steady incompressible Navier-Stokes equations in rotational form that is pressure-robust and parameter-free.


<details>
  <summary>Details</summary>
Motivation: To create a method that addresses the pressure-robustness issue in Navier-Stokes simulations while avoiding parameter dependencies.

Method: Enriched Galerkin method using first-order continuous Galerkin space enriched with piecewise constants along edges/faces, with modified gradient and divergence operators and velocity reconstruction for pressure robustness.

Result: Established existence, uniqueness under small-data assumption, and convergence of the method, with effectiveness confirmed through numerical experiments.

Conclusion: The proposed EG method successfully achieves pressure robustness without parameters and is well-suited for the rotational form of Navier-Stokes equations.

Abstract: In this paper, we develop a novel enriched Galerkin (EG) method for the steady incompressible Navier-Stokes equations in rotational form, which is both pressure-robust and parameter-free. The EG space employed here, originally proposed in [1], differs from traditional EG methods: it enriches the first-order continuous Galerkin (CG) space with piecewise constants along edges in two dimensions or on faces in three dimensions, rather than with elementwise polynomials. Within this framework, the gradient and divergence are modified to incorporate the edge/face enrichment, while the curl remains applied only to the CG component, an inherent feature that makes the space particularly suitable for the rotational form. The proposed EG method achieves pressure robustness through a velocity reconstruction operator. We establish existence, uniqueness under a small-data assumption, and convergence of the method, and confirm its effectiveness by numerical experiments.

</details>


### [14] [A Quantum Spectral Method for Non-Periodic Boundary Value Problems](https://arxiv.org/abs/2511.11494)
*Eky Febrianto,Yiren Wang,Burigede Liu,Michael Ortiz,Fehmi Cirak*

Main category: math.NA

TL;DR: A quantum spectral method with polylogarithmic complexity for solving non-periodic boundary value problems with arbitrary Dirichlet boundary conditions, extending Fourier-based approaches to handle boundary conditions via quantum transforms.


<details>
  <summary>Details</summary>
Motivation: To leverage quantum computing's potential for solving computational mechanics problems in polylogarithmic time, particularly extending existing periodic problem methods to handle non-periodic boundary conditions efficiently.

Method: Extends Fourier spectral methods using quantum Fourier transform (QFT), implements boundary conditions via domain doubling and antisymmetric reflection (quantum sine transform), and decomposes solutions for non-zero boundary conditions into homogeneous and boundary-conforming parts.

Result: Successfully demonstrates the approach on Dirichlet-Poisson problems and fractional stochastic PDEs, with circuit implementation and numerical evidence confirming polylogarithmic complexity.

Conclusion: The proposed quantum spectral method efficiently handles arbitrary Dirichlet boundary conditions with polylogarithmic complexity, providing a general framework for solving boundary value problems on quantum computers.

Abstract: Quantum computing holds the promise of solving computational mechanics problems in polylogarithmic time, meaning computational time scales as $\mathscr{O}((\log N)^c)$, where $N$ is the problem size and $c$ a constant. We propose a quantum spectral method with polylogarithmic complexity for solving non-periodic boundary value problems with arbitrary Dirichlet boundary conditions. Our method extends the recently proposed approach by Liu et al. (2025), in which periodic problems are discretised using truncated Fourier series. In such spectral methods, the discretisation of boundary value problems with constant coefficients leads to a set of algebraic equations in the Fourier space. We implement the respective diagonal solution operator by first approximating it with a polynomial and then quantum encoding the polynomial. The mapping between the physical and Fourier spaces is accomplished using the quantum Fourier transform (QFT). To impose zero Dirichlet boundary conditions, we double the domain size and reflect all physical fields antisymmetrically. The respective reflection matrix defines the quantum sine transform (QST) by pre- and post-multiplying with the QFT. For non-zero Dirichlet boundary conditions, the solution is decomposed into a boundary-conforming and a homogeneous part. The homogenous part is determined by solving a problem with a suitably modified forcing vector. We illustrate the basic approach with a Dirichlet-Poisson problem and demonstrate its generality by applying it to a fractional stochastic PDE for modelling spatial random fields. We discuss the circuit implementation of the proposed approach and provide numerical evidence confirming its polylogarithmic complexity.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Local controllability of free boundary three-dimensional semilinear radial parabolic equations](https://arxiv.org/abs/2511.10795)
*Juan Límaco,Luis P. Yapu*

Main category: math.AP

TL;DR: Local null controllability proven for free boundary semilinear heat equation with Stefan boundary condition using radial symmetry and Carleman inequality.


<details>
  <summary>Details</summary>
Motivation: First controllability result for this problem with Stefan boundary condition in more than one spatial dimension.

Method: Reduce to 1D formulation, adapt Carleman inequality, use Schauder fixed-point theorem.

Result: Established local null controllability for the free-boundary problem.

Conclusion: Successfully proved local null controllability, marking first such result for Stefan boundary condition in higher dimensions.

Abstract: We prove that a free boundary semilinear heat equation with Stefan boundary condition and radially symmetric data is locally null controllable. The strategy involves reducing the problem to the corresponding one-dimensional formulation and adapting a Carleman inequality in that setting. The local null controllability of the free-boundary problem is then established via the Schauder fixed-point theorem. To the best of our knowledge, this is the first controllability result for this problem with Stefan boundary condition in more than one spatial dimension.

</details>


### [16] [Guaranteeing Higher Order Convergence Rates for Accelerated Wasserstein Gradient Flow Schemes](https://arxiv.org/abs/2511.10884)
*Raymond Chu,Matt Jacobs*

Main category: math.AP

TL;DR: This paper introduces an accelerated second-order scheme for Wasserstein gradient flows that achieves optimal quadratic convergence for smooth functionals and maintains first-order convergence with strong stability under weaker assumptions.


<details>
  <summary>Details</summary>
Motivation: To develop higher-order accurate time schemes for Wasserstein gradient flows that overcome limitations of classical first-order methods like the JKO scheme, providing faster convergence while maintaining stability.

Method: A novel accelerated second-order minimizing movements scheme that leverages the differential structure of Wasserstein space in both Eulerian and Lagrangian coordinates.

Result: For smooth functionals: provable optimal quadratic convergence rate. Under weaker assumptions (Wasserstein differentiability and λ-displacement convexity): first-order convergence rate with strong numerical stability, near monotone energy decrease, and exponential decrease of Wasserstein gradient norm for L-smooth λ-displacement convex functionals.

Conclusion: This work provides the first fully rigorous proof of accelerated second-order convergence for smooth functionals and demonstrates that the scheme performs at least as well as the classical JKO scheme for λ-displacement convex Wasserstein differentiable functionals.

Abstract: In this paper, we study higher-order-accurate-in-time minimizing movements schemes for Wasserstein gradient flows. We introduce a novel accelerated second-order scheme, leveraging the differential structure of the Wasserstein space in both Eulerian and Lagrangian coordinates. For sufficiently smooth energy functionals, we show that our scheme provably achieves an optimal quadratic convergence rate. Under the weaker assumptions of Wasserstein differentiability and $λ$-displacement convexity (for any $λ\in \mathbb{R}$), we show that our scheme still achieves a first-order convergence rate and has strong numerical stability. In particular, we show that the energy is nearly monotone in general, while when the energy is $L$-smooth and $λ$-displacement convex (with $λ>0$), we prove the energy is non-increasing and the norm of the Wasserstein gradient is exponentially decreasing along the iterates. Taken together, our work provides the first fully rigorous proof of accelerated second-order convergence rates for smooth functionals and shows that the scheme performs no worse than the classical scheme JKO scheme for functionals that are $λ$-displacement convex and Wasserstein differentiable.

</details>


### [17] [The energy scaling behaviour of singular perturbation models of staircase type in linearized elasticity for higher order laminates](https://arxiv.org/abs/2511.11102)
*Lennart Machill,Angkana Rüland*

Main category: math.AP

TL;DR: This paper analyzes scaling behavior in geometrically linearized elasticity with singular perturbations and higher-order lamination data, showing that scaling bounds depend on two key parameters: lamination order and number of rank-one directions.


<details>
  <summary>Details</summary>
Motivation: To understand how scaling laws in geometrically linearized elasticity differ from settings without gauge invariances, particularly when dealing with staircase-type boundary data and higher lamination orders.

Method: The study investigates lower scaling bounds for Dirichlet boundary data and periodic configurations with mean value constraints, comparing them with upper bounds in specific geometries and well constellations.

Result: Lower scaling bounds depend on two parameters (lamination order and number of rank-one directions), unlike single-parameter dependencies in gauge-invariant settings. Upper bound analysis suggests these lower bounds are sharp.

Conclusion: Tracking both lamination order and number of rank-one directions is essential for accurate scaling laws in geometrically linearized elasticity theory.

Abstract: We investigate the scaling behaviour of a singular perturbation model within the geometrically linearized theory of elasticity involving data of higher lamination order. We study boundary data which are of staircase type and show rather general lower scaling bounds, both in the setting of prescribed Dirichlet data and for periodic configurations with a mean value constraint. In contrast to the setting without gauge invariances, these lower scaling bounds depend on \emph{two} parameters -- the order of lamination of the boundary data as well as the number of involved (non-)degenerate symmetrized rank-one directions. By discussing upper bounds in specific geometries and for a specific constellation of wells, we give evidence of the sharpness of these lower bound estimates. Hence, it is necessary to keep track of the outlined \emph{two} parameters in deducing scaling laws within the geometrically linearized theory of elasticity.

</details>


### [18] [On non-uniqueness of solutions to degenerate parabolic equations in the context of option pricing in the Heston model](https://arxiv.org/abs/2511.11288)
*Ruslan R. Boyko*

Main category: math.AP

TL;DR: Analysis of non-unique call option pricing in Heston model using uniqueness theory for degenerate parabolic equations, with new example showing sublinear growth class accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the non-uniqueness problem in Heston model call option pricing by applying mathematical uniqueness theory for degenerate parabolic equations.

Method: Apply existing mathematical theory of uniqueness classes for degenerate parabolic equations to analyze the Heston model, constructing a new example for the special degeneracy case.

Result: Demonstrated the accuracy of uniqueness theorem for solutions in the class of functions with sublinear growth at infinity through constructed example.

Conclusion: The mathematical theory of uniqueness classes provides a framework to address non-uniqueness in Heston model pricing, with sublinear growth class proving effective for the analyzed degeneracy case.

Abstract: It is known that the price of call options in the Heston model is determined in a non-unique way. In this paper, this problem is analyzed from the point of view of the existing mathematical theory of uniqueness classes for degenerate parabolic equations. For the special case of degeneracy, a new example is constructed demonstrating the accuracy of the uniqueness theorem for a solution in the class of functions with sublinear growth at infinity.

</details>


### [19] [A quantitative Talenti-type comparison result with Robin boundary conditions](https://arxiv.org/abs/2511.11316)
*Vincenzo Amato,Rosa Barbato,Simone Cito,Alba Lia Masiello,Gloria Paoli*

Main category: math.AP

TL;DR: This paper establishes a quantitative version of the Talenti comparison principle for Poisson equations with Robin boundary conditions, measuring asymmetry of domain. It also provides alternative proofs for quantitative Saint-Venant and Faber-Krahn inequalities.


<details>
  <summary>Details</summary>
Motivation: To enhance the classical Talenti comparison principle by providing quantitative bounds in terms of domain asymmetry for Poisson equations with Robin boundary conditions.

Method: Careful analysis of the propagation of asymmetry for level sets of PDE solutions, establishing quantitative enhancements to classical inequalities.

Result: Proved quantitative Talenti comparison principle, obtained alternative proofs for quantitative Saint-Venant inequality (Robin torsion) and quantitative Faber-Krahn inequality (first Robin eigenvalue in planar case).

Conclusion: Completed the framework for rigidity results of Talenti inequalities with Robin boundary conditions through quantitative analysis of domain asymmetry.

Abstract: The purpose of this paper is to establish a quantitative version of the Talenti comparison principle for solutions to the Poisson equation with Robin boundary conditions. This quantitative enhancement is proved in terms of the asymmetry of domain. The key role is played by a careful analysis of the propagation of asymmetry for the level sets of the solutions of a PDE. As a byproduct, we obtain an alternative proof of the quantitative Saint-Venant inequality for the Robin torsion and, in the planar case, of the quantitative Faber-Krahn inequality for the first Robin eigenvalue. In addition, we complete the framework of the rigidity result of the Talenti inequalities with Robin boundary conditions.

</details>


### [20] [Global attractor for a Cahn-Hilliard-chemotaxis model with logistic degradation](https://arxiv.org/abs/2511.11363)
*Giulio Schimperna,Antonio Segatti*

Main category: math.AP

TL;DR: Analysis of a Cahn-Hilliard system coupled with nutrient diffusion for tumor modeling, focusing on long-time behavior, dissipativity, and global attractor existence.


<details>
  <summary>Details</summary>
Motivation: Model tumor progression where phase separation represents cancer cell distribution and chemical concentration represents nutrients/drugs, with chemotaxis-like cross-diffusion effects.

Method: Mathematical analysis using infinite-dimensional dynamical systems theory, identifying regularity settings for well-posedness, proving strong dissipativity and asymptotic compactness of the semigroup.

Result: System generates a closed semigroup, is strongly dissipative and asymptotically compact, ensuring existence of global attractor in suitable phase space. Nutrient concentration remains positive on finite time intervals under additional assumptions.

Conclusion: The coupled Cahn-Hilliard-nutrient system exhibits robust long-time behavior with global attractor existence, providing mathematical foundation for tumor progression modeling with nutrient dynamics.

Abstract: We consider a mathematical model coupling the Cahn-Hilliard system for phase separation with an additional equation describing the diffusion process of a chemical quantity whose concentration influences the physical process. The main application of the model refers to tumor progression, where the phase variable denotes the local proportion of active cancer cells and the chemical concentration may refer to a nutrient transported by the blood flow or to a drug administered to the patient. The resulting system is characterized by cross-diffusion effects similar to those appearing in the Keller-Segel model for chemotaxis; in particular, the nutrient tends to be attracted towards the regions where more active tumor cells are present (and consume it in a quickier way). Complementing various recent results on related models, we investigate here the long-time behavior of solutions under the perspective of infinite-dimensional dynamical systems. To this aim, we first identify a regularity setting in which the system is well posed and generates a closed semigroup according to the terminology introduced by Pata and Zelik. Then, partly based on the approach introduced by Rocca and the first author for the Cahn-Hilliard system with singular potential, we prove that the semigroup is strongly dissipative and asymptotically compact so guaranteeing the existence of the global attractor in a suitable phase space. Finally, we discuss the sign properties of the nutrient and prove that, under additional assumptions on the initial data, its concentration is uniformly larger than some strictly positive constant at least on finite time intervals.

</details>


### [21] [Entire Nodal Solutions with Prescribed Symmetry to Caffarelli-Kohn-Nirenberg Equations](https://arxiv.org/abs/2511.11543)
*Edward Chernysh*

Main category: math.AP

TL;DR: Existence of sign-changing entire solutions for weighted critical p-Laplace equations of Caffarelli-Kohn-Nirenberg type, with analysis of symmetry classes and conditions for compatibility.


<details>
  <summary>Details</summary>
Motivation: To establish the existence of sign-changing entire solutions to weighted critical p-Laplace equations and understand the role of symmetry in these solutions.

Method: Investigation of symmetry classes and configurations, analysis of conditions under which symmetry types are compatible or incompatible.

Result: Demonstrated existence of non-trivial sign-changing solutions that respect prescribed symmetry, and existence of entire nodal solutions for infinite distinct symmetry types.

Conclusion: Successfully established existence of sign-changing entire solutions with various symmetry types, providing insights into symmetry compatibility in weighted critical p-Laplace equations.

Abstract: We establish the existence of sign-changing entire solutions to weighted critical $p$-Laplace equations of the Caffarelli-Kohn-Nirenberg type. In doing so, we investigate classes of symmetry and show that, for suitable symmetry configurations, there exists a non-trivial solution which changes sign and respects the corresponding prescribed symmetry. In addition, we describe conditions under which these symmetry-types are incompatible. Especially, we demonstrate the existence of entire nodal solutions for an infinite number of distinct symmetry types.

</details>


### [22] [Structure theory of parabolic nodal and singular sets](https://arxiv.org/abs/2511.11570)
*Max Hallgren,Robert Koirala,Zilu Ma*

Main category: math.AP

TL;DR: The paper establishes new estimates for nodal and singular sets of solutions to parabolic inequalities, showing these sets are mostly covered by regular parabolic Lipschitz graphs and satisfy parabolic Minkowski estimates.


<details>
  <summary>Details</summary>
Motivation: To understand the size and structure of nodal sets {u=0} and singular sets {u=|∇u|=0} for solutions to parabolic inequalities with parabolic Lipschitz coefficients, extending results even for the heat equation.

Method: Develops new estimates using parabolic Lipschitz coefficients and doubling quantity at a point, proving coverage by regular parabolic Lipschitz graphs.

Result: Shows almost all nodal and singular sets are covered by regular parabolic Lipschitz graphs with estimates, and both sets satisfy parabolic Minkowski estimates depending only on doubling quantity.

Conclusion: Provides comprehensive structural analysis of nodal and singular sets in parabolic inequalities, with many results being novel even for the standard heat equation.

Abstract: We establish new estimates for the size and structure of the nodal set $\{u=0\}$ and the singular set $\{u=|\nabla u|=0\}$ of solutions $u$ to parabolic inequalities with parabolic Lipschitz coefficients. In particular, we show that almost all of the nodal and singular sets are covered by regular parabolic Lipschitz graphs with estimates, and that both sets satisfy parabolic Minkoswki estimates depending only on a doubling quantity at a point. Many of our results are new even for the heat equation on $\mathbb{R}^{n}\times \mathbb{R}$.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [23] [BOA Constrictor: A Mamba-based lossless compressor for High Energy Physics data](https://arxiv.org/abs/2511.11337)
*Akshat Gupta,Caterina Doglioni,Thomas Joseph Elliott*

Main category: physics.comp-ph

TL;DR: BOA Constrictor is a novel streaming-capable lossless compressor using Mamba architecture that achieves state-of-the-art compression ratios on HEP data, though with lower throughput than traditional methods.


<details>
  <summary>Details</summary>
Motivation: HEP experiments generate petabyte-scale data with deep structure that traditional compression algorithms like LZMA and ZLIB fail to fully exploit, creating significant storage challenges.

Method: BOA Constrictor combines an autoregressive Mamba model for next-byte prediction with a parallelized streaming range coder, built upon the Mamba architecture to capture long-range dependencies in sequences.

Result: Achieved state-of-the-art compression ratios: 2.21× on ATLAS dataset (vs 1.69× for LZMA-9) and 44.14× on highly-structured CMS dataset (vs 27.14× for LZMA-9), with modest ~4.5MB model size. However, throughput (Storage-Saving Rate) lags behind optimized CPU algorithms like ZLIB.

Conclusion: Mamba-based approach is a highly promising proof-of-principle but requires significant performance optimization and hardware portability work to become production-ready for HEP community.

Abstract: The petabyte-scale data generated annually by High Energy Physics (HEP) experiments like those at the Large Hadron Collider present a significant data storage challenge. Whilst traditional algorithms like LZMA and ZLIB are widely used, they often fail to exploit the deep structure inherent in scientific data. We investigate the application of modern state space models (SSMs) to this problem, which have shown promise for capturing long-range dependencies in sequences. We present the Bytewise Online Autoregressive (BOA) Constrictor, a novel, streaming-capable lossless compressor built upon the Mamba architecture. BOA combines an autoregressive Mamba model for next-byte prediction with a parallelised streaming range coder. We evaluate our method on three distinct structured datasets in HEP, demonstrating state-of-the-art compression ratios, improving upon LZMA-9 across all datasets. These improvements range from 2.21$\times$ (vs. 1.69$\times$) on the ATLAS dataset to a substantial 44.14$\times$ (vs. 27.14$\times$) on the highly-structured CMS dataset, with a modest $\sim 4.5$MB model size. However, this gain in compression ratio comes with a trade-off in throughput; the Storage-Saving Rate ($σ_{SSR}$) of our prototype currently lags behind highly-optimised CPU-based algorithms like ZLIB. We conclude that while this Mamba-based approach is a highly promising proof-of-principle, significant future work on performance optimisation and hardware portability is required to develop it into a production-ready tool for the HEP community.

</details>


### [24] [Beyond quantum mean-field approximation: Phase-space formulation of many-body time-dependent density functional theory and efficient spectral approximations](https://arxiv.org/abs/2511.11354)
*Jiong-Hang Liang,Yunfeng Xiong*

Main category: physics.comp-ph

TL;DR: This paper develops a computational method to overcome the prohibitive dimensionality of time-dependent 2-reduced density matrix (2-RDM) dynamics by using Wigner phase-space formulation and spectral approximations, enabling first real simulations of 2-RDM dynamics.


<details>
  <summary>Details</summary>
Motivation: TDDFT is inadequate for describing two-body observables like double-excitation probability and two-body dynamic correlation. While 2-RDM directly represents these observables, its usage is prohibitive due to high dimensionality (4-D space for unidimensional 2-RDM).

Method: Uses Wigner phase-space formulation of 2-RDM and efficient spectral approximations to nonlocal quantum potentials. For periodic cases, employs pseudo-difference operators; for non-periodic cases, uses Chebyshev spectral element method. Integrates these with distributed characteristics method for massively parallel computation.

Result: Successfully implemented the first real simulations of 2-RDM dynamics. Numerical experiments demonstrate two-body correction to quantum kinetic theory and show increase in system entropy induced by two-body interactions.

Conclusion: This work paves the way for accurate description of 2-RDM dynamics and advances practical application of many-body TDDFT by overcoming the computational challenges of high-dimensional 2-RDM simulations.

Abstract: As a universal quantum mechanical approach to the dynamical many-body problem, the time-dependent density functional theory (TDDFT) might be inadequate to describe crucial observables that rely on two-body evolution behavior, like the double-excitation probability and two-body dynamic correlation. One promising remedy is to utilize the time-dependent 2-reduced density matrix (2-RDM) that directly represents two-body observables in an N-particle system, and resort to the extended TDDFT for multibody densities to break the confines of spatial local on one-body density [Phys. Rev. Lett. 26(6) (2024) 263001]. However, the usage of 2-RDM is prohibitive due to the augmented dimensionality, e.g., 4-D space for unidimensional 2-RDM. This work addresses the high-dimensional numerical challenges by using an equivalent Wigner phase-space formulation of 2-RDM and seeking efficient spectral approximations to nonlocal quantum potentials. For spatial periodic case, a pseudo-difference operator approach is derived for both the Hartree-exchange-correlation term and two-body collision operator, while the discretization via the Chebyshev spectral element method is provided for non-periodic case. A massively parallel numerical scheme, which integrates these spectral approximations with a distributed characteristics method, allows us to make the first attempt for real simulations of 2-RDM dynamics. Numerical experiments demonstrate the two-body correction to the quantum kinetic theory, and show the increase in the system's entropy induced by the two-bdoy interaction. Thus it may pave the way for an accurate description of 2-RDM dynamics and advance to a practical application of many-body TDDFT.

</details>


### [25] [Power law attention biases for molecular transformers](https://arxiv.org/abs/2511.11489)
*Jay Shen,Yifeng Tang,Andrew Ferguson*

Main category: physics.comp-ph

TL;DR: Transformers for molecular property prediction underperform compared to other domains due to lack of structural biases. The paper proposes simple attention biases based on physical power laws to encode atomic relationships.


<details>
  <summary>Details</summary>
Motivation: Transformers dominate most data modalities but not molecular property prediction, likely due to missing structural biases that capture atomic relationships effectively.

Method: Proposes attention biases using physical power laws: b_ij = p log||r_i - r_j|| to weight attention probabilities based on interatomic distances.

Result: Outperforms positional encodings and graph attention on QM9 and SPICE datasets, remaining competitive with more complex Gaussian kernel biases.

Conclusion: Simple attention biases can compensate for scaled dot-product attention ablation, offering a low-cost path to interpretable molecular transformers.

Abstract: Transformers are the go-to architecture for most data modalities due to their scalability. While they have been applied extensively to molecular property prediction, they do not dominate the field as they do elsewhere. One cause may be the lack of structural biases that effectively capture the relationships between atoms. Here, we investigate attention biases as a simple and natural way to encode structure. Motivated by physical power laws, we propose a family of low-complexity attention biases $b_{ij} = p \log|| \mathbf{r}_i - \mathbf{r}_j||$ which weigh attention probabilities according to interatomic distances. On the QM9 and SPICE datasets, this approach outperforms positional encodings and graph attention while remaining competitive with more complex Gaussian kernel biases. We also show that good attention biases can compensate for a complete ablation of scaled dot-product attention, suggesting a low-cost path toward interpretable molecular transformers.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [26] [Bremsstrahlung constraints on proton-Boron 11 inertial fusion](https://arxiv.org/abs/2511.10885)
*I. E. Ochs,E. J. Kolmes,A. S. Glasser,N. J. Fisch*

Main category: physics.plasm-ph

TL;DR: Proton-Boron fusion faces bremsstrahlung radiation losses exceeding fusion power, requiring extremely high areal density and pressure for breakeven in inertial confinement fusion.


<details>
  <summary>Details</summary>
Motivation: Proton-Boron 11 fusion is clean and safe but difficult for net power due to excessive bremsstrahlung radiation losses, especially with alpha particle poisoning.

Method: Used burn simulations incorporating bremsstrahlung emission and reabsorption to analyze conditions needed for radiation trapping in pB11 ICF plasma.

Result: Found that breakeven requires stagnation areal density ~100x higher than current state-of-the-art and pressures ~1000x higher.

Conclusion: Extreme plasma conditions far beyond current capabilities are needed to achieve scientific breakeven in proton-Boron inertial confinement fusion.

Abstract: Proton-Boron 11 (pB11) fusion is relatively safe and clean, but difficult to use for net power production, since bremsstrahlung radiation tends to radiate away power more quickly than it can be generated by fusion power, particularly once poisoning by alpha particles is taken into account. While in magnetic confinement fusion (MCF), this problem can be addressed by deconfining the alphas, in inertial confinement fusion (ICF) the alphas that heat the plasma linger for the duration of the reaction. Thus, it becomes essential to trap the bremsstrahlung radiation in the hotspot. Through burn simulations incorporating bremsstrahlung emission and reabsorption, we infer the necessary conditions to capture enough radiation to produce scientific breakeven in a pB11 ICF plasma. We find that breakeven requires a stagnation areal density roughly two orders of magnitude higher than the current state-of-the-art, at pressures three orders of magnitude higher.

</details>


### [27] [Decomposition of Methane Diluted with Inert Gas in an RF Discharge Cell](https://arxiv.org/abs/2511.10937)
*Sophia Gershman,Mikhail N. Shneider,Yevgeny Raitses*

Main category: physics.plasm-ph

TL;DR: Study shows that plasma discharge modes (contracted vs uniform) significantly affect methane decomposition efficiency, with contracted modes achieving 99.7% decomposition producing acetylene and graphitized carbon.


<details>
  <summary>Details</summary>
Motivation: Understanding how plasma discharge modes influence methane decomposition to optimize plasma-assisted chemical conversion for hydrogen-rich gases and carbon nanomaterials production.

Method: Used RF capacitively coupled discharges with methane/inert gas mixtures (Ar, Kr, He, Ne) at 2-3 torr pressure, analyzed discharge mode transitions, and developed 0-D model for Ar/CH4 discharge.

Result: Contracted discharge modes in Ar/Kr with ≤5% CH4 achieved 99.7% methane decomposition, producing acetylene and graphitized carbon. Ar/Kr showed mode transitions while He/Ne remained uniform. Ar metastables crucial for dissociation.

Conclusion: Discharge contraction plays crucial role in effective methane decomposition, with both electronic and thermal properties of plasma gas responsible for the effect, enabling high-value product formation.

Abstract: Decomposition of methane using non-thermal plasmas is an attractive route for producing hydrogen-rich gases and valuable carbon nanomaterials. Understanding how plasma discharge modes influence methane decomposition in optimizing plasma-assisted chemical conversion remains unexplored. This study explores the coupling between the discharge structure and product selectivity in RF capacitively coupled discharges operating in methane/inert gas mixtures in the pressure range of 2 to 3 torr. The discharge exhibits mode transitions from uniform to striated in Ar and Kr and from diffuse to contracted in Ar and Kr with 5 percent or less CH4. The discharges in He and Ne remained uniform under our operating conditions, and their mixtures with CH4 remained diffuse. A 0-D model for Ar/CH4 discharge established a threshold for contraction while also asserting the importance of Ar metastable in the dissociation and ionization processes. The highest degree of methane decomposition, 99.7% with the main products of acetylene and graphitized solid carbon was achieved in the contracted discharge mode for both Kr or Ar with 5% or less CH4. We demonstrate that contraction can play a crucial role in the effective decomposition of methane with value-added products and that both the electronic and thermal properties of plasma gas are responsible for this effect.

</details>


### [28] [Studies of laser stimulated photodetachment from nanoparticles for particle charge measurements](https://arxiv.org/abs/2511.10956)
*Y. A. Ussenov,M. N. Shneider,S. Yatom,Y. Raitses*

Main category: physics.plasm-ph

TL;DR: Laser-stimulated photodetachment (LSPD) method used to measure nanoparticle charge in dusty plasma, revealing charges lower than OML theory predictions due to electron depletion.


<details>
  <summary>Details</summary>
Motivation: Conventional microparticle charge diagnostics are ineffective for nanoparticles due to growth-induced size changes, plasma property variations, and visualization difficulties.

Method: Used LSPD with electron current monitoring via Langmuir probe, analyzed across dust growth phases, and estimated charge using particle density from laser-light extinction.

Result: For 154.3 nm particles, charge found to be ~16 elementary units, lower than OML theory predictions due to electron depletion effects.

Conclusion: LSPD is applicable for nanoparticle charge estimation but requires careful consideration of electron detachment from residual negative ions affecting current decay.

Abstract: Determining nanoparticle charge is more challenging than that for microparticles due to growth-induced size changes, substantial plasma property variations, and difficulties in visualizing individual particles, rendering conventional microparticle charge diagnostics ineffective in dusty plasma. In this work, we utilized laser-stimulated photodetachment (LSPD) to deduce the mean charge of nanoparticles. Nanoparticles were grown in an Ara and C2H2 mixture using a capacitively coupled RF discharge and the LSPD induced changes in the electron current monitored by a cylindrical Langmuir probe. LSPD signals were obtained and analyzed across different dust growth phases. The prolonged decay of electron current pulses was attributed to the presence of residual negative ions, caused by the effective electrostatic trapping of these ions and the potential post - LSPD re-formation of new ones. The charge per particle was estimated using known values of particle density obtained by laser-light extinction method. For particles with mean diameter Dp ~ 154.3 nm, the charge found to be of Qd~16 elementary charge units. The charge values appear to be lower than the values predicted by orbital motion limited (OML) theory. This deviation is commonly observed in nanodusty plasmas due to significant electron depletion. LSPD results in Ar and C2H2 nano-dusty plasma confirm the applicability of these method for estimating individual nanoparticle charges. However, it has also been demonstrated that electron detachment from residual background negative ions can influence the detachment current decay and must be carefully considered

</details>


### [29] [Optimization of Argon Plasma Working Pressure through Parallel PIC Simulations for Enhancement of Material Surface Treatment](https://arxiv.org/abs/2511.11145)
*Hadi Barati,Ali Torkaman,Mehdi Fardmanesh*

Main category: physics.plasm-ph

TL;DR: Developed parallel PIC method for plasma simulation using 35 processors, finding optimal argon pressure (100 mTorr at -500V) for maximum ion flux to electrodes.


<details>
  <summary>Details</summary>
Motivation: To efficiently simulate plasma dynamics and identify optimal conditions for surface modification using argon plasma.

Method: Parallel Particle-in-Cell (PIC) method with 35 processors dividing plasma volume, solving equations separately then combining results.

Result: Found optimal argon pressure of 100 mTorr at -500V for maximum ion flux; higher electrode potential increases flux.

Conclusion: Optimal pressure selection is crucial for effective argon plasma surface modification at given potentials.

Abstract: In this study, a novel method for simulating plasma dynamics using parallel programming has been developed. The equations based on Particle-in-Cell (PIC) method were utilized and adapted for this purpose. We utilized 35 processors from Sharif High Performance Computing (HPC) center and divided the plasma volume into 35 parts, with each part's PIC equation solved on a separate processor. Once the computations were completed, the results from all processors were combined to form a complete plasma volume. The simulations revealed that there is an optimal pressure for argon, at which the ion flux onto the electrode surface is maximized. Increasing the absolute value of the electrode potential also increases this flux. Therefore, for a given potential, selecting the optimal pressure is crucial for the most effective surface modification using argon plasma. In this work, for applied voltage of -500 V, the optimum pressure was 100 mTorr.

</details>


### [30] [Extending the Numerical Flow Iteration to the multi-species Vlasov-Maxwell system through Hamiltonian Splitting](https://arxiv.org/abs/2511.11322)
*Rostislav-Paul Wilhelm,Fabio Bacchini,Sebastian Schöps,Manuel Torrilhon,Melina Merkel,Matthias Kirchhart*

Main category: physics.plasm-ph

TL;DR: Extends the Numerical Flow Iteration (NuFI) method from Vlasov-Poisson to Vlasov-Maxwell systems, preserving its structure-preserving properties for electromagnetic kinetic plasma dynamics.


<details>
  <summary>Details</summary>
Motivation: To handle electromagnetic kinetic plasma dynamics while maintaining NuFI's advantages of high accuracy with low memory requirements.

Method: Leverages the Hamiltonian structure of the full Vlasov-Maxwell system to extend NuFI, storing temporal evolution of electromagnetic fields instead of distribution functions and reconstructing solutions via backward characteristics.

Result: Successfully extends NuFI to electromagnetic case while preserving its structure-preserving properties.

Conclusion: NuFI can be effectively extended to handle electromagnetic plasma dynamics while maintaining its high-fidelity, low-memory characteristics.

Abstract: The Numerical Flow Iteration (NuFI) method has recently been proposed as a memory-slim solution method for the Vlasov--Poisson system. It stores the temporal evolution of the electric field, instead of the distribution functions, and reconstructs the solution in each time step by following the characteristics backwards in time and reconstructing the solution from the initial distribution. NuFI has been shown to be more accurate than other state-of-the-art Vlasov solvers given the same amount of degrees of freedom as well as interpolation order, essentially making NuFI a high-fidelity but low-memory cost scheme. In this paper, we build on the Hamiltonian structure of the full Vlasov--Maxwell system to extend NuFI to handle electro-magnetic kinetic plasma dynamics. We show that the advanced structure-preserving properties of NuFI are preserved when extending to the electro-magnetic case.

</details>


### [31] [Investigation of Stimulated Brillouin Scattering Driven by Broadband Lasers in High-Z Plasmas](https://arxiv.org/abs/2511.11395)
*Xiaoran Li,Jie Qiu,Liang Hao,Shiyang Zou*

Main category: physics.plasm-ph

TL;DR: Broadband lasers suppress stimulated Brillouin scattering (SBS) in high-Z plasmas when bandwidth exceeds SBS growth rate by tens of times, with AuB showing better suppression than Au.


<details>
  <summary>Details</summary>
Motivation: To understand how broadband lasers can mitigate SBS in high-Z plasmas for inertial confinement fusion applications, as temporal incoherence affects laser-plasma interactions.

Method: Used 1D collisional particle-in-cell simulations with varying laser intensities and bandwidths in Au and AuB plasmas to study SBS evolution.

Result: Broadband lasers generate stochastic intensity pulses that intermittently drive SBS. Suppression occurs when laser bandwidth exceeds SBS temporal growth rate by several tens of times. AuB shows reduced SBS growth and earlier suppression onset than Au.

Conclusion: Broadband lasers effectively suppress SBS in high-Z plasmas when bandwidth sufficiently exceeds growth rate, with AuB offering better performance, providing guidance for fusion laser optimization.

Abstract: The evolution of stimulated Brillouin scattering (SBS) driven by broadband lasers in high-Z plasmas is investigated using one-dimensional collisional particle-in-cell simulations. The temporal incoherence of broadband lasers modulates the pump intensity, generating stochastic intensity pulses that intermittently drive SBS. The shortened coherence time weakens the three-wave coupling and continuously reduces the temporal growth rate, while the saturated reflectivity remains nearly unchanged until the bandwidth exceeds a critical threshold. Simulations with varying laser intensities and bandwidths reveal a consistent scaling behavior, indicating that effective suppression occurs only when the laser bandwidth exceeds the temporal growth rate of SBS by several tens of times. Comparative simulations in Au and AuB plasmas exhibit similar suppression trends, with AuB showing reduced SBS growth rate and reflectivity, and the onset of suppression occurring at a lower bandwidth. These findings elucidate the coupled dependence of SBS mitigation on bandwidth and laser intensity in high-Z plasmas, offering useful guidance for optimizing broadband laser designs in inertial confinement fusion.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [32] [Beyond Exascale: Dataflow Domain Translation on a Cerebras Cluster](https://arxiv.org/abs/2511.11542)
*Tomas Oppelstrup,Nicholas Giamblanco,Delyan Z. Kalchev,Ilya Sharapov,Mark Taylor,Dirk Van Essendelft,Sivasankaran Rajamanickam,Michael James*

Main category: cs.DC

TL;DR: The paper introduces a novel algorithm that enables high-performance physical system simulations, achieving 1.6M time steps/sec and 84 PFLOP/s with 90% peak performance utilization.


<details>
  <summary>Details</summary>
Motivation: Domain decomposition methods perform poorly in network computing environments, with Exascale systems delivering only a small fraction of their peak performance for physical system simulations.

Method: The paper introduces the novel \algorithmpropernoun{} algorithm designed to overcome limitations of domain decomposition methods and enable high-performance simulations in clustered environments.

Result: The method achieves simulations running at 1.6 million time steps per second, delivers 84 PFLOP/s performance, and achieves 90% of peak performance in both single-node and clustered environments.

Conclusion: The proposed algorithm successfully overcomes the limitations of traditional domain decomposition methods, enabling high-performance physical system simulations at planetary scale with exceptional computational efficiency.

Abstract: Simulation of physical systems is essential in many scientific and engineering domains. Commonly used domain decomposition methods are unable to deliver high simulation rate or high utilization in network computing environments. In particular, Exascale systems deliver only a small fraction their peak performance for these workloads. This paper introduces the novel \algorithmpropernoun{} algorithm, designed to overcome these limitations. We apply this method and show simulations running in excess of 1.6 million time steps per second and simulations achieving 84 PFLOP/s. Our implementation can achieve 90\% of peak performance in both single-node and clustered environments. We illustrate the method by applying the shallow-water equations to model a tsunami following an asteroid impact at 460m-resolution on a planetary scale running on a cluster of 64 Cerebras CS-3 systems.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [33] [The equivalence of isocapacitary notions of mass](https://arxiv.org/abs/2511.11155)
*Luca Benatti*

Main category: math.DG

TL;DR: The paper proves the equivalence between different isocapacitary notions of mass, including Huisken's isoperimetric mass and Jauregui's isocapacitary mass.


<details>
  <summary>Details</summary>
Motivation: To establish the mathematical equivalence between various isocapacitary definitions of mass in geometric analysis.

Method: Mathematical proof demonstrating the equivalence of isocapacitary mass concepts through analytical methods.

Result: Successfully proved that Huisken's isoperimetric mass and Jauregui's isocapacitary mass are equivalent notions.

Conclusion: The different isocapacitary definitions of mass are mathematically equivalent, unifying these concepts in geometric analysis.

Abstract: In this short note, we will prove the equivalence of the isocapacitary notions of mass. This family also includes G. Huisken's isoperimetric mass and J. L. Jauregui's isocapacitary mass.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [34] [Correlated Purification for Restoring $N$-Representability in Quantum Simulation](https://arxiv.org/abs/2511.10789)
*Yuchen Wang,Irma Avdic,Michael Rose,Lillian I. Payne Torres,Anna O. Schouten,Kevin J. Sung,David A. Mazziotti*

Main category: quant-ph

TL;DR: A correlated purification method using semidefinite programming to restore N-representability to noisy 2-electron reduced density matrices from shadow tomography, achieving chemical accuracy in hydrogen chain simulations.


<details>
  <summary>Details</summary>
Motivation: Shadow tomography produces noisy RDMs that violate N-representability conditions due to statistical and hardware noise, requiring purification to restore physical accuracy.

Method: Bi-objective semidefinite programming that minimizes both many-electron energy and nuclear norm of 2-RDM changes, promoting low-rank physically meaningful corrections.

Result: Substantial reductions in energy and 2-RDM error, achieving chemical accuracy across dissociation curves for large hydrogen chains.

Conclusion: Provides robust strategy for quantum tomography in many-body simulations, particularly effective for ground states but applicable to excited states with parameter adjustments.

Abstract: Classical shadow tomography offers a scalable route to estimating properties of quantum states, but the resulting reduced density matrices (RDMs) often violate constraints that ensure they represent $N$-electron states -- known as $N$-representability conditions -- because of statistical and hardware noise. We present a correlated purification framework based on semidefinite programming to restore accuracy to these noisy, unphysical two-electron RDMs. The method performs a bi-objective optimization that minimizes both the many-electron energy and the nuclear norm of the change in the measured 2-RDM. The nuclear norm, often employed in matrix completion, promotes low-rank, physically meaningful corrections to the 2-RDM, while the energy term acts as a regularization term that can improve the purity of the ground state. While the method is particularly effective for the ground state, it can also be applied to excited and non-stationary states by decreasing the weight of the energy relative to the error norm. In an application to fermionic shadow tomography of large hydrogen chains, correlated purification yields substantial reductions in both energy and 2-RDM error, achieving chemical accuracy across dissociation curves. This framework provides a robust strategy for tomography in many-body quantum simulations.

</details>


<div id='physics.atm-clus'></div>

# physics.atm-clus [[Back]](#toc)

### [35] [SCULPT: An Interactive Machine Learning Platform for Analyzing Multi-Particle Coincidence Data from Cold Target Recoil Ion Momentum Spectroscopy](https://arxiv.org/abs/2511.11153)
*Hazem Daoud,Sarvesh Kumar,Jin Qian,Daniel Slaughter,Tanny Chavez,Thorsten Weber*

Main category: physics.atm-clus

TL;DR: SCULPT is a web-based software platform that integrates machine learning with physics-informed analysis for COLTRIMS data, featuring UMAP dimensionality reduction, adaptive confidence scoring, and interactive visualization to analyze complex multi-dimensional particle coincidence data.


<details>
  <summary>Details</summary>
Motivation: To address challenges in modern momentum spectroscopy by providing an accessible tool that combines advanced ML techniques with physics analysis for complex high-dimensional data from COLTRIMS experiments.

Method: Integrates UMAP for non-linear dimensionality reduction, adaptive confidence scoring system, configurable molecular profiles, interactive visualization tools, and comprehensive data filtering. Uses modular web-based architecture with potential extensions to deep autoencoders and genetic programming.

Result: Successfully analyzed photo double ionization data for D2O molecule 3-body dissociation, revealing distinct fragmentation channels and their correlations with physics parameters. Significantly reduces analysis time and enables real-time rare event detection during experiments.

Conclusion: SCULPT provides an accessible, efficient platform for the atomic and molecular physics community to perform complex multi-dimensional analyses, enabling real-time data exploration and improved experimental efficiency.

Abstract: We present SCULPT (Supervised Clustering and Uncovering Latent Patterns with Training), a comprehensive software platform for analyzing tabulated high-dimensional multi-particle coincidence data from Cold Target Recoil Ion Momentum Spectroscopy (COLTRIMS) experiments. The software addresses critical challenges in modern momentum spectroscopy by integrating advanced machine learning techniques with physics-informed analysis in an interactive web-based environment. SCULPT implements Uniform Manifold Approximation and Projection (UMAP) for non-linear dimensionality reduction to reveal correlations in highly dimensional data. We also discuss potential extensions to deep autoencoders for feature learning, and genetic programming for automated discovery of physically meaningful observables. A novel adaptive confidence scoring system provides quantitative reliability assessments by evaluating user-selected clustering quality metrics with predefined weights that reflect each metric's robustness. The platform features configurable molecular profiles for different experimental systems, interactive visualization with selection tools, and comprehensive data filtering capabilities. Utilizing a subset of SCULPT's capabilities, we analyze photo double ionization data measured using the COLTRIMS method for 3-body dissociation of the D2O molecule, revealing distinct fragmentation channels and their correlations with physics parameters. The software's modular architecture and web-based implementation make it accessible to the broader atomic and molecular physics community, significantly reducing the time required for complex multi-dimensional analyses. This opens the door to finding and isolating rare events exhibiting non-linear correlations on the fly during experimental measurements, which can help steer exploration and improve the efficiency of experiments.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [36] [Bayesian inference for the fractional Calderón problem with a single measurement](https://arxiv.org/abs/2511.11068)
*Pu-Zhao Kow,Janne Nurminen,Jesse Railo*

Main category: math.ST

TL;DR: The paper analyzes posterior consistency in the fractional Calderón problem with Gaussian noise, showing concentration around the true parameter and establishing convergence rates for the posterior mean reconstruction error.


<details>
  <summary>Details</summary>
Motivation: To investigate the consistency of posterior distributions in inverse problems involving fractional elliptic operators, which lack the strong regularity properties of classical elliptic problems.

Method: Uses Bayesian framework with rescaled and Gaussian sieve priors, analyzing noisy discrete measurements from an exterior domain. Develops refined stability estimates for both forward and inverse problems.

Result: Shows posterior distribution concentrates around true parameter as measurements increase, and establishes tight convergence rates for posterior mean reconstruction error.

Conclusion: Successfully demonstrates posterior consistency in the fractional Calderón problem despite the technical challenges posed by weaker regularity theory in fractional elliptic problems.

Abstract: This paper investigates the consistency of a posterior distribution in the single-measurement fractional Calderón problem with additive Gaussian noise. We consider a Bayesian framework with rescaled and Gaussian sieve priors, using a collection of noisy, discrete observations taken from a suitable exterior domain. Our main result shows that the posterior distribution concentrates around the true parameter as the number of measurements increases. Furthermore, we establish tight convergence rates for the reconstruction error of the posterior mean. A central technical challenge is to obtain refined stability estimates for both the forward and inverse problems. In particular, the required forward estimates are delicate to obtain because the fractional elliptic problems do not enjoy as strong regularity theory as their classical counterparts.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [37] [Counterintuitive Potential-Barrier Affinity Effect](https://arxiv.org/abs/2511.11160)
*Qiang Xu,Zhao Liu,Yanming Ma*

Main category: cond-mat.mtrl-sci

TL;DR: The paper reveals a counterintuitive quantum effect called potential-barrier affinity (PBA) that drives significant interatomic electron accumulation when electron energy exceeds barrier maximum, fundamentally explaining electron distribution patterns in materials.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental origin of electron accumulation in interatomic regions, which dictates chemical bonding and material properties but remains elusive across disciplines.

Method: Solving the Schrödinger equation for a crystalline potential to reveal the potential-barrier affinity (PBA) effect.

Result: PBA effect drives significant interatomic electron accumulation when electron energy exceeds the barrier maximum, essentially determining interatomic electron density patterns and governing microstructures and properties of condensed matters.

Conclusion: This work overturns traditional wisdom about electron localization, provides the fundamental mechanism underlying conventional chemical bond formation, and establishes a theoretical foundation for microscopic design of material properties.

Abstract: Electron accumulation in interatomic regions is a fundamental quantum phenomenon dictating chemical bonding and material properties, yet its origin remains elusive across disciplines. Here, we report a counterintuitive quantum effect -- potential-barrier affinity (PBA) -- revealed by solving the Schrödinger equation for a crystalline potential. PBA effect drives significant interatomic electron accumulation when electron energy exceeds the barrier maximum. This effect essentially determines interatomic electron density patterns, governing microstructures and properties of condensed matters. Our theory overturns the traditional wisdom that the interstitial electron localization in electride requires potential-well constraints or hybrid orbitals, and it serves the fundamental mechanism underlying the formation of conventional chemical bonds. This work delivers a paradigm shift in understanding electron distribution and establishes a theoretical foundation for the microscopic design of material properties.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [38] [In situ Evidence of 5-minute Oscillations from Parker Solar Probe](https://arxiv.org/abs/2511.10906)
*Zesen Huang,Marco Velli,Olga Panasenco,Richard J. Morton,Chen Shi,Yeimy J. Rivera,Benjamin Chandran,Samuel T. Badman,Yuliang Ding,Nour Raouafi,Stuart D. Bale,Michael Stevens,Tamar Ervin,Chuanpeng Hou,Kristopher G. Klein,Orlando Romeo,Jia Huang,Mingzhe Liu,Davin E. Larson,Marc Pulupa,Roberto Livi,Federico Fraschetti*

Main category: astro-ph.SR

TL;DR: First in situ detection of 5-minute solar oscillations (p-modes) in the upper corona using Parker Solar Probe data, showing statistically significant 3.1-3.2 mHz peaks in magnetic field power spectrum.


<details>
  <summary>Details</summary>
Motivation: To provide direct evidence that solar surface oscillations can propagate into the solar wind, which has been hypothesized but never directly observed in situ before.

Method: Analyzed magnetic field data from Parker Solar Probe's three closest perihelia, specifically looking for 5-minute oscillation signatures in the power spectrum at 9.9 solar radii.

Result: Detected statistically significant (~6σ) peaks at 3.1-3.2 mHz frequency in magnetic field power spectrum, appearing as large-amplitude, spherically polarized Alfvénic wave trains lasting ~35 minutes.

Conclusion: Global solar oscillations can reach and potentially influence the solar wind, providing the first direct in situ evidence of p-mode propagation into the upper corona.

Abstract: The Sun's surface vibrates in characteristic 5-minute oscillations, known as p-modes, generated by sound waves trapped within the convection zone. Although these oscillations have long been hypothesized to reach into the solar wind, direct in situ evidence has remained elusive, even during previous close encounters by Parker Solar Probe (PSP). Here, we present the first promising in situ detection of 5-minute oscillations in the upper solar corona, based on observations from PSP's three closest perihelia. In two events at 9.9 solar radii, we identify statistically significant ($\sim$ 6 $σ$) 3.1-3.2 mHz peaks in the magnetic field power spectrum, each appearing as a large-amplitude, spherically polarized Alfvénic wave train lasting approximately 35 minutes. These results demonstrate that global solar oscillations can reach and potentially influence the solar wind.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [39] [Improving conditional generative adversarial networks for inverse design of plasmonic structures](https://arxiv.org/abs/2511.11279)
*Petter Persson,Nils Henriksson,Nicolò Maccaferri*

Main category: physics.optics

TL;DR: This paper presents an improved conditional generative adversarial network (cGAN) approach for inverse design of plasmonic nanostructures, achieving significantly better performance than baseline models through label projection and embedding network modifications.


<details>
  <summary>Details</summary>
Motivation: Inverse design of plasmonic nanostructures is challenging due to non-unique solutions, high-dimensional design spaces, and computational intractability of simulation-based methods. Deep learning offers a promising alternative to handle these high-dimensional problems effectively.

Method: The authors train a conditional generative adversarial network (cGAN) model with two key modifications: label projection and a novel embedding network. They test this approach on two architectures (fully connected and convolutional networks) and use a pre-trained CNN as a surrogate model to evaluate design predictions.

Result: The modified cGAN model reduces mean absolute error by an order of magnitude in best cases and converges more than three times faster on average. Both network architectures show improved performance, with equally good or better predictions compared to baseline models.

Conclusion: The proposed modifications to cGAN provide an important step towards more efficient and precise inverse design methods for optical elements, demonstrating significant improvements in error reduction and training convergence speed.

Abstract: Deep learning has emerged as a key tool for designing nanophotonic structures that manipulate light at sub-wavelength scales. We investigate how to inversely design plasmonic nanostructures using conditional generative adversarial networks. Although a conventional approach of measuring the optical properties of a given nanostructure is conceptually straightforward, inverse design remains difficult because the existence and uniqueness of an acceptable design cannot be guaranteed. Furthermore, the dimensionality of the design space is often large, and simulation-based methods become quickly intractable. Deep learning methods are well-suited to tackle this problem because they can handle effectively high-dimensional input data. We train a conditional generative adversarial network model and use it for inverse design of plasmonic nanostructures based on their extinction cross section spectra. Our main result shows that adding label projection and a novel embedding network to the conditional generative adversarial network model, improves performance in terms of error estimates and convergence speed for the training algorithm. The mean absolute error is reduced by an order of magnitude in the best case, and the training algorithm converges more than three times faster on average. This is shown for two network architectures, a simpler one using a fully connected neural network architecture, and a more complex one using convolutional layers. We pre-train a convolutional neural network and use it as surrogate model to evaluate the performance of our inverse design model. The surrogate model evaluates the extinction cross sections of the design predictions, and we show that our modifications lead to equally good or better predictions of the original design compared to a baseline model. This provides an important step towards more efficient and precise inverse design methods for optical elements.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [40] [MMA-Sim: Bit-Accurate Reference Model of Tensor Cores and Matrix Cores](https://arxiv.org/abs/2511.10909)
*Peichen Xie,Yang Wang,Fan Yang,Mao Yang*

Main category: cs.AR

TL;DR: MMA-Sim is the first bit-accurate reference model that reveals undocumented arithmetic behaviors of matrix multiplication accelerators (MMAs) in modern GPUs, identifying numerical imprecision issues that affect DNN training stability.


<details>
  <summary>Details</summary>
Motivation: Modern GPUs integrate MMAs like NVIDIA Tensor Cores and AMD Matrix Cores, but their undocumented floating-point arithmetic specifications cause numerical imprecision and inconsistency, compromising DNN training stability and reproducibility.

Method: Developed MMA-Sim through targeted and randomized tests to derive nine arithmetic algorithms that simulate floating-point matrix multiplication of MMAs from ten GPU architectures (8 NVIDIA, 2 AMD), achieving bitwise equivalence with real hardware.

Result: Successfully created a bit-accurate reference model that identifies undocumented arithmetic behaviors in MMAs, revealing behaviors that could lead to significant errors in DNN training.

Conclusion: MMA-Sim provides crucial insights into MMA arithmetic behaviors, enabling better understanding and mitigation of numerical instability issues in DNN training and inference on modern GPU hardware.

Abstract: The rapidly growing computation demands of deep neural networks (DNNs) have driven hardware vendors to integrate matrix multiplication accelerators (MMAs), such as NVIDIA Tensor Cores and AMD Matrix Cores, into modern GPUs. However, due to distinct and undocumented arithmetic specifications for floating-point matrix multiplication, some MMAs can lead to numerical imprecision and inconsistency that can compromise the stability and reproducibility of DNN training and inference.
  This paper presents MMA-Sim, the first bit-accurate reference model that reveals the detailed arithmetic behaviors of the MMAs from ten GPU architectures (eight from NVIDIA and two from AMD). By dissecting the MMAs using a combination of targeted and randomized tests, our methodology derives nine arithmetic algorithms to simulate the floating-point matrix multiplication of the MMAs. Large-scale validation confirms bitwise equivalence between MMA-Sim and the real hardware. Using MMA-Sim, we investigate arithmetic behaviors that affect DNN training stability, and identify undocumented behaviors that could lead to significant errors.

</details>


<div id='cond-mat.supr-con'></div>

# cond-mat.supr-con [[Back]](#toc)

### [41] [Interpretable descriptors enable prediction of hydrogen-based superconductors at moderate pressures](https://arxiv.org/abs/2511.11284)
*Jiawei Chen,Junhao Peng,Yanwei Liang,Renhai Wang,Huafeng Dong,Wei Zhang*

Main category: cond-mat.supr-con

TL;DR: An interpretable symbolic regression framework predicts Tc in hydrogen-based superconductors using integrated density of states near Fermi level, enabling rapid screening and discovery of high-Tc materials at moderate pressures.


<details>
  <summary>Details</summary>
Motivation: Room temperature superconductivity in hydrogen-based compounds typically requires extreme pressures that limit practical applications, creating need for accelerated discovery under moderate pressures.

Method: Developed symbolic regression framework using integrated density of states (IDOS) within 1 eV of Fermi level as key descriptor, relying solely on electronic structure calculations for rapid screening.

Result: Model achieves high accuracy (RMSEtrain = 20.15 K) and identifies four candidates: Na2GaCuH6 with Tc=42.04 K at ambient pressure, and NaCaH12, NaSrH12, KSrH12 with Tc up to 162.35 K, 86.32 K, 55.13 K at various pressures.

Conclusion: The interpretable model clarifies how hydrogen-projected electronic weight near Fermi level governs Tc, offering mechanism-aware route to stabilize high-Tc phases at reduced pressures beyond rapid screening.

Abstract: Room temperature superconductivity remains elusive, and hydrogen-base compounds despite remarkable transition temperatures(Tc) typically require extreme pressures that hinder application. To accelerate discovery under moderate pressures, an interpretable framework based on symbolic regression is developed to predict Tc in hydrogen-based superconductors. A key descriptor is an integrated density of states (IDOS) within 1 eV of the Fermi level (EF), which exhibits greater robustness than conventional single-point DOS features. The resulting analytic model links electronic-structure characteristics to superconducting performance, achieves high accuracy (RMSEtrain = 20.15 K), and generalizes well to external datasets. By relying solely on electronic structure calculations, the approach greatly accelerates materials screening. Guided by this model, four hydrogen-based candidates are identified and validated via calculation: Na2GaCuH6 with Tc =42.04 K at ambient pressure (exceeding MgB2), and NaCaH12, NaSrH12, and KSrH12 with Tc up to 162.35 K, 86.32 K, and 55.13 K at 100 GPa, 25 GPa, and 25 GPa, respectively. Beyond rapid screening, the interpretable form clarifies how hydrogen-projected electronic weight near EF and related features govern Tc in hydrides, offering a mechanism-aware route to stabilize high-Tc phases at reduced pressures.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Differentiation Strategies for Acoustic Inverse Problems: Admittance Estimation and Shape Optimization](https://arxiv.org/abs/2511.11415)
*Nikolas Borrel-Jensen,Josiah Bjorgaard*

Main category: cs.LG

TL;DR: Differentiable programming approach for acoustic inverse problems using JAX-FEM's automatic differentiation for admittance estimation and combining JAX-FEM with PyTorch3D for shape optimization, achieving high precision and efficiency.


<details>
  <summary>Details</summary>
Motivation: To enable rapid prototyping of optimization workflows for physics-based inverse problems using modern differentiable software stacks, eliminating the need for manual derivation of adjoint equations.

Method: Uses JAX-FEM's automatic differentiation for boundary admittance estimation and combines JAX-FEM for forward simulation with PyTorch3D for mesh manipulation through AD, separating physics-driven boundary optimization from geometry-driven interior mesh adaptation.

Result: Achieved 3-digit precision in admittance estimation and 48.1% energy reduction at target frequencies with 30-fold fewer FEM solutions compared to standard finite difference methods.

Conclusion: Modern differentiable software stacks enable efficient and precise optimization workflows for acoustic inverse problems, with AD for parameter estimation and a hybrid approach of finite differences and AD for geometric design.

Abstract: We demonstrate a practical differentiable programming approach for acoustic inverse problems through two applications: admittance estimation and shape optimization for resonance damping. First, we show that JAX-FEM's automatic differentiation (AD) enables direct gradient-based estimation of complex boundary admittance from sparse pressure measurements, achieving 3-digit precision without requiring manual derivation of adjoint equations. Second, we apply randomized finite differences to acoustic shape optimization, combining JAX-FEM for forward simulation with PyTorch3D for mesh manipulation through AD. By separating physics-driven boundary optimization from geometry-driven interior mesh adaptation, we achieve 48.1% energy reduction at target frequencies with 30-fold fewer FEM solutions compared to standard finite difference on the full mesh. This work showcases how modern differentiable software stacks enable rapid prototyping of optimization workflows for physics-based inverse problems, with automatic differentiation for parameter estimation and a combination of finite differences and AD for geometric design.

</details>


### [43] [Fast Neural Tangent Kernel Alignment, Norm and Effective Rank via Trace Estimation](https://arxiv.org/abs/2511.10796)
*James Hazelden*

Main category: cs.LG

TL;DR: Matrix-free approach using trace estimation for fast computation of Neural Tangent Kernel properties like trace, Frobenius norm, and alignment, enabling orders of magnitude speedup.


<details>
  <summary>Details</summary>
Motivation: Computing the full NTK matrix is infeasible for large models, especially recurrent architectures, creating need for efficient approximation methods.

Method: Uses trace estimation (Hutch++ estimator) and introduces one-sided estimators that require only forward- or reverse-mode automatic differentiation, not both.

Result: Achieves speedups of many orders of magnitude, with one-sided estimators outperforming Hutch++ in low-sample regimes when parameter count exceeds model state.

Conclusion: Matrix-free randomized approaches enable fast NTK analysis and applications, making previously infeasible computations practical.

Abstract: The Neural Tangent Kernel (NTK) characterizes how a model's state evolves over Gradient Descent. Computing the full NTK matrix is often infeasible, especially for recurrent architectures. Here, we introduce a matrix-free perspective, using trace estimation to rapidly analyze the empirical, finite-width NTK. This enables fast computation of the NTK's trace, Frobenius norm, effective rank, and alignment. We provide numerical recipes based on the Hutch++ trace estimator with provably fast convergence guarantees. In addition, we show that, due to the structure of the NTK, one can compute the trace using only forward- or reverse-mode automatic differentiation, not requiring both modes. We show these so-called one-sided estimators can outperform Hutch++ in the low-sample regime, especially when the gap between the model state and parameter count is large. In total, our results demonstrate that matrix-free randomized approaches can yield speedups of many orders of magnitude, leading to faster analysis and applications of the NTK.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [44] [Lenard-Balescu thermalization: rigorous derivation from a toy model](https://arxiv.org/abs/2511.10778)
*Mitia Duerinckx,Corentin Le Bihan*

Main category: math-ph

TL;DR: Rigorous derivation of Lenard-Balescu thermalization for a tagged particle in mean-field interacting systems, showing convergence to linear Fokker-Planck equation on timescales t∼N.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous mathematical foundation for the slow thermalization predicted by Lenard-Balescu theory in many-body systems with long-range interactions.

Method: Developed a simplified hierarchical model from BBGKY hierarchy, used rigorous Dyson expansion with Feynman diagrams, and introduced novel renormalization scheme to remove leading recollisions. Combined phase mixing and hypoelliptic regularity to control phase-space filamentation.

Result: Proved that tagged-particle density converges to solution of linear Fokker-Planck equation (linearized Landau equation) on timescales t∼N in sufficiently large spatial dimensions.

Conclusion: The analysis provides new insight into Lenard-Balescu thermalization, showing that renormalization transforms free propagators into hypoelliptic ones, compensating for filamentation effects in phase space.

Abstract: We study the long-time dynamics of a tagged particle coupled to a background of $N$ other particles, all interacting through long-range pairwise forces in the mean-field scaling, with the background initially at thermal equilibrium. Starting from the $N$-particle BBGKY hierarchy, we introduce a simplified (truncated) hierarchical model and show, in sufficiently large spatial dimension, that the tagged-particle density converges, on timescales $t\sim N$, to the solution of a linear Fokker-Planck equation, viewed as the linearization of the Landau equation. This provides, in a simplified setting, a rigorous derivation of the slow thermalization predicted by Lenard-Balescu theory. Our approach relies on a rigorous Dyson expansion in terms of Feynman diagrams and on a novel renormalization scheme that removes leading recollisions. The main technical challenge is to control the effect of phase-space filamentation within the diagrams, which we achieve by combining phase mixing and hypoelliptic regularity. Although restricted to a simplified model, our analysis offers new insight into Lenard-Balescu thermalization: notably, the renormalization appears to transform free propagators into hypoelliptic ones, providing a key mechanism that compensates for filamentation.

</details>


### [45] [$Γ$-convergence of a diffeomorphism-natural MDL functional to Einstein-Hilbert with Gibbons-Hawking-York boundary term](https://arxiv.org/abs/2511.10867)
*Marko Lela*

Main category: math-ph

TL;DR: Γ-convergence of discrete MDL-type functional to Einstein-Hilbert action with boundary term, establishing interior and boundary blow-ups with identified densities.


<details>
  <summary>Details</summary>
Motivation: To prove convergence of discrete geometric functionals to continuum Einstein-Hilbert action with boundary corrections, providing foundational mathematical framework.

Method: Boundary-fitted shape-regular meshes, interior and boundary blow-ups, Carathéodory density identification, recovery sequence via reflected Fermi smoothing, boundary first-layer asymptotics.

Result: Identified densities f_in = α₀ + α₁R and f_bdry = β₁K, boundary cells contribute at order h^(d-1) with O(h) global boundary remainder, interior remainder O(h²).

Conclusion: Foundation established with reproducible protocol for rate checks and calibration of parameters α₀, α₁, β₁ in Appendix E.

Abstract: We prove a \(Γ\)-convergence result for a diffeomorphism-natural discrete MDL-type functional to the Einstein-Hilbert action with the Gibbons-Hawking-York boundary term. On boundary-fitted, shape-regular meshes we establish interior and boundary blow-ups, identify the Carathéodory densities \(f_{\mathrm{in}}=α_0+α_1 R\) and \(f_{\mathrm{bdry}}=β_1 K\), and obtain the \(\liminf/\limsup\) bounds via a recovery sequence based on reflected Fermi smoothing. A boundary first-layer asymptotics shows that boundary cells contribute at order \(h^{d-1}\), yielding a global \(O(h)\) boundary remainder, while the interior remainder is \(O(h^2)\). The paper is foundational; Appendix~E specifies a reproducible protocol for rate checks and calibration of \(α_0,α_1,β_1\).

</details>


### [46] [The Birman--Solomyak theorem revisited: a novel elementary proof, generalisation, and applications](https://arxiv.org/abs/2511.11058)
*V. Bach,A. F. M. ter Elst,J. Rehberg*

Main category: math-ph

TL;DR: Short proof of Birman-Solomyak theorem for Hilbert-Schmidt operators with application to Schrödinger-Poisson system


<details>
  <summary>Details</summary>
Motivation: To provide a more concise proof of the Birman-Solomyak theorem specifically for Hilbert-Schmidt operators

Method: Developed a new short proof approach for the theorem

Result: Successfully established a shorter proof and demonstrated its application to Schrödinger-Poisson systems

Conclusion: The new proof is valid and has practical applications in quantum mechanical systems

Abstract: We provide a new short proof for the Birman--Solomyak theorem for Hilbert--Schmidt operators and give an application to a Schrödinger--Poisson system.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [47] [Van-der-Waals exchange-correlation functionals and their high pressure and warm dense matter applications](https://arxiv.org/abs/2511.11061)
*Jan Vorberger,Gabriel J. Smith,William Z. Van Benschoten,Hayley R. Petras,Zhandos Moldabekov,Tobias Dornheim,James J. Shepherd*

Main category: physics.chem-ph

TL;DR: The paper evaluates exchange-correlation functionals for warm dense hydrogen, finding r2SCAN performs best for molecular properties, while vdW functionals and HSE06 show no significant improvement over PBE.


<details>
  <summary>Details</summary>
Motivation: To identify suitable exchange-correlation functionals for accurately modeling the molecular to metal transition in warm dense hydrogen by examining basic hydrogen properties.

Method: Investigated molecular bond length, dissociation energy, van-der-Waals interactions, static/dynamic ion structure factors, and electronic DOS using various exchange-correlation functionals including r2SCAN, vdW functionals, HSE06, and PBE.

Result: r2SCAN functional best reproduces bond length and dissociation energy; vdW functionals and HSE06 perform no better than PBE; differences observed in structure factors and DOS between functionals with and without vdW corrections.

Conclusion: r2SCAN is the most suitable functional for modeling the molecular to metal transition in warm dense hydrogen, while vdW corrections provide no significant advantage over standard functionals like PBE.

Abstract: We investigate basic hydrogen quantities like the molecular bond length, the molecular dissociation energy and the van-der-Waals interaction in idealized situations in an effort to discern a suitable exchange-correlation functional for the molecular to metal transition in warm dense hydrogen. The best reproduction of bond length and dissociation energy is given by the r2SCAN functional, several vdW functionals and also HSE06 fair qualitatively and quantitatively no better than PBE or worse. In addition we investigate quantities like the static and dynamic ion structure factor, and the electronic DOS to determine differences between exchange-correlation functionals with and without van-der-Waals corrections in the transition region from the molecular to the metallic regime of hydrogen.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [48] [Comparative study and critical assessment of phase-field lattice Boltzmann models for laminar and turbulent two-phase flow simulations](https://arxiv.org/abs/2511.11360)
*Xuming Li,Cheng Peng,Chunhua Zhang,Xinnan Wu,Wenrui Wang*

Main category: physics.flu-dyn

TL;DR: Systematic comparison of phase-field lattice Boltzmann models for multiphase flows shows all models perform well in 2D but suffer significant droplet volume loss in 3D turbulent flows, with conservative Allen-Cahn models offering the best balance.


<details>
  <summary>Details</summary>
Motivation: Phase field LB models have multiple variants but lack direct performance comparisons, especially for 3D cases, limiting informed model selection for multiphase flow simulations.

Method: Comparative analysis of four major phase-field LB model categories (conservative Allen-Cahn, nonlocal Allen-Cahn, hybrid Allen-Cahn, and Cahn-Hilliard) tested through canonical two-phase flow problems including 3D droplet-laden turbulent flows.

Result: All models perform satisfactorily in 2D laminar cases but exhibit substantial droplet volume loss in 3D turbulence, particularly at high Weber numbers.

Conclusion: Conservative Allen-Cahn-based LB models demonstrate the most favorable balance of numerical stability, accuracy and computational efficiency among the tested approaches.

Abstract: Phase field lattice Boltzmann (LB) models have undergone continuous development, resulting in multiple variants widely used for simulating multiphase flows. However, direct performance comparisons remain limited, especially for three-dimensional cases. In this study, we present a systematic comparative analysis of several recent and representative phase-field LB models, covering four major categories: conservative Allen-Cahn, nonlocal Allen-Cahn, hybrid Allen-Cahn, and Cahn-Hilliard models. Their accuracy, numerical stability and mass/volume conservation are assessed through a series of canonical two-phase flow problems. Beyond the commonly tested two-dimensional laminar cases, we extend the evaluation to three-dimensional droplet-laden turbulent flows, which expose more critical limitations of the existing models. The results show that while all models perform satisfactorily in two dimensions, they still suffer from substantial droplet volume loss in turbulence, particularly at high Weber numbers. Overall, conservative Allen-Cahn-based LB models exhibit the most favorable balance of numerical stability, accuracy and computational efficiency.

</details>


### [49] [Assessment of a Nonlinear Unsteady Vortex Lattice-Vortex Particle Method for Predicting Helicopter Rotor/Multirotor Aerodynamics](https://arxiv.org/abs/2511.11430)
*Jinbin Fu,Eric Laurendeau*

Main category: physics.flu-dyn

TL;DR: A nonlinear unsteady vortex lattice-vortex particle method (NL-UVLM-VPM) is developed for efficient helicopter rotor aerodynamics prediction, achieving comparable accuracy to URANS with over 100x computational cost reduction.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient computational method for helicopter rotor aerodynamics that can accurately predict complex flight conditions while significantly reducing computational costs compared to traditional methods like URANS.

Method: Couples nonlinear unsteady vortex lattice method for aerodynamics with mesh-free vortex particle method for wake evolution. Incorporates nonlinear viscous effects from 2D RANS data using viscous-inviscid alpha-coupling. Uses Vreman eddy-viscosity subgrid-scale model with PSE method in LES framework for wake stabilization. Includes adaptive particle conversion and multiple-reference-frame kinematics.

Result: Validated against experimental and URANS data for three benchmark cases, showing good agreement in aerodynamic loads and wake structures. Achieves comparable accuracy to URANS with over two orders of magnitude reduction in computational cost.

Conclusion: The NL-UVLM-VPM method demonstrates potential as a reliable and efficient tool for rotorcraft aerodynamic studies, enabling accurate simulation of complex flight conditions with dramatically reduced computational requirements.

Abstract: This study presents a nonlinear unsteady vortex lattice-vortex particle method (NL-UVLM-VPM) for the efficient prediction of helicopter rotor aerodynamics. The approach couples a nonlinear unsteady vortex lattice method for aerodynamic modeling with a mesh-free vortex particle method for wake evolution. Nonlinear viscous effects, derived from two-dimensional Reynolds-Averaged Navier-Stokes (RANS) sectional data, are incorporated through a low-cost viscous-inviscid alpha-coupling algorithm. To stabilize wake development, the Vreman eddy-viscosity subgrid-scale model is integrated with the particle strength exchange (PSE) method within a large eddy simulation (LES) framework, enabling robust and accurate resolution of the wake coherent, transient, and turbulent breakdown regions. The solver further includes a novel adaptive particle conversion technique, reducing computational wall-clock time by nearly 70%, and a multiple-reference-frame kinematics formulation, allowing simulations of complex flight conditions such as forward flight and multi-rotor interactions. Validation against experimental and unsteady RANS (URANS) data for three representative benchmark cases shows good agreement in aerodynamic loads and wake structures. Moreover, the NL-UVLM-VPM achieves comparable accuracy to URANS, with over two orders of magnitude reduction in computational cost, demonstrating its potential as a reliable and efficient tool for rotorcraft aerodynamic studies.

</details>
