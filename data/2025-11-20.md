<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 11]
- [math.AP](#math.AP) [Total: 29]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 5]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [math.FA](#math.FA) [Total: 2]
- [math.PR](#math.PR) [Total: 2]
- [gr-qc](#gr-qc) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [stat.ML](#stat.ML) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Adaptive Ch Method with Local Coupled Multiquadrics for Solving Partial Differential Equations](https://arxiv.org/abs/2511.14929)
*Ahmed E. Seleit*

Main category: math.NA

TL;DR: New adaptive collocation scheme using Local Coupled Multiquadrics with automatic cover size and nodal spacing refinement for PDEs


<details>
  <summary>Details</summary>
Motivation: To develop a truly meshless method for solving PDEs that automatically adapts to achieve prescribed accuracy without requiring elements, connectivity, or continuity

Method: Adaptive Ch Method using Local Coupled Multiquadrics in covers-and-nodes framework, prioritizing cover size adjustment then nodal spacing refinement

Result: Accurate solutions for 1D and 2D Poisson problems across wide range of shape parameters, preserving local collocation advantages

Conclusion: The proposed method is truly meshless and effective for adaptive PDE solution with automatic refinement strategy

Abstract: We present a new adaptive collocation scheme for solving partial differential equations based on Local Coupled Multiquadrics (LCMQs) within a covers-and-nodes framework. The method, referred to as the Adaptive Ch Method, automatically prioritizes adjusting the local cover size C then refines local nodal spacing h to achieve a prescribed tolerance. Numerical examples for one- and two-dimensional Poisson problems demonstrate accurate solutions across a wide range of shape parameter values, while preserving the advantages of local collocation. The proposed approximation approach is truly meshless, requiring no element, connectivity or continuity to construct trial functions or weights.

</details>


### [2] [Lippmann-Schwinger-Lanczos algorithm for inverse scattering problems with unknown reflectivity and loss distributions: One-dimensional Case](https://arxiv.org/abs/2511.15058)
*Jorn Zimmerling,Mikhail Zaslavsky,Alexander V. Mamonov,Vladimir Druskin,Anarzhan Abilgazy*

Main category: math.NA

TL;DR: Extension of Lippmann-Schwinger-Lanczos method to inverse scattering in attenuating media with unknown reflectivity and loss distributions, using data-driven reduced-order models for more accurate internal field approximations.


<details>
  <summary>Details</summary>
Motivation: To address the nonlinear inverse scattering problem in attenuating media where both reflectivity and loss distributions are unknown, overcoming limitations of traditional methods like Born approximation.

Method: Extend Lippmann-Schwinger-Lanczos method to dissipative problems, using two complementary constructions: one based on spectral data and another on frequency-domain measurements over finite intervals, linking data-driven reduced-order models with port-Hamiltonian systems.

Result: More accurate internal reconstructions and faster, more robust recovery of contrast compared to Born approximation, as demonstrated by numerical experiments.

Conclusion: The method successfully handles nonlinear inverse scattering in dissipative media by leveraging data-driven reduced-order modeling, providing superior performance over traditional linear approximations.

Abstract: We consider one-dimensional inverse scattering in attenuating media where both the reflectivity and loss distributions are unknown. Mathematically, this corresponds to recovering the coefficients of a damped wave operator, or equivalently, a quadratic operator pencil in the frequency domain.
  The Lippmann-Schwinger equation maps the unknown reflectivity and loss distribution to the measured scattered data. This mapping is nonlinear, as it requires knowledge of the internal wavefield, which itself depends on the reflectivity and loss distribution. The Lippmann-Schwinger-Lanczos method addresses this nonlinearity by approximating the internal solutions through the lifting of states from a reduced-order model constructed directly from the measured data.
  In this work, we extend the method to dissipative problems, enabling the approximation of internal partial differential equation (PDE) solutions in media with both reflectivity and loss distributions. We present two complementary constructions of such internal solutions: one based on spectral data and another on frequency-domain measurements over a finite interval. This development establishes a direct link between data-driven reduced-order models for inverse problems and port-Hamiltonian dynamical systems, with reduced models obtained either from the associated spectral measure or via rational approximation. Compared to the Born approximation, which replaces the internal field with the background field, our approach yields more accurate internal reconstructions and enables faster and more robust recovery of the contrast as evidenced by our numerical experiments.

</details>


### [3] [Error Analysis on a Novel Class of Exponential Integrators with Local Linear Extension Techniques for Highly Oscillatory ODEs](https://arxiv.org/abs/2511.15104)
*Zhihao Qi,Weibing Deng,Fuhai Zhu*

Main category: math.NA

TL;DR: New explicit exponential integrators for highly oscillatory ODEs with rigorous error analysis showing uniform convergence order O(h^{k+1}) for small steps and O(εh^k) for larger steps.


<details>
  <summary>Details</summary>
Motivation: To address computational challenges in solving non-autonomous highly oscillatory ODEs with small parameter ε, where rapid oscillations constrain step size selection and numerical accuracy.

Method: Developed a new class of explicit exponential integrators using auxiliary polynomial variables, without requiring diagonal linear parts or fixed eigenvalue multiples. Used algebraic techniques to establish equivalence between high-dimensional system and original problem.

Result: Proved uniform convergence order O(h^{k+1}) for step sizes smaller than ε, and O(εh^k) for larger steps under bounded oscillatory energy condition. Applied to second-order oscillatory equations with improved uniform accuracy.

Conclusion: The rigorous error analysis establishes optimal convergence rates for the new exponential integrators, confirmed by numerical experiments, providing efficient methods for highly oscillatory systems.

Abstract: This paper studies a class of non-autonomous highly oscillatory ordinary differential equations (ODEs) featuring a linear component inversely proportional to a small parameter $\varepsilon$ with purely imaginary eigenvalues, alongside an $\varepsilon$-independent nonlinear component. When $0<\varepsilon\ll 1$, the rapidly oscillatory solution constrains the step size selection and numerical accuracy, resulting in significant computational challenges. Motivated by linearization through introducing auxiliary polynomial variables, a new class of explicit exponential integrators (EIs) has recently been developed. The methods do not require the linear part to be diagonal or with all eigenvalues to be integer multiples of a fixed value - a general assumption in multiscale methods - and attain arbitrarily high convergence order without any order conditions. The main contribution of this work is to establish a rigorous error analysis for the new class of methods. To do this, we first demonstrate the equivalence between the high-dimensional system and the original problem by employing algebraic techniques. Building upon these fundamental results, we prove that the numerical schemes have a uniform convergence order of $O(h^{k+1})$ for the solution when using at most $k$-degree auxiliary polynomial variables with time step sizes smaller than $\varepsilon$. For larger step sizes under the bounded oscillatory energy condition, the methods achieve a convergence order of $O(\varepsilon h^k)$ for the solution. These theoretical results are further applied to second-order oscillatory equations, yielding improved uniform accuracy with respect to $\varepsilon$. Finally, numerical experiments confirm the optimality of the derived error estimates.

</details>


### [4] [An efficient fully explicit scheme for stochastic Navier-Stokes equations driven by multiplicative noise](https://arxiv.org/abs/2511.15230)
*Can Huang,Weiwen Wang,Chuanju Xu*

Main category: math.NA

TL;DR: Proposes an efficient, linear, fully decoupled pressure-correction scheme for 2D stochastic Navier-Stokes equations with multiplicative noise using auxiliary variable approach.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical scheme for stochastic Navier-Stokes equations that is fully explicit yet unconditionally stable, addressing computational challenges in solving such complex systems.

Method: Uses auxiliary variable approach to create a linear, fully decoupled pressure-correction scheme that requires solving only Poisson-type equations with constant coefficients at each time step.

Result: Achieves the first application of auxiliary variable method to stochastic Navier-Stokes equations, with detailed strong convergence analysis provided for the linearized equation.

Conclusion: The proposed scheme is efficient, linear, fully decoupled, and unconditionally stable, representing a significant advancement in numerical methods for stochastic fluid dynamics.

Abstract: This work proposes an efficient, linear, and fully decoupled pressure-correction scheme for the 2D stochastic Navier-Stokes equations with multiplicative noise and Dirichlet boundary condition. Leveraging the auxiliary variable approach, the scheme is fully explicit yet unconditionally stable. At each time step, it only requires solving Poisson-type equations with constant coefficients. To the best of our knowledge, this is the first application of the auxiliary variable method to stochastic Navier-Stokes equations. We provide a detailed strong convergence analysis for the linearized equation under standard assumptions.

</details>


### [5] [Numerical analysis of the high-frequency Helmholtz equation using semiclassical analysis](https://arxiv.org/abs/2511.15287)
*Jeffrey Galkowski,Euan A. Spence*

Main category: math.NA

TL;DR: Introduction to semiclassical analysis for non-experts and its application to numerical analysis of high-frequency scattering problems using finite-element, boundary-element, and domain-decomposition methods.


<details>
  <summary>Details</summary>
Motivation: To provide new perspectives on high-frequency scattering problems using semiclassical analysis tools that work in phase space and connect PDE solutions to geometric-optic rays, settling long-standing open problems.

Method: Using semiclassical analysis techniques that operate in phase space (position and frequency) to rigorously describe how high-frequency PDE solutions relate to geometric-optic ray properties.

Result: The paper showcases numerical-analysis results obtained using semiclassical techniques for finite-element methods, boundary-element methods, and domain-decomposition methods in solving Helmholtz equation scattering problems.

Conclusion: Semiclassical analysis provides powerful tools for understanding and solving high-frequency scattering problems, offering new insights that complement traditional approaches and have resolved several longstanding challenges in the field.

Abstract: We consider the numerical solution of high-frequency scattering problems modeled by the Helmholtz equation with a bounded obstacle. Although the analysis of this problem dates back at least 50 years, over the past decade or so, tools and techniques from $\textit{semiclassical analysis}$ have provided a new perspective and been used to settle several long-standing open problems in this area. Semiclassical analysis works in phase space (i.e., position and frequency) and describes rigorously the extent to which solutions of high-frequency PDEs are dictated by the properties of the corresponding geometric-optic rays.
  The goals of the article are to (i) give a introduction to semiclassical analysis aimed at non-experts and (ii) showcase some of the numerical-analysis results about finite-element methods, boundary-element methods, and domain-decomposition methods obtained using semiclassical techniques.

</details>


### [6] [A Hybrid-High Order method for fracture modelling](https://arxiv.org/abs/2511.15345)
*Alessandra Crippa,Julien Coatléven,Daniele A. Di Pietro,Nicolas Guy,Yousef Soleiman*

Main category: math.NA

TL;DR: A new Hybrid High-Order method for fracture propagation simulation using phase-field models, supporting general polygonal/polyhedral meshes with fully discontinuous spaces for displacement and damage variables.


<details>
  <summary>Details</summary>
Motivation: To develop a flexible numerical method for fracture propagation that can handle general mesh types and accommodate large variations in displacement and damage variables.

Method: Hybrid High-Order method with fully discontinuous spaces, general polygonal/polyhedral meshes, and staggered time stepping scheme with static condensation for algebraic problem resolution.

Result: Extensive numerical validation on classical 2D fracture propagation problems, including comparison with standard finite element schemes.

Conclusion: The proposed method provides an effective and flexible approach for fracture propagation simulation with advantages in mesh design and adaptation capabilities.

Abstract: In this work, we introduce a new Hybrid High-Order method for the numerical simulation of fracture propagation based on phase-field models. The proposed method supports general meshes made of polygonal/polyhedral elements, which provides great flexibility in mesh design and adaptation, and can accommodate large variations of both the displacement and damage variables thanks to the use of fully discontinuous spaces. The resolution of the corresponding algebraic problem is based on a staggered time stepping scheme which takes advantage of static condensation for each subproblem. We provide extensive numerical validation of the method on classical two-dimensional fracture propagation problems, including a comparison with a more standard finite element scheme.

</details>


### [7] [On the conditioning of histopolation](https://arxiv.org/abs/2511.15395)
*Ludovico Bruni Bruno,Stefano Serra-Capizzano*

Main category: math.NA

TL;DR: Analysis of histopolation matrices shows exponential conditioning with monomial basis but bounded conditioning with Chebyshev basis of second kind for appropriate supports.


<details>
  <summary>Details</summary>
Motivation: To study the unisolvence and conditioning properties of histopolation matrices as their size increases to infinity, comparing different basis choices.

Method: Asymptotic linear algebra analysis of histopolation matrices using monomial basis and Chebyshev basis of second kind, examining unisolvence and conditioning behavior.

Result: Unisolvence is sparse, but conditioning shows uniform behavior: exponential growth with monomial basis, bounded conditioning with Chebyshev basis of second kind for appropriate supports. Linear behavior observed in Frobenius norm.

Conclusion: The choice of basis significantly impacts conditioning in histopolation, with Chebyshev basis of second kind providing superior numerical stability compared to monomial basis.

Abstract: Histopolation is the approximation procedure that associates a degree $ d-1 $ polynomial $ p_{d-1} \in \mathscr{P}_{d-1} (I) $ with a locally integrable function $ f $ imposing that the integral (or, equivalently, the average) of $p$ coincides with that of $f$ on a collection of $ d $ distinct segments $s_i$. In this work we discuss unisolvence and conditioning of the associated matrices, in an asymptotic linear algebra perspective, i.e., when the matrix-size $d$ tends to infinity. While the unisolvence is a rather sparse topic, the conditioning in the unisolvent setting has a uniform behavior: as for the case of standard Vandermonde matrix-sequences with real nodes, the conditioning is inherently exponential as a function of $d$ when the monomial basis is chosen. In contrast, for an appropriate selection of supports, the Chebyshev basis of second kind has a bounded conditioning. A linear behavior is also observed in the Frobenius norm.

</details>


### [8] [Neural network-driven domain decomposition for efficient solutions to the Helmholtz equation](https://arxiv.org/abs/2511.15445)
*Victorita Dolean,Daria Hrebenshchykova,Stéphane Lanteri,Victor Michel-Dansac*

Main category: math.NA

TL;DR: FBPINNs and multilevel extensions are investigated as alternatives to traditional numerical methods for solving high-frequency Helmholtz equations in complex 2D domains, using domain decomposition with overlapping sub-domains and local neural networks.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods like finite difference and finite element approaches face computational challenges when solving high-frequency wave problems in complex 2D domains, which is crucial for applications in acoustics, electromagnetism, and seismic analysis.

Method: Uses Finite Basis Physics-Informed Neural Networks (FBPINNs) and their multilevel extensions with domain decomposition, partitioning the computational domain into overlapping sub-domains each governed by a local neural network.

Result: The methods demonstrate accuracy and computational efficiency in solving the Helmholtz equation for homogeneous cases.

Conclusion: FBPINNs and their multilevel extensions show potential to mitigate limitations of traditional approaches for high-frequency wave propagation problems.

Abstract: Accurately simulating wave propagation is crucial in fields such as acoustics, electromagnetism, and seismic analysis. Traditional numerical methods, like finite difference and finite element approaches, are widely used to solve governing partial differential equations (PDEs) such as the Helmholtz equation. However, these methods face significant computational challenges when applied to high-frequency wave problems in complex two-dimensional domains. This work investigates Finite Basis Physics-Informed Neural Networks (FBPINNs) and their multilevel extensions as a promising alternative. These methods leverage domain decomposition, partitioning the computational domain into overlapping sub-domains, each governed by a local neural network. We assess their accuracy and computational efficiency in solving the Helmholtz equation for the homogeneous case, demonstrating their potential to mitigate the limitations of traditional approaches.

</details>


### [9] [Fractional Quadrature rule and using its Exactness for the Müntz-Legendre Scaling Functions for Solving Fractional Differential Equations](https://arxiv.org/abs/2511.15478)
*Ritu Kumari,Mani Mehra,Abhishek Kumar Singh*

Main category: math.NA

TL;DR: A fractional quadrature rule is developed for exact integration of functions spanned by fractional power functions, with applications to solving fractional differential equations using Müntz-Legendre scaling functions.


<details>
  <summary>Details</summary>
Motivation: Traditional quadrature rules fail to provide exact evaluations for fractional power functions, introducing approximation errors in fractional operators.

Method: Formulated a fractional quadrature rule that achieves exact integration for functions within fractional power function sets, using roots of orthogonal Müntz polynomials as nodes. Applied Müntz-Legendre scaling functions to approximate Caputo derivatives and derived an operational matrix for Riemann-Liouville integration.

Result: Proved properties of the fractional quadrature rule, derived absolute error bounds, and demonstrated superior accuracy compared to Block-pulse method through L2-error estimates in fractional differential equation solutions.

Conclusion: The proposed fractional quadrature rule provides exact integration for fractional power functions and offers improved accuracy for solving fractional differential equations compared to existing methods.

Abstract: Fractional operators (derivatives/integrals) are defined via the integration of the functions. When the function is produced by a spanning set of fractional power functions, traditional quadrature rules often need to be revised, failing to provide exact evaluations for fractional power functions and thus introducing approximation errors. In this paper, we have formulated a fractional quadrature rule that achieves exact integration for functions within this specific set to address this issue. Some properties of the fractional quadrature rule have been proved, and the absolute error bound in the proposed fractional quadrature rule has been derived. The behavior of roots of the orthogonal Müntz polynomial has also been observed for its application as nodes in the fractional quadrature rule. To illustrate the effectiveness of the newly proposed fractional quadrature rule, we focus on fractional differential equations that incorporate the left Caputo fractional derivative. In this context, Müntz-Legendre scaling functions are utilized to approximate the Caputo derivative of functions involved in these equations. Additionally, we have derived an operational matrix for Riemann-Liouville integration to approximate the respective functions with the help of the fractional quadrature rule. To demonstrate the practical utility of our method, we provide illustrative examples that compare the $L_2$-error estimates in the solutions of fractional differential equations using our approach against those obtained with the Block-pulse method. These comparisons underscore the superior accuracy of our proposed method.

</details>


### [10] [Convergence and Sketching-Based Efficient Computation of Neural Tangent Kernel Weights in Physics-Based Loss](https://arxiv.org/abs/2511.15530)
*Max Hirsch,Federico Pichi*

Main category: math.NA

TL;DR: This paper analyzes adaptive NTK-based weighting in PINNs, proving convergence and developing efficient randomized algorithms for computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the unclear convergence and high computational burden of NTK-based adaptive weighting methods in physics-informed neural networks.

Method: Proves convergence under appropriate conditions, then develops a randomized algorithm using predictor-corrector approach and matrix sketching for efficient NTK estimation.

Result: Theoretical proof of convergence for adaptive NTK-based weighting, and development of computationally efficient randomized algorithm with numerical validation.

Conclusion: The proposed methods provide both theoretical guarantees and practical efficiency improvements for NTK-based adaptive weighting in PINNs.

Abstract: In multi-objective optimization, multiple loss terms are weighted and added together to form a single objective. These weights are chosen to properly balance the competing losses according to some meta-goal. For example, in physics-informed neural networks (PINNs), these weights are often adaptively chosen to improve the network's generalization error. A popular choice of adaptive weights is based on the neural tangent kernel (NTK) of the PINN, which describes the evolution of the network in predictor space during training. The convergence of such an adaptive weighting algorithm is not clear a priori. Moreover, these NTK-based weights would be updated frequently during training, further increasing the computational burden of the learning process. In this paper, we prove that under appropriate conditions, gradient descent enhanced with adaptive NTK-based weights is convergent in a suitable sense. We then address the problem of computational efficiency by developing a randomized algorithm inspired by a predictor-corrector approach and matrix sketching, which produces unbiased estimates of the NTK up to an arbitrarily small discretization error. Finally, we provide numerical experiments to support our theoretical findings and to show the efficacy of our randomized algorithm. Code Availability: https://github.com/maxhirsch/Efficient-NTK

</details>


### [11] [Numerical Stability of the Nyström Method](https://arxiv.org/abs/2511.15583)
*Alberto Bucci,Yuji Nakatsukasa,Taejun Park*

Main category: math.NA

TL;DR: The paper establishes conditions for numerical stability in the Nyström method, addressing its long-standing stability issues through proper column subset selection and pseudoinverse implementation.


<details>
  <summary>Details</summary>
Motivation: The Nyström method is widely used for scaling kernel-based algorithms but has unresolved numerical stability problems, particularly due to ill-conditioned submatrices that cause poor approximation quality.

Method: The authors analyze conditions for stability through appropriate column subset selection and careful implementation of the pseudoinverse operation in the Nyström method.

Result: The research provides theoretical conditions under which the Nyström method achieves numerical stability, with experimental validation showing practical guidance for stable large-scale kernel computations.

Conclusion: The work resolves the numerical stability problem of the Nyström method by establishing theoretical conditions and practical implementation guidelines for stable kernel computations.

Abstract: The Nyström method is a widely used technique for improving the scalability of kernel-based algorithms, including kernel ridge regression, spectral clustering, and Gaussian processes. Despite its popularity, the numerical stability of the method has remained largely an unresolved problem. In particular, the pseudo-inversion of the submatrix involved in the Nyström method may pose stability issues as the submatrix is likely to be ill-conditioned, resulting in numerically poor approximation. In this work, we establish conditions under which the Nyström method is numerically stable. We show that stability can be achieved through an appropriate choice of column subsets and a careful implementation of the pseudoinverse. Our results and experiments provide theoretical justification and practical guidance for the stable application of the Nyström method in large-scale kernel computations.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [12] [Yang-Mills instanton on a four dimensional wormhole: asymptotic stability in the energy space](https://arxiv.org/abs/2511.14880)
*Michał Kowalczyk,Javier Monreal*

Main category: math.AP

TL;DR: Analysis of SU(2) Yang-Mills field stability in 4+1D wormhole spacetime, focusing on odd perturbations of instanton solutions.


<details>
  <summary>Details</summary>
Motivation: To study the stability properties of instanton solutions in higher-dimensional wormhole geometries, particularly for SU(2) Yang-Mills fields.

Method: Used spherically symmetric magnetic ansatz to reduce problem to 1D nonlinear wave equation, analyzed small odd perturbations of degree one instanton solution.

Result: Showed that the instanton is conditionally asymptotically stable in the odd energy space under small odd perturbations.

Conclusion: The SU(2) Yang-Mills instanton in 4+1D wormhole spacetime exhibits conditional asymptotic stability for odd perturbations, providing insights into field behavior in higher-dimensional geometries.

Abstract: In this paper we consider an $SU(2)$ Yang-Mills field propagating in the $4+1$ dimensional wormhole spacetime. Assuming the spherically symmetric magnetic ansatz the problem reduces to a one dimensional non linear wave equation. This equation posses a degree one solution (instanton) which is odd in space. We consider small, odd perturbations of the instanton and show that it is conditionally asymptotically stable in the odd energy space.

</details>


### [13] [Evolutionary equations with state-dependent delay](https://arxiv.org/abs/2511.14883)
*Bernhard Aigner,Marcus Waurick*

Main category: math.AP

TL;DR: Extension of contraction mapping argument from ODEs to evolutionary PDEs for local well-posedness of generalized initial value problems with distributional formulation.


<details>
  <summary>Details</summary>
Motivation: To generalize contraction mapping methods from ordinary state-dependent delay differential equations to evolutionary partial differential equations in R. Picard's framework, enabling analysis of a broader class of PDEs.

Method: Uses contraction mapping argument applied to equations of form (∂ₜM(∂ₜ) + A)u(t) = F(t,u₍ₜ₎), where A is m-accretive linear operator and M is material law. Requires H¹ prehistories with bounded derivative, regularity increasing right-hand side, and consistency condition.

Result: Establishes local well-posedness (in weak solution sense) of generalized initial value problems from distributional formulation.

Conclusion: The approach is viable and applicable to classical examples (heat, wave, Maxwell equations), semigroup theory, port-Hamiltonian systems, and equations with fractional derivatives and time convolutions with bounded operators.

Abstract: We extend a contraction mapping argument for ordinary state-dependent delay differential equations to evolutionary partial differential equations in the sense of R. Picard, that is, to equations of the form $\bigl(\partial_{t} M(\partial_{t}) + A\bigr) u(t) = F\bigl(t,u_{(t)}\bigr)$, where $A$ is an $\mathrm{m}$-accretive (unbounded) linear operator and $M$ is a material law. We establish local well-posedness (in the sense of weak solutions) of generalized initial value problems that stem from a distributional formulation. We require prehistories in $H^{1}$ with bounded derivative, a regularity increasing right-hand side and a consistency condition. We showcase the viability of our results by applying them to classical examples (heat, wave and Maxwell's equations), examples from semigroup theory, port-Hamiltonian systems, as well as equations featuring fractional derivatives and convolutions (in time) with bounded operators.

</details>


### [14] [A strong quantitative form of the fractional isoperimetric inequality](https://arxiv.org/abs/2511.14885)
*Eleonora Cinti,Enzo Maria Merlino,Berardo Ruffini*

Main category: math.AP

TL;DR: The paper presents a strong fractional quantitative isoperimetric inequality that controls both Fraenkel asymmetry and boundary oscillation, generalizing previous local results, and derives stability estimates for fractional Cheeger inequalities.


<details>
  <summary>Details</summary>
Motivation: To extend the quantitative isoperimetric inequality to fractional cases with stronger control over boundary behavior, building upon previous local results by Fusco and Julin.

Method: The proof uses a regularization process similar to previous work but with a different approach in its conceptual foundation.

Result: A strong fractional quantitative isoperimetric inequality is established, and as a consequence, stability estimates for fractional Cheeger inequalities are proven.

Conclusion: The work successfully generalizes quantitative isoperimetric inequalities to fractional settings with enhanced boundary control and provides important stability results for fractional Cheeger problems.

Abstract: We show a strong version of the fractional quantitative isoperimetric inequality, in which the isoperimetric deficit controls not only the Fraenkel asymmetry but also a sort of oscillation of the boundary. This generalizes the local result by Fusco and Julin in \cite{FJ}. The proof follows a regularization process as in \cite{FJ} but it is quite different in its spirit. Then, as a consequence of the quantitative inequality, we prove some stability estimates for a fractional Cheeger inequality.

</details>


### [15] [A Liouville theorem for convex functions with periodic Monge-Ampère measure](https://arxiv.org/abs/2511.15021)
*Tianling Jin,YanYan Li,Hung V. Tran,Xushan Tu*

Main category: math.AP

TL;DR: Convex solutions to the Monge-Ampère equation with periodic measures decompose uniquely as quadratic polynomials plus periodic functions, extending previous results to general periodic measures.


<details>
  <summary>Details</summary>
Motivation: To extend previous decomposition results for Monge-Ampère equations from smooth density cases to general periodic measures, answering a question raised by Li and Lu.

Method: Developed a new dichotomous Harnack-type inequality for linearized Monge-Ampère equations with nonnegative periodic measures as a key technical tool.

Result: Proved that any convex solution to det D²u = μ (with μ periodic) admits unique decomposition as quadratic polynomial plus periodic function, up to constants.

Conclusion: The decomposition theorem holds in full generality for periodic measures, resolving the open question and providing complete extension of previous works.

Abstract: Let $μ\not\equiv 0$ be a nonnegative locally finite periodic Borel measure on $\mathbb{R}^n$. We show that any convex solution to the Monge-Ampère equation \[ \det D^2 u = μ\quad \text{in } \mathbb{R}^n \] admits a unique decomposition (up to addition of constants) as the sum of a quadratic polynomial and a periodic function. This result extends, in full generality, the earlier works for the case $μ=f(x)\,\mathrm{d} x$: when $\log f \in C^α$, it was established by Caffarelli and Li; and when $\log f$ is merely bounded, it was proved by Li and Lu. Our result thus answers a question raised by Li and Lu. A key ingredient in the proof is a new dichotomous Harnack-type inequality for linearized Monge-Ampère equations with nonnegative periodic measures.

</details>


### [16] [An inverse problem in optimal transport on closed Riemannian manifolds](https://arxiv.org/abs/2511.15037)
*Jian Zhai,Kelvin Shuangjian Zhang*

Main category: math.AP

TL;DR: The paper shows that Riemannian metrics on closed manifolds can be uniquely recovered from optimal transport maps using squared Riemann distance as cost function, up to a multiplicative constant.


<details>
  <summary>Details</summary>
Motivation: To establish whether Riemannian geometry can be reconstructed from optimal transport data, specifically using optimal transport maps with squared Riemann distance as the cost function.

Method: Analyzing the relationship between optimal transport maps and Riemannian metrics, proving uniqueness results for metric recovery from transport data.

Result: The Riemannian metric can be uniquely determined from optimal transport maps up to a multiplicative constant scaling factor.

Conclusion: Riemannian geometry is encoded in optimal transport maps with squared distance cost, allowing metric recovery up to scale from transport data.

Abstract: We consider the problem of recovering the Riemannian metric on a compact closed manifold from the optimal transport maps when the underlying cost function is the squared Riemann distance. We show that the metric can be uniquely determined up to a multiplicative constant.

</details>


### [17] [The existence and instability of blowing-up steady states for the Shigesada-Kawasaki-Teramoto competition model with cross-diffusion](https://arxiv.org/abs/2511.15039)
*Kousuke Kuto,Yaping Wu*

Main category: math.AP

TL;DR: Existence and instability analysis of blowing-up positive steady states in SKT competition models with large cross-diffusion, using transformation methods and Lyapunov-Schmidt reduction.


<details>
  <summary>Details</summary>
Motivation: To understand the structure and stability of coexistence steady states in SKT models with large cross-diffusion, building on previous work that showed blow-up behavior near Neumann eigenvalues.

Method: Transformations and Lyapunov-Schmidt reduction method to derive existence and asymptotic structure of positive steady state branches near blow-up points in shadow systems.

Result: Derived existence and detailed asymptotic structure of multiple branches of positive steady states near blow-up points, showing all large-amplitude branches are spectrally unstable.

Conclusion: Established existence and instability of corresponding perturbed positive steady states for original SKT model with sufficiently large cross-diffusion coefficients.

Abstract: We investigate the existence and instability of a class of blowing-up positive steady states arising in a shadow system of the Shigesada-Kawasaki-Teramoto (SKT) two-species competition model, as well as in the corresponding perturbed SKT model with a sufficiently large cross-diffusion coefficient and bounded random diffusion parameters. In their classical work (Lou-Ni, 1999), it was shown that, under the limit where one cross-diffusion parameter tends to infinity, coexistence steady states of the SKT model are characterized by three types of shadow systems. In a previous study (Kuto, 2015), the first author analyzed one of these shadow systems in one space dimension (the second shadow system) and proved that a component of a bifurcating branch blows up as the bifurcation parameter approaches the first positive Neumann eigenvalue of $-Δ$. In the present paper, using a new approach based on suitable transformations and the Lyapunov-Schmidt reduction method, we derive the existence and detailed asymptotic structure of several branches of positive steady states near blow-up points for shadow systems with one or two cross-diffusion terms, in both one- and multi-dimensional domains. We further show that all such large-amplitude steady state branches are spectrally unstable. Moreover, by means of perturbation arguments, we establish the existence and instability of corresponding branches of perturbed positive steady states for the original SKT model when one cross-diffusion coefficient is sufficiently large.

</details>


### [18] [Global Gevrey solution of 3D anisotropic Navier-Stokes system in a strip domain](https://arxiv.org/abs/2511.15050)
*Wei-Xi Li,Zhan Xu,Ping Zhang*

Main category: math.AP

TL;DR: Analysis of 3D anisotropic Navier-Stokes system with horizontal dissipation in strip domain, showing Gevrey-class regularization and enhanced regularity in diffusion direction.


<details>
  <summary>Details</summary>
Motivation: To overcome difficulties from boundary terms and absence of vertical dissipation in anisotropic Navier-Stokes systems, requiring specialized regularity conditions.

Method: Impose Gevrey-class regularity in vertical direction; analyze space-time analytic/Gevrey-class regularization in remaining directions; examine enhanced Gevrey regularity in strong diffusion direction.

Result: Solution exhibits space-time analytic or Gevrey-class regularization in horizontal directions; possesses enhanced Gevrey regularity in direction of strong diffusion unconstrained by boundaries.

Conclusion: Gevrey-class conditions successfully handle boundary difficulties and vertical dissipation absence, yielding enhanced regularity properties in anisotropic Navier-Stokes systems.

Abstract: We investigate the three-dimensional (3D) incompressible anisotropic Navier-Stokes system with dissipation only in the horizontal variables, posed in a strip domain. To overcome the difficulties arising from the boundary terms and the absence of vertical dissipation, we impose a Gevrey-class regularity condition in the vertical direction. For the remaining directions, we prove that the solution exhibits space-time analytic or Gevrey-class regularization. Furthermore, the solution is shown to possess an enhanced Gevrey regularity in the direction of strong diffusion, which is unconstrained by boundaries.

</details>


### [19] [Normalized solutions to subcritical Choquard systems with double couplings](https://arxiv.org/abs/2511.15103)
*Wenliang Pei,Chonghao Deng*

Main category: math.AP

TL;DR: This paper studies the Choquard system with linear and nonlinear couplings, proving existence of normalized ground states across mass subcritical, critical, and supercritical cases using variational methods.


<details>
  <summary>Details</summary>
Motivation: To understand the existence and classification of normalized ground states for coupled Choquard systems with both linear and nonlinear interactions, particularly as parameters vary across different critical ranges.

Method: Variational methods are employed to analyze the system and demonstrate the existence of normalized ground states.

Result: The authors prove the existence of a normalized ground state for the Choquard system in mass subcritical, critical, and supercritical cases, with classification based on parameter ranges.

Conclusion: Normalized ground states exist for the coupled Choquard system across all three parameter regimes (subcritical, critical, and supercritical), providing a comprehensive classification result.

Abstract: We consider the Choquard system with both linear and nonlinear couplings
  $-Δu + μ_1 u =λ_1 ( I_α* |u|^{r_1} ) |u|^{r_1-2} u + βp( I_α* |v|^q)|u|^{p-2} u + κv,$
  $-Δv + μ_2 v =λ_2 ( I_α* |v|^{r_2} ) |v|^{r_2-2} v + βq( I_α* |u|^p)|v|^{q-2} v + κu , $
  $\int_{\mathbb{R}^N} u^2 = ρ_1^2\, , \int_{\mathbb{R}^N} v^2 = ρ_2^2,$ where $N \in \{3,4\}$, $λ_1, λ_2, β, κ, ρ_1,ρ_2 > 0$, $2_{α,*} :=\frac{N+α}{N} <p,q , r_1, r_2 <2_α^*:=\frac{N+α}{N-2}$ and $p+q\leq 2r_1 \leq 2r_2$ . We investigate a classification result as the parameters $p+q$, $2r_1$ and $2r_2$ vary across the ranges $(\frac{2N+2α}{N},\frac{2N+2α+4}{N})$,
  $\{\frac{2N+2α+4}{N}\}$, and $(\frac{2N+2α+4}{N},\frac{2N+2α}{N-2})$. Employing variational methods, we demonstrate the existence of a normalized ground state for the system in the mass subcritical, critical, and supercritical cases.

</details>


### [20] [Pseudo-magnetic Fields and Effective Dynamics in Strained Honeycomb Structures](https://arxiv.org/abs/2511.15152)
*Chengyu Zhang,Borui Miao,Yi Zhu*

Main category: math.AP

TL;DR: Analysis of wave packet dynamics near Dirac points in strained honeycomb media, developing spectral methods to control strain-induced errors and deriving a Dirac equation with gauge fields that accurately approximates solutions over finite times.


<details>
  <summary>Details</summary>
Motivation: To mathematically understand and control wave propagation in strained honeycomb structures, which generate pseudo-magnetic fields enabling precise manipulation of optical and acoustic waves.

Method: Developed a novel spectral analysis approach to control error from second-order differential residue terms caused by strain, analyzing wave packet dynamics localized near Dirac points.

Result: Derived a two-dimensional Dirac equation with nontrivial gauge fields that governs envelope dynamics and is proven to well approximate the true solution over finite time periods.

Conclusion: The results advance mathematical understanding of pseudo-magnetic effects in strained honeycomb structures and provide foundation for handling systems with general higher-order perturbation terms.

Abstract: Strain offers a straightforward and effective method for generating pseudo-magnetic fields in optical and acoustic materials, thereby enabling precise manipulation of wave propagation. In this article, we investigate and justify wave packet dynamics localized near Dirac points in strained honeycomb-structured media. We develop a novel approach based on spectral analysis to control the error from second-order differential residue terms caused by the strain. The analysis yields a two-dimensional Dirac equation with nontrivial gauge fields governing the envelope dynamics, which is proved to well approximate the true solution in a long but finite time. These results contribute to the mathematical understanding of pseudo-magnetic effects in strained honeycomb structures and pave the way to systems with general higher-order perturbation terms.

</details>


### [21] [Sharp $L^4$ Strichartz estimate for Hyperbolic Schrödinger equation on $\mathbb{R}\times \mathbb{T}$](https://arxiv.org/abs/2511.15157)
*Yangkendi Deng,Chenjie Fan,Zehua Zhao*

Main category: math.AP

TL;DR: Sharp L^4 Strichartz estimate without derivative loss for hyperbolic Schrödinger equation on R×T, with global well-posedness for cubic hyperbolic Schrödinger equation in L^2-critical space.


<details>
  <summary>Details</summary>
Motivation: To establish the hyperbolic analogue of the classical Takaoka-Tzvetkov result for Strichartz estimates, extending the analysis to hyperbolic Schrödinger equations.

Method: Combination of robust kernel decomposition method with precise measure estimates for semi-algebraic sets.

Result: Proved sharp L^4 Strichartz estimate without derivative loss for hyperbolic Schrödinger equation on R×T.

Conclusion: Established global well-posedness for cubic hyperbolic Schrödinger equation on R×T in L^2-critical space with small initial data.

Abstract: We prove the sharp $L^4$ Strichartz estimate without derivative loss for the hyperbolic Schrödinger equation on $\mathbb{R}\times\mathbb{T}$, \begin{equation} \|e^{it (\partial_{x_{1}}^2-\partial_{x_{2}}^2)} φ\|_{L^4_{t,x_{1},x_{2}}([0,1]\times \mathbb{R} \times \mathbb{T})}\lesssim \|φ\|_{L_{x_{1},x_{2}}^2(\mathbb{R} \times \mathbb{T})}, \end{equation} which serves as the hyperbolic analogue of the classical result of Takaoka-Tzvetkov \cite{takaoka20012d}. The proof is based on the combination of a robust kernel decomposition method with precise measure estimates for semi-algebraic sets. As an immediate application, we establish the global well-posedness for the cubic hyperbolic Schrödinger equation on $\mathbb{R}\times\mathbb{T}$ in the $L^2$-critical space with sufficiently small initial data.

</details>


### [22] [A Wave Front Tracking Scheme for Flux Reconstruction in $2\times 2$ Hyperbolic Conservation Laws](https://arxiv.org/abs/2511.15261)
*Chaohua Duan,Yan Jiang,Hongyu Liu,Wenjian Peng*

Main category: math.AP

TL;DR: A wave front tracking framework for reconstructing unknown flux functions in 2×2 hyperbolic conservation laws using Riemann solutions at fixed times, with explicit formulas for shock and rarefaction waves.


<details>
  <summary>Details</summary>
Motivation: Extend flux reconstruction beyond scalar cases to handle 2×2 hyperbolic conservation laws, addressing the inverse problem of identifying complete equations of state from limited dynamic measurements in continuum mechanics.

Method: Analyze Riemann solutions at fixed observation times, develop explicit reconstruction formulas using unified equivalent shock concept, construct piecewise quadratic C¹ flux approximations.

Result: Method achieves quadratic convergence for function values and linear convergence for derivatives under C¹,¹ regularity, with enhanced cubic and quadratic convergence respectively under C³ regularity. Successfully applied to isentropic Euler equations and p-system.

Conclusion: Provides a systematic approach to reconstructing unknown flux functions in 2×2 hyperbolic systems, enabling identification of complete equations of state from limited measurements in continuum mechanics.

Abstract: This paper introduces a novel wave front tracking framework for reconstructing unknown flux functions in $2\times 2$ hyperbolic conservation laws, extending beyond the well-studied scalar case. By analyzing Riemann solutions at fixed observation times, we develop explicit reconstruction formulas that handle arbitrary combinations of shock and rarefaction waves through a unified equivalent shock concept. Our method constructs piecewise quadratic $C^1$ flux approximations with rigorous convergence guarantees: the approximation errors decrease quadratically with the discretization parameters for function values and linearly for derivatives under $C^{1,1}$ regularity, with enhanced cubic and quadratic convergence respectively under $C^3$ regularity. Applications to the isentropic Euler equations and the mathematically equivalent p-system in compressible fluid dynamics demonstrate the method's capability to identify complete equations of state from limited dynamic measurements, providing a systematic approach to a fundamental inverse problem in continuum mechanics.

</details>


### [23] [Finite time blow-up analysis for the generalized Proudman-Johnson model](https://arxiv.org/abs/2511.15166)
*Jie Guo,Quansen Jiu*

Main category: math.AP

TL;DR: The paper studies the generalized Proudman-Johnson equation on the torus, showing finite-time blow-up for a>1 and global existence for a<1, with self-similar blow-up patterns.


<details>
  <summary>Details</summary>
Motivation: To understand the critical behavior and blow-up phenomena in the generalized Proudman-Johnson equation, particularly near the critical parameter a=1.

Method: Mathematical analysis of the generalized Proudman-Johnson equation, establishing blow-up results through asymptotic self-similar analysis and comparison of regimes.

Result: For a slightly greater than 1: finite-time blow-up with self-similar behavior. For a slightly below 1: global existence. Hölder continuous data also leads to self-similar blow-up. Viscous case with a>1 also exhibits finite-time blow-up.

Conclusion: The parameter a=1 serves as a critical threshold separating regimes of finite-time blow-up (a>1) from global existence (a<1), with self-similar blow-up patterns observed in multiple scenarios.

Abstract: In this paper, we study the generalized Proudman-Johnson equation posed on the torus. In the critical regime where the parameter $a$ is close to and slightly greater than 1, we establish finite time blow-up of smooth solutions to the inviscid case. Moreover, we show that the blow-up is asymptotically self-similar for a class of smooth initial data. In contrast, when the parameter $a$ lies slightly below 1, we prove the global in time existence for the same initial data. In addition, we demonstrate that inviscid Proudman-Johnson equation with Hölder continuous data also develops a self-similar blow-up. Finally, for the viscous case with $a>1$, we prove that smooth initial data can still lead to finite time blow-up.

</details>


### [24] [Singularity formation for the supersonic inward wave of compressible Euler equations with radial symmetry](https://arxiv.org/abs/2511.15180)
*Geng Chen,Faris A. El-Katri,Yanbo Hu,Yannan Shen*

Main category: math.AP

TL;DR: Smooth solutions for compressible radially symmetric Euler equations develop singularities in finite time for polytropic ideal gases with γ≥3 and initial supersonic inward waves.


<details>
  <summary>Details</summary>
Motivation: To understand singularity formation in compressible fluid dynamics, specifically for radially symmetric Euler equations with polytropic gases.

Method: Applied characteristic method and invariant domain idea to analyze smooth solution behavior.

Result: Proved that smooth solutions develop singularities in finite time for γ≥3 polytropic gases with initial supersonic inward waves.

Conclusion: Singularity formation occurs in finite time for compressible radially symmetric Euler equations under specified conditions, demonstrating blow-up behavior in these fluid systems.

Abstract: In this paper, we consider the singularity formation of smooth solutions for the compressible radially symmetric Euler equations. By applying the characteristic method and the invariant domain idea, we show that, for polytropic ideal gases with $γ\geq3$, the smooth solution develops a singularity in finite time for a class of initial supersonic inward waves.

</details>


### [25] [Well-posedness and time-asymptotic of Boltzmann equations for monatomic and polyatomic mixtures](https://arxiv.org/abs/2511.15185)
*Ricardo Alonso,Zongguang Li*

Main category: math.AP

TL;DR: Analysis of Boltzmann equations for monatomic-polyatomic gas mixtures using L²-L∞ perturbation theory around modified Maxwellians, establishing well-posedness and optimal polynomial decay rates in whole space, exponential decay in torus.


<details>
  <summary>Details</summary>
Motivation: To model mixtures of gases with different molecular structures (monatomic and polyatomic) and dissimilar masses, accounting for internal energy effects, and to address gaps in classical multi-species Boltzmann theory.

Method: L²-L∞ perturbation theory around global modified Maxwellians; novel approach for handling internal energy variable and mass asymmetry; spatial Fourier analysis of linearized system; classical L²-L∞ method for well-posedness.

Result: Established well-posedness theory; obtained optimal polynomial time decay rates in whole space; proved exponential time decay in torus; revealed perturbed Euler-type structure for macroscopic quantities near steady state.

Conclusion: The framework successfully handles internal energy and mass asymmetry in gas mixtures, provides decay estimates, and offers potential applications for investigating fluid limit problems in multi-species Boltzmann systems.

Abstract: This paper considers a system of Boltzmann equations modelling the mixture of monatomic and polyatomic gases in an $L^{2}-L^{\infty}$ perturbation theory around global modified Maxwellians accounting for the internal energy of the mixture in the whole space and the torus. We investigate the pointwise decay in velocity and internal energy of the linearized Boltzmann operators in the four types of collisions. A novel approach is developed to deal with the additional internal energy variable $I\in \mathbb{R}_+$ and the loss of symmetry due to dissimilar masses of the mixture components. Subsequently, we carry out a classical $L^2-L^\infty$ method to establish the well-posedness theory of the system. The optimal polynomial time decay rate on the whole space is obtained accordingly based on the spatial Fourier's study of the linearized system. The analysis shows the structure of a perturbed Euler-type model for the solution's macroscopic quantities: density, bulk velocity, and temperature, near the steady state, which gives a potential application to investigate fluid limit problems. In addition, this work proves exponential time decay in the torus and fills the gap of classical multi-species Boltzmann in the whole space.

</details>


### [26] [On approximation theorems for solutions to strongly parabolic systems in anisotropic Sobolev spaces](https://arxiv.org/abs/2511.15216)
*Alexander Shlapunov,Pavel Vilkov*

Main category: math.AP

TL;DR: The paper studies Runge pairs for Sobolev solutions of strongly uniformly parabolic systems in non-cylindrical domains, establishing necessary and sufficient conditions for two domains to form a Runge pair when the parabolic operator has constant coefficients.


<details>
  <summary>Details</summary>
Motivation: To investigate the Runge pair problem for parabolic systems in non-cylindrical domains, which extends classical results from elliptic equations to the parabolic case.

Method: Analyzing strongly uniformly parabolic systems with constant coefficients in domains with sufficiently smooth boundaries that are not parallel to the plane t=0, using geometric conditions on domain sections.

Result: Proved that two domains form a Runge pair if and only if the complements of any section of the larger domain to the section of the smaller domain by constant time planes have no compact components in the larger section.

Conclusion: The geometric condition on domain sections provides a complete characterization of Runge pairs for parabolic systems with constant coefficients in non-cylindrical domains.

Abstract: We investigate the problem on Runge pairs for Sobolev solutions of strongly uniformly parabolic systems in non-cylindrical domains of a special kind. We prove that if the coefficients of a parabolic operator are constant, then two domains with sufficiently smooth boundaries, no parts of which are parallel to the plane $t=0$, form a Runge pair if and only if the complements of any section of the larger domain to the section of the smaller domain by planes $t = const$, have no compact components in the larger section.

</details>


### [27] [Solving Newell-Whitehead-Segel and Allen-Cahn Equations Employing Physics-Informed Neural Networks: A Comparative Analysis with Spline Methods](https://arxiv.org/abs/2511.15231)
*Ali Haider Shah,Naveed R. Butt,Asif Ahmad,Muhammad Omer Bin Saeed*

Main category: math.AP

TL;DR: PINNs outperform spline methods for solving NWS and Allen-Cahn equations with better computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To compare PINNs with state-of-the-art spline methods for solving fundamental PDEs like NWS and Allen-Cahn equations, which are important in various scientific fields.

Method: Used physics-informed neural networks (PINNs) and compared them with spline numerical solutions for solving NWS and Allen-Cahn equations, evaluating computational time and efficiency.

Result: PINN method performed significantly better than spline methods for both NWS and Allen-Cahn equations, showing superior computational efficiency.

Conclusion: PINNs are more effective than traditional spline methods for solving these fundamental PDEs, offering better performance and computational efficiency.

Abstract: This study focuses on the solution of partial differential equations (PDEs) by using physics-informed neural networks (PINNs). The Newell-Whitehead-Segel (NWS) equation and the Allen-Cahn equation belong to fundamental PDEs used mostly in various scientific disciplines. Different methods, including analytical and numerical approaches, have been proposed for solving these equations alongside the recently introduced PINN method. This study provides a detailed and comprehensive comparison between the developed PINN method and the state-of-the-art spline numerical solution for the NWS and Allen-Cahn equation. Furthermore, the computational time of the trained PINN models is evaluated to determine their computational efficiency. The findings show that PINN is significantly better than spline methods in solving both problems.

</details>


### [28] [On the Long Time Existence of a Fractional KdV-BBM Type Equation](https://arxiv.org/abs/2511.15233)
*Goksu Oruc*

Main category: math.AP

TL;DR: The paper extends the existence time of solutions for fractional KdV-BBM equations with small initial data from 1/ε to 1/ε² using modified energy methods and Fourier techniques, with numerical evidence suggesting existence beyond hyperbolic time scales.


<details>
  <summary>Details</summary>
Motivation: To enhance the existence time of solutions for fractional KdV-BBM equations with small initial data beyond the standard time scale of 1/ε, addressing long-time behavior of these dispersive equations.

Method: Combination of modified energy method with Fourier techniques for analytical proof, supplemented by numerical investigations of lifespan.

Result: Successfully extended existence time from 1/ε to 1/ε² for small initial data, with numerical evidence indicating solutions may exist beyond hyperbolic time scales.

Conclusion: The study provides comprehensive analytical and numerical evidence for extended existence of smooth solutions in fractional KdV-BBM equations, demonstrating improved time scales through combined mathematical approaches.

Abstract: We consider a fractional Korteweg de Vries-Benjamin Bona Mahony (KdV-BBM) type equation including both fractional dispersive terms of fractional KdV and fractional BBM equations. We aim to enhance the existence time of solutions with small initial data $|| u_0||_{H^{N+α/2}}= ε$ from $\frac{1}ε$ to $\frac{1}{ε^2}$. The proof relies on the combination of a modified energy method with Fourier techniques. In addition, the long time existence issues are investigated numerically. Numerical observations of the lifespan give an evidence of existence of solutions beyond the hyperbolic time scale. This study provides a detailed analysis from both analytical and numerical aspects for the existence of smooth solutions.

</details>


### [29] [Global Existence for Coupled 3-D Nonlinear Wave and Klein-Gordon Equations with Large Derivatives of Initial Data](https://arxiv.org/abs/2511.15234)
*Guocong Shang*

Main category: math.AP

TL;DR: Global existence for coupled 3-D wave and Klein-Gordon equations with quadratic nonlinearity under certain conditions including large derivative data and null conditions.


<details>
  <summary>Details</summary>
Motivation: To establish global existence results for coupled wave and Klein-Gordon equations in three dimensions with quadratic nonlinearities, which are important in mathematical physics.

Method: Analysis of the Cauchy problem for coupled 3-D wave and Klein-Gordon equations with quadratic nonlinearity, using conditions including large derivative data for wave equations and null conditions.

Result: Proved global existence under the specified conditions.

Conclusion: The paper successfully demonstrates global existence for the coupled system under conditions including large derivative data and null conditions, contributing to the understanding of long-time behavior for such equations.

Abstract: We consider the Cauchy problem of coupled 3-D wave and Klein-Gordon equations with a quadratic form of nonlinearity. We show global existence under several conditions, including large derivative data for wave equations and the null conditions.

</details>


### [30] [Normalized Solutions for the $(2,q)$-Laplacian Operator Between Mass-Critical Exponents](https://arxiv.org/abs/2511.15285)
*Laura Baldelli,Norihisa Ikoma*

Main category: math.AP

TL;DR: Existence of normalized solutions for (2,q)-Laplacian equations in intermediate regime between mass critical exponents, with both negative and positive energy solutions, plus zero-mass case analysis.


<details>
  <summary>Details</summary>
Motivation: To understand intermediate cases arising from non-homogeneous (2,q)-Laplacian operator and provide comprehensive analysis of solution behavior in different energy regimes.

Method: Variational methods, compactness arguments, and delicate energy estimates adapted to nonhomogeneous (2,q)-Laplacian operator; global minimization for negative energy, local minimization and mountain-pass for positive energy.

Result: Proved existence of solutions with negative energy via global minimization, and solutions with positive energy via local minimization and mountain-pass; derived existence and nonexistence results for zero-mass case.

Conclusion: Mixed diffusion in (2,q)-Laplacian operator plays crucial role in determining qualitative behavior of solutions, providing comprehensive understanding of intermediate cases in non-homogeneous operators.

Abstract: This paper concerns the existence of normalized solutions to a class of $(2,q)$-Laplacian equations with a power type nonlinearity in the intermediate regime between the two mass critical exponents $2(1+2/N)$, $q(1+2/N)$. More precisely, we prove the existence of solutions with negative energy obtained through a global minimization procedure, and of solutions with positive energy established via a local minimization technique and a mountain-pass argument. Furthermore, we derive both existence and nonexistence results for the zero-mass case $λ= 0$, highlighting the role of the mixed diffusion in determining the qualitative behavior of solutions. Specifically, this paper's novelty lies in providing a comprehensive understanding of the intermediate cases that arise when the non-homogeneous $(2,q)$-Laplacian operator appears. Our analysis combines variational methods, compactness arguments, and delicate energy estimates adapted to the nonhomogeneous nature of the $(2,q)$-Laplacian operator.

</details>


### [31] [The Rabinowitz continuum of subcritical Gelfand problems and free boundary-type equations arising in plasma physics](https://arxiv.org/abs/2511.15289)
*Daniele Bartolucci,Aleks Jevnikar,Juncheng Wei,Ruijun Wu*

Main category: math.AP

TL;DR: The paper provides a new approach to analyze the Rabinowitz continuum of Gelfand problems by connecting them to constrained free boundary problems from plasma physics, using energy instead of L∞ norm, and establishing uniqueness for Grad-Shafranov type equations.


<details>
  <summary>Details</summary>
Motivation: To find a new approach for describing the qualitative behavior of the Rabinowitz unbounded continuum of subcritical Gelfand problems beyond the known results on balls in any dimension.

Method: Describes solutions of Gelfand problems via constrained free boundary-type problems from plasma physics, replaces L∞ norm with energy, solves uniqueness problem for Grad-Shafranov type equations, and uses these unique solutions to detect curves containing both minimal and non-minimal solutions.

Result: Establishes a new global parametrization of the Rabinowitz continuum, with monotonicity of energy along the branch generalizing classical pointwise monotonicity, and confirms bell-shaped profile of full solution branch on balls in any dimension.

Conclusion: The approach successfully provides a comprehensive description of the Rabinowitz continuum for Gelfand problems, extending beyond minimal solutions to include non-minimal ones through energy-based parametrization.

Abstract: The qualitative behavior of the Rabinowitz unbounded continuum of subcritical Gelfand problems is well known on balls in any dimension. We don't know of any such sharp and detailed description otherwise, which is our motivation to look for a new approach to the problem. The underlying idea is to describe solutions of Gelfand problems via suitably defined constrained problems of free boundary-type arising in plasma physics and to replace the usual $L^\infty$ norm of the solution with the energy of the plasma. Toward this goal, we first solve a long standing open problem of independent interest about the uniqueness of solutions of Grad-Shafranov type equations. Thus, we exploit these unique solutions to detect a curve containing both minimal and non minimal solutions of the associated Gelfand problem. In other words we come up with a new global parametrization of the Rabinowitz continuum, the monotonicity of the energy along the branch providing a meaningful generalization of the classical pointwise monotonicity property of minimal solutions, suitable to describe non minimal solutions as well. On a ball in any dimension, we come up as expected with a bell-shaped profile of the full branch of solutions of the Gelfand problem.

</details>


### [32] [Optimizing Resource Distribution in a One-Dimensional Logistic Diffusion Model](https://arxiv.org/abs/2511.15428)
*Junyoung Heo,Yubin Lee*

Main category: math.AP

TL;DR: Analysis of optimal resource distribution in 1D logistic diffusive model to maximize equilibrium population. For small total resources, optimal distribution is concentrated due to superlinear advantage function.


<details>
  <summary>Details</summary>
Motivation: Previous research showed optimal resources are bang-bang and concentrated at large dispersal rates, but analysis becomes difficult for general dispersal rates where optimal resources may be fragmented.

Method: Introduced block decomposition to reduce fragmented resources to concentrated blocks, defined advantage function to measure population gain from resource allocation, and analyzed convexity properties of advantage function.

Result: Proved superlinearity of advantage function when total resource is small enough, leading to explicit characterization of optimal control for sufficiently small total resources.

Conclusion: For small total resources, optimal resource distribution in 1D logistic diffusive model is concentrated rather than fragmented, due to the superlinear property of the advantage function.

Abstract: In this article, we study the optimization of resource distributions in a one-dimensional logistic diffusive model. The goal is to determine a distribution on a bounded one-dimensional domain that maximizes the total population at equilibrium. Previous works have shown that optimal resources are bang-bang, and in one dimension, a sufficiently large dispersal rate forces the optimal resource to be concentrated. For general dispersal rates, however, the analysis becomes more difficult because the equilibrium population may behave irregularly, and the optimal resource may be fragmented. To address this, we introduce a block decomposition that reduces fragmented resources to a collection of concentrated blocks. We then define an advantage function, which measures the gain in the equilibrium population obtained by allocating resources on a fixed interval and is used to analyze the contribution of each block to the total population. This function also allows us to reformulate the optimization problem as a convexity analysis of the advantage function. We prove the superlinearity of this function when the total resource is small enough, and this property leads to an explicit characterization of the optimal control with sufficiently small total resource.

</details>


### [33] [Asymptotic stability of planar entropy wave for 3-d Navier-Stokes equations in Eulerian coordinates](https://arxiv.org/abs/2511.15498)
*Ren-Jun Duan,Feimin Huang,Rui Li,Lingda Xu*

Main category: math.AP

TL;DR: The paper analyzes the asymptotic behavior of 3D Navier-Stokes equations toward planar entropy waves, overcoming challenges from diffusion waves and structural condition failures through new transformations and weighted energy estimates.


<details>
  <summary>Details</summary>
Motivation: To resolve long-standing challenges in studying entropy waves for multi-dimensional Navier-Stokes equations, where generic perturbations create diffusion waves and structural conditions fail in Eulerian coordinates, preventing closure of a priori assumptions.

Method: Introduces a new transformation to ensure left-right structural conditions hold, uses the fixed sign property of entropy wave derivatives, and applies weighted energy estimates to control slowly decaying terms. For zero mass perturbations, develops Poincaré type inequality and key cancellation.

Result: Establishes asymptotic stability and derives optimal decay rates for both types of initial perturbations (with and without zero mass condition), overcoming previous limitations in the field.

Conclusion: The proposed transformation and weighted energy estimation techniques successfully resolve the long-standing problem of analyzing entropy waves in 3D Navier-Stokes equations, providing optimal decay rates and asymptotic stability results.

Abstract: We investigate the large-time asymptotic behavior toward the planar entropy wave for the three-dimensional Navier-Stokes equations in Eulerian coordinates, considering two types of initial perturbations -- with and without the assumption that the integral of the initial perturbation is zero. Generic perturbations generate diffusion waves, and structural conditions fail for multi-dimensional Navier-Stokes equations in Eulerian coordinates. These two aspects have posed significant challenges and left the problem unresolved for years. On one hand, since \cite{LX}, the study of the entropy wave has been based on the left-right structural conditions. Without these structural conditions, the decay rates of lower-order terms become too slow to close the {\it a priori} assumption. On the other hand, the presence of diffusion waves yields problematic error terms in the perturbation system. In this work, we introduce a new transformation to ensure that both left-right structural conditions hold for the perturbation system. Additionally, using the fact that the derivative of the entropy wave maintains a fixed sign, we employ well-designed weighted energy estimates to control the slowly decaying terms. This enables us to establish asymptotic stability and derive the optimal decay rate. Furthermore, we address the case of initial perturbations with the zero mass condition and obtain the optimal decay rate by additionally developing a Poincaré type inequality and a key cancellation.

</details>


### [34] [On Moffatt's magnetic relaxation for 2D and 2.5D flows](https://arxiv.org/abs/2511.15501)
*Sepehr Mohammadkhani,Huy Q. Nguyen*

Main category: math.AP

TL;DR: The paper studies Moffatt's magnetic relaxation equation with Darcy-type regularization, aiming to prove convergence to Euler equation equilibria for various flow configurations.


<details>
  <summary>Details</summary>
Motivation: To prove the conjectured property that solutions of the topology-preserving dissipative equation converge to equilibria of incompressible Euler equations in the infinite time limit.

Method: Using Moffatt's magnetic relaxation equation with Darcy-type regularization for constitutive law, analyzing different classes of equilibria in various domains through geometric approaches.

Result: Proves convergence for non-constant shear flows in 2D periodic channels, and for 2.5D equilibria in domains Ω×ℝ where Ω can be periodic channels or bounded domains.

Conclusion: The study successfully demonstrates convergence properties of the magnetic relaxation equation towards Euler equilibria for specific flow configurations in different geometric settings.

Abstract: We study the Moffatt's magnetic relaxation equation with Darcy-type regularization for the constitutive law. This is a topology-preserving dissipative equation, whose solutions are conjectured to converge in the infinite time limit towards equilibria of the incompressible Euler equations. Our goal is to prove this conjectured property for various equilibria in various domains. The first result concerns a class of non-constant shear flows in a 2D periodic channel. In the second result, by adopting a geometric approach, we address a class of 2.5D equilibria in $Ω\times \mathbb{R}$, where $Ω\subset \mathbb{R}^2$ can be a periodic channel or any bounded domain.

</details>


### [35] [On the optimal local well-posedness of the wave kinetic equation in $L^r$](https://arxiv.org/abs/2511.15587)
*Ioakeim Ampatzoglou,Tristan Léger*

Main category: math.AP

TL;DR: Unified treatment of local well-posedness for wave kinetic equation in almost critical weighted L^r spaces (2 ≤ r ≤ ∞) using kinetic tools without Fourier theory


<details>
  <summary>Details</summary>
Motivation: To establish local well-posedness for the wave kinetic equation in nearly critical weighted L^r spaces, extending previous work and providing a unified framework

Method: Builds on ideas from earlier works, uses purely kinetic tools without Fourier theory, focuses on weighted L^r spaces with 2 ≤ r ≤ ∞

Result: Proves local well-posedness for the wave kinetic equation in almost critical weighted L^r spaces

Conclusion: The approach provides a unified treatment using kinetic methods, successfully establishing local well-posedness without relying on Fourier analysis

Abstract: In this paper, we give a unified treatment of the local well-posedness for the wave kinetic equation in almost critical weighted $L^r$ spaces with $2 \leq r \leq \infty.$ The proof builds on ideas from our earlier works \cite{AmLe24, AmLemain25}. Our approach is based solely on kinetic tools, with no appeal to Fourier theory.

</details>


### [36] [Kinetic and mean-field modeling of muscular dystrophies](https://arxiv.org/abs/2511.15599)
*Tommaso Lorenzi,Horacio Tettamanti,Mattia Zanella*

Main category: math.AP

TL;DR: A mathematical modeling framework using integro-differential equations and Fokker-Planck equations to analyze cell dynamics in muscular dystrophies, focusing on muscle fiber and immune cell distributions across patient cohorts.


<details>
  <summary>Details</summary>
Motivation: To gain insights into the balance between degeneration and regeneration mechanisms in muscular dystrophies by developing quantitative models of cell dynamics that can capture disease progression across patient populations.

Method: Developed a system of integro-differential equations for statistical distributions of muscle fibers and immune cells, derived mean-field Fokker-Planck equations, and obtained macroscopic models for mean densities and variances. Analyzed long-time asymptotics and quasi-equilibrium distributions.

Result: Established that quasi-equilibrium cell distributions follow inverse Gamma probability density functions, proved long-time convergence to these distributions, and demonstrated the approach through numerical simulations.

Conclusion: The modeling framework provides new insights into muscular dystrophy progression and serves as a foundation for future extensions including therapeutic intervention modeling.

Abstract: We present a new class of models for assessing the cell dynamics characterising muscular dystrophies. The proposed approach comprises a system of integro-differential equations for the statistical distributions, over a large patient cohort, of the densities of muscle fibers and immune cells implicated in muscle inflammation, degeneration, and regeneration, which underpin disease development. Considering an appropriately scaled version of this model, we formally derive, as the corresponding mean-field limit, a system of Fokker-Planck equations, from which we subsequently derive, as a macroscopic model counterpart, a system of differential equations for the mean densities of muscle and immune cells in the cohort of patients and the related variances. Then, we study long-time asymptotics for the mean-field model by determining the quasi-equilibrium cell distribution functions, which are in the form of probability density functions of inverse Gamma distributions, and proving the long-time convergence to such quasi-equilibrium distributions. The analytical results obtained are illustrated by means of a sample of results of numerical simulations. The modeling approach presented here has the potential to offer new insights into the balance between degeneration and regeneration mechanisms in the progression of muscular dystrophies, and provides a basis for future extensions, including the modeling of therapeutic interventions.

</details>


### [37] [A Green's function approach to linearized Monge-Ampère equations in divergence form and application to singular Abreu type equations](https://arxiv.org/abs/2511.15621)
*Chong Gu,Nam Q. Le*

Main category: math.AP

TL;DR: Establishes local/global regularity for linearized Monge-Ampère equations via Lorentz space estimates for Green's function, with applications to singular fourth-order Abreu equations.


<details>
  <summary>Details</summary>
Motivation: To develop regularity theory for linearized Monge-Ampère equations in divergence form, which are important in geometric analysis and variational problems with convexity constraints.

Method: Uses critical Lorentz space estimates for the Green's function and its gradient of the linearized Monge-Ampère operator, assuming bounded Hessian determinant of the Monge-Ampère potential.

Result: Obtains local and global regularity estimates under suitable data conditions, and proves solvability of second boundary value problems for singular fourth-order Abreu type equations in all dimensions.

Conclusion: The developed regularity theory successfully applies to solve boundary value problems for singular fourth-order equations arising from variational problems with convexity constraints.

Abstract: In this paper, we establish local and global regularity estimates for linearized Monge-Ampère equations in divergence form via critical Lorentz space estimates for the Green's function of the linearized Monge-Ampère operator and its gradient. These estimates hold under suitable conditions on the data and the convex Monge-Ampère potential is assumed to have Hessian determinant bounded between two positive constants. As an application, we obtain the solvability in all dimensions of the second boundary value problem for a class of singular fourth-order Abreu type equations that arise from the approximation analysis of variational problems subject to convexity constraints.

</details>


### [38] [Singular limit for a class of nonlocal conservation laws via compensated compactness](https://arxiv.org/abs/2511.15631)
*Giuseppe Maria Coclite,Nicola De Nitti,Kuang Huang*

Main category: math.AP

TL;DR: The paper establishes strong convergence of solutions from nonlocal conservation laws to local conservation laws as the convolution kernel concentrates to a Dirac delta, without requiring total variation bounds.


<details>
  <summary>Details</summary>
Motivation: To resolve the long-standing open problem of nonlocal-to-local convergence for non-convex kernels in traffic flow models, particularly addressing cases where traditional methods relying on total variation bounds fail.

Method: Uses L²-type bounds on entropy production and compensated compactness theory, assuming only L¹∩L∞ initial data. Analyzes piecewise constant kernels with affine velocity functions and strictly monotone kernels with decreasing velocity functions.

Result: Proves strong L¹_loc-convergence of weak solutions to entropy-admissible solutions of local conservation laws for both piecewise constant kernels (Greenshields model) and strictly monotone kernels.

Conclusion: Successfully settles the open problem of nonlocal-to-local convergence for non-convex kernels, providing a new approach that doesn't rely on total variation estimates or Oleĭnik-type conditions.

Abstract: We consider a class of nonlocal conservation laws modeling traffic flows, given by $ \partial_t u_\varepsilon + \partial_x(V(u_\varepsilon \ast γ_\varepsilon) u_\varepsilon) = 0$, with a rescaled convolution kernel $γ_\varepsilon(\cdot) := \varepsilon^{-1}γ(\cdot/\varepsilon)$. We establish the strong $\mathrm L^1_{\mathrm{loc}}$-convergence of weak solutions $u_\varepsilon$ toward the entropy-admissible solution of the corresponding local conservation law as the kernel $γ_\varepsilon$ concentrates to a Dirac delta distribution when $\varepsilon \searrow 0$. In contrast to previous literature, we obtain compactness of the family $\{u_\varepsilon \ast γ_\varepsilon\}_{\varepsilon>0}$ without relying on total variation bounds or Oleĭnik-type estimates. Instead, we establish $\mathrm L^2$-type bounds on its entropy production and use the theory of compensated compactness, assuming that the initial datum merely belongs to $\mathrm L^1\cap \mathrm L^\infty$. Our results are twofold. First, we establish the nonlocal-to-local limit for the piecewise constant kernel $γ(\cdot) := {1}_{[-1,0]}(\cdot)$ combined with the affine velocity function from Greenshields' traffic model. Second, we prove the limit for strictly monotone kernels along with decreasing velocity functions. These results settle a long-standing open problem concerning the nonlocal-to-local convergence for non-convex kernels.

</details>


### [39] [Spatial scale separation and emergent patterns in coupled diffusive-nondiffusive systems](https://arxiv.org/abs/2511.15648)
*Théo André,Szymon Cygan,Anna Marciniak-Czochra,Finn Münnich*

Main category: math.AP

TL;DR: This paper extends Turing pattern theory to mixed reaction-diffusion systems with both diffusive and nondiffusive components, revealing new pattern formation mechanisms including branch-switching and discontinuities not possible in classical systems.


<details>
  <summary>Details</summary>
Motivation: To understand pattern formation in systems where some components don't diffuse, extending beyond classical reaction-diffusion equations where all components diffuse.

Method: Mathematical analysis establishing necessary/sufficient conditions for diffusion-driven instability, classification of DDI sources in specific cases, and numerical bifurcation analysis with receptor-based model simulations.

Result: Proved existence of far-from-equilibrium patterns with branch-switching and discontinuities in nondiffusive components, identified new DDI mechanisms from subsystems with nondiffusive/slow-diffusive components, and provided complete classification for 2 diffusive + 1 nondiffusive component systems.

Conclusion: Coupling between diffusive and nondiffusive dynamics enables pattern formation beyond classical reaction-diffusion framework, extending theoretical foundations of pattern formation.

Abstract: This paper investigates pattern formation in reaction-diffusion systems with both diffusive and nondiffusive components, establishing the existence of far-from-equilibrium patterns and providing necessary and sufficient conditions for diffusion-driven instability (DDI). In particular, we prove the existence of far-from-equilibrium patterns exhibiting branch-switching and discontinuities in the nondiffusive components, which cannot occur in classical reaction-diffusion equations. While previous work has linked DDI to instability in the purely nondiffusive subsystem -- thereby destabilizing all regular Turing patterns -- we show that DDI can also arise from subsystems involving nondiffusive and slow-diffusive components. This leads to simple sufficient conditions for DDI in systems with arbitrary numbers of components. Further, we fully classify all possible sources of DDI in the case of two diffusive and one nondiffusive component, illustrating our results with a receptor-based model supported by numerical bifurcation analysis and simulations. These findings extend the theoretical foundations of pattern formation, demonstrating how coupling between diffusive and nondiffusive dynamics can generate patterns beyond the reach of the classical reaction-diffusion framework.

</details>


### [40] [Nonlinear scalar field equations with a critical Hardy potential](https://arxiv.org/abs/2511.15668)
*Bartosz Bieganowski,Daniel Strzelecki*

Main category: math.AP

TL;DR: Existence of solutions for nonlinear scalar field equations with critical Hardy potential using variational methods in non-standard functional spaces.


<details>
  <summary>Details</summary>
Motivation: Study existence of solutions for equations with critical Hardy potential, which presents mathematical challenges due to the singular nature of the potential and requires non-standard functional analysis.

Method: Variational methods in the space X^1(R^N) - completion of H^1(R^N) with norm from quadratic functional part; minimization on Pohožaev constraint M.

Result: Existence of nontrivial solution u_0 in X^1(R^N) minimizing energy on M; additional non-radial solution exists when g is odd.

Conclusion: Variational approach successfully establishes existence of solutions for critical Hardy potential problems, including both radial and non-radial solutions under appropriate conditions.

Abstract: We study the existence of solutions for the nonlinear scalar field equation $$-Δu - \frac{(N-2)^2}{4|x|^2} u = g(u), \quad \mbox{in } \mathbb{R}^N \setminus \{0\},$$ where the potential $-\frac{(N-2)^2}{4|x|^2}$ is the critical Hardy potential and $N \geq 3$. The nonlinearity $g$ is continuous and satisfies general subcritical growth assumptions of the Berestycki-Lions type. The problem is approached using variational methods within a non-standard functional setting. The natural energy functional associated with the equation is defined on the space $X^1(\mathbb{R}^N)$, which is the completion of $H^1(\mathbb{R}^N)$ with respect to the norm induced by the quadratic part of the functional. We establish the existence of a nontrivial solution $u_0 \in X^1(\mathbb{R}^N)$ that satisfies the Pohožaev constraint $\mathcal{M}$ and minimizes the energy functional on $\mathcal{M}$. Furthermore, assuming $g$ is odd, we prove the existence of at least one non-radial solution.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [41] [PAS-Net: Physics-informed Adaptive Scale Deep Operator Network](https://arxiv.org/abs/2511.14925)
*Changhong Mou,Yeyu Zhang,Xuewen Zhu,Qiao Zhuang*

Main category: physics.comp-ph

TL;DR: PAS-Net is a physics-informed Adaptive-Scale Deep Operator Network that improves learning of nonlinear PDEs with small parameters and localized features by adding multiscale feature embeddings to accelerate convergence and enhance accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning solution operators for nonlinear and singularly perturbed evolution PDEs with small parameters and localized features, which exhibit complex multiscale interactions and are difficult for standard neural networks to capture efficiently.

Method: Augments the trunk input in physics-informed DeepONet with a prescribed or learnable locally rescaled coordinate transformation centered at reference points, creating multiscale feature embeddings that act as architecture-independent preconditioners.

Result: PAS-Net consistently achieves higher accuracy and faster convergence than standard DeepONet and PI-DeepONet models on three test problems: 1D viscous Burgers equation, nonlinear diffusion-reaction system with sharp gradients, and 2D eikonal equation.

Conclusion: The adaptive-scale mechanism in PAS-Net effectively improves spectral conditioning, accelerates gradient-based convergence, and enhances representation of localized, stiff, and multiscale dynamics in PDE learning tasks.

Abstract: Nonlinear physical phenomena often show complex multiscale interactions; motivated by the principles of multiscale modeling in scientific computing, we propose PAS-Net, a physics-informed Adaptive-Scale Deep Operator Network for learning solution operators of nonlinear and singularly perturbed evolution PDEs with small parameters and localized features. Specifically, PAS-Net augments the trunk input in the physics informed Deep Operator Network (PI-DeepONet) with a prescribed (or learnable) locally rescaled coordinate transformation centered at reference points. This addition introduces a multiscale feature embedding that acts as an architecture-independent preconditioner which improves the representation of localized, stiff, and multiscale dynamics. From an optimization perspective, the adaptive-scale embedding in PAS-Net modifies the geometry of the Neural Tangent Kernel (NTK) associated with the neural network by increasing its smallest eigenvalue, which in turn improves spectral conditioning and accelerates gradient-based convergence. We further show that this adaptive-scale mechanism explicitly accelerates neural network training in approximating functions with steep transitions and strong asymptotic behavior, and we provide a rigorous proof of this function-approximation result within the finite-dimensional NTK matrix framework. We test the proposed PAS-Net on three different problems: (i) the one-dimensional viscous Burgers equation, (ii) a nonlinear diffusion-reaction system with sharp spatial gradients, and (iii) a two-dimensional eikonal equation. The numerical results show that PAS-Net consistently achieves higher accuracy and faster convergence than the standard DeepONet and PI-DeepONet models under a similar training cost.

</details>


### [42] [Reconstruction of three-dimensional shapes of normal and disease-related erythrocytes from partial observations using multi-fidelity neural networks](https://arxiv.org/abs/2511.14962)
*Haizhou Wen,He Li,Zhen Li*

Main category: physics.comp-ph

TL;DR: A multi-fidelity neural network approach reconstructs 3D red blood cell morphology from partial cross-sections with over 95% accuracy using combined high-fidelity cross-sections and low-fidelity reference shapes.


<details>
  <summary>Details</summary>
Motivation: Reconstruction of 3D erythrocyte morphology from partial observations is essential for understanding RBC aging physiology and various RBC disorders, as conventional microscope images only provide limited cross-sectional views.

Method: Multi-fidelity neural network combining convolutional neural network on low-fidelity reference RBC data with feedforward neural network for nonlinear correlations, augmented with surface area and volume constraints for regularization, based on topological homeomorphism between sphere and 3D RBC surfaces.

Result: MFNN reconstructs complex RBC morphologies with over 95% coordinate accuracy using at least two orthogonal cross-sections. Informative oblique cross-sections intersecting spicule tips improve feature reconstruction. Enhanced robustness demonstrated under various sampling strategies, shape dissimilarity, and noise conditions.

Conclusion: MFNN effectively reconstructs 3D shapes of normal and aged RBCs from partial cross-sections, facilitating quantitative analysis of RBC morphological parameters in both normal and disease-related samples.

Abstract: Reconstruction of 3D erythrocyte or red blood cell (RBC) morphology from partial observations, such as microscope images, is essential for understanding the physiology of RBC aging and the pathology of various RBC disorders. In this study, we propose a multi-fidelity neural network (MFNN) approach to fuse high-fidelity cross-sections of an RBC, with a morphologically similar low-fidelity reference 3D RBC shape to recover its full 3D surface. The MFNN predictor combines a convolutional neural network trained on low-fidelity reference RBC data with a feedforward neural network that captures nonlinear morphological correlations, and augments training with surface area and volume constraints for regularization in the low-fidelity branch. This approach is theoretically grounded by a topological homeomorphism between a sphere and 3D RBC surfaces, with training data generated by dissipative particle dynamics simulations of stomatocyte-discocyte-echinocyte transformation. Benchmarking across diverse RBC shapes observed in normal and aged populations, our results show that the MFNN predictor can reconstruct complex RBC morphologies with over 95% coordinate accuracy when provided with at least two orthogonal cross-sections. It is observed that informative oblique cross-sections intersecting spicule tips of echinocytes improve both local and global feature reconstruction, highlighting the value of feature-aware sampling. Our study further evaluates the influence of sampling strategies, shape dissimilarity, and noise, showing enhanced robustness under physically constrained training. Altogether, these results demonstrate the capability of MFNN to reconstruct the 3D shape of normal and aged RBCs from partial cross-sections as observed in conventional microscope images, which could facilitate the quantitative analysis of RBC morphological parameters in normal and disease-related RBC samples.

</details>


### [43] [jaxFMM: An Adaptive, GPU-Parallel Implementation of the Fast Multipole Method in JAX](https://arxiv.org/abs/2511.15269)
*Robert Kraft,Florian Bruckner,Dieter Suess,Claas Abert*

Main category: physics.comp-ph

TL;DR: jaxFMM is an open-source, adaptive Fast Multipole Method implementation in JAX for Laplace kernel, featuring parallel computation and simple code structure.


<details>
  <summary>Details</summary>
Motivation: To provide an efficient solution for point-charge computations that can handle non-uniform distributions and enable future applications in inverse-design and machine learning.

Method: Uses non-uniform refinement strategy with JAX implementation for parallel computation, focusing on the Laplace kernel for point-charge interactions.

Result: Benchmarks show excellent performance for non-uniform charge distributions and significant speedup in micromagnetics stray-field computations.

Conclusion: jaxFMM offers a concise, efficient FMM implementation that enables new applications through JAX features like autodiff, making it suitable for inverse-design and ML tasks.

Abstract: We introduce jaxFMM, an open-source, adaptive, highly parallel point-charge Fast Multipole Method implementation for the Laplace kernel written in JAX. It is based on a non-uniform refinement strategy, which results in extremely concise and simple code. Benchmarks show that the algorithm performs well even for highly non-uniform charge distributions. JaxFMM already massively speeds up stray-field computations in micromagnetics and with JAX features like autodiff, novel applications such as inverse-design problems and machine-learning tasks can be tackled with ease in the future.

</details>


### [44] [A Full-Induction Magnetohydrodynamics Solver for Liquid Metal Fusion Blankets in Vertex-CFD](https://arxiv.org/abs/2511.15549)
*Eirik Endeve,Doug Stefanski,Marc-Olivier G. Delchini,Stuart Slattery,Cory D. Hauck,Bruno Turcksin,Sergey Smolentsev*

Main category: physics.comp-ph

TL;DR: Development and verification of a full-induction MHD solver in Vertex-CFD framework for transient multiphysics modeling of liquid metal fusion blankets, enabling accurate simulation of magnetic field dynamics during millisecond-scale plasma variations.


<details>
  <summary>Details</summary>
Motivation: Traditional inductionless MHD approximations are insufficient for transient scenarios in fusion blankets where magnetic fields vary rapidly on millisecond timescales, requiring full-induction MHD modeling to accurately capture dynamic magnetic field evolution.

Method: Implemented finite element spatial discretization with implicit Runge-Kutta time integration and inexact Newton method using Trilinos packages, integrated within open-source Vertex-CFD framework for multiphysics coupling and performance portability.

Result: Solver verification shows accuracy and robustness against benchmark problems, with good agreement between Vertex-CFD results and published quasi-2D simulations for idealized blanket models in 2.5D and 3D.

Conclusion: Establishes computational foundation for transient MHD simulations in liquid metal fusion blankets and enables future extensions and performance optimizations within the Vertex-CFD framework.

Abstract: Multiphysics modeling of liquid metal fusion blankets, which produce tritium and convert energy of neutrons created via fusion reactions into heat, is crucial for predicting performance, ensuring structural integrity, and optimizing energy production. While traditional blanket modeling of liquid metal flows during normal steady operating conditions commonly employs the inductionless approximation of the magnetohydrodynamics (MHD) equations, transient scenarios, when the plasma-confining magnetic field varies on millisecond time scales, require a full-induction MHD approach that dynamically evolves the magnetic field via the time-dependent induction equation. This paper presents the formulation, implementation, and initial verification of a full-induction MHD solver integrated within the open-source Vertex-CFD framework, which aims to achieve tight multiphysics coupling, a flexible software design enabling easy extension and addition of physics models, and performance portability across computing platforms. The solver utilizes finite element spatial discretization, implicit Runge--Kutta time integration, and an inexact Newton method to solve the resulting discrete nonlinear system, leveraging Trilinos packages for efficient computation. Verification against selected benchmark problems demonstrates accuracy and robustness of the solver. Furthermore, when the solver is applied to an idealized blanket model in 2.5D and full 3D, results obtained with Vertex-CFD are in good agreement with recently published quasi-2D simulations. These findings establish a computational foundation for future simulations of transient MHD phenomena in liquid metal blankets with Vertex-CFD, and open avenues for future extensions and performance optimizations.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [45] [Relativistic resistive magnetohydrodynamics for a two-component plasma](https://arxiv.org/abs/2511.14787)
*Khwahish Kushwah,Caio V. P. de Brito,Gabriel S Denicol*

Main category: physics.plasm-ph

TL;DR: Derivation of relativistic resistive magnetohydrodynamics from kinetic theory using 14-moment approximation in Landau frame, obtaining coupled equations for charge diffusion and shear-stress tensor.


<details>
  <summary>Details</summary>
Motivation: To develop relativistic resistive magnetohydrodynamics directly from kinetic theory rather than using phenomenological approaches, providing a more fundamental derivation.

Method: Starting from Boltzmann-Vlasov equation and using 14-moment approximation in Landau frame to derive coupled evolution equations for charge diffusion four-current and shear-stress tensor.

Result: The simplified description is accurate for small viscosity-to-entropy ratio, vanishing magnetic field, and moderate electric fields. Strong electric fields introduce nonlinear back-reaction that delays and reduces current peaks, and shear-stress is produced even without flow profiles.

Conclusion: The kinetic theory derivation provides controlled departures from Israel-Stewart type relaxation forms, with nonlinear effects emerging in strong field regimes.

Abstract: We derive relativistic resistive magnetohydrodynamics for a two-component ultrarelativistic plasma directly from kinetic theory. Starting with the Boltzmann--Vlasov equation and using the 14-moment approximation in the Landau frame, we obtain coupled evolution equations for the charge diffusion four-current and the shear-stress tensor. Benchmarking against the usual Israel-Stewart type relaxation form shows that this simplified description is accurate for small viscosity to entropy ($η/s$) ratio, vanishing magnetic field, and not so strong electric field. Outside this regime the dynamics depart in a controlled way, i.e., strong electric fields introduce nonlinear back-reaction that delays and reduces current peaks, and a sizable shear-stress is produced even without a flow profile.

</details>


### [46] [On the Theory of Bulk Viscosity of Cold Plasmas](https://arxiv.org/abs/2511.14790)
*Albert M. Varonov,Todor M. Mishonov*

Main category: physics.plasm-ph

TL;DR: Derivation of bulk viscosity in cold plasmas showing it can be orders of magnitude larger than shear viscosity, with applications to solar chromosphere heating.


<details>
  <summary>Details</summary>
Motivation: To understand ionization-recombination processes in cold plasmas and derive explicit expressions for bulk viscosity, particularly for temperatures much lower than first ionization potentials.

Method: Solving the kinetic equation for ionization-recombination processes in cold plasmas, deriving explicit expressions for bulk viscosity and relaxation time.

Result: Bulk viscosity can be many orders of magnitude larger than shear viscosity. The Mandelstam-Leontovich approximation is practically exact for cold plasmas. Numerical examples show application to solar chromosphere plasma.

Conclusion: The derived bulk viscosity theory has potential applications for acoustic heating of the inner solar atmosphere and can be confirmed by laboratory plasma experiments.

Abstract: Solving the kinetic equation for ionization-recombination processes in cold plasmas for temperatures much lower than the first ionization potentials, we derive an explicit expression for the bulk viscosity. We obtain that bulk viscosity can be many order of magnitude bigger than the shear viscosity. Our result for the relaxation time reveals that the Mandelstam-Leontovich approximation for the frequency dependence of the bulk viscosity is in practice an exact result for the cold plasmas. The illustrative numerical examples correspond to the plasma cocktail of the solar chromosphere at the height of the minimal polytropic index. The possible application for the acoustic heating of the inner solar atmosphere up to the transition region is shortly discussed together with the evaluation to confirm the theory by laboratory plasmas.

</details>


### [47] [Impact of edge turbulence spreading on broadening the heat flux width with plasma approaching the density limit](https://arxiv.org/abs/2511.15024)
*T. Wu,P. H. Diamond,L. Nie,R. Ke,Z. P. Chen,Q. H. Yang,W. J. Tian,T. Long,Z. J. Yang,Z. Y. Chen,M. Xu*

Main category: physics.plasm-ph

TL;DR: Edge turbulence spreading, particularly from blobs, broadens heat flux width in J-TEXT tokamak as plasma approaches density limit, with EXB shear flow collapse and enhanced turbulence transport.


<details>
  <summary>Details</summary>
Motivation: To understand how edge turbulence spreading affects heat flux width broadening in Ohmic-plasma approaching the density limit in tokamaks.

Method: Used energy production ratio model to quantify edge turbulence spreading contribution, analyzed experimental data from J-TEXT tokamak, and investigated blob-induced transport impact in detail.

Result: Heat flux width increases with normalized density; energy production ratio >>1 indicates turbulence spreading at separatrix is origin of SOL turbulence; blob-induced spreading accounts for ~81% of total edge spreading in high-density scenarios.

Conclusion: Edge turbulence spreading, dominated by blobs with larger radial scales, plays crucial role in broadening heat flux width as plasma approaches density limit.

Abstract: This paper investigates the impact of edge turbulence spreading on broadening the heat flux width in Ohmic-plasma approaching the density limit of the J-TEXT tokamak. At the plasma edge, the EXB shear flow collapses while turbulence transport and spreading enhances significantly when approaching the density limit. The heat flux width increases with normalized density. An energy production ratio model is used to quantify the contribution of edge turbulence spreading to the origin of the SOL turbulence. Experimental data show that the energy production ratio is much larger than 1, indicating that turbulence spreading at separatrix is the origin of the SOL turbulence. The heat flux widths increase with edge turbulence spreading as well as the energy production ratio. The impact of blob-induced transport on the heat flux width is investigated in detail. Especially, the average blob-induced spreading is about 81% of the total edge spreading in the high-density scenario. Blobs with larger radial scales enhance edge spreading into the SOL, thus dominating the SOL turbulence and consequently broadening the heat flux width. These results suggest that edge turbulence spreading plays a crucial role in broadening the heat flux width as plasma approaches the density limit.

</details>


### [48] [Characterisation of X- and O-points in Wendelstein 7-X with respect to coil currents](https://arxiv.org/abs/2511.15641)
*Robert Davies,Christopher B. Smiet,Charlotte Batzdorf,J. Geiger,J. Loizu,S. A. Henneberg*

Main category: physics.plasm-ph

TL;DR: Analysis of vacuum magnetic field topology in Wendelstein 7-X showing how coil currents affect fixed points (X/O-points) and island chains, with automated scheme for Tr(M) calculation.


<details>
  <summary>Details</summary>
Motivation: To understand how changes in superconducting coil currents affect magnetic field topology and island chain properties in stellarator fusion devices like W7-X.

Method: Developed automated scheme to locate fixed points and calculate Tr(M) of field line map; performed coil current scans: individual variations from standard/high/low iota configurations, and random sampling of 200,000+ configurations within normal W7-X operating range.

Result: Non-planar coils establish island chains with specific phase; planar coils modify location by controlling iota profile and shifting configuration; control coil affects island size and phase. |Tr(M)-2| increases with minor radius of fixed points, and X/O-points respond differently to control coil current.

Conclusion: |Tr(M)-2| serves as proxy for island size in internal island chains, potentially helping identify suitable experimental configurations for stellarator optimization.

Abstract: This work analyses vacuum magnetic field topology in Wendelstein 7-X (W7-X) with respect to changes in the current in the superconducting coils. We develop a fast automated scheme to locate fixed points (such as X- and O-points) and calculate the trace of the Jacobian of the field line map for them (Tr(M)), which represents several important properties of the fixed point. We perform two sets of coil current scans: (1) scans where each coil current is varied individually, using the "standard", "high iota" and "low iota" configurations as starting points; (2) a scan of over 200,000 magnetic configurations in which the coil currents are randomly sampled. In both cases we constrain the coil currents to the normal range of W7-X. We verify the principal roles of the non-planar, planar and control coils: the non-planar coils establish island chains with a certain phase; the planar coils modify the location of the island chain by both controlling the iota profile and shifting the configuration "inward" and "outward"; the control coil affects the island size and phase. We also find that |Tr(M)-2| (a quantity closely related to the magnitude of the Greene's residue) tends to increase with the minor radius of the fixed points, and that Tr(M) for X- and O-points can be very differently affected by the control coil current. Finally, we show that |Tr(M)-2| serves as a proxy for island size for internal island chains, which may help identification of suitable experimental candidates.

</details>


### [49] [Material processing by laser-plasma-filament-guided high voltage discharges](https://arxiv.org/abs/2511.15651)
*Kristian Cvecek,Markus Döring,Alexander Romboy,Johannes Heberle,Michael Schmidt*

Main category: physics.plasm-ph

TL;DR: Laser-plasma-filament guided electrical discharges enhance ablation volume by up to 12.5x compared to laser-only processing.


<details>
  <summary>Details</summary>
Motivation: To combine remote material processing capabilities with the ability to steer and deflect high voltage discharges using laser technology.

Method: Using fs-laser-generated plasma filaments to guide electrical discharges (up to 145 kV) across a 201 mm air gap onto steel samples.

Result: Ablated volume increased by factor of 1.67 compared to discharges without plasma filaments, and up to 12.5x compared to laser-only processing.

Conclusion: Laser-plasma-filament guided discharges significantly enhance ablation efficiency and enable remote steering of high voltage discharges.

Abstract: We investigate ablation experiments performed by laser-plasma-filament guided electrical discharges at high-voltages of up to 145 kV. The guiding was accomplished via fs-laser-generated plasma filaments across a gap of 201 mm of air onto steel 1.3343 samples. This method combines remote material processing and enables the steering and deflection of high voltage discharges with the ease-of-use of remote laser processing technology. We observe an increase of the per-pulse-(and-discharge)-ablated volume by a factor of 1.67 over an ablation regime when the discharges are not present and up to a factor of 12.5 over the case when neither discharges nor plasma filaments, only a loosely focused laser beam, are present.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [50] [Proton specific entropy as a proxy for the $O^{7+}/O^{6+}$ charge state ratio over heliocentric distance](https://arxiv.org/abs/2511.14938)
*Jack D. Collard,Tamar Ervin,Ryan M. Dewey,Yeimy J. Rivera,Aidan J. Nakhleh,Jean-Baptiste Dakeyo,Samuel T. Badman,Trevor A. Bowen,John W. Bonnell,Nicholeen M. Viall,Susan T. Lepri,Jim M. Raines,Stuart D. Bale*

Main category: astro-ph.SR

TL;DR: Proton specific entropy serves as a proxy for oxygen charge state ratio to identify solar wind source regions, validated using Solar Orbiter data from 0.28-1 AU.


<details>
  <summary>Details</summary>
Motivation: The source of slow solar wind remains uncertain, and in-situ heavy ion charge state measurements were previously limited to 1 AU and beyond, creating a need for alternative classification methods.

Method: Used Solar Orbiter's Heavy Ion Sensor and Proton and Alphas Sensor observations to investigate proton specific entropy as a proxy for oxygen charge state ratio (O7+/O6+), categorizing solar wind into fast, slow Alfvenic, and slow types.

Result: Found a strong anti-correlation between specific entropy and oxygen charge state ratio that persists across 0.28-1 AU, with clear distinctions in entropy values and charge state ratios across different solar wind types.

Conclusion: Proton specific entropy can effectively classify solar wind source regions and identify source regions when in-situ charge state measurements are unavailable, motivating future applications to Parker Solar Probe and inner heliosphere studies.

Abstract: While the fast solar wind has well-established origins in coronal holes, the source of the slow solar wind remains uncertain. Compositional metrics, such as heavy ion charge state ratios are set in the lower corona, providing insights into solar wind source regions. However, prior to the launch of Solar Orbiter, in situ measurements of heavy ion charge state were limited to distances of 1 AU and beyond. We investigate proton specific entropy as a proxy for the oxygen charge state ratio ($O^{7+}/O^{6+}$),which generally becomes frozen-in below ~1.8 Rsun, leveraging observations from Solar Orbiter's Heavy Ion Sensor and Proton and Alphas Sensor covering 0.28 to 1 AU. Our analysis confirms a strong anti-correlation between specific entropy and the oxygen charge state ratio that persists over a broad range of distances in the inner heliosphere. We categorize observed solar wind into fast solar wind, slow Alfvenic solar wind, and slow solar wind, identifying clear distinctions in specific entropy values and charge state ratios across these types. The work demonstrates the potential to use proton specific entropy as a classifier of solar wind source regions throughout the heliosphere. By establishing the $S_p$-$O^{7+}/O^{6+}$ relationship and quantifying its radial dependence, the specific entropy can be used as a quantity to identify the solar wind source region in the absence of in-situ charge state measurements. This motivates future studies as to the applicability of this proxy to near-Sun observations (such as Parker Solar Probe) and throughout the inner heliosphere.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [51] [Transformation from integral operator with separable kernel to matrix in eigenvalue problem](https://arxiv.org/abs/2511.14979)
*Soma Hirai,Ryoto Watanabe,Yuki Nishida,Masashi Iwasaki*

Main category: math.FA

TL;DR: The paper shows that for integral operators with separable kernels, solving eigenvalue problems reduces to computing matrix eigenpairs and generalized eigenvectors.


<details>
  <summary>Details</summary>
Motivation: To establish connections between integral operators with separable kernels and matrix eigenvalue problems, simplifying the solution of Fredholm integral equations.

Method: Represent separable kernels in matrix form, relate integral operator eigenpairs to matrix eigenpairs, generalize eigenfunctions using matrix generalized eigenvectors.

Result: Demonstrated that solving Fredholm integral equations of the second kind reduces to computing matrix eigenpairs and generalized eigenvectors.

Conclusion: The approach provides a systematic method to solve integral equations with separable kernels by leveraging matrix eigenvalue theory.

Abstract: This paper investigates the eigenvalue problem of integral operators whose kernels can be expressed as a finite sum of pairwise products of single-variable functions, making them separable. By consdiering the matrix form of the separable kernel in the integral operator, we establish the relationship between the eigenvalues and eigenfunctions of the integral operator and the eigenpairs of a matrix. We next generalize the eigenfunction of an integral operator based on the concept of generalized eigenvectors of matrices, and show that solving the Fredholm integral equation of the second kind reduces to computing matrix eigenpairs and generalized eigenvectors. We also provide several examples to validate our results.

</details>


### [52] [Beyond Tchakaloff Quadrature: Positive Functionals, Frames and Widths](https://arxiv.org/abs/2511.15425)
*Martin Schäfer,Tino Ullrich*

Main category: math.FA

TL;DR: The paper extends Tchakaloff's theorem on exact quadrature rules with non-negative weights, introducing the concept of strict S-positivity for functionals and showing equivalence to positive discretizability. It provides applications including L_p-Marcinkiewicz-Zygmund equalities and frame discretization.


<details>
  <summary>Details</summary>
Motivation: To generalize Tchakaloff's classical result on exact quadrature rules with non-negative weights and provide a comprehensive framework for positive discretizability of functionals on finite-dimensional spaces.

Method: Introduces the notion of strict S-positivity for C-linear functionals and establishes equivalence with positive discretizability. Investigates consequences for various discretization problems including quadrature rules, L_p-MZ equalities, and frame discretization.

Result: Proves equivalence between positive discretizability and strict S-positivity. Shows existence of L_p-Marcinkiewicz-Zygmund equalities for even p, exact discretizability of frames with rescaling, and provides bounds for Tchakaloff quadrature widths.

Conclusion: The framework of strict S-positivity provides a unifying approach to positive discretization problems, extending classical results and offering new insights into quadrature rules, MZ inequalities, and frame theory with practical implications for numerical analysis.

Abstract: Tchakaloff's theorem from 1957 asserts the existence of exact quadrature rules with non-negative weights for any polynomial space of finite degree on $\mathbb{R}^d$ if the underlying measure is positive, compactly supported, and absolutely continuous with respect to the Lebesgue measure. This classical result coined the term Tchakaloff quadrature for quadrature that is exact and only uses non-negative weights. It has been a long-standing endeavor, under which conditions such rules exist. A final answer was given in 2012 by Bisgaard with the insight that, in fact, every finite-dimensional space of integrable functions on a positive measure space admits them. In this article we recall this result and provide a major extension to the question of positive discretizability of $\mathbb{C}$-linear functionals on finite-dimensional spaces. We introduce the notion of strict $S$-positivity for such functionals, where $S$ are subsets of the functional's domain, and show the equivalence of positive discretizability to being strictly $S$-positive for a suitable choice of $S$. We further investigate consequences for other discretization problems. One fundamental implication is the guaranteed existence of $L_p$-Marcinkiewicz-Zygmund equalities in finite-dimensional spaces of $p$-integrable functions in case that $p$ is an even integer, another the exact discretizability of any frame in $\mathbb{K}^n$, where $\mathbb{K}\in\{\mathbb{R},\mathbb{C}\}$, if a rescaling of the frame elements is allowed. In addition, we provide bounds for Tchakaloff quadrature widths $κ_n^+$ and, addressing the question of constructibility of discretization points, establish a connection to $D$-optimal design.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [53] [Fluctuating Hydrodynamics of the Ising-Kac-Kawasaki Model and Nonlinear Fluctuations Near Criticality](https://arxiv.org/abs/2511.15263)
*Zhengyan Wu*

Main category: math.PR

TL;DR: The paper proves that rescaled fluctuating Ising-Kac-Kawasaki dynamics converge to the stochastic Cahn-Hilliard equation, solving a conjecture about nonlinear fluctuation phenomena and establishing multi-scale dynamical large deviations.


<details>
  <summary>Details</summary>
Motivation: To address a conjecture by Giacomin, Lebowitz, and Presutti (1999) concerning nonlinear fluctuation phenomena in conservative SPDEs, specifically the relationship between fluctuating Ising-Kac-Kawasaki dynamics and the stochastic Cahn-Hilliard equation.

Method: Studying scaling limit behavior of conservative SPDEs, specifically analyzing sequences of one-dimensional rescaled fluctuating Ising-Kac-Kawasaki equations and proving convergence properties through multi-scale dynamical large deviations analysis.

Result: Proved convergence of rescaled fluctuating Ising-Kac-Kawasaki equations to the stochastic Cahn-Hilliard equation solution, established multi-scale dynamical large deviations in small noise regime, and showed Γ-convergence of rate functions.

Conclusion: The work successfully resolves a version of the nonlinear fluctuation conjecture and establishes rigorous connections between microscopic Ising-Kac-Kawasaki dynamics and macroscopic Cahn-Hilliard equation behavior through scaling limits and large deviations theory.

Abstract: We study the scaling limit behavior of a family of conservative SPDEs as the fluctuating Ising-Kac-Kawasaki dynamics. Precisely, we show that there exists a sequence of the one-dimensional rescaled fluctuating Ising-Kac-Kawasaki equation converges to the solution of the stochastic Cahn-Hilliard equation. This solves a simple version of the conjecture concerning the nonlinear fluctuation phenomenon, proposed by [Giacomin, Lebowitz, Presutti; Math. Surveys Monogr., 1999]. Furthermore, we prove a multi-scale dynamical large deviations in a small noise regime. Finally, we show the $Γ$-convergence of the rate function for the rescaled fluctuating Ising-Kac-Kawasaki equation to the rate function of the Cahn-Hilliard equation.

</details>


### [54] [A Critical Drift-Diffusion Equation: Intermittent Behavior via Geometric Brownian Motion on $ \textbf{SL}(n)$](https://arxiv.org/abs/2511.15473)
*Peter S. Morfe,Felix Otto,Christian Wagner*

Main category: math.PR

TL;DR: This paper studies diffusion in the curl of the 2d Gaussian free field and its higher-dimensional generalizations, revealing a connection with geometric Brownian motion on SL(n) groups through scale-by-scale homogenization.


<details>
  <summary>Details</summary>
Motivation: To understand intermittent behavior in diffusion processes related to the Gaussian free field and extend the analysis to higher dimensions beyond the 2D case.

Method: Reformulates the scale-by-scale homogenization approach using SDEs in length scale L, connecting it to geometric Brownian motion on the special linear group SL(n).

Result: The analysis exposes an unexpected connection between the diffusion problem and geometric Brownian motion on SL(n), providing insights into intermittent behavior of Lagrangian coordinates.

Conclusion: The geometric Brownian motion framework on SL(n) groups offers new understanding of intermittent phenomena in diffusion processes related to the Gaussian free field across dimensions.

Abstract: This paper concerns the so-called diffusion in the curl of the 2d Gaussian free field, and its generalization to higher dimensions $n \geq 2$, building on the scale-by-scale homogenization approach developed recently by Chatzigeorgiou, Morfe, Otto, and Wang [13]. It begins by reformulating the approximation scheme of that work in terms of SDEs in the length scale $L$. This exposes an unexpected connection with a certain geometric Brownian motion on the special linear group $\textbf{SL}(n)$. The analysis of this process sheds light on the original problem, particularly as it pertains to intermittent behavior exhibited by the (averaged) Lagrangian coordinate.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [55] [Addressing the gravitational collapse of a massless scalar field with Physics-Informed Neural Networks](https://arxiv.org/abs/2511.15247)
*Antonio Ferrer-Sánchez,Nino Villanueva-Espinosa,Carlos Hernani Morales,Roberto Ruiz de Austri-Bazan,José A. Font,José David Martín-Guerrero,Matthew W. Choptuik*

Main category: gr-qc

TL;DR: PINNs and neural architectures are evaluated for solving gravitational collapse of massless scalar field, showing competitive accuracy with fewer points than traditional methods. ModPINN performs best near criticality.


<details>
  <summary>Details</summary>
Motivation: To benchmark neural methods for numerical relativity by testing Physics-Informed Neural Networks on the challenging gravitational collapse problem with critical behavior.

Method: Comparative assessment of PINN variants (vanilla, high-precision, sinusoidal-feature, quadratic-residual, KANs) using Einstein-massless-Klein-Gordon formulation with adaptive spacetime sampling and novel ModPINN architecture.

Result: Deep learning methods reproduce finite-difference solutions with competitive accuracy using significantly fewer collocation points. ModPINN achieves particularly stable and accurate solutions near criticality.

Conclusion: Suitably designed embeddings and adaptive sampling can enhance PINN robustness for challenging gravitational collapse scenarios, though no single architecture dominates all regimes.

Abstract: The gravitational collapse of a massless scalar field remains a demanding benchmark for numerical methods in numerical relativity, as it exhibits critical behavior at the boundary between dispersion and black hole formation. In this work we revisit this problem by relying on Physics-Informed Neural Networks (PINNs) as flexible partial differential equations solvers thereby providing a comparative assessment of several recent neural architectures. Building on the Einstein-massless-Klein-Gordon formulation in polar-areal coordinates, we consider four initial-value problems encompassing subcritical, critical, and supercritical regimes and use high-resolution finite-difference simulations as reference solutions. Our study is primarily comparative: we evaluate several state-of-the-art deep learning architectures, including vanilla and high-precision PINNs, sinusoidal-feature and quadratic-residual variants, Kolmogorov-Arnold Networks, all trained under a common loss design that encodes the field equations, boundary conditions, and causal time-space enforcement, together with a novel adaptive spacetime sampling. Within this framework we also introduce ModPINN, a modest modification of standard PINNs that augments standard multiplayer perceptrons with coordinate embeddings, quadratic layers among other common ingredients in recent literature. This study shows that deep-learning-based methods can reproduce finite-difference solutions for the scalar field and the spacetime metric with competitive accuracy using significantly fewer collocation points than more traditional methodologies. While no single architecture dominates in all regimes, ModPINN achieves particularly stable and accurate solutions near criticality, indicating that suitable designed embeddings and adaptive sampling can enhance the robustness of PINNs for challenging gravitational-collapse scenarios.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [56] [Drive beam depletion with multi-Joule energy transfer in a plasma wakefield accelerator](https://arxiv.org/abs/2511.15143)
*R. Ariniello,V. Lee,D. Storey,C. Emma,S. Gessner,M. J. Hogan,A. Knetsch,M. D. Litos,N. Majernik,B. O'Shea*

Main category: physics.acc-ph

TL;DR: Demonstrated significant progress in beam-driven plasma wakefield acceleration, achieving 5.6 J energy transfer from a 10 GeV electron beam to plasma with 37% efficiency and particle deceleration down to 0.93 GeV.


<details>
  <summary>Details</summary>
Motivation: Colliders based on plasma wakefield acceleration require high energy transfer efficiency (exceeding 70%) from drive beams to plasma, transferring 10-100s of Joules per stage.

Method: Used an all-optical plasma source with a 1.52 nC, 10 GeV electron beam interacting with a hydrogen plasma at density 4.5×10¹⁶ cm⁻³.

Result: Transferred at least 5.6 J from beam to plasma, achieved 37±3% drive-to-wake energy transfer efficiency, observed particle deceleration to less than 0.93 GeV with up to 90% charge participation.

Conclusion: Significant progress demonstrated toward plasma wakefield acceleration parameters needed for future colliders, though efficiency still below the required 70% threshold.

Abstract: A collider based on beam-driven plasma wakefield acceleration will require the drive beam to transfer 10-100s of Joules to the plasma in each stage, with a drive-to-wake energy transfer efficiency exceeding 70%. Using an all-optical plasma source, we demonstrate significant progress towards these parameters, transferring at least 5.6 J from a 1.52 nC, 10 GeV electron beam to a $4.5\times 10^{16}\,\mathrm{cm^{-3}}$ hydrogen plasma while achieving at least $37\pm 3\%$ drive-to-wake energy transfer efficiency. We observe deceleration of some particles to less than 0.93 GeV with up to 90% of the charge participating in the interaction.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [57] [Turbulence in the terrestrial magnetosheath: space-time correlation using the Magnetospheric Multiscale mission](https://arxiv.org/abs/2511.15579)
*Francesco Pecora,William H. Matthaeus,Antonella Greco,Pablo Dmitruk,Yan Yang,Vincenzo Carbone,Sergio Servidio*

Main category: physics.space-ph

TL;DR: First direct observation of turbulence propagator in magnetosheath reveals spatial and spectral anisotropy with different relaxation times parallel vs perpendicular to magnetic field, providing critical constraints for plasma turbulence models.


<details>
  <summary>Details</summary>
Motivation: To investigate spatiotemporal correlation of magnetic field fluctuations and understand the space-time structure of turbulent space plasmas through direct observation.

Method: Used Magnetospheric Multiscale mission data from over 1000 intervals in terrestrial magnetosheath, analyzing turbulence propagator and full space-time investigation of Taylor hypothesis.

Result: Found clear spatial/spectral anisotropy with distinct parallel vs perpendicular relaxation times. Perpendicular modes decorrelate via sweeping/Alfvénic propagation, while parallel modes' decorrelation time is independent of parallel wavenumber (possibly due to resonant interactions).

Conclusion: Provides unprecedented insight into space-time structure of turbulent space plasmas and critical constraints for theoretical/numerical models through direct observation of turbulence propagator.

Abstract: Spatiotemporal correlation of magnetic field fluctuations is investigated using the Magnetospheric Multiscale mission in the terrestrial magnetosheath. The first observation of the turbulence propagator in space emerges through analysis of more than a thousand intervals. Results show clear features of spatial and spectral anisotropy, leading to a distinct behavior of relaxation times in the directions parallel and perpendicular to the mean magnetic field. Full space-time investigation of the Taylor hypothesis reveals a scale-dependent anisotropy of magnetosheath fluctuations that can be compared to the effect of flow propagation on spacecraft frame time decorrelation rates as well as with Eulerian estimates. The turbulence propagator reveals that the amplitudes of the perpendicular modes decorrelate according to sweeping or Alfvénic propagation mechanisms. The decorrelation time of parallel modes instead does not depend on the parallel wavenumber, which could be due to resonant interactions. Through direct observation, this study provides unprecedented insight into the space-time structure of turbulent space plasmas, while giving critical constraints for theoretical and numerical models.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [58] [RLS Framework with Segmentation of the Forgetting Profile and Low Rank Updates](https://arxiv.org/abs/2511.15273)
*Alexander Stotsky*

Main category: math.OC

TL;DR: A new regularization method using segmented forgetting profiles in sliding window least squares to improve estimation speed, accuracy, and stability while reducing condition number of information matrix.


<details>
  <summary>Details</summary>
Motivation: To enhance estimator properties like rapidity, accuracy, numerical stability, and condition number reduction while incorporating a priori signal information into the estimation process.

Method: Divides forgetting profile into three segments: rapid exponential forgetting for speed, declining transition segment, and slow exponential forgetting for condition number reduction. Uses recursive algorithm with low rank updates based on new matrix inversion lemma for moving windows.

Result: Significantly improved approximation accuracy of low resolution daily temperature measurements from Stockholm Old Astronomical Observatory, enhancing temperature prediction reliability.

Conclusion: The segmented forgetting profile approach successfully balances estimation speed and accuracy while reducing condition number, leading to more reliable predictions in practical applications like temperature measurement.

Abstract: This report describes a new regularization approach based on segmentation of the forgetting profile in sliding window least squares estimation. Each segment is designed to enforce specific desirable properties of the estimator such as rapidity, desired condition number of the information matrix, accuracy, numerical stability, etc. The forgetting profile is divided in three segments, where the speed of estimation is ensured by the first segment, which employs rapid exponential forgetting of recent data.The second segment features a decline in the profile and marks the transition to the third segment, characterized by slow exponential forgetting to reduce the condition number of the information matrix using more distant data. Condition number reduction mitigates error propagation, thereby enhancing accuracy and stability. This approach facilitates the incorporation of a priori information regarding signal characteristics (i.e., the expected behavior of the signal) into the estimator. Recursive and computationally efficient algorithm with low rank updates based on new matrix inversion lemma for moving window associated with this regularization approach is developed. New algorithms significantly improve the approximation accuracy of low resolution daily temperature measurements obtained at the Stockholm Old Astronomical Observatory, thereby enhancing the reliability of temperature predictions.

</details>


### [59] [Optimal Neumann boundary and distributed control of the Westervelt equation with time-fractional attenuation](https://arxiv.org/abs/2511.15382)
*Vanja Nikolić,Belkacem Said-Houari*

Main category: math.OC

TL;DR: Optimal control of nonlinear acoustic waves governed by the Westervelt equation with time-fractional dissipation for medical ultrasound applications.


<details>
  <summary>Details</summary>
Motivation: To guide precise deposition of acoustic energy in medical ultrasound technologies like cancer therapy and targeted drug delivery.

Method: Extend well-posedness theory for time-fractional equations with inhomogeneous Neumann boundary control inputs, prove existence of globally optimal controls, analyze stability, and derive first-order necessary optimality conditions via adjoint equations.

Result: Established analytical framework for optimal control problems with time-fractional dissipation, including well-posedness, existence of optimal controls, and stability analysis.

Conclusion: Developed comprehensive theoretical foundation for optimal control of nonlinear acoustic waves with fractional dissipation, enabling precise energy deposition in medical ultrasound applications.

Abstract: Optimal control of nonlinear acoustic waves is relevant in many medical ultrasound technologies, ranging from cancer therapy to targeted drug delivery, where it can help guide the precise deposition of acoustic energy. In this work, we study Neumann boundary and distributed control problems for tracking a prescribed pressure field governed by the Westervelt equation with time-fractional dissipation. This model captures nonlinear ultrasonic wave propagation in biological media and accounts for the experimentally observed power-law attenuation. We begin by extending the existing well-posedness theory for time-fractional equations to include inhomogeneous Neumann boundary data used as control inputs, which requires constructing an appropriate data extension and regularization. Using these analytical results for the forward problem, we prove the existence of globally optimal controls and analyze the stability of the optimization problem with respect to perturbations in the target pressure field and to vanishing regularization parameters. Finally, we investigate the associated adjoint equation, which has state-dependent coefficients, and use it to derive first-order necessary optimality conditions.

</details>


### [60] [Generalized differentiation in Wasserstein space and application to multiagent control problem](https://arxiv.org/abs/2511.15455)
*Rossana Capuani,Antonio Marigonda,Marc Quincampoix*

Main category: math.OC

TL;DR: Introduces a concept of admissible variation for generalized differentiation in Wasserstein space, used to derive a comparison principle for viscosity solutions of HJB equations from multiagent optimal control.


<details>
  <summary>Details</summary>
Motivation: To address the intrinsic nonsmoothness in optimization problems in Wasserstein spaces by unifying various generalized differentiation concepts.

Method: Proposes an admissible variation concept that encompasses popular definitions as special cases, and applies it to derive comparison principles.

Result: Develops a framework that unifies different generalized differentiation approaches and enables comparison principles for viscosity solutions.

Conclusion: The proposed admissible variation concept provides a unified approach for handling nonsmoothness in Wasserstein space optimization and enables rigorous analysis of HJB equations in multiagent control.

Abstract: Several concepts of generalized differentiation in Wasserstein space have been proposed in order to deal with the intrinsic nonsmoothness arising in the context of optimization problems in Wasserstein spaces. In this paper we introduce a concept of admissible variation encompassing some of the most popular definitions as special cases, and using it to derive a comparison principle for viscosity solutions of an Hamilton Jacobi Bellman equation following from an optimal control of a multiagent systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [61] [Tensor-network approach to quantum optical state evolution beyond the Fock basis](https://arxiv.org/abs/2511.15295)
*Nikolay Kapridov,Egor Tiunov,Dmitry Chermoshentsev*

Main category: quant-ph

TL;DR: A tensor-network approach using matrix product states (MPS) efficiently simulates nonlinear optical quantum dynamics, achieving high compression ratios while maintaining accuracy in regimes where traditional methods fail.


<details>
  <summary>Details</summary>
Motivation: Modeling quantum evolution in nonlinear media is computationally demanding as resources grow rapidly with photon number and phase-space resolution, limiting the development of next-generation quantum technologies.

Method: Uses matrix product state (MPS) formalism to encode quantum states and operators in a compressed form, enabling direct numerical integration of the Schrödinger equation for simulating nonlinear optical systems.

Result: Successfully simulates degenerate spontaneous parametric down-conversion (SPDC), accurately reproducing energy conservation, pump depletion, and quadrature squeezing. Achieves compression ratios above 3×10³ for high-intensity pump fields (α=100) while preserving physical fidelity.

Conclusion: This tensor-network framework provides a scalable route to modeling multimode quantum light and nonlinear optical phenomena beyond the reach of traditional computational methods.

Abstract: Understanding the quantum evolution of light in nonlinear media is central to the development of next-generation quantum technologies. Yet modeling these processes remains computationally demanding, as the required resources grow rapidly with photon number and phase-space resolution. Here we introduce a tensor-network approach that efficiently captures the dynamics of nonlinear optical systems in a continuous-variable representation. Using the matrix product state (MPS) formalism, both quantum states and operators are encoded in a highly compressed form, enabling direct numerical integration of the Schrödinger equation. We demonstrate the method by simulating degenerate spontaneous parametric down-conversion (SPDC) and show that it accurately reproduces established theoretical benchmarks - energy conservation, pump depletion, and quadrature squeezing - even in regimes where conventional Fock-basis simulations become infeasible. For high-intensity pump fields ($α= 100$), the MPS representation achieves compression ratios above $3\cdot 10^3$ while preserving physical fidelity. This framework opens a scalable route to modeling multimode quantum light and nonlinear optical phenomena beyond the reach of traditional methods.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [62] [Decay order bound for mean curvature flow near compact singularities](https://arxiv.org/abs/2511.15297)
*Sourav Ghosh*

Main category: math.DG

TL;DR: The paper analyzes rescaled mean curvature flows with compact singularities of multiplicity one, proving uniform bounds on decay order and deriving a unique continuation result.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of mean curvature flows near singularities, particularly those of multiplicity one, and establish control over the decay properties of rescaled flows.

Method: Analysis of rescaled flows associated with mean curvature flows that develop compact singularities of multiplicity one, focusing on bounding the decay order.

Result: Proved that the decay order of such rescaled flows is uniformly bounded, leading to a unique continuation result.

Conclusion: The research provides important insights into singularity formation in mean curvature flows and establishes fundamental properties of rescaled flows near singularities.

Abstract: We consider the rescaled flow associated with a mean curvature flow that develops a compact singularity of multiplicity one. We prove that the ``decay order'' of such a rescaled flow is uniformly bounded. As a consequence, we prove a unique continuation result.

</details>


### [63] [An Information-Theoretic Route to Isoperimetric Inequalities via Heat Flow and Entropy Dissipation](https://arxiv.org/abs/2511.15356)
*Amandip Sangha*

Main category: math.DG

TL;DR: Information-theoretic proof of isoperimetric inequalities using entropy dissipation under heat flow, connecting diffusion geometry with information theory.


<details>
  <summary>Details</summary>
Motivation: To develop a unified information-theoretic framework for proving isoperimetric inequalities by viewing diffusion as a noisy information channel and measuring how mutual information about set membership decays over time.

Method: Using entropy dissipation under heat flow, analyzing how mutual information about set membership decays, with the decay rate determined by boundary measure. Extends to Riemannian manifolds with curvature-dimension conditions.

Result: New proof of Euclidean isoperimetric inequality with sharp constant, extension to Levy-Gromov and Gaussian isoperimetric results, quantitative and stability bounds from refined entropy inequalities.

Conclusion: The approach successfully connects geometric analysis and information theory, showing how entropy dissipation encodes the geometry of diffusion and boundaries, providing a unified analytic principle for various isoperimetric inequalities.

Abstract: We develop an information-theoretic approach to isoperimetric inequalities based on entropy dissipation under heat flow. By viewing diffusion as a noisy information channel, we measure how mutual information about set membership decays over time. This decay rate is shown to be determined by the boundary measure of the set, leading to a new proof of the Euclidean isoperimetric inequality with its sharp constant. The method extends to Riemannian manifolds satisfying curvature-dimension conditions, yielding Levy-Gromov and Gaussian isoperimetric results within a single analytic principle. Quantitative and stability bounds follow from refined entropy inequalities linking information loss to geometric rigidity. The approach connects geometric analysis and information theory, revealing how entropy dissipation encodes the geometry of diffusion and boundary.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [64] [Implicit Bias of the JKO Scheme](https://arxiv.org/abs/2511.14827)
*Peter Halmos,Boris Hanin*

Main category: stat.ML

TL;DR: The JKO scheme for Wasserstein gradient flow has an implicit bias at second order in step size η, which corresponds to minimizing a modified energy J^η that subtracts η/4 times the squared metric curvature of J.


<details>
  <summary>Details</summary>
Motivation: To better understand the remarkable properties of the JKO scheme (energy dissipation preservation, unconditional stability) that distinguish it from other first-order integrators for Wasserstein gradient flow.

Method: Characterize the implicit bias of the JKO scheme at second order in step size η, showing it approximates Wasserstein gradient flow on a modified energy functional J^η = J - (η/4)∫∥∇_g(δJ/δρ)∥²ρ(dx).

Result: The JKO scheme adds a deceleration in directions where the metric curvature of J changes rapidly, corresponding to canonical implicit biases: Fisher information for entropy, Fisher-Hyvärinen divergence for KL-divergence, and kinetic energy for Riemannian gradient descent.

Conclusion: JKO-Flow (Wasserstein gradient flow on J^η) is studied numerically to understand differences between minimizing J and J^η, including examples on Bures-Wasserstein space and 1D Langevin sampling.

Abstract: Wasserstein gradient flow provides a general framework for minimizing an energy functional $J$ over the space of probability measures on a Riemannian manifold $(M,g)$. Its canonical time-discretization, the Jordan-Kinderlehrer-Otto (JKO) scheme, produces for any step size $η>0$ a sequence of probability distributions $ρ_k^η$ that approximate to first order in $η$ Wasserstein gradient flow on $J$. But the JKO scheme also has many other remarkable properties not shared by other first order integrators, e.g. it preserves energy dissipation and exhibits unconditional stability for $λ$-geodesically convex functionals $J$. To better understand the JKO scheme we characterize its implicit bias at second order in $η$. We show that $ρ_k^η$ are approximated to order $η^2$ by Wasserstein gradient flow on a \emph{modified} energy \[ J^η(ρ) = J(ρ) - \fracη{4}\int_M \Big\lVert \nabla_g \frac{δJ}{δρ} (ρ) \Big\rVert_{2}^{2} \,ρ(dx), \] obtained by subtracting from $J$ the squared metric curvature of $J$ times $η/4$. The JKO scheme therefore adds at second order in $η$ a \textit{deceleration} in directions where the metric curvature of $J$ is rapidly changing. This corresponds to canonical implicit biases for common functionals: for entropy the implicit bias is the Fisher information, for KL-divergence it is the Fisher-Hyv{ä}rinen divergence, and for Riemannian gradient descent it is the kinetic energy in the metric $g$. To understand the differences between minimizing $J$ and $J^η$ we study \emph{JKO-Flow}, Wasserstein gradient flow on $J^η$, in several simple numerical examples. These include exactly solvable Langevin dynamics on the Bures-Wasserstein space and Langevin sampling from a quartic potential in 1D.

</details>
