<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 5]
- [math.AP](#math.AP) [Total: 15]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [math.OC](#math.OC) [Total: 2]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [physics.data-an](#physics.data-an) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [math.DG](#math.DG) [Total: 4]
- [physics.optics](#physics.optics) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Data-driven Model Reduction for Parameter-Dependent Matrix Equations via Operator Inference](https://arxiv.org/abs/2511.16033)
*Xuelian Wen,Qiuqi Li,Juan Zhang*

Main category: math.NA

TL;DR: A non-intrusive, data-driven surrogate modeling framework using Operator Inference for rapidly solving parameter-dependent matrix equations in many-query settings.


<details>
  <summary>Details</summary>
Motivation: To overcome the computational bottlenecks of intrusive methods in high-dimensional contexts and provide efficient solutions for parameter-dependent matrix equations.

Method: Reformulate matrix equations into structured polynomial parameter-dependent form, then construct reduced-order models via regression on solution snapshots without requiring expensive full-order operators.

Result: Numerical experiments confirm the accuracy and computational efficiency of the proposed approach.

Conclusion: The framework provides a scalable and practical solution for parameter-dependent matrix equations, demonstrating effectiveness in many-query settings.

Abstract: This work develops a non-intrusive, data-driven surrogate modeling framework based on Operator Inference (OpInf) for rapidly solving parameter-dependent matrix equations in many-query settings. Motivated by the requirements of the OpInf methodology, we reformulate the matrix equations into a structured representation that explicitly shows the parameter dependence in polynomial form. This reformulation is crucial for efficient model reduction. This approach constructs reduced-order models via regression on solution snapshots, bypassing the need for expensive full-order operators and thus overcoming the primary bottlenecks of intrusive methods in high-dimensional contexts. Numerical experiments confirm their accuracy and computational efficiency, demonstrating that our work is a scalable and practical solution for parameter-dependent matrix equations.

</details>


### [2] [Optimal error analysis of an interior penalty virtual element method for fourth-order singular perturbation problems](https://arxiv.org/abs/2511.16070)
*Fang Feng,Yuanyi Sun,Yue Yu*

Main category: math.NA

TL;DR: The paper shows that the Interior Penalty Virtual Element Method (IPVEM) achieves optimal and uniform error estimates for fourth-order singular perturbation problems, correcting previous suboptimal half-order convergence rates.


<details>
  <summary>Details</summary>
Motivation: Previous studies showed IPVEM had only half-order uniform convergence for fourth-order singular perturbation problems, which was suboptimal and needed improvement.

Method: The authors use the Interior Penalty Virtual Element Method (IPVEM) and conduct theoretical analysis supported by extensive numerical experiments.

Result: The IPVEM achieves optimal and uniform error estimates even with boundary layers, outperforming previous half-order convergence rates.

Conclusion: The proposed IPVEM is effective for singularly perturbed problems, with theoretical results validated by numerical experiments showing optimal convergence.

Abstract: In recent studies \cite{ZZ24, FY24}, the Interior Penalty Virtual Element Method (IPVEM) has been developed for solving a fourth-order singular perturbation problem, with uniform convergence established in the lowest-order case concerning the perturbation parameter. However, the resulting uniform convergence rate is only of half-order, which is suboptimal. In this work, we demonstrate that the proposed IPVEM in fact achieves optimal and uniform error estimates, even in the presence of boundary layers. The theoretical results are substantiated through extensive numerical experiments, which confirm the validity of the error estimates and highlight the method's effectiveness for singularly perturbed problems.

</details>


### [3] [Shallow neural network yields regularization for ill-posed inverse problems](https://arxiv.org/abs/2511.16171)
*Lan Wang,Qiao Zhu,Bangti Jin,Ye Zhang*

Main category: math.NA

TL;DR: Neural networks can approximate solutions to nonlinear ill-posed operator equations, with neuron count serving as regularization parameter to balance approximation and noise.


<details>
  <summary>Details</summary>
Motivation: To develop neural network methods for solving nonlinear ill-posed operator equations while handling measurement errors and providing theoretical guarantees.

Method: Proposed expanding neural network method as iterative regularization scheme, using neuron count as regularization parameter and iteration number.

Result: Small networks provide stable solutions for noisy data, while larger networks risk overfitting. Derived convergence rates under standard regularization theory assumptions.

Conclusion: Neural networks offer effective regularization for ill-posed problems, with network size controlling the trade-off between approximation accuracy and stability against noise.

Abstract: In this paper, we establish universal approximation theorems for neural networks applied to general nonlinear ill-posed operator equations. In addition to the approximation error, the measurement error is also taken into account in our error estimation. We introduce the expanding neural network method as a novel iterative regularization scheme and prove its regularization properties under different a priori assumptions about the exact solutions. Within this framework, the number of neurons serves as both the regularization parameter and iteration number. We demonstrate that for data with high noise levels, a small network architecture is sufficient to obtain a stable solution, whereas a larger architecture may compromise stability due to overfitting. Furthermore, under standard assumptions in regularization theory, we derive convergence rate results for neural networks in the context of variational regularization. Several numerical examples are presented to illustrate the robustness of the proposed neural network-based algorithms.

</details>


### [4] [Robust PAMPA Scheme in the DG Formulation on Unstructured Triangular Meshes: bound preservation, oscillation elimination, and boundary conditions](https://arxiv.org/abs/2511.16180)
*Rémi Abgrall,Yongle Liu*

Main category: math.NA

TL;DR: Improved PAMPA algorithm with globally continuous solutions, locally conservative scheme without mass matrix inversion, third-order accuracy for smooth solutions, bound preserving and non-oscillatory properties.


<details>
  <summary>Details</summary>
Motivation: To develop an improved version of PAMPA that maintains global continuity while being locally conservative, and to establish connections with discontinuous Galerkin methods for better implementation of boundary conditions and non-oscillatory behavior.

Method: Builds on PAMPA algorithm reinterpretation through connection with discontinuous Galerkin method for linear hyperbolic problems, defines a family of methods with rigorous boundary condition implementation, and develops bound preserving and non-oscillatory schemes.

Result: The scheme achieves third-order accuracy for smooth solutions as confirmed by truncation error analysis and numerical experiments, demonstrating bound preserving and non-oscillatory properties across various numerical benchmarks.

Conclusion: The improved PAMPA method successfully provides globally continuous solutions with local conservation, third-order accuracy, and robust bound preserving and non-oscillatory characteristics suitable for a wide range of applications.

Abstract: We propose an improved version of the PAMPA algorithm where the solution is sought as globally continuous. The scheme is locally conservative, and there is no mass matrix to invert. This method had been developed in a series of papers, see e.g \cite{Abgrall2024a} and the references therein. In \cite{Abgrall2025d}, we had shown the connection between PAMPA and the discontinuous Galerkin method, for the linear hyperbolic problem. Taking advantage of this reinterpretation, we use it to define a family of methods, show how to implement the boundary conditions in a rigorous manner. In addition, we propose a method that complements the bound preserving method developed in \cite{Abgrall2025d} in the sense that it is non oscillatory. A truncation error analysis is provided, it shows that the scheme should be third order accurate for smooth solutions. This is confirmed by numerical experiments. Several numerical examples are presented to show that the scheme is indeed bound preserving and non oscillatory on a wide range on numerical benchmarks.

</details>


### [5] [Numerical identification of the time-dependent coefficient in the heat equation with fractional Laplacian](https://arxiv.org/abs/2511.16238)
*Arshyn Altybay,Niyaz Tokmagambetov,Gulzat Nalzhupbayeva*

Main category: math.NA

TL;DR: This paper develops methods for identifying time-dependent source coefficients in fractional heat equations using nonlocal data, with focus on uniqueness, stability, and numerical implementation.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of identifying time-dependent source coefficients in fractional heat equations, which has applications in modeling anomalous diffusion processes where standard diffusion models are insufficient.

Method: Established a priori estimates for uniqueness and stability, proposed a fully implicit Crank-Nicolson finite-difference scheme, and developed an efficient noise-stable computation algorithm.

Result: The proposed method demonstrates accuracy and robustness in numerical experiments, even under noisy data conditions, validating the theoretical stability and convergence analysis.

Conclusion: The developed framework successfully solves the inverse source identification problem for fractional heat equations, providing both theoretical guarantees and practical computational tools for handling noisy measurement data.

Abstract: We address the inverse problem of identifying a time-dependent source coefficient in a one-dimensional heat equation with a fractional Laplacian subject to Dirichlet boundary conditions and an integral nonlocal data. An a priori estimate is established to ensure the uniqueness and stability of the solution. A fully implicit Crank-Nicolson (CN) finite-difference scheme is proposed and rigorously analysed for stability and convergence. An efficient noise-stable computation algorithm is developed and verified through numerical experiments, demonstrating accuracy and robustness under noisy data.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [6] [Shallow-water convergence of the intermediate long wave equation in $L^2$](https://arxiv.org/abs/2511.15905)
*Andreia Chapouto,Guopeng Li,Tadahiro Oh,Tengfei Zhao*

Main category: math.AP

TL;DR: This paper completes the convergence study of the intermediate long wave equation (ILW) to the Korteweg-de Vries equation (KdV) in both shallow-water and deep-water limits within the L²-framework on real line and circle geometries.


<details>
  <summary>Details</summary>
Motivation: To establish convergence of scaled ILW dynamics to KdV dynamics in the shallow-water limit at the L²-level, complementing recent deep-water convergence results and completing the well-posedness and convergence study of ILW.

Method: Uses complete integrability of ILW and normal form method. Employs Lax pair structure and perturbation determinant for ILW to establish weakly uniform equicontinuity in L², then treats low frequency part via infinite iteration of normal form reductions for KdV.

Result: Successfully establishes convergence of scaled ILW dynamics to KdV dynamics in the shallow-water limit at the L²-level, completing the well-posedness and convergence study of ILW on both real line and circle geometries.

Conclusion: The work completes the convergence framework for ILW by proving shallow-water convergence to KdV, complementing previous deep-water results, using integrability and normal form methods that apply uniformly to both geometries.

Abstract: We continue our study on the convergence issue of the intermediate long wave equation (ILW) on both the real line and the circle. In particular, we establish convergence of the scaled ILW dynamics to that of the Korteweg-de Vries equation (KdV) in the shallow-water limit at the $L^2$-level. Together with the recent work by the first three authors and D. Pilod (2024) on the deep-water convergence in $L^2$, this work completes the well-posedness and convergence study of ILW on both geometries within the $L^2$-framework. Our proof equally applies to both geometries and is based on the following two ingredients: the complete integrability of ILW and the normal form method. More precisely, by making use of the Lax pair structure and the perturbation determinant for ILW, recently introduced by Harrop-Griffths, Killip, and Vişan (2025), we first establish weakly uniform (in small depth parameters) equicontinuity in $L^2$ of solutions to the scaled ILW, providing a control on the high frequency part of solutions. Then, we treat the low frequency part by implementing a perturbative argument based on an infinite iteration of normal form reductions for KdV.

</details>


### [7] [Data-Driven Parameter Identification for Tumor Growth Models](https://arxiv.org/abs/2511.15940)
*Liu Liu,Yifei Wang,Qinyu Xu,Xiaoqian Xu*

Main category: math.AP

TL;DR: Using Physics-Informed Neural Networks (PINNs) to estimate parameters in nonlinear PDE tumor growth models, especially effective with scarce and noisy observational data.


<details>
  <summary>Details</summary>
Motivation: Accurate tumor growth modeling is crucial for understanding cancer progression and informing treatment strategies, particularly when dealing with limited and noisy real-world data.

Method: Adopted Physics-Informed Neural Networks (PINNs) to estimate parameters in nonlinear PDE tumor growth models, leveraging real-life lab data for data-driven modeling.

Result: Demonstrated the potential of deep learning tools for addressing data-driven tumor growth modeling in biology, showing PINNs' advantages with scarce and noisy observation data.

Conclusion: PINNs provide an effective approach for tumor growth parameter estimation, offering promising applications of deep learning in biological modeling with limited data availability.

Abstract: Modeling tumor growth accurately is essential for understanding cancer progression and informing treatment strategies. To estimate the parameters in the tumor growth model described by a nonlinear PDE, we adopt Physics-Informed Neural Networks (PINNs), which show advantages especially when the observation data is scarce and contains noise. With the help of real-life lab data, we have demonstrated the potential of applying deep learning tools to address data-driven modeling for tumor growth in biology.

</details>


### [8] [An $L^2$-quantitative global approximation for the Stokes initial-boundary value problem](https://arxiv.org/abs/2511.16079)
*Mitsuo Higaki*

Main category: math.AP

TL;DR: First quantitative Runge approximation theorem for 3D nonstationary Stokes system with explicit L²-estimates, improving on previous qualitative results.


<details>
  <summary>Details</summary>
Motivation: Address limitations of previous qualitative result [H.-Sueur, 2025]: bypass non-constructive Hahn-Banach theorem (preventing quantitative estimates) and extend from interior approximations to physically important initial-boundary value problem.

Method: Adapt modern quantitative framework [Rüland-Salo, 2019] to Stokes system by combining semigroup theory with quantitative approximation for associated resolvent problem.

Result: Established first quantitative Runge approximation theorem with explicit L²-estimates for 3D nonstationary Stokes system on bounded spatial domain.

Conclusion: Successfully developed constructive quantitative approximation theory for Stokes initial-boundary value problems, overcoming limitations of previous qualitative approaches.

Abstract: We establish the first quantitative Runge approximation theorem, with explicit $L^2$-estimates, for the 3d nonstationary Stokes system on a bounded spatial domain. This result addresses the two primary limitations of the qualitative result [H.-Sueur, 2025] obtained in collaboration with Franck Sueur: first, it bypasses the non-constructive Hahn-Banach theorem used in [H.-Sueur, 2025], precluding quantitative estimates; and second, it extends the scope of the theory from interior approximations to the physically important initial-boundary value problem. Our proof is founded on the modern quantitative framework of [Rüland-Salo, 2019], which we adapt to the Stokes system by combining semigroup theory with a quantitative approximation for the associated resolvent problem.

</details>


### [9] [Liouville--Type Results for Infinity Elliptic Equations Involving Gradient and Hardy--Hénon Nonlinearities](https://arxiv.org/abs/2511.16116)
*Tan-Dat Khuu,Trung-Hieu Huynh,Hoang-Hung Vo*

Main category: math.AP

TL;DR: Liouville-type properties for degenerate elliptic equations with fractional infinity Laplacian and nonlinear lower-order terms, establishing comparison principles and Lipschitz estimates for viscosity solutions.


<details>
  <summary>Details</summary>
Motivation: Extend Liouville theory from classical and normalized infinity Laplacian to fractional cases with Hamiltonian and Hardy-Hénon type nonlinearities.

Method: Weighted comparison principle, sharp local Lipschitz estimates, radial reduction, barrier constructions, and refined comparison arguments.

Result: Liouville theorems derived from growth conditions for bounded nonnegative solutions with power-type behavior, and partial results for exponential case under spatial growth assumptions.

Conclusion: Unified framework linking regularity, comparison principles, and Liouville-type phenomena for degenerate elliptic equations with fractional infinity Laplacians and nonlinear effects.

Abstract: In this paper we study Liouville-type properties for a class of degenerate elliptic equations driven by the fractional infinity Laplacian with nonlinear lower-order terms, \[ Δ_\infty^βu - c\,H(u,\nabla u) - λ\, f(|x|,u)=0 \qquad \text{in }\mathbb{R}^n, \] where $β\in[0,2]$, $Δ_\infty^β$ denotes the fractional infinity Laplace operator, and the nonlinearities $H$ and $f$ represent Hamiltonian and Hardy--Hénon type effects, respectively. We extend the Liouville theory for the classical and normalized infinity Laplacian by establishing a new weighted comparison principle together with sharp local Lipschitz estimates for viscosity solutions.
  Our Liouville theorems are derived from precise growth conditions for bounded nonnegative solutions when $f$ exhibits power-type behavior, i.e.\ $f\sim u^γ$. We also treat the exponential case $f\sim e^u$, for which the equation becomes strongly supercritical: under suitable assumptions on the growth of $u$ at spatial infinity, only partial Liouville-type conclusions can be obtained.
  The analysis relies on radial reduction, barrier constructions, and refined comparison arguments. Altogether, the results provide a unified framework linking regularity, comparison principles, and Liouville-type phenomena for degenerate elliptic equations involving fractional infinity Laplacians and nonlinear lower-order effects.

</details>


### [10] [On some uniqueness results](https://arxiv.org/abs/2511.16129)
*Patrizia Pucci,Jianjun Zhang,Xuexiu Zhong*

Main category: math.AP

TL;DR: This paper extends Serrin and Tang's results on overdetermined boundary value problems to the low-dimensional case (1≤N≤m, m>1), proving uniqueness of radial solutions under suitable assumptions on f, motivated by sharp Gagliardo-Nirenberg/Nash inequalities.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by extending previous results from Serrin and Tang to the low-dimensional case (N≤m), which was explicitly stated as challenging in their original work, and is also connected to sharp Gagliardo-Nirenberg/Nash inequalities.

Method: The paper studies radial solutions of an overdetermined boundary value problem involving the m-Laplacian operator in a ball domain, using a framework similar to Serrin and Tang but with significantly different proof details adapted for the low-dimensional case.

Result: The authors prove uniqueness of radial solutions when 1≤N≤m and m>1 under certain suitable assumptions on the nonlinearity f, successfully extending previous results to the previously unaddressed low-dimensional regime.

Conclusion: This work successfully overcomes the limitations identified by Serrin and Tang by developing new proof techniques that work for the low-dimensional case N≤m, providing a complete extension of their results across all dimensions.

Abstract: This paper aims to extend the results of Serrin and Tang in [{\it Indiana Univ. Math. J., 49 (2000), 897--923}] to the low-dimensional case. Specifically, the paper deals with the radial solutions of the following overdetermined problem $$ \begin{cases} -Δ_m u=f(u),\quad u>0~\hbox{in}~B_R,\\ u=\partial_νu=0~\hbox{on}~\partial B_R, \end{cases} $$ where $B_R$ is the open ball of $\mathbb{R}^N$ centered at 0 and with radius $R>0$. We prove uniqueness when $1\leq N\leq m$ {and $m>1$} under certain suitable assumptions on~$f$. Additionally, this work is motivated by the sharp Gagliardo-Nirenberg/Nash inequality. While the framework presented in this article is standard and closely resembles that of Serrin and Tang, the detail of our proofs differ significantly. It is important to note that Serrin and Tang explicitly stated (see Subsection~6.2 of their work) that {\it``the proofs in the present paper rely extensively on the assumption $N>m$ and cannot be extended easily to values $N\leq m$."}

</details>


### [11] [Liouville theorems for fully nonlinear elliptic equations on half spaces](https://arxiv.org/abs/2511.16152)
*Yuanyuan Lian*

Main category: math.AP

TL;DR: Proof of Liouville theorems for fully nonlinear uniformly elliptic equations on half spaces using boundary regularity and estimates.


<details>
  <summary>Details</summary>
Motivation: To establish Liouville theorems for fully nonlinear uniformly elliptic equations on half spaces, extending existing results.

Method: Using boundary pointwise regularity, Hopf type estimate, and Carleson type estimate for a short proof.

Result: Successfully proved two Liouville theorems for the specified equations on half spaces.

Conclusion: The paper provides a concise proof of Liouville theorems using boundary regularity techniques and estimates.

Abstract: In this note, we prove two Liouville theorems for fully nonlinear uniformly elliptic equations on half spaces. The main tools are the boundary pointwise regularity, the Hopf type estimate and the Carleson type estimate. Our new proof is rather short.

</details>


### [12] [Spreading Properties of a City-Road Reaction-Diffusion Model on One-Dimensional Lattice](https://arxiv.org/abs/2511.16157)
*Grégory Faye,Jean-Michel Roquejoffre,Min Zhao*

Main category: math.AP

TL;DR: A new PDE-ODE model for biological invasions on infinite 1D metric graphs, with logistic equations at vertices and diffusion on edges with Robin boundary conditions, showing asymptotic spreading speed and deriving a novel asymptotic model in fast diffusion regime.


<details>
  <summary>Details</summary>
Motivation: To describe biological invasions constrained on infinite homogeneous one-dimensional metric graphs, extending classical models to more complex network structures.

Method: Infinite PDE-ODE system with logistic equations at each vertex of the 1D lattice Z, connected by diffusion equations on edges with Robin boundary conditions at vertices.

Result: Established main properties of the system, characterized asymptotic spreading speed, and derived a novel asymptotic model in fast diffusion regime that exhibits similar propagation properties as classical discrete Fisher-KPP on Z.

Conclusion: The proposed model successfully describes biological invasions on metric graphs and provides insights into propagation dynamics, with the fast diffusion regime yielding behavior comparable to classical Fisher-KPP models.

Abstract: We propose and study a new model to describe biological invasions constrained on infinite homogeneous one dimensional metric graphs. Our model consists of an infinite PDE-ODE system where, at each vertex of the one-dimensional lattice $\mathbb{Z}$, we have a logistic equation, and connections between vertices are given by diffusion equations on the edges supplemented with Robin like boundary conditions at the vertices. We establish the main properties of the system and study the long time behavior of the solutions, especially by characterizing an asymptotic spreading speed for the system. In the fast diffusion regime, we derive a novel asymptotic model which exhibits similar propagation properties as the classical discrete Fisher-KPP on the one-dimensional lattice $\mathbb{Z}$.

</details>


### [13] [The Immersed Boundary Problem in 2-D: the Navier-Stokes Case](https://arxiv.org/abs/2511.16189)
*Jiajun Tong,Dongyi Wei*

Main category: math.AP

TL;DR: Existence, uniqueness, and regularity of mild solutions for 2-D immersed boundary problem with elastic string in Navier-Stokes fluid, including convergence to Stokes case, energy law, and global existence near equilibrium.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundation for the 2-D immersed boundary problem, which models elastic strings moving in viscous fluids, addressing existence, regularity, and stability questions.

Method: Introduce mild solution concept, prove existence/uniqueness for C^1 initial string configurations with well-stretched condition and L^p initial flow fields, analyze convergence to Stokes limit, establish energy law, and study global existence near equilibrium.

Result: Proved existence, uniqueness, and optimal regularity of mild solutions; derived convergence to Stokes case with error estimates; established energy law; showed global existence for data near equilibrium states.

Conclusion: The paper provides comprehensive mathematical analysis of 2-D immersed boundary problem, establishing well-posedness, regularity, stability properties, and convergence behavior for this fluid-structure interaction system.

Abstract: We study the immersed boundary problem in 2-D. It models a 1-D elastic closed string immersed and moving in a fluid that fills the entire plane, where the fluid motion is governed by the 2-D incompressible Navier-Stokes equation with a positive Reynolds number subject to a singular forcing exerted by the string. We introduce the notion of mild solutions to this system, and prove its existence, uniqueness, and optimal regularity estimates when the initial string configuration is $C^1$ and satisfies the well-stretched condition and when the initial flow field $u_0$ lies in $L^p(\mathbb{R}^2)$ with $p\in (2,\infty)$. A blow-up criterion is also established. When the Reynolds number is sent to zero, we show convergence in short time of the solution to that of the Stokes case of 2-D immersed boundary problem, with the optimal error estimates derived. We prove the energy law of the system when $u_0$ additionally belongs to $L^2(\mathbb{R}^2)$. Lastly, we show that the solution is global when the initial data is sufficiently close to an equilibrium state.

</details>


### [14] [Dynamics of Ideal Fluid Flows](https://arxiv.org/abs/2511.16254)
*Tarek M. Elgindi*

Main category: math.AP

TL;DR: Overview of key problems in incompressible Euler equations including least action principle, special solutions, solvability, singularity formation, and asymptotic behavior


<details>
  <summary>Details</summary>
Motivation: To systematically examine fundamental mathematical challenges and open problems in the theory of incompressible Euler equations

Method: Analytical discussion and theoretical examination of various mathematical aspects of the Euler equations

Result: Identification and categorization of major unsolved problems and research directions in Euler equation theory

Conclusion: The paper provides a comprehensive framework for understanding current challenges in incompressible fluid dynamics mathematics

Abstract: We will discuss various aspects of the incompressible Euler equation. We will discuss, in particular, problems related to the least action principle, the existence of special solutions, the problem of solvability, singularity formation, and asymptotic behavior.

</details>


### [15] [Asymptotic behavior and sharp estimates for spreading fronts in a cooperative system with free boundaries](https://arxiv.org/abs/2511.16300)
*Qian Qin,JinJing Jiao,Zhiguo Wang,Hua Nie*

Main category: math.AP

TL;DR: Study of cooperative species invasion using reaction-diffusion system with two free boundaries, showing spreading-vanishing dichotomy and determining asymptotic spreading speeds.


<details>
  <summary>Details</summary>
Motivation: To understand the long-term dynamics of cooperative species invasion and expanding fronts in reaction-diffusion systems with free boundaries.

Method: Analysis of reaction-diffusion system with two free boundaries using semi-wave system approach to study spreading behavior and front dynamics.

Result: System exhibits spreading-vanishing dichotomy; in spreading case, asymptotic spreading speed determined and solution converges to semi-wave solution as time increases.

Conclusion: Provides deeper understanding of cooperative species dynamics in free boundary reaction-diffusion systems, with implications for biological invasion modeling.

Abstract: This paper investigates the dynamics of a reaction-diffusion system with two free boundaries, modeling the invasion of two cooperative species, where the free boundaries represent expanding fronts. We first analyze the long-term behavior of the system, showing that it follows a spreading-vanishing dichotomy: the two species either spread across the entire region or eventually die out. In the case of spreading, we determine the asymptotic spreading speed of the fronts by using a semi-wave system and provide sharp estimates for the moving fronts. Additionally, we show that the solution to the system converges to the corresponding semi-wave solution as time tends to infinity. These results contribute to a deeper understanding of the long-term dynamics of cooperative species in reaction-diffusion systems with free boundaries.

</details>


### [16] [The analysis of resonant frequencies and blow-up estimates of close-to-touching subwavelength resonators in the two-dimensional Helmholtz system](https://arxiv.org/abs/2511.16387)
*Hongjie Dong,Hongjie Li,Longjuan Xu*

Main category: math.AP

TL;DR: Analysis of wave scattering by closely spaced high-contrast inclusions in 2D Helmholtz equation reveals two sub-wavelength resonant modes with distinct asymptotic behaviors, differing from 3D results, and quantifies gradient blow-up rates between resonators.


<details>
  <summary>Details</summary>
Motivation: To understand wave scattering behavior in systems with closely spaced high-contrast inclusions, particularly focusing on resonant modes and field localization effects that differ from three-dimensional cases.

Method: Modeling wave scattering using the two-dimensional Helmholtz equation for a pair of closely spaced inclusions with high contrast physical parameters in a homogeneous medium.

Result: Identified two sub-wavelength resonant modes with distinct leading-order asymptotic frequency behaviors, and quantified gradient blow-up rates for the wave field localized between the two resonators.

Conclusion: The 2D Helmholtz configuration exhibits unique resonant behavior that significantly differs from 3D settings, with important implications for understanding wave localization and gradient enhancement in closely spaced resonator systems.

Abstract: In this paper, we investigate wave scattering by a pair of closely spaced inclusions embedded in a homogeneous medium, characterized by a high contrast physical parameters. The system is modeled by the two-dimensional Helmholtz equation. We show that this configuration exhibits two sub-wavelength resonant modes, whose frequencies display distinct leading-order asymptotic behaviors. These findings differ significantly from those in the three-dimensional Helmholtz setting. Furthermore, we provide a quantitative analysis of the gradient blow-up rates for the wave field localized between the two resonators.

</details>


### [17] [Mosco convergence framework for singular limits of gradient flows on Hilbert spaces with applications](https://arxiv.org/abs/2511.16486)
*Yoshikazu Giga,Michał Łasica,Piotr Rybka*

Main category: math.AP

TL;DR: The paper introduces connecting operators to generalize Mosco convergence for gradient flows on different Hilbert spaces, proving convergence results with applications to thin domains, dynamic boundary conditions, and discrete-to-continuum limits.


<details>
  <summary>Details</summary>
Motivation: To establish a framework for analyzing convergence of gradient flows defined on different Hilbert spaces, which is relevant for various applications where the underlying spaces change.

Method: Introduce connecting operators to link different Hilbert spaces, generalize Mosco convergence of functionals to this setting, and prove convergence results for gradient flows.

Result: Successfully proved convergence results for gradient flows using the generalized Mosco convergence framework with connecting operators.

Conclusion: The proposed framework with connecting operators effectively handles convergence of gradient flows across different Hilbert spaces, with demonstrated applicability to important mathematical problems.

Abstract: We consider the question of convergence of a sequence of gradient flows defined on different Hilbert spaces. In order to give meaning to this idea, we introduce a notion of connecting operators. This permits us to generalize the concept of Mosco convergence of functionals to our present setting, and state a desired convergence result for gradient flows, which we then prove. We present a variety of examples, including thin domains, dynamic boundary conditions, and discrete-to-continuum limits.

</details>


### [18] [Regularity for elliptic equations with monomial weights](https://arxiv.org/abs/2511.16516)
*Gabriele Cora,Gabriele Fioravanti,Francesco Pagliarin,Stefano Vita*

Main category: math.AP

TL;DR: The paper studies regularity properties of solutions to elliptic equations that are degenerate or singular along orthogonal hyperplanes, proving C^{0,α} and C^{1,α} estimates up to corners using regularization, blow-up arguments, and Liouville theorems.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity of solutions to degenerate elliptic equations with weights that are products of powers of distance functions to orthogonal hyperplanes, particularly near corners where hyperplanes intersect.

Method: Uses a regularization-approximation procedure, blow-up argument, and Liouville theorems to analyze the degenerate elliptic equations with monomial weight terms.

Result: Proves C^{0,α} and C^{1,α} regularity estimates for solutions up to corners formed by intersections of hyperplanes, and provides smoothness results for isotropic homogeneous equations.

Conclusion: The analysis provides regularity results for degenerate elliptic equations with monomial weights, with applications to Caffarelli-Kohn-Nirenberg inequalities involving such weights.

Abstract: We study regularity properties for solutions to elliptic equations that are degenerate or singular along orthogonal hyperplanes. The degenerate ellipticity is carried out by a weight term which is the monomial product of different powers of the distance functions to each hyperplane; that is, given the space dimension $d\geq2$, the number of orthogonally crossing hyperplanes $1\leq n\leq d$ and the generic variable point $z=(x,y)\in\mathbb R^{d-n}\times\mathbb R^n$, then the weight is given by $ω(y)=\prod_{i=1}^ny_i^{a_i}$ with $a_i>-1$, $y_i=\mathrm{dist}(z,Σ_i)$ and $Σ_i=\{y_i=0\}$. We prove $C^{0,α}$ and $C^{1,α}$ estimates up to the corners formed by the intersections of two or more hyperplanes, for solutions of the conormal problem with variable coefficients. This is done by a regularization-approximation procedure, a blow-up argument and Liouville theorems. Finally, we provide smoothness of solutions when the equation is isotropic and homogeneous, and we show an application to Caffarelli-Kohn-Nirenberg inequalities with monomial weights.

</details>


### [19] [Non-isoparametric Serrin domains of $\mathbb{S}^3$ with connected toric boundary](https://arxiv.org/abs/2511.16531)
*Andrea Bisterzo,Shigeru Sakaguchi*

Main category: math.AP

TL;DR: The paper proves the existence of two types of non-spherical Serrin domains in S³ with connected boundaries, neither geodesic spheres nor Clifford tori, through bifurcation from symmetric solutions.


<details>
  <summary>Details</summary>
Motivation: To investigate overdetermined torsion problems in curved spaces and show that Serrin's rigidity result (which holds in Euclidean space) fails in the presence of curvature, by finding nontrivial Serrin domains in S³.

Method: Uses the Crandall-Rabinowitz bifurcation theorem to construct branches of non-radial solutions bifurcating from a family of radial solutions, employing an implicit construction approach.

Result: Established existence of two distinct types of Serrin domains in S³ (small and large volume) with connected boundaries that are neither geodesic spheres nor Clifford tori.

Conclusion: The results demonstrate that curvature can break the rigidity of Serrin-type results, revealing new geometric configurations for the torsion problem in the three-dimensional sphere.

Abstract: We investigate the overdetermined torsion problem
  $\begin{cases} -Δu = 1 & \text{in}\ Ω\\ u=0 & \text{on}\ \partial Ω\\ \frac{\partial u}{\partial ν}=\text{const.} & \text{on}\ \partial Ω, \end{cases}$
  where $Ω$ is a smooth Riemannian domain. Domains admitting a solution to this problem are called \textit{Serrin domains}, after the celebrated work of Serrin \cite{Se71}, where is proved that in $\mathbb{R}^n$ such domains are geodesic balls. In the present paper we establish the existence of two distinct types of Serrin domains of $\mathbb{S}^3$, respectively of small and large volume, each of whose boundary is connected and is neither isometric to a geodesic sphere nor to a Clifford torus. These domains arise as nontrivial perturbations of some classical symmetric solutions to the same problem. Our approach relies on an implicit construction based on the Crandall-Rabinowitz bifurcation theorem, which allows us to detect branches of non-radial solutions bifurcating from a family of radial ones. The resulting examples highlight new geometric configurations of the torsion problem in the three-dimensional sphere, providing another proof of the fact that the rigidity of Serrin-type results can fail in the presence of curvature.

</details>


### [20] [A critical Hardy-Rellich inequality](https://arxiv.org/abs/2511.16537)
*Hernán Castro*

Main category: math.AP

TL;DR: Proves a critical Hardy-Rellich type inequality showing that the gradient of u/|x| in L^N norm is bounded by the Laplacian of u in L^N norm for smooth compactly supported functions on R^N excluding the origin.


<details>
  <summary>Details</summary>
Motivation: To establish a critical version of Hardy-Rellich inequalities, which are fundamental in analysis and PDEs, particularly for functions defined on R^N excluding the origin.

Method: Mathematical proof using functional analysis and inequalities, specifically working with smooth compactly supported functions on R^N\{0} to derive the inequality.

Result: Proves the existence of a positive constant C_N such that the inequality holds for all N≥1 and any u in the specified function space.

Conclusion: Successfully establishes a critical Hardy-Rellich type inequality that provides an important bound between the gradient of u/|x| and the Laplacian of u in L^N norms.

Abstract: In this work, we prove a critical version of a Hardy-Rellich type inequality. We show that for $N\geq 1$ there exists a constant $C_N>0$ such that \[ \int_{\mathbb R^N}\left|\nabla\left(\frac{u(x)}{|x|}\right)\right|^N\,\mathrm{d}x\leq C_N\int_{\mathbb R^N}\left|Δu(x)\right|^N\,\mathrm{d}x, \] for any $u\in C^\infty_c(\mathbb R^N\setminus\left\{0\right\})$.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [21] [Implicit and explicit treatments of model error in numerical simulation](https://arxiv.org/abs/2511.15934)
*Danny Smyl*

Main category: physics.comp-ph

TL;DR: Review of model error approximation techniques for numerical simulations, covering Bayesian frameworks, probabilistic methods, machine learning approaches, and error estimators to improve predictive accuracy and uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Numerical simulations inherently suffer from model errors from unmodeled physics, idealizations, and discretization, which affect reliability in inverse problems, data assimilation, and predictive modeling.

Method: Survey of major approaches including Bayesian approximation error framework, embedded internal error models, probabilistic numerical methods, model discrepancy modeling, machine-learning-based correction, multi-fidelity strategies, and various error estimators.

Result: Comprehensive overview of techniques developed over two decades that improve predictive performance and uncertainty quantification across engineering design and Earth-system science applications.

Conclusion: These methods provide practical approaches for incorporating error treatment into PDE solvers, inverse problem workflows, and data assimilation systems, emphasizing the distinction between implicit and explicit error treatment strategies.

Abstract: Numerical simulations of physical systems invariably suffer from model errors stemming from unmodeled physics, idealizations, and discretization. This article provides a review of techniques developed in the past two decades to approximate and account for these model errors, both implicitly and explicitly. Beginning from fundamental definitions of model-form versus numerical error, we frame model error in inverse problems, data assimilation, and predictive modeling contexts. We then survey major approaches: the Bayesian approximation error framework for implicit error quantification, embedded internal error models for structural uncertainty, probabilistic numerical methods for discretization uncertainty, model discrepancy modeling in Bayesian calibration and its recent extensions, machine-learning-based discrepancy correction, multi-fidelity and hybrid modeling strategies, as well as residual-based, variational, and adjoint-driven error estimators. Throughout, we emphasize conceptual underpinnings of implicit versus explicit error treatment and highlight how these methods improve predictive performance and uncertainty quantification in practical applications ranging from engineering design to Earth-system science. Each section provides an overview of key developments with an extensive list of references to facilitate further reading. The review is written for practitioners of large-scale computational physics and engineering simulation, emphasizing how these methods can be incorporated into PDE solvers, inverse problem workflows, and data assimilation systems.

</details>


### [22] [A physics-inspired momentum-based gradient method](https://arxiv.org/abs/2511.16441)
*Jianing Zhang,Rumei Liu*

Main category: physics.comp-ph

TL;DR: A nonlinear momentum method inspired by non-Newtonian mechanical systems that improves convergence in gradient optimization algorithms through generalized kinetic energy and nonlinear damping.


<details>
  <summary>Details</summary>
Motivation: To enhance convergence performance of momentum-based gradient optimization by drawing analogies from non-Newtonian mechanical systems where conventional momentum corresponds to quadratic kinetic energy and linear damping.

Method: Constructs generalized optimization dynamics by extending kinetic energy formulation and incorporating nonlinear damping. Uses anharmonic kinetic energy for inertial effects of accumulated gradients and nonlinear damping for flexible momentum control.

Result: Numerical experiments show faster convergence and higher robustness compared to classical momentum algorithms, with strong performance on nonconvex objectives particularly suitable for inverse photonic design.

Conclusion: Dynamical systems from physics provide valuable insights for developing efficient optimization methods, as demonstrated by the improved performance of this nonlinear momentum approach.

Abstract: In this work, a nonlinear momentum method is introduced to improve the convergence performance of momentum-based gradient optimization algorithms. The method is motivated by the dynamics of non-Newtonian mechanical systems, where conventional momentum schemes can be interpreted as a dynamical model with quadratic kinetic energy and linear damping. Based on this analogy, a generalized optimization dynamics is constructed by extending the kinetic energy formulation and incorporating a nonlinear damping term. An anharmonic kinetic energy function can be employed to represent the inertial effect of accumulated gradient information during the iterations, while the nonlinear damping mechanism enables a more flexible control of the momentum contribution along the convergence trajectory. Numerical experiments indicate that the method exhibits faster convergence and higher robustness compared to classical momentum algorithms. Moreover, its strong performance on nonconvex objectives makes it particularly suitable for inverse photonic design problems. The results suggest that dynamical systems from physics can provide a view towards the development of efficient optimization methods.

</details>


### [23] [Deep Learning Framework for Enhanced Neutrino Reconstruction of Single-line Events in the ANTARES Telescope](https://arxiv.org/abs/2511.16614)
*A. Albert,S. Alves,M. André,M. Ardid,S. Ardid,J. -J. Aubert,J. Aublin,B. Baret,S. Basa,Y. Becherini,B. Belhorma,F. Benfenati,V. Bertin,S. Biagi,J. Boumaaza,M. Bouta,M. C. Bouwhuis,H. Brânzaş,R. Bruijn,J. Brunner,J. Busto,B. Caiffi,D. Calvo,S. Campion,A. Capone,F. Carenini,J. Carr,V. Carretero,T. Cartraud,S. Celli,L. Cerisy,M. Chabab,R. Cherkaoui El Moursli,T. Chiarusi,M. Circella,J. A. B. Coelho,A. Coleiro,R. Coniglione,P. Coyle,A. Creusot,A. F. Díaz,B. De Martino,C. Distefano,I. Di Palma,C. Donzaud,D. Dornic,D. Drouhin,T. Eberl,A. Eddymaoui,T. van Eeden,D. van Eijk,S. El Hedri,N. El Khayati,A. Enzenhöfer,P. Fermani,G. Ferrara,F. Filippini,L. Fusco,S. Gagliardini,J. García-Méndez,C. Gatius Oliver,P. Gay,N. Geißelbrecht,H. Glotin,R. Gozzini,R. Gracia Ruiz,K. Graf,C. Guidi,L. Haegel,H. van Haren,A. J. Heijboer,Y. Hello,L. Hennig,J. J. Hernández-Rey,J. Hößl,F. Huang,G. Illuminati,B. Jisse-Jung,M. de Jong,P. de Jong,M. Kadler,O. Kalekin,U. Katz,A. Kouchner,I. Kreykenbohm,V. Kulikovskiy,R. Lahmann,M. Lamoureux,A. Lazo,D. Lefèvre,E. Leonora,G. Levi,S. Le Stum,S. Loucatos,J. Manczak,M. Marcelin,A. Margiotta,A. Marinelli,J. A. Martínez-Mora,P. Migliozzi,A. Moussa,R. Muller,S. Navas,E. Nezri,B. Ó Fearraigh,E. Oukacha,A. M. Păun,G. E. Păvălaş,S. Peña-Martínez,M. Perrin-Terrin,P. Piattelli,C. Poiré,V. Popa,T. Pradier,N. Randazzo,D. Real,G. Riccobene,A. Romanov,A. Sánchez Losa,A. Saina,F. Salesa Greus,D. F. E. Samtleben,M. Sanguineti,P. Sapienza,F. Schüssler,J. Seneca,M. Spurio,Th. Stolarczyk,M. Taiuti,Y. Tayalati,B. Vallage,G. Vannoye,V. Van Elewyck,S. Viola,D. Vivolo,J. Wilms,S. Zavatarelli,A. Zegarelli,J. D. Zornoza,J. Zúñiga*

Main category: physics.comp-ph

TL;DR: N-Fit is a neural network algorithm that improves reconstruction of single-line neutrino events in ANTARES telescope, using deep learning with convolutional layers, mixture density outputs, and transfer learning for direction, position, energy estimation, and event classification.


<details>
  <summary>Details</summary>
Motivation: To improve reconstruction of low-energy neutrino events (~100 GeV) detected by single lines in ANTARES telescope, which traditional χ²-fit methods struggle with, particularly for azimuthal angle predictions and energy estimation.

Method: Deep neural network with dedicated branches for track and shower topologies, using convolutional layers, mixture density output layers, and transfer learning. Divides reconstruction into spatial estimation (direction/position) and energy inference, then combines for event classification.

Result: Significant refinement of zenithal angle estimation, reliable azimuthal angle predictions (previously unattainable), improved energy estimation using transfer learning, and reduced mean/median absolute errors across all reconstructed parameters in Monte Carlo simulations and data.

Conclusion: N-Fit demonstrates significant improvements over traditional methods, highlighting its potential for advancing multimessenger astrophysics and probing fundamental physics beyond Standard Model using single-line ANTARES events.

Abstract: We present the $N$-fit algorithm designed to improve the reconstruction of neutrino events detected by a single line of the ANTARES underwater telescope, usually associated with low energy neutrino events ($\sim$ 100 GeV). $N$-Fit is a neural network model that relies on deep learning and combines several advanced techniques in machine learning --deep convolutional layers, mixture density output layers, and transfer learning. This framework divides the reconstruction process into two dedicated branches for each neutrino event topology --tracks and showers-- composed of sub-models for spatial estimation --direction and position-- and energy inference, which later on are combined for event classification. Regarding the direction of single-line events, the $N$-Fit algorithm significantly refines the estimation of the zenithal angle, and delivers reliable azimuthal angle predictions that were previously unattainable with traditional $χ^2$-fit methods. Improving on energy estimation of single-line events is a tall order; $N$-Fit benefits from transfer learning to efficiently integrate key characteristics, such as the estimation of the closest distance from the event to the detector. $N$-Fit also takes advantage from transfer learning in event topology classification by freezing convolutional layers of the pretrained branches. Tests on Monte Carlo simulations and data demonstrate a significant reduction in mean and median absolute errors across all reconstructed parameters. The improvements achieved by $N$-Fit highlight its potential for advancing multimessenger astrophysics and enhancing our ability to probe fundamental physics beyond the Standard Model using single-line events from ANTARES data.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [24] [Asymptotic-preserving semi-implicit finite volume scheme for Extended Magnetohydrodynamics](https://arxiv.org/abs/2511.15937)
*Yi Han Toh,Joshua Dolence,Karthik Duraisamy*

Main category: physics.plasm-ph

TL;DR: A finite volume scheme for extended magnetohydrodynamics (XMHD) that handles ideal, resistive, and Hall MHD limits using modified equations to maintain ideal MHD Riemann solvers and constrained transport, with semi-implicit time integration for stability.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical scheme capable of solving XMHD equations accurately across different MHD regimes while maintaining computational efficiency and stability, particularly addressing numerical stiffness from electron inertia and displacement current.

Method: Reformulate XMHD equations to use ideal MHD Riemann solvers and constrained transport; implement semi-implicit finite volume scheme with relaxation system approach; use 2nd-order Runge-Kutta time stepping with operator splitting; add density-dependent slope limiter for stability at low densities.

Result: The algorithm asymptotically approaches ideal MHD limit naturally and shows promising performance in resistive and Hall MHD regimes, verified against published test problems across all three MHD limits.

Conclusion: The developed finite volume scheme successfully handles extended MHD equations while maintaining compatibility with existing ideal MHD infrastructure, providing accurate results across ideal, resistive, and Hall MHD regimes with improved numerical stability.

Abstract: A Finite Volume (FV) scheme is developed for solving the extended magnetohydrodynamic (XMHD) equations, yielding accurate results in the ideal, resistive, and Hall MHD limits. This is accomplished by first re-writing the XMHD equations such that it allows the algorithm to retain the use of ideal MHD Riemann solvers and the constrained transport method to preserve divergence-free magnetic fields. Incorporation of electron inertia and displacement current introduces additional numerical stiffness which motivates a semi-implicit FV scheme that re-formulates the XMHD model as a relaxation system. The equations are then advanced in time using an explicit 2nd-order Runge-Kutta scheme with operator splitting applied to the implicit source term updates at each sub-stage. For additional numerical stability, density-dependent slope limiter is implemented to increase flux diffusivity at low density where non-ideal effects become significant. The algorithm is subsequently implemented on Artemis, a multifluid radiation hydrodynamics code built on Parthenon framework for high-performance computing (HPC) and adaptive mesh refinement (AMR) support. As the new algorithm retains many aspects of the ideal MHD formulations, it asymptotes naturally to the ideal MHD limit. Moreover, it shows promising results at the resistive and Hall MHD limits. This is verified against published test problems for ideal, resistive and Hall MHD.

</details>


### [25] [Effects of Multi-scale Coupling on Particle Acceleration and Energy Partition in Magnetic Reconnection](https://arxiv.org/abs/2511.15988)
*Alexander Velberg,Adam Stanier,Xiaocan Li,Fan Guo,William Daughton,Nuno F. Loureiro*

Main category: physics.plasm-ph

TL;DR: Particle-in-cell simulations show that in large-scale magnetic reconnection events, secondary current sheets and downstream turbulence dominate energy dissipation, creating additional particle acceleration channels that modify non-thermal particle spectra.


<details>
  <summary>Details</summary>
Motivation: To understand how kinetic and macroscopic scales interact during magnetic reconnection in strongly-magnetized relativistic pair plasmas, particularly focusing on energy dissipation mechanisms.

Method: Used particle-in-cell simulations to study magnetic island coalescence in strongly-magnetized relativistic pair plasma regime, comparing with isolated force-free current sheet simulations.

Result: For large systems, secondary current sheet formation and downstream turbulence from reconnection outflows dominate global energy dissipation, activating additional particle acceleration channels that significantly modify particle energy spectra.

Conclusion: Energy dissipation in large-scale magnetic reconnection is causally connected but spatially and temporally decoupled from primary reconnection sites, with secondary processes creating important particle acceleration mechanisms.

Abstract: The interplay between kinetic and macroscopic scales during magnetic reconnection is investigated using particle-in-cell simulations of magnetic island coalescence in the strongly-magnetized, relativistic pair plasma regime. For large system sizes, secondary current sheet formation and downstream turbulence driven by the reconnection outflows dominate the global energy dissipation so that it is causally connected, but spatially and temporally de-coupled from the primary reconnecting current sheet. When compared to simulations of an isolated, force-free current sheet, these dynamics activate additional particle acceleration channels which are responsible for a significant population of the non-thermal particles, modifying the particle energy spectra.

</details>


### [26] [Algorithms and optimizations for global non-linear hybrid fluid-kinetic finite element stellarator simulations](https://arxiv.org/abs/2511.16412)
*Luca Venerando Greco*

Main category: physics.plasm-ph

TL;DR: A novel globally coupled projection scheme in JOREK framework enables accurate hybrid fluid-kinetic modeling for stellarator plasmas using FFT acceleration and 3D R-Tree indexing.


<details>
  <summary>Details</summary>
Motivation: Predictive modeling of stellarator plasmas faces computational challenges due to non-axisymmetric geometry that couples toroidal Fourier modes, requiring accurate simulation of particle species not captured by fluid models.

Method: Developed a globally coupled projection scheme using Fast Fourier Transform acceleration and 3D R-Tree spatial indexing to handle kinetic marker transfer to fluid grid across all toroidal harmonics simultaneously.

Result: The coupled scheme achieves theoretically anticipated spectral convergence on Wendelstein 7-X geometries, significantly outperforming uncoupled approaches.

Conclusion: Provides a validated, high-fidelity computational tool crucial for predictive analysis and optimization of next-generation stellarator designs.

Abstract: Predictive modeling of stellarator plasmas is crucial for advancing nuclear fusion energy, yet it faces unique computational difficulties. One of the main challenges is accurately simulating the dynamics of specific particle species that are not well captured by fluid models, which necessitates the use of hybrid fluid-kinetic models. The non-axisymmetric geometry of stellarators fundamentally couples the toroidal Fourier modes, in contrast to what happens in tokamaks, requiring different numerical and computational treatment.
  This work presents a novel, globally coupled projection scheme inside the JOREK finite element framework. The approach ensures a self-consistent and physically accurate transfer of kinetic markers to the fluid grid, effectively handling the complex 3D mesh by constructing and solving a unified linear system that encompasses all toroidal harmonics simultaneously. To manage the computational complexity of this coupling, the construction of the system's matrix is significantly accelerated using the Fast Fourier Transform (FFT). The efficient localization of millions of particles is made possible by implementing a 3D R-Tree spatial index, which supports this projection and ensures computational tractability at scale.
  On realistic Wendelstein 7-X stellarator geometries, the fidelity of the framework is rigorously shown. In sharp contrast to the uncoupled approaches' poor performance, quantitative convergence tests verify that the coupled scheme attains the theoretically anticipated spectral convergence.
  This study offers a crucial capability for the predictive analysis and optimization of next-generation stellarator designs by developing a validated, high-fidelity computational tool.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [27] [A Fast Relax-and-Round Approach to Unit Commitment for Data Center Own Generation](https://arxiv.org/abs/2511.16420)
*Shaked Regev,Eve Tsybina,Slaven Peles*

Main category: math.OC

TL;DR: Proposes a relaxed unit commitment formulation that allows fractional generator states instead of binary decisions, enabling fast continuous solvers and GPU acceleration for large-scale systems with thousands of generators.


<details>
  <summary>Details</summary>
Motivation: Data centers increasingly use their own generation alongside utility power, creating systems with thousands of flexible generators where traditional mixed-integer unit commitment becomes computationally intractable.

Method: Relaxes binary commitment decisions to allow generators to be fractionally on, uses continuous solvers, then applies rounding to get feasible unit commitment solutions.

Result: Reduced solution time from 10 hours to less than 1 second for a 276-unit system with minor accuracy loss, and scales to tens of thousands of generators.

Conclusion: The approach enables solving large-scale unit commitment problems at the scale of major North American interconnections, with most computation being parallel and GPU-compatible for future acceleration.

Abstract: The rapid growth of data centers increasingly requires data center operators to "bring own generation" to complement the available utility power plants to supply all or part of data center load. This practice sharply increases the number of generators on the bulk power system and shifts operational focus toward fuel costs rather than traditional startup and runtime constraints. Conventional mixed-integer unit commitment formulations are not well suited for systems with thousands of flexible, fast-cycling units. We propose a unit commitment formulation that relaxes binary commitment decisions by allowing generators to be fractionally on, enabling the use of algorithms for continuous solvers. We then use a rounding approach to get a feasible unit commitment. For a 276-unit system, solution time decreases from 10 hours to less than a second, with minor accuracy degradation. Our approach scales with no issues to tens of thousands of generators, which allows solving problems on the scale of the major North America interconnections. The bulk of computation is parallel and GPU compatible, enabling further acceleration in future work.

</details>


### [28] [A novel way of computing the shape derivative for a class of non-smooth PDEs and its impact on deriving necessary conditions for locally optimal shapes](https://arxiv.org/abs/2511.16127)
*Livia Betz*

Main category: math.OC

TL;DR: Derives necessary conditions for locally optimal shapes in non-smooth PDE-constrained design problems using functional variational approach, avoiding domain extensions or PDE approximations.


<details>
  <summary>Details</summary>
Motivation: To handle shape optimization problems governed by non-smooth PDEs where traditional methods fail due to lack of differentiability, requiring new sensitivity analysis techniques.

Method: Uses functional variational approach (FVA) to transform geometric optimization into optimal control problems, with novel sensitivity analysis that computes directional derivatives of states via functional variations.

Result: Developed a new method for computing shape derivatives that handles pointwise observations, state derivatives on observation sets, and distributed observation terms without domain extensions.

Conclusion: Established connection between locally optimal shapes and control problem minimizers, providing necessary conditions for locally optimal shapes in non-smooth settings using directional differentiability.

Abstract: We derive necessary conditions for locally optimal shapes of a design problem governed by a non-smooth PDE. The main particularity of the state system is the lack of differentiability of the nonlinearity. We work in the framework of the functional variational approach (FVA), which has the capacity to transfer geometric optimization problems into optimal control problems, the set of admissible shapes being parametrized by a large class of continuous mappings. In the FVA setting, we introduce a sensitivity analysis technique that is novel even for smooth PDEs. We emphasize that we do not resort to extensions on the hold-all domain or any kind of approximation of the original PDE. The computation of the directional derivative of the state w.r.t. functional variations results in a new way of computing the shape derivative. The presented approach allows us to handle in the objective pointwise observation and derivatives of the state on an observation set as well as distributed observation terms. In addition, we introduce the concept of locally optimal shapes and we put into evidence its connection to locally minimizers of the corresponding control problem. With directional differentiability results for the control-to-state map at our disposal, we can then state necessary conditions for locally optimal shapes in general non-smooth settings.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [29] [Age-structured model of dengue transmission dynamics with time-varying parameters, and its application to Brazil](https://arxiv.org/abs/2511.16179)
*Ihtisham Ul Haq,Serge Richard*

Main category: q-bio.PE

TL;DR: Developed an age-structured mathematical model for dengue transmission dynamics, analyzed its properties including reproduction numbers, and applied it to Brazil using 2021-2024 data to estimate transmission rates and predict future outbreaks.


<details>
  <summary>Details</summary>
Motivation: To better understand dengue transmission dynamics in Brazil by accounting for age structure, time-dependent parameters, and environmental factors like temperature and humidity.

Method: Created an age-structured mathematical model with time-dependent parameters, analyzed disease-free steady states and reproduction numbers, then applied to Brazil using weekly time series data from 2021-2024 with parameter estimation from epidemiological and environmental data.

Result: Estimated time-varying effective reproduction numbers, identified sensitive parameters affecting model dynamics, and generated predictions for future years using different transmission rates.

Conclusion: Population age distribution, vector population dynamics, and climate are crucial factors in dengue transmission dynamics in Brazil, providing deeper insights for outbreak prediction and control.

Abstract: An age structured mathematical model with time dependent parameters is developed to investigate the dynamics of dengue transmission. Its properties are thoroughly analyzed in the first part of this work, as for example its disease free steady state, the corresponding effective reproduction numbers, its basic reproduction number (obtained via the Euler and Lotka equation and the next generation matrix approach). We also provide formulas for the time-varying effective reproduction number, and draw relations with the instantaneous growth rate. In the second part, we apply this model to Brazil and use weekly time series data from this country. Various medical parameters are firstly evaluated from these data, and an extensive numerical simulations for the period 2021 to 2024 is then carried out. Estimation of the transmission rates are derived both from epidemiological data and from environmental data such as temperature and humidity. The time-varying effective reproduction numbers are then estimated on these data, following the theoretical investigations performed in the first part. The sensitive parameters that significantly affect the model dynamics are presented graphically. Model predictions for following year by using different transmission rates are finally presented. Our findings show the importance of population age distribution, vector population dynamics, and climate, contributing to a deeper understanding of dengue transmission dynamics in Brazil.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [30] [Human-aligned Quantification of Numerical Data](https://arxiv.org/abs/2511.15723)
*Anton Kolonin*

Main category: physics.data-an

TL;DR: The paper evaluates metrics for quantifying numerical data into meaningful categories, finding that Silhouette coefficient above 0.65 and Dip Test below 0.5 indicate classifiable data, with Silhouette aligning better with human intuition than compression-based methods.


<details>
  <summary>Details</summary>
Motivation: To determine when numerical data can be meaningfully quantified into discrete categories (quantums) and identify which computational metrics best align with human intuition for this task.

Method: Assessed applicability of metrics based on information compression and Silhouette coefficient for quantifying numerical data, and investigated their correlation with each other and human intuition.

Result: Data can be classified into distinct categories when Silhouette coefficient > 0.65 and Dip Test < 0.5; otherwise follows unimodal normal distribution. Silhouette coefficient aligns better with human intuition than normalized centroid distance method.

Conclusion: Silhouette coefficient is a reliable metric for quantifying numerical data that correlates well with human intuition, providing clear thresholds for determining when data can be meaningfully categorized.

Abstract: Quantifying numerical data involves addressing two key challenges: first, determining whether the data can be naturally quantified, and second, identifying the numerical intervals or ranges of values that correspond to specific value classes, referred to as "quantums," which represent statistically meaningful states. If such quantification is feasible, continuous streams of numerical data can be transformed into sequences of "symbols" that reflect the states of the system described by the measured parameter. People often perform this task intuitively, relying on common sense or practical experience, while information theory and computer science offer computable metrics for this purpose. In this study, we assess the applicability of metrics based on information compression and the Silhouette coefficient for quantifying numerical data. We also investigate the extent to which these metrics correlate with one another and with what is commonly referred to as "human intuition." Our findings suggest that the ability to classify numeric data values into distinct categories is associated with a Silhouette coefficient above 0.65 and a Dip Test below 0.5; otherwise, the data can be treated as following a unimodal normal distribution. Furthermore, when quantification is possible, the Silhouette coefficient appears to align more closely with human intuition than the "normalized centroid distance" method derived from information compression perspective.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [31] [Fourth branch of instability of Stokes' wave and dependence of corresponding growth rate on nonlinearity](https://arxiv.org/abs/2511.16436)
*A. O. Korotkevich,A. O. Prokofiev*

Main category: physics.flu-dyn

TL;DR: The paper identifies the fourth superharmonic instability branch of Stokes' waves and validates phenomenological formulas for instability growth rates across all four branches.


<details>
  <summary>Details</summary>
Motivation: To extend understanding of Stokes' wave instabilities by discovering the fourth superharmonic branch and testing the applicability of existing growth rate formulas.

Method: Massive computational analysis of Stokes' waves, using least squares fitting with data from first three instability branches and phenomenological asymptotics.

Result: Successfully reached the fourth superharmonic instability branch; validated that phenomenological formulas work for all four branches; corrected applicability ranges; reported growth rates for all branches.

Conclusion: The phenomenological formulas derived from first three instability branches remain valid for the newly discovered fourth branch, confirming their broader applicability across multiple instability branches.

Abstract: Through a massive computation we reached the fourth superharmonic instability branch of the Stokes' wave. Using the obtained results we checked phenomenological formulae for the dependence of the instability growth rates corresponding to different branches of instability on the nonlinearity parameter (steepness, defined as the wave swing to wavelength ratio $H/Λ$) in the vicinity of the new instability branch appearance and far from it. It is demonstrated, that the formulae, the one obtained as a least squares fit using the information from the first three branches of instability and a phenomenological asymptotics, work for the fourth branch and previously reported branches as well. Range of applicability of the relations was corrected. Growth rates for all four instability branches are reported.

</details>


### [32] [General-purpose Data-driven Wall Model for Low-speed Flows Part I: Baseline Model](https://arxiv.org/abs/2511.16511)
*Yuenong Ling,Imran Hayat,Konrad Goc,Adrian Lozano-Duran*

Main category: physics.flu-dyn

TL;DR: A general-purpose wall model for LES using building-block flow principle with neural network regression to predict wall shear stress, outperforming traditional equilibrium wall models in 90% of test cases.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional equilibrium wall models and improve upon earlier building-block approaches for large-eddy simulation across complex geometries and flow conditions.

Method: Four-component model: baseline wall model (neural network regression), error model, classifier, and confidence score. Uses localized dimensionless inputs guided by information theory, trained on diverse datasets including new DNS data for pressure gradient conditions.

Result: Baseline model outperforms equilibrium wall model in 90% of test scenarios, maintains errors below 20% for 98% of cases across 140 diverse validation datasets including turbulent boundary layers, airfoils, and aircraft geometries.

Conclusion: The building-block approach with neural network regression provides an effective general-purpose wall model that significantly improves upon traditional methods while handling complex flow phenomena including pressure gradients and separation.

Abstract: We present a general-purpose wall model for large-eddy simulation. The model builds on the building-block flow principle, leveraging essential physics from simple flows to train a generalizable model applicable across complex geometries and flow conditions. The model addresses key limitations of traditional equilibrium wall models (EQWM) and improves upon shortcomings of earlier building-block-based approaches. The model comprises four components: (i) a baseline wall model, (ii) an error model, (iii) a classifier, and (iv) a confidence score. The baseline model predicts the wall-shear stress, while the error model estimates epistemic errors and aleatoric errors, both used for uncertainty quantification. In Part I of this work, we present the baseline model, while the remaining three components are introduced in Part II. The baseline model is designed to capture a broad range of flow phenomena, including turbulence over curved walls and zero, adverse, and favorable mean pressure gradients, as well as flow separation and laminar flow. The problem is formulated as a regression task to predict wall shear stress using a neural network. Model inputs are localized in space and dimensionless, with their selection guided by information-theoretic criteria. Training data include, among other cases, a newly generated direct numerical simulation dataset of turbulent boundary layers under favorable and adverse PG conditions. Validation is carried out through both a priori and a posteriori tests. The a priori evaluation spans 140 diverse high-fidelity numerical datasets and experiments (67 training cases included), covering turbulent boundary layers, airfoils, Gaussian bumps, and full aircraft geometries, among others. We demonstrate that the baseline wall model outperforms the EQWM in 90% of test scenarios, while maintaining errors below 20% for 98% of the cases.

</details>


### [33] [Micro-Macro Simulation of Shallow Water Moment Equations](https://arxiv.org/abs/2511.15737)
*Vilém Rožek*

Main category: physics.flu-dyn

TL;DR: Micro-macro method applied to shallow water moment equations achieves significant computational speed-up while maintaining accuracy for shallow flow modeling.


<details>
  <summary>Details</summary>
Motivation: Shallow water equations are too simplified for accurate modeling, while shallow water moment equations are accurate but computationally expensive. Need to speed up computations while retaining accuracy.

Method: Formulated micro-macro method for shallow water moment equations, which switches between models of varying detail levels to allow larger stable time steps.

Result: Theoretical runtime analysis and test results (dam break and wave transport) show significant speed-up with sufficient accuracy maintained.

Conclusion: Micro-macro method successfully speeds up shallow water moment equation computations while preserving adequate accuracy levels.

Abstract: Shallow flows are governed by the Navier-Stokes equations. They are commonly modelled using the shallow water equations, a great simplification of the Navier-Stokes equations, which often yields inaccurate results. For that reason, a model called shallow water moment equations has been developed. It uses more equations and variables than the shallow water equations. While this model is significantly more accurate, it is also computationally more expensive. To speed up computations, the micro-macro method may be used. The micro-macro method switches between two models of varying levels of detail allowing for larger stable time steps. In this paper we formulate the micro-macro method for shallow water moment equations. We perform a theoretical runtime analysis of the method and present a series of results for a dam break test and a wave transport test. The micro-macro method achieves a significant speed-up while retaining a sufficient level of accuracy.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [34] [Charge-Ordered States and the Phase Diagram of the Extended Hubbard Model on the Bethe lattice](https://arxiv.org/abs/2511.16603)
*Aleksey Alekseev,Konrad Jerzy Kapcia*

Main category: cond-mat.str-el

TL;DR: The extended Hubbard model on Bethe lattice shows charge-ordered insulating/metallic and non-charge-ordered states, with onsite repulsion suppressing charge order and inducing metal-insulator transitions.


<details>
  <summary>Details</summary>
Motivation: To study the extended Hubbard model with both onsite and intersite interactions using mean-field approximation to understand charge ordering phenomena and metal-insulator transitions in a simplified analytical framework.

Method: Standard Hartree mean-field approximation on Bethe lattice, analyzing charge-order parameter, spectral function, and charge carrier concentration at finite temperatures.

Result: Increasing onsite repulsion suppresses charge order and changes system from insulating to metallic; finite-temperature phase diagrams reveal charge-ordered insulating, charge-ordered metallic, and non-charge-ordered states.

Conclusion: Mean-field approximation provides simplified analysis of complex phenomena in extended Hubbard model, allowing analytical derivations and revealing geometry-independent features while avoiding numerical issues.

Abstract: We study the extended Hubbard model (EHM) with both onsite Hubbard interaction and the intersite density-density interaction between nearest-neighbors using the standard Hartree mean-field approximation (MFA) on the Bethe lattice. We found that, at the ground state, the system can be in a charge-ordered insulating (COI), a charge-order metallic (COM) or a non-charge-ordered (NO) state. Moreover, the finite-temperature phase diagrams are presented. Several observables like a charge-order parameter, a spectral function, and particularly at finite temperatures, a charge carrier concentration (to visualize the degree of metallicity) are analyzed. The results show that increasing onsite repulsion suppresses charge order and change the properties of the system from insulating to metallic. Worth noting, that a number of phenomena can be found within the MFA, where their analysis is much simpler than in more advanced approaches. The method used for the EHM on the Bethe lattice also allows for a series of analytical derivations and simplification to see general geometry-independent features and analytical results, avoiding the numerical inaccuracies and other issues that appear with a purely numerical solution.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [35] [Gradient estimates for $(p,V)$-harmonic functions on Riemannian manifolds](https://arxiv.org/abs/2511.16058)
*Yuxin Dong,Hezi Lin,Weihao Zheng*

Main category: math.DG

TL;DR: Study of (p,V)-harmonic functions on Riemannian manifolds using Moser iteration, establishing volume comparison and Sobolev embedding theorems under Bakry-Émery curvature, with explicit global gradient estimates for positive entire solutions.


<details>
  <summary>Details</summary>
Motivation: To extend harmonic function theory to (p,V)-harmonic functions on complete Riemannian manifolds and establish fundamental results under curvature conditions.

Method: Moser iteration method, volume comparison theorem, Sobolev embedding theorem under Bakry-Émery curvature condition.

Result: Established volume comparison and Sobolev embedding theorems, obtained explicit global gradient estimate for positive entire (p,V)-harmonic functions.

Conclusion: Successfully developed analytical framework for (p,V)-harmonic functions with important geometric applications under curvature assumptions.

Abstract: In this paper, we study $(p,V)$-harmonic functions on complete Riemannian manifolds using the Moser iteration method. A volume comparison theorem and a Sobolev embedding theorem are established under the Bakry-$\acute{E}$mery curvature condition. Moreover, we obtain an explicit global gradient estimate for positive entire $(p,V)$-harmonic functions.

</details>


### [36] [Full flexibility of isometric immersions of metrics with low Hölder regularity in Poznyak theorem's dimension](https://arxiv.org/abs/2511.16305)
*Marta Lewicka*

Main category: math.DG

TL;DR: The paper proves that any 2D Riemannian metric has C¹,α isometric immersions into R⁴ with α < min{(r+β)/2,1}, achieving full flexibility (C¹,¹⁻) for C² metrics, which contrasts with lower regularity results in R³ and higher dimensional cases.


<details>
  <summary>Details</summary>
Motivation: To extend Poznyak's classical result on smooth isometric immersions into R⁴ to rougher metrics using convex integration techniques, and to demonstrate full flexibility in the C² case, contrasting with rigidity phenomena in lower codimensions.

Method: Using techniques of convex integration to construct isometric immersions for C^{r,β} metrics, achieving regularity C^{1,α} with α < min{(r+β)/2,1}, and showing these can be found arbitrarily close to any short immersion.

Result: For any 2D C^{r,β} metric, there exist C^{1,α} isometric immersions into R⁴ with α < min{(r+β)/2,1}, reaching full flexibility (C¹,¹⁻) for C² metrics. This improves upon previous results in R³ and higher dimensions.

Conclusion: The paper establishes optimal regularity results for isometric immersions of 2D metrics into R⁴ using convex integration, achieving full flexibility that contrasts with rigidity in lower codimensions and extends previous work on the Monge-Ampère system.

Abstract: A classical result by Poznyak asserts that any smooth $2$-dimensional Riemannian metric $g$, posed on the closure of a simply connected domain $ω\subset\mathbb{R}^2$, has a smooth isometric immersion into $\mathbb{R}^4$. Using techniques of convex integration, we prove that for any $2$-dimensional $g\in\mathcal{C}^{r,β}$, an isometric immersion of regularity $\mathcal{C}^{1,α}(\barω,\mathbb{R}^4)$ for any $α<\min\{\frac{r+β}{2},1\}$, may be found arbitrarily close to any short immersion. The fact that this result's regularity reaches $\mathcal{C}^{1,1-}$ for $g\in \mathcal{C}^2$, which is referred to as "full flexibility", should be contrasted with: (i) the regularity $\mathcal{C}^{1,1/3-}$ achieved by Cao, Hirsch and Inauen for isometric immersions into $\mathbb{R}^{3}$ and the lack of flexibility (rigidity) of such isometric immersions with regularity $\mathcal{C}^{1, 2/3+}$ proved by Borisov and then by Conti, de Lellis and Szekelyhidi; (ii) the regularity $\mathcal{C}^{1,1-}$ obtained by Källen for isometric immersions into higher codimensional space; and (iii) the regularity $\mathcal{C}^{1,\frac{1}{d(d+1)/k}-}$ achieved by the author in the general case of $d$-dimensional metrics and $(d+k)$-dimensional immersions for the closely related Monge-Ampère system.

</details>


### [37] [Horizontal and Vertical Regularity of Elastic Wave Geometry](https://arxiv.org/abs/2511.16466)
*Joonas Ilmavirta,Pieti Kirkkopelto,Antti Kykkänen*

Main category: math.DG

TL;DR: Characterizes conditions for applying Finsler-geometric methods to inverse problems in elastic wave imaging by analyzing stiffness tensor field properties.


<details>
  <summary>Details</summary>
Motivation: To enable Finsler-geometric approaches for solving inverse problems in elastic wave imaging by identifying necessary analytic and algebraic properties of anisotropic stiffness tensor fields.

Method: Analysis of analytic and algebraic properties that anisotropic stiffness tensor fields must satisfy for Finsler-geometric methods to be applicable.

Result: Identified specific conditions that stiffness tensor fields need to meet for Finsler-geometric techniques to work in elastic wave inverse problems.

Conclusion: Finsler-geometric methods can be applied to elastic wave imaging inverse problems when stiffness tensor fields satisfy certain analytic and algebraic properties.

Abstract: The elastic properties of a material are encoded in a stiffness tensor field and the propagation of elastic waves is modeled by the elastic wave equation. We characterize analytic and algebraic properties a general anisotropic stiffness tensor field has to satisfy in order for Finsler-geometric methods to be applicable in studying inverse problems related to imaging with elastic waves.

</details>


### [38] [An Information-Theoretic Reconstruction of Curvature](https://arxiv.org/abs/2511.16601)
*Amandip Sangha*

Main category: math.DG

TL;DR: The paper develops an information-theoretic method to recover Riemannian curvature from heat diffusion behavior, showing that directional entropy distortion encodes curvature without traditional geometric tools.


<details>
  <summary>Details</summary>
Motivation: To establish a new connection between geometric analysis and information theory by recovering curvature purely from heat diffusion data, avoiding traditional geometric methods like Jacobi fields or variational formulas.

Method: Compare heat mass transport along planes with Euclidean counterparts using relative entropy of finite measures, analyzing the leading small-time distortion of directional entropy.

Result: The directional entropy distortion precisely encodes both scalar and sectional curvature, and assembling these values produces a bilinear tensor identical to the classical Riemannian curvature operator.

Conclusion: Curvature can be realized as an infinitesimal information defect of diffusion, providing a principled analytic mechanism for detecting and reconstructing curvature using only heat diffusion data.

Abstract: We develop an intrinsic information-theoretic approach for recovering Riemannian curvature from the small-time behaviour of heat diffusion. Given a point and a two-plane in the tangent space, we compare the heat mass transported along that plane with its Euclidean counterpart using the relative entropy of finite measures. We show that the leading small-time distortion of this directional entropy encodes precisely the local curvature of the manifold. In particular, the planar information imbalance determines both the scalar curvature and the sectional curvature at a point, and assembling these directional values produces a bilinear tensor that coincides exactly with the classical Riemannian curvature operator.
  The method is entirely analytic and avoids Jacobi fields, curvature identities, or variational formulas. Curvature appears solely through the behaviour of heat flow under the exponential map, providing a new viewpoint in which curvature is realized as an infinitesimal information defect of diffusion. This perspective suggests further connections between geometric analysis and information theory and offers a principled analytic mechanism for detecting and reconstructing curvature using only heat diffusion data.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [39] [Eigenvalue-accelerated LDOS optimization of high-Q optical resonances](https://arxiv.org/abs/2511.16643)
*George Shaker,Beñat Martinez de Aguirre Jokisch,Pengning Chao,Steven G. Johnson*

Main category: physics.optics

TL;DR: A new method for accelerating inverse design of high-Q resonant cavities by using shift-invert eigensolver to maintain LDOS at resonance peaks, eliminating ill-conditioning issues.


<details>
  <summary>Details</summary>
Motivation: Conventional LDOS optimization becomes dramatically slow for high-Q resonances (Q >> 100) due to ill-conditioning at sharp resonances, limiting practical design of high-performance resonant cavities.

Method: Use shift-invert eigensolver after identifying strong resonances to ensure LDOS remains centered at resonance peaks, eliminating ill-conditioning that slows optimization.

Result: Achieved orders-of-magnitude acceleration in inverse design, demonstrated by designing Q > 10^6 resonant cavities in 1D and 2D dielectric systems.

Conclusion: The method enables efficient design of ultra-high-Q resonant cavities and is applicable to other resonant-response metrics beyond LDOS optimization.

Abstract: We demonstrate a new method that yields orders-of-magnitude acceleration in inverse design (e.g. topology optimization) of high-$Q$ resonant cavities to maximize the local density of states (LDOS), and which is also applicable to other resonant-response metrics. The key idea is that, once conventional LDOS optimization has identified a strong resonance, subsequent optimizations can exploit a fast shift-invert eigensolver to ensure that the LDOS remains centered at the resonance peak. We show that this eliminates ill-conditioning at sharp resonances that otherwise dramatically slows LDOS (and similar) optimization for $Q \gg 100$. Our method is demonstrated by design of $Q > 10^6$ resonant cavities in 1d and 2d dielectric systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [40] [Approximation rates of quantum neural networks for periodic functions via Jackson's inequality](https://arxiv.org/abs/2511.16149)
*Ariel Neufeld,Philipp Schmocker,Viet Khoa Tran*

Main category: quant-ph

TL;DR: QNNs achieve quadratic reduction in parameters for periodic function approximation using Jackson inequality and trigonometric polynomials.


<details>
  <summary>Details</summary>
Motivation: To study QNN approximation capabilities for periodic functions with respect to supremum norm, building on universal approximation properties.

Method: Use Jackson inequality to approximate functions via trigonometric polynomials implemented in QNNs, focusing on periodic function class.

Result: Quadratic reduction in parameter count compared to literature, with smoother functions requiring fewer parameters for accurate approximation.

Conclusion: Periodic function restriction enables more efficient QNN parameterization and better approximation performance than existing approaches.

Abstract: Quantum neural networks (QNNs) are an analog of classical neural networks in the world of quantum computing, which are represented by a unitary matrix with trainable parameters. Inspired by the universal approximation property of classical neural networks, ensuring that every continuous function can be arbitrarily well approximated uniformly on a compact set of a Euclidean space, some recent works have established analogous results for QNNs, ranging from single-qubit to multi-qubit QNNs, and even hybrid classical-quantum models. In this paper, we study the approximation capabilities of QNNs for periodic functions with respect to the supremum norm. We use the Jackson inequality to approximate a given function by implementing its approximating trigonometric polynomial via a suitable QNN. In particular, we see that by restricting to the class of periodic functions, one can achieve a quadratic reduction of the number of parameters, producing better approximation results than in the literature. Moreover, the smoother the function, the fewer parameters are needed to construct a QNN to approximate the function.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [41] [Enhancing Forex Forecasting Accuracy: The Impact of Hybrid Variable Sets in Cognitive Algorithmic Trading Systems](https://arxiv.org/abs/2511.16657)
*Juan C. King,Jose M. Amigo*

Main category: cs.AI

TL;DR: Implementation of an AI-based algorithmic trading system for EUR-USD Forex pair using both fundamental and technical analysis features, with comparative evaluation of which feature type provides better predictive capacity.


<details>
  <summary>Details</summary>
Motivation: To develop an advanced AI trading system for high-frequency Forex trading that integrates both fundamental macroeconomic variables and technical indicators, and determine which type of analysis provides superior predictive performance.

Method: Integrates fundamental macroeconomic variables (GDP, Unemployment Rate from Euro Zone and US) with technical indicators (oscillators, Fibonacci levels, price divergences) into an AI-based algorithmic trading system, evaluated using machine learning metrics and backtesting simulations.

Result: The algorithm's performance was quantified using standard machine learning metrics for predictive accuracy and backtesting simulations across historical data to assess trading profitability and risk.

Conclusion: Comparative analysis reveals which class of input features (fundamental or technical) provides greater and more reliable predictive capacity for generating profitable trading signals in the EUR-USD Forex market.

Abstract: This paper presents the implementation of an advanced artificial intelligence-based algorithmic trading system specifically designed for the EUR-USD pair within the high-frequency environment of the Forex market. The methodological approach centers on integrating a holistic set of input features: key fundamental macroeconomic variables (for example, Gross Domestic Product and Unemployment Rate) collected from both the Euro Zone and the United States, alongside a comprehensive suite of technical variables (including indicators, oscillators, Fibonacci levels, and price divergences). The performance of the resulting algorithm is evaluated using standard machine learning metrics to quantify predictive accuracy and backtesting simulations across historical data to assess trading profitability and risk. The study concludes with a comparative analysis to determine which class of input features, fundamental or technical, provides greater and more reliable predictive capacity for generating profitable trading signals.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [42] [Entrywise Approximate Solutions for SDDM Systems in Almost-Linear Time](https://arxiv.org/abs/2511.16570)
*Angelo Farfan,Mehrdad Ghadiri,Junzhao Yang*

Main category: cs.DS

TL;DR: Fast algorithm for solving SDDM linear systems with near-linear time complexity


<details>
  <summary>Details</summary>
Motivation: Need efficient solvers for symmetric diagonally dominant M-matrices (SDDM) which are principal submatrices of graph Laplacians, commonly arising in graph algorithms and scientific computing

Method: Developed an algorithm that computes entrywise approximation to solution of Lx = b for invertible SDDM matrices using nearly linear time complexity O~(m n^o(1))

Result: Achieves high probability approximation in O~(m n^o(1)) time, where m is number of nonzero entries and n is system dimension

Conclusion: Provides near-optimal solver for SDDM systems, significantly improving computational efficiency for graph-related linear algebra problems

Abstract: We present an algorithm that given any invertible symmetric diagonally dominant M-matrix (SDDM), i.e., a principal submatrix of a graph Laplacian, $\boldsymbol{\mathit{L}}$ and a nonnegative vector $\boldsymbol{\mathit{b}}$, computes an entrywise approximation to the solution of $\boldsymbol{\mathit{L}} \boldsymbol{\mathit{x}} = \boldsymbol{\mathit{b}}$ in $\tilde{O}(m n^{o(1)})$ time with high probability, where $m$ is the number of nonzero entries and $n$ is the dimension of the system.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [43] [Li-P-S Electrolyte Materials as a Benchmark for Machine-Learned Interatomic Potentials](https://arxiv.org/abs/2511.16569)
*Natascia L. Fragapane,Volker L. Deringer*

Main category: cond-mat.mtrl-sci

TL;DR: LiPS-25 is a curated benchmark dataset for solid-state electrolyte materials from the Li2S-P2S5 system, with performance tests ranging from numerical error metrics to physically motivated evaluation tasks.


<details>
  <summary>Details</summary>
Motivation: Growing availability of machine-learned interatomic potential models creates demand for robust, automated, and chemically insightful benchmarking methodologies for materials simulations.

Method: Introduce LiPS-25 dataset with crystalline and amorphous configurations, and a performance test suite including numerical error metrics and physically motivated evaluation tasks. Assess hyperparameter effects and fine-tuning behavior of pre-trained MLIP models.

Result: Developed a comprehensive benchmarking framework that can evaluate graph-based MLIP architectures and their performance on solid-state electrolyte materials.

Conclusion: The benchmarks and code implementations can be readily adapted to other material systems beyond Li-P-S solid-state electrolytes.

Abstract: With the growing availability of machine-learned interatomic potential (MLIP) models for materials simulations, there is an increasing demand for robust, automated, and chemically insightful benchmarking methodologies. In response, we here introduce LiPS-25, a curated benchmark dataset for a canonical series of solid-state electrolyte materials from the Li2S-P2S5 pseudo-binary compositional line, including crystalline and amorphous configurations. Together with the dataset, we present a suite of performance tests that range from conventional numerical error metrics to physically motivated evaluation tasks. With a focus on graph-based MLIP architectures, we run numerical experiments that assess (i) the effect of hyperparameters and (ii) the fine-tuning behavior of selected pre-trained ("foundational") MLIP models. Beyond the Li-P-S solid-state electrolytes, we expect that such benchmarks and their code implementations can be readily adapted to other material systems.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [44] [Geant4 based library SCoRe4 for Surface Contamination and Roughness Effects simulations in rare event search experiments](https://arxiv.org/abs/2511.15844)
*Christoph Grüner*

Main category: physics.ins-det

TL;DR: SCoRe4 is a Geant4-based library that simulates realistic surface roughness to improve accuracy in particle interaction modeling for rare event searches, addressing limitations of standard smooth-surface assumptions.


<details>
  <summary>Details</summary>
Motivation: Standard Geant4 simulations assume perfectly smooth surfaces, which neglects real microscopic roughness and leads to inaccurate energy deposition predictions in rare event searches like dark matter or neutrinoless double beta decay experiments.

Method: Developed SCoRe4, an open-source Geant4 library that generates simplified rough surface geometries using experimentally measurable parameters, scalable from square millimeters to square meters while maintaining computational efficiency.

Result: Created a functional library that can be easily integrated into existing Geant4 setups, enabling more realistic surface roughness simulations for improved background modeling.

Conclusion: SCoRe4 provides a practical solution to enhance simulation accuracy in rare event physics by incorporating realistic surface roughness, potentially improving background modeling performance.

Abstract: Surface simulations are important for accurately modeling particle interactions in experiments where background contributions from surface contaminants can significantly affect detector performance. In rare event searches, such as dark matter or neutrinoless double beta decay experiments, standard Geant4 simulations typically assume perfectly smooth surfaces, neglecting the microscopic roughness that exists in real materials. This simplification can lead to inaccurate predictions of energy deposition. To address this limitation, I developed SCoRe4, a Geant4-based library designed to simulate more realistic surface roughness based on experimentally measurable parameters. The code allows users to generate patches of simplified rough surface geometries across a wide range of scales - from square millimeters to square meters - while maintaining computational efficiency. SCoRe4 is open source and can be easily integrated into existing Geant4 setups. This work presents the structure, implementation, and example application of SCoRe4,as well as its potential use in improving the accuracy of background modeling in rare event physics.

</details>
