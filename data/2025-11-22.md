<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 5]
- [math.AP](#math.AP) [Total: 15]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [cs.DS](#cs.DS) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [physics.data-an](#physics.data-an) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math.DG](#math.DG) [Total: 4]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Data-driven Model Reduction for Parameter-Dependent Matrix Equations via Operator Inference](https://arxiv.org/abs/2511.16033)
*Xuelian Wen,Qiuqi Li,Juan Zhang*

Main category: math.NA

TL;DR: A non-intrusive, data-driven surrogate modeling framework using Operator Inference (OpInf) for efficiently solving parameter-dependent matrix equations in many-query scenarios.


<details>
  <summary>Details</summary>
Motivation: To overcome the computational bottlenecks of intrusive methods in high-dimensional contexts and provide a scalable solution for parameter-dependent matrix equations.

Method: Reformulates matrix equations into structured representation showing parameter dependence in polynomial form, then constructs reduced-order models via regression on solution snapshots without requiring full-order operators.

Result: Numerical experiments confirm the accuracy and computational efficiency of the proposed approach.

Conclusion: The framework provides a scalable and practical solution for parameter-dependent matrix equations, demonstrating effectiveness in many-query settings.

Abstract: This work develops a non-intrusive, data-driven surrogate modeling framework based on Operator Inference (OpInf) for rapidly solving parameter-dependent matrix equations in many-query settings. Motivated by the requirements of the OpInf methodology, we reformulate the matrix equations into a structured representation that explicitly shows the parameter dependence in polynomial form. This reformulation is crucial for efficient model reduction. This approach constructs reduced-order models via regression on solution snapshots, bypassing the need for expensive full-order operators and thus overcoming the primary bottlenecks of intrusive methods in high-dimensional contexts. Numerical experiments confirm their accuracy and computational efficiency, demonstrating that our work is a scalable and practical solution for parameter-dependent matrix equations.

</details>


### [2] [Optimal error analysis of an interior penalty virtual element method for fourth-order singular perturbation problems](https://arxiv.org/abs/2511.16070)
*Fang Feng,Yuanyi Sun,Yue Yu*

Main category: math.NA

TL;DR: The paper proves that the Interior Penalty Virtual Element Method (IPVEM) achieves optimal and uniform error estimates for fourth-order singular perturbation problems, correcting previous suboptimal half-order convergence rates.


<details>
  <summary>Details</summary>
Motivation: Previous studies showed IPVEM had only half-order uniform convergence for fourth-order singular perturbation problems, which was suboptimal and needed improvement.

Method: Theoretical analysis of the Interior Penalty Virtual Element Method (IPVEM) for solving fourth-order singular perturbation problems, with extensive numerical validation.

Result: IPVEM achieves optimal and uniform error estimates even with boundary layers, correcting the previously reported suboptimal half-order convergence.

Conclusion: The proposed IPVEM method is effective for singularly perturbed problems with optimal convergence rates, as confirmed by both theory and numerical experiments.

Abstract: In recent studies \cite{ZZ24, FY24}, the Interior Penalty Virtual Element Method (IPVEM) has been developed for solving a fourth-order singular perturbation problem, with uniform convergence established in the lowest-order case concerning the perturbation parameter. However, the resulting uniform convergence rate is only of half-order, which is suboptimal. In this work, we demonstrate that the proposed IPVEM in fact achieves optimal and uniform error estimates, even in the presence of boundary layers. The theoretical results are substantiated through extensive numerical experiments, which confirm the validity of the error estimates and highlight the method's effectiveness for singularly perturbed problems.

</details>


### [3] [Shallow neural network yields regularization for ill-posed inverse problems](https://arxiv.org/abs/2511.16171)
*Lan Wang,Qiao Zhu,Bangti Jin,Ye Zhang*

Main category: math.NA

TL;DR: Neural networks can approximate solutions to nonlinear ill-posed operator equations, with network size serving as regularization parameter to balance approximation and stability against noise.


<details>
  <summary>Details</summary>
Motivation: To develop neural network methods for solving nonlinear ill-posed operator equations while accounting for both approximation and measurement errors in error estimation.

Method: Proposed expanding neural network method as iterative regularization scheme, using number of neurons as regularization parameter and iteration number.

Result: Small networks provide stable solutions for high noise data, while larger networks risk instability from overfitting. Derived convergence rates for neural networks in variational regularization framework.

Conclusion: Neural networks are effective for solving ill-posed problems when properly regularized, with network size controlling the trade-off between approximation accuracy and stability against noise.

Abstract: In this paper, we establish universal approximation theorems for neural networks applied to general nonlinear ill-posed operator equations. In addition to the approximation error, the measurement error is also taken into account in our error estimation. We introduce the expanding neural network method as a novel iterative regularization scheme and prove its regularization properties under different a priori assumptions about the exact solutions. Within this framework, the number of neurons serves as both the regularization parameter and iteration number. We demonstrate that for data with high noise levels, a small network architecture is sufficient to obtain a stable solution, whereas a larger architecture may compromise stability due to overfitting. Furthermore, under standard assumptions in regularization theory, we derive convergence rate results for neural networks in the context of variational regularization. Several numerical examples are presented to illustrate the robustness of the proposed neural network-based algorithms.

</details>


### [4] [Robust PAMPA Scheme in the DG Formulation on Unstructured Triangular Meshes: bound preservation, oscillation elimination, and boundary conditions](https://arxiv.org/abs/2511.16180)
*Rémi Abgrall,Yongle Liu*

Main category: math.NA

TL;DR: Improved PAMPA algorithm with globally continuous solutions, locally conservative scheme without mass matrix inversion, connection to discontinuous Galerkin method, bound preserving and non-oscillatory properties with third-order accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop an improved version of PAMPA that provides globally continuous solutions while maintaining local conservation properties and avoiding mass matrix inversion, building on previous connections to discontinuous Galerkin methods.

Method: Leverages reinterpretation of PAMPA as discontinuous Galerkin method for linear hyperbolic problems, implements rigorous boundary conditions, develops bound preserving and non-oscillatory methods, with truncation error analysis.

Result: Numerical experiments confirm third-order accuracy for smooth solutions, scheme demonstrates bound preserving and non-oscillatory behavior across various numerical benchmarks.

Conclusion: The improved PAMPA method successfully achieves globally continuous solutions with local conservation, bound preservation, non-oscillatory properties, and third-order accuracy, validated through comprehensive numerical testing.

Abstract: We propose an improved version of the PAMPA algorithm where the solution is sought as globally continuous. The scheme is locally conservative, and there is no mass matrix to invert. This method had been developed in a series of papers, see e.g \cite{Abgrall2024a} and the references therein. In \cite{Abgrall2025d}, we had shown the connection between PAMPA and the discontinuous Galerkin method, for the linear hyperbolic problem. Taking advantage of this reinterpretation, we use it to define a family of methods, show how to implement the boundary conditions in a rigorous manner. In addition, we propose a method that complements the bound preserving method developed in \cite{Abgrall2025d} in the sense that it is non oscillatory. A truncation error analysis is provided, it shows that the scheme should be third order accurate for smooth solutions. This is confirmed by numerical experiments. Several numerical examples are presented to show that the scheme is indeed bound preserving and non oscillatory on a wide range on numerical benchmarks.

</details>


### [5] [Numerical identification of the time-dependent coefficient in the heat equation with fractional Laplacian](https://arxiv.org/abs/2511.16238)
*Arshyn Altybay,Niyaz Tokmagambetov,Gulzat Nalzhupbayeva*

Main category: math.NA

TL;DR: This paper develops a method to identify time-dependent source coefficients in fractional heat equations using nonlocal data, with proven uniqueness, stability, and an efficient numerical algorithm.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of identifying time-dependent source coefficients in fractional heat equations with Dirichlet boundary conditions and integral nonlocal data, addressing challenges in uniqueness and computational stability.

Method: Established a priori estimate for uniqueness and stability, proposed a fully implicit Crank-Nicolson finite-difference scheme with rigorous stability and convergence analysis, and developed an efficient noise-stable computation algorithm.

Result: Numerical experiments verified the algorithm's accuracy and robustness under noisy data conditions, demonstrating effective performance in solving the inverse problem.

Conclusion: The proposed approach successfully addresses the inverse source identification problem in fractional heat equations with proven theoretical guarantees and practical computational efficiency.

Abstract: We address the inverse problem of identifying a time-dependent source coefficient in a one-dimensional heat equation with a fractional Laplacian subject to Dirichlet boundary conditions and an integral nonlocal data. An a priori estimate is established to ensure the uniqueness and stability of the solution. A fully implicit Crank-Nicolson (CN) finite-difference scheme is proposed and rigorously analysed for stability and convergence. An efficient noise-stable computation algorithm is developed and verified through numerical experiments, demonstrating accuracy and robustness under noisy data.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [6] [Shallow-water convergence of the intermediate long wave equation in $L^2$](https://arxiv.org/abs/2511.15905)
*Andreia Chapouto,Guopeng Li,Tadahiro Oh,Tengfei Zhao*

Main category: math.AP

TL;DR: This paper establishes convergence of the scaled Intermediate Long Wave (ILW) equation to the Korteweg-de Vries (KdV) equation in the shallow-water limit at the L²-level, completing the well-posedness and convergence study of ILW on both real line and circle geometries.


<details>
  <summary>Details</summary>
Motivation: To complete the convergence study of ILW dynamics by establishing the shallow-water limit convergence to KdV, complementing previous work on deep-water convergence, and to provide a unified treatment for both real line and circle geometries within the L²-framework.

Method: The proof uses complete integrability of ILW and normal form methods. It employs the Lax pair structure and perturbation determinant for ILW to establish weakly uniform equicontinuity in L² for high frequencies, and implements an infinite iteration of normal form reductions for KdV to handle low frequencies.

Result: The authors successfully establish convergence of scaled ILW dynamics to KdV dynamics in the shallow-water limit at the L²-level, completing the well-posedness and convergence study of ILW on both geometries.

Conclusion: This work completes the convergence study of ILW to KdV in both shallow-water and deep-water limits within the L²-framework, providing a unified approach that applies equally to both real line and circle geometries using integrability and normal form methods.

Abstract: We continue our study on the convergence issue of the intermediate long wave equation (ILW) on both the real line and the circle. In particular, we establish convergence of the scaled ILW dynamics to that of the Korteweg-de Vries equation (KdV) in the shallow-water limit at the $L^2$-level. Together with the recent work by the first three authors and D. Pilod (2024) on the deep-water convergence in $L^2$, this work completes the well-posedness and convergence study of ILW on both geometries within the $L^2$-framework. Our proof equally applies to both geometries and is based on the following two ingredients: the complete integrability of ILW and the normal form method. More precisely, by making use of the Lax pair structure and the perturbation determinant for ILW, recently introduced by Harrop-Griffths, Killip, and Vişan (2025), we first establish weakly uniform (in small depth parameters) equicontinuity in $L^2$ of solutions to the scaled ILW, providing a control on the high frequency part of solutions. Then, we treat the low frequency part by implementing a perturbative argument based on an infinite iteration of normal form reductions for KdV.

</details>


### [7] [Data-Driven Parameter Identification for Tumor Growth Models](https://arxiv.org/abs/2511.15940)
*Liu Liu,Yifei Wang,Qinyu Xu,Xiaoqian Xu*

Main category: math.AP

TL;DR: PINNs are used to estimate parameters in tumor growth models from scarce, noisy data, showing potential for data-driven cancer modeling.


<details>
  <summary>Details</summary>
Motivation: Accurate tumor growth modeling is crucial for understanding cancer progression and informing treatment strategies, especially with limited and noisy observational data.

Method: Physics-Informed Neural Networks (PINNs) are adopted to estimate parameters in nonlinear PDE tumor growth models using real-life lab data.

Result: The approach demonstrates the potential of deep learning tools for data-driven tumor growth modeling in biology.

Conclusion: PINNs show promise for addressing parameter estimation challenges in tumor growth modeling with limited and noisy data.

Abstract: Modeling tumor growth accurately is essential for understanding cancer progression and informing treatment strategies. To estimate the parameters in the tumor growth model described by a nonlinear PDE, we adopt Physics-Informed Neural Networks (PINNs), which show advantages especially when the observation data is scarce and contains noise. With the help of real-life lab data, we have demonstrated the potential of applying deep learning tools to address data-driven modeling for tumor growth in biology.

</details>


### [8] [An $L^2$-quantitative global approximation for the Stokes initial-boundary value problem](https://arxiv.org/abs/2511.16079)
*Mitsuo Higaki*

Main category: math.AP

TL;DR: First quantitative Runge approximation theorem for 3D nonstationary Stokes system with explicit L²-estimates, improving on qualitative results by providing constructive proofs and handling initial-boundary value problems.


<details>
  <summary>Details</summary>
Motivation: Address limitations of previous qualitative results [H.-Sueur, 2025] that used non-constructive Hahn-Banach theorem (preventing quantitative estimates) and only covered interior approximations, not the physically important initial-boundary value problem.

Method: Adapt modern quantitative framework from [Rüland-Salo, 2019] to Stokes system by combining semigroup theory with quantitative approximation for the associated resolvent problem.

Result: Established first quantitative Runge approximation theorem for 3D nonstationary Stokes system with explicit L²-estimates on bounded spatial domains.

Conclusion: Successfully developed constructive quantitative approximation theory for Stokes system that handles both interior and boundary value problems, overcoming limitations of previous qualitative approaches.

Abstract: We establish the first quantitative Runge approximation theorem, with explicit $L^2$-estimates, for the 3d nonstationary Stokes system on a bounded spatial domain. This result addresses the two primary limitations of the qualitative result [H.-Sueur, 2025] obtained in collaboration with Franck Sueur: first, it bypasses the non-constructive Hahn-Banach theorem used in [H.-Sueur, 2025], precluding quantitative estimates; and second, it extends the scope of the theory from interior approximations to the physically important initial-boundary value problem. Our proof is founded on the modern quantitative framework of [Rüland-Salo, 2019], which we adapt to the Stokes system by combining semigroup theory with a quantitative approximation for the associated resolvent problem.

</details>


### [9] [Liouville--Type Results for Infinity Elliptic Equations Involving Gradient and Hardy--Hénon Nonlinearities](https://arxiv.org/abs/2511.16116)
*Tan-Dat Khuu,Trung-Hieu Huynh,Hoang-Hung Vo*

Main category: math.AP

TL;DR: Study of Liouville-type properties for degenerate elliptic equations with fractional infinity Laplacian and nonlinear lower-order terms, establishing new comparison principles and Lipschitz estimates.


<details>
  <summary>Details</summary>
Motivation: Extend Liouville theory from classical and normalized infinity Laplacian to fractional cases with Hamiltonian and Hardy-Hénon type nonlinearities.

Method: Weighted comparison principle, sharp local Lipschitz estimates for viscosity solutions, radial reduction, barrier constructions, and refined comparison arguments.

Result: Liouville theorems derived from growth conditions for bounded nonnegative solutions with power-type and exponential nonlinearities; partial results for strongly supercritical exponential case.

Conclusion: Provides unified framework linking regularity, comparison principles, and Liouville-type phenomena for degenerate elliptic equations with fractional infinity Laplacians and nonlinear effects.

Abstract: In this paper we study Liouville-type properties for a class of degenerate elliptic equations driven by the fractional infinity Laplacian with nonlinear lower-order terms, \[ Δ_\infty^βu - c\,H(u,\nabla u) - λ\, f(|x|,u)=0 \qquad \text{in }\mathbb{R}^n, \] where $β\in[0,2]$, $Δ_\infty^β$ denotes the fractional infinity Laplace operator, and the nonlinearities $H$ and $f$ represent Hamiltonian and Hardy--Hénon type effects, respectively. We extend the Liouville theory for the classical and normalized infinity Laplacian by establishing a new weighted comparison principle together with sharp local Lipschitz estimates for viscosity solutions.
  Our Liouville theorems are derived from precise growth conditions for bounded nonnegative solutions when $f$ exhibits power-type behavior, i.e.\ $f\sim u^γ$. We also treat the exponential case $f\sim e^u$, for which the equation becomes strongly supercritical: under suitable assumptions on the growth of $u$ at spatial infinity, only partial Liouville-type conclusions can be obtained.
  The analysis relies on radial reduction, barrier constructions, and refined comparison arguments. Altogether, the results provide a unified framework linking regularity, comparison principles, and Liouville-type phenomena for degenerate elliptic equations involving fractional infinity Laplacians and nonlinear lower-order effects.

</details>


### [10] [On some uniqueness results](https://arxiv.org/abs/2511.16129)
*Patrizia Pucci,Jianjun Zhang,Xuexiu Zhong*

Main category: math.AP

TL;DR: Extends Serrin and Tang's results on radial solutions of overdetermined m-Laplacian problems to low-dimensional cases (1≤N≤m, m>1), proving uniqueness under suitable assumptions on f.


<details>
  <summary>Details</summary>
Motivation: Motivated by sharp Gagliardo-Nirenberg/Nash inequality and the explicit gap in Serrin and Tang's work which stated their proofs couldn't extend to N≤m cases.

Method: Uses standard framework similar to Serrin and Tang but with significantly different proof details to handle the low-dimensional case where N≤m.

Result: Proves uniqueness of radial solutions for the overdetermined m-Laplacian problem when 1≤N≤m and m>1 under certain assumptions on f.

Conclusion: Successfully extends previous results to the low-dimensional case that was explicitly identified as problematic in prior work, filling an important gap in the theory.

Abstract: This paper aims to extend the results of Serrin and Tang in [{\it Indiana Univ. Math. J., 49 (2000), 897--923}] to the low-dimensional case. Specifically, the paper deals with the radial solutions of the following overdetermined problem $$ \begin{cases} -Δ_m u=f(u),\quad u>0~\hbox{in}~B_R,\\ u=\partial_νu=0~\hbox{on}~\partial B_R, \end{cases} $$ where $B_R$ is the open ball of $\mathbb{R}^N$ centered at 0 and with radius $R>0$. We prove uniqueness when $1\leq N\leq m$ {and $m>1$} under certain suitable assumptions on~$f$. Additionally, this work is motivated by the sharp Gagliardo-Nirenberg/Nash inequality. While the framework presented in this article is standard and closely resembles that of Serrin and Tang, the detail of our proofs differ significantly. It is important to note that Serrin and Tang explicitly stated (see Subsection~6.2 of their work) that {\it``the proofs in the present paper rely extensively on the assumption $N>m$ and cannot be extended easily to values $N\leq m$."}

</details>


### [11] [Liouville theorems for fully nonlinear elliptic equations on half spaces](https://arxiv.org/abs/2511.16152)
*Yuanyuan Lian*

Main category: math.AP

TL;DR: Proves two Liouville theorems for fully nonlinear uniformly elliptic equations on half spaces using boundary regularity, Hopf, and Carleson estimates.


<details>
  <summary>Details</summary>
Motivation: To establish Liouville-type results for fully nonlinear elliptic equations on half spaces, extending classical results to more general settings.

Method: Uses boundary pointwise regularity, Hopf type estimates, and Carleson type estimates to provide a short proof.

Result: Successfully proves two Liouville theorems for fully nonlinear uniformly elliptic equations on half spaces.

Conclusion: The paper presents a concise new proof method for Liouville theorems in half space settings using boundary regularity techniques.

Abstract: In this note, we prove two Liouville theorems for fully nonlinear uniformly elliptic equations on half spaces. The main tools are the boundary pointwise regularity, the Hopf type estimate and the Carleson type estimate. Our new proof is rather short.

</details>


### [12] [Spreading Properties of a City-Road Reaction-Diffusion Model on One-Dimensional Lattice](https://arxiv.org/abs/2511.16157)
*Grégory Faye,Jean-Michel Roquejoffre,Min Zhao*

Main category: math.AP

TL;DR: The paper proposes a new PDE-ODE model for biological invasions on infinite 1D metric graphs, with logistic equations at vertices and diffusion on edges with Robin boundary conditions. It analyzes system properties, long-term behavior, and derives asymptotic spreading speed.


<details>
  <summary>Details</summary>
Motivation: To develop a mathematical model that describes biological invasions constrained on infinite homogeneous one-dimensional metric graphs, capturing the interaction between local population dynamics and spatial diffusion through network connections.

Method: An infinite PDE-ODE system where logistic equations govern population dynamics at vertices of the 1D lattice Z, with diffusion equations on edges connecting vertices using Robin boundary conditions at vertices.

Result: Established main properties of the system, characterized long-time behavior of solutions, and determined an asymptotic spreading speed. In fast diffusion regime, derived a novel asymptotic model with similar propagation properties to classical discrete Fisher-KPP on 1D lattice Z.

Conclusion: The proposed model successfully captures biological invasion dynamics on metric graphs, with the fast diffusion regime yielding an asymptotic model that parallels classical Fisher-KPP behavior, providing insights into propagation patterns in networked environments.

Abstract: We propose and study a new model to describe biological invasions constrained on infinite homogeneous one dimensional metric graphs. Our model consists of an infinite PDE-ODE system where, at each vertex of the one-dimensional lattice $\mathbb{Z}$, we have a logistic equation, and connections between vertices are given by diffusion equations on the edges supplemented with Robin like boundary conditions at the vertices. We establish the main properties of the system and study the long time behavior of the solutions, especially by characterizing an asymptotic spreading speed for the system. In the fast diffusion regime, we derive a novel asymptotic model which exhibits similar propagation properties as the classical discrete Fisher-KPP on the one-dimensional lattice $\mathbb{Z}$.

</details>


### [13] [The Immersed Boundary Problem in 2-D: the Navier-Stokes Case](https://arxiv.org/abs/2511.16189)
*Jiajun Tong,Dongyi Wei*

Main category: math.AP

TL;DR: Study of 2-D immersed boundary problem with elastic string in fluid, proving existence, uniqueness, regularity estimates, convergence to Stokes case, energy law, and global solutions near equilibrium.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundation for immersed boundary problems, addressing existence, uniqueness, and regularity of solutions for elastic strings in viscous fluids with singular forcing.

Method: Introduce mild solutions concept, prove existence/uniqueness for C^1 initial string configurations with well-stretched condition and L^p initial flow fields, establish blow-up criterion, analyze convergence to Stokes case as Reynolds number → 0.

Result: Proved existence, uniqueness, optimal regularity estimates for mild solutions; derived convergence to Stokes case with optimal error estimates; established energy law; showed global solutions near equilibrium states.

Conclusion: The immersed boundary problem has well-defined solutions with good regularity properties, converges properly to Stokes limit, satisfies energy conservation, and exhibits global existence near equilibrium configurations.

Abstract: We study the immersed boundary problem in 2-D. It models a 1-D elastic closed string immersed and moving in a fluid that fills the entire plane, where the fluid motion is governed by the 2-D incompressible Navier-Stokes equation with a positive Reynolds number subject to a singular forcing exerted by the string. We introduce the notion of mild solutions to this system, and prove its existence, uniqueness, and optimal regularity estimates when the initial string configuration is $C^1$ and satisfies the well-stretched condition and when the initial flow field $u_0$ lies in $L^p(\mathbb{R}^2)$ with $p\in (2,\infty)$. A blow-up criterion is also established. When the Reynolds number is sent to zero, we show convergence in short time of the solution to that of the Stokes case of 2-D immersed boundary problem, with the optimal error estimates derived. We prove the energy law of the system when $u_0$ additionally belongs to $L^2(\mathbb{R}^2)$. Lastly, we show that the solution is global when the initial data is sufficiently close to an equilibrium state.

</details>


### [14] [Dynamics of Ideal Fluid Flows](https://arxiv.org/abs/2511.16254)
*Tarek M. Elgindi*

Main category: math.AP

TL;DR: Analysis of the incompressible Euler equation covering least action principle, special solutions, solvability, singularity formation, and asymptotic behavior.


<details>
  <summary>Details</summary>
Motivation: To comprehensively examine fundamental mathematical aspects and challenges associated with the incompressible Euler equation in fluid dynamics.

Method: Theoretical mathematical analysis and discussion of various problem domains related to the Euler equation.

Result: Provides systematic overview of key problems and research directions in Euler equation theory.

Conclusion: The paper organizes and discusses major mathematical challenges in understanding the incompressible Euler equation's behavior and properties.

Abstract: We will discuss various aspects of the incompressible Euler equation. We will discuss, in particular, problems related to the least action principle, the existence of special solutions, the problem of solvability, singularity formation, and asymptotic behavior.

</details>


### [15] [Asymptotic behavior and sharp estimates for spreading fronts in a cooperative system with free boundaries](https://arxiv.org/abs/2511.16300)
*Qian Qin,JinJing Jiao,Zhiguo Wang,Hua Nie*

Main category: math.AP

TL;DR: This paper studies a reaction-diffusion system with two free boundaries modeling cooperative species invasion, showing a spreading-vanishing dichotomy and determining asymptotic spreading speeds using semi-wave analysis.


<details>
  <summary>Details</summary>
Motivation: To understand the long-term dynamics of cooperative species invasion in reaction-diffusion systems with free boundaries, particularly how two species either spread or vanish over time.

Method: Analysis of a reaction-diffusion system with two free boundaries using semi-wave systems to study asymptotic behavior and spreading speeds.

Result: The system exhibits a spreading-vanishing dichotomy; when spreading occurs, the fronts move at specific asymptotic speeds and solutions converge to semi-wave solutions as time increases.

Conclusion: The research provides deeper insights into cooperative species dynamics in reaction-diffusion systems with free boundaries, establishing fundamental spreading patterns and convergence properties.

Abstract: This paper investigates the dynamics of a reaction-diffusion system with two free boundaries, modeling the invasion of two cooperative species, where the free boundaries represent expanding fronts. We first analyze the long-term behavior of the system, showing that it follows a spreading-vanishing dichotomy: the two species either spread across the entire region or eventually die out. In the case of spreading, we determine the asymptotic spreading speed of the fronts by using a semi-wave system and provide sharp estimates for the moving fronts. Additionally, we show that the solution to the system converges to the corresponding semi-wave solution as time tends to infinity. These results contribute to a deeper understanding of the long-term dynamics of cooperative species in reaction-diffusion systems with free boundaries.

</details>


### [16] [The analysis of resonant frequencies and blow-up estimates of close-to-touching subwavelength resonators in the two-dimensional Helmholtz system](https://arxiv.org/abs/2511.16387)
*Hongjie Dong,Hongjie Li,Longjuan Xu*

Main category: math.AP

TL;DR: Analysis of wave scattering by two closely spaced high-contrast inclusions in 2D Helmholtz equation, revealing distinct sub-wavelength resonant modes with different asymptotic behaviors compared to 3D case, and quantifying gradient blow-up rates between resonators.


<details>
  <summary>Details</summary>
Motivation: To understand wave scattering phenomena in systems with closely spaced high-contrast inclusions, particularly how resonant modes behave in 2D versus 3D Helmholtz settings and the resulting field concentration effects.

Method: Modeling wave scattering using the two-dimensional Helmholtz equation for a pair of closely spaced inclusions with high contrast physical parameters, employing asymptotic analysis to study resonant modes.

Result: Identified two distinct sub-wavelength resonant modes with different leading-order asymptotic frequency behaviors that differ significantly from 3D case, and quantified the gradient blow-up rates of the wave field localized between the two resonators.

Conclusion: The 2D Helmholtz configuration with closely spaced high-contrast inclusions exhibits unique resonant behavior distinct from 3D settings, with specific gradient blow-up characteristics that provide insights into wave field concentration phenomena.

Abstract: In this paper, we investigate wave scattering by a pair of closely spaced inclusions embedded in a homogeneous medium, characterized by a high contrast physical parameters. The system is modeled by the two-dimensional Helmholtz equation. We show that this configuration exhibits two sub-wavelength resonant modes, whose frequencies display distinct leading-order asymptotic behaviors. These findings differ significantly from those in the three-dimensional Helmholtz setting. Furthermore, we provide a quantitative analysis of the gradient blow-up rates for the wave field localized between the two resonators.

</details>


### [17] [Mosco convergence framework for singular limits of gradient flows on Hilbert spaces with applications](https://arxiv.org/abs/2511.16486)
*Yoshikazu Giga,Michał Łasica,Piotr Rybka*

Main category: math.AP

TL;DR: The paper establishes convergence results for gradient flows across different Hilbert spaces using connecting operators and generalized Mosco convergence.


<details>
  <summary>Details</summary>
Motivation: To develop a framework for analyzing convergence of gradient flows defined on different Hilbert spaces, which is important for applications like thin domains, dynamic boundary conditions, and discrete-to-continuum limits.

Method: Introduce connecting operators to link different Hilbert spaces, generalize Mosco convergence of functionals to this setting, and prove convergence results for gradient flows.

Result: Successfully proved convergence results for gradient flows using the developed framework, with applications demonstrated in various examples.

Conclusion: The proposed framework using connecting operators and generalized Mosco convergence provides a robust method for analyzing gradient flow convergence across different Hilbert spaces.

Abstract: We consider the question of convergence of a sequence of gradient flows defined on different Hilbert spaces. In order to give meaning to this idea, we introduce a notion of connecting operators. This permits us to generalize the concept of Mosco convergence of functionals to our present setting, and state a desired convergence result for gradient flows, which we then prove. We present a variety of examples, including thin domains, dynamic boundary conditions, and discrete-to-continuum limits.

</details>


### [18] [Regularity for elliptic equations with monomial weights](https://arxiv.org/abs/2511.16516)
*Gabriele Cora,Gabriele Fioravanti,Francesco Pagliarin,Stefano Vita*

Main category: math.AP

TL;DR: The paper studies regularity properties for solutions to degenerate elliptic equations with monomial weights along orthogonal hyperplanes, proving C^{0,α} and C^{1,α} estimates up to corners using regularization, blow-up arguments, and Liouville theorems.


<details>
  <summary>Details</summary>
Motivation: To understand regularity properties of solutions to degenerate elliptic equations where the degeneracy occurs along orthogonal hyperplanes, which is important for applications in PDE theory and inequalities like Caffarelli-Kohn-Nirenberg.

Method: Uses a regularization-approximation procedure, blow-up arguments, and Liouville theorems to analyze the degenerate elliptic equations with monomial weights.

Result: Proves C^{0,α} and C^{1,α} estimates for solutions up to the corners formed by intersections of hyperplanes, and provides smoothness results for isotropic homogeneous equations.

Conclusion: The developed techniques successfully establish regularity estimates for degenerate elliptic equations with monomial weights, with applications to Caffarelli-Kohn-Nirenberg inequalities.

Abstract: We study regularity properties for solutions to elliptic equations that are degenerate or singular along orthogonal hyperplanes. The degenerate ellipticity is carried out by a weight term which is the monomial product of different powers of the distance functions to each hyperplane; that is, given the space dimension $d\geq2$, the number of orthogonally crossing hyperplanes $1\leq n\leq d$ and the generic variable point $z=(x,y)\in\mathbb R^{d-n}\times\mathbb R^n$, then the weight is given by $ω(y)=\prod_{i=1}^ny_i^{a_i}$ with $a_i>-1$, $y_i=\mathrm{dist}(z,Σ_i)$ and $Σ_i=\{y_i=0\}$. We prove $C^{0,α}$ and $C^{1,α}$ estimates up to the corners formed by the intersections of two or more hyperplanes, for solutions of the conormal problem with variable coefficients. This is done by a regularization-approximation procedure, a blow-up argument and Liouville theorems. Finally, we provide smoothness of solutions when the equation is isotropic and homogeneous, and we show an application to Caffarelli-Kohn-Nirenberg inequalities with monomial weights.

</details>


### [19] [Non-isoparametric Serrin domains of $\mathbb{S}^3$ with connected toric boundary](https://arxiv.org/abs/2511.16531)
*Andrea Bisterzo,Shigeru Sakaguchi*

Main category: math.AP

TL;DR: The paper proves existence of two types of non-radial Serrin domains in S³ with connected boundaries that are neither geodesic spheres nor Clifford tori, using bifurcation theory from symmetric solutions.


<details>
  <summary>Details</summary>
Motivation: To investigate overdetermined torsion problems in curved spaces and show that Serrin's rigidity result (which holds in Euclidean space) fails in the presence of curvature, by constructing nontrivial geometric configurations.

Method: Uses the Crandall-Rabinowitz bifurcation theorem to construct branches of non-radial solutions bifurcating from a family of radial symmetric solutions, employing an implicit function approach.

Result: Established existence of two distinct types of Serrin domains in S³ - one with small volume and one with large volume - both having connected boundaries that are neither geodesic spheres nor Clifford tori.

Conclusion: The examples demonstrate that rigidity of Serrin-type results fails in curved spaces, highlighting new geometric configurations for the torsion problem in the three-dimensional sphere.

Abstract: We investigate the overdetermined torsion problem
  $\begin{cases} -Δu = 1 & \text{in}\ Ω\\ u=0 & \text{on}\ \partial Ω\\ \frac{\partial u}{\partial ν}=\text{const.} & \text{on}\ \partial Ω, \end{cases}$
  where $Ω$ is a smooth Riemannian domain. Domains admitting a solution to this problem are called \textit{Serrin domains}, after the celebrated work of Serrin \cite{Se71}, where is proved that in $\mathbb{R}^n$ such domains are geodesic balls. In the present paper we establish the existence of two distinct types of Serrin domains of $\mathbb{S}^3$, respectively of small and large volume, each of whose boundary is connected and is neither isometric to a geodesic sphere nor to a Clifford torus. These domains arise as nontrivial perturbations of some classical symmetric solutions to the same problem. Our approach relies on an implicit construction based on the Crandall-Rabinowitz bifurcation theorem, which allows us to detect branches of non-radial solutions bifurcating from a family of radial ones. The resulting examples highlight new geometric configurations of the torsion problem in the three-dimensional sphere, providing another proof of the fact that the rigidity of Serrin-type results can fail in the presence of curvature.

</details>


### [20] [A critical Hardy-Rellich inequality](https://arxiv.org/abs/2511.16537)
*Hernán Castro*

Main category: math.AP

TL;DR: Proves a critical Hardy-Rellich type inequality showing that the gradient of u/|x| in L^N norm is bounded by the Laplacian of u in L^N norm for functions vanishing at the origin.


<details>
  <summary>Details</summary>
Motivation: To establish a critical version of Hardy-Rellich inequalities, which are fundamental in analysis and PDEs, particularly for functions with singular behavior at the origin.

Method: Mathematical proof using functional analysis and inequality techniques for functions in C^∞_c(R^N\{0}), establishing the existence of a positive constant C_N.

Result: Successfully proves the inequality ∫|∇(u/|x|)|^N dx ≤ C_N ∫|Δu|^N dx for all N≥1 and u vanishing at the origin.

Conclusion: The critical Hardy-Rellich inequality holds with an optimal constant C_N>0, extending classical results to the critical case.

Abstract: In this work, we prove a critical version of a Hardy-Rellich type inequality. We show that for $N\geq 1$ there exists a constant $C_N>0$ such that \[ \int_{\mathbb R^N}\left|\nabla\left(\frac{u(x)}{|x|}\right)\right|^N\,\mathrm{d}x\leq C_N\int_{\mathbb R^N}\left|Δu(x)\right|^N\,\mathrm{d}x, \] for any $u\in C^\infty_c(\mathbb R^N\setminus\left\{0\right\})$.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [21] [Implicit and explicit treatments of model error in numerical simulation](https://arxiv.org/abs/2511.15934)
*Danny Smyl*

Main category: physics.comp-ph

TL;DR: Review of techniques developed over the past two decades to approximate and account for model errors in numerical simulations, covering both implicit and explicit error treatment methods across various computational physics contexts.


<details>
  <summary>Details</summary>
Motivation: Numerical simulations inherently suffer from model errors due to unmodeled physics, idealizations, and discretization, which affect accuracy in inverse problems, data assimilation, and predictive modeling.

Method: Survey of major approaches including Bayesian approximation error framework, embedded internal error models, probabilistic numerical methods, model discrepancy modeling, machine-learning-based correction, multi-fidelity strategies, and various error estimators.

Result: Comprehensive overview showing how these methods improve predictive performance and uncertainty quantification in practical applications ranging from engineering design to Earth-system science.

Conclusion: The review provides practitioners with accessible methods for incorporating error quantification into PDE solvers, inverse problem workflows, and data assimilation systems, emphasizing conceptual differences between implicit and explicit error treatment.

Abstract: Numerical simulations of physical systems invariably suffer from model errors stemming from unmodeled physics, idealizations, and discretization. This article provides a review of techniques developed in the past two decades to approximate and account for these model errors, both implicitly and explicitly. Beginning from fundamental definitions of model-form versus numerical error, we frame model error in inverse problems, data assimilation, and predictive modeling contexts. We then survey major approaches: the Bayesian approximation error framework for implicit error quantification, embedded internal error models for structural uncertainty, probabilistic numerical methods for discretization uncertainty, model discrepancy modeling in Bayesian calibration and its recent extensions, machine-learning-based discrepancy correction, multi-fidelity and hybrid modeling strategies, as well as residual-based, variational, and adjoint-driven error estimators. Throughout, we emphasize conceptual underpinnings of implicit versus explicit error treatment and highlight how these methods improve predictive performance and uncertainty quantification in practical applications ranging from engineering design to Earth-system science. Each section provides an overview of key developments with an extensive list of references to facilitate further reading. The review is written for practitioners of large-scale computational physics and engineering simulation, emphasizing how these methods can be incorporated into PDE solvers, inverse problem workflows, and data assimilation systems.

</details>


### [22] [A physics-inspired momentum-based gradient method](https://arxiv.org/abs/2511.16441)
*Jianing Zhang,Rumei Liu*

Main category: physics.comp-ph

TL;DR: A nonlinear momentum method inspired by non-Newtonian mechanical systems improves convergence in gradient optimization by using anharmonic kinetic energy and nonlinear damping.


<details>
  <summary>Details</summary>
Motivation: To enhance convergence performance of momentum-based gradient optimization algorithms by drawing inspiration from non-Newtonian mechanical systems dynamics.

Method: Constructs generalized optimization dynamics by extending kinetic energy formulation and incorporating nonlinear damping term, using anharmonic kinetic energy for inertial effects and nonlinear damping for flexible momentum control.

Result: Numerical experiments show faster convergence and higher robustness compared to classical momentum algorithms, with strong performance on nonconvex objectives.

Conclusion: Dynamical systems from physics provide valuable insights for developing efficient optimization methods, particularly suitable for inverse photonic design problems.

Abstract: In this work, a nonlinear momentum method is introduced to improve the convergence performance of momentum-based gradient optimization algorithms. The method is motivated by the dynamics of non-Newtonian mechanical systems, where conventional momentum schemes can be interpreted as a dynamical model with quadratic kinetic energy and linear damping. Based on this analogy, a generalized optimization dynamics is constructed by extending the kinetic energy formulation and incorporating a nonlinear damping term. An anharmonic kinetic energy function can be employed to represent the inertial effect of accumulated gradient information during the iterations, while the nonlinear damping mechanism enables a more flexible control of the momentum contribution along the convergence trajectory. Numerical experiments indicate that the method exhibits faster convergence and higher robustness compared to classical momentum algorithms. Moreover, its strong performance on nonconvex objectives makes it particularly suitable for inverse photonic design problems. The results suggest that dynamical systems from physics can provide a view towards the development of efficient optimization methods.

</details>


### [23] [Deep Learning Framework for Enhanced Neutrino Reconstruction of Single-line Events in the ANTARES Telescope](https://arxiv.org/abs/2511.16614)
*A. Albert,S. Alves,M. André,M. Ardid,S. Ardid,J. -J. Aubert,J. Aublin,B. Baret,S. Basa,Y. Becherini,B. Belhorma,F. Benfenati,V. Bertin,S. Biagi,J. Boumaaza,M. Bouta,M. C. Bouwhuis,H. Brânzaş,R. Bruijn,J. Brunner,J. Busto,B. Caiffi,D. Calvo,S. Campion,A. Capone,F. Carenini,J. Carr,V. Carretero,T. Cartraud,S. Celli,L. Cerisy,M. Chabab,R. Cherkaoui El Moursli,T. Chiarusi,M. Circella,J. A. B. Coelho,A. Coleiro,R. Coniglione,P. Coyle,A. Creusot,A. F. Díaz,B. De Martino,C. Distefano,I. Di Palma,C. Donzaud,D. Dornic,D. Drouhin,T. Eberl,A. Eddymaoui,T. van Eeden,D. van Eijk,S. El Hedri,N. El Khayati,A. Enzenhöfer,P. Fermani,G. Ferrara,F. Filippini,L. Fusco,S. Gagliardini,J. García-Méndez,C. Gatius Oliver,P. Gay,N. Geißelbrecht,H. Glotin,R. Gozzini,R. Gracia Ruiz,K. Graf,C. Guidi,L. Haegel,H. van Haren,A. J. Heijboer,Y. Hello,L. Hennig,J. J. Hernández-Rey,J. Hößl,F. Huang,G. Illuminati,B. Jisse-Jung,M. de Jong,P. de Jong,M. Kadler,O. Kalekin,U. Katz,A. Kouchner,I. Kreykenbohm,V. Kulikovskiy,R. Lahmann,M. Lamoureux,A. Lazo,D. Lefèvre,E. Leonora,G. Levi,S. Le Stum,S. Loucatos,J. Manczak,M. Marcelin,A. Margiotta,A. Marinelli,J. A. Martínez-Mora,P. Migliozzi,A. Moussa,R. Muller,S. Navas,E. Nezri,B. Ó Fearraigh,E. Oukacha,A. M. Păun,G. E. Păvălaş,S. Peña-Martínez,M. Perrin-Terrin,P. Piattelli,C. Poiré,V. Popa,T. Pradier,N. Randazzo,D. Real,G. Riccobene,A. Romanov,A. Sánchez Losa,A. Saina,F. Salesa Greus,D. F. E. Samtleben,M. Sanguineti,P. Sapienza,F. Schüssler,J. Seneca,M. Spurio,Th. Stolarczyk,M. Taiuti,Y. Tayalati,B. Vallage,G. Vannoye,V. Van Elewyck,S. Viola,D. Vivolo,J. Wilms,S. Zavatarelli,A. Zegarelli,J. D. Zornoza,J. Zúñiga*

Main category: physics.comp-ph

TL;DR: N-Fit is a neural network algorithm that improves reconstruction of single-line neutrino events in ANTARES telescope using deep learning with convolutional layers, mixture density outputs, and transfer learning for direction, position, energy estimation, and event classification.


<details>
  <summary>Details</summary>
Motivation: To improve reconstruction of low-energy neutrino events (~100 GeV) detected by single lines in ANTARES telescope, which traditional χ²-fit methods struggle with, particularly for azimuthal angle predictions and energy estimation.

Method: Uses neural network with deep convolutional layers, mixture density output layers, and transfer learning. Divides reconstruction into two branches for track and shower topologies, with sub-models for spatial estimation (direction/position) and energy inference, combined for event classification.

Result: Significantly refines zenithal angle estimation and delivers reliable azimuthal angle predictions previously unattainable. Shows reduction in mean and median absolute errors across all reconstructed parameters in Monte Carlo simulations and data.

Conclusion: N-Fit demonstrates potential for advancing multimessenger astrophysics and enhancing ability to probe fundamental physics beyond Standard Model using single-line events from ANTARES data.

Abstract: We present the $N$-fit algorithm designed to improve the reconstruction of neutrino events detected by a single line of the ANTARES underwater telescope, usually associated with low energy neutrino events ($\sim$ 100 GeV). $N$-Fit is a neural network model that relies on deep learning and combines several advanced techniques in machine learning --deep convolutional layers, mixture density output layers, and transfer learning. This framework divides the reconstruction process into two dedicated branches for each neutrino event topology --tracks and showers-- composed of sub-models for spatial estimation --direction and position-- and energy inference, which later on are combined for event classification. Regarding the direction of single-line events, the $N$-Fit algorithm significantly refines the estimation of the zenithal angle, and delivers reliable azimuthal angle predictions that were previously unattainable with traditional $χ^2$-fit methods. Improving on energy estimation of single-line events is a tall order; $N$-Fit benefits from transfer learning to efficiently integrate key characteristics, such as the estimation of the closest distance from the event to the detector. $N$-Fit also takes advantage from transfer learning in event topology classification by freezing convolutional layers of the pretrained branches. Tests on Monte Carlo simulations and data demonstrate a significant reduction in mean and median absolute errors across all reconstructed parameters. The improvements achieved by $N$-Fit highlight its potential for advancing multimessenger astrophysics and enhancing our ability to probe fundamental physics beyond the Standard Model using single-line events from ANTARES data.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [24] [Asymptotic-preserving semi-implicit finite volume scheme for Extended Magnetohydrodynamics](https://arxiv.org/abs/2511.15937)
*Yi Han Toh,Joshua Dolence,Karthik Duraisamy*

Main category: physics.plasm-ph

TL;DR: A finite volume scheme for extended magnetohydrodynamics (XMHD) that maintains divergence-free magnetic fields and naturally transitions to ideal, resistive, and Hall MHD limits.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical scheme that can handle extended MHD equations including electron inertia and displacement current while maintaining numerical stability and compatibility with existing ideal MHD methods.

Method: Reformulates XMHD equations to use ideal MHD Riemann solvers and constrained transport method. Uses semi-implicit FV scheme with relaxation system approach, explicit 2nd-order Runge-Kutta time integration with operator splitting, and density-dependent slope limiter for stability.

Result: The algorithm successfully asymptotes to ideal MHD limit and shows promising results for resistive and Hall MHD limits, verified against published test problems.

Conclusion: The developed scheme provides an effective approach for solving XMHD equations while maintaining compatibility with existing ideal MHD formulations and achieving numerical stability across different MHD regimes.

Abstract: A Finite Volume (FV) scheme is developed for solving the extended magnetohydrodynamic (XMHD) equations, yielding accurate results in the ideal, resistive, and Hall MHD limits. This is accomplished by first re-writing the XMHD equations such that it allows the algorithm to retain the use of ideal MHD Riemann solvers and the constrained transport method to preserve divergence-free magnetic fields. Incorporation of electron inertia and displacement current introduces additional numerical stiffness which motivates a semi-implicit FV scheme that re-formulates the XMHD model as a relaxation system. The equations are then advanced in time using an explicit 2nd-order Runge-Kutta scheme with operator splitting applied to the implicit source term updates at each sub-stage. For additional numerical stability, density-dependent slope limiter is implemented to increase flux diffusivity at low density where non-ideal effects become significant. The algorithm is subsequently implemented on Artemis, a multifluid radiation hydrodynamics code built on Parthenon framework for high-performance computing (HPC) and adaptive mesh refinement (AMR) support. As the new algorithm retains many aspects of the ideal MHD formulations, it asymptotes naturally to the ideal MHD limit. Moreover, it shows promising results at the resistive and Hall MHD limits. This is verified against published test problems for ideal, resistive and Hall MHD.

</details>


### [25] [Effects of Multi-scale Coupling on Particle Acceleration and Energy Partition in Magnetic Reconnection](https://arxiv.org/abs/2511.15988)
*Alexander Velberg,Adam Stanier,Xiaocan Li,Fan Guo,William Daughton,Nuno F. Loureiro*

Main category: physics.plasm-ph

TL;DR: Particle-in-cell simulations show that in large-scale magnetic reconnection, secondary current sheets and downstream turbulence dominate energy dissipation, decoupling it from the primary reconnection site and enabling additional particle acceleration channels.


<details>
  <summary>Details</summary>
Motivation: To understand how kinetic and macroscopic scales interact during magnetic reconnection in strongly-magnetized relativistic pair plasmas, particularly focusing on energy dissipation mechanisms.

Method: Used particle-in-cell simulations of magnetic island coalescence in strongly-magnetized, relativistic pair plasma regime with large system sizes.

Result: Secondary current sheet formation and downstream turbulence driven by reconnection outflows dominate global energy dissipation, activating additional particle acceleration channels that modify particle energy spectra and produce significant non-thermal particle populations.

Conclusion: In large-scale magnetic reconnection, energy dissipation becomes causally connected but spatially and temporally decoupled from the primary reconnection site, with secondary processes playing crucial roles in particle acceleration and energy distribution.

Abstract: The interplay between kinetic and macroscopic scales during magnetic reconnection is investigated using particle-in-cell simulations of magnetic island coalescence in the strongly-magnetized, relativistic pair plasma regime. For large system sizes, secondary current sheet formation and downstream turbulence driven by the reconnection outflows dominate the global energy dissipation so that it is causally connected, but spatially and temporally de-coupled from the primary reconnecting current sheet. When compared to simulations of an isolated, force-free current sheet, these dynamics activate additional particle acceleration channels which are responsible for a significant population of the non-thermal particles, modifying the particle energy spectra.

</details>


### [26] [Algorithms and optimizations for global non-linear hybrid fluid-kinetic finite element stellarator simulations](https://arxiv.org/abs/2511.16412)
*Luca Venerando Greco*

Main category: physics.plasm-ph

TL;DR: Novel globally coupled projection scheme in JOREK framework for stellarator plasma modeling, using FFT-accelerated linear system and 3D R-Tree for efficient particle localization, achieving spectral convergence on Wendelstein 7-X geometry.


<details>
  <summary>Details</summary>
Motivation: Predictive modeling of stellarator plasmas faces computational challenges due to non-axisymmetric geometry that couples toroidal Fourier modes, requiring hybrid fluid-kinetic models for accurate particle species dynamics.

Method: Globally coupled projection scheme using unified linear system for all toroidal harmonics simultaneously, accelerated with FFT for matrix construction, and 3D R-Tree spatial index for efficient particle localization.

Result: Achieves spectral convergence on Wendelstein 7-X stellarator geometries, significantly outperforming uncoupled approaches which show poor performance.

Conclusion: Provides validated high-fidelity computational tool crucial for predictive analysis and optimization of next-generation stellarator designs.

Abstract: Predictive modeling of stellarator plasmas is crucial for advancing nuclear fusion energy, yet it faces unique computational difficulties. One of the main challenges is accurately simulating the dynamics of specific particle species that are not well captured by fluid models, which necessitates the use of hybrid fluid-kinetic models. The non-axisymmetric geometry of stellarators fundamentally couples the toroidal Fourier modes, in contrast to what happens in tokamaks, requiring different numerical and computational treatment.
  This work presents a novel, globally coupled projection scheme inside the JOREK finite element framework. The approach ensures a self-consistent and physically accurate transfer of kinetic markers to the fluid grid, effectively handling the complex 3D mesh by constructing and solving a unified linear system that encompasses all toroidal harmonics simultaneously. To manage the computational complexity of this coupling, the construction of the system's matrix is significantly accelerated using the Fast Fourier Transform (FFT). The efficient localization of millions of particles is made possible by implementing a 3D R-Tree spatial index, which supports this projection and ensures computational tractability at scale.
  On realistic Wendelstein 7-X stellarator geometries, the fidelity of the framework is rigorously shown. In sharp contrast to the uncoupled approaches' poor performance, quantitative convergence tests verify that the coupled scheme attains the theoretically anticipated spectral convergence.
  This study offers a crucial capability for the predictive analysis and optimization of next-generation stellarator designs by developing a validated, high-fidelity computational tool.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [27] [Entrywise Approximate Solutions for SDDM Systems in Almost-Linear Time](https://arxiv.org/abs/2511.16570)
*Angelo Farfan,Mehrdad Ghadiri,Junzhao Yang*

Main category: cs.DS

TL;DR: Fast algorithm for approximating solutions to SDDM linear systems in near-linear time


<details>
  <summary>Details</summary>
Motivation: Need efficient solvers for symmetric diagonally dominant M-matrices (principal submatrices of graph Laplacians) which arise in many applications

Method: Developed an algorithm that computes entrywise approximations to Lx = b for SDDM matrices using iterative methods with high probability guarantees

Result: Achieves Õ(mn^o(1)) time complexity where m is nonzero entries and n is system dimension

Conclusion: Provides near-linear time solver for SDDM systems with high probability approximation guarantees

Abstract: We present an algorithm that given any invertible symmetric diagonally dominant M-matrix (SDDM), i.e., a principal submatrix of a graph Laplacian, $\boldsymbol{\mathit{L}}$ and a nonnegative vector $\boldsymbol{\mathit{b}}$, computes an entrywise approximation to the solution of $\boldsymbol{\mathit{L}} \boldsymbol{\mathit{x}} = \boldsymbol{\mathit{b}}$ in $\tilde{O}(m n^{o(1)})$ time with high probability, where $m$ is the number of nonzero entries and $n$ is the dimension of the system.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [28] [Charge-Ordered States and the Phase Diagram of the Extended Hubbard Model on the Bethe lattice](https://arxiv.org/abs/2511.16603)
*Aleksey Alekseev,Konrad Jerzy Kapcia*

Main category: cond-mat.str-el

TL;DR: Study of extended Hubbard model on Bethe lattice using Hartree mean-field approximation, revealing charge-ordered insulating/metallic and non-charge-ordered states with temperature-dependent phase diagrams.


<details>
  <summary>Details</summary>
Motivation: To understand the effects of onsite and intersite interactions in the extended Hubbard model using a simplified analytical approach that avoids numerical complexities.

Method: Standard Hartree mean-field approximation applied to the extended Hubbard model on the Bethe lattice, allowing analytical derivations and geometry-independent analysis.

Result: Increasing onsite repulsion suppresses charge order and transitions the system from insulating to metallic behavior; finite-temperature phase diagrams show charge-ordered insulating, charge-ordered metallic, and non-charge-ordered states.

Conclusion: The mean-field approach provides a simpler yet effective framework for analyzing complex phenomena in the extended Hubbard model, yielding analytical insights and avoiding numerical issues.

Abstract: We study the extended Hubbard model (EHM) with both onsite Hubbard interaction and the intersite density-density interaction between nearest-neighbors using the standard Hartree mean-field approximation (MFA) on the Bethe lattice. We found that, at the ground state, the system can be in a charge-ordered insulating (COI), a charge-order metallic (COM) or a non-charge-ordered (NO) state. Moreover, the finite-temperature phase diagrams are presented. Several observables like a charge-order parameter, a spectral function, and particularly at finite temperatures, a charge carrier concentration (to visualize the degree of metallicity) are analyzed. The results show that increasing onsite repulsion suppresses charge order and change the properties of the system from insulating to metallic. Worth noting, that a number of phenomena can be found within the MFA, where their analysis is much simpler than in more advanced approaches. The method used for the EHM on the Bethe lattice also allows for a series of analytical derivations and simplification to see general geometry-independent features and analytical results, avoiding the numerical inaccuracies and other issues that appear with a purely numerical solution.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [29] [Li-P-S Electrolyte Materials as a Benchmark for Machine-Learned Interatomic Potentials](https://arxiv.org/abs/2511.16569)
*Natascia L. Fragapane,Volker L. Deringer*

Main category: cond-mat.mtrl-sci

TL;DR: LiPS-25 benchmark dataset for solid-state electrolyte materials with performance tests for MLIP models, focusing on hyperparameter effects and fine-tuning of pre-trained models.


<details>
  <summary>Details</summary>
Motivation: Growing need for robust benchmarking of machine-learned interatomic potentials due to increasing availability of MLIP models for materials simulations.

Method: Created LiPS-25 dataset with crystalline and amorphous configurations from Li2S-P2S5 system, developed performance tests from numerical error metrics to physical evaluation tasks, and conducted experiments on graph-based MLIP architectures.

Result: Benchmark assesses hyperparameter effects and fine-tuning behavior of pre-trained MLIP models, providing chemically insightful evaluation methodology.

Conclusion: The benchmarking approach and code implementations can be readily adapted to other material systems beyond Li-P-S solid-state electrolytes.

Abstract: With the growing availability of machine-learned interatomic potential (MLIP) models for materials simulations, there is an increasing demand for robust, automated, and chemically insightful benchmarking methodologies. In response, we here introduce LiPS-25, a curated benchmark dataset for a canonical series of solid-state electrolyte materials from the Li2S-P2S5 pseudo-binary compositional line, including crystalline and amorphous configurations. Together with the dataset, we present a suite of performance tests that range from conventional numerical error metrics to physically motivated evaluation tasks. With a focus on graph-based MLIP architectures, we run numerical experiments that assess (i) the effect of hyperparameters and (ii) the fine-tuning behavior of selected pre-trained ("foundational") MLIP models. Beyond the Li-P-S solid-state electrolytes, we expect that such benchmarks and their code implementations can be readily adapted to other material systems.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [30] [Age-structured model of dengue transmission dynamics with time-varying parameters, and its application to Brazil](https://arxiv.org/abs/2511.16179)
*Ihtisham Ul Haq,Serge Richard*

Main category: q-bio.PE

TL;DR: Developed an age-structured mathematical model for dengue transmission dynamics, analyzed its properties including reproduction numbers, applied it to Brazil using 2021-2024 data, and identified key factors like age distribution, vector dynamics, and climate.


<details>
  <summary>Details</summary>
Motivation: To investigate dengue transmission dynamics using mathematical modeling, particularly focusing on how age structure, time-dependent parameters, and environmental factors influence disease spread in Brazil.

Method: Created an age-structured mathematical model with time-dependent parameters, analyzed disease-free steady states and reproduction numbers, applied to Brazilian weekly time series data from 2021-2024, estimated transmission rates from epidemiological and environmental data.

Result: Successfully modeled dengue transmission in Brazil, estimated time-varying reproduction numbers, identified sensitive parameters affecting dynamics, and provided predictions using different transmission rates.

Conclusion: Population age distribution, vector population dynamics, and climate are crucial factors in dengue transmission dynamics in Brazil, providing deeper understanding for disease control strategies.

Abstract: An age structured mathematical model with time dependent parameters is developed to investigate the dynamics of dengue transmission. Its properties are thoroughly analyzed in the first part of this work, as for example its disease free steady state, the corresponding effective reproduction numbers, its basic reproduction number (obtained via the Euler and Lotka equation and the next generation matrix approach). We also provide formulas for the time-varying effective reproduction number, and draw relations with the instantaneous growth rate. In the second part, we apply this model to Brazil and use weekly time series data from this country. Various medical parameters are firstly evaluated from these data, and an extensive numerical simulations for the period 2021 to 2024 is then carried out. Estimation of the transmission rates are derived both from epidemiological data and from environmental data such as temperature and humidity. The time-varying effective reproduction numbers are then estimated on these data, following the theoretical investigations performed in the first part. The sensitive parameters that significantly affect the model dynamics are presented graphically. Model predictions for following year by using different transmission rates are finally presented. Our findings show the importance of population age distribution, vector population dynamics, and climate, contributing to a deeper understanding of dengue transmission dynamics in Brazil.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [31] [Human-aligned Quantification of Numerical Data](https://arxiv.org/abs/2511.15723)
*Anton Kolonin*

Main category: physics.data-an

TL;DR: The paper evaluates metrics for quantifying numerical data into meaningful categories, finding that Silhouette coefficient above 0.65 and Dip Test below 0.5 indicate good classification, with Silhouette aligning better with human intuition than compression-based methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of determining when numerical data can be naturally quantified and identifying meaningful value ranges (quantums) that represent system states, and to compare computational metrics with human intuition.

Method: Assessment of information compression-based metrics and Silhouette coefficient for quantifying numerical data, investigating their correlation with each other and with human intuition.

Result: Classification of numeric data into distinct categories is associated with Silhouette coefficient > 0.65 and Dip Test < 0.5; otherwise data follows unimodal normal distribution. Silhouette coefficient aligns better with human intuition than normalized centroid distance method.

Conclusion: The Silhouette coefficient is more effective than compression-based methods for quantifying numerical data and better matches human intuition, providing clear thresholds for determining when data can be meaningfully categorized.

Abstract: Quantifying numerical data involves addressing two key challenges: first, determining whether the data can be naturally quantified, and second, identifying the numerical intervals or ranges of values that correspond to specific value classes, referred to as "quantums," which represent statistically meaningful states. If such quantification is feasible, continuous streams of numerical data can be transformed into sequences of "symbols" that reflect the states of the system described by the measured parameter. People often perform this task intuitively, relying on common sense or practical experience, while information theory and computer science offer computable metrics for this purpose. In this study, we assess the applicability of metrics based on information compression and the Silhouette coefficient for quantifying numerical data. We also investigate the extent to which these metrics correlate with one another and with what is commonly referred to as "human intuition." Our findings suggest that the ability to classify numeric data values into distinct categories is associated with a Silhouette coefficient above 0.65 and a Dip Test below 0.5; otherwise, the data can be treated as following a unimodal normal distribution. Furthermore, when quantification is possible, the Silhouette coefficient appears to align more closely with human intuition than the "normalized centroid distance" method derived from information compression perspective.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [32] [A Fast Relax-and-Round Approach to Unit Commitment for Data Center Own Generation](https://arxiv.org/abs/2511.16420)
*Shaked Regev,Eve Tsybina,Slaven Peles*

Main category: math.OC

TL;DR: Proposes a relaxed unit commitment formulation that allows fractional generator states instead of binary decisions, enabling fast continuous optimization with minor accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Data centers are increasingly using their own generators, creating systems with thousands of fast-cycling units where traditional mixed-integer unit commitment becomes computationally infeasible.

Method: Relaxes binary commitment decisions to allow generators to be fractionally on, uses continuous solvers, then applies rounding to get feasible solutions.

Result: Solution time reduced from 10 hours to less than 1 second for 276-unit system, scales to tens of thousands of generators with minor accuracy degradation.

Conclusion: The approach enables solving large-scale unit commitment problems at interconnection scale with massive speed improvements and GPU-compatible parallel computation.

Abstract: The rapid growth of data centers increasingly requires data center operators to "bring own generation" to complement the available utility power plants to supply all or part of data center load. This practice sharply increases the number of generators on the bulk power system and shifts operational focus toward fuel costs rather than traditional startup and runtime constraints. Conventional mixed-integer unit commitment formulations are not well suited for systems with thousands of flexible, fast-cycling units. We propose a unit commitment formulation that relaxes binary commitment decisions by allowing generators to be fractionally on, enabling the use of algorithms for continuous solvers. We then use a rounding approach to get a feasible unit commitment. For a 276-unit system, solution time decreases from 10 hours to less than a second, with minor accuracy degradation. Our approach scales with no issues to tens of thousands of generators, which allows solving problems on the scale of the major North America interconnections. The bulk of computation is parallel and GPU compatible, enabling further acceleration in future work.

</details>


### [33] [A novel way of computing the shape derivative for a class of non-smooth PDEs and its impact on deriving necessary conditions for locally optimal shapes](https://arxiv.org/abs/2511.16127)
*Livia Betz*

Main category: math.OC

TL;DR: Derives necessary conditions for locally optimal shapes in non-smooth PDE-constrained optimization using functional variational approach, avoiding domain extensions or PDE approximations.


<details>
  <summary>Details</summary>
Motivation: To handle shape optimization problems governed by non-smooth PDEs where traditional differentiability assumptions fail, requiring new sensitivity analysis techniques.

Method: Uses functional variational approach (FVA) to transform geometric optimization into optimal control, introduces novel sensitivity analysis technique, and computes directional derivatives of state w.r.t. functional variations.

Result: Develops new way to compute shape derivatives, handles pointwise observations and state derivatives on observation sets, establishes connection between locally optimal shapes and control problem minimizers.

Conclusion: Provides necessary conditions for locally optimal shapes in general non-smooth settings using directional differentiability of control-to-state map.

Abstract: We derive necessary conditions for locally optimal shapes of a design problem governed by a non-smooth PDE. The main particularity of the state system is the lack of differentiability of the nonlinearity. We work in the framework of the functional variational approach (FVA), which has the capacity to transfer geometric optimization problems into optimal control problems, the set of admissible shapes being parametrized by a large class of continuous mappings. In the FVA setting, we introduce a sensitivity analysis technique that is novel even for smooth PDEs. We emphasize that we do not resort to extensions on the hold-all domain or any kind of approximation of the original PDE. The computation of the directional derivative of the state w.r.t. functional variations results in a new way of computing the shape derivative. The presented approach allows us to handle in the objective pointwise observation and derivatives of the state on an observation set as well as distributed observation terms. In addition, we introduce the concept of locally optimal shapes and we put into evidence its connection to locally minimizers of the corresponding control problem. With directional differentiability results for the control-to-state map at our disposal, we can then state necessary conditions for locally optimal shapes in general non-smooth settings.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [34] [Micro-Macro Simulation of Shallow Water Moment Equations](https://arxiv.org/abs/2511.15737)
*Vilém Rožek*

Main category: physics.flu-dyn

TL;DR: The paper presents a micro-macro method for shallow water moment equations that achieves significant computational speed-up while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Shallow water equations are simplified models that often yield inaccurate results for shallow flows, while the more accurate shallow water moment equations are computationally expensive.

Method: Developed a micro-macro method that switches between models of varying detail levels, allowing for larger stable time steps in shallow water moment equations.

Result: The method achieves significant speed-up in computations while retaining sufficient accuracy, as demonstrated in dam break and wave transport tests.

Conclusion: The micro-macro method successfully balances computational efficiency and accuracy for shallow water moment equations, providing a practical solution for faster simulations.

Abstract: Shallow flows are governed by the Navier-Stokes equations. They are commonly modelled using the shallow water equations, a great simplification of the Navier-Stokes equations, which often yields inaccurate results. For that reason, a model called shallow water moment equations has been developed. It uses more equations and variables than the shallow water equations. While this model is significantly more accurate, it is also computationally more expensive. To speed up computations, the micro-macro method may be used. The micro-macro method switches between two models of varying levels of detail allowing for larger stable time steps. In this paper we formulate the micro-macro method for shallow water moment equations. We perform a theoretical runtime analysis of the method and present a series of results for a dam break test and a wave transport test. The micro-macro method achieves a significant speed-up while retaining a sufficient level of accuracy.

</details>


### [35] [Fourth branch of instability of Stokes' wave and dependence of corresponding growth rate on nonlinearity](https://arxiv.org/abs/2511.16436)
*A. O. Korotkevich,A. O. Prokofiev*

Main category: physics.flu-dyn

TL;DR: The study identifies the fourth superharmonic instability branch of Stokes' waves and validates phenomenological formulas for instability growth rates across different branches using computational results.


<details>
  <summary>Details</summary>
Motivation: To extend understanding of Stokes' wave instabilities by discovering the fourth superharmonic branch and testing the applicability of existing growth rate formulas beyond the first three known branches.

Method: Conducted massive computational analysis to reach the fourth superharmonic instability branch, then used these results to test phenomenological formulas for instability growth rates as functions of wave steepness (H/Λ).

Result: Successfully identified the fourth instability branch and demonstrated that phenomenological formulas derived from the first three branches remain valid for the fourth branch. Corrected the range of applicability for these relations and reported growth rates for all four instability branches.

Conclusion: The phenomenological formulas for instability growth rates are robust and applicable to the newly discovered fourth superharmonic branch, extending their validity beyond previously established ranges.

Abstract: Through a massive computation we reached the fourth superharmonic instability branch of the Stokes' wave. Using the obtained results we checked phenomenological formulae for the dependence of the instability growth rates corresponding to different branches of instability on the nonlinearity parameter (steepness, defined as the wave swing to wavelength ratio $H/Λ$) in the vicinity of the new instability branch appearance and far from it. It is demonstrated, that the formulae, the one obtained as a least squares fit using the information from the first three branches of instability and a phenomenological asymptotics, work for the fourth branch and previously reported branches as well. Range of applicability of the relations was corrected. Growth rates for all four instability branches are reported.

</details>


### [36] [General-purpose Data-driven Wall Model for Low-speed Flows Part I: Baseline Model](https://arxiv.org/abs/2511.16511)
*Yuenong Ling,Imran Hayat,Konrad Goc,Adrian Lozano-Duran*

Main category: physics.flu-dyn

TL;DR: A general-purpose wall model for LES using building-block flow principle with neural network regression to predict wall shear stress, outperforming traditional equilibrium wall models in 90% of test cases.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional equilibrium wall models and improve upon earlier building-block approaches by creating a more generalizable model applicable across complex geometries and flow conditions.

Method: Four-component model with baseline wall model (Part I), error model, classifier, and confidence score. Uses neural network regression with localized dimensionless inputs selected via information-theoretic criteria. Trained on 140 diverse datasets including new DNS data.

Result: Outperforms equilibrium wall model in 90% of test scenarios, maintains errors below 20% for 98% of cases across turbulent boundary layers, airfoils, Gaussian bumps, and full aircraft geometries.

Conclusion: The baseline wall model successfully captures diverse flow phenomena and demonstrates superior performance over traditional approaches, providing a robust foundation for the complete four-component framework.

Abstract: We present a general-purpose wall model for large-eddy simulation. The model builds on the building-block flow principle, leveraging essential physics from simple flows to train a generalizable model applicable across complex geometries and flow conditions. The model addresses key limitations of traditional equilibrium wall models (EQWM) and improves upon shortcomings of earlier building-block-based approaches. The model comprises four components: (i) a baseline wall model, (ii) an error model, (iii) a classifier, and (iv) a confidence score. The baseline model predicts the wall-shear stress, while the error model estimates epistemic errors and aleatoric errors, both used for uncertainty quantification. In Part I of this work, we present the baseline model, while the remaining three components are introduced in Part II. The baseline model is designed to capture a broad range of flow phenomena, including turbulence over curved walls and zero, adverse, and favorable mean pressure gradients, as well as flow separation and laminar flow. The problem is formulated as a regression task to predict wall shear stress using a neural network. Model inputs are localized in space and dimensionless, with their selection guided by information-theoretic criteria. Training data include, among other cases, a newly generated direct numerical simulation dataset of turbulent boundary layers under favorable and adverse PG conditions. Validation is carried out through both a priori and a posteriori tests. The a priori evaluation spans 140 diverse high-fidelity numerical datasets and experiments (67 training cases included), covering turbulent boundary layers, airfoils, Gaussian bumps, and full aircraft geometries, among others. We demonstrate that the baseline wall model outperforms the EQWM in 90% of test scenarios, while maintaining errors below 20% for 98% of the cases.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [37] [Approximation rates of quantum neural networks for periodic functions via Jackson's inequality](https://arxiv.org/abs/2511.16149)
*Ariel Neufeld,Philipp Schmocker,Viet Khoa Tran*

Main category: quant-ph

TL;DR: QNNs can approximate periodic functions with quadratic reduction in parameters using trigonometric polynomials, achieving better results than existing literature.


<details>
  <summary>Details</summary>
Motivation: To establish approximation capabilities of quantum neural networks for periodic functions using the universal approximation property analogy from classical neural networks.

Method: Use Jackson inequality to approximate periodic functions by implementing their approximating trigonometric polynomials via suitable QNN architectures.

Result: Achieved quadratic reduction in number of parameters for periodic function approximation, with smoother functions requiring fewer parameters.

Conclusion: Restricting to periodic functions enables more efficient QNN parameterization and better approximation performance than general function approximation approaches.

Abstract: Quantum neural networks (QNNs) are an analog of classical neural networks in the world of quantum computing, which are represented by a unitary matrix with trainable parameters. Inspired by the universal approximation property of classical neural networks, ensuring that every continuous function can be arbitrarily well approximated uniformly on a compact set of a Euclidean space, some recent works have established analogous results for QNNs, ranging from single-qubit to multi-qubit QNNs, and even hybrid classical-quantum models. In this paper, we study the approximation capabilities of QNNs for periodic functions with respect to the supremum norm. We use the Jackson inequality to approximate a given function by implementing its approximating trigonometric polynomial via a suitable QNN. In particular, we see that by restricting to the class of periodic functions, one can achieve a quadratic reduction of the number of parameters, producing better approximation results than in the literature. Moreover, the smoother the function, the fewer parameters are needed to construct a QNN to approximate the function.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [38] [Enhancing Forex Forecasting Accuracy: The Impact of Hybrid Variable Sets in Cognitive Algorithmic Trading Systems](https://arxiv.org/abs/2511.16657)
*Juan C. King,Jose M. Amigo*

Main category: cs.AI

TL;DR: Implementation of an AI-based algorithmic trading system for EUR-USD Forex pair using both fundamental and technical analysis features, with comparative analysis of which feature type provides better predictive capacity.


<details>
  <summary>Details</summary>
Motivation: To develop an advanced AI trading system for high-frequency Forex trading that integrates both fundamental macroeconomic variables and technical indicators to generate profitable trading signals.

Method: Integration of fundamental macroeconomic variables (GDP, Unemployment Rate from Euro Zone and US) with technical variables (indicators, oscillators, Fibonacci levels, price divergences) in an AI-based algorithmic trading system.

Result: Performance evaluated using machine learning metrics for predictive accuracy and backtesting simulations on historical data to assess trading profitability and risk.

Conclusion: Comparative analysis determines which class of input features (fundamental or technical) provides greater and more reliable predictive capacity for generating profitable trading signals.

Abstract: This paper presents the implementation of an advanced artificial intelligence-based algorithmic trading system specifically designed for the EUR-USD pair within the high-frequency environment of the Forex market. The methodological approach centers on integrating a holistic set of input features: key fundamental macroeconomic variables (for example, Gross Domestic Product and Unemployment Rate) collected from both the Euro Zone and the United States, alongside a comprehensive suite of technical variables (including indicators, oscillators, Fibonacci levels, and price divergences). The performance of the resulting algorithm is evaluated using standard machine learning metrics to quantify predictive accuracy and backtesting simulations across historical data to assess trading profitability and risk. The study concludes with a comparative analysis to determine which class of input features, fundamental or technical, provides greater and more reliable predictive capacity for generating profitable trading signals.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [39] [Geant4 based library SCoRe4 for Surface Contamination and Roughness Effects simulations in rare event search experiments](https://arxiv.org/abs/2511.15844)
*Christoph Grüner*

Main category: physics.ins-det

TL;DR: SCoRe4 is a Geant4-based library that simulates realistic surface roughness to improve accuracy in particle interaction modeling for rare event searches, addressing limitations of smooth surface assumptions.


<details>
  <summary>Details</summary>
Motivation: Standard Geant4 simulations assume perfectly smooth surfaces, neglecting real microscopic roughness, which leads to inaccurate energy deposition predictions in rare event searches like dark matter and neutrinoless double beta decay experiments.

Method: Developed SCoRe4, a Geant4-based library that generates simplified rough surface geometries using experimentally measurable parameters, scalable from square millimeters to square meters while maintaining computational efficiency.

Result: Created an open-source library that can be easily integrated into existing Geant4 setups, providing more realistic surface roughness simulation capabilities.

Conclusion: SCoRe4 improves background modeling accuracy in rare event physics by enabling more realistic surface roughness simulations based on measurable experimental parameters.

Abstract: Surface simulations are important for accurately modeling particle interactions in experiments where background contributions from surface contaminants can significantly affect detector performance. In rare event searches, such as dark matter or neutrinoless double beta decay experiments, standard Geant4 simulations typically assume perfectly smooth surfaces, neglecting the microscopic roughness that exists in real materials. This simplification can lead to inaccurate predictions of energy deposition. To address this limitation, I developed SCoRe4, a Geant4-based library designed to simulate more realistic surface roughness based on experimentally measurable parameters. The code allows users to generate patches of simplified rough surface geometries across a wide range of scales - from square millimeters to square meters - while maintaining computational efficiency. SCoRe4 is open source and can be easily integrated into existing Geant4 setups. This work presents the structure, implementation, and example application of SCoRe4,as well as its potential use in improving the accuracy of background modeling in rare event physics.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [40] [Eigenvalue-accelerated LDOS optimization of high-Q optical resonances](https://arxiv.org/abs/2511.16643)
*George Shaker,Beñat Martinez de Aguirre Jokisch,Pengning Chao,Steven G. Johnson*

Main category: physics.optics

TL;DR: A new method accelerates inverse design of high-Q resonant cavities by using shift-invert eigensolvers to maintain LDOS optimization at resonance peaks, eliminating ill-conditioning issues.


<details>
  <summary>Details</summary>
Motivation: Conventional LDOS optimization becomes dramatically slow for high-Q resonances (Q >> 100) due to ill-conditioning at sharp resonances, limiting practical design of high-performance resonant cavities.

Method: After initial LDOS optimization identifies a strong resonance, subsequent optimizations use a fast shift-invert eigensolver to ensure the LDOS remains centered at the resonance peak, preventing ill-conditioning.

Result: The method achieves orders-of-magnitude acceleration in inverse design and successfully designs Q > 10^6 resonant cavities in 1D and 2D dielectric systems.

Conclusion: The shift-invert eigensolver approach effectively eliminates ill-conditioning in high-Q LDOS optimization, enabling efficient design of extremely high-Q resonant cavities.

Abstract: We demonstrate a new method that yields orders-of-magnitude acceleration in inverse design (e.g. topology optimization) of high-$Q$ resonant cavities to maximize the local density of states (LDOS), and which is also applicable to other resonant-response metrics. The key idea is that, once conventional LDOS optimization has identified a strong resonance, subsequent optimizations can exploit a fast shift-invert eigensolver to ensure that the LDOS remains centered at the resonance peak. We show that this eliminates ill-conditioning at sharp resonances that otherwise dramatically slows LDOS (and similar) optimization for $Q \gg 100$. Our method is demonstrated by design of $Q > 10^6$ resonant cavities in 1d and 2d dielectric systems.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [41] [Gradient estimates for $(p,V)$-harmonic functions on Riemannian manifolds](https://arxiv.org/abs/2511.16058)
*Yuxin Dong,Hezi Lin,Weihao Zheng*

Main category: math.DG

TL;DR: Study of (p,V)-harmonic functions on Riemannian manifolds using Moser iteration, establishing volume comparison and Sobolev embedding under Bakry-Émery curvature, with explicit global gradient estimates for positive entire functions.


<details>
  <summary>Details</summary>
Motivation: To extend harmonic function theory to (p,V)-harmonic functions on complete Riemannian manifolds and establish fundamental analytical tools under curvature conditions.

Method: Moser iteration method applied to (p,V)-harmonic functions, combined with volume comparison theorem and Sobolev embedding theorem under Bakry-Émery curvature condition.

Result: Established volume comparison theorem, Sobolev embedding theorem, and obtained explicit global gradient estimate for positive entire (p,V)-harmonic functions.

Conclusion: The Moser iteration approach successfully extends harmonic analysis to (p,V)-harmonic functions, providing key estimates and embedding results under Bakry-Émery curvature conditions.

Abstract: In this paper, we study $(p,V)$-harmonic functions on complete Riemannian manifolds using the Moser iteration method. A volume comparison theorem and a Sobolev embedding theorem are established under the Bakry-$\acute{E}$mery curvature condition. Moreover, we obtain an explicit global gradient estimate for positive entire $(p,V)$-harmonic functions.

</details>


### [42] [Full flexibility of isometric immersions of metrics with low Hölder regularity in Poznyak theorem's dimension](https://arxiv.org/abs/2511.16305)
*Marta Lewicka*

Main category: math.DG

TL;DR: The paper proves that for 2D Riemannian metrics in C^{r,β}, isometric immersions into R^4 with regularity C^{1,α} can be found arbitrarily close to any short immersion, achieving full flexibility with C^{1,1-} regularity for C^2 metrics.


<details>
  <summary>Details</summary>
Motivation: To extend Poznyak's classical result on smooth isometric immersions into R^4 to lower regularity settings using convex integration techniques, and compare the achieved regularity with existing results in different codimensions.

Method: Using techniques of convex integration to construct isometric immersions with specified regularity properties, building on classical results and comparing with related work in different dimensions and codimensions.

Result: For any 2D metric g in C^{r,β}, isometric immersions of regularity C^{1,α} for any α < min{(r+β)/2,1} can be found arbitrarily close to any short immersion, achieving C^{1,1-} regularity for C^2 metrics.

Conclusion: The paper demonstrates full flexibility for isometric immersions into R^4 with C^{1,1-} regularity for C^2 metrics, contrasting with lower regularity results in R^3 and higher dimensional cases, highlighting the advantages of higher codimension.

Abstract: A classical result by Poznyak asserts that any smooth $2$-dimensional Riemannian metric $g$, posed on the closure of a simply connected domain $ω\subset\mathbb{R}^2$, has a smooth isometric immersion into $\mathbb{R}^4$. Using techniques of convex integration, we prove that for any $2$-dimensional $g\in\mathcal{C}^{r,β}$, an isometric immersion of regularity $\mathcal{C}^{1,α}(\barω,\mathbb{R}^4)$ for any $α<\min\{\frac{r+β}{2},1\}$, may be found arbitrarily close to any short immersion. The fact that this result's regularity reaches $\mathcal{C}^{1,1-}$ for $g\in \mathcal{C}^2$, which is referred to as "full flexibility", should be contrasted with: (i) the regularity $\mathcal{C}^{1,1/3-}$ achieved by Cao, Hirsch and Inauen for isometric immersions into $\mathbb{R}^{3}$ and the lack of flexibility (rigidity) of such isometric immersions with regularity $\mathcal{C}^{1, 2/3+}$ proved by Borisov and then by Conti, de Lellis and Szekelyhidi; (ii) the regularity $\mathcal{C}^{1,1-}$ obtained by Källen for isometric immersions into higher codimensional space; and (iii) the regularity $\mathcal{C}^{1,\frac{1}{d(d+1)/k}-}$ achieved by the author in the general case of $d$-dimensional metrics and $(d+k)$-dimensional immersions for the closely related Monge-Ampère system.

</details>


### [43] [Horizontal and Vertical Regularity of Elastic Wave Geometry](https://arxiv.org/abs/2511.16466)
*Joonas Ilmavirta,Pieti Kirkkopelto,Antti Kykkänen*

Main category: math.DG

TL;DR: Characterizes conditions for applying Finsler-geometric methods to inverse elastic wave imaging problems in anisotropic media.


<details>
  <summary>Details</summary>
Motivation: To understand when Finsler-geometric approaches can be used for solving inverse problems in elastic wave imaging, particularly for anisotropic materials.

Method: Analysis of analytic and algebraic properties that anisotropic stiffness tensor fields must satisfy to enable Finsler-geometric methods.

Result: Identified necessary conditions for stiffness tensor fields that allow application of Finsler-geometric techniques in elastic wave inverse problems.

Conclusion: Finsler-geometric methods can be applied to elastic wave imaging inverse problems when stiffness tensor fields meet specific analytic and algebraic criteria.

Abstract: The elastic properties of a material are encoded in a stiffness tensor field and the propagation of elastic waves is modeled by the elastic wave equation. We characterize analytic and algebraic properties a general anisotropic stiffness tensor field has to satisfy in order for Finsler-geometric methods to be applicable in studying inverse problems related to imaging with elastic waves.

</details>


### [44] [An Information-Theoretic Reconstruction of Curvature](https://arxiv.org/abs/2511.16601)
*Amandip Sangha*

Main category: math.DG

TL;DR: The paper develops an information-theoretic method to recover Riemannian curvature from heat diffusion behavior, showing that directional entropy distortion encodes curvature without traditional geometric tools.


<details>
  <summary>Details</summary>
Motivation: To establish a new intrinsic approach connecting geometric curvature with information theory through heat diffusion, avoiding traditional methods like Jacobi fields or variational formulas.

Method: Compare heat mass transported along planes using relative entropy of finite measures, analyzing small-time distortion of directional entropy to extract curvature information.

Result: The leading small-time distortion of directional entropy precisely encodes both scalar and sectional curvature, producing a bilinear tensor that matches the classical Riemannian curvature operator.

Conclusion: Curvature emerges as an infinitesimal information defect of diffusion, providing new connections between geometric analysis and information theory for curvature detection using only heat diffusion data.

Abstract: We develop an intrinsic information-theoretic approach for recovering Riemannian curvature from the small-time behaviour of heat diffusion. Given a point and a two-plane in the tangent space, we compare the heat mass transported along that plane with its Euclidean counterpart using the relative entropy of finite measures. We show that the leading small-time distortion of this directional entropy encodes precisely the local curvature of the manifold. In particular, the planar information imbalance determines both the scalar curvature and the sectional curvature at a point, and assembling these directional values produces a bilinear tensor that coincides exactly with the classical Riemannian curvature operator.
  The method is entirely analytic and avoids Jacobi fields, curvature identities, or variational formulas. Curvature appears solely through the behaviour of heat flow under the exponential map, providing a new viewpoint in which curvature is realized as an infinitesimal information defect of diffusion. This perspective suggests further connections between geometric analysis and information theory and offers a principled analytic mechanism for detecting and reconstructing curvature using only heat diffusion data.

</details>
