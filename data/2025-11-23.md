<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 5]
- [math.AP](#math.AP) [Total: 15]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math.DG](#math.DG) [Total: 4]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [physics.data-an](#physics.data-an) [Total: 1]
- [math.OC](#math.OC) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Data-driven Model Reduction for Parameter-Dependent Matrix Equations via Operator Inference](https://arxiv.org/abs/2511.16033)
*Xuelian Wen,Qiuqi Li,Juan Zhang*

Main category: math.NA

TL;DR: A non-intrusive, data-driven surrogate modeling framework using Operator Inference (OpInf) for efficiently solving parameter-dependent matrix equations in many-query scenarios.


<details>
  <summary>Details</summary>
Motivation: To overcome the computational bottlenecks of intrusive methods in high-dimensional contexts and enable rapid solution of parameter-dependent matrix equations.

Method: Reformulates matrix equations into structured polynomial parameter dependence, then constructs reduced-order models via regression on solution snapshots without requiring full-order operators.

Result: Numerical experiments confirm the accuracy and computational efficiency of the approach.

Conclusion: The framework provides a scalable and practical solution for parameter-dependent matrix equations, demonstrating effectiveness through experimental validation.

Abstract: This work develops a non-intrusive, data-driven surrogate modeling framework based on Operator Inference (OpInf) for rapidly solving parameter-dependent matrix equations in many-query settings. Motivated by the requirements of the OpInf methodology, we reformulate the matrix equations into a structured representation that explicitly shows the parameter dependence in polynomial form. This reformulation is crucial for efficient model reduction. This approach constructs reduced-order models via regression on solution snapshots, bypassing the need for expensive full-order operators and thus overcoming the primary bottlenecks of intrusive methods in high-dimensional contexts. Numerical experiments confirm their accuracy and computational efficiency, demonstrating that our work is a scalable and practical solution for parameter-dependent matrix equations.

</details>


### [2] [Optimal error analysis of an interior penalty virtual element method for fourth-order singular perturbation problems](https://arxiv.org/abs/2511.16070)
*Fang Feng,Yuanyi Sun,Yue Yu*

Main category: math.NA

TL;DR: The paper shows that the Interior Penalty Virtual Element Method (IPVEM) achieves optimal and uniform error estimates for fourth-order singular perturbation problems, correcting previous suboptimal half-order convergence rates.


<details>
  <summary>Details</summary>
Motivation: Previous studies showed IPVEM had only half-order uniform convergence for fourth-order singular perturbation problems, which was suboptimal and needed improvement.

Method: The authors use the Interior Penalty Virtual Element Method (IPVEM) and conduct theoretical analysis to establish error estimates, supported by extensive numerical experiments.

Result: The IPVEM achieves optimal and uniform error estimates for fourth-order singular perturbation problems, even with boundary layers, as confirmed by numerical experiments.

Conclusion: The proposed IPVEM method is effective for singularly perturbed problems, providing optimal convergence rates that outperform previous suboptimal results.

Abstract: In recent studies \cite{ZZ24, FY24}, the Interior Penalty Virtual Element Method (IPVEM) has been developed for solving a fourth-order singular perturbation problem, with uniform convergence established in the lowest-order case concerning the perturbation parameter. However, the resulting uniform convergence rate is only of half-order, which is suboptimal. In this work, we demonstrate that the proposed IPVEM in fact achieves optimal and uniform error estimates, even in the presence of boundary layers. The theoretical results are substantiated through extensive numerical experiments, which confirm the validity of the error estimates and highlight the method's effectiveness for singularly perturbed problems.

</details>


### [3] [Shallow neural network yields regularization for ill-posed inverse problems](https://arxiv.org/abs/2511.16171)
*Lan Wang,Qiao Zhu,Bangti Jin,Ye Zhang*

Main category: math.NA

TL;DR: Neural networks can universally approximate solutions to nonlinear ill-posed operator equations, with neuron count serving as regularization parameter to balance approximation and stability.


<details>
  <summary>Details</summary>
Motivation: To develop neural network methods that handle both approximation error and measurement error in solving nonlinear ill-posed operator equations, providing stable solutions even with noisy data.

Method: Proposed expanding neural network method as iterative regularization scheme, using number of neurons as regularization parameter and iteration number, with convergence analysis under different a priori assumptions.

Result: Small networks provide stable solutions for high noise levels, while larger networks risk overfitting; derived convergence rates for neural networks in variational regularization framework with numerical validation.

Conclusion: Neural networks offer effective regularization for ill-posed problems, with network size controlling the trade-off between approximation accuracy and solution stability, supported by theoretical guarantees and numerical evidence.

Abstract: In this paper, we establish universal approximation theorems for neural networks applied to general nonlinear ill-posed operator equations. In addition to the approximation error, the measurement error is also taken into account in our error estimation. We introduce the expanding neural network method as a novel iterative regularization scheme and prove its regularization properties under different a priori assumptions about the exact solutions. Within this framework, the number of neurons serves as both the regularization parameter and iteration number. We demonstrate that for data with high noise levels, a small network architecture is sufficient to obtain a stable solution, whereas a larger architecture may compromise stability due to overfitting. Furthermore, under standard assumptions in regularization theory, we derive convergence rate results for neural networks in the context of variational regularization. Several numerical examples are presented to illustrate the robustness of the proposed neural network-based algorithms.

</details>


### [4] [Robust PAMPA Scheme in the DG Formulation on Unstructured Triangular Meshes: bound preservation, oscillation elimination, and boundary conditions](https://arxiv.org/abs/2511.16180)
*Rémi Abgrall,Yongle Liu*

Main category: math.NA

TL;DR: Improved PAMPA algorithm with global continuity, local conservation, and no mass matrix inversion. Provides third-order accuracy, bound preservation, and non-oscillatory behavior.


<details>
  <summary>Details</summary>
Motivation: To develop a globally continuous version of PAMPA that maintains local conservation properties while avoiding mass matrix inversion, building on connections with discontinuous Galerkin methods.

Method: Leverages reinterpretation of PAMPA as discontinuous Galerkin method for linear hyperbolic problems, defines family of methods with rigorous boundary condition implementation, and complements bound preserving methods with non-oscillatory properties.

Result: Truncation error analysis shows third-order accuracy for smooth solutions, confirmed by numerical experiments. Scheme demonstrates bound preservation and non-oscillatory behavior across various benchmarks.

Conclusion: The improved PAMPA algorithm successfully achieves global continuity, local conservation, third-order accuracy, bound preservation, and non-oscillatory properties, validated through theoretical analysis and extensive numerical testing.

Abstract: We propose an improved version of the PAMPA algorithm where the solution is sought as globally continuous. The scheme is locally conservative, and there is no mass matrix to invert. This method had been developed in a series of papers, see e.g \cite{Abgrall2024a} and the references therein. In \cite{Abgrall2025d}, we had shown the connection between PAMPA and the discontinuous Galerkin method, for the linear hyperbolic problem. Taking advantage of this reinterpretation, we use it to define a family of methods, show how to implement the boundary conditions in a rigorous manner. In addition, we propose a method that complements the bound preserving method developed in \cite{Abgrall2025d} in the sense that it is non oscillatory. A truncation error analysis is provided, it shows that the scheme should be third order accurate for smooth solutions. This is confirmed by numerical experiments. Several numerical examples are presented to show that the scheme is indeed bound preserving and non oscillatory on a wide range on numerical benchmarks.

</details>


### [5] [Numerical identification of the time-dependent coefficient in the heat equation with fractional Laplacian](https://arxiv.org/abs/2511.16238)
*Arshyn Altybay,Niyaz Tokmagambetov,Gulzat Nalzhupbayeva*

Main category: math.NA

TL;DR: This paper develops methods for identifying time-dependent source coefficients in fractional heat equations using nonlocal data, with rigorous analysis of uniqueness, stability, and numerical implementation.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of identifying time-dependent source coefficients in fractional heat equations, which has applications in modeling anomalous diffusion processes where standard diffusion models are insufficient.

Method: Established a priori estimates for uniqueness and stability, proposed a fully implicit Crank-Nicolson finite-difference scheme, and developed an efficient noise-stable computation algorithm.

Result: The numerical experiments demonstrated accurate and robust performance of the proposed method even under noisy data conditions, validating the theoretical analysis.

Conclusion: The developed framework provides an effective and reliable approach for solving inverse source coefficient problems in fractional heat equations with practical applications in various scientific and engineering domains.

Abstract: We address the inverse problem of identifying a time-dependent source coefficient in a one-dimensional heat equation with a fractional Laplacian subject to Dirichlet boundary conditions and an integral nonlocal data. An a priori estimate is established to ensure the uniqueness and stability of the solution. A fully implicit Crank-Nicolson (CN) finite-difference scheme is proposed and rigorously analysed for stability and convergence. An efficient noise-stable computation algorithm is developed and verified through numerical experiments, demonstrating accuracy and robustness under noisy data.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [6] [Shallow-water convergence of the intermediate long wave equation in $L^2$](https://arxiv.org/abs/2511.15905)
*Andreia Chapouto,Guopeng Li,Tadahiro Oh,Tengfei Zhao*

Main category: math.AP

TL;DR: This paper establishes convergence of scaled Intermediate Long Wave (ILW) dynamics to Korteweg-de Vries (KdV) equation in the shallow-water limit at L²-level, completing the well-posedness and convergence study of ILW.


<details>
  <summary>Details</summary>
Motivation: To complete the convergence study of ILW equation on both real line and circle geometries within the L²-framework, building on previous work on deep-water convergence.

Method: Uses complete integrability of ILW and normal form method, leveraging Lax pair structure and perturbation determinant to establish weakly uniform equicontinuity in L², then treats low frequency part via infinite iteration of normal form reductions for KdV.

Result: Successfully proves convergence of scaled ILW dynamics to KdV in shallow-water limit at L²-level, completing the well-posedness and convergence study for both geometries.

Conclusion: The work completes the convergence analysis of ILW equation to KdV in both shallow-water and deep-water limits within the L²-framework, with proof applicable to both real line and circle geometries.

Abstract: We continue our study on the convergence issue of the intermediate long wave equation (ILW) on both the real line and the circle. In particular, we establish convergence of the scaled ILW dynamics to that of the Korteweg-de Vries equation (KdV) in the shallow-water limit at the $L^2$-level. Together with the recent work by the first three authors and D. Pilod (2024) on the deep-water convergence in $L^2$, this work completes the well-posedness and convergence study of ILW on both geometries within the $L^2$-framework. Our proof equally applies to both geometries and is based on the following two ingredients: the complete integrability of ILW and the normal form method. More precisely, by making use of the Lax pair structure and the perturbation determinant for ILW, recently introduced by Harrop-Griffths, Killip, and Vişan (2025), we first establish weakly uniform (in small depth parameters) equicontinuity in $L^2$ of solutions to the scaled ILW, providing a control on the high frequency part of solutions. Then, we treat the low frequency part by implementing a perturbative argument based on an infinite iteration of normal form reductions for KdV.

</details>


### [7] [Data-Driven Parameter Identification for Tumor Growth Models](https://arxiv.org/abs/2511.15940)
*Liu Liu,Yifei Wang,Qinyu Xu,Xiaoqian Xu*

Main category: math.AP

TL;DR: Using Physics-Informed Neural Networks (PINNs) to estimate parameters in tumor growth models from scarce and noisy observational data.


<details>
  <summary>Details</summary>
Motivation: Accurate tumor growth modeling is essential for understanding cancer progression and informing treatment strategies, especially when dealing with limited and noisy real-world data.

Method: Adopted Physics-Informed Neural Networks (PINNs) to estimate parameters in nonlinear PDE tumor growth models, leveraging the advantages of deep learning for handling scarce and noisy observation data.

Result: Demonstrated the potential of applying deep learning tools to address data-driven modeling for tumor growth in biology using real-life lab data.

Conclusion: PINNs show promise for tumor growth modeling, particularly in scenarios with limited and noisy observational data, offering a viable approach for biological data-driven modeling.

Abstract: Modeling tumor growth accurately is essential for understanding cancer progression and informing treatment strategies. To estimate the parameters in the tumor growth model described by a nonlinear PDE, we adopt Physics-Informed Neural Networks (PINNs), which show advantages especially when the observation data is scarce and contains noise. With the help of real-life lab data, we have demonstrated the potential of applying deep learning tools to address data-driven modeling for tumor growth in biology.

</details>


### [8] [An $L^2$-quantitative global approximation for the Stokes initial-boundary value problem](https://arxiv.org/abs/2511.16079)
*Mitsuo Higaki*

Main category: math.AP

TL;DR: First quantitative Runge approximation theorem with explicit L²-estimates for 3D nonstationary Stokes system on bounded domains, improving on previous qualitative results.


<details>
  <summary>Details</summary>
Motivation: Address limitations of previous qualitative results: bypass non-constructive Hahn-Banach theorem and extend from interior approximations to physically important initial-boundary value problems.

Method: Adapt modern quantitative framework from Rüland-Salo (2019) to Stokes system by combining semigroup theory with quantitative approximation for associated resolvent problem.

Result: Established quantitative Runge approximation theorem with explicit L²-estimates for 3D nonstationary Stokes system.

Conclusion: Successfully developed constructive quantitative approximation method for Stokes system that overcomes limitations of previous qualitative approaches.

Abstract: We establish the first quantitative Runge approximation theorem, with explicit $L^2$-estimates, for the 3d nonstationary Stokes system on a bounded spatial domain. This result addresses the two primary limitations of the qualitative result [H.-Sueur, 2025] obtained in collaboration with Franck Sueur: first, it bypasses the non-constructive Hahn-Banach theorem used in [H.-Sueur, 2025], precluding quantitative estimates; and second, it extends the scope of the theory from interior approximations to the physically important initial-boundary value problem. Our proof is founded on the modern quantitative framework of [Rüland-Salo, 2019], which we adapt to the Stokes system by combining semigroup theory with a quantitative approximation for the associated resolvent problem.

</details>


### [9] [Liouville--Type Results for Infinity Elliptic Equations Involving Gradient and Hardy--Hénon Nonlinearities](https://arxiv.org/abs/2511.16116)
*Tan-Dat Khuu,Trung-Hieu Huynh,Hoang-Hung Vo*

Main category: math.AP

TL;DR: Study of Liouville-type properties for degenerate elliptic equations with fractional infinity Laplacian and nonlinear lower-order terms, establishing new comparison principles and Lipschitz estimates.


<details>
  <summary>Details</summary>
Motivation: Extend Liouville theory for classical and normalized infinity Laplacian to include fractional cases with Hamiltonian and Hardy-Hénon type nonlinearities.

Method: Weighted comparison principle, sharp local Lipschitz estimates for viscosity solutions, radial reduction, barrier constructions, and refined comparison arguments.

Result: Liouville theorems derived from growth conditions for bounded nonnegative solutions with power-type and exponential nonlinearities; partial results for strongly supercritical exponential case.

Conclusion: Provides unified framework linking regularity, comparison principles, and Liouville-type phenomena for degenerate elliptic equations with fractional infinity Laplacians and nonlinear effects.

Abstract: In this paper we study Liouville-type properties for a class of degenerate elliptic equations driven by the fractional infinity Laplacian with nonlinear lower-order terms, \[ Δ_\infty^βu - c\,H(u,\nabla u) - λ\, f(|x|,u)=0 \qquad \text{in }\mathbb{R}^n, \] where $β\in[0,2]$, $Δ_\infty^β$ denotes the fractional infinity Laplace operator, and the nonlinearities $H$ and $f$ represent Hamiltonian and Hardy--Hénon type effects, respectively. We extend the Liouville theory for the classical and normalized infinity Laplacian by establishing a new weighted comparison principle together with sharp local Lipschitz estimates for viscosity solutions.
  Our Liouville theorems are derived from precise growth conditions for bounded nonnegative solutions when $f$ exhibits power-type behavior, i.e.\ $f\sim u^γ$. We also treat the exponential case $f\sim e^u$, for which the equation becomes strongly supercritical: under suitable assumptions on the growth of $u$ at spatial infinity, only partial Liouville-type conclusions can be obtained.
  The analysis relies on radial reduction, barrier constructions, and refined comparison arguments. Altogether, the results provide a unified framework linking regularity, comparison principles, and Liouville-type phenomena for degenerate elliptic equations involving fractional infinity Laplacians and nonlinear lower-order effects.

</details>


### [10] [On some uniqueness results](https://arxiv.org/abs/2511.16129)
*Patrizia Pucci,Jianjun Zhang,Xuexiu Zhong*

Main category: math.AP

TL;DR: This paper extends Serrin and Tang's results on overdetermined boundary value problems to the low-dimensional case (1≤N≤m), proving uniqueness of radial solutions under suitable assumptions on f.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by extending previous results that were limited to N>m cases, addressing the gap explicitly noted by Serrin and Tang who stated their proofs couldn't be extended to N≤m values. It also connects to the sharp Gagliardo-Nirenberg/Nash inequality.

Method: The paper studies radial solutions of an overdetermined boundary value problem involving the m-Laplacian operator in a ball domain. While following a standard framework similar to Serrin and Tang, the proofs differ significantly to handle the low-dimensional case.

Result: The authors prove uniqueness of radial solutions when 1≤N≤m and m>1 under certain suitable assumptions on the nonlinear function f.

Conclusion: This work successfully extends previous results to the challenging low-dimensional case that was explicitly excluded in prior work, providing new mathematical insights for overdetermined boundary value problems in the m-Laplacian context.

Abstract: This paper aims to extend the results of Serrin and Tang in [{\it Indiana Univ. Math. J., 49 (2000), 897--923}] to the low-dimensional case. Specifically, the paper deals with the radial solutions of the following overdetermined problem $$ \begin{cases} -Δ_m u=f(u),\quad u>0~\hbox{in}~B_R,\\ u=\partial_νu=0~\hbox{on}~\partial B_R, \end{cases} $$ where $B_R$ is the open ball of $\mathbb{R}^N$ centered at 0 and with radius $R>0$. We prove uniqueness when $1\leq N\leq m$ {and $m>1$} under certain suitable assumptions on~$f$. Additionally, this work is motivated by the sharp Gagliardo-Nirenberg/Nash inequality. While the framework presented in this article is standard and closely resembles that of Serrin and Tang, the detail of our proofs differ significantly. It is important to note that Serrin and Tang explicitly stated (see Subsection~6.2 of their work) that {\it``the proofs in the present paper rely extensively on the assumption $N>m$ and cannot be extended easily to values $N\leq m$."}

</details>


### [11] [Liouville theorems for fully nonlinear elliptic equations on half spaces](https://arxiv.org/abs/2511.16152)
*Yuanyuan Lian*

Main category: math.AP

TL;DR: Proves two Liouville theorems for fully nonlinear uniformly elliptic equations on half spaces using boundary regularity tools.


<details>
  <summary>Details</summary>
Motivation: To establish Liouville-type results for fully nonlinear elliptic equations on half spaces, which are important for understanding solution behavior in unbounded domains.

Method: Uses boundary pointwise regularity, Hopf type estimates, and Carleson type estimates to provide a short proof.

Result: Successfully proves two Liouville theorems for the specified class of equations on half spaces.

Conclusion: The paper presents a concise new proof method for Liouville theorems in half spaces using boundary regularity techniques.

Abstract: In this note, we prove two Liouville theorems for fully nonlinear uniformly elliptic equations on half spaces. The main tools are the boundary pointwise regularity, the Hopf type estimate and the Carleson type estimate. Our new proof is rather short.

</details>


### [12] [Spreading Properties of a City-Road Reaction-Diffusion Model on One-Dimensional Lattice](https://arxiv.org/abs/2511.16157)
*Grégory Faye,Jean-Michel Roquejoffre,Min Zhao*

Main category: math.AP

TL;DR: A new PDE-ODE model for biological invasions on infinite 1D graphs with logistic equations at vertices and diffusion on edges, showing asymptotic spreading speed and connection to discrete Fisher-KPP in fast diffusion regime.


<details>
  <summary>Details</summary>
Motivation: To model biological invasions constrained on infinite homogeneous one-dimensional metric graphs, extending classical reaction-diffusion models to network structures.

Method: Infinite PDE-ODE system with logistic equations at vertices of 1D lattice Z, diffusion equations on edges with Robin boundary conditions at vertices.

Result: Established system properties, characterized asymptotic spreading speed, derived novel asymptotic model in fast diffusion regime with similar propagation properties as discrete Fisher-KPP.

Conclusion: The model successfully describes biological invasions on metric graphs and connects to classical discrete Fisher-KPP in appropriate regimes.

Abstract: We propose and study a new model to describe biological invasions constrained on infinite homogeneous one dimensional metric graphs. Our model consists of an infinite PDE-ODE system where, at each vertex of the one-dimensional lattice $\mathbb{Z}$, we have a logistic equation, and connections between vertices are given by diffusion equations on the edges supplemented with Robin like boundary conditions at the vertices. We establish the main properties of the system and study the long time behavior of the solutions, especially by characterizing an asymptotic spreading speed for the system. In the fast diffusion regime, we derive a novel asymptotic model which exhibits similar propagation properties as the classical discrete Fisher-KPP on the one-dimensional lattice $\mathbb{Z}$.

</details>


### [13] [The Immersed Boundary Problem in 2-D: the Navier-Stokes Case](https://arxiv.org/abs/2511.16189)
*Jiajun Tong,Dongyi Wei*

Main category: math.AP

TL;DR: Existence, uniqueness, and regularity of mild solutions for 2D immersed boundary problem with elastic string in Navier-Stokes fluid, including convergence to Stokes case, energy law, and global existence near equilibrium.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for the 2D immersed boundary problem, which models elastic strings moving in viscous fluids, addressing existence, regularity, and long-time behavior.

Method: Introduce mild solution concept, prove existence/uniqueness for C^1 initial string configurations with well-stretched condition, analyze convergence to Stokes case as Reynolds number → 0, establish energy law and blow-up criterion.

Result: Proved existence and uniqueness of mild solutions with optimal regularity, derived convergence to Stokes case with error estimates, established energy law, and showed global existence near equilibrium states.

Conclusion: The paper provides comprehensive mathematical analysis of 2D immersed boundary problem, establishing well-posedness, regularity, convergence properties, and long-time behavior under appropriate conditions.

Abstract: We study the immersed boundary problem in 2-D. It models a 1-D elastic closed string immersed and moving in a fluid that fills the entire plane, where the fluid motion is governed by the 2-D incompressible Navier-Stokes equation with a positive Reynolds number subject to a singular forcing exerted by the string. We introduce the notion of mild solutions to this system, and prove its existence, uniqueness, and optimal regularity estimates when the initial string configuration is $C^1$ and satisfies the well-stretched condition and when the initial flow field $u_0$ lies in $L^p(\mathbb{R}^2)$ with $p\in (2,\infty)$. A blow-up criterion is also established. When the Reynolds number is sent to zero, we show convergence in short time of the solution to that of the Stokes case of 2-D immersed boundary problem, with the optimal error estimates derived. We prove the energy law of the system when $u_0$ additionally belongs to $L^2(\mathbb{R}^2)$. Lastly, we show that the solution is global when the initial data is sufficiently close to an equilibrium state.

</details>


### [14] [Dynamics of Ideal Fluid Flows](https://arxiv.org/abs/2511.16254)
*Tarek M. Elgindi*

Main category: math.AP

TL;DR: Overview of key problems and aspects related to the incompressible Euler equation including least action principle, special solutions, solvability, singularity formation, and asymptotic behavior.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive discussion of fundamental mathematical challenges and research directions in the study of incompressible Euler equations.

Method: Analytical discussion and review of various mathematical approaches to different aspects of the Euler equation.

Result: Systematic examination of major open problems and current understanding in Euler equation research.

Conclusion: The incompressible Euler equation presents numerous challenging mathematical problems that require further investigation across multiple domains including existence theory, singularity analysis, and asymptotic behavior.

Abstract: We will discuss various aspects of the incompressible Euler equation. We will discuss, in particular, problems related to the least action principle, the existence of special solutions, the problem of solvability, singularity formation, and asymptotic behavior.

</details>


### [15] [Asymptotic behavior and sharp estimates for spreading fronts in a cooperative system with free boundaries](https://arxiv.org/abs/2511.16300)
*Qian Qin,JinJing Jiao,Zhiguo Wang,Hua Nie*

Main category: math.AP

TL;DR: Study of a reaction-diffusion system with two free boundaries modeling cooperative species invasion, showing spreading-vanishing dichotomy and determining asymptotic spreading speeds using semi-wave analysis.


<details>
  <summary>Details</summary>
Motivation: To understand the long-term dynamics of cooperative species invasion in reaction-diffusion systems with free boundaries, particularly how two species interact and spread through expanding fronts.

Method: Analysis of reaction-diffusion system with two free boundaries using semi-wave systems to study asymptotic behavior, spreading speeds, and front movement estimates.

Result: The system exhibits spreading-vanishing dichotomy: species either spread completely or die out. In spreading cases, asymptotic spreading speeds are determined and solutions converge to semi-wave solutions as time increases.

Conclusion: The analysis provides deeper understanding of cooperative species dynamics in reaction-diffusion systems with free boundaries, particularly regarding spreading behavior and asymptotic convergence to semi-wave solutions.

Abstract: This paper investigates the dynamics of a reaction-diffusion system with two free boundaries, modeling the invasion of two cooperative species, where the free boundaries represent expanding fronts. We first analyze the long-term behavior of the system, showing that it follows a spreading-vanishing dichotomy: the two species either spread across the entire region or eventually die out. In the case of spreading, we determine the asymptotic spreading speed of the fronts by using a semi-wave system and provide sharp estimates for the moving fronts. Additionally, we show that the solution to the system converges to the corresponding semi-wave solution as time tends to infinity. These results contribute to a deeper understanding of the long-term dynamics of cooperative species in reaction-diffusion systems with free boundaries.

</details>


### [16] [The analysis of resonant frequencies and blow-up estimates of close-to-touching subwavelength resonators in the two-dimensional Helmholtz system](https://arxiv.org/abs/2511.16387)
*Hongjie Dong,Hongjie Li,Longjuan Xu*

Main category: math.AP

TL;DR: Analysis of wave scattering by two closely spaced high-contrast inclusions in 2D Helmholtz equation, revealing distinct sub-wavelength resonant modes with different asymptotic behaviors compared to 3D case.


<details>
  <summary>Details</summary>
Motivation: To understand wave scattering behavior in systems with closely spaced high-contrast inclusions, particularly focusing on resonant modes and their differences from 3D configurations.

Method: Modeling the system using the two-dimensional Helmholtz equation and performing asymptotic analysis of resonant frequencies and wave field behavior.

Result: Identified two sub-wavelength resonant modes with distinct leading-order asymptotic behaviors, different from 3D Helmholtz results, and quantified gradient blow-up rates in the inter-resonator region.

Conclusion: The 2D configuration exhibits unique resonant behavior that differs significantly from 3D systems, with important implications for wave field localization and gradient enhancement between closely spaced resonators.

Abstract: In this paper, we investigate wave scattering by a pair of closely spaced inclusions embedded in a homogeneous medium, characterized by a high contrast physical parameters. The system is modeled by the two-dimensional Helmholtz equation. We show that this configuration exhibits two sub-wavelength resonant modes, whose frequencies display distinct leading-order asymptotic behaviors. These findings differ significantly from those in the three-dimensional Helmholtz setting. Furthermore, we provide a quantitative analysis of the gradient blow-up rates for the wave field localized between the two resonators.

</details>


### [17] [Mosco convergence framework for singular limits of gradient flows on Hilbert spaces with applications](https://arxiv.org/abs/2511.16486)
*Yoshikazu Giga,Michał Łasica,Piotr Rybka*

Main category: math.AP

TL;DR: The paper introduces connecting operators to generalize Mosco convergence for gradient flows on different Hilbert spaces, proving convergence results with applications to thin domains, dynamic boundary conditions, and discrete-to-continuum limits.


<details>
  <summary>Details</summary>
Motivation: To establish a framework for analyzing convergence of gradient flows defined on different Hilbert spaces, which requires extending the concept of Mosco convergence to handle varying ambient spaces.

Method: Introduces connecting operators to link different Hilbert spaces, generalizes Mosco convergence of functionals to this setting, and proves convergence results for gradient flows.

Result: Develops a mathematical framework that successfully establishes convergence of gradient flows across different Hilbert spaces using the generalized Mosco convergence concept.

Conclusion: The connecting operator approach provides a robust method for analyzing gradient flow convergence in diverse settings including thin domains, dynamic boundary conditions, and discrete approximations.

Abstract: We consider the question of convergence of a sequence of gradient flows defined on different Hilbert spaces. In order to give meaning to this idea, we introduce a notion of connecting operators. This permits us to generalize the concept of Mosco convergence of functionals to our present setting, and state a desired convergence result for gradient flows, which we then prove. We present a variety of examples, including thin domains, dynamic boundary conditions, and discrete-to-continuum limits.

</details>


### [18] [Regularity for elliptic equations with monomial weights](https://arxiv.org/abs/2511.16516)
*Gabriele Cora,Gabriele Fioravanti,Francesco Pagliarin,Stefano Vita*

Main category: math.AP

TL;DR: The paper studies regularity properties for solutions to elliptic equations that are degenerate or singular along orthogonal hyperplanes, proving C⁰,α and C¹,α estimates up to corners using regularization, blow-up arguments, and Liouville theorems.


<details>
  <summary>Details</summary>
Motivation: To understand regularity properties of solutions to elliptic equations with degeneracies or singularities along orthogonal hyperplanes, which arise in various mathematical contexts including weighted Sobolev spaces and Caffarelli-Kohn-Nirenberg inequalities.

Method: Uses a regularization-approximation procedure, blow-up argument, and Liouville theorems to analyze solutions of the conormal problem with variable coefficients and monomial weights.

Result: Proves C⁰,α and C¹,α regularity estimates for solutions up to corners formed by intersections of hyperplanes, and provides smoothness results for isotropic homogeneous equations.

Conclusion: The developed techniques successfully establish regularity properties for degenerate elliptic equations with monomial weights, with applications to Caffarelli-Kohn-Nirenberg inequalities.

Abstract: We study regularity properties for solutions to elliptic equations that are degenerate or singular along orthogonal hyperplanes. The degenerate ellipticity is carried out by a weight term which is the monomial product of different powers of the distance functions to each hyperplane; that is, given the space dimension $d\geq2$, the number of orthogonally crossing hyperplanes $1\leq n\leq d$ and the generic variable point $z=(x,y)\in\mathbb R^{d-n}\times\mathbb R^n$, then the weight is given by $ω(y)=\prod_{i=1}^ny_i^{a_i}$ with $a_i>-1$, $y_i=\mathrm{dist}(z,Σ_i)$ and $Σ_i=\{y_i=0\}$. We prove $C^{0,α}$ and $C^{1,α}$ estimates up to the corners formed by the intersections of two or more hyperplanes, for solutions of the conormal problem with variable coefficients. This is done by a regularization-approximation procedure, a blow-up argument and Liouville theorems. Finally, we provide smoothness of solutions when the equation is isotropic and homogeneous, and we show an application to Caffarelli-Kohn-Nirenberg inequalities with monomial weights.

</details>


### [19] [Non-isoparametric Serrin domains of $\mathbb{S}^3$ with connected toric boundary](https://arxiv.org/abs/2511.16531)
*Andrea Bisterzo,Shigeru Sakaguchi*

Main category: math.AP

TL;DR: The paper proves existence of two types of non-spherical Serrin domains in S³ with connected boundaries that are neither geodesic spheres nor Clifford tori, using bifurcation theory.


<details>
  <summary>Details</summary>
Motivation: To show that Serrin's classical rigidity result (that Serrin domains in Rⁿ must be balls) fails in curved spaces like S³, by constructing counterexamples.

Method: Uses Crandall-Rabinowitz bifurcation theorem to construct branches of non-radial solutions bifurcating from radial ones, via implicit function methods.

Result: Found two distinct types of Serrin domains in S³ (small and large volume) with connected boundaries that are not isometric to standard symmetric shapes.

Conclusion: Demonstrates curvature can break rigidity in Serrin-type problems, providing new geometric configurations for the torsion problem in S³.

Abstract: We investigate the overdetermined torsion problem
  $\begin{cases} -Δu = 1 & \text{in}\ Ω\\ u=0 & \text{on}\ \partial Ω\\ \frac{\partial u}{\partial ν}=\text{const.} & \text{on}\ \partial Ω, \end{cases}$
  where $Ω$ is a smooth Riemannian domain. Domains admitting a solution to this problem are called \textit{Serrin domains}, after the celebrated work of Serrin \cite{Se71}, where is proved that in $\mathbb{R}^n$ such domains are geodesic balls. In the present paper we establish the existence of two distinct types of Serrin domains of $\mathbb{S}^3$, respectively of small and large volume, each of whose boundary is connected and is neither isometric to a geodesic sphere nor to a Clifford torus. These domains arise as nontrivial perturbations of some classical symmetric solutions to the same problem. Our approach relies on an implicit construction based on the Crandall-Rabinowitz bifurcation theorem, which allows us to detect branches of non-radial solutions bifurcating from a family of radial ones. The resulting examples highlight new geometric configurations of the torsion problem in the three-dimensional sphere, providing another proof of the fact that the rigidity of Serrin-type results can fail in the presence of curvature.

</details>


### [20] [A critical Hardy-Rellich inequality](https://arxiv.org/abs/2511.16537)
*Hernán Castro*

Main category: math.AP

TL;DR: Proves a critical Hardy-Rellich type inequality showing that the gradient of u/|x| in L^N norm is bounded by the Laplacian of u in L^N norm for functions vanishing at the origin.


<details>
  <summary>Details</summary>
Motivation: To establish a critical version of Hardy-Rellich inequalities, which are fundamental in analysis and PDE theory, particularly in the borderline case where standard Hardy inequalities fail.

Method: Mathematical proof using functional analysis techniques, working with smooth compactly supported functions vanishing at the origin in R^N.

Result: Proves existence of constant C_N>0 such that the inequality holds for all N≥1, providing a sharp bound in the critical case.

Conclusion: Successfully establishes a critical Hardy-Rellich inequality that extends classical results to the borderline case, with potential applications in PDE theory and functional analysis.

Abstract: In this work, we prove a critical version of a Hardy-Rellich type inequality. We show that for $N\geq 1$ there exists a constant $C_N>0$ such that \[ \int_{\mathbb R^N}\left|\nabla\left(\frac{u(x)}{|x|}\right)\right|^N\,\mathrm{d}x\leq C_N\int_{\mathbb R^N}\left|Δu(x)\right|^N\,\mathrm{d}x, \] for any $u\in C^\infty_c(\mathbb R^N\setminus\left\{0\right\})$.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [21] [Implicit and explicit treatments of model error in numerical simulation](https://arxiv.org/abs/2511.15934)
*Danny Smyl*

Main category: physics.comp-ph

TL;DR: Review of techniques for approximating and accounting for model errors in numerical simulations, covering both implicit and explicit error treatment methods developed over the past two decades.


<details>
  <summary>Details</summary>
Motivation: Numerical simulations of physical systems suffer from model errors due to unmodeled physics, idealizations, and discretization, which need to be properly addressed to improve predictive performance and uncertainty quantification.

Method: Surveys multiple approaches including Bayesian approximation error framework, embedded internal error models, probabilistic numerical methods, model discrepancy modeling, machine-learning-based correction, multi-fidelity strategies, and various error estimators (residual-based, variational, adjoint-driven).

Result: Provides comprehensive overview of key developments with extensive references, showing how these methods improve predictive performance and uncertainty quantification in practical applications from engineering to Earth-system science.

Conclusion: These error treatment methods can be effectively incorporated into PDE solvers, inverse problem workflows, and data assimilation systems to enhance computational physics and engineering simulations.

Abstract: Numerical simulations of physical systems invariably suffer from model errors stemming from unmodeled physics, idealizations, and discretization. This article provides a review of techniques developed in the past two decades to approximate and account for these model errors, both implicitly and explicitly. Beginning from fundamental definitions of model-form versus numerical error, we frame model error in inverse problems, data assimilation, and predictive modeling contexts. We then survey major approaches: the Bayesian approximation error framework for implicit error quantification, embedded internal error models for structural uncertainty, probabilistic numerical methods for discretization uncertainty, model discrepancy modeling in Bayesian calibration and its recent extensions, machine-learning-based discrepancy correction, multi-fidelity and hybrid modeling strategies, as well as residual-based, variational, and adjoint-driven error estimators. Throughout, we emphasize conceptual underpinnings of implicit versus explicit error treatment and highlight how these methods improve predictive performance and uncertainty quantification in practical applications ranging from engineering design to Earth-system science. Each section provides an overview of key developments with an extensive list of references to facilitate further reading. The review is written for practitioners of large-scale computational physics and engineering simulation, emphasizing how these methods can be incorporated into PDE solvers, inverse problem workflows, and data assimilation systems.

</details>


### [22] [A physics-inspired momentum-based gradient method](https://arxiv.org/abs/2511.16441)
*Jianing Zhang,Rumei Liu*

Main category: physics.comp-ph

TL;DR: A nonlinear momentum method based on non-Newtonian mechanical systems improves convergence in gradient optimization by using anharmonic kinetic energy and nonlinear damping.


<details>
  <summary>Details</summary>
Motivation: To enhance convergence performance of momentum-based gradient optimization algorithms by drawing inspiration from non-Newtonian mechanical systems dynamics.

Method: Generalized optimization dynamics with extended kinetic energy formulation and nonlinear damping term, using anharmonic kinetic energy for inertial effects and nonlinear damping for flexible momentum control.

Result: Numerical experiments show faster convergence and higher robustness compared to classical momentum algorithms, with strong performance on nonconvex objectives.

Conclusion: Dynamical systems from physics provide valuable insights for developing efficient optimization methods, particularly suitable for inverse photonic design problems.

Abstract: In this work, a nonlinear momentum method is introduced to improve the convergence performance of momentum-based gradient optimization algorithms. The method is motivated by the dynamics of non-Newtonian mechanical systems, where conventional momentum schemes can be interpreted as a dynamical model with quadratic kinetic energy and linear damping. Based on this analogy, a generalized optimization dynamics is constructed by extending the kinetic energy formulation and incorporating a nonlinear damping term. An anharmonic kinetic energy function can be employed to represent the inertial effect of accumulated gradient information during the iterations, while the nonlinear damping mechanism enables a more flexible control of the momentum contribution along the convergence trajectory. Numerical experiments indicate that the method exhibits faster convergence and higher robustness compared to classical momentum algorithms. Moreover, its strong performance on nonconvex objectives makes it particularly suitable for inverse photonic design problems. The results suggest that dynamical systems from physics can provide a view towards the development of efficient optimization methods.

</details>


### [23] [Deep Learning Framework for Enhanced Neutrino Reconstruction of Single-line Events in the ANTARES Telescope](https://arxiv.org/abs/2511.16614)
*A. Albert,S. Alves,M. André,M. Ardid,S. Ardid,J. -J. Aubert,J. Aublin,B. Baret,S. Basa,Y. Becherini,B. Belhorma,F. Benfenati,V. Bertin,S. Biagi,J. Boumaaza,M. Bouta,M. C. Bouwhuis,H. Brânzaş,R. Bruijn,J. Brunner,J. Busto,B. Caiffi,D. Calvo,S. Campion,A. Capone,F. Carenini,J. Carr,V. Carretero,T. Cartraud,S. Celli,L. Cerisy,M. Chabab,R. Cherkaoui El Moursli,T. Chiarusi,M. Circella,J. A. B. Coelho,A. Coleiro,R. Coniglione,P. Coyle,A. Creusot,A. F. Díaz,B. De Martino,C. Distefano,I. Di Palma,C. Donzaud,D. Dornic,D. Drouhin,T. Eberl,A. Eddymaoui,T. van Eeden,D. van Eijk,S. El Hedri,N. El Khayati,A. Enzenhöfer,P. Fermani,G. Ferrara,F. Filippini,L. Fusco,S. Gagliardini,J. García-Méndez,C. Gatius Oliver,P. Gay,N. Geißelbrecht,H. Glotin,R. Gozzini,R. Gracia Ruiz,K. Graf,C. Guidi,L. Haegel,H. van Haren,A. J. Heijboer,Y. Hello,L. Hennig,J. J. Hernández-Rey,J. Hößl,F. Huang,G. Illuminati,B. Jisse-Jung,M. de Jong,P. de Jong,M. Kadler,O. Kalekin,U. Katz,A. Kouchner,I. Kreykenbohm,V. Kulikovskiy,R. Lahmann,M. Lamoureux,A. Lazo,D. Lefèvre,E. Leonora,G. Levi,S. Le Stum,S. Loucatos,J. Manczak,M. Marcelin,A. Margiotta,A. Marinelli,J. A. Martínez-Mora,P. Migliozzi,A. Moussa,R. Muller,S. Navas,E. Nezri,B. Ó Fearraigh,E. Oukacha,A. M. Păun,G. E. Păvălaş,S. Peña-Martínez,M. Perrin-Terrin,P. Piattelli,C. Poiré,V. Popa,T. Pradier,N. Randazzo,D. Real,G. Riccobene,A. Romanov,A. Sánchez Losa,A. Saina,F. Salesa Greus,D. F. E. Samtleben,M. Sanguineti,P. Sapienza,F. Schüssler,J. Seneca,M. Spurio,Th. Stolarczyk,M. Taiuti,Y. Tayalati,B. Vallage,G. Vannoye,V. Van Elewyck,S. Viola,D. Vivolo,J. Wilms,S. Zavatarelli,A. Zegarelli,J. D. Zornoza,J. Zúñiga*

Main category: physics.comp-ph

TL;DR: N-Fit is a neural network algorithm that improves reconstruction of low-energy neutrino events in ANTARES telescope using deep learning, transfer learning, and dedicated branches for track/shower topologies.


<details>
  <summary>Details</summary>
Motivation: To improve reconstruction of single-line neutrino events (~100 GeV) in ANTARES telescope, which traditional χ²-fit methods struggle with, especially for azimuthal angle prediction and energy estimation.

Method: Uses deep convolutional layers, mixture density output layers, and transfer learning with two dedicated branches for track/shower topologies, including sub-models for direction, position, and energy inference.

Result: Significantly refines zenithal angle estimation, delivers reliable azimuthal angle predictions (previously unattainable), improves energy estimation using transfer learning, and reduces mean/median absolute errors across all parameters.

Conclusion: N-Fit demonstrates significant improvements over traditional methods, highlighting its potential for advancing multimessenger astrophysics and probing fundamental physics beyond Standard Model using ANTARES single-line events.

Abstract: We present the $N$-fit algorithm designed to improve the reconstruction of neutrino events detected by a single line of the ANTARES underwater telescope, usually associated with low energy neutrino events ($\sim$ 100 GeV). $N$-Fit is a neural network model that relies on deep learning and combines several advanced techniques in machine learning --deep convolutional layers, mixture density output layers, and transfer learning. This framework divides the reconstruction process into two dedicated branches for each neutrino event topology --tracks and showers-- composed of sub-models for spatial estimation --direction and position-- and energy inference, which later on are combined for event classification. Regarding the direction of single-line events, the $N$-Fit algorithm significantly refines the estimation of the zenithal angle, and delivers reliable azimuthal angle predictions that were previously unattainable with traditional $χ^2$-fit methods. Improving on energy estimation of single-line events is a tall order; $N$-Fit benefits from transfer learning to efficiently integrate key characteristics, such as the estimation of the closest distance from the event to the detector. $N$-Fit also takes advantage from transfer learning in event topology classification by freezing convolutional layers of the pretrained branches. Tests on Monte Carlo simulations and data demonstrate a significant reduction in mean and median absolute errors across all reconstructed parameters. The improvements achieved by $N$-Fit highlight its potential for advancing multimessenger astrophysics and enhancing our ability to probe fundamental physics beyond the Standard Model using single-line events from ANTARES data.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [24] [Asymptotic-preserving semi-implicit finite volume scheme for Extended Magnetohydrodynamics](https://arxiv.org/abs/2511.15937)
*Yi Han Toh,Joshua Dolence,Karthik Duraisamy*

Main category: physics.plasm-ph

TL;DR: A finite volume scheme for extended magnetohydrodynamics (XMHD) that works across ideal, resistive, and Hall MHD regimes using modified equations to maintain compatibility with ideal MHD solvers and constrained transport.


<details>
  <summary>Details</summary>
Motivation: To develop a unified numerical scheme that can handle extended MHD physics (electron inertia, displacement current) while maintaining numerical stability and naturally asymptoting to ideal MHD limits.

Method: Reformulated XMHD equations as relaxation system; semi-implicit FV scheme with explicit 2nd-order Runge-Kutta time stepping; operator splitting for implicit source terms; density-dependent slope limiter for stability; implemented on Artemis code with AMR support.

Result: The algorithm produces accurate results across ideal, resistive, and Hall MHD regimes, verified against published test problems, and maintains divergence-free magnetic fields.

Conclusion: The developed scheme successfully bridges multiple MHD regimes while preserving the benefits of ideal MHD formulations and demonstrating promising performance in non-ideal regimes.

Abstract: A Finite Volume (FV) scheme is developed for solving the extended magnetohydrodynamic (XMHD) equations, yielding accurate results in the ideal, resistive, and Hall MHD limits. This is accomplished by first re-writing the XMHD equations such that it allows the algorithm to retain the use of ideal MHD Riemann solvers and the constrained transport method to preserve divergence-free magnetic fields. Incorporation of electron inertia and displacement current introduces additional numerical stiffness which motivates a semi-implicit FV scheme that re-formulates the XMHD model as a relaxation system. The equations are then advanced in time using an explicit 2nd-order Runge-Kutta scheme with operator splitting applied to the implicit source term updates at each sub-stage. For additional numerical stability, density-dependent slope limiter is implemented to increase flux diffusivity at low density where non-ideal effects become significant. The algorithm is subsequently implemented on Artemis, a multifluid radiation hydrodynamics code built on Parthenon framework for high-performance computing (HPC) and adaptive mesh refinement (AMR) support. As the new algorithm retains many aspects of the ideal MHD formulations, it asymptotes naturally to the ideal MHD limit. Moreover, it shows promising results at the resistive and Hall MHD limits. This is verified against published test problems for ideal, resistive and Hall MHD.

</details>


### [25] [Effects of Multi-scale Coupling on Particle Acceleration and Energy Partition in Magnetic Reconnection](https://arxiv.org/abs/2511.15988)
*Alexander Velberg,Adam Stanier,Xiaocan Li,Fan Guo,William Daughton,Nuno F. Loureiro*

Main category: physics.plasm-ph

TL;DR: Particle-in-cell simulations show that in large-scale magnetic reconnection events, secondary current sheets and downstream turbulence dominate energy dissipation, creating additional particle acceleration channels that modify non-thermal particle spectra.


<details>
  <summary>Details</summary>
Motivation: To understand how kinetic and macroscopic scales interact during magnetic reconnection in strongly-magnetized relativistic pair plasmas, particularly focusing on energy dissipation mechanisms.

Method: Used particle-in-cell simulations of magnetic island coalescence in strongly-magnetized, relativistic pair plasma regime with large system sizes.

Result: Secondary current sheet formation and downstream turbulence driven by reconnection outflows dominate global energy dissipation, activating additional particle acceleration channels that significantly modify particle energy spectra compared to isolated current sheet simulations.

Conclusion: In large-scale magnetic reconnection, energy dissipation becomes causally connected but spatially and temporally decoupled from primary reconnecting current sheets, with secondary dynamics playing crucial roles in particle acceleration and spectral modification.

Abstract: The interplay between kinetic and macroscopic scales during magnetic reconnection is investigated using particle-in-cell simulations of magnetic island coalescence in the strongly-magnetized, relativistic pair plasma regime. For large system sizes, secondary current sheet formation and downstream turbulence driven by the reconnection outflows dominate the global energy dissipation so that it is causally connected, but spatially and temporally de-coupled from the primary reconnecting current sheet. When compared to simulations of an isolated, force-free current sheet, these dynamics activate additional particle acceleration channels which are responsible for a significant population of the non-thermal particles, modifying the particle energy spectra.

</details>


### [26] [Algorithms and optimizations for global non-linear hybrid fluid-kinetic finite element stellarator simulations](https://arxiv.org/abs/2511.16412)
*Luca Venerando Greco*

Main category: physics.plasm-ph

TL;DR: Novel globally coupled projection scheme in JOREK framework for stellarator plasma simulation, enabling accurate kinetic marker transfer to fluid grid using FFT-accelerated linear system and 3D R-Tree spatial indexing.


<details>
  <summary>Details</summary>
Motivation: Predictive modeling of stellarator plasmas faces computational challenges due to non-axisymmetric geometry that couples toroidal Fourier modes, requiring hybrid fluid-kinetic models for accurate particle species dynamics.

Method: Globally coupled projection scheme using unified linear system for all toroidal harmonics simultaneously, accelerated by Fast Fourier Transform and supported by 3D R-Tree spatial indexing for efficient particle localization.

Result: Framework achieves theoretically anticipated spectral convergence on Wendelstein 7-X geometries, significantly outperforming uncoupled approaches which show poor performance.

Conclusion: Provides validated high-fidelity computational tool crucial for predictive analysis and optimization of next-generation stellarator designs.

Abstract: Predictive modeling of stellarator plasmas is crucial for advancing nuclear fusion energy, yet it faces unique computational difficulties. One of the main challenges is accurately simulating the dynamics of specific particle species that are not well captured by fluid models, which necessitates the use of hybrid fluid-kinetic models. The non-axisymmetric geometry of stellarators fundamentally couples the toroidal Fourier modes, in contrast to what happens in tokamaks, requiring different numerical and computational treatment.
  This work presents a novel, globally coupled projection scheme inside the JOREK finite element framework. The approach ensures a self-consistent and physically accurate transfer of kinetic markers to the fluid grid, effectively handling the complex 3D mesh by constructing and solving a unified linear system that encompasses all toroidal harmonics simultaneously. To manage the computational complexity of this coupling, the construction of the system's matrix is significantly accelerated using the Fast Fourier Transform (FFT). The efficient localization of millions of particles is made possible by implementing a 3D R-Tree spatial index, which supports this projection and ensures computational tractability at scale.
  On realistic Wendelstein 7-X stellarator geometries, the fidelity of the framework is rigorously shown. In sharp contrast to the uncoupled approaches' poor performance, quantitative convergence tests verify that the coupled scheme attains the theoretically anticipated spectral convergence.
  This study offers a crucial capability for the predictive analysis and optimization of next-generation stellarator designs by developing a validated, high-fidelity computational tool.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [27] [Approximation rates of quantum neural networks for periodic functions via Jackson's inequality](https://arxiv.org/abs/2511.16149)
*Ariel Neufeld,Philipp Schmocker,Viet Khoa Tran*

Main category: quant-ph

TL;DR: QNNs can approximate periodic functions with quadratic reduction in parameters using trigonometric polynomials via Jackson inequality, achieving better results than existing literature.


<details>
  <summary>Details</summary>
Motivation: To establish approximation capabilities of quantum neural networks for periodic functions using the universal approximation property analogy from classical neural networks.

Method: Use Jackson inequality to approximate periodic functions by implementing approximating trigonometric polynomials via suitable QNN architectures.

Result: Achieved quadratic reduction in number of parameters for periodic function approximation, with smoother functions requiring fewer parameters.

Conclusion: Restricting to periodic functions enables more efficient QNN parameterization and better approximation performance than general function approximation approaches.

Abstract: Quantum neural networks (QNNs) are an analog of classical neural networks in the world of quantum computing, which are represented by a unitary matrix with trainable parameters. Inspired by the universal approximation property of classical neural networks, ensuring that every continuous function can be arbitrarily well approximated uniformly on a compact set of a Euclidean space, some recent works have established analogous results for QNNs, ranging from single-qubit to multi-qubit QNNs, and even hybrid classical-quantum models. In this paper, we study the approximation capabilities of QNNs for periodic functions with respect to the supremum norm. We use the Jackson inequality to approximate a given function by implementing its approximating trigonometric polynomial via a suitable QNN. In particular, we see that by restricting to the class of periodic functions, one can achieve a quadratic reduction of the number of parameters, producing better approximation results than in the literature. Moreover, the smoother the function, the fewer parameters are needed to construct a QNN to approximate the function.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [28] [Li-P-S Electrolyte Materials as a Benchmark for Machine-Learned Interatomic Potentials](https://arxiv.org/abs/2511.16569)
*Natascia L. Fragapane,Volker L. Deringer*

Main category: cond-mat.mtrl-sci

TL;DR: LiPS-25 is a curated benchmark dataset for solid-state electrolyte materials from the Li2S-P2S5 system, with performance tests ranging from numerical error metrics to physically motivated evaluation tasks.


<details>
  <summary>Details</summary>
Motivation: There is growing demand for robust, automated, and chemically insightful benchmarking methodologies for machine-learned interatomic potential (MLIP) models used in materials simulations.

Method: Created LiPS-25 dataset with crystalline and amorphous configurations from Li2S-P2S5 system, developed performance tests including numerical error metrics and physical evaluation tasks, and conducted experiments on graph-based MLIP architectures.

Result: The benchmark assesses hyperparameter effects and fine-tuning behavior of pre-trained MLIP models, providing insights into model performance for solid-state electrolyte materials.

Conclusion: The LiPS-25 benchmark and its code implementations can be readily adapted to other material systems beyond Li-P-S solid-state electrolytes.

Abstract: With the growing availability of machine-learned interatomic potential (MLIP) models for materials simulations, there is an increasing demand for robust, automated, and chemically insightful benchmarking methodologies. In response, we here introduce LiPS-25, a curated benchmark dataset for a canonical series of solid-state electrolyte materials from the Li2S-P2S5 pseudo-binary compositional line, including crystalline and amorphous configurations. Together with the dataset, we present a suite of performance tests that range from conventional numerical error metrics to physically motivated evaluation tasks. With a focus on graph-based MLIP architectures, we run numerical experiments that assess (i) the effect of hyperparameters and (ii) the fine-tuning behavior of selected pre-trained ("foundational") MLIP models. Beyond the Li-P-S solid-state electrolytes, we expect that such benchmarks and their code implementations can be readily adapted to other material systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [29] [Enhancing Forex Forecasting Accuracy: The Impact of Hybrid Variable Sets in Cognitive Algorithmic Trading Systems](https://arxiv.org/abs/2511.16657)
*Juan C. King,Jose M. Amigo*

Main category: cs.AI

TL;DR: Implementation of an AI-based algorithmic trading system for EUR-USD Forex pair using both fundamental and technical analysis features, with comparative evaluation of which feature type provides better predictive capacity.


<details>
  <summary>Details</summary>
Motivation: To develop an advanced AI trading system for high-frequency Forex trading and determine whether fundamental macroeconomic variables or technical indicators provide more reliable predictive power for profitable trading signals.

Method: Integrated holistic input features including fundamental macroeconomic variables (GDP, Unemployment Rate from Euro Zone and US) and technical variables (indicators, oscillators, Fibonacci levels, price divergences), evaluated using machine learning metrics and backtesting simulations.

Result: Performance evaluated through predictive accuracy metrics and backtesting on historical data to assess trading profitability and risk.

Conclusion: Comparative analysis determines which class of input features (fundamental or technical) provides greater and more reliable predictive capacity for generating profitable trading signals.

Abstract: This paper presents the implementation of an advanced artificial intelligence-based algorithmic trading system specifically designed for the EUR-USD pair within the high-frequency environment of the Forex market. The methodological approach centers on integrating a holistic set of input features: key fundamental macroeconomic variables (for example, Gross Domestic Product and Unemployment Rate) collected from both the Euro Zone and the United States, alongside a comprehensive suite of technical variables (including indicators, oscillators, Fibonacci levels, and price divergences). The performance of the resulting algorithm is evaluated using standard machine learning metrics to quantify predictive accuracy and backtesting simulations across historical data to assess trading profitability and risk. The study concludes with a comparative analysis to determine which class of input features, fundamental or technical, provides greater and more reliable predictive capacity for generating profitable trading signals.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [30] [Charge-Ordered States and the Phase Diagram of the Extended Hubbard Model on the Bethe lattice](https://arxiv.org/abs/2511.16603)
*Aleksey Alekseev,Konrad Jerzy Kapcia*

Main category: cond-mat.str-el

TL;DR: Study of extended Hubbard model on Bethe lattice using mean-field approximation reveals charge-ordered insulating/metallic and non-charge-ordered states, with analytical derivations showing how onsite repulsion suppresses charge order.


<details>
  <summary>Details</summary>
Motivation: To understand charge ordering phenomena in the extended Hubbard model using a simplified analytical approach that avoids numerical complexities while capturing essential physics.

Method: Standard Hartree mean-field approximation applied to the extended Hubbard model on Bethe lattice, enabling analytical derivations and geometry-independent analysis.

Result: Identified three ground states: charge-ordered insulating, charge-ordered metallic, and non-charge-ordered states. Onsite repulsion suppresses charge order and drives insulator-to-metal transitions. Finite-temperature phase diagrams and various observables analyzed.

Conclusion: Mean-field approximation provides simplified yet insightful analysis of charge ordering phenomena in extended Hubbard model, with analytical approach offering advantages over purely numerical methods.

Abstract: We study the extended Hubbard model (EHM) with both onsite Hubbard interaction and the intersite density-density interaction between nearest-neighbors using the standard Hartree mean-field approximation (MFA) on the Bethe lattice. We found that, at the ground state, the system can be in a charge-ordered insulating (COI), a charge-order metallic (COM) or a non-charge-ordered (NO) state. Moreover, the finite-temperature phase diagrams are presented. Several observables like a charge-order parameter, a spectral function, and particularly at finite temperatures, a charge carrier concentration (to visualize the degree of metallicity) are analyzed. The results show that increasing onsite repulsion suppresses charge order and change the properties of the system from insulating to metallic. Worth noting, that a number of phenomena can be found within the MFA, where their analysis is much simpler than in more advanced approaches. The method used for the EHM on the Bethe lattice also allows for a series of analytical derivations and simplification to see general geometry-independent features and analytical results, avoiding the numerical inaccuracies and other issues that appear with a purely numerical solution.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [31] [Geant4 based library SCoRe4 for Surface Contamination and Roughness Effects simulations in rare event search experiments](https://arxiv.org/abs/2511.15844)
*Christoph Grüner*

Main category: physics.ins-det

TL;DR: SCoRe4 is a Geant4-based library that simulates realistic surface roughness to improve accuracy in particle interaction modeling for rare event searches.


<details>
  <summary>Details</summary>
Motivation: Standard Geant4 simulations assume perfectly smooth surfaces, neglecting real microscopic roughness, which leads to inaccurate energy deposition predictions in rare event searches like dark matter experiments.

Method: Developed SCoRe4 library that generates simplified rough surface geometries across various scales (mm² to m²) using experimentally measurable parameters while maintaining computational efficiency.

Result: Created an open-source tool that can be easily integrated into existing Geant4 setups to simulate more realistic surface roughness.

Conclusion: SCoRe4 improves background modeling accuracy in rare event physics by addressing the limitation of smooth surface assumptions in standard simulations.

Abstract: Surface simulations are important for accurately modeling particle interactions in experiments where background contributions from surface contaminants can significantly affect detector performance. In rare event searches, such as dark matter or neutrinoless double beta decay experiments, standard Geant4 simulations typically assume perfectly smooth surfaces, neglecting the microscopic roughness that exists in real materials. This simplification can lead to inaccurate predictions of energy deposition. To address this limitation, I developed SCoRe4, a Geant4-based library designed to simulate more realistic surface roughness based on experimentally measurable parameters. The code allows users to generate patches of simplified rough surface geometries across a wide range of scales - from square millimeters to square meters - while maintaining computational efficiency. SCoRe4 is open source and can be easily integrated into existing Geant4 setups. This work presents the structure, implementation, and example application of SCoRe4,as well as its potential use in improving the accuracy of background modeling in rare event physics.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [32] [Entrywise Approximate Solutions for SDDM Systems in Almost-Linear Time](https://arxiv.org/abs/2511.16570)
*Angelo Farfan,Mehrdad Ghadiri,Junzhao Yang*

Main category: cs.DS

TL;DR: Fast algorithm for solving SDDM linear systems with near-linear time complexity


<details>
  <summary>Details</summary>
Motivation: Need efficient solvers for symmetric diagonally dominant M-matrices (SDDM), which are principal submatrices of graph Laplacians, to handle large-scale linear systems

Method: Developed an algorithm that computes entrywise approximations to solutions of Lx = b for SDDM matrices L and nonnegative vectors b

Result: Achieves Õ(m n^o(1)) time complexity with high probability, where m is number of nonzero entries and n is system dimension

Conclusion: Provides near-linear time solver for SDDM systems, significantly improving computational efficiency for this important class of linear systems

Abstract: We present an algorithm that given any invertible symmetric diagonally dominant M-matrix (SDDM), i.e., a principal submatrix of a graph Laplacian, $\boldsymbol{\mathit{L}}$ and a nonnegative vector $\boldsymbol{\mathit{b}}$, computes an entrywise approximation to the solution of $\boldsymbol{\mathit{L}} \boldsymbol{\mathit{x}} = \boldsymbol{\mathit{b}}$ in $\tilde{O}(m n^{o(1)})$ time with high probability, where $m$ is the number of nonzero entries and $n$ is the dimension of the system.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [33] [Age-structured model of dengue transmission dynamics with time-varying parameters, and its application to Brazil](https://arxiv.org/abs/2511.16179)
*Ihtisham Ul Haq,Serge Richard*

Main category: q-bio.PE

TL;DR: Developed an age-structured mathematical model for dengue transmission dynamics, analyzed its theoretical properties, and applied it to Brazil using 2021-2024 weekly data to estimate transmission rates and reproduction numbers.


<details>
  <summary>Details</summary>
Motivation: To better understand dengue transmission dynamics by incorporating age structure, time-dependent parameters, and environmental factors like temperature and humidity, specifically focusing on the Brazilian context.

Method: Created an age-structured mathematical model with time-dependent parameters, analyzed disease-free steady state and reproduction numbers, then applied to Brazil using weekly time series data from 2021-2024 to estimate transmission rates from epidemiological and environmental data.

Result: Estimated time-varying effective reproduction numbers, identified sensitive parameters affecting model dynamics, and provided predictions for future transmission under different scenarios, showing the importance of age distribution, vector dynamics, and climate.

Conclusion: The study demonstrates that population age distribution, vector population dynamics, and climate are crucial factors in dengue transmission dynamics in Brazil, providing valuable insights for understanding and predicting disease spread.

Abstract: An age structured mathematical model with time dependent parameters is developed to investigate the dynamics of dengue transmission. Its properties are thoroughly analyzed in the first part of this work, as for example its disease free steady state, the corresponding effective reproduction numbers, its basic reproduction number (obtained via the Euler and Lotka equation and the next generation matrix approach). We also provide formulas for the time-varying effective reproduction number, and draw relations with the instantaneous growth rate. In the second part, we apply this model to Brazil and use weekly time series data from this country. Various medical parameters are firstly evaluated from these data, and an extensive numerical simulations for the period 2021 to 2024 is then carried out. Estimation of the transmission rates are derived both from epidemiological data and from environmental data such as temperature and humidity. The time-varying effective reproduction numbers are then estimated on these data, following the theoretical investigations performed in the first part. The sensitive parameters that significantly affect the model dynamics are presented graphically. Model predictions for following year by using different transmission rates are finally presented. Our findings show the importance of population age distribution, vector population dynamics, and climate, contributing to a deeper understanding of dengue transmission dynamics in Brazil.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [34] [Eigenvalue-accelerated LDOS optimization of high-Q optical resonances](https://arxiv.org/abs/2511.16643)
*George Shaker,Beñat Martinez de Aguirre Jokisch,Pengning Chao,Steven G. Johnson*

Main category: physics.optics

TL;DR: A new method for accelerating inverse design of high-Q resonant cavities by using shift-invert eigensolvers to maintain LDOS optimization at resonance peaks, eliminating ill-conditioning issues.


<details>
  <summary>Details</summary>
Motivation: Conventional LDOS optimization becomes dramatically slow for high-Q resonances (Q >> 100) due to ill-conditioning at sharp resonances, limiting practical design of high-performance resonant cavities.

Method: Uses shift-invert eigensolvers to ensure LDOS remains centered at resonance peaks after initial conventional optimization identifies strong resonances, enabling efficient optimization for high-Q cavities.

Result: Achieved orders-of-magnitude acceleration in inverse design, demonstrated by designing Q > 10^6 resonant cavities in 1D and 2D dielectric systems.

Conclusion: The shift-invert eigensolver approach effectively eliminates ill-conditioning issues and enables efficient optimization of high-Q resonant cavities for LDOS and similar resonant-response metrics.

Abstract: We demonstrate a new method that yields orders-of-magnitude acceleration in inverse design (e.g. topology optimization) of high-$Q$ resonant cavities to maximize the local density of states (LDOS), and which is also applicable to other resonant-response metrics. The key idea is that, once conventional LDOS optimization has identified a strong resonance, subsequent optimizations can exploit a fast shift-invert eigensolver to ensure that the LDOS remains centered at the resonance peak. We show that this eliminates ill-conditioning at sharp resonances that otherwise dramatically slows LDOS (and similar) optimization for $Q \gg 100$. Our method is demonstrated by design of $Q > 10^6$ resonant cavities in 1d and 2d dielectric systems.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [35] [Gradient estimates for $(p,V)$-harmonic functions on Riemannian manifolds](https://arxiv.org/abs/2511.16058)
*Yuxin Dong,Hezi Lin,Weihao Zheng*

Main category: math.DG

TL;DR: Study of (p,V)-harmonic functions on complete Riemannian manifolds using Moser iteration, establishing volume comparison and Sobolev embedding under Bakry-Émery curvature, with explicit global gradient estimates for positive entire solutions.


<details>
  <summary>Details</summary>
Motivation: To extend the analysis of harmonic functions to the more general (p,V)-harmonic case on Riemannian manifolds, particularly under curvature conditions.

Method: Moser iteration method combined with volume comparison theorem and Sobolev embedding theorem under Bakry-Émery curvature condition.

Result: Established volume comparison and Sobolev embedding theorems, and obtained explicit global gradient estimates for positive entire (p,V)-harmonic functions.

Conclusion: The Moser iteration approach successfully provides analytical tools and estimates for (p,V)-harmonic functions under Bakry-Émery curvature conditions on complete Riemannian manifolds.

Abstract: In this paper, we study $(p,V)$-harmonic functions on complete Riemannian manifolds using the Moser iteration method. A volume comparison theorem and a Sobolev embedding theorem are established under the Bakry-$\acute{E}$mery curvature condition. Moreover, we obtain an explicit global gradient estimate for positive entire $(p,V)$-harmonic functions.

</details>


### [36] [Full flexibility of isometric immersions of metrics with low Hölder regularity in Poznyak theorem's dimension](https://arxiv.org/abs/2511.16305)
*Marta Lewicka*

Main category: math.DG

TL;DR: The paper proves that any 2D Riemannian metric g in C^r,β has C^1,α isometric immersions into R^4 for α < min{(r+β)/2,1}, reaching C^1,1- regularity for C^2 metrics (full flexibility).


<details>
  <summary>Details</summary>
Motivation: To extend Poznyak's classical result on smooth isometric immersions into R^4 to lower regularity settings and establish optimal regularity bounds, contrasting with known rigidity results in R^3 and other dimensions.

Method: Using convex integration techniques to construct isometric immersions that are arbitrarily close to any short immersion, achieving higher regularity than previous results.

Result: Achieved C^1,α regularity for α < min{(r+β)/2,1}, with full flexibility (C^1,1-) for C^2 metrics, which exceeds the C^1,1/3- regularity known for R^3 immersions.

Conclusion: The paper demonstrates that isometric immersions into R^4 enjoy significantly better regularity properties than those into R^3, reaching near-optimal C^1,1- regularity and full flexibility for C^2 metrics.

Abstract: A classical result by Poznyak asserts that any smooth $2$-dimensional Riemannian metric $g$, posed on the closure of a simply connected domain $ω\subset\mathbb{R}^2$, has a smooth isometric immersion into $\mathbb{R}^4$. Using techniques of convex integration, we prove that for any $2$-dimensional $g\in\mathcal{C}^{r,β}$, an isometric immersion of regularity $\mathcal{C}^{1,α}(\barω,\mathbb{R}^4)$ for any $α<\min\{\frac{r+β}{2},1\}$, may be found arbitrarily close to any short immersion. The fact that this result's regularity reaches $\mathcal{C}^{1,1-}$ for $g\in \mathcal{C}^2$, which is referred to as "full flexibility", should be contrasted with: (i) the regularity $\mathcal{C}^{1,1/3-}$ achieved by Cao, Hirsch and Inauen for isometric immersions into $\mathbb{R}^{3}$ and the lack of flexibility (rigidity) of such isometric immersions with regularity $\mathcal{C}^{1, 2/3+}$ proved by Borisov and then by Conti, de Lellis and Szekelyhidi; (ii) the regularity $\mathcal{C}^{1,1-}$ obtained by Källen for isometric immersions into higher codimensional space; and (iii) the regularity $\mathcal{C}^{1,\frac{1}{d(d+1)/k}-}$ achieved by the author in the general case of $d$-dimensional metrics and $(d+k)$-dimensional immersions for the closely related Monge-Ampère system.

</details>


### [37] [Horizontal and Vertical Regularity of Elastic Wave Geometry](https://arxiv.org/abs/2511.16466)
*Joonas Ilmavirta,Pieti Kirkkopelto,Antti Kykkänen*

Main category: math.DG

TL;DR: Characterizes conditions for applying Finsler geometry to elastic wave inverse problems in anisotropic materials


<details>
  <summary>Details</summary>
Motivation: To understand when Finsler-geometric methods can be used for studying inverse problems in elastic wave imaging of anisotropic materials

Method: Analysis of analytic and algebraic properties that anisotropic stiffness tensor fields must satisfy for Finsler-geometric applicability

Result: Identified necessary conditions for stiffness tensor fields to enable Finsler-geometric approaches in elastic wave inverse problems

Conclusion: Established mathematical framework for determining when Finsler geometry can be applied to elastic wave imaging inverse problems in anisotropic media

Abstract: The elastic properties of a material are encoded in a stiffness tensor field and the propagation of elastic waves is modeled by the elastic wave equation. We characterize analytic and algebraic properties a general anisotropic stiffness tensor field has to satisfy in order for Finsler-geometric methods to be applicable in studying inverse problems related to imaging with elastic waves.

</details>


### [38] [An Information-Theoretic Reconstruction of Curvature](https://arxiv.org/abs/2511.16601)
*Amandip Sangha*

Main category: math.DG

TL;DR: This paper develops an information-theoretic approach to recover Riemannian curvature from heat diffusion behavior, showing that directional entropy distortion encodes local curvature.


<details>
  <summary>Details</summary>
Motivation: To establish a new intrinsic connection between information theory and geometric analysis, providing an analytic method to detect curvature without traditional geometric tools like Jacobi fields or variational formulas.

Method: Compare heat mass transported along planes with Euclidean counterparts using relative entropy of finite measures, analyzing the leading small-time distortion of directional entropy.

Result: The planar information imbalance determines both scalar and sectional curvature, and assembling directional values produces a bilinear tensor identical to the classical Riemannian curvature operator.

Conclusion: Curvature emerges as an infinitesimal information defect of diffusion, offering new connections between geometric analysis and information theory, and providing an analytic mechanism for curvature detection using only heat diffusion data.

Abstract: We develop an intrinsic information-theoretic approach for recovering Riemannian curvature from the small-time behaviour of heat diffusion. Given a point and a two-plane in the tangent space, we compare the heat mass transported along that plane with its Euclidean counterpart using the relative entropy of finite measures. We show that the leading small-time distortion of this directional entropy encodes precisely the local curvature of the manifold. In particular, the planar information imbalance determines both the scalar curvature and the sectional curvature at a point, and assembling these directional values produces a bilinear tensor that coincides exactly with the classical Riemannian curvature operator.
  The method is entirely analytic and avoids Jacobi fields, curvature identities, or variational formulas. Curvature appears solely through the behaviour of heat flow under the exponential map, providing a new viewpoint in which curvature is realized as an infinitesimal information defect of diffusion. This perspective suggests further connections between geometric analysis and information theory and offers a principled analytic mechanism for detecting and reconstructing curvature using only heat diffusion data.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [39] [Micro-Macro Simulation of Shallow Water Moment Equations](https://arxiv.org/abs/2511.15737)
*Vilém Rožek*

Main category: physics.flu-dyn

TL;DR: The paper presents a micro-macro method for shallow water moment equations that achieves significant computational speed-up while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Shallow water equations are inaccurate for modeling shallow flows, while more accurate shallow water moment equations are computationally expensive. The micro-macro method is needed to speed up computations.

Method: Formulated the micro-macro method for shallow water moment equations, which switches between models of varying detail to allow larger stable time steps. Performed theoretical runtime analysis and tested with dam break and wave transport scenarios.

Result: The micro-macro method achieves significant speed-up while retaining sufficient accuracy in both dam break and wave transport tests.

Conclusion: The micro-macro method successfully accelerates shallow water moment equation computations while maintaining adequate accuracy, making it a practical approach for modeling shallow flows.

Abstract: Shallow flows are governed by the Navier-Stokes equations. They are commonly modelled using the shallow water equations, a great simplification of the Navier-Stokes equations, which often yields inaccurate results. For that reason, a model called shallow water moment equations has been developed. It uses more equations and variables than the shallow water equations. While this model is significantly more accurate, it is also computationally more expensive. To speed up computations, the micro-macro method may be used. The micro-macro method switches between two models of varying levels of detail allowing for larger stable time steps. In this paper we formulate the micro-macro method for shallow water moment equations. We perform a theoretical runtime analysis of the method and present a series of results for a dam break test and a wave transport test. The micro-macro method achieves a significant speed-up while retaining a sufficient level of accuracy.

</details>


### [40] [Fourth branch of instability of Stokes' wave and dependence of corresponding growth rate on nonlinearity](https://arxiv.org/abs/2511.16436)
*A. O. Korotkevich,A. O. Prokofiev*

Main category: physics.flu-dyn

TL;DR: The paper identifies the fourth superharmonic instability branch of Stokes' waves and validates phenomenological formulas for instability growth rates across all four branches.


<details>
  <summary>Details</summary>
Motivation: To extend understanding of Stokes' wave instabilities by discovering the fourth superharmonic branch and verifying existing growth rate formulas.

Method: Massive computational analysis of Stokes' waves, using least squares fitting with data from first three instability branches and phenomenological asymptotics.

Result: Successfully reached the fourth superharmonic instability branch; validated that existing formulas work for all four branches; corrected applicability ranges; reported growth rates for all branches.

Conclusion: The phenomenological formulas for instability growth rates are valid across all four superharmonic branches, with corrected applicability ranges, enhancing understanding of Stokes' wave behavior.

Abstract: Through a massive computation we reached the fourth superharmonic instability branch of the Stokes' wave. Using the obtained results we checked phenomenological formulae for the dependence of the instability growth rates corresponding to different branches of instability on the nonlinearity parameter (steepness, defined as the wave swing to wavelength ratio $H/Λ$) in the vicinity of the new instability branch appearance and far from it. It is demonstrated, that the formulae, the one obtained as a least squares fit using the information from the first three branches of instability and a phenomenological asymptotics, work for the fourth branch and previously reported branches as well. Range of applicability of the relations was corrected. Growth rates for all four instability branches are reported.

</details>


### [41] [General-purpose Data-driven Wall Model for Low-speed Flows Part I: Baseline Model](https://arxiv.org/abs/2511.16511)
*Yuenong Ling,Imran Hayat,Konrad Goc,Adrian Lozano-Duran*

Main category: physics.flu-dyn

TL;DR: A general-purpose wall model for LES using building-block flow principle with neural network regression to predict wall shear stress, outperforming traditional equilibrium wall models in 90% of test cases.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional equilibrium wall models and earlier building-block approaches by creating a more generalizable model applicable across complex geometries and flow conditions.

Method: Uses neural network regression with localized dimensionless inputs selected via information theory. Model has four components: baseline wall model, error model, classifier, and confidence score. Training includes new DNS data for turbulent boundary layers under various pressure gradients.

Result: Outperforms equilibrium wall model in 90% of test scenarios, maintains errors below 20% for 98% of cases across 140 diverse datasets including turbulent boundary layers, airfoils, and full aircraft geometries.

Conclusion: The baseline wall model successfully captures diverse flow phenomena and demonstrates superior performance compared to traditional approaches, providing a robust foundation for the complete wall modeling framework.

Abstract: We present a general-purpose wall model for large-eddy simulation. The model builds on the building-block flow principle, leveraging essential physics from simple flows to train a generalizable model applicable across complex geometries and flow conditions. The model addresses key limitations of traditional equilibrium wall models (EQWM) and improves upon shortcomings of earlier building-block-based approaches. The model comprises four components: (i) a baseline wall model, (ii) an error model, (iii) a classifier, and (iv) a confidence score. The baseline model predicts the wall-shear stress, while the error model estimates epistemic errors and aleatoric errors, both used for uncertainty quantification. In Part I of this work, we present the baseline model, while the remaining three components are introduced in Part II. The baseline model is designed to capture a broad range of flow phenomena, including turbulence over curved walls and zero, adverse, and favorable mean pressure gradients, as well as flow separation and laminar flow. The problem is formulated as a regression task to predict wall shear stress using a neural network. Model inputs are localized in space and dimensionless, with their selection guided by information-theoretic criteria. Training data include, among other cases, a newly generated direct numerical simulation dataset of turbulent boundary layers under favorable and adverse PG conditions. Validation is carried out through both a priori and a posteriori tests. The a priori evaluation spans 140 diverse high-fidelity numerical datasets and experiments (67 training cases included), covering turbulent boundary layers, airfoils, Gaussian bumps, and full aircraft geometries, among others. We demonstrate that the baseline wall model outperforms the EQWM in 90% of test scenarios, while maintaining errors below 20% for 98% of the cases.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [42] [Human-aligned Quantification of Numerical Data](https://arxiv.org/abs/2511.15723)
*Anton Kolonin*

Main category: physics.data-an

TL;DR: The paper evaluates metrics for quantifying numerical data into meaningful categories, finding that Silhouette coefficient above 0.65 and Dip Test below 0.5 indicate classifiable data, with Silhouette aligning better with human intuition than compression-based methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of determining when numerical data can be naturally quantified into meaningful categories (quantums) and finding appropriate numerical intervals, as people do intuitively but computers need computable metrics.

Method: Assessed information compression-based metrics and Silhouette coefficient for quantifying numerical data, comparing their correlation with each other and with human intuition.

Result: Found that data can be classified into distinct categories when Silhouette coefficient > 0.65 and Dip Test < 0.5; otherwise data follows unimodal normal distribution. Silhouette coefficient correlates better with human intuition than normalized centroid distance method.

Conclusion: Silhouette coefficient is more aligned with human intuition for data quantification than compression-based metrics, and specific threshold values can determine when numerical data can be meaningfully categorized.

Abstract: Quantifying numerical data involves addressing two key challenges: first, determining whether the data can be naturally quantified, and second, identifying the numerical intervals or ranges of values that correspond to specific value classes, referred to as "quantums," which represent statistically meaningful states. If such quantification is feasible, continuous streams of numerical data can be transformed into sequences of "symbols" that reflect the states of the system described by the measured parameter. People often perform this task intuitively, relying on common sense or practical experience, while information theory and computer science offer computable metrics for this purpose. In this study, we assess the applicability of metrics based on information compression and the Silhouette coefficient for quantifying numerical data. We also investigate the extent to which these metrics correlate with one another and with what is commonly referred to as "human intuition." Our findings suggest that the ability to classify numeric data values into distinct categories is associated with a Silhouette coefficient above 0.65 and a Dip Test below 0.5; otherwise, the data can be treated as following a unimodal normal distribution. Furthermore, when quantification is possible, the Silhouette coefficient appears to align more closely with human intuition than the "normalized centroid distance" method derived from information compression perspective.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [43] [A Fast Relax-and-Round Approach to Unit Commitment for Data Center Own Generation](https://arxiv.org/abs/2511.16420)
*Shaked Regev,Eve Tsybina,Slaven Peles*

Main category: math.OC

TL;DR: Proposes a relaxed unit commitment formulation that allows fractional generator states instead of binary decisions, enabling fast continuous optimization and GPU acceleration for large-scale systems with thousands of generators.


<details>
  <summary>Details</summary>
Motivation: Data centers increasingly use their own generation alongside utility power, creating systems with thousands of flexible generators where traditional mixed-integer unit commitment becomes computationally intractable.

Method: Relaxes binary commitment decisions to allow generators to be fractionally on, uses continuous solvers, then applies rounding to get feasible unit commitment solutions.

Result: For a 276-unit system, solution time reduced from 10 hours to less than 1 second with minor accuracy loss. Scales to tens of thousands of generators and enables GPU acceleration.

Conclusion: The proposed approach enables efficient unit commitment for large-scale systems with thousands of generators, making it practical for major power interconnections while maintaining computational tractability.

Abstract: The rapid growth of data centers increasingly requires data center operators to "bring own generation" to complement the available utility power plants to supply all or part of data center load. This practice sharply increases the number of generators on the bulk power system and shifts operational focus toward fuel costs rather than traditional startup and runtime constraints. Conventional mixed-integer unit commitment formulations are not well suited for systems with thousands of flexible, fast-cycling units. We propose a unit commitment formulation that relaxes binary commitment decisions by allowing generators to be fractionally on, enabling the use of algorithms for continuous solvers. We then use a rounding approach to get a feasible unit commitment. For a 276-unit system, solution time decreases from 10 hours to less than a second, with minor accuracy degradation. Our approach scales with no issues to tens of thousands of generators, which allows solving problems on the scale of the major North America interconnections. The bulk of computation is parallel and GPU compatible, enabling further acceleration in future work.

</details>


### [44] [A novel way of computing the shape derivative for a class of non-smooth PDEs and its impact on deriving necessary conditions for locally optimal shapes](https://arxiv.org/abs/2511.16127)
*Livia Betz*

Main category: math.OC

TL;DR: Derives necessary conditions for locally optimal shapes in non-smooth PDE-constrained design problems using functional variational approach, avoiding domain extensions or PDE approximations.


<details>
  <summary>Details</summary>
Motivation: To handle shape optimization problems governed by non-smooth PDEs where traditional methods fail due to lack of differentiability, and to develop a framework that can handle pointwise observations and state derivatives.

Method: Uses functional variational approach (FVA) to transform geometric optimization into optimal control problems, introduces novel sensitivity analysis technique, computes directional derivatives of states via functional variations to obtain shape derivatives.

Result: Developed a new method for computing shape derivatives without domain extensions or PDE approximations, established connection between locally optimal shapes and control problem minimizers, obtained necessary conditions for optimal shapes in non-smooth settings.

Conclusion: The FVA framework provides a rigorous approach for non-smooth PDE-constrained shape optimization, enabling derivation of necessary optimality conditions while handling complex observation terms and avoiding traditional limitations.

Abstract: We derive necessary conditions for locally optimal shapes of a design problem governed by a non-smooth PDE. The main particularity of the state system is the lack of differentiability of the nonlinearity. We work in the framework of the functional variational approach (FVA), which has the capacity to transfer geometric optimization problems into optimal control problems, the set of admissible shapes being parametrized by a large class of continuous mappings. In the FVA setting, we introduce a sensitivity analysis technique that is novel even for smooth PDEs. We emphasize that we do not resort to extensions on the hold-all domain or any kind of approximation of the original PDE. The computation of the directional derivative of the state w.r.t. functional variations results in a new way of computing the shape derivative. The presented approach allows us to handle in the objective pointwise observation and derivatives of the state on an observation set as well as distributed observation terms. In addition, we introduce the concept of locally optimal shapes and we put into evidence its connection to locally minimizers of the corresponding control problem. With directional differentiability results for the control-to-state map at our disposal, we can then state necessary conditions for locally optimal shapes in general non-smooth settings.

</details>
