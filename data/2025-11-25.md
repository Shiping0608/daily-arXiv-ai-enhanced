<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 17]
- [math.AP](#math.AP) [Total: 32]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [math.SP](#math.SP) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [math.CV](#math.CV) [Total: 1]
- [physics.atm-clus](#physics.atm-clus) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 5]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [math.PR](#math.PR) [Total: 3]
- [math-ph](#math-ph) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math.CA](#math.CA) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Variational Tensor-Product Splines](https://arxiv.org/abs/2511.17791)
*Vincent Guillemet,Michael Unser*

Main category: math.NA

TL;DR: The paper proposes a novel regularization scheme for multidimensional inverse problems using a combination of tensor-product differential operators and bounded-variation norms, showing that solutions are tensor products of 1D splines with bounded complexity.


<details>
  <summary>Details</summary>
Motivation: To develop an effective regularization method for multidimensional continuous-domain inverse problems that provides structured solutions with bounded complexity and handles the infinite-dimensional nullspace of differential operators.

Method: Constructs regularization as sum of two terms: 1) M norm of tensor-product differential operator L1‚äóL2, and 2) bounded-variation norm that regularizes the nullspace of L1‚äóL2.

Result: Proves that extreme points of solution set are tensor products of one-dimensional splines, with number of atoms bounded by data points. For localized data, shows bounded-variation term must be sum of norms precomposed with partial derivatives.

Conclusion: The proposed regularization framework yields structured solutions with bounded complexity and provides mathematical guarantees about solution properties for multidimensional inverse problems.

Abstract: Multidimensional continuous-domain inverse problems are often solved by the minimization of a loss functional, formed as the sum of a data fidelity and a regularization. In this work, we present a new construction where the regularization is itself built as the sum of two terms: i) the M norm of the regularizing operator L1 b L2, with L1 and L2 being two one-dimensional differential operators; ii) a bounded-variation norm that regularizes on the infinite-dimensional nullspace of L1 b L2. In this construction, we show that the extreme points of the solution set are the tensor product of one-dimensional splines, with a number of atoms upper-bounded in term of the number of data points. Further, when the data of the inverse problem is localized, we reveal that the term ii) must take the form of a sum of bounded-variation norms, precomposed with partial derivative of different orders.

</details>


### [2] [The Closest Point Method for Surface PDEs with General Boundary Conditions](https://arxiv.org/abs/2511.17820)
*Tony Wong,Colin B. Macdonald,Byungjoon Lee*

Main category: math.NA

TL;DR: Generalizes the closest point method to handle surface PDEs with general boundary conditions, providing a unified framework for inhomogeneous Neumann and Robin boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To extend the closest point method's capabilities beyond simple boundary conditions and create a unified approach for handling complex boundary conditions in surface PDEs.

Method: Proposes an extrapolation method within the closest point method framework to treat inhomogeneous Neumann and Robin boundary conditions.

Result: Demonstrates accuracy and robustness through numerical convergence studies on elliptic problems, Steklov eigenvalue problems, and nonlinear reaction-diffusion systems.

Conclusion: The generalized closest point method successfully handles general boundary conditions and shows promising performance across various PDE problems.

Abstract: We generalize the closest point method (CPM) to solve surface partial differential equations with general boundary conditions. The proposed extrapolation method provides a unified framework for treating a broad class of inhomogeneous Neumann and Robin boundary conditions within the framework of CPM. The accuracy and robustness of the method are demonstrated through numerical convergence studies of an elliptic problem, Steklov eigenvalue problems, and a nonlinear reaction-diffusion system.

</details>


### [3] [Fast and stable global interpolation based on equidistant points](https://arxiv.org/abs/2511.17911)
*Xu-Qing Liu,Hao Liu,Jian-Ying Rong*

Main category: math.NA

TL;DR: Symmetric wave interpolation method enables stable global interpolation using equidistant points, matching Chebyshev accuracy while avoiding Runge phenomenon.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between practical equidistant point accessibility and numerical stability in global interpolation, addressing the limitations of traditional methods.

Method: Symmetric wave interpolation method that leverages readily available equidistant points while incorporating numerical stability principles from Chebyshev interpolation.

Result: Effectively suppresses Runge phenomenon and achieves accuracy that matches or surpasses Chebyshev interpolation, providing robust performance with practical point accessibility.

Conclusion: Provides a robust and practical solution that successfully integrates the utility of equidistant points with the numerical stability of Chebyshev interpolation, solving a long-standing challenge in global interpolation.

Abstract: This paper presents the symmetric wave interpolation method for stable global interpolation using readily available equidistant points. Its key achievement is the integration of the practical utility of such points with the numerical stability of Chebyshev interpolation. Experimental results demonstrate that symmetric wave interpolation effectively suppresses the Runge phenomenon and, crucially, delivers accuracy that matches or even surpasses Chebyshev interpolation. This work thereby provides a robust and practical solution that bridges the long-standing gap between point accessibility and numerical stability in global interpolation.

</details>


### [4] [Coupling of conforming and mixed finite element methods for a model of wave propagation in thermo-poroelasticity in the frequency domain](https://arxiv.org/abs/2511.17984)
*Hongpeng Li,Cristian Carcamo,Hongxing Rui,Volker John*

Main category: math.NA

TL;DR: Analysis of a dynamic linear thermo-poroelasticity model with inertial and relaxation terms using frequency domain methods, variational formulation with Fredholm's alternative and T-coercivity, and stabilized finite element discretization.


<details>
  <summary>Details</summary>
Motivation: To develop a robust mathematical and numerical framework for dynamic thermo-poroelasticity problems that can handle inertial effects, relaxation terms, and avoid numerical issues like volumetric locking and oscillations.

Method: Frequency domain analysis, variational formulation using Fredholm's alternative and T-coercivity, stabilized coupling of conforming and mixed finite element spaces with projections in sesquilinear forms.

Result: Proved well-posedness of the continuous problem under appropriate coefficient assumptions, established well-posedness of finite element solution, derived optimal error estimates, and validated accuracy through numerical studies.

Conclusion: The proposed method provides a robust numerical framework for dynamic thermo-poroelasticity with proven mathematical properties and validated performance through numerical experiments.

Abstract: A dynamic linear thermo-poroelasticity model, containing inertial and relaxation terms with second-order time derivatives, is investigated in this paper. The mathematical and numerical analysis of this model is performed in the frequency domain. The variational formulation is analyzed within the framework of Fredholm's alternative and T-coercivity. Under appropriate assumptions on the coefficients, the well-posedness of the problem is proved. For its discretization, we propose a stabilized coupling of conforming and mixed finite element spaces, which are free of volumetric locking, and both, pressure as well as temperature oscillations. By incorporating projections in certain sesquilinear forms, the well-posedness of the finite element solution can be obtained through a similar reasoning as in the continuous case. Optimal error estimates are derived for all variables. Numerical studies validate the accuracy and robustness of the proposed method.

</details>


### [5] [Two-step Generalized RBF-Generated Finite Difference Method on Manifolds](https://arxiv.org/abs/2511.18049)
*Rongji Li,Haichuan Di,Shixiao Willing Jiang*

Main category: math.NA

TL;DR: A two-step generalized radial basis function-generated finite difference (gRBF-FD) method is developed for solving PDEs on manifolds from randomly sampled point clouds, using polyharmonic spline kernels and polynomials in local tangent spaces with automatic stencil size tuning.


<details>
  <summary>Details</summary>
Motivation: Solving PDEs on manifolds defined by randomly sampled point clouds is challenging but has broad applications in scientific computing and various fields.

Method: Two-step approach: (1) regress local target function using generalized moving least squares (GMLS), (2) compensate residual using polyharmonic spline (PHS) interpolation. Uses PHS+Poly kernels in local Monge coordinate system with automatic stencil size tuning and specific weight functions.

Result: Method produces Laplacian matrix with specific coefficient structure that enhances stability and reduces solution error. Error bounds established for operator approximation. Numerical tests demonstrate high accuracy on various smooth manifolds.

Conclusion: The gRBF-FD method provides an effective approach for solving PDEs on manifolds from point cloud data, with improved stability and accuracy through its two-step interpolation strategy and automatic parameter tuning.

Abstract: Solving partial differential equations (PDEs) on manifolds defined by randomly sampled point clouds is a challenging problem in scientific computing and has broad applications in various fields. In this paper, we develop a two-step generalized radial basis function-generated finite difference (gRBF-FD) method for solving PDEs on manifolds without boundaries, identified by randomly sampled point cloud data. The gRBF-FD is based on polyharmonic spline kernels and multivariate polynomials (PHS+Poly) defined over the tangent space in a local Monge coordinate system. The first step is to regress the local target function using a generalized moving least squares (GMLS) while the second step is to compensate for the residual using a PHS interpolation. Our gRBF-FD method has the same interpolant form with the standard RBF-FD but differs in interpolation coefficients. Our approach utilizes a specific weight function in both the GMLS and PHS steps and implements an automatic tuning strategy for the stencil size K (i.e., the number of nearest neighbors) at each point. These strategies are designed to produce a Laplacian matrix with a specific coefficient structure, thereby enhancing stability and reducing the solution error. We establish an error bound for the operator approximation in terms of the so-called local stencil diameter as well as in terms of the number of data. We further demonstrate the high accuracy of gRBF-FD through numerical tests on various smooth manifolds.

</details>


### [6] [Space-time adaptive methods for parabolic evolution equations](https://arxiv.org/abs/2511.18180)
*Jun Wang,Jie Su,Leslie Greengard,Shidong Jiang,Shravan Veerapaneni*

Main category: math.NA

TL;DR: Development of integral equation-based solvers for PDEs with adaptive quadtree refinement in 2D space.


<details>
  <summary>Details</summary>
Motivation: To efficiently track complex solution features in space-time for various PDEs using adaptive methods.

Method: Integral equation-based solvers with adaptive quadtree refinement and coarsening at each time step, applied to periodic boundary problems in square domains.

Result: Demonstrated performance and robustness through several numerical examples.

Conclusion: The methods effectively follow complex solution features in space-time using adaptive refinement techniques.

Abstract: We present a family of integral equation-based solvers for the heat equation, reaction-diffusion systems, the unsteady Stokes equation and the incompressible Navier-Stokes equations in two space dimensions. Our emphasis is on the development of methods that can efficiently follow complex solution features in space-time by refinement and coarsening at each time step on an adaptive quadtree. For simplicity, we focus on problems posed in a square domain with periodic boundary conditions. The performance and robustness of the methods are illustrated with several numerical examples.

</details>


### [7] [Trigonometric-Interpolation Based Approach for Second-Order Volterra Integro-Differential Equations](https://arxiv.org/abs/2511.18193)
*Xiaorong Zou*

Main category: math.NA

TL;DR: Trigonometric interpolation is extended from solving second-order Fredholm integro-differentiable equations (FIDE) to second-order Volterra integro-differentiable equations (VIDE), maintaining high accuracy with moderate grid points and handling singularities effectively.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient algorithm for solving second-order Volterra integro-differentiable equations (VIDE) that inherits the advantages demonstrated in previous work on FIDEs, including high accuracy with moderate grid points and effective handling of kernel singularities.

Method: Apply trigonometric interpolation to solve second-order Volterra integro-differentiable equations (VIDE), extending the same methodology previously used for FIDEs.

Result: Numerical experiments with various boundary conditions show decent performance as expected, demonstrating the method's effectiveness for VIDE problems.

Conclusion: The trigonometric interpolation method successfully extends from FIDE to VIDE problems, maintaining its advantages of high accuracy with moderate grid points, effective singularity handling, and compatibility with general boundary conditions.

Abstract: The trigonometric interpolation has been recently applied to solve a second-order Fredholm integro-differentiable equation (FIDE). It achieves high accuracy with a moderate size of grid points and effectively addresses singularities of kernel functions. In addition, it work well with general boundary conditions and the framework can be generalized to work for FIDEs with a high-order ODE component. In this paper, we apply the same idea to develop an algorithm for the solution of a second-order Volterra integro-differentiable equation (VIDE) with the same advantages as in the study of FIDE. Numerical experiments with various boundary conditions are conducted with decent performances as expected.

</details>


### [8] [Numerical Approximation of Lambert W Function For Real Values By Unique Method of Quadratic Approximation](https://arxiv.org/abs/2511.18381)
*Narinder Kumar Wadhawan*

Main category: math.NA

TL;DR: New iterative method for approximating Lambert W function in real domain with strategies for both positive and negative inputs


<details>
  <summary>Details</summary>
Motivation: Need for flexible Lambert W approximation method that works for both branches without restrictive initial assumptions

Method: Transform function into simpler form and use iterative refinement with different strategies for positive inputs, extended to handle negative inputs

Result: Method achieves accurate approximations for Lambert W function across both branches with demonstrated accuracy and flexibility

Conclusion: Proposed iterative approach provides effective Lambert W approximation without restrictive assumptions, suitable for various applications

Abstract: This paper introduces a new numerical method for approximating the Lambert W function in the real domain. The method transforms the function into a simpler form that allows iterative refinement of an initial guess. Two iterative strategies are proposed for positive inputs, and the method is extended to handle negative inputs within a defined range. Unlike standard methods, this approach works for both branches without restrictive initial assumptions. Examples and software demonstrate the accuracy and flexibility of the method.

</details>


### [9] [Stationarity preservation and the low Mach number behaviour of the Discontinuous Galerkin method on Cartesian grids](https://arxiv.org/abs/2511.18505)
*Wasilij Barsukow*

Main category: math.NA

TL;DR: Analysis of DG method stationary states for linear acoustics shows polynomial degree threshold for stationarity preservation, with potential accuracy reduction depending on numerical flux choice.


<details>
  <summary>Details</summary>
Motivation: To understand why numerical methods for hyperbolic problems can have inconsistent stationary states compared to PDEs, particularly addressing low Mach number consistency issues in Euler equations.

Method: Theoretical and experimental exploration of Discontinuous Galerkin method stationary states for linear acoustics on Cartesian grids, extending previous finite difference studies.

Result: DG method is stationarity preserving for polynomial degrees above a threshold, but numerical flux choice can reduce order of accuracy at stationary state.

Conclusion: The findings explain DG method behavior for Euler equations at low Mach number, revealing trade-offs between stationarity preservation and accuracy depending on polynomial degree and flux selection.

Abstract: Due to added numerical stabilization (diffusion), the stationary states of numerical methods for hyperbolic problems need not be consistent discretizations of those of the PDEs. A closely related phenomenon is the lack of consistency of common finite volume methods for the Euler equations in the limit of low Mach number. In this work, the stationary states of the Discontinuous Galerkin (DG) method for linear acoustics on Cartesian grids are explored theoretically and experimentally, thus extending previous studies in the context of first-order finite difference methods. It is found that for a polynomial degree above some threshold, DG is stationarity preserving, but depending on the choice of numerical flux can suffer from a reduction of the order of accuracy at stationary state. This allows to explain the behaviour of the method for the Euler equations at low Mach number.

</details>


### [10] [Introduction and Numerical Validation of an Open-Source MATLAB Package for Quantitative Ultrasound Tomography via Ray-Born Inversion](https://arxiv.org/abs/2511.18511)
*Ashkan Javaherian*

Main category: math.NA

TL;DR: MATLAB package for ultrasound sound-speed imaging using two-point ray tracing with ToF and ray-Born inversion methods


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive software package for reconstructing sound-speed images from transmission ultrasound data with complementary reconstruction strategies

Method: Two-point ray tracing with four algorithms, combining time-of-flight (ToF) for initial estimates and ray-Born inversion for high-resolution reconstructions

Result: Validated ray-tracing algorithms with analytical trajectories, tested reconstruction methods on synthetic and experimental datasets

Conclusion: The package offers both low-artefact ToF and high-resolution ray-Born methods for sound-speed image reconstruction from ultrasound data

Abstract: We present a MATLAB package for reconstructing sound-speed images from transmission ultrasound data. The package is based on two-point ray tracing and implements two complementary inversion strategies for image reconstruction. The first is a time-of-flight (ToF) method that produces low-resolution, low-contrast images with minimal artefacts. The second is a ray-Born inversion method, which integrates high-frequency ray theory with the Born approximation to generate high-resolution sound-speed reconstructions. Early iterations of the ToF reconstruction are used to provide an initial estimate for the more advanced ray-Born approach. The core of this software package consists of four ray-tracing algorithms, whose accuracy is assessed in this study with respect to known analytical trajectories and accumulated acoustic path lengths. Furthermore, both image-reconstruction strategies have been validated numerically with simulated synthetic datasets and experimentally with open-source in-vitro and in-vivo datasets in related parallel studies.

</details>


### [11] [Benchmarking stabilized and self-stabilized p-virtual element methods with variable coefficients](https://arxiv.org/abs/2511.18943)
*Paola Pia Foligno,Daniele Boffi,Fabio Credali,Riccardo Vescovini*

Main category: math.NA

TL;DR: This paper investigates stabilized and self-stabilized formulations for p-version VEM, showing self-stabilized methods achieve optimal accuracy but worse conditioning, and introduces a new approach for handling variable coefficients.


<details>
  <summary>Details</summary>
Motivation: Standard VEM requires stabilization terms that don't conform to problem physics and lack theoretical support, while accuracy deteriorates with variable coefficients, limiting real-world applications.

Method: Numerical investigation of different stabilized and self-stabilized p-VEM formulations, plus a new approach modifying discrete space to account for variable coefficients in polynomial projectors.

Result: Self-stabilized and stabilization-free formulations achieve optimal accuracy but suffer from worse conditioning; new variable coefficient approach shows improved robustness over standard methods.

Conclusion: The new approach for variable coefficients is more robust than standard methods, though self-stabilized formulations trade optimal accuracy for worse conditioning.

Abstract: Standard Virtual Element Methods (VEM) are based on polynomial projections and require a stabilization term to evaluate the contribution of the non-polynomial component of the discrete space. However, the stabilization term does not generally conform to the physics of the considered problem and a criterion for its choice is not typically supported by theoretical arguments. Hence, stabilization-free and self-stabilized formulations have been proposed. Moreover, the accuracy of VEM deteriorates in case of problems with variable coefficients, that are not usually accounted when constructing polynomial projections. The mentioned aspects might limit the use of the method when tackling real-world applications. This paper provides an in-depth numerical investigation into different stabilized and self-stabilized formulations for the p-version of VEM. The results show that self-stabilized and stabilization-free formulations achieve optimal accuracy, while suffering from worse conditioning. Moreover, a new approach for dealing with variable coefficients is introduced: the discrete space is modified so that general coefficients can be accounted when computing the polynomial projectors. Numerical results show that this new approach is more robust than the standard ones.

</details>


### [12] [Local Multilevel Preconditioned Jacobi-Davidson Method for Elliptic Eigenvalue Problems on Adaptive Meshes](https://arxiv.org/abs/2511.18996)
*Jianing Guo,Qigang Liang,Xuejun Xu*

Main category: math.NA

TL;DR: Efficient adaptive multilevel preconditioned Jacobi-Davidson method for eigenvalue problems with singularity, achieving O(N) computational complexity and uniform convergence unaffected by discontinuous coefficients.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient solution for eigenvalue problems with singularity that maintains optimal computational complexity and robust convergence properties.

Method: Multilevel preconditioned Jacobi-Davidson method with local smoothing strategy for solving algebraic systems from adaptive finite element methods.

Result: Algorithm achieves O(N) computational complexity, uniform convergence rate independent of mesh levels and degrees of freedom, and robustness against highly discontinuous coefficients.

Conclusion: The proposed method is theoretically sound and numerically verified, providing an efficient and robust solution for singular eigenvalue problems with optimal complexity.

Abstract: In this work, we propose an efficient adaptive multilevel preconditioned Jacobi-Davidson (PJD) method for eigenvalue problems with singularity. Our multilevel method utilizes a local smoothing strategy to solve the preconditioned Jacobi-Davidson algebraic systems arising from adaptive finite element methods (AFEM). As a result, the algorithm holds optimal computational complexity $O(N)$. The theoretical analysis reveals that our method has a uniform convergence rate with respect to mesh levels and degrees of freedom. Further, the convergence rate is not affected by highly discontinuous coefficients within the domain. Numerical results verify our theoretical findings.

</details>


### [13] [A Smoothly Varying Quadrature Approach for 3D IgA-BEM Discretizations: Application to Stokes Flow Simulations](https://arxiv.org/abs/2511.19006)
*Cesare Bracco,Francesco Patrizi,Alessandra Sestini*

Main category: math.NA

TL;DR: Novel quadrature strategy for IgA-BEM collocation methods using desingularizing variable transformation that adapts to distance from singularities, eliminating traditional integral classification and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Boundary integrals in IgA-BEM face challenges with singular/nearly singular kernels, especially when integration regions are close to collocation points, requiring efficient quadrature rules.

Method: Desingularizing change of variables that automatically adapts to physical distance from singularities, intensifying near polar points and weakening farther away, integrated over B-spline supports rather than individual elements.

Result: Validated through 3D Stokes problem benchmarks, achieving excellent convergence rates with enhanced accuracy and robustness.

Conclusion: The method provides automatic calibration that eliminates sensitivity to traditional integral classification, reduces computational cost for higher-degree splines, and improves quadrature performance in IgA-BEM.

Abstract: We introduce a novel quadrature strategy for Isogeometric Analysis (IgA) boundary element discretizations, specifically tailored to collocation methods. Thanks to the dimensionality reduction and the natural handling of unbounded domains, boundary integral formulations are particularly appealing in the IgA framework. However, they require the evaluation of boundary integrals whose kernels exhibit singular or nearly singular behavior. Even when the kernel is not singular, its numerical evaluation becomes challenging whenever the integration region lies close to a collocation point. These integrals of polar and nearly singular functions represent the main computational difficulty of IgA-BEM and motivate the development of efficient and accurate quadrature rules. Unlike traditional methods that classify integrals as singular, nearly singular, or regular, our approach employs a desingularizing change of variables that smoothly adapts to the physical distance from singularities in the boundary integral kernels. The transformation intensifies near the polar point and progressively weakens when integrating over portions of the domain that are farther from it, ultimately leaving the integrand unchanged in the limit of a diametrically opposed region. This automatic calibration enhances accuracy and robustness by eliminating the traditional classification step, to which the approximation quality is often highly sensitive. Moreover, integration is performed directly over B-spline supports rather than over individual elements, reducing computational cost, particularly for higher-degree splines. The proposed method is validated through boundary element benchmarks for the three dimensional Stokes problem, where we achieve excellent convergence rates.

</details>


### [14] [Multigrid with Linear Storage Complexity](https://arxiv.org/abs/2511.19036)
*Daniel Bauer,Nils Kohl,Stephen F. McCormick,Rasmus Tamstorf*

Main category: math.NA

TL;DR: A full multigrid method that compresses solution vectors to reduce storage from O(n log n) to O(n) bits, achieving 4-12 bits per DoF for elliptic PDE solutions.


<details>
  <summary>Details</summary>
Motivation: As PDE discretization errors decrease, coefficient storage requirements increase. Current methods require O(n log n) bits, which becomes problematic for large-scale problems on memory-limited supercomputers.

Method: Full multigrid method with compressed storage format for solution and intermediate vectors. Matrix-free implementation with adaptive bit allocation across grid levels.

Result: Achieved storage reduction to 4-12 bits per DoF for solutions, 3-6 bits for residuals/corrections. Expected memory footprint reduction of about one order of magnitude compared to mixed-precision methods.

Conclusion: The compressed multigrid method enables linear space complexity for solving elliptic PDEs, making large-scale problems feasible on current supercomputers with significantly reduced memory requirements.

Abstract: As the discretization error for the solution of a partial differential equation (PDE) decreases, the precision required to store the corresponding coefficients naturally increases. Storing the solution's finite element coefficients explicitly requires $\mathcal O(n \log n)$ bits of storage, where $n$ is the number of degrees of freedom (DoFs). This paper presents a full multigrid method to compute the solution in a compressed format that reduces the storage complexity of the solution and intermediate vectors to $\mathcal O(n)$ bits. This reduction allows a matrix-free implementation to solve elliptic PDEs with an overall linear space complexity. For problems limited by the memory capacity of current supercomputers, we expect a memory footprint reduction of about an order of magnitude compared to state-of-the-art mixed-precision methods. We demonstrate the applicability of our algorithm by solving two model problems. Depending on the PDE and polynomial degree, but irrespective of the problem size, the solution vector on the finest grid requires between 4 and 12 bits per DoF, and the residual and correction require 3 to 6 bits each. Additional data is stored on the coarse grids with modestly increasing bit widths toward coarser grids.

</details>


### [15] [Scientific Calculator With The Aid Of Geometry And Based Upon It, A Mechanical Calculator](https://arxiv.org/abs/2511.19214)
*Narinder Kumar Wadhawan*

Main category: math.NA

TL;DR: Geometric methods using similar right-angled triangles enable scientific calculations without electronic calculators


<details>
  <summary>Details</summary>
Motivation: To perform battery-free scientific calculations using geometric principles instead of electronic devices

Method: Drawing a right-angled triangle with unit perpendicular side and constructing multiple perpendiculars to create similar triangles, then using geometric series properties for calculations

Result: Developed a mechanical analogue calculator that can perform multiplication, division, exponents, roots, and other scientific operations geometrically

Conclusion: Geometric methods provide an effective alternative to electronic calculators for scientific calculations, offering error-free results without battery dependency

Abstract: Scientific calculations involving multiplication, division, exponents, inverse exponents of real numbers, geometric mean, reciprocal, Euler number, logarithm, and antilogarithm are generally carried out using battery operated electronic calculators. In this paper, geometric methods employing properties of similar right angled triangles have been devised for performing error free scientific calculations without the use of batteries. Based on this method, a mechanical analogue calculator has also been designed. A right-angled triangle with one perpendicular side of unit length is drawn, and from the vertex of the right angle, a perpendicular is drawn onto the hypotenuse. From the point of intersection of this perpendicular with the hypotenuse, another perpendicular is drawn to the base, and this process is continued until n perpendiculars are drawn. The resultant figure is a right angled triangle containing n similar triangles. Using properties of similar triangles, it is found that the lengths of the first, second, third so on till last perpendicular form a geometric series. If the length of the nth perpendicular is equated to a given real positive quantity less than one, and the base angle is adjusted, then the length of the first perpendicular represents the nth root of the given quantity. If the given quantity is greater than one, its reciprocal is equated to the length of the nth perpendicular, and the reciprocal of the first perpendicular gives the nth root. In this manner, several scientific calculations can be performed using geometric techniques.

</details>


### [16] [Numerical Approximation In Real Domain Of Special Function Of Product Of A Variable And Its Double Exponential](https://arxiv.org/abs/2511.19270)
*Narinder Kumar Wadhawan*

Main category: math.NA

TL;DR: A novel approximation method for solving transcendental functions involving products of variables and double exponentials by transforming them into quadratic equations through linear approximations of logarithms.


<details>
  <summary>Details</summary>
Motivation: To solve transcendental functions containing products of variables and double exponentials, which are difficult to solve analytically, through an efficient approximation approach.

Method: Use linear approximation of natural logarithm to transform the transcendental function into a quadratic equation, then iterate the process multiple times for increased precision.

Result: The method successfully approximates solutions to complex transcendental functions, with accuracy improving with more iterations as demonstrated in tabular examples.

Conclusion: The proposed iterative approximation method provides an effective solution for transcendental functions involving products and double exponentials, with verifiable accuracy through multiple examples.

Abstract: Purpose of writing this paper is to solve a transcendental function containing a product of a variable and its double exponential by a unique method of approximation. If the value of the said product is given, then its inverse function is approximated by use of linear expression in place of natural logarithm of a positive real quantity and, that transforms the function to a quadratic equation. Roots of the equation are, then used for solving the function. For precise approximation, the process is iterated a number of times and more the number of iterations, more precise will be the approximation. To prove truthfulness of the formulae derived, a number of examples are given in tabular form.

</details>


### [17] [Finite Element Spaces of Double Two-Forms With Polynomial Coefficients](https://arxiv.org/abs/2511.19297)
*Yakov Berchenko-Kogan,Lily DiPaulo*

Main category: math.NA

TL;DR: Development of finite element spaces for symmetric tensor products of two-forms with polynomial coefficients, applicable to elasticity and numerical relativity.


<details>
  <summary>Details</summary>
Motivation: To create higher-order finite element spaces for matrix fields with normal-normal continuity, needed for TDNNS elasticity methods and representing Riemann curvature tensor in numerical relativity.

Method: Develop finite element spaces of symmetric tensor products of two-forms with polynomial coefficients, obtaining explicit geometrically decomposed basis of shape functions similar to Li's approach.

Result: Created spaces with shape functions distributed differently than Regge elements - one per triangle and two per tetrahedron in constant coefficient case, addressing the presence of shape functions of two different types.

Conclusion: Successfully developed finite element spaces that parallel Li's generalization of Regge calculus but handle symmetric tensor products of two-forms with distinct shape function distribution.

Abstract: We develop finite element spaces of symmetric tensor products of two-forms with polynomial coefficients. In three dimensions, these give higher order finite element spaces of matrix fields with normal-normal continuity, which have applications to the TDNNS method for elasticity, for example. In general dimension, these spaces can be used to represent the Riemann curvature tensor in numerical relativity. In many ways, our methods parallel Li's work generalizing Regge calculus to higher order, as Regge elements can be thought of as symmetric tensor products of one-forms. However, whereas the constant coefficient Regge space has one shape function per edge, the constant coefficient space of double-forms in our paper has one shape function per triangle and two shape functions per tetrahedron, so we must address the fact that there are shape functions of two different types. Like Li, we obtain an explicit geometrically decomposed basis of shape functions.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [18] [Global existence of smooth solution to evolutionary Faddeev model with short-pulse data](https://arxiv.org/abs/2511.17534)
*Shaoying Luo,Jinhua Wang,Changhua Wei*

Main category: math.AP

TL;DR: The paper proves global existence of smooth solutions for the evolutionary Faddeev model with large initial data of short pulse type using energy estimates and specially adapted multipliers.


<details>
  <summary>Details</summary>
Motivation: To establish global existence of smooth solutions for the evolutionary Faddeev model, which maps from Minkowski space to the unit sphere, particularly for large initial data in energy norm.

Method: Energy estimates with specially adapted multipliers that account for the geometry of the system, applied to the nonlinear wave equations with null structure.

Result: The evolutionary Faddeev model admits globally smooth solutions for a class of large initial data of short pulse type.

Conclusion: By using appropriate multipliers adapted to the system's geometry, global smooth solutions can be established for the evolutionary Faddeev model with large initial data.

Abstract: This paper is concerned with the Cauchy problem of the evolutionary Faddeev model, a system that maps from the Minkowski space $\mathbb{R}^{1+3}$ to the unit sphere $\mathbb{S}^2$. The model is a system of nonlinear wave equations whose nonlinearities exhibit a null structure and include semilinear terms, quasilinear terms, and the unknowns themselves. By considering a class of large initial data (in energy norm) of the short pulse type, we prove that the evolutionary Faddeev model admits a globally smooth solution via energy estimates. The main result is achieved through the selection of appropriate multipliers that are specially adapted to the geometry of the system.

</details>


### [19] [Biharmonic non-linear Schr√∂dinger equation with an unbounded inhomogeneous term](https://arxiv.org/abs/2511.17548)
*Taif Abdullah Enaoufal,Tarek Saanouni*

Main category: math.AP

TL;DR: Analysis of a focusing nonlinear biharmonic Schr√∂dinger equation with unbounded inhomogeneous term, establishing global existence vs. finite-time blow-up dichotomy for solutions below ground state energy under radial symmetry.


<details>
  <summary>Details</summary>
Motivation: To study the long-time behavior of solutions to biharmonic Schr√∂dinger equations with unbounded growing inhomogeneous terms, which break space translation invariance and present new analytical challenges.

Method: Derived an inhomogeneous Gagliardo-Nirenberg inequality adapted to the unbounded weight, then analyzed solution behavior using this inequality and Strauss type Sobolev estimates under radial symmetry assumption.

Result: Established a sharp dichotomy: solutions with initial data below ground state energy either exist globally or experience finite-time blow-up. Radial symmetry is essential for the analysis due to the unbounded inhomogeneous term.

Conclusion: The work provides new insights into biharmonic Schr√∂dinger equations with unbounded inhomogeneities, highlighting the necessity of radial symmetry and the distinct phenomena arising from breaking space translation invariance.

Abstract: This paper is devoted to the analysis of a focusing nonlinear biharmonic Schr√∂dinger equation in the presence of an unbounded growing up inhomogeneous term. The first main contribution of this work is the derivation of an inhomogeneous Gagliardo-Nirenberg inequality adapted to the unbounded weight, which provides the necessary control over the nonlinear term in terms of Sobolev norms. Building on this inequality, we then investigate the long-time behavior of solutions and establish a sharp dichotomy: solutions with initial data below the ground state energy either exist globally in time or experience finite-time blow-up. A distinctive feature of our results is that the analysis of the unbounded inhomogeneous term requires the imposition of radial symmetry on the initial data, which allows us to exploit certain Strauss type Sobolev estimates that would not hold in the general non-radial case. This work complements previous studies on biharmonic Schr√∂dinger equations with singular inhomogeneities, highlighting both the challenges and the new phenomena that arise when the nonlinearity is weighted by a growing up unbounded function, which broke the space translation invariance of the standard homogeneous associated equation.

</details>


### [20] [Cahn-Hilliard Equations on Lattices: Dynamic Transitions and Pattern Formations](https://arxiv.org/abs/2511.17642)
*Jared Grossman,Evan Halloran,Shouhong Wang*

Main category: math.AP

TL;DR: Analysis of dynamic phase transitions and pattern formations in binary systems using Cahn-Hilliard equation on 2D lattices, showing geometry-dependent transitions to hexagonal, roll, and square patterns.


<details>
  <summary>Details</summary>
Motivation: To understand how domain geometry and physical parameters influence dynamic phase transitions and pattern formations in binary systems modeled by the Cahn-Hilliard equation.

Method: Used Cahn-Hilliard equation on 2D lattice structures, decomposed function space into stable/unstable eigenspaces, calculated center manifold, analyzed reduced equations with different eigenvalue multiplicities, and compared with long-range interaction model.

Result: Found that dynamic transitions are geometry-dependent, with emergence of hexagonally-packed circles, rolls, and square structures. Critical eigenvalue multiplicity depends on domain geometry. Long-range interaction model produces similar results.

Conclusion: Domain geometry significantly influences dynamic phase transitions and pattern formations in binary systems, with different geometries producing distinct patterns like hexagonal, roll, and square structures.

Abstract: This article examines the dynamic phase transitions and pattern formations attributed to binary systems modeled by the Cahn-Hilliard equation. In particular, we consider a two-dimensional lattice structure and determine how different choices of the spanning vectors influence the resulting dynamical tramsitions and pattern formations. As the basic steady-state loses its linear stability, the binary system undergoes a dynamic transition which is shown to be characterized by both the geometry of the domain and the choice of physical parameters of the model. Unlike rectangular domains, we are able to observe the emergence of hexagonally-packed circles, as well as the familiar rolls and square structures. We begin with the decomposition of our function space into a stable and unstable eigenspace before calculating the center manifold that maps the former to the later. In analyzing the resulting reduced equations, we consider the different multiplicities that the critical eigenvalue can have, which is shown to be geometry-dependent. We briefly consider the long-range interaction model and determine that it produces similar results to the original model.

</details>


### [21] [A note on two Collatz evolution flows](https://arxiv.org/abs/2511.17650)
*Francisco Alegr√≠a,Mat√≠as Morales,Claudio Mu√±oz,Felipe Poblete*

Main category: math.AP

TL;DR: This paper introduces two evolution models based on the generalized Collatz operator, analyzing their dynamical properties and establishing connections between continuous flows and discrete Collatz orbits.


<details>
  <summary>Details</summary>
Motivation: To develop continuous and discrete evolution models that generalize the classical Collatz operator, enabling deeper analysis of Collatz dynamics through mathematical frameworks like Fourier analysis and discrete energy methods.

Method: Two approaches: (1) A continuum Fourier-based model inspired by the Cubic Szeg≈ë operator, analyzed in L¬≤(ùïã) space; (2) A discrete model examining time derivatives of Collatz orbits with conserved discrete energy.

Result: Proved local/global existence for the continuous model in L¬≤(ùïã), established one-to-one characterization of periodic/unbounded Collatz orbits via continuous flows, introduced conserved discrete energy with growth bounds, and showed increasing sum property for discrete time derivatives in generalized operators.

Conclusion: The paper successfully bridges continuous and discrete approaches to Collatz dynamics, providing new mathematical tools and characterizations that enhance understanding of Collatz operator properties and their evolution.

Abstract: Two evolution models based on the generalized Collatz operator are introduced. These models are characterized by coefficients $Œ±$ and $Œ≤$ in the Collatz dynamics, and are suitably defined. Here, $Œ±=Œ≤=1$, and $Œ±=3$, $Œ≤=1$ correspond to the Nollatz and classical Collatz operators, respectively. In general, the first evolution model is a continuum, Fourier side based, motivated by the Cubic Szeg≈ë operator of G√©rard and Grellier. The second evolution considers discrete time derivatives of the Collatz orbits. In this paper we describe the evolution of both models, with particular emphasis on dynamical properties. For the first one, it is proved local and global existence in the space $L^2(\mathbb T)$, and a one-to-one characterization of the existence of nontrivial periodic and unbounded orbits of the Collatz mapping in terms of particular set of solutions of this continuous Collatz flow. For the discrete part, a sort of discrete energy is introduced. This energy has the property of being conserved by the discrete flow. An estimate of each term in this energy is given, proving suitable growth bounds. Finally, the meaning of the discrete time derivative for the generalized Collatz orbits is discussed. It is proved that, except for the Nollatz and Collatz operators, the sum of coefficients related to this discrete time derivative is an increasing sequence in $n$ as the iteration parameter $n$ evolves.

</details>


### [22] [Long-Time Dynamics of the Zakharov-Kuznetsov Equation](https://arxiv.org/abs/2511.17830)
*Roberto de A. Capistrano Filho,Ailton Nascimento*

Main category: math.AP

TL;DR: The paper proves local and global exponential stabilization for the Zakharov-Kuznetsov equation using damping and anti-damping mechanisms, determining optimal constants and minimal decay time.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous stabilization results for the two-dimensional Zakharov-Kuznetsov equation, extending Korteweg-de Vries-type analysis to higher dimensions with damping mechanisms.

Method: Two different rigorous proof approaches using damping mechanisms and internal delay terms (anti-damping) to achieve stabilization.

Result: Proved both local and global exponential stabilization properties, determined optimal constant and minimal time required for exponential energy decay.

Conclusion: The Zakharov-Kuznetsov equation can be effectively stabilized using damping and anti-damping terms, with precise characterization of stabilization parameters.

Abstract: This manuscript presents the results of stabilization for the Zakharov--Kuznetsov equation, a two-dimensional Korteweg--de Vries-type equation. We provide rigorous proofs using two different approaches, showing that when a damping mechanism and an internal delay term (anti-damping) are introduced, the solutions of the Zakharov--Kuznetsov equation exhibit both local and global exponential stabilization properties. A significant contribution of our work is the determination of the optimal constant and the minimal time required to ensure exponential decay of the energy associated with this two-dimensional system.

</details>


### [23] [Oscillatory behavior of solutions to the critical Fujita equation in 6D](https://arxiv.org/abs/2511.17891)
*Junichi Harada*

Main category: math.AP

TL;DR: The paper studies long-term dynamics of solutions to the 6D energy critical heat equation, constructing a radially symmetric global solution that exhibits oscillatory behavior in scaling parameter Œª(t) between 0 and infinity.


<details>
  <summary>Details</summary>
Motivation: To investigate how solutions to the 6D energy critical heat equation behave over long time scales, particularly showing that dynamics in the homogeneous Sobolev space ƒ§¬π differ significantly from the standard H¬π space.

Method: Construct a radially symmetric global solution of the form u(x,t) = Œª(t)^{-2}Q(x/Œª(t)) + error term, where Œª(t) oscillates between 0 and infinity, and the error term vanishes in the ƒ§¬π norm as t‚Üí‚àû.

Result: Existence of a solution where the scaling parameter Œª(t) satisfies liminf Œª(t)=0 and limsup Œª(t)=‚àû, while the error term converges to zero in ƒ§¬π norm, demonstrating oscillatory behavior.

Conclusion: The constructed solution shows fundamentally different dynamical behavior in ƒ§¬π(‚Ñù‚Å∂) compared to H¬π(‚Ñù‚Å∂), highlighting the importance of the function space choice for understanding long-time dynamics of energy critical equations.

Abstract: Long time dynamics of solutions to the 6D energy critical heat equation
  $u_t=Œîu+|u|^{p-1}u$ on $\R^6\times(0,\infty)$ is investigated.
  It is shown that
  there exists a radially symmetric global solution $u(x,t)\in C([0,\infty);\dot H^1(\R^6))$
  of the form
  \begin{align*}
  u(x,t)
  =
  Œª(t)^{-\frac{n-2}{2}}
  {\sf Q}(\tfrac{x}{Œª(t)})
  +
  \text{error}
  (x,t),
  \end{align*}
  where the function \( Œª(t) \) satisfies:
  \begin{itemize}
  \item
  $\dis\lim_{t\to\infty}\|\text{error}(\cdot,t)\|_{\dot H_x^1(\R^6)}=0$,
  \item
  $\dis\liminf_{t\to\infty}Œª(t)=0$,
  \item
  $\dis\limsup_{t\to\infty}Œª(t)=\infty$.
  \end{itemize}
  The solutions constructed here
  demonstrate that the dynamical behavior in
  \( \dot H^1(\mathbb{R}^n) \) can differ significantly
  from the behavior in \( H^1(\mathbb{R}^n) \).

</details>


### [24] [Stability of constant steady states of an attraction-repulsion chemotaxis system](https://arxiv.org/abs/2511.17917)
*Hiroshi Wakui,Tetsuya Yamada*

Main category: math.AP

TL;DR: The paper analyzes stability of constant steady states in attraction-repulsion chemotaxis systems, identifying conditions where stable constant steady states exist.


<details>
  <summary>Details</summary>
Motivation: To understand when attraction-repulsion chemotaxis systems have stable constant steady states, given that attraction systems have stable states in certain regions and repulsion systems always have stable states.

Method: Mathematical analysis of the Cauchy problem for attraction-repulsion chemotaxis systems in n-dimensional space, examining stability conditions for constant steady states.

Result: The paper provides a suitable condition under which the attraction-repulsion chemotaxis system has stable constant steady states.

Conclusion: There exists a specific condition that ensures the existence of stable constant steady states in attraction-repulsion chemotaxis systems.

Abstract: The Cauchy problem for the attraction-repulsion chemotaxis system in the whole $n$-dimensional space has uncountable constant steady states. In the attraction chemotaxis system, each positive constant steady state is stable if it is in a certain region. On the other hand, in the repulsion chemotaxis system, every positive constant steady state is stable. Our main purpose of this paper is to give a suitable condition under which the attraction-repulsion chemotaxis system has also stable constant steady states.

</details>


### [25] [The nonlinear porous medium equation for the f-Laplacian: Hamilton-Souplet-Zhang type gradient estimates and implications](https://arxiv.org/abs/2511.17997)
*Ali Taheri,Vahideh Vahidifar*

Main category: math.AP

TL;DR: New gradient estimates for positive solutions to nonlinear porous medium equation with f-Laplacian in smooth metric measure spaces, extending Hamilton-Souplet-Zhang type estimates.


<details>
  <summary>Details</summary>
Motivation: To establish improved gradient estimates for nonlinear porous medium equations in evolving metric spaces with time-dependent metrics and potentials.

Method: Using various analytical methods and techniques with natural lower bounds on time derivative of metric and Bakry-√âmery m-Ricci curvature tensors.

Result: Derived new gradient estimates and obtained parabolic Liouville-type results with characterization of ancient solutions.

Conclusion: Results extend and improve existing literature on gradient estimates for nonlinear porous medium equations in evolving metric spaces.

Abstract: This article presents new gradient estimates for positive solutions to the nonlinear porous medium equation (NPME) in the context of smooth metric measure spaces. The diffusion operator here is the f-Laplacian and the gradient estimates of interest are mainly of Hamilton-Souplet-Zhang types. These estimates are established using a variety of methods and techniques and several implications, most notably, to parabolic Liouville-type results and characterisation of ancient solutions are given. The problem is posed in the general framework where the metric and potential evolve with time and the proofs make use of natural lower bounds on the time derivative of the metric and the Bakry-√âmery m-Ricci curvature tensors. Our results extend and improve various existing ones in the literature.

</details>


### [26] [Quantitative unique continuation property for fourth-order Baouendi-Grushin type subelliptic operators with a potential](https://arxiv.org/abs/2511.18070)
*Yusheng Qiu,Jinggang Tan,Aliang Xia*

Main category: math.AP

TL;DR: This paper establishes quantitative unique continuation properties for solutions to fourth-order subelliptic equations of Baouendi-Grushin type, using an adapted Almgren's frequency function approach.


<details>
  <summary>Details</summary>
Motivation: To understand the quantitative unique continuation property for solutions to higher-order subelliptic equations, particularly for Baouendi-Grushin type operators, which are important in the study of degenerate elliptic equations and their applications.

Method: The authors adapt Almgren's approach to establish an almost monotonicity formula for the frequency function, considering the subelliptic operator Œî¬≤_X u = V u with bounded potential V satisfying specific gradient conditions.

Result: An almost monotonicity formula for the frequency function is established, leading to a quantitative unique continuation result for solutions to the fourth-order subelliptic equation.

Conclusion: The adapted Almgren's frequency function method successfully provides quantitative unique continuation properties for solutions to fourth-order subelliptic equations of Baouendi-Grushin type.

Abstract: We investigate the quantitative unique continuation property for solutions to
  $$Œî^2_{X} u = V u,$$
  where $Œî_{X} = Œî_{x} + |x|^{2Œ≤} Œî_{y}$ ($0 < Œ≤\leq 1$), with $x \in \mathbb{R}^{m}$ and $y \in \mathbb{R}^{n}$, denotes a class of subelliptic operators of Baouendi-Grushin type. The potential $V$ is assumed to be bounded and satisfy $|Z V| \leq K œà$ for some constant $K>0$,
  where $Z= \sum_{i=1}^m x_i \partial_{x_i} + (Œ≤+1)\sum_{j=1}^n y_j \partial_{y_j}$, $œà$ is the angle function given by $œà= \frac{|x|^{2Œ≤}}{œÅ^{2Œ≤}}$,
  and $$œÅ(x,y) = \left(|x|^{2(Œ≤+1)} + (Œ≤+1)^2 |y|^2\right)^{\frac{1}{2(Œ≤+1)}}$$ defines the associated pseudo-gauge. By adapting Almgren's approach, we establish an almost monotonicity formula for the frequency function. As a consequence, we derive a quantitative unique continuation result for solutions to the fourth-order subelliptic equation.

</details>


### [27] [Well-posedness to nonlinear Schr√∂dinger-Gerdjikov-Ivanon equation](https://arxiv.org/abs/2511.18228)
*Sucai Niu,Junyi Zhu*

Main category: math.AP

TL;DR: Extension of Riemann-Hilbert approach to analyze well-posedness of nonlinear Schr√∂dinger-Gerdjikov-Ivanov equation, establishing Lipschitz continuity between potential and scattering data.


<details>
  <summary>Details</summary>
Motivation: To study the well-posedness of the nonlinear Schr√∂dinger-Gerdjikov-Ivanov equation using Riemann-Hilbert methods and establish continuity properties between potential and scattering data.

Method: Extended Riemann-Hilbert approach with direct scattering transform, constructed two Riemann-Hilbert problems with reflection coefficients r(k) and r¬±(z), and used potential reconstruction to estimate Lipschitz continuity.

Result: Obtained Lipschitz continuity of potential in H¬≤(‚Ñù)‚à©H¬π,¬π(‚Ñù) to scattering data, and from reflection coefficients r¬±(z) in H¬π(‚Ñù)‚à©L¬≤,¬π(‚Ñù) to potential. Established existence of global solutions without eigenvalues or resonances.

Conclusion: The Riemann-Hilbert method successfully demonstrates well-posedness and global existence for the NLS-GI equation, with established continuity relationships between potential and scattering data.

Abstract: The Riemann-Hilbert approach is extended to discuss the well-posedness of the nonlinear Schr√∂dinger-Gerdjikov-Ivanon equation. The Lipschitz continuity of potential in $H^{2}(\mathbb{R})\cap H^{1,1}(\mathbb{R})$ to scattering data is obtained through direct scattering transform. Two Riemann-Hilbert problems are constructed, and two sets of the reflection coefficients, that is $r(k)$ and $r_\pm(z)$, are introduced. The Lipschitz continuity from the reflection coefficients $r_\pm(z)$ in $H^{1}(\mathbb{R})\cap L^{2,1}(\mathbb{R})$ to the potential is estimated via the potential reconstruction. Existence of global solutions of NLS-GI equation is considered by the Riemann-Hilbert problem without eigenvalues or resonances.

</details>


### [28] [Expanding Solutions to Free Boundary 3D Spherically Symmetric Compressible Navier-Stokes-Poisson Equations near the Lane-Emden Stars](https://arxiv.org/abs/2511.18339)
*Han Cao*

Main category: math.AP

TL;DR: Existence of global weak solutions for viscous polytropic gaseous stars modeled by gravitational Navier-Stokes-Poisson equations with constant viscosity and radially symmetric initial data, with specific mass constraints for different Œ≥ values.


<details>
  <summary>Details</summary>
Motivation: To establish the existence of global weak solutions for viscous polytropic gaseous stars and analyze the instability of Lane-Emden solutions in the Navier-Stokes-Poisson framework.

Method: Prove existence of global weak solutions using constant viscosity and radially symmetric initial data, with different constraints for Œ≥=4/3 and Œ≥‚àà(6/5,4/3). Analyze invariant sets and support expansion of strong solutions.

Result: Global weak solutions exist for specified Œ≥ ranges with appropriate initial data constraints. For Œ≥‚àà(6/5,4/3), invariant set contains previously disallowed initial data. Strong solutions expand to infinity, indicating Lane-Emden instability.

Conclusion: The study establishes existence results for viscous polytropic stars and demonstrates the strong instability of Lane-Emden solutions in the Navier-Stokes-Poisson system.

Abstract: We consider the gravitational Navier-Stokes-Poisson equations with the equation of state $P(œÅ)=KœÅ^Œ≥$, where $Œ≥\in(\frac{6}{5},\frac{4}{3}]$, which models the viscous polytropic gaseous stars. We prove the existence of global weak solutions to the equations with constant viscosity and radially symmetric initial data. For $Œ≥=\frac{4}{3}$, we require the initial data having mass less than the mass of the Lane-Emden stars; for $Œ≥\in(\frac{6}{5},\frac{4}{3})$, we require that the initial data belong to an invariant set where initial initial data can be taken near the Lane-Emden stars. For $Œ≥\in(\frac{6}{5},\frac{4}{3})$, we show that the invariant set contains some initial data that are not allowed in previous literature. We also prove the support of any strong solution expands to infinity for the Navier-Stokes-Poisson equations with constant viscosity and a class of density-dependent viscosity, which indicates the strong instability of Lane-Emden solutions for the Navier-Stokes-Poisson equations.

</details>


### [29] [On the Reverse Inequality of Riesz transform on metric cone with potential](https://arxiv.org/abs/2511.18365)
*Dangyang He*

Main category: math.AP

TL;DR: The paper proves Lorentz-type endpoint estimates for the Riesz transform on metric cones and establishes sharp reverse inequalities for Schrodinger operators, with precise conditions on the boundedness range.


<details>
  <summary>Details</summary>
Motivation: To complement previous results on Schrodinger operators on metric cones by proving endpoint estimates for Riesz transforms and establishing sharp reverse inequalities with exact boundedness conditions.

Method: Analyzes the Schrodinger operator H=Œî+V‚ÇÄ/r¬≤ on d-dimensional metric cones, using harmonic analysis techniques to prove Lorentz-type endpoint estimates and derive sharp reverse inequalities.

Result: Proves restricted weak type estimates at endpoints for the Riesz transform ‚àáH^{-1/2} and establishes the sharp inequality ||H^{1/2}f||_{L^p} ‚â§ C(||‚àáf||_{L^p} + ||f/r||_{L^p}) with precise p-range conditions.

Conclusion: The results provide complete endpoint behavior for Riesz transforms on metric cones and sharp reverse inequalities that characterize the boundedness range of Schrodinger operators in this geometric setting.

Abstract: Let $M=(0,\infty)_r\times Y$ be a $d$-dimensional ($d\ge 3$) metric cone with metric<br/>$g=dr^2+r^2h$, where $(Y,h)$ is a closed Riemannian manifold. Let<br/>$H=Œî+V_0/r^2$ be the associated Schrodinger operator, with<br/>$V_0\in C^\infty(Y)$ satisfying the positivity condition<br/>$Œî_Y+V_0+(d-2)^2/4>0$. First, we complement previous results by proving<br/>Lorentz-type endpoint estimates for the Riesz transform $\nabla H^{-1/2}$:<br/>it is of restricted weak type at both endpoints of its $L^p$-boundedness range.<br/>Second, we establish the sharp reverse inequality<br/>$\|H^{1/2}f\|_{L^p}\le C\big(\|\nabla f\|_{L^p}+\|f/r\|_{L^p}\big)$<br/>which holds if and only if<br/>\[<br/>\frac{d}{\min\big((d+4)/2+Œº_0,\,d\big)}<br/> < p <<br/>\frac{d}{\max\big((d-2)/2-Œº_0,\,0\big)}.\]

</details>


### [30] [Emergent behaviors of relativistic thermodynamic flocks with Synge energy](https://arxiv.org/abs/2511.18492)
*Ziming Bian,Seung-Yeal Ha,Tommaso Ruggeri,Qinghua Xiao*

Main category: math.AP

TL;DR: The paper introduces polyatomic classical and relativistic models based on Cucker-Smale models to study collective motion in multi-species systems, establishing asymptotic flocking under various conditions using entropy principles.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between collective motion in particle systems (like flocking and swarming) and nonequilibrium gas dynamics, particularly for multi-species systems where inertia and internal energy depend on temperature, as seen in astrophysical plasmas or relativistic fluids.

Method: Developed polyatomic classical and relativistic models using Synge energy, applied entropy principles to derive uniform temperature bounds, and analyzed large-time behavior under various communication kernels.

Result: Established uniform lower bounds for temperature and proved asymptotic flocking occurs for nearly constant interactions from arbitrary initial data. The relativistic formulation provides physically consistent treatment of thermodynamic effects.

Conclusion: The study clarifies how thermodynamic effects and relativistic corrections influence coherent motion emergence in particle systems, connecting kinetic theory, relativistic fluid mixtures, and collective dynamics.

Abstract: Collective motion and self-organization of interacting particles, such as flocking and swarming, can be viewed as nonequilibrium analogues of collective dynamics in gases. Motivated by the analogy between gas mixtures and Cucker--Smale models, we introduce a polyatomic classical model and its relativistic counterpart based on the Synge energy, and analyze their large-time behavior. The relativistic formulation provides a physically consistent setting for multi-species systems where inertia and internal energy depend on temperature, as occurs in astrophysical plasmas or relativistic fluids. Using the entropy principle, we derive uniform lower bounds for temperature and establish asymptotic flocking under various communication kernels. For nearly constant interactions, flocking emerges from arbitrary initial data. The results clarify how thermodynamic effects and relativistic corrections modify the emergence of coherent motion in particle systems, bridging kinetic theory, relativistic fluid mixtures, and collective dynamics.

</details>


### [31] [Classical limit of the relativistic Vlasov-Maxwell-Landau system](https://arxiv.org/abs/2511.18500)
*Chuqi Cao,Ling-Bing He,Yuanjie Lei,Qinghua Xiao*

Main category: math.AP

TL;DR: Rigorous mathematical justification of the non-relativistic limit from relativistic Vlasov-Maxwell-Landau to Vlasov-Poisson-Landau systems as light speed c‚Üí‚àû.


<details>
  <summary>Details</summary>
Motivation: To understand the physical transition from finite-speed electromagnetic waves to instantaneous Coulomb interactions and from relativistic to Newtonian particle dynamics.

Method: Three key technical advances: uniform-in-c coercivity estimate for relativistic Landau collision operator, novel weighted energy functional to handle weakening electromagnetic dissipation, and global well-posedness proof.

Result: Successfully justified the non-relativistic limit in a periodic box as c‚Üí‚àû.

Conclusion: The paper provides rigorous mathematical foundation for the physical transition between relativistic and non-relativistic plasma dynamics.

Abstract: The physical essence of the non-relativistic limit, from the relativistic Vlasov-Maxwell-Landau system to the Vlasov-Poisson-Landau system, lies in the transition from finite-speed electromagnetic waves to instantaneous Coulomb interactions, and from relativistic to Newtonian particle dynamics. We rigorously justify this limit (mathematically corresponding to the light speed $c \to \infty$) in a periodic box via three key technical advances: establishing a uniform-in-$c$ coercivity estimate for the relativistic Landau collision operator, constructing a novel weighted energy functional to overcome the weakening dissipation of the electromagnetic field at large $c$, and proving a corresponding global well-posedness result.

</details>


### [32] [A note on multiple solutions for Kirchhoff-type equations with a Neumann condition](https://arxiv.org/abs/2511.18523)
*Biagio Ricceri*

Main category: math.AP

TL;DR: The paper establishes a multiplicity theorem for a nonlinear elliptic boundary value problem using a strict minimax inequality.


<details>
  <summary>Details</summary>
Motivation: To prove the existence of multiple solutions for a specific type of nonlinear elliptic partial differential equation with Neumann boundary conditions.

Method: Uses a recent result on strict minimax inequality from reference [5] as the main tool to establish the multiplicity theorem.

Result: Successfully proves a multiplicity theorem for the given nonlinear elliptic problem.

Conclusion: The strict minimax inequality approach provides an effective method for establishing multiple solutions to this class of nonlinear elliptic boundary value problems.

Abstract: Using as a main tool our recent result on the strict minimax inequality proved in [5], in this note we establish a multiplicity theorem for a problem of the type $$\cases{-K\left(\int_Œ©|\nabla u(x)|^2dx\right)Œîu = h(x,u) & in $Œ©$\cr & \cr {{\partial u}\over {\partialŒΩ}}=0 & on $\partialŒ©$.\cr}$$

</details>


### [33] [Sharp uniform-in-diffusivity mixing rates for passive scalars in parallel shear flows](https://arxiv.org/abs/2511.18536)
*Dallas Albritton,Rajendra Beekie*

Main category: math.AP

TL;DR: Optimal uniform-in-diffusivity mixing rate for passive scalar in shear flow: ‚Äñf‚Äñ_{H^{-1}} ‚â≤ ‚ü®t‚ü©^{-1/(N+1)}, where N is maximal order of vanishing of shear profile derivative.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous bounds on mixing rates for passive scalars in background shear flows, particularly understanding how flow degeneracy affects mixing efficiency.

Method: Proof based on resolvent description of solutions with pointwise estimates on resolvent kernel; asymptotic analysis of shear layers around critical points in non-degenerate case.

Result: Proved optimal mixing rate ‚Äñf‚Äñ_{H^{-1}} ‚â≤ ‚ü®t‚ü©^{-1/(N+1)}; verified formal asymptotics from previous work for shear layer behavior in non-degenerate flows.

Conclusion: Established rigorous mixing rate bounds that depend on flow degeneracy through parameter N, providing mathematical foundation for mixing behavior in shear flows.

Abstract: We consider the advection-diffusion equation describing the evolution of a passive scalar in a background shear flow. We prove the optimal uniform-in-diffusivity mixing rate $\| f \|_{H^{-1}} \lesssim \langle t \rangle^{-1/(N+1)}$, $t \geq 0$, where $N$ is the maximal order of vanishing of the derivative $b'(y)$ of the shear profile, e.g., $N=1$ for plane Pouseille flow. Our proof is based on the description of the solution in terms of resolvents and involves pointwise estimates on the resolvent kernel. In the non-degenerate case, we further give a rigorous asymptotic description of generic solutions in terms of shear layers localized around the critical points. This verifies formal asymptotics in [McLaughlin-Camassa-Viotti, \textit{Physics of Fluids}, 22(11), 2010].

</details>


### [34] [Desingularization of nondegenerate rotating vortex patches](https://arxiv.org/abs/2511.18592)
*RƒÉzvan-Octavian Radu,Noah Stevenson*

Main category: math.AP

TL;DR: This paper constructs smooth rotating Euler solutions near nondegenerate vortex patches, proving they can be desingularized as limits of infinitely smooth solutions with compact vorticity support.


<details>
  <summary>Details</summary>
Motivation: To address the desingularization question for steady rotating solutions to 2D incompressible Euler equations near vortex patch solutions, and to construct exotic families of singular rotating vortex patch-like solutions.

Method: Uses a local stream function formulation in a tubular neighborhood of patch boundary, combining analysis on thin domains, nonlinear a priori estimates, and a custom Newton's method.

Result: Proves vortex patch states are limits of rotating Euler solutions that are smooth to infinite order with compact vorticity support, respecting dihedral symmetry. Nondegeneracy condition verified for Kirchhoff ellipses and Rankine vortex bifurcation curves.

Conclusion: This work provides the first general desingularization procedure applicable to steady rotating vortex patches, with robust techniques enabling construction of exotic singular solutions near nondegenerate states.

Abstract: This paper analyses the space of steady rotating solutions to the two-dimensional incompressible Euler equations nearby vortex patch solutions satisfying a natural nondegeneracy condition. We address the question of desingularization and prove that such vortex patch states are the limit of rotating Euler solutions that are smooth to infinite order, have compact vorticity support, and respect dihedral symmetry. Our nondegeneracy condition is proved to be satisfied by Kirchhoff ellipses and along the local bifurcation curves emanating from the Rankine vortex. The construction, that is based on a local stream function formulation in a tubular neighborhood of the patch boundary, is a synthesis of delicate analysis on thin domains, nonlinear a priori estimates, and a custom version of Newton's method. Our techniques are robust enough to additionally allow us to construct exotic families of singular rotating vortex patch-like solutions nearby a given nondegenerate state. To the best of the authors' knowledge, this work constitutes the first desingularization procedure applicable to general families of steady rotating vortex patches.

</details>


### [35] [Quantitative Stability of Two Weakly Interacting Kinks in the Stationary phi^6 Model](https://arxiv.org/abs/2511.18754)
*Xin Liao*

Main category: math.AP

TL;DR: Sharp quantitative stability estimates for configurations near two weakly interacting kinks in the stationary phi^6 model.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous stability bounds for solutions close to interacting kink configurations in nonlinear field theories.

Method: Analyze the phi^6 model equation and prove existence of constants such that for functions close to two kinks in H1 norm, there exist translations providing stability estimates involving the residual norm.

Result: Proved that for functions sufficiently close to two kinks with sufficient separation, there exist translations making the distance bounded by a constant times the residual of the phi^6 equation.

Conclusion: The phi^6 model exhibits quantitative stability for configurations near weakly interacting kinks, with explicit bounds relating approximation error to the equation residual.

Abstract: We study the stationary phi^6 model given by the equation -phi''(x) + 2 phi(x) - 8 phi(x)^3 + 6 phi(x)^5 = 0 for x in R, and establish sharp quantitative stability estimates for configurations close to two weakly interacting kinks. More precisely, there exist constants a > 0 and epsilon > 0 such that, for any function u in L-infinity satisfying || u - H_{0,1}(x + x1) - H_{-1,0}(x + x2) ||_{H1} < epsilon with x2 - x1 > a, there exist constants y1, y2 such that || u - H_{0,1}(x + y1) - H_{-1,0}(x + y2) ||_{H1} + exp(-sqrt(2) (y2 - y1)) <= C * || u'' - 2 u + 8 u^3 - 6 u^5 ||_{L2}.

</details>


### [36] [A direct proof of the equivalence between Dirichlet's principle and Perron's method](https://arxiv.org/abs/2511.18762)
*Tsogtgerel Gantumur*

Main category: math.AP

TL;DR: Short proof showing Dirichlet energy minimizer equals Perron solution for harmonic functions with continuous boundary data.


<details>
  <summary>Details</summary>
Motivation: To provide a simpler, more direct proof that connects Dirichlet energy minimization with Perron's method for solving the Laplace equation, avoiding complex functional analysis tools.

Method: Uses strong convergence via strict convexity of Dirichlet energy, Friedrichs' inequality, Weyl's lemma, and Wiener's exhaustion by regular subdomains within H¬π(Œ©) framework.

Result: Establishes equivalence between the minimizer of Dirichlet energy and Perron solution h_g for harmonic functions with continuous boundary data.

Conclusion: The proof demonstrates that classical Dirichlet energy minimization and Perron's method yield identical solutions for harmonic boundary value problems, using only elementary functional analysis tools.

Abstract: We give a short proof that for a bounded domain $Œ©\subset\mathbb{R}^n$ and continuous boundary data $g\in C(\partialŒ©)$ admitting a continuous finite-energy extension $œÜ\in H^{1}(Œ©)\cap C(\barŒ©)$, the minimizer of the Dirichlet energy \[
  E(v) = \int_Œ© |\nabla v|^{2}\,dx,
  \qquad v-œÜ\in H^{1}_{0}(Œ©), \] coincides with the Perron solution $h_g$ of the Dirichlet problem $Œîu = 0$ in $Œ©$ with boundary data $g$. The argument stays entirely in $H^{1}(Œ©)$ and uses only strong convergence via strict convexity of the Dirichlet energy, Friedrichs' inequality, Weyl's lemma, and Wiener's exhaustion by regular subdomains. No weak convergence, Poisson problems with distributional right hand sides, or general elliptic theory are needed.

</details>


### [37] [Elastic scattering by locally rough interfaces](https://arxiv.org/abs/2511.18799)
*Chengyu Wu,Yushan Xue,Jiaqing Yang*

Main category: math.AP

TL;DR: First well-posedness result for elastic scattering by locally rough interfaces in 2D and 3D, establishing existence and uniqueness of solutions through fundamental stress vector identity, Green's tensor analysis, and variational methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of well-posedness results for elastic scattering problems involving rough interfaces, which are important in geophysics and material science applications.

Method: Used Helmholtz decomposition to discover fundamental stress vector identity, applied steepest descent method for Green's tensor analysis, and employed variational methods with boundary integral equations.

Result: Established first uniqueness result for all frequencies, derived explicit expression for two-layered Green's tensor, and proved existence of solutions for elastic scattering by rough interfaces.

Conclusion: Successfully achieved the first well-posedness result for elastic scattering by locally rough interfaces through a comprehensive mathematical framework combining fundamental identities, Green's tensor analysis, and variational methods.

Abstract: In this paper, we present the first well-posedness result for elastic scattering by locally rough interfaces in both two and three dimensions. Inspired by the Helmholtz decomposition, we first discover a fundamental identity for the stress vector, revealing an intrinsic relationship among the generalized stress vector, the Lame constants and certain tangential differential operators. This identity leads to two key limits for surface integrals involving scattered solutions, from which we deduce the first uniqueness result of direct problem for all frequencies. Through a detailed analysis, applying the steepest descent method, subsequently we derive the existence and uniqueness of the corresponding two-layered Green's tensor along with its explicit expression when the transmission coefficient equals 1. Finally, by leveraging properties of the Green's tensor, we establish the existence of solutions via the variational method and the boundary integral equation, thereby achieving the first well-posedness result for elastic scattering by rough interfaces.

</details>


### [38] [Recovering discontinuous viscosity coefficients for inverse Stokes problems by boundary measurements](https://arxiv.org/abs/2511.18812)
*Yu Jia,Chengyu Wu,Hao Wu,Jiaqing Yang*

Main category: math.AP

TL;DR: Global uniqueness theorem for determining discontinuous viscosity coefficient in 3D Stokes flow from boundary measurements


<details>
  <summary>Details</summary>
Motivation: To solve the inverse Stokes problem of determining a discontinuous viscosity coefficient in bounded 3D domains, which has applications in fluid dynamics and material characterization

Method: Analyzed singularity of Dirichlet Green's functions in H1-norm and constructed a specifically coupled Stokes-Brinkman system in localized domains

Result: Proved that the viscosity coefficient Œº can be uniquely determined from boundary measurements

Conclusion: The discontinuous viscosity coefficient in Stokes flow is uniquely identifiable through boundary measurements using the developed analytical framework

Abstract: In this paper, we investigate the inverse Stokes problem of determining a discontinuous viscosity coefficient $Œº$ in a bounded domain $Œ©\subset\mathbb{R}^3$. By analyzing the singularity of the Dirichlet Green's functions in $H^1$-norm and constructing a specifically coupled Stokes-Brinkman system in a localized domain, we prove a global uniqueness theorem that the viscosity coefficient $Œº$ can be uniquely determined from boundary measurements.

</details>


### [39] [Local-in-time existence of strong solutions to a class of compressible Power-Law flows](https://arxiv.org/abs/2511.18819)
*Fang Li,Chang Mengge*

Main category: math.AP

TL;DR: Study of compressible non-Newtonian fluids with power-law flow in periodic 3D domain, proving local strong solution existence for exponent range 7/5 < p(t,x) ‚â§ 2, and providing improved blow-up criterion.


<details>
  <summary>Details</summary>
Motivation: To establish existence theory for compressible non-Newtonian fluids with variable power-law exponents and develop criteria for solution breakdown.

Method: Mathematical analysis of compressible non-Newtonian fluid model with p(t,x)-structure potential, using techniques from partial differential equations and fluid dynamics.

Result: Proved local-in-time existence of strong solutions for exponent range 7/5 < p(t,x) ‚â§ 2, and established blow-up criterion based on L‚àû(0,T;L¬≥(Œ©))-norm of velocity gradient.

Conclusion: The paper provides rigorous mathematical foundation for compressible non-Newtonian fluids with variable power-law structure and identifies precise conditions for solution breakdown.

Abstract: We consider a model of the compressible non-Newtonian fluids for power-law flow fulfilling a periodic domain in ${\mathbb R}^3,$ in which the extra stress tensor is induced by a potential with $p(t,x)$-structure. The local-in-time existence of strong solution is proved for all $\frac{7}{5} < \inf p(t,x) \leqslant \sup p(t,x) \leqslant 2.$ Further, an improved blow-up criterion for strong solutions is given in terms of the $L^\infty(0,T;L^3(Œ©))$-norm of the gradient of the velocity.

</details>


### [40] [H{√∂}lder regularity of parabolic equations with Dirichlet boundary conditions and application to reaction-diffusion and reaction-cross-diffusion systems](https://arxiv.org/abs/2511.18872)
*Hector Bouton,Laurent Desvillettes,Helge Dietert*

Main category: math.AP

TL;DR: Extension of previous work to Dirichlet boundary conditions, focusing on parabolic equations with rough coefficients and monotonic time derivatives, applied to SKT cross-diffusion and reaction-diffusion systems.


<details>
  <summary>Details</summary>
Motivation: To extend the analysis from previous work [BDD25] to handle Dirichlet boundary conditions, which is important for studying cross-diffusion systems and reaction-diffusion models in bounded domains.

Method: Study of parabolic equation a‚àÇ_t w - Œîw = f with rough coefficient a, homogeneous Dirichlet boundary conditions, and assumption ‚àÇ_t w ‚â• 0; application to triangular SKT cross-diffusion system with Lotka-Volterra reactions in 3D.

Result: Proved existence of global strong solutions to triangular SKT cross-diffusion system with Lotka-Volterra reactions in three dimensions with Dirichlet boundary conditions; obtained estimates for solutions to reaction-diffusion systems modeling reversible chemistry.

Conclusion: The adapted framework successfully handles Dirichlet boundary conditions and provides existence results and estimates for cross-diffusion and reaction-diffusion systems in bounded domains.

Abstract: In this work, we adapt our recent article [BDD25] to the setting of Dirichlet boundary conditions. A key part is the study of the parabolic equation $a\partial_t w - Œîw = f$ with a rough coefficient $a$, homogeneous Dirichlet boundary conditions, and the special assumption $\partial_tw \ge 0$. We then apply it to prove existence of global strong solutions to the triangular Shigesada-Kawasaki-Teramoto (SKT) cross-diffusion system with Lotka-Volterra reaction terms in three dimensions and Dirichlet boundary conditions, and to obtain estimates for solutions to reaction-diffusion systems modeling reversible chemistry (still when Dirichlet boundary conditions are considered).

</details>


### [41] [The local Turnpike Property in Mean Field Control and Games with quadratic Hamiltonian](https://arxiv.org/abs/2511.18923)
*Marco Cirant,Nicol√≤ De Bernardi*

Main category: math.AP

TL;DR: The paper analyzes local stability of solutions to ergodic and discounted mean field games with quadratic Hamiltonians, replacing monotonicity with a weaker local stability condition to derive exponential turnpike properties and existence of stable solutions.


<details>
  <summary>Details</summary>
Motivation: To study stability properties of mean field games around stationary equilibria under weaker assumptions than the usual monotonicity condition, allowing for non-unique equilibria.

Method: Uses a local stability assumption from second-order strict positivity, symmetry properties of the system, and fixed-point arguments to establish existence of stable solutions on finite and infinite horizons.

Result: Derives exponential turnpike property for solutions near stationary equilibria and proves existence of stable solutions when initial/terminal data are sufficiently close to the stationary equilibrium.

Conclusion: The weaker local stability condition combined with system symmetry enables rigorous stability analysis and existence proofs for mean field games solutions around stationary equilibria, even without global monotonicity.

Abstract: We study the local stability properties of solutions to ergodic and discounted mean field games systems in the long time horizon, around stationary equilibria, when the Hamiltonian is quadratic. We replace the usual monotonicity of the coupling term with a weaker, local assumption on the stationary equilibrium (that need not be unique), stemming from a second-order strict positivity condition. This new stability assumption, together with a symmetry property of the system, allows us to derive an exponential turnpike property for those solutions that are close to the stationary one. Finally, through a fixed-point argument, we establish the actual existence of stable solutions, both on the finite horizon $[0,T]$ and on the infinite horizon, provided that the initial (and terminal) data are close enough to the stationary equilibrium.

</details>


### [42] [Self-Similar Radially Symmetric Solutions of the Relativistic Euler Equations with Synge Energy](https://arxiv.org/abs/2511.18971)
*Tommaso Ruggeri,Ferdinand Thein,Qinghua Xiao*

Main category: math.AP

TL;DR: Existence and uniqueness of self-similar relativistic Euler solutions with Synge energies for monatomic/diatomic gases, valid across all relativistic regimes from classical to ultra-relativistic limits.


<details>
  <summary>Details</summary>
Motivation: Extend classical self-similar flow theory to relativistic framework with kinetic-theory-based constitutive relations, covering both monatomic and diatomic gases.

Method: Analyze self-similar, radially symmetric solutions of relativistic Euler equations using Synge energies, prove existence/uniqueness for initial-boundary value problems including spherical piston problem.

Result: Proved existence and uniqueness of solutions for all relativistic parameter values, established structural properties of Synge energies including negative second derivative and monotonic characteristic velocity dependence.

Conclusion: Successfully extended classical self-similar flow theory to relativistic regime with kinetic-theory-based constitutive equations, providing comprehensive framework across all relativistic limits.

Abstract: We consider self-similar, radially symmetric solutions of the relativistic Euler equations with constitutive relations from relativistic kinetic theory, based on Synge energies for monatomic and its extension to diatomic gases. For the corresponding initial--boundary value problem, including the spherical piston problem, we prove existence and uniqueness of solutions valid for all values of the relativistic parameter $Œ≥= mc^{2}/(k_{B}T)$, thus covering both the classical limit $(Œ≥\to \infty)$ and the ultra-relativistic regime $(Œ≥\to 0)$. We further establish key structural properties of Synge energies, showing the strict negativity of the second derivative with respect to pressure at constant entropy and the monotone dependence of the characteristic velocity on $Œ≥$. These results extend the classical theory of self-similar flows to the relativistic framework with kinetic-theory-based constitutive equations.

</details>


### [43] [Long-time behavior of resonant time-dependent perturbations of periodic transport equations on $\mathbb{R}$](https://arxiv.org/abs/2511.19034)
*Maria Teresa Rotolo*

Main category: math.AP

TL;DR: Analysis of linear, time-dependent skew-adjoint perturbations of periodic transport equations on 1D torus, showing either exponential Sobolev norm explosion or long-term stability depending on perturbation degeneracy.


<details>
  <summary>Details</summary>
Motivation: To understand long-time behavior of solutions to perturbed periodic transport equations in resonant regimes, particularly energy transfer phenomena and stability properties.

Method: Combines pseudodifferential tools with dynamical systems: resonant normal form procedure to reduce analysis to classical dynamics, positive commutator estimates, and microlocal analysis for constructing escape functions.

Result: For non-degenerate perturbations in resonant regime, either solutions exist with exponentially exploding Sobolev norms (energy transfer) or all solutions remain stable for arbitrarily long time scales.

Conclusion: The paper establishes a dichotomy in long-time behavior of perturbed periodic transport equations, with explicit construction of escape functions playing crucial role in proving instability results.

Abstract: We consider linear, time-dependent and skew-adjoint perturbations of periodic transport equations on the one-dimensional torus. We describe the long-time behavior of solutions for all non-degenerate perturbations in resonant regime, proving that either there exist solutions whose Sobolev norms explode exponentially fast, provoking energy transfer phenomena, or all solutions remain stable for arbitrarily long time scales. The proof combines pseudodifferential tools with dynamical systems results: we perform a resonant normal form procedure to reduce our analysis to the classical dynamics for the resonant equation. The main difficulty lies in the proof of the instability result, for which we explicitly construct an escape function associated to the dynamics. This is obtained by means of a positive commutator estimate on the operator associated with the escape function, exploiting microlocal analysis.

</details>


### [44] [Optimal regularity results for the Stokes--Dirichlet problem](https://arxiv.org/abs/2511.19091)
*Dominic Breit,Anatole Gaudin*

Main category: math.AP

TL;DR: Develops maximal regularity theory for Stokes equations with no-slip boundary conditions on bounded domains of low regularity, covering full scales of Besov and Sobolev spaces including endpoint cases like L^‚àû.


<details>
  <summary>Details</summary>
Motivation: To establish a comprehensive framework for Stokes equations that works across all Besov and Sobolev spaces, including challenging endpoint cases, and to extend classical L^p-theory to provide a complete picture for both Bessel potential and Besov spaces.

Method: Uses resolvent estimates in half-space and bounded domains, functional calculus analysis, and Sobolev multiplier theory for rough domains with minimal boundary regularity assumptions.

Result: Obtains resolvent bounds, proves boundedness of H^‚àû-functional calculus for Stokes-Dirichlet operator, characterizes domains of fractional powers, and provides explicit description of Stokes-Dirichlet operator on L^‚àû(R^n_+).

Conclusion: Develops sharp maximal regularity theory for Stokes equations that works for low-regularity bounded domains and provides complete toolkit for studying incompressible fluid flows across all relevant function spaces.

Abstract: We develop a sharp maximal regularity theory for the resolvent and evolution Stokes equations with no-slip boundary conditions, focusing on bounded domains of low regularity. Our framework covers the full scales of Besov and Sobolev spaces, $B^s_{p,q}$ and $H^{s,p}$, including endpoint cases such as $L^\infty$. Our approach also allows extending the classical $L^p$-theory for $1\leqslant p\leqslant\infty$, giving a complete picture that includes both Bessel potential spaces $H^{s,p}$ and Besov spaces $B^s_{p,q}$, $p,q\in[1,\infty]$.\\ Our first main result establishes resolvent estimates in the half-space encompassing endpoint function spaces, while the second addresses bounded domains of minimal boundary regularity. In both cases we derive resolvent bounds, prove boundedness of the $\mathbf{H}^\infty$-functional calculus for the Stokes--Dirichlet operator, and characterize precisely the domains of its fractional powers.\\ In the half space setting, we work with homogeneous Sobolev and Besov spaces following the notion due to Bahouri, Chemin and Danchin, further refined by the second author. The analysis of solenoidal function spaces provides here a complete toolkit for the study of incompressible fluid flows. As a consequence of our analysis, we obtain an explicit description for the Stokes--Dirichlet operator on $L^\infty(\mathbb R^n_+)$, which seems completely new.\\ For bounded domains, we obtain sharp results for a wide class of rough domains under minimal assumptions on boundary regularity. To this end, we rely on Sobolev multiplier theory. The assumptions coincide with those of Maz'ya--Shaposhnikova, already shown to be optimal in the case of the Laplace equation with Dirichlet boundary conditions.

</details>


### [45] [On the problem of stability of viscous shocks](https://arxiv.org/abs/2511.19167)
*Sergey Bolotin,Dmitry Treschev*

Main category: math.AP

TL;DR: The paper analyzes spectral stability of traveling wave solutions in viscous conservation laws, proposing a piece-wise linear model to simplify stability analysis and demonstrating examples of stability loss.


<details>
  <summary>Details</summary>
Motivation: To develop a method for analyzing spectral stability of traveling wave solutions in systems of viscous conservation laws, where general stability conditions are typically only obtainable numerically.

Method: Proposes a model class of piece-wise linear (discontinuous) vector fields that reduces the stability problem to a linear algebra problem, and constructs examples of stability loss that can be smoothed to create smooth examples.

Result: Shows that the stability problem remains meaningful even with low regularity and provides explicit examples demonstrating stability loss phenomena.

Conclusion: The piece-wise linear approach provides a tractable framework for analyzing spectral stability in viscous conservation laws, with the constructed examples serving as prototypes that can be smoothed to study the same phenomena in smooth systems.

Abstract: We consider the problem of spectral stability of traveling wave solutions $u=Œ≥(x-Wt)$ for a system of viscous conservation laws $\partial_t u + \partial_x F(u) = \partial^2_x u$. Such solutions correspond to heteroclinic trajectories $Œ≥$ of a system of ODE. In general conditions of stability can be obtained only numerically. We propose a model class of piece-wise linear (discontinuous) vector fields $F$ for which the stability problem is reduced to a linear algebra problem. We show that the stability problem makes sense in such low regularity and construct several examples of stability loss. Every such example can be smoothed to provide a smooth example of the same phenomenon.

</details>


### [46] [Normalized solutions for the nonlinear Schr√∂dinger equation with trapping potential](https://arxiv.org/abs/2511.19271)
*Junwei Yu*

Main category: math.AP

TL;DR: Study of positive normalized solutions with prescribed L¬≤-norm for Sobolev critical Schr√∂dinger equation with trapping potential, establishing existence of local minimum solutions (ground states) and mountain pass solutions.


<details>
  <summary>Details</summary>
Motivation: To investigate the existence and multiplicity of positive normalized solutions for Sobolev critical Schr√∂dinger equations with prescribed L¬≤-norm, which is important in quantum mechanics and nonlinear wave equations.

Method: Using variational methods to study the existence of local minimum solutions (ground states) for œÅ in (0, œÅ*) and mountain pass solutions under appropriate assumptions on the trapping potential V(x).

Result: Proved existence of local minimum solutions (ground states) for œÅ in (0, œÅ*) with suitable œÅ* > 0, and existence of mountain pass solutions under the same assumptions.

Conclusion: The paper establishes the existence of both local minimum (ground state) solutions and mountain pass solutions for the Sobolev critical Schr√∂dinger equation with prescribed L¬≤-norm and trapping potential.

Abstract: We study the existence and multiplicity of positive normalized solutions with prescribed $L^{2}$-norm for the Sobolev critical Schr√∂dinger equation $-ŒîU + V(x) U = ŒªU + |U|^{2^*-2} U$ in $\mathbb{R}^N$, $\int_{\mathbb{R}^N} U^2\,dx = œÅ^2$, where $N \ge 3$, $V\ge 0$ is a trapping potential, $Œª\in \mathbb{R}$ and $2^*=\frac{2N}{N-2}$. Our first result is that the existence of local minimum solutions for $œÅ\in (0, œÅ^*)$, for some suitable $œÅ^* > 0$, under appropriate assumptions on the potential. These solutions correspond to ground states. Our second result concerns the existence of mountain pass solutions, under the same assumptions.

</details>


### [47] [Flat flows of periodic Lipschitz subgraphs for generalized nonlocal perimeters](https://arxiv.org/abs/2511.19309)
*Lucia De Luca,Antonia Diana,Marcello Ponsiglione*

Main category: math.AP

TL;DR: Existence and 1/2-H√∂lder continuity of flat flows for periodic Lipschitz subgraphs governed by gradient flow of generalized nonlocal perimeters, with semigroup property and perimeter decrease, showing halfspaces as global minimizers and attractors.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for the evolution of geometric shapes under generalized nonlocal perimeters, extending classical perimeter theory to include fractional and Riesz-type cases.

Method: Proving existence and regularity properties of flat flows using gradient flow theory, establishing semigroup property, and analyzing minimization properties of generalized nonlocal perimeters.

Result: Flat flows exist, are 1/2-H√∂lder continuous in time, satisfy semigroup property, decrease generalized perimeter, and halfspaces are global minimizers and attractors for the dynamics.

Conclusion: The theory provides a comprehensive framework for nonlocal geometric evolutions, covering fractional perimeters, Riesz-type perimeters, and Minkowski pre-content, with halfspaces playing a fundamental role as attractors.

Abstract: We prove the existence and the 1/2-H√∂lder continuity in time of flat flows for periodic Lipschitz subgraphs, whose evolution is governed by the gradient flow of generalized nonlocal perimeters. Moreover, we show that the flat flow satisfies the semigroup property and, as a consequence, the generalized perimeter decreases along the evolution. Finally, we prove that halfspaces are global minimizers of the generalized nonlocal perimeters and act as attractors for the dynamics. Our theory covers several generalized perimeters, including fractional and Riesz-type perimeters (defined on entire periodic subgraphs through suitable renormalization procedures) and the Minkowski pre-content.

</details>


### [48] [Wigner and Gabor phase-space analysis of propagators for evolution equations](https://arxiv.org/abs/2511.19400)
*Elena Cordero,Gianluca Giacchi,Luigi Rodino*

Main category: math.AP

TL;DR: Analysis of Wigner kernels and Gabor matrices for linear evolution equations (complex heat, wave, Hermite), showing exponential decay and symplectic structure in phase-space representations.


<details>
  <summary>Details</summary>
Motivation: To provide a unified operator-theoretic and phase-space perspective on parabolic and hyperbolic evolution equations, linking symbol geometry with sparsity and localization properties of Gabor representations.

Method: Time-frequency analysis framework with explicit expressions for Wigner kernels of Fourier multipliers, quantitative decay estimates for Gabor matrices under Gelfand-Shilov symbol regularity conditions.

Result: Exponential off-diagonal decay/quasi-diagonality of matrix representations; closed-form formulas for complex heat equation showing dissipative/oscillatory behavior; Gaussian decay with temporal spreading for diffusion; pure rotation on time-frequency plane for Hermite equation.

Conclusion: The approach provides explicit formulas suitable for numerical computation and visualization of phase-space dynamics, with potential extension to more general pseudodifferential symbols.

Abstract: We study the Wigner kernel and the Gabor matrix associated with the propagators of a broad class
  of linear evolution equations, including the complex heat, wave,
  and Hermite equations. Within the framework of time-frequency analysis, we derive
  explicit expressions for the Wigner kernels of Fourier multipliers and establish quantitative
  decay estimates for the corresponding Gabor matrices. These results are obtained under symbol
  regularity conditions formulated in the Gelfand-Shilov scale and ensure exponential off-diagonal
  decay or quasi-diagonality of the matrix representation. We believe this approach can be extended to more general symbols in the pseudodifferential setting, improving the existing results in terms of their Gabor matrix decay. For the complex heat equation, we obtain closed-form formulas exhibiting both dissipative
  and oscillatory behavior governed respectively by the real and imaginary parts of the diffusion
  parameter. The modulus of the Gabor matrix is shown to display Gaussian decay and temporal
  spreading consistent with diffusion phenomena. In contrast, the complex Hermite equation
  is analyzed via H√∂rmander's metaplectic semigroup, where the propagator decomposes as the product
  of a real Hermite semigroup and a fractional Fourier transform. In this setting, the Gabor matrix
  retains its Gaussian shape while undergoing a pure rotation on the time-frequency plane,
  reflecting the symplectic structure of the underlying flow.
  The analysis provides a unified operator-theoretic and phase-space perspective on parabolic
  and hyperbolic evolution equations, linking the geometry of their symbols with the
  sparsity and localization properties of their Gabor representations. Explicit formulas are
  given in a form suitable for numerical computation and visualization of phase-space dynamics.

</details>


### [49] [On the Fujita Phenomenon for a Forced Spatio-Temporal Fractional Diffusion Equation](https://arxiv.org/abs/2511.19424)
*Rihab Ben Belgacem,Mohamed Majdoub*

Main category: math.AP

TL;DR: Analysis of a semilinear fractional diffusion equation with time-dependent forcing, examining local existence, blow-up in subcritical regime, and global existence in supercritical regime with critical exponent determination.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of solutions to fractional diffusion equations with external forcing, particularly focusing on existence, blow-up phenomena, and critical exponents that separate different solution behaviors.

Method: Mathematical analysis using fractional calculus, establishing local existence of mild solutions, proving finite-time blow-up in subcritical cases, and deriving global existence results under small data conditions in supercritical regime.

Result: Proved local existence and finite-time blow-up in subcritical regime when integral of forcing term is positive; established global existence for small data in supercritical case with critical exponent p_F = (NŒ± - 2sœÉ)/(NŒ± - 2s(Œ±+œÉ)); provided robust global existence under local smallness conditions.

Conclusion: The critical exponent p_F separates subcritical and supercritical regimes, with blow-up occurring below this threshold and global existence possible above it under appropriate smallness conditions on initial data and forcing term.

Abstract: We study the Cauchy problem for a semilinear fractional diffusion equation with a time-dependent forcing term: \[ \partial_t^Œ±u + (-Œî)^{\mathsf{s}} u = |u|^p + t^œÉ\,\mathbf{w}(x), \quad (t,x) \in (0,\infty) \times \mathbb{R}^N, \] with parameters $Œ±, \mathsf{s} \in (0,1)$, $œÉ> -Œ±$, and a continuous function $\mathbf{w}$. The operator $\partial_t^Œ±$ denotes the Caputo fractional derivative. Our main contributions are threefold. First, we prove the local existence of mild solutions and demonstrate a finite-time blow-up in the subcritical regime, provided $\int_{\mathbb{R}^N} \mathbf{w}(x)\,dx > 0$. Second, for the supercritical case with $-Œ±< œÉ< 0$, we establish the global existence for sufficiently small initial data and a forcing term, and we determine the critical exponent to be \[ p_F=\frac{NŒ±-2\mathsf{s}œÉ}{NŒ±-2\mathsf{s}(Œ±+œÉ)}. \] Finally, for this supercritical range, we prove a more robust global existence result under assumptions requiring only local smallness and controlled growth of the data.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [50] [H3PC: Hypersonic, High-Order, High-Performance Code with Adaptive Mesh Refinement and Real Chemistry](https://arxiv.org/abs/2511.17551)
*Ahmad Peyvan,Khemraj Shukla,George Em Karniadakis*

Main category: physics.comp-ph

TL;DR: Development of H3PC, a hypersonic high-order code using Trixi.jl framework for simulating compressible flows with chemical reactions, featuring parallel computing and adaptive mesh refinement.


<details>
  <summary>Details</summary>
Motivation: To create an efficient computational tool for simulating hypersonic turbulent flows with complex chemistry modeling across various speed regimes.

Method: Discontinuous Galerkin spectral element method with entropy/energy stability, integrated with Mutation.jl for chemical modeling, supporting parallel CPU computations and adaptive mesh refinement.

Result: Successful integration of Mutation++ into H3PC solver, verified through simulations of Taylor-Green vortex flow, supersonic flows past cylinders, and hypersonic P8-inlet.

Conclusion: H3PC demonstrates capability for accurate hypersonic flow simulations with chemical reactions using high-performance computing approaches.

Abstract: We have developed a hypersonic high-order, high-performance code (H$^3$PC) utilizing the ``Trixi.jl" framework in order to simulate both non-reactive and chemically reactive compressible Euler and Navier-Stokes equations for complex three-dimensional geometries. H$^3$PC is parallel on CPU platforms and can perform exascale parallel computations of hypersonic turbulent flows. The numerical approach is based on the discontinuous Galerkin spectral element method, satisfying the entropy and energy stability conditions for the Euler equations. H$^3$PC can perform simulations of high-speed flows from subsonic to hypersonic speeds based on frozen, equilibrium, and non-equilibrium chemistry modeling of the gas mixture, using the \texttt{Mutation.jl} , which is a Julia package developed to wrap the C++-based Mutation++ library. H$^3$PC can also perform parallel adaptive mesh refinement for two- and three-dimensional Euler and Navier-Stokes discretizations with non-conforming elements. In this study, we first demonstrate the successful integration of Mutation++ into the H$^3$PC solver, and then verify its accuracy through simulations of Taylor-Green vortex flow, supersonic flow past a square and circular cylinder, and hypersonic P8-inlet.

</details>


### [51] [Generation of Granular Deposition Interfaces using conditional Generative Adversarial Network (cGAN)](https://arxiv.org/abs/2511.18224)
*Seyed Feyzelloh Ghavami Mirmahalle,Seyed Ehsan Nedaaee Oskoee,Maniya Maleki*

Main category: physics.comp-ph

TL;DR: This paper uses a conditional GAN with U-Net generator and ResNet discriminator to generate 1D granular deposition interface profiles, replacing computationally expensive molecular dynamics simulations.


<details>
  <summary>Details</summary>
Motivation: To substitute computationally extensive molecular dynamics simulations (LAMMPS) with an AI model for generating granular deposition interfaces more efficiently.

Method: Conditional GAN with U-Net generator and ResNet discriminator trained on LAMMPS simulation data from different fluid media (water, acetone, hexane) with same hyperparameters.

Result: The ML-generated interfaces successfully reproduce statistical properties of granular deposition and enable analysis of interface growth trends with large sample sizes.

Conclusion: The cGAN model effectively substitutes molecular dynamics simulations for granular interface generation, providing stable statistical analysis of deposition processes across different media.

Abstract: This work aims at generating 1D interface profiles of granular deposition by a conditional generative adversarial network (cGAN). Our cGAN model employs a U-Net generator and a ResNet discriminator that, in competition with each other, produce granular interfaces. The network is trained on dynamic simulation data from the LAMMPS granular package. Different fluids (water, acetone, and hexane) were used for the medium of the deposition cell to check the model performance in different growing conditions. The same model with the same hyperparameters was trained on data from different media separately. The ML-generated interfaces are compared with those of dynamic simulations, and a large number of interfaces are then produced to obtain more stable statistical properties of granular deposition. This way, the computationally extensive molecular dynamics simulation is substituted by the AI model. The statistical trend of interface growth is diagrammed, and the generated interfaces are also analyzed in terms of statistical features. Keywords: Conditional Generative Adversarial Networks, ResNet, U-Net, Granular Deposition, Interface Growth.

</details>


### [52] [A fast-converging and asymptotic-preserving method for adjoint shape optimization of rarefied gas flows](https://arxiv.org/abs/2511.18433)
*Yanbing Zhang,Ruifeng Yuan,Lei Wu*

Main category: physics.comp-ph

TL;DR: GSIS-based adjoint method accelerates shape optimization for rarefied gas flows by solving primal and adjoint Boltzmann equations efficiently with asymptotic preservation.


<details>
  <summary>Details</summary>
Motivation: Adjoint shape optimization is computationally expensive for rarefied gas flows due to solving 6D primal and adjoint Boltzmann equations for each candidate shape.

Method: Extends GSIS to solve adjoint kinetic equation using macroscopic synthetic equations with Newtonian stress law and higher-order rarefaction terms for fast convergence.

Result: Achieved 34.5% drag reduction in transition regime and 61.1% in slip-flow regime within ~10 iterations, with only dozens of velocity distribution updates per shape.

Conclusion: GSIS-based adjoint method dramatically reduces computational cost while maintaining accuracy across flow regimes, enabling efficient 3D shape optimization.

Abstract: Adjoint based shape optimization is a powerful technique in fluid-dynamics optimization, capable of identifying an optimal shape within only dozens of design iterations. However, when extended to rarefied gas flows, the computational cost becomes enormous because both the six dimensional primal and adjoint Boltzmann equations must be solved for each candidate shape. Building on the general synthetic iterative scheme (GSIS) for solving the primal Boltzmann model equation, this paper presents a fast converging and asymptotic preserving method for solving the adjoint kinetic equation. The GSIS accelerates the convergence of the adjoint kinetic equation by incorporating solutions of macroscopic synthetic equations, whose constitutive relations include the Newtonian stress law along with higher order terms capturing rarefaction effects. As a result, the method achieves asymptotic preservation (allowing the use of large spatial cell sizes in the continuum limit) while maintaining accuracy in highly rarefied regimes. Numerical tests demonstrate exceptional performance on drag minimization problems for 3D bodies, achieving drag reductions of 34.5% in the transition regime and 61.1% in the slip-flow regime within roughly ten optimization iterations. For each candidate shape, converged solutions of the primal and adjoint Boltzmann equation are obtained with only a few dozen updates of the velocity distribution function, dramatically reducing computational cost compared with conventional methods.

</details>


### [53] [Fast-Converging and Asymptotic-Preserving DSMC](https://arxiv.org/abs/2511.19061)
*Bin Hu,Liyan Luo,Kaiyuan Wang,Lei Wu*

Main category: physics.comp-ph

TL;DR: The DIG scheme enables DSMC to rapidly converge to steady-state solutions in near-continuum flows, achieving 100x speedup over traditional DSMC at Kn=0.01.


<details>
  <summary>Details</summary>
Motivation: Improving DSMC efficiency is urgent for space exploration applications, especially in near-continuum flow regimes where traditional DSMC requires small cell sizes.

Method: Direct intermittent general synthetic iteration (DIG) scheme that allows cell sizes much larger than mean free path, analyzed using linearized BGK model and tested on Poiseuille flow and hypersonic cylinder flow.

Result: DIG recovers Navier-Stokes equations with O(1) cell size in near continuum, reduces deviation by >5x after one cycle, achieves 100x speedup at Kn=0.01, with greater gains at smaller Knudsen numbers.

Conclusion: DIG method is highly efficient and accurate for multiscale flow simulation, showing great potential for engineering applications in space exploration.

Abstract: Improving the efficiency of the direct simulation Monte Carlo (DSMC) method has become increasingly urgent with the rapid development of space exploration. To address this issue, the direct intermittent general synthetic iteration (DIG) scheme has recently been proposed to enable DSMC's rapid and accurate convergence to steady-state solutions, even when the cell size is much larger than the mean free path in near-continuum flow regimes. The first part of the paper is devoted to the mathematical analysis of DIG's fast-converging and asymptotic-preserving properties. Because the Boltzmann equation is analytically intractable, the analysis is conducted using the linearized BGK model. It is found that, in the near continuum flow regime, the DIG method asymptotically recovers the Navier Stokes equations when the cell size is O(1), rather than being constrained by the mean free path. Moreover, after a single cycle of DIG evolution, the deviation from the final steady state solution is reduced by more than a factor of five. In the second part of the paper, the Poiseuille flow and hypersonic flow passing over cylinder are investigated using the DIG scheme, with different time step and cell sizes, thereby demonstrating its efficiency and accuracy in multiscale flow simulation. Specifically, when the Knudsen number is 0.01, the DIG method is found to be faster than the traditional DSMC method by two orders of magnitude. The performance gain becomes even greater at smaller Knudsen numbers. The proposed method holds great potential for engineering applications.

</details>


### [54] [Electrochemical Interfaces at Constant Potential: Data-Efficient Transfer Learning for Machine-Learning-Based Molecular Dynamics](https://arxiv.org/abs/2511.19338)
*Michele Giovanni Bianchi,Michele Re Fiorentin,Francesca Risplendi,Candido Fabrizio Pirri,Michele Parrinello,Luigi Bonati,Giancarlo Cicero*

Main category: physics.comp-ph

TL;DR: TRECI is a data-efficient workflow for constructing machine learning force fields that achieve ab initio-level accuracy in electronically grand-canonical molecular dynamics simulations of electrified metal/water interfaces.


<details>
  <summary>Details</summary>
Motivation: Simulating electrified metal/water interfaces with explicit solvent under constant potential is essential for understanding electrochemical processes but remains prohibitively expensive with traditional ab initio methods.

Method: TRECI uses transfer learning from general-purpose and domain-specific models to construct machine learning force fields, enabling stable and accurate simulations across a wide potential range with reduced reference configurations.

Result: Applied to Cu(111)/water, models trained on just one thousand configurations yield accurate molecular dynamics simulations, capturing bias-dependent solvent restructuring effects not previously reported.

Conclusion: TRECI offers a general strategy for characterizing diverse materials and interfacial chemistries, significantly lowering the cost of realistic constant-potential simulations and expanding access to quantitative electrochemical modelling.

Abstract: Simulating electrified metal/water interfaces with explicit solvent under constant potential is essential for understanding electrochemical processes, yet remains prohibitively expensive with ab initio methods. We present TRECI, a data-efficient workflow for constructing machine learning force-fields (ML-FFs) that achieve ab initio-level accuracy in electronically grand-canonical molecular dynamics. By leveraging transfer learning from general-purpose and domain-specific models, TRECI enables stable and accurate simulations across a wide potential range using a reduced number of reference configurations. This efficiency allows the use of high-level meta-GGA functionals and rigorous surface-electrification schemes. Applied to Cu(111)/water, models trained on just one thousand configurations yield accurate molecular dynamics simulations, capturing bias-dependent solvent restructuring effects not previously reported. TRECI offers a general strategy for characterising diverse materials and interfacial chemistries, significantly lowering the cost of realistic constant-potential simulations and expanding access to quantitative electrochemical modelling.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [55] [Three Dimensional Effects on Proton Acceleration with Grooved Hydrocarbon Targets](https://arxiv.org/abs/2511.18061)
*Imran Khan,Mohammad Yasir,Vikrant Saxena*

Main category: physics.plasm-ph

TL;DR: 3D simulations show cylindrical and cuboidal groove geometries in laser proton acceleration behave differently, with cylindrical symmetry and circular polarization causing 15% decay in proton cut-off energies compared to linear polarization.


<details>
  <summary>Details</summary>
Motivation: To investigate differences between cylindrical and cuboidal groove geometries in laser-based proton acceleration and study the effect of circular polarization on proton spectra.

Method: Used three-dimensional particle-in-cell simulations with waveguide model analysis to compare cylindrical and cuboidal groove geometries, and studied circular vs linear polarization effects.

Result: Cylindrical and cuboidal grooves show distinct laser pulse and electron behaviors. Cylindrical symmetry with circular polarization causes 15% decay in proton cut-off energies compared to linear polarization.

Conclusion: Cylindrical symmetry and circular polarization do not work well together for proton acceleration, contrary to initial expectations, leading to significant energy decay.

Abstract: Recently, using two-dimensional particle-in-cell simulations, it has been demonstrated that in laser based proton acceleration with micro-structured targets, a single rectangular groove on the target front offers significant proton cut-off enhancement with linearly polarised laser pulses. In the present work, three-dimensional investigations are carried out to identify notable differences between cylindrical and cuboidal groove geometries both of which correspond to a rectangular groove in a two-dimensional case. In particular, a waveguide model is employed to analyse the effect of the groove geometry and extensive three-dimensional particle-in-cell simulations are performed to demonstrate the distinct behaviour of laser pulse and electrons for cylindrical and cuboidal grooves. Further, the effect of a circular polarisation of the incident laser pulse on the spectra of accelerated protons is studied. It is shown that contrary to our initial expectations, cylindrical symmetry and circular polarisation do not play well together and cause as much as 15$\%$ decay in proton cut-off energies as compared to the case of cylindrical symmetry and linear polarisation.

</details>


### [56] [Ion Temperature Anisotropy Limits from Magnetic Curvature Scattering in Magnetotail Reconnection Jets](https://arxiv.org/abs/2511.18956)
*Louis Richard,Anton V. Artemyev,Cecilia Norgren,Xin An,Sergey R. Kamaletdinov,Yuri V. Khotyaintsev*

Main category: physics.plasm-ph

TL;DR: Curvature scattering limits ion temperature anisotropy in Earth's magnetotail current sheets, maintaining stability through analytical thresholds validated by observations.


<details>
  <summary>Details</summary>
Motivation: To understand how curvature scattering limits deviations of ion velocity distributions from equilibrium in magnetotail current sheets, as this mechanism's role remains insufficiently understood despite being a leading candidate.

Method: Modeled a quasi-1D current sheet with finite magnetic field curvature and three ion populations, deriving analytical thresholds for anisotropy based on current sheet stability.

Result: Demonstrated that curvature scattering imposes limits on ion anisotropies, with analytical thresholds validated against spacecraft observations and numerical results.

Conclusion: Curvature scattering plays a crucial role in maintaining current sheet stability by limiting ion temperature anisotropy in the Earth's magnetotail.

Abstract: In collisionless plasmas, relaxation of the deviations of ion velocity distribution functions (VDFs) from local thermodynamic equilibrium (LTE) occurs through particle interactions with electromagnetic fields. In particular, in the Earth's magnetotail, the deviations of the ion VDFs, typically consisting of multiple components, from the equilibrium must be limited to maintain stability of the current sheet. Curvature scattering is a leading candidate mechanism to limit such deviations, but its role remains insufficiently understood. We investigate the limits of ion temperature anisotropy in a magnetotail-like configuration by modeling a quasi-1D current sheet with a finite magnetic field curvature and three ion populations. We derive analytical thresholds for anisotropy based on current sheet stability and validate against spacecraft observations and numerical results. Our findings demonstrate that curvature scattering imposes limits on ion anisotropies, thereby maintaining the stability of the current sheet.

</details>


### [57] [Bell Plesset Effects on Rayleigh Taylor Instability of Three Dimensional Spherical Geometry](https://arxiv.org/abs/2511.19028)
*Xilai Li,Yilin Wu,Zhengnuo Chen,Mengqi Yang,Jie Zhang*

Main category: physics.plasm-ph

TL;DR: A weakly nonlinear multi-mode theory for Rayleigh-Taylor instability on spherical interfaces incorporating mode couplings and Bell-Plesset effects, showing preferential energy transfer to axisymmetric modes and dramatic amplification by convergence effects.


<details>
  <summary>Details</summary>
Motivation: Extend prior analyses limited to static backgrounds, 2D geometries, or single-mode conditions to understand time-dependent interface instabilities in convergent flows relevant to astrophysics and inertial confinement fusion.

Method: Developed a weakly nonlinear multi-mode theory for RTI on time-varying spherical interfaces, fully incorporating mode couplings and Bell-Plesset effects from interface convergence, capable of evolving arbitrary 3D initial perturbations.

Result: BP effects dramatically amplify instability growth by orders of magnitude, with preferential energy channeling into axisymmetric (m=0) modes. Second-order amplitudes remain small relative to first-order, validating perturbative approach.

Conclusion: Axisymmetric modes play uniquely dominant role in BP-driven convergent flows, providing new insights into time-dependent interface instabilities for applications like astrophysical shell collapse and inertial confinement fusion.

Abstract: We develop a weakly nonlinear, multi-mode theory for the Rayleigh-Taylor instability (RTI) on a time-varying spherical interface, fully incorporating mode couplings and the Bell-Plesset (BP) effects arising from interface convergence. Our model extends prior analyses, which have been largely restricted to static backgrounds, 2D cylindrical geometries, or single-mode initial conditions. We present a framework capable of evolving arbitrary, fully three-dimensional initial perturbations on a dynamic background. At the first order, mode amplitudes respond to the time-varying interface acceleration with an exponential-like growth, in qualitative agreement with classic static results. At second order, nonlinear mode coupling reveals a powerful selection rule: energy is preferentially channeled into axisymmetric (m=0) modes. We find that the BP effects dramatically amplify the instability growth by a few orders of magnitude, with this amplification being even more significant for second order couplings. Despite this strong channeling, the second order amplitudes remain small relative to the first order, validating the perturbative approach. These findings offer new physical insights into time-dependent interface instabilities relevant to applications such as astrophysical shell collapse and inertial confinement fusion, highlighting the uniquely dominant role of axisymmetric modes in BP-driven convergent flows.

</details>


### [58] [Physics-informed Neural Operator Learning for Nonlinear Grad-Shafranov Equation](https://arxiv.org/abs/2511.19114)
*Siqi Ding,Zitong Zhang,Guoyang Shi,Xingyu Li,Xiang Gu,Yanan Xu,Huasheng Xie,Hanyue Zhao,Yuejiang Shi,Tianyuan Liu*

Main category: physics.plasm-ph

TL;DR: Physics-Informed Neural Operator (PINO) combines neural networks with physics constraints to solve the Grad-Shafranov equation for fusion plasma control, achieving fast inference and improved accuracy over purely data-driven methods.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical solvers for the Grad-Shafranov equation are computationally expensive, while data-driven surrogates lack physical consistency and generalize poorly. There's a need for fast, accurate, and physically consistent solvers for real-time fusion plasma control.

Method: Developed PINO framework using neural operators, particularly identifying Transformer-KAN Neural Operator (TKNO) as optimal. Used semi-supervised learning combining sparse labeled data (100 interior points) with physics-based loss terms to enforce physical laws.

Result: TKNO achieved 0.25% mean L2 error under supervised training. Semi-supervised learning achieved optimal balance: 0.48% interpolation error and robust extrapolation (4.76% error, 8.9x degradation factor vs 39.8x for supervised models). Models enable millisecond-level inference after TensorRT optimization.

Conclusion: PINO provides a promising pathway for next-generation fusion control systems by combining neural networks with physics constraints, achieving fast inference while maintaining physical consistency and generalization capabilities.

Abstract: As artificial intelligence emerges as a transformative enabler for fusion energy commercialization, fast and accurate solvers become increasingly critical. In magnetic confinement nuclear fusion, rapid and accurate solution of the Grad-Shafranov equation (GSE) is essential for real-time plasma control and analysis. Traditional numerical solvers achieve high precision but are computationally prohibitive, while data-driven surrogates infer quickly but fail to enforce physical laws and generalize poorly beyond training distributions. To address this challenge, we present a Physics-Informed Neural Operator (PINO) that directly learns the GSE solution operator, mapping shape parameters of last closed flux surface to equilibrium solutions for realistic nonlinear current profiles. Comprehensive benchmarking of five neural architectures identifies the novel Transformer-KAN (Kolmogorov-Arnold Network) Neural Operator (TKNO) as achieving highest accuracy (0.25% mean L2 relative error) under supervised training (only data-driven). However, all data-driven models exhibit large physics residuals, indicating poor physical consistency. Our unsupervised training can reduce the residuals by nearly four orders of magnitude through embedding physics-based loss terms without labeled data. Critically, semi-supervised learning--integrating sparse labeled data (100 interior points) with physics constraints--achieves optimal balance: 0.48% interpolation error and the most robust extrapolation performance (4.76% error, 8.9x degradation factor vs 39.8x for supervised models). Accelerated by TensorRT optimization, our models enable millisecond-level inference, establishing PINO as a promising pathway for next-generation fusion control systems.

</details>


### [59] [Abnormal Polarity Effect on the DC Breakdown Voltage in Short SF6 Gap](https://arxiv.org/abs/2511.19136)
*Zihao Feng*

Main category: physics.plasm-ph

TL;DR: This paper reveals the dynamic mechanism behind abnormal polarity effects in SF6 short-gap DC breakdown and proposes a novel ion-ion plasma breakdown criterion for accurately predicting negative breakdown voltage.


<details>
  <summary>Details</summary>
Motivation: Traditional single-streamer breakdown criterion fails to reproduce the abnormal polarity effect observed in SF6 short-gap DC breakdown experiments, particularly for negative polarity where simulated results deviate significantly from measurements.

Method: The authors compare experimental measurements with numerical simulations, propose a new ion-ion plasma breakdown criterion for negative polarity, and analyze the spatiotemporal evolution of key physical parameters.

Result: When using the novel ion-ion plasma breakdown criterion, simulated negative breakdown voltage agrees with experimental measurements and successfully reproduces the abnormal polarity effect. The dynamic mechanism involves four distinct stages including photoionization-driven negative-ion accumulation.

Conclusion: The ion-ion plasma breakdown criterion effectively explains and predicts negative breakdown voltage in SF6 short-gap DC breakdown, revealing that the process is dominated by photoionization-driven negative-ion accumulation rather than conventional impact ionization.

Abstract: In this Letter, through the comparison between experiment and numerical simulation, we reveal the dynamic mechanism underlying the abnormal polarity effect in SF6 short-gap DC breakdown, as well as a novel criterion for predicting negative breakdown voltage. Using the traditional single-streamer breakdown criterion, the simulated positive breakdown voltage agrees well with experimental measurements, whereas the simulated negative breakdown voltage deviates markedly from the experiments, so the single-streamer breakdown criterion fails to reproduce the abnormal polarity effect observed experimentally. In addressing this, we propose an ion-ion plasma breakdown criterion for negative breakdown voltage. When this novel criterion is applied, the simulated negative breakdown voltage agrees with the experiments and reflects the abnormal polarity effect. Analysis of the spatiotemporal evolution of key physical parameters reveals that, the dynamic mechanism for ion-ion plasma breakdown for negative polarity can be divided into four stages: primary streamer stage, ion accumulation stage, reconstructive ionization stage, and ion-ion plasma propagation stage. Notably, the ion-ion plasma propagation stage is dominated by photoionization-driven negative-ion accumulation rather than conventional impact ionization.

</details>


### [60] [Two-Stream Instability and Bernstein-Greene-Kruskal Mode Formation in Coulomb One Component Plasma](https://arxiv.org/abs/2511.19281)
*Ajaz Mir,Rauoof Wani,Sanat Tiwari,Abhijit Sen*

Main category: physics.plasm-ph

TL;DR: Molecular dynamics simulations reveal that the Two-Stream Instability in strongly coupled plasmas leads to BGK mode formation, with long-range Coulomb interactions being crucial for instability development.


<details>
  <summary>Details</summary>
Motivation: To study nonlinear evolution of Two-Stream Instability in strongly coupled plasmas using molecular dynamics, capturing microscopic effects missing in traditional fluid and kinetic models.

Method: Classical molecular dynamics simulations with long-range Coulomb interactions between particles, analyzing linear growth, nonlinear saturation, and BGK mode formation.

Result: Instability grows rapidly in linear regime, saturates within tens of plasma periods, forming single BGK mode that becomes dynamically unstable with continuous electrostatic energy decay.

Conclusion: Long-range Coulomb interactions are essential for pronounced instability and BGK mode formation, with single BGK mode emerging across coupling strengths when streaming velocity exceeds critical threshold.

Abstract: We investigate the Two-Stream Instability in a strongly coupled plasma using classical molecular dynamics simulations with long-range Coulomb interactions between particles. The nonlinear evolution of the instability is identified by the emergence of a Bernstein-Greene-Kruskal (BGK) mode. Our simulations capture key microscopic effects, such as inter-particle correlations, collisional dynamics, and coherent wave-particle interactions-features often absent in traditional fluid and kinetic models, including Particle-In-Cell and Vlasov approaches. In the linear regime, the instability grows rapidly and saturates within a few tens of plasma periods. As the system transitions into the nonlinear saturation phase, a single BGK mode emerges. This mode (or phase-space hole) becomes dynamically unstable in the nonlinear regime, characterized by a continuous decay of electrostatic energy over time. An energy budget analysis reveals a bump in an otherwise thermal spectrum, indicating the excitation of a coherent mode, further confirmed through a numerical rendering of the dispersion relation. The pairwise interaction plays a crucial role: pronounced instability and BGK mode formation occur with long-range Coulomb forces, while such structures are suppressed under shielded Coulomb interactions. We observe the emergence of a single BGK mode across all coupling strengths in the fluid regime, provided the streaming velocity exceeds a critical threshold.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [61] [Duality Perspective on Nonlinear Eigenproblems](https://arxiv.org/abs/2511.19188)
*Jonathan Laubmann,Manuel Friedrich,Daniel Tenbrinck*

Main category: math.SP

TL;DR: This paper studies nonlinear eigenproblems for convex functionals in reflexive Banach spaces, develops dual formulations using Fenchel conjugates, analyzes numerical methods including inverse power methods, and validates theory with p-Laplacian experiments.


<details>
  <summary>Details</summary>
Motivation: To establish a comprehensive theoretical framework for nonlinear eigenproblems in Banach spaces and develop effective numerical methods for solving them, particularly for convex functionals.

Method: Developed dual formulation using Fenchel conjugate, established equivalence between primal and dual problems, introduced duality gap and geometric characterization, analyzed inverse power method as dual power method, and conducted numerical experiments with p-Laplacian.

Result: Proved strong convergence of inverse power method for absolutely p-homogeneous functionals, established connection between flow-based proximal power method and inverse power method, and validated theoretical results through extensive numerical experiments.

Conclusion: The paper provides a unified theoretical framework for nonlinear eigenproblems with effective numerical methods, demonstrating strong convergence properties and practical applicability through p-Laplacian experiments.

Abstract: We investigate nonlinear eigenproblems for a broad class of proper, closed, convex functionals in reflexive Banach spaces. We develop a dual formulation of the nonlinear eigenproblem using the Fenchel conjugate and establish an equivalence to the primal problem. Further, we introduce a duality gap and a geometric characterization of eigenvectors that apply in general Banach spaces. We interpret the dual problem as the eigenproblem for the inverse operator of the primal problem. Concerning numerical methods for solving nonlinear eigenproblems, we analyze the inverse power method, framed as a dual power method, showing strong convergence in the case of absolutely p-homogeneous functionals. Our theoretical results are validated by extensive numerical experiments for the p-Laplacian. We further connect the flow-based proximal power method from the literature to the inverse power method and discuss two numerical approaches to approximate higher-order nonlinear eigenfunctions.

</details>


### [62] [A new universal inequality for Neumann eigenvalues of the Laplacian on a planar convex domain](https://arxiv.org/abs/2511.18043)
*Kei Funano*

Main category: math.SP

TL;DR: New universal inequality for Neumann eigenvalues of Laplacian on planar convex domains


<details>
  <summary>Details</summary>
Motivation: To establish fundamental mathematical bounds for Neumann eigenvalue problems in convex planar domains

Method: Mathematical analysis and proof techniques for Laplacian eigenvalue inequalities

Result: Derived a new universal inequality that applies to all planar convex domains

Conclusion: Provides important theoretical bounds for Neumann eigenvalue problems in convex planar geometry

Abstract: In this paper we establish a new universal inequality for Neumann eigenvalues of the Laplacian on a planar convex domain.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [63] [Atomistic Framework for Glassy Polymer Viscoelasticity Across 20 Frequency Decades](https://arxiv.org/abs/2511.18406)
*Ankit Singh,Vinay Vaibhav,Caterina Czibula,Astrid Macher,Petra Christ√∂fl,Karin Bartl,Gregor Trimmel,Timothy W. Sirk,Alessio Zaccone*

Main category: cond-mat.soft

TL;DR: Extends non-affine deformation theory with time-dependent memory kernel to predict PMMA's viscoelastic response across 20+ decades of frequency, bridging molecular to macroscopic scales.


<details>
  <summary>Details</summary>
Motivation: Glassy polymers' viscoelastic response is difficult to characterize across broad frequency and temperature ranges, requiring unified multiscale approaches.

Method: Incorporates time-dependent memory kernel within Generalized Langevin Equation for atomistic non-affine motions to yield frequency-dependent mechanical response.

Result: Captures PMMA's shear modulus and relaxation spectrum across 20+ decades in frequency, showing quantitative consistency with multiple experimental techniques.

Conclusion: Provides unified theoretical-computational route for multiscale characterization of polymer glasses, bridging mechanics from molecular to macroscopic scales.

Abstract: Glassy polymers are central to engineering applications, yet their viscoelastic response over broad frequency and temperature ranges remains difficult to characterize. We extend non-affine deformation theory by incorporating a time-dependent memory kernel within the Generalized Langevin Equation for atomistic non-affine motions, yielding frequency-dependent mechanical response. Applied to poly(methyl methacrylate) (PMMA), the method captures the shear modulus and relaxation spectrum across more than twenty decades in frequency, from hundreds of terahertz down to the millihertz regime, thus bridging polymer mechanics from ordinary to extreme scales. Our predictions show quantitative consistency with independent estimates from oscillatory-shear molecular dynamics, Brillouin scattering, ultrasonic spectroscopy, Split-Hopkinson testing, and dynamic mechanical analysis (DMA), demonstrating a unified theoretical-computational route for multiscale characterization of polymer glasses.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [64] [Appraising the absolute limits of nanotubes and nanospheres to preserve high-pressure materials](https://arxiv.org/abs/2511.17868)
*Yin L. Xu,Guang F. Yang,Yi Sun,Hong X. Song,Yu S. Huang,Hao Wang,Xiao Z. Yan,Hua Y. Geng*

Main category: cond-mat.mtrl-sci

TL;DR: A theoretical model is developed to evaluate the pressure-bearing capability of nanomaterials (nanotubes/nanospheres) to preserve high-pressure phases at ambient conditions, with graphene showing the best performance.


<details>
  <summary>Details</summary>
Motivation: High-pressure materials often lose their attractive properties when released to ambient conditions, and nanostructures with exceptional mechanical strength could potentially contain these phases, but no theoretical model exists to analyze this possibility.

Method: Proposed a physical model to assess the absolute theoretical limit of nanomaterials, combined with first-principles calculations to screen graphene, h-BN, biphenylene, and Œ≥-graphyne, and performed systematic investigations.

Result: Graphene exhibits the best pressure-bearing capability, followed by h-BN, biphenylene, and Œ≥-graphyne. Structures with largest average binding energy per bond and highest bond density have highest pressure limits. Multi-layer structures can contain higher pressures.

Conclusion: Nanotubes/nanospheres, particularly multi-layer graphene structures, can effectively preserve high-pressure materials at ambient conditions, enabling retrieval of compressed materials like LaH10 superconductors and metallic hydrogen.

Abstract: Matter under high pressure often exhibits attractive properties, which, unfortunately, are typically irretrievable when released to ambient conditions. Intuitively, nanostructure engineering might provide a promising route to contain high-pressure phase of materials because of the exceptional mechanical strength at nanoscale. However, there is no available theoretical model that can analyze this possibility, not to mention to quantitatively evaluate the pressure-bearing capability of nano-cavities. Here, a physical model is proposed to appraise the absolute theoretical limit of various nanotubes/nanospheres to preserve high-pressure materials to ambient conditions. By incorporating with first-principles calculations, we screen and select four types of representative nanomaterials: graphene, hexagonal boron nitride (h-BN), biphenylene, and Œ≥-graphyne, and perform systematic investigations. The results indicate that nanotube/nanosphere of graphene exhibits the best pressure-bearing capability, followed by h-BN, biphenylene and Œ≥-graphyne. Our model reveals that the structure with the largest average binding energy per bond and the highest density of bonds will have the highest absolute limit to contain pressure materials, while electron/hole doping and interlayer interactions have minor effects. Our finding suggests that one can utilize nanotube/nanosphere with multiple layers to retrieve compressed material with higher pressures. For example, a single layer graphene sphere can retrieve compressed LaH10 with a volume size of 26 nm3 that corresponding to a pressure of 170 GPa and with a near room temperature superconductor transition of Tc=250 K. Similarly, in order to retrieve the metastable atomic hydrogen or molecular metallic hydrogen at about 250 GPa, it requires only three layers of a nanosphere to contain a volume size of 173 nm^3.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [65] [Entanglement Witnesses of Condensation for Enhanced Quantum Sensing](https://arxiv.org/abs/2511.17749)
*Lilian I. Payne Torres,Irma Avdic,Anna O. Schouten,Olivia C. Wedig,Gregory S. Engel,David A. Mazziotti*

Main category: quant-ph

TL;DR: Collective entanglement of spin qubits from particle-hole pair condensation amplifies spin transitions, potentially improving quantum sensor sensitivity with O(‚àöN) enhancement.


<details>
  <summary>Details</summary>
Motivation: To enhance classical sensing by leveraging quantum entanglement resources, specifically using collective entanglement to amplify signal contrast in optically detected magnetic resonance.

Method: Theoretical analysis and computational realization using an ensemble of N triplet spins with magnetic dipole interactions, examining geometries that maximize particle-hole pair condensation.

Result: Demonstrated O(‚àöN) enhancement of transition amplitude with applied microwave field, with strongest effects at geometries where particle-hole pair condensation is maximized. Effect is robust to noise.

Conclusion: Provides a design principle for quantum sensors using condensation-inspired entanglement to boost sensitivity in spin-based platforms.

Abstract: Quantum phenomena such as entanglement provide powerful resources for enhancing classical sensing. Here, we theoretically show that collective entanglement of spin qubits, arising from a condensation of particle-hole pairs, can strongly amplify transitions between ground and excited spin states, potentially improving signal contrast in optically detected magnetic resonance. This collective state exhibits an $\mathcal{O}(\sqrt{N})$ enhancement of the transition amplitude with respect to an applied microwave field, where $N$ is the number of entangled spin qubits. We computationally realize this amplification using an ensemble of $N$ triplet spins with magnetic dipole interactions, where the largest transition amplitudes occur at geometries for which the condensation of particle-hole pairs is strongest. This effect, robust to noise, originates from the concentration of entanglement into a single collective mode, reflected in a large eigenvalue of the particle-hole reduced density matrix -- an entanglement witness of condensation analogous to off-diagonal long-range order, though realized here in a finite system. These results offer a design principle for quantum sensors that exploit condensation-inspired entanglement to boost sensitivity in spin-based platforms.

</details>


### [66] [Probing Antiferromagnetic Hysteresis on Programmable Quantum Annealers](https://arxiv.org/abs/2511.17779)
*Elijah Pelofske,Pratik Sathe,Cristiano Nisoli,Frank Barrows*

Main category: quant-ph

TL;DR: Using quantum annealing processors, researchers demonstrated magnetic memory in antiferromagnets through hysteresis measurements with transverse and longitudinal field control.


<details>
  <summary>Details</summary>
Motivation: To investigate the counterintuitive concept of magnetic memory in antiferromagnets, which typically don't exhibit magnetic hysteresis like ferromagnets.

Method: Implemented a sampling-based magnetic hysteresis protocol using programmable analog quantum annealing processors, with transverse field enabling state transitions and longitudinal field for magnetic field sweeps.

Result: Observed full saturation and reversal of hysteresis curves, and discovered emergent magnetic domains mediated by quantum fluctuations that create magnetic memory effects.

Conclusion: Quantum fluctuations can induce magnetic memory effects in antiferromagnets, demonstrating their potential for information storage applications despite their typical lack of magnetic hysteresis.

Abstract: Using programmable analog quantum annealing processors, we implement a sampling-based magnetic hysteresis protocol to probe the counterintuitive notion of magnetic memory of antiferromagnets. A key component of this protocol responsible for the hysteresis is a transverse field, which enables state transitions, while the magnetic field sweep is done via a longitudinal control field. We present evidence of full saturation and reversal of the hysteresis curve, as well as emergent magnetic domain mediated by quantum fluctuations that give rise to the magnetic memory effect in antiferromagnets.

</details>


### [67] [Noise-Adaptive Quantum Circuit Mapping for Multi-Chip NISQ Systems via Deep Reinforcement Learning](https://arxiv.org/abs/2511.18079)
*Atiye Zeynali,Zahra Bakhshi*

Main category: quant-ph

TL;DR: DeepQMap is a deep reinforcement learning framework for quantum circuit compilation that dynamically adapts to hardware noise variations, achieving 49.3% higher fidelity and 79.8% reduction in inter-chip communication compared to static QUBO methods.


<details>
  <summary>Details</summary>
Motivation: The transition to distributed multi-chip quantum architectures introduces challenges in managing temporal noise variations and minimizing expensive inter-chip operations, requiring dynamic rather than static optimization approaches.

Method: Integrates bidirectional LSTM-based Dynamic Noise Adaptation (DNA) network with multi-head attention mechanisms and Rainbow DQN architecture to continuously adapt to hardware dynamics through learned temporal representations of quantum system behavior.

Result: Achieves mean circuit fidelity of 0.920 ¬± 0.023 (49.3% improvement over QUBO), reduces inter-chip communication by 79.8%, maintains noise prediction accuracy with R¬≤ = 0.912, and scales well to 100 qubits while training 8.2√ó faster than baselines.

Conclusion: DeepQMap provides practical significance for near-term quantum computing with very large effect sizes, demonstrating superior performance in dynamic hardware environments compared to static optimization methods.

Abstract: The transition from monolithic to distributed multi-chip quantum architectures has fundamentally altered the circuit compilation landscape, introducing challenges in managing temporal noise variations and minimizing expensive inter-chip operations. We present DeepQMap, a deep reinforcement learning framework that integrates a bidirectional Long Short-Term Memory based Dynamic Noise Adaptation (DNA) network with multi-head attention mechanisms and Rainbow DQN architecture. Unlike conventional static optimization approaches such as QUBO formulations, our method continuously adapts to hardware dynamics through learned temporal representations of quantum system behavior. Comprehensive evaluation across 270 benchmark circuits spanning Quantum Fourier Transform, Grover's algorithm, and Variational Quantum Eigensolver demonstrates that DeepQMap achieves mean circuit fidelity of $0.920 \pm 0.023$, representing a statistically significant 49.3\% improvement over state-of-the-art QUBO methods ($0.618 \pm 0.031$, $t_{98} = 4.87$, $p = 0.0023$, Cohen's $d = 2.34$). Inter-chip communication overhead reduces by 79.8\%, decreasing from 2.34 operations per circuit to 0.47. The DNA network maintains noise prediction accuracy with coefficient of determination $R^2 = 0.912$ and mean absolute error of 0.87\%, enabling proactive compensation for hardware fluctuations. Scalability analysis confirms sustained performance across 20-100 qubit systems, with fidelity remaining above 0.87 even at maximum scale where competing methods degrade below 0.60. Training convergence occurs 8.2$\times$ faster than baseline approaches, completing in 45 minutes versus 370 minutes for QUBO optimization. Very large effect sizes validate practical significance for near-term noisy intermediate-scale quantum computing applications.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [68] [Toward end-to-end quantum simulation of rapidly distorted turbulence](https://arxiv.org/abs/2511.18802)
*Zhaoyuan Meng,Leyu Chen,Jin-Peng Liu,Guowei He*

Main category: physics.flu-dyn

TL;DR: Proposes an end-to-end quantum algorithm using Linear Combination of Hamiltonian (LCHS) to simulate rapidly distorted turbulence, showing quantum speedup potential and validation against classical solutions.


<details>
  <summary>Details</summary>
Motivation: To develop quantum algorithms for turbulence simulation that can offer practical speedup over classical methods and establish foundations for more complex turbulent phenomena on quantum computers.

Method: Three-stage algorithm: efficient preparation of initial turbulent state with prescribed energy spectrum, time evolution via LCHS, and direct measurement of key turbulence statistics.

Result: Algorithm shows excellent agreement with ground-truth solutions, capturing both qualitative evolution of turbulent fields and quantitative behavior of statistics including Reynolds stresses and velocity spectrum.

Conclusion: The work establishes a foundation for addressing more complex turbulent phenomena on future fault-tolerant quantum computers, with rapidly distorted turbulence capturing essential mechanisms despite its linearity.

Abstract: We propose an end-to-end quantum algorithm to simulate rapidly distorted turbulence via linear combination of Hamiltonian (LCHS). The algorithm comprises three primary stages: the efficient preparation of an initial turbulent state with a prescribed energy spectrum, its subsequent time evolution via LCHS, and the direct measurement of key turbulence statistics. Our analysis indicates that the algorithm can offer a practical quantum speedup over the classical simulation methods for a sufficiently large computational grid. We evaluate the quantum resource requirements for simulating a minimal instance of non-trivial turbulence with classical validation. The numerical results show excellent agreement with ground-truth solutions, capturing both the qualitative evolution of turbulent fields and the quantitative behavior of statistics, including the Reynolds stresses and the fluctuating velocity spectrum. Despite its linearity, rapidly distorted turbulence captures essential turbulence mechanisms and may inform the development of quantum algorithms for the Navier-Stokes equations. Our work establishes a foundation for addressing more complex turbulent phenomena on future fault-tolerant quantum computers.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [69] [The Willmore energy and curvature concentration](https://arxiv.org/abs/2511.18982)
*Raz Kupferman,Cy Maor,David Padilla-Garza*

Main category: math.DG

TL;DR: New lower bounds for bending energy of Riemannian surfaces in R^3, with optimal blowup rates for concentrated curvature in cones and dipoles, applicable to non-Euclidean elasticity.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by non-Euclidean elasticity, where cones and curvature-dipoles play a central role in understanding the elastic energy of thin sheets.

Method: Analysis of isometric immersions of Riemannian surfaces into R^3, focusing on bending energy (equivalent to Willmore functional). Derivation involves an isoperimetric inequality for framed loops.

Result: Two new lower bounds for bending energy: one in terms of Gaussian curvature, another using Burgers vector (measure of non-flatness). Optimal blowup rates obtained for concentrated curvature in conical geometry and dipoles.

Conclusion: The derived lower bounds provide optimal energy blowup rates for concentrated curvature scenarios and yield direct lower bounds for elastic energy of thin elastic sheets, with potential independent interest in the isoperimetric inequality for framed loops.

Abstract: We study isometric immersions of a Riemannian surface $(\W,\g)$, where $\W\subset \R^2$, into $\R^3$. We consider their bending energy, i.e., the square of the $L^2$-norm of their second fundamental form, which is equivalent to the Willmore functional. We obtain two new lower bounds for this energy, one in terms of the Gaussian curvature of the surface, and the other in terms of a Burgers vector -- a measure of non-flatness connected to torsion. These new estimates provide optimal blowup rates of the energy when the curvature is concentrated (e.g., in a conical geometry). In the more subtle case of dipoles of concentrated curvature, we use the Burgers vector estimates to obtain an optimal blowup rate in terms of the size of the system. Our motivation comes from non-Euclidean elasticity, in which cones and curvature-dipoles play a central role. The lower bounds derived in this work directly yield lower bounds for the elastic energy of thin elastic sheets. The derivation of the curvature-based lower bound involves an isoperimetric inequality for framed loops, which we believe to be of independent interest.

</details>


### [70] [Some aspects of Cheng-Yau gradient estimates](https://arxiv.org/abs/2511.19042)
*Qixuan Hu,Chengjie Yu*

Main category: math.DG

TL;DR: Extends Cheng-Yau gradient estimate to surfaces with lower Ricci curvature bound, obtains pointwise estimates for higher dimensions, and derives monotonicity formulas for positive harmonic functions.


<details>
  <summary>Details</summary>
Motivation: To generalize the rigidity of Cheng-Yau gradient estimate from previous work to surfaces with lower Ricci curvature bounds, and extend these sharp estimates to higher dimensional Riemannian manifolds.

Method: Extends the Cheng-Yau gradient estimate methodology to surfaces with lower Ricci curvature bounds, then generalizes to obtain pointwise gradient estimates for higher dimensional Riemannian manifolds.

Result: Successfully extends Cheng-Yau gradient estimate to surfaces with lower Ricci curvature bound, obtains pointwise gradient estimates for higher dimensional manifolds, and derives monotonicity formulas for positive harmonic functions as applications.

Conclusion: The extension of Cheng-Yau gradient estimates to surfaces with lower Ricci curvature bounds enables the derivation of pointwise estimates in higher dimensions and leads to useful monotonicity formulas for positive harmonic functions.

Abstract: In this note, we extend the rigidity of Cheng-Yau gradient estimate in \cite{HXY} to surfaces with lower Ricci curvature bound. Motivated by these sharp Cheng-Yau gradient estimates, pointwise Cheng-Yau gradient estimates for higher dimensional Riemannian manifolds are obtained, and as their applications, monotonicity formulas for positive harmonic functions are obtained.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [71] [A necessary and sufficient condition for H√∂lder-class solutions to the complex Monge--Amp√®re equation](https://arxiv.org/abs/2511.18605)
*Annapurna Banik*

Main category: math.CV

TL;DR: Existence and regularity conditions for H√∂lder continuous solutions to the complex Monge-Amp√®re equation on bounded domains in C^n


<details>
  <summary>Details</summary>
Motivation: Motivated by S.-Y. Li's work, to establish necessary and sufficient conditions for existence of H√∂lder continuous solutions

Method: Mathematical analysis providing necessary and sufficient conditions for solution existence, plus regularity results on B-regular domains

Result: Proved existence criterion for H√∂lder continuous solutions and regularity results on B-regular domains

Conclusion: Established complete characterization of when H√∂lder continuous solutions exist and their regularity properties

Abstract: We provide a necessary and sufficient condition for the existence of H√∂lder continuous solutions to the complex Monge--Amp√®re equation on bounded domains in $\mathbb{C}^n$. This condition is motivated by a paper by S.-Y. Li. We also prove a result on the regularity of solutions to the complex Monge--Amp√®re equation on general $B$-regular domains in $\mathbb{C}^n$.

</details>


<div id='physics.atm-clus'></div>

# physics.atm-clus [[Back]](#toc)

### [72] [Curvature-Dependent Polarity of Interfacial Energy Flow in Functionalized CNT Polymer Nanocomposites: A Reactive Molecular Dynamics Perspective](https://arxiv.org/abs/2511.18560)
*Mehedi Hasan,Khayrul Islam,Michael T. Kio,AKM Masud*

Main category: physics.atm-clus

TL;DR: Nanotube curvature combined with polydopamine functionalization controls interfacial energy flow in CNT-polymer composites, creating opposite regimes: dissipative interphases in high-curvature CNTs and cohesive shells in low-curvature CNTs.


<details>
  <summary>Details</summary>
Motivation: To resolve the paradox where identical PDA functionalization strengthens some CNT-polymer systems while weakening others, and understand how nanotube curvature modulates interfacial energy flow and mechanical polarity.

Method: Reactive molecular dynamics (ReaxFF) simulations to study how curvature and PDA functionalization jointly govern interfacial energy evolution in CNT-polyvinyl alcohol nanocomposites.

Result: Curvature and PDA functionalization produce opposite interfacial energy flow regimes: high-curvature CNTs create dissipative, frictional interphases while low-curvature CNTs confine energy in rigid, cohesive shells due to curvature-induced transition in PDA adsorption geometry.

Conclusion: Curvature is a fundamental design parameter for engineering polymer-nanotube interfaces, enabling predictive tuning of interfacial energy flow, mechanical resilience, and transport properties beyond conventional chemical functionalization.

Abstract: Carbon nanotube (CNT)-polymer composites are widely engineered using surface coatings and chemical treatments to improve interfacial bonding and load transfer. It has been suggested in the nanocomposite literature that nanotube curvature, in conjunction with surface functionalization such as polydopamine (PDA) coating, could serve as an additional control knob for tuning interfacial bonding and energy dissipation in polymer-CNT systems. While experimental and simulation studies have demonstrated the benefits of PDA functionalization, the fundamental mechanism by which nanotube curvature modulates interfacial energy flow and mechanical polarity remains unresolved. This gap is sharpened by a persistent paradox: identical PDA functionalization strengthens some CNT-polymer systems while weakening others, a curvature-dependent inconsistency that has remained unexplained. Here, we employ reactive molecular dynamics (ReaxFF) simulations to resolve how curvature and PDA functionalization jointly govern interfacial energy evolution in CNT-polyvinyl alcohol (PVA) nanocomposites. Our investigation reveals that curvature and PDA functionalization jointly produce opposite regimes of interfacial energy flow: high-curvature CNTs generate dissipative, frictional interphases, whereas low-curvature CNTs confine energy in rigid, cohesive shells. This polarity inversion originates from a curvature-induced transition in PDA adsorption geometry that transforms the interphase from an energy-releasing to an energy-storing configuration. These results establish curvature as a fundamental design parameter for engineering polymer-nanotube interfaces, offering a predictive route to tune interfacial energy flow, mechanical resilience, and transport properties beyond the limits of conventional chemical functionalization.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [73] [Iterative improvement of free energy landscape reconstructions with optimal protocols derived from differentiable simulations](https://arxiv.org/abs/2511.17831)
*Oliver Cheng,Zosia Adamska,Michael P. Brenner,Megan C. Engel*

Main category: physics.bio-ph

TL;DR: An iterative algorithm for optimizing free energy landscape reconstructions without requiring prior knowledge of the landscape, using experimental data and automatic differentiation to derive optimal control protocols.


<details>
  <summary>Details</summary>
Motivation: Current approaches for deriving optimal driving protocols to estimate free energy landscapes require a priori knowledge of the landscape, which limits their practical application to unknown systems.

Method: Iterative algorithm that: (i) takes trajectory data, (ii) reconstructs approximate energy landscape, (iii) derives optimal control protocols using automatic differentiation on Brownian dynamics simulations, (iv) re-runs experiments with updated protocol, and (v) iterates until convergence.

Result: Successfully recovered known benchmarks and probed far-from-equilibrium regimes for various energy landscapes. Control protocols derived without prior knowledge substantially reduced variance and bias in reconstructions compared to naive linear protocols.

Conclusion: The approach enables optimized free energy landscape reconstructions without requiring prior landscape knowledge, providing substantially improved accuracy over conventional methods.

Abstract: Free energy landscapes encode the kinetics, intermediates, and transition states that govern molecular processes and are thus a key target of single biomolecule research. Typical approaches to deriving optimal, error-minimizing, non-equilibrium driving protocols for estimating these landscapes require a priori knowledge of the landscape. Here, we present an alternative: an iterative algorithm for optimizing full free energy landscape reconstructions which can be used alongside experiments on unknown landscapes. Our approach (i) takes experimental or simulated trajectory data; (ii) reconstructs an `approximate' energy landscape; (iii) derives optimal control protocols from low-dimensional differentiable Brownian dynamics simulations on the candidate landscape using automatic differentiation; (iv) re-runs the experiment or simulation using the updated protocol; and (v) iterates until convergence. Using this approach, we recover known benchmarks from the literature and probe far-from-equilibrium regimes for symmetric, asymmetric, and triple-well energy landscapes under both 1- and 2-dimensional control. Our control protocols -- derived with no a priori knowledge of the energy landscape -- yield substantially reduced variance and bias in free energy landscape reconstructions compared to naive linear protocols.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [74] [Solving Equilibrium Problem with New Inertial Technique](https://arxiv.org/abs/2511.18642)
*Chidi Elijah Nwakpa,Chinedu Izuchukwu,Chibueze CHristian Okeke,Dilber Uzun Ozsahin,Abubakar Adamu*

Main category: math.OC

TL;DR: A subgradient extragradient method with inertial and correction terms for solving equilibrium problems in Hilbert spaces, achieving weak convergence for pseudomonotone functions and linear convergence for strongly pseudomonotone cases.


<details>
  <summary>Details</summary>
Motivation: To develop an improved optimization method for equilibrium problems that combines inertial techniques with multiple correction terms to enhance convergence performance.

Method: Proposed a subgradient extragradient method incorporating inertial and multiple correction terms for solving equilibrium problems in real Hilbert spaces.

Result: The method achieves weak convergence to solutions for pseudomonotone functions with Lipschitz conditions, and linear convergence rate for strongly pseudomonotone cases. Numerical experiments show significant improvement over existing methods.

Conclusion: The incorporation of inertial and multiple correction terms in the subgradient extragradient method significantly enhances convergence performance for equilibrium problems, with proven convergence guarantees and superior numerical performance compared to existing approaches.

Abstract: We propose in this work a subgradient extragradient method with inertial and correction terms for solving equilibrium problems in a real Hilbert space. We obtain that the sequence generated by our proposed method converges weakly to a point in the solutions set of the equilibrium problem when the associated bivariate function is pseudomonotone and satisfies Lipschitz conditions. Furthermore, in a case where the bifunction is strongly pseudomonotone, we establish a linear convergence rate. Lastly, through different numerical examples, we demonstrate that the incorporation of multiple correction terms significantly improves our proposed method when compared with other methods in the literature.

</details>


### [75] [Proximal and Contraction method with Relaxed Inertial and Correction Terms for Solving Mixed Variational Inequality Problems](https://arxiv.org/abs/2511.18652)
*Chidi Elijah Nwakpa,Austine Efut Ofem,Kalu Okam Okorie,Chinedu Izuchukwu,Chibueze Christian Okeke*

Main category: math.OC

TL;DR: Proposes an accelerated proximal-contraction method with inertial extrapolation, correction terms, and relaxation for solving convex mixed variational inequalities in Hilbert spaces.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for solving convex mixed variational inequality problems with faster convergence.

Method: Proximal-contraction method enhanced with inertial extrapolation, two correction terms, and relaxation technique.

Result: Achieves weak convergence under mild assumptions and demonstrates effectiveness through numerical examples.

Conclusion: The proposed accelerated method effectively solves convex mixed variational inequalities with improved convergence.

Abstract: We propose in this paper a proximal and contraction method for solving a convex mixed variational inequality problem in a real Hilbert space. To accelerate the convergence of our proposed method, we incorporate an inertial extrapolation term, two correction terms, and a relaxation technique. We therefore obtain a weak convergence result under some mild assumptions. Finally, we present numerical examples to practically demonstrate the effectiveness of the relaxation technique, the inertial extrapolation term, and the correction terms in our proposed method.

</details>


### [76] [A Trust-region Funnel Algorithm for Grey Box Optimisation](https://arxiv.org/abs/2511.18998)
*Gul Hameed,Tao Chen,Antonio del Rio Chanona,Lorenz T. Biegler,Michael Short*

Main category: math.OC

TL;DR: A novel trust-region funnel algorithm for grey-box optimization that replaces filter acceptance with a generalizable funnel approach, providing simpler implementation and comparable/improved performance over classical methods.


<details>
  <summary>Details</summary>
Motivation: Existing trust-region methods for grey-box optimization are complex with extensive parameter tuning requirements and lack open-source implementations, creating barriers for practical application.

Method: Proposes a trust-region funnel algorithm using a uni-dimensional funnel to maintain monotonically non-increasing upper bounds on black-box approximation errors, with global convergence proof to first-order critical points.

Result: Implemented in open-source Pyomo framework, benchmark tests on seven problems show comparable and often improved performance relative to classical trust-region filter method.

Conclusion: The trust-region funnel method provides a simpler, extensible alternative for large-scale grey-box optimization with robust convergence guarantees.

Abstract: Grey-box optimisation, where some parts of an optimisation problem are represented by explicit algebraic (glass-box) models while others are treated as black-box models lacking analytic derivatives, remains a challenge in process systems engineering. Trust-region (TR) methods provide a robust framework for grey-box problems by combining accurate glass-box derivatives with local reduced models (RMs) for black-box components. However, existing TR approaches often involve complex multi-layered formulations requiring extensive parameter tuning, or lack open-source implementations. Motivated by the recent advances in funnel-based convergence theory for nonlinear optimisation and the TR filter method, we propose a novel TR funnel algorithm for grey-box optimisation that replaces the filter acceptance criterion with a generalisable uni-dimensional funnel, maintaining a monotonically non-increasing upper bound on approximation error of the local black-box RMs. A global convergence proof to a first-order critical point is established. The algorithm, implemented in an open-source Pyomo framework, supports multiple RM forms and globalisation strategies (filter or funnel). Benchmark tests on seven numerical and engineering problems show that the TR funnel algorithm achieves comparable and often improved performance relative to the classical TR filter method. The TR funnel method thus provides a simpler, and extensible alternative for large-scale grey-box optimisation.

</details>


### [77] [Deterministic Mean Field Games on Networks and Related Optimal Control Problems](https://arxiv.org/abs/2511.19038)
*Yves Achdou,Claudio Marchi,Nicoletta Tchou*

Main category: math.OC

TL;DR: Analysis of deterministic mean field games and optimal control problems on networks with finite time horizon, focusing on existence of optimal trajectories and relaxed equilibria.


<details>
  <summary>Details</summary>
Motivation: To study mean field games on networks with more general cost assumptions and arbitrary number of vertices, addressing difficulties that arise from this higher generality compared to previous work.

Method: Lagrangian formulation approach for mean field games, analyzing optimal control problems with velocity control on networks, and studying regularity of value functions and probability measures.

Result: Existence of optimal trajectories for control problems, existence of relaxed equilibria as probability measures on trajectories, and correspondence between relaxed equilibria and mild solutions consisting of value functions and probability measure families.

Conclusion: The framework enables analysis of mean field games on networks with non-local costs, establishing connections between optimal control theory and mean field game solutions through viscosity solutions of Hamilton-Jacobi problems and weak Fokker-Planck equations.

Abstract: We study a class of deterministic mean field games and related optimal control problems, with a finite time horizon and in which the state space is a network.
  An agent controls her velocity, and, when she occupies a vertex, she can either remain still or enter any adjacent edge. The running and terminal costs are assumed to be continuous in each edge, but may jump at the vertices. Compared to the companion paper [4], we make more general assumptions about the costs and consider networks with an arbitrary number of vertices; this higher degree of generality brings new difficulties.
  For the optimal control problems mentioned above, we obtain in particular the existence of optimal trajectories and regularity results concerning the optimal trajectories and the value function.
  These control theoretic results make it possible to address a class of mean field games on networks, with costs that do not depend separately on the control and on the distribution of states, and that are non-local with respect to the latter. Focusing on a Lagrangian formulation, we obtain the existence of relaxed equilibria consisting of probability measures on admissible trajectories. To any relaxed equilibrium corresponds a mild solution, i.e. a pair $(u, m)$ made of the value function $u$ of a related optimal control problem and a family $m = (m(t))_t$ of probability measures on the network. Given $m$, the value function $u$ is a viscosity solution of a Hamilton-Jacobi problem on the network. We then investigate the regularity properties of $u$ and a weak form of a Fokker-Planck equation satisfied by $m$.

</details>


### [78] [Families of Lorentzian problems on the Heisenberg group](https://arxiv.org/abs/2511.19140)
*Yu. Sachkov*

Main category: math.OC

TL;DR: Analysis of Lorentzian problems on the Heisenberg group and their asymptotic behavior as parameter approaches limit


<details>
  <summary>Details</summary>
Motivation: To study the behavior of Lorentzian problems on the Heisenberg group under parameter limits

Method: Consider two families of Lorentzian problems and analyze their asymptotic behavior as parameter tends to limit

Result: Not specified in abstract

Conclusion: Not specified in abstract

Abstract: We consider two families of Lorentzian problems on the Heisenberg group and their asymptotic behaviour as the parameter of a family tends to a limit.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [79] [Neural network approximation of regularized density functionals](https://arxiv.org/abs/2511.18512)
*Mih√°ly A. Csirik,Andre Laestadius,Mathias Oster*

Main category: physics.chem-ph

TL;DR: Proposes a mathematically consistent neural network approximation for the universal density functional in DFT, using Moreau-Yosida regularization to ensure continuity and differentiability.


<details>
  <summary>Details</summary>
Motivation: Current DFT approximations lack first-principles, mathematically consistent procedures. The universal density functional is complex and typically replaced with approximations that don't guarantee convergence or mathematical rigor.

Method: Apply Moreau-Yosida regularization to make exact functionals continuous/differentiable, then approximate the regularized functional using neural networks that preserve positivity and convexity.

Result: Developed a differentiable neural network functional that maintains key mathematical properties (positivity, convexity) and can be directly used in Kohn-Sham calculations.

Conclusion: Provides a first-principles, mathematically consistent approximation procedure for DFT that overcomes limitations of current approaches and enables differentiable functionals for practical quantum calculations.

Abstract: Density functional theory is one of the most efficient and widely used computational methods of quantum mechanics, especially in fields such as solid state physics and quantum chemistry. From the theoretical perspecive, its central object is the universal density functional which contains all intrinsic information about the quantum system in question. Once the external potential is provided, in principle one can obtain the exact ground-state energy via a simple minimization. However, the universal density functional is a very complicated mathematical object and almost always it is replaced with its approximate variants. So far, no ``first principles'', mathematically consistent and convergent approximation procedure has been devised that has general applicability. In this paper, we propose such a procedure by first applying Moreau--Yosida regularization to make the exact functionals continuous (even differentiable) and then approximate the regularized functional by a neural network. The resulting neural network preserves the positivity and convexity of the exact functionals. More importantly, it is differentiable, so it can be directly used in a Kohn--Sham calculation.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [80] [Entity -- Hardware-agnostic Particle-in-Cell Code for Plasma Astrophysics. I: Curvilinear Special Relativistic Module](https://arxiv.org/abs/2511.17710)
*Hayk Hakobyan,Ludwig M. B√∂ss,Yangyang Cai,Alexander Chernoglazov,Alisa Galishnikova,Evgeny A. Gorbunov,Jens F. Mahlmann,Alexander Philippov,Siddhant Solanki,Arno Vanthieghem,Muni Zhou*

Main category: astro-ph.HE

TL;DR: Entity is an open-source PIC code that achieves hardware-agnostic performance across GPUs/CPUs using Kokkos, with general-coordinate relativistic capabilities enabling coordinate-agnostic plasma modeling.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in astrophysical plasma modeling, particularly extreme scale separation and performance challenges with evolving GPU-centric computing infrastructures.

Method: Solves Vlasov-Maxwell system in general coordinates using particle equations of motion in global orthonormal Cartesian basis, with charge conservation via specialized conformal current deposition.

Result: Achieves robust scalability on major GPU platforms with particle pusher and current deposition operating at ~2ns per particle per timestep, validated through standard plasma tests and relativistic magnetosphere modeling.

Conclusion: Entity is the first PIC code with general-coordinate capabilities, providing a coordinate-agnostic framework for astrophysical plasma modeling with high performance portability and extensibility.

Abstract: Entity is a new-generation, fully open-source particle-in-cell (PIC) code developed to overcome key limitations in astrophysical plasma modeling, particularly the extreme separation of scales and the performance challenges associated with evolving, GPU-centric computing infrastructures. It achieves hardware-agnostic performance portability across various GPU and CPU architectures using the Kokkos library. Crucially, Entity maintains a high standard for usability, clarity, and customizability, offering a robust and easy-to-use framework for developing new algorithms and grid geometries, which allows extensive control without requiring edits to the core source code. This paper details the core general-coordinate special-relativistic module. Entity is the first PIC code designed to solve the Vlasov-Maxwell system in general coordinates, enabling a coordinate-agnostic framework that provides the foundational structure for straightforward extension to arbitrary coordinate geometries. The core methodology achieves numerical stability by solving particle equations of motion in the global orthonormal Cartesian basis, despite using generalized coordinates like Cartesian, axisymmetric spherical, and quasi-spherical grids. Charge conservation is ensured via a specialized current deposition technique using conformal currents. The code exhibits robust scalability and performance portability on major GPU platforms (AMD MI250X, NVIDIA A100, and Intel Max Series), with the 3D particle pusher and the current deposition operating efficiently at about 2 nanoseconds per particle per timestep. Functionality is validated through a comprehensive suite of standard Cartesian plasma tests and the accurate modeling of relativistic magnetospheres in curvilinear axisymmetric geometries.

</details>


### [81] [Guesswork in the gap: the impact of uncertainty in the compact binary population on source classification](https://arxiv.org/abs/2511.19393)
*Utkarsh Mali,Reed Essick*

Main category: astro-ph.HE

TL;DR: Analysis of 66 GWTC-3 events shows neutron star classification probability (P(NS)) varies significantly based on pairing preferences and spin distributions, with GW230529's primary having 1%-67% P(NS) and GW190425's primary 51%-100% P(NS).


<details>
  <summary>Details</summary>
Motivation: To address uncertainty in classifying compact objects in the 'lower mass gap' and understand how gravitational wave observations distinguish neutron stars from black holes given measurement noise, population models, and EOS constraints.

Method: Analyzed 66 confident gravitational wave events from GWTC-3, quantifying how neutron star probability varies across population parameters including pairing preferences with other compact objects and neutron star spin distributions.

Result: P(NS) varies substantially: GW230529's primary has 1%-67% probability, GW190425's primary 51%-100%, while GW190814's secondary shows <10% variation due to high SNR and small mass ratio. EOS affects P(NS) mainly through maximum neutron star mass rather than maximum spin.

Conclusion: Current P(NS) estimates remain highly sensitive to population parameters, limiting reliability and potentially causing ambiguous classifications of future gravitational wave events.

Abstract: The nature of the compact objects within the supposed "lower mass gap" remains uncertain. Observations of GW190814 and GW230529 highlight the challenges gravitational waves face in distinguishing neutron stars from black holes. Interpreting these systems is especially difficult because classifications depend simultaneously on measurement noise, compact binary population models, and equation of state (EOS) constraints on the maximum neutron star mass. We analyze 66 confident events from GWTC-3 to quantify how the probability of a component being a neutron star, P(NS), varies across the population. The effects are substantial, the dominant drivers of classification are the pairing preferences of neutron stars with other compact objects, and the neutron star spin distributions. The data reveals that P(NS) varies between 1% - 67% for GW230529's primary and between 51% - 100% for GW190425's primary. By contrast, P(NS) for GW190814's secondary varies by <10%, demonstrating robustness from its high signal-to-noise ratio and small mass ratio. Analysis using EOS information tends to affect P(NS) through the inferred maximum neutron star mass rather than the maximum spin. As it stands, P(NS) remains sensitive to numerous population parameters, limiting its reliability and potentially leading to ambiguous classifications of future GW events.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [82] [Reduced-Basis Deep Operator Learning for Parametric PDEs with Independently Varying Boundary and Source Data](https://arxiv.org/abs/2511.18260)
*Yueqi Wang,Guang Lin*

Main category: cs.LG

TL;DR: RB-DeepONet is a hybrid operator-learning framework that combines reduced-basis numerical methods with DeepONet architecture, providing interpretable, stable, and efficient solutions for parametric PDEs with certified error control.


<details>
  <summary>Details</summary>
Motivation: Existing operator-learning approaches for parametric PDEs often lack interpretability, require extensive labeled data, or fail when boundary/source data vary independently from physical parameters.

Method: Fuses reduced-basis structure with DeepONet architecture using fixed RB trunks generated offline via Greedy selection, branch networks predicting RB coefficients trained label-free with projected variational residual, and boundary/source modal encodings for independently varying loads.

Result: Achieves accuracy competitive with intrusive RB-Galerkin, POD-DeepONet, and FEONet while using dramatically fewer parameters and significant speedups, with convergence guarantees separating RB approximation from learning error.

Conclusion: RB-DeepONet establishes an efficient, stable, and interpretable operator learner for large-scale parametric PDEs with strict offline-online computational split.

Abstract: Parametric PDEs power modern simulation, design, and digital-twin systems, yet their many-query workloads still hinge on repeatedly solving large finite-element systems. Existing operator-learning approaches accelerate this process but often rely on opaque learned trunks, require extensive labeled data, or break down when boundary and source data vary independently from physical parameters. We introduce RB-DeepONet, a hybrid operator-learning framework that fuses reduced-basis (RB) numerical structure with the branch-trunk architecture of DeepONet. The trunk is fixed to a rigorously constructed RB space generated offline via Greedy selection, granting physical interpretability, stability, and certified error control. The branch network predicts only RB coefficients and is trained label-free using a projected variational residual that targets the RB-Galerkin solution. For problems with independently varying loads or boundary conditions, we develop boundary and source modal encodings that compress exogenous data into low-dimensional coordinates while preserving accuracy. Combined with affine or empirical interpolation decompositions, RB-DeepONet achieves a strict offline-online split: all heavy lifting occurs offline, and online evaluation scales only with the RB dimension rather than the full mesh. We provide convergence guarantees separating RB approximation error from statistical learning error, and numerical experiments show that RB-DeepONet attains accuracy competitive with intrusive RB-Galerkin, POD-DeepONet, and FEONet while using dramatically fewer trainable parameters and achieving significant speedups. This establishes RB-DeepONet as an efficient, stable, and interpretable operator learner for large-scale parametric PDEs.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [83] [A modified Consensus-Based Optimization model: consensus formation and uniform-in-time propagation of chaos](https://arxiv.org/abs/2511.19116)
*Young-Pil Choi,Seungchan Lee,Sihyun Song*

Main category: math.PR

TL;DR: A modified Consensus-Based Optimization model with regularized Gibbs weight that stabilizes consensus and avoids degeneracies, enabling unified analysis of finite-particle dynamics, McKean-Vlasov equation, and optimization behavior under relaxed assumptions.


<details>
  <summary>Details</summary>
Motivation: To develop a rigorous theoretical framework for consensus-based optimization that eliminates the need for cutoffs, rescaling, or boundedness assumptions on objective functions, addressing degeneracies in classical formulations.

Method: Introduces a modified CBO model with regularized Gibbs weight that stabilizes the consensus point and avoids degeneracies. Analyzes finite-particle dynamics, McKean-Vlasov equation, and their optimization behavior under a single structural framework.

Result: 1) Large-time consensus: particles converge exponentially to common random limit near global minimizer when drift exceeds explicit threshold; 2) Uniform-in-time propagation of chaos: dimension-free convergence of empirical measure to McKean-Vlasov dynamics; 3) Mean-field system reaches deterministic consensus with consensus point approaching global minimizer for highly concentrated Gibbs weights.

Conclusion: Provides a unified and internally consistent theoretical framework for consensus-based optimization under substantially relaxed regularity assumptions on objective functions, with rigorous guarantees for consensus formation and optimization performance.

Abstract: We introduce a modified Consensus-Based Optimization model that admits a fully unified and rigorous analysis of its finite-particle dynamics, the associated McKean--Vlasov equation, and their optimization behavior under a single set of structural framework. The key ingredient is a regularized Gibbs weight that stabilizes the consensus point and avoids degeneracies present in the classical formulation, eliminating the need for cutoffs, rescaling, or boundedness assumptions on the objective function. Our first main result establishes large-time consensus for the particle system: when the drift exceeds an explicit threshold, all particles converge exponentially to a common random limit that concentrates near the global minimizer. Our second result proves uniform-in-time propagation of chaos, providing quantitative and dimension-free convergence of the empirical measure to the McKean--Vlasov dynamics. Finally, we show that the mean-field system reaches deterministic consensus and that its consensus point approaches the global minimizer in the regime of highly concentrated Gibbs weights. Together, these results yield a unified and internally consistent theoretical framework for consensus-based optimization under substantially relaxed regularity assumptions on the objective function.

</details>


### [84] [Transportation cost inequalities for singular SPDEs](https://arxiv.org/abs/2511.19216)
*I. Bailleul,M. Hoshino,R. Takano*

Main category: math.PR

TL;DR: The paper proves that BPHZ random models satisfy transportation cost inequalities in the full subcritical regime, extending these inequalities to solutions of singular SPDEs including Œ¶^4_{4-Œ¥} measures.


<details>
  <summary>Details</summary>
Motivation: To establish transportation cost inequalities for BPHZ random models and extend these results to solutions of singular stochastic partial differential equations.

Method: Mathematical proof showing that under conditions of no variance blowup, translation invariant noise, and existing transportation cost inequalities, the laws satisfy transportation cost inequalities in the subcritical regime.

Result: Proved that BPHZ random models satisfy transportation cost inequalities in full subcritical regime, and consequently these inequalities hold for solutions to singular SPDEs including Œ¶^4_{4-Œ¥} measures on 4D torus for all 0<Œ¥<4.

Conclusion: Transportation cost inequalities are established for BPHZ models and extended to singular SPDE solutions, providing important mathematical properties for these stochastic models.

Abstract: We prove that the laws of the BPHZ random models satisfy some transportation cost inequalities in the full subcritical regime if there is no variance blowup and the law of the noise is translation invariant and satisfies some transportation cost inequality. As a consequence, the laws of a number of solutions to some singular stochastic partial differential equations also satisfy some transportation cost inequalities. This is in particular the case of the $Œ¶^4_{4-Œ¥}$ measures over the 4-dimensional torus, for all $0<Œ¥<4$.

</details>


### [85] [Chernoff-Mehler Approximation for L√©vy Processes with Drift](https://arxiv.org/abs/2511.19414)
*Max Nendel*

Main category: math.PR

TL;DR: The paper develops an approximation scheme for L√©vy processes with drift using a Chernoff product formula approach, providing convergence criteria for probability measure families and extending to processes with deterministic components.


<details>
  <summary>Details</summary>
Motivation: To establish a general approximation framework for L√©vy processes with drift that can handle various numerical schemes and provide explicit convergence criteria.

Method: Uses Chernoff product formula on bounded continuous functions with mixed topology, analyzes convergence of probability measure families, and enriches measures with deterministic components.

Result: Provides sufficient and necessary conditions for convergence of subsequences and entire families, with applications to Lipschitz ODE flows, Euler schemes, Runge-Kutta methods, and Central Limit Theorem.

Conclusion: The framework successfully approximates L√©vy processes with drift and encompasses various numerical methods under unified convergence criteria.

Abstract: In this paper, we study an approximation scheme for L√©vy processes with drift in terms of a representation that is akin to the celebrated Mehler formula for L√©vy-Ornstein-Uhlenbeck processes. The approximation scheme is based on a variant of the Chernoff product formula on the space of bounded continuous functions. In a first step, we provide sufficient and necessary conditions for arbitrary families of probability measures, indexed by positive real numbers, to give rise to a convolution semigroup via a Chernoff approximation on the space of bounded continuous functions, equipped with the mixed topology. In this context, we provide explicit criteria both for the convergence of subsequences and the entire family, and discuss fine properties related to the domain of the associated generator of the L√©vy process and the infinitesimal behavior of the approximating family of measures. In a second step, we enrich the family of measures by a deterministic component and derive explicit conditions that ensure both the convergence of subsequences and the entire family to a L√©vy process with drift under a Chernoff approximation. In a series of examples, we show that our general conditions on the dynamics are satisfied, for example, by flows of Lipschitz ordinary differential equations, Euler schemes, and arbitrary Runge-Kutta methods, and that the Central Limit Theorem can be subsumed under our framework.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [86] [Local Laws and Fluctuations for Super-Coulombic Riesz Gases](https://arxiv.org/abs/2511.18623)
*Luke Peilen,Sylvia Serfaty*

Main category: math-ph

TL;DR: Local statistical analysis of super-Coulombic Riesz gases with inverse power repulsion, proving microscopic local laws, fluctuation controls, and deriving CLT for small inverse powers at mesoscopic scales.


<details>
  <summary>Details</summary>
Motivation: To understand the local statistical behavior of Riesz gases with nonlocal interactions, extending beyond Coulomb cases to more general repulsive potentials, and addressing challenges from nonlocality.

Method: Bootstrap procedure to prove local laws on energy and fluctuations, analyzing degenerate singular elliptic PDEs, and developing transport approach for nonlocal interactions.

Result: Established local laws valid down to microscopic scales, controls on particle numbers, limit point processes, and derived CLT for Riesz gases with small inverse powers at mesoscopic scales.

Conclusion: Successfully extended statistical analysis to nonlocal Riesz gases, overcoming technical challenges from nonlocality, and demonstrated convergence to fractional Gaussian fields in appropriate regimes.

Abstract: We study the local statistical behavior of the super-Coulombic Riesz gas of particles in Euclidean space of arbitrary dimension, with inverse power distance repulsion integrable near $0$, and with a general confinement potential, in a certain regime of inverse temperature. Using a bootstrap procedure, we prove local laws on the next order energy and control on fluctuations of linear statistics that are valid down to the microscopic lengthscale, and provide controls for instance, on the number of particles in a (mesoscopic or microscopic) box, and the existence of a limit point process up to subsequences.
  As a consequence of the local laws, we derive an almost additivity of the free energy that allows us to exhibit for the first time a CLT for Riesz gases corresponding to small enough inverse powers, at small mesoscopic length scales, which can be interpreted as the convergence of the associated potential to a fractional Gaussian field.
  Compared to the Coulomb interaction case, the main new issues arise from the nonlocal aspect of the Riesz kernel. This manifests in (i) a novel technical difficulty in generalizing the transport approach of Lebl√© and the second author to the Riesz gas which now requires analyzing a degenerate and singular elliptic PDE, (ii) the fact that the transport map is not localized, which makes it more delicate to localize the estimates, (iii) the need for coupling the local laws and the fluctuations control inside the same bootstrap procedure.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [87] [Dynamical interplay between coupled scalar dark sectors and gravity](https://arxiv.org/abs/2511.18585)
*Mihai Marciu*

Main category: gr-qc

TL;DR: A cosmological model with coupled axion-dilaton fields non-minimally interacting with gravity, analyzed using linear stability theory to show compatibility with cosmic history and potential solutions to the coincidence problem.


<details>
  <summary>Details</summary>
Motivation: To extend recent axion-dilaton systems by introducing viable couplings with spacetime geometry through scalar curvature, exploring novel interactions between gravity and scalar fields in scalar tensor theories.

Method: Employ linear stability theory to investigate physical properties after introducing the action and field equations for the coupled axion-dilaton system non-minimally coupled with gravity.

Result: The model shows compatibility with the recent history of the Universe and provides viable constraints for model parameters in specific cases, with distinct phase-space structure features.

Conclusion: The independent non-minimal coupling of axion-dilaton system with gravity offers potential alleviation of the cosmic coincidence problem through distinct physical features in phase-space structure.

Abstract: We explore a novel cosmological model based on coupled fields in the framework of scalar tensor theories, considering the specific interplay between gravity and scalar fields. The model further extends a recent axion-dilaton system by introducing viable couplings with the space--time geometry encoded into the scalar curvature. After briefly introducing the action and the corresponding field equations, we employ linear stability theory to investigate the physical properties. The analysis showed the compatibility of the current theoretical model with the recent history of the Universe, obtaining viable constraints for the model's parameters in some specific cases. In the present setup, the axion--dilaton system is non--minimally coupled with gravity in an independent manner, leading to distinct physical features in the phase-space structure, possible alleviating the cosmic coincidence problem.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [88] [Hyperbolic Dispersion and Low-Frequency Plasmons in Electrides](https://arxiv.org/abs/2511.17859)
*Qi-Dong Hao,Hao Wang,Hong-Xing Song,Xiang-Rong Chen,Hua Y. Geng*

Main category: physics.optics

TL;DR: Non-cubic electrides are identified as promising natural hyperbolic materials due to charge localization in interstitial sites, challenging the previous requirement of structural anisotropy for hyperbolic dispersion.


<details>
  <summary>Details</summary>
Motivation: To explore natural hyperbolic materials beyond the limited set of structurally anisotropic crystals, leveraging the unique properties of electrides where electrons are localized in interstitial sites.

Method: Analysis of non-cubic electrides including elemental, binary, and two-dimensional materials, examining their charge localization properties and optical dispersion characteristics.

Result: Electrides exhibit low plasma frequencies and broad hyperbolic windows spanning infrared to ultraviolet wavelengths, with semiconductor electrides showing additional hyperbolic behavior through anisotropic interband transitions.

Conclusion: The study removes the prerequisite of structural anisotropy for natural hyperbolic materials, opening new opportunities for discovering and designing photonic materials based on electride properties.

Abstract: Natural hyperbolic materials have attracted significant interest in the field of photonics due to their unique optical properties. Based on the initial successful explorations on layered crystalline materials, hyperbolic dispersion was associated with extreme structural anisotropy, despite the rarity of natural materials exhibiting this property. Here we show that non cubic electrides are generally promising natural hyperbolic materials owing to charge localization in interstitial sites. This includes elemental and binary electrides, as well as some two-dimensional materials that show prominent in-plane hyperbolic dispersion. They exhibit low plasma frequencies and a broad hyperbolic window spanning the infrared to the ultraviolet. In semiconductor electrides, anisotropic interband transitions provide an additional mechanism for hyperbolic behaviour. These findings remove the previously held prerequisite of structural anisotropy for natural hyperbolic materials, and open up new opportunities, which might change the current strategy for searching and design photonic materials.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [89] [Weighted graphs in the sense of John and a global Poincar√© inequality](https://arxiv.org/abs/2511.17921)
*Fernando L√≥pez-Garc√≠a,John Rodriguez*

Main category: math.CA

TL;DR: Establishes a condition on weighted graphs with finite measure that ensures global Poincar√© inequality validity, serving as a discrete analogue to Boman's criterion for Whitney cubes.


<details>
  <summary>Details</summary>
Motivation: To develop a discrete counterpart to classical Poincar√© inequality criteria, bridging continuous and discrete settings by adapting Boman's Whitney cube criterion to weighted graphs.

Method: Proposes a specific condition on weighted graphs with finite measure, drawing inspiration from J. Boman's 1982 criterion for Whitney cubes, which itself characterizes F. John's 1961 condition.

Result: Successfully establishes a condition that guarantees the validity of global Poincar√© inequality on weighted graphs with finite measure.

Conclusion: The paper provides a discrete analogue to classical Poincar√© inequality criteria, extending the theoretical framework from continuous settings to weighted graphs while maintaining connections to foundational work by John and Boman.

Abstract: In this paper, we establish a condition on weighted graphs with finite measure that guarantees the validity of a global Poincar√© inequality. This condition can be viewed as a discrete analogue of the criterion introduced by J. Boman in 1982 for Whitney cubes, which in turn characterizes the condition originally proposed by F. John in his seminal 1961 work.

</details>


### [90] [Growth Estimates for Solutions to the Wave Equation on Damek--Ricci Spaces](https://arxiv.org/abs/2511.18995)
*Yunxiang Wang,Lixin Yan,Hong-Wei Zhang*

Main category: math.CA

TL;DR: Establishes sharp L^p bounds for wave equations on Damek-Ricci spaces with critical regularity exponents, proving a conjecture by M√ºller, Thiele, and Vallarino.


<details>
  <summary>Details</summary>
Motivation: To prove the conjecture raised by M√ºller, Thiele, and Vallarino regarding sharp-in-regularity L^p bounds for wave equations on Damek-Ricci spaces.

Method: Analyzes the wave equation with the left-invariant distinguished Laplacian on Damek-Ricci spaces using right Haar measure and establishes estimates for solution norms.

Result: Proves the sharp L^p bounds for wave equation solutions with critical exponents Œ±‚ÇÄ = n|1/p-1/2| and Œ±‚ÇÅ = n|1/p-1/2|-1 for all t‚àà‚Ñù* and 1<p<‚àû.

Conclusion: The conjecture is settled in full generality, establishing optimal regularity requirements for wave equations on Damek-Ricci spaces.

Abstract: Let $\mathcal{L}$ be the left-invariant distinguished Laplacian, and let $\mathrm{d}œÅ$ denote the right Haar measure on a Damek--Ricci space $S$. Let $u(t,x)$ denote the solution to the wave equation $\partial_t^2 u-\mathcal{L} u=0$ with initial data $(u,\partial_t u)|_{t=0}=(f,g)$. In this paper, we establish the sharp-in-regularity $L^p$ bounds \begin{align*} \|u(t,\cdot)\|_{L^p(S ,\mathrm{d}œÅ)} \lesssim_p (1+|t|)^{2|\frac{1}{p}-\frac{1}{2}|}\|(\mathrm{Id}+\mathcal{L})^{\frac{Œ±_0}{2}}\!f\|_{L^p(S ,\mathrm{d}œÅ)}+(1+|t|)\,\|(\mathrm{Id}+\mathcal{L})^{\frac{Œ±_1}{2}}\!g\|_{L^p(S,\mathrm{d}œÅ)} \end{align*} for all $t\in\mathbb{R}^*$ and $1<p<\infty$, where the exponents $Œ±_0 = n\left|1/p-1/2\right|$ and $Œ±_1 = n\left|1/p-1/2\right| -1$ attain their critical values. This result settles, in full generality, the conjecture raised by M√ºller, Thiele, and Vallarino.

</details>
