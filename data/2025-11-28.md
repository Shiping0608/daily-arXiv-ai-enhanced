<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 19]
- [math.AP](#math.AP) [Total: 25]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [math.DG](#math.DG) [Total: 2]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cond-mat.supr-con](#cond-mat.supr-con) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A Modified BGK Collision Operator for Exact Conservation in Numerical Solutions of Boltzmann-BGK](https://arxiv.org/abs/2511.20664)
*Vienna B. Rossmanith*

Main category: math.NA

TL;DR: A novel numerical method for solving the 1D1V Boltzmann-BGK equation that combines operator splitting, third-order transport scheme, second-order collision scheme, and conservation-enforcing polynomial modification.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate, efficient, and robust numerical method for simulating gas dynamics using the Boltzmann equation that works across all flow regimes while ensuring exact conservation of mass, momentum, and energy.

Method: Operator splitting separates transport and collision steps. Uses third-order Lax-Wendroff-type scheme for transport and second-order L-stable TR-BDF method for collision. Introduces quadratic Hermite polynomial modification to Maxwell-Boltzmann distribution to enforce exact conservation despite truncated velocity range and quadrature errors.

Result: The method achieves exact conservation of mass, momentum, and energy up to machine precision, verified on piecewise constant initial data with periodic boundary conditions.

Conclusion: The developed scheme provides an accurate and robust numerical method for the Boltzmann-BGK equation with guaranteed conservation properties, implemented in freely available Java code with Python plotting.

Abstract: Ideal gases can be modeled by the Boltzmann equation from statistical physics. Instead of trying to track the position and velocity of a large number of gas molecules, it is possible to describe the particles with a particle distribution function. The Boltzmann equation provides the rule for evolving the distribution function over time, allowing one to simulate the gas dynamics. In this work, we develop a novel numerical method for solving the 1D1V Boltzmann-BGK equation. Several important ingredients are combined to create an accurate, efficient, robust numerical method valid in all flow regimes. First, we make use of operator splitting to create separate transport and collision sub-steps, each of which is easier to discretize than the whole system; second, we introduce a third-order accurate Lax-Wendroff-type scheme for the transport sub-step; third, we make use of the second-order L-stable TR-BDF method for the collision sub-step. The final component we introduce is a novel treatment of the collision sub-step to guarantee that the total mass, momentum, and energy are conserved even in the presence of a truncated velocity range and quadrature errors in computing moments. The key idea is to multiply the Maxwell-Boltzmann distribution in the collision sub-step by a quadratic Hermite polynomial, where the coefficients in the polynomial are chosen to ensure exact conservation. The resulting scheme is verified on piecewise constant initial data with periodic boundary conditions; exact conservation up to machine precision is demonstrated for mass, momentum, and energy. The resulting method is implemented in a freely available Java code with Python plotting routines.

</details>


### [2] [Inverse Electromagnetic Scattering for Doubly-Connected Cylinders using Convolutional Neural Networks](https://arxiv.org/abs/2511.20681)
*Leonidas Mindrinos,Nikolaos Pallikarakis,Nikolaos L Tsitsas*

Main category: math.NA

TL;DR: A divide-and-conquer framework using 1D multi-channel CNNs with circular padding solves the inverse electromagnetic scattering problem for magneto-dielectric cylinders covering impedance cylinders.


<details>
  <summary>Details</summary>
Motivation: To address the inverse electromagnetic scattering problem for complex magneto-dielectric cylinders covering impedance cylinders of arbitrary shape, which is challenging to solve with traditional methods.

Method: Uses a divide-and-conquer framework with specially designed 1D multi-channel convolutional neural networks with circular padding. First classifies the impedance cylinder shape, then reconstructs the boundary curve and impedance function using far-field measurements from the direct problem.

Result: Extensive numerical experiments, including noisy scenarios, demonstrate the efficiency and robustness of the approach in accurately reconstructing boundary curves and impedance functions.

Conclusion: The proposed CNN-based divide-and-conquer framework provides an effective and robust solution for inverse electromagnetic scattering problems involving complex magneto-dielectric and impedance cylinder configurations.

Abstract: In this work, we consider the inverse electromagnetic scattering problem for a magneto-dielectric cylinder covering an impedance cylinder of arbitrary shape. We solve it by introducing a divide-and-conquer framework using specially designed 1D multi-channel, circular-padding Convolutional Neural Networks. The solution of the direct problem provides us with the real and imaginary components of the far-field measurements representing the input data. We first classify the shape of the impedance cylinder and then reconstruct the unknown boundary curve and the impedance function. Through extensive numerical experiments, including noisy scenarios, we demonstrate the efficiency and robustness of our approach.

</details>


### [3] [Dual-Domain Deep Learning Method to Accelerate Local Basis Functions Computation for Reservoir Simulation in High-Contrast Porous Media](https://arxiv.org/abs/2511.20685)
*Peiqi Li,Jie Chen*

Main category: math.NA

TL;DR: A dual-domain deep learning framework accelerates multiscale basis function computation in MGMsFEM for Darcy flow problems by extracting permeability field features in frequency and spatial domains.


<details>
  <summary>Details</summary>
Motivation: Darcy flow in heterogeneous porous media has pronounced multiscale characteristics that challenge conventional numerical methods with high computational demands. While MGMsFEM provides an effective framework, constructing multiscale basis functions remains computationally expensive.

Method: Proposes a dual-domain deep learning framework that extracts and decodes permeability field features in both frequency and spatial domains to rapidly generate numerical matrices of multiscale basis functions for MGMsFEM.

Result: Numerical experiments demonstrate significant computational acceleration while maintaining high approximation accuracy.

Conclusion: The framework offers potential for future applications in real-world reservoir engineering by enabling efficient computation of multiscale basis functions.

Abstract: In energy science, Darcy flow in heterogeneous porous media is a central problem in reservoir sim-ulation. However, the pronounced multiscale characteristics of such media pose significant challenges to conventional numerical methods in terms of computational demand and efficiency. The Mixed Generalized Multiscale Finite Element Method (MGMsFEM) provides an effective framework for addressing these challenges, yet the construction of multiscale basis functions remains computationally expensive. In this work, we propose a dual-domain deep learning framework to accelerate the computation of multiscale basis functions within MGMsFEM for solving Darcy flow problems. By extracting and decoding permeability field features in both the frequency and spatial domains, the method enables rapid generation of numerical matrices of multiscale basis functions. Numerical experiments demonstrate that the proposed framework achieves significant computational acceleration while maintaining high approximation accuracy, thereby offering the potential for future applications in real-world reservoir engineering.

</details>


### [4] [Hybrid coupling with operator inference and the overlapping Schwarz alternating method](https://arxiv.org/abs/2511.20687)
*Irina Tezaur,Eric Parish,Anthony Gruber,Ian Moore,Christopher Wentland,Alejandro Mota*

Main category: math.NA

TL;DR: A hybrid approach coupling local non-intrusive Operator Inference ROMs with FOMs using overlapping Schwarz alternating method for efficient multiscale simulations.


<details>
  <summary>Details</summary>
Motivation: Address long runtime and complex mesh generation challenges in traditional high-fidelity simulations for multiscale modeling.

Method: Hybrid approach combining subdomain-local OpInf ROMs with FOMs using overlapping Schwarz alternating method, enabling integration of disparate models and meshes.

Result: Achieved up to 106x speedup compared to conventional FOM-FOM couplings in 3D solid dynamics problems while maintaining high accuracy.

Conclusion: The method enables efficient simulation workflows with potential extensions to various PDEs, paving way for improved engineering applications.

Abstract: This paper presents a novel hybrid approach for coupling subdomain-local non-intrusive Operator Inference (OpInf) reduced order models (ROMs) with each other and with subdomain-local high-fidelity full order models (FOMs) with using the overlapping Schwarz alternating method (O-SAM). The proposed methodology addresses significant challenges in multiscale modeling and simulation, particularly the long runtime and complex mesh generation requirements associated with traditional high-fidelity simulations. By leveraging the flexibility of O-SAM, we enable the seamless integration of disparate models, meshes, and time integration schemes, enhancing computational efficiency while maintaining high accuracy. Our approach is demonstrated through a series of numerical experiments on complex three-dimensional (3D) solid dynamics problems, showcasing speedups of up to 106x compared to conventional FOM-FOM couplings. This work paves the way for more efficient simulation workflows in engineering applications, with potential extensions to a wide range of partial differential equations.

</details>


### [5] [Data-driven model order reduction for wave propagation in materials with nonlinearities or damage](https://arxiv.org/abs/2511.20815)
*Saddam Hijazi,Nikiema Fulgence,Hannah Burmester,Natalie Rauter,Carmen Gräßle*

Main category: math.NA

TL;DR: This paper applies model order reduction methods to accelerate simulations of wave propagation in nonlinear or damaged materials, comparing intrusive (POD-based) and non-intrusive (DMD, OpInf) approaches.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of simulating high-dimensional wave propagation problems in nonlinear materials and damaged structures by using efficient model order reduction techniques.

Method: Uses both intrusive (projection-based POD) and non-intrusive (DMD, operator inference) model reduction methods, with theoretical foundations reviewed and applied to wave propagation problems.

Result: Evaluates the performance of reduction techniques through three different numerical examples of wave propagation.

Conclusion: Model order reduction methods provide effective acceleration for wave propagation simulations in nonlinear and damaged materials, with both intrusive and non-intrusive approaches showing promise.

Abstract: In this work, we consider wave propagation in materials characterized by nonlinear properties or damage. To accelerate the simulations of the resulting high-dimensional problems, we apply model order reduction methods. Depending on the knowledge of the underlying equations and the availability of their discrete operators, intrusive methods (here projection-based approaches based on proper orthogonal decomposition (POD)) or non-instrusive methods (here data-driven approaches including dynamic mode decomposition (DMD) and operator inference (OpInf)) can be used. We recall the theoretical foundations of the methods and apply them to the problem of wave propagation. In three different numerical examples, we evaluate the performance of the reduction techniques.

</details>


### [6] [Truncated kernel windowed Fourier projection: a fast algorithm for the 3D free-space wave equation](https://arxiv.org/abs/2511.20824)
*Nour G. Al Hassanieh,Alex H. Barnett,Leslie Greengard*

Main category: math.NA

TL;DR: A fast algorithm for evaluating scalar wave equation solutions from multiple point sources using spectral methods with O((M + N³log N)N_t) complexity instead of O(M²N_t).


<details>
  <summary>Details</summary>
Motivation: Need efficient computation for wave equation solutions driven by many point sources, as naive evaluation scales poorly with large M and N_t.

Method: Uses windowed Fourier projection (WFP) to split potential into local and smooth history parts, with Fourier coefficients computed via recursion and spatial truncation of hyperbolic Green's function.

Result: Achieves 6-digit accuracy with up to 1 million sources and targets, avoiding need for absorbing boundary conditions.

Conclusion: Method enables efficient time-domain wave equation scattering problems and serves as key component for such applications.

Abstract: We present a spectrally accurate fast algorithm for evaluating the solution to the scalar wave equation in free space driven by a large collection of point sources in a bounded domain. With $M$ sources temporally discretized by $N_t$ time steps of size $Δt$, a naive potential evaluation at $M$ targets on the same time grid requires $\mathcal O(M^2 N_t)$ work. Our scheme requires $\mathcal{O}\left((M + N^3\log N)N_t\right)$ work, where $N$ scales as $\mathcal O(1/Δt)$, i.e., the maximum signal frequency. This is achieved by using the recently-proposed windowed Fourier projection (WFP) method to split the potential into a local part, evaluated directly, plus a smooth history part approximated by an $N^3$-point equispaced discretization of the Fourier transform, where each Fourier coefficient obeys a simple recursion relation. The growing oscillations in the spectral representation (which would be present with a naive use of the Fourier transform) are controlled by spatially truncating the hyperbolic Green's function itself. Thus, the method avoids the need for absorbing boundary conditions. We demonstrate the performance of our algorithm with up to a million sources and targets at 6-digit accuracy. We believe it can serve as a key component in addressing time-domain wave equation scattering problems.

</details>


### [7] [Beyond Expectation: Concentration Inequalities for Randomized Iterative Methods](https://arxiv.org/abs/2511.20877)
*Toby Anderson,Max Collins,Jamie Haddock,Jackie Lok,Elizaveta Rebrova*

Main category: math.NA

TL;DR: The paper provides upper bounds for the concentration and variance of error in stochastic iterative methods, addressing worst-case behavior beyond average case analysis.


<details>
  <summary>Details</summary>
Motivation: Stochastic iterative methods are widely used but most theoretical results only provide expected error bounds. Understanding near-worst-case behavior through variance and concentration bounds is needed for confidence intervals.

Method: The authors analyze a general class of linear stochastic iterative methods (including randomized Kaczmarz and Gauss-Seidel) and nonlinear methods (including randomized Kaczmarz for linear inequalities), deriving theoretical bounds.

Result: Upper bounds are provided for the concentration and variance of error in these stochastic iterative methods.

Conclusion: The paper establishes theoretical foundations for understanding worst-case behavior of stochastic iterative methods through variance and concentration analysis, enabling confidence interval construction.

Abstract: Stochastic iterative methods are useful in a variety of large-scale numerical linear algebraic, machine learning, and statistical problems, in part due to their low-memory footprint. They are frequently used in a variety of applications, and thus it is imperative to have a thorough theoretical understanding of their behavior. Most theoretical convergence results for stochastic iterative methods provide bounds on the expected error of the iterates, and yield a type of average case analysis. However, understanding the behavior of these methods in the near-worst-case is desirable. For stochastic methods, this motivates providing bounds on the variance and concentration of their error, which can be used to generate confidence intervals around the bounds on their expected error.
  Here, we provide upper bounds for the concentration and variance of the error of a general class of linear stochastic iterative methods, including the randomized Kaczmarz method and the randomized Gauss--Seidel method, and a more general class of nonlinear stochastic iterative methods, including the randomized Kaczmarz method for systems of linear inequalities.

</details>


### [8] [Alleviating missing boundary conditions in elliptic partial differential equations using interior point measurements](https://arxiv.org/abs/2511.20901)
*Andrea Bonito,Alan Demlow,Joshua M. Siktar*

Main category: math.NA

TL;DR: Optimal recovery for Poisson problem with unknown boundary data using interior point measurements, with improved error estimates for Riesz representers.


<details>
  <summary>Details</summary>
Motivation: To address the Poisson problem when boundary data is unknown, using compensating interior point measurements that lower regularity requirements compared to previous approaches.

Method: Finite element algorithm using Riesz representers associated with interior point measurements, with focus on pointwise error estimates for these representers.

Result: Improved pointwise error estimates for Riesz representers, leading to enhanced performance measurements of the recovery algorithm in various norms.

Conclusion: The approach provides better recovery performance for Poisson problems with unknown boundary data by leveraging interior point measurements and improved Riesz representer analysis.

Abstract: We consider an optimal recovery problem for the Poisson problem when the boundary data is unknown. Compensating information is provided in the form of a finite number of measurements of the solution. A finite element algorithm for this problem was given in Binev et al. (2024), where measurements were assumed to be either bounded linear functionals of the solution or point measurements at locations lying anywhere in the closure of the computational domain. In contrast, we focus on the case of point measurements at locations lying in the interior of the domain. This lowers the regularity requirements placed on the solution. Also, a key ingredient in the recovery process is the finite element approximation of Riesz representers associated with the measurements. Our main result is a pointwise error estimate for the Riesz representers. We apply this to obtain improved estimates which measure the performance of the recovery algorithm in various norms.

</details>


### [9] [Sharp Ascent--Descent Spectral Stability under Strong Resolvent Convergence](https://arxiv.org/abs/2511.20971)
*Marwa Ennaceur*

Main category: math.NA

TL;DR: Sharp stability results for non-selfadjoint operator spectra under strong resolvent convergence, with computable finite-element diagnostics for practical applications.


<details>
  <summary>Details</summary>
Motivation: To establish spectral stability for non-selfadjoint and singularly perturbed operators in finite element approximations, addressing limitations of norm resolvent convergence in rough or singular limits.

Method: Uses reduced minimum modulus γ(T-λ)>0 as key quantitative hypothesis, transfers Kaashoek-Taylor criteria via gap convergence of operator graphs, and introduces computable finite-element diagnostic γ_h as surrogate for γ(T-λ).

Result: Numerical experiments confirm that liminf_{h→0}γ_h>0 is necessary and sufficient for spectral stability, with B-Fredholm theory extending stability to powers (T-λ)^m under appropriate conditions.

Conclusion: Strong resolvent convergence combined with quantitative control of γ_h rescues ascent-descent stability in computational settings, providing practical framework for finite element approximations of non-selfadjoint operators.

Abstract: We establish sharp stability results for of non--selfadjoint the ascent and descent spectra under strong resolvent convergence (SRS), a natural framework for finite element approximations of non-selfadjoint and singularly perturbed operators. The key quantitative hypothesis is the reduced minimum modulus $γ(T-λ)>0$, which guarantees closed range and enables the transfer of the Kaashoek -- Taylor criteria via gap convergence of operator graphs. At the essential level, B--Fredholm theory extends stability to powers $(T-λ)^m$ provided $γ((T-λ)^j)>0$ for all $1\le j\le m$. We introduce a computable finite-element diagnostic $γ_h = σ_{\min}(M^{-1/2}(A_h-λM)M^{-1/2})$, which serves as a practical surrogate for $γ(T-λ)$ and remains uniformly positive even in convection-dominated regimes when stabilized schemes (e.g., SUPG) are employed. Numerical experiments confirm that $\liminf_{h\to0}γ_h>0$ is both necessary and sufficient for spectral stability, while a Volterra-type counterexample demonstrates the indispensability of the closed-range condition for powers. The analysis clarifies why norm resolvent convergence fails for rough or singular limits, and how SRS-combined with quantitative control of $γ_h$--rescues ascent--descent stability in realistic computational settings.

</details>


### [10] [A new analytical technique of the fully implicit Crank-Nicolson discontinuous Galerkin method for the Ginzburg-Landau Model](https://arxiv.org/abs/2511.21168)
*Xianxian Cao,Zhen Guan,Junjun Wang*

Main category: math.NA

TL;DR: A fully implicit Crank-Nicolson discontinuous Galerkin method is developed for solving the Ginzburg-Landau equation, with rigorous proofs of unique solvability and unconditionally optimal error estimates.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for the Ginzburg-Landau equation that guarantees unique solvability and provides optimal error estimates without time step restrictions.

Method: A fully implicit Crank-Nicolson discontinuous Galerkin method with novel analytical techniques to handle the cubic nonlinear term and establish rigorous mathematical proofs.

Result: The method achieves unique solvability and unconditionally optimal error estimates under both L²-norm and energy norm, validated by numerical examples.

Conclusion: The proposed method provides a reliable and accurate numerical framework for solving the Ginzburg-Landau equation with guaranteed mathematical properties.

Abstract: In this paper, a fully implicit Crank-Nicolson discontinuous Galerkin method is proposed for solving the Ginzburg-Landau equation. By leveraging a novel analytical technique, we rigorously establish the unique solvability of the constructed numerical scheme, as well as its unconditionally optimal error estimates under both the \(L^2\)-norm and the energy norm. The core of the proof hinges on the \(L^2\)-norm boundedness of the numerical solution and the refined estimation of the cubic nonlinear term. Finally, two numerical examples are presented to validate the theoretical findings.

</details>


### [11] [Optimal preconditioning techniques for finite volume approximation of three-dimensional conservative space-fractional diffusion equations](https://arxiv.org/abs/2511.21198)
*Wei Qu,Siu-Long Lei,Sean Y. Hon,Yuan-Yuan Huang*

Main category: math.NA

TL;DR: Preconditioned Krylov subspace methods with sine transform-based preconditioners are developed to efficiently solve large dense linear systems from 3D space-fractional diffusion equations, achieving linear convergence rates independent of discretization stepsizes.


<details>
  <summary>Details</summary>
Motivation: Three-dimensional conservative space-fractional diffusion equations result in large and dense three-level Toeplitz discrete linear systems that are computationally challenging to solve efficiently.

Method: Developed preconditioned Krylov subspace methods with sine transform-based preconditioners, using PCG for symmetric cases and PGMRES for non-symmetric cases, with detailed convergence analysis.

Result: For symmetric cases, preconditioned matrices have spectra bounded in (1/2, 3/2), ensuring linear convergence of PCG. For non-symmetric cases, PGMRES also achieves linear convergence independent of discretization stepsizes. Iteration counts are uniformly bounded and matrix-size independent.

Conclusion: The proposed preconditioned Krylov subspace methods demonstrate optimal performance with linear convergence rates independent of discretization parameters, validated by numerical experiments in 2D and 3D cases.

Abstract: A Crank-Nicolson finite volume approximation for three-dimensional conservative space-fractional diffusion equation results in large and dense three-level Toeplitz discrete linear systems. Preconditioned Krylov subspace methods with sine transform-based preconditioners are developed to solve these systems, including the preconditioned conjugate gradient (PCG) method for the symmetric case and the preconditioned generalized minimal residual (PGMRES) method for the non-symmetric case. Moreover, we provide detailed analysis of the convergence of these Krylov subspace methods. Specifically, for the symmetric case, we prove the spectra of the preconditioned matrices are uniformly bounded in the open interval (1/2, 3/2), which results in a linear convergence rate of the PCG method. For the non-symmetric case, we demonstrate that the PGMRES method also achieves a linear convergence rate independent of discretization stepsizes from the residual point of view. These results imply that the iteration counts of the PCG and PGMRES methods are uniformly bounded and independent of the matrix sizes. Numerical experiments in both symmetric and non-symmetric cases in two- and three-dimensions are conducted to confirm the optimal performance of the proposed preconditioned Krylov subspace methods.

</details>


### [12] [A p-adaptive high-order mesh-free framework for fluid simulations in complex geometries](https://arxiv.org/abs/2511.21224)
*Ruofeng Feng,Jack R. C. King,Steven J. Lind*

Main category: math.NA

TL;DR: A p-adaptive high-order mesh-free framework for fluid flow simulation that dynamically adjusts polynomial order using local error estimates, achieving 50% computational cost savings while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable accurate and efficient simulation of fluid flows in complex geometries by developing an adaptive mesh-free method that can locally refine polynomial order where needed, reducing computational costs while preserving accuracy.

Method: High-order differential operators constructed locally using anisotropic basis functions, with dynamic p-refinement strategy based on local Laplacian error estimates. A new refinement indicator is proposed and incorporated into solution procedure.

Result: The method achieved up to 50% computational cost savings while maintaining specified accuracy levels. Successfully captured highly non-linear regions requiring high-order approximation and demonstrated effectiveness in 2D compressible reacting flow simulations in porous media.

Conclusion: The p-adaptive high-order mesh-free method effectively captures non-linear regions requiring high-order approximation, reduces computational costs compared to non-adaptive methods, and maintains high accuracy and solution stability.

Abstract: This paper presents a novel p-adaptive, high-order mesh-free framework for the accurate and efficient simulation of fluid flows in complex geometries. High-order differential operators are constructed locally for arbitrary node distributions using linear combinations of anisotropic basis functions, formulated to ensure the exact reproduction of polynomial fields up to the specified p order. A dynamic p-refinement strategy is developed to refine (increase) or de-refine (decrease) the polynomial order used to approximate derivatives at each node. A new refinement indicator for mesh-free methods is proposed, based on local error estimates of the Laplacian operator, and is incorporated into the solution procedure at minimal added computational cost. Based on this error indicator, a refinement criterion is established to locally adjust the polynomial order p for the solution. The proposed adaptive mesh-free scheme is then applied to a range of canonical PDEs, and its potential is demonstrated in two-dimensional simulations of a compressible reacting flow in porous media. For the test cases studied, the proposed method exhibits potential to save up to 50% of computational costs while maintaining the specified level of accuracy. The results confirm that the developed p-adaptive high-order mesh-free method effectively captures highly non-linear regions where high-order approximation is necessary and reduces computational costs compared to the non-adaptive method, preserving high accuracy and solution stability.

</details>


### [13] [Rodas6P and Tsit5DA - two new Rosenbrock-type methods for DAEs](https://arxiv.org/abs/2511.21252)
*Gerd Steinebach*

Main category: math.NA

TL;DR: Two new Rosenbrock methods for solving index-1 differential algebraic equations: Rodas6P (sixth-order) and Tsit5DA (fifth-order) based on Tsit5 method.


<details>
  <summary>Details</summary>
Motivation: To develop improved higher-order methods for solving index-1 differential algebraic equations, building on existing Rosenbrock method designs and explicit-implicit approaches.

Method: Rodas6P follows the same design principles as previous Rodas methods (Rodas3P, Rodas4P, Rodas5P). Tsit5DA uses an explicit approach for differential equations and linear-implicit approach for algebraic equations, building on Tsit5 method.

Result: Theoretical properties of both new methods are verified through order tests and benchmarks, demonstrating their effectiveness as sixth-order and fifth-order methods respectively.

Conclusion: Two new Rosenbrock methods (Rodas6P and Tsit5DA) have been successfully developed for solving index-1 differential algebraic equations, providing higher-order alternatives to existing methods with verified theoretical properties.

Abstract: Two new Rosenbrock methods for solving index-1 differential algebraic equations are presented. Rodas6P is a sixth-order method based on the same design principles as the Rodas3P, Rodas4P, and Rodas5P methods. Tsit5DA is based on an explicit solution approach for the differential equations and a linear-implicit approach for the algebraic equations. Such a fourth-order method has already been presented in Rentrop, Roche & Steinebach, 1989. Tsit5DA now provides a significantly improved fifth-order method which is based on the well known Tsit5 method. The theoretical properties of the new methods are verified by some order tests and benchmarks.

</details>


### [14] [Parallel matching-based AMG preconditioners for elliptic equations discretized by IgA](https://arxiv.org/abs/2511.21268)
*Pasqua D'Ambra,Fabio Durastante,Salvatore Filippone*

Main category: math.NA

TL;DR: This paper investigates the use of algebraic multigrid (AMG) preconditioners for solving large, ill-conditioned linear systems in isogeometric analysis (IgA), focusing on performance in high-performance computing environments.


<details>
  <summary>Details</summary>
Motivation: Isogeometric analysis produces large, sparse, and ill-conditioned linear systems due to higher interconnectivity among degrees of freedom, especially in 3D or high-order scenarios, requiring efficient preconditioners for Krylov subspace methods.

Method: The study uses algebraic multigrid (AMG) preconditioners tailored for IgA discretizations, implemented using the Parallel Sparse Computation Toolkit (PSCToolkit) with distributed-memory and GPU-accelerated strategies.

Result: AMG preconditioners achieve robust and scalable performance across benchmark tests, demonstrating their effectiveness for large IgA systems.

Conclusion: AMG preconditioners are practical and scalable solvers for large isogeometric analysis systems in engineering and scientific applications.

Abstract: Isogeometric analysis (IgA) offers enhanced approximation capabilities for the discretization of elliptic boundary-value problems, yet it results in large, sparse, and increasingly ill-conditioned linear systems due to higher interconnectivity among degrees of freedom. In particular, the discretization with tensor-product B-splines or NURBS of degree $p$ on a mesh with $n$ elements per parametric direction leads to symmetric positive-definite systems of the form $K\mathbf{u} = \mathbf{F}$, where the matrix bandwidth and condition number scale unfavorably with both $p$ and spatial dimension $d$. To address the computational challenges posed by such systems, especially in three-dimensional or high-order scenarios, Krylov subspace methods with specialized preconditioners become essential. This paper investigates the efficacy of algebraic multigrid (AMG) preconditioners tailored for IgA-based discretizations, with a focus on performance in modern high-performance computing (HPC) environments. Leveraging the Parallel Sparse Computation Toolkit (PSCToolkit), we explore distributed-memory and GPU-accelerated strategies for solving large-scale problems. The study assesses algorithmic efficiency and scalability across a range of benchmark tests. The results demonstrate that AMG preconditioners can achieve robust and scalable performance, confirming their potential as practical solvers for large IgA systems in engineering and scientific applications.

</details>


### [15] [The Zipped Finite Element Method: High-order Shape Functions for Polygons](https://arxiv.org/abs/2511.21302)
*Stefano Berrone,Lorenzo Neva,Moreno Pintore,Gioana Teora,Fabio Vicini*

Main category: math.NA

TL;DR: The Zipped Finite Element Method is a new polygonal finite element approach for star-shaped polygons that constructs high-order shape functions using local sub-triangulations without increasing degrees of freedom.


<details>
  <summary>Details</summary>
Motivation: To develop a finite element method that can handle star-shaped polygons while maintaining high-order accuracy and conformity without increasing computational complexity.

Method: Constructs high-order shape functions as linear combinations of standard finite element basis functions defined on local sub-triangulations of each element, using refinement only for shape function construction.

Result: The method creates a finite element space that includes polynomials of desired order, preserves conformity across elements, and inherits convergence properties from standard finite element framework.

Conclusion: Numerical experiments confirm the expected convergence rates, validating the effectiveness of the Zipped Finite Element Method for star-shaped polygons.

Abstract: In this paper, we present a new polygonal finite element method, called the Zipped Finite Element Method, for star-shaped polygons. The proposed approach constructs high-order shape functions as linear combinations of standard finite element basis functions defined on a local trivial sub-triangulation of each element. This refinement is used solely for the construction of the shape functions and does not affect the final number of degrees of freedom. The resulting finite element space includes polynomials of the desired order and preserves conformity across elements. Consequently, the method inherits the convergence properties of the finite element framework under suitable mesh assumptions. Numerical experiments confirm the expected rates of convergence.

</details>


### [16] [Lopsided HSS Iterative Method and Preconditioner for a class of Complex Symmetric Linear System](https://arxiv.org/abs/2511.21393)
*Yusong Zhang,Zeng-Qi Wang*

Main category: math.NA

TL;DR: Proposes lopsided HSS (LHSS) iteration method for complex symmetric indefinite linear systems, reducing computational costs by using real symmetric matrices. Extends to preconditioned version (PLHSS) with improved convergence.


<details>
  <summary>Details</summary>
Motivation: To reduce high computational costs associated with complex arithmetic in solving complex symmetric indefinite linear systems.

Method: Alternating iterative scheme solving two systems with symmetric real coefficient matrices. Develops PLHSS preconditioner based on iterative method.

Result: Method guarantees convergence under certain eigenvalue conditions. PLHSS shows superior convergence properties with well-clustered eigenvalues and orthogonal eigenvectors.

Conclusion: LHSS and PLHSS methods demonstrate mesh size independent and parameter-insensitive convergence behavior in numerical experiments, making them efficient for complex symmetric systems.

Abstract: In this study, we propose the lopsided HSS (LHSS) iteration method for solving a class of complex symmetric indefinite systems of linear equations. This method employs an alternating iterative scheme, where each iteration entails solving two systems of equations with symmetric real coefficient matrices. This design is intended to reduce the high computational costs associated with complex arithmetic. Theoretical analysis shows that the upper bound of the convergence rate depends only on the maximum and minimum eigenvalues of the real symmetric matrices, as well as the iteration parameters. When the eigenvalues satisfy certain conditions, the method guarantees convergence for any positive iteration parameter. Building on this insight, we developed the preconditioned lopsided HSS iteration method (PLHSS). Theoretical results demonstrate that PLHSS exhibits superior convergence properties compared to the original method. Additionally, we derived the optimal parameters for the new approaches and corresponding optimal convergence rate. Furthermore, we derive the PLHSS preconditioner on the basis of the iterative method. The eigenvalues of the preconditioned matrix are well-clustered, and the eigenvectors are orthogonal with a specific inner product. Numerical experiments demonstrate the efficiency of the preconditioned GMRES and COCG methods. LHSS iteration methods and the relevant preconditioners show mesh size independent and parameter-insensitive convergence behavior for the test numerical examples.

</details>


### [17] [Vertex-based Graph Neural Solver and its Application to Linear Elasticity Equations](https://arxiv.org/abs/2511.21491)
*Yun Liu,Chen Cui,Shi Shu,Zhen Wang*

Main category: math.NA

TL;DR: A vertex-based graph Fourier neural solver is introduced for linear elasticity equations on unstructured grids, combining block-Jacobi smoothing with a learnable Fourier-domain module to handle heterogeneous and anisotropic materials.


<details>
  <summary>Details</summary>
Motivation: Numerical solution of linear elasticity equations on unstructured grids is challenging, especially with heterogeneous and anisotropic materials, requiring robust and efficient solvers.

Method: The method integrates a block-Jacobi smoothing stage for high-frequency error attenuation with a learnable Fourier-domain module to correct remaining frequency spectrum errors.

Result: Extensive numerical experiments on 2D and 3D problems show superior robustness and efficiency compared to classical smoothed aggregation algebraic multigrid method, with frequency domain analysis confirming complementarity between smoother and neural corrector.

Conclusion: The proposed solver effectively addresses challenges in linear elasticity equations on unstructured grids with heterogeneous materials, demonstrating improved performance over traditional methods.

Abstract: The numerical solution of linear elasticity equations on unstructured grids presents significant difficulties, especially in the presence of heterogeneous and anisotropic materials. To tackle these challenges, we introduce a novel vertex-based graph Fourier neural solver. This framework seamlessly integrates a block-Jacobi smoothing stage, known for rapidly attenuating certain high-frequency errors, with a learnable Fourier-domain module designed to correct errors across the remained frequency spectrum. Extensive numerical experiments on 2D and 3D problems verify that our proposed solver exhibits superior robustness and efficiency, significantly surpassing the classical smoothed aggregation algebraic multigrid method. Moreover, frequency domain analysis empirically confirms the essential complementarity between the smoother and the neural corrector.

</details>


### [18] [Low-Rank Solvers for Energy-Conserving Hamiltonian Boundary Value Methods](https://arxiv.org/abs/2511.21597)
*Fabio Durastante,Mariarosa Mazza*

Main category: math.NA

TL;DR: HBVMs preserve energy and symplecticity in Hamiltonian systems. The paper presents efficient solvers using Krylov methods for linear systems and Newton-Krylov with adaptive time-stepping for nonlinear systems.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for Hamiltonian systems that preserve energy and symplecticity over long time periods, which is crucial for applications requiring accurate long-term simulations.

Method: Uses Hamiltonian Boundary Value Methods (HBVMs) with matrix equation reformulation. For linear systems: Krylov projection solvers. For nonlinear systems: simplified Newton iterations and Newton-Krylov framework with adaptive time-stepping.

Result: Numerical experiments on semi-discretized wave equations show the approach is efficient and robust, demonstrating good performance in preserving energy and symplecticity.

Conclusion: The proposed HBVMs with specialized solvers provide an effective framework for long-term simulation of Hamiltonian systems, maintaining structural properties while achieving computational efficiency.

Abstract: We study energy-conserving Hamiltonian Boundary Value Methods (HBVMs) for Hamiltonian systems, which arise in applications where long-term preservation of energy and symplecticity is essential. HBVMs are multi-stage schemes whose stage equations reformulate as matrix equations with a low-rank right-hand side. For linear systems, we exploit this structure directly via Krylov projection solvers. For nonlinear systems, we leverage it within simplified Newton iterations and as a preconditioner in a Newton--Krylov framework, combined with adaptive time-stepping for robust convergence. Numerical experiments on semi-discretized wave equations demonstrate the efficiency and robustness of the proposed approach.

</details>


### [19] [Mean-square exponential stability of exact and numerical solutions for neutral stochastic delay differential equations with Markovian switching](https://arxiv.org/abs/2511.21620)
*Jina Yang,Ky Quan Tran*

Main category: math.NA

TL;DR: This paper establishes stability criteria for neutral stochastic differential delay equations with Markovian switching and proves that the Euler-Maruyama method can accurately reproduce the exponential decay rate of the true solution.


<details>
  <summary>Details</summary>
Motivation: To address the complex stability issues arising from the interaction between neutral terms, time-varying delays, and Markovian switching in stochastic differential equations, which are important for understanding system behavior in various applications.

Method: Theoretical analysis of mean-square exponential stability using novel criteria, combined with numerical approximation via the Euler-Maruyama method and validation through numerical examples.

Result: Established practical stability criteria for both the original system and its numerical approximations, and proved that the numerical scheme can reproduce the exponential decay rate with arbitrary accuracy given sufficiently small step sizes.

Conclusion: The proposed approach successfully addresses stability analysis for complex NSDDEs with Markovian switching, and the Euler-Maruyama method is shown to be effective for numerical approximation while preserving stability properties.

Abstract: This paper investigates the mean-square exponential stability of neutral stochastic differential delay equations (NSDDEs) with Markovian switching. The analysis addresses the complexities arising from the interaction between the neutral term, time-varying delays, and structural changes governed by a continuous-time Markov chain. We establish novel and practical criteria for the mean-square exponential stability of both the underlying system and its numerical approximations via the Euler-Maruyama method. Furthermore, we prove that the numerical scheme can reproduce the exponential decay rate of the true solution with arbitrary accuracy, provided the step size is sufficiently small. The theoretical results are supported by a numerical example that illustrates their effectiveness.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [20] [A note on the $L^{p}$-solvability of a strongly-coupled nonlocal system of equations](https://arxiv.org/abs/2511.20772)
*Tadele Mengesha,Miriam Abbate*

Main category: math.AP

TL;DR: This paper studies the L^p-solvability of strongly-coupled nonlocal vector systems, proving existence and uniqueness of strong solutions for f in L^p spaces via continuity method and establishing necessary a priori estimates.


<details>
  <summary>Details</summary>
Motivation: To extend recent results from scalar nonlocal operators to coupled vector-valued systems, addressing the L^p-solvability of strongly-coupled nonlocal systems with singular kernels comparable to fractional Laplacian kernels.

Method: Uses the method of continuity, establishes operator continuity and a priori estimates through study of corresponding parabolic systems, combining commutator estimates, Sobolev embeddings, level set estimates, and bootstrap arguments.

Result: Proves existence and uniqueness of strong solutions u in [H^{2s,p}(ℝ^d)]^d for any f in [L^p(ℝ^d)]^d with 1 < p < ∞, for strongly-coupled nonlocal systems with kernels comparable to |y|^{-(d+2s)}.

Conclusion: Successfully extends scalar nonlocal operator theory to coupled vector systems, providing comprehensive L^p-solvability results for strongly-coupled nonlocal systems through continuity method and careful analysis of parabolic counterparts.

Abstract: The goal of this paper is to study the $L^p$-solvability of the strongly-coupled nonlocal system \[
  \mathbb{L} \mathbf{u} (\mathbf{x}) + λ\mathbf{u}(\mathbf{x})= \mathbf{f}(\mathbf{x}) \quad \text{in $\mathbb{R}^{d}$ } \] where $\mathbb{L}$ is a linear nonlocal coupled vector-valued operator associated with a kernel $K$ comparable to $|\mathbf{y}|^{-(d+2s)}$ for $s \in (0,1)$, satisfying certain ellipticity and cancellation conditions. For any $\mathbf{f} \in [L^p(\mathbb{R}^d)]^d$, $1< p < \infty$, the existence of a unique strong solution $\mathbf{u} \in [H^{2s,p}(\mathbb{R}^d)]^d$ is proved via the method of continuity. To apply this method, we establish the continuity of the operator $\mathbb{L}$ and the necessary \textit{a priori} estimates. These are obtained through the study of the corresponding parabolic system. The proof strategy follows and extends recent ideas developed for the scalar setting, combining commutator estimates, Sobolev embeddings, a level set estimates and a bootstrap argument.

</details>


### [21] [Higher integrability for minimizers under weighted generalized Orlicz growth conditions](https://arxiv.org/abs/2511.20898)
*Vertti Hietanen,Mikyoung Lee*

Main category: math.AP

TL;DR: The paper extends higher integrability results for gradients of quasiminimizers to weighted Orlicz spaces, covering weighted variable exponent and double phase cases, and proves existence of minimizers.


<details>
  <summary>Details</summary>
Motivation: To generalize previous higher integrability results to weighted frameworks with generalized Orlicz growth conditions, including important special cases like weighted variable exponent and double phase problems.

Method: Using variational integrals in weighted Orlicz spaces with Muckenhoupt weights and structural conditions on growth functions to prove local higher integrability of gradients for quasiminimizers.

Result: Proved that gradients of local quasiminimizers have local higher integrability in weighted Orlicz settings, and established existence of minimizers for the associated functional.

Conclusion: Successfully extended higher integrability theory to weighted generalized Orlicz frameworks, providing a unified approach that includes weighted variable exponent and double phase problems as special cases.

Abstract: We investigate variational integrals in a weighted framework under generalized Orlicz growth conditions. Assuming that the weight belongs to a Muckenhoupt class and the growth function satisfies appropriate structural conditions, we prove that the gradient of any local quasiminimizer has local higher integrability. This result extends the previous higher integrability result to the weighted setting, encompassing as particular cases the weighted variable exponent and weighted double phase frameworks. In addition, we establish the existence of minimizers for the associated functional.

</details>


### [22] [The singular anisotropic Adams' type inequality in $\mathbb{R}^n$](https://arxiv.org/abs/2511.20978)
*Tao Zhang,Meixia Li,Fan Yang,Chunqin Zhou*

Main category: math.AP

TL;DR: Establishes best constants for singular anisotropic Adams' type inequality with exact growth in ℝⁿ and extends to bounded domains.


<details>
  <summary>Details</summary>
Motivation: To address singular anisotropic Adams' type inequalities with precise growth conditions and optimal constants, extending results from Euclidean space to bounded domains.

Method: Uses anisotropic rearrangement techniques to derive optimal constants and applies the same approach to bounded domains.

Result: Obtains best constants for singular anisotropic Adams' type inequality with exact growth in ℝⁿ and proves similar inequality on bounded domains.

Conclusion: Anisotropic rearrangement techniques successfully establish optimal constants for singular Adams' type inequalities in both ℝⁿ and bounded domains.

Abstract: In this paper, using anisotropic rearrangement techniques, we first establish the best constants for the singular anisotropic Adams' type inequality with exact growth in $\mathbb{R}^n$. Furthermore, by the same trick, we also prove the singular anisotropic Adams' type inequality on bounded domain $Ω\subset \mathbb{R}^n$.

</details>


### [23] [A sharp bound for the ratio of the first two eigenvalues of Robin Laplacian](https://arxiv.org/abs/2511.20988)
*Guowei Dai,Yingxin Sun*

Main category: math.AP

TL;DR: The paper proves that for the Robin Laplacian problem, the ratio of the first two eigenvalues μ₂/μ₁ is maximized by a disk for all dimensions N≥2 and boundary parameters σ≥σ*, and that this maximum ratio strictly decreases with σ.


<details>
  <summary>Details</summary>
Motivation: To resolve the Payne-Schaefer conjecture (2001) and Henrot's restatement (2003) about maximizing the ratio of the first two Robin eigenvalues, which extends the classical Payne-Pólya-Weinberger conjecture from the fixed membrane problem to elastically supported drums.

Method: Mathematical analysis of the Robin Laplacian eigenvalue problem, establishing properties of the eigenvalue ratio μ₂/μ₁ as a function of the boundary parameter σ and proving optimization results for all dimensions N≥2.

Result: The ratio μ₂/μ₁ is maximized by a disk for all N≥2 and σ≥σ* (where σ* is a critical positive value), and the maximum value of μ₂/μ₁ is strictly decreasing over (0,+∞).

Conclusion: The work affirmatively resolves the Payne-Schaefer conjecture and provides a positive answer to a variant of Yau's Problem 77, showing that measuring the ratio of the first two eigenfrequencies can determine if an elastically supported drum is circular.

Abstract: The celebrated conjecture by Payne, Pólya and Weinberger (1956) states that for the fixed membrane problem, the ratio of the first two eigenvalues, $λ_2/λ_1$, is maximized by a disk. A more general dimensional version of this conjecture was later resolved by Ashbaugh and Benguria in the 1990s. For the Robin Laplacian, Payne and Schaefer (2001) formulated an analogous conjecture, positing that the ratio $μ_2/μ_1$ is also maximized by a disk for a range of the boundary parameter $σ$. This was later restated by Henrot in 2003. In this work, we affirm this conjecture for all dimensions $N\geq2$ and for all $σ$ greater than or equal to a critical positive value $σ_*$ which may depend on $Ω$. Furthermore, we prove that the maximum value of $μ_2/μ_1$ is strictly decreasing in $σ$ over the entire interval $(0,+\infty)$. Our result provides a positive answer to a variant of Yau's Problem 77: by measuring the ratio of the first two eigenfrequencies, one can determine whether an elastically supported drum is circular.

</details>


### [24] [Simultaneously recover two constant coefficients and a polygon with a single pair of Cauchy data for the Helmholtz equation](https://arxiv.org/abs/2511.21023)
*Xiaoxu Xu,Guanghui Hu*

Main category: math.AP

TL;DR: Reconstructs constant coefficients and polygonal obstacle shape from single Cauchy data using modified factorization method.


<details>
  <summary>Details</summary>
Motivation: Solve inverse boundary value problem for Helmholtz equation to recover both obstacle geometry and material coefficients simultaneously from limited data.

Method: Adapted one-wave factorization method with modified factorization using Dirichlet-to-Neumann operator to handle eigenvalue issues.

Result: Uniqueness verified under a priori assumptions; intensive numerical tests show method is efficient.

Conclusion: Proposed approach successfully reconstructs both coefficients and obstacle shape from single Cauchy data pair.

Abstract: This paper is concerned with an inverse boundary value problem for the Helmholtz equation over a bounded domain. The aim is to reconstruct two constant coefficients together with the location and shape of a Dirichlet polygonal obstacle from a single pair of Cauchy data. Uniqueness results are verified under some a priori assumptions and the one-wave factorization method has been adapted to recover the polygonal obstacle as well as the two coefficients. A modified factorization using the Dirichlet-to-Neumann operator is employed to overcome difficulties arising from possible eigenvalues. Intensive numerical examples indicate that our method is efficient.

</details>


### [25] [Curvature-driven pattern formation in biomembranes: A gradient flow approach](https://arxiv.org/abs/2511.21230)
*Patrik Knopf,Anastasija Pešić,Dennis Trautwein*

Main category: math.AP

TL;DR: Analysis of a phase-field model for curvature-driven pattern formation in biomembranes, including existence proofs for weak solutions, regularity results, and numerical simulations showing various membrane patterns.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze a mathematical model for understanding curvature-driven pattern formation in biological membranes, which is important for studying cellular processes and membrane dynamics.

Method: Derived a phase-field model as gradient flow of energy functional approximating Canham-Helfrich energy, resulting in coupled Cahn-Hilliard and fourth-order reaction-diffusion equations. Used minimizing movement scheme for existence proofs and Moreau-Yosida regularization for singular potentials. Developed finite element discretization for numerical simulations.

Result: Proved existence of weak solutions for regular and singular potentials, established higher regularity and uniqueness of solutions. Numerical experiments revealed different membrane patterns (striped, dotted, snake-like) depending on physical parameters.

Conclusion: The developed phase-field model provides a rigorous mathematical framework for studying curvature-driven pattern formation in biomembranes, with proven well-posedness and numerical validation showing diverse pattern morphologies.

Abstract: In this work, we study a phase-field model for curvature-driven pattern formation in biomembranes. The model is derived as a gradient flow of an energy functional that approximates the two-phase Canham--Helfrich energy. This leads to a Cahn--Hilliard-type equation with cross diffusion for the relative chemical concentration of one lipid phase, coupled to a fourth-order reaction-diffusion equation describing the height profile of the membrane. We first prove the existence of weak solutions for the case of regular double-well potentials, using a minimizing movement scheme to construct approximate solutions. The analysis is then extended to singular potentials, e.g., the Flory--Huggins potential, by approximating them with a Moreau--Yosida regularization. For both cases, we establish higher regularity, continuous dependence on the initial data, and consequently the uniqueness of weak solutions. Finally, we propose a well-posed finite element discretization of the model and present numerical experiments illustrating the effect of different physical parameters on the resulting membrane patterns. Depending on the parameter regime, we observe purely striped, dotted, or snake-like structures.

</details>


### [26] [A Liouville-type theorem for Schrödinger equations with nonnegative potential](https://arxiv.org/abs/2511.21275)
*Henrik Ueberschaer*

Main category: math.AP

TL;DR: The paper proves that solutions to Δu=Vu with nonnegative bounded potential V must vanish identically if their L² norm decays to zero over annular regions, establishing the Landis conjecture for algebraically decaying solutions.


<details>
  <summary>Details</summary>
Motivation: To establish conditions under which solutions to Schrödinger-type equations must vanish identically, particularly addressing the Landis conjecture about decay properties of solutions.

Method: Uses analysis of solutions to Δu=Vu with continuous, nonnegative bounded potential V, examining L² norm decay over annular regions |x| between r_j and r_j+1 as r_j→∞.

Result: Proves that if ∫_{r_j≤|x|≤r_j+1}|u(x)|²dx→0 along any sequence r_j→∞, then u≡0 on ℝ^d. This establishes the Landis conjecture for solutions with sufficiently fast algebraic decay.

Conclusion: The results extend to exterior domains and certain nonlinear Schrödinger equations, providing conditions on when solutions must vanish based on decay properties and the zero set of the potential.

Abstract: Let $u$ be a solution of $Δu=Vu$ on $\mathbb{R}^d$, where $V$ be continuous, nonnegative and bounded. We prove that the condition $$\int_{r_j\leq|x|\leq r_j+1}|u(x)|^2dx\to 0,$$ along any sequence $(r_j)$, $r_j\nearrow+\infty$, implies $u\equiv 0$ on $\mathbb{R}^d$. In particular, this implies the Landis conjecture for solutions satisfying a sufficiently fast algebraic decay. These results are generalized to exterior domains as well as for a class of nonlinear Schrödinger equations under suitable conditions on the zero set of the potential.

</details>


### [27] [Transition threshold for the Navier-Stokes-Coriolis system at high Reynolds numbers](https://arxiv.org/abs/2511.21294)
*Minling Li,Changzhen Sun,Chao Wang,Dongyi Wei,Zhifei Zhang*

Main category: math.AP

TL;DR: Improved stability threshold for Couette flow in rotating Navier-Stokes system: perturbations of size Re^{-α} with α>2/3 (2D domain) or α≥5/6 (periodic domain) ensure global stability.


<details>
  <summary>Details</summary>
Motivation: To understand the transition from laminar to turbulent flow by quantifying perturbation thresholds that trigger instability, building on Trefethen et al.'s transition threshold problem for high Reynolds number flows.

Method: Combined analysis of rotation (dispersion) and mixing mechanisms using anisotropic Sobolev spaces tailored to zero modes, new dispersive structures, and Strichartz-type estimates to handle interactions between zero and non-zero modes.

Result: Proved that initial perturbations satisfying ||v_in - (y,0,0)|| ≤ ε₀Re^{-α} with α>2/3 (2D) or α≥5/6 (periodic) lead to globally stable solutions that remain close to Couette flow.

Conclusion: The improved stability threshold scaling Re^{-α} with α>2/3/5/6 is achieved by exploiting both nonlinear structure and enhanced dispersive behavior of zero modes through carefully designed analytical tools.

Abstract: The transition mechanism from laminar flow to turbulent flow is a central problem in hydrodynamic stability theory. To shed light on this transition mechanism, Trefethen et al.({\it \small Science 1993}) proposed the transition threshold problem, aiming to quantify the magnitude of perturbations required to trigger instability and determine their scaling with the Reynolds number. In this paper, we investigate the transition threshold of Couette flow for the three-dimensional incompressible Navier-Stokes-Coriolis system in the high Reynolds number regime ($\mathrm{Re}\gg 1$). By exploiting the combined effects of rotation (dispersion) and mixing mechanisms, we derive an improved stability threshold scaling in $\mathrm{Re}$. Precisely, we show that if the initial perturbation satisfies $$\|v_{in}-(y, 0, 0)\|_{\tilde{H}(\mathbb T \times \mathbb D)}\leq ε_0 \,\mathrm{Re}^{-α},$$ with any $α>\frac 23$ and $\tilde{H}=H^6 \cap W^{3,1}$ for $\mathbb D=\mathbb{R}^2$, and with any $α\geq\frac 56$ and $\tilde{H}=H^6$ for $\mathbb D=\mathbb{R}\times\mathbb{T}$, the corresponding solution of the Navier-Stokes-Coriolis system exists globally in time and remains asymptotically close to the Couette flow. The main analytical challenge arises from the anisotropic nature of the estimates for the zero modes and from the interactions between zero and non-zero modes, which we address using an anisotropic Sobolev space directly tailored to the zero modes. Additionally, we introduce a new dispersive structure for the zero modes and derive suitable Strichartz-type estimates. These tools enable us to exploit both the nonlinear structure and the improved dispersive behavior of certain good components of the zero modes, which play a crucial role in achieving the improved stability threshold.

</details>


### [28] [Normalized Solutions for Schroedinger-Bopp-Podolsky Systems in Bounded Domains with General Nonlinearities](https://arxiv.org/abs/2511.21303)
*Kai Sheng*

Main category: math.AP

TL;DR: Existence and multiplicity of normalized standing wave solutions for nonlinear Schrödinger-Bopp-Podolsky system with mass constraint.


<details>
  <summary>Details</summary>
Motivation: Study normalized solutions (standing waves with prescribed L^2 norm) for coupled Schrödinger-Bopp-Podolsky system, which combines quantum mechanics with Bopp-Podolsky electrodynamics.

Method: Adapt perturbation method to analyze the coupled system with mass constraint, considering both Navier and Neumann boundary conditions for the Bopp-Podolsky field.

Result: Existence of normalized solutions for all masses μ in interval (0, μ₀); multiplicity when f is odd; normalized ground state when domain is star-shaped.

Conclusion: The perturbation method successfully establishes existence and multiplicity results for normalized standing waves in the Schrödinger-Bopp-Podolsky system under various conditions.

Abstract: In this paper, by adapting the perturbation method, we study normalized standing wave solutions for the following nonlinear Schrodinger-Bopp-Podolsky system:
  - Delta u + q(x) phi u = omega u + f(u) in Omega, - Delta phi + a^2 Delta^2 phi = q(x) u^2 in Omega,
  where Omega is a smooth bounded domain in R^3, a > 0, and omega is the Lagrange multiplier associated with the L^2 mass constraint integral over Omega of u^2 equals mu, and f: R -> R is a continuous function satisfying some technical conditions. In particular, we prove the existence of normalized solutions for all masses mu in an interval (0, mu_0), under either Navier or Neumann boundary conditions for phi. Moreover, when f is odd, we obtain multiplicity of normalized solutions; and if Omega is star-shaped, we further obtain a normalized ground state solution.

</details>


### [29] [Existence and nonexistence of viscosity solutions for a class of degenerate/singular eigenvalue type equations](https://arxiv.org/abs/2511.21321)
*Mengni Li,You Li*

Main category: math.AP

TL;DR: Complete classification of existence/nonexistence of viscosity solutions for eigenvalue-type Dirichlet problems with distance function terms, including all exponent cases and global estimates.


<details>
  <summary>Details</summary>
Motivation: To systematically study degenerate/singular eigenvalue equations with distance functions near boundaries of convex domains, addressing the full range of possible exponents.

Method: Adaptations of Perron method and comparison principle, construction of classical sub-solutions and super-solutions.

Result: Complete classification of solution existence/nonexistence for all exponent cases, derivation of global estimates based on distance function.

Conclusion: Successfully classified all cases and established global estimates for viscosity solutions when they exist, using adapted classical methods.

Abstract: This paper is devoted to a complete classification on the existence and nonexistence results of viscosity solutions to the general Dirichlet problem for a class of eigenvalue type equations. With the distance function included in the right-hand side, this type of equations can be degenerate and (or) singular near the boundary of uniformly convex domains. One highlight is that all cases related to the exponent of the distance function are investigated. Moreover, when viscosity solutions exist, we derive a series of global estimates based on the distance function. The key ingredients of this paper include adaptions of the Perron method and comparison principle as well as constructions of suitable classical sub-solutions and super-solutions.

</details>


### [30] [Multiplicity of solutions for Gross-Pitaevskii equations on Riemannian manifolds](https://arxiv.org/abs/2511.21349)
*Dario Corona,Stefano Nardulli,Ramon Oliver-Bonafoux,Giandomenico Orlandi*

Main category: math.AP

TL;DR: Multiplicity results for time-independent Gross-Pitaevskii equations on closed Riemannian manifolds, with lower bounds on solution multiplicity based on topology of maximum velocity set in small momentum regime.


<details>
  <summary>Details</summary>
Motivation: To study critical points of Ginzburg-Landau energy with prescribed momentum according to tangent velocity fields, particularly non-minimizing solutions that arise in physical systems.

Method: Uses critical point theory and Γ-convergence for Ginzburg-Landau functionals, plus new results for codimension 2 isoperimetric-type problems in small flux regime.

Result: Establishes lower bounds on multiplicity of solutions in terms of topology of maximum velocity set, valid for small momentum and vorticity core size.

Conclusion: The approach provides multiplicity guarantees for Gross-Pitaevskii equations and introduces new isoperimetric results that may have independent mathematical interest.

Abstract: We provide a multiplicity result for solutions of time-independent Gross-Pitaevskii equations on closed Riemannian manifolds. Such solutions arise as (possibly non-minimizing) critical points of the Ginzburg-Landau energy having prescribed momentum according to a given tangent velocity field. Lower bounds on the multiplicity of solutions are obtained in terms of the topology of the maximum velocity set, in the small momentum and vorticity core size regime. The proof relies on methods from critical point theory and $Γ$-convergence for Ginzburg-Landau functionals as well as on some new results for codimension 2 isoperimetric-type problems in the small flux regime, possibly of independent interest.

</details>


### [31] [Qualitative properties of single blow-up solutions for nonlinear Hartree equation with slightly subcritical exponent](https://arxiv.org/abs/2511.21372)
*Alessandro Cannone,Silvia Cingolani,Minbo Yang,Shunneng Zhao*

Main category: math.AP

TL;DR: Analysis of qualitative properties and Morse index of single blow-up solutions to nonlocal equations with slightly subcritical exponents in bounded domains.


<details>
  <summary>Details</summary>
Motivation: To understand the qualitative behavior of eigenpairs and derive Morse index for single-bubble solutions in nonlocal equations with critical exponents.

Method: Using local Pohozaev identities and blow-up analysis to estimate eigenvalues and eigenfunctions, examining linearized problem eigenpairs.

Result: Provides estimates on first (n+2)-eigenvalues and their eigenfunctions, derives Morse index of single-bubble solution in nondegenerate setting.

Conclusion: The analysis yields comprehensive understanding of blow-up solution properties and establishes Morse index results for nonlocal critical equations.

Abstract: In this paper, we study the qualitative properties of single blow-up solutions to the nonlocal equations with slightly subcritical exponents \begin{equation*}
  -Δu=(|x|^{-(n-2)}\ast u^{p-ε})u^{p-1-ε}\quad \mbox{in}~~Ω,~~ u=0\quad \mbox{on}~~\partialΩ,
  \end{equation*} where $Ω$ is a smooth bounded domain in $\mathbb{R}^n$ for $n=3,4,5$, $\ast$ denotes the standard convolution, $ε>0$ is a small parameter and $p=\frac{n+2}{n-2}$ is $\mathcal{D}^{1,2}$ energy-critical exponent. By exploiting various local Pohozaev identities and blow-up analysis, we provide a number of estimates on the first $(n+2)$-eigenvalues and their corresponding eigenfunctions, and examine the qualitative behavior of the eigenpairs $(λ_{i,ε}, v_{i,ε})$ to the linearied problem of the above nonlocal equations for $i=1,\cdots,n+2$. As a corollary, we derive the Morse index of a single-bubble solution in a nondegenerate setting.

</details>


### [32] [Stationary equation of the relativistic heat diffusion in transparent media having $L^1$--data](https://arxiv.org/abs/2511.21390)
*Francesco Balducci,Sergio Segura de León*

Main category: math.AP

TL;DR: Existence and regularity analysis for Dirichlet problem in radiation hydrodynamics with L^1 data


<details>
  <summary>Details</summary>
Motivation: To extend previous work that only handled regular data (L^N) to more general L^1 data, since the solution cannot be bounded in this framework

Method: Develop new mathematical framework for defining solutions when Anzellotti theory doesn't apply, study regularity for L^p data with 1<p<N

Result: Proved existence of solution for L^1 data and established regularity results for L^p data that are consistent with previous findings

Conclusion: Successfully extended the theory to handle more general data types while maintaining coherence with existing regularity results

Abstract: Our objective is to prove existence of a solution to the Dirichlet problem for an equation arising in the theory of radiation hydrodynamics to deal with the radiating energy in transparent media. We study its stationary equation with $L^1$--datum in a bounded domain. This problem was addressed in [11] for regular data (data belonging to $L^N(Ω)$) and a bounded solution was obtained. In our framework, the proof of existence is far from trivial since the solution sought cannot be bounded. Consequently, the Anzellotti theory of pairings does not apply and we have to use new developments to introduce the meaning of solution. We also study the regularity of solutions when data belong to $L^p(Ω)$, with $1<p<N$. Our result is coherent with the regularity found in [11].

</details>


### [33] [A Laplacian System With Sign-Changing Weight Function](https://arxiv.org/abs/2511.21447)
*Seyyed Sadegh Kazemipoor,Hadiseh Ebrahimi*

Main category: math.AP

TL;DR: Existence of positive solutions for Laplacian system using Nehari manifold and fibering maps


<details>
  <summary>Details</summary>
Motivation: To prove existence of at least one positive solution for a coupled Laplacian system with nonlinear terms involving parameters λ and β

Method: Using Nehari manifold and fibering maps associated with the Euler functional for the system

Result: Proved existence of at least one positive solution for the Laplacian system on a bounded region Ω

Conclusion: The Nehari manifold and fibering maps approach successfully establishes existence of positive solutions for the coupled Laplacian system

Abstract: We prove the existence of at least one positive solution for the Laplacian system\\ -Δv=λa(x)|v|^{q-2}v+β\fracβ{α+β}b(x)|u|^α|v|^{β-2}v&$for~$x\inΩ$$ $$ \end{array}\right.$$ On a bounded region $Ω$ by using the Nehari manifold and the fibering maps associated with the Euler functional for the system.

</details>


### [34] [Polychromatic Localized Waves with Complex Frequencies in Nonlinear Maxwell Equations with Material Dispersion](https://arxiv.org/abs/2511.21467)
*Tomas Dohnal,Maximilian Hanisch,Runan He*

Main category: math.AP

TL;DR: Existence of polychromatic breather solutions for cubically nonlinear Maxwell equations in waveguides with dispersive media, constructed as Fourier series with complex frequencies leading to temporal decay.


<details>
  <summary>Details</summary>
Motivation: Study nonlinear electromagnetic wave propagation in dispersive media where dielectric functions produce complex frequencies, resulting in temporal decay, focusing on waveguide geometries and TM-polarized solutions.

Method: Construct solutions as Fourier series in propagation direction and time, with leading frequency as eigenvalue of operator pencil. Solve iteratively via sequence of linear ODEs under spectral assumptions and resolvent estimates.

Result: Proved existence of polychromatic breather solutions under specific spectral conditions, with concrete example of waveguide interface between two homogeneous media satisfying assumptions.

Conclusion: Established existence theory for nonlinear polychromatic surface plasmons in waveguide interfaces, providing mathematical framework for breather solutions in dispersive nonlinear Maxwell systems.

Abstract: We study the existence of polychromatic solutions of cubically nonlinear Maxwell equations in the whole space and with dispersive media, i.e., with a time delayed polarization. Due to the complex nature of the dielectric function, the frequencies are complex, resulting in a decay in time. The geometry is that of a waveguide in $x$ with the propagation direction being $y$ and the solutions are localized in $x$ and TM-polarized. These are often referred to as breathers. They are given as a Fourier series in $y$ and $t$ with the leading frequency $ω$ being an eigenvalue of a corresponding operator pencil on $\mathbb{R}$ (in the $x$ variable). Each term in the series corresponds to a different temporal decay rate or a different frequency. The series is constructed iteratively via a sequence of linear ordinary differential equations. Our general result provides the existence under some assumptions on the spectrum and on estimates of the resolvent of the corresponding linear operator. We also produce an example of a waveguide given by the interface of two spatially homogeneous physically relevant media for which these assumptions are satisfied. For such an interface setting the constructed solutions correspond to nonlinear polychromatic surface plasmons.

</details>


### [35] [A Hamilton-Jacobi Framework in a Field-Road System with Unidirectional Advection under Wentzell-Type Boundary Condition](https://arxiv.org/abs/2511.21469)
*Xinye Xiao,Haomin Huang*

Main category: math.AP

TL;DR: Develops Hamilton-Jacobi framework for propagation dynamics in field-road systems with unidirectional advection and Wentzell boundary conditions, deriving variational inequalities from reaction-diffusion systems.


<details>
  <summary>Details</summary>
Motivation: To analyze asymptotic propagation dynamics in complex field-road systems where classical methods fail, particularly for non-order-preserving systems with degenerate media and boundary conditions.

Method: Synthesizes viscosity solution theory, optimal control formulation, and variational analysis to derive Hamilton-Jacobi variational inequalities as singular limits of reaction-diffusion systems in half-planes with degenerate roads.

Result: Establishes existence, uniqueness, and explicit variational representation of viscosity solutions, revealing critical transition in propagation behavior between rectilinear and road-assisted regimes governed by geometrically derived curves.

Conclusion: The framework successfully extends to non-order-preserving systems and conical domains, demonstrating robustness through variational methodology and numerical simulations showing parameter-dependent propagation patterns.

Abstract: This paper develops a comprehensive Hamilton-Jacobi framework to analyze asymptotic propagation dynamics in a field-road system featuring unidirectional advection and Wentzell-type boundary conditions. We rigorously derive a Hamilton-Jacobi variational inequality as the singular limit of a reaction-diffusion system in the upper half-plane, where the road is modeled as a degenerate one-dimensional medium with enhanced diffusion and tangential drift. By synthesizing viscosity solution theory, optimal control formulation, and variational analysis, we establish the existence, uniqueness, and explicit variational representation of the viscosity solution. The solution is characterized by a fundamental solution constructed via optimal paths, revealing a critical transition in propagation behavior governed by a geometrically derived curve that separates rectilinear and road-assisted regimes. Our framework extends to non-order-preserving systems where classical comparison methods fail, and we provide a detailed asymptotic derivation of the Wentzell boundary condition from flux continuity principles. Furthermore, we generalize the approach to conical domains with intersecting roads, demonstrating the robustness of our variational methodology. Numerical simulations illustrate how advection and diffusion parameters shape the invaded region, highlighting the interplay between field and road dynamics in determining propagation patterns.

</details>


### [36] [Existence results for quasimonotone semilinear coupled elliptic systems via sub-supersolution method](https://arxiv.org/abs/2511.21482)
*Shalmali Bandyopadhyay,Briceyda B. Delgado,Nsoki Mavinga,Maria Amarakristi Onydio*

Main category: math.AP

TL;DR: Existence of weak solutions for coupled elliptic PDE systems with quasimonotone nonlinearities, using monotone iteration for ordered solutions and growth conditions for non-monotone cases.


<details>
  <summary>Details</summary>
Motivation: To establish existence results for coupled elliptic PDE systems with quasimonotone nonlinearities, addressing both monotonicity and non-monotonicity scenarios.

Method: Monotone iteration techniques for ordered sub-supersolution pairs when nonlinearities are monotone; growth condition approach for non-monotone cases.

Result: Proved existence of minimal and maximal weak solutions between ordered sub-supersolution pairs under monotonicity, and general existence under growth conditions for non-monotone cases.

Conclusion: Theoretical existence results established with concrete examples demonstrating applicability across different nonlinearity conditions.

Abstract: We establish the existence of weak solutions of coupled systems of elliptic partial differential equations with quasimonotone nonlinearities in the domain interior and on the boundary. When the nonlinearities satisfy some monotonicity conditions, we employ monotone iteration techniques to establish the existence of minimal and maximal weak solutions between an ordered pair of sub- and supersolution. In the absence of monotonicity, we prove an existence result when the nonlinearities satisfy certain growth conditions. In addition, we provide concrete examples that illustrate the applicability of our theoretical results.

</details>


### [37] [On the hyperbolic relaxation of the chemical potential in a phase field tumor growth model](https://arxiv.org/abs/2511.21489)
*Pierluigi Colli,Elisabetta Rocca,Jürgen Sprekels*

Main category: math.AP

TL;DR: The paper analyzes a hyperbolic Cahn-Hilliard tumor growth model with inertial terms, proving well-posedness, continuous dependence on therapy functions, and convergence to viscous models.


<details>
  <summary>Details</summary>
Motivation: To study tumor growth models where the chemical potential relaxation is hyperbolic rather than parabolic, allowing for inertial effects and broader application to various therapy scenarios.

Method: Mathematical analysis of initial-boundary value problems for hyperbolic Cahn-Hilliard tumor models, including well-posedness proofs, continuous dependence on therapy functions, and asymptotic analysis as inertial coefficients vanish.

Result: Established well-posedness of the hyperbolic tumor model, continuous dependence on drug and antiangiogenic therapy functions, and convergence to viscous Cahn-Hilliard models when inertial terms approach zero.

Conclusion: The hyperbolic relaxation approach provides a valid alternative to parabolic models, with rigorous mathematical foundations and applicability to broad classes of double-well potentials including nonsmooth cases.

Abstract: In this paper, we study a phase field model for a tumor growth model of Cahn--Hilliard type in which the often assumed parabolic relaxation of the chemical potential is replaced by a hyperbolic one. We show that the resulting initial-boundary value problem is well posed and that its solutions depend continuously on two given functions: one appearing in the mass balance equation and one in the nutrient equation, representing, respectively, sources of drugs (e.g. chemotherapy) and antiangiogenic therapy. We also discuss regularity properties of the solutions. Moreover, in the case of a constant proliferation function, we rigorously analyze the asymptotic behavior as the coefficient of the inertial term tends to zero, establishing convergence to the corresponding viscous Cahn--Hilliard tumor growth model. Our results apply to a broad class of double-well potentials, including nonsmooth ones.

</details>


### [38] [Variational Principle and Stochastic Lagrangian Formulation of Viscous Hydrodynamic Equations](https://arxiv.org/abs/2511.21498)
*Anna Mazzucato,Anping Pan*

Main category: math.AP

TL;DR: Extension of Lagrangian formulation for Navier-Stokes equations to broader hydrodynamic models using stochastic Hamilton-Pontryagin variational principle, with generalized Kelvin circulation theorem and local well-posedness results.


<details>
  <summary>Details</summary>
Motivation: To extend the Lagrangian formulation beyond Navier-Stokes equations to a wider class of hydrodynamic models and establish mathematical foundations using variational principles.

Method: Stochastic Hamilton-Pontryagin type variational principle, Lagrangian-Eulerian formulation with fixed point arguments for well-posedness analysis.

Result: Successfully extended Lagrangian formulation to broader hydrodynamic models, derived generalized Kelvin circulation theorem for viscous fluids, and established local well-posedness results.

Conclusion: The Lagrangian formulation can be systematically extended to various hydrodynamic models through stochastic variational principles, providing mathematical foundations for viscous fluid dynamics with circulation theorems and well-posedness guarantees.

Abstract: In this manuscript, we extend the Lagrangian formulation of \cite{CI08} for Navier-Stokes Equation to a wider class of hydrodynamic models. Moreover, we prove that such Lagrangian formulation is naturally derived from a stochastic Hamilton-Pontryagin type variational principle. Generalized version of Kelvin circulation theorem in viscous fluids is discussed. We also derive self-contained local well-posedness results of some fluid equations based on Lagrangian-Eulerian formulation using fixed point argument.

</details>


### [39] [Large data global well-posedness for the modified Novikov-Veselov system](https://arxiv.org/abs/2511.21564)
*Adrian Nachman,Peter Perry,Daniel Tataru*

Main category: math.AP

TL;DR: The paper proves global well-posedness and scattering for the modified Novikov-Veselov (mNV) system with large L² data using inverse scattering methods, and extends results to the related Novikov-Veselov problem.


<details>
  <summary>Details</summary>
Motivation: Previous work by Schottdorf established global well-posedness for small L² data in the mNV system. This paper addresses the large data problem for this L²-critical, completely integrable system.

Method: Uses inverse scattering methods and proves a new nonlinear Gagliardo-Nirenberg inequality for the scattering transform. Also employs the Miura map to connect mNV and NV flows, and proves a sharp Agmon-Allegretto-Piepenbrink principle.

Result: The mNV system is globally well-posed for large L² data with solutions scattering as time goes to ±∞. Also proves global well-posedness for the Novikov-Veselov problem at critical regularity for soliton-free data.

Conclusion: The paper successfully extends the global well-posedness theory for the mNV system from small to large data, providing important mathematical tools and connections between related integrable systems.

Abstract: The modified Novikov-Veselov system (mNV) is a cubic third order dispersive evolution in two space dimensions. It is also completely integrable, belonging to the same hierarchy as the defocusing Davey-Stewartson II (DS II) system.
  The mNV system is $L^2$ critical. Some time ago, Schottdorf proved that for small $L^2$ initial data, the mNV equation is globally well-posed. In this article, we consider instead the large data problem, using inverse scattering methods. Our main result asserts that the mNV system is globally well-posed for large $L^2$ data, with the solutions scattering as time goes to $\pm \infty$. One key ingredient in the proof, which is of independent interest, is a new nonlinear Gagliardo-Nirenberg inequality for the associated scattering transform.
  As a byproduct of our main result, we are also able to prove a global well-posedness result for the closely related Novikov-Veselov problem at the critical $\dot H^{-1} + L^1$ level, for a range of data which can heuristically be described as soliton-free. Here we use the associated Miura map to connect the mNV and the NV flows. In order to characterize the range of the Miura map, we prove another result of independent interest, namely a sharp, scale invariant form of the Agmon-Allegretto-Piepenbrink principle in the critical case of two space dimensions.

</details>


### [40] [Long time inviscid damping near Couette in Sobolev spaces](https://arxiv.org/abs/2511.21583)
*Dengjun Guo,Xiaoyutao Luo*

Main category: math.AP

TL;DR: Elementary proof of long-time inviscid damping for Sobolev perturbations near Couette flow in 2D Euler equations, with velocity damping estimates up to time scales O(ε^{-δ_s}) where δ_s depends on Sobolev regularity.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical understanding of inviscid damping phenomena in 2D Euler equations near shear flows, particularly the Couette flow, which is fundamental in fluid dynamics.

Method: Elementary proof approach for analyzing Sobolev perturbations near Couette flow (y,0) in 2D Euler equations on T × R, focusing on vorticity perturbations in H^s spaces.

Result: For any s>1 and initial vorticity perturbation of size O(ε) in H^s, velocity damping estimates are obtained up to time scale t = O(ε^{-δ_s}), with δ_s=1/3 when s→1+ and δ_s=1/2 for s>2.

Conclusion: The paper provides an elementary proof establishing long-time inviscid damping behavior for 2D Euler equations near Couette flow, with explicit time scales depending on the Sobolev regularity of initial perturbations.

Abstract: We give an elementary proof of long time inviscid damping for Sobolev perturbations near the Couette flow $(y,0)$ for the 2D Euler equations on $\mathbb{T} \times \mathbb{R}$. For any $s>1$ and any initial vorticity perturbation of size $O(ε)$ in $H^s$, we obtain velocity damping estimates up to a time scale $ t = O(ε^{-δ_s} )$, where $δ_s=1/3$ when $s\to 1+$ and $δ_s=1/2$ for $s>2$.

</details>


### [41] [Machine Learning and Deep Learning in Computational Finance: A Systematic Review](https://arxiv.org/abs/2511.21588)
*Soufiane El Amine El Alami,Abderazzak Mouiha,Abdelatif Hafid,Ahmed El Hilali Alaoui*

Main category: math.AP

TL;DR: Systematic review of ML/DL applications in finance (2024-2026) showing superior performance over traditional models in areas like credit risk, cryptocurrency, and asset pricing, with emerging trends in explainable AI and responsible AI integration.


<details>
  <summary>Details</summary>
Motivation: To examine how machine learning and deep learning have transformed forecasting, decision-making, and financial modeling, promoting innovation and efficiency in financial systems.

Method: Systematic review following PRISMA 2020 guidelines analyzing 22 peer-reviewed articles (2024-2026) from Scopus, covering ML/DL models including Random Forest, XG-Boost, SVM, LSTM, BiLSTM, CNN, and hybrid approaches.

Result: ML and DL techniques outperform traditional models by capturing nonlinear dependencies and enhancing predictive accuracy. Explainable AI methods (SHAP, feature importance) improve transparency. Emerging trends include cross-domain applications and responsible AI integration.

Conclusion: AI-driven computational finance shows significant progress but faces challenges in interpretability, generalizability, and data quality. The review provides comprehensive overview and outlines future research directions.

Abstract: This systematic review examines how machine learning (ML) and deep learning (DL) have transformed forecasting, decision-making, and financial modelling, promoting innovation and efficiency in financial systems. Following PRISMA 2020 guidelines, we analyze 22 peer-reviewed and open-access articles (2024 to 2026) indexed in Scopus, applying ML and DL models across credit risk prediction, cryptocurrency, asset pricing, and macroeconomic policy modeling. The most used models include Random Forest, XG-Boost, Support Vector Machine, Long Short-Term Memory (LSTM), Bidirectional LSTM, Convolutional Neural Network (CNN), and hybrid or ensemble approaches combining statistical and AI methods. ML and DL techniques outperform traditional models by capturing nonlinear dependencies and enhancing predictive accuracy, while explainable AI methods (e.g., SHAP and feature importance analysis) improve transparency and interpretability. Emerging trends include cross-domain applications and the integration of responsible AI in finance. Despite notable progress, challenges remain in interpretability, generalizability, and data quality. Overall, this review provides a comprehensive overview of AI-driven computational finance and outlines future research directions.

</details>


### [42] [Dissipative solutions to Stochastic 3D Euler equations](https://arxiv.org/abs/2511.21616)
*Umberto Pappalettera,Francesco Triggiano*

Main category: math.AP

TL;DR: Construction of probabilistically strong solutions to 3D Euler equations with additive noise that are continuous in time, Hölder in space, and satisfy local energy inequality up to large stopping times.


<details>
  <summary>Details</summary>
Motivation: To establish existence of strong solutions for the challenging 3D Euler equations by incorporating stochastic perturbations, overcoming deterministic ill-posedness issues.

Method: Additive noise perturbation to the three-dimensional Euler equations, probabilistic analysis of solutions, and local energy inequality verification up to large stopping times.

Result: Successfully constructed probabilistically strong solutions that are continuous in time, Hölder continuous in space, and satisfy the local energy inequality almost surely up to arbitrarily large stopping times.

Conclusion: Stochastic perturbations enable construction of strong solutions for 3D Euler equations with desired regularity properties, providing a probabilistic approach to overcome deterministic limitations.

Abstract: We construct probabilistically strong solutions to the three-dimensional Euler equations perturbed by additive noise that are $\mathbb{P}$-almost surely continuous in time, Hölder in space, and satisfy the local energy inequality up to an arbitrarily large stopping time.

</details>


### [43] [Dynamics of generalized abcd Boussinesq solitary waves under a slowly variable bottom](https://arxiv.org/abs/2511.21632)
*André de Laire,Olivier Goubet,María Eugenia Martínez,Claudio Muñoz,Felipe Poblete*

Main category: math.AP

TL;DR: Analysis of generalized solitary waves in the Boussinesq abcd system with variable bottom topography, focusing on existence and collision problems in the physically relevant regime.


<details>
  <summary>Details</summary>
Motivation: To investigate the existence of generalized solitary waves and their collision behavior in the Boussinesq abcd system with variable bottom topography, which represents more realistic physical scenarios compared to flat bottom cases.

Method: Constructed new approximate solutions to capture interactions between solitary waves and slowly varying bottom topography represented by h=εh₀(εt,εx), where ε is a small parameter and h₀ is a fixed smooth profile.

Result: Established the existence of generalized solitary waves in the variable bottom regime and described weak long-range interactions and wave evolution without destruction.

Conclusion: The paper successfully demonstrates that generalized solitary waves can persist and interact with variable bottom topography, providing a more comprehensive understanding of wave behavior in realistic shallow water scenarios.

Abstract: The Boussinesq $abcd$ system is a 4-parameter set of equations posed in $\mathbb R_t\times\mathbb R_x$, originally derived by Bona, Chen and Saut as first-order 2-wave approximations of the incompressible and irrotational, two-dimensional water wave equations in the shallow water wave regime, in the spirit of the original Boussinesq derivation. Among the various particular regimes, each determined by the values of the parameters $(a, b, c, d)$ appearing in the equations, the \emph{generic} regime is characterized by the conditions $b, d > 0$ and $a, c < 0$. If additionally $b=d$, the $abcd$ system is Hamiltonian.
  In this paper, we investigate the existence of generalized solitary waves and the corresponding collision problem in the physically relevant \emph{variable bottom regime}, introduced by M.\ Chen. More precisely, the bottom is represented by a smooth space-time dependent function $h=\varepsilon h_0(\varepsilon t,\varepsilon x)$, where $\varepsilon$ is a small parameter and $h_0$ is a fixed smooth profile. This formulation allows for a detailed description of weak long-range interactions and the evolution of the solitary wave without its destruction. We establish this result by constructing a new approximate solution that captures the interaction between the solitary wave and the slowly varying bottom.

</details>


### [44] [The hydrodynamic limit of viscoelastic granular gases](https://arxiv.org/abs/2511.21645)
*R. Alonso,B. Lods,I. Tristani*

Main category: math.AP

TL;DR: First rigorous derivation of incompressible Navier-Stokes-Fourier system with time-dependent forcing from inelastic hard-spheres Boltzmann equation for viscoelastic granular gases, using velocity-dependent restitution coefficient.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous hydrodynamic limits for granular gases with realistic viscoelastic particle interactions, where energy dissipation depends on collision velocities, unlike constant restitution models.

Method: Used self-similar change of variables to balance energy inflow and outflow, resulting in non-autonomous rescaled Boltzmann equation. The scaling explicitly depends on Knudsen number and restitution coefficient.

Result: Successfully derived incompressible Navier-Stokes-Fourier system with self-consistent time-dependent forcing terms. Solutions converge weakly to hydrodynamic limit. Determined exact dissipation rate of granular temperature (Haff's law).

Conclusion: The approach captures nontrivial inelastic-hydrodynamic effects and provides new hydrodynamic equations for viscoelastic granular gases, overcoming limitations of constant restitution models.

Abstract: We obtain the first rigorous derivation of an incompressible Navier-Stokes-Fourier system with self-consistent and time-dependent forcing terms from the inelastic hard-spheres Boltzmann equation associated to the relevant case of viscoelastic granular gases. The model's inelasticity is measured by the so-called restitution coefficient which, for viscoelastic particles, depends on the relative velocities of particles. Through a suitable self-similar change of variables, a balanced dynamic between energy inflow and outflow naturally emerges in the model which permits its analysis. In contrast, such balanced dynamic does not emerge naturally in the constant restitution case and has to be imposed in our previous contribution (Alonso, Lods, Tristani, Mémoires SMF). The exact self-similar rescaling, which allows to capture nontrivial inelastic-hydrodynamic effects, presents itself explicitly in terms of the Knudsen number and the restitution coefficient. The consequence of such scaling is a non-autonomous rescaled Boltzmann equation whose solutions converge, in a specific weak sense, towards the aforementioned hydrodynamic limit. The incompressible Navier-Stokes-Fourier system obtained by this process appears to be new in this context. As a byproduct of the analysis, we determine the exact dissipation rate of the granular temperature known as \emph{Haff's law}.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [45] [Differentiable Physics-Neural Models enable Learning of Non-Markovian Closures for Accelerated Coarse-Grained Physics Simulations](https://arxiv.org/abs/2511.21369)
*Tingkai Xue,Chin Chun Ooi,Zhengwei Ge,Fong Yew Leong,Hongying Li,Chang Wei Kang*

Main category: physics.comp-ph

TL;DR: A hybrid physics-neural model that predicts scalar transport in complex domains orders of magnitude faster than 3D simulations, using differentiable physics and neural closure models.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D simulations are computationally expensive (hours), while most analysis only requires reduced metrics. There's a need for faster, data-efficient surrogate models.

Method: End-to-end differentiable framework combining physical model parameterization (orthotropic diffusivity) with non-Markovian neural closure models to capture unresolved coarse-grained effects, enabling stable long-term rollouts.

Result: Achieved 0.96 Spearman correlation coefficient, reduced simulation time from hours to under 1 minute, learned with only 26 training samples, and successfully extended to out-of-distribution scenarios with moving sources.

Conclusion: The differentiable physics-neural framework enables fast, accurate, and generalizable coarse-grained surrogates for physical phenomena with high computational efficiency and data efficiency.

Abstract: Numerical simulations provide key insights into many physical, real-world problems. However, while these simulations are solved on a full 3D domain, most analysis only require a reduced set of metrics (e.g. plane-level concentrations). This work presents a hybrid physics-neural model that predicts scalar transport in a complex domain orders of magnitude faster than the 3D simulation (from hours to less than 1 min). This end-to-end differentiable framework jointly learns the physical model parameterization (i.e. orthotropic diffusivity) and a non-Markovian neural closure model to capture unresolved, 'coarse-grained' effects, thereby enabling stable, long time horizon rollouts. This proposed model is data-efficient (learning with 26 training data), and can be flexibly extended to an out-of-distribution scenario (with a moving source), achieving a Spearman correlation coefficient of 0.96 at the final simulation time. Overall results show that this differentiable physics-neural framework enables fast, accurate, and generalizable coarse-grained surrogates for physical phenomena.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [Physics Steering: Causal Control of Cross-Domain Concepts in a Physics Foundation Model](https://arxiv.org/abs/2511.20798)
*Rio Alexa Fear,Payel Mukhopadhyay,Michael McCabe,Alberto Bietti,Miles Cranmer*

Main category: cs.LG

TL;DR: Physics foundation models develop internal representations of abstract physical concepts that can be manipulated to steer model behavior, demonstrating causal control over physical simulations.


<details>
  <summary>Details</summary>
Motivation: To investigate whether the phenomenon of internal representations corresponding to human-understandable concepts is unique to language/image models or a general property of foundation models, specifically in scientific/physics domains.

Method: Extracted activation vectors from a physics foundation model during forward passes over different physical regimes, computed 'delta' representations between regimes as concept directions, and injected these directions back during inference to steer predictions.

Result: Successfully demonstrated causal control over physical behaviors by inducing or removing specific physical features from simulations through concept direction manipulation.

Conclusion: Scientific foundation models learn generalized representations of physical principles rather than relying on superficial correlations, opening new avenues for understanding and controlling AI in scientific discovery.

Abstract: Recent advances in mechanistic interpretability have revealed that large language models (LLMs) develop internal representations corresponding not only to concrete entities but also distinct, human-understandable abstract concepts and behaviour. Moreover, these hidden features can be directly manipulated to steer model behaviour. However, it remains an open question whether this phenomenon is unique to models trained on inherently structured data (ie. language, images) or if it is a general property of foundation models. In this work, we investigate the internal representations of a large physics-focused foundation model. Inspired by recent work identifying single directions in activation space for complex behaviours in LLMs, we extract activation vectors from the model during forward passes over simulation datasets for different physical regimes. We then compute "delta" representations between the two regimes. These delta tensors act as concept directions in activation space, encoding specific physical features. By injecting these concept directions back into the model during inference, we can steer its predictions, demonstrating causal control over physical behaviours, such as inducing or removing some particular physical feature from a simulation. These results suggest that scientific foundation models learn generalised representations of physical principles. They do not merely rely on superficial correlations and patterns in the simulations. Our findings open new avenues for understanding and controlling scientific foundation models and has implications for AI-enabled scientific discovery.

</details>


### [47] [SUPN: Shallow Universal Polynomial Networks](https://arxiv.org/abs/2511.21414)
*Zachary Morrow,Michael Penwarden,Brian Chen,Aurya Javeed,Akil Narayan,John D. Jakeman*

Main category: cs.LG

TL;DR: SUPNs (shallow universal polynomial networks) achieve better function approximation with fewer parameters than DNNs and KANs, reducing approximation error and variability while maintaining theoretical convergence rates.


<details>
  <summary>Details</summary>
Motivation: DNNs and KANs require many parameters leading to overparameterization, which reduces transparency and creates optimization challenges with local minima affecting generalization. There's a need for more efficient function approximation methods.

Method: Replace all but the last hidden layer with a single layer of polynomials with learnable coefficients, combining strengths of DNNs and polynomials for better expressivity with fewer parameters.

Result: SUPNs achieve lower approximation error and variability than DNNs and KANs by an order of magnitude for same parameter count, and even outperform polynomial projection on non-smooth functions in numerical experiments.

Conclusion: SUPNs provide an efficient alternative to DNNs and KANs for function approximation, offering better performance with fewer parameters while maintaining theoretical guarantees.

Abstract: Deep neural networks (DNNs) and Kolmogorov-Arnold networks (KANs) are popular methods for function approximation due to their flexibility and expressivity. However, they typically require a large number of trainable parameters to produce a suitable approximation. Beyond making the resulting network less transparent, overparameterization creates a large optimization space, likely producing local minima in training that have quite different generalization errors. In this case, network initialization can have an outsize impact on the model's out-of-sample accuracy. For these reasons, we propose shallow universal polynomial networks (SUPNs). These networks replace all but the last hidden layer with a single layer of polynomials with learnable coefficients, leveraging the strengths of DNNs and polynomials to achieve sufficient expressivity with far fewer parameters. We prove that SUPNs converge at the same rate as the best polynomial approximation of the same degree, and we derive explicit formulas for quasi-optimal SUPN parameters. We complement theory with an extensive suite of numerical experiments involving SUPNs, DNNs, KANs, and polynomial projection in one, two, and ten dimensions, consisting of over 13,000 trained models. On the target functions we numerically studied, for a given number of trainable parameters, the approximation error and variability are often lower for SUPNs than for DNNs and KANs by an order of magnitude. In our examples, SUPNs even outperform polynomial projection on non-smooth functions.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [48] [Flows of conformally coclosed $G_2$-structures with dilaton](https://arxiv.org/abs/2511.21055)
*Spiro Karigiannis,Sébastien Picard,Caleb Suan*

Main category: math.DG

TL;DR: The paper studies G2-structure flows through dimensional reduction, showing how natural geometric flows in G2-geometry reduce to complex geometry flows, with focus on G2-Laplacian coflow and G2-anomaly flow.


<details>
  <summary>Details</summary>
Motivation: To understand how geometric flows in G2-geometry relate to flows in complex geometry through dimensional reduction, providing insights into the connections between these geometric structures.

Method: Using dimensional reduction principle to show that natural G2-geometry flows reduce to complex geometry flows. Specifically analyzes G2-Laplacian coflow (lifting Kähler-Ricci flow) and 7D lift of anomaly flow on complex threefolds.

Result: The G2-anomaly flow deforms conformally coclosed G2-structures. The paper compares G2-anomaly flow with G2-Laplacian coflow and investigates short-time existence and fixed points of these flows.

Conclusion: Dimensional reduction provides a powerful framework connecting G2-geometry flows with complex geometry flows, with specific relationships established between G2-Laplacian coflow/Kähler-Ricci flow and G2-anomaly flow.

Abstract: We study flows of $G_2$-structures guided by the principle of dimensional reduction: natural geometric flows in $G_2$-geometry reduce to natural flows in complex geometry. Our main examples are the $G_2$-Laplacian coflow, which lifts the Kähler--Ricci flow, and a 7-dimensional lift of the anomaly flow on complex threefolds. The $G_2$-lift of the anomaly flow deforms conformally coclosed $G_2$-structures. We compare the $G_2$-anomaly flow to the $G_2$-Laplacian coflow, and investigate short-time existence and fixed points.

</details>


### [49] [The Critical LYZ equation in Kähler Geometry](https://arxiv.org/abs/2511.21492)
*Jixiang Fu,Shing-Tung Yau,Dekai Zhang*

Main category: math.DG

TL;DR: Existence of smooth solutions for LYZ equation at critical phase θ=(n-2)π/2, solving Collins-Jacob-Yau and Li's problem. Applications include solving 3D Hessian equation σ₂=1 and 4D Hessian quotient equation σ₃=σ₁ under weaker assumptions.


<details>
  <summary>Details</summary>
Motivation: To solve the critical case of the problem posed by Collins-Jacob-Yau and Li concerning solvability for phase θ≤(n-2)π/2 in the LYZ equation.

Method: Establishing existence of smooth solutions for the LYZ equation at the critical phase θ=(n-2)π/2.

Result: Successfully proved existence of smooth solutions at critical phase, enabling solutions to 3D Hessian equation σ₂=1 and 4D Hessian quotient equation σ₃=σ₁ under weaker assumptions.

Conclusion: The paper solves the critical case of the LYZ equation problem and provides applications to Hessian equations with reduced assumptions.

Abstract: We establish the existence of smooth solutions for the LYZ equation at the critical phase $θ=(n-2)\fracπ{2}$, thereby solving the critical case of a problem posed by Collins-Jacob-Yau
  and Li concerning the solvability for phase $θ\leq (n-2)\fracπ{2}$. As applications, we solve the 3D Hessian equation $σ_2 = 1$ and the 4D Hessian quotient equation $σ_3 = σ_1$ under weaker assumptions than previously required.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [50] [AI4X Roadmap: Artificial Intelligence for the advancement of scientific pursuit and its future directions](https://arxiv.org/abs/2511.20976)
*Stephen G. Dale,Nikita Kazeev,Alastair J. A. Price,Victor Posligua,Stephan Roche,O. Anatole von Lilienfeld,Konstantin S. Novoselov,Xavier Bresson,Gianmarco Mengaldo,Xudong Chen,Terence J. O'Kane,Emily R. Lines,Matthew J. Allen,Amandine E. Debus,Clayton Miller,Jiayu Zhou,Hiroko H. Dodge,David Rousseau,Andrey Ustyuzhanin,Ziyun Yan,Mario Lanza,Fabio Sciarrino,Ryo Yoshida,Zhidong Leong,Teck Leong Tan,Qianxiao Li,Adil Kabylda,Igor Poltavsky,Alexandre Tkatchenko,Sherif Abdulkader Tawfik,Prathami Divakar Kamath,Theo Jaffrelot Inizan,Kristin A. Persson,Bryant Y. Li,Vir Karan,Chenru Duan,Haojun Jia,Qiyuan Zhao,Hiroyuki Hayashi,Atsuto Seko,Isao Tanaka,Omar M. Yaghi,Tim Gould,Bun Chan,Stefan Vuckovic,Tianbo Li,Min Lin,Zehcen Tang,Yang Li,Yong Xu,Amrita Joshi,Xiaonan Wang,Leonard W. T. Ng,Sergei V. Kalinin,Mahshid Ahmadi,Jiyizhe Zhang,Shuyuan Zhang,Alexei Lapkin,Ming Xiao,Zhe Wu,Kedar Hippalgaonkar,Limsoon Wong,Lorenzo Bastonero,Nicola Marzari,Dorye Luis Esteras Cordoba,Andrei Tomut,Alba Quinones Andrade,Jose-Hugo Garcia*

Main category: physics.soc-ph

TL;DR: AI and machine learning are extending scientific discovery capabilities across multiple domains by enabling researchers to probe, predict, and design more effectively.


<details>
  <summary>Details</summary>
Motivation: To provide a forward-looking roadmap of AI-enabled science across various domains and identify shared challenges and opportunities for accelerating scientific discovery.

Method: Analysis and synthesis of AI applications across biology, chemistry, climate science, mathematics, materials science, physics, self-driving laboratories, and unconventional computing, identifying common themes and requirements.

Result: Identified key requirements for AI-enabled science: diverse and trustworthy data, transferable models, integration into end-to-end workflows, and generative systems grounded in synthesisability. Highlighted the role of large foundation models, active learning, and self-driving laboratories in closing prediction-validation loops.

Conclusion: The roadmap outlines current state of AI-enabled science, identifies bottlenecks in data, methods and infrastructure, and charts directions for building more transparent and capable AI systems that can accelerate discovery in complex real-world environments.

Abstract: Artificial intelligence and machine learning are reshaping how we approach scientific discovery, not by replacing established methods but by extending what researchers can probe, predict, and design. In this roadmap we provide a forward-looking view of AI-enabled science across biology, chemistry, climate science, mathematics, materials science, physics, self-driving laboratories and unconventional computing. Several shared themes emerge: the need for diverse and trustworthy data, transferable electronic-structure and interatomic models, AI systems integrated into end-to-end scientific workflows that connect simulations to experiments and generative systems grounded in synthesisability rather than purely idealised phases. Across domains, we highlight how large foundation models, active learning and self-driving laboratories can close loops between prediction and validation while maintaining reproducibility and physical interpretability. Taken together, these perspectives outline where AI-enabled science stands today, identify bottlenecks in data, methods and infrastructure, and chart concrete directions for building AI systems that are not only more powerful but also more transparent and capable of accelerating discovery in complex real-world environments.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [51] [Stochastic Optimal Control of Interacting Particle Systems in Hilbert Spaces and Applications](https://arxiv.org/abs/2511.21646)
*Filippo de Feo,Fausto Gozzi,Andrzej Święch,Lukas Wessels*

Main category: math.PR

TL;DR: This paper establishes the theoretical foundations for optimal control of interacting particle systems in Hilbert spaces, proving convergence of finite particle value functions to mean-field Hamilton-Jacobi-Bellman equations and connecting particle system controls to lifted limit controls.


<details>
  <summary>Details</summary>
Motivation: Optimal control of interacting particles governed by stochastic evolution equations in Hilbert spaces is an open research area, with applications in economics where particles are modeled by stochastic delay differential equations and stochastic partial differential equations.

Method: Prove convergence of finite particle system value functions to mean-field Hamilton-Jacobi-Bellman equations, establish C^{1,1}-regularity of lifted value functions, and show correspondence between optimal controls of particle systems and lifted control problems.

Result: First results for stochastic optimal control of interacting particle systems in Hilbert spaces, with applications to economics problems involving stochastic delay differential equations and stochastic partial differential equations.

Conclusion: The developed theory provides the first comprehensive framework for optimal control of interacting particle systems governed by stochastic evolution equations in Hilbert spaces, with practical applications in economics.

Abstract: Optimal control of interacting particles governed by stochastic evolution equations in Hilbert spaces is an open area of research. Such systems naturally arise in formulations where each particle is modeled by stochastic partial differential equations, path-dependent stochastic differential equations (such as stochastic delay differential equations or stochastic Volterra integral equations), or partially observed stochastic systems. The purpose of this manuscript is to build the foundations for a limiting theory as the number of particles tends to infinity. We prove the convergence of the value functions $u_n$ of finite particle systems to a function $\mathcal{V}$, {which} is the unique {$L$}-viscosity solution of the corresponding mean-field Hamilton-Jacobi-Bellman equation {in the space of probability measures}, and we identify its lift with the value function $U$ of the so-called ``lifted'' limit optimal control problem. Under suitable additional assumptions, we show $C^{1,1}$-regularity of $U$, we prove that $\mathcal{V}$ projects precisely onto the value functions $u_n$, and that optimal (resp. optimal feedback) controls of the particle system correspond to optimal (resp. optimal feedback) controls of the lifted control problem started at the corresponding initial condition. To the best of our knowledge, these are the first results of this kind for stochastic optimal control problems for interacting particle systems of stochastic evolution equations in Hilbert spaces. We apply the developed theory to problems arising in economics where the particles are modeled by stochastic delay differential equations and stochastic partial differential equations.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [52] [Impact of Cosmic Ray Distribution on the Growth and Saturation of Bell Instability](https://arxiv.org/abs/2511.21154)
*Saikat Das,Siddhartha Gupta,Prateek Sharma*

Main category: astro-ph.HE

TL;DR: The paper studies how cosmic ray distributions affect non-resonant streaming instability growth and saturation using kinetic simulations, finding that low-energy CRs dominate saturation and proposing a modified saturation model with layered confinement near shocks.


<details>
  <summary>Details</summary>
Motivation: To understand how different cosmic ray momentum distributions influence the growth and saturation of non-resonant streaming instability, which is crucial for cosmic ray confinement and acceleration in astrophysical shocks.

Method: One-dimensional kinetic simulations comparing mono-energetic and power-law cosmic ray momentum distributions to analyze linear growth and saturation mechanisms of non-resonant streaming instability.

Result: Linear growth depends only on CR current regardless of distribution, but saturation strongly depends on CR distribution. Low-energy CRs dominate current relaxation and magnetic growth, while high-energy CRs remain weakly scattered. Mono-energetic CRs effectively amplify fields and isotropize.

Conclusion: A modified saturation prescription accounting for CR distribution effects is provided, along with a layered CR-confinement scenario upstream of astrophysical shocks that is relevant for particle acceleration to high energies.

Abstract: Cosmic rays (CRs) streaming in weakly magnetized plasmas can drive large-amplitude magnetic fluctuations via nonresonant streaming instability (NRSI), or Bell instability. Using one-dimensional kinetic simulations, we investigate how mono-energetic and power-law CR momentum distributions influence the growth and saturation of NRSI. The linear growth is governed solely by the CR current and is largely insensitive to the CR distribution. However, the saturation depends strongly on the CR distribution and is achieved through CR isotropization, which quenches the driving current. Mono-energetic CRs effectively amplify the magnetic field and isotropize. For power-law distributions, the lowest-energy CRs dominate current relaxation and magnetic growth, while the highest-energy CRs remain weakly scattered, limiting their contribution to saturation. In the absence of low-energy CRs, high-energy particles amplify magnetic fields effectively and isotropize. We provide a modified saturation prescription accounting for these effects and propose a layered CR-confinement scenario upstream of astrophysical shocks, relevant to particle acceleration to high energies.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [53] [Quantum Hard Spheres with Affine Quantization](https://arxiv.org/abs/2511.21119)
*Riccardo Fantoni*

Main category: cond-mat.stat-mech

TL;DR: Study of quantum hard-sphere fluid using affine-quantization and path integral Monte Carlo method for Bose-Einstein statistics systems.


<details>
  <summary>Details</summary>
Motivation: To investigate the thermodynamic properties of quantum hard-sphere fluids using affine-quantization approach.

Method: Path integral Monte Carlo method applied to Bose-Einstein statistics systems with affine-quantization treatment.

Result: Thermodynamic properties are solved for quantum hard-sphere fluids.

Conclusion: Affine-quantization combined with path integral Monte Carlo provides a framework for studying quantum hard-sphere fluid thermodynamics.

Abstract: We study a fluid of quantum hard-spheres treated with affine-quantization. Assuming that the fluid obeys to Bose-Einstein statistics we solve for its thermodynamic properties using the path integral Monte Carlo method.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [54] [Unified interface dipole theory for Fermi level pinning effect at metal-semiconductor contacts](https://arxiv.org/abs/2511.21494)
*Ziying Xiang,Jun-Wei Luo,Shu-Shen Li*

Main category: cond-mat.mtrl-sci

TL;DR: Unified bond dipole theory explains Fermi level pinning at metal-semiconductor interfaces through localized bonding between semiconductor dangling bonds and metal orbitals, showing that even a single metal monolayer can cause strong pinning.


<details>
  <summary>Details</summary>
Motivation: To provide a unified microscopic explanation for interface dipoles and Fermi level pinning by showing that metal-semiconductor bonding is the fundamental mechanism, rather than treating MIGS, DBSS, and bonding states as independent causes.

Method: Combined first-principles calculations with tight-binding analysis using Harrison's bond-orbital model to study interface bonding and dipole formation.

Result: Localized bonding between semiconductor surface dangling bonds and metal orbitals generates large interface dipoles and strong Fermi level pinning, with pinning strength governed by the density of available dangling bonds for bonding.

Conclusion: The bond dipole theory provides a unified framework explaining FLP phenomena, naturally accounts for weaker pinning in ionic semiconductors, and offers practical guidance for engineering metal-semiconductor interfaces and tuning Schottky barriers.

Abstract: We present a unified bond dipole theory for metal-semiconductor interfaces to explain the microscopic origin of interface dipoles and Fermi level pinning (FLP) in terms of Harrison's bond-orbital model. By combining first-principles calculations with tight-binding analysis, we show that localized bonding between semiconductor surface dangling bonds and metal orbitals is sufficient to generate a large interface dipole and induce strong FLP, even when only a single metal monolayer is present. Within this framework, metal-induced gap states (MIGS), dangling-bond-induced surface states (DBSS), and bonding states embedded in the valence band are all understood as different outcomes of the same underlying interface bonding mechanism, rather than as independent causes of FLP. We further establish that the key parameter governing FLP strength is the density of surface dangling bonds that can form new chemical bonds with the metal, which directly controls the magnitude of the bond-induced interface dipole. This picture naturally explains the weaker pinning observed in more ionic semiconductors than in covalent ones and provides practical guidance for engineering metal-semiconductor interfaces and tuning Schottky barrier heights.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [55] [Approximate Bayesian Computation Made Easy: A Practical Guide to ABC-SMC for Dynamical Systems with \texttt{pymc}](https://arxiv.org/abs/2511.21587)
*Mario Castro*

Main category: q-bio.PE

TL;DR: A practical tutorial introducing ABC-SMC (Approximate Bayesian Computation with Sequential Monte Carlo) for likelihood-free parameter inference in mechanistic models using Python and PyMC.


<details>
  <summary>Details</summary>
Motivation: Many researchers avoid ABC-SMC due to perceived complexity, despite its power for likelihood-free inference in mechanistic models where likelihood functions are intractable.

Method: Example-driven approach using Python and PyMC's probabilistic programming interface, covering predator-prey dynamics and hierarchical epidemic models to demonstrate implementation, diagnosis, and interpretation.

Result: Provides working code examples that build intuition about ABC-SMC performance, parameter identifiability under partial observability, and hierarchical Bayesian structures.

Conclusion: Bridges the adoption gap by making ABC-SMC accessible through practical examples, ensuring reproducibility and easy adaptation to new problems in ecology, epidemiology, and life sciences.

Abstract: Mechanistic models are essential tools across ecology, epidemiology, and the life sciences, but parameter inference remains challenging when likelihood functions are intractable. Approximate Bayesian Computation with Sequential Monte Carlo (ABC-SMC) offers a powerful likelihood-free alternative that requires only the ability to simulate data from mechanistic models. Despite its potential, many researchers remain hesitant to adopt these methods due to perceived complexity. This tutorial bridges that gap by providing a practical, example-driven introduction to ABC-SMC using Python. From predator-prey dynamics to hierarchical epidemic models, we illustrate by example how to implement, diagnose, and interpret ABC-SMC analyses. Each example builds intuition about when and why ABC-SMC works, how partial observability affects parameter identifiability, and how hierarchical structures naturally emerge in Bayesian frameworks. All code leverages PyMC's modern probabilistic programming interface, ensuring reproducibility and easy adaptation to new problems. The code its fully available for download at \href{https://github.com/mariocastro73/ABCSMC_pymc_by_example}{mariocastro73/ABCSMC\_pymc\_by\_example}

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [56] [Sublinear Time Low-Rank Approximation of Hankel Matrices](https://arxiv.org/abs/2511.21418)
*Michael Kapralov,Cameron Musco,Kshiteej Sheth*

Main category: cs.DS

TL;DR: First sublinear-time algorithm for low-rank approximation of positive semidefinite Hankel matrices, achieving O(log n log(1/ε)) rank approximation in polylog(n, 1/ε) time with robustness to noise.


<details>
  <summary>Details</summary>
Motivation: PSD Hankel matrices are known to be approximately low-rank (Beckermann-Townsend theorem), making them natural targets for efficient low-rank approximation algorithms. However, existing methods don't achieve sublinear runtime.

Method: 1) Structure-preserving existence proof showing optimal Hankel approximations exist; 2) Sampling-based algorithm leveraging Vandermonde structure and universal ridge leverage score bounds to achieve sublinear time.

Result: Developed first sublinear-time algorithm (polylog(n, 1/ε)) that computes rank-O(log n log(1/ε)) Hankel approximation matching Beckermann-Townsend error bounds, with robustness to non-Hankel noise.

Conclusion: The work provides both theoretical foundations (structure-preserving existence) and practical algorithm for efficient Hankel matrix approximation, bridging infinite-dimensional AAK theory with finite-dimensional computational methods.

Abstract: Hankel matrices are an important class of highly-structured matrices, arising across computational mathematics, engineering, and theoretical computer science. It is well-known that positive semidefinite (PSD) Hankel matrices are always approximately low-rank. In particular, a celebrated result of Beckermann and Townsend shows that, for any PSD Hankel matrix $H \in \mathbb{R}^{n \times n}$ and any $ε> 0$, letting $H_k$ be the best rank-$k$ approximation of $H$, $\|H-H_k\|_F \leq ε\|H\|_F$ for $k = O(\log n \log(1/ε))$. As such, PSD Hankel matrices are natural targets for low-rank approximation algorithms. We give the first such algorithm that runs in \emph{sublinear time}. In particular, we show how to compute, in $\polylog(n, 1/ε)$ time, a factored representation of a rank-$O(\log n \log(1/ε))$ Hankel matrix $\widehat{H}$ matching the error guarantee of Beckermann and Townsend up to constant factors. We further show that our algorithm is \emph{robust} -- given input $H+E$ where $E \in \mathbb{R}^{n \times n}$ is an arbitrary non-Hankel noise matrix, we obtain error $\|H - \widehat{H}\|_F \leq O(\|E\|_F) + ε\|H\|_F$. Towards this algorithmic result, our first contribution is a \emph{structure-preserving} existence result - we show that there exists a rank-$k$ \emph{Hankel} approximation to $H$ matching the error bound of Beckermann and Townsend. Our result can be interpreted as a finite-dimensional analog of the widely applicable AAK theorem, which shows that the optimal low-rank approximation of an infinite Hankel operator is itself Hankel. Armed with our existence result, and leveraging the well-known Vandermonde structure of Hankel matrices, we achieve our sublinear time algorithm using a sampling-based approach that relies on universal ridge leverage score bounds for Vandermonde matrices.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [57] [Opportunities and Challenges of Computational Electromagnetics Methods for Superconducting Circuit Quantum Device Modeling: A Practical Review](https://arxiv.org/abs/2511.20774)
*Samuel T. Elkin,Ghazi Khan,Ebrahim Forati,Brandon W. Langley,Dogan Timucin,Reza Molavi,Sara Sussman,Thomas E. Roth*

Main category: quant-ph

TL;DR: This paper provides a practical introduction to computational electromagnetics (CEM) methods for modeling superconducting circuit quantum devices, highlighting challenges in multiscale modeling and offering guidance for tool selection and problem mitigation.


<details>
  <summary>Details</summary>
Motivation: Emerging superconducting circuit quantum devices present unique challenges for CEM methods due to unconventional material properties and multiscale features (nanometer to centimeter scales), which can lead to increased simulation times, accuracy loss, or unreliable solutions.

Method: The review covers fundamental aspects of major CEM techniques, provides practical guidance for tool selection, discusses advanced topics related to multiscale modeling challenges, and presents specific examples from superconducting circuit quantum devices.

Result: The paper serves as an accessible introduction that bridges knowledge gaps between CEM researchers and tool users, offering insights into mitigating modeling challenges without requiring deep technical derivations.

Conclusion: Future research directions are needed to improve the design capabilities for increasingly sophisticated superconducting circuit quantum devices, and the insights provided are valuable for researchers across multiple fields beyond the specific application area.

Abstract: High-fidelity numerical methods that model the physical layout of a device are essential for the design of many technologies. For methods that characterize electromagnetic effects, these numerical methods are referred to as computational electromagnetics (CEM) methods. Although the CEM research field is mature, emerging applications can still stress the capabilities of the techniques in use today. The design of superconducting circuit quantum devices falls in this category due to the unconventional material properties and important features of the devices covering nanometer to centimeter scales. Such multiscale devices can stress the fundamental properties of CEM tools which can lead to an increase in simulation times, a loss in accuracy, or even cause no solution to be reliably found. While these challenges are being investigated by CEM researchers, knowledge about them is limited in the broader community of users of these CEM tools. This review is meant to serve as a practical introduction to the fundamental aspects of the major CEM techniques that a researcher may need to choose between to model a device, as well as provide insight into what steps they may take to alleviate some of their challenges. Our focus is on highlighting the main concepts without rigorously deriving all the details, which can be found in many textbooks and articles. After covering the fundamentals, we discuss more advanced topics related to the challenges of modeling multiscale devices with specific examples from superconducting circuit quantum devices. We conclude with a discussion on future research directions that will be valuable for improving the ability to successfully design increasingly more sophisticated superconducting circuit quantum devices. Although our focus and examples are taken from this area, researchers from other fields will still benefit from the details discussed here.

</details>


### [58] [Rapid ground state energy estimation with a Sparse Pauli Dynamics-enabled Variational Double Bracket Flow](https://arxiv.org/abs/2511.21651)
*Chinmay Shrikhande,Arnab Bachhar,Aaron Rodriguez Jimenez,Nicholas J. Mayhall*

Main category: quant-ph

TL;DR: A variational double bracket flow algorithm using Sparse Pauli Dynamics achieves efficient ground state energy estimation for strongly correlated quantum systems with <1% error and significant speedups over DMRG.


<details>
  <summary>Details</summary>
Motivation: Ground state energy estimation for strongly correlated quantum systems is challenging, especially in higher dimensions where tensor network methods like DMRG struggle.

Method: Variational double bracket flow (vDBF) algorithm leveraging Sparse Pauli Dynamics with greedy operator selection, coefficient truncation, and energy-variance extrapolation.

Result: Achieved <1% error relative to DMRG benchmarks for Heisenberg and Hubbard models in 1D and 2D. For 100-qubit Heisenberg lattice: 10 minutes on single CPU vs 50+ hours on 64 threads for DMRG. Even larger speedup for 128-qubit Hubbard model.

Conclusion: Classical simulation techniques from quantum advantage benchmarking can provide practical tools for many-body physics problems.

Abstract: Ground state energy estimation for strongly correlated quantum systems remains a central challenge in computational physics and chemistry. While tensor network methods like DMRG provide efficient solutions for one-dimensional systems, higher-dimensional problems remain difficult. Here we present a variational double bracket flow (vDBF) algorithm that leverages Sparse Pauli Dynamics, a technique originally developed for classical simulation of quantum circuits, to efficiently approximate ground state energies. By combining greedy operator selection with coefficient truncation and energy-variance extrapolation, the method achieves less than 1% error relative to DMRG benchmarks for both Heisenberg and Hubbard models in one and two dimensions. For a 10x10 Heisenberg lattice (100 qubits), vDBF obtains accurate results in approximately 10 minutes on a single CPU thread, compared to over 50 hours on 64 threads for DMRG. For an 8x8 Hubbard model (128 qubits), the speedup is even more pronounced. These results demonstrate that classical simulation techniques developed in the context of quantum advantage benchmarking can provide practical tools for many-body physics.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [59] [Direct numerical simulation of thermo-diffusively unstable premixed hydrogen-air flames in a fully-developed turbulent channel flow at $Re_τ=530$](https://arxiv.org/abs/2511.20930)
*Felix Rong,Max Schneider,Hendrik Nicolai,Christian Hasse,Andrea Gruber*

Main category: physics.flu-dyn

TL;DR: DNS study of hydrogen-air flames in turbulent channel flow shows intrinsic thermo-diffusive phenomena enhance flame reactivity, with synergistic interactions between thermo-diffusive effects and wall turbulence significantly increasing flame speed.


<details>
  <summary>Details</summary>
Motivation: To understand how intrinsic thermo-diffusive phenomena interact with realistic near-wall shear turbulence in premixed hydrogen-air flames, particularly how varying turbulence intensity affects flame propagation characteristics.

Method: Direct Numerical Simulations (DNS) of premixed hydrogen-air flames in turbulent channel flow at Re_τ=530, analyzing two flame conditions (φ=0.25 and φ=0.35) with different propagation characteristics using local stretch factor I_0 to quantify reactivity enhancements.

Result: At φ=0.25, flame shows augmented I_0 in core flow due to thermo-diffusive interaction with weak turbulence. At φ=0.35, flame propagates toward walls with increasing turbulence intensity, and peak I_0 co-locates with peak Reynolds stresses (y+∼10), indicating strong synergistic interaction between thermo-diffusive phenomena and wall turbulence.

Conclusion: Thermo-diffusive phenomena significantly enhance flame reactivity in turbulent conditions, with particularly strong synergistic effects near walls where thermo-diffusive effects interact with wall turbulence to substantially increase flame speed.

Abstract: Direct Numerical Simulations (DNS) of premixed hydrogen-air flames anchored in a fully-developed turbulent channel flow (TCF) are performed at a friction Reynolds number of $\mathrm{Re}_τ=530$ and thermochemical conditions susceptible to the emergence of intrinsic thermo-diffusive (TD) phenomena acting on the turbulent flame. Two premixed flames are studied: a slower flame ($\varphi=0.25$), predominantly propagating within the core flow, and a faster one ($\varphi=0.35$), reaching closer to the channel walls and intermittently quenching on it.
  The present DNS database provides new insights into the characteristics of premixed flames susceptible to TD phenomena and propagating in realistic near-wall shear turbulence. The influence of varying turbulence intensity, and of wall-distance dependent time and length scales, on the flame propagation characteristics is evaluated through a detailed analysis of the local stretch factor $I_0$, quantifying reactivity enhancements caused by TD phenomena.
  At $\varphi=0.25$, the flame response to the fluid motions is mainly forced by the weaker turbulence present in the core flow. This results in an augmented $I_0$ compared to the laminar reference value, suggesting reactivity enhancement by the strongly non-linear interaction of TD phenomena with (relatively) weak turbulent motions present within the core flow. At $\varphi=0.35$, as the flame propagates from the core flow towards the channel walls, the flame response is forced by turbulence of increasing intensity, resulting in a corresponding augmentation of the Karlovitz number. Crucially, as the flame propagates into the near-wall region, the peak value of $I_0$ is co-located with the peak Reynolds stresses ($y^+ \sim 10$). This observation suggests a strong (local) synergistic interaction between TD phenomena and wall turbulence, ultimately resulting in significantly enhanced flame speed.

</details>


### [60] [$\texttt{CRLS}$: Convolutional Regularized Least Squares Framework for Reduced Order Modeling of Transonic Flows](https://arxiv.org/abs/2511.21425)
*Muhammad Bilal,Ashwin Renganathan*

Main category: physics.flu-dyn

TL;DR: CRLS framework combines Gaussian convolution smoothing, POD basis extraction, and regularized deconvolution to create accurate reduced-order models for transonic flows with shocks, outperforming standard POD methods.


<details>
  <summary>Details</summary>
Motivation: Standard POD-based reduced models perform poorly when snapshots contain parameter-dependent discontinuities like shocks, leading to smeared shocks, stair-stepping, or non-physical oscillations in transonic flow simulations.

Method: 1) Apply 1D Gaussian convolution with reflect padding to smooth snapshots, 2) Use Bayesian optimization to select convolution hyperparameters, 3) Extract POD bases from smoothed data, 4) Learn parametric dependence via RBF interpolation, 5) Perform regularized deconvolution to recover sharp shock structures.

Result: CRLS achieves markedly improved shock location and strength, lower surface-pressure and field-level errors, and 42% reduction in POD modes required to capture fixed energy fraction compared to standard POD and smoothed-POD baselines.

Conclusion: CRLS provides an accurate, data-efficient, and largely automated route to shock-aware reduced order models for high-speed aerodynamic design, effectively handling parameter-dependent discontinuities in transonic flows.

Abstract: We develop a convolutional regularized least squares ($\texttt{CRLS}$) framework for reduced-order modeling of transonic flows with shocks. Conventional proper orthogonal decomposition (POD) based reduced models are attractive because of their optimality and low online cost; however, but they perform poorly when snapshots contain parameter-dependent discontinuities, leading to smeared shocks, stair-stepping, or non-physical oscillations. In $\texttt{CRLS}$, we first map each full-order snapshot to a smoother representation by applying a one-dimensional Gaussian convolution with reflect padding along the flow field coordinates. The convolution hyperparameters (kernel width and support) are selected automatically by Bayesian optimization on a held-out set of snapshots. POD bases are then extracted from the smoothed data, and the parametric dependence of the POD coefficients is learned via radial basis function interpolation. To recover sharp shock structures, we introduce an efficient deconvolution step formulated as a regularized least squares problem, where the regularization centers the reconstruction around a nearest-neighbor reference snapshot in parameter space. The resulting $\texttt{CRLS}$ surrogate is evaluated on inviscid transonic flow over the RAE2822 airfoil, modeled by the steady compressible Euler equations solved with SU2 over a Latin hypercube sample of Mach number and angle of attack. Compared with standard POD and smoothed-POD baselines, $\texttt{CRLS}$ yields markedly improved shock location and strength, lower surface-pressure and field-level errors, and a $42$\% reduction in the number of POD modes required to capture a fixed fraction of snapshot energy. These results demonstrate that $\texttt{CRLS}$ provides an accurate, data-efficient, and largely automated route to shock-aware reduced order models for high-speed aerodynamic design.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [61] [A Review of Pseudospectral Optimal Control: From Theory to Flight](https://arxiv.org/abs/2511.20843)
*I. M. Ross,M. Karpenko*

Main category: math.OC

TL;DR: Pseudospectral optimal control theory combines pseudospectral methods with optimal control in Sobolev spaces, with successful flight demonstrations on NASA spacecraft and emerging embedded implementations.


<details>
  <summary>Details</summary>
Motivation: Both optimal control and pseudospectral theory share Sobolev spaces as their natural mathematical framework, making their combination theoretically natural for solving challenging aerospace control problems.

Method: Combines pseudospectral theory with optimal control theory to create pseudospectral optimal control theory, with implementation on NASA spacecraft and embedded platforms.

Result: Successful flight demonstrations onboard NASA spacecraft and the 2011 launch of pseudospectral optimal control in embedded platforms, enabling new solutions to challenging control problems.

Conclusion: Pseudospectral optimal control is changing how we solve challenging control problems in aerospace and autonomous systems through embedded implementations and flight-proven demonstrations.

Abstract: The home space for optimal control is a Sobolev space. The home space for pseudospectral theory is also a Sobolev space. It thus seems natural to combine pseudospectral theory with optimal control theory and construct ``pseudospectral optimal control theory,'' a term coined by Ross. In this paper, we review key theoretical results in pseudospectral optimal control that have proven to be critical for a successful flight. Implementation details of flight demonstrations onboard NASA spacecraft are discussed along with emerging trends and techniques in both theory and practice. The 2011 launch of pseudospectral optimal control in embedded platforms is changing the way in which we see solutions to challenging control problems in aerospace and autonomous systems.

</details>


<div id='cond-mat.supr-con'></div>

# cond-mat.supr-con [[Back]](#toc)

### [62] [Edge-Dependent Superconductivity in Twisted Bismuth Bilayers](https://arxiv.org/abs/2511.21657)
*Isaías Rodríguez,Renela M. Valladares,Alexander Valladares,David Hinojosa-Romero,Flor B. Quiroga,Ariel A. Valladares*

Main category: cond-mat.supr-con

TL;DR: Study shows twisted bismuth bilayers exhibit enhanced electronic density of states at flake edges due to structural disorder, potentially enabling significantly increased superconducting critical temperatures.


<details>
  <summary>Details</summary>
Motivation: Address boundary heterogeneity concerns in twisted bilayer materials, which are typically overlooked in twist-angle focused studies but are crucial for functional devices.

Method: Used ab initio density functional theory to systematically map electronic properties (band topology, density of states, superconductivity) and vibrational properties at flake edges of twisted bismuth bilayers.

Result: Found dramatic enhancement of electronic density of states at Fermi level (up to 10× perfect crystalline bismuth) at flake edges due to structural disorder, with twist-angle identified as critical parameter for topological enhancement.

Conclusion: Structural disorder at flake edges provides powerful mechanism for increasing superconducting Tc, establishes theoretical framework for twistronic heterogeneous materials, and opens path to search for metastable structures with enhanced superconductivity.

Abstract: Twisted bilayers offer a compelling and, at times, confounding platform for the engineering of new twistronic materials. Whereas standard studies almost exclusively focus on the explicit enigma that is presented by twist-angles, perhaps better epitomized by the related phenomena that have been observed in twisted bilayer graphene, functional devices necessarily face a fundamental concern: boundary heterogeneity in their structures. In this study, we address this concern by strictly investigating the electronic properties of twisted bismuth bilayers at the flake's edges and the vibrational properties of the flake. Twisted flakes exhibit continuous variations of these properties, away from the bulk, as we herein report using ab initio density functional theory, by systematically mapping the drastic evolution of band topology, electronic density of states, and possible superconductivity. Our work reveals a dramatic, non-fortuitous consequence of the structural disorder at the edges of the flakes: an enhanced electronic density of states at the Fermi level. This enhancement reaches a maximum of 10 times that of perfect-crystalline bismuth. Given that the superconducting critical temperature, Tc, is exponentially dependent on the electronic density of states at the Fermi level, this substantial structural variation immediately suggests a powerful mechanism for vastly increasing Tc. We also identify the twist-angle as a new critical parameter in designing novel engineering devices with topologically enhanced properties. Our results provide a necessary theoretical framework for interpreting new data for the upcoming generation of twistronic heterogeneous materials, and pave the way to search for atomic disordered metastable structures that could lead to enhanced superconducting transition temperatures.

</details>
