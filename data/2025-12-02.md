<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 26]
- [math.AP](#math.AP) [Total: 38]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 12]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.optics](#physics.optics) [Total: 2]
- [cs.LG](#cs.LG) [Total: 3]
- [gr-qc](#gr-qc) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [math.RA](#math.RA) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 5]
- [math-ph](#math-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 5]
- [math.DG](#math.DG) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Neural Multiscale Decomposition for Solving The Nonlinear Klein-Gordon Equation](https://arxiv.org/abs/2512.00266)
*Zhangyong Liang,Zhiping Mao,Xiaofei Zhao*

Main category: math.NA

TL;DR: NeuralMD: A neural multiscale decomposition method using multiscale time integrator and gated gradient correlation correction to solve nonlinear Klein-Gordon equation across relativistic to nonrelativistic regimes, overcoming spectral bias and propagation failure.


<details>
  <summary>Details</summary>
Motivation: The nonlinear Klein-Gordon equation (NKGE) with dimensionless parameter ε∈(0,1] exhibits challenging wave propagation with O(1) spatial wavelength and O(ε²) temporal wavelength, causing time oscillation. Existing collocation-based methods suffer from spectral bias (high-frequency time oscillation) and propagation failure (medium-frequency time oscillation), limiting their effectiveness across the relativistic to nonrelativistic regimes.

Method: Proposes NeuralMD with two key components: 1) Multiscale time integrator (MTI) to absorb high-frequency time oscillation into phase, decomposing NKGE into nonlinear Schrödinger equation with wave operator (NLSW) with well-prepared initial data and remainder equation with small initial data. 2) Gated gradient correlation correction strategy to enforce temporal coherence and alleviate propagation failure caused by medium-frequency time oscillation in collocation-based methods.

Result: The method achieves convergence rate O(ε²) as ε→0, with NKGE converging to NLSW. The remainder equation contribution becomes negligible, and approximation of remainder term is no longer affected by propagation failure. Comparative experiments show superior performance over existing collocation-based methods for solving NKGE with various initial data regularities across the entire regime.

Conclusion: NeuralMD effectively addresses both spectral bias and propagation failure in solving the nonlinear Klein-Gordon equation across relativistic to nonrelativistic regimes through multiscale decomposition and temporal coherence enforcement, demonstrating robust performance superior to existing methods.

Abstract: In this paper, we propose a neural multiscale decomposition method (NeuralMD) for solving the nonlinear Klein-Gordon equation (NKGE) with a dimensionless parameter $\varepsilon\in(0,1]$ from the relativistic regime to the nonrelativistic limit regime. The solution of the NKGE propagates waves with wavelength at $O(1)$ and $O(\varepsilon^2)$ in space and time, respectively, which brings the oscillation in time. Existing collocation-based methods for solving this equation lead to spectral bias and propagation failure. To mitigate the spectral bias induced by high-frequency time oscillation, we employ a multiscale time integrator (MTI) to absorb the time oscillation into the phase. This decomposes the NKGE into a nonlinear Schrödinger equation with wave operator (NLSW) with well-prepared initial data and a remainder equation with small initial data. As $\varepsilon \to 0$, the NKGE converges to the NLSW at rate $O(\varepsilon^{2})$, and the contribution of the remainder equation becomes negligible. Furthermore, to alleviate propagation failure caused by medium-frequency time oscillation, we propose a gated gradient correlation correction strategy to enforce temporal coherence in collocation-based methods. As a result, the approximation of the remainder term is no longer affected by propagation failure. Comparative experiments with existing collocation-based methods demonstrate the superior performance of our method for solving the NKGE with various regularities of initial data over the whole regime.

</details>


### [2] [Finite Difference Method for Global Stabilization of the Viscous Burgers' Equation with Nonlinear Neumann Boundary Feedback Control](https://arxiv.org/abs/2512.00317)
*Shishu Pal Singh,Sudeep Kundu*

Main category: math.NA

TL;DR: The paper develops finite difference schemes (θ-scheme) for stabilizing viscous Burgers' equation with nonlinear Neumann boundary feedback control, proving stability and convergence properties.


<details>
  <summary>Details</summary>
Motivation: To develop effective numerical methods for stabilizing the viscous Burgers' equation with nonlinear Neumann boundary feedback control, which has applications in fluid dynamics and control theory.

Method: Proposes a θ-scheme (θ∈[0,1]) that unifies explicit and implicit time discretizations for finite difference approximation. Uses discrete energy method for stability analysis and error analysis for convergence rates.

Result: The scheme is conditionally stable for 0≤θ<½ and unconditionally stable for θ≥½. Shows exponential stability of fully discrete solution. Achieves first-order convergence in L²-, H¹-, and L∞-norms for state variable (θ≥½) and boundary control inputs.

Conclusion: The proposed θ-scheme provides an effective numerical framework for stabilizing viscous Burgers' equation with nonlinear Neumann boundary feedback control, with proven stability and convergence properties validated by numerical experiments.

Abstract: This article focuses on a nonlinear Neumann boundary feedback control formulation for the viscous Burgers' equation and develops a class of finite difference schemes to achieve global stabilization. The proposed procedure, known as the $θ$-scheme with $θ\in [0,1]$, unifies explicit and implicit time discretizations and is suitable for handling the nonlinear boundary feedback control problem. Using the discrete energy method, we prove that the proposed difference scheme is conditionally stable for $0 \leq θ< \frac{1}{2}$ and unconditionally stable for $θ\geq \frac{1}{2}$. In addition, we establish the exponential stability of the fully discrete solution. The error analysis shows a first-order convergence rate of the state variable in the discrete $L^{2}$-, $H^{1}$-, and $L^{\infty}$-norms for $θ\geq \frac{1}{2}$, while preserving the exponential stability property. A first-order convergence rate for the boundary control inputs is also obtained. Numerical experiments are conducted to validate the theoretical findings and to demonstrate the effectiveness of the method for the inhomogeneous nonlinear Neumann boundary feedback control of the viscous Burgers' equation.

</details>


### [3] [Finite Element Analysis for the Chafee-Infante Equation Using Distributed Feedback Control](https://arxiv.org/abs/2512.00320)
*Shishu Pal Singh,Sudeep Kundu*

Main category: math.NA

TL;DR: A C⁰-conforming FEM for Chafee-Infante equation with finite-parameter feedback control, with error analysis for state/control variables in semi-discrete and fully discrete settings using backward Euler time discretization.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method for solving the Chafee-Infante equation with finite-parameter feedback control, which requires rigorous error analysis for both state and control variables in both spatially discretized and fully discrete settings.

Method: C⁰-conforming finite element method for spatial discretization, backward Euler method for time discretization, with error analysis for state and control variables in semi-discrete and fully discrete schemes.

Result: Established error estimates for state and control variables in both spatially discretized and fully discrete settings, with stability analysis of the fully discrete scheme, verified through numerical experiments.

Conclusion: The proposed numerical method effectively solves the Chafee-Infante equation with finite-parameter feedback control, with rigorous error analysis and numerical validation confirming theoretical results.

Abstract: In this paper, we propose a \( C^0 \)-conforming finite element method for the Chafee-Infante equation with a finite-parameter feedback control. We establish error analysis for both the state variable and the control variable for the spatially discretized solution. Furthermore, we employ the backward Euler method for time discretization and discuss the stability analysis of the fully discrete scheme. Additionally, we develop error estimates for both the state variable and the control input in the fully discrete setting. Finally, we verify our theoretical conclusions using some numerical experiments.

</details>


### [4] [Iterative inversion schemes for the Born series and the reduced inverse Born series](https://arxiv.org/abs/2512.00423)
*Akari Ishida,Manabu Machida*

Main category: math.NA

TL;DR: A fast Newton-type iterative scheme is developed to invert the Born series for nonlinear inverse problems, avoiding exponential computational cost of standard recursive methods while maintaining nonlinear capabilities.


<details>
  <summary>Details</summary>
Motivation: Nonlinear inverse problems have complex landscapes where naive iterative methods get trapped in local minima. The Born approximation avoids this but requires linearization. The inverse Born series solves nonlinear problems without linearization, but its standard recursive implementation has exponentially growing computational cost when including nonlinear terms.

Method: The authors revisit a Newton-type iterative scheme to invert the Born series and develop a fast variant. They show the relation between this fast scheme and the reduced inverse Born series.

Result: The paper presents a fast computational method for inverting the Born series that avoids the exponential cost of standard recursive implementations while maintaining the ability to handle nonlinear inverse problems without linearization.

Conclusion: The developed fast Newton-type scheme provides an efficient computational approach for solving nonlinear inverse problems using the inverse Born series, bridging the gap between computational feasibility and nonlinear inversion capabilities.

Abstract: Nonlinear inverse problems have complicated landscapes. Hence the calculation with naive iterative schemes (e.g., Gauss-Newton or conjugate gradients) is trapped in local minima. The (first) Born approximation can avoid this trapping but linearization is required. Nonlinear inverse problems can be solved without linearization by means of the inverse Born series. However, the computational cost of its standard recursive implementation grows exponentially when nonlinear terms are taken into account. In this work we revisit a Newton-type iterative scheme to invert the Born series and develop a fast variant. The relation between this fast scheme and the reduced inverse Born series is shown.

</details>


### [5] [Asymptotic Compatibility of the Approximate-Ball Finite Element Method for 2D Nonlocal Poisson Problem with Neumann Boundary Conditions](https://arxiv.org/abs/2512.00426)
*Yuchen Shi,Jihong Wang,Jiwei Zhang*

Main category: math.NA

TL;DR: Asymptotic compatibility error estimates for finite element discretization of 2D nonlocal Poisson problems with Neumann boundary conditions, showing convergence to classical local limit as horizon parameter vanishes.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous error estimates for finite element methods applied to nonlocal Poisson problems with Neumann boundary conditions, particularly focusing on how these methods behave as the nonlocal horizon parameter approaches zero (recovering the classical local limit).

Method: 1) Derive two types of nonlocal Neumann boundary operators based on nonlocal Green's identities. 2) Prove weak convergence of these operators to classical Neumann operator as horizon parameter δ→0 for convex domains. 3) Analyze asymptotic properties of nonlocal Neumann boundary-value problems. 4) Study asymptotic compatibility error estimates for approximate-ball-strategy finite element discretization.

Result: Theoretical analysis shows that the finite element discretization is asymptotically compatible, meaning the numerical solutions converge to the correct local limit as δ→0. Numerical examples confirm the theoretical convergence results.

Conclusion: The paper provides rigorous error estimates for finite element methods applied to nonlocal Poisson problems with Neumann conditions, demonstrating asymptotic compatibility as the nonlocal horizon vanishes, which is crucial for bridging nonlocal and classical local models.

Abstract: In this paper, asymptotic compatibility error estimates of a finite element discretization is presented for 2D nonlocal Poisson problems with Neumann boundary conditions. To this end, we begin with deriving two kind of nonlocal Neumann boundary operators based on nonlocal Green's identities, and establish the corresponding weak convergence to the classical Neumann operator as the horizon parameter δ vanishes for general convex domains. After that, we consider the asymptotic properties (i.e. the so-called local limit) of two nonlocal Neumann boundary-value problems as δ approaches zero. Finally, we analyze the asymptotical compatable error estimates of the approximate-ball-strategy finite element discretization proposed by D'Elia, Gunzburger, and Vollmann (2021), and provide numerical examples to confirm the theoretical results.

</details>


### [6] [Inverse spectral problem for glassy state relaxation approximated by Prony series](https://arxiv.org/abs/2512.00507)
*Shuli Chen,Marrten V. de Hoop,Youjun Deng,Ching-Lung Lin,Gen Nakamura*

Main category: math.NA

TL;DR: The paper examines the performance of an inversion method for identifying relaxation tensors in the Extended Burgers Model (EBM), which approximates stretched exponential relaxation functions for glassy state data analysis.


<details>
  <summary>Details</summary>
Motivation: Stretched exponential relaxation functions are inconvenient for data analysis due to singularity at the origin. The EBM (Prony series approximation) addresses this issue, but requires effective inversion methods to identify relaxation tensors from experimental data.

Method: Numerical examination of an inversion method previously developed by the authors that uses clustered eigenvalues of the quasi-static EBM to identify relaxation tensors.

Result: The performance analysis reveals that the inversion method is powerful for analyzing relaxation of glassy state data.

Conclusion: The inversion method for EBM relaxation tensor identification is effective and provides a practical tool for analyzing glassy state relaxation data.

Abstract: The stretched exponential relaxation function is used to analyze the relaxation of the glassy state data. Due to the singularity of this function at the origin, this function is inconvenient for data analysis. Concerning this, a Prony series approximation of the stretched exponential relaxation function (J. Mauro, Y. Mauro, 2018), which is the extended Burgers model (abbreviated by EBM) known for viscoelasticity equations, was introduced. In our previous paper [arXiv:2509.16714], we gave an inversion method to identify the relaxation tensor of the EBM using clustered eigenvalues of the quasi-static EBM. As a next important research subject of this study, we numerically examine the performance of the inversion method. The performance reveals that it is a powerful method of data analysis, analyzing the relaxation of the glassy state data.

</details>


### [7] [Projected iterated Tikhonov regularization in low precision](https://arxiv.org/abs/2512.00669)
*Chelsea Drum,James. G. Nagy,Lucas Onisk*

Main category: math.NA

TL;DR: Krylov subspace method for linear inverse problems in low precision achieves comparable accuracy to double precision through regularization effects.


<details>
  <summary>Details</summary>
Motivation: To investigate the regularizing behavior of iterative Krylov subspace methods for solving linear inverse problems using lower than double precision, as recent works have shown computational efficiency and additional regularization benefits from projecting iterated Tikhonov methods onto Krylov subspaces.

Method: Formulate the iterates as a filtered solution using the preconditioned Landweber method with a Tikhonov-type preconditioner in a Krylov subspace. Project the algorithm onto Krylov subspaces for severely ill-posed problems and test with multiple low precision choices.

Result: Numerical examples demonstrate the filtering properties of the method and show that comparable working accuracy (within a few decimal places in relative error) can be achieved for discrete inverse problems compared to results computed in traditional double precision.

Conclusion: Krylov subspace methods with Tikhonov-type preconditioning in low precision can provide effective regularization for severely ill-posed inverse problems while maintaining accuracy comparable to double precision computations.

Abstract: We investigate the regularizing behavior of an iterative Krylov subspace method for the solution of linear inverse problems in precisions lower than double. Recent works have considered the projection of iterated Tikhonov methods using Krylov subspaces for both computational efficiency and an additional regularizing effect. To investigate the regularizing behavior of this projected algorithm applied to problems that are naturally severely ill-posed, we formulate the iterates as a filtered solution using the preconditioned Landweber method with a Tikhonov-type preconditioner in a Krylov subspace. Through numerical examples simulating multiple low precision choices, we showcase the filtering properties of the method and the achievement of comparable working accuracy applied to discrete inverse problems (i.e., to within a few decimal places in relative error) compared to results computed in traditional double precision.

</details>


### [8] [Error analysis of an acceleration corrected diffusion approximation of Langevin dynamics with background flow](https://arxiv.org/abs/2512.00685)
*Yoichiro Mori,Chanoknun Sintavanuruk,Truong-Son P. Van*

Main category: math.NA

TL;DR: The paper analyzes error bounds for an acceleration-corrected advection-diffusion approximation to Langevin dynamics of inertial particles in background flow, showing O(ε) strong and O(ε²) weak error rates.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous error estimates for a popular approximation method used in turbulent transport studies of inertial particles, specifically the acceleration-corrected advection-diffusion approximation to Langevin dynamics.

Method: Mathematical analysis of error bounds in the averaging regime where dimensionless relaxation timescale ε is small parameter. Proves error estimates for finite time intervals and validates optimality through computational experiments.

Result: For any finite time interval, the approximation error is O(ε) in strong sense and O(ε²) in weak sense. Numerical evidence suggests the approximation also captures long-time behavior of Langevin dynamics.

Conclusion: The acceleration-corrected advection-diffusion approximation provides mathematically justified error bounds, with optimal rates verified computationally, making it a reliable tool for studying inertial particle transport in turbulent flows.

Abstract: We consider the problem of approximating the Langevin dynamics of inertial particles being transported by a background flow. In particular, we study an acceleration corrected advection-diffusion approximation to the Langevin dynamics, a popular approximation in the study of turbulent transport. We prove error estimates in the averaging regime in which the dimensionless relaxation timescale $\varepsilon$ is the small parameter. We show that for any finite time interval, the approximation error is of order $\mathcal{O}(\varepsilon)$ in the strong sense and $\mathcal{O}(\varepsilon^2)$ in the weak sense, whose optimality is checked against computational experiment. Furthermore, we present numerical evidence suggesting that this approximation also captures the long-time behavior of the Langevin dynamics.

</details>


### [9] [Coarse spaces using extended generalized eigenproblems for heterogeneous Helmholtz problems](https://arxiv.org/abs/2512.01002)
*Emile Parolin,Frédéric Nataf*

Main category: math.NA

TL;DR: Extends previous non-Hermitian coarse space construction to heterogeneous Helmholtz problems, moving from matrix-level to continuous-level analysis.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze domain decomposition preconditioners specifically for heterogeneous Helmholtz problems, building on previous general non-Hermitian framework but focusing on continuous formulation.

Method: Extends generalized eigenproblem-based coarse space construction from previous work to heterogeneous Helmholtz problems, performing derivation and analysis at continuous level rather than matrix level.

Result: Develops continuous-level framework for non-Hermitian domain decomposition preconditioners tailored to heterogeneous Helmholtz problems.

Conclusion: Successfully adapts previous non-Hermitian coarse space methodology to the specific case of heterogeneous Helmholtz problems with continuous-level analysis.

Abstract: An abstract construction of coarse spaces for non-Hermitian problems and non-Hermitian domain decomposition preconditioners based on extended generalized eigenproblems was proposed in [Nataf and Parolin, arXiv:2404.02758] and analyzed on the matrix formulation. Building upon this work, we consider instead here the specific case of heterogeneous Helmholtz problems, and the derivation and analysis is performed at the continuous level.

</details>


### [10] [A Provably Efficient Method for Tensor Ring Decomposition and Its Applications](https://arxiv.org/abs/2512.01016)
*Han Chen,Sitan Chen,Anru R. Zhang*

Main category: math.NA

TL;DR: First deterministic finite-step algorithm for exact tensor ring decomposition using blockwise simultaneous diagonalization, with extensions to symmetric TR and robust noisy recovery.


<details>
  <summary>Details</summary>
Motivation: Addresses open question about existence of deterministic finite-step algorithms for exact tensor ring decomposition, which has applications in quantum information, physics-based modeling, and machine learning foundations.

Method: Leverages blockwise simultaneous diagonalization to recover TR-cores from limited tensor observations, extends to symmetric TR setting, and develops robust recovery scheme combining initialization with alternating least squares for noisy data.

Result: First deterministic finite-step algorithm for exact TR decomposition, with reduced parameter complexity in symmetric setting, faster convergence and improved accuracy compared to classic methods for noisy observations.

Conclusion: Advances algorithmic foundations of TR decomposition, opens new opportunities for scalable tensor network computation, and provides new algorithms for matrix product state tomography and learning pushforward distributions.

Abstract: We present the first deterministic, finite-step algorithm for exact tensor ring (TR) decomposition, addressing an open question about the existence of such procedures. Our method leverages blockwise simultaneous diagonalization to recover TR-cores from a limited number of tensor observations, providing both algebraic insight and practical efficiency. We extend the approach to the symmetric TR setting, where parameter complexity is significantly reduced and applications arise naturally in physics-based modeling and exchangeable data analysis. To handle noisy observations, we develop a robust recovery scheme that couples our initialization with alternating least squares, achieving faster convergence and improved accuracy compared to classic methods. As applications, we obtain new algorithms for questions in other domains where tensor ring decomposition is a key primitive, namely matrix product state tomography in quantum information, and provable learning of pushforward distributions in the foundations of machine learning. These contributions advance the algorithmic foundations of TR decomposition and open new opportunities for scalable tensor network computation.

</details>


### [11] [Randomized-Accelerated FEAST: A Hybrid Approach for Large-Scale Eigenvalue Problems](https://arxiv.org/abs/2512.01257)
*Ayush Nadiger*

Main category: math.NA

TL;DR: RA-FEAST combines contour-integration eigensolvers with randomized linear algebra for faster partial eigendecomposition of large matrices in statistical applications, achieving up to 38x speedup on sparse graph Laplacians.


<details>
  <summary>Details</summary>
Motivation: Need efficient computation of partial eigendecompositions for large-scale matrices in statistical applications, particularly for spectral methods dealing with sparse graph Laplacians where standard methods are computationally expensive.

Method: Hybrid algorithm combining contour-integration-based FEAST eigensolver with randomized numerical linear algebra techniques, using randomized subspace initialization to enable aggressive quadrature reduction and truncated refinement iterations.

Result: Achieves significant computational speedups (up to 38x on sparse graph Laplacian benchmarks at n=8000) while maintaining high-accuracy approximations; more than an order of magnitude faster than standard FEAST while preserving accuracy.

Conclusion: RA-FEAST provides an efficient hybrid approach for partial eigendecomposition in statistical applications, with theoretical error bounds and stability guarantees, demonstrating practical speed advantages for sparse Laplacian problems common in modern spectral methods.

Abstract: We present Randomized-Accelerated FEAST (RA-FEAST), a hybrid algorithm that combines contour-integration-based eigensolvers with randomized numerical linear algebra techniques for efficiently computing partial eigendecompositions of large-scale matrices arising in statistical applications. By incorporating randomized subspace initialization to enable aggressive quadrature reduction and truncated refinement iterations, our method achieves significant computational speedups (up to 38x on sparse graph Laplacian benchmarks at n = 8000) while maintaining high-accuracy approximations to the target eigenspace. We provide a probabilistic error bound for the randomized warmstart, a stability result for inexact FEAST iterations under general perturbations, and a simple complexity model characterizing the trade-off between initialization cost and solver speedup. Empirically, we demonstrate that RA-FEAST can be more than an order of magnitude faster than standard FEAST while preserving accuracy on sparse Laplacian problems representative of modern spectral methods in statistics.

</details>


### [12] [A high-order weighted positive and flux conservative method for the Vlasov equation](https://arxiv.org/abs/2512.01322)
*Takashi Minoshima,Yosuke Matsumoto*

Main category: math.NA

TL;DR: A 5th-order conservative, positivity-preserving, non-oscillatory scheme for Vlasov equation using convex combination of substencil polynomials with nonlinear weights prioritizing larger L2 norms.


<details>
  <summary>Details</summary>
Motivation: To develop a high-order numerical scheme for Vlasov equation that maintains positivity, avoids oscillations, and improves conservation properties while achieving high accuracy.

Method: Uses convex combination of positive and non-oscillatory polynomials from substencils with nonlinear weights that prioritize substencils with larger L2 norm. Formal 5th-order accuracy achieved through this weighted combination approach.

Result: Scheme achieves 5th-order accuracy while preserving positivity and avoiding oscillations. Spectral analysis shows better properties than underlying 5th-order scheme and even surpasses 7th-order scheme in some wavenumber ranges. Successfully applied to 1D Vlasov-Ampere and 2D Vlasov-Maxwell equations with improved entropy conservation.

Conclusion: The developed scheme provides an effective high-order method for Vlasov equations that maintains critical physical properties (positivity, non-oscillatory behavior) while achieving superior spectral properties and improved conservation of entropy.

Abstract: We present a high-order conservative, positivity-preserving, and non-oscillatory scheme for solving the Vlasov equation. The scheme attains formal fifth-order accuracy through a convex combination of positive and non-oscillatory polynomials in substencils. Nonlinear weights for these polynomials are formulated that assign higher priority to substencils with larger L2 norm to enhance resolution while maintaining positivity and non-oscillatory properties. An approximate dispersion relation indicates that the spectral properties of the present scheme outperform those of an underlying fifth-order scheme and even surpass those of a seventh-order scheme in certain wavenumber ranges. We apply this scheme to the one-dimensional Vlasov-Ampere equations and the two-dimensional Vlasov-Maxwell equations, and demonstrate high-resolution simulations with improved conservation of entropy.

</details>


### [13] [A note on the central-upwind scheme for nonlocal conservation laws](https://arxiv.org/abs/2512.01344)
*Jan Friedrich,Samala Rathan,Sanjibanee Sudha*

Main category: math.NA

TL;DR: Derivation and analysis of central-upwind flux schemes for nonlocal conservation laws, including first-order convergence proof and second-order extension with numerical comparisons.


<details>
  <summary>Details</summary>
Motivation: The central-upwind flux is widely used for local conservation laws but needs adaptation for nonlocal conservation laws. The paper aims to derive and analyze central-upwind schemes specifically for nonlocal conservation laws, addressing the challenges posed by nonlocal terms.

Method: Detailed derivation of fully-discrete second-order scheme with focus on nonlocal terms. Derivation of central-upwind flux for a class of nonlocal conservation laws using an estimate on nonlocal speed to fix nonlocality at cell interfaces. Convergence proof for first-order scheme and extension to second-order scheme under additional assumptions.

Result: Proved convergence of first-order numerical scheme to correct solution. Presented similar result for second-order central-upwind scheme under additional assumptions. Numerical examples comparing central-upwind schemes to Godunov-type schemes and fully-discrete scheme.

Conclusion: Central-upwind flux schemes can be successfully adapted for nonlocal conservation laws with proper handling of nonlocal terms, providing convergent numerical methods that compare favorably with existing approaches.

Abstract: The central-upwind flux is a widely used numerical flux function for local conservation laws. It has been investigated by Kurganov and Polizzi (2009) for a specific nonlocal conservation law and can be derived from a fully-discrete second-order scheme. Here, we derive this fully-discrete scheme in detail with a particular focus on the occurring nonlocal terms. In addition, we derive the central-upwind flux for a class of nonlocal conservation laws and use an estimate on the nonlocal speed which fixes the nonlocality at the cell interfaces. We prove that the resulting first-order numerical scheme converges to the correct solution. Under additional assumptions on the analytical flux we present a similar result for a second-order central-upwind scheme. Numerical examples compare the central-upwind schemes to Godunov-type schemes and the fully-discrete scheme.

</details>


### [14] [The Power Method for Non-Hermitian Dual Quaternion Matrices](https://arxiv.org/abs/2512.01414)
*Hao Yang,Liqun Qi,Chunfeng Cui*

Main category: math.NA

TL;DR: Power method for computing dominant eigenvalues of non-Hermitian dual quaternion matrices, with theoretical analysis addressing complex eigenvalue existence and convergence properties.


<details>
  <summary>Details</summary>
Motivation: Non-Hermitian dual quaternion matrices have complex eigenvalue behavior (may have no eigenvalues or infinitely many), and their eigenvalues are not necessarily dual numbers, requiring new theoretical analysis and computational methods.

Method: Power method adapted for non-Hermitian DQMs, with new Jordan-like decomposition to address lack of conventional Jordan decomposition. Also develops adjoint method for dual complex matrices.

Result: Establishes sufficient conditions for eigenvalue existence, proves linear convergence of power method to strict dominant eigenvalue under stronger conditions, and verifies condition necessity. Numerical experiments demonstrate efficiency.

Conclusion: Successfully extends power method to non-Hermitian dual quaternion matrices with rigorous theoretical foundation, addressing complex eigenvalue behavior through novel decomposition techniques.

Abstract: This paper proposes a power method for computing the dominant eigenvalues of a non-Hermitian dual quaternion matrix (DQM). Although the algorithmic framework parallels the Hermitian case, the theoretical analysis is substantially more complex since a non-Hermitian dual matrix may possess no eigenvalues or infinitely many eigenvalues. Besides, its eigenvalues are not necessarily dual numbers, leading to non-commutative behavior that further complicates the analysis. We first present a sufficient condition that ensures the existence of an eigenvalue whose standard part corresponds to the largest magnitude eigenvalue of the standard part matrix. Under a stronger condition, we then establish that the sequence generated by the power method converges linearly to the strict dominant eigenvalue and its associated eigenvectors. We also verify that this condition is necessary. The key to our analysis is a new Jordan-like decomposition, which addresses a gap arising from the lack of a conventional Jordan decomposition for non-Hermitian dual matrices. Our framework readily extends to non-Hermitian dual complex and dual number matrices. We also develop an adjoint method that reformulates the eigenvalue problem into an equivalent form for dual complex matrices. Numerical experiments on non-Hermitian DQMs are presented to demonstrate the efficiency of the power method.

</details>


### [15] [Feedback Integrators Revisited](https://arxiv.org/abs/2512.01528)
*Juho Bae,Dong Eui Chang*

Main category: math.NA

TL;DR: The paper provides theoretical guarantees for Feedback Integrators, proving preservation of first integrals over entire integration region with arbitrarily small deviation, and introduces an adaptive gain scheme that improves performance.


<details>
  <summary>Details</summary>
Motivation: Feedback Integrators were introduced in 2016 for numerical integration on manifolds while preserving first integrals, but their performance was only proven asymptotically, creating a gap between empirical success and theoretical understanding.

Method: The authors prove preservation of first integrals over entire integration region up to arbitrarily small deviation under the Feedback Integrator framework, and propose an adaptive gain selection scheme to improve performance.

Result: Numerical demonstrations on free rigid body motion in SO(3), the Kepler problem, and a perturbed Kepler problem with rotational symmetry show improved performance. All codes are publicly available.

Conclusion: The paper bridges the theoretical gap for Feedback Integrators by providing rigorous guarantees for first integral preservation over entire integration regions and introduces practical improvements through adaptive gain selection.

Abstract: We revisit the notion of Feedback Integrators introduced by D. E. Chang in 2016. Feedback integrators allow for numerically integrating dynamical systems on manifold while preserving the first integrals of the system. However, its performance was stated and proved in an asymptotic manner, which left a gap between its empirical success and theoretical understandings. In response, we prove preservation of first integrals over entire integration region up to arbitrarily small deviation under Feedback Integrator framework. Furthermore, we propose an adaptive gain selection scheme that significantly improves the performance. Numerical demonstrations are conducted on free rigid body motion in SO(3), the Kepler problem, and a perturbed Kepler problem with rotational symmetry. All demonstration codes are available at: https://github.com/johnbae1901/Feedback-Integrator.

</details>


### [16] [Tree-cotree gauging for two-dimensional hierarchical splines](https://arxiv.org/abs/2512.01580)
*Melina Merkel,Rafael Vázquez*

Main category: math.NA

TL;DR: Extends tree-cotree gauging technique from finite elements/isogeometric analysis to hierarchical splines for adaptive refinement, enabling unique solutions in magnetostatics problems.


<details>
  <summary>Details</summary>
Motivation: Magnetostatics and eddy current problems using magnetic vector potential formulation lack solution uniqueness due to gauge freedom. Tree-cotree decomposition addresses this but needs extension to hierarchical splines for adaptive refinement.

Method: Extends tree-cotree gauging to hierarchical splines by constructing spanning trees for each refinement level. For degree p=1, this also applies to quadrilateral finite element meshes with hanging nodes.

Result: Numerical results for Maxwell eigenvalue problem demonstrate correctness of the method for hierarchical splines with adaptive refinement.

Conclusion: Successfully extends tree-cotree gauging technique to hierarchical splines, enabling unique solutions in magnetostatics problems with adaptive refinement, also applicable to finite element meshes with hanging nodes.

Abstract: In magnetostatics and eddy current problems, formulated in terms of the magnetic vector potential, the solution is not unique, because the addition of an irrotational function to the solution remains a valid solution. The tree-cotree decomposition is a gauging technique to recover uniqueness when using finite elements, which consists in considering the mesh as a graph, and building a spanning tree on that graph. The idea has been recently extended to isogeometric analysis, applying the construction of the spanning tree on the control mesh, or equivalently, on the Greville grid. In the present paper we extend the construction to hierarchical splines, a set of splines with multi-level structure for adaptive refinement, by constructing a spanning tree for each single level. Since for degree $p=1$ the spaces of finite elements and hierarchical splines coincide, the presented construction is also valid for quadrilateral finite element meshes with hanging nodes. To assess the correctness of the method, we present numerical results for Maxwell eigenvalue problem.

</details>


### [17] [Convergence of long-time stable variable-step arbitrary order ETD-MS scheme for gradient flows with Lipschitz nonlinearity](https://arxiv.org/abs/2512.01601)
*Wenbin Chen,Zhaohui Fu,Shun Wang,Xiaoming Wang*

Main category: math.NA

TL;DR: Variable-step extension of high-order exponential time differencing multistep schemes with unconditional stability and optimal-order convergence for thin film epitaxial growth models.


<details>
  <summary>Details</summary>
Motivation: To develop efficient time-adaptive solutions for stiff PDEs like thin film epitaxial growth models by extending existing high-order ETD-MS schemes to variable time-step implementations while maintaining stability and accuracy.

Method: Extends family of high-order exponential time differencing multistep (ETD-MS) schemes to variable-step versions. Proves unconditional stability via modified energy dissipation and establishes optimal-order convergence under mild conditions on time-step size and local ratio. Tests with novel variable-step second-order scheme on thin film epitaxial growth model without slope selection.

Result: Schemes are unconditionally stable with monotonically decreasing modified energy (perturbation of original energy) for Lipschitz nonlinearities. Optimal-order convergence achieved under mild time-step conditions. Numerical experiments validate theoretical findings and demonstrate potential for highly efficient time-adaptive solutions.

Conclusion: Variable-step ETD-MS schemes provide unconditionally stable, optimally convergent methods for stiff PDEs, enabling efficient time-adaptive solutions as demonstrated on thin film epitaxial growth models.

Abstract: We analyze a variable-step extension of a family of arbitrarily high-order exponential time differencing multistep (ETD-MS) schemes recently developed by the authors. We prove that the schemes are unconditionally stable in the sense that a modified energy-representing a slight perturbation of the original energy-decreases monotonically over time, provided the nonlinearity is Lipschitz continuous in some appropriate sense. Moreover, we establish optimal-order convergence under mild conditions on the time-step size and local time-step ratio. Numerical experiments on the thin film epitaxial growth model without slope selection, employing a novel variable-step second-order scheme, validate the theoretical findings as well as its potential in developing highly efficient time-adaptive solution.

</details>


### [18] [Ergodicity and invariant measure approximation of the stochastic Cahn-Hilliard equation via an explicit fully discrete scheme](https://arxiv.org/abs/2512.01621)
*Nan Deng,Yibo Wang,Wanrong Cao*

Main category: math.NA

TL;DR: This paper develops a complete analytical and numerical framework for studying the stochastic Cahn-Hilliard equation with additive space-time white noise, including refined ergodic theory, a fully discrete numerical scheme with uniform convergence, and error bounds for invariant measure approximation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to provide a comprehensive framework for investigating the long-time statistical behavior of the stochastic Cahn-Hilliard equation (SCHE), extending classical ergodic theory results and developing reliable numerical methods for approximating long-time dynamics and invariant measures.

Method: The authors refine analytical ergodic theory by proving unique invariant measures in more regular state spaces, then introduce an explicit fully discrete scheme combining finite-difference spatial discretization with a strongly tamed exponential Euler method. They establish uniform moment bounds, strong convergence estimates, and prove geometric ergodicity using mass-preserving minorization tailored to Neumann boundary conditions.

Result: Key results include: extension of invariant measure existence to H_α space, uniform-in-time moment bounds in L^∞-norm, explicit strong convergence rates for fully discrete approximation, geometric ergodicity of numerical scheme, polynomial-order error bounds for invariant measure approximation, and strong laws of large numbers for both continuous and discrete systems.

Conclusion: The paper provides a complete analytical and numerical framework for studying long-time statistical behavior of SCHE, with theoretical guarantees for numerical approximation of invariant measures and ergodic limits, validated by numerical experiments.

Abstract: This paper investigates the stochastic Cahn-Hilliard equation (SCHE) driven by additive space-time white noise. We first refine the analytical ergodic theory by proving that the continuum equation admits a unique invariant measure in the more regular state space H_α, extending the classical result of Da Prato and Debussche (1996) on the negative Sobolev space $\dot{H}^{-1}_α$. To approximate long-time behaviour, we introduce an explicit fully discrete scheme that combines a finite-difference spatial discretization with a strongly tamed exponential Euler method in time. Uniform-in-time moment bounds in the $L^\infty$-norm are established for the numerical solution, and a uniform strong convergence estimate with an explicit rate is derived for the fully discrete approximation. Exploiting a mass-preserving minorization tailored to Neumann boundary conditions, we further show that the numerical scheme is geometrically ergodic and possesses a unique invariant measure, together with polynomial-order error bounds for approximating the exact invariant measure. Strong laws of large numbers are proved for both the continuous and discrete systems, ensuring almost-sure convergence of temporal averages to the corresponding ergodic limits. Numerical experiments corroborate the theoretical findings, including the long-time strong convergence and the accuracy of invariant measure approximation. Overall, the results provide a complete analytical and numerical framework for investigating the long-time statistical behaviour of the SCHE.

</details>


### [19] [A Two-Stage Fourth-Order Implicit Scheme for Stiff problems](https://arxiv.org/abs/2512.01628)
*Zhixin Huo*

Main category: math.NA

TL;DR: Novel two-stage fourth-order implicit scheme that overcomes limitations of existing explicit spatiotemporal coupling methods, enabling efficient handling of stiff problems while maintaining fourth-order accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing spatiotemporal coupling two-stage fourth-order methods are limited to explicit frameworks, which suffer from CFL condition constraints on computational efficiency for stiff problems. This conflicts with the core advantage of spatiotemporal coupled methods that embed stiff source terms.

Method: Develops a novel two-stage fourth-order implicit scheme by fully leveraging temporal derivatives of physical variables within a compact mathematical framework. Establishes sufficient condition for A-stability through theoretical and numerical investigation, and provides Newton iterative procedure for convergence acceleration.

Result: The scheme achieves fourth-order temporal accuracy in only two stages. Extensive numerical experiments on classical stiff problems confirm the method's effectiveness and competitiveness.

Conclusion: The proposed implicit scheme successfully overcomes the limitations of explicit spatiotemporal coupling methods, enabling efficient computation of stiff problems while maintaining high-order accuracy and stability.

Abstract: This paper presents a novel two-stage fourth-order implicit scheme designed to overcome the limitations of existing spatiotemporal coupling two-stage fourth-order methods, which have thus far been confined to explicit frameworks. In such frameworks, computational efficiency is severely constrained by the CFL condition when addressing stiff problems, and they fundamentally conflicts with spatiotemporal coupled methods whose core advantage lies in embedding stiff source terms. By fully leveraging temporal derivatives of physical variables within a rigorously derived compact mathematical framework, the scheme achieves fourth-order temporal accuracy in only two stages. Furthermore, a sufficient condition for A-stability is established through systematic theoretical and numerical investigation, and a Newton iterative procedure is provided to accelerate convergence. Extensive numerical experiments on classical stiff problems confirm the method's effectiveness and competitiveness.

</details>


### [20] [Two-level additive Schwarz preconditioners for reduced integration methods](https://arxiv.org/abs/2512.01706)
*Filipe Cumaru,Alexander Heinlein,Joachim Schöberl*

Main category: math.NA

TL;DR: Two-level overlapping additive Schwarz method with RGDSW coarse space for scalable parallel solution of Stokes equations discretized via reduced integration penalty method.


<details>
  <summary>Details</summary>
Motivation: Stokes equations for incompressible flow yield large, ill-conditioned linear systems after discretization. Need scalable parallel solvers for efficient computation.

Method: Reduced integration penalty method for Stokes equations (eliminates pressure unknowns). Two-level overlapping additive Schwarz preconditioner with reduced dimension GDSW (RGDSW) coarse space. Implemented using FROSch package and NGSolve library.

Result: Investigation of numerical scalability of the proposed method. Parallel implementation demonstrated using FROSch and NGSolve.

Conclusion: The RGDSW coarse space enables scalable parallel solution of Stokes equations discretized via reduced integration penalty method, with practical implementation using available software libraries.

Abstract: Incompressible fluid flow problems appear frequently in different applications. The discretization of such problems may result in large and ill-conditioned systems of linear equations. We consider the case of the Stokes equations discretized using a reduced integration method which approximates the incompressibility constraint by a penalty term thus allowing the problem to be solved only in terms of the velocity unknowns. We investigate the numerical scalability of a two-level overlapping additive Schwarz method with a reduced dimension generalized Dryja-Smith-Widlund (RGDSW) coarse space. In addition, we discuss the parallel implementation of the examples using the Fast and Robust Overlapping Schwarz (FROSch) package for additive Schwarz preconditioners and the NGSolve library, which implements multiple finite element space formulations.

</details>


### [21] [A decoupled, unconditionally stable and second-order integrator for the Landau--Lifshitz--Gilbert equation with magnetoelastic effects](https://arxiv.org/abs/2512.01741)
*Martin Kružík,Hywel Normington,Michele Ruggeri*

Main category: math.NA

TL;DR: A fully discrete numerical scheme for magnetostriction modeling combining Newmark-β for displacement and midpoint method for magnetization, proven unconditionally stable with second-order accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop a stable and accurate numerical method for solving the coupled nonlinear PDE system modeling magnetostriction in small-strain regime, which combines magnetization dynamics (Landau-Lifshitz-Gilbert) with mechanical displacement.

Method: First-order finite elements for spatial discretization, time discretization using Newmark-β scheme for displacement and midpoint scheme for magnetization, implemented in a decoupled fashion resulting in a fully linear method.

Result: The method is formally second-order in time, proven unconditionally stable, and numerical experiments demonstrate good stability, accuracy, and energy conservation properties.

Conclusion: The proposed decoupled linear scheme provides an effective numerical approach for magnetostriction problems with proven stability and second-order accuracy, validated through comprehensive numerical experiments.

Abstract: We consider the numerical approximation of a nonlinear system of partial differential equations modeling magnetostriction in the small-strain regime consisting of the Landau--Lifshitz--Gilbert equation for the magnetization and the conservation of linear momentum law for the displacement. We propose a fully discrete numerical scheme based on first-order finite elements for the spatial discretization. The time discretization employs a combination of the classical Newmark-$β$ scheme for the displacement and the midpoint scheme for the magnetization, applied in a decoupled fashion. The resulting method is fully linear, formally of second order in time, and we prove that it is unconditionally stable. Finally, we assess the stability, accuracy, and energy conservation properties of the proposed method in a collection of numerical experiments.

</details>


### [22] [The mixed discontinuous Galerkin method for the Oseen eigenvalue problem](https://arxiv.org/abs/2512.01839)
*Lingling Sun,Shixi Wang,Hai Bi,Yidu Yang*

Main category: math.NA

TL;DR: A mixed discontinuous Galerkin method using Pk-Pk-1 elements is developed for solving the Oseen eigenvalue problem, with adjoint-consistent formulation, optimal error estimates, and reliable a posteriori error estimators validated through numerical computations.


<details>
  <summary>Details</summary>
Motivation: The Oseen eigenvalue problem is crucial for fluid stability analysis but is non-self-adjoint due to convection fields, requiring specialized numerical methods that can handle this complexity while providing accurate and reliable solutions.

Method: Mixed discontinuous Galerkin method with Pk-Pk-1 elements (k≥1) for Oseen eigenvalue problems in 2D/3D, featuring adjoint-consistent DG formulation, derivation of optimal a priori error estimates, and development of residual-type a posteriori error estimators.

Result: Theoretical proofs of reliability and effectiveness for eigenfunction estimators, reliability for eigenvalue estimators, and numerical validation showing computational efficiency and high-accuracy eigenvalue approximations on both uniform and adaptive meshes.

Conclusion: The proposed mixed DG scheme provides an effective, reliable, and computationally efficient approach for solving non-self-adjoint Oseen eigenvalue problems with proven error estimates and validated numerical performance.

Abstract: The Oseen eigenvalue problem plays a important role in the stability analysis of fluids. The problem is non-self-adjoint due to the presence of convection field. In this paper, we present a comprehensive investigation of the mixed discontinuous Galerkin (DG) method, employing Pk-Pk-1(k>=1) elements to solve the Oseen eigenvalue problem in Rd(d=2,3). We first develop an adjoint-consistent DG formulation for the problem. We then derive optimal a priori error estimates for the approximate eigenpairs, and propose residual type a posteriori error estimators. Furthermore, we prove the reliability and effectiveness of these estimators for approximate eigenfunctions, as well as the reliability of the estimator for approximate eigenvalues. To validate our approach, we conduct numerical computations on both uniform and adaptively refined meshes. The numerical results demonstrate that our scheme is computationally efficient and capable of yielding high-accuracy approximate eigenvalues.

</details>


### [23] [Uniform Norm Error Estimates for 2D Turning Point Problem](https://arxiv.org/abs/2512.01841)
*Shallu,Sudipto Chowdhury,Vikas Gupta*

Main category: math.NA

TL;DR: Finite element error analysis for 2D singularly perturbed convection-diffusion turning point problem using Shishkin mesh, proving uniform convergence in maximum norm for coarse and x-layer regions with improved linear convergence order.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of error analysis for coarse regions in 2D singularly perturbed turning point problems, and improving upon existing convergence results (specifically Stynes' work).

Method: Finite element method with layer-adapted Shishkin mesh, using discrete Green's function properties for error analysis to prove uniform convergence.

Result: Proved uniform convergence in maximum norm for coarse layer and x-layer regions, achieving linear order of convergence that is better than Stynes' article.

Conclusion: The method is robust and accurate for capturing sharp solution layers in 2D singularly perturbed turning point problems, with improved convergence rates and comprehensive error analysis including previously unaddressed coarse regions.

Abstract: This work presents error analysis for a finite element method applied to a two-dimensional singularly perturbed convection-diffusion turning point problem. Utilizing a layer-adapted Shishkin mesh, we prove uniform convergence in the maximum norm in the coarse layer and x-layer regions. The analysis, critically based on the properties of a discrete Green's function, guarantees the method's robustness and accuracy in capturing sharp solution layers. There is no work for the coarse region for two-dimensional singularly perturbed turning point problems, and also, we are getting a linear order of convergence better than Stynes' article.

</details>


### [24] [Preconditioning a Fluid--Structure Interaction Problem Using Monolithic and Block Domain Decomposition Methods for the Fluid](https://arxiv.org/abs/2512.01887)
*Axel Klawonn,Jascha Knepper,Lea Saßmannshausen*

Main category: math.NA

TL;DR: Monolithic FSI solver with FaCSI preconditioner outperforms SIMPLE approach for patient-specific artery simulations.


<details>
  <summary>Details</summary>
Motivation: To develop robust and scalable preconditioners for monolithic fluid-structure interaction problems, particularly for realistic biomedical applications like patient-specific arteries.

Method: Monolithic coupling of fluid, structure, and geometry subproblems using GMRES solver accelerated with FaCSI block preconditioner. Fluid subproblem approximated with either monolithic or SIMPLE preconditioner, with two-level overlapping Schwarz methods for inverse approximations.

Result: Monolithic preconditioning of fluid subproblem performs better than SIMPLE approach. Method tested with different flow rates and shows good parallel strong scaling.

Conclusion: Monolithic preconditioning approach is more effective than SIMPLE for FSI problems in patient-specific artery simulations, demonstrating better robustness and scalability.

Abstract: A fluid-structure interaction (FSI) problem is solved via a monolithic coupling of the fluid, structure, and geometry subproblems. The iterative GMRES solver is accelerated with the FaCSI block preconditioner. In the FaCSI factorization, the fluid subproblem is approximated using either a monolithic preconditioner or the block preconditioner SIMPLE. Two-level overlapping Schwarz methods are then used to approximate the arising inverses. The robustness and scalability of the monolithic and SIMPLE preconditioners are compared for a realistic patient-specific artery. The results indicate that the monolithic preconditioning of the fluid subproblem performs better than the SIMPLE approach. Different flow rates are tested and parallel strong scaling has been evaluated.

</details>


### [25] [The Lebesgue constant for uniform approximation of differential forms](https://arxiv.org/abs/2512.01944)
*Ludovico Bruni Bruno,Fwderico Piazzon*

Main category: math.NA

TL;DR: The paper analyzes approximation of differential forms from weak data via projection operators, showing their norm equals the Lebesgue constant under certain conditions, and estimates how this constant changes under smooth domain mappings.


<details>
  <summary>Details</summary>
Motivation: To develop approximation methods for differential forms using weak data (integration on rectifiable sets), which is relevant for numerical methods like finite elements where such approximations are needed.

Method: Study projection operators L defined by generalized weighted least squares or interpolation for approximating differential forms from weak data. Analyze the operator norm and its relationship to the Lebesgue constant under measure-theoretic conditions.

Result: Under natural measure-theoretic conditions, the norm of the projection operator equals the Lebesgue constant of the approximation problem. The paper also provides estimates for how the Lebesgue constant varies under smooth mappings between reference and physical domains.

Conclusion: The work establishes fundamental connections between projection operator norms and Lebesgue constants for differential form approximation, with practical implications for finite element methods where domain transformations are common.

Abstract: In this work we address the problem of uniform approximation of differential forms starting from weak data defined by integration on rectifiable sets. We study approximation schemes defined by the projection operator L given by either generalized weighted least squares or interpolation. We show that, under a natural measure theoretic condition, the norm of such operator equals the Lebesgue constant of the problem. We finally estimate how the Lebesgue constant varies under the action of smooth mappings from the reference domain to a physical one, as is customarily done e.g. in finite element method.

</details>


### [26] [Basis Choices for Frequency Domain Statistical Independence Tests and Algorithms for Algebraic Relation Extraction](https://arxiv.org/abs/2512.01963)
*Juan Shi,Wenbo Wang,Wan Zhang,Han Bao,Sergio Chavez,Jingfang Huang,Yichao Wu,Kai Zhang*

Main category: math.NA

TL;DR: The paper analyzes how different basis function selections affect frequency domain statistical independence tests and studies algorithms for extracting low-dimensional algebraic relations from dependent data.


<details>
  <summary>Details</summary>
Motivation: To understand how different basis function choices impact the effectiveness of frequency domain techniques in statistical independence testing, and to develop robust methods for extracting algebraic relations from dependent data.

Method: Examines various complete orthonormal basis functions (Legendre polynomials, Fourier series, Walsh functions, standard and nonstandard Haar wavelets), uses fast transformation algorithms to convert physical domain data to frequency domain coefficients, and studies different optimization formulations for relation extraction.

Result: Numerical results demonstrate the effectiveness of frequency domain-based statistical analysis methods and provide guidance for selecting appropriate basis functions and algorithms for detecting specific types of relations.

Conclusion: The selection of basis functions significantly affects statistical power in independence tests, especially for small noisy datasets, and different optimization formulations show varying stability for extracting algebraic relations from dependent data.

Abstract: In this paper, we explore how different selections of basis functions impact the efficacy of frequency domain techniques in statistical independence tests, and study different algorithms for extracting low-dimensional algebraic relations from dependent data. We examine a range of complete orthonormal bases functions including the Legendre polynomials, Fourier series, Walsh functions, and standard and nonstandard Haar wavelet bases. We utilize fast transformation algorithms to efficiently transform physical domain data to frequency domain coefficients. The main focuses of this paper are the effectiveness of different basis selections in detecting data dependency using frequency domain data, e.g., whether varying basis choices significantly influence statistical power loss for small data with large noise; and on the stability of different optimization formulations for finding proper algebraic relations when data are dependent. We present numerical results to demonstrate the effectiveness of frequency domain-based statistical analysis methods and provide guidance for selecting the proper basis and algorithm to detect a particular type of relations.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [27] [The weak maximum principle for solutions of degenerate elliptic equations with lower order terms](https://arxiv.org/abs/2512.00244)
*David Cruz-Uribe,Scott Rodney*

Main category: math.AP

TL;DR: Weak maximum principle for subsolutions of degenerate linear elliptic operators with lower order terms


<details>
  <summary>Details</summary>
Motivation: To establish a weak maximum principle for subsolutions of degenerate elliptic operators, building on recent existence results by the authors and collaborators

Method: Mathematical analysis and proof techniques for degenerate linear second order elliptic operators with lower order terms

Result: Proof of a weak maximum principle for subsolutions of the specified class of degenerate elliptic operators

Conclusion: The paper successfully establishes a weak maximum principle that extends previous existence results to include maximum principle properties for subsolutions

Abstract: We prove a weak maximum principle for subsolutions of a degenerate, linear, second order elliptic operator with lower order terms, building on the existence results recently proved by the authors and Çetin, Dal and Zeren.

</details>


### [28] [Local and Global Results on Three Dimensional Rarefaction Waves in Spherical Symmetry](https://arxiv.org/abs/2512.00353)
*Ruotong Zhang*

Main category: math.AP

TL;DR: Construct centered rarefaction wave solutions for compressible Euler equations in 3D spherical symmetry, proving both local existence for general backgrounds and global existence for near-constant states.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical existence results for centered rarefaction waves in compressible fluid dynamics, particularly for the spherical symmetric case which corresponds to one side of the Riemann problem solution.

Method: Analyze homentropic flow of perfect gas governed by compressible Euler equations with gamma-law equation of state in 3-D spherical symmetry. Use mathematical analysis techniques to prove existence theorems for rarefaction waves.

Result: Proved existence of local-in-time rarefaction waves for general background solutions, and global-in-time rarefaction waves for background solutions close to constant states with reasonable decay properties.

Conclusion: Successfully established rigorous existence results for centered rarefaction waves in spherical symmetry, providing mathematical foundation for this important class of solutions in compressible fluid dynamics.

Abstract: We construct centered rarefaction wave solutions given background solutions to the compressible Euler equations. The flow considered in this article is the homentropic flow of perfect gas governed by compressible Euler equations and the gamma-law equation of state in 3-D spherical symmetry. We prove the existence of local in time rarefactions for general background solutions, which corresponds to one side of the solution to the Riemann problem in spherical symmetry. We also prove the existence of global in time rarefactions for background solutions that are close to constant states with reasonable decay.

</details>


### [29] [Vanishing layer thickness limit of convection in multilayer porous media](https://arxiv.org/abs/2512.00430)
*Kaijian Sha,Xiaoming Wang*

Main category: math.AP

TL;DR: Analysis of convection in multilayered porous media as one layer thickness approaches zero, showing convergence to a model with one fewer layer.


<details>
  <summary>Details</summary>
Motivation: To understand how convection in multilayered porous media behaves when one layer becomes infinitesimally thin, and to establish rigorous mathematical convergence results to simplified models.

Method: Using Darcy-Boussinesq framework, analyzing the singular limit where thickness of one layer tends to zero, establishing convergence in two complementary senses: strong L²-convergence over finite time intervals and upper semi-continuity of global attractors.

Result: Proves that solutions of the full multilayered system converge to those of the limiting model with one fewer layer, both for finite-time dynamics and large-time asymptotic behavior via attractor theory.

Conclusion: The mathematical framework justifies simplifying multilayered porous media models by removing vanishingly thin layers while preserving essential dynamics, providing rigorous foundation for model reduction in porous media convection.

Abstract: Within the Darcy-Boussinesq framework for convection in multilayered porous media, we investigate the singular limit in which the thickness of one layer tends to zero. We establish that the solution of the full system converges to that of the corresponding limiting model with one fewer layer. The convergence is established in two complementary senses: (i) strong $L^{2}$-convergence over arbitrary finite time intervals, and (ii) upper semi-continuity of the global attractors describing the large-time asymptotic behavior.

</details>


### [30] [Controllability of a Semilinear System of Parabolic Equations with Nonlocal Terms](https://arxiv.org/abs/2512.00445)
*Juan Limaco,Rafael Martins Lobosco,Luis P. Yapu*

Main category: math.AP

TL;DR: Extends controllability results for coupled parabolic systems with nonlocal interactions, establishing local null controllability for semilinear systems with single control, using fixed-point theorem and linearized dynamics analysis.


<details>
  <summary>Details</summary>
Motivation: Motivated by applications in finance, particularly generalized Black-Scholes models, the paper aims to extend controllability results for coupled linear parabolic systems with nonlocal interactions to more general settings.

Method: Combines Kakutani's fixed-point theorem with controllability/observability estimates for the associated linearized dynamics to prove local null controllability for semilinear, nonlocally coupled systems driven by a single internal control.

Result: Establishes local null controllability at fixed time T>0 for semilinear nonlocally coupled systems with single control, and obtains controllability for a broader class of linear systems than previously considered.

Conclusion: The paper successfully extends controllability results to more general systems, concludes with remarks on boundary controllability within the same nonlocal framework, and provides perspectives for future research directions.

Abstract: This paper extends our previous controllability results for a class of coupled linear parabolic systems with nonlocal interactions, motivated by applications in finance such as generalized Black--Scholes models. We establish local null controllability at a fixed time T>0 for a class of semilinear, nonlocally coupled systems driven by a single internal control acting on one component. The proof combines Kakutani's fixed-point theorem with a controllability/observability estimate for the associated linearized dynamics. In addition, we obtain controllability for a broader class of linear systems than those considered in the first article. The paper concludes with remarks on boundary controllability within the same nonlocal framework and with perspectives for future research.

</details>


### [31] [Existence and bounds of nonlinear singularity-free cosmological solutions in a string-inspired gravity](https://arxiv.org/abs/2512.00455)
*Chihang He,Chao Liu*

Main category: math.AP

TL;DR: Existence proof for singularity-free cosmological solutions in Einstein-dilaton-Gauss-Bonnet gravity with exponential coupling, using novel power identity method to overcome strong nonlinearities.


<details>
  <summary>Details</summary>
Motivation: Previous numerical studies suggested existence of homogeneous, isotropic, globally singularity-free cosmological solutions in EdGB gravity, but a formal mathematical proof was lacking, especially for the challenging exponential coupling case.

Method: Employed a novel "power identity method" to overcome significant challenges posed by strong nonlinearities of exponential coupling, building on companion paper's analysis of quadratic coupling.

Result: Established existence of FLRW solution valid for all time t∈(-∞,+∞) with positive Hubble parameter vanishing asymptotically and monotonically evolving scalar field, aligning with numerical simulations.

Conclusion: Provides rigorous mathematical foundation for singularity-free cosmology in string-inspired EdGB gravity, confirming numerical predictions and extending beyond simpler quadratic coupling cases.

Abstract: We provide a rigorous proof for the existence of homogeneous, isotropic and globally singularity-free cosmological solutions in Einstein-dilaton-Gauss-Bonnet (EdGB) gravity with exponential coupling. While numerical studies suggested such solutions exist, a formal proof remained elusive. By employing a novel ``power identity method'' and overcoming significant challenges posed by the strong nonlinearities of the exponential coupling, which are not present in the quadratic coupling analyzed in our companion paper \cite{he2025proofssingularityfreesolutionsscalarization}, we establish a FLRW solution valid for all time $t\in(-\infty,+\infty)$, where the Hubble parameter remains positive and vanishes asymptotically, while the scalar field evolves monotonically. This result align with numerical simulations and offer a firm mathematical foundation for singularity-free cosmology in a string-inspired setting.

</details>


### [32] [Existence, Stability and Controllability of the parabolic-parabolic thermistor model](https://arxiv.org/abs/2512.00478)
*Miguel R. Nuñez-Chávez,Luis P. Yapu,Juan Límaco*

Main category: math.AP

TL;DR: The paper establishes well-posedness, energy estimates, stability, and local null controllability for a thermistor system modeled by a parabolic-parabolic system with control acting on only one equation.


<details>
  <summary>Details</summary>
Motivation: To study control properties of thermistor systems, which are coupled parabolic systems with applications in electronics and thermal management, where control is applied to only one component of the system.

Method: Uses Carleman estimates and Liusternik's inverse function theorem to prove local null controllability. Special Carleman estimates are developed to handle the coupling in both zero-order and first-order terms.

Result: Establishes well-posedness, energy estimates, stability, and local null controllability for the thermistor system with control acting on just one equation.

Conclusion: The thermistor system can be locally controlled to null state using control on only one equation, despite the strong coupling in both zero-order and first-order terms, through careful application of Carleman estimates and inverse function theory.

Abstract: In this article we establish the well-posedness, energy estimates, stability, and local null controllability for the thermistor system modeled by a parabolic-parabolic system using a control force acting on just one equation of the system. The proof of the controllability is based on appropriate Carleman estimates and Liusternik's inverse function theorem to obtain the local controllability of the nonlinear system. The coupling of the system happens both in the terms of order zero and one, which requires the use of a special Carleman estimate for the system.

</details>


### [33] [Anisotropic elliptic equations involving unbounded coefficients and singular nonlinearities](https://arxiv.org/abs/2512.00485)
*Fessel achhoud,Hichem Khelifi*

Main category: math.AP

TL;DR: Study of existence and regularity for nonlinear singular elliptic equations with unbounded coefficients and singular right-hand side, focusing on a model equation with anisotropic p-Laplacian type operator and singular source term.


<details>
  <summary>Details</summary>
Motivation: To analyze a class of nonlinear singular elliptic equations that combine several challenging features: unbounded coefficients, singular right-hand side, and anisotropic p-Laplacian type operators, which arise in various physical and mathematical contexts.

Method: Mathematical analysis of a model equation involving anisotropic p-Laplacian type operator with coefficient depending on solution, singular source term f/u^γ, and Dirichlet boundary conditions. The approach likely involves variational methods, approximation techniques, and regularity theory for elliptic PDEs.

Result: The paper establishes existence and regularity results for solutions to this class of equations under specified conditions on parameters (γ≥0, q>0, p_j>2) and data (f∈L¹, f≥0, f≠0).

Conclusion: The study provides rigorous mathematical foundation for understanding solutions to this challenging class of nonlinear singular elliptic equations with combined singularities and anisotropic structure.

Abstract: In this paper, we study the existence and regularity of solutions for a class of nonlinear singular elliptic equations involving unbounded coefficients and a singular right-hand side. Specifically, we are interested to problem whose simplest model is \begin{equation*} -\sum_{j=1}^N\partial_{j}\left([1+u^{q}]\vert \partial_{j} u \vert^{p_{j}-2} \partial_{j} u\right)= \frac{f}{u^γ}\text{ in $\mathcal{D},$}\quad u>0 \text{ in $\mathcal{D},$} \quad u=0 \hbox{ on}\;\; \partial\mathcal{D}, \end{equation*} where $\mathcal{D}$ is a bounded open subset of $\mathbb{R}^{N}$ with $N>2$, $ γ\geq0$, $q >0 $, $p_{j}>2$ for all $j=1,...,N$ and the source term $f$ belongs to $L^1(\mathcal{D})$, with $f \geq 0$ and $f \not\equiv 0$.

</details>


### [34] [Parabolic problems with slightly superlinear convection terms](https://arxiv.org/abs/2512.00495)
*Fessel Achhoud*

Main category: math.AP

TL;DR: Existence and uniqueness of bounded/unbounded weak solutions for a nonlinear parabolic PDE with superlinear convection term involving logarithmic growth.


<details>
  <summary>Details</summary>
Motivation: Study nonlinear parabolic problems with convection terms having superlinear growth, particularly those involving logarithmic nonlinearities, which arise in various physical applications and present mathematical challenges for existence/uniqueness proofs.

Method: Analyze the PDE ∂u/∂t - div(M(x,t)∇u) = -div(u log(e+|u|)E(x,t)) + f(x,t) where M is bounded measurable matrix, E and f belong to suitable Lebesgue spaces. Prove existence of weak solutions using functional analysis and PDE techniques.

Result: Proves existence of both bounded and unbounded weak solutions, and establishes uniqueness of these solutions for the nonlinear parabolic problem with superlinear convection term.

Conclusion: The paper successfully establishes well-posedness results (existence and uniqueness) for a class of nonlinear parabolic equations with logarithmic convection terms, extending the theory for such superlinear growth problems.

Abstract: In this paper we deal with a non-linear parabolic problem which involving a convection term with super--linear growth, whose model is \[ \frac{\partial u}{\partial t}-÷(\mathcal{M}(x,t)\nabla u)= -÷(u\log (e+|u|)E(x,t))+f(x,t), \] where $\mathcal{M}$ is a bounded measurable matrix, the vector field $E$ and the function $f$ belong to suitable Lebesgue spaces. We prove the existence of a unique bounded and unbounded weak solution.

</details>


### [35] [Differential and Variational Approach to First Order Mean Field Games in a Generalized Form](https://arxiv.org/abs/2512.00587)
*Antonio Siconolfi*

Main category: math.AP

TL;DR: The paper bridges differential and variational approaches in Mean Field Games, showing that fixed points of a multivalued map on probability measures correspond to solutions of continuity equations driven by Hamilton-Jacobi value functions, requiring only Hamiltonian differentiability in momentum.


<details>
  <summary>Details</summary>
Motivation: To unify classical differential formulations (Hamilton-Jacobi + continuity equations) with variational approaches in Mean Field Games, providing a framework that connects these different perspectives without requiring excessive regularity assumptions.

Method: The authors use a variational approach based on fixed points of a multivalued map acting on probability measures over trajectories. They prove existence of fixed points for general Hamiltonians, and when the Hamiltonian is differentiable in momentum, they show these fixed points solve continuity equations driven by vector fields from the Hamilton-Jacobi equation.

Result: Existence of fixed points for very general Hamiltonians is proven. When the Hamiltonian is momentum-differentiable, evaluation curves of fixed points solve continuity equations driven by vector fields from the Hamilton-Jacobi final condition, without requiring additional regularity on the value function. The field matches classical Mean Field Systems at differentiability points.

Conclusion: The analysis provides a unified framework bridging differential and variational viewpoints in Mean Field Games, showing how aggregate optimality conditions naturally lead to continuity-equation descriptions under minimal assumptions (only Hamiltonian differentiability in momentum).

Abstract: We investigate time dependent, first order Mean Field Games on the torus comparing, in a broad and general framework, the classical differential formulation , given by a Hamilton Jacobi equation coupled with a continuity equation, with a variational approach based on fixed points of a multivalued map acting on probability measures over trajectories.
  We prove existence of fixed points for very general Hamiltonians. When the Hamiltonian is differentiable with respect to the momentum, we show that the evaluation curve of any such fixed point solves a continuity equation driven by a vector field associated with the final condition in the Hamilton Jacobi equation.
  This field is defined without requiring additional regularity conditions on the value function solving the Hamilton--Jacobi equation. The field coincides with the classical vector field of Mean Field Systems at space--differentiability points of the value function lying in the space--time regions where optimal trajectories concentrate.
  Our analysis therefore provides a unified framework that bridges the differential and variational viewpoints in Mean Field Games, showing how aggregate optimality conditions naturally lead to continuity-equation descriptions under the sole assumption of differentiability of the Hamiltonian in the momentum variable.

</details>


### [36] [Non-Euclidean elasticity for rods and almost isometric embeddings of geodesic tubes](https://arxiv.org/abs/2512.00643)
*Milan Kroemer,Stefan Müller*

Main category: math.AP

TL;DR: The paper studies elastic energy of maps between Riemannian manifolds in thin tubular domains, proving compactness and Γ-convergence results that generalize thin rod theory to curved settings.


<details>
  <summary>Details</summary>
Motivation: To understand the elastic behavior of maps between Riemannian manifolds in thin tubular domains around geodesics, generalizing previous work on thin rods in Euclidean space to curved geometries.

Method: Study an elastic energy per unit volume based on squared distance from orientation-preserving linear maps, analyze sequences with bounded rescaled energy, use blow-up techniques in radial direction, and apply Γ-convergence theory.

Result: Proved compactness for sequences with bounded rescaled energy, established Γ-convergence to a limiting energy, and obtained explicit expression for minimum energy involving curvature tensor differences.

Conclusion: The work successfully generalizes thin rod elasticity theory to Riemannian manifolds, providing a complete Γ-convergence analysis and answering a question about the relationship between limiting energy and curvature differences.

Abstract: We consider a geodesic $γ$ of length $2L$ in an oriented Riemannian manifold $(\mathcal M, g)$ and a thin tube $Ω^*_h$ around $γ$ of radius $h$. We study an 'elastic' energy per unit volume $E_h(u)$ of maps $u$ from $Ω^*_h$ into another oriented Riemannian manifold $(\tilde {\mathcal M},\tilde g)$. The energy $E_h$ is based on the squared distance of the differentials $du$ from the set of orientation preserving linear maps between the corresponding tangent spaces.
  We prove a compactness result for sequences of maps $u_h$ for which $h^{-4} E_h(u_h)$ remains bounded and we study the $Γ$-Limit of $h^{-4} E_h(u_h)$ as $h \to 0$ with respect to a suitable notion of convergence for $u_h$ that involves certain blow-ups in the radial direction. This $Γ$-convergence result ge\-ne\-ra\-lizes work by Mora and Müller on the limiting energy of thin rods in the Euclidean setting.
  We also obtain an expression for the minimum of the limiting energy as a specific quadratic functional in the difference of the pullbacks of the curvature tensors of $\mathcal M$ and $\tilde{\mathcal M}$ along the curves $γ$ and $u \circ γ$, respectively, thus answering a question by Maor and Shachar, J. Elasticity 134 (2019), pp. 149--173.

</details>


### [37] [Nonlinear instability of rolls in the 2-dimensional generalized Swift-Hohenberg equation](https://arxiv.org/abs/2512.00764)
*Myeongju Chae,Soyeun Jung*

Main category: math.AP

TL;DR: The paper proves nonlinear instability of roll solutions in the 2D generalized Swift-Hohenberg equation by constructing small perturbations that grow and cause deviation from rolls in finite time.


<details>
  <summary>Details</summary>
Motivation: To rigorously establish nonlinear instability of roll solutions in the 2D generalized Swift-Hohenberg equation, providing a clear transition from spectral to nonlinear instability in a genuinely two-dimensional setting.

Method: Based on spectral information near the maximally unstable Bloch mode combined with precise semigroup estimates. Constructs a certain class of small initial perturbations that grow in time.

Result: Successfully proves nonlinear instability by showing that small perturbations grow and cause solutions to deviate from underlying roll solutions within finite time.

Conclusion: The analysis provides rigorous nonlinear instability results for roll solutions in 2D gSHE, establishing a clear spectral-to-nonlinear instability transition in an unbounded Bloch parameter domain.

Abstract: Within the framework developed in \cite{Gr, JLL, RT1}, we rigorously establish the nonlinear instability of roll solutions to the two-dimensional generalized Swift-Hohenberg equation (gSHE). Our analysis is based on spectral information near the maximally unstable Bloch mode, combined with precise semigroup estimates. We construct a certain class of small initial perturbations that grow in time and cause the solution to deviate from the underlying roll solution within a finite time. This result provides a clear transition from spectral to nonlinear instability in a genuinely two-dimensional setting, where the Bloch parameter $σ$ ranges over an unbounded domain.

</details>


### [38] [Higher derivative estimates for Stokes equations with closely spaced rigid inclusions in three dimensions](https://arxiv.org/abs/2512.00866)
*Hongjie Dong,Haigang Li,Huaijun Teng,Peihao Zhang*

Main category: math.AP

TL;DR: Higher-order derivative estimates for Stokes equations in 3D domains with closely spaced rigid inclusions, obtaining blow-up rates for stress derivatives in narrow gaps.


<details>
  <summary>Details</summary>
Motivation: To understand the singular behavior of higher-order derivatives in Stokes flow between closely spaced rigid inclusions, which is important for stress analysis in narrow gaps and has applications in fluid-structure interaction problems.

Method: Construct sequence of auxiliary functions via inductive process to isolate leading singular terms; for 3D convex inclusions, use decay properties of solutions to 2D PDEs with singular coefficients; obtain pointwise upper bounds up to seventh order for general inclusions, and optimal estimates for arbitrary order under symmetry conditions.

Result: Obtained pointwise upper bounds for derivatives up to seventh order for general inclusions; derived optimal estimates for derivatives of arbitrary order under symmetry conditions; obtained precise blow-up rates for Cauchy stress and its higher-order derivatives in narrow region between inclusions.

Conclusion: Successfully established higher-order derivative estimates for Stokes equations in 3D domains with closely spaced inclusions, revealing precise blow-up behavior of stress derivatives in narrow gaps, with 3D analysis requiring different techniques than 2D case.

Abstract: In this paper, we establish higher-order derivative estimates for the Stokes equations in a three-dimensional domain containing two closely spaced rigid inclusions. We construct a sequence of auxiliary functions via an inductive process to isolate the leading singular terms of higher-order derivatives within the narrow region between the inclusions. For a class of convex inclusions of general shapes, the construction of three-dimensional auxiliary functions -- unlike the two-dimensional case -- relies on the decay properties of solutions to a class of two-dimensional partial differential equations with singular coefficients. Taking advantage of this, we obtain pointwise upper bounds of derivatives up to the seventh order for general inclusions. Under additional symmetry conditions, we derive optimal estimates for derivatives of arbitrary order. Consequently, we obtain precise blow-up rates for the Cauchy stress and its higher-order derivatives in the narrow region between the inclusions.

</details>


### [39] [Multiplicity of normalized solutions to the upper critical fractional Choquard equation with $L^2$-supercritical perturbation](https://arxiv.org/abs/2512.00922)
*Yergen Aikyn,Yongpeng Chen,Michael Ruzhansky,Zhipeng Yang*

Main category: math.AP

TL;DR: The paper studies normalized solutions with prescribed L²-norm for a fractional Choquard equation with two nonlocal nonlinearities, both L²-supercritical, where the p-term has upper critical Hartree-type growth. Using variational methods with truncation-penalization to handle compactness issues, the authors prove existence of multiple solutions concentrating near minima of a slowly varying potential.


<details>
  <summary>Details</summary>
Motivation: The motivation is to study normalized solutions (with fixed L²-norm) for fractional Choquard equations with critical Hartree-type nonlinearities. Such problems arise in quantum physics and have mathematical interest due to the combination of fractional Laplacian, nonlocal Hartree terms, critical growth, and L²-supercriticality, which presents significant analytical challenges including lack of compactness.

Method: The authors develop a constrained variational approach on the L²-sphere. They use a truncation-penalization technique for the critical term in the energy functional to overcome compactness issues. The method involves studying the problem with a slowly varying potential V(εx) and proving concentration results as ε→0. The approach combines variational methods with topological arguments based on the Lusternik-Schnirelmann category.

Result: For all sufficiently small ε>0, the problem admits at least cat_{M_δ}(M) distinct normalized solutions, where M is the set of global minima of the potential V. These solutions concentrate near M as ε→0. The result provides multiplicity of solutions based on the topological structure of the potential's minima.

Conclusion: The paper successfully establishes existence and multiplicity of normalized solutions for fractional Choquard equations with upper critical Hartree-type nonlinearities. The constrained variational approach with truncation-penalization effectively handles the compactness challenges, and the solutions exhibit concentration phenomena near minima of the slowly varying potential.

Abstract: We investigate normalized solutions with prescribed $L^2$-norm for the upper critical fractional Choquard equation \[(-Δ)^s u+V(\varepsilon x)u=λu+\big(I_α*|u|^{p}\big)|u|^{p-2}u+\big(I_α*|u|^{q}\big)|u|^{q-2}u\quad\text{in }\mathbb{R}^N,\] where $N>2s$, $0<s<1$, $(N-4s)^+<α<N$, and the nonlocal exponents satisfy \[\frac{N+2s+α}{N}< q< p=\frac{N+α}{N-2s},\] so that both nonlinearities are $L^2$-supercritical and the $p$ term has upper critical growth of Hartree type. Under standard assumptions on the slowly varying potential $V$, we develop a constrained variational approach on the $L^2$-sphere, based on a truncation-penalization of the critical term in the energy functional, to overcome the lack of compactness. We prove that, for all sufficiently small $\varepsilon>0$, the problem admits at least $\mathrm{cat}_{M_δ}(M)$ distinct normalized solutions, where $M$ is the set of global minima of $V$ and these solutions concentrate near $M$ as $\varepsilon\to0$.

</details>


### [40] [On the smoothness of solutions of fully nonlinear second order equations in the plane](https://arxiv.org/abs/2512.00951)
*Alessandro Goffi*

Main category: math.AP

TL;DR: The paper establishes interior C^{2,α} regularity for solutions of fully nonlinear uniformly elliptic equations in two variables without geometric conditions on F, providing explicit exponents depending on ellipticity constants.


<details>
  <summary>Details</summary>
Motivation: To prove interior regularity estimates for solutions of fully nonlinear uniformly elliptic equations in two independent variables without imposing geometric conditions on the operator F, which is a fundamental problem in elliptic PDE theory.

Method: The authors use two complementary approaches: 1) Employing divergence form equation theory to prove C^{2,α} regularity with exponent α(λ/Λ), and 2) Exploiting nondivergence equation theory in the plane to obtain C^{2,α} regularity with an explicit exponent α̃(λ/Λ) > λ/Λ.

Result: The paper proves that C^2 solutions of fully nonlinear uniformly elliptic equations F(D^2u)=0 in two variables are C^{2,α} in the interior, with explicit exponents α(λ/Λ) and α̃(λ/Λ) > λ/Λ depending on ellipticity constants λ and Λ.

Conclusion: The work establishes interior C^{2,α} regularity for fully nonlinear elliptic equations in two dimensions without geometric assumptions on F, providing explicit regularity exponents that depend only on the ellipticity constants, advancing the understanding of regularity theory for nonlinear elliptic PDEs.

Abstract: We study interior $C^{2,α}$ regularity estimates for solutions of fully nonlinear uniformly elliptic equations of the general form $F(D^2u)=0$ in two independent variables and without any geometric condition on $F$. By means of the theory of divergence form equations we prove that $C^2$ solutions of the previous equation are $C^{2,\barα(λ/Λ)}$ in the interior of the domain, where $0<λ\leqΛ$ are the ellipticity constants. We finally exploit the theory of nondivergence equations in the plane to obtain $C^{2,\tildeα}$ regularity for an explicit exponent $\tildeα=\tildeα(λ/Λ)>λ/Λ$.

</details>


### [41] [Homogenization of a thin linear elastic plate reinforced with a periodic mosaic of small rigid plates](https://arxiv.org/abs/2512.01042)
*Amartya Chakrabortty,Georges Griso,Julia Orlik*

Main category: math.AP

TL;DR: Thin elastic composite plates with periodic rigid inclusions are studied using homogenization and dimension reduction, yielding a 2D limit model with decoupled plates having three degrees of freedom each.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanical behavior of thin composite plates containing periodically distributed rigid rectangular plates separated by elastic beams, and to derive effective models through simultaneous homogenization and dimension reduction.

Method: Using linearized elasticity framework, the study employs simultaneous homogenization (periodic microstructure) and dimension reduction (thin plate limit) to analyze plates with thickness δ containing rigid plates distributed periodically along ε, with elastic beams of thickness δ < ε/3 < 1 between them.

Result: The analysis yields Korn-type inequalities adapted to the rigid-elastic geometry, characterizes limit deformation and displacement fields, and shows that in the 2D limit problem, bending is the sum of two functions each depending on only one variable (mixed derivatives vanish). The limiting 2D problem consists of two decoupled plates/strips with three degrees of freedom each: shear along strip axis, cross-contraction/extension, and cross-bending.

Conclusion: The homogenization and dimension reduction process successfully derives an effective 2D model for thin composite plates with periodic rigid inclusions, revealing simplified mechanical behavior with decoupled components and specific degrees of freedom, with all correctors decomposable in the linearized setting.

Abstract: In the framework of linearized elasticity, we study thin elastic composite plates with thickness $δ$. The plates contain small, rigid rectangular plates distributed periodically along $\varepsilon$. Between two neighboring rigid plates is an elastic beam with thickness $δ< \varepsilon/3 < 1$. Through a simultaneous process of homogenization and dimension reduction, we obtain the limit model. Our analysis yields Korn-type inequalities adapted to the rigid-elastic geometry of the structure and provides a precise characterization of the limit deformation and displacement fields. In the $2$D limit problem, the bending is the sum of two functions, each depending on only one variable. This is due to the fact that the mixed derivatives of the outer-plane displacement vanish. Finally, the limiting 2D problem is two decoupled plates or strips, each one with just three degrees of freedom: shear along the strip axis, the cross-contraction (-extension), and the cross-bending. The corresponding correctors are defined in the same way in the periodicity cell. In the linearized setting, all the correctors are decomposed.

</details>


### [42] [$\mathcal{C}^α$-regularity for nonlinear non-diagonal parabolic systems](https://arxiv.org/abs/2512.01122)
*Miroslav Bulíček,Jens Frehse*

Main category: math.AP

TL;DR: Extends Hölder continuity results from elliptic p-Laplacian problems to parabolic systems, achieving C^α regularity for p > d/2, improving the classical p > d-2 threshold.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend elliptic regularity theory for p-Laplacian-like problems to the parabolic setting, particularly for systems that don't satisfy the radial (Uhlenbeck) structure condition. Classical parabolic regularity results only work for p > d-2, leaving a gap for many applications.

Method: The authors study nonlinear parabolic systems with structure parallel to elliptic p-Laplacian problems but with time dependence. They assume space-time regularity of F and structural conditions analogous to stationary theory, then use analytical techniques to establish regularity.

Result: The main result establishes C^α-regularity (Hölder continuity) of weak solutions in both space and time for growth parameter p > d/2. This improves upon the classical requirement of p > d-2 and applies to systems without radial structure.

Conclusion: This work successfully extends elliptic regularity theory to parabolic systems, providing the only known regularity result for systems far from Uhlenbeck structure with improved threshold p > d/2.

Abstract: In the elliptic theory for $p$-Laplacian-like problems, the Hölder continuity of solutions has been proven for problems arising as Euler--Lagrange equations of a convex potential with $p$-growth that additionally satisfies the splitting condition.
  In this article, we extend these results to the parabolic setting. We investigate nonlinear parabolic systems whose structure parallels the elliptic case but incorporates time dependence. Assuming suitable space-time regularity of $F$ and natural structural conditions analogous to the stationary theory, we establish $\mathcal{C}^α$-regularity of weak solutions in space and time whenever the growth parameter $p>d/2$. This extends the classical result for parabolic systems, which is valid only for $p>d-2$. This is the only regularity result for systems that are far from the radial (Uhlenbeck) structure.

</details>


### [43] [Stability threshold of the 2D Boussinesq system near Couette flow in an infinite channel](https://arxiv.org/abs/2512.01159)
*Tao Liang,Jiahong Wu,Xiaoping Zhai*

Main category: math.AP

TL;DR: The paper studies stability of 2D Boussinesq equations around Couette flow in an infinite channel, improving temperature stability threshold from ν^{11/12} to ν^{5/6} compared to previous finite channel results.


<details>
  <summary>Details</summary>
Motivation: To establish asymptotic stability of Couette flow in 2D Boussinesq equations under no-slip boundary conditions in an infinite channel, improving upon previous stability thresholds for temperature perturbations in finite channel settings.

Method: Analyze the two-dimensional Boussinesq equations around Couette flow in an infinite channel ℝ × [-1,1] with no-slip boundary conditions, using perturbation analysis and establishing stability thresholds for velocity and temperature perturbations in Sobolev spaces.

Result: Prove asymptotic stability of Couette flow under initial perturbations: velocity perturbations ≤ ε₀ν^{1/2} in H² norm, and temperature perturbations ≤ ε₁ν^{5/6} in H¹ norm plus fractional derivative condition. This improves temperature stability threshold from ν^{11/12} to ν^{5/6} compared to previous finite channel results.

Conclusion: The Couette flow is asymptotically stable in infinite channel under improved perturbation thresholds, with temperature stability threshold enhanced from ν^{11/12} to ν^{5/6}, demonstrating better stability properties in infinite channel compared to finite channel settings.

Abstract: In this paper, we study the stability threshold of the two-dimensional Boussinesq equations around the Couette flow in an infinite channel $\mathbb{R} \times [-1, 1]$ under no-slip boundary conditions. We prove that the Couette flow is asymptotically stable under initial perturbations satisfying $\| \mathbf{v}^{\mathrm{in}} -(y,0)\|_{H^2} \le \varepsilon_0 ν^{\frac12}$, and $\| ρ^{\mathrm{in}}-1 \|_{H^1} + \big\| |\partial_x|^{\frac13} ρ^{\mathrm{in}} \big\|_{H^1} \le \varepsilon_1 ν^{\frac56}$. Compared with the work of Masmoudi, Zhai, and Zhao [J. Funct. Anal., 284 (2023), 109736], where the asymptotic stability of the 2D Navier-Stokes-Boussinesq system around Couette flow in a finite channel $\mathbb{T} \times [-1, 1]$ was established, our result improves the stability threshold for the temperature from $ν^{\frac{11}{12}}$ to $ν^{\frac56}$.

</details>


### [44] [On solvability of parabolic equations with singular coefficients in odd mixed-norm Morrey-Sobolev spaces](https://arxiv.org/abs/2512.01168)
*N. V. Krylov*

Main category: math.AP

TL;DR: Existence and uniqueness theorem for second-order parabolic equations in mixed-norm Morrey-Sobolev spaces with measurable coefficients and rough singularities.


<details>
  <summary>Details</summary>
Motivation: To establish well-posedness for parabolic equations with coefficients that have minimal regularity assumptions, allowing for rough singularities in the first-order coefficients while working in non-standard function spaces.

Method: Uses mixed-norm Morrey-Sobolev spaces with "odd" structure (interior integration in time rather than space), assumes main coefficient a is measurable in t and BMO in x, and first-order coefficients b are in appropriate mixed-norm Morrey classes.

Result: Proves existence and uniqueness theorem for second-order parabolic equations in the whole space with constant zeroth-order coefficient under these minimal regularity conditions.

Conclusion: Establishes well-posedness in non-standard function spaces for parabolic equations with coefficients having rough singularities, extending classical results to more general coefficient classes.

Abstract: We prove an existence and uniqueness theorem for second-order parabolic equations in the whole space with constant zeroth-order coefficient in mixed-norm Morrey-Sobolev spaces. The main coefficient $a$ is assumed to be measurable in $t$ and BMO in $x$ and the first-order coefficients $b$ are in an appropriate mixed-norm Morrey classes (thus admitting rather rough singularities). The mixed-norm Morrey-Sobolev spaces are ``odd'' in the sense that the interior integration in the formula defining the norm is performed with respect to $t$ and not to $x$ as is customary.

</details>


### [45] [Asymptotic stability of solitary waves for the b-family of equations](https://arxiv.org/abs/2512.01225)
*Jun Wu,Yue Liu,Zhong Wang*

Main category: math.AP

TL;DR: Asymptotic stability of lefton solutions in non-integrable b-family equations for b < -1, adapting Martel-Merle framework to nonlocal structure.


<details>
  <summary>Details</summary>
Motivation: Previous studies focused on integrable cases (Camassa-Holm b=2, Degasperis-Procesi b=3), but non-integrable regime b < -1 lacks rigorous stability theory for lefton solutions despite their existence.

Method: Adapts Martel-Merle framework for generalized KdV equations to nonlocal, non-integrable b-family structure. Combines nonlinear Liouville property for near-lefton solutions with refined spectral analysis of linearized operator.

Result: First rigorous proof of asymptotic stability for lefton solutions in non-integrable b-family equations with positive momentum density for b < -1.

Conclusion: Establishes asymptotic stability theory for leftons in non-integrable parameter regime, extending understanding beyond integrable cases and demonstrating applicability of Martel-Merle framework to nonlocal equations.

Abstract: We establish the asymptotic stability of lefton solutions-exponentially localized stationary solitary waves-for the $b$-family of equations with positive momentum density in the regime $b < -1$. Unlike the completely integrable Camassa-Holm $(b=2)$ and Degasperis-Procesi $(b=3)$ cases, this parameter range lies outside integrability and exhibits distinct nonlinear dynamics. Our analysis adapts the Martel-Merle framework for generalized KdV equations to the nonlocal, non-integrable structure of the $b$-family of equations. The proof combines a nonlinear Liouville property for solutions localized near leftons with a refined spectral analysis of the associated linearized operator. These results provide the first rigorous asymptotic stability theory for leftons in the non-integrable $b$-family of equations.

</details>


### [46] [Existence of two thresholds in a bistable equation with nonlocal competition](https://arxiv.org/abs/2512.01435)
*Matthieu Alfaro,Cédric Chane Ki Chune,Lionel Roques*

Main category: math.AP

TL;DR: Nonlocal bistable reaction-diffusion model with phenotypic traits shows extinction for both small AND large initial data, with persistence only for intermediate sizes - revealing two thresholds, unlike local models.


<details>
  <summary>Details</summary>
Motivation: To understand population dynamics in replicator-mutator framework with phenotypic traits, mutation, fitness-dependent selection, and nonlocal competition, incorporating a "pseudo-Allee effect" where initial population size affects long-term survival vs extinction.

Method: Analyze nonlocal bistable reaction-diffusion equation modeling population structured by phenotypic trait. Prove well-posedness of Cauchy problem, then investigate long-time behavior through mathematical analysis showing different extinction/persistence regimes based on initial data size.

Result: 1) Small initial data lead to extinction. 2) Surprisingly, too large initial data can also lead to extinction, especially when selection is weak. 3) Intermediate initial data can lead to persistence, revealing existence of at least two thresholds for survival.

Conclusion: The nonlocal bistable model exhibits fundamentally different behavior from local models, with extinction possible for both small AND large initial populations, creating a "sweet spot" for persistence at intermediate sizes - a counterintuitive result with implications for evolutionary dynamics.

Abstract: We consider a nonlocal bistable reaction-diffusion equation, which serves as a model for a population structured by a phenotypic trait, subject to mutation, trait-dependent fitness, and nonlocal competition. Within this replicator-mutator framework, we further incorporate a ''pseudo-Allee effect'' so that the long time behavior (extinction vs. survival) depends on the size of the initial data. After proving the well-posedness of the associated Cauchy problem, we investigate its long-time behavior. We first show that small initial data lead to extinction. More surprisingly, we then prove that that extinction may also occur for too large initial data, in particular when selection is not strong enough. Finally, we exhibit situations where intermediate initial data lead to persistence, thereby revealing the existence of (at least) two thresholds. These results stand in sharp contrast with the behavior observed in local bistable equations.

</details>


### [47] [Mathematical and numerical study of a model for navigation in stratified waters](https://arxiv.org/abs/2512.01445)
*Zeina Rammal,Matthieu Brachet,Germain Rousseaux,Morgan Pierre*

Main category: math.AP

TL;DR: Linear model of ship navigation in two-layer fluid with variable velocity, analyzed spectrally with Rayleigh damping. Unique solution proven for regular data, constant speed case studied with critical speed regimes. Numerical scheme using discrete Fourier transform and exponential integrator with error estimate.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze a mathematical model for ship navigation in stratified fluids (two-layer fluid) with variable ship velocity, which is important for understanding ship-wave interactions in oceanographic contexts where density stratification occurs.

Method: Derived linear model of navigation in two-layer fluid with variable ship velocity. Analyzed spectral version with Rayleigh damping term. Proved existence/uniqueness for Cauchy problem with regular data. Investigated constant speed case identifying critical speed regimes. Developed numerical scheme using discrete Fourier transform for space discretization and exponential integrator for time discretization with error estimate.

Result: Proved unique solution exists for Cauchy problem with sufficiently regular velocity and initial data. Identified critical speed separating two types of regimes in constant speed case. Developed and analyzed numerical scheme with error estimate for exponential integrator. Validated with 1D and 2D numerical experiments.

Conclusion: The paper provides rigorous mathematical analysis of ship navigation in stratified fluids, establishing well-posedness conditions, identifying critical speed regimes, and developing validated numerical methods for simulation.

Abstract: We derive a linear model of navigation in a two-layer fluid with a variable velocity of the ship. A spectral version of the model including a Rayleigh damping term is analyzed. We prove that the Cauchy problem has a unique solution if the velocity and if the initial data are sufficiently regular. The case of a constant speed is thoroughly investigated and the importance of a critical speed which separates two types of regimes is pointed out. We propose a numerical scheme based on the discrete Fourier transform for the space discretization and on an exponential integrator for the time discretization. We prove an error estimate for the exponential integrator. Numerical experiments in one and two space dimensions complete the theoretical results.

</details>


### [48] [A Generalization of Caffarelli's Contraction Theorem to Nearly Spherical Manifolds](https://arxiv.org/abs/2512.01496)
*Yuxin Ge,Jordan Serres*

Main category: math.AP

TL;DR: Every nearly spherical manifold can be realized as a volume-preserving image of a round sphere via optimal transport, extending Caffarelli's theorem and proving a perturbative form of Milman's conjecture.


<details>
  <summary>Details</summary>
Motivation: To extend Caffarelli's contraction theorem to nearly spherical manifolds and prove a perturbative form of Milman's conjecture about isoperimetric properties of nearly spherical manifolds.

Method: Using Brenier-McCann optimal transport maps and proving a novel stability result for optimal transport maps on the sphere to show that nearly spherical manifolds can be obtained as volume-preserving images of round spheres.

Result: Established that every nearly spherical manifold can be realized as a volume-preserving image of a round sphere via optimal transport, extending Caffarelli's contraction theorem and proving Milman's conjecture in perturbative form.

Conclusion: The paper provides a fundamental connection between nearly spherical manifolds and optimal transport theory, with important implications for geometric analysis and isoperimetric inequalities.

Abstract: We show that every nearly spherical manifold can be realized as the volume-preserving image of a round sphere, via the Brenier-McCann optimal transport map. This theorem extends Caffarelli's contraction theorem to nearly spherical manifolds and yields, as a corollary, a proof of a perturbative form of Milman's conjecture. The proof is based on a novel stability result for optimal transport maps on the sphere.

</details>


### [49] [Mountain pass for the Ginzburg-Landau energy in a strip: solitons and solitonic vortices](https://arxiv.org/abs/2512.01506)
*Amandine Aftalion,Luc Nguyen*

Main category: math.AP

TL;DR: Study of Ginzburg-Landau energy critical points in infinite strip with phase imprinting on half domain, analyzing soliton vs vortex solutions based on cross-section width.


<details>
  <summary>Details</summary>
Motivation: Motivated by recent experiments, the paper investigates critical points of Ginzburg-Landau energy in an infinite strip geometry where phase imprinting is applied to half of the domain, exploring how geometry affects solution types.

Method: Mathematical analysis of Ginzburg-Landau energy critical points using variational methods, proving existence of critical width threshold, and studying mountain pass solutions and minimizers within subspaces with specific symmetries (odd functions).

Result: Proves existence of critical width: below it, soliton solution is mountain pass solution and minimizer within odd functions subspace; above it, mountain pass solution becomes solitonic vortex. Also shows minimizer can display one or multiple solitonic vortices depending on width.

Conclusion: Geometry significantly affects solution types in Ginzburg-Landau systems, with strip geometry introducing analytical differences compared to disk or plane cases. Results extend to 3D infinite cylinder geometry.

Abstract: Motivated by recent experiments, we study critical points of the Ginzburg-Landau energy in an infinite strip where phase imprinting is applied to half of the domain. We prove that there is a critical width of the cross section below which the soliton solution is a mountain pass solution and the minimizer within the subspace of odd functions. Above the critical width, we find that the mountain pass solution is a vortex with a solitonic behaviour in the infinite direction, called a solitonic vortex. Moreover, depending on the width, we prove that the minimizer in a space with some symmetries can display one or several solitonic vortices. While the problem shares some similarities with the analysis of stability and minimality of the Ginzburg-Landau vortex of degree one in a disk or the whole plane, the change in geometry introduces subtle analytical differences. Extensions to the case of an infinite cylinder in 3D are also given.

</details>


### [50] [Velocity Averaging Lemmas: Classical, Quantum and Semi-Classical](https://arxiv.org/abs/2512.01529)
*François Golse,Norbert J. Mauser,Jakob Möller*

Main category: math.AP

TL;DR: Averaging lemmas, originally developed for classical kinetic equations, are extended to quantum kinetic theory via the Wigner equation, with particular focus on semi-classical limits and connections to quantum hydrodynamics.


<details>
  <summary>Details</summary>
Motivation: To address the long-standing question of whether averaging lemmas (which provide regularity gains in classical kinetic theory) can be extended to quantum kinetic theory, specifically for the Wigner equation that describes quantum systems in phase space.

Method: Analyzes the application of averaging lemmas to the Wigner equation, comparing classical and quantum cases, with particular attention to semi-classical asymptotics (vanishing Planck constant) and the distinction between pure vs. mixed quantum states.

Result: Provides answers about the extent to which averaging lemmas apply to quantum kinetic theory, with detailed results for classical and quantum cases, and outlines the semi-classical case which is fully developed in a follow-up article.

Conclusion: Averaging lemmas can be extended to quantum kinetic theory through the Wigner equation, with semi-classical limits requiring careful treatment of pure vs. mixed states and connections to quantum hydrodynamics, bridging classical and quantum kinetic analysis.

Abstract: Averaging lemmas were introduced as a tool of the mathematical analysis of kinetic equations, i.e. PDEs for functions in phase space $(x,v)$ containing a transport ("advection") term. By integrating over $v$ in velocity space $\mathbb{R}_v^d$ (velocity averaging), one gains regularity for the density in position space $\mathbb{R}_x^d$. The concept was invented independently by V.I. Agoshkov and by F. Golse, B. Perthame, R. Sentis and P.-L. Lions, and successfully applied to the analysis of Vlasov or Boltzmann equations in "classical kinetic theory". In "quantum kinetic theory", the Schrödinger equation for the complex-valued "wave function" in the physical space is converted into the Wigner equation for the real-valued Wigner function in phase space (which can take negative values). The Wigner ("Quantum Vlasov") equation contains the transport term of classical kinetic equations plus a pseudo-differential operator containing the potential. We give answers to the long standing question of whether and to which extent averaging lemmas apply to the "quantum" case of the Wigner equation. The hard part are the "semi-classical" averaging lemmas, where one considers the asymptotics of vanishing Planck constant towards the non-negative Wigner measure. In that context "pure vs. mixed states" play a crucial role, as well as the connection between the Schrödinger equation and Quantum Hydrodynamics (QHD). We present the results for the classical and quantum cases and sketch the "semi-classical" case which is worked out in full detail in a follow-up article.

</details>


### [51] [Functional-Analytic Justification of the Time-Domain Foldy-Lax Approximation for Dispersive Acoustic Media: A Feynman-Diagram Viewpoint](https://arxiv.org/abs/2512.01532)
*Arpan Mukherjee,Mourad Sini*

Main category: math.AP

TL;DR: Rigorous functional-analytic justification of time-domain Foldy-Lax framework for multiple acoustic scattering by dispersive resonators (gas-filled bubbles), incorporating dispersion via Minnaert resonance.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous mathematical foundation for modeling multiple acoustic scattering by clusters of dispersive resonators (bubbles), explicitly incorporating dispersion effects through Minnaert resonance, which is crucial for accurate transient wave prediction in applications like cavitation therapy, seismic imaging, and metamaterial engineering.

Method: Formulates the model as a delayed-coupled hyperbolic system for bubble amplitude interactions. Combines time-domain integral equations, Laplace transforms, and Hardy-Sobolev space techniques to analyze the system. Establishes unique solvability in anisotropic Hilbert spaces with solutions expressed as convergent Neumann series of convolution operators.

Result: Establishes unique solvability of the system, derives geometric decay of truncation errors for resonant incident waves, quantifies N-th order multi-scattering contribution scaling as ε^{N(1-p)+1} (relating bubble radius ε and inter-bubble distance scaling as ε^p, p<1). Shows this dominates measurement errors of order ε^2, allowing capture of fields from inter-bubble interactions of order N < 1/(1-p). Provides novel connection to Feynman diagrams mapping multi-scattering paths to diagrammatic vertices and propagators.

Conclusion: The framework provides rigorous functional-analytic justification for time-domain Foldy-Lax modeling of multiple acoustic scattering by dispersive resonators, establishing quantitative relations between source field spectra bandwidth, bubble proximity, and relevant interaction orders. This advances accurate transient wave prediction in dispersive media with applications in medical therapy, geophysics, and metamaterials.

Abstract: This work provides a rigorous functional-analytic justification for a time-domain Foldy-Lax framework that describes multiple acoustic scattering by a cluster of dispersive resonators (modeling gas-filled bubbles), explicitly incorporating dispersion via the Minnaert resonance. The model is formulated as a delayed-coupled hyperbolic system for bubble amplitude interactions. We combine time-domain integral equations, Laplace transforms, and Hardy-Sobolev space techniques to analyze this system, establishing its unique solvability in anisotropic Hilbert spaces, with solutions expressed as convergent Neumann series of convolution operators. We derive geometric decay of truncation errors for resonant incident waves and quantify the contribution of $N$-th order multi-scattering, showing it scales with \(\varepsilon^{N(1-p)+1}\) (relating bubble radius \(\varepsilon\) and inter-bubble distance scaling as $\varepsilon^p$, $p<1$). This dominates the measurement errors, which are of order $\varepsilon^2$, thereby allowing us to capture fields generated by inter-bubble interactions of order $N<\frac{1}{1-p}$. This provides a quantitative relation between the spectra band width of the source field, the closeness distance between the bubbles and the order $N$ of the relevant interactions between the bubbles. Furthermore, a novel connection to Feynman diagrams maps multi-scattering paths to diagrammatic vertices and propagators, simplifying the interpretation of higher-order interactions and kinematic constraints. This framework advances accurate transient wave prediction in dispersive media, with implications for cavitation therapy, seismic imaging, and metamaterial engineering.

</details>


### [52] [Existence and Nonlocal-to-Local Convergence for Singular, Anisotropic Nonlocal Cahn-Hilliard Equations](https://arxiv.org/abs/2512.01545)
*Helmut Abels,Yutaka Terasawa*

Main category: math.AP

TL;DR: The paper studies convergence of weak solutions from a nonlocal Cahn-Hilliard equation to an anisotropic Cahn-Hilliard equation, and proves existence of weak solutions for the nonlocal equation under certain kernel conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between nonlocal and local models in phase field equations, specifically how nonlocal Cahn-Hilliard equations with anisotropic and singular kernels converge to their local anisotropic counterparts.

Method: Analysis of weak solutions for the nonlocal Cahn-Hilliard equation with anisotropic and singular kernels, studying convergence properties as the nonlocal effects become localized or kernels become singular.

Result: Proves convergence of weak solutions from the nonlocal equation to the anisotropic Cahn-Hilliard equation for suitable subsequences, and establishes existence of weak solutions for the nonlocal equation under conditions that guarantee existence for localized or singular kernels.

Conclusion: The nonlocal-to-local convergence holds for anisotropic Cahn-Hilliard equations with singular kernels, providing mathematical justification for using nonlocal models as approximations of local anisotropic phase field models.

Abstract: We study the nonlocal-to-local convergence for a nonlocal Cahn-Hilliard equation with anisotropic and singular kernels. In particular, we show convergence of weak solutions of the nonlocal Cahn-Hilliard equation to weak solutions of a corresponding anisotropic Cahn-Hilliard equation for suitable subsequences. Moreover, we show existence of weak solutions for the nonlocal equation under a condition, which guarantees existence of weak solutions for suitably localized or singular kernels.

</details>


### [53] [Normalized solutions for the planar Schrödinger-Poisson system with two electrons interaction](https://arxiv.org/abs/2512.01655)
*Baihong Li,Yuanhong Wei,Xiangjian Zeng*

Main category: math.AP

TL;DR: Existence of normalized solutions for planar Schrödinger-Poisson system with two-electron interaction and logarithmic convolution terms, establishing ground states and excited states using compactness methods.


<details>
  <summary>Details</summary>
Motivation: Study normalized solutions for planar Schrödinger-Poisson system modeling electron-electron interactions and electrostatic potential effects, addressing challenges with logarithmic convolution terms.

Method: Develop compactness method for functionals with logarithmic convolution terms, establish Pohožaev identity for coupled system, use variational techniques to find solutions.

Result: Existence results established as parameters vary: ground state solutions for general cases, two solutions for mass-supercritical case (ground state and excited state).

Conclusion: Successfully established existence of normalized solutions including ground and excited states for planar Schrödinger-Poisson system with logarithmic convolution terms using compactness methods and Pohožaev identity.

Abstract: This paper focuses on the normalized solutions for the planar Schrödinger-Poisson system with a two-electron interaction, which models the effect between electrons and the electrostatic potential they generate. As the parameters vary, some existence results are established. Specifically, a ground state solution is obtained for some general cases. The existence of two solutions is established for the mass-supercritical case, one of which is a ground state solution and the other one is an excited state solution. We develop a compactness method to deal with the functionals involving logarithmic convolution terms. The Pohožaev identity for the coupled Schrödinger-Poisson system with a logarithmic convolution term is also shown, which is crucial for addressing the mass-supercritical problem.

</details>


### [54] [Theory And Applications Of One-Sided Coupled Operator Matrices](https://arxiv.org/abs/2512.01719)
*Marjeta Kramar,Delio Mugnolo,Rainer Nagel*

Main category: math.AP

TL;DR: Survey paper on one-sided coupled operator matrices theory for analyzing well-posedness and stability of initial-boundary value problems with unbounded boundary feedbacks.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of the theory of one-sided coupled operator matrices developed by K.-J. Engel, which offers an abstract framework for analyzing concrete initial value problems, particularly focusing on applications to problems with unbounded boundary feedbacks.

Method: Uses the abstract framework of one-sided coupled operator matrices to analyze well-posedness and stability properties. Applies this theory to specific initial-boundary value problems, including a diffusion-transport system with dynamical boundary conditions and a wave equation with dynamical boundary conditions.

Result: Demonstrates the applicability of the theory to concrete problems, showing well-posedness of both the diffusion-transport system with dynamical boundary conditions and the wave equation with dynamical boundary conditions as a by-product.

Conclusion: The theory of one-sided coupled operator matrices provides a powerful abstract framework for analyzing well-posedness and stability of various initial-boundary value problems, particularly those with unbounded boundary feedbacks, with successful applications to diffusion-transport systems and wave equations with dynamical boundary conditions.

Abstract: The theory of one-sided coupled operator matrices, recently introduced by K.-J. Engel, is an abstract framework for concrete initial value problems and allows complete information on well-posedness, and stability of solutions. These notes are meant as a survey on this rich theory, with a particular stress on applications to initial-boundary value problems with unbounded boundary feedbacks. A diffusion-transport system with dynamical boundary conditions is discussed, and its well-posedness and various other properties are investigated. As a by-product, the well-posedness of a wave equation with dynamical boundary condition is also obtained.

</details>


### [55] [Time-periodic non-radial solutions near monotone vortices in linearized 2D Euler](https://arxiv.org/abs/2512.01730)
*Ángel Castro,Daniel Lear*

Main category: math.AP

TL;DR: Linearized 2D Euler equations around radial vortices: strict monotonicity leads to axisymmetrization and inviscid damping, but these properties are not robust under small low-regularity perturbations that violate strict monotonicity.


<details>
  <summary>Details</summary>
Motivation: To investigate the robustness of axisymmetrization and inviscid damping phenomena in linearized 2D Euler equations around radial vortex profiles when strict monotonicity conditions are slightly violated.

Method: Construct arbitrarily close radial profiles (in low Hölder norms C^α, 0<α<1) that are merely non-increasing (not strictly decreasing) from any strictly decreasing radial vortex, and show existence of non-radial, time-periodic solutions to the linearized equation.

Result: Both axisymmetrization and inviscid damping are not robust under small, low-regularity perturbations of the background profile that violate strict monotonicity. Non-radial, time-periodic solutions exist for these perturbed profiles.

Conclusion: The stability properties (axisymmetrization and inviscid damping) of linearized 2D Euler equations around radial vortices are highly sensitive to strict monotonicity conditions and can be broken by arbitrarily small perturbations in low-regularity norms.

Abstract: We study the linearized 2D Euler equations around radial vortex profiles. Previous works have shown that the strict monotonicity of the vorticity profile leads to axisymmetrization and inviscid damping of non-radial perturbations.
  Given any strictly decreasing radial vortex, we construct arbitrarily close (in low Hölder norms $C^α$, with $0<α< 1$) radial profiles that are merely non-increasing, for which non-radial, time-periodic solutions to the linearized equation exist. This shows that both axisymmetrization and inviscid damping are not robust under small, low-regularity perturbations of the background profile that violate strict monotonicity.

</details>


### [56] [Resonance analysis of one-dimensional acoustic media: a propagation matrix approach](https://arxiv.org/abs/2512.01734)
*Yi Huang,Bowen Li,Ping Liu,Yingjie Shao*

Main category: math.AP

TL;DR: Analysis of acoustic scattering resonances in 1D using propagation matrix approach, characterizing resonant frequencies as zeros of trigonometric polynomial, establishing distribution properties, and deriving asymptotics in high-contrast regimes.


<details>
  <summary>Details</summary>
Motivation: To understand scattering resonances in general acoustic media in one-dimensional settings, particularly contrasting with three-dimensional cases where imaginary parts of resonances are not uniformly bounded. The work aims to establish connections between propagation matrix framework and discrete capacitance matrix approximations.

Method: Uses propagation matrix approach to characterize resonant frequencies as zeros of explicit trigonometric polynomial. Applies Nevanlinna's value distribution theory to establish resonance distribution properties. Derives asymptotics in high-contrast regimes using Newton polygon method to recover discrete capacitance matrix approximations.

Result: Shows that imaginary parts of resonances are uniformly bounded in 1D (unlike 3D). Derives asymptotics for both subwavelength and non-subwavelength resonances in high-contrast regimes. Recovers discrete capacitance matrix approximation for Minnaert resonances in Hermitian and non-Hermitian cases, establishing connection to propagation matrix framework.

Conclusion: The propagation matrix approach provides a powerful framework for analyzing acoustic scattering resonances in 1D, revealing fundamental differences from 3D cases and enabling systematic derivation of asymptotics and connections to discrete capacitance matrix approximations in various physical regimes.

Abstract: This work analyzes the scattering resonances of general acoustic media in a one-dimensional setting using the propagation matrix approach. Specifically, we characterize the resonant frequencies as the zeros of an explicit trigonometric polynomial. Leveraging Nevanlinna's value distribution theory, we establish the distribution properties of the resonances and demonstrate that their imaginary parts are uniformly bounded, which contrasts with the three-dimensional case. In two classes of high-contrast regimes, we derive the asymptotics of both subwavelength and non-subwavelength resonances with respect to the contrast parameter. Furthermore, by applying the Newton polygon method, we recover the discrete capacitance matrix approximation for subwavelength Minnaert resonances in both Hermitian and non-Hermitian cases, thereby establishing its connection to the propagation matrix framework.

</details>


### [57] [Global and local existence of solutions for a novel type of parabolic Kirchhoff system with singular term](https://arxiv.org/abs/2512.01792)
*Aberqi Ahmed,Abdesslam Ouaziz,Maria Alessandra Ragusa*

Main category: math.AP

TL;DR: The paper studies a fractional Kirchhoff-type system with logarithmic nonlinearity, proving existence of weak solutions via Faedo-Galerkin method, analyzing blow-up/global existence based on initial energy levels, and establishing stabilization for positive initial energy.


<details>
  <summary>Details</summary>
Motivation: To investigate the mathematical properties of a fractional system combining Kirchhoff functions and logarithmic nonlinearity, which appears in various physical applications but presents analytical challenges due to the complex interaction between fractional operators and nonlinear terms.

Method: Uses Faedo-Galerkin method to prove existence of weak solutions under suitable assumptions on Kirchhoff function. Analyzes finite-time blow-up and global existence based on critical, subcritical, and supercritical initial energy levels. Applies Komornik's integral inequality for stabilization analysis.

Result: Proves existence of weak solutions, characterizes conditions for finite-time blow-up versus global existence depending on initial energy levels, and establishes stabilization of solutions with positive initial energy.

Conclusion: The fractional Kirchhoff system with logarithmic nonlinearity exhibits rich dynamical behavior including blow-up phenomena and stabilization, with initial energy playing a crucial role in determining solution behavior over time.

Abstract: In this paper, we investigate solutions for a fractional system involving a novel class of Kirchhoff functions and logarithmic nonlinearity:
  \begin{equation*}
  \left\{\begin{array}{lll} \displaystyle \mathfrak{u}_{t}+\mathcal{K}\left([\mathfrak{u}]_p^s\right) \mathscr{L}_p^s u=\vert \mathfrak{v} \vert^{σ}\vert \mathfrak{u} \vert^{σ-2} u \log | \mathfrak{u} \mathfrak{v}|, \, \, & \mbox{in}\quad &\mathcal{U} \times[0, T),\\ \mathfrak{v}_t+\mathcal{K}\left([\mathfrak{v}]_q^s\right) \mathscr{L}_q^s \mathfrak{v}=\vert \mathfrak{u} \vert^{σ}|\mathfrak{v}|^{σ-2} \mathfrak{v} \log | \mathfrak{u} \mathfrak{v}|, & \text { in } & \mathcal{U} \times[0, T), \\ \mathfrak{u}(\mathrm{x}, t)=\mathfrak{v}(\mathrm{x}, t)=0, & \text { in } & \partial \mathcal{U} \times[0, T), \\ \mathfrak{u}(\mathrm{x}, 0)=\mathfrak{u}_0(\mathrm{x}), \mathfrak{v}(\mathrm{x}, 0)=\mathfrak{v}_0(\mathrm{x}), & \text { in } & \mathcal{U}, \end{array}% \right. \end{equation*}
  where $\mathcal{K}$ is Kirchhoff function, and $\mathscr{L}_{p}^{s}$ is the fractional $p-$ Laplacian operator. We prove the existence of a weak solution using the Faedo-Galerkin method under suitable assumptions on the Kirchhoff function. We investigate the finite-time blow-up and global existence of solutions based on critical, subcritical, and supercritical initial energy levels. Subsequently, we establish the stabilization of the solution with positive initial energy by applying Komornik's integral inequality.

</details>


### [58] [Solutions to Sobolev Supercritical Nonlinear Schrodinger Equations on an Annulus via a Hopf Reduction Method](https://arxiv.org/abs/2512.01794)
*Jian Liang,Hua-Yang Wang*

Main category: math.AP

TL;DR: Existence of positive mass-prescribed solutions for nonlinear Schrödinger equations on annuli, including Sobolev supercritical regime, using Hopf fibration reduction to lower dimensions.


<details>
  <summary>Details</summary>
Motivation: Study existence of positive solutions with prescribed mass for nonlinear Schrödinger equations on annulus domains, particularly in challenging Sobolev supercritical regimes where standard methods fail.

Method: Reduction method based on Hopf fibration to transform the problem into lower-dimensional one, enabling analysis of mass critical thresholds and solution existence.

Result: New mass critical threshold identified; in mass subcritical/critical regimes: positive solution exists as global minimizer; in supercritical regime: two positive solutions exist (local minimizer and mountain pass solution).

Conclusion: Hopf fibration reduction successfully enables analysis of mass-prescribed solutions on annuli across different mass regimes, including supercritical cases where multiple solutions emerge.

Abstract: This paper investigates the existence of positive solutions with a prescribed mass for nonlinear Schrodinger equations on an annulus, possibly in the Sobolev supercritical regime. A reduction method based on the Hopf fibration is used to transform the problem into a lower-dimensional one. We obtain a new mass critical threshold and we show that in the new mass subcritical or critical regimes there exists a positive solution which corresponds to a global minimizer, while in the mass supercritical regime, there exists two positive solutions which correspond to a local minimizer and a mountain pass solution respectively. Some other problems are also discussed in this paper.

</details>


### [59] [Conservative Formulations of the Standard Enskog and Povzner Equations](https://arxiv.org/abs/2512.01800)
*Zhe Chen*

Main category: math.AP

TL;DR: Extends Villani's divergence formulation of Boltzmann collision operator to dense gases via Standard Enskog and Povzner equations, representing collision integrals as velocity divergences of mass currents and phase-space divergences of momentum/energy currents.


<details>
  <summary>Details</summary>
Motivation: To generalize Villani's divergence formulation from classical Boltzmann equation to dense gases where particle volume effects matter, specifically for Standard Enskog and Povzner equations that incorporate finite particle size in collisions.

Method: Develops conservative formulation of Standard Enskog and Povzner equations, expressing collision integrals C[f,f] as divergence of mass current with respect to velocity variable v, and terms vC[f,f] and |v|^2C[f,f] as phase-space divergences of momentum and energy currents.

Result: Successfully extends Villani's result to dense gases, showing that Standard Enskog and Povzner collision operators can be written in divergence form, enabling representation of mass, momentum, and energy conservation laws through divergence expressions.

Conclusion: The divergence formulation previously established for Boltzmann equation can be generalized to dense gas models (Standard Enskog and Povzner equations), providing mathematical framework for conservation laws in systems with finite particle volume effects.

Abstract: This article introduces a conservative formulation of the Standard Enskog equation and the Povzner equation, both of which generalize the Boltzmann equation by incorporating the contribution of particle volume in collisions. The primary result expresses these collision integrals as the divergence with respect to the velocity variable v of a mass current. Moreover, the terms v C[f,f] and |v|^2 C[f,f], where C[f,f] denotes the Standard Enskog or Povzner collision integral, are represented as phase-space divergences (that is, divergences in both position and velocity) of corresponding momentum and energy currents. This work extends Villani's earlier result (Math. Modelling Numer. Anal. M2AN 33 (1999), 209--227) for the classical Boltzmann equation to the case of dense gases.

</details>


### [60] [Biharmonic Equations](https://arxiv.org/abs/2512.01860)
*Dirk Pauly,Alberto Valli*

Main category: math.AP

TL;DR: Analysis of well-posed variational formulations and operator methods for biharmonic boundary value problems, focusing on Neumann type and over-/underdetermined problems.


<details>
  <summary>Details</summary>
Motivation: To develop rigorous mathematical frameworks for solving boundary value problems associated with the biharmonic operator, particularly addressing challenging cases like Neumann type and over-/underdetermined boundary conditions that lack comprehensive theoretical treatment.

Method: Devises and analyzes well-posed variational formulations and employs operator theoretical methods to study boundary value problems for the biharmonic operator.

Result: Establishes well-posed variational formulations and operator theoretical frameworks for Neumann type and over-/underdetermined boundary value problems associated with the biharmonic operator.

Conclusion: The paper provides rigorous mathematical foundations for solving various boundary value problems involving the biharmonic operator, particularly addressing previously challenging cases through variational formulations and operator theory.

Abstract: In this note we devise and analyse well-posed variational formulations and operator theoretical methods for boundary value problems associated to the biharmonic operator. Of particular interest are Neumann type and over- and underdetermined (maximal and minimal) boundary value problems.

</details>


### [61] [Dirichlet heat kernel estimates for parabolic nonlocal equations](https://arxiv.org/abs/2512.01919)
*Philipp Svinger,Marvin Weidner*

Main category: math.AP

TL;DR: Optimal C^s boundary regularity and higher order boundary Harnack principle for nonlocal parabolic equations in divergence form in C^{1,α} domains, with applications to sharp Dirichlet heat kernel estimates.


<details>
  <summary>Details</summary>
Motivation: To establish optimal boundary regularity and higher order boundary Harnack principles for nonlocal parabolic equations, particularly for operators with merely Hölder continuous coefficients, and to obtain sharp heat kernel estimates including time-dependent coefficients.

Method: Develops an approach for nonlocal parabolic equations in divergence form in C^{1,α} domains, applicable to broad classes of nonlocal operators with Hölder continuous coefficients, including translation invariant cases.

Result: Establishes optimal C^s boundary regularity, proves higher order boundary Harnack principle, and obtains sharp two-sided estimates for Dirichlet heat kernel, including cases with time-dependent coefficients.

Conclusion: The paper provides comprehensive boundary regularity results for nonlocal parabolic equations, filling gaps in the literature particularly for time-dependent coefficients, with applications to heat kernel analysis.

Abstract: In this article we establish the optimal $C^s$ boundary regularity for solutions to nonlocal parabolic equations in divergence form in $C^{1,α}$ domains and prove a higher order boundary Harnack principle in this setting. Our approach applies to a broad class of nonlocal operators with merely Hölder continuous coefficients, but our results are new even in the translation invariant case. As an application, we obtain sharp two-sided estimates for the associated Dirichlet heat kernel. Notably, our estimates cover nonlocal operators with time-dependent coefficients, which had remained open in the literature.

</details>


### [62] [Prescribed energy solutions of concave-convex type problems involving sign-changing or vanishing weights](https://arxiv.org/abs/2512.01931)
*Kanishka Perera,Humberto Ramos Quoirin,Kaye Silva*

Main category: math.AP

TL;DR: Abstract approach for finding solutions to nonlinear eigenvalue problems with concave-convex structure, yielding existence, multiplicity, and bifurcation results.


<details>
  <summary>Details</summary>
Motivation: To develop a general framework for solving nonlinear eigenvalue problems of the form Φ'_λ(u)=0 with Φ_λ(u)=c, particularly for concave-convex problems with sign-changing or vanishing weights, which appear in various applications.

Method: Abstract functional analytic approach on Banach spaces, working with C^1 functionals Φ_λ that satisfy certain structural conditions. The method finds couples (λ,u) satisfying both Φ_λ(u)=c and Φ'_λ(u)=0 for suitable c values.

Result: Derives several existence, multiplicity, and bifurcation type results for the equation Φ'_λ(u)=0 with fixed λ. The approach is applicable to energy functionals associated with concave-convex problems.

Conclusion: Provides a general abstract framework that unifies and extends previous results for concave-convex problems, offering a systematic way to obtain existence, multiplicity, and bifurcation results for nonlinear eigenvalue problems with sign-changing weights.

Abstract: We provide an abstract approach to find couples $(λ,u) \in \mathbb{R} \times X$ satisfying $$Φ_λ(u)=c \quad \mbox{and} \quad Φ'_λ(u)=0,$$ for some suitable values of $c \in \mathbb{R}$. Here $Φ_λ$ is a $C^1$ functional (set on a Banach space $X$) whose main prototype is the energy functional associated to a concave-convex problem with sign-changing or vanishing weights. This approach allows us to derive several existence, multiplicity and bifurcation type results for the equation $Φ'_λ(u)=0$ with $λ$ fixed.

</details>


### [63] [Absence of Exponential Stability and Polynomial Stabilization in a Class of Beam Models with Tip Rotary Inertia](https://arxiv.org/abs/2512.01964)
*Gerardo Gómez Ávalos,Jaime Muñoz Rivera,Elena Ochoa Ochoa*

Main category: math.AP

TL;DR: Hybrid dissipative boundary conditions in Euler-Bernoulli beams don't change decay characteristics - they either produce slow polynomial decay when used alone, or don't affect existing decay when combined with other mechanisms.


<details>
  <summary>Details</summary>
Motivation: To investigate how dissipative dynamic boundary conditions affect the stability and decay behavior of Euler-Bernoulli beam models, particularly understanding whether hybrid dissipation mechanisms can enhance or alter decay characteristics.

Method: Analyzed Euler-Bernoulli beam framework with dissipative dynamic boundary conditions applied at one end. Examined two scenarios: 1) hybrid dissipation as the sole dissipative mechanism, and 2) hybrid dissipation combined with other dissipative mechanisms.

Result: Hybrid dissipation doesn't alter original model's decay characteristics. When used alone, it fails to induce exponential decay and instead produces slow polynomial decay of t^{-1/2} for large t. When combined with other mechanisms, it neither enhances nor diminishes the original model's decay behavior.

Conclusion: Dissipative dynamic boundary conditions in Euler-Bernoulli beams are ineffective for improving decay characteristics - they either provide only slow polynomial decay when acting alone, or have no effect when combined with other dissipation mechanisms.

Abstract: We investigate the impact of dissipative dynamic boundary conditions applied at one end of a beam, analyzing their influence on model stability within the Euler-Bernoulli framework. Our primary finding is that hybrid dissipation does not alter the decay characteristics of the original model. We examine two scenarios: first, when hybrid dissipation is the sole dissipative mechanism, and second, when it complements other dissipative mechanisms. In the first case, we demonstrate that hybrid dissipation fails to induce exponential decay, instead producing a slow decay rate of $t^{-1/2}$ for large $t$. In the second case, when acting as a complementary mechanism, hybrid dissipation neither enhances nor diminishes the decay behavior of the original model.

</details>


### [64] [Semigroups For Initial-Boundary Value Problems](https://arxiv.org/abs/2512.01966)
*Marjeta Kramar,Delio Mugnolo,Rainer Nagel*

Main category: math.AP

TL;DR: Review of theory for one-sided coupled operator matrices, focusing on evolution equations with inhomogeneous boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive review of the mathematical theory for one-sided coupled operator matrices, particularly in the context of evolution equations with inhomogeneous boundary conditions, which are important in various applied mathematics and physics problems.

Method: Theoretical review and analysis of existing mathematical framework for one-sided coupled operator matrices, examining their properties and applications to evolution equations with boundary conditions.

Result: A systematic review of the theory, organizing and clarifying existing mathematical results on operator matrices and their application to boundary value problems in evolution equations.

Conclusion: The review provides a consolidated theoretical foundation for working with one-sided coupled operator matrices in the context of evolution equations with inhomogeneous boundary conditions, serving as a reference for researchers in this area.

Abstract: We review the theory of one-sided coupled operator matrices with a focus on evolution equations with inhomogeneous boundary conditions. (The original article had no abstract.)

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [65] [Control of localized states of itinerant electrons and their magnetic interactions](https://arxiv.org/abs/2512.00776)
*Yaxin Sun,I. S. Lobanov,Jiahao Su,Ho-Kin Tang,V. M. Uzdin*

Main category: physics.comp-ph

TL;DR: Electric field control of magnetic properties in nanosystems using noncollinear Alexander-Anderson model shows magnetic dimer states (FM, AFM, noncollinear) depend on d-level position relative to Fermi level, enabling current-free magnetic manipulation.


<details>
  <summary>Details</summary>
Motivation: To enable electric field control of magnetic properties for spintronics applications without requiring current flow, offering advantages for magnetic manipulation in nanosystems.

Method: Using the noncollinear Alexander-Anderson model to study magnetic dimers formed by itinerant electrons, analyzing how d-level position relative to Fermi level affects magnetic interactions and ground states.

Result: Magnetic dimer ground states can be ferromagnetic, antiferromagnetic, or noncollinear depending on d-level position; electric field can shift d-level to control magnetic state; multiple self-consistent solutions exist for large hopping parameters.

Conclusion: Electric field control of magnetic properties enables new manipulation possibilities for nanosystem magnetic structures and provides new interpretation of magnetization reversal mechanisms in tunneling spectroscopy experiments.

Abstract: Controlling the magnetic properties of nanosystems by an electric field offers a number of advantages for spintronics applications. Using the noncollinear Alexander-Anderson model, we have shown that the interaction of localized magnetic moments formed by itinerant electrons strongly depends on the position of the d-level relative to the Fermi level, which determines the number of localized electrons. Depending on this parameter, the ground state of the magnetic dimer can be ferromagnetic, antiferromagnetic, or noncollinear without the effects of spin-orbit interaction. The magnetic state can be controlled by shifting the d-level with an electric field, even without current flow. For a sufficiently large value of the hopping parameter between localized states there can be several self-consistent solutions with different values of magnetic moments. This opens new possibilities for manipulation of the magnetic structure of nanosystems. The results obtained lead to a new interpretation of the mechanisms of magnetization reversal, recording, and deleting of magnetic structures in tunneling spectroscopy experiments.

</details>


### [66] [Neural Network Perturbation Theory (NNPT): Learning Residual Corrections from Exact Solutions](https://arxiv.org/abs/2512.01558)
*Zhenhao Chen,Mutian Shen,Boris Fain,Zohar Nussinov*

Main category: physics.comp-ph

TL;DR: Neural Network Perturbation Theory (NNPT) trains networks to predict only residual perturbations after subtracting known exact solutions, achieving 28-54x lower error than learning complete trajectories from scratch. Using the three-body problem, they find sharp capacity transitions in networks at chaos onset.


<details>
  <summary>Details</summary>
Motivation: Many physical systems can be decomposed into exactly solvable components plus perturbations. Rather than training neural networks to learn complete trajectories from scratch, the authors aim to develop a more parameter-efficient approach by having networks focus only on the residual perturbations after analytically subtracting known exact solutions.

Method: Introduce Neural Network Perturbation Theory (NNPT) where networks predict only residual perturbations. Validate using the gravitational three-body problem as a test bed, varying Jovian mass from 0.05 to 30 times physical value. Use fixed-architecture multilayer perceptrons and an equalized-accuracy protocol to measure minimal network capacity and training time requirements. Employ symplectic integrator to maintain energy conservation.

Result: Correction learning achieves 28-54x lower validation error compared to networks trained on complete trajectories. Sharp transitions occur at f_c = 15.6±1.0 where the system enters strongly chaotic regime. At this transition, minimal network capacity jumps ~7x from ~1,200 to ~8,600 parameters. Symplectic integrator maintains relative energy conservation below 2×10^-7, confirming transitions reflect physical complexity rather than numerical error.

Conclusion: Correction learning is an effective strategy for parameter-efficient surrogate models. Physical complexity imposes fundamental capacity barriers on fixed-architecture networks at chaos onset, with sharp transitions in required network capacity when systems become strongly chaotic.

Abstract: Many complex physical systems admit natural decomposition into an exactly solvable component and a perturbative correction. Rather than training neural networks to learn complete trajectories from scratch, we introduce Neural Network Perturbation Theory (NNPT), where networks predict only residual perturbations after analytically subtracting known exact solutions.
  We validate this framework through systematic comparison: using identical 2x32 architectures, correction learning achieves 28-54x lower validation error compared to networks trained on complete trajectories. Using the gravitational three-body problem as a test bed, we investigate capacity transitions in fixed-architecture multilayer perceptrons as Jovian mass varies from 0.05 to 30 times its physical value. An equalized-accuracy protocol reveals that both minimal network capacity and training time exhibit sharp transitions at f_c = 15.6+-1.0, where the system enters a strongly chaotic regime. At this transition, minimal capacity jumps approximately sevenfold from ~1,200 to ~8,600 parameters (architectures 2x32 and 3x64).
  Preliminary exploration of sequential two-stage corrections suggests that first-stage networks already capture dominant perturbative features. Our symplectic integrator maintains relative energy conservation below 2x10^-7 throughout, confirming that transitions reflect physical complexity rather than numerical error. Our results establish correction learning as a general strategy for parameter-efficient surrogates and demonstrate that physical complexity imposes fundamental capacity barriers on fixed-architecture networks at chaos onset.

</details>


### [67] [Comparison of shock compaction models for granular materials: P -α model and mesoscale simulation](https://arxiv.org/abs/2512.01940)
*Dawa Seo,Darby J. Luscher,Nitin Daphalapurkar*

Main category: physics.comp-ph

TL;DR: Comparison of continuum P-alpha model vs mesoscale simulations for predicting particle velocity in granular sugar under weak shock, showing both can match experimental data but through different physical mechanisms.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the predictive accuracy of two computational approaches (continuum P-alpha model and mesoscale simulations) for modeling weak shock propagation in granular materials under flyer-plate impact conditions.

Method: Used flyer-plate impact experiments on granular sugar as benchmark. Compared two 2D models: (1) continuum-based P-alpha Menko model requiring pressure-dependent yield strength calibration, and (2) mesoscale simulations with explicit particle and pore representations using porosity as key physical state variable.

Result: Both models can reproduce measured particle velocity histories but through fundamentally different mechanisms. P-alpha model requires careful calibration of constitutive parameters (pressure-dependent yield strength, crush-out pressure). Mesoscale simulations are less parameter-sensitive and rely on porosity as dominant control variable.

Conclusion: Continuum parameters act as effective surrogates for grain-scale physics, while mesoscale modeling reveals porosity as the dominant control of macroscopic wave onset. The two approaches provide different mechanical interpretations of the same macroscopic behavior.

Abstract: This study examines particle velocity measurements in granular sugar to evaluate the predictive accuracy of two computational models for weak shock under flyer-plate impact: the continuum-based P-alpha Menko model and mesoscale simulations with explicit particle and pore representations. Using flyer-plate impact experiments as a benchmark, we show that both two-dimensional (2D) models can reproduce the measured particle velocity histories, but through fundamentally different mechanisms. In the P-alpha framework, applying a pressure-dependent yield strength is essential to capture the particle velocity evolution, though calibration of other constitutive parameters, such as crush-out pressure, still strongly influences the response. In contrast, mesoscale simulations are less sensitive to parameter tuning and rely critically on the physical state variable of porosity, represented in 2D as an equivalent measure of the 3D specimen. Together, these results establish that their mechanical interpretations differ: continuum parameters act as effective surrogates for grain-scale physics, whereas mesoscale modeling reveals porosity as the dominant control of macroscopic wave onset.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [68] [An Interpretable Operator-Learning Model for Electric Field Profile Reconstruction in Discharges Based on the EFISH Method](https://arxiv.org/abs/2512.00359)
*Zhijian Yang,Edwin Setiadi Sugeng,Mhedine Alicherif,Tat Loon Chng*

Main category: physics.plasm-ph

TL;DR: A novel ML model called Decoder-DeepONet (DDON) outperforms previous methods for reconstructing electric field distributions from EFISH signal profiles, offering better accuracy, generalizability, and ability to handle incomplete data.


<details>
  <summary>Details</summary>
Motivation: To improve upon previous ML approaches (ANNs and CNNs) for solving the inverse EFISH problem, addressing inaccuracies caused by Gouy phase shift in focused beams, and to create a more robust model for electric field reconstruction.

Method: Developed Decoder-DeepONet (DDON), an operator-learning architecture specialized for function-to-function mappings, enabling recovery of electric field profiles of unknown shape. Used Integrated Gradients to identify critical signal regions for reconstruction accuracy.

Result: DDON demonstrated superior performance compared to published CNN models and classical mathematical methods, showing better generalizability, higher prediction accuracy, wider applicability, and ability to work with incomplete input profiles.

Conclusion: DDON is a robust and comprehensive model suitable for reconstructing bell-shaped electric field profiles with symmetry axes, particularly useful in non-equilibrium plasmas, with practical guidance for optimal EFISH signal sampling.

Abstract: Machine learning (ML) models have recently been used to reconstruct electric field distributions from EFISH signal profiles-the 'inverse EFISH problem'. This addresses the line-of-sight EFISH inaccuracy caused by the Gouy phase shift in focused beams. A key benefit of this approach is that the accuracy of the reconstructed profile can be directly checked via a 'forward transform' of the EFISH equation. Motivated by this latest success, the present study introduces a novel ML model with markedly improved performance. Based on a more powerful operator-learning architecture, it goes beyond the ANNs and CNNs employed previously. Termed Decoder-DeepONet (DDON), its main strength is learning function-to-function mappings, essential for recovering electric field profiles of unknown shape. The superior performance of DDON is exemplified via a comparison with our published CNN model and the feasibility of a classical mathematical method, as well as its application to both discharge simulations and experimental EFISH data from a nanosecond pulsed discharge. In almost all cases, the DDON model exhibits better generalizability, higher prediction accuracy, and wider applicability. Furthermore, the intrinsic nature of this operator-learning architecture renders it less sensitive to the exact location(s) of the acquired data, enabling electric field reconstruction even with seemingly 'incomplete' input profiles--an issue often accompanying poor signal sensitivity. We also employ Integrated Gradients (IG) to identify the signal regions most critical to reconstruction accuracy, providing guidance on the optimal sampling window for EFISH acquisition. Overall, we believe that the DDON model is a robust and comprehensive model which can be readily applied to reconstruct 'bell-shaped' electric field profiles with an existing axis of symmetry, especially in non-equilibrium plasmas.

</details>


### [69] [Investigation of the effects of transient heat loads on plasma-facing materials in Tokamaks](https://arxiv.org/abs/2512.00529)
*Ali Masoudi,Davoud Iraji,Chapar Rasouli*

Main category: physics.plasm-ph

TL;DR: Simulation of tungsten fracture under extreme heat loads in nuclear fusion devices like ITER, addressing thermal damage threats to plasma-facing materials.


<details>
  <summary>Details</summary>
Motivation: Plasma-facing materials in nuclear fusion devices face severe damage from extreme thermal heat loads during transient events like edge localized modes, disruptions, and vertical displacement events, which can reach GW/m² levels and threaten device integrity.

Method: Simulation study of tungsten material fracture under Tokamak heat loads, analyzing material response to extreme thermal conditions.

Result: Results of tungsten fracture simulation under fusion device heat loads are presented, showing material behavior under extreme thermal stress conditions.

Conclusion: Understanding tungsten fracture under extreme heat loads is crucial for developing durable plasma-facing components in nuclear fusion devices like ITER to withstand transient thermal events.

Abstract: Nuclear fusion devices are constantly under the threat of malfunctions coming from the damages of plasma-facing materials due to being affected by thermal heat loads. The frequent heat loads during some transient events in large-scale Tokamaks have always been a great concern for researchers. In ITER, the heat load of GW/m2 is estimated to impose plasma-facing components during edge localized modes, beside the Tokamak steady state load which is about 20 MW/m2. Moreover, there are also other transient thermal loads occurring due to off-normal operation of ITER such as vertical displacement events or disruptions, at the orders of hundreds of MW/m2 and tens of GW/m2, respectively. These loads are great enough to result in severe damages of plasma-facing materials. In this study, the facture of tungsten material under the heat loads of Tokamaks is simulated and the results are presented.

</details>


### [70] [Edge-localized-mode heat load effects on plasma-facing materials studied using runaway electrons in the Damavand tokamak](https://arxiv.org/abs/2512.00530)
*Ali Masoudi,Davoud Iraji,Chapar Rasouli*

Main category: physics.plasm-ph

TL;DR: Small-scale Tokamaks like Damavand can simulate ELM heat loads on plasma-facing materials using runaway electrons, achieving comparable MJ/m² heat densities to large-scale devices.


<details>
  <summary>Details</summary>
Motivation: Edge localized modes (ELMs) and runaway electrons (REs) create damaging heat loads on plasma-facing materials in Tokamaks, potentially causing melting, sputtering, and degradation. While ELMs are inherent to H-mode operation, REs threaten all Tokamak scales. Even with mitigation, ELMs in devices like ITER can impose ~1 MJ/m² heat loads on PFMs.

Method: Using small-scale Tokamaks (Damavand experiment) to investigate RE energy deposition as a proxy for ELM heat loads. Analyzing experimental data from RE populations generated by plasma instabilities, calculating average RE energy (1 MeV), discharge current, and total energy deposition (~1 kJ per discharge) on the limiter. Assuming 40% energy transfer to the limiter surface.

Result: RE populations in Damavand Tokamak discharges produce heat densities on the order of MJ/m² on 1 cm² PFM areas. Calculations show average total RE energy per discharge is ~1 kJ, and with 40% energy transfer to the limiter, heat load density reaches MJ/m² levels comparable to ELM events in large-scale fusion devices.

Conclusion: Small-scale Tokamaks like Damavand can effectively simulate ELM thermal load effects on plasma-facing materials, providing a valuable experimental platform for studying PFM degradation under conditions similar to those in large-scale fusion devices.

Abstract: Edge localized modes (ELMs) and runaway electrons (REs) pose significant challenges for all Tokamak devices and act as potent heat sources, potentially shortening the lifespan of plasma-facing materials (PFMs). These thermal loads can manifest in various detrimental effects, including melting, sputtering, cracking, blistering, and other forms of material degradation. While the ELMs are an intrinsic feature of H-mode operation in Tokamaks, runaway electrons pose a potential threat across all Tokamak device scales.In devices such as ITER, even with mitigation strategies, the ELMs can still impose considerable heat loads on the PFMs, reaching levels of approximately 1 MJ/m2. Various methods exist to experimentally simulate the heat load effects of ELMs on PFMs. In this study, the thermal loads from REs in small-scale Tokamaks are considered for this purpose.The presence of small-scale Tokamaks, exemplified by the Damavand experiment, facilitates the investigation of RE energy deposition. Analysis of the experimental data indicates that the REs populations generated by plasma instabilities within the Damavand Tokamak discharges exhibit heat densities on the order of MJ/m2 to 1 cm2 area PFMs. Furthermore, considering an average REs energy of 1 MeV and the prevailing discharge current, calculations indicate that the average total energy of the REs population per discharge is approximately 1 kJ, which is subsequently deposited on the Tokamak limiter. Considering the limiter's surface area and assuming that only 40% of the REs energy is transferred to it, the calculated heat load density reaches the order of MJ/m2, comparable to that observed during the ELMs events in large-scale fusion devices. This correspondence enables the Damavand Tokamak to investigate thermal loas effects of ELMs on PFMs in large-scale Tokamaks.

</details>


### [71] [Investigation of glow discharge plasma energy distribution using a gridded energy analyzer considering plasma-facing materials related processes](https://arxiv.org/abs/2512.00533)
*Ali Masoudi,Davoud Iraji*

Main category: physics.plasm-ph

TL;DR: Simulation, design, and experimental testing of a gridded energy analyzer to measure ion energy distributions in DC glow discharge plasmas.


<details>
  <summary>Details</summary>
Motivation: To investigate ion energies in DC glow discharge plasmas for understanding plasma-material interactions and applications like coating, cleaning, and surface treatment.

Method: Simulated plasma chamber using COMSOL Multiphysics, designed and constructed a gridded energy analyzer with negatively biased grid (same potential as cathode), measured ion energy distributions experimentally.

Result: Experimental results reveal multiple ion groups in local thermal equilibrium within DC glow discharge plasmas; measured energy distribution functions compared with simulations.

Conclusion: Successfully developed and validated an energy analyzer system showing DC glow discharges contain distinct ion groups in thermal equilibrium, enabling better understanding of plasma-surface interactions.

Abstract: Considering the effects of glow discharge plasmas on plasma-facing materials and the applications such as coating, cleaning and surface treating, this work has been done to investigate the energy of ions of dc glow discharge plasmas. On the way towards this goal, a plasma chamber has been simulated via COMSOL Multiphysics software. Then, a gridded energy analyzer has been simulated and designed. In the next step, the analyzer has been constructed and tested to measure the energy of plasma ions. The devise contains a grid which is negatively biased to the same potential as the glow discharge cathode electrode. It discriminates plasma ions based on their energies, which are accelerated due to sheath potential drop before colliding with the cathode. The obtained energy distribution function from experiments has been compared to that of simulated plasma. The experimental results show that there are different groups of ions each in local thermal equilibrium in dc glow discharge plasmas.

</details>


### [72] [Gigagauss magnetic field generation by bladed microtube implosion](https://arxiv.org/abs/2512.00795)
*D. Pan,M. Murakami*

Main category: physics.plasm-ph

TL;DR: Ultrahigh gigagauss magnetic fields generated using bladed microtube targets irradiated by intense laser pulses, creating vortex currents through asymmetric plasma implosion.


<details>
  <summary>Details</summary>
Motivation: To generate ultrahigh magnetic fields (gigagauss scale) for applications in high-energy-density physics, laboratory astrophysics, and particle acceleration using laser-driven plasma dynamics.

Method: Use bladed microtube targets with periodically slanted inner surfaces (sawtooth pattern). Irradiate with ultra-intense, ultrashort laser pulses to produce MeV hot electrons at outer surface. These electrons transport to inner surface, initiating rapid plasma implosion toward central axis. Blade-induced asymmetry creates vortex-shaped ion/electron flows forming azimuthal loop currents.

Result: Generation of ultrahigh magnetic fields in the gigagauss order. Two-dimensional particle-in-cell simulations and analytical model reveal underlying physics and scaling laws for field strength and spatial confinement.

Conclusion: Bladed microtube targets enable efficient generation of gigagauss magnetic fields through laser-driven asymmetric plasma implosion, with predictable scaling laws for field strength and confinement.

Abstract: We demonstrate the generation of ultrahigh magnetic fields in the order of gigagauss using a bladed microtube target whose inner surface is periodically slanted in a sawtooth-like pattern. When irradiated by ultra-intense, ultrashort laser pulses, hot electrons with MeV energies are produced at the outer surface and swiftly transported to the inner surface, initiating a rapid implosion of plasma toward the central axis. The unique blade-induced asymmetry gives rise to vortex-shaped flows of ions and electrons near the center, forming strong azimuthal loop currents that generate ultrahigh magnetic fields at the center. Two-dimensional particle-in-cell simulations, supported by a simple analytical model, elucidate the underlying physics and reveal key scaling laws governing the field strength and spatial confinement.

</details>


### [73] [Generation of giga-electron-volt proton beams by micronozzle acceleration](https://arxiv.org/abs/2512.00803)
*M. Murakami,D. Balusu,S. Maruyama,Y. Murakami,B. Ramakrishna*

Main category: physics.plasm-ph

TL;DR: Micronozzle acceleration (MNA) scheme produces GeV-order proton beams using laser-driven electrostatic fields in micron-scale nozzle targets.


<details>
  <summary>Details</summary>
Motivation: To develop a novel ion acceleration scheme that can generate proton beams with extremely high kinetic energies (GeV order) for applications requiring high-energy particle beams.

Method: Uses micron-sized hydrogen rod embedded in hollow micronozzle target, illuminated by ultraintense ultrashort laser pulse along symmetric axis. Studied with 2D particle-in-cell simulations.

Result: Generates > GeV protons at laser intensity of 10^22 W/cm². Laser creates strong electrostatic field with long lifetime and extensive space around nozzle downstream tail, significantly amplifying proton kinetic energies.

Conclusion: MNA is an effective scheme for generating GeV-order proton beams using laser-driven micronozzle targets, demonstrating potential for high-energy ion acceleration applications.

Abstract: Our proposed ion acceleration scheme, micronozzle acceleration (MNA), generates proton beams with extremely high kinetic energies on the giga-electron-volt (GeV) order. The underlying physics and performance of MNA are studied with two-dimensional particle-in-cell simulations. In MNA targets, a micron-sized hydrogen rod is embedded inside a hollow micronozzle. Subsequent illumination of the target along the symmetric axis by an ultraintense ultrashort laser pulse forms a strong electrostatic field with a long lifetime and an extensive space around the downstream tail of the nozzle. The electric field significantly amplifies the kinetic energies of the accelerated protons, and > GeV protons are generated at an applied laser intensity of 10^22 W/cm^2 .

</details>


### [74] [Time Evolution of the Pinch Region of a Deflagration Plasma Accelerator](https://arxiv.org/abs/2512.01364)
*A. A. T. Jibodu,J. D. Strickland,M. A. Cappelli*

Main category: physics.plasm-ph

TL;DR: Spectroscopic study of plasma deflagration accelerator using Hβ and Hα lines to measure temporal evolution of plasma density in pinch region, revealing density differences between deflagration and detonation modes.


<details>
  <summary>Details</summary>
Motivation: To investigate the temporal evolution of plasma density within the pinch region of a plasma deflagration accelerator and understand the formation, growth, and decay of the pinch in both deflagration and detonation modes.

Method: Used half-meter imaging monochromator with fast camera (1 MHz) to collect chord-integrated spectral lines (Hβ and Hα) from pinch region. Applied Voigt fits to Abel inverted Hβ emission lines to determine radial density profiles at 1 μs intervals. Used Hα for background continuum subtraction.

Result: Deflagration mode showed density increasing from ~10²⁰ m⁻³ at 1.4 cm from pinch core to ~10²³ m⁻³ at core. Detonation mode had broader zone of relatively constant density ~10²⁰ m⁻³. Energy balance analysis suggested potential temperatures around 550 eV in pinch core.

Conclusion: The study successfully measured plasma density evolution in pinch region, revealing significant differences between deflagration and detonation modes. The high core density in deflagration and estimated 550 eV temperatures suggest departures from previous analytical assumptions.

Abstract: A spectroscopic study of a plasma deflagration accelerator was carried out to investigate the temporal evolution of plasma density within the pinch region. A half-meter imaging monochromator, paired with a fast (10 MHz) camera operating at 1 MHz, was used to collect broadened chord-integrated spectral lines from the pinch region of a plasma deflagration device. Specifically, images of the $H_β$ and the $H_α$ lines were taken - with the $H_α$ used to find the background continuum. Voigt fits of the Abel inverted H$_β$ emission lines allowed for determination of the radial profile of the number density in the pinch at intervals of 1 $μ$s. This provided insight into the formation, growth, and decay of the pinch in both the deflagration and detonation modes of the accelerator. It was found that the maximum density for the deflagration increased from $\sim 10^{20}$ m$^{-3}$ 1.4 cm away from the core of the pinch to $\sim 10^{23}$ m$^{-3}$ at the core of the deflagration pinch. In contrast, the detonation pinch featured a broader zone of relatively constant density on the order of $\sim 10^{20}$ m$^{-3}$. Furthermore, an energy balance of the plasma in the deflagration pinch and downstream, as informed by prior work, was done to get an estimate of temperatures in the core of the pinch where measurements are not currently possible revealing potential temperatures on the 550 eV in the core of the pinch suggesting departures from some of the assumptions in the analysis.

</details>


### [75] [Effects of Turbulent Energy Exchange between Electrons and Ions on Global Temperature Profiles](https://arxiv.org/abs/2512.01618)
*T. Kato,H. Sugama,M. Honda*

Main category: physics.plasm-ph

TL;DR: Turbulent energy exchange between electrons and ions in fusion plasmas can significantly affect temperature profiles in certain scenarios, but has minimal impact in steady-state reactor conditions.


<details>
  <summary>Details</summary>
Motivation: To understand how turbulent energy exchange between particle species affects global temperature profiles in fusion plasmas, particularly in steady-state conditions of future reactors.

Method: Used one-dimensional transport solver GOTRESS to examine turbulent energy exchange impact on global temperature profiles at steady state, analyzing DIII-D discharge data and simulating enhanced electron heating scenarios.

Result: Turbulent energy exchange has minimal effect on DIII-D temperature profiles, but becomes significant with enhanced electron heating where TEM turbulence transfers energy from hot electrons to cold ions. For ITER and SPARC scenarios, turbulent exchange is largely compensated by collisional exchange, producing only small effects.

Conclusion: Turbulent energy exchange has negligible impact on global temperature profiles in steady-state fusion reactor conditions, but can become significant in unbalanced heating situations like plasma startup phases.

Abstract: Microscale turbulence drives not only particle and heat transport but also energy exchange between different particle species. Previous local gyrokinetic studies have shown that turbulent energy exchange can exceed collisional exchange in weakly collisional plasmas, and that ion temperature gradient (ITG) turbulence may hinder ion heating by alpha-heated electrons. In addition, it has been clarified that trapped electron mode (TEM) turbulence transfers energy from electrons to ions, thereby enhancing ion heating. In this work, we extend these studies by examining the impact of turbulent energy exchange on the global temperature profiles at a steady state using the one-dimensional transport solver GOTRESS. For the case of DIII-D discharge 128913 [A. E. White et al., Phys. Plasmas 15, 056116 (2008)], turbulent energy exchange has minimal influence on temperature profiles. However, in the case of enhanced electron heating in a DIIID like tokamak plasma, energy transfer from hot electrons to cold ions driven by TEM turbulence becomes comparable to, or even exceeds, the collisional contribution, leading to a significant increase in the ion temperature profile. For ITER Baseline and SPARC standard H-mode scenarios [N.T. Howard et al., Nucl. Fusion 65, 016002(2024), P. Rodriguez Fernandez et al., J. Plasma Phys. 86, 865860503(2020)], the turbulent energy exchange is largely compensated by the collisional one, producing only small effects. These results indicate that the impact of turbulent energy exchange on the global temperature profiles in steady state conditions of future fusion reactor scenarios is expected to be negligibly small, although it can become significant in situations such as plasma start up phases, where the heating power is strongly unbalanced between electrons and ions.

</details>


### [76] [Light drag in nonuniformly moving anisotropic media through the lens of gradient-index optics](https://arxiv.org/abs/2512.01718)
*Julien Langlois,Renaud Gueroult*

Main category: physics.plasm-ph

TL;DR: Light ray trajectories in moving anisotropic media analyzed using Fresnel drag and gradient index methods, verified against optical metrics, and applied to magnetized plasmas.


<details>
  <summary>Details</summary>
Motivation: To develop a method for determining light ray trajectories in nonuniformly moving anisotropic media, which has applications in plasma physics and relativistic optics where both motion and material anisotropy affect light propagation.

Method: Uses Fresnel drag to account for medium motion at each point along the ray, identifies symmetries in velocity field that translate to symmetries in effective wave index, then applies classical gradient index media techniques to obtain analytical ray trajectories.

Result: The method successfully determines ray trajectories in moving media, with results for isotropic media consistent with those from optical (Gordon) metric approaches. The approach also works for anisotropic media, demonstrated with magnetized plasma examples showing competition between nonuniform motion and anisotropy.

Conclusion: The Fresnel drag-based approach provides an effective framework for analyzing light propagation in moving anisotropic media, bridging classical gradient index methods with relativistic optics and enabling analysis of complex scenarios like moving magnetized plasmas.

Abstract: The trajectory of light rays propagating through a nonuniformly moving anisotropic medium is determined by considering the Fresnel drag experienced by the wave at each point along the ray. By showing that symmetries in the velocity field manifest as symmetries in the effective wave index representing the moving medium, methods classically employed to model gradient index media are then used to obtain analytical forms for the ray trajectory. When applied to isotropic media, the results are verified to be consistent with those obtained using an optical (Gordon) metric. The potential of this method to model light rays in anisotropic media is finally demonstrated by considering waves in a nonuniformly moving magnetized plasma, exposing how nonuniform motion and anisotropy can compete with one another.

</details>


### [77] [Injection and Acceleration of Electrons by Radially Polarized Laser Pulses in a Plasma Channel](https://arxiv.org/abs/2512.01737)
*P. Hadjisolomou,P. Valenta,R. Shaisultanov,T. M. Jeong,D. Gorlova,C. P. Ridgers,S. V. Bulanov*

Main category: physics.plasm-ph

TL;DR: Radially polarized laser pulses outperform linearly polarized ones for electron injection and acceleration in plasma channels, with tighter focusing further enhancing performance.


<details>
  <summary>Details</summary>
Motivation: To investigate how laser polarization (linear vs radial) and focusing geometry affect electron injection and acceleration in plasma channels for optimizing laser-driven electron acceleration setups.

Method: Used 3D particle-in-cell simulations to compare electron injection and acceleration in narrow plasma channels irradiated by: 1) f/10 linearly polarized laser, 2) f/10 radially polarized laser, and 3) f/5 radially polarized laser (matched peak intensity).

Result: Radially polarized beams significantly enhance electron release from channel walls. The f/10 radially polarized case injected ~33% more charge than linearly polarized case. The f/5 radially polarized case achieved about 2x higher maximum electron energy compared to linearly polarized case.

Conclusion: Both polarization (radial vs linear) and focusing geometry are crucial parameters for optimizing laser-driven electron acceleration, with radially polarized beams providing superior performance.

Abstract: We consider injection and subsequent acceleration of electrons in narrow plasma channels irradiated by linearly and radially polarized ultraintense laser pulses. Using three-dimensional particle-in-cell simulations, we show that radially polarized beams significantly promote electron release from the channel walls and lead to enhanced injection. We compare an f/10 linearly polarized laser beam with two radially polarized cases: one focused more tightly (f/5) to match peak intensity, and one at equal f/10 to capture polarization effects. The radially polarized f/10 case injects approximately one-third more charge than the linearly polarized case, while the f/5 radially polarized case outperforms the linearly polarized one by about a factor of two in terms of maximum electron energy. These results highlight polarization and focusing geometry as key parameters for optimizing laser-driven electron acceleration setups.

</details>


### [78] [Evolution of ion distribution functions in ionospheric plasmas perturbed by Alfvén waves](https://arxiv.org/abs/2512.01757)
*Dario Recchiuti,Luca Franci,Lorenzo Matteini,Emanuele Papini,Roberto Battiston,Mirko Piersanti*

Main category: physics.plasm-ph

TL;DR: Hybrid simulations show parametric decay instability of Alfvén waves in Earth's ionosphere causes significant ion kinetic effects including parallel heating, ion beam formation, and bidirectional acceleration, with hydrogen ions more affected than oxygen.


<details>
  <summary>Details</summary>
Motivation: To investigate ion kinetic effects during parametric decay instability of Alfvén waves in Earth's ionosphere conditions, particularly how microscopic wave-particle interactions contribute to particle precipitation in space weather events.

Method: Used hybrid particle-in-cell simulations to systematically study PDI evolution in ultra-low-beta plasmas, varying key parameters (plasma beta, pump-wave amplitude, polarization, ion composition) with dispersive mother waves at ion inertial length scales.

Result: Found pronounced nonthermal VDF modifications including parallel heating and secondary ion beam formation, identified critical regime for rapid VDF broadening and bidirectional acceleration, showed hydrogen ions more affected than oxygen, and quantified time delay between wave impact and VDF modification.

Conclusion: PDI-driven ion kinetics provide plausible mechanism for particle precipitation in space weather, representing first comprehensive hybrid simulation study in ultra-low-beta plasmas with new insights into wave-particle interactions for ion acceleration and space plasma dynamics.

Abstract: This study investigates ion kinetic effects during the parametric decay instability (PDI) of parallel-propagating Alfvén waves under plasma conditions characteristic of the Earth's ionosphere. By using a series of hybrid particle-in-cell simulations, we examine the evolution of ion velocity distribution functions (VDFs) in ultra-low-beta plasmas. Our numerical campaign systematically explores the dependence on key parameters (plasma beta, pump-wave amplitude and polarization, and ion composition). To emphasize the role of kinetic effects, we choose to trigger the PDI with a dispersive mother wave with wavelength comparable to the ion characteristic inertial length. Our results reveal pronounced nonthermal VDF modifications, including parallel heating and the formation of secondary ion beams, linked to the nonlinear evolution of parametric decay instability. By varying the plasma beta and the pump-wave amplitude, we identify a critical regime where rapid and complete broadening of the velocity distribution function is observed, triggering bidirectional ion acceleration. Notably, simulations modeling realistic ionospheric conditions demonstrate that even low-amplitude Alfvénic perturbations can induce significant VDF spreading and ion beam generation, with hydrogen ions exhibiting stronger effects than oxygen. These nonthermal microscopic processes offer a plausible mechanism for particle precipitation in space weather events. This work represents the first comprehensive study with hybrid simulations of PDI-driven ion kinetics in ultra-low-beta plasmas, providing quantitative estimates for the time delay between electromagnetic wave impact and ion VDF modification and new insights into wave-particle interactions that may contribute to ion acceleration, precipitation processes and space plasma dynamics.

</details>


### [79] [High-beta equilibrium in mirror machine with population of fast sloshing ions](https://arxiv.org/abs/2512.01780)
*Ivan Chernoshtanov*

Main category: physics.plasm-ph

TL;DR: Method for constructing high-beta diamagnetic bubble equilibria with fast sloshing ions using adiabatic invariant conservation


<details>
  <summary>Details</summary>
Motivation: To develop equilibrium configurations with high-beta plasma (diamagnetic bubble-like) that include populations of fast sloshing ions, which can arise from off-axis neutral beam injection in fusion devices

Method: Proposes conservation of adiabatic invariant for fast ions moving along betatron orbits, presents simplified expression for the invariant, constructs numerical equilibrium examples with sloshing ions

Result: Numerical examples demonstrate equilibrium with sloshing ions, and conservation of the invariant is validated through direct numerical simulation of fast ion motion even at beta equal to 1

Conclusion: The method successfully constructs high-beta equilibria with fast sloshing ions, with adiabatic invariant conservation validated numerically even at extreme beta conditions

Abstract: A method of constructing the high-beta (diamagnetic-bubble-like) equilibrium with a population of fast sloshing ions is discussed. Fast ions move along betatron orbits; such ions can arise because of off-axis neutral beam injection. Conservation of the adiabatic invariant of these ions is proposed; a simplified expression for the invariant is presented. Numerical examples of equilibrium with sloshing ions are shown and conservation of the invariant is justified by a direct numerical simulation of motion of fast ions even in the case with beta equal to 1.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [80] [On the importance of numerical integration details for homogeneous flow simulation](https://arxiv.org/abs/2512.01318)
*Stephen Sanderson,Debra J. Searles*

Main category: cond-mat.soft

TL;DR: The paper presents a reversible, energy-conserving integration scheme for Sllod equations of motion in molecular dynamics simulations, implemented in LAMMPS to improve accuracy of fluid property calculations.


<details>
  <summary>Details</summary>
Motivation: Existing codes for Sllod equations often lack reversible numerical integration schemes or have subtle problems, limiting accurate simulation of homogeneous flow and fluid properties like viscosity at atomic scale.

Method: Developed a reversible and energy-conserving integration scheme for Sllod equations with O(δt³) error, comparable to standard MD integrators. Implemented the scheme in LAMMPS with careful attention to implementation details.

Result: The new scheme enables more accurate simulation of transient responses, mixed flows, and steady states, especially at high flow rates. Shows that lack of energy conservation causes systematic errors in pressure tensor averages, leading to significant viscosity calculation errors at high flow rates.

Conclusion: A properly implemented reversible, energy-conserving integration scheme for Sllod equations is crucial for accurate molecular dynamics simulations of fluid flow properties, particularly at high flow rates where energy conservation errors become significant.

Abstract: The Sllod equations of motion enable modeling of homogeneous flow at the atomic scale, and are commonly used to predict fluid properties such as viscosity. However, few publicly available codes support such simulations, and those that do often do not implement a reversible numerical integration scheme or have other subtle problems. Here, we demonstrate a reversible and energy-conserving integration scheme for the Sllod equations of motion with error on the order of $δt^3$, in line with typical operator splitting integrators used in standard molecular dynamics simulations. We discuss various implementation details, and implement the scheme in LAMMPS where we find that our changes enable more accurate simulation of transient responses, mixed flows, and steady states, especially at high rates of flow. Importantly, we show that a lack of energy conservation can manifest as a systematic error in the direct ensemble average of the pressure tensor, leading to an error in the calculated viscosity which becomes significant at high flow rates.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [81] [The Spin-MInt Algorithm: an Accurate and Symplectic Propagator for the Spin-Mapping Representation of Nonadiabatic Dynamics](https://arxiv.org/abs/2512.01579)
*Lauren E. Cook,James R. Rampton,Timothy J. H. Hele*

Main category: physics.chem-ph

TL;DR: Developed Spin-MInt algorithm - first symplectic algorithm for spin-mapping Hamiltonian without converting to MMST variables, preserving geometric structure and improving computational efficiency.


<details>
  <summary>Details</summary>
Motivation: No symplectic algorithm existed for spin-mapping representation without converting to MMST variables and using MInt algorithm. Need for structure-preserving, efficient algorithm for nonadiabatic dynamics simulations.

Method: Developed Spin-MInt algorithm that directly propagates spin-mapping variables. For two-level systems, maps onto spin-vector on Bloch sphere. Proved symplecticity using canonical variable transformation despite non-invertible structure matrix. Extended methodology to more than two electronic states.

Result: Spin-MInt is symplectic, symmetrical, second-order, time-reversible, angle invariant and geometric structure preserving. More accurate than previous angle-based algorithms. Significantly faster than MInt algorithm for two electronic states. Successfully applied to three-state morse potential with accurate population results.

Conclusion: First known symplectic algorithm for propagating nonadiabatic spin-mapping Hamiltonian. One of first rigorously symplectic algorithms for non-trivial coupling between canonical and spin systems. Should guide and improve future nonadiabatic dynamics simulations.

Abstract: Mapping methods, including the Meyer-Miller-Stock-Thoss (MMST) mapping and spin-mapping, are commonly utilised to simulate nonadiabatic dynamics by propagating classical mapping variable trajectories. Recent work confirmed the Momentum Integral (MInt) algorithm is the only known symplectic algorithm for the MMST Hamiltonian. To our knowledge, no symplectic algorithm has been published for the spin-mapping representation without converting to MMST variables and utilising the MInt algorithm. Here, we present the Spin-MInt algorithm which directly propagates the spin-mapping variables. First, we consider a two-level system which maps onto a spin-vector on a Bloch sphere and determine that the Spin-MInt is a symplectic, symmetrical, second-order, time-reversible, angle invariant and geometric structure preserving algorithm. Despite spin-variables resulting in a non-invertible structure matrix, we rigorously prove the Spin-MInt is symplectic using a canonical variable transformation. Computationally, we find that the Spin-MInt and MInt algorithms are symplectic, satisfy Liouville's theorem, provide second-order energy conservation and are more accurate than a previously-published angle-based algorithm. The Spin-MInt is significantly faster than the MInt algorithm for two electronic states. Secondly, we extend this methodology to more than two electronic states and present accurate population results for a three-state morse potential. We believe this to be the first known symplectic algorithm for propagating the nonadiabatic spin-mapping Hamiltonian and one of the first rigorously symplectic algorithms in the case of non-trivial coupling between canonical and spin systems. These results should guide and improve future simulations.

</details>


### [82] [Accelerated Machine Learning Force Field for Predicting Thermal Conductivity of Organic Liquids](https://arxiv.org/abs/2512.01627)
*Wei Feng,Siyuan Liu,Hongyi Wang,Zhenliang Mu,Zhichen Pu,Xu Han,Tianze Zheng,Zhenze Yang,Zhi Wang,Weihao Gao,Yidan Cao,Kuang Yu,Sheng Gong,Wen Yan*

Main category: physics.chem-ph

TL;DR: MLFF-based molecular dynamics workflow predicts thermal conductivity of organic liquids with 14% error using differential attention and density alignment.


<details>
  <summary>Details</summary>
Motivation: Thermal conductivity of organic liquids is important for industrial applications, but atomistic simulation has been limited by inaccurate classical force fields and computationally expensive ab initio methods.

Method: Developed machine learning force field (MLFF) workflow with differential attention architecture, aligned with experimental densities, and optimized with Triton language for speed.

Result: Achieved 14% mean absolute percentage error for thermal conductivity prediction, significantly better than classical force field's 78% error.

Conclusion: MLFF-based workflow enables accurate and rapid prediction of thermal conductivity for organic liquids, overcoming limitations of traditional simulation methods.

Abstract: The thermal conductivity of organic liquids is a vital parameter influencing various industrial and environmental applications, including energy conversion, electronics cooling, and chemical processing. However, atomistic simulation of thermal conductivity of organic liquids has been hindered by the limited accuracy of classical force fields and the huge computational demand of ab initio methods. In this work, we present a machine learning force field (MLFF)-based molecular dynamics simulation workflow to predict the thermal conductivity of 20 organic liquids. Here, we introduce the concept of differential attention into the MLFF architecture for enhanced learning ability, and we use density of the liquids to align the MLFF with experiments. As a result, this workflow achieves a mean absolute percentage error of 14% for the thermal conductivity of various organic liquids, significantly lower than that of the current off-the-shelf classical force field (78%). Furthermore, the MLFF is rewritten using Triton language to maximize simulation speed, enabling rapid prediction of thermal conductivity.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [83] [Granite sliding on granite: friction, wear rates, surface topography, and the scale-dependence of rate-state effects](https://arxiv.org/abs/2512.01765)
*Sergey V. Sukhomlinov,Martin H. Müser,B. N. J. Persson*

Main category: physics.geo-ph

TL;DR: Granite friction is dominated by cold welding at asperity junctions, not wear particles. Water reduces wear but not friction, and rate-state effects are negligible. Molecular simulations match experiments and reveal shear strength decreases above 600°C.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental mechanisms controlling friction in granite-granite contacts as a model for tectonic faulting, challenging conventional assumptions about wear-dominated friction and rate-state effects in earthquake models.

Method: Combined experimental tribology, theoretical analysis, and molecular dynamics simulations of granite contacts. Experiments measured friction, wear, and temperature effects. Simulations modeled silica tip sliding on α-quartz.

Result: Friction is dominated by cold welding at plastically deformed asperities, not particulate wear. Water reduces wear 10x but barely affects friction. Rate-state effects negligible (-40°C to 20°C, velocity steps). Surface topography shows quartz smoothing with scale-dependent anisotropy. Simulations reproduce friction coefficients and reveal shear strength decrease above 600°C.

Conclusion: The granite-granite model system captures essential rock friction physics. Cold welding at asperity junctions is the primary friction mechanism, not wear particles. Rate-state effects are scale-dependent and negligible in macroscopic samples. Good correspondence between experiments, simulations, theory, and field observations.

Abstract: We study tribological granite-granite contacts as a model for tectonic faulting, combining experiments, theory, and molecular dynamics simulations. The high friction in this system is not dominated by particulate wear or plowing, as frequently assumed, but by cold welding within plastically deformed asperity junctions. We base this conclusion on the observation that wear is repeatedly high after cleaning contacts but decreases as gouge accumulates, while friction shows the opposite trend. Moreover, adding water reduces wear by a factor of ten but barely decreases friction. Thermal and rate-dependent effects - central to most earthquake models-are negligible: friction remains unchanged between -40°C and 20°C, across abrupt velocity steps, and after hours of stationary contact. The absence of rate-state effects in our macroscopic samples is rationalized by the scale-dependence of pre-slip. The evolution of surface topography shows that quartz grains become locally smooth, with height spectra isotropic for wavelength below 10 microns but anisotropic at longer wavelengths, similar to natural faults. The resulting gouge particles have the usual characteristic sizes near 100 nm. Molecular dynamics simulations of a rigid, amorphous silica tip sliding on α-quartz reproduce not only similar friction coefficients near unity but also other experimentally observed features, including stress-introduced transitions to phases observed in post-mortem faults, as well as theoretical estimates of local flash temperatures. Additionally, they reveal a marked decrease of interfacial shear strength above 600°C. The overall correspondence between experiments, simulations, theory, and field observations indicates that our model system captures essential aspects of rock friction.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [84] [GPU-native Embedding of Complex Geometries in Adaptive Octree Grids Applied to the Lattice Boltzmann Method](https://arxiv.org/abs/2512.01251)
*Khodr Jaber,Ebenezer E. Essel,Pierre E. Sullivan*

Main category: cs.CE

TL;DR: GPU-native algorithm for embedding complex triangle-mesh geometries into AMR octree grids, performing solid voxelization and near-wall refinement entirely on GPU, with efficient ray casting and coalesced memory access for LBM boundary conditions.


<details>
  <summary>Details</summary>
Motivation: Adaptive mesh refinement reduces computational costs in CFD, but efficiently embedding complex, non-aligned geometries on GPUs remains challenging, especially for GPU-native implementations that avoid CPU-GPU synchronization bottlenecks.

Method: GPU-native algorithm using local ray casting accelerated by spatial bin hierarchy, efficient grid-block traversal without index orderings/hash tables, flattened lookup table of cut-link distances for LBM bounce-back boundaries, implemented as extension of AGAL framework.

Result: Benchmarked with Stanford Bunny (112K triangles) and XYZ RGB Dragon (7.2M triangles), validated for external flows past circular/square cylinder (2D, Re=100) and sphere (3D, Re∈{10,15,20}), showing modest overhead with accurate force predictions and stable near-wall resolution.

Conclusion: The approach successfully embeds complex geometries on GPU-resident AMR grids with efficient memory access, enabling accurate LBM simulations and applicable to other explicit solvers requiring GPU geometry embedding.

Abstract: Adaptive mesh refinement (AMR) reduces computational costs in CFD by concentrating resolution where needed, but efficiently embedding complex, non-aligned geometries on GPUs remains challenging. We present a GPU-native algorithm for incorporating stationary triangle-mesh geometries into block-structured forest-of-octrees grids, performing both solid voxelization and automated near-wall refinement entirely on the device. The method employs local ray casting accelerated by a hierarchy of spatial bins, leveraging efficient grid-block traversal to eliminate the need for index orderings and hash tables commonly used in CPU pipelines, and enabling coalesced memory access without CPU-GPU synchronization. A flattened lookup table of cut-link distances between fluid and solid cells is constructed to support accurate interpolated bounce-back boundary conditions for the lattice Boltzmann method (LBM). We implement this approach as an extension of the AGAL framework for GPU-based AMR and benchmark the geometry module using the Stanford Bunny (112K triangles) and XYZ RGB Dragon (7.2M triangles) models from the Stanford 3D Scanning Repository. The extended solver is validated for external flows past a circular/square cylinder (2D, $Re = 100$), and a sphere (3D, $\text{Re}\in\{10, 15, 20\}$). Results demonstrate that geometry handling and interpolation impose modest overhead while delivering accurate force predictions and stable near-wall resolution on adaptive Cartesian grids. The approach is general and applicable to other explicit solvers requiring GPU-resident geometry embedding.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [85] [Turing pattern induced by cross-diffusion in cancer immunotherapy model](https://arxiv.org/abs/2512.00599)
*C. F. Munafò,S. Bonfiglio,P. Rogolino*

Main category: math.DS

TL;DR: Analysis of a spatial cancer immunotherapy model with cross-diffusion effects for ACI-treated and untreated patients, examining equilibrium stability, Turing patterns, and numerical simulations.


<details>
  <summary>Details</summary>
Motivation: To develop a more realistic spatial model of cancer immunotherapy that incorporates cross-diffusion effects, which are biologically relevant for immune-tumor interactions but often neglected in previous models. The model aims to describe both treated (ACI) and untreated patient scenarios.

Method: Generalized spatial cancer immunotherapy model with cross-diffusion effects, analysis of equilibrium points and stability, Turing approach for reaction-diffusion systems to study pattern formation, and numerical simulations using Finite Difference Method for both ACI-treated and untreated scenarios.

Result: The model successfully captures diffusion-driven instability and emergence of spatial patterns (stationary structures) through Turing analysis. Numerical simulations demonstrate different pattern formations for treated vs. untreated patient scenarios, showing how cross-diffusion affects immune response dynamics.

Conclusion: Cross-diffusion effects are crucial for accurately modeling cancer-immune system interactions and cannot be neglected. The generalized model provides insights into pattern formation in both treated and untreated scenarios, with implications for understanding immunotherapy dynamics and potential treatment optimization.

Abstract: In this paper, we investigate a mathematical model describing the interactions between effector cells (E), cancer cells (T), and the IL-2 compound (IL). The model considered here is a generalization, taking into account some cross-diffusion effects, of a spatial cancer immunotherapy model proposed by S. Suddin et al in 2021. These modifications allow us to describe two biologically relevant scenarios: a patient treated with Adoptive Cell Immunotherapy (ACI) and a patient not receiving any treatment/therapy. Cross-diffusion effects are particularly relevant in the interactions between tumor cells and the immune system, in fact they play a key role in immune response dynamics and cannot be neglected. We analyze the equilibrium points of the homogeneous system, along with their stability and bifurcation mechanisms. Furthermore, adopting the Turing approach for reaction-diffusion systems, we investigate the diffusion-driven instability and the emergence of spatial regular structures (stationary in time), i.e. the patterns. Finally, numerical simulations based on the Finite Difference Method (FDM) are presented for the two previously mentioned scenarios.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [86] [Weak coupling for Schrödinger operators with complex potentials](https://arxiv.org/abs/2512.01941)
*Jussi Behrndt,Markus Holzmann,Petr Siegl,Nicolas Weber*

Main category: math.SP

TL;DR: Non-self-adjoint Schrödinger operators with complex potentials in weak coupling: conditions for existence/absence of discrete eigenvalues near essential spectrum threshold, including uniqueness and multiplicity analysis.


<details>
  <summary>Details</summary>
Motivation: Extend classical weak coupling theory from self-adjoint to non-self-adjoint Schrödinger operators with complex potentials, generalizing Simon's 1976 results to complex-valued L^p potentials in 1D/2D.

Method: Study discrete eigenvalues emerging from essential spectrum threshold of 1D/2D Schrödinger operators with complex L^p potentials in weak coupling regime using spectral analysis techniques.

Result: Derived necessary and sufficient conditions on complex potentials for existence/absence of discrete eigenvalues in weak coupling, analyzed their uniqueness and algebraic multiplicity.

Conclusion: Provides natural non-self-adjoint extension of classical weak coupling theory, establishing comprehensive eigenvalue behavior for complex potentials near essential spectrum threshold.

Abstract: We study the discrete eigenvalues emerging from the threshold of the essential spectrum of one or two-dimensional Schrödinger operators with complex-valued $ L^p $-potentials in a weak coupling regime. We derive necessary and sufficient conditions on the potential for the existence or absence of discrete eigenvalues in this regime and also analyze their uniqueness and algebraic multiplicity. Our results can be viewed as natural non-self-adjoint extensions of the well-known classical weak coupling phenomenon for self-adjoint Schrödinger operators with real-valued potentials going back half a century to Simon's famous paper [Simon 1976].

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [87] [Chain of Unit-Physics: A Primitive-Centric Approach to Scientific Code Synthesis](https://arxiv.org/abs/2512.01010)
*Vansh Sharma,Venkat Raman*

Main category: cs.MA

TL;DR: Proposes Chain of Unit-Physics framework for reliable scientific code generation using first-principles tests to constrain LLMs, achieving human-expert accuracy on combustion benchmark.


<details>
  <summary>Details</summary>
Motivation: Current LLMs fail at reliable scientific code generation due to sparse domain code representation in training and limited RLHF feasibility with small expert communities, especially for high-stakes problems.

Method: Inverse code design approach using Chain of Unit-Physics: multi-agent system encoding human expert knowledge as unit-physics tests that explicitly constrain code generation, evaluated on combustion benchmark.

Result: Framework converges in 5-6 iterations, matches human-expert implementation (mean error 3.1×10⁻³%), with ~33.4% faster runtime and ~30% more efficient memory usage at comparable cost to commercial APIs.

Conclusion: Chain of Unit-Physics provides physics-grounded template for scientific code generation that embeds first-principles analysis, outperforming existing LLM approaches that suffer from interface hallucinations and physical incoherence.

Abstract: Agentic large language models are proposed as autonomous code generators for scientific computing, yet their reliability in high-stakes problems remains unclear. Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community. To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation. The framework is evaluated on a nontrivial combustion task, used here as a representative benchmark for scientific problem with realistic physical constraints. Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility. Open-weight models with chain-of-thought (CoT) decoding reduce interface errors but still yield incorrect solutions. On the benchmark task, the proposed framework converges within 5-6 iterations, matches the human-expert implementation (mean error of $3.1\times10^{-3}$ %), with a $\sim$33.4 % faster runtime and a $\sim$30 % efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation. As datasets and models evolve, zero-shot code accuracy will improve; however, the Chain of Unit-Physics framework goes further by embedding first-principles analysis that is foundational to scientific codes.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [88] [Programmable Switching of Molecular Transitions via Plasmonic Toroidal Nanoantennae](https://arxiv.org/abs/2512.01303)
*Arda Gulucu,Emre Ozan Polat*

Main category: physics.optics

TL;DR: Plasmonic toroidal nanoantenna enables complete switching of molecular transition energies with 99.9% modulation depth and 2840-fold radiative enhancement through Fano interference.


<details>
  <summary>Details</summary>
Motivation: To develop programmable molecular switching systems for applications in biosensing and quantum computing by leveraging the unique field-focusing properties of toroidal nanoantennae around quantum emitters.

Method: Using toroidal nanoantennae (TNAs) with optimized geometries to create Fano interference between broadband plasmonic continuum and narrow quantum transitions, suppressing both radiative and non-radiative decay channels near 850 nm.

Result: Achieved complete switching of molecular transition energies with 99.9% modulation depth and 2840-fold radiative enhancement. Demonstrated systems with multiple quantum objects where spectral degeneracy enhances transparency bandwidth and detuning creates individually addressable spectral responses.

Conclusion: Plasmonic TNA architecture enables high-sensitivity spectral detection of single or multi-molecule configurations and provides a platform for implementing quantum mode switches in photonic processing applications.

Abstract: The ability to switch and program molecular transitions via deterministically located plasmonic nanoantennae presents opportunities for wide spectrum of applications from biosensors to quantum computing. Due to its topology, toroidal nanoantenna (TNA) focuses immense amount of three-dimensional (3D) local electric field by toroidal moment while allowing pre and post positioning around quantum emitters (QEs). Here, we report complete switching of molecular transition energies of quantum objects (QOs) with modulation depth of 99.9% over 2840-fold radiative enhancement. At optimized TNA geometries, Fano interference between the broadband plasmonic continuum and narrow quantum transitions of QOs suppresses both radiative and non-radiative decay channels near 850 nm, yielding an observable full switching that traps energy within the hybrid mode instead of re-emitting it. To show the promises of the concept, we further demonstrate systems with multiple QOs where spectral degeneracy enhances the transparency bandwidth, while detuning generates distinct minima, enabling individually addressable spectral responses. These results establish plasmonic TNA as a promising architecture for spectral detection of single or multi-molecule configurations with high sensitivity and empowers the user for the implementation of quantum mode switches to be used in photonic processing.

</details>


### [89] [Near-field perturbation of laser filament enabling simultaneous far-field THz diagnosis and broadband calculus processing](https://arxiv.org/abs/2512.01215)
*Jiayu Zhao,Yifu Tian,Linlin Yuan,Jiajun Yang,Xiaofeng Li,Li Lao,Alexander Shkurinov,Yan Peng,Yiming Zhu*

Main category: physics.optics

TL;DR: Researchers developed a non-invasive method to characterize THz modes in laser filaments by using a metal plate to perturb the dielectric environment, enabling far-field detection of near-field THz mode confinement through calculus operations on THz waveforms.


<details>
  <summary>Details</summary>
Motivation: Direct near-field probing of THz modes within laser filaments is challenging due to the plasma's ultra-high intensity, and indirect far-field reconstruction methods are limited. There's a need for better characterization techniques to understand plasma-THz interactions.

Method: A metal plate is brought close to the filament at submillimeter distances (comparable to THz wavelengths) to perturb the dielectric environment. This converts symmetric annular THz modes into asymmetric states, enabling far-field detection of calculus behaviors on THz waveforms.

Result: The method successfully enabled detection of broadband calculus operations (first- and second-order differentiation/integration) on time-domain THz waveforms and characteristic spectral transfer functions with 1/f, 1/f^2, f or f^2 frequency dependencies, diagnosing near-field THz mode confinement.

Conclusion: This approach synergizes near-field modulation efficiency with far-field detection robustness, advancing understanding of plasma-THz interactions and enabling novel all-optical signal processing for filament-based THz technologies.

Abstract: Terahertz (THz) wave manipulation based on laser filaments-plasma channels formed by femtosecond laser-induced air ionization-has emerged as a promising platform for free-space THz applications. However, in-situ characterization of the spatially confined THz modes within filaments faces significant challenges due to the plasma's ultra-high intensity, which not only hinders direct near-field probing but also limits reliance on indirect far-field reconstruction. Here, we introduce a non-invasive near-field modulation scheme where a metal plate approaches the filament at submillimeter distances (comparable to THz wavelengths), perturbing the dielectric environment to convert the symmetric annular THz mode into an asymmetric state. This controlled transition enables far-field detection of broadband calculus behaviors (first- and second-order differentiation/integration) on time-domain THz waveforms and characteristic spectral transfer functions with 1/f, 1/f^2, f or f^2 dependency (where f is the THz frequency), thereby diagnosing the near-field THz mode confinement. Hence, the proposed approach synergizes near-field modulation efficiency with far-field detection robustness, advancing fundamental understanding of plasma-THz interactions and enabling novel all-optical signal processing for filament-based THz technologies.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [90] [Domain-Decomposed Graph Neural Network Surrogate Modeling for Ice Sheets](https://arxiv.org/abs/2512.01888)
*Adrienne M. Propp,Mauro Perego,Eric C. Cyr,Anthony Gruber,Amanda A. Howard,Alexander Heinlein,Panos Stinis,Daniel M. Tartakovsky*

Main category: cs.LG

TL;DR: A physics-inspired graph neural network surrogate model with domain decomposition and transfer learning for efficient large-scale PDE simulations, demonstrated on ice sheet dynamics.


<details>
  <summary>Details</summary>
Motivation: Need for accurate yet efficient surrogate models for large-scale PDE simulations, especially for uncertainty quantification tasks requiring hundreds/thousands of evaluations.

Method: Physics-inspired GNN operating on unstructured meshes with graph attention, domain decomposition strategy partitioning mesh into subdomains, parallel training of local GNN surrogates, aggregation of predictions, and transfer learning for fine-tuning across subdomains.

Result: Accurately predicts full-field velocities on high-resolution ice sheet meshes, substantially reduces training time compared to single global surrogate model, provides foundation for UQ objectives.

Conclusion: Graph-based domain decomposition combined with transfer learning provides scalable and reliable pathway for training GNN surrogates on massive PDE-governed systems, with broad potential beyond ice sheet dynamics.

Abstract: Accurate yet efficient surrogate models are essential for large-scale simulations of partial differential equations (PDEs), particularly for uncertainty quantification (UQ) tasks that demand hundreds or thousands of evaluations. We develop a physics-inspired graph neural network (GNN) surrogate that operates directly on unstructured meshes and leverages the flexibility of graph attention. To improve both training efficiency and generalization properties of the model, we introduce a domain decomposition (DD) strategy that partitions the mesh into subdomains, trains local GNN surrogates in parallel, and aggregates their predictions. We then employ transfer learning to fine-tune models across subdomains, accelerating training and improving accuracy in data-limited settings. Applied to ice sheet simulations, our approach accurately predicts full-field velocities on high-resolution meshes, substantially reduces training time relative to training a single global surrogate model, and provides a ripe foundation for UQ objectives. Our results demonstrate that graph-based DD, combined with transfer learning, provides a scalable and reliable pathway for training GNN surrogates on massive PDE-governed systems, with broad potential for application beyond ice sheet dynamics.

</details>


### [91] [Beyond Loss Guidance: Using PDE Residuals as Spectral Attention in Diffusion Neural Operators](https://arxiv.org/abs/2512.01370)
*Medha Sawhney,Abhilash Neog,Mridul Khurana,Anuj Karpatne*

Main category: cs.LG

TL;DR: PRISMA is a conditional diffusion neural operator that embeds PDE residuals directly into the model architecture via spectral attention, enabling gradient-free inference with competitive accuracy at much lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based PDE solvers suffer from slow gradient-based optimization, instability, and inability to adapt to noisy residuals during inference.

Method: PRISMA integrates PDE residuals as architectural features via attention mechanisms in the spectral domain, eliminating the need for gradient-based optimization during inference.

Result: PRISMA achieves competitive accuracy with 10x-100x fewer denoising steps, leading to 15x-250x faster inference across five benchmark PDEs, especially with noisy observations.

Conclusion: PRISMA provides a gradient-free, robust, and efficient alternative to traditional diffusion-based PDE solvers by architecturally integrating PDE residuals rather than using them as external optimization targets.

Abstract: Diffusion-based solvers for partial differential equations (PDEs) are often bottle-necked by slow gradient-based test-time optimization routines that use PDE residuals for loss guidance. They additionally suffer from optimization instabilities and are unable to dynamically adapt their inference scheme in the presence of noisy PDE residuals. To address these limitations, we introduce PRISMA (PDE Residual Informed Spectral Modulation with Attention), a conditional diffusion neural operator that embeds PDE residuals directly into the model's architecture via attention mechanisms in the spectral domain, enabling gradient-descent free inference. In contrast to previous methods that use PDE loss solely as external optimization targets, PRISMA integrates PDE residuals as integral architectural features, making it inherently fast, robust, accurate, and free from sensitive hyperparameter tuning. We show that PRISMA has competitive accuracy, at substantially lower inference costs, compared to previous methods across five benchmark PDEs, especially with noisy observations, while using 10x to 100x fewer denoising steps, leading to 15x to 250x faster inference.

</details>


### [92] [Data assimilation and discrepancy modeling with shallow recurrent decoders](https://arxiv.org/abs/2512.01170)
*Yuxuan Bao,J. Nathan Kutz*

Main category: cs.LG

TL;DR: DA-SHRED is a machine learning framework that bridges the simulation-to-real gap by combining computational models with real sensor data for accurate state reconstruction of complex physical systems.


<details>
  <summary>Details</summary>
Motivation: Modern sensing faces challenges with data efficiency, real-time processing, and limited coverage. Simulation models often neglect small-scale processes, are sensitive to perturbations, or oversimplify parameter correlations, leading to reconstructions that diverge from reality. There's a critical need for data assimilation to integrate observational data with predictive models for accurate state estimation.

Method: DA-SHRED (Data Assimilation with SHallow REcurrent Decoder) uses a shallow recurrent decoder to bridge SIM2REAL gap. It leverages latent space learned from reduced simulation models via SHRED, updates latent variables using real sensor data to reconstruct full system state, and incorporates sparse identification of nonlinear dynamics (SINDy) regression to identify missing dynamics in simulation models.

Result: The framework successfully closes the SIM2REAL gap and recovers missing dynamics in highly complex systems. The combination of efficient temporal encoding and physics-informed correction enables robust data assimilation.

Conclusion: DA-SHRED provides an effective machine learning approach for data assimilation that integrates computational modeling with experimental sensor data, addressing the limitations of simulation models while maintaining accuracy in reconstructing full system states from sparse measurements.

Abstract: The requirements of modern sensing are rapidly evolving, driven by increasing demands for data efficiency, real-time processing, and deployment under limited sensing coverage. Complex physical systems are often characterized through the integration of a limited number of point sensors in combination with scientific computations which approximate the dominant, full-state dynamics. Simulation models, however, inevitably neglect small-scale or hidden processes, are sensitive to perturbations, or oversimplify parameter correlations, leading to reconstructions that often diverge from the reality measured by sensors. This creates a critical need for data assimilation, the process of integrating observational data with predictive simulation models to produce coherent and accurate estimates of the full state of complex physical systems. We propose a machine learning framework for Data Assimilation with a SHallow REcurrent Decoder (DA-SHRED) which bridges the simulation-to-real (SIM2REAL) gap between computational modeling and experimental sensor data. For real-world physics systems modeling high-dimensional spatiotemporal fields, where the full state cannot be directly observed and must be inferred from sparse sensor measurements, we leverage the latent space learned from a reduced simulation model via SHRED, and update these latent variables using real sensor data to accurately reconstruct the full system state. Furthermore, our algorithm incorporates a sparse identification of nonlinear dynamics based regression model in the latent space to identify functionals corresponding to missing dynamics in the simulation model. We demonstrate that DA-SHRED successfully closes the SIM2REAL gap and additionally recovers missing dynamics in highly complex systems, demonstrating that the combination of efficient temporal encoding and physics-informed correction enables robust data assimilation.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [93] [Cauchy data for multiple collapsing boson stars](https://arxiv.org/abs/2512.01844)
*Elena Giorgi,Dawei Shen,Jingbo Wan*

Main category: gr-qc

TL;DR: Construction of initial data for EMKG system that evolves to form multiple trapped surfaces (black holes) from well-separated boson stars.


<details>
  <summary>Details</summary>
Motivation: Extend previous vacuum results on multiple trapped surface formation to the Einstein-Maxwell-Klein-Gordon system, which includes electromagnetic and scalar fields, allowing study of boson star collapse.

Method: Construct Cauchy initial data for the EMKG system that represents multiple well-separated boson stars, then analyze their evolution under the field equations.

Result: The constructed initial data evolves in finite time to form spacetimes containing multiple trapped surfaces, corresponding to multiple black holes forming from collapsing boson stars.

Conclusion: Successfully extends vacuum results to EMKG system, demonstrating formation of multiple black holes from boson star collapse, providing insights into gravitational collapse with matter fields.

Abstract: We construct Cauchy initial data for the Einstein-Maxwell-Klein-Gordon (EMKG) system, which evolves in finite time into spacetimes containing multiple trapped surfaces. From a physical perspective, this corresponds to preparing multiple well-separated boson stars, each of which collapses to form a spacelike black hole region. In particular, this extends the result of the second and third named authors on the formation of multiple trapped surfaces in vacuum to the EMKG system.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [94] [Cross-Geometry Transfer Learning in Fast Electromagnetic Shower Simulation](https://arxiv.org/abs/2512.00187)
*Frank Gaede,Gregor Kasieczka,Lorenzo Valente*

Main category: physics.ins-det

TL;DR: Transfer learning framework for generative calorimeter simulation enables adaptation across diverse detector geometries with high data efficiency, using only 100 target-domain samples for significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Traditional Monte Carlo methods like Geant4 are computationally expensive, and existing ML surrogates are tied to specific detector geometries, requiring complete retraining for each design change or alternative detector.

Method: Uses point cloud representations and pre-training on the International Large Detector detector, enabling handling of new configurations without re-voxelizing showers for each geometry. Implements parameter-efficient fine-tuning with bias-only adaptation.

Result: Transfer learning with only 100 target-domain samples achieves 44% improvement on geometric mean of Wasserstein distance over training from scratch. Bias-only adaptation achieves competitive performance while updating only 17% of model parameters.

Conclusion: The framework establishes a baseline for future progress of point cloud approaches in calorimeter simulation and provides insight into adaptation mechanisms for particle shower development.

Abstract: Accurate particle shower simulation remains a critical computational bottleneck for high-energy physics. Traditional Monte Carlo methods, such as Geant4, are computationally prohibitive, while existing machine learning surrogates are tied to specific detector geometries and require complete retraining for each design change or alternative detector. We present a transfer learning framework for generative calorimeter simulation models that enables adaptation across diverse geometries with high data efficiency. Using point cloud representations and pre-training on the International Large Detector detector, our approach handles new configurations without re-voxelizing showers for each geometry. On the CaloChallenge dataset, transfer learning with only 100 target-domain samples achieves a $44\%$ improvement on the geometric mean of Wasserstein distance over training from scratch. Parameter-efficient fine-tuning with bias-only adaptation achieves competitive performance while updating only $17\%$ of model parameters. Our analysis provides insight into adaptation mechanisms for particle shower development, establishing a baseline for future progress of point cloud approaches in calorimeter simulation.

</details>


<div id='math.RA'></div>

# math.RA [[Back]](#toc)

### [95] [Necessary and Sufficient Criterion for Singular or Nonsingular of Diagonally Dominant Matrices](https://arxiv.org/abs/2512.00463)
*Jidong Jin*

Main category: math.RA

TL;DR: Develops complete theory for determining singularity/nonsingularity of diagonally dominant matrices, reducing general problem to irreducible case and providing angle-based conditions.


<details>
  <summary>Details</summary>
Motivation: Classical problem in matrix theory with applications in network dynamical systems, affine multi-agent systems, and Kolmogorov differential equations.

Method: Extends Taussky's theorem, reduces general problem to irreducible diagonally dominant matrices, performs similarity/unitary similarity analysis, and derives angle equation system conditions.

Result: Establishes necessary and sufficient conditions for singularity/nonsingularity, with complete characterization of singular irreducible diagonally dominant matrices via angle equations.

Conclusion: Provides unified theoretical framework for diagonally dominant matrix singularity analysis with practical applications in various dynamical systems.

Abstract: The problem of determining whether a diagonally dominant matrix is singular or nonsingular is a classical topic in matrix theory. This paper develops necessary and sufficient conditions for the singularity or nonsingularity of diagonally dominant matrices. Starting from Taussky's theorem, we establish a unified line of theory which reduces the general problem to the study of irreducible diagonally dominant matrices. A complete similarity and unitary similarity analysis is given for singular irreducible diagonally dominant matrices, leading to a necessary and sufficient condition expressed in terms of an angle equation system associated with the nonzero off-diagonal entries. Applications and motivations from network dynamical systems, affine multi-agent systems, and Kolmogorov differential equations are also discussed.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [96] [Distortion-Driven Carrier Decoupling in Doped LiMgPO4](https://arxiv.org/abs/2511.12674)
*Zhihua Zheng,Xiaolong Yao,Cailian Yu,Menghao Gao,Fangping Ouyang,Shiwu Gao*

Main category: cond-mat.mtrl-sci

TL;DR: Researchers uncover how hierarchical lattice distortions in alkali-doped LiMgPO4 enable carrier decoupling - electrons localize as trapped polarons while holes form mobile polarons, explaining enhanced dosimetric response.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand the microscopic origin of enhanced dosimetric response in alkali-doped LiMgPO4, where the interplay between lattice distortions and charge carriers is crucial but not well understood.

Method: Using non-adiabatic molecular dynamics simulations to investigate carrier dynamics and polaron formation in alkali-doped LiMgPO4, analyzing how different lattice distortions affect electron and hole behavior.

Result: Electrons localize into stable small polarons on ultrafast timescales, trapped by strong local potentials from dopants, while holes form more delocalized polarons that migrate efficiently through globally strained lattice, leading to suppressed recombination and enhanced energy storage.

Conclusion: Multiscale lattice distortions can independently control electron and hole transport, providing a clear physical mechanism for enhanced dosimetric properties and offering new insights into polaron physics in complex functional oxides.

Abstract: The interplay between lattice distortions and charge carriers governs the properties of many functional oxides. In alkali-doped LiMgPO4, a significant enhancement in dosimetric response is observed, but its microscopic origin is not understood. Using non-adiabatic molecular dynamics, we reveal a fundamental mechanism of carrier decoupling driven by a hierarchy of lattice distortions. We show that electrons localize into stable small polarons on an ultrafast timescale, trapped by the strong local potential induced by the dopant, while holes form more delocalized polarons that migrate efficiently through a lattice smoothed by global strain. The stark contrast between the dynamics of trapped electrons and mobile holes explains the suppressed recombination and enhanced energy storage. These results present a clear physical picture of how multiscale lattice distortions can independently control electron and hole transport, offering new insights into the physics of polarons in complex materials.

</details>


### [97] [Electric Polarization from Nonpolar Phonons](https://arxiv.org/abs/2512.00628)
*Seongjoo Jung,Turan Birol*

Main category: cond-mat.mtrl-sci

TL;DR: Researchers extend Born effective charge framework to include second-order effects, showing nonpolar phonon modes in oxides can induce significant second-order polarizations comparable to polar modes.


<details>
  <summary>Details</summary>
Motivation: Born effective charge (BEC) only captures linear polarization response to ionic displacements, missing important higher-order effects that can be significant in materials like fluorite HfO₂.

Method: Introduce second-order dynamical charge and mode effective charge concepts, use first-principles calculations, and perform symmetry-based analysis of charge density to understand microscopic origins.

Result: Specific combinations of nonpolar phonon modes in oxides induce substantial second-order polarizations comparable to intrinsically polar modes; large effects found in SrTiO₃ and other perovskites.

Conclusion: Reveals previously unrecognized polarization mechanism in crystalline solids, offering new design principles for next-generation ferroelectric, piezoelectric, and multifunctional materials.

Abstract: Born effective charge (BEC), a fundamental quantity in lattice dynamics and ferroelectric theory, provides a quantitative measure of linear polarization response to ionic displacements. However, it does not account for higher-order effects, which can play a significant role in certain materials, such as fluorite HfO$_2$. In this letter, we extend the BEC framework by introducing the concept of second-order dynamical charge and mode effective charge. Using first-principles calculations, we demonstrate that specific combinations of nonpolar phonon modes in many oxides can induce substantial second-order polarizations, reaching magnitudes comparable to those of intrinsically polar modes. Through a symmetry-based analysis of the charge density, we elucidate the microscopic origin of these effects, tracing them to variations in bond covalency and local electronic rearrangements. We also demonstrate large second-order mode effective charge in well-studied perovskites such as SrTiO$_3$, highlighting the generality of these phenomena. Our results reveal a previously unrecognized mechanism that drives polarization in crystalline solids, offering new insights into the design principles of next-generation ferroelectric, piezoelectric and multifunctional materials.

</details>


### [98] [First-Principles Investigation of X2NiH6 (X = Ca, Sr, Ba) Hydrides for Hydrogen Storage Applications](https://arxiv.org/abs/2512.01072)
*K. Aafi,Z. El Fatouaki,A. Jabar,A. Tahiri,M. Idiri*

Main category: cond-mat.mtrl-sci

TL;DR: First-principles DFT study of Ca2NiH6, Sr2NiH6, and Ba2NiH6 hydrides reveals Ca2NiH6 as the most promising hydrogen storage material with 4.005 wt% capacity, good thermodynamic stability, and highest resistance to deformation.


<details>
  <summary>Details</summary>
Motivation: To investigate and compare the thermodynamic, kinetic, optical, and mechanical properties of alkaline earth nickel hydrides (Ca2NiH6, Sr2NiH6, Ba2NiH6) for hydrogen storage applications using first-principles calculations.

Method: First-principles Density Functional Theory (DFT) calculations were performed to analyze thermodynamic properties (entropy, heat capacity, free energy), hydrogen storage kinetics, optical properties (refractive index), and mechanical properties (compressibility, deformation resistance) of the three hydride compounds.

Result: Ca2NiH6 shows the highest hydrogen storage capacity (4.005 wt%), best thermodynamic stability with negative free energies at elevated temperatures, and highest resistance to deformation. Sr2NiH6 is incompressible with moderate malleability, while Ba2NiH6 is most compressible with high refractive index at low energies. All compounds exhibit increasing entropy and heat capacity with temperature.

Conclusion: Ca2NiH6 emerges as the most promising candidate for hydrogen storage technology due to its superior hydrogen storage capacity (4.005 wt%), excellent thermodynamic stability, and mechanical robustness, making it suitable for practical hydrogen storage applications.

Abstract: First-principles DFT calculations on the hydrides Ca2NiH6, Sr2NiH6, and Ba2NiH6 reveal key thermodynamic properties. These compounds exhibit increasing entropy and heat capacity with temperature, and are thermodynamically stable at elevated temperatures due to negative free energies. The kinetics of hydrogen storage is influenced by entropy changes during hydrogen adsorption and desorption. Optically, Ba2NiH6 shows a high refractive index at low energies. Mechanical assessments indicate Sr2NiH6 is incompressible with moderate malleability, Ca2NiH6 has the highest resistance to deformation, while Ba2NiH6 is most compressible. Formation energies and hydrogen storage capacities (4.005 wt% for Ca2NiH6, 2.548 wt% for Sr2NiH6, and 1.750 wt% for Ba2NiH6) highlight Ca2NiH6 as the most promising candidate for hydrogen storage technology.

</details>


### [99] [An Investigation of Thermal Properties of Cu-Au Janus Nanoparticles](https://arxiv.org/abs/2512.01425)
*Mehmet Akif Cebeci,Hatice Zor Oguz,Sevgi Ozdemir Kart,Hasan Huseyin Kart*

Main category: cond-mat.mtrl-sci

TL;DR: MD simulations reveal Cu-rich Cu-Au Janus nanoparticles have higher melting temperatures with defined phase transitions, while Au-rich structures show lower melting points with surface-initiated melting.


<details>
  <summary>Details</summary>
Motivation: To understand how composition ratio affects thermal and structural properties of Cu-Au Janus nanoparticles, particularly melting behavior and atomic mobility.

Method: Molecular dynamics simulations using embedded atom model (EAM) interactions on 5nm diameter nanoparticles with varying Cu/Au ratios, analyzing melting temperature, heat capacity, RDF, Lindemann index, MSD, and diffusion coefficients.

Result: Cu-rich nanoparticles exhibit higher melting temperatures and clearer phase transitions, while Au-rich structures show lower melting temperatures with surface-initiated melting; thermal stability and atomic mobility depend on composition ratio and material distribution.

Conclusion: Composition ratio significantly influences thermal properties of Cu-Au Janus nanoparticles, with Cu-rich structures being more thermally stable and Au-rich structures showing surface-dominated melting behavior.

Abstract: In this paper, the thermal and structural properties of Cu-Au (Copper-Gold) Janus nanoparticles with a diameter of 5 nm are investigated by using molecular dynamics (MD) simulations within the interactions defined by the many-body embedded atom model (EAM). A set of nanoparticle models has been constructed, with varying Cu and Au ratios. MD method is carried out to calculate the melting temperature, heat capacity, radial distribution function (RDF), Lindemann index, mean square displacement (MSD), and diffusion coefficients of these models. The findings demonstrate that nanoparticles rich in Cu exhibit a higher melting temperature and more defined phase transitions. In contrast, structures rich in gold exhibited reduced melting temperatures and showed surface-initiated melting behaviours. MD study highlights that the thermal stability and atomic mobility of Cu-Au Janus nanoparticles depend on the composition ratio and the dispersion of materials.

</details>


### [100] [First-principles screening of materials with extreme effective masses](https://arxiv.org/abs/2512.01631)
*Szymon Błazucki,Junfeng Qiao,Nicola Marzari*

Main category: cond-mat.mtrl-sci

TL;DR: High-throughput computational screening of ~20,000 inorganic materials to compute conductivity effective mass tensors using DFT and Wannier functions, identifying materials with extreme electronic properties from ultra-low to ultra-large effective masses.


<details>
  <summary>Details</summary>
Motivation: Effective mass is a fundamental descriptor for electronic structure that can predict performance in electronics, thermoelectrics, and transparent conductors, but comprehensive screening across chemical space is needed.

Method: Combined density-functional theory calculations with maximally localized Wannier functions to compute full conductivity effective mass tensors from Boltzmann transport equation in constant relaxation-time approximation, capturing band non-parabolicity, anisotropy, and valley multiplicity.

Result: Identified curated set of candidates with extreme electronic properties (ultra-low to ultra-large effective masses), validated workflow by recovering known high-mobility semiconductors, and discovered promising novel candidates.

Conclusion: The dataset provides systematic roadmap for searching high-performance materials in novel chemical spaces, with discussion of physical limits for defining conductivity effective mass in narrow-gap regimes at room temperature.

Abstract: The effective mass of charge carriers is a fundamental descriptor of the electronic structure of materials, and can be used to assess performance in electronics applications, or to screen for thermoelectrics and transparent conductors. Here, we perform a high-throughput computational screening of approximately 20,000 experimentally known three-dimensional stoichiometric inorganics obtained from the Materials Cloud 3D structure database. By combining density-functional theory calculations and maximally localized Wannier functions, we are able to compute the full conductivity effective mass tensor for electrons and holes from the Boltzmann transport equation in the constant relaxation-time approximation. This approach captures the effects of band non-parabolicity, anisotropy, and valley multiplicity that would be neglected by standard parabolic fittings. The screening identifies a curated set of candidates exhibiting extreme electronic properties, from ultra-low to ultra-large effective masses, these latter associated with flat-band physics. We validate the workflow by recovering established high-mobility semiconductors and highlight promising novel candidates. Furthermore, we classify materials by their mass anisotropy and discuss the physical limits of defining a conductivity effective mass in narrow-gap regimes at room temperature. The resulting dataset provides a systematic roadmap to search for high-performance materials in novel chemical spaces.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [101] [Exact quantum dynamics of Fermi--Hubbard systems using the Gaussian phase-space representation with diffusion gauges](https://arxiv.org/abs/2512.00987)
*F Rousse,M Fasi,A Dmytryshyn,M Gulliksson,J F Corney,M Ogren*

Main category: math-ph

TL;DR: The paper presents an improved Gaussian Phase-Space Representation method for simulating interacting fermion dynamics, using diffusion gauge optimization to extend practical simulation times and handle larger systems.


<details>
  <summary>Details</summary>
Motivation: The Gaussian Phase-Space Representation for simulating interacting fermion dynamics has a limitation - the "spiking point" that restricts practical simulation time. While gauge adjustments can delay this spiking, there's a need for more efficient methods to implement noise terms and extend simulation capabilities.

Method: The authors work with the diffusion gauge in the Gaussian Phase-Space Representation and propose a new algorithm to efficiently implement noise terms. This optimization allows for different equivalent stochastic differential equations that delay the spiking point.

Result: Compared to their previous work (Rousse et al. 2024), the new method achieves significantly longer practical simulation times and can be applied to substantially larger systems in 1D, 2D, and 3D.

Conclusion: The diffusion gauge optimization provides an effective approach to extend the practical simulation capabilities of the Gaussian Phase-Space Representation for interacting fermion dynamics, overcoming previous limitations on simulation time and system size.

Abstract: We use the Gaussian Phase-Space Representation to solve the real-time dynamic of interacting fermions in 1D, 2D, and 3D systems. The method is exact up to a spiking point, which represents a limit on the practical simulation time. The spiking can be delayed, and the practical simulation time extended, by adjusting the gauges of the representation, resulting in different equivalent stochastic differential equations. Here, we work on the so-called diffusion gauge and propose an algorithm to find efficiently new implementations of the noise terms. Compared with our initial results [F. Rousse \textit{et al.} 2024, J. Phys. A: Math. Theor. \textbf{57}, 015303], the new method achieves a significantly longer practical simulation time and can be applied to significantly larger systems.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [102] [A simulation approach including under-resolved scales for multi-component fluid flows in multi-scale porous structures](https://arxiv.org/abs/2512.01013)
*Hiroshi Otomo,Rafael Salazar-Tio,Jingjing Yang,Hongli Fan,Andrew Fager,Bernd Crouse,Raoyang Zhang,Hudong Chen*

Main category: physics.flu-dyn

TL;DR: Develops computational models for accurate multi-component flow simulation in under-resolved multi-scale porous structures using pre-computed physical properties to represent unresolved scales.


<details>
  <summary>Details</summary>
Motivation: It's impractical to fully resolve flow in porous structures with large length-scale differences due to high computational expense, requiring methods to account for under-resolved scales.

Method: Uses pre-computed physical properties (absolute permeability K0, capillary-pressure-saturation curve, relative permeability Kr) from resolved porous structures to model local fluid forces in under-resolved regions represented as porous media.

Result: Benchmark tests show quantitatively consistent results with analytic solutions and fine-resolution simulations for both single and multi-component flows, accurately capturing imbibition patterns, entry pressure, residual component patterns, and permeability.

Conclusion: The approach enables feasible and accurate simulation of flow in multi-scale porous structures by properly accounting for under-resolved scales through physics-based modeling with pre-computed properties.

Abstract: In this study, we develop computational models and methodology for accurate multi-component-flow simulation in under-resolved multi-scale porous structures. It is generally impractical to fully resolve the flow in porous structures with large length-scale difference due to tremendously high computational expense. The flow contributions from under-resolved scales need to be accounted for with proper physics modeling as well as simulation processes. Using pre-computed physical properties such as the absolute permeability, K0, the capillary-pressure-saturation curve, and the relative permeability, Kr, in typically resolved porous structures, local fluid force is conjectured and applied to simulation in the under-resolved regions that are represented by porous media. By doing so, accurate simulation of flow in multi-scale porous structures becomes feasible.
  In order to check the accuracy and robustness of this method, a set of benchmark test cases are performed for both single-component and multi-component flows in artificially constructed multi-scale porous structures, and simulation results are compared with analytic solutions and/or results with much finer resolution resolving the porous structures. Quantitatively consistent results are obtained with proper input of K0, capillary pressure, and Kr in all tested cases.
  Specifically, imbibition patterns, entry pressure, residual component patterns, and the absolute and relative permeability are accurately captured with this approach.

</details>


### [103] [Lattice Boltzmann models for the hydrodynamic equations in multiphase flow with high density ratio](https://arxiv.org/abs/2512.01027)
*H. Otomo,C. Sun,T. Inamuro,W. Li,M. Dressler,H. Chen,Y. Li,R. Zhang*

Main category: physics.flu-dyn

TL;DR: A new lattice kinetic scheme (LKS) is proposed to solve multiphase flows with high density ratios, addressing numerical errors from spatial density gradients that cause shear stress inaccuracies, Galilean invariance violations, and pressure dependencies in traditional lattice Boltzmann methods.


<details>
  <summary>Details</summary>
Motivation: Traditional lattice Boltzmann methods for high density ratio multiphase flows (like water-air) suffer from significant higher-order numerical truncation errors due to spatial density gradients, leading to inaccurate shear stress, violations of Galilean invariance, and undesirable pressure dependencies in pseudo-incompressible solutions.

Method: Proposes a new scheme based on the lattice kinetic scheme (LKS) that directly solves velocity and pressure fields in discrete space similar to LB methods. The LKS models are simplified for computational efficiency and implement a filter collision operator when mapped to LB models, eliminating iterative steps typically required in LKS.

Result: Benchmark tests confirm the proposed scheme effectively addresses the issues, achieving high accuracy and robustness. Most significant improvement is in the accuracy of airflow fields induced by water motion, likely due to improved momentum transfer across the interface.

Conclusion: The new LKS-based scheme successfully overcomes limitations of traditional LB methods for high density ratio multiphase flows by carefully designing moments and equilibrium states, eliminating numerical errors while maintaining computational efficiency and robustness.

Abstract: Multiphase flows with high density ratios, such as water and air flows, have recently been simulated using the lattice Boltzmann (LB) method. This approach corresponds to solving the phase field equations, such as the Cahn-Hilliard and Allen-Cahn equations, and the hydrodynamic equations, typically the Navier-Stokes and pressure equations for pseudo-incompressible fluids. Due to the high density ratio, the higher-order numerical truncation errors associated with spatial density gradients can become significant. These errors can lead to problems such as inaccuracies in shear stress, violations of Galilean invariance, and undesirable dependencies on absolute pressure for the pseudo-incompressible solutions. To overcome such problems, the moments of the distribution function and the equilibrium state must be carefully designed while ensuring robustness. In this work, we propose a new scheme based on the lattice kinetic scheme (LKS), which directly solves the velocity and pressure fields in the similar discrete space as the LB method. When mapping the LKS-based models to the LB models, the original LKS models are simplified for computational efficiency and the filter collision operator is implemented. Benchmark test cases confirm that the proposed scheme effectively addresses these issues, achieving high accuracy and robustness while eliminating the iterative steps typically required in the LKS. One of the most significant improvements is the accuracy of the airflow field induced by water motion, likely due to improved momentum transfer across the interface.

</details>


### [104] [Capillary flow simulation with the phase-field-based lattice Boltzmann solver](https://arxiv.org/abs/2512.01032)
*R. Thirumalaisamy,S. Kim,H. Otomo,J. Jilesen,R. Zhang*

Main category: physics.flu-dyn

TL;DR: A phase-field lattice Boltzmann model is developed for high-fidelity multiphase flow simulations, optimized with wettability and friction models, and validated against benchmark cases with good agreement to theory and experiments.


<details>
  <summary>Details</summary>
Motivation: Capillary flow patterns (slug, droplet, film flow) significantly affect velocity and pressure fields in engineering applications, but accurate simulation requires handling high density ratios and surface tension effects.

Method: Developed a phase-field-based lattice Boltzmann model with conservative Allen-Cahn equation, volumetric boundary conditions for complex geometries, and implemented optimized wettability and friction models.

Result: The solver produces results consistent with both theory and experiment across benchmark cases including droplet on curved surfaces, Lucas-Washburn law, critical pressure in 3D channels, and air-driven multiphase flow experiments.

Conclusion: The developed LB model successfully simulates multiphase flows with high accuracy, even for pressure fields often overlooked in previous studies, demonstrating its capability for engineering applications.

Abstract: The phase-field-based lattice Boltzmann (LB) model has been developed to perform high fidelity multiphase flow simulations. Its ability to accurately handle high density ratio and surface tension effects is expected to be beneficial for capillary flow simulation, leading to accurate reproduction of flow patterns such as slug flow, droplet flow, and film flow. This is critical in many engineering cases because the flow patterns significantly affect the velocity and pressure fields. In this study, on top of the LB models based on the conservative Allen-Cahn equation and the volumetric boundary conditions for the complex geometries, an optimized wettability and friction model are implemented. With these models, we conducted a set of benchmark test cases, including static and dynamic multiphase flow scenarios such as the droplet on the curved surfaces, water-filling channel for the Lucas-Washburn law, and the critical pressure in the three-dimensional channel, an air-driven multiphase flow in the experiments. In all of these cases, the solver produces results that are consistent with both theory and experiment, even with respect to the pressure field accuracy, which has often been overlooked in many previous studies.

</details>


### [105] [Self-similar multishock implosions for ultrahigh compression of matter](https://arxiv.org/abs/2512.00827)
*M. Murakami*

Main category: physics.flu-dyn

TL;DR: Self-similar solutions for ultrahigh compression using converging stacked shock waves, extending Guderley model with scaling law for final density, validated by simulations up to strongly nonlinear regime, offering instability-free compression for inertial confinement fusion.


<details>
  <summary>Details</summary>
Motivation: To develop a theoretical framework for ultrahigh compression of uniform-density targets using spherically converging stacked shock waves that avoids Rayleigh-Taylor instability, which typically compromises shell-based implosions in inertial confinement fusion.

Method: Extends classical Guderley model to derive scaling law for final density based on number of shocks and pressure ratio. Uses self-similar solutions and validates with one-dimensional hydrodynamic simulations across broad parameter range including strongly nonlinear regime.

Result: Derived scaling law ρ_r/ρ_0 ∝ P̂^{β(N-1)} where N is number of shocks, P̂ is stage pressure ratio, and β depends on adiabatic index γ. Simulations confirm validity across broad parameter range up to P̂ ∼ 70, well beyond perturbative limit. The volumetric geometry inherently avoids Rayleigh-Taylor instability.

Conclusion: The model provides robust theoretical benchmark for instability-free compression in inertial confinement fusion, with practical relevance demonstrated through accurate scaling even in strongly nonlinear regimes, offering advantages over traditional shell-based implosions.

Abstract: We present a class of self-similar solutions describing ultrahigh compression of a uniform-density target by spherically converging, stacked shock waves. Extending the classical Guderley model, we derive a scaling law for the final density of the form $ρ_{r}/ρ_{0} \propto \hat{P}^{β(N-1)}$, where $N$ is the number of shocks, $\hat{P}$ the stage pressure ratio, and $β$ a numerical exponent determined by the adiabatic index $γ$. One-dimensional hydrodynamic simulations confirm the validity of this scaling across a broad parameter range. Notably, the relation remains accurate even in the strongly nonlinear regime up to $\hat{P} \sim 70$, well beyond the perturbative limit, highlighting the robustness and practical relevance of the model. Owing to its volumetric geometry, this compression scheme inherently avoids the Rayleigh--Taylor instability, which typically compromises shell-based implosions, and thereby establishes a theoretical benchmark for instability-free compression in inertial confinement fusion.

</details>


### [106] [Neural Networks for Predicting Permeability Tensors of 2D Porous Media: Comparison of Convolution- and Transformer-based Architectures](https://arxiv.org/abs/2512.01517)
*Sigurd Vargdal,Paula Reis,Henrik Andersen Sveinsson,Gaute Linga*

Main category: physics.flu-dyn

TL;DR: Deep learning models (ResNet, Vision Transformers, ConvNeXt) accurately predict permeability tensors from 2D binary images of porous media, achieving R² up to 0.99460.


<details>
  <summary>Details</summary>
Motivation: Traditional permeability determination methods (flow simulations/experiments) are time-consuming and resource-intensive, while analytical methods like Kozeny-Carman are too simplistic for accurate pore-scale predictions. Deep learning offers a more efficient alternative.

Method: Generated 24,000 synthetic random periodic porous media samples with specified porosity and length scale. Computed permeability tensors using Lattice-Boltzmann simulations. Evaluated three deep learning architectures: ResNet (50 & 101), Vision Transformers (ViT-T16 & ViT-S16), and ConvNeXt (Tiny & Small). Used techniques like weight decay, learning rate scheduling, and data augmentation to improve generalization.

Result: ConvNeXt-Small achieved highest R² score of 0.99460 on 4,000 unseen test samples. Data augmentation and larger dataset size improved prediction accuracy. ConvNeXt and ResNet converged faster than ViT but degraded if overtrained. Permeability values spanned three orders of magnitude.

Conclusion: Image-based neural networks show strong potential for accurate permeability tensor prediction from 2D binary images of porous media, offering an efficient alternative to traditional methods.

Abstract: Permeability is a central concept in the macroscopic description of flow through porous media, with applications spanning from oil recovery to hydrology. Traditional methods for determining the permeability tensor involving flow simulations or experiments can be time consuming and resource-intensive, while analytical methods, e.g., based on the Kozeny-Carman equation, may be too simplistic for accurate prediction based on pore-scale features. In this work, we explore deep learning as a more efficient alternative for predicting the permeability tensor based on two-dimensional binary images of porous media, segmented into solid ($1$) and void ($0$) regions. We generate a dataset of 24,000 synthetic random periodic porous media samples with specified porosity and characteristic length scale. Using Lattice-Boltzmann simulations, we compute the permeability tensor for flow through these samples with values spanning three orders of magnitude. We evaluate three families of image-based deep learning models: ResNet (ResNet-$50$ and ResNet-$101$), Vision Transformers (ViT-T$16$ and ViT-S$16$) and ConvNeXt (Tiny and Small). To improve model generalisation, we employ techniques such as weight decay, learning rate scheduling, and data augmentation. The effect of data augmentation and dataset size on model performance is studied, and we find that they generally increase the accuracy of permeability predictions. We also show that ConvNeXt and ResNet converge faster than ViT and degrade in performance if trained for too long. ConvNeXt-Small achieved the highest $R^2$ score of $0.99460$ on $4,000$ unseen test samples. These findings underscore the potential to use image-based neural networks to predict permeability tensors accurately.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [107] [Completeness of reparametrization-invariant Sobolev metrics on the space of surfaces](https://arxiv.org/abs/2512.01566)
*Martin Bauer,Cy Maor,Benedikt Wirth*

Main category: math.DG

TL;DR: First extension of completeness results from curves to surfaces for reparametrization-invariant Sobolev metrics, establishing metric/geodesic completeness and existence of minimizing geodesics.


<details>
  <summary>Details</summary>
Motivation: Extend completeness results from immersed curves (Bruveris, Michor, Mumford) to immersed surfaces, validating Mumford's conjecture about completeness properties of general spaces of immersions.

Method: 1) Recast earlier approaches to completeness on manifolds of mappings as a general completeness criterion for infinite-dimensional Riemannian manifolds that are open subsets of complete Riemannian manifolds. 2) Combine with geometric estimates based on Michael-Simon-Sobolev inequality to establish completeness for specific Sobolev metrics on immersed surfaces.

Result: Established conditions ensuring metric and geodesic completeness as well as existence of minimizing geodesics for reparametrization-invariant Sobolev-type Riemannian metrics on space of immersed surfaces.

Conclusion: Successfully extended completeness theory from curves to surfaces, providing first such results for immersed surfaces and validating Mumford's conjecture about completeness properties of general immersion spaces.

Abstract: We study reparametrization-invariant Sobolev-type Riemannian metrics on the space of immersed surfaces and establish conditions ensuring metric and geodesic completeness as well as the existence of minimizing geodesics. This provides the first extension of completeness results for immersed curves, originating from works of Bruveris, Michor, and Mumford, and validates an earlier conjecture of Mumford on completeness properties of general spaces of immersions in this important case.
  The result is obtained by recasting earlier approaches to completeness on manifolds of mappings as a general completeness criterion for infinite-dimensional Riemannian manifolds that are open subsets of a complete Riemannian manifold and by combining it with geometric estimates based on the Michael--Simon--Sobolev inequality to establish the completeness for specific Sobolev metrics on immersed surfaces.

</details>
