<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 16]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [math.NT](#math.NT) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [nucl-th](#nucl-th) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A Discrete Neural Operator with Adaptive Sampling for Surrogate Modeling of Parametric Transient Darcy Flows in Porous Media](https://arxiv.org/abs/2512.03113)
*Zhenglong Chen,Zhao Zhang,Xia Yan,Jiayu Zhai,Piyang Liu,Kai Zhang*

Main category: math.NA

TL;DR: A new discrete neural operator for surrogate modeling of transient Darcy flow in porous media with random parameters, using temporal encoding, operator learning, and UNet to map random parameters to spatiotemporal flow fields.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate and efficient surrogate model for predicting transient Darcy flow fields in heterogeneous porous media with random parameters, which is computationally expensive with traditional numerical methods.

Method: Integrates temporal encoding, operator learning, and UNet architecture; uses transmissibility matrices (derived from finite volume method) instead of permeability as inputs; develops generative latent space adaptive sampling with Gaussian mixture model for density estimation of generalization error.

Result: The proposed discrete neural operator achieves higher prediction accuracy than state-of-the-art attention-residual-UNet structure; validation on 2D/3D single- and two-phase Darcy flow cases shows consistent enhancement in prediction accuracy with limited training data.

Conclusion: The new method provides an effective surrogate modeling approach for transient Darcy flow prediction, offering improved accuracy and sampling efficiency for applications in porous media flow simulation.

Abstract: This study proposes a new discrete neural operator for surrogate modeling of transient Darcy flow fields in heterogeneous porous media with random parameters. The new method integrates temporal encoding, operator learning and UNet to approximate the mapping between vector spaces of random parameter and spatiotemporal flow fields. The new discrete neural operator can achieve higher prediction accuracy than the SOTA attention-residual-UNet structure. Derived from the finite volume method, the transmissibility matrices rather than permeability is adopted as the inputs of surrogates to enhance the prediction accuracy further. To increase sampling efficiency, a generative latent space adaptive sampling method is developed employing the Gaussian mixture model for density estimation of generalization error. Validation is conducted on test cases of 2D/3D single- and two-phase Darcy flow field prediction. Results reveal consistent enhancement in prediction accuracy given limited training set.

</details>


### [2] [Simpson variational integrator for nonlinear systems: a tutorial on the Lagrange top](https://arxiv.org/abs/2512.03330)
*Juan Antonio Rojas-Quintero,François Dubois,Frédéric Jourdan*

Main category: math.NA

TL;DR: A fourth-order symplectic integrator using Simpson quadrature for nonlinear mechanical systems derived from variational principles, compared with implicit midpoint method on double pendulum and Lagrange top examples.


<details>
  <summary>Details</summary>
Motivation: To develop a high-order symplectic integrator for finite-dimensional nonlinear mechanical systems that preserves geometric structure and provides better accuracy than existing methods like the implicit midpoint variational integrator.

Method: Discretizes the action using quadratic finite elements interpolation of the state combined with Simpson's quadrature, resulting in implicit, symplectic discrete motion equations that are fourth-order accurate.

Result: The proposed Simpson-based integrator demonstrates fourth-order accuracy and symplecticity, outperforming the implicit midpoint method on examples including nonlinear double pendulum and Lagrange top systems, with convergence analysis showing consistent order.

Conclusion: The Simpson quadrature-based variational integrator provides an effective fourth-order symplectic method for nonlinear mechanical systems, offering improved accuracy while preserving geometric structure, with applications to multibody systems and separable/non-separable Hamiltonian systems.

Abstract: This contribution presents an integration method based on the Simpson quadrature. The integrator is designed for finite-dimensional nonlinear mechanical systems that derive from variational principles. The action is discretized using quadratic finite elements interpolation of the state and Simpson's quadrature, leading to discrete motion equations. The scheme is implicit, symplectic, and fourth-order accurate. The proposed integrator is compared with the implicit midpoint variational integrator on two examples of systems with inseparable Hamiltonians. First, the example of the nonlinear double pendulum illustrates how the method can be applied to multibody systems. The analytical solution of the Lagrange top is then used as a reference to analyze accuracy, convergence, and precision of the numerical method. A reduced Lagrange top system is also proposed and solved with a classical fourth-order method. Its solution is compared with the Simpson solution of the complete system, and the convergence order of the difference between both is consistent with the order of the classical method.

</details>


### [3] [A fast stochastic interacting particle-field method for 3D parabolic parabolic Chemotaxis systems: numerical algorithms and error analysis](https://arxiv.org/abs/2512.03452)
*Jingyuan Hu,Zhongjian Wang,Jack Xin,Zhiwen Zhang*

Main category: math.NA

TL;DR: A novel SIPF-PIC method for efficient 3D Keller-Segel simulation with O(P + H³logH) complexity, improving from O(PH³), with rigorous error analysis and demonstration of complex blowup dynamics.


<details>
  <summary>Details</summary>
Motivation: Need for efficient numerical simulation of 3D parabolic-parabolic Keller-Segel systems, which model chemotaxis phenomena, with existing methods having high computational complexity (O(PH³)).

Method: Stochastic interacting particle-field method with particle-in-cell acceleration (SIPF-PIC) that integrates Lagrangian particle dynamics with spectral field solvers using localized particle-grid interpolations and FFT techniques.

Result: Achieves O(P + H³logH) complexity per time step (vs. O(PH³) in original SIPF), with error bound O(H^{-16/13} + P^{-1/2}H^{4/13}), and captures complex blowup dynamics including ring-type singularities.

Conclusion: SIPF-PIC provides significant computational efficiency improvement while maintaining accuracy for 3D Keller-Segel systems, enabling simulation of complex blowup phenomena beyond single-point collapse.

Abstract: In this paper, we develop a novel numerical framework, the stochastic interacting particle-field method with particle-in-cell acceleration (SIPF-PIC), for the efficient simulation of the three-dimensional (3D) parabolic-parabolic Keller-Segel (KS) systems. The SIPF-PIC method integrates Lagrangian particle dynamics with spectral field solvers, by leveraging localized particle-grid interpolations and fast Fourier transform (FFT) techniques. For $P$ particles and $H$ Fourier modes per spatial dimension, the SIPF-PIC method achieves a computational complexity of $\mathcal{O}(P + H^3 \log H)$ per time step, a significant improvement over the original SIPF method (proposed in \cite{SIPF1}), which has a complexity of $\mathcal{O}(PH^3)$, while preserving numerical accuracy. Moreover, we establish a rigorous error analysis, proving that the discretization errors are of order $\mathcal{O}(H^{-16/13}+P^{-1/2}H^{4/13})$. Finally, we present numerical experiments to validate the theoretical convergence rates and demonstrate the computational efficiency of our new method. Notably, these experiments also show that the method captures complex blowup dynamics beyond single-point collapse, including ring-type singularities, where mass dynamically concentrates into evolving annular structures.

</details>


### [4] [Numerical Analysis of the 2D Stochastic Navier-Stokes Equations: Convergence under Transport Noise and No-slip Boundary Conditions](https://arxiv.org/abs/2512.03483)
*Binjie Li,Qin Zhou*

Main category: math.NA

TL;DR: Numerical analysis of 2D stochastic Navier-Stokes equations with transport noise on convex polygonal domains, addressing low regularity and non-Lipschitz nonlinearity through spatial semidiscretization and full discretization convergence rates.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of numerically approximating 2D stochastic Navier-Stokes equations with transport noise on convex polygonal domains with no-slip boundary conditions. The main difficulties arise from the solution's low spatial regularity and the non-Lipschitz nature of the nonlinear term, which complicate numerical analysis.

Method: The authors develop a numerical analysis framework for both spatial semidiscretization and full discretization. For spatial discretization, they derive convergence rates in the mean-square sense. For the full discretization (both space and time), they prove convergence in probability and establish explicit convergence rates with respect to the time step.

Result: The paper establishes rigorous convergence results: (1) For spatial semidiscretization, they obtain a convergence rate in the mean-square sense. (2) For full discretization, they prove convergence in probability and provide explicit convergence rates with respect to the time step, addressing the challenges of low regularity and non-Lipschitz nonlinearity.

Conclusion: The work provides a solid theoretical foundation for numerical approximation of 2D stochastic Navier-Stokes equations with transport noise on convex polygonal domains, overcoming the analytical challenges posed by low spatial regularity and non-Lipschitz nonlinearity through carefully derived convergence rates for both spatial and full discretizations.

Abstract: This work is concerned with the numerical approximation of the two-dimensional stochastic Navier-Stokes equation with transport noise and no-slip boundary conditions on a convex polygonal domain. The analysis is challenged by the solution's low spatial regularity and the non-Lipschitz nonlinearity. We derive a convergence rate in the mean-square sense for a spatial semidiscretization. Furthermore, for the full discretization, we prove convergence in probability and establish an explicit rate with respect to the time step.

</details>


### [5] [Constraint-Preserving High-Order Compact OEDG Method for Spherically Symmetric Einstein-Euler System](https://arxiv.org/abs/2512.03496)
*Yuchen Huang,Manting Peng,Kailiang Wu*

Main category: math.NA

TL;DR: A high-order Constraint-Preserving compact Oscillation-Eliminating Discontinuous Galerkin method for spherically symmetric Einstein-Euler systems that maintains physical realizability and geometric validity without complex checks or limiters.


<details>
  <summary>Details</summary>
Motivation: Numerical simulation of spherically symmetric Einstein-Euler systems faces severe challenges due to stringent physical admissibility constraints of relativistic fluids and geometric singularities in metric evolution. Existing methods struggle with maintaining physical realizability (positive density, subluminal velocity) and geometric validity during evolution.

Method: Integrates a scale-invariant oscillation-eliminating mechanism into a compact Runge-Kutta DG framework. Characterizes convex invariant region of hydrodynamic subsystem with general barotropic EOS. Uses bijective transformation of metric potentials - evolves unconstrained auxiliary variables whose inverse mapping automatically enforces strict positivity and asymptotic bounds without limiters. Includes compatible high-order boundary treatment.

Result: The CPcOEDG method preserves physical realizability (positive density and subluminal velocity) directly in conservative variables, eliminating need for complex primitive-variable checks. Maintains geometric validity of spacetime through automatic enforcement of positivity and bounds. Demonstrates robust stability and design-order accuracy in capturing strong gravity-fluid interactions in simulations of black hole accretion and relativistic shock waves.

Conclusion: The proposed CPcOEDG method successfully addresses key challenges in numerical simulation of spherically symmetric Einstein-Euler systems by preserving physical constraints and geometric validity without complex checks or limiters, enabling accurate simulation of strong gravity-fluid interactions.

Abstract: Numerical simulation of the spherically symmetric Einstein--Euler (EE) system faces severe challenges due to the stringent physical admissibility constraints of relativistic fluids and the geometric singularities inherent in metric evolution. This paper proposes a high-order Constraint-Preserving (CP) compact Oscillation-Eliminating Discontinuous Galerkin (cOEDG) method specifically tailored to address these difficulties. The method integrates a scale-invariant oscillation-eliminating mechanism [M. Peng, Z. Sun, K. Wu, Math. Comp., 94: 1147--1198, 2025] into a compact Runge--Kutta DG framework. By characterizing the convex invariant region of the hydrodynamic subsystem with general barotropic equations of state, we prove that the proposed scheme preserves physical realizability (specifically, positive density and subluminal velocity) directly in terms of conservative variables, thereby eliminating the need for complex primitive-variable checks. To ensure the geometric validity of the spacetime, we introduce a bijective transformation of the metric potentials. Rather than evolving the constrained metric components directly, the scheme advances unconstrained auxiliary variables whose inverse mapping automatically enforces strict positivity and asymptotic bounds without any limiters. Combined with a compatible high-order boundary treatment, the resulting CPcOEDG method exhibits robust stability and design-order accuracy in capturing strong gravity-fluid interactions, as demonstrated by simulations of black hole accretion and relativistic shock waves.

</details>


### [6] [A Coupled IMEX Domain Decomposition Method for High-Order Time Integration of the ES-BGK Model of the Boltzmann Equation](https://arxiv.org/abs/2512.03586)
*Domenico Caparello,Tommaso Tenna*

Main category: math.NA

TL;DR: High-order domain decomposition method for ES-BGK model that dynamically switches between Euler equations in equilibrium regions and ES-BGK in non-equilibrium regions using coupled IMEX method.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient computational method for the ES-BGK model of the Boltzmann equation that can handle both equilibrium and non-equilibrium regions efficiently by automatically switching between different solvers based on local flow conditions.

Method: A high-order domain decomposition method with automatic detection of equilibrium/non-equilibrium regions, coupled IMEX method across decomposed subdomains and solvers, and switching between Euler equations (equilibrium) and ES-BGK model (non-equilibrium).

Result: The method preserves overall temporal order of accuracy, achieves high accuracy and computational efficiency, and demonstrates robustness and expected temporal high-order convergence in 2D numerical simulations.

Conclusion: The proposed coupled domain decomposition approach successfully addresses the challenge of preserving temporal accuracy while efficiently handling mixed equilibrium/non-equilibrium flows through automatic solver switching and coupled IMEX methods.

Abstract: In this paper, we propose a high-order domain decomposition method for the ES-BGK model of the Boltzmann equation, which dynamically detects regions of equilibrium and non-equilibrium. Our implementation automatically switches between Euler equations in regions where the fluid is at equilibrium, and the ES-BGK model elsewhere. The main challenge addressed in this work is the development of a coupled strategy between the macroscopic and the kinetic solvers, which preserves the overall temporal order of accuracy of the scheme. A coupled IMEX method is introduced across decomposed subdomains and solvers. This approach is based on a coupled IMEX method and allows high accuracy and computational efficiency. Several numerical simulations in two space dimensions are performed, in order to validate the robustness of our approach and the expected temporal high-order convergence.

</details>


### [7] [Three-dimensional modelling of drag anchor penetration using the material point method](https://arxiv.org/abs/2512.03632)
*Robert E. Bird,William M. Coombs,Michael J. Brown,Charles E. Augarde,Yaseen U. Sharif,Giuliano Pretti,Catriona Macdonald,Duncan Stevens,Gareth Carter*

Main category: math.NA

TL;DR: Developed an efficient MPM-based tool for predicting anchor penetration in seabeds using CPT data, validated against centrifuge tests, and identified issues with current UK cable burial risk assessment methods.


<details>
  <summary>Details</summary>
Motivation: Drag embedment anchors pose significant threats to buried subsea cables and pipelines. Current cable burial depth selection is a compromise between protection from anchor strikes and installation costs, requiring better predictive tools for anchor penetration assessment.

Method: Developed a large deformation elasto-plastic Material Point Method (MPM)-based soil-structure interaction tool with three key improvements: modeling rigid body assemblies for articulated anchors, partitioned domain approach for long anchor pulls, and improved rotational inertia modeling. Validated against scaled physical tests in geotechnical centrifuge on sands with varying relative densities.

Result: Good agreement between numerical predictions and centrifuge tests across tested conditions. Identified key issues with UK Cable Burial Risk Assessment (CBRA) approach, revealing potential non-conservatism for sandy seabeds. Tool enables site-specific anchor penetration assessment along cable routes.

Conclusion: The developed MPM-based tool provides an efficient predictive method for anchor penetration assessment using CPT data, offering improvements over current CBRA framework and enabling better evaluation of anchor designs and burial depth optimization for subsea infrastructure protection.

Abstract: Drag embedment anchors are a key threat to buried subsea linear infrastructure, such as power/data cables and pipelines. For cables, selecting a burial depth is a compromise between protecting the cable from anchor strike and the increased cost of deeper installation. This presents an efficient large deformation, elasto-plastic Material Point Method-based soil-structure interaction predictive tool for the estimation of anchor penetration based on Cone Penetration Test (CPT) site investigation data. The tool builds on earlier work by the authors supplemented by three developments: modelling assemblies of rigid bodies (necessary for articulated anchors), a partitioned domain approach to enable accurate and efficient modelling of long anchor pulls and an improved means of modelling rotational inertia. The tool is validated against scaled physical tests conducted in a geotechnical centrifuge on sands with a range of relative densities with good agreement across the tested conditions. Numerical simulations identify key issues with the UK Cable Burial Risk Assessment (CBRA) approach for estimating anchor penetration and reveal the potentially non-conservatism of the CBRA framework for sandy seabeds. The numerical model enables site-specific anchor-penetration assessment along cable routes and can be used to evaluate the performance of different anchor designs and sizes in varied soil conditions.

</details>


### [8] [Convergence analysis of a Crank-Nicolson scheme for strongly magnetized plasmas](https://arxiv.org/abs/2512.03650)
*Francis Filbet,L Miguel Rodrigues,Kim Han Trinh*

Main category: math.NA

TL;DR: Convergence analysis of an asymptotic preserving particle scheme for PIC methods in Vlasov equations with strong magnetic fields, removing restrictive stability constraints while capturing large-scale dynamics.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze particle schemes that can handle Vlasov equations with strong inhomogeneous magnetic fields without restrictive stability constraints on discretization steps, enabling efficient simulation of large-scale dynamics even with coarse discretization.

Method: Asymptotic preserving particle scheme designed as a particle pusher in Particle-In-Cell (PIC) method, using implicit-explicit schemes on augmented formulations to remove stability constraints while capturing large-scale dynamics.

Result: Error bounds are explicit regarding discretization and stiffness parameters and match numerical tests sharply. The scheme successfully removes classical strong restrictive stability constraints while maintaining accuracy for large-scale dynamics.

Conclusion: The analysis provides rigorous convergence results for asymptotic preserving particle schemes in magnetic field contexts, with the approach expected to be representative of a broader class of implicit-explicit schemes on augmented formulations developed by the authors.

Abstract: The present paper is devoted to the convergence analysis of an asymptotic preserving particle scheme designed to serve as a particle pusher in a Particle-In-Cell (PIC) method for the Vlasov equation with a strong inhomogeneous magnetic field. The asymptotic preserving scheme that we study removes classical strong restrictive stability constraints on discretization steps while capturing the large-scale dynamics, even when the discretization is too coarse to capture fastest scales. Our error bounds are explicit regarding the discretization and stiffness parameters and match sharply numerical tests. The present analysis is expected to be representative of the general analysis of a class of schemes, developed by the authors, conceived as implicit-explicit schemes on augmented formulations.

</details>


### [9] [From Memory Model to CPU Time: Exponential Integrators for Advection-Dominated Problems](https://arxiv.org/abs/2512.03679)
*Thi Tam Dang,Trung Hau Hoang*

Main category: math.NA

TL;DR: Exponential integrators for advection-dominated problems: Krylov subspace and Leja interpolation methods compared to explicit Runge-Kutta schemes, showing performance depends on problem settings and time step sizes.


<details>
  <summary>Details</summary>
Motivation: To investigate the computational efficiency of exponential integrators for advection-dominated problems, extending previous work with higher-order Krylov approximations and new numerical regimes.

Method: Uses Krylov subspace methods and Leja interpolation to compute matrix exponential actions, compares with explicit Runge-Kutta schemes, and evaluates CPU-time efficiency across different problem settings.

Result: Exponential integrators can outperform or match explicit Runge-Kutta schemes depending on problem settings. Leja-based methods excel with large time steps, while Krylov-based methods perform better with small time steps.

Conclusion: The choice between exponential integrators and explicit Runge-Kutta schemes should consider problem characteristics and time step sizes, with different exponential methods (Krylov vs. Leja) optimal for different regimes.

Abstract: In this paper, we investigate the application of exponential integrators to advection-dominated problems. We focus on Krylov subspace and Leja interpolation methods to compute the action of exponential and related matrix functions. Complementing our earlier paper, arXiv:2410.12765 (to appear in Advances in Applied Mathematics and Mechanics, 2025) based on a performance model, we extend the numerical investigation to higher-order Krylov approximations and new numerical regime, and assess their CPU-time efficiency relative to explicit Runge--Kutta schemes. We show that, depending on the problem setting, exponential integrators can either outperform or match explicit Runge--Kutta schemes. We also observe that Leja-based methods outperform Krylov iterations for large time steps, whereas for small time steps, Krylov-based methods provide better results than Leja-based methods.

</details>


### [10] [A Superfast Direct Solver for Type-III Inverse Nonuniform Discrete Fourier Transform](https://arxiv.org/abs/2512.03733)
*Yingzhou Li,Jingyu Liu*

Main category: math.NA

TL;DR: A superfast direct inversion method for type-III NUDFT using hierarchical semi-separable (HSS) matrix decompositions with quasi-linear complexity.


<details>
  <summary>Details</summary>
Motivation: The nonuniform discrete Fourier transform (NUDFT) and its inverse are widely used in scientific computing, but efficient inversion methods are needed for practical applications.

Method: Approximates type-III NUDFT matrix as product of type-II NUDFT matrix and HSS matrix, with type-II NUDFT further decomposed into HSS matrix and uniform DFT matrix, enabling quasi-linear complexity for both forward application and backward inversion.

Result: Provides error bound for approximation under specific sample distributions, and numerical results verify theoretical properties and demonstrate efficiency.

Conclusion: The method serves as high-accuracy direct solver or efficient preconditioner for type-III NUDFT inversion with quasi-linear complexity.

Abstract: The nonuniform discrete Fourier transform (NUDFT) and its inverse are widely used in various fields of scientific computing. In this article, we propose a novel superfast direct inversion method for type-III NUDFT. The proposed method approximates the type-III NUDFT matrix as a product of a type-II NUDFT matrix and an HSS matrix, where the type-II NUDFT matrix is further decomposed into the product of an HSS matrix and an uniform discrete Fourier transform (DFT) matrix as in [Wilber, Epperly, and Barnett, SIAM Journal on Scientific Computing, 47(3):A1702-A1732, 2025]. This decomposition enables both the forward application and the backward inversion to be accomplished with quasi-linear complexity. The fast inversion can serve as a high-accuracy direct solver or as an efficient preconditioner. Additionally, we provide an error bound for the approximation under specific sample distributions. Numerical results are presented to verify the relevant theoretical properties and demonstrate the efficiency of the proposed methods.

</details>


### [11] [An arbitrary Lagrangian-Eulerian semi-implicit hybrid method for continuum mechanics with GLM cleaning](https://arxiv.org/abs/2512.03741)
*Saray Busto*

Main category: math.NA

TL;DR: A semi-implicit ALE method for unified continuum mechanics using GPR model with GLM approach, operator splitting, staggered grids, and implicit-explicit discretization for all Mach number flows.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method that can handle both fluid and solid mechanics problems within a unified continuum mechanics framework, addressing curl-free involutions in the solid limit and enabling all Mach number flows.

Method: Semi-implicit ALE method with operator splitting (transport, pressure, ODE subsystems), GLM approach for involutions, staggered unstructured grids (pressure on primal mesh, other variables on dual grid), explicit FVM for transport, implicit FEM for pressure, implicit DIRK for stiff ODEs, and mesh motion driven by fluid velocity and boundary displacement.

Result: The method successfully handles both fluid and solid mechanics problems, verified through comprehensive benchmarks demonstrating accuracy and robustness across all Mach number flows.

Conclusion: The proposed semi-implicit ALE method with operator splitting and staggered grid discretization provides an effective unified approach for continuum mechanics problems spanning fluid and solid regimes.

Abstract: This paper proposes a semi-implicit arbitrary Lagrangian-Eulerian (ALE) method for the solution of the unified Godunov-Peshkov-Romenski (GPR) model of continuum mechanics. To handle the curl free involutions arising in the solid limit of the model, the original system is augmented by adopting a thermodynamically compatible generalized Lagrangian multiplier (GLM) approach. Next, an operator splitting strategy decouples the computation of fast pressure waves from the bulk velocity of the medium yielding a transport subsystem, containing convective terms and non-conservative products, and a Poisson-type subsystem, for the pressure. A second splitting yields an ODE subsystem comprising only the potentially stiff source terms, responsible for the relaxation of the model between its fluid and solid limits.
  The mesh motion can be driven by two sources: the local fluid velocity and a prescribed boundary displacement. For the spatial discretization, we employ unstructured staggered grids, with the pressure defined on the primal mesh and all remaining variables on the dual grid. The transport subsystem is advanced via an explicit finite volume method, in which integration over closed space-time control volumes ensures verification of the geometric conservation law (GCL). On the other hand, implicit continuous finite elements are used for the discretization of the pressure subsystem and an implicit DIRK scheme is employed to solve the ODE subsystem. Consequently, the proposed approach is well suited to address all Mach number flows. A comprehensive set of benchmarks is employed to assess the accuracy and robustness of the proposed methodology in tackling both fluid and solid mechanics problems.

</details>


### [12] [Symplectic methods for stochastic Hamiltonian systems: asymptotic error distributions and Hamiltonian-specific analysis](https://arxiv.org/abs/2512.03840)
*Chuchu Chen,Xinyu Chen,Jialin Hong,Yuqian Miao*

Main category: math.NA

TL;DR: Symplectic methods for stochastic Hamiltonian systems have asymptotic error distributions that retain Hamiltonian structure, with new analysis showing their superiority for long-time Hamiltonian preservation.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic error behavior of symplectic methods for stochastic Hamiltonian systems and demonstrate their superiority over non-symplectic methods for long-time simulations, particularly in preserving Hamiltonian structure.

Method: Derived asymptotic error distributions for symplectic methods with multiplicative and additive noise; proposed new approach connecting stochastic modified equations to error distributions; characterized limiting distribution of normalized Hamiltonian deviation.

Result: Limiting error processes satisfy Hamiltonian formulations; new connection between stochastic modified equations and error distributions established; symplectic methods shown superior for long-time Hamiltonian preservation even as step size approaches zero.

Conclusion: Symplectic methods maintain Hamiltonian structure in their error distributions and are superior for long-time simulations of stochastic Hamiltonian systems, providing theoretical justification for their use.

Abstract: In this paper, we investigate the asymptotic error distributions of symplectic methods for stochastic Hamiltonian systems and further provide Hamiltonian-specific analysis that clarifies the superiority of symplectic methods. Our contribution is threefold. First, we derive the asymptotic error distributions of symplectic methods for stochastic Hamiltonian systems with multiplicative noise and additive noise, respectively, and show that the obtained limiting stochastic processes satisfy equations retaining the Hamiltonian formulations. Second, we propose a new approach for calculating the asymptotic error distribution, revealing the connection between the stochastic modified equation and the asymptotic error distribution. Third, we characterize the limiting distribution of the normalized Hamiltonian deviation, thereby illustrating through test equations the superiority of symplectic methods for long-time simulations of the Hamiltonians, even in the limit as the step size tends to zero.

</details>


### [13] [Mixed finite element approximation for non-divergence form elliptic equations with random input data](https://arxiv.org/abs/2512.04003)
*Amireh Mousavi*

Main category: math.NA

TL;DR: Mixed finite element method with collocation for elliptic PDEs with random coefficients and forcing, using mesh-dependent cost functional and error analysis.


<details>
  <summary>Details</summary>
Motivation: To solve elliptic partial differential equations in non-divergence form with random diffusion matrices and random forcing terms, which arise in uncertainty quantification for physical systems with uncertain parameters.

Method: Mixed-type continuous finite element discretization in physical domain combined with collocation discretization in stochastic domain. Uses stochastic cost functional at continuous level, enhanced with vanishing tangential trace constraint in mesh-dependent cost functional. Collocation at zeros of tensor product orthogonal polynomials to create uncoupled deterministic problems.

Result: Established a priori error bound for fully discrete approximation with convergence rates. Numerical results validate theoretical findings. System of uncoupled deterministic problems simplifies computation.

Conclusion: The proposed mixed finite element method with stochastic collocation effectively handles elliptic PDEs with random coefficients, providing rigorous error analysis and computationally efficient uncoupled deterministic problems.

Abstract: We consider an elliptic partial differential equation in non-divergence form with a random diffusion matrix and random forcing term. To address this, we propose a mixed-type continuous finite element discretization in the physical domain, combined with a collocation discretization in the stochastic domain. For the mixed formulation, we first introduce a stochastic cost functional at the continuous level. This formulation is then enhanced to incorporate the vanishing tangential trace constraint directly into a mesh-dependent cost functional, rather than enforcing it in the solution's function space. In this context, we define a mesh-dependent norm and provide an error analysis based on this norm. We employ the collocation method by collocating the stochastic equation at the zeros of suitable tensor product orthogonal polynomials. This approach leads to a system of uncoupled deterministic problems, simplifying computation. Furthermore, we establish an a poriori error bound for the fully discrete approximation, detailing the convergence rates with respect to the discretization parameters. Finally, numerical results are presented to confirm and validate the theoretical findings.

</details>


### [14] [Greedy techniques for inverse problems](https://arxiv.org/abs/2512.04046)
*L. Bruni Bruno,P. Massa,E. Perracchione,M. Trombini*

Main category: math.NA

TL;DR: Novel greedy framework for optimal selection of indirect measurements in inverse imaging problems using kernel-based interpolation/extrapolation with residual-based or error-based point selection.


<details>
  <summary>Details</summary>
Motivation: Inverse imaging problems rely on limited indirect measurements, making reconstruction quality highly dependent on both regularization and sample locations. There's a need for optimal measurement selection strategies to improve reconstruction with minimal measurements.

Method: Two-step greedy framework combining kernel-based interpolation and extrapolation. Two schemes: 1) residual-based (points selected by current approximation error for target function), 2) error-based (points selected using a priori error indicators independent of residual). Derived explicit error bounds for error-based scheme quantifying approximation error propagation.

Result: Numerical applications to solar hard X-ray imaging demonstrate that the proposed greedy sampling strategy achieves high-quality reconstructions using only a few available measurements.

Conclusion: The greedy framework provides an effective approach for optimal measurement selection in inverse problems, enabling high-quality reconstructions with minimal measurements through systematic point selection strategies.

Abstract: Inverse imaging problems rely on limited and indirect measurements, making reconstruction highly dependent on both regularization and sample locations. We introduce a novel greedy framework for the optimal selection of indirect measurements in the operator codomain, specifically tailored to inverse problems. Our approach employs a two-step scheme combining kernel-based interpolation and extrapolation. Within this framework, greedy schemes can be residual-based, where points are selected according to the current approximation error for a specific target function, or error-based, where points are chosen using a priori error indicators independent of the residual. For the latter, we derive explicit error bounds that quantify the propagation of approximation errors through both interpolation and extrapolation. Numerical applications to solar hard X-ray imaging demonstrate that the proposed greedy sampling strategy achieves high-quality reconstructions using only a few available measurements.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Nonlinear diffusion limit of non-local interactions on a sphere](https://arxiv.org/abs/2512.03185)
*Mark A. Peletier,Anna Shalova*

Main category: math.AP

TL;DR: The paper studies an aggregation PDE with competing attractive/repulsive forces on spheres, proving convergence to aggregation-diffusion equations with porous-medium diffusion in the strongly localized repulsion limit.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by a toy model of transformers introduced by Geshkovski et al. (2025), aiming to understand aggregation dynamics on spheres with competing forces relevant to transformer architectures.

Method: Combines variational techniques with harmonic analysis on spheres, characterizing the square root of convolution operators in terms of spherical harmonics to handle non-commutative convolution on spheres.

Result: Proves convergence of solutions to aggregation-diffusion equations with porous-medium-type diffusion in the limit of strongly localized repulsion with constant attraction.

Conclusion: The results provide mathematical foundations for understanding aggregation dynamics on spheres with competing forces, with potential applications to transformer models in machine learning.

Abstract: We study an aggregation PDE with competing attractive and repulsive forces on a sphere of arbitrary dimension. In particular, we consider the limit of strongly localized repulsion with a constant attraction term. We prove convergence of solutions of such a system to solutions of the aggregation-diffusion equation with a porous-medium-type diffusion term. The proof combines variational techniques with elements of harmonic analysis on a sphere. In particular, we characterize the square root of the convolution operator in terms of the spherical harmonics, which allows us to overcome difficulties arising due to the convolution on a sphere being non-commutative. The study is motivated by the toy model of transformers introduced by Geshkovski et al. (2025); and we discuss the applicability of the results to this model.

</details>


### [16] [A Lagrangian Approach to the Inhomogeneous Incompressible Euler Equation](https://arxiv.org/abs/2512.03246)
*Anping Pan*

Main category: math.AP

TL;DR: The paper studies Lagrangian aspects of the inhomogeneous incompressible Euler equation, establishing its geodesic description, deriving it from Hamilton-Pontryagin principle, and proving Lagrangian analyticity.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric and Lagrangian structure of the inhomogeneous incompressible Euler equation, which extends classical fluid dynamics to variable density flows.

Method: Geometric approach using geodesic description on diffeomorphism groups, Hamilton-Pontryagin action principle, and Lagrangian formulation analysis.

Result: Established geodesic interpretation of IIE, derived Lagrangian formulation from Hamilton-Pontryagin principle, and proved Lagrangian analyticity of the equation.

Conclusion: The inhomogeneous incompressible Euler equation has rich geometric structure describable as geodesic flow, with Lagrangian analyticity properties that provide deeper understanding of its mathematical structure.

Abstract: In this paper, we study the Lagrangian aspects of the inhomogeneous incompress- ible Euler equation (IIE in short). We establish a geodesic description of this equation and discuss the associated geometric structures. We also find the derivation of IIE from the Hamilton-Pontryagin action principle and derive the corresponding Lagrangian for- mulation. Appealing to this Lagrangian perspective, we prove Lagrangian analyticity of IIE

</details>


### [17] [Large-time behavior in a nonlocal heat equation with absorption. The absorption dominated case with fast decaying initial data](https://arxiv.org/abs/2512.03265)
*Carmen Cortázar,Fernando Quirós,Noemi Wolanski*

Main category: math.AP

TL;DR: Nonlocal dispersal equation with absorption term -u^p studied for large-time behavior, with specific initial data conditions and decay rate analysis.


<details>
  <summary>Details</summary>
Motivation: To understand the large-time asymptotic behavior of solutions to nonlocal dispersal equations with absorption terms, particularly how the decay rate and limit profiles compare between nonlocal and local diffusion problems.

Method: Study nonnegative solutions to nonlocal dispersal equation with absorption term -u^p (1<p<1+2/N), with bounded initial data satisfying |x|^{2/(p-1)}u_0(x)→A≥0 as |x|→∞. Analyze large-time behavior through decay rates and limit profiles.

Result: Decay rate matches purely absorbing problem; limit profile is a very singular solution to local diffusion problem with absorption if A=0, and a solution to local problem with initial datum A|x|^{-2/(p-1)} if A>0.

Conclusion: Nonlocal dispersal with absorption exhibits same decay as purely absorbing case, but converges to solutions of corresponding local diffusion problem, showing connection between nonlocal and local models in asymptotic regime.

Abstract: We study the large-time behavior of nonnegative solutions to a nonlocal dispersal equation in $\mathbb R^N$ with an absorption term modeled by $-u^p$, with $1<p<1+\frac2N$. The initial datum $u_0$ is assumed to be bounded, and to satisfy $|x|^{\frac2{p-1}}u_0(x)\to A\ge0$ as $|x|\to\infty$. Under these assumptions, we prove that the decay rate is that of the purely absorbing problem, while the limit profile is a very singular solution to a local diffusion problem with absorption if $A=0$, and a solution to this same local problem with initial datum $A|x|^{-\frac2{p-1}}$ if $A>0$.

</details>


### [18] [On Bridging Analyticity and Sparseness in Hyperdissipative Navier-Stokes Systems](https://arxiv.org/abs/2512.03378)
*Moses Patson Phiri*

Main category: math.AP

TL;DR: The paper proves global regularity for 3D hyper-dissipative Navier-Stokes equations below Lions threshold using analyticity-sparseness framework, ruling out blow-up through time-weighted inequalities and harmonic measure techniques.


<details>
  <summary>Details</summary>
Motivation: To establish global regularity for the 3D hyper-dissipative Navier-Stokes system in the near-critical regime below the Lions threshold, addressing the long-standing open problem of whether solutions can develop singularities in finite time.

Method: Uses quantified analyticity-sparseness gap, time-weighted bridge inequality across derivative levels, focused-extremizer hypothesis for peak concentration, and harmonic-measure contraction on one-dimensional sparse sets to enforce quantitative decay of high-derivative norms.

Result: Proves that solutions remain analytic past prospective singular times, rules out blow-up, and establishes global regularity while remaining consistent with recent non-uniqueness results.

Conclusion: The analyticity-sparseness framework successfully excludes blow-up scenarios for 3D hyper-dissipative Navier-Stokes below Lions threshold, complementing recent results on rapid-rate blow-up exclusions and providing refined regularity theory.

Abstract: We study the three-dimensional hyper-dissipative Navier-Stokes system in the near-critical regime below the Lions threshold. Leveraging a quantified analyticity-sparseness gap, we introduce a time-weighted bridge inequality across derivative levels and a focused-extremizer hypothesis capturing peak concentration at a fixed point. Together with a harmonic-measure contraction on one-dimensional sparse sets, these mechanisms enforce quantitative decay of high-derivative $L^{\infty}-$norms and rule out blow-up. Under scale-refined, slowly varying time weights, solutions extend analytically past the prospective singular time, thereby refining the analyticity-sparseness framework, complementing recent exclusions of rapid-rate blow-up scenarios, and remaining consistent with recent non-uniqueness results.

</details>


### [19] [The Dirichlet-to-Neumann map on asymptotically anti-de Sitter spaces and holography](https://arxiv.org/abs/2512.03587)
*Alberto Enciso,Gunther Uhlmann,Michał Wrochna*

Main category: math.AP

TL;DR: The paper shows that the forward Dirichlet-to-Neumann map for Klein-Gordon equations on asymptotically anti-de Sitter spacetimes is a fractional power of the boundary wave operator, and uses this to recover bulk metric information from boundary data.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between boundary scattering data and bulk geometry in asymptotically anti-de Sitter spacetimes, particularly for inverse problems in general relativity and holography.

Method: Analyzes the Klein-Gordon equation on asymptotically AdS spacetimes using paired Lagrangian distributions, studies the Dirichlet-to-Neumann map as a fractional power of boundary wave operator, and applies this to metric recovery problems.

Result: 1) Dirichlet-to-Neumann map equals fractional power of boundary wave operator modulo lower order terms. 2) For most mass parameters, this map determines Taylor series of bulk metric at boundary. 3) Proves Lorentzian version of Graham-Zworski theorem relating poles to conformally invariant powers.

Conclusion: Boundary scattering data (Dirichlet-to-Neumann map) contains sufficient information to recover bulk geometry in asymptotically AdS spacetimes, establishing important connections between boundary operators and bulk metric properties.

Abstract: We consider the Klein-Gordon equation on asymptotically anti-de Sitter spacetimes, and show that the forward Dirichlet-to-Neumann map (or scattering matrix) is a fractional power of the boundary wave operator modulo lower order terms in the sense of paired Lagrangian distributions. We use it to show that, outside of a countable set of mass parameters, the Dirichlet-to-Neumann map determines the Taylor series of the bulk metric at the boundary, and hence allows the recovery of a real analytic metric or Einstein metric modulo isometries. Furthermore, we prove a Lorentzian version of the Graham-Zworski theorem relating poles of the Dirichlet-to-Neumann map to conformally invariant powers of the boundary wave operator.

</details>


### [20] [Dynamics of the reversible Gray-Scott model and convergence to its irreversible limit](https://arxiv.org/abs/2512.03595)
*Philippe Laurençot,Christoph Walker*

Main category: math.AP

TL;DR: The paper proves well-posedness and convergence properties for a reversible Gray-Scott variant, showing trajectories converge to homogeneous steady states with exponential stability via linearized stability and center manifold theory.


<details>
  <summary>Details</summary>
Motivation: To analyze a reversible variant of the Gray-Scott model, establishing its mathematical well-posedness and understanding the long-term behavior of solutions, particularly their convergence to steady states.

Method: Uses principle of linearized stability for local exponential attractivity of stable steady states, center manifold theory to identify long-term limits, and parameter analysis to show convergence to classical Gray-Scott model.

Result: Proves well-posedness of the reversible Gray-Scott variant, shows each trajectory converges to one of two spatially homogeneous steady states, demonstrates exponential convergence rate via linearized stability, and establishes convergence to classical model for appropriate parameters.

Conclusion: The reversible Gray-Scott variant is mathematically well-posed with predictable long-term behavior converging to homogeneous steady states, and properly converges to the classical model under suitable parameter choices.

Abstract: Well-posedness of a reversible variant of the Gray-Scott model is shown, along with the convergence of each trajectory to one of the two spatially homogeneous steady states. The principle of linearized stability provides the local attractivity at an exponential rate of the stable steady state, while the long-term limit is identified with the help of center manifold theory. Finally, convergence to the classical Gray-Scott model is proved for an appropriate choice of parameters.

</details>


### [21] [Elastic scattering problems by penetrable obstacles with embedded objects](https://arxiv.org/abs/2512.03624)
*Chun Liu,Jiaqing Yang,Bo Zhang*

Main category: math.AP

TL;DR: This paper studies 3D elastic scattering by penetrable obstacles containing embedded objects, proves well-posedness of transmission problems using integral equations, and shows unique determination of obstacles from far-field patterns at fixed frequency.


<details>
  <summary>Details</summary>
Motivation: To analyze elastic scattering problems involving penetrable obstacles with embedded objects, establish mathematical foundations for transmission problems, and address inverse problems in obstacle reconstruction from far-field measurements.

Method: Uses integral equation method to prove well-posedness of transmission problems, then applies far-field pattern measurements at fixed frequency for inverse problem analysis.

Result: Proves well-posedness of transmission problems and demonstrates that inhomogeneous penetrable obstacles can be uniquely determined from far-field pattern measurements at a fixed frequency.

Conclusion: The paper establishes mathematical foundations for 3D elastic scattering by penetrable obstacles with embedded objects and provides uniqueness results for inverse obstacle reconstruction from far-field data.

Abstract: This paper considers 3-D elastic scattering problems by penetrable obstacles with embedded objects. The well-posedness of transmission
  problem is proved by employing integral equation method. Then the Inverse Problems , which is to recover the obstacle
  by the far-field pattern measurement, is considered. It is shown that the inhomogeneous penetrable obstacle can be
  uniquely determined from the far-field pattern at a fixed frequency.

</details>


### [22] [A Decay estimate for cubic defocusing non-linear Schrödinger equation in three dimensions](https://arxiv.org/abs/2512.03631)
*Yi Sun*

Main category: math.AP

TL;DR: Proves decay estimates for non-linear solutions of 3D cubic defocusing NLS


<details>
  <summary>Details</summary>
Motivation: To establish quantitative decay bounds for solutions to the 3D cubic defocusing nonlinear Schrödinger equation, which is important for understanding long-time behavior and scattering properties of nonlinear wave equations.

Method: Short note approach using analytical techniques to prove decay estimates for non-linear solutions of the 3D cubic defocusing NLS equation.

Result: Successfully proves decay estimates for non-linear solutions of the 3D cubic defocusing nonlinear Schrödinger equation.

Conclusion: The paper establishes important decay bounds for solutions to the 3D cubic defocusing NLS, contributing to the understanding of long-time behavior in nonlinear dispersive equations.

Abstract: In this short note, we prove a decay estimate for non-linear solutions of 3D cubic defocusing non-linear Schrödinger equation.

</details>


### [23] [On the doubling of variables technique in first order Hamilton-Jacobi equations](https://arxiv.org/abs/2512.03652)
*Charles Bertucci,Giacomo Ceccherini Silberstein*

Main category: math.AP

TL;DR: Revisiting doubling variables technique for Hamilton-Jacobi equations in optimal control, showing penalization tuning shifts regularity requirements to geometric properties of penalization, applied to finite dimensions and Wasserstein spaces.


<details>
  <summary>Details</summary>
Motivation: To improve the doubling variables technique for first-order Hamilton-Jacobi equations in optimal control by adjusting penalization between points, allowing flexibility in regularity assumptions.

Method: Tuning the penalization between two points in the doubling variables method, transforming regularity hypotheses into geometric properties of the penalization, demonstrated in finite dimensional settings and extended to Wasserstein spaces.

Result: Shows that adjusting penalization can drastically change the proof structure, enabling shifting of regularity requirements to geometric properties of penalization, applicable to both finite-dimensional and Wasserstein space equations.

Conclusion: The penalization tuning approach provides a flexible framework for analyzing Hamilton-Jacobi equations in optimal control, allowing adaptation of regularity conditions through geometric properties rather than strict analytical assumptions.

Abstract: In this paper, we revisit the technique of doubling variables in first order Hamilton-Jacobi equations, especially when the equations arise in optimal control. We show that by tuning the penalization between the two points, we can change drastically the proof, somehow shifting the regularity hypotheses into geometrical properties of the penalization. We present this idea in a finite dimensional setting and then exploit it on equations posed on Wasserstein spaces.

</details>


### [24] [The time fractional stochastic partial differential equations with non-local operator on $\mathbb{R}^{d}$](https://arxiv.org/abs/2512.03754)
*Yong Zhen Yang,Yong Zhou*

Main category: math.AP

TL;DR: This paper studies time-fractional stochastic PDEs with Caputo derivatives, driven by Gaussian and Lévy noises, establishing existence, uniqueness, and regularity results using both Lp and Sobolev space frameworks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend and unify previous work on fractional SPDEs by incorporating general non-local operators (generated by Bernstein functions) and Lévy noises, addressing gaps in existing theory particularly for p > 2 cases.

Method: The authors use two frameworks: 1) Lp-theory for 1 ≤ p ≤ 2 to establish local existence/uniqueness of mild solutions, and 2) Sobolev space framework based on Besov and Triebel-Lizorkin spaces associated with φ(Δ) for p > 2 cases. The analysis involves fundamental solution estimates, stochastic convolutions, Littlewood-Paley theory, and fixed point arguments.

Result: The paper proves local existence and uniqueness of mild solutions in Lp-framework (1 ≤ p ≤ 2), and establishes solvability and regularity of weak solutions in Sobolev space framework for p > 2 cases, extending previous fractional SPDE results.

Conclusion: The research successfully develops a comprehensive theory for time-fractional SPDEs with general non-local operators and Lévy noises, providing both Lp and Sobolev space frameworks that unify and extend previous fractional SPDE results.

Abstract: This paper investigates a class of time-fractional stochastic partial differential equations (SPDEs) on $\mathbb{R}^d$, driven by both Gaussian and Lévy space-time white noises. The equation involves Caputo time-derivatives of orders $α, σ_1, σ_2$ and a non-local operator $φ(Δ)$ generated by a Bernstein function $φ$. We first establish the local existence and uniqueness of mild solutions in the $L_p$-framework for $1 \leq p \leq 2$. For the case $p > 2$, where the $L_p$-theory fails, we develop a Sobolev space framework based on the scales of Besov and Triebel--Lizorkin spaces associated with $φ(Δ)$. Within this setting, we prove the solvability and regularity of weak solutions. Our analysis relies on detailed estimates of the fundamental solutions, stochastic convolutions, Littlewood--Paley theory, and a fixed point argument. The results presented here extend and unify several previous works on fractional SPDEs by incorporating a general class of non-local operators and Lévy noises.

</details>


### [25] [Spectra and pseudospectra of non-Hermitian Toeplitz operators: Eigenvector decay transitions in banded and dense matrices](https://arxiv.org/abs/2512.03757)
*Yannick de Bruijn,Bryn Davies,Sacha Dupuy,Erik Orvehed Hiltunen*

Main category: math.AP

TL;DR: Mathematical method for constructing eigenvectors of non-Hermitian Toeplitz operators with algebraic decay, applied to understand non-Hermitian skin effect transitions.


<details>
  <summary>Details</summary>
Motivation: To develop analytical tools for non-Hermitian Toeplitz operators with slow algebraic decay, which are challenging to analyze and appear in physical systems like subwavelength resonators.

Method: Generalized Floquet-Bloch theory extended to construct eigenvectors for both banded and fully dense non-Hermitian Toeplitz operators with algebraically decaying off-diagonal entries.

Result: Sharp decay estimates for bulk eigenmodes and defect eigenfrequencies; demonstration that banded approximations fail for dense operators due to slow algebraic decay; application to model non-Hermitian skin effect transitions.

Conclusion: The method successfully analyzes dense non-Hermitian Toeplitz operators and reveals fundamental mechanisms governing transitions between non-Hermitian skin effect and defect-induced localization.

Abstract: Using a generalised Floquet-Bloch theory, we present a mathematical method to construct eigenvectors for non-Hermitian Toeplitz operators. We extend the method to both banded Toeplitz operators and those with algebraically decaying, fully dense off-diagonal structure. We present sharp decay estimates for the amplitude of bulk eigenmodes as well as eigenmodes associated with defect eigenfrequencies inside the spectral band gap. The validity of those results is illustrated numerically and we show that banded approximations give poor reconstructions of the dense operators, due to the slow algebraic decay. We apply the insights gained to model the non-Hermitian skin effect in a three-dimensional system of subwavelength resonators, where the corresponding operator exhibits only algebraic decay of off-diagonal entries. We use our approach to demonstrate the fundamental mechanism responsible for the transition between the non-Hermitian skin effect and defect-induced localisation in the bulk.

</details>


### [26] [Counter-examples to the fractal Weyl law for semiclassical resonances](https://arxiv.org/abs/2512.03773)
*Jean-Francois Bony,Setsuro Fujiie,Thierry Ramond,Maher Zerzeri*

Main category: math.AP

TL;DR: Paper shows that known upper bounds on semiclassical resonance counts (based on fractal dimension) are not always sharp, providing examples with far fewer resonances.


<details>
  <summary>Details</summary>
Motivation: Previous results established upper bounds on semiclassical resonance counts using fractal dimension of the trapped set, but it was unclear whether these bounds were optimal or could be improved in certain cases.

Method: Constructs specific examples/counterexamples of operators that demonstrate significantly fewer resonances than the established upper bounds would suggest.

Result: Shows that the known upper bounds based on fractal dimension are not always sharp, as operators can exist with much smaller resonance counts than these bounds predict.

Conclusion: The fractal dimension-based upper bounds on semiclassical resonance counts are not optimal in all cases, indicating room for improvement in resonance counting theory.

Abstract: Under general assumptions, the numbers of semiclassical resonances is known to be bounded from above by a negative power of $h$ which is given by the fractal dimension of the trapped set. In this paper we provide examples of operators with much less resonances, showing that these upper bounds are not always sharp.

</details>


### [27] [Spectral properties of the Frechet derivatives of stratified steady Stokes waves](https://arxiv.org/abs/2512.03831)
*Vladimir Kozlov*

Main category: math.AP

TL;DR: The paper analyzes spectral properties of Frechet derivatives for steady water Stokes waves, proving no subharmonic waves exist near a Stokes wave when the second eigenvalue is positive.


<details>
  <summary>Details</summary>
Motivation: To understand spectral properties of Frechet derivatives for stratified steady water waves and determine conditions for existence/non-existence of subharmonic waves near Stokes waves.

Method: Analyzes eigenvalues of Frechet derivatives evaluated at Stokes waves in the class of periodic solutions with same period. Uses spectral analysis approach to study wave stability and bifurcation.

Result: Shows that if the second eigenvalue of the Frechet derivative is positive, then there are no waves with multiple periods (subharmonic waves) in a neighborhood of the Stokes wave.

Conclusion: Spectral properties of Frechet derivatives provide crucial information about existence of subharmonic waves near Stokes waves, with positivity of the second eigenvalue ensuring absence of such waves.

Abstract: We consider stratified steady water waves in a two dimensional channel. Our main subject is spectral properties of the Frechet derivatives of steady water Stokes waves. One of main results is the absence of subharmonic water waves in a neighborhood of a Stokes wave. The main assumption is formulated in terms of the eigenvalues of the Frechet derivative evaluated at this wave and considered in the class of periodic solutions of the same period. The first eigenvalue is always negative. We show that if the second eigenvalue is positive then there are no waves with multiple periods in a neighborhood of the Stokes wave.

</details>


### [28] [A remark on the log-Sobolev inequality for the Gibbs measure of the focusing Schrödinger equation](https://arxiv.org/abs/2512.03897)
*Guopeng Li,Jiawei Li,Leonardo Tolomeo*

Main category: math.AP

TL;DR: The paper proves log-Sobolev inequalities for Gibbs measures of focusing Schrödinger equations with nonlinearity p∈[2,4], but shows limitations for p>4 where known techniques fail.


<details>
  <summary>Details</summary>
Motivation: To establish functional inequalities (log-Sobolev) for Gibbs measures arising in the statistical mechanics of focusing nonlinear Schrödinger equations, which is important for understanding their ergodic properties and convergence to equilibrium.

Method: Analyzes the Gibbs measure formally defined by Lebowitz-Rose-Speer (1988) with L² constraint. Uses analytical techniques to prove log-Sobolev inequalities for p∈[2,4] by studying the measure's properties, and shows for p>4 that the Hessian of the potential has a lower bound that prevents application of standard techniques.

Result: 1) For 2≤p≤4, the Gibbs measure satisfies a log-Sobolev inequality. 2) For p>4, a lower bound for the Hessian of the potential is established, demonstrating that known techniques for proving log-Sobolev inequalities cannot apply to these measures.

Conclusion: The paper establishes a sharp threshold at p=4 for proving log-Sobolev inequalities using current techniques for focusing Schrödinger Gibbs measures, with successful proofs for subcritical/critical cases (p≤4) and identified obstacles in supercritical cases (p>4).

Abstract: We consider the question of showing a log-Sobolev inequality for the Gibbs measure of the focusing Schrödinger equation built by Lebowitz-Rose-Speer (1988), formally given by
  $$ dρ\propto \exp\big(\frac 1 p\int_{\mathbb T} |u|^p d x - \frac 12\int_{\mathbb T} |\nabla u|^2 d x - \frac 12\int_{\mathbb T} |u|^2 d x\big) \mathbf 1_{\| u \|_{L^2(\mathbb T)}^2 \le K}dud\overline{u}. $$
  When $2 \le p \le 4$, we show that these measures indeed satisfy a log-Sobolev inequality. When $p> 4$, we show a lower bound for the Hessian of the potential, which implies that the known techniques to show these inequalities cannot apply to the measure $ρ$.

</details>


### [29] [On well-posedness for second-order degenerate parabolic equations with unbounded lower-order terms](https://arxiv.org/abs/2512.03978)
*Khalid Baadi*

Main category: math.AP

TL;DR: Well-posedness of Cauchy problems for second-order degenerate parabolic equations with non-smooth time-dependent coefficients and unbounded lower-order terms in mixed Lebesgue/Lorentz spaces.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for degenerate parabolic equations with minimal regularity assumptions, accommodating real-world applications where coefficients may be non-smooth and lower-order terms unbounded.

Method: Purely variational approach without a priori regularity assumptions; uses norm inequalities for fractional powers of degenerate Laplacian and embeddings ensuring time continuity; avoids regularization via smooth approximations.

Result: Existence and uniqueness of fundamental solution coinciding with evolution family; representation formula for all weak solutions; L^2 off-diagonal estimates; Gaussian upper bounds under weak Moser estimates assumption.

Conclusion: Establishes comprehensive well-posedness theory for degenerate parabolic equations with minimal assumptions, extending classical Lions regularity theorem to accommodate wide class of source terms and non-smooth coefficients.

Abstract: In this paper, we establish the well-posedness of Cauchy problems for weak solutions to second-order degenerate parabolic equations with a non-smooth, time-dependent degenerate elliptic part that includes both bounded and unbounded lower-order terms. The unbounded lower-order terms are allowed to lie in mixed time-space Lebesgue or even Lorentz spaces. Our notion of weak solutions is formulated under minimal assumptions. We prove the existence and uniqueness of a fundamental solution, which coincides with the associated evolution family for the homogeneous problem (i.e., with zero source term) and provides a representation formula for all weak solutions. We also establish $L^2$ off-diagonal estimates for the fundamental solution and derive Gaussian upper bounds under the weak assumption of Moser's $L^2$-$L^\infty$ estimates for weak solutions. Our approach is purely variational and avoids any a priori regularity assumptions on weak solutions or regularization via smooth approximations. Two key ingredients are norm inequalities for fractional powers of the degenerate Laplacian, and a set of embeddings that ensure time continuity of weak solutions, extending the classical Lions regularity theorem and accommodating a wide class of source terms.

</details>


### [30] [A Strict Comparison Principle for Integro-Differential Hamilton-Jacobi-Bellman Equations on Domains with Boundary](https://arxiv.org/abs/2512.04005)
*Serena Della Corte,Fabian Fuchs,Richard C. Kraaij,Max Nendel*

Main category: math.AP

TL;DR: Comparison principle for viscosity solutions on bounded cylindrical spaces using test functions for diffusive/jump terms, with Lyapunov functions for unbounded solutions.


<details>
  <summary>Details</summary>
Motivation: Need a unified framework for viscosity solutions on bounded cylindrical spaces that handles both diffusive and jump terms, and accommodates unbounded solutions.

Method: Test function framework for simultaneous treatment of diffusive and jump terms, using Lyapunov functions as growth bounds in the comparison principle proof.

Result: Establishes comparison principle for viscosity solutions on (partially) bounded cylindrical spaces, enabling theory for unbounded viscosity solutions.

Conclusion: Framework successfully applied to parabolic equations and elliptic problems on spaces with corners, providing robust comparison principle for complex boundary value problems.

Abstract: This work provides a comparison principle for viscosity solutions to boundary value problems on (partially) bounded, cylindrical spaces. The comparison principle is based on a test function framework, that allows for the simultaneous treatment of diffusive as well as jump terms. Estimates in the proof of the comparison principle incorporate the use of Lyapunov functions that act as growth bounds for the solutions, effectively yielding a theory for unbounded viscosity solutions. We apply the results to a wide class of parabolic equations and elliptic problems on a space with corners.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [31] [A high-order regularized delta-Chebyshev method for computing spectral densities](https://arxiv.org/abs/2512.03149)
*Jinjing Yi,Daniel Massatt,Andrew Horning,Mitchell Luskin,J. H. Pixley,Jason Kaye*

Main category: physics.comp-ph

TL;DR: High-order delta-Chebyshev method for computing spectral densities of sparse Hamiltonians, achieving rapid convergence to thermodynamic limit through high-order accurate δ-function approximation.


<details>
  <summary>Details</summary>
Motivation: Need efficient numerical methods for computing spectral densities and local density of states (LDOS) of sparse Hamiltonians from tight-binding models, with improved convergence to thermodynamic limit compared to existing methods like Chebyshev kernel polynomial method (KPM).

Method: Developed high-order delta-Chebyshev method as variant of regularized Chebyshev KPM, using high-order accurate approximation of δ-function. Maintains same computational steps as KPM but adds inexpensive post-processing for high-order accuracy.

Result: Demonstrated high-order convergence to LDOS at non-singular points when applied to tight-binding models of graphene and twisted bilayer graphene.

Conclusion: The method provides efficient computation of spectral densities with rapid convergence to thermodynamic limit while maintaining computational efficiency similar to KPM.

Abstract: We introduce a numerical method for computing spectral densities, and apply it to the evaluation of the local density of states (LDOS) of sparse Hamiltonians derived from tight-binding models. The approach, which we call the high-order delta-Chebyshev method, can be viewed as a variant of the popular regularized Chebyshev kernel polynomial method (KPM), but it uses a high-order accurate approximation of the $δ$-function to achieve rapid convergence to the thermodynamic limit for smooth spectral densities. The costly computational steps are identical to those for KPM, with high-order accuracy achieved by an inexpensive post-processing procedure. We apply the algorithm to tight-binding models of graphene and twisted bilayer graphene, demonstrating high-order convergence to the LDOS at non-singular points.

</details>


### [32] [Consistent Projection of Langevin Dynamics: Preserving Thermodynamics and Kinetics in Coarse-Grained Models](https://arxiv.org/abs/2512.03706)
*Vahid Nateghi,Lara Neureither,Selma Moqvist,Carsten Hartmann,Simon Olsson,Feliks Nüske*

Main category: physics.comp-ph

TL;DR: A projection-based coarse-graining method for underdamped Langevin dynamics using Zwanzig projection and gEDMD for kinetic analysis, combined with thermodynamic interpolation for multi-state modeling.


<details>
  <summary>Details</summary>
Motivation: Coarse-graining is essential for efficient modeling of complex multi-scale systems like biomolecular conformational dynamics, but existing methods need improvement for accurately capturing both thermodynamic and kinetic properties across different thermodynamic states.

Method: Developed a projection-based coarse-graining formalism for underdamped Langevin dynamics using Zwanzig projection approach. Applied gEDMD (generator Extended Dynamic Mode Decomposition) from Koopman operator theory to model CG dynamics and evaluate kinetic properties. Combined with thermodynamic interpolation (TI) to extend approach across thermodynamic states without repeated simulations.

Result: Demonstrated on a 2D model system that the method accurately captures both thermodynamic and kinetic properties of the full-space model, enabling efficient multi-scale modeling across different thermodynamic conditions.

Conclusion: The proposed projection-based coarse-graining approach combined with gEDMD and thermodynamic interpolation provides an effective framework for accurately modeling complex multi-scale systems' thermodynamic and kinetic properties across different states without repeated simulations.

Abstract: Coarse graining (CG) is an important task for efficient modeling and simulation of complex multi-scale systems, such as the conformational dynamics of biomolecules. This work presents a projection-based coarse-graining formalism for general underdamped Langevin dynamics. Following the Zwanzig projection approach, we derive a closed-form expression for the coarse grained dynamics. In addition, we show how the generator Extended Dynamic Mode Decomposition (gEDMD) method, which was developed in the context of Koopman operator methods, can be used to model the CG dynamics and evaluate its kinetic properties, such as transition timescales. Finally, we combine our approach with thermodynamic interpolation (TI), a generative approach to transform samples between thermodynamic conditions, to extend the scope of the approach across thermodynamic states without repeated numerical simulations. Using a two-dimensional model system, we demonstrate that the proposed method allows to accurately capture the thermodynamic and kinetic properties of the full-space model.

</details>


### [33] [Comparing time and frequency domain numerical methods with Born-Rytov approximations for far-field electromagnetic scattering from single biological cells](https://arxiv.org/abs/2512.03858)
*Cael Warner*

Main category: physics.comp-ph

TL;DR: The paper compares Born-Rytov approximation with other numerical methods (analytical, Yee-lattice FDTD, DDA) for modeling electromagnetic scattering from biological cells, showing Born-Rytov and DDA perform better for far-field scattering patterns of yeast cells.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare different numerical methods for estimating effective refractive index of biological cells from light scattering measurements, which is important for determining cell dry mass, volume, and internal morphology directly from scattering patterns.

Method: Comparison of Born-Rytov approximation with three other methods: analytical solutions, Yee-lattice finite-difference time-domain (FDTD), and discrete-dipole approximation (DDA). The methods were tested on electromagnetic scattering from a sphere and tomographic reconstruction of Saccharomyces cerevisiae yeast cells, evaluating accuracy, memory usage, and compute time for both near-field intensity and far-field projected intensity.

Result: Born-Rytov scattering approximation and discrete dipole approximation showed better agreement with far-field light scattering patterns from Saccharomyces cerevisiae compared to a commercial software implementation of Yee-lattice FDTD method.

Conclusion: Born-Rytov and DDA methods are more effective than commercial FDTD implementations for modeling far-field scattering patterns of biological cells, providing practical advantages for estimating cell properties from light scattering measurements.

Abstract: The Born-Rytov approximation estimates effective refractive index of biological cells from measurements of scattered light intensity, polarization and phase. Effective refractive index is useful for estimating a biological cell's dry mass, volume, and internal morphology directly from its elastic light scattering pattern. This work compares the Born-Rytov approximation with analytical, Yee-lattice finite-difference time-domain, and discrete-dipole approximations to Maxwell's equations in the cases of electromagnetic scattering from a sphere and a tomographic reconstruction of Saccharomyces cerevisiae. Practical advantages and limitations of each numerical method are compared for modeling electromagnetic scattering of both near-field intensity and the far-field projected intensity, in terms of accuracy, memory, and compute time. When compared with a commercial software implementation of the Yee-lattice finite-difference time domain method, the Born-Rytov scattering approximation and discrete dipole approximation show better agreement with the far-field light scattering pattern from Saccharomyces cerevisiae.

</details>


### [34] [Refining Machine Learning Potentials through Thermodynamic Theory of Phase Transitions](https://arxiv.org/abs/2512.03974)
*Paul Fuchs,Julija Zavadlav*

Main category: physics.comp-ph

TL;DR: A fine-tuning strategy using Differentiable Trajectory Reweighting to correct ML potential phase transition temperatures to match experimental data, demonstrated on Titanium phase diagram.


<details>
  <summary>Details</summary>
Motivation: Foundational ML potentials have accuracy limitations for phase transitions (deviations of hundreds of kelvins), requiring fine-tuning for practical applications. Current models suffer from insufficient/bias reference data affecting predictive quality.

Method: Top-down learning via Differentiable Trajectory Reweighting algorithm to minimize free energy differences between phases at experimental target pressures/temperatures. Directly corrects wrongly predicted transition temperatures.

Result: Successfully corrected Titanium phase diagram up to 5 GPa, matching experimental reference within tenths of kelvins and improving liquid-state diffusion constant.

Conclusion: Model-agnostic approach applicable to multi-component systems with solid-solid/solid-liquid transitions, enabling highly accurate application-specific foundational ML potentials through top-down training on experimental properties.

Abstract: Foundational Machine Learning Potentials can resolve the accuracy and transferability limitations of classical force fields. They enable microscopic insights into material behavior through Molecular Dynamics simulations, which can crucially expedite material design and discovery. However, insufficiently broad and systematically biased reference data affect the predictive quality of the learned models. Often, these models exhibit significant deviations from experimentally observed phase transition temperatures, in the order of several hundred kelvins. Thus, fine-tuning is necessary to achieve adequate accuracy in many practical problems. This work proposes a fine-tuning strategy via top-down learning, directly correcting the wrongly predicted transition temperatures to match the experimental reference data. Our approach leverages the Differentiable Trajectory Reweighting algorithm to minimize the free energy differences between phases at the experimental target pressures and temperatures. We demonstrate that our approach can accurately correct the phase diagram of pure Titanium in a pressure range of up to 5 GPa, matching the experimental reference within tenths of kelvins and improving the liquid-state diffusion constant. Our approach is model-agnostic, applicable to multi-component systems with solid-solid and solid-liquid transitions, and compliant with top-down training on other experimental properties. Therefore, our approach can serve as an essential step towards highly accurate application-specific and foundational machine learning potentials.

</details>


### [35] [Predicting parameters of a model cuprate superconductor using machine learning](https://arxiv.org/abs/2512.04024)
*V. A. Ulitko,D. N. Yasinskaya,S. A. Bezzubin,A. A. Koshelev,Y. D. Panov*

Main category: physics.comp-ph

TL;DR: Machine learning approach using U-Net architecture accurately predicts Hamiltonian parameters from phase diagrams for cuprate superconductors, enabling inverse problem solving in condensed matter physics.


<details>
  <summary>Details</summary>
Motivation: The computational complexity of calculating phase diagrams for multi-parameter models limits the ability to select parameters that match experimental data, creating a need for efficient inverse problem solving methods.

Method: Comparative study of three deep learning architectures (VGG, ResNet, U-Net) adapted for regression tasks, with U-Net showing best performance. Training on extensive dataset of phase diagrams calculated within mean-field approximation, validated using semi-classical heat bath algorithm for Monte Carlo simulations.

Result: U-Net model accurately predicts all considered Hamiltonian parameters. Areas of low prediction accuracy correspond to regions of parametric insensitivity in phase diagrams, allowing extraction of physically interpretable patterns and validation of parameter significance.

Conclusion: Machine learning shows promising potential for analyzing complex physical models in condensed matter physics by solving inverse problems and providing physically interpretable insights into parameter significance.

Abstract: The computational complexity of calculating phase diagrams for multi-parameter models significantly limits the ability to select parameters that correspond to experimental data. This work presents a machine learning method for solving the inverse problem - forecasting the parameters of a model Hamiltonian for a cuprate superconductor based on its phase diagram. A comparative study of three deep learning architectures was conducted: VGG, ResNet, and U-Net. The latter was adapted for regression tasks and demonstrated the best performance. Training the U-Net model was performed on an extensive dataset of phase diagrams calculated within the mean-field approximation, followed by validation on data obtained using a semi-classical heat bath algorithm for Monte Carlo simulations. It is shown that the model accurately predicts all considered Hamiltonian parameters, and areas of low prediction accuracy correspond to regions of parametric insensitivity in the phase diagrams. This allows for the extraction of physically interpretable patterns and validation of the significance of parameters for the system. The results confirm the promising potential of applying machine learning to analyze complex physical models in condensed matter physics.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [36] [Energetic-particle orbits near rational flux surfaces in stellarators: I. Passing particles](https://arxiv.org/abs/2512.03165)
*Thomas E. Foster,Felix I. Parra,Roscoe B. White,José Luis Velasco,Iván Calvo,Elizabeth J. Paul*

Main category: physics.plasm-ph

TL;DR: Drift islands in stellarators can enhance alpha-particle transport; they can be avoided if magnetic fields satisfy a weaker version of the Cary-Shasharina condition (called "cyclometric").


<details>
  <summary>Details</summary>
Motivation: Recent simulations show that passing energetic particles in stellarators can exhibit drift islands near rational flux surfaces, which could enhance alpha-particle transport and flatten alpha density profiles unless avoided by proper magnetic field design.

Method: Derived equation for drift-island shape in general stellarators by solving fundamental problem of calculating passing particle orbits near rational flux surfaces. Used conservation of "transit adiabatic invariant" associated with closed rational-surface field lines. Computed higher-order corrections for energetic particles and validated with ASCOT5 guiding-centre and full-orbit simulations.

Result: No drift islands exist for all passing particles if and only if magnetic field satisfies weaker version of Cary-Shasharina condition (cyclometric). Drift-island width scales as ∼(ρ*δ/s)^{1/2}a. Calculations agree extremely well with ASCOT5 simulations even at 3.5MeV. Results also derivable using Hamiltonian perturbation theory.

Conclusion: Drift islands can be avoided in stellarator design by ensuring magnetic fields are cyclometric. Large drift islands could arise in low-shear stellarators that are insufficiently cyclometric, potentially affecting alpha-particle transport in reactor-scale equilibria.

Abstract: Recent simulations have shown that, even when the magnetic field of a stellarator possesses nested toroidal flux surfaces, the orbits of passing energetic particles can exhibit islands. These 'drift islands' arise near rational flux surfaces, where they are likely to enhance alpha-particle transport -- flattening the alpha density profile locally -- unless they can be avoided by suitable design of the stellarator magnetic field. To investigate how this might be achieved, we derive an equation for the drift-island shape in a general stellarator. This result follows from the solution to a more fundamental problem: that of calculating the orbits of passing particles near a rational flux surface. We show that these orbits are determined by conservation of an adiabatic invariant associated with the closed rational-surface field lines. We use this 'transit adiabatic invariant' to prove that there are no drift islands, for all passing particles, if and only if the magnetic field satisfies a weaker version of the Cary-Shasharina condition for omnigeneity; we call such magnetic fields 'cyclometric'. The drift-island width scales as $\sim (ρ_\starδ/s)^{1/2} a$ ($ρ_\star$ is the normalized gyroradius, $δ$ is the deviation from cyclometry, $s$ is the magnetic shear, and $a$ is the minor radius), so large drift islands could arise in low-shear stellarators that are insufficiently cyclometric. To ensure accurate results for very energetic particles, we compute higher-order corrections to the transit invariant. Our calculations agree extremely well with ASCOT5 guiding-centre and full-orbit simulations of alpha particles in reactor-scale equilibria, even at $3.5\text{MeV}$. Finally, we show how our results can also be derived using Hamiltonian perturbation theory, which provides a systematic framework for calculating passing-particle orbits on both rational and irrational surfaces.

</details>


### [37] [Understanding cold electron impact on parallel-propagating whistler chorus waves via moment-based quasilinear theory](https://arxiv.org/abs/2512.03269)
*Opal Issan,Vadim Roytershteyn,Gian Luca Delzanno,Salomon Janhunen*

Main category: physics.plasm-ph

TL;DR: Secondary instabilities driven by parallel-propagating whistler waves transfer energy to cold electrons via oblique electrostatic whistler waves and Bernstein-mode turbulence, potentially limiting primary wave amplitudes in Earth's magnetosphere.


<details>
  <summary>Details</summary>
Motivation: Cold electrons (below 100eV) dominate plasma density but are poorly characterized due to measurement challenges. Understanding their role in wave-particle interactions is important, particularly how secondary instabilities enable energy transfer from parallel-propagating whistler waves to cold electrons.

Method: Developed a moment-based quasilinear theory of secondary instabilities to quantify energy exchange between parallel-propagating whistler-mode chorus waves and cold electrons through excitation of oblique electrostatic whistler waves and Bernstein-mode turbulence.

Result: Secondary instabilities persist across wide parameter ranges and often lead to nearly complete damping of primary waves. This might explain why high-amplitude oblique whistler or electron Bernstein waves are rarely observed simultaneously with high-amplitude field-aligned whistler waves in the inner magnetosphere.

Conclusion: Secondary drift-driven instabilities provide a new energy transfer channel to cold electrons and may limit parallel-propagating whistler wave amplitudes in Earth's magnetosphere, explaining observational patterns of wave coexistence.

Abstract: Earth's magnetosphere hosts a wide range of collisionless particle populations that interact through various wave-particle processes. Among these, cold electrons, with energies below 100eV, often dominate the plasma density but remain poorly characterized due to measurement challenges such as spacecraft charging and photoelectron contamination. Understanding the contribution of these cold populations to wave-particle interaction is of significant interest. Recent kinetic simulations identified a secondary drift-driven instability in which parallel-propagating whistler-mode chorus waves excite oblique electrostatic whistler waves near the resonance cone and Bernstein-mode turbulence. These secondary modes enable a new channel of energy transfer from the parallel-propagating whistler wave to the cold electrons. In this work, we develop a moment-based quasilinear theory of the secondary instabilities to quantify such energy exchange. Our results show that these secondary instabilities persist for a wide range of parameters and, in many cases, lead to nearly complete damping of the primary wave. Such secondary instability might limit the amplitude of parallel-propagating whistler waves in Earth's magnetosphere and might explain why high-amplitude oblique whistler or electron Bernstein waves are rarely observed simultaneously with high-amplitude field-aligned whistler waves in the inner magnetosphere.

</details>


### [38] [Microbubble implosions in finite hollow spheres](https://arxiv.org/abs/2512.03379)
*M. A. H. Zosa,M. Murakami*

Main category: physics.plasm-ph

TL;DR: Microbubble implosion (MBI) in finite hollow spheres with free electron redistribution achieves ultra-high density compression (10⁵× solid density) and generates strong electric fields for potential gamma-ray lensing/pair creation applications.


<details>
  <summary>Details</summary>
Motivation: MBI has promising applications but hasn't been studied for finite hollow spheres where electrons can freely redistribute after initial heating. Understanding MBI under these realistic conditions is crucial for practical implementation.

Method: Used electron distribution model to study electron redistribution after initial heating, calculated optimal parameters to fill hollow cavity, and simulated MBI dynamics using hybrid 1D code.

Result: MBI occurs even for finite spheres, achieving high-density compression. The study identified optimal target structure that maximizes ion flashing.

Conclusion: MBI remains viable under finite sphere conditions with free electron redistribution, enabling ultra-high density compression and strong electric field generation for potential advanced applications.

Abstract: Microbubble implosion (MBI) is a recently proposed novel mechanism with many interesting and exciting potential applications. MBI predicts that the inner layers of a spherical target with a hollow cavity can be compressed into a core with a density 105 times that of the solid density. Furthermore, this ultra-compressed core mostly consists of ions. This leads to the generation of ultra-high electric fields, which may be applicable to gamma-ray lensing or pair creation. However, MBI has yet to be studied for finite hollow spheres whose electrons are free to redistribute themselves after being given an initial temperature. This paper studies MBI under finite sphere conditions. Using an elec- tron distribution model, the electron distribution after receiving an initial temperature is studied. Then, the optimal parameters required to fill a hollow cavity with electrons are calculated. The dynamics of MBI is simulated using a hybrid one-dimensional code. The simulation demonstrates that MBI occurs even for finite spheres, and high-density compression is still achievable with this setup. It also shows the opti- mal target structure, which maximizes ion flashing.

</details>


### [39] [Magnetic field amplification driven by the gyro motion of charged particles](https://arxiv.org/abs/2512.03388)
*Y-J. Gu,M. Murakami*

Main category: physics.plasm-ph

TL;DR: Laser-irradiated microtube structure generates and amplifies giga-gauss magnetic fields via charged particle implosions, with field lifetime of hundreds of femtoseconds and spot size comparable to laser wavelength.


<details>
  <summary>Details</summary>
Motivation: Spontaneous magnetic field generation is crucial in laser-plasma interactions, especially for ultra-intense lasers where magnetic forces become as significant as electric forces. These strong quasi-static magnetic fields affect thermal conductivity and plasma dynamics.

Method: Kinetic simulations of magnetic field amplification using laser-irradiated microtube structures, plus development of an analytical model to explain the underlying physics.

Result: Giga-gauss magnetic fields are generated and amplified with opposite polarity to seed magnetic fields. The field spot size is comparable to laser wavelength, and lifetime is hundreds of femtoseconds.

Conclusion: The study provides insights into magnetic field generation mechanisms and should aid in designing future laser-plasma interaction experiments.

Abstract: Spontaneous magnetic field generation plays important role in laser-plasma interactions. Strong quasi-static magnetic fields affect the thermal conductivity and the plasma dynamics, particularly in the case of ultra intense laser where the magnetic part of Lorentz force becomes as significant as the electric part. Kinetic simulations of giga-gauss magnetic field amplification via a laser irradiated microtube structure reveal the dynamics of charged particle implosions and the mechanism of magnetic field growth. A giga-gauss magnetic field is generated and amplified with the opposite polarity to the seed magnetic field. The spot size of the field is comparable to the laser wavelength, and the lifetime is hundreds of femtoseconds. An analytical model is presented to explain the underlying physics. This study should aid in designing future experiments.

</details>


### [40] [Solar Wind Penetration into Dusty Magnetospheres creates Electrostatic Waves and Structures](https://arxiv.org/abs/2512.03856)
*Usman Saeed,Shaukat Ali Shan,Hamid Saleem*

Main category: physics.plasm-ph

TL;DR: Bi-ion plasma with static dust exhibits low-frequency electrostatic instabilities from field-aligned shear flow, forming nonlinear structures (double layers & solitons) relevant to Jupiter/Saturn magnetospheres.


<details>
  <summary>Details</summary>
Motivation: To investigate electrostatic perturbations in bi-ion plasmas with static dust, particularly in planetary magnetospheres where multiple ion species exist alongside charged dust particles.

Method: Developed theoretical model for bi-ion plasma with static dust, analyzing field-aligned shear flow effects on both ion species to study low-frequency electrostatic instabilities and nonlinear structure formation.

Result: Model predicts extremely low frequency electrostatic waves (mHz range) matching observed frequencies in Jupiter/Saturn magnetospheres, with nonlinear structures (double layers & solitons) having widths from hundreds of meters to kilometers.

Conclusion: Field-aligned shear flow in bi-ion dusty plasmas generates low-frequency electrostatic instabilities and nonlinear structures similar to those observed in Earth's upper ionosphere, providing insights into wave phenomena in planetary magnetospheres.

Abstract: The low frequency electrostatic perturbations have been investigated in a bi ion plasma in the background of static dust. It is shown that the field aligned shear flow of both the ions produce low frequency electrostatic instabilities and create nonlinear structures, the double layers and the solitons. The general theoretical model is applied to the magnetospheres of Jupiter (with positively charged dust) and Saturn (with negatively charged dust) which have oxygen ions in addition to protons. This model predicts the existence of extremely low frequency electrostatic waves with real frequencies of the order of a milli Hertz (mHz) to several mHz and this range of frequencies have been reported in literature for these plasma environments. The estimated width of the nonlinear structures vary from a few hundred meters to a few kilometers. These structures are similar to that observed in the oxygen and oxygen hydrogen plasmas in Earth's upper ionosphere which is free from dust.

</details>


### [41] [Integrating High Performance In-Memory Data Streaming and In-Situ Visualization in Hybrid MPI+OpenMP PIC MC Simulations Towards Exascale](https://arxiv.org/abs/2512.03914)
*Jeremy J. Williams,Stefan Costea,Daniel Medeiros,Jordy Trilaksono,Pratibha Hegde,David Tskhakaya,Leon Kos,Ales Podolnik,Jakub Hromadka,Kevin A. Huck,Allen D. Malony,Frank Jenko,Erwin Laure,Stefano Markidis*

Main category: physics.plasm-ph

TL;DR: BIT1 PIC-MC plasma simulation code enhanced with OpenMP task parallelism, openPMD streaming API, and ADIOS2 SST engine for improved I/O performance and real-time analysis at exascale.


<details>
  <summary>Details</summary>
Motivation: Traditional file I/O inefficiencies are a major bottleneck for exascale plasma simulations needed for fusion energy research. Efficient simulation of complex plasma dynamics (turbulence, confinement) is crucial for optimizing fusion reactor performance.

Method: Enhanced BIT1 electrostatic PIC-MC code with: 1) OpenMP task-based parallelism for particle mover, 2) openPMD streaming API integration, 3) ADIOS2 SST engine for in-memory data streaming, 4) time-dependent data checkpointing with openPMD API, and 5) profiling using gprof, perf, IPM, and Darshan tools.

Result: Demonstrated improvements in simulation runtime, data accessibility, and real-time insights by comparing traditional file I/O with ADIOS2 BP4 and SST backends. Enabled seamless data movement and in-situ visualization for real-time analysis without interrupting simulations.

Conclusion: The hybrid BIT1 openPMD SST enhancement introduces a new paradigm for real-time scientific discovery in plasma simulations, enabling faster insights and more efficient use of exascale computing resources for fusion energy research.

Abstract: Efficient simulation of complex plasma dynamics is crucial for advancing fusion energy research. Particle-in-Cell (PIC) Monte Carlo (MC) simulations provide insights into plasma behavior, including turbulence and confinement, which are essential for optimizing fusion reactor performance. Transitioning to exascale simulations introduces significant challenges, with traditional file input/output (I/O) inefficiencies remaining a key bottleneck. This work advances BIT1, an electrostatic PIC MC code, by improving the particle mover with OpenMP task-based parallelism, integrating the openPMD streaming API, and enabling in-memory data streaming with ADIOS2's Sustainable Staging Transport (SST) engine to enhance I/O performance, computational efficiency, and system storage utilization. We employ profiling tools such as gprof, perf, IPM and Darshan, which provide insights into computation, communication, and I/O operations. We implement time-dependent data checkpointing with the openPMD API enabling seamless data movement and in-situ visualization for real-time analysis without interrupting the simulation. We demonstrate improvements in simulation runtime, data accessibility and real-time insights by comparing traditional file I/O with the ADIOS2 BP4 and SST backends. The proposed hybrid BIT1 openPMD SST enhancement introduces a new paradigm for real-time scientific discovery in plasma simulations, enabling faster insights and more efficient use of exascale computing resources.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [42] [Quasi-linear theory of perpendicular ion heating by critically balanced turbulence](https://arxiv.org/abs/2512.03472)
*Zade Johnston,Jonathan Squire*

Main category: astro-ph.SR

TL;DR: Analytical computation of ion heating rates in turbulent astrophysical plasmas using quasi-linear theory, showing how turbulence imbalance transitions heating mechanisms between stochastic and resonant-cyclotron heating.


<details>
  <summary>Details</summary>
Motivation: Understanding anisotropic ion heating in collisionless astrophysical plasmas is crucial for plasma evolution, as ions are often observed to be preferentially heated perpendicular to the magnetic field. The paper aims to explain the mechanisms responsible for this heating.

Method: Uses quasi-linear theory framework to compute heating rates analytically for ions interacting with turbulent, large-scale Alfvénic fluctuations. Examines how turbulence imbalance modifies the spatiotemporal spectrum of fluctuations.

Result: Shows that turbulence imbalance causes transition between two heating mechanisms: stochastic heating in balanced turbulence to resonant-cyclotron heating in imbalanced turbulence. The heating rate has a general form regardless of imbalance level, showing suppression related to conservation of ions' magnetic moment at small amplitudes.

Conclusion: The work consolidates qualitative understanding of ion heating in astrophysical plasmas and provides specific quantitative predictions for analyzing simulations and observations, recovering previous empirical results in a formal calculation.

Abstract: In collisionless astrophysical plasmas, turbulence mediates the partitioning of free energy among cascade channels and its dissipation into ion and electron heat. The resulting ion heating is often anisotropic, with ions observed to be preferentially heated perpendicular to the local magnetic field; understanding the mechanisms responsible for this heating is a key step in understanding the evolution of such plasmas. In this paper, we use the framework of quasi-linear theory to compute analytically the heating rates of ions interacting with turbulent, large-scale Alfvénic fluctuations. We show how the imbalance of the turbulence (the difference in energies between Alfvénic fluctuations travelling parallel and antiparallel to the magnetic field) modifies the spatiotemporal spectrum of these fluctuations, allowing the heating mechanism to transition between two commonly-studied mechanisms: stochastic heating in balanced turbulence to resonant-cyclotron heating in imbalanced turbulence. The resultant heating rate is found to have a general form regardless of the level of imbalance, exhibiting a suppression related to the conservation of the ions' magnetic moment at small turbulent amplitudes and recovering previous empirical results in a formal calculation. The results of this work help to consolidate our qualitative understanding of ion heating within astrophysical plasmas, as well as yielding specific quantitative predictions to analyse simulations and observations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [Physics-Informed Machine Learning for Steel Development: A Computational Framework and CCT Diagram Modelling](https://arxiv.org/abs/2512.03050)
*Peter Hedström,Victor Lamelas Cubero,Jón Sigurdsson,Viktor Österberg,Satish Kolli,Joakim Odqvist,Ziyong Hou,Wangzhong Mu,Viswanadh Gowtham Arigela*

Main category: cs.LG

TL;DR: Physics-informed ML framework for predicting continuous cooling transformation (CCT) diagrams in steels, achieving high accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Applying general-purpose ML to complex industrial materials like steel is challenging due to intricate relationships between composition, processing, microstructure, and properties. Need physics-informed approaches for accurate predictions.

Method: Combined physical insights with ML to develop physics-informed CCT model for steels, trained on 4,100 diagrams dataset, validated against literature and experimental data.

Result: High computational efficiency (complete CCT diagrams with 100 cooling curves in <5s), strong generalizability across alloy steels (phase classification F1 scores >88% for all phases), phase transition temperature MAE <20°C except bainite (27°C).

Conclusion: Framework can be extended with additional ML models for universal digital twin platform for heat treatment, supporting accelerated materials design workflows through integration with simulation tools and experiments.

Abstract: Machine learning (ML) has emerged as a powerful tool for accelerating the computational design and production of materials. In materials science, ML has primarily supported large-scale discovery of novel compounds using first-principles data and digital twin applications for optimizing manufacturing processes. However, applying general-purpose ML frameworks to complex industrial materials such as steel remains a challenge. A key obstacle is accurately capturing the intricate relationship between chemical composition, processing parameters, and the resulting microstructure and properties. To address this, we introduce a computational framework that combines physical insights with ML to develop a physics-informed continuous cooling transformation (CCT) model for steels. Our model, trained on a dataset of 4,100 diagrams, is validated against literature and experimental data. It demonstrates high computational efficiency, generating complete CCT diagrams with 100 cooling curves in under 5 seconds. It also shows strong generalizability across alloy steels, achieving phase classification F1 scores above 88% for all phases. For phase transition temperature regression, it attains mean absolute errors (MAE) below 20 °C across all phases except bainite, which shows a slightly higher MAE of 27 °C. This framework can be extended with additional generic and customized ML models to establish a universal digital twin platform for heat treatment. Integration with complementary simulation tools and targeted experiments will further support accelerated materials design workflows.

</details>


### [44] [ATHENA: Agentic Team for Hierarchical Evolutionary Numerical Algorithms](https://arxiv.org/abs/2512.03476)
*Juan Diego Toscano,Daniel T. Chen,George Em Karniadakis*

Main category: cs.LG

TL;DR: ATHENA is an autonomous agentic framework that manages end-to-end computational research lifecycle through a hierarchical evolutionary loop, achieving super-human performance with validation errors down to 10^-14.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between theoretical conceptualization and computational implementation in Scientific Computing and Scientific Machine Learning, which is a major bottleneck in scientific discovery.

Method: ATHENA uses a HENA (Hierarchical Evolutionary Numerical Algorithms) loop framed as a Contextual Bandit problem. It analyzes prior trials to select structural actions from combinatorial spaces guided by expert blueprints, translates these into executable code, and generates scientific rewards. It combines hybrid symbolic-numeric workflows and can perform collaborative human-in-the-loop interventions.

Result: Achieves super-human performance with validation errors of 10^-14. In SciC, autonomously identifies mathematical symmetries for exact analytical solutions and derives stable numerical solvers. In SciML, performs deep diagnosis of ill-posed formulations and resolves multiphysics problems through hybrid workflows. Human intervention further improves results by an order of magnitude.

Conclusion: ATHENA represents a paradigm shift from implementation mechanics to methodological innovation, accelerating scientific discovery by autonomously managing the computational research lifecycle and bridging stability gaps through collaborative human intervention.

Abstract: Bridging the gap between theoretical conceptualization and computational implementation is a major bottleneck in Scientific Computing (SciC) and Scientific Machine Learning (SciML). We introduce ATHENA (Agentic Team for Hierarchical Evolutionary Numerical Algorithms), an agentic framework designed as an Autonomous Lab to manage the end-to-end computational research lifecycle. Its core is the HENA loop, a knowledge-driven diagnostic process framed as a Contextual Bandit problem. Acting as an online learner, the system analyzes prior trials to select structural `actions' ($A_n$) from combinatorial spaces guided by expert blueprints (e.g., Universal Approximation, Physics-Informed constraints). These actions are translated into executable code ($S_n$) to generate scientific rewards ($R_n$). ATHENA transcends standard automation: in SciC, it autonomously identifies mathematical symmetries for exact analytical solutions or derives stable numerical solvers where foundation models fail. In SciML, it performs deep diagnosis to tackle ill-posed formulations and combines hybrid symbolic-numeric workflows (e.g., coupling PINNs with FEM) to resolve multiphysics problems. The framework achieves super-human performance, reaching validation errors of $10^{-14}$. Furthermore, collaborative ``human-in-the-loop" intervention allows the system to bridge stability gaps, improving results by an order of magnitude. This paradigm shift focuses from implementation mechanics to methodological innovation, accelerating scientific discovery.

</details>


### [45] [Quantum-Classical Physics-Informed Neural Networks for Solving Reservoir Seepage Equations](https://arxiv.org/abs/2512.03923)
*Xiang Rao,Yina Liu,Yuxuan Shen*

Main category: cs.LG

TL;DR: A quantum-classical hybrid PINN (QCPINN) using discrete variable quantum circuits is proposed to solve reservoir seepage PDEs more efficiently than classical PINNs, with applications to three reservoir models.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods for reservoir seepage PDEs have mesh-dependent errors and high computational costs, while classical PINNs face limitations in parameter efficiency, high-dimensional expression, and strong nonlinear fitting capabilities.

Method: Proposes a Discrete Variable (DV)-Circuit Quantum-Classical Physics-Informed Neural Network (QCPINN) that integrates classical preprocessing/postprocessing networks with a DV quantum core, leveraging quantum superposition and entanglement for enhanced high-dimensional feature mapping while embedding physical constraints. Tests three quantum circuit topologies: Cascade, Cross-mesh, and Alternate.

Result: QCPINNs achieve high prediction accuracy with fewer parameters than classical PINNs. Alternate topology performs best for heterogeneous single-phase flow and two-phase Buckley-Leverett equation, while Cascade topology excels for compositional flow with convection-dispersion-adsorption coupling.

Conclusion: The work verifies the feasibility of QCPINN for reservoir engineering applications, bridging quantum computing research with industrial practice in oil and gas engineering.

Abstract: Solving partial differential equations (PDEs) for reservoir seepage is critical for optimizing oil and gas field development and predicting production performance. Traditional numerical methods suffer from mesh-dependent errors and high computational costs, while classical Physics-Informed Neural Networks (PINNs) face bottlenecks in parameter efficiency, high-dimensional expression, and strong nonlinear fitting. To address these limitations, we propose a Discrete Variable (DV)-Circuit Quantum-Classical Physics-Informed Neural Network (QCPINN) and apply it to three typical reservoir seepage models for the first time: the pressure diffusion equation for heterogeneous single-phase flow, the nonlinear Buckley-Leverett (BL) equation for two-phase waterflooding, and the convection-diffusion equation for compositional flow considering adsorption. The QCPINN integrates classical preprocessing/postprocessing networks with a DV quantum core, leveraging quantum superposition and entanglement to enhance high-dimensional feature mapping while embedding physical constraints to ensure solution consistency. We test three quantum circuit topologies (Cascade, Cross-mesh, Alternate) and demonstrate through numerical experiments that QCPINNs achieve high prediction accuracy with fewer parameters than classical PINNs. Specifically, the Alternate topology outperforms others in heterogeneous single-phase flow and two-phase BL equation simulations, while the Cascade topology excels in compositional flow with convection-dispersion-adsorption coupling. Our work verifies the feasibility of QCPINN for reservoir engineering applications, bridging the gap between quantum computing research and industrial practice in oil and gas engineering.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [46] [Subgrid Mean-field Dynamo Model with Dynamical Quenching in General Relativistic Magnetohydrodynamic Simulations](https://arxiv.org/abs/2512.03443)
*Hongzhe Zhou,Yosuke Mizuno,Zhenyu Zhu*

Main category: astro-ph.HE

TL;DR: Researchers develop a subgrid model for helical large-scale dynamos in general-relativistic MHD simulations of thin accretion disks, showing it reproduces known butterfly diagrams and can launch weak outflows under specific conditions.


<details>
  <summary>Details</summary>
Motivation: Large-scale magnetic fields are crucial for accretion disk dynamics but require computationally expensive simulations. A subgrid dynamo model is needed to enable self-consistent simulations that include intrinsic dynamo processes without prohibitive computational costs.

Method: Develop and implement a subgrid model of helical large-scale dynamo with dynamical quenching in general-relativistic resistive MHD simulations. The model incorporates previous numerical/analytical results and uses only one input parameter (viscosity parameter α_SS). Test the model in geometrically thin accretion disk simulations around black holes.

Result: The model reproduces butterfly diagrams seen in previous simulations. With aggressive parameters (α_SS=0.02, black hole spin a_BH=0.9375), weak collimated polar outflows (Lorentz factor ≈1.2) are launched. No outflows occur with less vigorous turbulence or less positive spin. Negative spin produces field configurations similar to Newtonian cases, while positive spin distorts poloidal field loops and makes cycle periods sporadic. The model can also determine α_SS dynamically from local plasma beta.

Conclusion: The developed subgrid dynamo model successfully captures large-scale magnetic field evolution in accretion disks and enables self-consistent amplification of these fields. It provides a computationally efficient approach for future simulations that include intrinsic dynamo processes.

Abstract: Large-scale magnetic fields are relevant for a number of dynamical processes in accretion disks, including driving turbulence, reconnection events, and launching outflows. Numerical simulations have indicated that the initial strengths and configurations of the large-scale magnetic fields have a direct imprint on the outcome of an accretion disk evolution. To facilitate future self-consistent simulations that include intrinsic dynamo processes, we derive and implement a subgrid model of a helical large-scale dynamo with dynamical quenching in general-relativistic resistive magnetohydrodynamical simulations of geometrically thin accretion disks. By incorporating previous numerical and analytical results of helical dynamos, our model features only one input parameter, the viscosity parameter $α_\text{SS}$. We demonstrate that our model can reproduce butterfly diagrams seen in previous local and global simulations. With rather aggressive parameter choice of $α_\text{SS}=0.02$ and black hole spin $a_\text{BH}=0.9375$, our thin-disk model launches weak collimated polar outflows with Lorentz factor $\simeq 1.2$, but no polar outflow is present with less vigorous turbulence or less positive $a_\text{BH}$. With negative $a_\text{BH}$, we find the field configurations to appear more similar to Newtonian cases, whereas for positive $a_\text{BH}$, the poloidal field loops become distorted and the cycle period becomes sporadic or even disappears. Moreover, we demonstrate how $α_\text{SS}$ can avoid to be prescribed and instead be determined by the local plasma beta. Such a fully dynamical subgrid dynamo allows for self-consistent amplification of the large-scale magnetic fields.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [47] [Sketch Tomography: Hybridizing Classical Shadow and Matrix Product State](https://arxiv.org/abs/2512.03333)
*Xun Tang,Haoxuan Chen,Yuehaw Khoo,Lexing Ying*

Main category: quant-ph

TL;DR: Sketch Tomography: Efficient quantum state tomography using classical shadows for MPS states with quadratic scaling in system size.


<details>
  <summary>Details</summary>
Motivation: Need efficient quantum state tomography methods that can handle matrix product states (MPS) with provable convergence and practical sample complexity.

Method: Based on classical shadow protocol, estimates tensor components of MPS density matrix through observable estimations, using tensor train ansatz.

Result: Procedure is provably convergent with quadratic scaling in system size, produces accurate approximations, outperforms classical shadows for moderately large subsystems and maximum likelihood estimation.

Conclusion: Sketch tomography provides efficient and accurate quantum state tomography for MPS states with theoretical guarantees and practical advantages over existing methods.

Abstract: We introduce Sketch Tomography, an efficient procedure for quantum state tomography based on the classical shadow protocol used for quantum observable estimations. The procedure applies to the case where the ground truth quantum state is a matrix product state (MPS). The density matrix of the ground truth state admits a tensor train ansatz as a result of the MPS assumption, and we estimate the tensor components of the ansatz through a series of observable estimations, thus outputting an approximation of the density matrix. The procedure is provably convergent with a sample complexity that scales quadratically in the system size. We conduct extensive numerical experiments to show that the procedure outputs an accurate approximation to the quantum state. For observable estimation tasks involving moderately large subsystems, we show that our procedure gives rise to a more accurate estimation than the classical shadow protocol. We also show that sketch tomography is more accurate in observable estimation than quantum states trained from the maximum likelihood estimation formulation.

</details>


### [48] [In Situ Quantum Analog Pulse Characterization via Structured Signal Processing](https://arxiv.org/abs/2512.03193)
*Yulong Dong,Christopher Kang,Murphy Yuezhen Niu*

Main category: quant-ph

TL;DR: Extends Quantum Signal Processing to characterize continuous pulse trajectories for analog quantum simulators, enabling high-fidelity time-dependent Hamiltonian control without mid-circuit measurements.


<details>
  <summary>Details</summary>
Motivation: Analog quantum simulators need accurate time-dependent pulse control for exploring physical phenomena, but existing calibration schemes are tailored for digital gates and cannot learn continuous pulse trajectories.

Method: Extends Quantum Signal Processing framework to analyze time-dependent pulses, combines QSP with logical-level analog-digital mapping to reconstruct smooth pulses directly from time-ordered propagator queries without mid-circuit measurements.

Result: Achieves high accuracy with strong efficiency and robustness against SPAM and depolarizing errors, avoids unscalable performance degradation from accumulated local truncation errors unlike Trotterization-based methods.

Conclusion: Provides lightweight and optimal validation protocol for analog quantum simulators capable of detecting major hardware faults, enabling accurate characterization of continuous pulse trajectories.

Abstract: Analog quantum simulators can directly emulate time-dependent Hamiltonian dynamics, enabling the exploration of diverse physical phenomena such as phase transitions, quench dynamics, and non-equilibrium processes. Realizing accurate analog simulations requires high-fidelity time-dependent pulse control, yet existing calibration schemes are tailored to digital gate characterization and cannot be readily extended to learn continuous pulse trajectories. We present a characterization algorithm for in situ learning of pulse trajectories by extending the Quantum Signal Processing (QSP) framework to analyze time-dependent pulses. By combining QSP with a logical-level analog-digital mapping paradigm, our method reconstructs a smooth pulse directly from queries of the time-ordered propagator, without requiring mid-circuit measurements or additional evolution. Unlike conventional Trotterization-based methods, our approach avoids unscalable performance degradation arising from accumulated local truncation errors as the logical-level segmentation increases. Through rigorous theoretical analysis and extensive numerical simulations, we demonstrate that our method achieves high accuracy with strong efficiency and robustness against SPAM as well as depolarizing errors, providing a lightweight and optimal validation protocol for analog quantum simulators capable of detecting major hardware faults.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [49] [Modeling and simulation of electrodiffusion in dense reconstructions of cerebral tissue](https://arxiv.org/abs/2512.03224)
*Halvor Herlyng,Marius Causemann,Gaute T. Einevoll,Ada J. Ellingsrud,Geir Halnes,Marie E. Rognes*

Main category: physics.med-ph

TL;DR: Framework for modeling electrodiffusion in cerebral tissue using realistic geometries from electron microscopy data, with demonstration of neuronal activity simulations.


<details>
  <summary>Details</summary>
Motivation: Excitable tissue is fundamental to brain function but extremely complex to study due to morphological complexity and physiological dynamics, requiring detailed computational modeling with efficient numerical methods and realistic geometries.

Method: Developed a computational framework that models electrodiffusion in excitable cerebral tissue using realistic geometries generated from electron microscopy data, with efficient numerical methods and image segmentation/meshing techniques.

Result: Successfully simulated electrodiffusive dynamics in cerebral tissue during neuronal activity, demonstrating the framework's application potential.

Conclusion: The work highlights the numerical and computational challenges of modeling electrodiffusion and other multiphysics in dense reconstructions of cerebral tissue, providing a framework for such complex simulations.

Abstract: Excitable tissue is fundamental to brain function, yet its study is complicated by extreme morphological complexity and the physiological processes governing its dynamics. Consequently, detailed computational modeling of this tissue represents a formidable task, requiring both efficient numerical methods and robust implementations. Meanwhile, efficient and robust methods for image segmentation and meshing are needed to provide realistic geometries for which numerical solutions are tractable. Here, we present a computational framework that models electrodiffusion in excitable cerebral tissue, together with realistic geometries generated from electron microscopy data. To demonstrate a possible application of the framework, we simulate electrodiffusive dynamics in cerebral tissue during neuronal activity. Our results and findings highlight the numerical and computational challenges associated with modeling and simulation of electrodiffusion and other multiphysics in dense reconstructions of cerebral tissue.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [50] [Weighted geodesic restrictions of arithmetic eigenfunctions](https://arxiv.org/abs/2512.03291)
*Jiaqi Hou,Xiaoqi Huang*

Main category: math.NT

TL;DR: Power saving over local bound for L² norm of Hecke-Maass forms on arithmetic hyperbolic surfaces with respect to measures on geodesic segments with dimension > 1/2, using arithmetic amplification.


<details>
  <summary>Details</summary>
Motivation: To improve upon the local bound established by Eswarathasan and Pramanik for the L² norm of Hecke-Maass forms with respect to measures on geodesic segments, and to extend similar results to general Riemannian manifolds.

Method: Apply the method of arithmetic amplification to obtain power saving over local bounds, and develop Kakeya-Nikodym bounds for Laplace-Beltrami eigenfunctions on general 2D Riemannian manifolds.

Result: Obtained power saving over local bound for arithmetic hyperbolic surfaces, and established Kakeya-Nikodym bounds for general 2D Riemannian manifolds for measures supported on geodesic segments with dimension > 1/2.

Conclusion: The arithmetic amplification method successfully yields improved bounds for L² norms of eigenfunctions with respect to measures on geodesic segments, extending previous results and providing new insights into eigenfunction concentration phenomena.

Abstract: Let $X$ be an arithmetic hyperbolic surface, $ψ$ a Hecke-Maass form, $\ell$ a geodesic segment on $X$, and $μ$ a Borel measure supported on $\ell$ with dimension greater than 1/2. We obtain a power saving over the local bound of Eswarathasan and Pramanik for the $L^2$ norm of $ψ$ with respect to $μ$, which is a weighted generalization of Marshall's geodesic restriction bound and is proved by applying the method of arithmetic amplification. On a general 2-dimensional Riemannian manifold, we also obtain a Kakeya-Nikodym bound for the $L^2$ norm of any Laplace-Beltrami eigenfunction with respect to a Borel measure supported on a geodesic segment with dimension greater than 1/2.

</details>


### [51] [Kakeya-Nikodym norms of Maass forms on $\rm{U}(2,1)$](https://arxiv.org/abs/2512.03482)
*Jiaqi Hou*

Main category: math.NT

TL;DR: Power savings for Kakeya-Nikodym norm of Hecke-Maass forms on compact arithmetic complex hyperbolic surfaces, leading to improved L^p bounds.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve bounds for Hecke-Maass forms on compact arithmetic complex hyperbolic surfaces. Specifically, it seeks to obtain power savings over trivial bounds for the Kakeya-Nikodym norm, which would then yield improved L^p bounds beyond the local Sogge bounds.

Method: The authors apply the amplification method to Hecke-Maass forms with large spectral parameters. This technique is used to obtain power savings over the trivial bound for the Kakeya-Nikodym norm of the form ψ.

Result: The main result is a power saving over the trivial bound for the Kakeya-Nikodym norm of ψ. As a consequence, the paper obtains power savings over the local bound of Sogge for ‖ψ‖_p when 2 < p < 10/3.

Conclusion: The amplification method successfully yields improved bounds for Hecke-Maass forms on compact arithmetic complex hyperbolic surfaces, providing power savings for both Kakeya-Nikodym norms and L^p norms in the range 2 < p < 10/3.

Abstract: Let $ψ$ be a Hecke-Maass form with a large spectral parameter on a compact arithmetic complex hyperbolic surface. We apply the amplification method to obtain a power saving over the trivial bound for the Kakeya-Nikodym norm of $ψ$. As a consequence, we obtain power savings over the local bound of Sogge for $\|ψ\|_p$ when $2<p<10/3$.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [52] [Computing Equilibrium Points of Electrostatic Potentials](https://arxiv.org/abs/2512.03249)
*Abheek Ghosh,Paul W. Goldberg,Alexandros Hollender*

Main category: cs.CC

TL;DR: Algorithm for finding approximate equilibrium points of electrostatic potentials using piecewise Taylor approximations with variable grid coarseness, achieving poly-logarithmic time but without guarantees of proximity to exact solutions.


<details>
  <summary>Details</summary>
Motivation: Electrostatic equilibrium points exist due to nonconstructive arguments but gradient descent fails due to singularities, creating a challenging optimization scenario that requires novel algorithmic approaches.

Method: Piecewise approximation of potential function by Taylor series with variable grid coarseness - exponentially smaller cells in rapidly changing regions, larger cells in slowly changing regions.

Result: Algorithm finds approximate equilibrium points in poly-logarithmic time under "strong non-degeneracy" assumption; problem shown to be CLS-hard and in PPAD, leaving precise complexity classification open.

Conclusion: The paper presents an efficient algorithm for approximate electrostatic equilibrium points under mild assumptions, while establishing the problem's complexity between CLS and PPAD as an open question.

Abstract: We study the computation of equilibrium points of electrostatic potentials: locations in space where the electrostatic force arising from a collection of charged particles vanishes. This is a novel scenario of optimization in which solutions are guaranteed to exist due to a nonconstructive argument, but gradient descent is unreliable due to the presence of singularities.
  We present an algorithm based on piecewise approximation of the potential function by Taylor series. The main insight is to divide the domain into a grid with variable coarseness, where grid cells are exponentially smaller in regions where the function changes rapidly compared to regions where it changes slowly. Our algorithm finds approximate equilibrium points in time poly-logarithmic in the approximation parameter, but these points are not guaranteed to be close to exact solutions. Nevertheless, we show that such points can be computed efficiently under a mild assumption that we call "strong non-degeneracy". We complement these algorithmic results by studying a generalization of this problem and showing that it is CLS-hard and in PPAD, leaving its precise classification as an intriguing open problem.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [53] [Generative Refinement:A New Paradigm for Determining Single Crystal Structures Directly from HKL Data](https://arxiv.org/abs/2512.03365)
*Wen-Lin Luo,Yi Yuan,Cheng-Hui Li,Yue Zhao,Jing-Lin Zuo*

Main category: cond-mat.mtrl-sci

TL;DR: RefrActor is an end-to-end deep learning framework that automates crystal structure refinement from X-ray diffraction data, eliminating the need for expert intervention and initial structural guesses.


<details>
  <summary>Details</summary>
Motivation: Current SC-XRD refinement requires significant expert intervention and subjective judgment, limiting accessibility and scalability of crystal structure determination.

Method: Combines a physics-informed reciprocal-space encoder (ReciEncoder) with a symmetry-aware diffusion-based generator (StruDiffuser) to produce refined atomic models directly from HKL data.

Result: Achieves low R1-factors on GenRef-10k benchmark across diverse systems, correctly resolves hydrogen positions, elemental assignments, and moderate disorder.

Conclusion: Establishes a new data-driven paradigm for autonomous crystallographic analysis, enabling fully automated, high-throughput crystal structure determination.

Abstract: Single-crystal X-ray diffraction (SC-XRD) is the gold standard technique to characterize crystal structures in solid state. Despite significant advances in automation for structure solution, the refinement stage still depends heavily on expert intervention and subjective judgment, limiting accessibility and scalability. Herein, we introduce RefrActor, an end-to-end deep learning framework that enables crystal structure determination directly from HKL data. By coupling a physics-informed reciprocal-space encoder (ReciEncoder) with a symmetry-aware diffusion-based generator (StruDiffuser), RefrActor produces fully refined atomic models without requiring initial structural guesses or manual input. Comprehensive evaluations on the GenRef-10k benchmark demonstrates that RefrActor achieves low R1-factors across diverse systems, including low-symmetry, light-atom, and heavy-atom crystals. Case studies further confirm that RefrActor can correctly resolve hydrogen positions, elemental assignments, and moderate disorder. This work establishes a new data-driven paradigm for autonomous crystallographic analysis, offering a foundation for fully automated, high-throughput crystal structure determination.

</details>


<div id='nucl-th'></div>

# nucl-th [[Back]](#toc)

### [54] [Generalized Beth--Uhlenbeck entropy formula from the $Φ-$derivable approach](https://arxiv.org/abs/2512.03876)
*David Blaschke,Gerd Röpke,Gordon Baym*

Main category: nucl-th

TL;DR: The paper derives a generalized Beth-Uhlenbeck formula for entropy in dense fermion systems with strong two-particle correlations, extending beyond low-density limits to include scattering states, bound states, Mott dissociation, and self-consistent back reaction.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive thermodynamic description of dense fermion systems with strong two-particle correlations that goes beyond the low-density limit, addressing limitations in existing approaches for systems like quark matter and nuclear matter.

Method: Using the Φ-derivable approach to the thermodynamic potential, deriving a generalized Beth-Uhlenbeck formula that expresses entropy as an energy-momentum integral over a statistical distribution function times a unique spectral density.

Result: The derived formula shows that in the near mass-shell limit, the spectral density reduces to a "squared Lorentzian" shape rather than a Lorentzian. The approach exactly relates to the Φ-derivable approach at the two-loop level and includes Mott dissociation of bound states according to Levinson's theorem.

Conclusion: The developed formalism successfully extends the Beth-Uhlenbeck approach beyond low-density limits, incorporating strong correlations, Mott dissociation, and self-consistent back reaction, with applications to complex systems like quark matter and nuclear matter.

Abstract: We derive a generalized Beth-Uhlenbeck formula for the entropy of a dense fermion system with strong two-particle correlations, including scattering states and bound states. We work within the $Φ-$derivable approach to the thermodynamic potential. The formula takes the form of an energy-momentum integral over a statistical distribution function times a unique spectral density. In the near mass-shell limit, the spectral density reduces, contrary to naïve expectations, not to a Lorentzian but rather to a "squared Lorentzian" shape. The relation of the Beth-Uhlenbeck formula to the $Φ$-derivable approach is exact at the two-loop level for $Φ$. The formalism we develop, which extends the Beth-Uhlenbeck approach beyond the low-density limit, includes Mott dissociation of bound states, in accordance with Levinson's theorem, and the self-consistent back reaction of correlations in the fermion propagation. We discuss applications to further systems, such as quark matter and nuclear matter.

</details>
