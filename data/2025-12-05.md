<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 11]
- [math.AP](#math.AP) [Total: 22]
- [physics.comp-ph](#physics.comp-ph) [Total: 9]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 5]
- [physics.optics](#physics.optics) [Total: 2]
- [gr-qc](#gr-qc) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [math.PR](#math.PR) [Total: 2]
- [math-ph](#math-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Iterative Contact-resolving Hybrid Methods for Multiscale Contact Mechanics](https://arxiv.org/abs/2512.04411)
*Eric T. Chung,Hyea Hyun Kim,Xiang Zhong*

Main category: math.NA

TL;DR: A hybrid iterative method for contact mechanics with high contrast coefficients using two-subdomain framework: nonlinear contact constraints localized in small subdomain, linear system in larger subdomain, with four discretization types combining standard/mixed FEM and multiscale methods.


<details>
  <summary>Details</summary>
Motivation: Contact mechanics with high contrast coefficients presents significant mathematical and computational challenges, especially for strongly symmetric stress approximations. Conventional monolithic approaches lead to high global complexity due to inherent nonlinearity of contact problems.

Method: Iterative contact-resolving hybrid method that localizes nonlinear contact constraints within a smaller subdomain while the larger subdomain is governed by a linear system. Uses variational inequality theory, minimization principles, and penalty methods. Proposes four discretization types: standard/mixed FEM across entire domain, and combinations of standard/mixed multiscale methods in larger subdomain with standard/mixed FEM in smaller subdomain.

Result: Method avoids excessive degrees of freedom in larger domain through multiscale reduction technique. Mixed formulation enables direct stress computation, ensures local momentum conservation, and resists locking in nearly incompressible materials. Convergence analysis and algorithms provided for all cases, validated through extensive numerical experiments.

Conclusion: The hybrid approach effectively addresses computational challenges in contact mechanics with high contrast coefficients by separating nonlinear and linear domains, reducing computational complexity while maintaining accuracy through various discretization strategies and multiscale techniques.

Abstract: Modeling contact mechanics with high contrast coefficients presents significant mathematical and computational challenges, especially in achieving strongly symmetric stress approximations. Due to the inherent nonlinearity of contact problems, conventional methods that treat the entire domain as a monolithic system often lead to high global complexity. To address this, we develop an iterative contact-resolving hybrid method by localizing nonlinear contact constraints within a smaller subdomain, while the larger subdomain is governed by a linear system. Our system employs variational inequality theory, minimization principles, and penalty methods. More importantly, we propose four discretization types within the two-subdomain framework, ranging from applying standard/mixed FEM across the entire domain to combining standard/mixed multiscale methods in the larger subdomain with standard/mixed FEM in the smaller one. % The standard finite element method and standard constraint energy minimizing generalized multiscale finite element method are simple and easy to demonstrate. By employing a multiscale reduction technique, the method avoids excessive degrees of freedom inherent in conventional methods in the larger domain, while the mixed formulation enables direct stress computation, ensures local momentum conservation, and resists locking in nearly incompressible materials. Convergence analysis and the corresponding algorithms are provided for all cases. Extensive numerical experiments are presented to validate the effectiveness of the approaches.

</details>


### [2] [Stable self-adaptive timestepping for Reduced Order Models for incompressible flows](https://arxiv.org/abs/2512.04592)
*Josep Plana-Riu,Henrik Rosenberger,Benjamin Sanderse,F. Xavier Trias*

Main category: math.NA

TL;DR: RedEigCD is the first self-adaptive timestepping method for ROMs of incompressible Navier-Stokes equations that uses linear stability theory and exact spectral information to adapt timesteps, proving ROMs can use larger stable timesteps than FOMs.


<details>
  <summary>Details</summary>
Motivation: Traditional error-based adaptive methods for reduced-order models (ROMs) of incompressible Navier-Stokes equations lack efficiency. There's a need for self-adaptive timestepping that preserves online efficiency while ensuring stability, leveraging the reduced scale of ROMs for better performance.

Method: RedEigCD adapts timesteps by bounding the stability function of time integration schemes using exact spectral information from reduced operators. It computes eigenbounds of convective and diffusive ROM operators, which is feasible at reduced scale and maintains online efficiency. The method is based on linear stability concepts and uses theorems of Bendixson and Rao.

Result: Theoretical proof shows maximum stable timestep for projection-based ROMs is larger than or equal to that of corresponding full-order models (FOMs). Numerical experiments demonstrate stable timestep increases up to 40x compared to FOMs without accuracy loss, for both periodic and non-homogeneous boundary conditions.

Conclusion: RedEigCD establishes a new connection between linear stability theory and reduced-order modeling, providing a systematic approach for efficient, self-regulating ROM integration in incompressible flow simulations with significant performance improvements.

Abstract: This work introduces RedEigCD, the first self-adaptive timestepping technique specifically tailored for reduced-order models (ROMs) of the incompressible Navier-Stokes equations. Building upon linear stability concepts, the method adapts the timestep by directly bounding the stability function of the employed time integration scheme using exact spectral information of matrices related to the reduced operators. Unlike traditional error-based adaptive methods, RedEigCD relies on the eigenbounds of the convective and diffusive ROM operators, whose computation is feasible at reduced scale and fully preserves the online efficiency of the ROM. A central theoretical contribution of this work is the proof, based on the combined theorems of Bendixson and Rao, that, under linearized assumptions, the maximum stable timestep for projection-based ROMs is shown to be larger than or equal to that of their corresponding full-order models (FOMs). Numerical experiments for both periodic and non-homogeneous boundary conditions demonstrate that RedEigCD yields stable timestep increases up to a factor 40 compared to the FOM, without compromising accuracy. The methodology thus establishes a new link between linear stability theory and reduced-order modeling, offering a systematic path towards efficient, self-regulating ROM integration in incompressible flow simulations.

</details>


### [3] [Interface layers and coupling conditions for discrete kinetic models on networks: a spectral approac](https://arxiv.org/abs/2512.04634)
*Raul Borsche,Tobias Damm,Axel Klar,Yizhou Zhou*

Main category: math.NA

TL;DR: The paper develops coupling conditions for macroscopic wave equations on networks from kinetic BGK models using asymptotic analysis and half-space problems, with analytical results for discrete velocity models and an efficient spectral solution method.


<details>
  <summary>Details</summary>
Motivation: To derive coupling conditions for macroscopic equations on networks from underlying kinetic models, enabling accurate modeling of wave propagation in networked systems where kinetic effects are important.

Method: Uses linear kinetic BGK models where wave equations emerge as small Knudsen number limits. Derives coupling conditions via asymptotic analysis near network nodes and coupled kinetic half-space problems. Develops analytical results for discrete velocity versions and an efficient spectral method to solve these half-space problems.

Result: Obtains coupling conditions for macroscopic equations from kinetic network problems, determining coefficients analogous to extrapolation length in kinetic boundary problems. Numerical results demonstrate accuracy and fast convergence of the spectral method, with comparisons showing agreement between kinetic and macroscopic solutions.

Conclusion: The approach successfully bridges kinetic and macroscopic modeling on networks, providing rigorous coupling conditions and efficient computational methods for wave propagation problems in networked systems with kinetic effects.

Abstract: We consider kinetic and related macroscopic equations on networks. A class of linear kinetic BGK models is considered, where the limit equation for small Knudsen numbers is given by the wave equation. Coupling conditions for the macroscopic equations are obtained from the kinetic coupling conditions via an asymptotic analysis near the nodes of the network and the consideration of coupled solutions of kinetic half-space problems. Analytical results are obtained for a discrete velocity version of the coupled half-space problems. Moreover, an efficient spectral method is developed to solve the coupled discrete velocity half-space problems. In particular, this allows to determine the relevant coefficients in the coupling conditions for the macroscopic equations
  from the underlying kinetic network problem. These coefficients correspond to the so-called extrapolation length for kinetic boundary value problems. Numerical results show the accuracy and fast convergence of the approach. Moreover, a comparison of the kinetic solution on the network with the macroscopic solution is presented.

</details>


### [4] [Weighted total variation regularization for inverse problems with significant null spaces](https://arxiv.org/abs/2512.04729)
*Martin Burger,Ole Løseth Elvetun,Bjørn Fredrik Nielsen*

Main category: math.NA

TL;DR: Weighted TV regularization improves source localization in inverse problems with large null spaces, recovering piecewise constant sources away from boundaries better than standard methods.


<details>
  <summary>Details</summary>
Motivation: Standard regularization methods for inverse problems with large null spaces (like inverse ECG/EEG) produce solutions near the orthogonal complement of the forward operator's null space, leading to inaccurate source localization where internal sources appear near data acquisition sites.

Method: Extends previous weighting schemes to total variation (TV) regularization, introducing weighted TV-regularization method with supporting analysis. Also explores hybrid weighted-sparsity and TV regularization approach.

Result: Weighted TV regularization successfully recovers location and size of large, piecewise constant sources away from boundaries (though not exact shape). Hybrid approach captures both small and large sources but with more blurred reconstructions than weighted TV alone.

Conclusion: Weighted TV regularization addresses limitations of standard methods for inverse problems with large null spaces, improving source localization for applications like ECG/EEG, with hybrid approaches offering additional benefits for capturing sources of different sizes.

Abstract: We consider inverse problems with large null spaces, which arise in important applications such as in inverse ECG and EEG procedures. Standard regularization methods typically produce solutions in or near the orthogonal complement of the forward operator's null space. This often leads to inadequate results, where internal sources are mistakenly interpreted as being near the data acquisition sites -- e.g., near or at the body surface in connection with EEG and ECG recordings.
  To mitigate this, we previously proposed weighting schemes for Tikhonov and sparsity regularization. Here, we extend this approach to total variation (TV) regularization, which is particularly suited for identifying spatially extended regions with approximately constant values. We introduce a weighted TV-regularization method, provide supporting analysis, and demonstrate its performance through numerical experiments. Unlike standard TV regularization, the weighted version successfully recovers the location and size of large, piecewise constant sources away from the boundary, though not their exact shape.
  Additionally, we explore a hybrid weighted-sparsity and TV regularization approach, which better captures both small and large sources, albeit with somewhat more blurred reconstructions than the weighted TV method alone.

</details>


### [5] [Recent advances in the numerical solution of multi-order fractional differential equations](https://arxiv.org/abs/2512.04737)
*Luigi Brugnano,Gianmarco Gurioli,Felice Iavernaro,Mikk Vikerpuur*

Main category: math.NA

TL;DR: Extension of Fractional HBVMs to solve multi-order fractional differential equations with different fractional derivative orders, with Matlab code implementation.


<details>
  <summary>Details</summary>
Motivation: Existing FHBVMs only handle systems with the same fractional derivative order, but multi-order problems (with different fractional orders) are important in applications and haven't been addressed yet.

Method: Extends Fractional HBVMs (FHBVMs) to handle fractional multi-order problems, providing full details for the approach. Specifically handles the case of two different fractional orders with corresponding Matlab code implementation.

Result: The proposed extension proves very effective for numerically solving multi-order fractional differential equations. The Matlab code is made available and shows competitive performance.

Conclusion: Successfully extends FHBVMs to solve multi-order fractional differential problems, addressing an important gap in applications where different fractional derivative orders occur simultaneously.

Abstract: The efficient numerical solution of fractional differential equations has been recently tackled through the definition of Fractional HBVMs (FHBVMs), a class of Runge-Kutta type methods. Corresponding Matlab (c) codes have been also made available on the internet, proving to be very competitive w.r.t. existing ones. However, so far, FHBVMs have been given for solving systems of fractional differential equations with the same order of fractional derivative, whereas the numerical solution of multi-order problems (i.e., problems in which different orders of fractional derivatives occur) has not been handled, yet. Due to their relevance in applications, in this paper we propose an extension of FHBVMs for addressing fractional multi-order problems, providing full details for such an approach. A corresponding Matlab (c) code, handling the case of two different fractional orders, is also made available, proving very effective for numerically solving these problems.

</details>


### [6] [Construction of the Nearest Nonnegative Hankel Matrix for a Prescribed Eigenpair](https://arxiv.org/abs/2512.04812)
*Prince Kanhya,Udit Raj*

Main category: math.NA

TL;DR: Computes minimal structured perturbations to make a prescribed eigenpair exact for nonnegative Hankel matrices, using optimization to find either exact solutions or nearest approximations.


<details>
  <summary>Details</summary>
Motivation: Need to assess eigenpair sensitivity under structured perturbations that preserve Hankel structure and nonnegativity constraints, which is important for applications where these properties must be maintained.

Method: Formulates as feasibility check of linear constraints encoding Hankel structure and nonnegativity. When feasible, computes minimum-norm perturbation; when infeasible, minimizes residual norm to find nearest nonnegative Hankel matrix.

Result: Provides numerical optimization framework for evaluating eigenpair sensitivity under nonnegativity-preserving Hankel perturbations, with examples demonstrating both feasible and infeasible cases.

Conclusion: The method offers a practical computational approach for structured backward error analysis when closed-form solutions are unavailable, handling both exact feasibility and nearest approximation scenarios.

Abstract: We study the problem of determining whether a prescribed eigenpair $(λ,x)$
  can be made an exact eigenpair of a nonnegative Hankel matrix through the smallest
  possible structured perturbation. The task reduces to check the feasibility of a
  set of linear constraints that encode both the Hankel structure and entrywise
  nonnegativity. When the feasibility set is nonempty, we compute the minimum-norm
  perturbation $ΔH$ such that $(H+ΔH)x=λx$. When no such perturbation
  exists, we compute the nearest nonnegative Hankel matrix in a residual sense by
  minimizing $\|(H+ΔH)x-λx\|_{2}$ subject to the imposed constraints.
  Because closed-form formulas for the structured backward error are generally
  unavailable, our method provides a fully numerical and optimization-based
  framework for evaluating eigenpair sensitivity under nonnegativity-preserving
  Hankel perturbations. Numerical examples illustrate both feasible and infeasible cases.

</details>


### [7] [Hierarchical matrix approximability of inverse of convection dominated finite element matrices](https://arxiv.org/abs/2512.04824)
*Arthur Saunier,Leo Agelas,Ani Anciaux Sedrakian,Ibtihel Ben Gharbia,Xavier Claeys*

Main category: math.NA

TL;DR: A novel partitioning strategy using "convection tubes" is proposed for hierarchical matrix compression of convection-dominated PDEs on unstructured grids, overcoming limitations of previous methods that required structured grids or constant convection fields.


<details>
  <summary>Details</summary>
Motivation: Hierarchical matrices (H-matrices) are effective for compressing large matrices from discretized PDEs, but their performance degrades for convection-dominated problems due to loss of coercivity. Existing methods only work for structured grids with constant convection, limiting practical applications.

Method: The paper introduces a "convection tubes" partitioning strategy that clusters elements aligned with the convection vector field. This approach doesn't require structured grids or constant convection and is based on a Péclet-robust Caccioppoli inequality for handling convection-dominated problems.

Result: Theoretical analysis and numerical experiments demonstrate the efficiency and robustness of the convection tubes method for convection-dominated PDEs on unstructured grids, overcoming limitations of previous approaches.

Conclusion: The proposed convection tubes partitioning strategy successfully extends hierarchical matrix techniques to convection-dominated problems on unstructured grids with general convection fields, providing a more flexible and practical framework than previous methods.

Abstract: Several researchers have developed a rich toolbox of matrix compression techniques that exploit structure and redundancy in large matrices. Classical methods such as the block low-rank format and the Fast Multipole Method make it possible to manipulate very large systems by representing them in a reduced form. Among the most sophisticated tools in this area are hierarchical matrices (H-matrices), which exploit local properties of the underlying kernel or operator to approximate matrix blocks by low-rank factors, organized in a recursive hierarchy. H-matrices offer a flexible and scalable framework, yielding nearly linear complexity in both storage and computation. Hierarchical matrix techniques, originally developed for boundary integral equations, have recently been applied to matrices stemming from the discretization of advection-dominated problems. However, their effectiveness is limited by the loss of coercivity induced by convection phenomena, where traditional methods fail. Initial work by Le Borne addressed this by modifying the admissibility criterion for structured grids with constant convection, but challenges remain for more general grids and advection fields. In this work, we propose a novel partitioning strategy based on "convection tubes", clusters aligned with the convection vector field. This method does not require a structured grid or constant convection, overcoming the limitations of previous approaches. We present both theoretical analyses and numerical experiments, that demonstrate the efficiency and robustness of our method for convection-dominated PDEs on unstructured grids. The approach builds on a Péclet-robust Caccioppoli inequality, crucial for handling convection-dominated problems.

</details>


### [8] [A High-Order Discretization Scheme for Surface Integral Equations for Analyzing the Electroencephalography Forward Problem](https://arxiv.org/abs/2512.04845)
*Rui Chen,Viviana Giunzioni,Adrien Merlini,Francesco P. Andriulli*

Main category: math.NA

TL;DR: A high-order Nyström discretization scheme for EEG forward problem using surface integral equations with flexible basis function order and simplified quadrature integration.


<details>
  <summary>Details</summary>
Motivation: To develop a more flexible and efficient high-order discretization scheme for EEG forward problem analysis that allows independent control of basis function order without mesh regeneration and simplifies numerical integration.

Method: Uses Nyström-based high-order discretization with separate interpolation points from mesh nodes, employs quadrature rules for integration, extends to multiple SIE formulations (double-layer, adjoint double-layer, isolated-skull-approach, indirect adjoint double-layer) with specialized singularity treatments.

Result: Numerical experiments demonstrate the scheme's accuracy, flexibility, and efficiency for all four surface integral equations in EEG forward problem analysis.

Conclusion: The proposed high-order Nyström scheme provides an accurate, flexible, and efficient approach for EEG forward problem analysis with advantages over existing isoparametric methods.

Abstract: A Nystrom-based high-order (HO) discretization scheme for surface integral equations (SIEs) for analyzing the electroencephalography (EEG) forward problem is proposed in this work. We use HO surface elements and interpolation functions for the discretization of the interfaces of the head volume and the unknowns on the elements, respectively. The advantage of this work over existing isoparametric HO discretization schemes resides in the fact that the interpolation points are different from the mesh nodes, allowing for the flexible manipulation of the order of the basis functions without regenerating the mesh of the interfaces. Moreover, the interpolation points are chosen from the quadrature rules with the same number of points on the elements simplifying the numerical computation of the surface integrals for the far-interaction case. In this contribution, we extend the implementation of the HO discretization scheme to the double-layer and the adjoint double-layer formulations, as well as to the isolated-skull-approach for the double-layer formulation and to the indirect adjoint double-layer formulation, employed to improve the solution accuracy in case of high conductivity contrast models, which requires the development of different techniques for the singularity treatment. Numerical experiments are presented to demonstrate the accuracy, flexibility, and efficiency of the proposed scheme for the four SIEs for analyzing the EEG forward problem.

</details>


### [9] [Data-driven Methods for Delay Differential Equations](https://arxiv.org/abs/2512.04894)
*Dimitri Breda,Xunbi A. Ji,Gábor Orosz,Muhammad Tanveer*

Main category: math.NA

TL;DR: Extends SINDy algorithm and introduces neural networks for data-driven identification of delay differential equations with unknown delays.


<details>
  <summary>Details</summary>
Motivation: Data-driven methods are expanding beyond traditional fields, but existing approaches like SINDy were originally developed for ODEs, not delay differential equations (DDEs) which are common in real-world systems with time delays.

Method: Two SINDy extensions: 1) direct approach for DDEs, 2) pseudospectral collocation to approximate DDEs as ODEs. Also introduces neural delay differential equations (NDDEs) with trainable delays using neural networks in continuous time.

Result: Methods tested on classical systems (delay logistic, Mackey-Glass, delayed Rössler) with MATLAB implementations provided. Approaches compared and connections between them analyzed.

Conclusion: Provides effective data-driven methods for identifying DDEs with unknown delays, with insights on connections between approaches and future directions for time delay systems.

Abstract: Data-driven methodologies are nowadays ubiquitous. Their rapid development and spread have led to applications even beyond the traditional fields of science. As far as dynamical systems and differential equations are concerned, neural networks and sparse identification tools have emerged as powerful approaches to recover the governing equations from available temporal data series. In this chapter we first illustrate possible extensions of the sparse identification of nonlinear dynamics (SINDy) algorithm, originally developed for ordinary differential equations (ODEs), to delay differential equations (DDEs) with discrete, possibly multiple and unknown delays. Two methods are presented for SINDy, one directly tackles the underlying DDE and the other acts on the system of ODEs approximating the DDE through pseudospectral collocation. We also introduce another way of capturing the dynamics of DDEs using neural networks and trainable delays in continuous time, and present the training algorithms developed for these neural delay differential equations (NDDEs). The relevant MATLAB implementations for both the SINDy approach and for the NDDE approach are provided. These approaches are tested on several examples, including classical systems such as the delay logistic and the Mackey-Glass equation, and directly compared to each other on the delayed Rössler system. We provide insights on the connection between the approaches and future directions on developing data-driven methods for time delay systems.

</details>


### [10] [Weak convergence rates for spectral regularization via sampling inequalities](https://arxiv.org/abs/2512.04929)
*Sabrina Guastavino,Gabriele Santin,Francesco Marchetti,Federico Benvenuto*

Main category: math.NA

TL;DR: The paper establishes weak convergence rate bounds for inverse problems using spectral regularization methods without requiring source conditions, by connecting inverse problems with kernel approximation through generalized sampling inequalities.


<details>
  <summary>Details</summary>
Motivation: Classical convergence rate analysis for inverse problems relies on source conditions to estimate truncation error. The authors aim to develop convergence rate bounds that don't depend on source conditions by leveraging connections between inverse problems and kernel approximation.

Method: Generalize sampling inequalities to spectral regularization methods, then exploit the connection between inverse problems and kernel approximation to derive weak convergence rate bounds. Analysis covers cases where the forward operator is compact and uniformly bounded, or the kernel operator is of trace class.

Result: Established weak convergence rate bounds for inverse problems that are independent of source conditions. Demonstrated that truncation error in Tikhonov regularization can be characterized entirely through sampling inequalities without invoking source conditions.

Conclusion: The paper provides a novel approach to analyzing convergence rates in spectral regularization by eliminating the need for source conditions, offering a more general framework for convergence analysis in inverse problems through kernel approximation techniques.

Abstract: Convergence rates in spectral regularization methods quantify the approximation error in inverse problems as a function of the noise level or the number of sampling points. Classical strong convergence rate results typically rely on source conditions, which are essential for estimating the truncation error. However, in the framework of kernel approximation, the truncation error in the case of Tikhonov regularization can be characterized entirely through sampling inequalities, without invoking source conditions. In this paper, we first generalize sampling inequalities to spectral regularization, and then, by exploiting the connection between inverse problems and kernel approximation, we derive weak convergence rate bounds for inverse problems, independently of source conditions. These weak convergence rates are established and analyzed when the forward operator is compact and uniformly bounded, or the kernel operator is of trace class.

</details>


### [11] [A tangential low-rank ADI method for solving indefinite Lyapunov equations](https://arxiv.org/abs/2512.04983)
*Rudi Smith,Steffen W. R. Werner*

Main category: math.NA

TL;DR: A novel tangential reformulation of ADI iteration enables efficient low-rank approximations for Lyapunov equations with indefinite right-hand sides, even for high-rank constant terms.


<details>
  <summary>Details</summary>
Motivation: Classical block-type approaches for solving large-scale Lyapunov equations become computationally expensive when the rank of the constant term grows, especially for indefinite right-hand sides.

Method: Proposes a tangential reformulation of the ADI iteration with adaptive selection of ADI parameters (shifts and tangential directions) for efficient low-rank approximations.

Result: Developed algorithms effectively handle Lyapunov equations with indefinite right-hand sides even with higher-rank constant terms, as demonstrated by numerical examples.

Conclusion: The tangential ADI approach provides an efficient computational method for large-scale Lyapunov equations with indefinite right-hand sides, overcoming limitations of classical block methods.

Abstract: Continuous-time algebraic Lyapunov equations have become an essential tool in various applications. In the case of large-scale sparse coefficient matrices and indefinite constant terms, indefinite low-rank factorizations have successfully been used to allow methods like the alternating direction implicit (ADI) iteration to efficiently compute accurate approximations to the solution of the Lyapunov equation. However, classical block-type approaches quickly increase in computational costs when the rank of the constant term grows. In this paper, we propose a novel tangential reformulation of the ADI iteration that allows for the efficient construction of low-rank approximations to the solution of Lyapunov equations with indefinite right-hand sides even in the case of constant terms with higher ranks. We provide adaptive methods for the selection of the corresponding ADI parameters, namely shifts and tangential directions, which allow for the automatic application of the method to any relevant problem setting. The effectiveness of the developed algorithms is illustrated by several numerical examples.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [12] [Regularity for minimizers of degenerate, non-autonomous, orthotropic integral functionals](https://arxiv.org/abs/2512.04281)
*Antonio Giuseppe Grimaldi,Stefania Russo*

Main category: math.AP

TL;DR: The paper proves higher differentiability (integer order) for locally bounded minimizers of anisotropic, non-autonomous integral functionals with variable exponents and coefficients.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend regularity theory to more general anisotropic functionals that are non-autonomous (depend on x) and anisotropic (different exponents p_i for different directions), which are more realistic models in applications like elasticity and materials science.

Method: The authors use techniques from calculus of variations and regularity theory, likely employing difference quotient methods and energy estimates to establish higher differentiability properties under suitable Sobolev regularity assumptions on the coefficients a_i(x).

Result: The main result proves that locally bounded minimizers of the given anisotropic functional possess higher differentiability of integer order, extending classical regularity results to this more general setting.

Conclusion: The paper successfully establishes higher regularity for minimizers of anisotropic, non-autonomous functionals, providing important theoretical foundations for studying such variational problems that appear in various applications.

Abstract: We prove the higher differentiability of integer order of locally bounded minimizers of integral functionals of the form \begin{equation*}
  \mathcal{F}(u,Ω):= \,\sum_{i=1}^{n} \dfrac{1}{p_i}\displaystyle \int_Ω\, a_i(x) \lvert u_{x_i} \rvert^{p_i} dx- \int_Ωω(x)u(x) dx, \end{equation*} where the exponents $ p_i \geq 2 $ and the coefficients $ a_i(x) $ satisfy a suitable Sobolev regularity. The main novelty consists in dealing with non-autonomous, anisotropic functionals, which depend also on the solution.

</details>


### [13] [Diffusive limit of the Boltzmann equation around Rayleigh profile in the half space](https://arxiv.org/abs/2512.04403)
*Hongxu Chen,Renjun Duan*

Main category: math.AP

TL;DR: The paper analyzes the diffusive limit of the Boltzmann equation in half-space with moving boundary, connecting it to the Navier-Stokes Rayleigh profile solution.


<details>
  <summary>Details</summary>
Motivation: To rigorously connect the Boltzmann equation with small Knudsen number to its fluid dynamic limit (Navier-Stokes equations) in half-space with moving boundary conditions, specifically analyzing the Rayleigh profile solution that accounts for boundary motion effects.

Method: Uses Hilbert expansion method to construct Boltzmann solutions around the Rayleigh profile for well-prepared initial data, working in the half-space domain with diffuse reflection boundary conditions involving tangent velocity proportional to Knudsen number.

Result: Successfully constructs Boltzmann solutions around the Rayleigh profile without initial singularity over any finite time interval, establishing the connection between Boltzmann equation and Navier-Stokes equations in this specific boundary configuration.

Conclusion: The paper provides rigorous justification of the diffusive limit from Boltzmann to Navier-Stokes equations in half-space with moving boundary, specifically capturing the Rayleigh profile solution that emerges from the boundary motion effects.

Abstract: This paper concerns the diffusive limit of the time evolutionary Boltzmann equation in the half space $\mathbb{T}^2\times\mathbb{R}^+$ for a small Knudsen number $\varepsilon>0$. For boundary conditions in the normal direction, it involves diffuse reflection moving with a tangent velocity proportional to $\varepsilon$ on the wall, whereas the far field is described by a global Maxwellian with zero bulk velocity. The incompressible Navier-Stokes equations, as the corresponding formal fluid dynamic limit, admit a specific time-dependent shearing solution known as the Rayleigh profile, which accounts for the effect of the tangentially moving boundary on the flow at rest in the far field. Using the Hilbert expansion method, for well-prepared initial data we construct the Boltzmann solution around the Rayleigh profile without initial singularity over any finite time interval.

</details>


### [14] [A note on lifespan estimates for higher-order parabolic equations](https://arxiv.org/abs/2512.04428)
*Nurdaulet N. Tobakhanov,Berikbol T. Torebek*

Main category: math.AP

TL;DR: This paper investigates the lifespan of solutions to a higher-order semilinear parabolic equation, deriving precise asymptotic bounds that refine previous results by obtaining both upper and lower bounds under improved initial data assumptions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the precise asymptotic behavior of the lifespan of nontrivial solutions to higher-order semilinear parabolic equations, particularly refining and extending earlier results that only provided upper bounds under restrictive initial data assumptions.

Method: The authors combine the test function method with semigroup estimates to derive both upper and lower bounds for the lifespan of solutions. They work under L¹∩L∞ initial data assumptions rather than the slowly decaying data used in previous works.

Result: The main result establishes precise asymptotic bounds for the lifespan T_ε: for 1 < p < p_Fuj, T_ε ≃ ε^{-(1/(p-1) - n/(2m))^{-1}}, and for p = p_Fuj, T_ε ≃ exp(ε^{-(p-1)}), where p_Fuj = 1 + 2m/n is the Fujita critical exponent.

Conclusion: The paper successfully refines and extends previous results by obtaining both upper and lower lifespan bounds under improved initial data conditions, providing a complete picture of the asymptotic behavior of solutions to higher-order semilinear parabolic equations near the Fujita critical exponent.

Abstract: We investigate the lifespan of solutions to the higher-order semilinear parabolic equation $$u_t+(-Δ)^m u=|u|^p, \quad x \in \mathbb{R}^n, t>0 $$ with initial data. We focus on the precise asymptotic behavior of the lifespan of nontrivial solutions. By combining the test function method and semigroup estimates, we derive both upper and lower bounds for the lifespan of solutions $$T_{\varepsilon} \simeq \left\{\begin{array}{l}\varepsilon^{-\left(\frac{1}{p-1}-\frac{n}{2m}\right)^{-1}}, \,\, 1<p<p_{\text {Fuj}}, \\ \exp\left(\varepsilon^{-(p-1)}\right), \,\, p=p_{\text {Fuj}},\end{array}\right.$$ where $p_{Fuj}=1+\frac{2m}{n}$ is the critical exponent of Fujita. These estimates refine and extend the earlier results of Caristi-Mitidieri [J. Math. Anal. Appl., 279:2 (2003), 710-722] and Sun [Electron. J. Differential Equations, 17 (2010)], who obtained only upper bounds under slowly decaying initial data assumptions. In our setting, the above condition on the initial data is replaced by the assumption $L^1\cap L^\infty$, which sharpens the results of the aforementioned works.

</details>


### [15] [Irreversibility condition and stability of equilibria in the inverse-deformation approach to fracture](https://arxiv.org/abs/2512.04479)
*Arnav Gupta*

Main category: math.AP

TL;DR: The paper derives an irreversibility condition for fracture using the inverse-deformation approach and second law of thermodynamics, showing that changes in crack location violate entropy production, leading to stability conditions for broken equilibria.


<details>
  <summary>Details</summary>
Motivation: To establish a thermodynamic foundation for fracture irreversibility using the inverse-deformation approach, addressing limitations in previous work (Rosakis et al 2021) and ensuring physically consistent constraints on crack evolution.

Method: Uses second law of thermodynamics to derive irreversibility condition, incorporates inequality constraint for nonnegative inverse strain, analyzes brittle failure in elastic bar, proves necessary and sufficient conditions for local stability with restricted admissible variations.

Result: Derived irreversibility condition from entropy production constraints, showed third derivative discontinuity at crack faces despite surface energy, proved stability conditions incorporating irreversibility, numerically verified all broken equilibria from previous work are locally stable.

Conclusion: The second law provides fundamental irreversibility condition in fracture mechanics, restricting admissible crack variations and ensuring physically consistent stability criteria, with all previously identified broken equilibria satisfying these thermodynamic constraints.

Abstract: We derive the irreversibility condition in fracture for the inverse-deformation approach using the second law of thermodynamics. We consider the problem of brittle failure in an elastic bar previously solved in (Rosakis et al 2021). Despite the presence of a non-zero interfacial/surface energy, the third derivative of the inverse-deformation map is discontinuous at the crack faces. This is due to the presence of the inequality constraint ensuring the inverse strain is nonnegative and the orientation of matter is preserved. A change in the material location of a crack results in negative entropy production, violating the second law. Consequently, such changes are disallowed giving the irreversibility condition. The inequality constraint and the irreversibility condition limit the space of admissible variations. We prove necessary and sufficient conditions for local stability that incorporate these restrictions. Their numerical implementation shows that all broken equilibria found in (Rosakis et al 2021) are locally stable.

</details>


### [16] [Parabolic problems whose Fujita critical exponent is not given by scaling](https://arxiv.org/abs/2512.04506)
*Ahmad Z. Fino,Berikbol T. Torebek*

Main category: math.AP

TL;DR: The paper studies a fractional heat equation with Riesz potential nonlinearity, identifies a Fujita-type critical exponent for global existence vs. blow-up, and extends results to more general convolution operators.


<details>
  <summary>Details</summary>
Motivation: To investigate the global behavior of solutions to fractional heat equations with nonlocal nonlinearities involving Riesz potentials, addressing a hypothesis by Mitidieri and Pohozaev about existence/nonexistence thresholds.

Method: Introduces Fujita-type critical exponent p_Fuj(n,β,α) = 1 + (β+α)/(n-α). Uses nonlinear capacity method for blow-up proofs and fixed-point argument with Hardy-Littlewood-Sobolev inequality for global existence.

Result: Establishes that p > p_Fuj yields global existence for small initial data, while p ≤ p_Fuj leads to finite-time blow-up. Extends results to general convolution operators, confirming Mitidieri-Pohozaev's hypothesis.

Conclusion: The critical Fujita exponent emerges unconventionally (not from usual scaling), characterizing solution behavior. Results generalize previous work and provide complete existence/nonexistence theory for this class of nonlocal PDEs.

Abstract: This paper investigates the (fractional) heat equation with a nonlocal nonlinearity involving a Riesz potential: \begin{equation*} u_{t}+(-Δ)^{\fracβ{2}} u= I_α(|u|^{p}),\qquad x\in \mathbb{R}^n,\,\,\,t>0, \end{equation*} where $α\in(0,n)$, $β\in(0,2]$, $n\geq1$, $p>1.$ We introduce the Fujita-type critical exponent $p_{\mathrm{Fuj}}(n,β,α)=1+(β+α)/(n-α)$, which characterizes the global behavior of solutions: global existence for small initial data when $p>p_{\mathrm{Fuj}}(n,β,α),$ and finite-time blow-up when $p\leq p_{\mathrm{Fuj}}(n,β,α)$.
  It is remarkable that the critical Fujita exponent is not determined by the usual scaling argument that yields $p_{sc}=1+(β+α)/n$, but instead arises in an unconventional manner, similar to the results of Cazenave et al. [Nonlinear Analysis, 68 (2008), 862-874] for the heat equation with a nonlocal nonlinearity of the form $\int_0^t(t-s)^{-γ}|u(s)|^{p-1}u(s)ds,\,0\leq γ<1.$
  The result on global existence for $p>p_{\mathrm{Fuj}}(n,2,α),$ provides a positive answer to the hypothesis proposed by Mitidieri and Pohozaev in [Proc. Steklov Inst. Math., 248 (2005) 164-185]. We further establish global nonexistence results for the above heat equation, where the Riesz potential term $I_α(|u|^{p})$ is replaced by a more general convolution operator $(\mathcal{K}\ast |u|^p),\,\mathcal{K}\in L^1_{loc}$, thereby extending the Mitidieri-Pohozaev's results established in the aforementioned work.
  Proofs of the blow-up results are obtained using a nonlinear capacity method specifically adapted to the structure of the problem, while global existence is established via a fixed-point argument combined with the Hardy-Littlewood-Sobolev inequality.

</details>


### [17] [Sharp stability on the second Robin eigenvalue with negative boundary parameters](https://arxiv.org/abs/2512.04584)
*Zhijie Chen,Zhen Song,Wenming Zou*

Main category: math.AP

TL;DR: Quantitative refinement of isoperimetric inequality for second Robin eigenvalue with negative boundary parameters, proving stability estimate when parameter is near zero, and showing sharpness of exponent for Fraenkel asymmetry.


<details>
  <summary>Details</summary>
Motivation: To refine and quantify the isoperimetric type inequality for the second Robin eigenvalue with negative boundary parameters previously established by Freitas and Laugesen, providing a stability estimate when the boundary parameter is close to zero.

Method: Proving a quantitative refinement of the inequality, constructing a suitable family of nearly spherical domains to demonstrate the sharpness of the exponent for Fraenkel asymmetry in the quantitative inequality.

Result: Established a new stability estimate for the second Robin eigenvalue with negative boundary parameters when the parameter is not too far from 0, and proved that the exponent for Fraenkel asymmetry in this quantitative inequality is sharp.

Conclusion: The paper successfully provides a quantitative refinement of the isoperimetric inequality for Robin eigenvalues, with sharp exponent for Fraenkel asymmetry when boundary parameters are near zero, extending previous work by Freitas and Laugesen.

Abstract: In this paper, we prove a quantitative refinement of the isoperimetric type inequality for the second Robin eigenvalue with negative boundary parameters established by Freitas and Laugesen [Amer.J.Math.143 (2021), no.3, 969-994].Such new stability estimate is proved when the boundary parameter is not too far from 0.By constructing a suitable family of nearly spherical domains, we prove that the exponent for the Fraenkel asymmetry in this quantitative type inequality is sharp.

</details>


### [18] [Critical concave-convex problems in Carnot group](https://arxiv.org/abs/2512.04640)
*Mattia Galeotti,Eugenio Vecchi*

Main category: math.AP

TL;DR: The paper proves existence of two positive solutions for a Dirichlet problem with concave-convex and critical nonlinearity in Carnot groups, extending Ambrosetti-Brezis-Cerami type results to sub-Riemannian settings.


<details>
  <summary>Details</summary>
Motivation: To extend classical Ambrosetti-Brezis-Cerami results on multiple positive solutions for concave-convex nonlinearities to Carnot groups, which are important in sub-Riemannian geometry and analysis on stratified Lie groups.

Method: Uses variational Perron method combined with careful estimates of minimizers of relevant Sobolev inequalities. Addresses challenges from lack of boundary regularity and proves first solution is local minimizer in proper topology.

Result: Proves existence of two positive solutions for the Dirichlet problem with concave-convex and critical nonlinearity in Carnot groups.

Conclusion: Successfully extends classical Euclidean results to Carnot group setting despite challenges from boundary regularity, establishing existence of multiple positive solutions through variational methods.

Abstract: We consider a model Dirichlet problem with concave-convex and critical nonlinearity settled in Carnot groups. Our aim is to prove the existence of two positve solutions in the spirit of a famous result by Ambrosetti, Brezis and Cerami. To this aim we use a variational Perron method combined with proper estimates of a family of functions which are minimizers of the relevant Sobolev inequality. Due to the lack of boundary regularity, we also have to be careful while proving that the first solution found is a local minimizer in the proper topology.

</details>


### [19] [Infinity of solutions to initial-boundary value problems for linear constant-coefficient evolution PDEs on semi-infinite intervals](https://arxiv.org/abs/2512.04670)
*Andreas Chatziafratis,Spyridon Kamvissis*

Main category: math.AP

TL;DR: Algorithmic procedure for constructing non-uniqueness counter-examples for linear evolution PDEs in quarter-plane, with application to heat and KdV equations.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic method for constructing counter-examples that demonstrate non-uniqueness of classical solutions to initial-boundary-value problems for linear evolution PDEs with constant coefficients.

Method: Uses analysis of regularity and asymptotic properties near domain boundaries, combined with closed-form integral-representation formulae derived via complex-analytic techniques and the Fokas unified transform method.

Result: Algorithmic procedure developed and demonstrated through explicit application to heat equation and linear KdV equation with Dirichlet data, along with new uniqueness theorems for these models.

Conclusion: Successfully presents a novel technique for constructing non-uniqueness counter-examples and establishes new uniqueness theorems for specific PDE models.

Abstract: In this short communication, we announce an algorithmic procedure for constructing non-uniqueness counter-examples of classical solutions to initial-boundary-value problems for a wide class of linear evolution partial differential equations, of any order and with constant coefficients, formulated in a quarter-plane. Our approach relies on analysis of regularity and asymptotic properties, near the boundary of the spatio-temporal domain, of closed-form integral-representation formulae derived via complex-analytic techniques and rigorous implementation of the modern PDE technique known as Fokas unified transform method. In order to elucidate the novel idea and demonstrate the proposed technique in a self-contained fashion, we explicitly present its application to two concrete examples, namely the heat equation and the linear KdV equation with Dirichlet data. New uniqueness theorems for these two models are also presented herein.

</details>


### [20] [On a fuzzy Landau Equation: Part III. The grazing collision limit](https://arxiv.org/abs/2512.04713)
*Manh Hong Duong,Boris Golubkov,Zihui He*

Main category: math.AP

TL;DR: The paper establishes a grazing limit connecting fuzzy Boltzmann equations to fuzzy Landau equations through variational formulations with GENERIC structure.


<details>
  <summary>Details</summary>
Motivation: To mathematically connect two important kinetic equations (fuzzy Boltzmann and fuzzy Landau) that describe particle interactions through delocalized collisions, particularly in the grazing collision limit where collisions become very small-angle.

Method: Using variational formulations corresponding to the GENERIC structure, the authors show convergence from non-quadratic dual dissipation pairs for fuzzy Boltzmann equations to quadratic dissipation pairs for fuzzy Landau equations.

Result: Successfully demonstrates the grazing limit from fuzzy Boltzmann equations to fuzzy Landau equations through convergence of their respective variational formulations and dissipation structures.

Conclusion: The variational approach provides a rigorous mathematical framework for connecting these kinetic equations, showing how the non-quadratic dissipation of fuzzy Boltzmann equations converges to the quadratic dissipation of fuzzy Landau equations in the grazing limit.

Abstract: In this paper, we study the grazing limit from the non-cutoff fuzzy Boltzmann equations to the fuzzy Landau equation, where particles interact through delocalised collisions. We show the grazing limit through variational formulations that correspond to the GENERIC (General Equations for Non-Equilibrium Reversible-Irreversible Coupling) structure of the respective equations. We show that the variational formulation associated with a non-quadratic dual dissipation pair for the fuzzy Boltzmann equations converges to a variational formulation of the fuzzy Landau equation corresponding to a quadratic dissipation pair.

</details>


### [21] [Optimal cost for the null controllability of the Stokes system with controls having $n-1$ components and applications](https://arxiv.org/abs/2512.04721)
*Felipe W. Chaves-Silva,Diego A. Souza,Marcos G. Ferreira-Silva*

Main category: math.AP

TL;DR: The paper shows that removing one component from the control in n-dimensional Stokes system doesn't increase the null controllability cost, which remains O(e^{C/T}) like with full n-component control.


<details>
  <summary>Details</summary>
Motivation: To understand how the cost of null controllability for Stokes systems is affected when controls have fewer components (n-1 instead of n), and whether this reduction increases the control cost.

Method: Develops a novel spectral estimate for low frequencies of the Stokes operator involving only n-1 components, then uses this estimate to analyze controllability cost scaling.

Result: The cost of null controllability with n-1 component controls remains O(e^{C/T}), same order as with full n-component controls, showing no cost penalty from missing one control component.

Conclusion: Reducing control components from n to n-1 in Stokes systems doesn't affect the exponential scaling of controllability cost, with applications demonstrating this robustness property.

Abstract: In this work, we investigate the optimal cost of null controllability for the $n$-dimensional Stokes system when the control acts on $n-1$ scalar components. We establish a novel spectral estimate for low frequencies of the Stokes operator, involving solely $n-1$ components, and use it to show that the cost of controllability with controls having $n-1$ components remains of the same order in time as in the case of controls with $n$ components, namely $O(e^{C/T})$, i.e. the cost of null controllability is not affected by the absence of one component of the control. We also give several applications of our results.

</details>


### [22] [Generalized Navier-Stokes equations, associated with the Dolbeault complex](https://arxiv.org/abs/2512.04777)
*Shlapunov Alexander,Polkovnikov Alexander*

Main category: math.AP

TL;DR: The paper analyzes a Cauchy problem in complex space for a system structurally similar to Navier-Stokes equations but generated by Cauchy-Riemann operators instead of standard gradient/divergence operators.


<details>
  <summary>Details</summary>
Motivation: To study a system of nonlinear differential equations in complex space that resembles Navier-Stokes equations but uses complex analysis operators (Cauchy-Riemann, Dolbeault complex) instead of standard vector calculus operators.

Method: Uses the multidimensional Cauchy-Riemann operator ∂̅, its formally adjoint operator ∂̅*, and the Dolbeault compatibility complex. Analyzes the Cauchy problem in ℂⁿ×[0,T] using specially constructed Bochner-Sobolev spaces.

Result: Proves existence of weak solutions and an open mapping theorem on the scale of Bochner-Sobolev spaces. Also obtains a criterion for existence of "strong" solutions in these spaces.

Conclusion: The structural similarity to Navier-Stokes equations allows proving analogous results for this complex-analytic system, including existence theorems and solution criteria in specialized function spaces.

Abstract: We consider the Cauchy problem in the band $\mathbb{C}^{n}\times[0, T], n>1,T>0$, for a system of nonlinear differential equations structurally similar to the classical Navier-Stokes equations for an incompressible fluid. The main difference of this system is that it is generated not by the standard gradient operators $\nabla$, divergence div and rotor rot, but by the multidimensional Cauchy-Riemann operator $\overline{\partial}$ in $\mathbb{C}^{n}$, its formally adjoint operator $\overline{\partial}^{*}$ and the compatibility complex for $\overline{\partial}$, which is usually called the Dolbeault complex. The similarity of the structure makes it possible to prove for this problem the theorem of the existence of weak solutions and the open mapping theorem on the scale of specially constructed Bochner-Sobolev spaces. In addition, a criterion for the existence of a ``strong'' solution in these spaces is obtained.

</details>


### [23] [Homogenized limits of Stokes flow and advective transport in thin perforated domains](https://arxiv.org/abs/2512.04782)
*Markus Gahn,Vlad Revnic*

Main category: math.AP

TL;DR: Homogenization and dimension reduction of Stokes flow and transport in thin periodic perforated layers with thickness ε^α (α∈(0,1)), deriving effective Darcy-type flow and diffusion-advection transport models as ε→0.


<details>
  <summary>Details</summary>
Motivation: To derive effective macroscopic models for fluid flow and transport in thin periodic perforated layers where the layer thickness (ε^α) is much larger than the pore size (ε), addressing the challenge of simultaneous homogenization and dimension reduction as the layer thickness tends to zero.

Method: Uses two-scale convergence adapted to the microscopic geometry, based on uniform a priori estimates. Critical techniques include constructing a Bogovskii-operator for thin perforated domains to control fluid pressure, and establishing strong two-scale convergence for the microscopic transport solution to handle the advective term.

Result: For Stokes flow: obtains Darcy-type law with Darcy-velocity depending only on vertical pressure gradient. For transport: obtains diffusion-advection equation with homogenized coefficients, where advection is vertical Darcy-velocity. Slow vertical diffusion yields effective diffusion only in vertical direction; fast horizontal diffusion yields effective diffusion in all directions.

Conclusion: Successfully derives rigorous effective models for coupled flow-transport in thin perforated layers, demonstrating how microscopic periodic structure and thin geometry combine to produce specific macroscopic behaviors with directional dependencies in both flow and transport.

Abstract: We deal with the rigorous homogenization and dimension reduction of flow and transport problems posed in thin $\varepsilon$-periodic perforated layers with thickness of order $\varepsilon^α$ with $α\in (0,1)$ and therefore the thickness of the layer is large compared its porosity. The aim is the derivation of effective models for $\varepsilon\to 0 $, when the thickness of the layer tends to zero. For the flow problem we consider incompressible Stokes equations with a pressure boundary condition on the top/bottom of the layer, and the transport problem is given by reaction-diffusion-advection problem with advective flow governed from the fluid velocity from the Stokes model and different scalings for the diffusion coefficient modelling low and fast diffusion in the horizontal direction. In the limit, a Darcy-type law is obtained for the Stokes flow with Darcy-velocity depending only on the derivative of the Darcy-pressure in the vertical direction. The effective equation for the transport problem is again of diffusion-advection-type including homogenized coefficients, and with advective flow given by the Darcy-velocity and only taking place in the vertical direction. In the case of slow diffusion in the vertical direction, effective diffusion only takes place in the vertical direction, where in the case of high diffusion in horizontal direction, we obtain effective diffusion in all space directions. To pass to the limit we use the method of two-scale convergence adapted to our microscopic geometry, which is based on uniform a priori estimates. Critical parts in the derivation of the macro-models are the control of the fluid pressure, for which we construct a Bogovskii-operator for thin perforated domains, as well as the strong two-scale convergence for the microscopic solution of the transport equation, necessary to pass to the limit in the advective term.

</details>


### [24] [The initial-to-final-state inverse problem with unbounded potentials and Strichartz estimates](https://arxiv.org/abs/2512.04796)
*Pedro Caro,Alberto Ruiz*

Main category: math.AP

TL;DR: The paper extends uniqueness results for the quantum Hamiltonian inverse problem to unbounded potentials, using Strichartz estimates and showing limitations of Bourgain spaces for this problem.


<details>
  <summary>Details</summary>
Motivation: To establish a theoretical framework explaining data-driven prediction viability in quantum mechanics, and extend previous uniqueness results from bounded to unbounded potentials in the initial-to-final-state inverse problem.

Method: Extends the inverse problem analysis to unbounded potentials by proving a family of suitable Strichartz estimates (including Keel-Tao endpoint), comparing with Carleman inequalities from inverse Calderón problem, and providing a counterexample showing Bourgain spaces cannot capture the needed mixed-norm Lebesgue spaces.

Result: Successfully extends uniqueness results to unbounded potentials, proves required Strichartz estimates, and demonstrates limitations of Bourgain spaces for this inverse problem through a counterexample.

Conclusion: The initial-to-final-state inverse problem can be solved for unbounded potentials using Strichartz estimates, but Bourgain spaces are insufficient for this framework, highlighting the need for different functional spaces.

Abstract: The initial-to-final-state inverse problem consists in determining a quantum Hamiltonian assuming the knowledge of the state of the system at some fixed time, for every initial state. We formulated this problem to establish a theoretical framework that would explain the viability of data-driven prediction in quantum mechanics. In a previous work, we analysed this inverse problem for Hamiltonians of the form $-Δ+ V$ with an electric potential $V = V({\rm t}, {\rm x})$, and we showed that uniqueness holds whenever the potentials are bounded and decay super-exponentially at infinity. In this paper, we extend this result for unbounded potentials. One of the key steps consists in proving a family of suitable Strichartz estimates -- including the corresponding endpoint of Keel and Tao.
  In the context of the inverse Calderón problem this family of inequalities corresponds to the Carleman inequality proved by Kenig, Ruiz and Sogge. Haberman showed that this inequality can be also retrieved as an embedding of a suitable Bourgain space. The corresponding Bourgain space in our context do not capture the mixed-norm Lebesgue spaces of Strichartz inequalities. In this paper, we give a counterexample that justifies this fact, and shows the limitations of Bourgain spaces to address the initial-to-final-state inverse problem.

</details>


### [25] [Time-periodic solutions to an energy balance model coupled with an active fluid under arbitrary large forces](https://arxiv.org/abs/2512.04800)
*Gianmarco Del Sarto,Matthias Hieber,Filippo Palma,Tarek Zöchling*

Main category: math.AP

TL;DR: The paper proves existence of time-periodic solutions for a coupled climate model without requiring small forcing terms.


<details>
  <summary>Details</summary>
Motivation: To establish existence of periodic climate patterns in a coupled atmosphere-ocean model when driven by periodic external forcing, without restrictive smallness assumptions.

Method: Analysis of a two-dimensional Sellers-type energy balance model coupled to 3D primitive equations via dynamic boundary conditions, using mathematical existence proofs for time-periodic solutions.

Result: Proves existence of at least one strong time-periodic solution when forcing is time-periodic, with no smallness condition required (forcing can be arbitrarily large).

Conclusion: The coupled climate model admits periodic solutions under periodic forcing, providing mathematical foundation for studying seasonal and other periodic climate phenomena.

Abstract: This article concerns time-periodic solutions to a two-dimensional Sellers-type energy balance model coupled to the three-dimensional primitive equations via a dynamic boundary condition. It is shown that the underlying equations admit at least one strong time-periodic solution, provided the forcing term is time-periodic. The forcing term does not need to satisfy a smallness condition and is allowed to be arbitrarily large.

</details>


### [26] [Spectral Theory of Krein-Feller Type Operators and Applications in Stochastic Fractional Elliptic and Parabolic Equations](https://arxiv.org/abs/2512.04826)
*Kelvin J. R. Almeida-Sousa,Alexandre B. Simas*

Main category: math.AP

TL;DR: The paper develops nonstandard methods to analyze Krein-Feller operators with singular measures, establishing series expansions for functions in the natural regularity space, characterizing eigenvectors, bounding eigenvalues, and proving nuclearity of the space with applications to differential equations.


<details>
  <summary>Details</summary>
Motivation: To analyze the eigenvalue problem for Krein-Feller operators Δ_{W,V} on the torus when W and V are singular measures with dense discontinuities, where classical analytical methods fail and nonstandard approaches are needed.

Method: Nonstandard analysis techniques to handle the singular setting where classical spectral theory fails; development of generalized Taylor expansions for functions in C^∞_{W,V}(𝕋); characterization of eigenvectors using generalized trigonometric functions.

Result: Established series expansions for functions in C^∞_{W,V}(𝕋); characterized eigenvectors of Δ_{W,V}; obtained asymptotic lower bound for eigenvalues and sharp upper bound for convergence exponent; proved C^∞_{W,V}(𝕋) is a nuclear space; derived applications to compact operators and differential equations.

Conclusion: The paper successfully develops nonstandard methods to analyze Krein-Feller operators in singular settings, establishing fundamental properties of the regularity space and obtaining spectral results with applications to various types of differential equations on nuclear spaces.

Abstract: It has been shown that the space $C^{\infty}_{W,V}(\mathbb{T})$, introduced in Simas and Sousa (Potential Analysis, 2025), is the natural regularity space for solutions of the eigenvalue problem $Δ_{W,V} u = λu$ on the torus $\mathbb{T}$, where $Δ_{W,V} = \frac{d^{+}}{dV}\frac{d^{-}}{dW}$ is the Krein Feller operator in the case where $W$ and $V$ are strictly increasing and right continuous (respectively left continuous), possibly with dense sets of discontinuities. In this work we provide conditions ensuring that every function in $C^{\infty}_{W,V}(\mathbb{T})$, which may be highly discontinuous, admits a series expansion that generalizes the classical Taylor expansion. A central feature of our approach is that all proofs are nonstandard, since classical analytical and spectral arguments cannot be adapted to this singular setting. Using these methods we characterize the eigenvectors of $Δ_{W,V}$ in terms of generalized trigonometric functions and obtain an asymptotic lower bound for the associated eigenvalues. We also derive a sharp upper bound for the convergence exponent of these eigenvalues, and as a consequence we prove that $C^{\infty}_{W,V}(\mathbb{T})$ is a nuclear space. Further consequences include results on the asymptotic behavior of eigenvalues of compact operators and improvements in traceability. As a final application we establish existence results for generalized fractional stochastic and deterministic differential equations, as well as for parabolic stochastic partial differential equations acting on nuclear spaces.

</details>


### [27] [On hyperbolic approximations for a class of dispersive and diffusive-dispersive equations](https://arxiv.org/abs/2512.04882)
*Rahul Barthwal,Firas Dhaouadi,Christian Rohde*

Main category: math.AP

TL;DR: Novel hyperbolic approximations for dispersive and diffusive-dispersive equations with nonlinear fluxes, enabling numerical simulation via standard hyperbolic balance law methods.


<details>
  <summary>Details</summary>
Motivation: To develop approximate systems for dispersive and diffusive-dispersive equations that allow application of standard numerical methods from hyperbolic balance laws, overcoming challenges in simulating these complex equations.

Method: Construct first-order strictly hyperbolic approximations for dispersive equations with unique symmetrizer; extend to diffusive-dispersive equations via viscoelastic damped systems; use relative entropy framework for convergence analysis.

Result: Proved local well-posedness for smooth solutions, global well-posedness for hyperbolic-parabolic approximations, and convergence to original equations; demonstrated numerical convergence even beyond theoretical validity range.

Conclusion: The developed approximate systems provide a practical framework for simulating dispersive and diffusive-dispersive equations using standard hyperbolic numerical methods, validated through theoretical analysis and numerical tests.

Abstract: We introduce novel approximate systems for dispersive and diffusive-dispersive equations with nonlinear fluxes. For purely dispersive equations, we construct a first-order, strictly hyperbolic approximation. Local well-posedness of smooth solutions is achieved by constructing a unique symmetrizer that applies to arbitrary smooth fluxes. Under stronger conditions on the fluxes, we provide a strictly convex entropy for the hyperbolic system that corresponds to the energy of the underlying dispersive equation. To approximate diffusive-dispersive equations, we rely on a viscoelastic damped system that is compatible with the found entropy for the hyperbolic approximation of the dispersive evolution. For the resulting hyperbolic-parabolic approximation, we provide a global well-posedness result. Using the relative entropy framework \cite{dafermos2005hyperbolic}, we prove that the solutions of the approximate systems converge to solutions of the original equations. The structure of the new approximate systems allows to apply standard numerical simulation methods from the field of hyperbolic balance laws. We confirm the convergence of our approximations even beyond the validity range of our theoretical findings on set of test cases covering different target equations. We show the applicability of the approach for strong nonlinear effects leading to oscillating or shock-layer-forming behavior.

</details>


### [28] [Quantitative rigidity of the Wasserstein contraction under convolution](https://arxiv.org/abs/2512.04928)
*Max Fathi,Michael Goldman,Daniel Tsodyks*

Main category: math.AP

TL;DR: Investigates contraction properties of p-Wasserstein distances under convolution, connects to uniform convexity of Kantorovich functional, extends results to p=1 case.


<details>
  <summary>Details</summary>
Motivation: To understand how p-Wasserstein distances behave under convolution operations in Euclidean spaces, both qualitatively and quantitatively. The connection to uniform convexity of Kantorovich functional provides a framework for analysis.

Method: Connects contraction properties of Wasserstein distances to uniform convexity of Kantorovich functional. Extends existing uniform convexity results (mostly for p=2, partially for p>1) to the p=1 case using analytical techniques.

Result: Establishes connection between contraction properties and uniform convexity. Successfully extends uniform convexity results to the p=1 case, which was previously not covered.

Conclusion: The paper provides new insights into contraction properties of Wasserstein distances under convolution and extends the theory of uniform convexity to include the important p=1 case, advancing the understanding of optimal transport theory.

Abstract: The aim of this paper is to investigate the contraction properties of $p$-Wasserstein distances with respect to convolution in Euclidean spaces both qualitatively and quantitatively. We connect this question to the question of uniform convexity of the Kantorovich functional on which there was substantial recent progress (mostly for $p=2$ and partially for $p>1$). Motivated by this connection we extend these uniform convexity results to the case $p=1$, which is of independent interest.

</details>


### [29] [Existence and a priori bounds for fully nonlinear PDEs with a harmonic map-like structure](https://arxiv.org/abs/2512.04961)
*Gabrielle Nornberg,Ricardo Ziegele*

Main category: math.AP

TL;DR: Study of fully nonlinear elliptic equations with harmonic map-like structure, establishing existence, multiplicity, and qualitative results under small coefficient conditions.


<details>
  <summary>Details</summary>
Motivation: To analyze a new class of fully nonlinear uniformly elliptic equations with harmonic map-like structure, which generalizes previous models and includes Pucci extremal operators with nonlinear gradient terms.

Method: Use Pucci extremal operators, establish Aleksandrov-Bakelman-Pucci estimates and comparison principles, apply smallness regime on coefficients, and analyze Dirichlet problems in noncoercive cases.

Result: Obtained existence results under small coefficient conditions, classical results (ABP estimate, comparison principle), a priori bounds for Dirichlet problems, multiplicity results, and new qualitative behavior even for Laplacian case.

Conclusion: The paper successfully establishes a comprehensive theory for this new class of equations, including existence, multiplicity, and qualitative properties that extend known results to more general nonlinear elliptic operators.

Abstract: In this paper, we study a new class of fully nonlinear uniformly elliptic equations with a so-called harmonic map-like structure, whose model case is given by \begin{equation*} \mathcal{M}^{\pm}_{λ,Λ}(D^2u) \pm b(x) |Du| \pm β(u)\langle M(x) Du,Du \rangle \pm c(x) u = f(x)\; \textrm{ in } Ω, \end{equation*} where $Ω\subset \mathbb{R}^n$ is a bounded $C^{1,1}$ domain, $\mathcal{M}^{\pm}$ are the Pucci extremal operators, $β(s) = s^k$ for some $k \in \mathbb{N} $ odd, $b \in L^{q}_{+}(Ω)$, $c,f \in L^p(Ω)$, and $n \leq p \leq q$, $q>n$.
  We obtain existence results under a smallness regime on the coefficients, along with some classical results such as the Aleksandrov--Bakelman--Pucci estimate and the comparison principle, as well as a priori bounds for the respective Dirichlet problem in the noncoercive case. We also establish multiplicity results and qualitative behavior, which seem to be new in the case of the Laplacian operator.

</details>


### [30] [Fractured Poroelastic Media in the Limit of Vanishing Aperture](https://arxiv.org/abs/2512.04978)
*Maximilian Hörl,Kundan Kumar,Christian Rohde*

Main category: math.AP

TL;DR: Derivation of limit models for poroelastic media with thin fractures as fracture width approaches zero, identifying different scaling regimes for hydraulic conductivity and elasticity.


<details>
  <summary>Details</summary>
Motivation: To mathematically model fluid flow and mechanical deformation in poroelastic media containing thin fractures, and understand how different scaling of fracture properties affects the limit behavior as fracture width becomes infinitesimally small.

Method: Using a priori estimates and rigorous mathematical analysis of the quasi-static Biot equations governing poroelastic behavior, with fracture material parameters scaled by powers of the width-to-length ratio ε, then deriving limit models as ε → 0.

Result: Identified five distinct regimes for hydraulic conductivity scaling and two regimes for elasticity scaling, with some cases yielding discrete fracture models while others produce two-scale limit problems dominated by normal flow or deformation.

Conclusion: The scaling of fracture properties relative to fracture width significantly influences the resulting limit models, with different regimes capturing distinct physical behaviors ranging from discrete fracture representations to complex two-scale interactions.

Abstract: We consider a poroelastic medium with a thin heterogeneity, also referred to as a fracture. Fluid flow and mechanical deformation inside both bulk and fracture are governed by the quasi-static Biot equations. The fracture's material parameters, such as hydraulic conductivity and elasticity, are assumed to scale with powers of the width-to-length ratio $\varepsilon$ of the fracture. Based on a priori estimates, we rigorously derive limit models as $\varepsilon \rightarrow 0$ and identify different limit regimes. We obtain five regimes for the hydraulic conductivity and two for the elasticity. While many cases yield discrete fracture models, others result in two-scale limit problems dominated by normal flow or deformation.

</details>


### [31] [Geophysical intensity problems: the axisymmetric case](https://arxiv.org/abs/2512.05010)
*Ralf Kaiser*

Main category: math.AP

TL;DR: The paper proves existence of infinitely many axisymmetric harmonic vector fields outside a sphere with prescribed intensity at the surface, characterized by specific zero patterns and decay orders.


<details>
  <summary>Details</summary>
Motivation: To address the intensity problem for gravitational/magnetic fields - a nonlinear boundary value problem where harmonic vector fields vanishing at infinity must have prescribed intensity at a celestial body's surface, whose general solvability is not yet established.

Method: Study axisymmetric harmonic fields outside unit sphere; solve nonlinear elliptic equation with discontinuous and singular coefficients using natural boundary conditions (vs fixed boundary conditions in previous work), requiring new solution techniques and sharper estimates.

Result: Proved existence of infinitely many solutions for axisymmetric Hölder continuous intensity functions; solutions characterized by specific zero patterns (symmetric points outside a unit circle) and exact decay orders at infinity, with uniqueness up to sign.

Conclusion: Established solvability of intensity problem for axisymmetric case, providing mathematical foundation for determining harmonic fields from intensity measurements, with applications to geophysics and planetary science.

Abstract: Considering the earth or any other celestial body the main sources of the gravitational as well as of the magnetic field lie inside the body. Above the surface both fields are in good approximation harmonic vector fields determined by their values at the body's surface or any other surface enclosing the body. The intensity problem seeks to determine harmonic vector fields vanishing at infinity and with prescribed intensity of the field at the surface. This problem constitutes a nonlinear boundary value problem, whose general solvability is not yet established. In this paper {\em axisymmetric} harmonic fields ${\bf H}$ outside the unit sphere $S^2$ are studied and, given an axisymmetric Hölder continuous intensity function $I\neq 0$ on $S^2$, the existence of infinitely many solutions of the intensity problem is proved. These solutions can more precisely be characterized as follows: fix a number $\de \in \nat\setminus \{1 \}$ and a meridional plane $M$ through the symmetry axis $S\!A$, and in $M$ a unit circle $S^1$ (symmetric with respect to $S\!A$) and, furthermore, $2\, N$, $N \in \nat_0$, points $z_n \in M$ (symmetric with respect to $S\!A$, avoiding $S\!A$, and outside $S^1$), then the existence of an (up to a sign) unique harmonic field ${\bf H}$ is established that vanishes at (the axisymmetric circles piercing $M$ at) $z_n$ and nowhere else, that has intensity $I$ at $S^2$ and (exact) decay order $\de$ at infinity. The proof is based on the solution of a nonlinear elliptic equation with discontinuous coefficients, which are, moreover, singular at the symmetry axis. Its combination with fixed boundary conditions was the basis of a recent treatment of the ``geomagnetic direction problem'' \cite{KR22}. Here we have instead natural boundary conditions, which provide less information, and which require, therefore, in part new solution techniques and sharper estimates.

</details>


### [32] [A Nehari manifold method for nonvariational problems](https://arxiv.org/abs/2512.05055)
*Radu Precup,Andrei Stan*

Main category: math.AP

TL;DR: Extends Nehari manifold method from variational to nonvariational fixed point equations via radial energy functional, yielding multiple localized solutions in conical annular sets.


<details>
  <summary>Details</summary>
Motivation: To generalize the Nehari manifold method beyond variational settings to handle nonvariational fixed point equations, expanding its applicability to broader classes of problems.

Method: Constructs a radial energy functional that generalizes the standard variational case, applies Nehari manifold method to nonvariational fixed point equations, and obtains solutions localized in conical annular sets.

Result: Achieves existence of multiple solutions through localization in conical annular sets, demonstrating the method's effectiveness in nonvariational contexts.

Conclusion: Successfully extends Nehari manifold method to nonvariational fixed point equations, providing a new approach for obtaining multiple solutions with specific geometric localization properties.

Abstract: The aim of this paper is to extend the Nehari manifold method from the variational setting to the nonvariational framework of fixed point equations. This is achieved by constructing a radial energy functional that generalizes the standard one from the variational case. Furthermore, the solutions obtained through our method are localized in conical annular sets, which leads to the existence of multiple solutions. The abstract results are illustrated by two representative applications.

</details>


### [33] [Mean curvature flow near a peanut solution](https://arxiv.org/abs/2512.05077)
*Sigurd Angenent,Panagiota Daskalopoulos,Natasa Sesum*

Main category: math.AP

TL;DR: The paper analyzes the instability of "peanut solutions" in mean curvature flow, showing they can be perturbed to develop either spherical or nondegenerate neckpinch singularities, and that sequences converging to peanut solutions with spherical singularities approach the Ancient oval solution.


<details>
  <summary>Details</summary>
Motivation: Peanut solutions are closed mean curvature flows that extinct to a point without becoming convex, developing degenerate neckpinch singularities. While their existence was proven, their stability properties remained unclear. The paper aims to characterize the instability of these special solutions.

Method: The authors analyze perturbations of peanut solutions and study the resulting mean curvature flow behavior. They examine sequences of solutions whose initial data converge to the peanut solution and investigate their rescaled limits, particularly focusing on those developing spherical singularities.

Result: Peanut solutions are highly unstable: any small neighborhood contains perturbations leading to either spherical singularities or nondegenerate neckpinch singularities. Additionally, appropriately rescaled subsequences of solutions converging to peanut solutions with spherical singularities converge to the Ancient oval solution.

Conclusion: Peanut solutions represent unstable critical points in the space of mean curvature flows, with their instability characterized by the existence of nearby perturbations leading to qualitatively different singularity types, and their connection to the Ancient oval solution through appropriate rescaling limits.

Abstract: It was shown by Angenent, Altschuler and Giga, and by Angenent and Velazquez that there exist closed mean curvature flow solutions that extinct to a point in finite time, without ever becoming convex prior to their extinction. These solutions develop a degenerate neckpinch singularity, meaning that the tangent flow at a singularity is a round cylinder, but at the same time for each of these solutions there exists a sequence of points in space and time, so that the pointed blow up limit around this sequence is the Bowl soliton. These solutions are called peanut solutions and they were first conjectured to exist by Richard Hamilton, while the existence of those solutions was shown by Angenent, Altschuler and Giga. In this paper we show that this type of solutions are highly unstable, in the sense that in every small neighborhood of any such peanut solution we can find a perturbation so that the mean curvature flow starting at that perturbation develops spherical singularity, and at the same time we can find a perturbation so that the mean curvature flow starting at that perturbation develops a nondegenerate neckpinch singularity. We also show that appropriately rescaled subsequence of any sequence of solutions whose initial data converge to the peanut solution, and all of which develop spherical singularities, converges to the Ancient oval solution.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [34] [Persistent-variable thermal compositional simulation of multiphase flow with phase separation in porous media](https://arxiv.org/abs/2512.04205)
*Veljko Lipovac,Omar Duran,Eirik Keilegavlen,Inga Berre*

Main category: physics.comp-ph

TL;DR: A persistent-variable formulation for thermal compositional multiphase flow using enthalpy, eliminating phase stability tests and providing continuous mathematical description for non-isothermal scenarios with embedded local solver for thermodynamic subproblems.


<details>
  <summary>Details</summary>
Motivation: Thermal compositional multiphase flow with phase transitions involves complex nonlinear interactions among flow, transport, and phase equilibrium. Existing approaches often require phase stability tests and lack seamless integration of equilibrium calculations into fully coupled models, especially for challenging non-isothermal scenarios.

Method: Persistent-variable formulation using enthalpy for energy balance and local equilibrium problem. Derives equilibrium conditions from thermodynamically consistent minimization problem. Embeds local solver for thermodynamic subproblem within global Newton solver for fully implicit system. Local solver exploits locality for parallelization and leverages modularity for both isothermal and isenthalpic equilibrium conditions.

Result: Demonstrated capability to simulate complex high-enthalpy systems including narrow-boiling phenomena. Embedded local solver reduces global nonlinear iterations by up to 23%. Local iterations controlled with tolerance, with no significant impact on global iterations for local residual tolerances as high as 1e-3.

Conclusion: The persistent-variable approach using enthalpy and modular embedded local solver advances equilibrium calculations in multiphase flow simulations, suitable for high-enthalpy applications without requiring phase stability tests.

Abstract: Thermal compositional multiphase flow in porous media with phase transitions involves complex nonlinear interactions among flow, transport, and phase equilibrium. This paper presents a persistent-variable formulation for thermal compositional flow using enthalpy to formulate the energy balance and the local equilibrium problem. Equilibrium conditions are derived from a thermodynamically consistent minimization problem using a persistent set of variables, allowing for seamless integration of equilibrium calculations into a fully coupled flow and transport model. This formulation does not require phase stability tests and provides a continuous and full mathematical description of the multiphysics system, suitable for challenging non-isothermal scenarios. To tackle the nonlinearities arising from phase transitions, we embed a local solver for the thermodynamic subproblem within a global Newton solver for the fully implicit system. The local solver exploits the locality of the subproblem for parallelization and leverages the modularity of the persistent-variable formulation for both isothermal and isenthalpic equilibrium conditions locally. We demonstrate the capability of our approach to simulate complex high-enthalpy systems, including narrow-boiling phenomena. The impact of the embedded local solver is analyzed through numerical experiments, demonstrating a reduction in global nonlinear iterations of up to 23 \% with increased use of the local solver. The number of local iterations is controlled with a local solver tolerance and no significant impact on the global iteration number was observed for local residual tolerances as high as $1e-3$. The persistent-variable approach using enthalpy and the modularity of the embedded local solver advance the usage of equilibrium calculations in multiphase flow simulations and are suitable for high-enthalpy applications.

</details>


### [35] [Bessel Functions and Analysis of Circular Waveguides](https://arxiv.org/abs/2512.04348)
*Jaime Mora-Paz,Leszek Demkowicz,Christina G. Taylor,Jacob Grosek,Stefan Henneking*

Main category: physics.comp-ph

TL;DR: The paper develops an analytical-numerical method to solve Bessel eigenvalue problems for circularly coiled optical slab waveguides, providing accurate loss factors and benchmark solutions.


<details>
  <summary>Details</summary>
Motivation: To study circularly coiled optical slab waveguides (also applicable to acoustical waveguides) and provide accurate solutions for verifying model implementations and analyzing waveguide stability.

Method: Uses change of variables and classical Frobenius method to compute Bessel functions of complex order/argument, combined with perfectly matched layer technique to solve Bessel eigenvalue problems for three-layer optical slab waveguides.

Result: Delivers accurate loss factors for eigensolutions, provides benchmark for verifying model implementations, and enables numerical verification of the Glazman criterion for waveguide stability analysis.

Conclusion: The developed method successfully solves complex Bessel eigenvalue problems for circular waveguides, establishing foundations for well-posedness and stability analysis of homogeneous circular waveguides with impedance boundary conditions.

Abstract: The paper is devoted to the study of circularly coiled optical slab waveguides, which is also applicable to acoustical waveguides. We use a change of variables and the classical Frobenius method to compute Bessel functions of complex order and complex argument, and combine it with a perfectly matched layer technique to solve the relevant Bessel eigenvalue problem and deliver accurate loss factors for eigensolutions to the three-layer optical slab waveguide problem. The solutions provide a benchmark for verifying model implementations of this problem and allow for a numerical verification of the Glazman criterion that provides a foundation for the well-posedness and stability analysis of homogeneous circular waveguides with impedance boundary conditions.

</details>


### [36] [GPU-Portable Real-Space Density Functional Theory Implementation on Unified-Memory Architectures](https://arxiv.org/abs/2512.04447)
*Atsushi M. Ito*

Main category: physics.comp-ph

TL;DR: GPU-portable DFT code QUMASUN achieves 2-2.8x speedup on AMD MI300A and NVIDIA GH200 GPUs compared to 256-core Xeon CPU, using lambda-based abstraction for cross-platform execution.


<details>
  <summary>Details</summary>
Motivation: To develop a GPU-portable implementation of real-space density functional theory (DFT) that can leverage modern GPU architectures (AMD MI300A and NVIDIA GH200) with unified/coherent memory features, enabling performance benefits for plasma-fusion simulations beyond DFT.

Method: Implemented QUMASUN DFT code with a lightweight C++ lambda-based abstraction layer that enables CPU, CUDA, and HIP execution without OpenMP/OpenACC directives. Benchmarked on Intel Xeon 6980P CPUs, AMD MI300A GPUs (unified memory), and NVIDIA GH200 GPUs (coherent memory interconnect).

Result: MI300A achieved 2.0-2.8x speedup and GH200 achieved 2.3-2.4x speedup over 256-core Xeon node for diamond (216 atoms) and tungsten (128 atoms) systems. Compute-bound kernels (FFT, GEMM, eigenvalue solver) showed substantial acceleration on both GPUs.

Conclusion: The GPU-portable approach successfully leverages modern GPU architectures for DFT calculations and can benefit a wide range of plasma-fusion simulation codes, demonstrating that the lambda-based abstraction enables efficient cross-platform execution without complex preprocessor directives.

Abstract: We present a GPU-portable implementation of a real-space density functional theory (DFT) code ``QUMASUN'' and benchmark it on the new Plasma Simulator featuring Intel Xeon 6980P CPUs, and AMD MI300A GPUs. Additional tests were performed on an NVIDIA GH200 GPU. In particular MI300A supports unified memory and GH200 supports coherent memory interconnect, simplifying GPU porting. A lightweight C++ lambda-based layer enables CPU, CUDA, and HIP execution without OpenMP/OpenACC preprocessor directives. For diamond (216 atoms) and tungsten (128 atoms) systems, MI300A and GH200 achieve 2.0-2.8 $\times$ and 2.3-2.4 $\times$ speedups over a 256-core Xeon node. The compute-bound kernels, which are fast Fourier transforms (FFT), dense matrix-matrix multiplications (GEMM) and eigenvalue solver, show substantial acceleration on both GPUs, indicating that the present GPU-portable approach can benefit a wide range of plasma-fusion simulation codes beyond DFT.

</details>


### [37] [On the Construction of High-Order and Exact Pressure Equilibrium Schemes for Arbitrary Equations of State](https://arxiv.org/abs/2512.04450)
*Christopher DeGrendele,Nguyen Ly,Francois Cadieux,Michael Barad,Dongwook Lee,Jared Duensing*

Main category: physics.comp-ph

TL;DR: Fully conservative methods for multi-component Euler equations that eliminate spurious pressure oscillations while preserving pressure equilibrium, compatible with arbitrary equations of state.


<details>
  <summary>Details</summary>
Motivation: Existing fully conservative discretizations of Euler compressible fluid equations with real-fluid equations of state suffer from spurious pressure oscillations due to nonlinear thermodynamic relations between pressure, density, and internal energy.

Method: Two methods: 1) A fully conservative, pressure-equilibrium preserving method, and 2) A high-order, fully conservative, approximate pressure-equilibrium preserving method. Both handle arbitrary equations of state and species without introducing non-conservative updates, overspecified equations, or equation-of-state-specific designs.

Result: Demonstrated on inviscid smooth interface advection problems with three equations of state (ideal-gas, stiffened-gas, van der Waals), showing orders of magnitude reduction in spurious pressure oscillations compared to existing schemes.

Conclusion: The proposed methods provide general, fully conservative discretizations that effectively eliminate pressure oscillations while maintaining pressure equilibrium for multi-component Euler equations with arbitrary real-fluid equations of state.

Abstract: Typical fully conservative discretizations of the Euler compressible single or multi-component fluid equations governed by a real-fluid equation of state exhibit spurious pressure oscillations due to the nonlinearity of the thermodynamic relation between pressure, density, and internal energy. A fully conservative, pressure-equilibrium preserving method and a high-order, fully conservative, approximate pressure-equilibrium preserving method are presented. Both methods are general and can handle an arbitrary equation of state and arbitrary number of species. Unlike existing approaches to discretize the multi-component Euler equations, we do not introduce non conservative updates, overspecified equations, or design for a specific equation of state. The proposed methods are demonstrated on inviscid smooth interface advection problems governed by three equations of state: ideal-gas, stiffened-gas, and van der Waals where we show orders of magnitude reductions in spurious pressure oscillations compared to existing schemes.

</details>


### [38] [Stochastic Density Functional Theory Through the Lens of Multilevel Monte Carlo Method](https://arxiv.org/abs/2512.04860)
*Xue Quan,Huajie Chen*

Main category: physics.comp-ph

TL;DR: sDFT with plane-wave discretization using multilevel Monte Carlo for variance reduction, making computational cost independent of discretization size or temperature.


<details>
  <summary>Details</summary>
Motivation: Stochastic DFT (sDFT) offers advantages over standard Kohn-Sham DFT for large-scale electronic structure calculations by avoiding expensive matrix diagonalization, but needs variance reduction for efficiency.

Method: Apply sDFT with plane-wave discretization and use multilevel Monte Carlo (MLMC) framework for variance reduction. Decompose density matrix evaluation into levels by increasing plane-wave cutoffs or Chebyshev polynomial orders.

Result: The method achieves computational cost independent of discretization size or temperature. Rigorous statistical error analysis and numerical experiments on material systems demonstrate algorithm efficiency.

Conclusion: MLMC-based variance reduction in sDFT with plane-wave discretization provides efficient large-scale electronic structure calculations with cost independent of system parameters.

Abstract: The stochastic density functional theory (sDFT) has exhibited advantages over the standard Kohn-Sham DFT method and has become an attractive approach for large-scale electronic structure calculations. The sDFT method avoids the expensive matrix diagonalization by introducing a set of random orbitals and approximating the density matrix via Chebyshev expansion of a matrix-valued function. In this work, we study the sDFT with a plane-wave discretization, and discuss variance reduction algorithms in the framework of multilevel Monte Carlo (MLMC) methods. In particular, we show that the density matrix evaluation in sDFT can be decomposed into many levels by increasing the plane-wave cutoffs or the Chebyshev polynomial orders. This decomposition renders the computational cost independent of the discretization size or temperature. To demonstrate the efficiency of the algorithm, we provide rigorous analysis of the statistical errors and present numerical experiments on some material systems.

</details>


### [39] [PENCO: A Physics-Energy-Numerical-Consistent Operator for 3D Phase Field Modeling](https://arxiv.org/abs/2512.04863)
*Mostafa Bamdad,Mohammad Sadegh Eshaghi,Cosmin Anitescu,Navid Valizadeh,Timon Rabczuk*

Main category: physics.comp-ph

TL;DR: PENCO is a hybrid neural operator framework that integrates physical laws, numerical structure, and energy constraints to solve spatio-temporal PDEs with superior accuracy, stability, and data efficiency compared to existing neural operators.


<details>
  <summary>Details</summary>
Motivation: Existing neural operators for solving spatio-temporal PDEs (like phase-field models) suffer from temporal error accumulation, poor long-horizon generalization, and require large training datasets. There's a need for more accurate, stable, and data-efficient alternatives that preserve physical consistency.

Method: PENCO integrates physical laws and numerical structure into a data-driven architecture through: 1) enhanced L² Gauss-Lobatto collocation residual for robust dynamics enforcement, 2) Fourier-space numerical consistency term capturing semi-implicit discretization behavior, 3) energy-dissipation constraint for thermodynamic consistency, plus low-frequency spectral anchoring and teacher-consistency mechanisms for stabilization.

Result: PENCO demonstrates superior accuracy, stability, and data efficiency compared to state-of-the-art neural operators (MHNO and FNO-4D) across 3D phase-field benchmarks including phase ordering, crystallization, epitaxial growth, and complex pattern formation, while maintaining physically consistent evolution.

Conclusion: The hybrid physics-energy-numerical-consistent operator framework successfully overcomes limitations of existing neural operators by preserving governing physics while mitigating long-term error growth, offering a promising approach for accurate and efficient PDE solutions in materials science and fluid mechanics.

Abstract: Accurate and efficient solutions of spatio-temporal partial differential equations (PDEs), such as phase-field models, are fundamental for understanding interfacial dynamics and microstructural evolution in materials science and fluid mechanics. Neural Operators (NOs) have recently emerged as powerful data-driven alternatives to traditional solvers; however, existing architectures often accumulate temporal errors, struggle to generalize in long-horizon simulations, and require large training datasets. To overcome these limitations, we propose PENCO (Physics-Energy-Numerical-Consistent Operator), a hybrid operator-learning framework that integrates physical laws and numerical structure within a data-driven architecture. The formulation introduces an enhanced L^2 Gauss-Lobatto collocation residual around the temporal midpoint that robustly enforces the governing dynamics and significantly improves accuracy, a Fourier-space numerical consistency term that captures the balanced behavior of semi-implicit discretizations, and an energy-dissipation constraint that ensures thermodynamic consistency. Additional low-frequency spectral anchoring and teacher-consistency mechanisms further stabilize learning and suppress long-term error growth. This hybrid design enables PENCO to preserve governing physics while mitigating long-term error growth. Through extensive three-dimensional phase-field benchmarks covering phase ordering, crystallization, epitaxial growth, and complex pattern formation, PENCO demonstrates superior accuracy, stability, and data efficiency compared to state-of-the-art neural operators, including Multi-Head Neural Operator (MHNO) and Fourier Neural Operator (FNO-4D), while maintaining physically consistent evolution. The associated dataset and implementation are available at github.com/MBamdad/PENCO.

</details>


### [40] [VNS Tokamak OpenMC-Serpent Validation for Medical Isotope Studies](https://arxiv.org/abs/2512.04873)
*Christopher Ehrich,Christian Bachmann,Pavel Pereslavtsev,Christian Reiter*

Main category: physics.comp-ph

TL;DR: Comparison of neutronics simulations between Serpent and OpenMC codes for Volumetric Neutron Source tokamak, showing good agreement for most detector responses but photon flux discrepancies depending on tracking method, with Serpent faster in coupled simulations but slower in neutron-only runs.


<details>
  <summary>Details</summary>
Motivation: To validate and compare neutronics simulations between Serpent and OpenMC codes for the proposed Volumetric Neutron Source (VNS) tokamak, which is designed for testing fusion reactor components and radioisotope production.

Method: Modeled VNS geometry in both Serpent and OpenMC neutronics codes, performed analog neutron-photon coupled simulations, compared vacuum vessel and blanket components across codes, calculated neutron/photon flux maps, spectra, and reaction rates, and tested different tracking methods (hybrid vs delta tracking).

Result: Neutron flux and (n,T) reactions showed excellent agreement; (n,2n) reactions had good agreement; photon flux showed regional discrepancies depending on Serpent tracking method (20% difference with hybrid tracking vs <1% with delta tracking); Serpent was faster than OpenMC in coupled simulations but slower in neutron-only simulations.

Conclusion: The study demonstrates successful cross-code validation for VNS neutronics simulations, identifies tracking method dependencies for photon transport, and shows performance differences between codes, while also highlighting VNS's potential for radioisotope production applications.

Abstract: The Volumetric Neutron Source (VNS) tokamak is a proposed fusion reactor for testing and qualification of reactor components for future use in a fusion power facility, and has potential use for radioisotope production. The VNS geometry is modeled in the Serpent and OpenMC neutronics codes. Analog neutron-photon coupled simulations are carried out to compare the model's vacuum vessel and blanket components across codes. In the vacuum vessel, neutron and photon flux maps are calculated, while in the blanket region, neutron and photon spectra, (n,T), and (n,2n) reaction rates are calculated and compared between models. The detector response comparisons found the following: neutron flux and (n,T) reactions achieved excellent agreement, the (n,2n) detector response had good agreement, and photon flux had regional discrepancies depending on Serpent tracking used. Hybrid tracking lead to a relative difference of about 20% in the outboard side blanket, where as employment of delta tracking resulted in less than 1% relative difference. On an HPC cluster, Serpent was found to have shorter computation time than OpenMC in neutron photon coupled simulations using both hybrid tracking and delta tracking, but longer in neutron only simulations. An exemplary radioisotope production case is presented for the demonstration of additional VNS capabilities.

</details>


### [41] [LEDDS: Portable LBM-DEM simulations on GPUs](https://arxiv.org/abs/2512.04997)
*Raphael Maggio-Aprile,Maxime Rambosson,Christophe Coreixas,Jonas Latt*

Main category: physics.comp-ph

TL;DR: LEDDS is an open-source GPU framework that implements Lattice Boltzmann-Discrete Element Method (LBM-DEM) simulations using only algorithmic primitives like map, sort, reduce, achieving performance comparable to hand-tuned CUDA while maintaining portability and code clarity.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that algorithmic programming paradigms (using parallel primitives rather than handcrafted GPU kernels) can be extended to complex computational physics problems like granular flows and fluid-particle interactions, while maintaining performance, portability, and code clarity.

Method: Developed LEDDS framework that performs fully coupled LBM-DEM simulations using only algorithmic primitives. The entire workflow (neighbor search, collision detection, fluid-particle coupling) is expressed as sequences of portable primitives, primarily using C++ Standard Library algorithms with selective Thrust primitives for performance.

Result: LEDDS achieves performance comparable to hand-tuned CUDA solvers despite high abstraction level. Validated through benchmarks including sphere/ellipsoid collisions, wall friction tests, single-particle settling, Jeffery's orbits, and particle-laden shear flows.

Conclusion: High-performance LBM-DEM coupling can be achieved without sacrificing generality or readability. LEDDS serves as a blueprint for portable multiphysics frameworks based on algorithmic primitives, applicable across modern GPU systems and future accelerators.

Abstract: Algorithmic formulations of GPU programs provide a high-level alternative to device-specific code by expressing computations as compositions of well-defined parallel primitives (e.g., map, sort, reduce), rather than through handcrafted GPU kernels. In this work, we demonstrate that this paradigm can be extended to complex and challenging problems in computational physics: the simulation of granular flows and fluid-particle interactions.
  LEDDS, our open-source framework, performs fully coupled Lattice Boltzmann -- Discrete Element Method (LBM-DEM) simulations using only algorithmic primitives, and runs efficiently on single-GPU platforms. The entire workflow, including neighbor search, collision detection, and fluid-particle coupling, is expressed as a sequence of portable primitives. While the current implementation illustrates these principles primarily through algorithms from the C++ Standard Library, with selective use of Thrust primitives for performance, the underlying concept is compatible with any HPC environment offering a rich set of parallel algorithms and is therefore applicable across a wide range of modern GPU systems and future accelerators.
  LEDDS is validated through benchmarks spanning both DEM and LBM-DEM configurations, including sphere and ellipsoid collisions, wall friction tests, single-particle settling, Jeffery's orbits, and particle-laden shear flows. Despite its high level of abstraction, LEDDS achieves performances comparable to those of hand-tuned CUDA solvers, while maintaining portability and code clarity. These results show that high-performance LBM-DEM coupling can be achieved without sacrificing generality or readability, establishing LEDDS as a blueprint for portable multiphysics frameworks based on algorithmic primitives.

</details>


### [42] [Engineered Inclined Energy Landscapes Enabling Free Flow of Magnetic Microstructures for Artificial Neuron Applications](https://arxiv.org/abs/2512.05020)
*Anmol Sharma,Ranjeet Kumar Brajpuriya,Vivek K. Malik,Vishakha Kaushik,Sachin Pathak*

Main category: physics.comp-ph

TL;DR: A spintronic neuromorphic computing design using magnetic microstructures with sawtooth energy landscape achieves low-energy integrate-and-fire neuron emulation (23.66 fJ/spike).


<details>
  <summary>Details</summary>
Motivation: Spintronic neuromorphic computing offers nanoscale, stable, low-energy magnetic microstructures, but practical integration faces challenges from complex fabrication, stochastic effects, pinning, and thermal instabilities that limit reliability and scalability.

Method: Engineered a sawtooth-type energy landscape for magnetic microstructures to enable free flow and emulate biological integrate-and-fire neuron function through an experimentally feasible, energy-efficient external stimuli approach.

Result: Successfully achieved free flow of magnetic microstructures and emulated integrate-and-fire neuron function with extremely low energy consumption of 23.66 fJ per spike.

Conclusion: The proposed design provides an experimentally reliable and energy-efficient approach for controlling magnetic microstructure dynamics, paving the way for skyrmion-based futuristic neuromorphic computing devices.

Abstract: Spintronic-based brain-inspired neuromorphic computing has recently attracted significant attention due to the exceptional properties of magnetic microstructures, including nanoscale dimensions, high stability, and low energy consumption. Despite these advantages, the practical integration of such microstructures into functional devices remains challenging. Fabrication processes are often complex and prone to stochastic effects, such as unwanted pinning and thermal-induced instabilities, which limit device reliability and scalability. Addressing these challenges is crucial for advancing spintronic neuromorphic architectures toward real-world applications. Thus, to reduce these effects we have proposed a design which is experimentally feasible and require less energy as compared to existing one. By engineering the system anisotropy into a sawtooth-type energy landscape, we have achieved free flow of these microstructures and successfully emulated integrate and fire (IF) function of biological neuron. Thus, proposed design presents an experimentally reliable and energy efficient external stimuli approach for tailoring magnetic microstructures dynamic behaviours, resulting in low energy consumption of 23.66 fJ per spike paving the way for the development of skyrmion-based futuristic neuromorphic computing device applications.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [43] [A simple procedure for generating a Kappa distribution in PIC simulation](https://arxiv.org/abs/2512.04272)
*Seiji Zenitani*

Main category: physics.plasm-ph

TL;DR: Proposes rejection-sampling method using Pareto distribution as envelope to generate Kappa distributions in PIC simulations with ~73-80% acceptance efficiency.


<details>
  <summary>Details</summary>
Motivation: Need efficient method to generate Kappa distributions for kinetic modeling of plasma processes in space using particle-in-cell simulations.

Method: Rejection-sampling procedure using Pareto distribution as envelope distribution, requiring only uniform random variates.

Result: Achieves acceptance efficiency of approximately 0.73 to 0.8 (73-80%).

Conclusion: Provides efficient method for generating Kappa distributions in PIC simulations for space plasma modeling.

Abstract: For kinetic modeling of plasma processes in space, a rejection-sampling procedure for generating a Kappa distribution in particle-in-cell (PIC) simulation is proposed. A Pareto distribution is employed as an envelope distribution. The procedure only requires uniform variates, and its acceptance efficiency is $\approx 0.73$--$0.8$.

</details>


### [44] [Numerical model for pellet rocket acceleration in PELOTON](https://arxiv.org/abs/2512.04484)
*J. Corbett,R. Samulyak,F. J. Artola,S. Jachmich,M. Kong,E. Nardon*

Main category: physics.plasm-ph

TL;DR: Developed and validated a numerical simulation model for rocket acceleration of pellets in fusion devices, showing consistent results with JET experiments and analyzing effects of pellet composition and plasma gradients.


<details>
  <summary>Details</summary>
Motivation: To understand and predict the rocket acceleration of pellets in thermonuclear fusion devices, which is crucial for pellet injection systems used for plasma disruption mitigation (particularly shattered pellet injection in fusion reactors like ITER).

Method: Developed a direct numerical simulation model within PELOTON (3D Lagrangian particle pellet code) that accounts for grad-B drift of ablation clouds, non-uniform charging by hot plasma electrons, local plasma gradients, and includes a new plasma cooling model for background plasma states.

Result: Simulations of rocket acceleration and trajectories of deuterium fragments are consistent with experimental measurements in JET. Deuterium-neon pellets (0.5% neon) showed smaller trajectory deviations than pure deuterium pellets. Demonstrated effects of fragment spatial configurations, cloud overlap, and plasma state gradients on rocket acceleration.

Conclusion: The validated model successfully simulates pellet rocket acceleration in fusion devices, with future work focusing on ITER plasma applications and developing scaling laws for rocket acceleration.

Abstract: A direct numerical simulation model for the rocket acceleration of pellets in thermonuclear fusion devices has been developed for PELOTON, a 3D Lagrangian particle pellet code [R. Samulyak et al, Nuclear Fusion 61 (4), 046007 (2021)], and validated using shattered pellet injection (SPI) experiments in JET. The pellet rocket acceleration is driven by grad-B drift of the ablation cloud that creates asymmetry and non-uniform heating of the cloud. The model accounts for non-uniform charging of the ablation cloud by hot plasma electrons as well as local plasma gradients. The increased pressure on the high-field-side compared to the low-field-side leads to pellet (fragment) rocket acceleration. Pure deuterium and deuterium-neon mixture models have been implemented. The background plasma states have been obtained by using a new plasma cooling model for PELOTON. The cooling model distributes the ablated material within the corresponding flux volumes and accounts for ionization and other energy losses, Ohmic heating by toroidal currents, and the energy exchange between ions and electrons. Plasma profiles predicted by PELOTON cooling model have been compared with JOREK and INDEX simulations. PELOTON simulations of rocket acceleration and the corresponding trajectories of deuterium fragments are consistent with experimentally measured trajectories in JET. We show that composite deuterium-neon pellets containing 0.5% of neon experienced smaller deviation of their trajectories compared to the pure deuterium case. We simulate various spatial configurations of pellet fragments and demonstrate the cloud overlap impact on rocket acceleration. Additionally, we demonstrate the effect of plasma state gradients on the rocket acceleration. Future work will focus on the rocket acceleration of SPI in projected ITER plasmas and the development of the corresponding scaling law for the rocket acceleration.

</details>


### [45] [Generation of ultrahigh field by micro-bubble implosion](https://arxiv.org/abs/2512.04715)
*M. Murakami,A. Arefiev,M. A. Zosa*

Main category: physics.plasm-ph

TL;DR: Bubble implosions using micro-bubbles and ultraintense lasers can accelerate protons beyond 100 MeV via spherically symmetric Coulomb forces, creating nano-pulsars that emit energetic proton flashes.


<details>
  <summary>Details</summary>
Motivation: Breaking the 100-MeV barrier for proton acceleration is crucial for fundamental physics research and practical applications including inertial confinement fusion and tumor therapy.

Method: Proposes bubble implosions combining micro-bubbles with ultraintense laser pulses (10^20-10^22 W/cm²). Bubble wall protons undergo volumetric acceleration toward the center via spherically symmetric Coulomb forces, creating ultrahigh electric fields at the center.

Result: 3D particle simulations confirm robust Coulomb-imploded bubbles that behave as nano-pulsars with repeated implosions/explosions to emit protons. The innermost protons reach densities comparable to white dwarf interiors, producing energetic proton flashes.

Conclusion: Current technologies should be sufficient to experimentally verify the bubble implosion concept, which offers a novel approach to achieving ultrahigh proton acceleration beyond 100 MeV.

Abstract: Breaking the 100-MeV barrier for proton acceleration will help elucidate fundamental physics and advance practical applications from inertial confinement fusion to tumour therapy. Herein we propose a novel concept of bubble implosions. A bubble implosion combines micro-bubbles and ultraintense laser pulses of 10^20-10^22W/cm^2 to generate ultrahigh fields and relativistic protons. The bubble wall protons undergo volumetric acceleration toward the centre due to the spherically symmetric Coulomb force and the innermost protons accumulate at the centre with a density comparable to the interior of a white dwarf. Then an unprecedentedly high electric field is formed, which produces an energetic proton flash. Three-dimensional particle simulations confirm the robustness of Coulomb-imploded bubbles, which behave as nano-pulsars with repeated implosions and explosions to emit protons. Current technologies should be sufficient to experimentally verify concept of bubble implosions.

</details>


### [46] [Optimization of laser illumination configuration for directly driven inertial confinement fusion](https://arxiv.org/abs/2512.04722)
*Masakatsu Murakami,Daiki Nishi*

Main category: physics.plasm-ph

TL;DR: Researchers developed optimal laser configurations for direct-drive inertial confinement fusion using charged particle models on spheres, finding new M48 and M60 configurations with superior illumination uniformity.


<details>
  <summary>Details</summary>
Motivation: To achieve high illumination uniformity in directly driven inertial confinement fusion targets, which is crucial for efficient and symmetric compression of fusion fuel.

Method: Used theoretical models of axisymmetric absorption patterns and developed a numerical model using self-organizing charged particles on a sphere to find optimal laser beam configurations for arbitrary numbers of beams.

Result: Discovered new laser configurations (M48 and M60) that show substantially higher illumination uniformity than existing direct drive systems, and proposed a new polar direct-drive scheme with off-center laser axes applicable to indirect-drive configurations.

Conclusion: The charged particle model on a sphere provides an effective method for designing optimal laser configurations, with the new M48 and M60 arrangements offering significantly improved illumination uniformity for direct-drive inertial confinement fusion.

Abstract: Optimum laser configurations are presented to achieve high illumination uniformity with directly driven inertial confinement fusion targets. Assuming axisymmetric absorption pattern of individual laser beams, theoretical models are reviewed in terms of the number of laser beams, system imperfection, and laser beam patterns. Utilizing a self-organizing system of charged particles on a sphere, a simple numerical model is provided to give an optimal configuration for an arbitrary number of laser beams. As a result, such new configurations as M48 and M60 are found to show substantially higher illumination uniformity than any other existing direct drive systems. A new polar direct-drive scheme is proposed with the laser axes keeping off the target center, which can be applied to laser configurations designed for indirectly driven inertial fusion.

</details>


### [47] [Operator Formalism for Laser-Plasma Wakefield Acceleration](https://arxiv.org/abs/2512.04982)
*Mostafa Behtouei,Carlos Salgado Lopez,Giancarlo Gatti*

Main category: physics.plasm-ph

TL;DR: Operator-based framework for laser-plasma wakefield acceleration using Hilbert-space operators and neural operators for efficient modeling and control.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic mathematical framework for understanding and optimizing laser-plasma wakefield acceleration in capillary discharges, bridging physics with computational methods.

Method: Uses operator formalism with key operators (transverse modal, nonlinear plasma, plasma oscillation, ponderomotive source) to describe coupled dynamics. Integrates with neural operator methods for efficient approximation of nonlinear operators.

Result: Establishes connection between LPWA and Hilbert-space operator theory, identifies invariant subspaces in linear regime, and enables reduced-order modeling through neural operator integration.

Conclusion: The hybrid physics-AI framework provides robust foundation for modeling, analysis, and optimization of high-intensity laser-plasma interactions in next-generation accelerators.

Abstract: In this paper, we develop an operator-based framework for laser--plasma wakefield acceleration (LPWA) in capillary discharges, providing a compact and systematic description of the coupled dynamics of laser fields and plasma response. The formalism employs key operators: the transverse modal operator $\hat{K}$, the nonlinear plasma operator $\hat{N}[Ψ]$, the plasma oscillation operator $\hatΩ_p^{\,2}$, and the ponderomotive source operator $\hatα$, which together describe mode coupling, plasma oscillations, and nonlinear feedback induced by the ponderomotive force. In the linear regime, the system is characterized by invariant subspaces associated with stable modal structures, while nonlinear interactions break these invariances, leading to mode mixing and complex dynamics. The approach establishes a direct connection between LPWA and Hilbert-space operator theory, including the invariant subspace, providing a formal mathematical interpretation of energy transfer and wakefield formation. Furthermore, the operator formalism integrates with neural operator methods, allowing efficient approximation of $\hat{N}$ and $\hatα$ for reduced-order modeling and predictive control. This hybrid physics--AI framework offers a robust foundation for modeling, analysis, and optimization of high-intensity laser--plasma interactions in next-generation accelerator experiments.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [48] [An all-optical convolutional neural network for image identification](https://arxiv.org/abs/2512.04569)
*Wei-Wei Fu,Dong Zhao,Qing-Hong Rao,Heng-Yi Wang,Ben-Li Yu,Zhi-Jia Hu,Fang-Wen Sun,Kun Huang*

Main category: physics.optics

TL;DR: All-optical CNN using spatial-differentiation convolution and diffractive fully-connected layer achieves high-speed, energy-efficient image classification without explicit optical nonlinearities.


<details>
  <summary>Details</summary>
Motivation: Electronic CNN hardware faces speed and energy efficiency bottlenecks due to resistive/capacitive losses. Photonic alternatives are promising but hindered by difficulty implementing optical nonlinearities needed for end-to-end image classification.

Method: Single spatial-differentiation convolutional stage with 24 directional kernels (360°) plus mean-filtering kernel, followed by diffractive fully-connected layer. Uses weak inherent optical diffraction nonlinearity instead of explicit nonlinear activations.

Result: 86.8% accuracy on MNIST, 94.8% on ten-class gesture dataset. Computational throughput: 1.13×10^5 TOPS, energy efficiency: 1.51×10^3 TOPS/W (highest reported for CNN hardware). Potential for 5-6 orders magnitude improvement with nanosecond detectors.

Conclusion: Demonstrates scalable pathway for ultralow-latency, ultralow-energy vision processing using all-optical CNN without explicit nonlinearities, enabling real-time intelligent systems.

Abstract: In modern artificial intelligence, convolutional neural networks (CNNs) have become a cornerstone for visual and perceptual tasks. However, their implementation on conventional electronic hardware faces fundamental bottlenecks in speed and energy efficiency due to resistive and capacitive losses. Photonic alternatives offer a promising route, yet the difficulty of realizing optical nonlinearities has prevented the realization of all-optical CNNs capable of end-to-end image classification. Here, we demonstrate an all-optical CNN that bypasses the need for explicit optical nonlinear activations. Our architecture comprises a single spatial-differentiation convolutional stage--using 24 directional kernels spanning 360°, along with a mean-filtering kernel--followed by a diffractive fully-connected layer. The directional convolution enhances feature selectivity, suppresses noise and crosstalk, and simplifies the classification task, allowing the weak nonlinearity inherent in optical diffraction to achieve high accuracy. We report experimentally classification accuracies of 86.8% on handwritten digits (MNIST) and 94.8% on a ten-class gesture dataset. The system delivers a computational throughput of 1.13X10^5 tera-operations per second (TOPS) and an energy efficiency of 1.51X10^3 TOPS/W--the highest reported among CNN hardware--with the potential to improve by a further 5-6 orders of magnitude using nanosecond-scale detectors. This work establishes a scalable pathway toward ultralow-latency, ultralow-energy vision processing for real-time intelligent systems.

</details>


### [49] [Structured Light at the Extreme: Harnessing Spatiotemporal Control for High-Field Laser-Matter Interactions](https://arxiv.org/abs/2512.05042)
*Sergio Carbajo,Seung-Whan Bahk,Justin Baker,Andrea Bertozzi,Abhimanyu Borthakur,Antonino Di Piazza,Andrew Forbes,Spencer Gessner,Jack Hirschman,Franz Kärtner,Maciej Lewenstein,Yuhang Li,Inhyuk Nam,Eileen Otte,Aydogan Ozcan,James Rozensweig,Yijie Shen,Liwei Song,Ye Tian,Yu Wang,Yuntian Wang,Logan Wright,Xiaojun Wu,Hao Zhang*

Main category: physics.optics

TL;DR: This review presents intelligent structured light as a new paradigm for controlling laser-matter interactions through precise spatiotemporal and vectorial light manipulation, enabled by advanced optics, AI-driven optimization, and leading to transformative applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to move beyond merely observing light-matter interactions at extreme intensities to actively commanding them through precise control of light's spatiotemporal and vectorial properties, enabling new capabilities in high-field laser physics.

Method: The paper proposes a three-pillar framework: 1) Advanced electromagnetic toolkit including static optics and plasma light modulators, 2) Optimization engines using physics-informed digital twins and AI-driven inverse design, and 3) Integration of these tools for practical applications.

Result: The review identifies emerging applications including programmable electron beams, orbital-angular-momentum-carrying γ-rays, compact THz accelerators, and robust communications systems enabled by intelligent structured light.

Conclusion: The field requires overcoming challenges in material science, real-time adaptive control at MHz rates, and quantum extensions, calling for interdisciplinary collaboration to actively command rather than just observe extreme light-matter interactions.

Abstract: This review charts the emerging paradigm of intelligent structured light for high-field laser-matter interactions, where the precise spatiotemporal and vectorial control of light is a critical degree of freedom. We outline a transformative framework built upon three synergistic pillars. First, we survey the advanced electromagnetic toolkit, moving beyond conventional spatial light modulators to include robust static optics and the promising frontier of plasma light modulators. Second, we detail the optimization engine for this high-dimensional design space, focusing on physics-informed digital twins and AI-driven inverse design to automate the discovery of optimal light structures. Finally, we explore the groundbreaking applications enabled by this integrated approach, including programmable electron beams, orbital-angular-momentum-carrying γ-rays, compact THz accelerators, and robust communications. The path forward necessitates overcoming grand challenges in material science, real-time adaptive control at MHz rates, and the extension of these principles to the quantum realm. This review serves as a call to action for a coordinated, interdisciplinary effort to command, rather than merely observe, light-matter interactions at the extreme.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [50] [Phase mixing and the Vlasov equation in cosmology](https://arxiv.org/abs/2512.04214)
*Martin Taylor,Renato Velozo Ruiz*

Main category: gr-qc

TL;DR: The paper analyzes Vlasov equation on expanding cosmological spacetimes, showing density decay rates depend on expansion rate and initial data regularity, with phase mixing effects enhancing decay.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time behavior of collisionless matter (Vlasov equation) in expanding cosmological spacetimes (FLRW models), particularly how expansion rates affect density decay and phase mixing effects.

Method: Uses commuting vector fields and combinatorial properties of associated differential operators, with physical space dyadic localization to handle non-compactly supported solutions. The vector fields are not explicit but have good properties for large times relative to momentum support.

Result: For expansion rate t^q (0<q<1/2), density decays at t^{-6q}, with enhanced decay when spatial average removed: polynomial for Sobolev data, super-polynomial but sub-exponential for analytic data. For borderline rate t^{1/2} (radiation universe), logarithmic enhancement for Sobolev data and super-logarithmic (exp(-μ(log t)^ε)) for analytic data.

Conclusion: Expansion rate significantly affects density decay in cosmological spacetimes, with phase mixing providing enhanced decay rates that depend on initial data regularity. The borderline radiation case exhibits different qualitative behavior with logarithmic/super-logarithmic enhancements.

Abstract: We consider the Vlasov equation on slowly expanding isotropic homogeneous tori, described by the Friedmann--Lemaître--Robertson--Walker cosmological spacetimes. For expansion rate $t^q$, with $0< q<\frac{1}{2}$ (excluding certain exceptional values), we show that the spatial density decays at the rate $t^{-6q}$ and that, when the spatial average is removed, the density decays at an enhanced rate due to a phase mixing effect. This enhancement is polynomial for Sobolev initial data and super-polynomial, but sub-exponential, for real analytic initial data. We further show that, when the expansion rate is the borderline $t^{\frac{1}{2}}$ -- the rate which describes a radiation filled universe -- a degenerate phase mixing effect results in a logarithmic enhancement for Sobolev initial data and a super-logarithmic enhancement (in fact, a gain of $\exp(-μ(\log t)^ε)$ for some $μ,ε>0$) for analytic initial data. The proof is based on a collection of commuting vector fields, and certain combinatorial properties of an associated collection of differential operators. The vector fields are not explicit, but are shown to have good properties when $t$ is large with respect to the momentum support of the solution. A physical space dyadic localisation is employed to treat non-compactly supported (in particular, non-trivial real analytic) but suitably decaying solutions.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [51] [Magnetic Field Amplification and Particle Acceleration in Weakly Magnetized Trans-relativistic Electron-ion Shocks](https://arxiv.org/abs/2512.03169)
*Taiki Jikei,Daniel Groselj,Lorenzo Sironi*

Main category: astro-ph.HE

TL;DR: Quasi-parallel trans-relativistic shocks in weakly magnetized plasmas show different acceleration behaviors depending on magnetization: Bell instability dominates at higher magnetizations (σ≳10⁻³) and efficiently accelerates ions, while Weibel instability dominates at lower magnetizations (σ≲10⁻⁴) and accelerates both ions and electrons.


<details>
  <summary>Details</summary>
Motivation: To understand particle acceleration mechanisms in trans-relativistic shocks with weak magnetization, which are relevant to astrophysical phenomena like termination shocks of extragalactic jets, gamma-ray burst afterglows, and fast blue optical transients.

Method: Long-duration two-dimensional particle-in-cell (PIC) simulations to study the physics of quasi-parallel trans-relativistic shocks in weakly magnetized plasmas.

Result: Bell-dominated shocks (σ≳10⁻³) efficiently accelerate ions (ε_i∼0.2) with Bohm scaling (E_max∝t), while Weibel-dominated shocks (σ≲10⁻⁴) accelerate both ions and electrons (ε_i∼ε_e∼0.1) with slower scaling (E_max∝t¹/²).

Conclusion: The competition between Bell and Weibel instabilities shapes shock precursor structure and determines particle acceleration efficiency, with different regimes applicable to various astrophysical trans-relativistic shocks.

Abstract: We investigate the physics of quasi-parallel trans-relativistic shocks propagating in weakly magnetized plasmas by means of long-duration two-dimensional particle-in-cell simulations. The structure of the shock precursor is shaped by a competition between the Bell instability and the Weibel instability. The Bell instability is dominant at relatively high magnetizations $(σ\gtrsim10^{-3})$, whereas the Weibel instability prevails at lower magnetizations $(σ\lesssim10^{-4})$. Bell-dominated shocks efficiently accelerate ions, converting a fraction $\varepsilon_{\mathrm{i}}\sim0.2$ of the upstream flow energy into downstream nonthermal ion energy. The maximum energy of nonthermal ions exhibits a Bohm scaling in time, as $E_{\max}\propto t$. A much smaller fraction $\varepsilon_{\mathrm{e}}\ll0.1$ of the upstream flow energy goes into downstream nonthermal electrons in the Bell-dominated regime. On the other hand, Weibel-dominated shocks efficiently generate both nonthermal ions and electrons with $\varepsilon_{\mathrm{i}}\sim\varepsilon_{\mathrm{e}}\sim0.1$, albeit with a slower scaling for the maximum energy, $E_{\mathrm{max}}\propto t^{1/2}$. Our results are applicable to a wide range of trans-relativistic shocks, including the termination shocks of extragalactic jets, the late stages of gamma-ray burst afterglows, and shocks in fast blue optical transients.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [52] [The Dirichlet heat trace for domains with curved corners](https://arxiv.org/abs/2512.04422)
*Shi Zhuo Looi,David Sher*

Main category: math.SP

TL;DR: The paper studies short-time asymptotics of the Dirichlet heat trace on curvilinear polygons, showing the t^{1/2} coefficient splits into boundary curvature integral plus local corner contributions depending on interior angles and limiting curvatures.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric information encoded in the short-time asymptotics of heat traces on domains with curved boundaries and corners, particularly how curvature and corner geometry contribute to the heat trace expansion.

Method: Uses conformal modeling and parametrix construction on the sector heat space to analyze the Dirichlet heat trace. Expresses corner contributions as a product of a function c_{1/2}(α) (computed via Hadamard finite part of explicit trace) and a factor r_0 depending on angle and curvatures.

Result: Shows the t^{1/2} coefficient splits into boundary integral of κ^2 plus local corner contributions. Computes c_{1/2}(π/2)=1/(16√π) for right-angled corners and obtains closed formula for the t^{1/2} coefficient. Proves that any curvilinear polygon Dirichlet isospectral to a polygon must have straight sides.

Conclusion: The short-time heat trace asymptotics on curvilinear polygons reveals detailed geometric information about both boundary curvature and corner geometry, with applications to spectral geometry including isospectrality results.

Abstract: We study the short-time asymptotics of the Dirichlet heat trace on planar curvilinear polygons. For such domains we show that the coefficient of $t^{1/2}$ in the expansion splits into a boundary integral of $κ^2$ and a sum of local corner contributions, one for each vertex. Each curved corner contribution depends only on the interior angle $α$ and on the limiting curvatures $κ_{\pm}$ on the adjacent sides. Using a conformal model and a parametrix construction on the sector heat space, we express this contribution in the form $c_{1/2}(α)\,r_0(α,κ_+, κ_-)$, where $c_{1/2}(α)$ is given by a Hadamard finite part of an explicit trace over the exact sector. For right-angled corners we compute $c_{1/2}(π/2)=1/(16\sqrtπ)$ and obtain a closed formula for the $t^{1/2}$ coefficient. As an application we extend a previous result in the literature by showing that any admissible curvilinear polygon that is Dirichlet isospectral to a polygon must itself be a polygon with straight sides.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [53] [Mixing at the Batchelor Scale for White-In-Time Flows](https://arxiv.org/abs/2512.04297)
*Robin Chemnitz,Dennis Chemnitz*

Main category: math.PR

TL;DR: The paper proves lower bounds on dissipation rates for advection-diffusion equations with white-in-time velocity fields, verifying the Batchelor scale conjecture for specific velocity fields in 2D and 3D.


<details>
  <summary>Details</summary>
Motivation: To understand mixing properties of advection-diffusion equations with vanishing diffusivity, particularly verifying the Batchelor scale conjecture for specific velocity fields, and characterizing mixing rates without diffusion.

Method: Analyze solutions to advection-diffusion equations on 2D torus with four forced modes and white-in-time velocity fields. Use mathematical analysis to establish lower bounds on exponential dissipation rates as diffusivity approaches zero, and extend results to three-dimensional velocity fields.

Result: Proved that almost-sure exponential dissipation rate stays bounded from below as diffusivity goes to zero, complementing existing upper bounds. Verified Batchelor scale conjecture for these specific velocity fields. Characterized exponential mixing rate without diffusion. Extended results to three-dimensional velocity fields with similar properties.

Conclusion: The paper provides a concrete example where the Batchelor scale conjecture can be verified, establishes rigorous bounds on dissipation rates for vanishing diffusivity, and demonstrates that similar mixing properties hold in both two and three dimensions for white-in-time velocity fields.

Abstract: We consider the mixing properties of solutions to the advection-diffusion equation of a white-in-time velocity field on the 2-dimensional torus with four forced modes. As the diffusivity parameter goes to zero, we show that the almost-sure exponential dissipation rate stays bounded from below. Together with the corresponding upper bound established by Gess and Yaroslavtsev, this constitutes an example of a velocity field for which the Batchelor scale conjecture can be verified. In addition, we characterize the exponential mixing rate without diffusion of this system. Our results are not restricted to two dimensions, and we construct a three-dimensional white-in-time velocity field with the same properties.

</details>


### [54] [Homogenizationof non-divergence form operators in i.i.d. random environments](https://arxiv.org/abs/2512.04410)
*Xiaoqin Guo,Timo Sprekeler,Hung V. Tran*

Main category: math.PR

TL;DR: Improved convergence rates for homogenization of Dirichlet problems in random environments: O(R^{-3/2}) for d=3 and O(R^{-2}log R) for d≥4, surpassing optimal O(R^{-1}) rate for finite-range dependent environments.


<details>
  <summary>Details</summary>
Motivation: To establish improved convergence rates for homogenization of Dirichlet problems associated with non-divergence form difference operators in random environments, going beyond the expected optimal O(R^{-1}) rate for environments with finite range of dependence.

Method: Study random walks in balanced, i.i.d. random environments in ℤ^d for d≥3, analyzing the homogenization of Dirichlet problems for corresponding non-divergence form difference operators.

Result: Achieved improved convergence rates: O(R^{-3/2}) when d=3, and O(R^{-2}log R) when d≥4, which surpass the expected optimal O(R^{-1}) rate for environments with finite range of dependence.

Conclusion: The paper demonstrates that better-than-optimal convergence rates can be achieved for homogenization in random environments, with dimension-dependent improvements that significantly exceed the expected O(R^{-1}) bound for finite-range dependent settings.

Abstract: We study random walks in a balanced, i.i.d. random environment in $\mathbb Z^d$ for $d\geq 3$. We establish improved convergence rates for the homogenization of the Dirichlet problem associated with the corresponding non-divergence form difference operators, surpassing the $O(R^{-1})$ rate, which is expected to be optimal for environments with a finite range of dependence. In particular, the improved rates are $O(R^{-3/2})$ when $d=3$, and $O(R^{-2}\log R)$ when $d\geq 4$.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [55] [Cumulant expansions of operator groups of quantum many-particle systems](https://arxiv.org/abs/2512.05036)
*V. I. Gerasimenko,I. V. Gapyak*

Main category: math-ph

TL;DR: Cluster expansion method for operator groups in von Neumann and Heisenberg equations to construct generating operators for nonperturbative solutions to many-particle quantum systems.


<details>
  <summary>Details</summary>
Motivation: To develop nonperturbative solutions for many-particle quantum systems by addressing the Cauchy problem for hierarchies of evolution equations, which is challenging to solve directly.

Method: Uses cluster expansions for groups of operators associated with von Neumann equations (for states) and Heisenberg equations (for observables) to construct generating operators.

Result: Construction of generating operators that provide nonperturbative solutions to the Cauchy problem for hierarchies of evolution equations in many-particle quantum systems.

Conclusion: The cluster expansion method enables systematic construction of nonperturbative solutions for quantum many-body problems through generating operators for evolution equations.

Abstract: The article presents a method of cluster expansions for groups of operators associated with the von Neumann equations for states and the Heisenberg equations for observables, aiming to construct generating operators for nonperturbative solutions to the Cauchy problem for hierarchies of evolution equations of many-particle quantum systems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [56] [Double Perovskites K2NbTaO6 and Rb2NbTaO6 from First-Principles: Towards Efficient Materials for Green Energy](https://arxiv.org/abs/2512.04134)
*Ouendadji Salima,Aissani Ali,El Haj Hassan Fouad,Benahmedi Lakhdar*

Main category: cond-mat.mtrl-sci

TL;DR: First-principles study of K2NbTaO6 and Rb2NbTaO6 double perovskites reveals their structural, electronic, elastic, optical, and thermoelectric properties, showing mechanical stability, semiconducting behavior, and UV absorption potential.


<details>
  <summary>Details</summary>
Motivation: Double perovskite oxides are attractive for coupled optical, mechanical, and thermal applications due to their structural flexibility and multifunctional nature. This study aims to comprehensively characterize K2NbTaO6 and Rb2NbTaO6 to understand their potential for optoelectronic and photocatalytic applications.

Method: The study uses first-principles computations to examine structural, electronic, elastic, optical, and thermoelectric properties of K2NbTaO6 and Rb2NbTaO6. Analysis includes Born stability criteria for mechanical stability, Pugh's ratio for ductility assessment, band structure analysis for electronic properties, and optical spectra evaluation.

Result: Both compounds form cubic double perovskite structures with ordered Nb⁵⁺ and Ta⁵⁺ cations. They are mechanically stable (satisfying Born criteria) but brittle (per Pugh's ratio). They exhibit semiconducting behavior with band gaps of 2.79 eV (K2NbTaO6) and 2.63 eV (Rb2NbTaO6). Optical spectra show noticeable UV absorption, relevant for optoelectronic and photocatalytic studies, though not implying practical device efficiency.

Conclusion: K2NbTaO6 and Rb2NbTaO6 double perovskites show promising properties for theoretical studies of optoelectronic and photocatalytic applications, particularly due to their UV absorption characteristics and semiconducting behavior, though their brittle nature may limit practical device applications requiring ductility.

Abstract: The structural flexibility and multifunctional nature of double perovskite oxides make them attractive for applications requiring coupled optical, mechanical, and thermal performance. Using first-principles computations, this study examines the structural, electronic, elastic, optical, and thermoelectric stability of K2NbTaO6 and Rb2NbTaO6. The two compounds combine to form a cubic double perovskite structure with ordered Nb$^{5+}$ and Ta$^{5+}$ cations. The calculated elastic constants satisfy the Born stability criteria, confirming mechanical stability; however, both K2NbTaO6 and Rb2NbTaO6 exhibit brittle behavior according to Pugh's ratio, reflecting limited ductility. Semiconducting behavior is revealed by band structure analysis with energy gaps of 2.79 eV for K2NbTaO6 and 2.63 eV for Rb2NbTaO6. Optical spectra show noticeable absorption in the high-energy region near the UV, indicating relevance for theoretical studies of optoelectronic and photocatalytic processes, without implying practical device efficiency. Therm

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [Amortized Inference of Multi-Modal Posteriors using Likelihood-Weighted Normalizing Flows](https://arxiv.org/abs/2512.04954)
*Rajneil Baruah*

Main category: cs.LG

TL;DR: Amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling, showing that Gaussian Mixture Model base distributions improve multi-modal posterior reconstruction.


<details>
  <summary>Details</summary>
Motivation: Efficient inference of theoretical parameters in high-dimensional inverse problems without needing posterior training samples, addressing limitations of standard unimodal base distributions in capturing disconnected support.

Method: Normalizing Flows trained with likelihood-weighted importance sampling, initialized with Gaussian Mixture Models matching target mode cardinality, tested on multi-modal benchmark tasks in 2D and 3D.

Result: Standard unimodal base distributions fail to capture disconnected support, creating spurious probability bridges between modes. GMM initialization significantly improves reconstruction fidelity as measured by distance and divergence metrics.

Conclusion: The topology of base distributions critically impacts modeled posteriors; using GMM base distributions matching target mode cardinality enables better multi-modal posterior estimation in amortized inference.

Abstract: We present a novel technique for amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling. This approach allows for the efficient inference of theoretical parameters in high-dimensional inverse problems without the need for posterior training samples. We implement the method on multi-modal benchmark tasks in 2D and 3D to check for the efficacy. A critical observation of our study is the impact of the topology of the base distributions on the modelled posteriors. We find that standard unimodal base distributions fail to capture disconnected support, resulting in spurious probability bridges between modes. We demonstrate that initializing the flow with a Gaussian Mixture Model that matches the cardinality of the target modes significantly improves reconstruction fidelity, as measured by some distance and divergence metrics.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [58] [A Unified Low-rank ADI Framework with Shared Linear Solves for Simultaneously Solving Multiple Lyapunov, Sylvester, and Riccati Equations](https://arxiv.org/abs/2512.04676)
*Umair Zulfiqar,Zhong-Yi Huang*

Main category: eess.SY

TL;DR: A unified ADI framework that solves multiple Lyapunov, Sylvester, and Riccati equations simultaneously using shared linear solves, while also generating reduced-order models as a byproduct.


<details>
  <summary>Details</summary>
Motivation: Existing ADI methods for different matrix equations (Lyapunov, Sylvester, Riccati) share similar computational structure but are typically solved separately, leading to redundant computational effort. The authors aim to unify these methods to share expensive linear solves across multiple equations.

Method: Proposes a unified ADI framework that recognizes all ADI methods as Petrov-Galerkin projection algorithms. The key insight is that ADI methods differ only in pole placement, not in their interpolatory nature. The framework shares shifted linear solves (the most expensive operations) across multiple equations, requiring only two linear solves per iteration to simultaneously solve 17 different equations (6 Lyapunov, 1 Sylvester, 10 Riccati).

Result: The unified framework substantially reduces computational cost by sharing expensive linear solves. Additionally, it generates reduced-order models as a byproduct that interpolate the original transfer function at mirror images of ADI shifts while preserving important system properties like stability, minimum-phase property, positive-realness, bounded-realness, and passivity.

Conclusion: The proposed unified ADI framework efficiently solves multiple matrix equations simultaneously while also serving as a recursive, interpolation-based model order reduction method that preserves important system properties in the reduced-order models.

Abstract: It is known in the literature that the low-rank ADI method for Lyapunov equations is a Petrov-Galerkin projection algorithm that implicitly performs model order reduction. In this paper, we show that the low-rank ADI methods for Sylvester and Riccati equations are also Petrov-Galerkin projection algorithms that implicitly perform model order reduction. By observing that the ADI methods for Lyapunov, Sylvester, and Riccati equations differ only in pole placement and not in their interpolatory nature, we show that the shifted linear solves-which constitute the bulk of the computational cost-can be shared. The pole-placement step involves only small-scale operations and is therefore inexpensive. We propose a unified ADI framework that requires only two shifted linear solves per iteration to simultaneously solve six Lyapunov equations, one Sylvester equation, and ten Riccati equations, thus substantially increasing the return on investment for the computational cost spent on the linear solves. All operations needed to extract the individual solutions from these shared linear solves are small-scale and inexpensive.
  Since all ADI methods implicitly perform model order reduction when solving these linear matrix equations, we show that the resulting reduced-order models can be obtained as an additional byproduct. These models not only interpolate the original transfer function at the mirror images of the ADI shifts but also preserve important system properties such as stability, minimum-phase property, positive-realness, bounded-realness, and passivity. Consequently, the proposed unified ADI framework also serves as a recursive, interpolation-based model order reduction method, which can preserve several important properties of the original model in the reduced-order model.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [59] [CNN on `Top': In Search of Scalable & Lightweight Image-based Jet Taggers](https://arxiv.org/abs/2512.05031)
*Rajneil Baruah,Subhadeep Mondal,Sunando Kumar Patra,Satyajit Roy*

Main category: hep-ph

TL;DR: Lightweight EfficientNet-based architecture for jet classification achieves competitive performance with lower computational cost compared to Transformers and GNNs.


<details>
  <summary>Details</summary>
Motivation: Transformer-based and standard GNNs show strong performance for jet classification but require substantial computational power, creating a need for more efficient alternatives.

Method: Uses a lightweight and scalable version of EfficientNet architecture combined with global jet features to create a computationally inexpensive network.

Result: The network achieves competitive performance for tagging top-quark jets among light-quark and gluon jets while being computationally inexpensive.

Conclusion: EfficientNet-based approach provides a viable, computationally efficient alternative to Transformer and GNN models for jet classification tasks.

Abstract: While Transformer-based and standard Graph Neural Networks (GNNs) have proven to be the best performers in classifying different types of jets, they require substantial computational power. We explore the scope of using a lightweight and scalable version of the EfficientNet architecture, along with global features of the jet. The end product is computationally inexpensive but is capable of competitive performance. We showcase the efficacy of our network for tagging top-quark jets in a sea of other light-quark and gluon jets.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [60] [High-repetition-rate, all-reflective optical guiding and electron acceleration in helium using an off-axis axicon](https://arxiv.org/abs/2512.04788)
*Jiří Šišma,Michal Nevrkla,Filip Vitha,Sebastian Lorenz,Illia Zymak,Alžběta Špádová,Andrea Kollárová,Matěj Jech,Alexandr Jančárek,Davorin Peceli,Carlo M. Lazzarini,Leonardo V. N. Goncalves,Gabriele M. Grittani,Sergei V. Bulanov,Jaron E. Shrock,Ela Rockafellow,Ari J. Sloss,Bo Miao,Scott W. Hancock,Howard M. Milchberg*

Main category: physics.acc-ph

TL;DR: High-power laser wakefield acceleration using self-waveguiding in plasma channels achieves stable 5 GeV electron beams with novel all-reflective optics enabling high repetition rates.


<details>
  <summary>Details</summary>
Motivation: To develop stable, high-repetition-rate laser wakefield acceleration technology that can be broadly adopted across user facilities without requiring complex laser system modifications.

Method: Used the L3-HAPLS laser system (13 J, 30 fs, 0.2 Hz) with self-waveguiding in a 20 cm plasma channel in helium. Implemented a novel all-reflective optical setup including an off-axis reflective axicon for efficient acceleration and guiding at high repetition rates.

Result: Achieved stable acceleration of electron beams to energies approaching 5 GeV. Demonstrated efficient acceleration at 0.2 Hz and guiding at repetition rates up to 3.3 Hz. The compact single laser, single compressor implementation stabilizes electron pointing and enhances energy gain.

Conclusion: This approach provides a practical pathway for broader adoption of laser wakefield acceleration technology across user facilities by offering stable, high-energy electron beams without requiring modifications to existing laser systems.

Abstract: We present recent results on high-power guiding and laser wakefield acceleration (LWFA) in the ELBA beamline at ELI Beamlines, using the L3-HAPLS laser system (13 J, 30 fs, 0.2 Hz). By employing self-waveguiding in a 20 cm plasma channel in helium, we achieved stable acceleration of electron beams to energies approaching 5 GeV. A novel all-reflective optical setup, including an off-axis reflective axicon, enabled efficient acceleration at 0.2 Hz and guiding at repetition rates up to 3.3 Hz. This compact single laser, single compressor implementation of plasma channels for electron acceleration stabilizes electron pointing and enhances energy gain without requiring modifications to the laser system, paving the way for broader adoption of the technology across user facilities.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [61] [Degrees of universality in wave turbulence](https://arxiv.org/abs/2512.04866)
*Jiasheng Liu,Vladimir Rosenhaus,Gregory Falkovich*

Main category: cond-mat.stat-mech

TL;DR: Weak wave turbulence shows universality, but inverse cascades transition from weak to strong turbulence with new universalities. Spin-wave turbulence becomes nonlocal and strong far from pumping, unlike NSE models. Strong turbulence in multi-component models shows critical balance, with large-scale turbulence decreasing with pumping level.


<details>
  <summary>Details</summary>
Motivation: To understand how inverse turbulent cascades transition from weak to strong turbulence, and to explore the universality properties of wave turbulence across different systems (spin waves, NSE, MMT-like models). The paper investigates how spectral nonlocality affects turbulence strength and dependence on pumping scales.

Method: Comparative analysis of turbulence in different wave systems: spin waves in ferromagnets, Nonlinear Schrödinger Equation (NSE), and MMT-like models in higher dimensions. Examination of vertex renormalization effects, spectral nonlocality, and one-loop corrections. Study of multi-component versions with large number of components to analyze strong turbulence behavior.

Result: Spin-wave turbulence becomes nonlocal and transitions to strong turbulence far from pumping scale, even when wave interactions are weak. Vertex renormalization causes dependence on UV pumping scale in spin waves but not in NSE models. Strong spin-wave turbulence exhibits critical balance similar to focusing NSE. Large-scale turbulence decreases with increasing pumping level, eventually becoming independent of pumping.

Conclusion: Weak wave turbulence universality breaks down in inverse cascades, revealing new universalities in strong turbulence. Spectral nonlocality enhances nonlinearity, causing spin-wave turbulence to become strong despite weak local interactions. Strong turbulence in multi-component models shows critical balance behavior, with the counterintuitive result that increased pumping can decrease large-scale turbulence levels.

Abstract: Turbulence of weakly interacting waves displays a great deal of universality: independence of the details of the interaction and of the pumping and dissipation scales. Here we study how inverse turbulent cascades (from small to large scales) transition from weak to strong. We find that while one-loop corrections can be dependent on excitation and dissipation scales, new types of universality appear in strong turbulence. We contrast turbulence of spin waves in ferromagnets with turbulent cascades in the Nonlinear Schrödinger Equation (NSE) and in an MMT-like model in higher dimensions having a multiplicative interaction vertex: vertex renormalization gives rise to dependence on the pumping (UV scale) in the former but not in the latter. As a result of this spectral nonlocality, spin-wave turbulence stops being weak if one is sufficiently far from the pumping scale, even when the interaction of waves with comparable wavenumbers is weak. We paraphrase this as: nonlocality enhances nonlinearity.
  We then describe strong turbulence in a multi-component version of these models with a large number of components. We argue that strong spin-wave turbulence is similar to turbulence of the focusing NSE, as it realizes a critical-balance state. However, UV nonlocality causes the level of spin-wave turbulence at large scales to decrease with increasing pumping level, culminating in a state that is independent of the level of pumping.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [62] [Fermionic neural Gibbs states](https://arxiv.org/abs/2512.04663)
*Jannes Nys,Juan Carrasquilla*

Main category: quant-ph

TL;DR: fNGS is a neural network framework for modeling finite-temperature properties of strongly interacting fermions, achieving accurate thermal energy predictions for doped Fermi-Hubbard models beyond exact methods.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable method for studying finite-temperature properties of strongly correlated fermionic systems beyond one dimension, which are challenging for exact computational methods.

Method: Uses fermionic neural Gibbs states (fNGS) starting from mean-field thermofield-double reference state, applying neural-network transformations with imaginary-time evolution to build strong correlations.

Result: Accurately reproduces thermal energies for doped Fermi-Hubbard model across broad temperature ranges, interaction strengths, and large dopings, for system sizes beyond exact method capabilities.

Conclusion: fNGS provides a scalable route to study finite-temperature properties of strongly correlated fermionic systems using neural-network quantum state representations.

Abstract: We introduce fermionic neural Gibbs states (fNGS), a variational framework for modeling finite-temperature properties of strongly interacting fermions. fNGS starts from a reference mean-field thermofield-double state and uses neural-network transformations together with imaginary-time evolution to systematically build strong correlations. Applied to the doped Fermi-Hubbard model, a minimal lattice model capturing essential features of strong electronic correlations, fNGS accurately reproduces thermal energies over a broad range of temperatures, interaction strengths, even at large dopings, for system sizes beyond the reach of exact methods. These results demonstrate a scalable route to studying finite-temperature properties of strongly correlated fermionic systems beyond one dimension with neural-network representations of quantum states.

</details>


### [63] [Convergence of sample-based quantum diagonalization on a variable-length cuprate chain](https://arxiv.org/abs/2512.04962)
*L. Andrew Wray,Cheng-Ju Lin,Vincent Su,Hrant Gharibyan*

Main category: quant-ph

TL;DR: SQD algorithm for NISQ devices shows improved convergence with all-to-all connectivity, higher expansion order, non-Hartree-Fock basis, and surprisingly benefits from quantum noise.


<details>
  <summary>Details</summary>
Motivation: SQD is promising for NISQ-era quantum chemistry but suffers from convergence issues that limit practical application.

Method: Tested SQD on variable-length copper oxide plaquettes (2-6 units) with minimal basis, exploring connectivity, expansion order, orbital basis, and real quantum hardware noise effects.

Result: All-to-all connectivity, higher expansion order, and non-Hartree-Fock basis help overcome sampling bottlenecks. Real quantum noise on Quantinuum H2 device actually improves energy convergence beyond noise-free simulations.

Conclusion: Multiple strategies exist to improve SQD convergence, with tradeoffs to consider against hardware capabilities. Surprisingly, quantum noise can be beneficial rather than detrimental for algorithm performance.

Abstract: Sample-based quantum diagonalization (SQD) is an algorithm for hybrid quantum-classical molecular simulation that has been of broad interest for application with noisy intermediate scale quantum (NISQ) devices. However, SQD does not always converge on a practical timescale. Here, we explore scaling of the algorithm for a variable-length molecule made up of 2 to 6 copper oxide plaquettes with a minimal molecular orbital basis. The results demonstrate that enabling all-to-all connectivity, instituting a higher expansion order for the SQD algorithm, and adopting a non-Hartree-Fock molecular orbital basis can all play significant roles in overcoming sampling bottlenecks, though with tradeoffs that need to be weighed against the capabilities of quantum and classical hardware. Additionally, we find that noise on a real quantum computer, the Quantinuum H2 trapped ion device, can improve energy convergence beyond expectations based on noise-free statevector simulations.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [64] [NORi: An ML-Augmented Ocean Boundary Layer Parameterization](https://arxiv.org/abs/2512.04452)
*Xin Kai Lee,Ali Ramadhan,Andre Souza,Gregory LeClaire Wagner,Simone Silvestri,John Marshall,Raffaele Ferrari*

Main category: physics.ao-ph

TL;DR: NORi is a physics-based ML parameterization using neural ODEs to model ocean boundary layer turbulence, trained on LES data to capture entrainment dynamics with excellent generalization and numerical stability.


<details>
  <summary>Details</summary>
Motivation: Traditional local diffusive closures fail to capture entrainment through the base of ocean boundary layers, which is crucial for climate modeling. There's a need for parameterizations that combine physical rigor with data-driven approaches while maintaining numerical stability and generalization across different ocean conditions.

Method: NORi combines a physics-based Richardson number-dependent diffusivity/viscosity closure with neural ODEs. It's trained "a posteriori" using large-eddy simulations with a loss function that depends on time-integrated variables rather than noisy instantaneous subgrid fluxes. The approach targets the realistic nonlinear equation of state of seawater.

Result: NORi demonstrates excellent prediction and generalization across different convective strengths, oceanic stratifications, rotation strengths, and surface wind forcings. It remains numerically stable for at least 100 years of integration despite being trained on only 2-day horizons, and can run with hour-long time steps.

Conclusion: The combination of highly expressive neural networks with physically-rigorous base closures provides a robust paradigm for climate model parameterizations, drastically reducing data requirements while enabling direct optimization of inference performance and implicit encouragement of numerical stability during training.

Abstract: NORi is a machine-learned (ML) parameterization of ocean boundary layer turbulence that is physics-based and augmented with neural networks. NORi stands for neural ordinary differential equations (NODEs) Richardson number (Ri) closure. The physical parameterization is controlled by a Richardson number-dependent diffusivity and viscosity. The NODEs are trained to capture the entrainment through the base of the boundary layer, which cannot be represented with a local diffusive closure. The parameterization is trained using large-eddy simulations in an "a posteriori" fashion, where parameters are calibrated with a loss function that explicitly depends on the actual time-integrated variables of interest rather than the instantaneous subgrid fluxes, which are inherently noisy. NORi is designed for the realistic nonlinear equation of state of seawater and demonstrates excellent prediction and generalization capabilities in capturing entrainment dynamics under different convective strengths, oceanic background stratifications, rotation strengths, and surface wind forcings. NORi is numerically stable for at least 100 years of integration time in large-scale simulations, despite only being trained on 2-day horizons, and can be run with time steps as long as one hour. The highly expressive neural networks, combined with a physically-rigorous base closure, prove to be a robust paradigm for designing parameterizations for climate models where data requirements are drastically reduced, inference performance can be directly targeted and optimized, and numerical stability is implicitly encouraged during training.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [65] [A hybrid Green-Kubo (hGK) framework for calculating viscosity from short MD simulations](https://arxiv.org/abs/2512.04546)
*Akash K. Meel,Santosh Mogurampelly*

Main category: cond-mat.soft

TL;DR: Hybrid Green-Kubo (hGK) framework for viscosity calculation from MD simulations that partitions stress autocorrelation function into short-time ballistic component from MD and long-time relaxation tail using analytical functions, achieving orders of magnitude computational savings.


<details>
  <summary>Details</summary>
Motivation: Traditional Green-Kubo viscosity calculations suffer from poor convergence and require extensive phase space sampling, making them computationally demanding for soft matter and polymer systems.

Method: Hybrid Green-Kubo framework partitions stress autocorrelation function into: (1) short-time ballistic component extracted from short MD simulations, and (2) long-time relaxation tail represented using analytically motivated functions fitted to short trajectories.

Result: Excellent agreement with established results for SPC/E water; successful application to challenging electrolyte systems (EC-LiTFSI and PEO-LiTFSI) where traditional GK fails; substantial computational savings of several orders of magnitude without compromising accuracy.

Conclusion: The hGK framework provides a conceptually simple, broadly applicable, and computationally efficient route for viscosity prediction in molecular liquids, polymer melts, and ionically conducting soft materials, with clear avenues for refinement.

Abstract: Viscosity calculation from equilibrium molecular dynamics (MD) simulations relies on the traditional Green-Kubo (GK) framework, which integrates the stress autocorrelation function (SACF) over time. While the formalism is exact in the linear response regime, the traditional approach often suffers from poor convergence and requires extensive phase space sampling, which is computationally demanding for soft matter and polymer systems. In this Letter, we introduce a hybrid Green-Kubo (hGK) framework that alleviates these limitations by partitioning the SACF into two physically meaningful regimes: (i) a short time ballistic component extracted directly from short MD simulations, and (ii) a long time relaxation tail represented using analytically motivated functions, $φ(τ)$, fitted only to short trajectories. This strategy bypasses the need for extensive sampling while preserving physical rigor. Benchmarking against SPC/E water confirms excellent agreement with established results, and we further demonstrate the efficacy of the method for challenging electrolyte systems (EC-LiTFSI and PEO-LiTFSI), for which the GK framework fails to converge. The computational savings are substantial, with reductions of several orders of magnitude in required sampling, achieved without compromising predictive accuracy. We also discuss the limitations of the hGK framework and outline clear avenues for refinement, including optimal tail selection and robust identification of relaxation regimes in noisy stress data. The hGK framework presented in this Letter provides a conceptually simple, broadly applicable, and computationally efficient route for viscosity prediction in molecular liquids, polymer melts, and ionically conducting soft materials.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [66] [Effective permeabilities for flow through anisotropic microscopic geometries](https://arxiv.org/abs/2512.04133)
*Loïc Balazi,Fabian Holzberger,Stephan B. Lunowa,Malte A. Peter,Daniel Peterseim,Barbara Wohlmuth*

Main category: physics.flu-dyn

TL;DR: A computational framework for determining anisotropic permeability in fibrous microstructures, validated for coiled aneurysm domains, showing significant directional flow effects missed by isotropic models.


<details>
  <summary>Details</summary>
Motivation: Accurately model flow in coiled aneurysm domains by capturing directional permeability variations induced by fiber orientation in dense, fibrous microstructures.

Method: Combines homogenization theory with fully resolved simulations in Representative Elementary Volumes (REVs) to validate Boutin's permeability model, then incorporates resulting permeability tensors into macroscopic Darcy flow simulations.

Result: Anisotropy significantly impacts local flow direction and magnitude, creating directional permeability contrasts that isotropic approximations cannot reproduce, enabling more realistic assessment of post-treatment aneurysm flow.

Conclusion: The framework integrates coil-induced microstructural effects into continuum-scale hemodynamic models for better aneurysm flow assessment, with broader applicability to other fibrous porous systems.

Abstract: This work develops a computational and theoretical framework for determining effective permeabilities in anisotropic microscopic geometries containing dense, fibre-like obstacles, motivated by the need to model flow in coiled aneurysm domains accurately. Building on homogenisation theory and fully resolved simulations in Representative Elementary Volumes (REVs), we validate the permeability model introduced in [C. Boutin, Study of permeability by periodic and self-consistent homogenisation. Eur. J. Mech. A Solids, 19(4):603-632, 2000] and propose a systematic methodology for capturing the directional variations induced by fibre orientation. The resulting permeability tensors are incorporated into macroscopic flow simulations based on the Darcy equation, enabling direct comparison of anisotropic and isotropic permeability models across several benchmark configurations. Our findings show that anisotropy has a significant impact on local flow direction and magnitude, generating directional permeability contrasts which cannot be reproduced by classical isotropic approximations. By integrating coil-induced microstructural effects into continuum-scale hemodynamic models, the proposed approach enables more realistic assessment of post-treatment aneurysm flow behaviour. Beyond this clinical application, the framework is broadly applicable to other biomedical and engineering systems involving fibrous or filamentous porous microstructures.

</details>


### [67] [Can Explicit Subgrid Models Enhance Implicit LES Simulations? A GPU-Oriented High-Order-Solver Perspective](https://arxiv.org/abs/2512.04574)
*Gonzalo Rubio,Gerasimos Ntoukas,Miguel Chávez-Módena,Oscar Mariño,Bernat Font,Oriol Lehmkuhl,Eusebio Valero,Esteban Ferrer*

Main category: physics.flu-dyn

TL;DR: High-order DG methods on GPUs are efficient for turbulence, but under-resolution at high Re requires careful balance between implicit DG dissipation (split forms) and explicit LES models.


<details>
  <summary>Details</summary>
Motivation: High-order DG methods on GPUs are efficient for turbulence simulation, but even high-order discretizations at high Reynolds numbers suffer from under-resolution, making simulations sensitive to numerical dissipation and aliasing effects. Need to understand interplay between implicit DG dissipation mechanisms and explicit LES models.

Method: Investigate interplay between intrinsic DG dissipation mechanisms (split forms and Riemann solvers) and explicit subgrid-scale LES models. Use 3D Taylor-Green vortex at Re=1600 and inviscid case (Re→∞). Evaluate kinetic energy dissipation, spectral accuracy, and numerical stability.

Result: 1) When stability ensured through energy-/entropy-stable split forms, explicit SGS models not strictly necessary. 2) At moderate Re with sufficient resolution (well-resolved LES), adding SGS models doesn't improve accuracy due to overlap with inherent DG dissipation. 3) At high Re with insufficient resolution, explicit SGS models complement numerical dissipation and enhance accuracy by removing excess energy that numerical fluxes alone cannot dissipate.

Conclusion: Provides practical guidance for choosing numerical strategies in high-order turbulence simulations: Use split forms for stability at moderate Re; add explicit SGS models only when resolution is insufficient at high Re to complement inherent DG dissipation.

Abstract: High-order Discontinuous Galerkin (DG) methods offer excellent accuracy for turbulent flow simulations, especially when implemented on GPU-oriented architectures that favor very high polynomial orders. On modern GPUs, high-order polynomial evaluations cost roughly the same as low-order ones, provided the DG degrees of freedom fit within device memory. However, even with high-order discretizations, simulations at high Reynolds numbers still require some level of under-resolution, leaving them sensitive to numerical dissipation and aliasing effects. Here, we investigate the interplay between intrinsic DG dissipation mechanisms (implicit dissipation) -- in particular split forms and Riemann solvers -- and explicit subgrid-scale models in Large Eddy Simulations (LES). Using the three-dimensional Taylor--Green vortex at $Re = 1600$ and an inviscid case ($Re \to \infty$), we evaluate kinetic energy dissipation, spectral accuracy, and numerical stability.
  Our results show that when stability for under-resolved turbulence is ensured through split-forms (energy- or entropy-stable) schemes, subgrid-scale (SGS) LES models are not strictly necessary. At moderate Reynolds numbers, when the spatial resolution is sufficient to capture the relevant turbulence scales (i.e., in well-resolved LES), adding SGS models does not improve accuracy because the wavenumber range where they act overlaps with the inherent numerical dissipation of the DG scheme. In contrast, when the resolution is insufficient, as is typical at high Reynolds numbers, explicit subgrid-scale models complement the numerical dissipation and enhance accuracy by removing the excess energy that numerical fluxes alone cannot dissipate.
  These findings provide practical guidance for choosing numerical strategies in high-order turbulence simulations.

</details>
