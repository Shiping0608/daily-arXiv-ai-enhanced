<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 6]
- [math.AP](#math.AP) [Total: 21]
- [physics.comp-ph](#physics.comp-ph) [Total: 8]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [physics.optics](#physics.optics) [Total: 2]
- [gr-qc](#gr-qc) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Weighted total variation regularization for inverse problems with significant null spaces](https://arxiv.org/abs/2512.04729)
*Martin Burger,Ole L√∏seth Elvetun,Bj√∏rn Fredrik Nielsen*

Main category: math.NA

TL;DR: Weighted TV regularization improves source localization for inverse problems with large null spaces, recovering location and size of extended sources away from boundaries, unlike standard methods that bias toward data acquisition sites.


<details>
  <summary>Details</summary>
Motivation: Standard regularization methods for inverse problems with large null spaces (like inverse ECG/EEG) produce solutions biased toward data acquisition sites, misinterpreting internal sources as being near measurement boundaries. This leads to inadequate reconstructions in medical applications.

Method: Extends previous weighting schemes to total variation (TV) regularization, introducing weighted TV regularization with supporting analysis. Also explores hybrid weighted-sparsity and TV regularization to capture both small and large sources.

Result: Weighted TV successfully recovers location and size of large, piecewise constant sources away from boundaries, though not their exact shape. Hybrid approach captures both small and large sources but with more blurred reconstructions than weighted TV alone.

Conclusion: Weighted TV regularization addresses the bias problem in inverse problems with large null spaces, providing better source localization for extended regions, with hybrid methods offering additional flexibility for mixed source scenarios.

Abstract: We consider inverse problems with large null spaces, which arise in important applications such as in inverse ECG and EEG procedures. Standard regularization methods typically produce solutions in or near the orthogonal complement of the forward operator's null space. This often leads to inadequate results, where internal sources are mistakenly interpreted as being near the data acquisition sites -- e.g., near or at the body surface in connection with EEG and ECG recordings.
  To mitigate this, we previously proposed weighting schemes for Tikhonov and sparsity regularization. Here, we extend this approach to total variation (TV) regularization, which is particularly suited for identifying spatially extended regions with approximately constant values. We introduce a weighted TV-regularization method, provide supporting analysis, and demonstrate its performance through numerical experiments. Unlike standard TV regularization, the weighted version successfully recovers the location and size of large, piecewise constant sources away from the boundary, though not their exact shape.
  Additionally, we explore a hybrid weighted-sparsity and TV regularization approach, which better captures both small and large sources, albeit with somewhat more blurred reconstructions than the weighted TV method alone.

</details>


### [2] [Construction of the Nearest Nonnegative Hankel Matrix for a Prescribed Eigenpair](https://arxiv.org/abs/2512.04812)
*Prince Kanhya,Udit Raj*

Main category: math.NA

TL;DR: Computes minimum perturbation to make a given eigenpair exact for nonnegative Hankel matrices, or finds nearest matrix when impossible.


<details>
  <summary>Details</summary>
Motivation: Need to assess eigenpair sensitivity under structured perturbations that preserve Hankel structure and nonnegativity constraints, which lacks closed-form solutions.

Method: Formulates as feasibility check of linear constraints for Hankel structure and nonnegativity; solves optimization problems for minimum-norm perturbation or residual minimization.

Result: Provides numerical optimization framework for computing structured backward errors; demonstrates both feasible and infeasible cases with examples.

Conclusion: Offers practical computational approach for eigenpair sensitivity analysis under nonnegative Hankel perturbations where analytical solutions are unavailable.

Abstract: We study the problem of determining whether a prescribed eigenpair $(Œª,x)$
  can be made an exact eigenpair of a nonnegative Hankel matrix through the smallest
  possible structured perturbation. The task reduces to check the feasibility of a
  set of linear constraints that encode both the Hankel structure and entrywise
  nonnegativity. When the feasibility set is nonempty, we compute the minimum-norm
  perturbation $ŒîH$ such that $(H+ŒîH)x=Œªx$. When no such perturbation
  exists, we compute the nearest nonnegative Hankel matrix in a residual sense by
  minimizing $\|(H+ŒîH)x-Œªx\|_{2}$ subject to the imposed constraints.
  Because closed-form formulas for the structured backward error are generally
  unavailable, our method provides a fully numerical and optimization-based
  framework for evaluating eigenpair sensitivity under nonnegativity-preserving
  Hankel perturbations. Numerical examples illustrate both feasible and infeasible cases.

</details>


### [3] [Weak convergence rates for spectral regularization via sampling inequalities](https://arxiv.org/abs/2512.04929)
*Sabrina Guastavino,Gabriele Santin,Francesco Marchetti,Federico Benvenuto*

Main category: math.NA

TL;DR: The paper establishes weak convergence rates for inverse problems using spectral regularization methods without requiring source conditions, by connecting inverse problems with kernel approximation and generalizing sampling inequalities.


<details>
  <summary>Details</summary>
Motivation: Classical convergence rate results for inverse problems rely on source conditions to estimate truncation error. The authors aim to derive convergence rates independently of source conditions by exploiting connections between inverse problems and kernel approximation.

Method: 1) Generalize sampling inequalities to spectral regularization methods. 2) Exploit the connection between inverse problems and kernel approximation. 3) Derive weak convergence rate bounds for inverse problems without source conditions.

Result: Established weak convergence rate bounds for inverse problems when: 1) the forward operator is compact and uniformly bounded, or 2) the kernel operator is of trace class. These results are independent of source conditions.

Conclusion: The paper demonstrates that weak convergence rates for inverse problems can be obtained without source conditions by leveraging connections with kernel approximation and generalizing sampling inequalities to spectral regularization methods.

Abstract: Convergence rates in spectral regularization methods quantify the approximation error in inverse problems as a function of the noise level or the number of sampling points. Classical strong convergence rate results typically rely on source conditions, which are essential for estimating the truncation error. However, in the framework of kernel approximation, the truncation error in the case of Tikhonov regularization can be characterized entirely through sampling inequalities, without invoking source conditions. In this paper, we first generalize sampling inequalities to spectral regularization, and then, by exploiting the connection between inverse problems and kernel approximation, we derive weak convergence rate bounds for inverse problems, independently of source conditions. These weak convergence rates are established and analyzed when the forward operator is compact and uniformly bounded, or the kernel operator is of trace class.

</details>


### [4] [A tangential low-rank ADI method for solving indefinite Lyapunov equations](https://arxiv.org/abs/2512.04983)
*Rudi Smith,Steffen W. R. Werner*

Main category: math.NA

TL;DR: Novel tangential ADI iteration for solving large-scale Lyapunov equations with indefinite right-hand sides, enabling efficient low-rank approximations even for high-rank constant terms.


<details>
  <summary>Details</summary>
Motivation: Classical block-type approaches for solving continuous-time algebraic Lyapunov equations become computationally expensive when the rank of the constant term grows, especially for large-scale sparse systems with indefinite right-hand sides.

Method: Proposes a tangential reformulation of the ADI (alternating direction implicit) iteration that efficiently constructs low-rank approximations to Lyapunov equation solutions with indefinite right-hand sides. Includes adaptive methods for selecting ADI parameters (shifts and tangential directions) for automatic application.

Result: Developed algorithms effectively handle Lyapunov equations with high-rank constant terms, as demonstrated through several numerical examples showing the method's efficiency.

Conclusion: The tangential ADI iteration provides an efficient computational framework for solving large-scale Lyapunov equations with indefinite right-hand sides, overcoming limitations of classical block approaches when dealing with high-rank constant terms.

Abstract: Continuous-time algebraic Lyapunov equations have become an essential tool in various applications. In the case of large-scale sparse coefficient matrices and indefinite constant terms, indefinite low-rank factorizations have successfully been used to allow methods like the alternating direction implicit (ADI) iteration to efficiently compute accurate approximations to the solution of the Lyapunov equation. However, classical block-type approaches quickly increase in computational costs when the rank of the constant term grows. In this paper, we propose a novel tangential reformulation of the ADI iteration that allows for the efficient construction of low-rank approximations to the solution of Lyapunov equations with indefinite right-hand sides even in the case of constant terms with higher ranks. We provide adaptive methods for the selection of the corresponding ADI parameters, namely shifts and tangential directions, which allow for the automatic application of the method to any relevant problem setting. The effectiveness of the developed algorithms is illustrated by several numerical examples.

</details>


### [5] [Stable self-adaptive timestepping for Reduced Order Models for incompressible flows](https://arxiv.org/abs/2512.04592)
*Josep Plana-Riu,Henrik Rosenberger,Benjamin Sanderse,F. Xavier Trias*

Main category: math.NA

TL;DR: RedEigCD: First self-adaptive timestepping for ROMs of incompressible Navier-Stokes equations using linear stability concepts and exact spectral information to bound stability functions.


<details>
  <summary>Details</summary>
Motivation: Traditional error-based adaptive methods for reduced-order models (ROMs) may not be optimal. Need for self-adaptive timestepping that preserves online efficiency while ensuring stability for ROMs of incompressible flow simulations.

Method: Adapts timestep by bounding stability function using exact spectral information of reduced operators. Uses eigenbounds of convective and diffusive ROM operators computed at reduced scale. Based on linear stability concepts and theorems of Bendixson and Rao.

Result: Proof that maximum stable timestep for projection-based ROMs is ‚â• that of full-order models under linearized assumptions. Numerical experiments show stable timestep increases up to 40√ó compared to FOM without accuracy loss.

Conclusion: RedEigCD establishes new link between linear stability theory and ROMs, offering systematic path for efficient, self-regulating ROM integration in incompressible flow simulations.

Abstract: This work introduces RedEigCD, the first self-adaptive timestepping technique specifically tailored for reduced-order models (ROMs) of the incompressible Navier-Stokes equations. Building upon linear stability concepts, the method adapts the timestep by directly bounding the stability function of the employed time integration scheme using exact spectral information of matrices related to the reduced operators. Unlike traditional error-based adaptive methods, RedEigCD relies on the eigenbounds of the convective and diffusive ROM operators, whose computation is feasible at reduced scale and fully preserves the online efficiency of the ROM. A central theoretical contribution of this work is the proof, based on the combined theorems of Bendixson and Rao, that, under linearized assumptions, the maximum stable timestep for projection-based ROMs is shown to be larger than or equal to that of their corresponding full-order models (FOMs). Numerical experiments for both periodic and non-homogeneous boundary conditions demonstrate that RedEigCD yields stable timestep increases up to a factor 40 compared to the FOM, without compromising accuracy. The methodology thus establishes a new link between linear stability theory and reduced-order modeling, offering a systematic path towards efficient, self-regulating ROM integration in incompressible flow simulations.

</details>


### [6] [Hierarchical matrix approximability of inverse of convection dominated finite element matrices](https://arxiv.org/abs/2512.04824)
*Arthur Saunier,Leo Agelas,Ani Anciaux Sedrakian,Ibtihel Ben Gharbia,Xavier Claeys*

Main category: math.NA

TL;DR: Novel convection tube partitioning strategy for hierarchical matrices enables efficient compression of convection-dominated PDEs on unstructured grids, overcoming limitations of previous methods.


<details>
  <summary>Details</summary>
Motivation: Hierarchical matrices (H-matrices) are effective for compressing large matrices from boundary integral equations, but their performance degrades for convection-dominated problems due to loss of coercivity. Existing methods only work for structured grids with constant convection, limiting practical applications.

Method: Proposes a novel partitioning strategy based on "convection tubes" - clusters aligned with the convection vector field. This approach doesn't require structured grids or constant convection, and builds on a P√©clet-robust Caccioppoli inequality for handling convection-dominated problems.

Result: Theoretical analysis and numerical experiments demonstrate the efficiency and robustness of the method for convection-dominated PDEs on unstructured grids, overcoming limitations of previous approaches.

Conclusion: The convection tube partitioning strategy extends hierarchical matrix techniques to convection-dominated problems on general unstructured grids, providing a flexible and scalable framework with nearly linear complexity.

Abstract: Several researchers have developed a rich toolbox of matrix compression techniques that exploit structure and redundancy in large matrices. Classical methods such as the block low-rank format and the Fast Multipole Method make it possible to manipulate very large systems by representing them in a reduced form. Among the most sophisticated tools in this area are hierarchical matrices (H-matrices), which exploit local properties of the underlying kernel or operator to approximate matrix blocks by low-rank factors, organized in a recursive hierarchy. H-matrices offer a flexible and scalable framework, yielding nearly linear complexity in both storage and computation. Hierarchical matrix techniques, originally developed for boundary integral equations, have recently been applied to matrices stemming from the discretization of advection-dominated problems. However, their effectiveness is limited by the loss of coercivity induced by convection phenomena, where traditional methods fail. Initial work by Le Borne addressed this by modifying the admissibility criterion for structured grids with constant convection, but challenges remain for more general grids and advection fields. In this work, we propose a novel partitioning strategy based on "convection tubes", clusters aligned with the convection vector field. This method does not require a structured grid or constant convection, overcoming the limitations of previous approaches. We present both theoretical analyses and numerical experiments, that demonstrate the efficiency and robustness of our method for convection-dominated PDEs on unstructured grids. The approach builds on a P√©clet-robust Caccioppoli inequality, crucial for handling convection-dominated problems.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [7] [Regularity for minimizers of degenerate, non-autonomous, orthotropic integral functionals](https://arxiv.org/abs/2512.04281)
*Antonio Giuseppe Grimaldi,Stefania Russo*

Main category: math.AP

TL;DR: The paper proves higher differentiability (integer order) for locally bounded minimizers of anisotropic, non-autonomous integral functionals with variable exponents and coefficients.


<details>
  <summary>Details</summary>
Motivation: To establish higher regularity properties for minimizers of anisotropic variational problems with non-autonomous structure, extending known results to more general functionals that depend on both spatial variables and the solution itself.

Method: Analytical approach using variational methods and Sobolev space theory to prove higher differentiability of integer order for locally bounded minimizers, under suitable Sobolev regularity assumptions on the coefficients a_i(x) and exponents p_i ‚â• 2.

Result: Successfully proves that locally bounded minimizers of the given anisotropic, non-autonomous integral functionals possess higher differentiability of integer order.

Conclusion: The main contribution is extending regularity theory to anisotropic, non-autonomous functionals with variable exponents and coefficients, providing new insights into the higher differentiability properties of minimizers in such general settings.

Abstract: We prove the higher differentiability of integer order of locally bounded minimizers of integral functionals of the form \begin{equation*}
  \mathcal{F}(u,Œ©):= \,\sum_{i=1}^{n} \dfrac{1}{p_i}\displaystyle \int_Œ©\, a_i(x) \lvert u_{x_i} \rvert^{p_i} dx- \int_Œ©œâ(x)u(x) dx, \end{equation*} where the exponents $ p_i \geq 2 $ and the coefficients $ a_i(x) $ satisfy a suitable Sobolev regularity. The main novelty consists in dealing with non-autonomous, anisotropic functionals, which depend also on the solution.

</details>


### [8] [A note on lifespan estimates for higher-order parabolic equations](https://arxiv.org/abs/2512.04428)
*Nurdaulet N. Tobakhanov,Berikbol T. Torebek*

Main category: math.AP

TL;DR: The paper investigates lifespan estimates for solutions to higher-order semilinear parabolic equations, deriving precise asymptotic bounds for both subcritical and critical Fujita exponent cases.


<details>
  <summary>Details</summary>
Motivation: To establish precise asymptotic behavior of lifespan for solutions to higher-order semilinear parabolic equations, improving upon previous results that only provided upper bounds under restrictive initial data assumptions.

Method: Combines test function method with semigroup estimates to derive both upper and lower bounds for solution lifespan, using L¬π‚à©L‚àû initial data assumptions instead of slowly decaying data.

Result: Derived sharp lifespan estimates: T_Œµ ‚àº Œµ^{-(1/(p-1)-n/2m)^{-1}} for 1<p<p_Fuj, and T_Œµ ‚àº exp(Œµ^{-(p-1)}) for p=p_Fuj, where p_Fuj=1+2m/n is the Fujita critical exponent.

Conclusion: The paper refines and extends earlier results by obtaining both upper and lower bounds under weaker initial data assumptions, providing complete lifespan asymptotics for higher-order parabolic equations.

Abstract: We investigate the lifespan of solutions to the higher-order semilinear parabolic equation $$u_t+(-Œî)^m u=|u|^p, \quad x \in \mathbb{R}^n, t>0 $$ with initial data. We focus on the precise asymptotic behavior of the lifespan of nontrivial solutions. By combining the test function method and semigroup estimates, we derive both upper and lower bounds for the lifespan of solutions $$T_{\varepsilon} \simeq \left\{\begin{array}{l}\varepsilon^{-\left(\frac{1}{p-1}-\frac{n}{2m}\right)^{-1}}, \,\, 1<p<p_{\text {Fuj}}, \\ \exp\left(\varepsilon^{-(p-1)}\right), \,\, p=p_{\text {Fuj}},\end{array}\right.$$ where $p_{Fuj}=1+\frac{2m}{n}$ is the critical exponent of Fujita. These estimates refine and extend the earlier results of Caristi-Mitidieri [J. Math. Anal. Appl., 279:2 (2003), 710-722] and Sun [Electron. J. Differential Equations, 17 (2010)], who obtained only upper bounds under slowly decaying initial data assumptions. In our setting, the above condition on the initial data is replaced by the assumption $L^1\cap L^\infty$, which sharpens the results of the aforementioned works.

</details>


### [9] [Irreversibility condition and stability of equilibria in the inverse-deformation approach to fracture](https://arxiv.org/abs/2512.04479)
*Arnav Gupta*

Main category: math.AP

TL;DR: The paper derives an irreversibility condition for fracture using inverse-deformation approach and thermodynamics, showing that crack location changes violate the second law, and proves stability conditions for broken equilibria.


<details>
  <summary>Details</summary>
Motivation: To establish a thermodynamic basis for fracture irreversibility using the inverse-deformation approach, addressing limitations in previous formulations and ensuring consistency with the second law of thermodynamics.

Method: Uses the second law of thermodynamics to derive irreversibility condition, applies inverse-deformation approach to brittle fracture in an elastic bar, incorporates inequality constraints for nonnegative inverse strain, and proves necessary/sufficient conditions for local stability with numerical implementation.

Result: Shows that changes in crack material location produce negative entropy production (violating second law), establishes irreversibility condition, proves local stability conditions incorporating these restrictions, and numerically demonstrates all broken equilibria from previous work are locally stable.

Conclusion: The thermodynamic approach successfully derives an irreversibility condition for fracture that prevents crack location changes, and the stability analysis confirms the physical validity of broken equilibrium solutions found in previous research.

Abstract: We derive the irreversibility condition in fracture for the inverse-deformation approach using the second law of thermodynamics. We consider the problem of brittle failure in an elastic bar previously solved in (Rosakis et al 2021). Despite the presence of a non-zero interfacial/surface energy, the third derivative of the inverse-deformation map is discontinuous at the crack faces. This is due to the presence of the inequality constraint ensuring the inverse strain is nonnegative and the orientation of matter is preserved. A change in the material location of a crack results in negative entropy production, violating the second law. Consequently, such changes are disallowed giving the irreversibility condition. The inequality constraint and the irreversibility condition limit the space of admissible variations. We prove necessary and sufficient conditions for local stability that incorporate these restrictions. Their numerical implementation shows that all broken equilibria found in (Rosakis et al 2021) are locally stable.

</details>


### [10] [Parabolic problems whose Fujita critical exponent is not given by scaling](https://arxiv.org/abs/2512.04506)
*Ahmad Z. Fino,Berikbol T. Torebek*

Main category: math.AP

TL;DR: The paper studies the fractional heat equation with nonlocal Riesz potential nonlinearity, identifying the Fujita-type critical exponent that determines global existence vs. finite-time blow-up of solutions.


<details>
  <summary>Details</summary>
Motivation: To investigate the global behavior of solutions to the fractional heat equation with nonlocal nonlinearity involving Riesz potentials, addressing a hypothesis proposed by Mitidieri and Pohozaev about the critical exponent for such equations.

Method: Introduces the Fujita-type critical exponent p_Fuj(n,Œ≤,Œ±) = 1 + (Œ≤+Œ±)/(n-Œ±). Uses nonlinear capacity method adapted to the problem structure for blow-up proofs, and fixed-point argument combined with Hardy-Littlewood-Sobolev inequality for global existence results.

Result: Establishes that p_Fuj(n,Œ≤,Œ±) is the critical exponent: global existence for small initial data when p > p_Fuj, and finite-time blow-up when p ‚â§ p_Fuj. Extends results to more general convolution operators beyond Riesz potentials.

Conclusion: The critical Fujita exponent for fractional heat equations with nonlocal nonlinearity is determined by p_Fuj(n,Œ≤,Œ±) = 1 + (Œ≤+Œ±)/(n-Œ±), not by the usual scaling argument, confirming Mitidieri-Pohozaev's hypothesis and extending their results to more general operators.

Abstract: This paper investigates the (fractional) heat equation with a nonlocal nonlinearity involving a Riesz potential: \begin{equation*} u_{t}+(-Œî)^{\fracŒ≤{2}} u= I_Œ±(|u|^{p}),\qquad x\in \mathbb{R}^n,\,\,\,t>0, \end{equation*} where $Œ±\in(0,n)$, $Œ≤\in(0,2]$, $n\geq1$, $p>1.$ We introduce the Fujita-type critical exponent $p_{\mathrm{Fuj}}(n,Œ≤,Œ±)=1+(Œ≤+Œ±)/(n-Œ±)$, which characterizes the global behavior of solutions: global existence for small initial data when $p>p_{\mathrm{Fuj}}(n,Œ≤,Œ±),$ and finite-time blow-up when $p\leq p_{\mathrm{Fuj}}(n,Œ≤,Œ±)$.
  It is remarkable that the critical Fujita exponent is not determined by the usual scaling argument that yields $p_{sc}=1+(Œ≤+Œ±)/n$, but instead arises in an unconventional manner, similar to the results of Cazenave et al. [Nonlinear Analysis, 68 (2008), 862-874] for the heat equation with a nonlocal nonlinearity of the form $\int_0^t(t-s)^{-Œ≥}|u(s)|^{p-1}u(s)ds,\,0\leq Œ≥<1.$
  The result on global existence for $p>p_{\mathrm{Fuj}}(n,2,Œ±),$ provides a positive answer to the hypothesis proposed by Mitidieri and Pohozaev in [Proc. Steklov Inst. Math., 248 (2005) 164-185]. We further establish global nonexistence results for the above heat equation, where the Riesz potential term $I_Œ±(|u|^{p})$ is replaced by a more general convolution operator $(\mathcal{K}\ast |u|^p),\,\mathcal{K}\in L^1_{loc}$, thereby extending the Mitidieri-Pohozaev's results established in the aforementioned work.
  Proofs of the blow-up results are obtained using a nonlinear capacity method specifically adapted to the structure of the problem, while global existence is established via a fixed-point argument combined with the Hardy-Littlewood-Sobolev inequality.

</details>


### [11] [Sharp stability on the second Robin eigenvalue with negative boundary parameters](https://arxiv.org/abs/2512.04584)
*Zhijie Chen,Zhen Song,Wenming Zou*

Main category: math.AP

TL;DR: The paper proves a sharp quantitative refinement of an isoperimetric inequality for the second Robin eigenvalue with negative boundary parameters, showing the exponent for Fraenkel asymmetry is optimal.


<details>
  <summary>Details</summary>
Motivation: To establish a quantitative stability estimate for the isoperimetric inequality for the second Robin eigenvalue with negative boundary parameters, improving upon previous work by Freitas and Laugesen.

Method: Constructing a suitable family of nearly spherical domains to analyze the behavior of the inequality and prove sharpness of the exponent.

Result: Proved a quantitative refinement of the isoperimetric inequality for the second Robin eigenvalue with negative boundary parameters, showing the exponent for Fraenkel asymmetry is sharp when the boundary parameter is not too far from 0.

Conclusion: The paper establishes optimal quantitative stability for the second Robin eigenvalue isoperimetric inequality with negative boundary parameters, demonstrating the sharpness of the Fraenkel asymmetry exponent through careful domain construction.

Abstract: In this paper, we prove a quantitative refinement of the isoperimetric type inequality for the second Robin eigenvalue with negative boundary parameters established by Freitas and Laugesen [Amer.J.Math.143 (2021), no.3, 969-994].Such new stability estimate is proved when the boundary parameter is not too far from 0.By constructing a suitable family of nearly spherical domains, we prove that the exponent for the Fraenkel asymmetry in this quantitative type inequality is sharp.

</details>


### [12] [Critical concave-convex problems in Carnot group](https://arxiv.org/abs/2512.04640)
*Mattia Galeotti,Eugenio Vecchi*

Main category: math.AP

TL;DR: Existence of two positive solutions for Dirichlet problem with concave-convex and critical nonlinearity in Carnot groups, extending Ambrosetti-Brezis-Cerami result to non-Euclidean setting.


<details>
  <summary>Details</summary>
Motivation: Extend the classical Ambrosetti-Brezis-Cerami result on existence of two positive solutions for concave-convex critical problems from Euclidean spaces to Carnot groups, which are non-commutative nilpotent Lie groups with sub-Riemannian structure.

Method: Variational Perron method combined with careful estimates of minimizers of the relevant Sobolev inequality. Special attention to proving the first solution is a local minimizer despite lack of boundary regularity in Carnot groups.

Result: Successfully proves existence of two positive solutions for the Dirichlet problem with concave-convex and critical nonlinearity in Carnot groups, analogous to the Euclidean case result.

Conclusion: The Ambrosetti-Brezis-Cerami result can be extended to Carnot groups using variational methods adapted to handle the geometric and analytical challenges of sub-Riemannian settings, particularly addressing boundary regularity issues.

Abstract: We consider a model Dirichlet problem with concave-convex and critical nonlinearity settled in Carnot groups. Our aim is to prove the existence of two positve solutions in the spirit of a famous result by Ambrosetti, Brezis and Cerami. To this aim we use a variational Perron method combined with proper estimates of a family of functions which are minimizers of the relevant Sobolev inequality. Due to the lack of boundary regularity, we also have to be careful while proving that the first solution found is a local minimizer in the proper topology.

</details>


### [13] [Infinity of solutions to initial-boundary value problems for linear constant-coefficient evolution PDEs on semi-infinite intervals](https://arxiv.org/abs/2512.04670)
*Andreas Chatziafratis,Spyridon Kamvissis*

Main category: math.AP

TL;DR: Algorithmic procedure for constructing non-uniqueness counter-examples for linear evolution PDEs in quarter-plane domains, with applications to heat and linear KdV equations.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic method for constructing counter-examples that demonstrate non-uniqueness of classical solutions to initial-boundary-value problems for linear evolution PDEs with constant coefficients.

Method: Uses analysis of regularity and asymptotic properties near domain boundaries, combined with closed-form integral representations derived via complex-analytic techniques and the Fokas unified transform method.

Result: An algorithmic procedure for constructing non-uniqueness counter-examples applicable to a wide class of linear evolution PDEs of any order, demonstrated explicitly for heat and linear KdV equations with Dirichlet data.

Conclusion: The paper presents a novel technique for constructing non-uniqueness examples and also provides new uniqueness theorems for the heat and linear KdV equations.

Abstract: In this short communication, we announce an algorithmic procedure for constructing non-uniqueness counter-examples of classical solutions to initial-boundary-value problems for a wide class of linear evolution partial differential equations, of any order and with constant coefficients, formulated in a quarter-plane. Our approach relies on analysis of regularity and asymptotic properties, near the boundary of the spatio-temporal domain, of closed-form integral-representation formulae derived via complex-analytic techniques and rigorous implementation of the modern PDE technique known as Fokas unified transform method. In order to elucidate the novel idea and demonstrate the proposed technique in a self-contained fashion, we explicitly present its application to two concrete examples, namely the heat equation and the linear KdV equation with Dirichlet data. New uniqueness theorems for these two models are also presented herein.

</details>


### [14] [On a fuzzy Landau Equation: Part III. The grazing collision limit](https://arxiv.org/abs/2512.04713)
*Manh Hong Duong,Boris Golubkov,Zihui He*

Main category: math.AP

TL;DR: The paper establishes the grazing limit from fuzzy Boltzmann equations to fuzzy Landau equations using variational formulations with GENERIC structure.


<details>
  <summary>Details</summary>
Motivation: To mathematically justify the connection between fuzzy Boltzmann equations (with delocalized collisions) and fuzzy Landau equations through the grazing limit, using the GENERIC framework for non-equilibrium systems.

Method: Uses variational formulations corresponding to the GENERIC structure of both equations. Shows convergence from non-quadratic dual dissipation pairs for fuzzy Boltzmann equations to quadratic dissipation pairs for fuzzy Landau equations.

Result: Successfully demonstrates the grazing limit from fuzzy Boltzmann equations to fuzzy Landau equations through variational convergence of their respective dissipation structures.

Conclusion: The grazing limit connecting fuzzy Boltzmann and Landau equations can be rigorously established using variational formulations within the GENERIC framework, providing mathematical justification for this important physical limit.

Abstract: In this paper, we study the grazing limit from the non-cutoff fuzzy Boltzmann equations to the fuzzy Landau equation, where particles interact through delocalised collisions. We show the grazing limit through variational formulations that correspond to the GENERIC (General Equations for Non-Equilibrium Reversible-Irreversible Coupling) structure of the respective equations. We show that the variational formulation associated with a non-quadratic dual dissipation pair for the fuzzy Boltzmann equations converges to a variational formulation of the fuzzy Landau equation corresponding to a quadratic dissipation pair.

</details>


### [15] [Optimal cost for the null controllability of the Stokes system with controls having $n-1$ components and applications](https://arxiv.org/abs/2512.04721)
*Felipe W. Chaves-Silva,Diego A. Souza,Marcos G. Ferreira-Silva*

Main category: math.AP

TL;DR: The paper shows that removing one component from the control in n-dimensional Stokes system doesn't increase the null controllability cost, which remains O(e^{C/T}).


<details>
  <summary>Details</summary>
Motivation: To investigate whether reducing the number of control components in Stokes systems affects the controllability cost, specifically when controlling n-1 instead of all n components.

Method: Develops a novel spectral estimate for low frequencies of the Stokes operator involving only n-1 components, then uses this estimate to analyze controllability cost.

Result: The cost of null controllability with n-1 control components remains O(e^{C/T}), same as with full n-component controls, showing no cost penalty from removing one control component.

Conclusion: The absence of one control component doesn't affect the null controllability cost in Stokes systems, and the spectral estimate has broader applications beyond this specific problem.

Abstract: In this work, we investigate the optimal cost of null controllability for the $n$-dimensional Stokes system when the control acts on $n-1$ scalar components. We establish a novel spectral estimate for low frequencies of the Stokes operator, involving solely $n-1$ components, and use it to show that the cost of controllability with controls having $n-1$ components remains of the same order in time as in the case of controls with $n$ components, namely $O(e^{C/T})$, i.e. the cost of null controllability is not affected by the absence of one component of the control. We also give several applications of our results.

</details>


### [16] [Generalized Navier-Stokes equations, associated with the Dolbeault complex](https://arxiv.org/abs/2512.04777)
*Shlapunov Alexander,Polkovnikov Alexander*

Main category: math.AP

TL;DR: The paper studies a Cauchy problem in complex space for a Navier-Stokes-like system generated by Cauchy-Riemann operators instead of standard gradient/divergence operators.


<details>
  <summary>Details</summary>
Motivation: To extend fluid dynamics analysis to complex spaces using Cauchy-Riemann operators instead of standard gradient operators, creating a structurally similar but mathematically different system.

Method: Uses multidimensional Cauchy-Riemann operator $\overline{\partial}$, its adjoint $\overline{\partial}^{*}$, and the Dolbeault complex to generate the system. Analyzes the Cauchy problem in Bochner-Sobolev spaces.

Result: Proves existence of weak solutions and open mapping theorem on specially constructed Bochner-Sobolev spaces. Obtains criterion for existence of "strong" solutions in these spaces.

Conclusion: Successfully extends Navier-Stokes analysis to complex spaces using Cauchy-Riemann operators, establishing existence theorems and solution criteria in appropriate function spaces.

Abstract: We consider the Cauchy problem in the band $\mathbb{C}^{n}\times[0, T], n>1,T>0$, for a system of nonlinear differential equations structurally similar to the classical Navier-Stokes equations for an incompressible fluid. The main difference of this system is that it is generated not by the standard gradient operators $\nabla$, divergence div and rotor rot, but by the multidimensional Cauchy-Riemann operator $\overline{\partial}$ in $\mathbb{C}^{n}$, its formally adjoint operator $\overline{\partial}^{*}$ and the compatibility complex for $\overline{\partial}$, which is usually called the Dolbeault complex. The similarity of the structure makes it possible to prove for this problem the theorem of the existence of weak solutions and the open mapping theorem on the scale of specially constructed Bochner-Sobolev spaces. In addition, a criterion for the existence of a ``strong'' solution in these spaces is obtained.

</details>


### [17] [Homogenized limits of Stokes flow and advective transport in thin perforated domains](https://arxiv.org/abs/2512.04782)
*Markus Gahn,Vlad Revnic*

Main category: math.AP

TL;DR: Homogenization and dimension reduction of flow/transport in thin Œµ-periodic perforated layers with thickness Œµ^Œ± (Œ±‚àà(0,1)), deriving effective Darcy-type flow and diffusion-advection transport models as Œµ‚Üí0.


<details>
  <summary>Details</summary>
Motivation: To derive effective macroscopic models for flow and transport in thin perforated layers where thickness is large compared to porosity, which is relevant for applications like porous media, filtration, and microfluidics.

Method: Uses two-scale convergence adapted to microscopic geometry with uniform a priori estimates. Constructs Bogovskii-operator for thin perforated domains to control fluid pressure, and establishes strong two-scale convergence for transport solutions.

Result: Obtains Darcy-type law for Stokes flow with Darcy-velocity depending only on vertical pressure derivative. For transport, gets diffusion-advection equation with homogenized coefficients. Slow vertical diffusion yields only vertical effective diffusion; fast horizontal diffusion yields effective diffusion in all directions.

Conclusion: Successfully derives rigorous homogenized models for flow and transport in thin perforated layers, establishing effective Darcy flow and diffusion-advection transport with direction-dependent diffusion effects based on scaling regimes.

Abstract: We deal with the rigorous homogenization and dimension reduction of flow and transport problems posed in thin $\varepsilon$-periodic perforated layers with thickness of order $\varepsilon^Œ±$ with $Œ±\in (0,1)$ and therefore the thickness of the layer is large compared its porosity. The aim is the derivation of effective models for $\varepsilon\to 0 $, when the thickness of the layer tends to zero. For the flow problem we consider incompressible Stokes equations with a pressure boundary condition on the top/bottom of the layer, and the transport problem is given by reaction-diffusion-advection problem with advective flow governed from the fluid velocity from the Stokes model and different scalings for the diffusion coefficient modelling low and fast diffusion in the horizontal direction. In the limit, a Darcy-type law is obtained for the Stokes flow with Darcy-velocity depending only on the derivative of the Darcy-pressure in the vertical direction. The effective equation for the transport problem is again of diffusion-advection-type including homogenized coefficients, and with advective flow given by the Darcy-velocity and only taking place in the vertical direction. In the case of slow diffusion in the vertical direction, effective diffusion only takes place in the vertical direction, where in the case of high diffusion in horizontal direction, we obtain effective diffusion in all space directions. To pass to the limit we use the method of two-scale convergence adapted to our microscopic geometry, which is based on uniform a priori estimates. Critical parts in the derivation of the macro-models are the control of the fluid pressure, for which we construct a Bogovskii-operator for thin perforated domains, as well as the strong two-scale convergence for the microscopic solution of the transport equation, necessary to pass to the limit in the advective term.

</details>


### [18] [The initial-to-final-state inverse problem with unbounded potentials and Strichartz estimates](https://arxiv.org/abs/2512.04796)
*Pedro Caro,Alberto Ruiz*

Main category: math.AP

TL;DR: The paper extends uniqueness results for the initial-to-final-state inverse problem to unbounded potentials by proving suitable Strichartz estimates, while showing limitations of Bourgain spaces for this problem.


<details>
  <summary>Details</summary>
Motivation: To establish a theoretical framework explaining the viability of data-driven prediction in quantum mechanics, extending previous results from bounded to unbounded potentials.

Method: Extends the initial-to-final-state inverse problem analysis to unbounded potentials by proving a family of suitable Strichartz estimates (including Keel-Tao endpoint), and provides a counterexample showing limitations of Bourgain spaces for this problem.

Result: Uniqueness holds for the inverse problem with unbounded potentials, achieved through Strichartz estimates, while demonstrating that Bourgain spaces cannot capture the mixed-norm Lebesgue spaces needed for these inequalities.

Conclusion: The paper successfully extends uniqueness results to unbounded potentials using Strichartz estimates, while revealing fundamental limitations of Bourgain spaces for addressing the initial-to-final-state inverse problem.

Abstract: The initial-to-final-state inverse problem consists in determining a quantum Hamiltonian assuming the knowledge of the state of the system at some fixed time, for every initial state. We formulated this problem to establish a theoretical framework that would explain the viability of data-driven prediction in quantum mechanics. In a previous work, we analysed this inverse problem for Hamiltonians of the form $-Œî+ V$ with an electric potential $V = V({\rm t}, {\rm x})$, and we showed that uniqueness holds whenever the potentials are bounded and decay super-exponentially at infinity. In this paper, we extend this result for unbounded potentials. One of the key steps consists in proving a family of suitable Strichartz estimates -- including the corresponding endpoint of Keel and Tao.
  In the context of the inverse Calder√≥n problem this family of inequalities corresponds to the Carleman inequality proved by Kenig, Ruiz and Sogge. Haberman showed that this inequality can be also retrieved as an embedding of a suitable Bourgain space. The corresponding Bourgain space in our context do not capture the mixed-norm Lebesgue spaces of Strichartz inequalities. In this paper, we give a counterexample that justifies this fact, and shows the limitations of Bourgain spaces to address the initial-to-final-state inverse problem.

</details>


### [19] [Time-periodic solutions to an energy balance model coupled with an active fluid under arbitrary large forces](https://arxiv.org/abs/2512.04800)
*Gianmarco Del Sarto,Matthias Hieber,Filippo Palma,Tarek Z√∂chling*

Main category: math.AP

TL;DR: A two-dimensional Sellers-type energy balance model coupled to 3D primitive equations via dynamic boundary condition admits at least one strong time-periodic solution when forced by time-periodic forcing, without requiring smallness conditions.


<details>
  <summary>Details</summary>
Motivation: To establish existence of time-periodic solutions in coupled climate models where energy balance equations interact with atmospheric primitive equations through dynamic boundary conditions, allowing for realistic large-amplitude forcing.

Method: Analysis of a coupled system combining 2D Sellers-type energy balance model with 3D primitive equations using dynamic boundary conditions. Mathematical proof techniques for existence of strong time-periodic solutions without smallness assumptions on forcing.

Result: Proves existence of at least one strong time-periodic solution when the forcing term is time-periodic. The forcing can be arbitrarily large - no smallness condition required.

Conclusion: The coupled climate model system admits time-periodic solutions under time-periodic forcing, even for large-amplitude forcing, demonstrating mathematical robustness of the coupled system's periodic behavior.

Abstract: This article concerns time-periodic solutions to a two-dimensional Sellers-type energy balance model coupled to the three-dimensional primitive equations via a dynamic boundary condition. It is shown that the underlying equations admit at least one strong time-periodic solution, provided the forcing term is time-periodic. The forcing term does not need to satisfy a smallness condition and is allowed to be arbitrarily large.

</details>


### [20] [Spectral Theory of Krein-Feller Type Operators and Applications in Stochastic Fractional Elliptic and Parabolic Equations](https://arxiv.org/abs/2512.04826)
*Kelvin J. R. Almeida-Sousa,Alexandre B. Simas*

Main category: math.AP

TL;DR: The paper develops nonstandard methods for analyzing Krein-Feller operators with singular measures, establishing series expansions for highly discontinuous functions, characterizing eigenvectors as generalized trigonometric functions, and proving nuclearity of the regularity space.


<details>
  <summary>Details</summary>
Motivation: To develop analytical tools for Krein-Feller operators Œî_{W,V} with singular measures W and V (strictly increasing, right/left continuous with dense discontinuities), where classical analytical and spectral methods fail due to the highly singular nature of the operators.

Method: Nonstandard analysis methods are employed since classical analytical and spectral arguments cannot be adapted to this singular setting. The approach involves characterizing the eigenvectors of Œî_{W,V} in terms of generalized trigonometric functions and establishing series expansions that generalize classical Taylor expansions.

Result: 1) Conditions ensuring series expansions for functions in C^‚àû_{W,V}(ùïã) (generalizing Taylor expansions); 2) Characterization of eigenvectors as generalized trigonometric functions; 3) Asymptotic lower bound for eigenvalues; 4) Sharp upper bound for convergence exponent of eigenvalues; 5) Proof that C^‚àû_{W,V}(ùïã) is a nuclear space; 6) Applications to generalized fractional stochastic/deterministic differential equations and parabolic SPDEs on nuclear spaces.

Conclusion: The paper successfully develops nonstandard analytical methods for singular Krein-Feller operators, establishing fundamental properties of the associated regularity space and obtaining spectral results that enable applications to various classes of differential equations, including stochastic PDEs on nuclear spaces.

Abstract: It has been shown that the space $C^{\infty}_{W,V}(\mathbb{T})$, introduced in Simas and Sousa (Potential Analysis, 2025), is the natural regularity space for solutions of the eigenvalue problem $Œî_{W,V} u = Œªu$ on the torus $\mathbb{T}$, where $Œî_{W,V} = \frac{d^{+}}{dV}\frac{d^{-}}{dW}$ is the Krein Feller operator in the case where $W$ and $V$ are strictly increasing and right continuous (respectively left continuous), possibly with dense sets of discontinuities. In this work we provide conditions ensuring that every function in $C^{\infty}_{W,V}(\mathbb{T})$, which may be highly discontinuous, admits a series expansion that generalizes the classical Taylor expansion. A central feature of our approach is that all proofs are nonstandard, since classical analytical and spectral arguments cannot be adapted to this singular setting. Using these methods we characterize the eigenvectors of $Œî_{W,V}$ in terms of generalized trigonometric functions and obtain an asymptotic lower bound for the associated eigenvalues. We also derive a sharp upper bound for the convergence exponent of these eigenvalues, and as a consequence we prove that $C^{\infty}_{W,V}(\mathbb{T})$ is a nuclear space. Further consequences include results on the asymptotic behavior of eigenvalues of compact operators and improvements in traceability. As a final application we establish existence results for generalized fractional stochastic and deterministic differential equations, as well as for parabolic stochastic partial differential equations acting on nuclear spaces.

</details>


### [21] [On hyperbolic approximations for a class of dispersive and diffusive-dispersive equations](https://arxiv.org/abs/2512.04882)
*Rahul Barthwal,Firas Dhaouadi,Christian Rohde*

Main category: math.AP

TL;DR: Novel hyperbolic approximations for dispersive and diffusive-dispersive equations with proven convergence to original solutions via relative entropy framework.


<details>
  <summary>Details</summary>
Motivation: To develop computationally tractable approximate systems for dispersive and diffusive-dispersive equations that allow application of standard numerical methods from hyperbolic balance laws.

Method: Construct first-order strictly hyperbolic approximations for dispersive equations with unique symmetrizer; use viscoelastic damped systems for diffusive-dispersive equations; apply relative entropy framework for convergence proofs.

Result: Local well-posedness for dispersive approximations, global well-posedness for hyperbolic-parabolic approximations, proven convergence to original equations, and successful numerical validation on test cases with strong nonlinear effects.

Conclusion: The proposed hyperbolic approximations provide a practical framework for simulating complex dispersive phenomena using standard hyperbolic numerical methods, with rigorous convergence guarantees.

Abstract: We introduce novel approximate systems for dispersive and diffusive-dispersive equations with nonlinear fluxes. For purely dispersive equations, we construct a first-order, strictly hyperbolic approximation. Local well-posedness of smooth solutions is achieved by constructing a unique symmetrizer that applies to arbitrary smooth fluxes. Under stronger conditions on the fluxes, we provide a strictly convex entropy for the hyperbolic system that corresponds to the energy of the underlying dispersive equation. To approximate diffusive-dispersive equations, we rely on a viscoelastic damped system that is compatible with the found entropy for the hyperbolic approximation of the dispersive evolution. For the resulting hyperbolic-parabolic approximation, we provide a global well-posedness result. Using the relative entropy framework \cite{dafermos2005hyperbolic}, we prove that the solutions of the approximate systems converge to solutions of the original equations. The structure of the new approximate systems allows to apply standard numerical simulation methods from the field of hyperbolic balance laws. We confirm the convergence of our approximations even beyond the validity range of our theoretical findings on set of test cases covering different target equations. We show the applicability of the approach for strong nonlinear effects leading to oscillating or shock-layer-forming behavior.

</details>


### [22] [Quantitative rigidity of the Wasserstein contraction under convolution](https://arxiv.org/abs/2512.04928)
*Max Fathi,Michael Goldman,Daniel Tsodyks*

Main category: math.AP

TL;DR: The paper investigates contraction properties of p-Wasserstein distances under convolution in Euclidean spaces, connecting this to uniform convexity of the Kantorovich functional, and extends uniform convexity results to the p=1 case.


<details>
  <summary>Details</summary>
Motivation: To understand how p-Wasserstein distances behave under convolution operations in Euclidean spaces, both qualitatively and quantitatively. The research is motivated by connecting this contraction question to the uniform convexity properties of the Kantorovich functional, where recent progress has been made mostly for p=2 and partially for p>1.

Method: The paper connects the contraction properties of p-Wasserstein distances under convolution to the question of uniform convexity of the Kantorovich functional. Building on recent progress in this area (primarily for p=2 and partially for p>1), the authors extend these uniform convexity results to cover the case p=1.

Result: The main result is the extension of uniform convexity results for the Kantorovich functional to the case p=1, which was previously not covered in the existing literature. This extension is presented as being of independent interest beyond the original convolution contraction problem.

Conclusion: The paper successfully connects the contraction properties of p-Wasserstein distances under convolution to uniform convexity of the Kantorovich functional, and makes a significant contribution by extending these uniform convexity results to the important case p=1, filling a gap in the existing theory.

Abstract: The aim of this paper is to investigate the contraction properties of $p$-Wasserstein distances with respect to convolution in Euclidean spaces both qualitatively and quantitatively. We connect this question to the question of uniform convexity of the Kantorovich functional on which there was substantial recent progress (mostly for $p=2$ and partially for $p>1$). Motivated by this connection we extend these uniform convexity results to the case $p=1$, which is of independent interest.

</details>


### [23] [Existence and a priori bounds for fully nonlinear PDEs with a harmonic map-like structure](https://arxiv.org/abs/2512.04961)
*Gabrielle Nornberg,Ricardo Ziegele*

Main category: math.AP

TL;DR: The paper studies a new class of fully nonlinear uniformly elliptic equations with harmonic map-like structure, establishing existence, multiplicity, and qualitative results under coefficient constraints.


<details>
  <summary>Details</summary>
Motivation: To analyze a novel class of fully nonlinear elliptic equations featuring a harmonic map-like structure, which includes Pucci extremal operators and nonlinear gradient terms, extending classical theory to more complex operators.

Method: Employ analytical techniques including Aleksandrov-Bakelman-Pucci estimates, comparison principles, and a priori bounds for Dirichlet problems in noncoercive cases, with smallness conditions on coefficients.

Result: Obtained existence results under coefficient constraints, classical results (ABP estimate, comparison principle), a priori bounds for Dirichlet problems, multiplicity results, and novel qualitative behavior even for Laplacian case.

Conclusion: The paper successfully develops a comprehensive theory for this new class of elliptic equations, establishing foundational results that extend to both fully nonlinear operators and classical cases like the Laplacian.

Abstract: In this paper, we study a new class of fully nonlinear uniformly elliptic equations with a so-called harmonic map-like structure, whose model case is given by \begin{equation*} \mathcal{M}^{\pm}_{Œª,Œõ}(D^2u) \pm b(x) |Du| \pm Œ≤(u)\langle M(x) Du,Du \rangle \pm c(x) u = f(x)\; \textrm{ in } Œ©, \end{equation*} where $Œ©\subset \mathbb{R}^n$ is a bounded $C^{1,1}$ domain, $\mathcal{M}^{\pm}$ are the Pucci extremal operators, $Œ≤(s) = s^k$ for some $k \in \mathbb{N} $ odd, $b \in L^{q}_{+}(Œ©)$, $c,f \in L^p(Œ©)$, and $n \leq p \leq q$, $q>n$.
  We obtain existence results under a smallness regime on the coefficients, along with some classical results such as the Aleksandrov--Bakelman--Pucci estimate and the comparison principle, as well as a priori bounds for the respective Dirichlet problem in the noncoercive case. We also establish multiplicity results and qualitative behavior, which seem to be new in the case of the Laplacian operator.

</details>


### [24] [Fractured Poroelastic Media in the Limit of Vanishing Aperture](https://arxiv.org/abs/2512.04978)
*Maximilian H√∂rl,Kundan Kumar,Christian Rohde*

Main category: math.AP

TL;DR: Derivation of limit models for poroelastic media with thin fractures as fracture width approaches zero, identifying different scaling regimes for hydraulic conductivity and elasticity.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of poroelastic media with thin fractures when the fracture width becomes infinitesimally small, and to derive simplified limit models that capture different physical regimes.

Method: Using a priori estimates and rigorous mathematical analysis to derive limit models as the fracture width-to-length ratio Œµ approaches zero, with material parameters scaling as powers of Œµ.

Result: Identified five distinct regimes for hydraulic conductivity scaling and two regimes for elasticity scaling, resulting in various limit models including discrete fracture models and two-scale problems dominated by normal flow or deformation.

Conclusion: The asymptotic analysis reveals rich mathematical structure in poroelastic fracture problems, with different parameter scalings leading to fundamentally different limit models that can be used for simplified simulations while preserving essential physics.

Abstract: We consider a poroelastic medium with a thin heterogeneity, also referred to as a fracture. Fluid flow and mechanical deformation inside both bulk and fracture are governed by the quasi-static Biot equations. The fracture's material parameters, such as hydraulic conductivity and elasticity, are assumed to scale with powers of the width-to-length ratio $\varepsilon$ of the fracture. Based on a priori estimates, we rigorously derive limit models as $\varepsilon \rightarrow 0$ and identify different limit regimes. We obtain five regimes for the hydraulic conductivity and two for the elasticity. While many cases yield discrete fracture models, others result in two-scale limit problems dominated by normal flow or deformation.

</details>


### [25] [Geophysical intensity problems: the axisymmetric case](https://arxiv.org/abs/2512.05010)
*Ralf Kaiser*

Main category: math.AP

TL;DR: The paper proves existence of infinitely many axisymmetric harmonic fields outside a sphere with prescribed intensity, characterized by specific zero patterns and decay orders.


<details>
  <summary>Details</summary>
Motivation: The intensity problem for gravitational/magnetic fields seeks harmonic vector fields with prescribed intensity at a surface, which is a nonlinear boundary value problem with unknown general solvability.

Method: Study axisymmetric harmonic fields outside unit sphere; solve nonlinear elliptic equation with discontinuous and singular coefficients using natural boundary conditions and new solution techniques with sharper estimates.

Result: Proved existence of infinitely many solutions for axisymmetric intensity problem; each solution characterized by specific zero patterns (symmetric points outside unit circle) and exact decay order at infinity.

Conclusion: The intensity problem has infinitely many axisymmetric solutions with prescribed intensity, characterized by their zero patterns and decay properties, extending previous work on geomagnetic direction problem.

Abstract: Considering the earth or any other celestial body the main sources of the gravitational as well as of the magnetic field lie inside the body. Above the surface both fields are in good approximation harmonic vector fields determined by their values at the body's surface or any other surface enclosing the body. The intensity problem seeks to determine harmonic vector fields vanishing at infinity and with prescribed intensity of the field at the surface. This problem constitutes a nonlinear boundary value problem, whose general solvability is not yet established. In this paper {\em axisymmetric} harmonic fields ${\bf H}$ outside the unit sphere $S^2$ are studied and, given an axisymmetric H√∂lder continuous intensity function $I\neq 0$ on $S^2$, the existence of infinitely many solutions of the intensity problem is proved. These solutions can more precisely be characterized as follows: fix a number $\de \in \nat\setminus \{1 \}$ and a meridional plane $M$ through the symmetry axis $S\!A$, and in $M$ a unit circle $S^1$ (symmetric with respect to $S\!A$) and, furthermore, $2\, N$, $N \in \nat_0$, points $z_n \in M$ (symmetric with respect to $S\!A$, avoiding $S\!A$, and outside $S^1$), then the existence of an (up to a sign) unique harmonic field ${\bf H}$ is established that vanishes at (the axisymmetric circles piercing $M$ at) $z_n$ and nowhere else, that has intensity $I$ at $S^2$ and (exact) decay order $\de$ at infinity. The proof is based on the solution of a nonlinear elliptic equation with discontinuous coefficients, which are, moreover, singular at the symmetry axis. Its combination with fixed boundary conditions was the basis of a recent treatment of the ``geomagnetic direction problem'' \cite{KR22}. Here we have instead natural boundary conditions, which provide less information, and which require, therefore, in part new solution techniques and sharper estimates.

</details>


### [26] [A Nehari manifold method for nonvariational problems](https://arxiv.org/abs/2512.05055)
*Radu Precup,Andrei Stan*

Main category: math.AP

TL;DR: Extends Nehari manifold method from variational to nonvariational fixed point equations via radial energy functional, yielding multiple solutions localized in conical annular sets.


<details>
  <summary>Details</summary>
Motivation: To generalize the Nehari manifold method beyond variational problems to nonvariational fixed point equations, expanding its applicability to broader classes of problems.

Method: Constructs a radial energy functional that generalizes the standard variational functional, applies it within the Nehari manifold framework to nonvariational fixed point equations.

Result: Obtains multiple solutions localized in conical annular sets, demonstrating the method's effectiveness through two representative applications.

Conclusion: Successfully extends the Nehari manifold method to nonvariational settings, providing a new approach for obtaining multiple solutions to fixed point equations with geometric localization properties.

Abstract: The aim of this paper is to extend the Nehari manifold method from the variational setting to the nonvariational framework of fixed point equations. This is achieved by constructing a radial energy functional that generalizes the standard one from the variational case. Furthermore, the solutions obtained through our method are localized in conical annular sets, which leads to the existence of multiple solutions. The abstract results are illustrated by two representative applications.

</details>


### [27] [Mean curvature flow near a peanut solution](https://arxiv.org/abs/2512.05077)
*Sigurd Angenent,Panagiota Daskalopoulos,Natasa Sesum*

Main category: math.AP

TL;DR: The paper shows that peanut solutions (mean curvature flows that extinct to a point without becoming convex) are highly unstable - small perturbations can lead to either spherical or nondegenerate neckpinch singularities.


<details>
  <summary>Details</summary>
Motivation: Peanut solutions are mean curvature flows that develop degenerate neckpinch singularities and were conjectured by Hamilton and proven to exist. The motivation is to understand the stability properties of these special solutions.

Method: The authors analyze perturbations of peanut solutions and study the resulting mean curvature flows. They examine how different types of perturbations lead to different singularity types (spherical vs nondegenerate neckpinch).

Result: 1. Peanut solutions are highly unstable - small perturbations can lead to either spherical singularities or nondegenerate neckpinch singularities. 2. Appropriately rescaled subsequences of solutions converging to peanut solutions with spherical singularities converge to the Ancient oval solution.

Conclusion: Peanut solutions represent an unstable boundary case between flows developing spherical singularities and those developing nondegenerate neckpinch singularities, with the Ancient oval solution appearing as a limiting object.

Abstract: It was shown by Angenent, Altschuler and Giga, and by Angenent and Velazquez that there exist closed mean curvature flow solutions that extinct to a point in finite time, without ever becoming convex prior to their extinction. These solutions develop a degenerate neckpinch singularity, meaning that the tangent flow at a singularity is a round cylinder, but at the same time for each of these solutions there exists a sequence of points in space and time, so that the pointed blow up limit around this sequence is the Bowl soliton. These solutions are called peanut solutions and they were first conjectured to exist by Richard Hamilton, while the existence of those solutions was shown by Angenent, Altschuler and Giga. In this paper we show that this type of solutions are highly unstable, in the sense that in every small neighborhood of any such peanut solution we can find a perturbation so that the mean curvature flow starting at that perturbation develops spherical singularity, and at the same time we can find a perturbation so that the mean curvature flow starting at that perturbation develops a nondegenerate neckpinch singularity. We also show that appropriately rescaled subsequence of any sequence of solutions whose initial data converge to the peanut solution, and all of which develop spherical singularities, converges to the Ancient oval solution.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [28] [Persistent-variable thermal compositional simulation of multiphase flow with phase separation in porous media](https://arxiv.org/abs/2512.04205)
*Veljko Lipovac,Omar Duran,Eirik Keilegavlen,Inga Berre*

Main category: physics.comp-ph

TL;DR: A persistent-variable formulation for thermal compositional multiphase flow using enthalpy, with embedded local solver for thermodynamic equilibrium, enabling efficient simulation of high-enthalpy systems without phase stability tests.


<details>
  <summary>Details</summary>
Motivation: Thermal compositional multiphase flow with phase transitions involves complex nonlinear interactions between flow, transport, and phase equilibrium. Existing approaches often require phase stability tests and struggle with challenging non-isothermal scenarios.

Method: Persistent-variable formulation using enthalpy for energy balance, deriving equilibrium conditions from thermodynamically consistent minimization. Embedded local solver for thermodynamic subproblem within global Newton solver, exploiting locality for parallelization and modularity for both isothermal and isenthalpic conditions.

Result: The approach successfully simulates complex high-enthalpy systems including narrow-boiling phenomena. Embedded local solver reduces global nonlinear iterations by up to 23%. Local solver tolerance of 1e-3 shows no significant impact on global iteration count.

Conclusion: The persistent-variable approach using enthalpy and modular embedded local solver advances equilibrium calculations in multiphase flow simulations, making it suitable for high-enthalpy applications without requiring phase stability tests.

Abstract: Thermal compositional multiphase flow in porous media with phase transitions involves complex nonlinear interactions among flow, transport, and phase equilibrium. This paper presents a persistent-variable formulation for thermal compositional flow using enthalpy to formulate the energy balance and the local equilibrium problem. Equilibrium conditions are derived from a thermodynamically consistent minimization problem using a persistent set of variables, allowing for seamless integration of equilibrium calculations into a fully coupled flow and transport model. This formulation does not require phase stability tests and provides a continuous and full mathematical description of the multiphysics system, suitable for challenging non-isothermal scenarios. To tackle the nonlinearities arising from phase transitions, we embed a local solver for the thermodynamic subproblem within a global Newton solver for the fully implicit system. The local solver exploits the locality of the subproblem for parallelization and leverages the modularity of the persistent-variable formulation for both isothermal and isenthalpic equilibrium conditions locally. We demonstrate the capability of our approach to simulate complex high-enthalpy systems, including narrow-boiling phenomena. The impact of the embedded local solver is analyzed through numerical experiments, demonstrating a reduction in global nonlinear iterations of up to 23 \% with increased use of the local solver. The number of local iterations is controlled with a local solver tolerance and no significant impact on the global iteration number was observed for local residual tolerances as high as $1e-3$. The persistent-variable approach using enthalpy and the modularity of the embedded local solver advance the usage of equilibrium calculations in multiphase flow simulations and are suitable for high-enthalpy applications.

</details>


### [29] [GPU-Portable Real-Space Density Functional Theory Implementation on Unified-Memory Architectures](https://arxiv.org/abs/2512.04447)
*Atsushi M. Ito*

Main category: physics.comp-ph

TL;DR: QUMASUN is a GPU-portable DFT code that achieves 2-2.8x speedup on AMD MI300A and NVIDIA GH200 GPUs compared to 256-core Xeon CPUs, using a lightweight C++ lambda layer for cross-platform execution.


<details>
  <summary>Details</summary>
Motivation: To develop a portable GPU implementation of real-space DFT that can run efficiently on different GPU architectures (AMD MI300A and NVIDIA GH200) without complex preprocessor directives, enabling broader adoption in plasma-fusion simulations beyond DFT.

Method: Implemented a lightweight C++ lambda-based abstraction layer that enables CPU, CUDA, and HIP execution without OpenMP/OpenACC directives. Leveraged unified memory on MI300A and coherent memory interconnect on GH200 to simplify GPU porting. Focused on compute-bound kernels: FFT, GEMM, and eigenvalue solver.

Result: Achieved 2.0-2.8√ó speedup on MI300A and 2.3-2.4√ó speedup on GH200 over 256-core Xeon nodes for diamond (216 atoms) and tungsten (128 atoms) systems. Compute-bound kernels showed substantial acceleration on both GPU platforms.

Conclusion: The GPU-portable approach successfully accelerates DFT calculations and can benefit a wide range of plasma-fusion simulation codes beyond DFT, demonstrating the viability of portable GPU programming for scientific computing.

Abstract: We present a GPU-portable implementation of a real-space density functional theory (DFT) code ``QUMASUN'' and benchmark it on the new Plasma Simulator featuring Intel Xeon 6980P CPUs, and AMD MI300A GPUs. Additional tests were performed on an NVIDIA GH200 GPU. In particular MI300A supports unified memory and GH200 supports coherent memory interconnect, simplifying GPU porting. A lightweight C++ lambda-based layer enables CPU, CUDA, and HIP execution without OpenMP/OpenACC preprocessor directives. For diamond (216 atoms) and tungsten (128 atoms) systems, MI300A and GH200 achieve 2.0-2.8 $\times$ and 2.3-2.4 $\times$ speedups over a 256-core Xeon node. The compute-bound kernels, which are fast Fourier transforms (FFT), dense matrix-matrix multiplications (GEMM) and eigenvalue solver, show substantial acceleration on both GPUs, indicating that the present GPU-portable approach can benefit a wide range of plasma-fusion simulation codes beyond DFT.

</details>


### [30] [On the Construction of High-Order and Exact Pressure Equilibrium Schemes for Arbitrary Equations of State](https://arxiv.org/abs/2512.04450)
*Christopher DeGrendele,Nguyen Ly,Francois Cadieux,Michael Barad,Dongwook Lee,Jared Duensing*

Main category: physics.comp-ph

TL;DR: Fully conservative methods for Euler compressible fluid equations that eliminate spurious pressure oscillations caused by nonlinear thermodynamics, applicable to any equation of state and multiple species.


<details>
  <summary>Details</summary>
Motivation: Existing fully conservative discretizations of Euler compressible fluid equations suffer from spurious pressure oscillations due to nonlinear thermodynamic relations between pressure, density, and internal energy, especially with real-fluid equations of state.

Method: Two methods: 1) A fully conservative, pressure-equilibrium preserving method, and 2) A high-order, fully conservative, approximate pressure-equilibrium preserving method. Both handle arbitrary equations of state and arbitrary number of species without introducing non-conservative updates, overspecified equations, or equation-of-state-specific designs.

Result: Demonstrated on inviscid smooth interface advection problems with three equations of state (ideal-gas, stiffened-gas, and van der Waals), showing orders of magnitude reduction in spurious pressure oscillations compared to existing schemes.

Conclusion: The proposed methods provide general, fully conservative approaches that effectively eliminate pressure oscillations in multi-component Euler equations while maintaining generality across different equations of state and species.

Abstract: Typical fully conservative discretizations of the Euler compressible single or multi-component fluid equations governed by a real-fluid equation of state exhibit spurious pressure oscillations due to the nonlinearity of the thermodynamic relation between pressure, density, and internal energy. A fully conservative, pressure-equilibrium preserving method and a high-order, fully conservative, approximate pressure-equilibrium preserving method are presented. Both methods are general and can handle an arbitrary equation of state and arbitrary number of species. Unlike existing approaches to discretize the multi-component Euler equations, we do not introduce non conservative updates, overspecified equations, or design for a specific equation of state. The proposed methods are demonstrated on inviscid smooth interface advection problems governed by three equations of state: ideal-gas, stiffened-gas, and van der Waals where we show orders of magnitude reductions in spurious pressure oscillations compared to existing schemes.

</details>


### [31] [Stochastic Density Functional Theory Through the Lens of Multilevel Monte Carlo Method](https://arxiv.org/abs/2512.04860)
*Xue Quan,Huajie Chen*

Main category: physics.comp-ph

TL;DR: sDFT with plane-wave discretization uses MLMC for variance reduction, decomposing density matrix evaluation into levels via increasing cutoffs or Chebyshev orders, making cost independent of discretization size/temperature.


<details>
  <summary>Details</summary>
Motivation: sDFT offers advantages over standard Kohn-Sham DFT for large-scale electronic structure calculations by avoiding expensive matrix diagonalization, but needs variance reduction for efficiency.

Method: Apply sDFT with plane-wave discretization and use multilevel Monte Carlo (MLMC) methods for variance reduction. Decompose density matrix evaluation into levels by increasing plane-wave cutoffs or Chebyshev polynomial orders.

Result: The decomposition renders computational cost independent of discretization size or temperature. Rigorous statistical error analysis and numerical experiments on material systems demonstrate algorithm efficiency.

Conclusion: MLMC-based variance reduction enables efficient sDFT calculations with plane-wave discretization, making large-scale electronic structure computations more practical by eliminating dependence on system size and temperature.

Abstract: The stochastic density functional theory (sDFT) has exhibited advantages over the standard Kohn-Sham DFT method and has become an attractive approach for large-scale electronic structure calculations. The sDFT method avoids the expensive matrix diagonalization by introducing a set of random orbitals and approximating the density matrix via Chebyshev expansion of a matrix-valued function. In this work, we study the sDFT with a plane-wave discretization, and discuss variance reduction algorithms in the framework of multilevel Monte Carlo (MLMC) methods. In particular, we show that the density matrix evaluation in sDFT can be decomposed into many levels by increasing the plane-wave cutoffs or the Chebyshev polynomial orders. This decomposition renders the computational cost independent of the discretization size or temperature. To demonstrate the efficiency of the algorithm, we provide rigorous analysis of the statistical errors and present numerical experiments on some material systems.

</details>


### [32] [PENCO: A Physics-Energy-Numerical-Consistent Operator for 3D Phase Field Modeling](https://arxiv.org/abs/2512.04863)
*Mostafa Bamdad,Mohammad Sadegh Eshaghi,Cosmin Anitescu,Navid Valizadeh,Timon Rabczuk*

Main category: physics.comp-ph

TL;DR: PENCO is a hybrid neural operator framework that integrates physical laws and numerical structure to solve spatio-temporal PDEs with improved accuracy, stability, and data efficiency compared to existing neural operators.


<details>
  <summary>Details</summary>
Motivation: Existing neural operators for solving spatio-temporal PDEs accumulate temporal errors, struggle with long-horizon generalization, and require large training datasets, limiting their practical application for phase-field models and interfacial dynamics.

Method: PENCO integrates physical laws and numerical structure through: 1) enhanced L^2 Gauss-Lobatto collocation residual for robust dynamics enforcement, 2) Fourier-space numerical consistency term capturing semi-implicit discretization behavior, 3) energy-dissipation constraint for thermodynamic consistency, plus low-frequency spectral anchoring and teacher-consistency mechanisms.

Result: PENCO demonstrates superior accuracy, stability, and data efficiency in 3D phase-field benchmarks (phase ordering, crystallization, epitaxial growth, pattern formation) compared to state-of-the-art neural operators like MHNO and FNO-4D, while maintaining physically consistent evolution.

Conclusion: PENCO successfully addresses limitations of existing neural operators by hybridizing physics, energy constraints, and numerical consistency, enabling accurate long-term simulations of complex interfacial phenomena with reduced data requirements.

Abstract: Accurate and efficient solutions of spatio-temporal partial differential equations (PDEs), such as phase-field models, are fundamental for understanding interfacial dynamics and microstructural evolution in materials science and fluid mechanics. Neural Operators (NOs) have recently emerged as powerful data-driven alternatives to traditional solvers; however, existing architectures often accumulate temporal errors, struggle to generalize in long-horizon simulations, and require large training datasets. To overcome these limitations, we propose PENCO (Physics-Energy-Numerical-Consistent Operator), a hybrid operator-learning framework that integrates physical laws and numerical structure within a data-driven architecture. The formulation introduces an enhanced L^2 Gauss-Lobatto collocation residual around the temporal midpoint that robustly enforces the governing dynamics and significantly improves accuracy, a Fourier-space numerical consistency term that captures the balanced behavior of semi-implicit discretizations, and an energy-dissipation constraint that ensures thermodynamic consistency. Additional low-frequency spectral anchoring and teacher-consistency mechanisms further stabilize learning and suppress long-term error growth. This hybrid design enables PENCO to preserve governing physics while mitigating long-term error growth. Through extensive three-dimensional phase-field benchmarks covering phase ordering, crystallization, epitaxial growth, and complex pattern formation, PENCO demonstrates superior accuracy, stability, and data efficiency compared to state-of-the-art neural operators, including Multi-Head Neural Operator (MHNO) and Fourier Neural Operator (FNO-4D), while maintaining physically consistent evolution. The associated dataset and implementation are available at github.com/MBamdad/PENCO.

</details>


### [33] [VNS Tokamak OpenMC-Serpent Validation for Medical Isotope Studies](https://arxiv.org/abs/2512.04873)
*Christopher Ehrich,Christian Bachmann,Pavel Pereslavtsev,Christian Reiter*

Main category: physics.comp-ph

TL;DR: Comparison of neutronics codes Serpent and OpenMC for modeling the Volumetric Neutron Source tokamak, showing differences in photon flux calculations depending on tracking methods and performance variations.


<details>
  <summary>Details</summary>
Motivation: The VNS tokamak is proposed for testing fusion reactor components and radioisotope production, requiring accurate neutronics modeling to validate component performance across different simulation codes.

Method: Modeled VNS geometry in Serpent and OpenMC codes, performed analog neutron-photon coupled simulations, compared vacuum vessel flux maps and blanket region spectra/reaction rates, tested different tracking methods (hybrid vs delta tracking).

Result: Neutron flux and (n,T) reactions showed excellent agreement; (n,2n) reactions had good agreement; photon flux had regional discrepancies (20% difference with hybrid tracking vs <1% with delta tracking); Serpent was faster for neutron-photon coupled simulations but slower for neutron-only simulations.

Conclusion: Serpent and OpenMC produce comparable results for neutron transport, but photon transport requires careful tracking method selection; Serpent offers computational advantages for coupled simulations; VNS demonstrates potential for radioisotope production applications.

Abstract: The Volumetric Neutron Source (VNS) tokamak is a proposed fusion reactor for testing and qualification of reactor components for future use in a fusion power facility, and has potential use for radioisotope production. The VNS geometry is modeled in the Serpent and OpenMC neutronics codes. Analog neutron-photon coupled simulations are carried out to compare the model's vacuum vessel and blanket components across codes. In the vacuum vessel, neutron and photon flux maps are calculated, while in the blanket region, neutron and photon spectra, (n,T), and (n,2n) reaction rates are calculated and compared between models. The detector response comparisons found the following: neutron flux and (n,T) reactions achieved excellent agreement, the (n,2n) detector response had good agreement, and photon flux had regional discrepancies depending on Serpent tracking used. Hybrid tracking lead to a relative difference of about 20% in the outboard side blanket, where as employment of delta tracking resulted in less than 1% relative difference. On an HPC cluster, Serpent was found to have shorter computation time than OpenMC in neutron photon coupled simulations using both hybrid tracking and delta tracking, but longer in neutron only simulations. An exemplary radioisotope production case is presented for the demonstration of additional VNS capabilities.

</details>


### [34] [LEDDS: Portable LBM-DEM simulations on GPUs](https://arxiv.org/abs/2512.04997)
*Raphael Maggio-Aprile,Maxime Rambosson,Christophe Coreixas,Jonas Latt*

Main category: physics.comp-ph

TL;DR: LEDDS: A portable GPU framework for granular flow and fluid-particle simulations using algorithmic primitives instead of handcrafted CUDA kernels, achieving performance comparable to hand-tuned solvers.


<details>
  <summary>Details</summary>
Motivation: To extend the algorithmic programming paradigm (using parallel primitives like map, sort, reduce) to complex computational physics problems like granular flows and fluid-particle interactions, moving away from device-specific GPU kernels.

Method: Developed LEDDS, an open-source framework that performs fully coupled Lattice Boltzmann-Discrete Element Method (LBM-DEM) simulations using only algorithmic primitives. The entire workflow (neighbor search, collision detection, fluid-particle coupling) is expressed as portable primitives, primarily using C++ Standard Library algorithms with selective Thrust primitives for performance.

Result: LEDDS achieves performance comparable to hand-tuned CUDA solvers despite high abstraction level. Validated through benchmarks including sphere/ellipsoid collisions, wall friction tests, single-particle settling, Jeffery's orbits, and particle-laden shear flows.

Conclusion: High-performance LBM-DEM coupling can be achieved without sacrificing generality or readability. LEDDS serves as a blueprint for portable multiphysics frameworks based on algorithmic primitives, applicable across various GPU systems and future accelerators.

Abstract: Algorithmic formulations of GPU programs provide a high-level alternative to device-specific code by expressing computations as compositions of well-defined parallel primitives (e.g., map, sort, reduce), rather than through handcrafted GPU kernels. In this work, we demonstrate that this paradigm can be extended to complex and challenging problems in computational physics: the simulation of granular flows and fluid-particle interactions.
  LEDDS, our open-source framework, performs fully coupled Lattice Boltzmann -- Discrete Element Method (LBM-DEM) simulations using only algorithmic primitives, and runs efficiently on single-GPU platforms. The entire workflow, including neighbor search, collision detection, and fluid-particle coupling, is expressed as a sequence of portable primitives. While the current implementation illustrates these principles primarily through algorithms from the C++ Standard Library, with selective use of Thrust primitives for performance, the underlying concept is compatible with any HPC environment offering a rich set of parallel algorithms and is therefore applicable across a wide range of modern GPU systems and future accelerators.
  LEDDS is validated through benchmarks spanning both DEM and LBM-DEM configurations, including sphere and ellipsoid collisions, wall friction tests, single-particle settling, Jeffery's orbits, and particle-laden shear flows. Despite its high level of abstraction, LEDDS achieves performances comparable to those of hand-tuned CUDA solvers, while maintaining portability and code clarity. These results show that high-performance LBM-DEM coupling can be achieved without sacrificing generality or readability, establishing LEDDS as a blueprint for portable multiphysics frameworks based on algorithmic primitives.

</details>


### [35] [Engineered Inclined Energy Landscapes Enabling Free Flow of Magnetic Microstructures for Artificial Neuron Applications](https://arxiv.org/abs/2512.05020)
*Anmol Sharma,Ranjeet Kumar Brajpuriya,Vivek K. Malik,Vishakha Kaushik,Sachin Pathak*

Main category: physics.comp-ph

TL;DR: A spintronic neuromorphic design using magnetic microstructures with sawtooth anisotropy achieves energy-efficient integrate-and-fire neuron emulation at 23.66 fJ per spike.


<details>
  <summary>Details</summary>
Motivation: Spintronic neuromorphic computing offers nanoscale, stable, low-energy magnetic microstructures, but practical integration faces challenges from complex fabrication, stochastic effects, pinning, and thermal instabilities that limit reliability and scalability.

Method: Engineered a sawtooth-type energy landscape anisotropy to enable free flow of magnetic microstructures, creating an experimentally feasible design that emulates biological integrate-and-fire neuron function with reduced energy requirements.

Result: Successfully achieved integrate-and-fire neuron emulation with free flow of magnetic microstructures, achieving extremely low energy consumption of 23.66 fJ per spike, demonstrating experimental reliability and energy efficiency.

Conclusion: The proposed design provides an experimentally reliable and energy-efficient approach for controlling magnetic microstructure dynamics, paving the way for skyrmion-based neuromorphic computing devices with practical real-world applications.

Abstract: Spintronic-based brain-inspired neuromorphic computing has recently attracted significant attention due to the exceptional properties of magnetic microstructures, including nanoscale dimensions, high stability, and low energy consumption. Despite these advantages, the practical integration of such microstructures into functional devices remains challenging. Fabrication processes are often complex and prone to stochastic effects, such as unwanted pinning and thermal-induced instabilities, which limit device reliability and scalability. Addressing these challenges is crucial for advancing spintronic neuromorphic architectures toward real-world applications. Thus, to reduce these effects we have proposed a design which is experimentally feasible and require less energy as compared to existing one. By engineering the system anisotropy into a sawtooth-type energy landscape, we have achieved free flow of these microstructures and successfully emulated integrate and fire (IF) function of biological neuron. Thus, proposed design presents an experimentally reliable and energy efficient external stimuli approach for tailoring magnetic microstructures dynamic behaviours, resulting in low energy consumption of 23.66 fJ per spike paving the way for the development of skyrmion-based futuristic neuromorphic computing device applications.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [36] [Operator Formalism for Laser-Plasma Wakefield Acceleration](https://arxiv.org/abs/2512.04982)
*Mostafa Behtouei,Carlos Salgado Lopez,Giancarlo Gatti*

Main category: physics.plasm-ph

TL;DR: Operator-based framework for laser-plasma wakefield acceleration using Hilbert-space operators to describe coupled laser-plasma dynamics, with neural operator integration for reduced-order modeling.


<details>
  <summary>Details</summary>
Motivation: To provide a compact, systematic mathematical framework for understanding laser-plasma wakefield acceleration in capillary discharges, connecting it to formal operator theory for better analysis and optimization.

Method: Develops operator formalism with key operators (transverse modal operator, nonlinear plasma operator, plasma oscillation operator, ponderomotive source operator) to describe mode coupling, plasma oscillations, and nonlinear feedback. Integrates with neural operator methods for efficient approximation.

Result: Establishes direct connection between LPWA and Hilbert-space operator theory, characterizes linear regime with invariant subspaces, shows nonlinear interactions break invariances leading to mode mixing, and creates hybrid physics-AI framework.

Conclusion: The operator-based approach provides robust foundation for modeling, analysis, and optimization of high-intensity laser-plasma interactions in next-generation accelerator experiments through formal mathematical interpretation and AI integration.

Abstract: In this paper, we develop an operator-based framework for laser--plasma wakefield acceleration (LPWA) in capillary discharges, providing a compact and systematic description of the coupled dynamics of laser fields and plasma response. The formalism employs key operators: the transverse modal operator $\hat{K}$, the nonlinear plasma operator $\hat{N}[Œ®]$, the plasma oscillation operator $\hatŒ©_p^{\,2}$, and the ponderomotive source operator $\hatŒ±$, which together describe mode coupling, plasma oscillations, and nonlinear feedback induced by the ponderomotive force. In the linear regime, the system is characterized by invariant subspaces associated with stable modal structures, while nonlinear interactions break these invariances, leading to mode mixing and complex dynamics. The approach establishes a direct connection between LPWA and Hilbert-space operator theory, including the invariant subspace, providing a formal mathematical interpretation of energy transfer and wakefield formation. Furthermore, the operator formalism integrates with neural operator methods, allowing efficient approximation of $\hat{N}$ and $\hatŒ±$ for reduced-order modeling and predictive control. This hybrid physics--AI framework offers a robust foundation for modeling, analysis, and optimization of high-intensity laser--plasma interactions in next-generation accelerator experiments.

</details>


### [37] [Numerical model for pellet rocket acceleration in PELOTON](https://arxiv.org/abs/2512.04484)
*J. Corbett,R. Samulyak,F. J. Artola,S. Jachmich,M. Kong,E. Nardon*

Main category: physics.plasm-ph

TL;DR: Developed and validated a 3D Lagrangian particle code (PELOTON) for simulating rocket acceleration of pellets in fusion devices, showing consistency with JET experiments and effects of pellet composition and fragment configurations.


<details>
  <summary>Details</summary>
Motivation: To understand and predict the rocket acceleration of pellets in thermonuclear fusion devices, particularly for shattered pellet injection (SPI) systems used for disruption mitigation, which is crucial for ITER operations.

Method: Developed a direct numerical simulation model within PELOTON code using 3D Lagrangian particle approach, accounting for grad-B drift, non-uniform cloud charging, plasma gradients, and implementing new plasma cooling model with ionization losses, Ohmic heating, and energy exchange.

Result: Simulations matched experimentally measured trajectories in JET, showed deuterium-neon pellets (0.5% neon) experienced smaller trajectory deviations than pure deuterium, demonstrated cloud overlap impact on acceleration, and validated plasma profiles against JOREK and INDEX simulations.

Conclusion: The PELOTON model successfully simulates pellet rocket acceleration with experimental validation, providing insights for SPI optimization in fusion devices, with future work focusing on ITER applications and scaling laws.

Abstract: A direct numerical simulation model for the rocket acceleration of pellets in thermonuclear fusion devices has been developed for PELOTON, a 3D Lagrangian particle pellet code [R. Samulyak et al, Nuclear Fusion 61 (4), 046007 (2021)], and validated using shattered pellet injection (SPI) experiments in JET. The pellet rocket acceleration is driven by grad-B drift of the ablation cloud that creates asymmetry and non-uniform heating of the cloud. The model accounts for non-uniform charging of the ablation cloud by hot plasma electrons as well as local plasma gradients. The increased pressure on the high-field-side compared to the low-field-side leads to pellet (fragment) rocket acceleration. Pure deuterium and deuterium-neon mixture models have been implemented. The background plasma states have been obtained by using a new plasma cooling model for PELOTON. The cooling model distributes the ablated material within the corresponding flux volumes and accounts for ionization and other energy losses, Ohmic heating by toroidal currents, and the energy exchange between ions and electrons. Plasma profiles predicted by PELOTON cooling model have been compared with JOREK and INDEX simulations. PELOTON simulations of rocket acceleration and the corresponding trajectories of deuterium fragments are consistent with experimentally measured trajectories in JET. We show that composite deuterium-neon pellets containing 0.5% of neon experienced smaller deviation of their trajectories compared to the pure deuterium case. We simulate various spatial configurations of pellet fragments and demonstrate the cloud overlap impact on rocket acceleration. Additionally, we demonstrate the effect of plasma state gradients on the rocket acceleration. Future work will focus on the rocket acceleration of SPI in projected ITER plasmas and the development of the corresponding scaling law for the rocket acceleration.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [38] [Amortized Inference of Multi-Modal Posteriors using Likelihood-Weighted Normalizing Flows](https://arxiv.org/abs/2512.04954)
*Rajneil Baruah*

Main category: cs.LG

TL;DR: Amortized posterior estimation using Normalizing Flows trained with importance sampling, showing that Gaussian Mixture Model base distributions improve multi-modal posterior reconstruction compared to unimodal bases.


<details>
  <summary>Details</summary>
Motivation: Need for efficient inference of theoretical parameters in high-dimensional inverse problems without requiring posterior training samples, particularly for multi-modal posterior distributions.

Method: Normalizing Flows trained with likelihood-weighted importance sampling, with investigation of different base distributions (unimodal vs Gaussian Mixture Models matching target mode cardinality).

Result: Standard unimodal base distributions fail to capture disconnected support, creating spurious probability bridges between modes. Gaussian Mixture Model initialization matching target mode cardinality significantly improves reconstruction fidelity.

Conclusion: The topology of base distributions critically impacts posterior modeling quality for multi-modal distributions, and using appropriately structured base distributions (like GMMs matching mode cardinality) is essential for accurate amortized posterior estimation.

Abstract: We present a novel technique for amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling. This approach allows for the efficient inference of theoretical parameters in high-dimensional inverse problems without the need for posterior training samples. We implement the method on multi-modal benchmark tasks in 2D and 3D to check for the efficacy. A critical observation of our study is the impact of the topology of the base distributions on the modelled posteriors. We find that standard unimodal base distributions fail to capture disconnected support, resulting in spurious probability bridges between modes. We demonstrate that initializing the flow with a Gaussian Mixture Model that matches the cardinality of the target modes significantly improves reconstruction fidelity, as measured by some distance and divergence metrics.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [39] [CNN on `Top': In Search of Scalable & Lightweight Image-based Jet Taggers](https://arxiv.org/abs/2512.05031)
*Rajneil Baruah,Subhadeep Mondal,Sunando Kumar Patra,Satyajit Roy*

Main category: hep-ph

TL;DR: Lightweight EfficientNet architecture with global jet features achieves competitive performance for top-quark jet tagging with lower computational cost than Transformers/GNNs.


<details>
  <summary>Details</summary>
Motivation: Transformer-based and standard Graph Neural Networks (GNNs) achieve best performance for jet classification but require substantial computational power, creating need for more efficient alternatives.

Method: Use lightweight and scalable version of EfficientNet architecture combined with global features of jets to create computationally inexpensive network.

Result: Network achieves competitive performance for tagging top-quark jets among light-quark and gluon jets while being computationally inexpensive.

Conclusion: Lightweight EfficientNet with global jet features provides effective, computationally efficient alternative to heavy Transformer/GNN models for jet classification tasks.

Abstract: While Transformer-based and standard Graph Neural Networks (GNNs) have proven to be the best performers in classifying different types of jets, they require substantial computational power. We explore the scope of using a lightweight and scalable version of the EfficientNet architecture, along with global features of the jet. The end product is computationally inexpensive but is capable of competitive performance. We showcase the efficacy of our network for tagging top-quark jets in a sea of other light-quark and gluon jets.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [40] [Mixing at the Batchelor Scale for White-In-Time Flows](https://arxiv.org/abs/2512.04297)
*Robin Chemnitz,Dennis Chemnitz*

Main category: math.PR

TL;DR: The paper proves lower bounds for dissipation rates in advection-diffusion equations with white-in-time velocity fields, verifying the Batchelor scale conjecture for specific 2D and 3D systems.


<details>
  <summary>Details</summary>
Motivation: To understand mixing properties in advection-diffusion equations and verify the Batchelor scale conjecture for specific velocity fields with vanishing diffusivity.

Method: Analyzes solutions to advection-diffusion equations on 2D torus with four forced modes, examines almost-sure exponential dissipation rates as diffusivity approaches zero, and extends analysis to three-dimensional velocity fields.

Result: Shows that almost-sure exponential dissipation rate stays bounded from below as diffusivity goes to zero, verifying the Batchelor scale conjecture for these specific velocity fields. Also characterizes exponential mixing rate without diffusion.

Conclusion: The paper provides concrete examples where the Batchelor scale conjecture can be verified, demonstrating bounded dissipation rates in the vanishing diffusivity limit for specific white-in-time velocity fields in both 2D and 3D.

Abstract: We consider the mixing properties of solutions to the advection-diffusion equation of a white-in-time velocity field on the 2-dimensional torus with four forced modes. As the diffusivity parameter goes to zero, we show that the almost-sure exponential dissipation rate stays bounded from below. Together with the corresponding upper bound established by Gess and Yaroslavtsev, this constitutes an example of a velocity field for which the Batchelor scale conjecture can be verified. In addition, we characterize the exponential mixing rate without diffusion of this system. Our results are not restricted to two dimensions, and we construct a three-dimensional white-in-time velocity field with the same properties.

</details>


### [41] [Homogenizationof non-divergence form operators in i.i.d. random environments](https://arxiv.org/abs/2512.04410)
*Xiaoqin Guo,Timo Sprekeler,Hung V. Tran*

Main category: math.PR

TL;DR: Improved convergence rates for homogenization of Dirichlet problems in random environments: O(R^{-3/2}) for d=3 and O(R^{-2}log R) for d‚â•4, surpassing optimal O(R^{-1}) rate.


<details>
  <summary>Details</summary>
Motivation: Study random walks in balanced i.i.d. random environments in higher dimensions (d‚â•3) to understand homogenization convergence rates for non-divergence form difference operators, aiming to improve upon the expected optimal O(R^{-1}) rate for finite-range dependent environments.

Method: Analyze random walks in balanced, i.i.d. random environments in ‚Ñ§^d for d‚â•3, focusing on the homogenization of Dirichlet problems associated with non-divergence form difference operators, using probabilistic and analytical techniques to establish improved convergence rates.

Result: Achieved improved convergence rates: O(R^{-3/2}) for d=3 and O(R^{-2}log R) for d‚â•4, which surpass the expected optimal O(R^{-1}) rate for environments with finite range of dependence.

Conclusion: The paper demonstrates that in higher dimensions (d‚â•3), homogenization convergence rates for Dirichlet problems in random environments can be significantly improved beyond the expected optimal rate, revealing dimension-dependent scaling behavior in the convergence.

Abstract: We study random walks in a balanced, i.i.d. random environment in $\mathbb Z^d$ for $d\geq 3$. We establish improved convergence rates for the homogenization of the Dirichlet problem associated with the corresponding non-divergence form difference operators, surpassing the $O(R^{-1})$ rate, which is expected to be optimal for environments with a finite range of dependence. In particular, the improved rates are $O(R^{-3/2})$ when $d=3$, and $O(R^{-2}\log R)$ when $d\geq 4$.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [42] [Effective permeabilities for flow through anisotropic microscopic geometries](https://arxiv.org/abs/2512.04133)
*Lo√Øc Balazi,Fabian Holzberger,Stephan B. Lunowa,Malte A. Peter,Daniel Peterseim,Barbara Wohlmuth*

Main category: physics.flu-dyn

TL;DR: Develops computational framework for anisotropic permeability in fibrous microstructures, validated for coiled aneurysm domains, showing significant directional flow effects missed by isotropic models.


<details>
  <summary>Details</summary>
Motivation: Accurately model flow in coiled aneurysm domains by capturing directional permeability variations induced by dense, fibre-like obstacles in anisotropic microscopic geometries.

Method: Builds on homogenisation theory and fully resolved simulations in Representative Elementary Volumes (REVs), validates Boutin's permeability model, proposes systematic methodology for capturing fibre orientation effects, incorporates resulting permeability tensors into macroscopic Darcy flow simulations.

Result: Anisotropy significantly impacts local flow direction and magnitude, generating directional permeability contrasts that cannot be reproduced by classical isotropic approximations; enables more realistic assessment of post-treatment aneurysm flow behavior.

Conclusion: The framework integrates coil-induced microstructural effects into continuum-scale hemodynamic models and is broadly applicable to other biomedical and engineering systems with fibrous porous microstructures.

Abstract: This work develops a computational and theoretical framework for determining effective permeabilities in anisotropic microscopic geometries containing dense, fibre-like obstacles, motivated by the need to model flow in coiled aneurysm domains accurately. Building on homogenisation theory and fully resolved simulations in Representative Elementary Volumes (REVs), we validate the permeability model introduced in [C. Boutin, Study of permeability by periodic and self-consistent homogenisation. Eur. J. Mech. A Solids, 19(4):603-632, 2000] and propose a systematic methodology for capturing the directional variations induced by fibre orientation. The resulting permeability tensors are incorporated into macroscopic flow simulations based on the Darcy equation, enabling direct comparison of anisotropic and isotropic permeability models across several benchmark configurations. Our findings show that anisotropy has a significant impact on local flow direction and magnitude, generating directional permeability contrasts which cannot be reproduced by classical isotropic approximations. By integrating coil-induced microstructural effects into continuum-scale hemodynamic models, the proposed approach enables more realistic assessment of post-treatment aneurysm flow behaviour. Beyond this clinical application, the framework is broadly applicable to other biomedical and engineering systems involving fibrous or filamentous porous microstructures.

</details>


### [43] [Can Explicit Subgrid Models Enhance Implicit LES Simulations? A GPU-Oriented High-Order-Solver Perspective](https://arxiv.org/abs/2512.04574)
*Gonzalo Rubio,Gerasimos Ntoukas,Miguel Ch√°vez-M√≥dena,Oscar Mari√±o,Bernat Font,Oriol Lehmkuhl,Eusebio Valero,Esteban Ferrer*

Main category: physics.flu-dyn

TL;DR: High-order DG methods on GPUs enable efficient high-order turbulence simulations, but under-resolution at high Re requires careful handling of dissipation. Study shows split-form DG schemes provide sufficient implicit dissipation for stability, making explicit LES models unnecessary in well-resolved cases, but beneficial in under-resolved high-Re scenarios.


<details>
  <summary>Details</summary>
Motivation: High-order DG methods on GPU architectures enable efficient high-order turbulence simulations, but under-resolution at high Reynolds numbers makes simulations sensitive to numerical dissipation and aliasing effects. Need to understand interplay between intrinsic DG dissipation mechanisms (split forms, Riemann solvers) and explicit LES models.

Method: Investigate interplay between implicit DG dissipation (split forms, Riemann solvers) and explicit subgrid-scale LES models using 3D Taylor-Green vortex at Re=1600 and inviscid case (Re‚Üí‚àû). Evaluate kinetic energy dissipation, spectral accuracy, and numerical stability.

Result: When stability is ensured through energy-/entropy-stable split-form schemes, explicit LES models are not strictly necessary. In well-resolved LES, SGS models don't improve accuracy due to overlap with inherent DG dissipation. In under-resolved high-Re cases, explicit SGS models complement numerical dissipation and enhance accuracy by removing excess energy.

Conclusion: Split-form DG schemes provide sufficient implicit dissipation for stability, making explicit LES models unnecessary in well-resolved cases. For under-resolved high-Re turbulence, explicit SGS models complement numerical dissipation. Provides practical guidance for numerical strategy selection in high-order turbulence simulations.

Abstract: High-order Discontinuous Galerkin (DG) methods offer excellent accuracy for turbulent flow simulations, especially when implemented on GPU-oriented architectures that favor very high polynomial orders. On modern GPUs, high-order polynomial evaluations cost roughly the same as low-order ones, provided the DG degrees of freedom fit within device memory. However, even with high-order discretizations, simulations at high Reynolds numbers still require some level of under-resolution, leaving them sensitive to numerical dissipation and aliasing effects. Here, we investigate the interplay between intrinsic DG dissipation mechanisms (implicit dissipation) -- in particular split forms and Riemann solvers -- and explicit subgrid-scale models in Large Eddy Simulations (LES). Using the three-dimensional Taylor--Green vortex at $Re = 1600$ and an inviscid case ($Re \to \infty$), we evaluate kinetic energy dissipation, spectral accuracy, and numerical stability.
  Our results show that when stability for under-resolved turbulence is ensured through split-forms (energy- or entropy-stable) schemes, subgrid-scale (SGS) LES models are not strictly necessary. At moderate Reynolds numbers, when the spatial resolution is sufficient to capture the relevant turbulence scales (i.e., in well-resolved LES), adding SGS models does not improve accuracy because the wavenumber range where they act overlaps with the inherent numerical dissipation of the DG scheme. In contrast, when the resolution is insufficient, as is typical at high Reynolds numbers, explicit subgrid-scale models complement the numerical dissipation and enhance accuracy by removing the excess energy that numerical fluxes alone cannot dissipate.
  These findings provide practical guidance for choosing numerical strategies in high-order turbulence simulations.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [44] [Degrees of universality in wave turbulence](https://arxiv.org/abs/2512.04866)
*Jiasheng Liu,Vladimir Rosenhaus,Gregory Falkovich*

Main category: cond-mat.stat-mech

TL;DR: Weak wave turbulence transitions from weak to strong with new universalities; spin-wave turbulence becomes strong due to spectral nonlocality even when local interactions are weak, similar to focusing NSE critical-balance states.


<details>
  <summary>Details</summary>
Motivation: To understand how inverse turbulent cascades transition from weak to strong turbulence, and to explore the universality properties that emerge in strong turbulence regimes, particularly contrasting different wave systems.

Method: Study turbulence of spin waves in ferromagnets and contrast with turbulent cascades in Nonlinear Schr√∂dinger Equation (NSE) and MMT-like models in higher dimensions; analyze vertex renormalization effects and spectral nonlocality; examine multi-component versions with large number of components.

Result: Spin-wave turbulence becomes strong far from pumping scale even when local wave interactions are weak due to spectral nonlocality; strong spin-wave turbulence resembles focusing NSE critical-balance states; UV nonlocality causes large-scale turbulence level to decrease with increasing pumping, eventually becoming independent of pumping level.

Conclusion: Spectral nonlocality enhances nonlinearity and drives transition from weak to strong turbulence; new universalities emerge in strong turbulence regimes; spin-wave turbulence exhibits critical-balance behavior similar to focusing NSE but with unique pumping dependence due to UV nonlocality.

Abstract: Turbulence of weakly interacting waves displays a great deal of universality: independence of the details of the interaction and of the pumping and dissipation scales. Here we study how inverse turbulent cascades (from small to large scales) transition from weak to strong. We find that while one-loop corrections can be dependent on excitation and dissipation scales, new types of universality appear in strong turbulence. We contrast turbulence of spin waves in ferromagnets with turbulent cascades in the Nonlinear Schr√∂dinger Equation (NSE) and in an MMT-like model in higher dimensions having a multiplicative interaction vertex: vertex renormalization gives rise to dependence on the pumping (UV scale) in the former but not in the latter. As a result of this spectral nonlocality, spin-wave turbulence stops being weak if one is sufficiently far from the pumping scale, even when the interaction of waves with comparable wavenumbers is weak. We paraphrase this as: nonlocality enhances nonlinearity.
  We then describe strong turbulence in a multi-component version of these models with a large number of components. We argue that strong spin-wave turbulence is similar to turbulence of the focusing NSE, as it realizes a critical-balance state. However, UV nonlocality causes the level of spin-wave turbulence at large scales to decrease with increasing pumping level, culminating in a state that is independent of the level of pumping.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [45] [An all-optical convolutional neural network for image identification](https://arxiv.org/abs/2512.04569)
*Wei-Wei Fu,Dong Zhao,Qing-Hong Rao,Heng-Yi Wang,Ben-Li Yu,Zhi-Jia Hu,Fang-Wen Sun,Kun Huang*

Main category: physics.optics

TL;DR: All-optical CNN using spatial-differentiation convolution and diffractive fully-connected layer achieves high-speed, energy-efficient image classification without explicit optical nonlinearities.


<details>
  <summary>Details</summary>
Motivation: Electronic CNN hardware faces speed and energy efficiency bottlenecks due to resistive/capacitive losses. Photonic alternatives are promising but struggle with optical nonlinearities needed for end-to-end classification.

Method: Single spatial-differentiation convolutional stage with 24 directional kernels spanning 360¬∞ plus mean-filtering kernel, followed by diffractive fully-connected layer. Directional convolution enhances feature selectivity and suppresses noise, allowing weak optical diffraction nonlinearity to achieve classification.

Result: 86.8% accuracy on MNIST handwritten digits, 94.8% on ten-class gesture dataset. Computational throughput of 1.13√ó10^5 TOPS and energy efficiency of 1.51√ó10^3 TOPS/W - highest reported among CNN hardware.

Conclusion: Demonstrates scalable pathway for ultralow-latency, ultralow-energy vision processing using all-optical CNN architecture that bypasses need for explicit optical nonlinear activations.

Abstract: In modern artificial intelligence, convolutional neural networks (CNNs) have become a cornerstone for visual and perceptual tasks. However, their implementation on conventional electronic hardware faces fundamental bottlenecks in speed and energy efficiency due to resistive and capacitive losses. Photonic alternatives offer a promising route, yet the difficulty of realizing optical nonlinearities has prevented the realization of all-optical CNNs capable of end-to-end image classification. Here, we demonstrate an all-optical CNN that bypasses the need for explicit optical nonlinear activations. Our architecture comprises a single spatial-differentiation convolutional stage--using 24 directional kernels spanning 360¬∞, along with a mean-filtering kernel--followed by a diffractive fully-connected layer. The directional convolution enhances feature selectivity, suppresses noise and crosstalk, and simplifies the classification task, allowing the weak nonlinearity inherent in optical diffraction to achieve high accuracy. We report experimentally classification accuracies of 86.8% on handwritten digits (MNIST) and 94.8% on a ten-class gesture dataset. The system delivers a computational throughput of 1.13X10^5 tera-operations per second (TOPS) and an energy efficiency of 1.51X10^3 TOPS/W--the highest reported among CNN hardware--with the potential to improve by a further 5-6 orders of magnitude using nanosecond-scale detectors. This work establishes a scalable pathway toward ultralow-latency, ultralow-energy vision processing for real-time intelligent systems.

</details>


### [46] [Structured Light at the Extreme: Harnessing Spatiotemporal Control for High-Field Laser-Matter Interactions](https://arxiv.org/abs/2512.05042)
*Sergio Carbajo,Seung-Whan Bahk,Justin Baker,Andrea Bertozzi,Abhimanyu Borthakur,Antonino Di Piazza,Andrew Forbes,Spencer Gessner,Jack Hirschman,Franz K√§rtner,Maciej Lewenstein,Yuhang Li,Inhyuk Nam,Eileen Otte,Aydogan Ozcan,James Rozensweig,Yijie Shen,Liwei Song,Ye Tian,Yu Wang,Yuntian Wang,Logan Wright,Xiaojun Wu,Hao Zhang*

Main category: physics.optics

TL;DR: This review presents intelligent structured light as a transformative paradigm for controlling high-field laser-matter interactions through precise spatiotemporal and vectorial light manipulation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to move beyond conventional approaches to laser-matter interactions by establishing a framework for precise control of light as a critical degree of freedom, enabling more sophisticated manipulation of extreme light-matter phenomena.

Method: The method involves a three-pillar framework: 1) Advanced electromagnetic toolkit including static optics and plasma light modulators, 2) Optimization engine using physics-informed digital twins and AI-driven inverse design, and 3) Integrated application development.

Result: The approach enables groundbreaking applications including programmable electron beams, orbital-angular-momentum-carrying Œ≥-rays, compact THz accelerators, and robust communications systems.

Conclusion: The review calls for interdisciplinary collaboration to overcome challenges in material science, real-time adaptive control, and quantum extensions, aiming to actively command rather than merely observe extreme light-matter interactions.

Abstract: This review charts the emerging paradigm of intelligent structured light for high-field laser-matter interactions, where the precise spatiotemporal and vectorial control of light is a critical degree of freedom. We outline a transformative framework built upon three synergistic pillars. First, we survey the advanced electromagnetic toolkit, moving beyond conventional spatial light modulators to include robust static optics and the promising frontier of plasma light modulators. Second, we detail the optimization engine for this high-dimensional design space, focusing on physics-informed digital twins and AI-driven inverse design to automate the discovery of optimal light structures. Finally, we explore the groundbreaking applications enabled by this integrated approach, including programmable electron beams, orbital-angular-momentum-carrying Œ≥-rays, compact THz accelerators, and robust communications. The path forward necessitates overcoming grand challenges in material science, real-time adaptive control at MHz rates, and the extension of these principles to the quantum realm. This review serves as a call to action for a coordinated, interdisciplinary effort to command, rather than merely observe, light-matter interactions at the extreme.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [47] [Phase mixing and the Vlasov equation in cosmology](https://arxiv.org/abs/2512.04214)
*Martin Taylor,Renato Velozo Ruiz*

Main category: gr-qc

TL;DR: The paper analyzes Vlasov equation on expanding cosmological spacetimes, showing density decay rates depend on expansion rate and initial data regularity, with phase mixing effects enhancing decay.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time behavior of collisionless matter (Vlasov equation) in expanding cosmological spacetimes (FLRW models), particularly how expansion rates affect density decay and phase mixing effects.

Method: Uses commuting vector fields approach with combinatorial properties of differential operators, physical space dyadic localization for non-compactly supported solutions, and analysis of vector field properties for large time relative to momentum support.

Result: For expansion rate t^q (0<q<1/2): density decays at t^{-6q}; with spatial average removed, polynomial enhancement for Sobolev data, super-polynomial/sub-exponential enhancement for analytic data. For borderline rate t^{1/2}: logarithmic enhancement for Sobolev data, super-logarithmic enhancement (exp(-Œº(log t)^Œµ)) for analytic data.

Conclusion: Expansion rate significantly affects density decay in cosmological spacetimes, with phase mixing providing enhanced decay rates that depend on initial data regularity, and borderline expansion rates exhibit different enhancement behaviors.

Abstract: We consider the Vlasov equation on slowly expanding isotropic homogeneous tori, described by the Friedmann--Lema√Ætre--Robertson--Walker cosmological spacetimes. For expansion rate $t^q$, with $0< q<\frac{1}{2}$ (excluding certain exceptional values), we show that the spatial density decays at the rate $t^{-6q}$ and that, when the spatial average is removed, the density decays at an enhanced rate due to a phase mixing effect. This enhancement is polynomial for Sobolev initial data and super-polynomial, but sub-exponential, for real analytic initial data. We further show that, when the expansion rate is the borderline $t^{\frac{1}{2}}$ -- the rate which describes a radiation filled universe -- a degenerate phase mixing effect results in a logarithmic enhancement for Sobolev initial data and a super-logarithmic enhancement (in fact, a gain of $\exp(-Œº(\log t)^Œµ)$ for some $Œº,Œµ>0$) for analytic initial data. The proof is based on a collection of commuting vector fields, and certain combinatorial properties of an associated collection of differential operators. The vector fields are not explicit, but are shown to have good properties when $t$ is large with respect to the momentum support of the solution. A physical space dyadic localisation is employed to treat non-compactly supported (in particular, non-trivial real analytic) but suitably decaying solutions.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [48] [A hybrid Green-Kubo (hGK) framework for calculating viscosity from short MD simulations](https://arxiv.org/abs/2512.04546)
*Akash K. Meel,Santosh Mogurampelly*

Main category: cond-mat.soft

TL;DR: Hybrid Green-Kubo (hGK) framework partitions stress autocorrelation function into short-time ballistic (from MD) and long-time relaxation (analytical fitting) components, enabling efficient viscosity calculation with orders-of-magnitude computational savings.


<details>
  <summary>Details</summary>
Motivation: Traditional Green-Kubo viscosity calculations suffer from poor convergence and require extensive sampling, making them computationally demanding for soft matter and polymer systems.

Method: Hybrid Green-Kubo (hGK) framework partitions stress autocorrelation function into: (1) short-time ballistic component from short MD simulations, and (2) long-time relaxation tail using analytically motivated functions fitted to short trajectories.

Result: Excellent agreement with established results for SPC/E water; successful application to challenging electrolyte systems (EC-LiTFSI and PEO-LiTFSI) where traditional GK fails; orders-of-magnitude reduction in required sampling without compromising accuracy.

Conclusion: hGK provides conceptually simple, broadly applicable, and computationally efficient route for viscosity prediction in molecular liquids, polymer melts, and ionically conducting soft materials, with clear avenues for refinement.

Abstract: Viscosity calculation from equilibrium molecular dynamics (MD) simulations relies on the traditional Green-Kubo (GK) framework, which integrates the stress autocorrelation function (SACF) over time. While the formalism is exact in the linear response regime, the traditional approach often suffers from poor convergence and requires extensive phase space sampling, which is computationally demanding for soft matter and polymer systems. In this Letter, we introduce a hybrid Green-Kubo (hGK) framework that alleviates these limitations by partitioning the SACF into two physically meaningful regimes: (i) a short time ballistic component extracted directly from short MD simulations, and (ii) a long time relaxation tail represented using analytically motivated functions, $œÜ(œÑ)$, fitted only to short trajectories. This strategy bypasses the need for extensive sampling while preserving physical rigor. Benchmarking against SPC/E water confirms excellent agreement with established results, and we further demonstrate the efficacy of the method for challenging electrolyte systems (EC-LiTFSI and PEO-LiTFSI), for which the GK framework fails to converge. The computational savings are substantial, with reductions of several orders of magnitude in required sampling, achieved without compromising predictive accuracy. We also discuss the limitations of the hGK framework and outline clear avenues for refinement, including optimal tail selection and robust identification of relaxation regimes in noisy stress data. The hGK framework presented in this Letter provides a conceptually simple, broadly applicable, and computationally efficient route for viscosity prediction in molecular liquids, polymer melts, and ionically conducting soft materials.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [49] [High-repetition-rate, all-reflective optical guiding and electron acceleration in helium using an off-axis axicon](https://arxiv.org/abs/2512.04788)
*Ji≈ô√≠ ≈†i≈°ma,Michal Nevrkla,Filip Vitha,Sebastian Lorenz,Illia Zymak,Al≈æbƒõta ≈†p√°dov√°,Andrea Koll√°rov√°,Matƒõj Jech,Alexandr Janƒç√°rek,Davorin Peceli,Carlo M. Lazzarini,Leonardo V. N. Goncalves,Gabriele M. Grittani,Sergei V. Bulanov,Jaron E. Shrock,Ela Rockafellow,Ari J. Sloss,Bo Miao,Scott W. Hancock,Howard M. Milchberg*

Main category: physics.acc-ph

TL;DR: High-power laser wakefield acceleration using self-waveguiding in plasma channels achieves stable 5 GeV electron beams at 0.2 Hz with a compact setup.


<details>
  <summary>Details</summary>
Motivation: To develop a stable, high-repetition-rate laser wakefield acceleration system that doesn't require complex laser modifications, making the technology more accessible for user facilities.

Method: Used ELI Beamlines' L3-HAPLS laser system (13 J, 30 fs) with self-waveguiding in a 20 cm helium plasma channel. Implemented a novel all-reflective optical setup including an off-axis reflective axicon for efficient acceleration and guiding.

Result: Achieved stable acceleration of electron beams to energies approaching 5 GeV at 0.2 Hz, with guiding demonstrated at repetition rates up to 3.3 Hz. The compact setup stabilized electron pointing and enhanced energy gain.

Conclusion: This single laser, single compressor implementation provides a practical pathway for broader adoption of laser wakefield acceleration technology across user facilities without requiring laser system modifications.

Abstract: We present recent results on high-power guiding and laser wakefield acceleration (LWFA) in the ELBA beamline at ELI Beamlines, using the L3-HAPLS laser system (13 J, 30 fs, 0.2 Hz). By employing self-waveguiding in a 20 cm plasma channel in helium, we achieved stable acceleration of electron beams to energies approaching 5 GeV. A novel all-reflective optical setup, including an off-axis reflective axicon, enabled efficient acceleration at 0.2 Hz and guiding at repetition rates up to 3.3 Hz. This compact single laser, single compressor implementation of plasma channels for electron acceleration stabilizes electron pointing and enhances energy gain without requiring modifications to the laser system, paving the way for broader adoption of the technology across user facilities.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [50] [Cumulant expansions of operator groups of quantum many-particle systems](https://arxiv.org/abs/2512.05036)
*V. I. Gerasimenko,I. V. Gapyak*

Main category: math-ph

TL;DR: The paper develops cluster expansion methods for operator groups in quantum dynamics to construct generating operators for nonperturbative solutions to evolution hierarchies in many-particle quantum systems.


<details>
  <summary>Details</summary>
Motivation: To address the Cauchy problem for hierarchies of evolution equations in many-particle quantum systems, particularly for von Neumann equations (states) and Heisenberg equations (observables), without relying on perturbation theory approaches.

Method: Uses cluster expansions for groups of operators associated with quantum dynamical equations, constructing generating operators that provide nonperturbative solutions to the Cauchy problem for evolution hierarchies.

Result: Develops a mathematical framework for generating operators that yield nonperturbative solutions to quantum evolution hierarchies, applicable to both state dynamics (von Neumann equations) and observable dynamics (Heisenberg equations).

Conclusion: The cluster expansion method provides a systematic approach to construct nonperturbative solutions for quantum many-body dynamics, offering an alternative to perturbation theory for solving evolution hierarchies in quantum systems.

Abstract: The article presents a method of cluster expansions for groups of operators associated with the von Neumann equations for states and the Heisenberg equations for observables, aiming to construct generating operators for nonperturbative solutions to the Cauchy problem for hierarchies of evolution equations of many-particle quantum systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [51] [A Unified Low-rank ADI Framework with Shared Linear Solves for Simultaneously Solving Multiple Lyapunov, Sylvester, and Riccati Equations](https://arxiv.org/abs/2512.04676)
*Umair Zulfiqar,Zhong-Yi Huang*

Main category: eess.SY

TL;DR: A unified ADI framework that shares shifted linear solves across Lyapunov, Sylvester, and Riccati equations, reducing computational cost while enabling simultaneous solution of multiple matrix equations and producing reduced-order models with preserved system properties.


<details>
  <summary>Details</summary>
Motivation: To reduce computational cost by recognizing that ADI methods for different matrix equations (Lyapunov, Sylvester, Riccati) share the same interpolatory nature and differ only in pole placement, allowing shifted linear solves to be shared across multiple equations.

Method: Proposes a unified ADI framework that performs only two shifted linear solves per iteration to simultaneously solve multiple equations (6 Lyapunov, 1 Sylvester, 10 Riccati), with pole placement as the only distinguishing inexpensive step. The method leverages the Petrov-Galerkin projection nature of ADI methods.

Result: Substantially increases computational efficiency by sharing linear solves across multiple equations. Additionally produces reduced-order models as byproducts that preserve important system properties (stability, minimum-phase, positive-realness, bounded-realness, passivity) and interpolate the original transfer function.

Conclusion: The unified ADI framework serves both as an efficient solver for multiple matrix equations and as a recursive, interpolation-based model order reduction method that preserves key system properties, providing high return on computational investment.

Abstract: It is known in the literature that the low-rank ADI method for Lyapunov equations is a Petrov-Galerkin projection algorithm that implicitly performs model order reduction. In this paper, we show that the low-rank ADI methods for Sylvester and Riccati equations are also Petrov-Galerkin projection algorithms that implicitly perform model order reduction. By observing that the ADI methods for Lyapunov, Sylvester, and Riccati equations differ only in pole placement and not in their interpolatory nature, we show that the shifted linear solves-which constitute the bulk of the computational cost-can be shared. The pole-placement step involves only small-scale operations and is therefore inexpensive. We propose a unified ADI framework that requires only two shifted linear solves per iteration to simultaneously solve six Lyapunov equations, one Sylvester equation, and ten Riccati equations, thus substantially increasing the return on investment for the computational cost spent on the linear solves. All operations needed to extract the individual solutions from these shared linear solves are small-scale and inexpensive.
  Since all ADI methods implicitly perform model order reduction when solving these linear matrix equations, we show that the resulting reduced-order models can be obtained as an additional byproduct. These models not only interpolate the original transfer function at the mirror images of the ADI shifts but also preserve important system properties such as stability, minimum-phase property, positive-realness, bounded-realness, and passivity. Consequently, the proposed unified ADI framework also serves as a recursive, interpolation-based model order reduction method, which can preserve several important properties of the original model in the reduced-order model.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [52] [Fermionic neural Gibbs states](https://arxiv.org/abs/2512.04663)
*Jannes Nys,Juan Carrasquilla*

Main category: quant-ph

TL;DR: fNGS is a neural network framework for modeling finite-temperature properties of strongly interacting fermions, showing accurate results for the doped Fermi-Hubbard model beyond exact methods.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable method for studying finite-temperature properties of strongly correlated fermionic systems beyond one dimension, where exact methods become intractable.

Method: Uses fermionic neural Gibbs states (fNGS) starting from a reference mean-field thermofield-double state, applying neural-network transformations with imaginary-time evolution to build strong correlations systematically.

Result: Accurately reproduces thermal energies over broad temperature ranges, interaction strengths, and large dopings for the doped Fermi-Hubbard model, for system sizes beyond exact methods.

Conclusion: fNGS demonstrates a scalable route to studying finite-temperature properties of strongly correlated fermionic systems with neural-network quantum state representations.

Abstract: We introduce fermionic neural Gibbs states (fNGS), a variational framework for modeling finite-temperature properties of strongly interacting fermions. fNGS starts from a reference mean-field thermofield-double state and uses neural-network transformations together with imaginary-time evolution to systematically build strong correlations. Applied to the doped Fermi-Hubbard model, a minimal lattice model capturing essential features of strong electronic correlations, fNGS accurately reproduces thermal energies over a broad range of temperatures, interaction strengths, even at large dopings, for system sizes beyond the reach of exact methods. These results demonstrate a scalable route to studying finite-temperature properties of strongly correlated fermionic systems beyond one dimension with neural-network representations of quantum states.

</details>


### [53] [Convergence of sample-based quantum diagonalization on a variable-length cuprate chain](https://arxiv.org/abs/2512.04962)
*L. Andrew Wray,Cheng-Ju Lin,Vincent Su,Hrant Gharibyan*

Main category: quant-ph

TL;DR: SQD algorithm for quantum chemistry has convergence issues; study explores scaling with copper oxide systems, finding connectivity, expansion order, and orbital basis choices help overcome sampling bottlenecks, with noise sometimes improving convergence.


<details>
  <summary>Details</summary>
Motivation: Sample-based quantum diagonalization (SQD) is promising for NISQ-era quantum chemistry but suffers from convergence problems on practical timescales, motivating investigation of scaling behavior and optimization strategies.

Method: Study scaling of SQD algorithm on variable-length copper oxide plaquette molecules (2-6 units) with minimal basis; test effects of all-to-all connectivity, higher expansion order, non-Hartree-Fock orbital basis, and real quantum hardware noise.

Result: All-to-all connectivity, higher expansion order, and non-Hartree-Fock basis help overcome sampling bottlenecks but involve tradeoffs; surprisingly, noise on Quantinuum H2 device can improve energy convergence beyond noise-free simulations.

Conclusion: SQD convergence can be improved through hardware connectivity, algorithmic expansion, and orbital basis choices, with noise sometimes beneficial; practical implementation requires balancing tradeoffs against quantum/classical hardware capabilities.

Abstract: Sample-based quantum diagonalization (SQD) is an algorithm for hybrid quantum-classical molecular simulation that has been of broad interest for application with noisy intermediate scale quantum (NISQ) devices. However, SQD does not always converge on a practical timescale. Here, we explore scaling of the algorithm for a variable-length molecule made up of 2 to 6 copper oxide plaquettes with a minimal molecular orbital basis. The results demonstrate that enabling all-to-all connectivity, instituting a higher expansion order for the SQD algorithm, and adopting a non-Hartree-Fock molecular orbital basis can all play significant roles in overcoming sampling bottlenecks, though with tradeoffs that need to be weighed against the capabilities of quantum and classical hardware. Additionally, we find that noise on a real quantum computer, the Quantinuum H2 trapped ion device, can improve energy convergence beyond expectations based on noise-free statevector simulations.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [54] [NORi: An ML-Augmented Ocean Boundary Layer Parameterization](https://arxiv.org/abs/2512.04452)
*Xin Kai Lee,Ali Ramadhan,Andre Souza,Gregory LeClaire Wagner,Simone Silvestri,John Marshall,Raffaele Ferrari*

Main category: physics.ao-ph

TL;DR: NORi is a physics-based ML parameterization using neural ODEs to model ocean boundary layer turbulence, trained on LES data to capture entrainment dynamics with excellent generalization and numerical stability.


<details>
  <summary>Details</summary>
Motivation: Traditional local diffusive closures fail to capture entrainment through the base of ocean boundary layers, which is crucial for accurate climate modeling. There's a need for parameterizations that combine physical rigor with machine learning efficiency while maintaining numerical stability for long-term climate simulations.

Method: NORi combines physics-based Richardson number-dependent diffusivity/viscosity with neural ODEs trained in an "a posteriori" fashion using large-eddy simulations. The training optimizes parameters based on time-integrated variables rather than noisy instantaneous subgrid fluxes, using 2-day training horizons.

Result: NORi demonstrates excellent prediction and generalization across different convective strengths, stratifications, rotation strengths, and wind forcings. It remains numerically stable for at least 100 years of integration despite 2-day training, supports 1-hour time steps, and works with realistic seawater equations of state.

Conclusion: The hybrid approach of expressive neural networks with physically-rigorous base closure provides a robust paradigm for climate model parameterizations, reducing data requirements while targeting inference performance and encouraging numerical stability during training.

Abstract: NORi is a machine-learned (ML) parameterization of ocean boundary layer turbulence that is physics-based and augmented with neural networks. NORi stands for neural ordinary differential equations (NODEs) Richardson number (Ri) closure. The physical parameterization is controlled by a Richardson number-dependent diffusivity and viscosity. The NODEs are trained to capture the entrainment through the base of the boundary layer, which cannot be represented with a local diffusive closure. The parameterization is trained using large-eddy simulations in an "a posteriori" fashion, where parameters are calibrated with a loss function that explicitly depends on the actual time-integrated variables of interest rather than the instantaneous subgrid fluxes, which are inherently noisy. NORi is designed for the realistic nonlinear equation of state of seawater and demonstrates excellent prediction and generalization capabilities in capturing entrainment dynamics under different convective strengths, oceanic background stratifications, rotation strengths, and surface wind forcings. NORi is numerically stable for at least 100 years of integration time in large-scale simulations, despite only being trained on 2-day horizons, and can be run with time steps as long as one hour. The highly expressive neural networks, combined with a physically-rigorous base closure, prove to be a robust paradigm for designing parameterizations for climate models where data requirements are drastically reduced, inference performance can be directly targeted and optimized, and numerical stability is implicitly encouraged during training.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [55] [The Dirichlet heat trace for domains with curved corners](https://arxiv.org/abs/2512.04422)
*Shi Zhuo Looi,David Sher*

Main category: math.SP

TL;DR: The paper studies short-time heat trace asymptotics for curvilinear polygons, showing the t^{1/2} coefficient splits into boundary curvature integral plus local corner contributions depending on angles and limiting curvatures.


<details>
  <summary>Details</summary>
Motivation: To understand the short-time behavior of heat traces on domains with curved boundaries and corners, which has applications in spectral geometry and inverse problems.

Method: Uses conformal modeling and parametrix construction on sector heat space to analyze heat trace asymptotics, expressing corner contributions as finite parts of explicit traces over exact sectors.

Result: Derives explicit formula for t^{1/2} coefficient with boundary curvature integral plus local corner terms; computes specific value for right-angled corners; shows Dirichlet isospectral curvilinear polygons must have straight sides.

Conclusion: The heat trace expansion reveals geometric information about domains with corners and curved boundaries, enabling spectral determination results for polygons.

Abstract: We study the short-time asymptotics of the Dirichlet heat trace on planar curvilinear polygons. For such domains we show that the coefficient of $t^{1/2}$ in the expansion splits into a boundary integral of $Œ∫^2$ and a sum of local corner contributions, one for each vertex. Each curved corner contribution depends only on the interior angle $Œ±$ and on the limiting curvatures $Œ∫_{\pm}$ on the adjacent sides. Using a conformal model and a parametrix construction on the sector heat space, we express this contribution in the form $c_{1/2}(Œ±)\,r_0(Œ±,Œ∫_+, Œ∫_-)$, where $c_{1/2}(Œ±)$ is given by a Hadamard finite part of an explicit trace over the exact sector. For right-angled corners we compute $c_{1/2}(œÄ/2)=1/(16\sqrtœÄ)$ and obtain a closed formula for the $t^{1/2}$ coefficient. As an application we extend a previous result in the literature by showing that any admissible curvilinear polygon that is Dirichlet isospectral to a polygon must itself be a polygon with straight sides.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [56] [Double Perovskites K2NbTaO6 and Rb2NbTaO6 from First-Principles: Towards Efficient Materials for Green Energy](https://arxiv.org/abs/2512.04134)
*Ouendadji Salima,Aissani Ali,El Haj Hassan Fouad,Benahmedi Lakhdar*

Main category: cond-mat.mtrl-sci

TL;DR: First-principles study shows K2NbTaO6 and Rb2NbTaO6 double perovskites are mechanically stable, brittle semiconductors with band gaps ~2.6-2.8 eV, exhibiting UV absorption for potential optoelectronic applications.


<details>
  <summary>Details</summary>
Motivation: Double perovskite oxides offer structural flexibility and multifunctional properties for coupled optical, mechanical, and thermal applications, making them attractive materials for various technological uses.

Method: First-principles computations were used to examine structural, electronic, elastic, optical, and thermoelectric properties of K2NbTaO6 and Rb2NbTaO6 double perovskites, analyzing their cubic structures with ordered Nb5+ and Ta5+ cations.

Result: Both compounds form cubic double perovskite structures, are mechanically stable (satisfying Born criteria) but brittle (Pugh's ratio). They are semiconductors with band gaps of 2.79 eV (K2NbTaO6) and 2.63 eV (Rb2NbTaO6), showing UV absorption relevant for optoelectronic studies.

Conclusion: K2NbTaO6 and Rb2NbTaO6 double perovskites show promising structural and electronic properties for theoretical studies of optoelectronic and photocatalytic applications, though their brittleness and specific absorption characteristics may limit practical device efficiency.

Abstract: The structural flexibility and multifunctional nature of double perovskite oxides make them attractive for applications requiring coupled optical, mechanical, and thermal performance. Using first-principles computations, this study examines the structural, electronic, elastic, optical, and thermoelectric stability of K2NbTaO6 and Rb2NbTaO6. The two compounds combine to form a cubic double perovskite structure with ordered Nb$^{5+}$ and Ta$^{5+}$ cations. The calculated elastic constants satisfy the Born stability criteria, confirming mechanical stability; however, both K2NbTaO6 and Rb2NbTaO6 exhibit brittle behavior according to Pugh's ratio, reflecting limited ductility. Semiconducting behavior is revealed by band structure analysis with energy gaps of 2.79 eV for K2NbTaO6 and 2.63 eV for Rb2NbTaO6. Optical spectra show noticeable absorption in the high-energy region near the UV, indicating relevance for theoretical studies of optoelectronic and photocatalytic processes, without implying practical device efficiency. Therm

</details>
