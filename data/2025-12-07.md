<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 11]
- [math.AP](#math.AP) [Total: 22]
- [physics.comp-ph](#physics.comp-ph) [Total: 9]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 5]
- [hep-ph](#hep-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [math-ph](#math-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.optics](#physics.optics) [Total: 2]
- [gr-qc](#gr-qc) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Iterative Contact-resolving Hybrid Methods for Multiscale Contact Mechanics](https://arxiv.org/abs/2512.04411)
*Eric T. Chung,Hyea Hyun Kim,Xiang Zhong*

Main category: math.NA

TL;DR: A hybrid iterative method for contact mechanics with high contrast coefficients that localizes nonlinear contact constraints to a small subdomain while using linear methods in the larger domain, with four discretization types combining standard/mixed FEM and multiscale methods.


<details>
  <summary>Details</summary>
Motivation: Contact mechanics with high contrast coefficients presents significant mathematical and computational challenges, especially for strongly symmetric stress approximations. Conventional monolithic approaches lead to high global complexity due to inherent nonlinearity of contact problems.

Method: Developed an iterative contact-resolving hybrid method that localizes nonlinear contact constraints within a smaller subdomain while the larger subdomain is governed by a linear system. Uses variational inequality theory, minimization principles, and penalty methods. Proposed four discretization types: standard/mixed FEM across entire domain, and combinations of standard/mixed multiscale methods in larger domain with standard/mixed FEM in smaller domain.

Result: The method avoids excessive degrees of freedom in the larger domain through multiscale reduction while enabling direct stress computation, ensuring local momentum conservation, and resisting locking in nearly incompressible materials. Convergence analysis and algorithms provided for all cases, with extensive numerical experiments validating effectiveness.

Conclusion: The hybrid approach successfully addresses computational challenges in contact mechanics with high contrast coefficients by efficiently separating nonlinear contact constraints from the larger linear domain, offering multiple discretization options with proven convergence and practical effectiveness.

Abstract: Modeling contact mechanics with high contrast coefficients presents significant mathematical and computational challenges, especially in achieving strongly symmetric stress approximations. Due to the inherent nonlinearity of contact problems, conventional methods that treat the entire domain as a monolithic system often lead to high global complexity. To address this, we develop an iterative contact-resolving hybrid method by localizing nonlinear contact constraints within a smaller subdomain, while the larger subdomain is governed by a linear system. Our system employs variational inequality theory, minimization principles, and penalty methods. More importantly, we propose four discretization types within the two-subdomain framework, ranging from applying standard/mixed FEM across the entire domain to combining standard/mixed multiscale methods in the larger subdomain with standard/mixed FEM in the smaller one. % The standard finite element method and standard constraint energy minimizing generalized multiscale finite element method are simple and easy to demonstrate. By employing a multiscale reduction technique, the method avoids excessive degrees of freedom inherent in conventional methods in the larger domain, while the mixed formulation enables direct stress computation, ensures local momentum conservation, and resists locking in nearly incompressible materials. Convergence analysis and the corresponding algorithms are provided for all cases. Extensive numerical experiments are presented to validate the effectiveness of the approaches.

</details>


### [2] [Stable self-adaptive timestepping for Reduced Order Models for incompressible flows](https://arxiv.org/abs/2512.04592)
*Josep Plana-Riu,Henrik Rosenberger,Benjamin Sanderse,F. Xavier Trias*

Main category: math.NA

TL;DR: RedEigCD is the first self-adaptive timestepping method for ROMs of incompressible Navier-Stokes equations that uses eigenbounds of reduced operators to determine stable timesteps, proving ROMs can use larger timesteps than FOMs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Current ROMs for incompressible flow simulations lack specialized adaptive timestepping methods. Traditional error-based adaptive methods are not tailored for ROMs, and there's a need for efficient, self-regulating integration techniques that leverage the reduced scale of ROM operators while maintaining online efficiency.

Method: RedEigCD adapts timesteps by bounding the stability function of time integration schemes using exact spectral information from reduced operators. It computes eigenbounds of convective and diffusive ROM operators, which is feasible at reduced scale. The method is based on linear stability concepts and uses theorems of Bendixson and Rao to prove theoretical properties.

Result: Numerical experiments show RedEigCD yields stable timestep increases up to 40× compared to full-order models without compromising accuracy. The method proves theoretically that maximum stable timestep for projection-based ROMs is larger than or equal to that of corresponding FOMs under linearized assumptions.

Conclusion: RedEigCD establishes a new link between linear stability theory and reduced-order modeling, offering a systematic path toward efficient, self-regulating ROM integration for incompressible flow simulations while preserving online efficiency of ROMs.

Abstract: This work introduces RedEigCD, the first self-adaptive timestepping technique specifically tailored for reduced-order models (ROMs) of the incompressible Navier-Stokes equations. Building upon linear stability concepts, the method adapts the timestep by directly bounding the stability function of the employed time integration scheme using exact spectral information of matrices related to the reduced operators. Unlike traditional error-based adaptive methods, RedEigCD relies on the eigenbounds of the convective and diffusive ROM operators, whose computation is feasible at reduced scale and fully preserves the online efficiency of the ROM. A central theoretical contribution of this work is the proof, based on the combined theorems of Bendixson and Rao, that, under linearized assumptions, the maximum stable timestep for projection-based ROMs is shown to be larger than or equal to that of their corresponding full-order models (FOMs). Numerical experiments for both periodic and non-homogeneous boundary conditions demonstrate that RedEigCD yields stable timestep increases up to a factor 40 compared to the FOM, without compromising accuracy. The methodology thus establishes a new link between linear stability theory and reduced-order modeling, offering a systematic path towards efficient, self-regulating ROM integration in incompressible flow simulations.

</details>


### [3] [Interface layers and coupling conditions for discrete kinetic models on networks: a spectral approac](https://arxiv.org/abs/2512.04634)
*Raul Borsche,Tobias Damm,Axel Klar,Yizhou Zhou*

Main category: math.NA

TL;DR: Kinetic BGK models on networks with wave equation limit; coupling conditions derived via asymptotic analysis; spectral method for solving half-space problems; coefficients determined for macroscopic coupling.


<details>
  <summary>Details</summary>
Motivation: To develop coupling conditions for macroscopic equations on networks from underlying kinetic models, enabling accurate modeling of transport phenomena in networked systems where kinetic effects are important.

Method: Asymptotic analysis near network nodes using kinetic half-space problems; analytical treatment of discrete velocity models; development of efficient spectral method for solving coupled half-space problems.

Result: Derived coupling conditions for macroscopic wave equations from kinetic models; developed spectral method with fast convergence; determined coefficients (extrapolation length) for macroscopic coupling; validated through numerical comparisons between kinetic and macroscopic solutions.

Conclusion: The approach successfully bridges kinetic and macroscopic modeling on networks, providing accurate coupling conditions and efficient computational methods for network transport problems with kinetic effects.

Abstract: We consider kinetic and related macroscopic equations on networks. A class of linear kinetic BGK models is considered, where the limit equation for small Knudsen numbers is given by the wave equation. Coupling conditions for the macroscopic equations are obtained from the kinetic coupling conditions via an asymptotic analysis near the nodes of the network and the consideration of coupled solutions of kinetic half-space problems. Analytical results are obtained for a discrete velocity version of the coupled half-space problems. Moreover, an efficient spectral method is developed to solve the coupled discrete velocity half-space problems. In particular, this allows to determine the relevant coefficients in the coupling conditions for the macroscopic equations
  from the underlying kinetic network problem. These coefficients correspond to the so-called extrapolation length for kinetic boundary value problems. Numerical results show the accuracy and fast convergence of the approach. Moreover, a comparison of the kinetic solution on the network with the macroscopic solution is presented.

</details>


### [4] [Weighted total variation regularization for inverse problems with significant null spaces](https://arxiv.org/abs/2512.04729)
*Martin Burger,Ole Løseth Elvetun,Bjørn Fredrik Nielsen*

Main category: math.NA

TL;DR: The paper proposes weighted total variation (TV) regularization for inverse problems with large null spaces (like EEG/ECG), which helps recover internal sources away from boundaries, unlike standard methods that bias solutions toward data acquisition sites.


<details>
  <summary>Details</summary>
Motivation: Standard regularization methods for inverse problems with large null spaces (common in EEG/ECG applications) produce solutions biased toward data acquisition sites, leading to misinterpretation of internal sources as being near body surfaces.

Method: Extends previous weighting schemes to total variation (TV) regularization, introducing weighted TV-regularization with supporting analysis. Also explores a hybrid approach combining weighted-sparsity and TV regularization.

Result: Weighted TV successfully recovers location and size of large, piecewise constant sources away from boundaries (though not exact shape). Hybrid approach captures both small and large sources but with more blurred reconstructions than weighted TV alone.

Conclusion: Weighted TV regularization effectively addresses the bias problem in inverse problems with large null spaces, enabling recovery of internal sources away from boundaries, with hybrid methods offering additional flexibility for handling different source sizes.

Abstract: We consider inverse problems with large null spaces, which arise in important applications such as in inverse ECG and EEG procedures. Standard regularization methods typically produce solutions in or near the orthogonal complement of the forward operator's null space. This often leads to inadequate results, where internal sources are mistakenly interpreted as being near the data acquisition sites -- e.g., near or at the body surface in connection with EEG and ECG recordings.
  To mitigate this, we previously proposed weighting schemes for Tikhonov and sparsity regularization. Here, we extend this approach to total variation (TV) regularization, which is particularly suited for identifying spatially extended regions with approximately constant values. We introduce a weighted TV-regularization method, provide supporting analysis, and demonstrate its performance through numerical experiments. Unlike standard TV regularization, the weighted version successfully recovers the location and size of large, piecewise constant sources away from the boundary, though not their exact shape.
  Additionally, we explore a hybrid weighted-sparsity and TV regularization approach, which better captures both small and large sources, albeit with somewhat more blurred reconstructions than the weighted TV method alone.

</details>


### [5] [Recent advances in the numerical solution of multi-order fractional differential equations](https://arxiv.org/abs/2512.04737)
*Luigi Brugnano,Gianmarco Gurioli,Felice Iavernaro,Mikk Vikerpuur*

Main category: math.NA

TL;DR: Extension of Fractional HBVMs (FHBVMs) to solve fractional differential equations with multiple different fractional derivative orders, providing Matlab implementation for two-order cases.


<details>
  <summary>Details</summary>
Motivation: Existing FHBVMs only handle systems with the same fractional derivative order, but multi-order problems (with different fractional orders) are important in applications and haven't been addressed yet.

Method: Extends Fractional HBVMs (Runge-Kutta type methods) to handle fractional multi-order problems, providing full implementation details and a Matlab code for two different fractional orders.

Result: The proposed extension proves very effective for numerically solving fractional multi-order problems, with competitive performance compared to existing methods.

Conclusion: Successfully extends FHBVMs to handle fractional differential equations with multiple different derivative orders, providing practical implementation tools for important application problems.

Abstract: The efficient numerical solution of fractional differential equations has been recently tackled through the definition of Fractional HBVMs (FHBVMs), a class of Runge-Kutta type methods. Corresponding Matlab (c) codes have been also made available on the internet, proving to be very competitive w.r.t. existing ones. However, so far, FHBVMs have been given for solving systems of fractional differential equations with the same order of fractional derivative, whereas the numerical solution of multi-order problems (i.e., problems in which different orders of fractional derivatives occur) has not been handled, yet. Due to their relevance in applications, in this paper we propose an extension of FHBVMs for addressing fractional multi-order problems, providing full details for such an approach. A corresponding Matlab (c) code, handling the case of two different fractional orders, is also made available, proving very effective for numerically solving these problems.

</details>


### [6] [Construction of the Nearest Nonnegative Hankel Matrix for a Prescribed Eigenpair](https://arxiv.org/abs/2512.04812)
*Prince Kanhya,Udit Raj*

Main category: math.NA

TL;DR: Computes minimal structured perturbations to make a prescribed eigenpair exact for nonnegative Hankel matrices, using optimization to find either exact solutions or nearest approximations.


<details>
  <summary>Details</summary>
Motivation: Need to assess eigenpair sensitivity for nonnegative Hankel matrices, which arise in signal processing and system identification, where preserving structure and nonnegativity constraints is crucial.

Method: Formulates as feasibility problem with linear constraints for Hankel structure and nonnegativity. Uses optimization to compute minimum-norm perturbation when feasible, or minimizes residual norm when infeasible.

Result: Provides numerical optimization framework for structured backward error analysis, handling both feasible cases (exact eigenpair achievable) and infeasible cases (nearest approximation).

Conclusion: Offers practical computational approach for eigenpair sensitivity analysis in structured nonnegative Hankel matrices where closed-form solutions are unavailable.

Abstract: We study the problem of determining whether a prescribed eigenpair $(λ,x)$
  can be made an exact eigenpair of a nonnegative Hankel matrix through the smallest
  possible structured perturbation. The task reduces to check the feasibility of a
  set of linear constraints that encode both the Hankel structure and entrywise
  nonnegativity. When the feasibility set is nonempty, we compute the minimum-norm
  perturbation $ΔH$ such that $(H+ΔH)x=λx$. When no such perturbation
  exists, we compute the nearest nonnegative Hankel matrix in a residual sense by
  minimizing $\|(H+ΔH)x-λx\|_{2}$ subject to the imposed constraints.
  Because closed-form formulas for the structured backward error are generally
  unavailable, our method provides a fully numerical and optimization-based
  framework for evaluating eigenpair sensitivity under nonnegativity-preserving
  Hankel perturbations. Numerical examples illustrate both feasible and infeasible cases.

</details>


### [7] [Hierarchical matrix approximability of inverse of convection dominated finite element matrices](https://arxiv.org/abs/2512.04824)
*Arthur Saunier,Leo Agelas,Ani Anciaux Sedrakian,Ibtihel Ben Gharbia,Xavier Claeys*

Main category: math.NA

TL;DR: A novel partitioning strategy using "convection tubes" aligned with the convection vector field enables effective hierarchical matrix compression for convection-dominated PDEs on unstructured grids, overcoming limitations of previous methods.


<details>
  <summary>Details</summary>
Motivation: Hierarchical matrices (H-matrices) are powerful for compressing large matrices with nearly linear complexity, but their effectiveness is limited for convection-dominated problems due to loss of coercivity. Previous approaches only worked for structured grids with constant convection.

Method: Proposes a novel partitioning strategy based on "convection tubes" - clusters aligned with the convection vector field. This approach doesn't require structured grids or constant convection and builds on a Péclet-robust Caccioppoli inequality for handling convection-dominated problems.

Result: Theoretical analyses and numerical experiments demonstrate the efficiency and robustness of the method for convection-dominated PDEs on unstructured grids, overcoming limitations of previous approaches.

Conclusion: The convection tube partitioning strategy successfully extends hierarchical matrix techniques to convection-dominated problems on general unstructured grids, providing a flexible and scalable framework where traditional methods fail.

Abstract: Several researchers have developed a rich toolbox of matrix compression techniques that exploit structure and redundancy in large matrices. Classical methods such as the block low-rank format and the Fast Multipole Method make it possible to manipulate very large systems by representing them in a reduced form. Among the most sophisticated tools in this area are hierarchical matrices (H-matrices), which exploit local properties of the underlying kernel or operator to approximate matrix blocks by low-rank factors, organized in a recursive hierarchy. H-matrices offer a flexible and scalable framework, yielding nearly linear complexity in both storage and computation. Hierarchical matrix techniques, originally developed for boundary integral equations, have recently been applied to matrices stemming from the discretization of advection-dominated problems. However, their effectiveness is limited by the loss of coercivity induced by convection phenomena, where traditional methods fail. Initial work by Le Borne addressed this by modifying the admissibility criterion for structured grids with constant convection, but challenges remain for more general grids and advection fields. In this work, we propose a novel partitioning strategy based on "convection tubes", clusters aligned with the convection vector field. This method does not require a structured grid or constant convection, overcoming the limitations of previous approaches. We present both theoretical analyses and numerical experiments, that demonstrate the efficiency and robustness of our method for convection-dominated PDEs on unstructured grids. The approach builds on a Péclet-robust Caccioppoli inequality, crucial for handling convection-dominated problems.

</details>


### [8] [A High-Order Discretization Scheme for Surface Integral Equations for Analyzing the Electroencephalography Forward Problem](https://arxiv.org/abs/2512.04845)
*Rui Chen,Viviana Giunzioni,Adrien Merlini,Francesco P. Andriulli*

Main category: math.NA

TL;DR: A high-order Nyström discretization scheme for surface integral equations in EEG forward modeling that uses separate interpolation points from mesh nodes for flexible basis function order adjustment without remeshing.


<details>
  <summary>Details</summary>
Motivation: To develop a more flexible and efficient high-order discretization scheme for EEG forward problem analysis that overcomes limitations of existing isoparametric approaches, particularly the need to regenerate meshes when changing basis function order.

Method: Uses Nyström-based high-order discretization with separate interpolation points from mesh nodes, allowing flexible basis function order adjustment without mesh regeneration. Interpolation points are chosen from quadrature rules to simplify far-interaction integral computation. Extends implementation to multiple formulations including double-layer, adjoint double-layer, isolated-skull-approach, and indirect adjoint double-layer formulations with specialized singularity treatment techniques.

Result: Numerical experiments demonstrate the accuracy, flexibility, and efficiency of the proposed scheme for all four surface integral equations in EEG forward problem analysis.

Conclusion: The proposed high-order Nyström discretization scheme provides an accurate, flexible, and efficient approach for EEG forward modeling with advantages over existing methods, particularly in allowing basis function order changes without mesh regeneration and simplifying far-interaction computations.

Abstract: A Nystrom-based high-order (HO) discretization scheme for surface integral equations (SIEs) for analyzing the electroencephalography (EEG) forward problem is proposed in this work. We use HO surface elements and interpolation functions for the discretization of the interfaces of the head volume and the unknowns on the elements, respectively. The advantage of this work over existing isoparametric HO discretization schemes resides in the fact that the interpolation points are different from the mesh nodes, allowing for the flexible manipulation of the order of the basis functions without regenerating the mesh of the interfaces. Moreover, the interpolation points are chosen from the quadrature rules with the same number of points on the elements simplifying the numerical computation of the surface integrals for the far-interaction case. In this contribution, we extend the implementation of the HO discretization scheme to the double-layer and the adjoint double-layer formulations, as well as to the isolated-skull-approach for the double-layer formulation and to the indirect adjoint double-layer formulation, employed to improve the solution accuracy in case of high conductivity contrast models, which requires the development of different techniques for the singularity treatment. Numerical experiments are presented to demonstrate the accuracy, flexibility, and efficiency of the proposed scheme for the four SIEs for analyzing the EEG forward problem.

</details>


### [9] [Data-driven Methods for Delay Differential Equations](https://arxiv.org/abs/2512.04894)
*Dimitri Breda,Xunbi A. Ji,Gábor Orosz,Muhammad Tanveer*

Main category: math.NA

TL;DR: Extends SINDy algorithm and introduces neural networks for delay differential equations with unknown delays, providing MATLAB implementations and comparing approaches on classical systems.


<details>
  <summary>Details</summary>
Motivation: Data-driven methods are increasingly applied to dynamical systems, but existing tools like SINDy were originally developed for ODEs, not DDEs with unknown delays. There's a need to extend these methodologies to handle delay differential equations.

Method: Two extensions of SINDy for DDEs: one directly tackles DDEs, another uses pseudospectral collocation to approximate DDEs as ODEs. Also introduces neural delay differential equations (NDDEs) with trainable delays using neural networks in continuous time.

Result: Approaches tested on classical systems including delay logistic equation, Mackey-Glass equation, and delayed Rössler system. MATLAB implementations provided for both SINDy and NDDE approaches, with direct comparison between methods.

Conclusion: Provides data-driven methods for time delay systems, insights on connections between approaches, and future directions for developing data-driven methods for delay differential equations.

Abstract: Data-driven methodologies are nowadays ubiquitous. Their rapid development and spread have led to applications even beyond the traditional fields of science. As far as dynamical systems and differential equations are concerned, neural networks and sparse identification tools have emerged as powerful approaches to recover the governing equations from available temporal data series. In this chapter we first illustrate possible extensions of the sparse identification of nonlinear dynamics (SINDy) algorithm, originally developed for ordinary differential equations (ODEs), to delay differential equations (DDEs) with discrete, possibly multiple and unknown delays. Two methods are presented for SINDy, one directly tackles the underlying DDE and the other acts on the system of ODEs approximating the DDE through pseudospectral collocation. We also introduce another way of capturing the dynamics of DDEs using neural networks and trainable delays in continuous time, and present the training algorithms developed for these neural delay differential equations (NDDEs). The relevant MATLAB implementations for both the SINDy approach and for the NDDE approach are provided. These approaches are tested on several examples, including classical systems such as the delay logistic and the Mackey-Glass equation, and directly compared to each other on the delayed Rössler system. We provide insights on the connection between the approaches and future directions on developing data-driven methods for time delay systems.

</details>


### [10] [Weak convergence rates for spectral regularization via sampling inequalities](https://arxiv.org/abs/2512.04929)
*Sabrina Guastavino,Gabriele Santin,Francesco Marchetti,Federico Benvenuto*

Main category: math.NA

TL;DR: The paper establishes weak convergence rates for spectral regularization methods in inverse problems without requiring source conditions, by connecting inverse problems to kernel approximation and using sampling inequalities.


<details>
  <summary>Details</summary>
Motivation: Classical convergence rate analysis for inverse problems relies on source conditions to estimate truncation error. The authors aim to develop convergence rate bounds that don't depend on these restrictive source conditions by leveraging connections with kernel approximation theory.

Method: 1) Generalize sampling inequalities to spectral regularization methods. 2) Exploit the connection between inverse problems and kernel approximation. 3) Derive weak convergence rate bounds independently of source conditions for compact/uniformly bounded forward operators or trace class kernel operators.

Result: Established weak convergence rate bounds for inverse problems using spectral regularization methods without requiring source conditions. The results apply to compact and uniformly bounded forward operators, or trace class kernel operators.

Conclusion: The paper successfully develops a framework for analyzing convergence rates in inverse problems that doesn't rely on traditional source conditions, instead using sampling inequalities and connections to kernel approximation theory to obtain weak convergence rate bounds.

Abstract: Convergence rates in spectral regularization methods quantify the approximation error in inverse problems as a function of the noise level or the number of sampling points. Classical strong convergence rate results typically rely on source conditions, which are essential for estimating the truncation error. However, in the framework of kernel approximation, the truncation error in the case of Tikhonov regularization can be characterized entirely through sampling inequalities, without invoking source conditions. In this paper, we first generalize sampling inequalities to spectral regularization, and then, by exploiting the connection between inverse problems and kernel approximation, we derive weak convergence rate bounds for inverse problems, independently of source conditions. These weak convergence rates are established and analyzed when the forward operator is compact and uniformly bounded, or the kernel operator is of trace class.

</details>


### [11] [A tangential low-rank ADI method for solving indefinite Lyapunov equations](https://arxiv.org/abs/2512.04983)
*Rudi Smith,Steffen W. R. Werner*

Main category: math.NA

TL;DR: A novel tangential ADI iteration method for efficiently solving large-scale Lyapunov equations with indefinite right-hand sides, even when the constant term has high rank.


<details>
  <summary>Details</summary>
Motivation: Classical block-type approaches for solving Lyapunov equations with indefinite constant terms become computationally expensive when the rank of the constant term grows, creating a need for more efficient methods.

Method: Proposes a tangential reformulation of the ADI iteration that enables efficient construction of low-rank approximations to Lyapunov equation solutions with indefinite right-hand sides, including adaptive methods for selecting ADI parameters (shifts and tangential directions).

Result: The developed algorithms effectively handle Lyapunov equations with indefinite right-hand sides even when constant terms have higher ranks, as demonstrated through several numerical examples.

Conclusion: The tangential ADI iteration provides an efficient computational approach for large-scale Lyapunov equations with indefinite constant terms, overcoming limitations of classical block-type methods when dealing with high-rank constant terms.

Abstract: Continuous-time algebraic Lyapunov equations have become an essential tool in various applications. In the case of large-scale sparse coefficient matrices and indefinite constant terms, indefinite low-rank factorizations have successfully been used to allow methods like the alternating direction implicit (ADI) iteration to efficiently compute accurate approximations to the solution of the Lyapunov equation. However, classical block-type approaches quickly increase in computational costs when the rank of the constant term grows. In this paper, we propose a novel tangential reformulation of the ADI iteration that allows for the efficient construction of low-rank approximations to the solution of Lyapunov equations with indefinite right-hand sides even in the case of constant terms with higher ranks. We provide adaptive methods for the selection of the corresponding ADI parameters, namely shifts and tangential directions, which allow for the automatic application of the method to any relevant problem setting. The effectiveness of the developed algorithms is illustrated by several numerical examples.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [12] [Regularity for minimizers of degenerate, non-autonomous, orthotropic integral functionals](https://arxiv.org/abs/2512.04281)
*Antonio Giuseppe Grimaldi,Stefania Russo*

Main category: math.AP

TL;DR: The paper proves higher differentiability of integer order for locally bounded minimizers of anisotropic, non-autonomous integral functionals with variable exponents and coefficients.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend regularity theory to more general anisotropic functionals that are non-autonomous (depend on x) and anisotropic (different exponents p_i in different directions), which arise in various applications but present analytical challenges beyond standard isotropic cases.

Method: The method involves proving higher differentiability results for minimizers of functionals with anisotropic growth conditions and variable coefficients. The approach likely uses techniques from calculus of variations, Sobolev space theory, and regularity theory for elliptic equations, handling the anisotropic structure and non-autonomous nature of the functional.

Result: The main result establishes that locally bounded minimizers of the given anisotropic, non-autonomous integral functional possess higher differentiability of integer order, under suitable Sobolev regularity assumptions on the coefficients a_i(x).

Conclusion: The paper successfully extends regularity theory to anisotropic, non-autonomous functionals, demonstrating that higher differentiability properties can be established even when the functional depends on the solution and has variable exponents and coefficients.

Abstract: We prove the higher differentiability of integer order of locally bounded minimizers of integral functionals of the form \begin{equation*}
  \mathcal{F}(u,Ω):= \,\sum_{i=1}^{n} \dfrac{1}{p_i}\displaystyle \int_Ω\, a_i(x) \lvert u_{x_i} \rvert^{p_i} dx- \int_Ωω(x)u(x) dx, \end{equation*} where the exponents $ p_i \geq 2 $ and the coefficients $ a_i(x) $ satisfy a suitable Sobolev regularity. The main novelty consists in dealing with non-autonomous, anisotropic functionals, which depend also on the solution.

</details>


### [13] [Diffusive limit of the Boltzmann equation around Rayleigh profile in the half space](https://arxiv.org/abs/2512.04403)
*Hongxu Chen,Renjun Duan*

Main category: math.AP

TL;DR: The paper analyzes the diffusive limit of the Boltzmann equation in half-space with moving boundary conditions, showing convergence to Navier-Stokes with Rayleigh profile solutions.


<details>
  <summary>Details</summary>
Motivation: To rigorously establish the connection between kinetic theory (Boltzmann equation) and fluid dynamics (Navier-Stokes equations) in the presence of moving boundary conditions, specifically when the boundary has tangential motion proportional to the Knudsen number.

Method: Uses the Hilbert expansion method to construct Boltzmann solutions around the Rayleigh profile (time-dependent shearing solution of Navier-Stokes) for well-prepared initial data in the half-space domain.

Result: Successfully constructs Boltzmann solutions without initial singularity over any finite time interval, demonstrating convergence to the Navier-Stokes Rayleigh profile in the diffusive limit as Knudsen number approaches zero.

Conclusion: The Boltzmann equation with moving boundary conditions converges to the incompressible Navier-Stokes equations with Rayleigh profile solutions in the diffusive limit, providing rigorous mathematical justification for this fluid dynamic limit.

Abstract: This paper concerns the diffusive limit of the time evolutionary Boltzmann equation in the half space $\mathbb{T}^2\times\mathbb{R}^+$ for a small Knudsen number $\varepsilon>0$. For boundary conditions in the normal direction, it involves diffuse reflection moving with a tangent velocity proportional to $\varepsilon$ on the wall, whereas the far field is described by a global Maxwellian with zero bulk velocity. The incompressible Navier-Stokes equations, as the corresponding formal fluid dynamic limit, admit a specific time-dependent shearing solution known as the Rayleigh profile, which accounts for the effect of the tangentially moving boundary on the flow at rest in the far field. Using the Hilbert expansion method, for well-prepared initial data we construct the Boltzmann solution around the Rayleigh profile without initial singularity over any finite time interval.

</details>


### [14] [A note on lifespan estimates for higher-order parabolic equations](https://arxiv.org/abs/2512.04428)
*Nurdaulet N. Tobakhanov,Berikbol T. Torebek*

Main category: math.AP

TL;DR: The paper establishes precise lifespan estimates for solutions to higher-order semilinear parabolic equations, deriving both upper and lower bounds that refine previous results.


<details>
  <summary>Details</summary>
Motivation: Previous works by Caristi-Mitidieri and Sun only provided upper bounds for lifespan estimates under slowly decaying initial data assumptions. The authors aim to obtain both upper and lower bounds with sharper initial data conditions (L¹∩L∞), providing a more complete understanding of solution lifespan behavior.

Method: Combines the test function method with semigroup estimates to derive lifespan bounds. The approach allows handling of higher-order parabolic equations and works with L¹∩L∞ initial data rather than slowly decaying assumptions.

Result: Obtains precise asymptotic behavior of lifespan T_ε: for 1<p<p_Fuj, T_ε ≃ ε^[-(1/(p-1)-n/(2m))⁻¹]; for p=p_Fuj, T_ε ≃ exp(ε^[-(p-1)]). These are both upper and lower bounds, refining previous results that only gave upper bounds.

Conclusion: The paper successfully establishes sharp lifespan estimates for higher-order semilinear parabolic equations with L¹∩L∞ initial data, extending and improving upon earlier works by providing both upper and lower bounds and relaxing initial data assumptions.

Abstract: We investigate the lifespan of solutions to the higher-order semilinear parabolic equation $$u_t+(-Δ)^m u=|u|^p, \quad x \in \mathbb{R}^n, t>0 $$ with initial data. We focus on the precise asymptotic behavior of the lifespan of nontrivial solutions. By combining the test function method and semigroup estimates, we derive both upper and lower bounds for the lifespan of solutions $$T_{\varepsilon} \simeq \left\{\begin{array}{l}\varepsilon^{-\left(\frac{1}{p-1}-\frac{n}{2m}\right)^{-1}}, \,\, 1<p<p_{\text {Fuj}}, \\ \exp\left(\varepsilon^{-(p-1)}\right), \,\, p=p_{\text {Fuj}},\end{array}\right.$$ where $p_{Fuj}=1+\frac{2m}{n}$ is the critical exponent of Fujita. These estimates refine and extend the earlier results of Caristi-Mitidieri [J. Math. Anal. Appl., 279:2 (2003), 710-722] and Sun [Electron. J. Differential Equations, 17 (2010)], who obtained only upper bounds under slowly decaying initial data assumptions. In our setting, the above condition on the initial data is replaced by the assumption $L^1\cap L^\infty$, which sharpens the results of the aforementioned works.

</details>


### [15] [Irreversibility condition and stability of equilibria in the inverse-deformation approach to fracture](https://arxiv.org/abs/2512.04479)
*Arnav Gupta*

Main category: math.AP

TL;DR: The paper derives an irreversibility condition for fracture using inverse-deformation approach and thermodynamics, showing that crack location changes violate the second law, and proves local stability conditions for broken equilibria.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous thermodynamic foundation for fracture irreversibility using the inverse-deformation approach, addressing limitations in previous formulations and ensuring consistency with the second law of thermodynamics.

Method: Uses the inverse-deformation approach with second law of thermodynamics to derive irreversibility condition, incorporates inequality constraints for nonnegative inverse strain, analyzes brittle failure in an elastic bar, and proves necessary/sufficient conditions for local stability with numerical implementation.

Result: Derived irreversibility condition showing crack location changes produce negative entropy production (violating second law), proved local stability conditions incorporating constraints, and numerically verified all broken equilibria from previous work are locally stable.

Conclusion: The inequality constraint and irreversibility condition limit admissible variations, ensuring thermodynamic consistency; all previously found broken equilibria satisfy the derived local stability conditions, validating the approach.

Abstract: We derive the irreversibility condition in fracture for the inverse-deformation approach using the second law of thermodynamics. We consider the problem of brittle failure in an elastic bar previously solved in (Rosakis et al 2021). Despite the presence of a non-zero interfacial/surface energy, the third derivative of the inverse-deformation map is discontinuous at the crack faces. This is due to the presence of the inequality constraint ensuring the inverse strain is nonnegative and the orientation of matter is preserved. A change in the material location of a crack results in negative entropy production, violating the second law. Consequently, such changes are disallowed giving the irreversibility condition. The inequality constraint and the irreversibility condition limit the space of admissible variations. We prove necessary and sufficient conditions for local stability that incorporate these restrictions. Their numerical implementation shows that all broken equilibria found in (Rosakis et al 2021) are locally stable.

</details>


### [16] [Parabolic problems whose Fujita critical exponent is not given by scaling](https://arxiv.org/abs/2512.04506)
*Ahmad Z. Fino,Berikbol T. Torebek*

Main category: math.AP

TL;DR: The paper studies a fractional heat equation with Riesz potential nonlinearity, identifies the Fujita-type critical exponent p_Fuj = 1 + (β+α)/(n-α), and proves global existence for p > p_Fuj and finite-time blow-up for p ≤ p_Fuj.


<details>
  <summary>Details</summary>
Motivation: To investigate the global behavior of solutions to fractional heat equations with nonlocal nonlinearities involving Riesz potentials, addressing a hypothesis proposed by Mitidieri and Pohozaev about such equations.

Method: Uses nonlinear capacity method adapted to the problem structure for blow-up proofs, and fixed-point argument combined with Hardy-Littlewood-Sobolev inequality for global existence results.

Result: Establishes the Fujita-type critical exponent p_Fuj(n,β,α) = 1 + (β+α)/(n-α), characterizes solution behavior: global existence for small initial data when p > p_Fuj, finite-time blow-up when p ≤ p_Fuj.

Conclusion: The critical Fujita exponent emerges unconventionally, not from usual scaling arguments, confirming Mitidieri-Pohozaev's hypothesis and extending their results to more general convolution operators.

Abstract: This paper investigates the (fractional) heat equation with a nonlocal nonlinearity involving a Riesz potential: \begin{equation*} u_{t}+(-Δ)^{\fracβ{2}} u= I_α(|u|^{p}),\qquad x\in \mathbb{R}^n,\,\,\,t>0, \end{equation*} where $α\in(0,n)$, $β\in(0,2]$, $n\geq1$, $p>1.$ We introduce the Fujita-type critical exponent $p_{\mathrm{Fuj}}(n,β,α)=1+(β+α)/(n-α)$, which characterizes the global behavior of solutions: global existence for small initial data when $p>p_{\mathrm{Fuj}}(n,β,α),$ and finite-time blow-up when $p\leq p_{\mathrm{Fuj}}(n,β,α)$.
  It is remarkable that the critical Fujita exponent is not determined by the usual scaling argument that yields $p_{sc}=1+(β+α)/n$, but instead arises in an unconventional manner, similar to the results of Cazenave et al. [Nonlinear Analysis, 68 (2008), 862-874] for the heat equation with a nonlocal nonlinearity of the form $\int_0^t(t-s)^{-γ}|u(s)|^{p-1}u(s)ds,\,0\leq γ<1.$
  The result on global existence for $p>p_{\mathrm{Fuj}}(n,2,α),$ provides a positive answer to the hypothesis proposed by Mitidieri and Pohozaev in [Proc. Steklov Inst. Math., 248 (2005) 164-185]. We further establish global nonexistence results for the above heat equation, where the Riesz potential term $I_α(|u|^{p})$ is replaced by a more general convolution operator $(\mathcal{K}\ast |u|^p),\,\mathcal{K}\in L^1_{loc}$, thereby extending the Mitidieri-Pohozaev's results established in the aforementioned work.
  Proofs of the blow-up results are obtained using a nonlinear capacity method specifically adapted to the structure of the problem, while global existence is established via a fixed-point argument combined with the Hardy-Littlewood-Sobolev inequality.

</details>


### [17] [Sharp stability on the second Robin eigenvalue with negative boundary parameters](https://arxiv.org/abs/2512.04584)
*Zhijie Chen,Zhen Song,Wenming Zou*

Main category: math.AP

TL;DR: Quantitative refinement of isoperimetric inequality for second Robin eigenvalue with negative boundary parameters, proving stability estimate and sharp exponent for Fraenkel asymmetry.


<details>
  <summary>Details</summary>
Motivation: To refine and quantify the isoperimetric type inequality for the second Robin eigenvalue with negative boundary parameters established by Freitas and Laugesen, providing stability estimates when boundary parameter is near zero.

Method: Proving quantitative refinement using mathematical analysis techniques, constructing a family of nearly spherical domains to demonstrate sharpness of the exponent for Fraenkel asymmetry in the inequality.

Result: Established a quantitative stability estimate for the second Robin eigenvalue inequality when boundary parameter is not too far from 0, and proved the exponent for Fraenkel asymmetry is sharp.

Conclusion: The paper provides a refined quantitative version of the isoperimetric inequality for Robin eigenvalues with negative parameters, with optimal exponent for domain asymmetry measurement.

Abstract: In this paper, we prove a quantitative refinement of the isoperimetric type inequality for the second Robin eigenvalue with negative boundary parameters established by Freitas and Laugesen [Amer.J.Math.143 (2021), no.3, 969-994].Such new stability estimate is proved when the boundary parameter is not too far from 0.By constructing a suitable family of nearly spherical domains, we prove that the exponent for the Fraenkel asymmetry in this quantitative type inequality is sharp.

</details>


### [18] [Critical concave-convex problems in Carnot group](https://arxiv.org/abs/2512.04640)
*Mattia Galeotti,Eugenio Vecchi*

Main category: math.AP

TL;DR: Existence of two positive solutions for Dirichlet problem with concave-convex and critical nonlinearity in Carnot groups using variational Perron method.


<details>
  <summary>Details</summary>
Motivation: Extend the famous Ambrosetti-Brezis-Cerami result on existence of two positive solutions to the setting of Carnot groups, which are non-Euclidean spaces with sub-Riemannian geometry.

Method: Variational Perron method combined with estimates of minimizers of the relevant Sobolev inequality. Careful treatment needed due to lack of boundary regularity in Carnot groups.

Result: Proves existence of two positive solutions for the Dirichlet problem with concave-convex and critical nonlinearity in Carnot groups.

Conclusion: Successfully extends the Ambrosetti-Brezis-Cerami result to Carnot groups, overcoming challenges from lack of boundary regularity through careful variational analysis.

Abstract: We consider a model Dirichlet problem with concave-convex and critical nonlinearity settled in Carnot groups. Our aim is to prove the existence of two positve solutions in the spirit of a famous result by Ambrosetti, Brezis and Cerami. To this aim we use a variational Perron method combined with proper estimates of a family of functions which are minimizers of the relevant Sobolev inequality. Due to the lack of boundary regularity, we also have to be careful while proving that the first solution found is a local minimizer in the proper topology.

</details>


### [19] [Infinity of solutions to initial-boundary value problems for linear constant-coefficient evolution PDEs on semi-infinite intervals](https://arxiv.org/abs/2512.04670)
*Andreas Chatziafratis,Spyridon Kamvissis*

Main category: math.AP

TL;DR: Algorithmic procedure for constructing non-uniqueness counter-examples for linear evolution PDEs with constant coefficients in quarter-plane domains, using Fokas unified transform method.


<details>
  <summary>Details</summary>
Motivation: To address uniqueness questions for classical solutions to initial-boundary-value problems of linear evolution PDEs by developing systematic methods to construct counter-examples that demonstrate non-uniqueness.

Method: Uses Fokas unified transform method to derive closed-form integral representations, then analyzes regularity and asymptotic properties near the boundary to construct counter-examples. Demonstrated on heat equation and linear KdV equation with Dirichlet data.

Result: Developed algorithmic procedure for constructing non-uniqueness counter-examples applicable to wide class of linear evolution PDEs of any order with constant coefficients. Also presented new uniqueness theorems for heat equation and linear KdV equation.

Conclusion: The method provides systematic approach to study uniqueness questions for linear evolution PDEs in quarter-plane domains, with applications to specific equations yielding both counter-examples and new uniqueness theorems.

Abstract: In this short communication, we announce an algorithmic procedure for constructing non-uniqueness counter-examples of classical solutions to initial-boundary-value problems for a wide class of linear evolution partial differential equations, of any order and with constant coefficients, formulated in a quarter-plane. Our approach relies on analysis of regularity and asymptotic properties, near the boundary of the spatio-temporal domain, of closed-form integral-representation formulae derived via complex-analytic techniques and rigorous implementation of the modern PDE technique known as Fokas unified transform method. In order to elucidate the novel idea and demonstrate the proposed technique in a self-contained fashion, we explicitly present its application to two concrete examples, namely the heat equation and the linear KdV equation with Dirichlet data. New uniqueness theorems for these two models are also presented herein.

</details>


### [20] [On a fuzzy Landau Equation: Part III. The grazing collision limit](https://arxiv.org/abs/2512.04713)
*Manh Hong Duong,Boris Golubkov,Zihui He*

Main category: math.AP

TL;DR: The paper studies the grazing limit from fuzzy Boltzmann equations to fuzzy Landau equation using GENERIC structure and variational formulations.


<details>
  <summary>Details</summary>
Motivation: To establish the mathematical connection between fuzzy Boltzmann equations (with delocalized collisions) and fuzzy Landau equation through the grazing limit, using the GENERIC framework to understand the structural relationship between these kinetic equations.

Method: Using variational formulations corresponding to the GENERIC structure, showing convergence from non-quadratic dual dissipation pairs for fuzzy Boltzmann equations to quadratic dissipation pairs for fuzzy Landau equation in the grazing limit.

Result: Successfully demonstrates the grazing limit from fuzzy Boltzmann equations to fuzzy Landau equation through convergence of variational formulations, establishing the structural relationship between the two models.

Conclusion: The GENERIC framework provides a rigorous mathematical approach to connect fuzzy Boltzmann and Landau equations through grazing limits, showing how non-quadratic dissipation structures converge to quadratic ones, which has implications for understanding kinetic theory with delocalized collisions.

Abstract: In this paper, we study the grazing limit from the non-cutoff fuzzy Boltzmann equations to the fuzzy Landau equation, where particles interact through delocalised collisions. We show the grazing limit through variational formulations that correspond to the GENERIC (General Equations for Non-Equilibrium Reversible-Irreversible Coupling) structure of the respective equations. We show that the variational formulation associated with a non-quadratic dual dissipation pair for the fuzzy Boltzmann equations converges to a variational formulation of the fuzzy Landau equation corresponding to a quadratic dissipation pair.

</details>


### [21] [Optimal cost for the null controllability of the Stokes system with controls having $n-1$ components and applications](https://arxiv.org/abs/2512.04721)
*Felipe W. Chaves-Silva,Diego A. Souza,Marcos G. Ferreira-Silva*

Main category: math.AP

TL;DR: The paper shows that removing one component from Stokes system controls doesn't increase null controllability cost, maintaining O(e^{C/T}) scaling.


<details>
  <summary>Details</summary>
Motivation: To understand how the cost of null controllability for Stokes systems is affected when controls have fewer components (n-1 instead of n), and whether this reduction increases control effort.

Method: Develops a novel spectral estimate for low frequencies of the Stokes operator involving only n-1 components, then uses this estimate to analyze controllability cost scaling.

Result: The cost of null controllability with n-1 component controls remains O(e^{C/T}), same as with n components, showing no cost penalty from missing one control component.

Conclusion: Reducing control components from n to n-1 in Stokes systems doesn't affect null controllability cost scaling, with applications demonstrating this robustness.

Abstract: In this work, we investigate the optimal cost of null controllability for the $n$-dimensional Stokes system when the control acts on $n-1$ scalar components. We establish a novel spectral estimate for low frequencies of the Stokes operator, involving solely $n-1$ components, and use it to show that the cost of controllability with controls having $n-1$ components remains of the same order in time as in the case of controls with $n$ components, namely $O(e^{C/T})$, i.e. the cost of null controllability is not affected by the absence of one component of the control. We also give several applications of our results.

</details>


### [22] [Generalized Navier-Stokes equations, associated with the Dolbeault complex](https://arxiv.org/abs/2512.04777)
*Shlapunov Alexander,Polkovnikov Alexander*

Main category: math.AP

TL;DR: The paper studies a Cauchy problem in complex space for a system resembling Navier-Stokes equations but using Cauchy-Riemann operators instead of standard gradient/divergence operators, proving existence theorems in Bochner-Sobolev spaces.


<details>
  <summary>Details</summary>
Motivation: To extend fluid dynamics analysis to complex spaces by replacing standard differential operators with complex-analytic counterparts (Cauchy-Riemann operators), creating a mathematical framework for studying Navier-Stokes-like systems in complex domains.

Method: Uses the multidimensional Cauchy-Riemann operator $\overline{\partial}$, its adjoint $\overline{\partial}^{*}$, and the Dolbeault complex to generate a system structurally similar to Navier-Stokes. Analyzes the Cauchy problem in $\mathbb{C}^{n}\times[0,T]$ using specially constructed Bochner-Sobolev spaces.

Result: Proves existence of weak solutions and an open mapping theorem on the scale of Bochner-Sobolev spaces. Also obtains a criterion for existence of "strong" solutions in these spaces.

Conclusion: The structural similarity between the complex-analytic system and classical Navier-Stokes equations allows for analogous existence theorems, establishing a mathematical foundation for studying fluid-like systems in complex domains with complex differential operators.

Abstract: We consider the Cauchy problem in the band $\mathbb{C}^{n}\times[0, T], n>1,T>0$, for a system of nonlinear differential equations structurally similar to the classical Navier-Stokes equations for an incompressible fluid. The main difference of this system is that it is generated not by the standard gradient operators $\nabla$, divergence div and rotor rot, but by the multidimensional Cauchy-Riemann operator $\overline{\partial}$ in $\mathbb{C}^{n}$, its formally adjoint operator $\overline{\partial}^{*}$ and the compatibility complex for $\overline{\partial}$, which is usually called the Dolbeault complex. The similarity of the structure makes it possible to prove for this problem the theorem of the existence of weak solutions and the open mapping theorem on the scale of specially constructed Bochner-Sobolev spaces. In addition, a criterion for the existence of a ``strong'' solution in these spaces is obtained.

</details>


### [23] [Homogenized limits of Stokes flow and advective transport in thin perforated domains](https://arxiv.org/abs/2512.04782)
*Markus Gahn,Vlad Revnic*

Main category: math.AP

TL;DR: Homogenization and dimension reduction of Stokes flow and transport problems in thin ε-periodic perforated layers, deriving effective Darcy-type flow and diffusion-advection transport models as ε→0.


<details>
  <summary>Details</summary>
Motivation: To derive rigorous effective models for flow and transport in thin perforated layers where thickness (ε^α) is large compared to porosity, as layer thickness tends to zero (ε→0).

Method: Uses two-scale convergence adapted to microscopic geometry with uniform a priori estimates. Constructs Bogovskii-operator for thin perforated domains to control fluid pressure, and establishes strong two-scale convergence for transport solutions.

Result: Obtains Darcy-type law for Stokes flow with Darcy-velocity depending only on vertical pressure derivative. Effective transport equation is diffusion-advection-type with homogenized coefficients, advection only in vertical direction. Diffusion behavior depends on scaling: slow vertical diffusion yields only vertical effective diffusion, high horizontal diffusion yields effective diffusion in all directions.

Conclusion: Successfully derives rigorous homogenized models for flow and transport in thin perforated layers, providing effective Darcy-type flow and diffusion-advection transport equations with precise characterization of diffusion behavior based on scaling parameters.

Abstract: We deal with the rigorous homogenization and dimension reduction of flow and transport problems posed in thin $\varepsilon$-periodic perforated layers with thickness of order $\varepsilon^α$ with $α\in (0,1)$ and therefore the thickness of the layer is large compared its porosity. The aim is the derivation of effective models for $\varepsilon\to 0 $, when the thickness of the layer tends to zero. For the flow problem we consider incompressible Stokes equations with a pressure boundary condition on the top/bottom of the layer, and the transport problem is given by reaction-diffusion-advection problem with advective flow governed from the fluid velocity from the Stokes model and different scalings for the diffusion coefficient modelling low and fast diffusion in the horizontal direction. In the limit, a Darcy-type law is obtained for the Stokes flow with Darcy-velocity depending only on the derivative of the Darcy-pressure in the vertical direction. The effective equation for the transport problem is again of diffusion-advection-type including homogenized coefficients, and with advective flow given by the Darcy-velocity and only taking place in the vertical direction. In the case of slow diffusion in the vertical direction, effective diffusion only takes place in the vertical direction, where in the case of high diffusion in horizontal direction, we obtain effective diffusion in all space directions. To pass to the limit we use the method of two-scale convergence adapted to our microscopic geometry, which is based on uniform a priori estimates. Critical parts in the derivation of the macro-models are the control of the fluid pressure, for which we construct a Bogovskii-operator for thin perforated domains, as well as the strong two-scale convergence for the microscopic solution of the transport equation, necessary to pass to the limit in the advective term.

</details>


### [24] [The initial-to-final-state inverse problem with unbounded potentials and Strichartz estimates](https://arxiv.org/abs/2512.04796)
*Pedro Caro,Alberto Ruiz*

Main category: math.AP

TL;DR: The paper extends uniqueness results for the initial-to-final-state inverse quantum Hamiltonian problem from bounded to unbounded potentials, using Strichartz estimates and showing limitations of Bourgain spaces for this problem.


<details>
  <summary>Details</summary>
Motivation: To establish a theoretical framework explaining the viability of data-driven prediction in quantum mechanics by solving the inverse problem of determining quantum Hamiltonians from final states given initial states.

Method: Extends previous work on Hamiltonians of form -Δ+V with electric potential V(t,x) by proving a family of suitable Strichartz estimates (including Keel-Tao endpoint) for unbounded potentials, and provides a counterexample showing limitations of Bourgain spaces for this problem.

Result: Uniqueness holds for the inverse problem even with unbounded potentials, and Bourgain spaces cannot capture the mixed-norm Lebesgue spaces needed for Strichartz inequalities in this context.

Conclusion: The paper successfully extends uniqueness results to unbounded potentials in the initial-to-final-state inverse problem, while demonstrating that Bourgain spaces are insufficient for addressing this problem due to their inability to capture necessary mixed-norm spaces.

Abstract: The initial-to-final-state inverse problem consists in determining a quantum Hamiltonian assuming the knowledge of the state of the system at some fixed time, for every initial state. We formulated this problem to establish a theoretical framework that would explain the viability of data-driven prediction in quantum mechanics. In a previous work, we analysed this inverse problem for Hamiltonians of the form $-Δ+ V$ with an electric potential $V = V({\rm t}, {\rm x})$, and we showed that uniqueness holds whenever the potentials are bounded and decay super-exponentially at infinity. In this paper, we extend this result for unbounded potentials. One of the key steps consists in proving a family of suitable Strichartz estimates -- including the corresponding endpoint of Keel and Tao.
  In the context of the inverse Calderón problem this family of inequalities corresponds to the Carleman inequality proved by Kenig, Ruiz and Sogge. Haberman showed that this inequality can be also retrieved as an embedding of a suitable Bourgain space. The corresponding Bourgain space in our context do not capture the mixed-norm Lebesgue spaces of Strichartz inequalities. In this paper, we give a counterexample that justifies this fact, and shows the limitations of Bourgain spaces to address the initial-to-final-state inverse problem.

</details>


### [25] [Time-periodic solutions to an energy balance model coupled with an active fluid under arbitrary large forces](https://arxiv.org/abs/2512.04800)
*Gianmarco Del Sarto,Matthias Hieber,Filippo Palma,Tarek Zöchling*

Main category: math.AP

TL;DR: A 2D Sellers-type energy balance model coupled to 3D primitive equations via dynamic boundary conditions admits at least one strong time-periodic solution when forced by time-periodic forcing, even for arbitrarily large forcing.


<details>
  <summary>Details</summary>
Motivation: To establish existence of time-periodic solutions in coupled climate models without requiring small forcing conditions, allowing for more realistic and potentially large periodic forcings.

Method: Couples a two-dimensional Sellers-type energy balance model with three-dimensional primitive equations through dynamic boundary conditions, then proves existence of strong time-periodic solutions.

Result: The coupled system admits at least one strong time-periodic solution when the forcing term is time-periodic, without requiring any smallness condition on the forcing amplitude.

Conclusion: Time-periodic solutions exist for coupled climate models under periodic forcing, even for arbitrarily large forcing amplitudes, demonstrating robust periodic behavior in these complex systems.

Abstract: This article concerns time-periodic solutions to a two-dimensional Sellers-type energy balance model coupled to the three-dimensional primitive equations via a dynamic boundary condition. It is shown that the underlying equations admit at least one strong time-periodic solution, provided the forcing term is time-periodic. The forcing term does not need to satisfy a smallness condition and is allowed to be arbitrarily large.

</details>


### [26] [Spectral Theory of Krein-Feller Type Operators and Applications in Stochastic Fractional Elliptic and Parabolic Equations](https://arxiv.org/abs/2512.04826)
*Kelvin J. R. Almeida-Sousa,Alexandre B. Simas*

Main category: math.AP

TL;DR: The paper develops a generalized Taylor expansion for functions in C∞_{W,V}(𝕋) space, characterizes eigenvectors of Krein-Feller operators, analyzes eigenvalue asymptotics, proves nuclearity of the space, and applies results to differential equations.


<details>
  <summary>Details</summary>
Motivation: To establish series expansions for highly discontinuous functions in C∞_{W,V}(𝕋) space, which serves as the natural regularity space for solutions of eigenvalue problems involving Krein-Feller operators with singular measures W and V.

Method: Nonstandard analytical methods (since classical approaches fail in this singular setting), characterization of eigenvectors using generalized trigonometric functions, asymptotic analysis of eigenvalues, and nuclear space theory.

Result: Proved existence of generalized Taylor expansions for C∞_{W,V}(𝕋) functions, obtained asymptotic lower bounds for eigenvalues, sharp upper bounds for convergence exponents, proved C∞_{W,V}(𝕋) is nuclear, and established existence results for various differential equations.

Conclusion: The work provides a comprehensive framework for analyzing singular Krein-Feller operators, establishes fundamental properties of the associated function spaces, and yields applications to stochastic and deterministic differential equations in nuclear spaces.

Abstract: It has been shown that the space $C^{\infty}_{W,V}(\mathbb{T})$, introduced in Simas and Sousa (Potential Analysis, 2025), is the natural regularity space for solutions of the eigenvalue problem $Δ_{W,V} u = λu$ on the torus $\mathbb{T}$, where $Δ_{W,V} = \frac{d^{+}}{dV}\frac{d^{-}}{dW}$ is the Krein Feller operator in the case where $W$ and $V$ are strictly increasing and right continuous (respectively left continuous), possibly with dense sets of discontinuities. In this work we provide conditions ensuring that every function in $C^{\infty}_{W,V}(\mathbb{T})$, which may be highly discontinuous, admits a series expansion that generalizes the classical Taylor expansion. A central feature of our approach is that all proofs are nonstandard, since classical analytical and spectral arguments cannot be adapted to this singular setting. Using these methods we characterize the eigenvectors of $Δ_{W,V}$ in terms of generalized trigonometric functions and obtain an asymptotic lower bound for the associated eigenvalues. We also derive a sharp upper bound for the convergence exponent of these eigenvalues, and as a consequence we prove that $C^{\infty}_{W,V}(\mathbb{T})$ is a nuclear space. Further consequences include results on the asymptotic behavior of eigenvalues of compact operators and improvements in traceability. As a final application we establish existence results for generalized fractional stochastic and deterministic differential equations, as well as for parabolic stochastic partial differential equations acting on nuclear spaces.

</details>


### [27] [On hyperbolic approximations for a class of dispersive and diffusive-dispersive equations](https://arxiv.org/abs/2512.04882)
*Rahul Barthwal,Firas Dhaouadi,Christian Rohde*

Main category: math.AP

TL;DR: Novel hyperbolic approximations for dispersive and diffusive-dispersive equations with nonlinear fluxes, enabling standard numerical methods and proving convergence to original solutions.


<details>
  <summary>Details</summary>
Motivation: To develop approximate systems for dispersive and diffusive-dispersive equations that allow application of standard numerical simulation methods from hyperbolic balance laws, overcoming challenges in simulating these complex equations.

Method: Construct first-order strictly hyperbolic approximations for dispersive equations with unique symmetrizer for local well-posedness. For diffusive-dispersive equations, use viscoelastic damped systems compatible with hyperbolic approximations. Apply relative entropy framework to prove convergence.

Result: Achieved local well-posedness for dispersive approximations, global well-posedness for hyperbolic-parabolic approximations. Proved convergence of approximate solutions to original equations. Demonstrated applicability for strong nonlinear effects including oscillating and shock-layer-forming behavior.

Conclusion: The novel approximate systems successfully bridge dispersive/diffusive-dispersive equations with hyperbolic balance law methods, enabling standard numerical simulation while maintaining theoretical convergence guarantees even for complex nonlinear behaviors.

Abstract: We introduce novel approximate systems for dispersive and diffusive-dispersive equations with nonlinear fluxes. For purely dispersive equations, we construct a first-order, strictly hyperbolic approximation. Local well-posedness of smooth solutions is achieved by constructing a unique symmetrizer that applies to arbitrary smooth fluxes. Under stronger conditions on the fluxes, we provide a strictly convex entropy for the hyperbolic system that corresponds to the energy of the underlying dispersive equation. To approximate diffusive-dispersive equations, we rely on a viscoelastic damped system that is compatible with the found entropy for the hyperbolic approximation of the dispersive evolution. For the resulting hyperbolic-parabolic approximation, we provide a global well-posedness result. Using the relative entropy framework \cite{dafermos2005hyperbolic}, we prove that the solutions of the approximate systems converge to solutions of the original equations. The structure of the new approximate systems allows to apply standard numerical simulation methods from the field of hyperbolic balance laws. We confirm the convergence of our approximations even beyond the validity range of our theoretical findings on set of test cases covering different target equations. We show the applicability of the approach for strong nonlinear effects leading to oscillating or shock-layer-forming behavior.

</details>


### [28] [Quantitative rigidity of the Wasserstein contraction under convolution](https://arxiv.org/abs/2512.04928)
*Max Fathi,Michael Goldman,Daniel Tsodyks*

Main category: math.AP

TL;DR: The paper studies contraction properties of p-Wasserstein distances under convolution in Euclidean spaces, connecting this to uniform convexity of the Kantorovich functional and extending uniform convexity results to the p=1 case.


<details>
  <summary>Details</summary>
Motivation: To investigate contraction properties of p-Wasserstein distances with respect to convolution in Euclidean spaces, both qualitatively and quantitatively. The research is motivated by connecting this question to the problem of uniform convexity of the Kantorovich functional, which has seen recent progress for p=2 and partially for p>1.

Method: The paper connects the contraction properties of p-Wasserstein distances under convolution to the question of uniform convexity of the Kantorovich functional. Building on recent progress in uniform convexity results (mostly for p=2 and partially for p>1), the authors extend these uniform convexity results to the case p=1.

Result: The paper extends uniform convexity results for the Kantorovich functional to the p=1 case, which is of independent interest. This extension contributes to understanding contraction properties of Wasserstein distances under convolution in Euclidean spaces.

Conclusion: The research successfully connects contraction properties of p-Wasserstein distances with convolution to uniform convexity of the Kantorovich functional, and extends existing uniform convexity results to include the important p=1 case, advancing the theoretical understanding of Wasserstein distance properties in Euclidean spaces.

Abstract: The aim of this paper is to investigate the contraction properties of $p$-Wasserstein distances with respect to convolution in Euclidean spaces both qualitatively and quantitatively. We connect this question to the question of uniform convexity of the Kantorovich functional on which there was substantial recent progress (mostly for $p=2$ and partially for $p>1$). Motivated by this connection we extend these uniform convexity results to the case $p=1$, which is of independent interest.

</details>


### [29] [Existence and a priori bounds for fully nonlinear PDEs with a harmonic map-like structure](https://arxiv.org/abs/2512.04961)
*Gabrielle Nornberg,Ricardo Ziegele*

Main category: math.AP

TL;DR: Study of fully nonlinear elliptic equations with harmonic map-like structure, establishing existence, multiplicity, and qualitative results under small coefficient conditions.


<details>
  <summary>Details</summary>
Motivation: To analyze a new class of fully nonlinear uniformly elliptic equations with harmonic map-like structure, which extends classical elliptic theory to more complex nonlinear operators involving gradient-dependent terms and Pucci extremal operators.

Method: Use Pucci extremal operators as model case, impose smallness regime on coefficients (b, c, f), establish Aleksandrov-Bakelman-Pucci estimates and comparison principles, derive a priori bounds for Dirichlet problems in noncoercive cases.

Result: Obtained existence results under small coefficient conditions, established classical results (ABP estimate, comparison principle), derived a priori bounds for Dirichlet problems, and proved multiplicity results and qualitative behavior that are new even for Laplacian operators.

Conclusion: The paper successfully develops a comprehensive theory for fully nonlinear elliptic equations with harmonic map-like structure, providing existence, multiplicity, and qualitative results that extend classical elliptic theory to this more complex class of operators.

Abstract: In this paper, we study a new class of fully nonlinear uniformly elliptic equations with a so-called harmonic map-like structure, whose model case is given by \begin{equation*} \mathcal{M}^{\pm}_{λ,Λ}(D^2u) \pm b(x) |Du| \pm β(u)\langle M(x) Du,Du \rangle \pm c(x) u = f(x)\; \textrm{ in } Ω, \end{equation*} where $Ω\subset \mathbb{R}^n$ is a bounded $C^{1,1}$ domain, $\mathcal{M}^{\pm}$ are the Pucci extremal operators, $β(s) = s^k$ for some $k \in \mathbb{N} $ odd, $b \in L^{q}_{+}(Ω)$, $c,f \in L^p(Ω)$, and $n \leq p \leq q$, $q>n$.
  We obtain existence results under a smallness regime on the coefficients, along with some classical results such as the Aleksandrov--Bakelman--Pucci estimate and the comparison principle, as well as a priori bounds for the respective Dirichlet problem in the noncoercive case. We also establish multiplicity results and qualitative behavior, which seem to be new in the case of the Laplacian operator.

</details>


### [30] [Fractured Poroelastic Media in the Limit of Vanishing Aperture](https://arxiv.org/abs/2512.04978)
*Maximilian Hörl,Kundan Kumar,Christian Rohde*

Main category: math.AP

TL;DR: Derivation of limit models for poroelastic media with thin fractures as fracture width approaches zero, identifying different scaling regimes for hydraulic conductivity and elasticity.


<details>
  <summary>Details</summary>
Motivation: To understand how thin heterogeneities (fractures) in poroelastic media behave in the limit as their width becomes infinitesimally small, which is important for modeling fractured porous media in geomechanics and reservoir engineering.

Method: Using a priori estimates and rigorous mathematical analysis to derive limit models as the fracture width-to-length ratio ε → 0, with material parameters scaling as powers of ε.

Result: Identified five distinct regimes for hydraulic conductivity scaling and two regimes for elasticity scaling, resulting in either discrete fracture models or two-scale limit problems dominated by normal flow/deformation.

Conclusion: The scaling of fracture material parameters relative to fracture width determines whether the limit behavior reduces to discrete fracture models or requires more complex two-scale descriptions, providing a systematic framework for modeling thin heterogeneities in poroelastic media.

Abstract: We consider a poroelastic medium with a thin heterogeneity, also referred to as a fracture. Fluid flow and mechanical deformation inside both bulk and fracture are governed by the quasi-static Biot equations. The fracture's material parameters, such as hydraulic conductivity and elasticity, are assumed to scale with powers of the width-to-length ratio $\varepsilon$ of the fracture. Based on a priori estimates, we rigorously derive limit models as $\varepsilon \rightarrow 0$ and identify different limit regimes. We obtain five regimes for the hydraulic conductivity and two for the elasticity. While many cases yield discrete fracture models, others result in two-scale limit problems dominated by normal flow or deformation.

</details>


### [31] [Geophysical intensity problems: the axisymmetric case](https://arxiv.org/abs/2512.05010)
*Ralf Kaiser*

Main category: math.AP

TL;DR: The paper proves existence of infinitely many axisymmetric harmonic vector fields outside a sphere with prescribed intensity at the surface, characterized by specific zero patterns and decay orders.


<details>
  <summary>Details</summary>
Motivation: To solve the intensity problem for gravitational/magnetic fields - a nonlinear boundary value problem where general solvability is not established, focusing on axisymmetric harmonic fields outside a sphere.

Method: Study axisymmetric harmonic fields outside unit sphere, solve nonlinear elliptic equation with discontinuous and singular coefficients using natural boundary conditions (vs fixed boundary conditions in previous work), requiring new solution techniques and sharper estimates.

Result: Proves existence of infinitely many solutions for axisymmetric Hölder continuous intensity functions, characterized by: fixed decay order δ, meridional plane M, symmetric unit circle S¹, and 2N symmetric points zn outside S¹, yielding unique (up to sign) harmonic field vanishing only at those points.

Conclusion: The intensity problem has infinitely many axisymmetric solutions with specific geometric constraints, advancing understanding of nonlinear boundary value problems for harmonic vector fields with natural boundary conditions.

Abstract: Considering the earth or any other celestial body the main sources of the gravitational as well as of the magnetic field lie inside the body. Above the surface both fields are in good approximation harmonic vector fields determined by their values at the body's surface or any other surface enclosing the body. The intensity problem seeks to determine harmonic vector fields vanishing at infinity and with prescribed intensity of the field at the surface. This problem constitutes a nonlinear boundary value problem, whose general solvability is not yet established. In this paper {\em axisymmetric} harmonic fields ${\bf H}$ outside the unit sphere $S^2$ are studied and, given an axisymmetric Hölder continuous intensity function $I\neq 0$ on $S^2$, the existence of infinitely many solutions of the intensity problem is proved. These solutions can more precisely be characterized as follows: fix a number $\de \in \nat\setminus \{1 \}$ and a meridional plane $M$ through the symmetry axis $S\!A$, and in $M$ a unit circle $S^1$ (symmetric with respect to $S\!A$) and, furthermore, $2\, N$, $N \in \nat_0$, points $z_n \in M$ (symmetric with respect to $S\!A$, avoiding $S\!A$, and outside $S^1$), then the existence of an (up to a sign) unique harmonic field ${\bf H}$ is established that vanishes at (the axisymmetric circles piercing $M$ at) $z_n$ and nowhere else, that has intensity $I$ at $S^2$ and (exact) decay order $\de$ at infinity. The proof is based on the solution of a nonlinear elliptic equation with discontinuous coefficients, which are, moreover, singular at the symmetry axis. Its combination with fixed boundary conditions was the basis of a recent treatment of the ``geomagnetic direction problem'' \cite{KR22}. Here we have instead natural boundary conditions, which provide less information, and which require, therefore, in part new solution techniques and sharper estimates.

</details>


### [32] [A Nehari manifold method for nonvariational problems](https://arxiv.org/abs/2512.05055)
*Radu Precup,Andrei Stan*

Main category: math.AP

TL;DR: Extends Nehari manifold method from variational to nonvariational fixed point equations via radial energy functional, yielding multiple solutions localized in conical annular sets with two applications.


<details>
  <summary>Details</summary>
Motivation: To generalize the powerful Nehari manifold method beyond variational problems to nonvariational fixed point equations, expanding its applicability to a broader class of problems.

Method: Constructs a radial energy functional that generalizes the standard variational case, applies the Nehari manifold method to fixed point equations, and obtains solutions localized in conical annular sets.

Result: Successfully extends the Nehari manifold method to nonvariational framework, proves existence of multiple solutions, and demonstrates results with two representative applications.

Conclusion: The Nehari manifold method can be effectively extended to nonvariational fixed point equations, providing a new approach for obtaining multiple solutions in broader mathematical contexts.

Abstract: The aim of this paper is to extend the Nehari manifold method from the variational setting to the nonvariational framework of fixed point equations. This is achieved by constructing a radial energy functional that generalizes the standard one from the variational case. Furthermore, the solutions obtained through our method are localized in conical annular sets, which leads to the existence of multiple solutions. The abstract results are illustrated by two representative applications.

</details>


### [33] [Mean curvature flow near a peanut solution](https://arxiv.org/abs/2512.05077)
*Sigurd Angenent,Panagiota Daskalopoulos,Natasa Sesum*

Main category: math.AP

TL;DR: The paper analyzes the instability of "peanut solutions" in mean curvature flow, showing they can be perturbed to develop either spherical or nondegenerate neckpinch singularities, with convergence to Ancient oval solutions for certain sequences.


<details>
  <summary>Details</summary>
Motivation: Peanut solutions are closed mean curvature flow solutions that extinct to a point without becoming convex, developing degenerate neckpinch singularities. While their existence was proven, their stability properties remained unclear. The paper aims to characterize the instability of these special solutions.

Method: The authors analyze perturbations around peanut solutions and study the resulting mean curvature flows. They examine how small perturbations lead to different singularity types and investigate convergence properties of sequences of solutions approaching the peanut solution.

Result: Peanut solutions are highly unstable: any small neighborhood contains perturbations that lead to spherical singularities and perturbations that lead to nondegenerate neckpinch singularities. Additionally, appropriately rescaled subsequences of solutions converging to the peanut solution and developing spherical singularities converge to the Ancient oval solution.

Conclusion: The peanut solutions, while mathematically interesting as degenerate neckpinch examples, are unstable critical points in the space of mean curvature flows. Their instability manifests through sensitivity to perturbations that can lead to fundamentally different singularity types, with connections to the Ancient oval solution.

Abstract: It was shown by Angenent, Altschuler and Giga, and by Angenent and Velazquez that there exist closed mean curvature flow solutions that extinct to a point in finite time, without ever becoming convex prior to their extinction. These solutions develop a degenerate neckpinch singularity, meaning that the tangent flow at a singularity is a round cylinder, but at the same time for each of these solutions there exists a sequence of points in space and time, so that the pointed blow up limit around this sequence is the Bowl soliton. These solutions are called peanut solutions and they were first conjectured to exist by Richard Hamilton, while the existence of those solutions was shown by Angenent, Altschuler and Giga. In this paper we show that this type of solutions are highly unstable, in the sense that in every small neighborhood of any such peanut solution we can find a perturbation so that the mean curvature flow starting at that perturbation develops spherical singularity, and at the same time we can find a perturbation so that the mean curvature flow starting at that perturbation develops a nondegenerate neckpinch singularity. We also show that appropriately rescaled subsequence of any sequence of solutions whose initial data converge to the peanut solution, and all of which develop spherical singularities, converges to the Ancient oval solution.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [34] [Persistent-variable thermal compositional simulation of multiphase flow with phase separation in porous media](https://arxiv.org/abs/2512.04205)
*Veljko Lipovac,Omar Duran,Eirik Keilegavlen,Inga Berre*

Main category: physics.comp-ph

TL;DR: A persistent-variable formulation for thermal compositional multiphase flow using enthalpy, with embedded local thermodynamic solver for phase transitions, reducing global nonlinear iterations by up to 23%.


<details>
  <summary>Details</summary>
Motivation: Thermal compositional multiphase flow with phase transitions involves complex nonlinear interactions that are challenging to simulate, especially for high-enthalpy systems and narrow-boiling phenomena. Existing approaches often require phase stability tests and lack seamless integration of equilibrium calculations.

Method: Developed a persistent-variable formulation using enthalpy for energy balance and local equilibrium. Derived equilibrium conditions from thermodynamically consistent minimization. Embedded a local solver for thermodynamic subproblems within a global Newton solver, exploiting locality for parallelization and modularity for both isothermal and isenthalpic conditions.

Result: Demonstrated capability to simulate complex high-enthalpy systems including narrow-boiling phenomena. The embedded local solver reduced global nonlinear iterations by up to 23% with increased usage. Local iterations controlled by tolerance, with no significant impact on global iterations for tolerances as high as 1e-3.

Conclusion: The persistent-variable approach using enthalpy and modular embedded local solver advances equilibrium calculations in multiphase flow simulations, providing continuous mathematical description without phase stability tests, suitable for challenging non-isothermal and high-enthalpy applications.

Abstract: Thermal compositional multiphase flow in porous media with phase transitions involves complex nonlinear interactions among flow, transport, and phase equilibrium. This paper presents a persistent-variable formulation for thermal compositional flow using enthalpy to formulate the energy balance and the local equilibrium problem. Equilibrium conditions are derived from a thermodynamically consistent minimization problem using a persistent set of variables, allowing for seamless integration of equilibrium calculations into a fully coupled flow and transport model. This formulation does not require phase stability tests and provides a continuous and full mathematical description of the multiphysics system, suitable for challenging non-isothermal scenarios. To tackle the nonlinearities arising from phase transitions, we embed a local solver for the thermodynamic subproblem within a global Newton solver for the fully implicit system. The local solver exploits the locality of the subproblem for parallelization and leverages the modularity of the persistent-variable formulation for both isothermal and isenthalpic equilibrium conditions locally. We demonstrate the capability of our approach to simulate complex high-enthalpy systems, including narrow-boiling phenomena. The impact of the embedded local solver is analyzed through numerical experiments, demonstrating a reduction in global nonlinear iterations of up to 23 \% with increased use of the local solver. The number of local iterations is controlled with a local solver tolerance and no significant impact on the global iteration number was observed for local residual tolerances as high as $1e-3$. The persistent-variable approach using enthalpy and the modularity of the embedded local solver advance the usage of equilibrium calculations in multiphase flow simulations and are suitable for high-enthalpy applications.

</details>


### [35] [Bessel Functions and Analysis of Circular Waveguides](https://arxiv.org/abs/2512.04348)
*Jaime Mora-Paz,Leszek Demkowicz,Christina G. Taylor,Jacob Grosek,Stefan Henneking*

Main category: physics.comp-ph

TL;DR: The paper develops an analytical-numerical method to solve Bessel eigenvalue problems for circularly coiled optical slab waveguides, providing accurate loss factors and benchmark solutions for verifying waveguide models.


<details>
  <summary>Details</summary>
Motivation: To study circularly coiled optical slab waveguides (also applicable to acoustical waveguides) and provide accurate solutions for verifying model implementations and analyzing waveguide stability.

Method: Uses change of variables and classical Frobenius method to compute Bessel functions of complex order and argument, combined with perfectly matched layer technique to solve Bessel eigenvalue problems for three-layer optical slab waveguides.

Result: Delivers accurate loss factors for eigensolutions to the three-layer optical slab waveguide problem, providing benchmark solutions for model verification and enabling numerical verification of the Glazman criterion.

Conclusion: The developed method provides reliable solutions for circular waveguide problems, serving as benchmarks for model validation and supporting well-posedness and stability analysis through verification of the Glazman criterion.

Abstract: The paper is devoted to the study of circularly coiled optical slab waveguides, which is also applicable to acoustical waveguides. We use a change of variables and the classical Frobenius method to compute Bessel functions of complex order and complex argument, and combine it with a perfectly matched layer technique to solve the relevant Bessel eigenvalue problem and deliver accurate loss factors for eigensolutions to the three-layer optical slab waveguide problem. The solutions provide a benchmark for verifying model implementations of this problem and allow for a numerical verification of the Glazman criterion that provides a foundation for the well-posedness and stability analysis of homogeneous circular waveguides with impedance boundary conditions.

</details>


### [36] [GPU-Portable Real-Space Density Functional Theory Implementation on Unified-Memory Architectures](https://arxiv.org/abs/2512.04447)
*Atsushi M. Ito*

Main category: physics.comp-ph

TL;DR: QUMASUN is a GPU-portable DFT code achieving 2-2.8× speedup on AMD MI300A and NVIDIA GH200 GPUs over 256-core Xeon CPUs using a lightweight C++ lambda layer for cross-platform execution.


<details>
  <summary>Details</summary>
Motivation: To develop a portable real-space DFT implementation that can efficiently run on diverse GPU architectures (AMD MI300A, NVIDIA GH200) and CPUs, leveraging modern memory features like unified memory and coherent memory interconnects to simplify GPU porting for plasma-fusion simulations.

Method: Implemented a lightweight C++ lambda-based abstraction layer enabling CPU, CUDA, and HIP execution without OpenMP/OpenACC directives. Benchmarking on Intel Xeon 6980P CPUs, AMD MI300A GPUs (unified memory), and NVIDIA GH200 GPUs (coherent memory). Focused on compute-bound kernels: FFT, GEMM, and eigenvalue solvers.

Result: MI300A achieved 2.0-2.8× speedup, GH200 achieved 2.3-2.4× speedup over 256-core Xeon node for diamond (216 atoms) and tungsten (128 atoms) systems. Compute-bound kernels showed substantial acceleration on both GPUs, demonstrating effective GPU portability.

Conclusion: The GPU-portable approach using C++ lambda abstraction successfully enables cross-platform DFT execution with significant performance gains. This methodology can benefit broader plasma-fusion simulation codes beyond DFT applications.

Abstract: We present a GPU-portable implementation of a real-space density functional theory (DFT) code ``QUMASUN'' and benchmark it on the new Plasma Simulator featuring Intel Xeon 6980P CPUs, and AMD MI300A GPUs. Additional tests were performed on an NVIDIA GH200 GPU. In particular MI300A supports unified memory and GH200 supports coherent memory interconnect, simplifying GPU porting. A lightweight C++ lambda-based layer enables CPU, CUDA, and HIP execution without OpenMP/OpenACC preprocessor directives. For diamond (216 atoms) and tungsten (128 atoms) systems, MI300A and GH200 achieve 2.0-2.8 $\times$ and 2.3-2.4 $\times$ speedups over a 256-core Xeon node. The compute-bound kernels, which are fast Fourier transforms (FFT), dense matrix-matrix multiplications (GEMM) and eigenvalue solver, show substantial acceleration on both GPUs, indicating that the present GPU-portable approach can benefit a wide range of plasma-fusion simulation codes beyond DFT.

</details>


### [37] [On the Construction of High-Order and Exact Pressure Equilibrium Schemes for Arbitrary Equations of State](https://arxiv.org/abs/2512.04450)
*Christopher DeGrendele,Nguyen Ly,Francois Cadieux,Michael Barad,Dongwook Lee,Jared Duensing*

Main category: physics.comp-ph

TL;DR: Fully conservative methods for multi-component Euler equations that prevent spurious pressure oscillations while handling arbitrary equations of state.


<details>
  <summary>Details</summary>
Motivation: Existing conservative discretizations of Euler equations for real-fluid equations of state suffer from spurious pressure oscillations due to nonlinear thermodynamics, requiring non-conservative updates or equation-specific designs.

Method: Two methods: 1) fully conservative pressure-equilibrium preserving method, and 2) high-order fully conservative approximate pressure-equilibrium preserving method. Both handle arbitrary equations of state and species without non-conservative updates or overspecified equations.

Result: Demonstrated on inviscid smooth interface advection problems with ideal-gas, stiffened-gas, and van der Waals equations of state, showing orders of magnitude reduction in spurious pressure oscillations compared to existing schemes.

Conclusion: The proposed methods provide general, fully conservative solutions for multi-component Euler equations that eliminate spurious pressure oscillations while maintaining generality across equations of state.

Abstract: Typical fully conservative discretizations of the Euler compressible single or multi-component fluid equations governed by a real-fluid equation of state exhibit spurious pressure oscillations due to the nonlinearity of the thermodynamic relation between pressure, density, and internal energy. A fully conservative, pressure-equilibrium preserving method and a high-order, fully conservative, approximate pressure-equilibrium preserving method are presented. Both methods are general and can handle an arbitrary equation of state and arbitrary number of species. Unlike existing approaches to discretize the multi-component Euler equations, we do not introduce non conservative updates, overspecified equations, or design for a specific equation of state. The proposed methods are demonstrated on inviscid smooth interface advection problems governed by three equations of state: ideal-gas, stiffened-gas, and van der Waals where we show orders of magnitude reductions in spurious pressure oscillations compared to existing schemes.

</details>


### [38] [Stochastic Density Functional Theory Through the Lens of Multilevel Monte Carlo Method](https://arxiv.org/abs/2512.04860)
*Xue Quan,Huajie Chen*

Main category: physics.comp-ph

TL;DR: sDFT with plane-wave discretization using multilevel Monte Carlo for variance reduction, making computational cost independent of discretization size or temperature.


<details>
  <summary>Details</summary>
Motivation: Stochastic DFT (sDFT) offers advantages over standard Kohn-Sham DFT for large-scale electronic structure calculations by avoiding expensive matrix diagonalization, but needs variance reduction for efficiency.

Method: Combine sDFT with plane-wave discretization and apply multilevel Monte Carlo (MLMC) methods for variance reduction. Decompose density matrix evaluation into levels by increasing plane-wave cutoffs or Chebyshev polynomial orders.

Result: The MLMC approach makes computational cost independent of discretization size or temperature. Rigorous statistical error analysis and numerical experiments on material systems demonstrate algorithm efficiency.

Conclusion: MLMC provides effective variance reduction for sDFT with plane-wave discretization, enabling efficient large-scale electronic structure calculations with controllable statistical errors.

Abstract: The stochastic density functional theory (sDFT) has exhibited advantages over the standard Kohn-Sham DFT method and has become an attractive approach for large-scale electronic structure calculations. The sDFT method avoids the expensive matrix diagonalization by introducing a set of random orbitals and approximating the density matrix via Chebyshev expansion of a matrix-valued function. In this work, we study the sDFT with a plane-wave discretization, and discuss variance reduction algorithms in the framework of multilevel Monte Carlo (MLMC) methods. In particular, we show that the density matrix evaluation in sDFT can be decomposed into many levels by increasing the plane-wave cutoffs or the Chebyshev polynomial orders. This decomposition renders the computational cost independent of the discretization size or temperature. To demonstrate the efficiency of the algorithm, we provide rigorous analysis of the statistical errors and present numerical experiments on some material systems.

</details>


### [39] [PENCO: A Physics-Energy-Numerical-Consistent Operator for 3D Phase Field Modeling](https://arxiv.org/abs/2512.04863)
*Mostafa Bamdad,Mohammad Sadegh Eshaghi,Cosmin Anitescu,Navid Valizadeh,Timon Rabczuk*

Main category: physics.comp-ph

TL;DR: PENCO is a hybrid neural operator framework that integrates physical laws and numerical structures to solve spatio-temporal PDEs with improved accuracy, stability, and data efficiency compared to existing neural operators.


<details>
  <summary>Details</summary>
Motivation: Existing neural operators for solving spatio-temporal PDEs accumulate temporal errors, struggle with long-horizon generalization, and require large training datasets, limiting their practical application for phase-field models and interfacial dynamics.

Method: PENCO combines physics-based constraints with data-driven learning: enhanced L² Gauss-Lobatto collocation residual for dynamics, Fourier-space numerical consistency for semi-implicit discretizations, energy-dissipation constraint for thermodynamics, plus spectral anchoring and teacher-consistency mechanisms.

Result: PENCO demonstrates superior accuracy, stability, and data efficiency in 3D phase-field benchmarks (phase ordering, crystallization, epitaxial growth, pattern formation) compared to state-of-the-art neural operators like MHNO and FNO-4D.

Conclusion: The hybrid physics-numerics-data approach enables PENCO to preserve governing physics while mitigating long-term error growth, offering a robust solution for accurate and efficient simulation of complex interfacial dynamics.

Abstract: Accurate and efficient solutions of spatio-temporal partial differential equations (PDEs), such as phase-field models, are fundamental for understanding interfacial dynamics and microstructural evolution in materials science and fluid mechanics. Neural Operators (NOs) have recently emerged as powerful data-driven alternatives to traditional solvers; however, existing architectures often accumulate temporal errors, struggle to generalize in long-horizon simulations, and require large training datasets. To overcome these limitations, we propose PENCO (Physics-Energy-Numerical-Consistent Operator), a hybrid operator-learning framework that integrates physical laws and numerical structure within a data-driven architecture. The formulation introduces an enhanced L^2 Gauss-Lobatto collocation residual around the temporal midpoint that robustly enforces the governing dynamics and significantly improves accuracy, a Fourier-space numerical consistency term that captures the balanced behavior of semi-implicit discretizations, and an energy-dissipation constraint that ensures thermodynamic consistency. Additional low-frequency spectral anchoring and teacher-consistency mechanisms further stabilize learning and suppress long-term error growth. This hybrid design enables PENCO to preserve governing physics while mitigating long-term error growth. Through extensive three-dimensional phase-field benchmarks covering phase ordering, crystallization, epitaxial growth, and complex pattern formation, PENCO demonstrates superior accuracy, stability, and data efficiency compared to state-of-the-art neural operators, including Multi-Head Neural Operator (MHNO) and Fourier Neural Operator (FNO-4D), while maintaining physically consistent evolution. The associated dataset and implementation are available at github.com/MBamdad/PENCO.

</details>


### [40] [VNS Tokamak OpenMC-Serpent Validation for Medical Isotope Studies](https://arxiv.org/abs/2512.04873)
*Christopher Ehrich,Christian Bachmann,Pavel Pereslavtsev,Christian Reiter*

Main category: physics.comp-ph

TL;DR: Comparison of neutronics simulations between Serpent and OpenMC codes for the Volumetric Neutron Source tokamak, showing good agreement for most neutron responses but photon flux discrepancies depending on tracking method.


<details>
  <summary>Details</summary>
Motivation: The VNS tokamak is proposed for testing fusion reactor components and radioisotope production, requiring accurate neutronics simulations to validate component designs and compare different simulation codes.

Method: Modeled VNS geometry in Serpent and OpenMC codes, performed analog neutron-photon coupled simulations, compared vacuum vessel flux maps and blanket region spectra/reaction rates, tested different tracking methods (hybrid vs delta tracking).

Result: Neutron flux and (n,T) reactions showed excellent agreement; (n,2n) had good agreement; photon flux had regional discrepancies (20% difference with hybrid tracking vs <1% with delta tracking). Serpent was faster for coupled simulations but slower for neutron-only simulations on HPC.

Conclusion: The study demonstrates successful cross-code validation for VNS neutronics, identifies tracking method impact on photon flux accuracy, and shows Serpent's computational advantages for coupled simulations while highlighting VNS's potential for radioisotope production.

Abstract: The Volumetric Neutron Source (VNS) tokamak is a proposed fusion reactor for testing and qualification of reactor components for future use in a fusion power facility, and has potential use for radioisotope production. The VNS geometry is modeled in the Serpent and OpenMC neutronics codes. Analog neutron-photon coupled simulations are carried out to compare the model's vacuum vessel and blanket components across codes. In the vacuum vessel, neutron and photon flux maps are calculated, while in the blanket region, neutron and photon spectra, (n,T), and (n,2n) reaction rates are calculated and compared between models. The detector response comparisons found the following: neutron flux and (n,T) reactions achieved excellent agreement, the (n,2n) detector response had good agreement, and photon flux had regional discrepancies depending on Serpent tracking used. Hybrid tracking lead to a relative difference of about 20% in the outboard side blanket, where as employment of delta tracking resulted in less than 1% relative difference. On an HPC cluster, Serpent was found to have shorter computation time than OpenMC in neutron photon coupled simulations using both hybrid tracking and delta tracking, but longer in neutron only simulations. An exemplary radioisotope production case is presented for the demonstration of additional VNS capabilities.

</details>


### [41] [LEDDS: Portable LBM-DEM simulations on GPUs](https://arxiv.org/abs/2512.04997)
*Raphael Maggio-Aprile,Maxime Rambosson,Christophe Coreixas,Jonas Latt*

Main category: physics.comp-ph

TL;DR: LEDDS is an open-source GPU framework that performs Lattice Boltzmann-Discrete Element Method (LBM-DEM) simulations using only algorithmic primitives like map, sort, reduce, achieving performance comparable to hand-tuned CUDA while maintaining portability and readability.


<details>
  <summary>Details</summary>
Motivation: To extend the algorithmic programming paradigm (using parallel primitives rather than handcrafted GPU kernels) to complex computational physics problems like granular flows and fluid-particle interactions, enabling portable, high-performance multiphysics simulations without sacrificing code clarity.

Method: Developed LEDDS framework that expresses entire LBM-DEM workflow (neighbor search, collision detection, fluid-particle coupling) as sequences of portable parallel primitives, primarily using C++ Standard Library algorithms with selective Thrust primitives for performance optimization.

Result: LEDDS achieves performance comparable to hand-tuned CUDA solvers despite high abstraction level, validated through comprehensive benchmarks including sphere/ellipsoid collisions, wall friction tests, particle settling, Jeffery's orbits, and particle-laden shear flows.

Conclusion: High-performance LBM-DEM coupling can be achieved using algorithmic primitives without sacrificing generality or readability, establishing LEDDS as a blueprint for portable multiphysics frameworks that work across modern GPU systems and future accelerators.

Abstract: Algorithmic formulations of GPU programs provide a high-level alternative to device-specific code by expressing computations as compositions of well-defined parallel primitives (e.g., map, sort, reduce), rather than through handcrafted GPU kernels. In this work, we demonstrate that this paradigm can be extended to complex and challenging problems in computational physics: the simulation of granular flows and fluid-particle interactions.
  LEDDS, our open-source framework, performs fully coupled Lattice Boltzmann -- Discrete Element Method (LBM-DEM) simulations using only algorithmic primitives, and runs efficiently on single-GPU platforms. The entire workflow, including neighbor search, collision detection, and fluid-particle coupling, is expressed as a sequence of portable primitives. While the current implementation illustrates these principles primarily through algorithms from the C++ Standard Library, with selective use of Thrust primitives for performance, the underlying concept is compatible with any HPC environment offering a rich set of parallel algorithms and is therefore applicable across a wide range of modern GPU systems and future accelerators.
  LEDDS is validated through benchmarks spanning both DEM and LBM-DEM configurations, including sphere and ellipsoid collisions, wall friction tests, single-particle settling, Jeffery's orbits, and particle-laden shear flows. Despite its high level of abstraction, LEDDS achieves performances comparable to those of hand-tuned CUDA solvers, while maintaining portability and code clarity. These results show that high-performance LBM-DEM coupling can be achieved without sacrificing generality or readability, establishing LEDDS as a blueprint for portable multiphysics frameworks based on algorithmic primitives.

</details>


### [42] [Engineered Inclined Energy Landscapes Enabling Free Flow of Magnetic Microstructures for Artificial Neuron Applications](https://arxiv.org/abs/2512.05020)
*Anmol Sharma,Ranjeet Kumar Brajpuriya,Vivek K. Malik,Vishakha Kaushik,Sachin Pathak*

Main category: physics.comp-ph

TL;DR: Researchers developed an energy-efficient spintronic neuromorphic device using magnetic microstructures with a sawtooth energy landscape to emulate integrate-and-fire neuron functions, achieving 23.66 fJ per spike.


<details>
  <summary>Details</summary>
Motivation: Spintronic neuromorphic computing offers advantages like nanoscale size, stability, and low energy, but practical integration is limited by complex fabrication, stochastic effects, and reliability issues that hinder real-world applications.

Method: Engineered a sawtooth-type energy landscape for magnetic microstructures to enable free flow and emulate biological integrate-and-fire neuron functions through an experimentally feasible, low-energy design.

Result: Successfully achieved free flow of magnetic microstructures and emulated integrate-and-fire neuron functions with extremely low energy consumption of 23.66 fJ per spike.

Conclusion: The proposed design provides an experimentally reliable and energy-efficient approach for controlling magnetic microstructure dynamics, paving the way for skyrmion-based neuromorphic computing devices.

Abstract: Spintronic-based brain-inspired neuromorphic computing has recently attracted significant attention due to the exceptional properties of magnetic microstructures, including nanoscale dimensions, high stability, and low energy consumption. Despite these advantages, the practical integration of such microstructures into functional devices remains challenging. Fabrication processes are often complex and prone to stochastic effects, such as unwanted pinning and thermal-induced instabilities, which limit device reliability and scalability. Addressing these challenges is crucial for advancing spintronic neuromorphic architectures toward real-world applications. Thus, to reduce these effects we have proposed a design which is experimentally feasible and require less energy as compared to existing one. By engineering the system anisotropy into a sawtooth-type energy landscape, we have achieved free flow of these microstructures and successfully emulated integrate and fire (IF) function of biological neuron. Thus, proposed design presents an experimentally reliable and energy efficient external stimuli approach for tailoring magnetic microstructures dynamic behaviours, resulting in low energy consumption of 23.66 fJ per spike paving the way for the development of skyrmion-based futuristic neuromorphic computing device applications.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [43] [A simple procedure for generating a Kappa distribution in PIC simulation](https://arxiv.org/abs/2512.04272)
*Seiji Zenitani*

Main category: physics.plasm-ph

TL;DR: A rejection-sampling method using Pareto distribution as envelope for generating Kappa distributions in PIC simulations with high acceptance efficiency.


<details>
  <summary>Details</summary>
Motivation: Need efficient method to generate Kappa distributions for kinetic modeling of plasma processes in space using particle-in-cell simulations.

Method: Rejection-sampling procedure using Pareto distribution as envelope distribution, requiring only uniform variates.

Result: Acceptance efficiency of approximately 0.73-0.8, making it practical for PIC simulations.

Conclusion: Proposed method provides efficient way to generate Kappa distributions for space plasma modeling in PIC simulations.

Abstract: For kinetic modeling of plasma processes in space, a rejection-sampling procedure for generating a Kappa distribution in particle-in-cell (PIC) simulation is proposed. A Pareto distribution is employed as an envelope distribution. The procedure only requires uniform variates, and its acceptance efficiency is $\approx 0.73$--$0.8$.

</details>


### [44] [Numerical model for pellet rocket acceleration in PELOTON](https://arxiv.org/abs/2512.04484)
*J. Corbett,R. Samulyak,F. J. Artola,S. Jachmich,M. Kong,E. Nardon*

Main category: physics.plasm-ph

TL;DR: Developed and validated a rocket acceleration model for pellets in fusion devices using PELOTON code, showing consistency with JET experiments and demonstrating effects of neon doping, fragment configurations, and plasma gradients.


<details>
  <summary>Details</summary>
Motivation: To understand and model the rocket acceleration mechanism of pellets in thermonuclear fusion devices, particularly for shattered pellet injection (SPI) applications in tokamaks like JET and future ITER operations.

Method: Developed a 3D Lagrangian particle pellet code (PELOTON) with rocket acceleration model accounting for grad-B drift, non-uniform charging, plasma gradients, and implemented deuterium and deuterium-neon mixture models. Validated using JET SPI experiments and compared with JOREK and INDEX simulations.

Result: PELOTON simulations of rocket acceleration and deuterium fragment trajectories are consistent with JET experimental measurements. Deuterium-neon pellets (0.5% neon) showed smaller trajectory deviations than pure deuterium. Demonstrated effects of fragment spatial configurations, cloud overlap, and plasma state gradients on rocket acceleration.

Conclusion: The developed model successfully captures pellet rocket acceleration physics in fusion devices. Future work will focus on applying the model to ITER plasmas and developing scaling laws for rocket acceleration in SPI scenarios.

Abstract: A direct numerical simulation model for the rocket acceleration of pellets in thermonuclear fusion devices has been developed for PELOTON, a 3D Lagrangian particle pellet code [R. Samulyak et al, Nuclear Fusion 61 (4), 046007 (2021)], and validated using shattered pellet injection (SPI) experiments in JET. The pellet rocket acceleration is driven by grad-B drift of the ablation cloud that creates asymmetry and non-uniform heating of the cloud. The model accounts for non-uniform charging of the ablation cloud by hot plasma electrons as well as local plasma gradients. The increased pressure on the high-field-side compared to the low-field-side leads to pellet (fragment) rocket acceleration. Pure deuterium and deuterium-neon mixture models have been implemented. The background plasma states have been obtained by using a new plasma cooling model for PELOTON. The cooling model distributes the ablated material within the corresponding flux volumes and accounts for ionization and other energy losses, Ohmic heating by toroidal currents, and the energy exchange between ions and electrons. Plasma profiles predicted by PELOTON cooling model have been compared with JOREK and INDEX simulations. PELOTON simulations of rocket acceleration and the corresponding trajectories of deuterium fragments are consistent with experimentally measured trajectories in JET. We show that composite deuterium-neon pellets containing 0.5% of neon experienced smaller deviation of their trajectories compared to the pure deuterium case. We simulate various spatial configurations of pellet fragments and demonstrate the cloud overlap impact on rocket acceleration. Additionally, we demonstrate the effect of plasma state gradients on the rocket acceleration. Future work will focus on the rocket acceleration of SPI in projected ITER plasmas and the development of the corresponding scaling law for the rocket acceleration.

</details>


### [45] [Generation of ultrahigh field by micro-bubble implosion](https://arxiv.org/abs/2512.04715)
*M. Murakami,A. Arefiev,M. A. Zosa*

Main category: physics.plasm-ph

TL;DR: Proposes bubble implosions using micro-bubbles and ultraintense lasers to accelerate protons beyond 100 MeV via spherical Coulomb implosion, creating nano-pulsars that emit energetic proton flashes.


<details>
  <summary>Details</summary>
Motivation: To break the 100-MeV barrier for proton acceleration, which has important implications for fundamental physics research and practical applications including inertial confinement fusion and tumor therapy.

Method: Combines micro-bubbles with ultraintense laser pulses (10^20-10^22 W/cm²) to generate ultrahigh fields. Bubble wall protons undergo volumetric acceleration toward the center via spherically symmetric Coulomb force, creating extreme density conditions comparable to white dwarf interiors, producing unprecedentedly high electric fields.

Result: 3D particle simulations confirm robust Coulomb-imploded bubbles that behave as nano-pulsars with repeated implosions and explosions to emit protons. The concept should be experimentally verifiable with current technologies.

Conclusion: Bubble implosions represent a novel approach to achieve proton acceleration beyond 100 MeV, offering potential for both fundamental physics exploration and practical applications in fusion and medical therapy.

Abstract: Breaking the 100-MeV barrier for proton acceleration will help elucidate fundamental physics and advance practical applications from inertial confinement fusion to tumour therapy. Herein we propose a novel concept of bubble implosions. A bubble implosion combines micro-bubbles and ultraintense laser pulses of 10^20-10^22W/cm^2 to generate ultrahigh fields and relativistic protons. The bubble wall protons undergo volumetric acceleration toward the centre due to the spherically symmetric Coulomb force and the innermost protons accumulate at the centre with a density comparable to the interior of a white dwarf. Then an unprecedentedly high electric field is formed, which produces an energetic proton flash. Three-dimensional particle simulations confirm the robustness of Coulomb-imploded bubbles, which behave as nano-pulsars with repeated implosions and explosions to emit protons. Current technologies should be sufficient to experimentally verify concept of bubble implosions.

</details>


### [46] [Optimization of laser illumination configuration for directly driven inertial confinement fusion](https://arxiv.org/abs/2512.04722)
*Masakatsu Murakami,Daiki Nishi*

Main category: physics.plasm-ph

TL;DR: Researchers developed optimal laser configurations for inertial confinement fusion using charged particle models on a sphere, finding new configurations (M48, M60) with superior illumination uniformity and proposing a polar direct-drive scheme.


<details>
  <summary>Details</summary>
Motivation: To achieve high illumination uniformity in directly driven inertial confinement fusion targets, which is crucial for efficient compression and ignition in fusion experiments.

Method: Used theoretical models considering number of laser beams, system imperfections, and beam patterns; employed a self-organizing system of charged particles on a sphere to find optimal configurations; proposed a polar direct-drive scheme with off-center laser axes.

Result: Discovered new configurations M48 and M60 that show substantially higher illumination uniformity than existing direct drive systems; developed a polar direct-drive scheme applicable to laser configurations designed for indirect drive fusion.

Conclusion: The charged particle model on a sphere provides an effective method for optimizing laser configurations, yielding superior illumination uniformity and enabling new polar direct-drive approaches for inertial confinement fusion.

Abstract: Optimum laser configurations are presented to achieve high illumination uniformity with directly driven inertial confinement fusion targets. Assuming axisymmetric absorption pattern of individual laser beams, theoretical models are reviewed in terms of the number of laser beams, system imperfection, and laser beam patterns. Utilizing a self-organizing system of charged particles on a sphere, a simple numerical model is provided to give an optimal configuration for an arbitrary number of laser beams. As a result, such new configurations as M48 and M60 are found to show substantially higher illumination uniformity than any other existing direct drive systems. A new polar direct-drive scheme is proposed with the laser axes keeping off the target center, which can be applied to laser configurations designed for indirectly driven inertial fusion.

</details>


### [47] [Operator Formalism for Laser-Plasma Wakefield Acceleration](https://arxiv.org/abs/2512.04982)
*Mostafa Behtouei,Carlos Salgado Lopez,Giancarlo Gatti*

Main category: physics.plasm-ph

TL;DR: Operator-based framework for laser-plasma wakefield acceleration using Hilbert-space operators to describe coupled laser-plasma dynamics, with integration of neural operators for reduced-order modeling.


<details>
  <summary>Details</summary>
Motivation: To provide a compact, systematic mathematical framework for describing laser-plasma wakefield acceleration in capillary discharges, connecting LPWA with formal operator theory and enabling integration with AI methods for better modeling and optimization.

Method: Develops an operator-based formalism using four key operators: transverse modal operator (K̂), nonlinear plasma operator (N̂[Ψ]), plasma oscillation operator (Ω̂_p²), and ponderomotive source operator (α̂). The framework describes mode coupling, plasma oscillations, and nonlinear feedback, and integrates with neural operator methods for approximation.

Result: Establishes a direct connection between LPWA and Hilbert-space operator theory, identifies invariant subspaces in linear regime, shows how nonlinear interactions break invariances leading to mode mixing, and creates a hybrid physics-AI framework for efficient modeling.

Conclusion: The operator formalism provides a robust mathematical foundation for modeling, analysis, and optimization of high-intensity laser-plasma interactions, enabling predictive control and reduced-order modeling through integration with neural operators for next-generation accelerator experiments.

Abstract: In this paper, we develop an operator-based framework for laser--plasma wakefield acceleration (LPWA) in capillary discharges, providing a compact and systematic description of the coupled dynamics of laser fields and plasma response. The formalism employs key operators: the transverse modal operator $\hat{K}$, the nonlinear plasma operator $\hat{N}[Ψ]$, the plasma oscillation operator $\hatΩ_p^{\,2}$, and the ponderomotive source operator $\hatα$, which together describe mode coupling, plasma oscillations, and nonlinear feedback induced by the ponderomotive force. In the linear regime, the system is characterized by invariant subspaces associated with stable modal structures, while nonlinear interactions break these invariances, leading to mode mixing and complex dynamics. The approach establishes a direct connection between LPWA and Hilbert-space operator theory, including the invariant subspace, providing a formal mathematical interpretation of energy transfer and wakefield formation. Furthermore, the operator formalism integrates with neural operator methods, allowing efficient approximation of $\hat{N}$ and $\hatα$ for reduced-order modeling and predictive control. This hybrid physics--AI framework offers a robust foundation for modeling, analysis, and optimization of high-intensity laser--plasma interactions in next-generation accelerator experiments.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [48] [CNN on `Top': In Search of Scalable & Lightweight Image-based Jet Taggers](https://arxiv.org/abs/2512.05031)
*Rajneil Baruah,Subhadeep Mondal,Sunando Kumar Patra,Satyajit Roy*

Main category: hep-ph

TL;DR: Lightweight EfficientNet-based model for jet classification achieves competitive performance with lower computational cost compared to Transformers and GNNs.


<details>
  <summary>Details</summary>
Motivation: Transformer-based and standard Graph Neural Networks (GNNs) show strong performance in jet classification but require substantial computational power, creating a need for more efficient alternatives.

Method: Use a lightweight and scalable version of EfficientNet architecture combined with global features of the jet to create a computationally inexpensive model.

Result: The network achieves competitive performance for tagging top-quark jets among light-quark and gluon jets while being computationally inexpensive.

Conclusion: EfficientNet-based architecture provides a viable, computationally efficient alternative to Transformers and GNNs for jet classification tasks with competitive performance.

Abstract: While Transformer-based and standard Graph Neural Networks (GNNs) have proven to be the best performers in classifying different types of jets, they require substantial computational power. We explore the scope of using a lightweight and scalable version of the EfficientNet architecture, along with global features of the jet. The end product is computationally inexpensive but is capable of competitive performance. We showcase the efficacy of our network for tagging top-quark jets in a sea of other light-quark and gluon jets.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [49] [Mixing at the Batchelor Scale for White-In-Time Flows](https://arxiv.org/abs/2512.04297)
*Robin Chemnitz,Dennis Chemnitz*

Main category: math.PR

TL;DR: The paper proves lower bounds for exponential dissipation rates in advection-diffusion equations with white-in-time velocity fields, verifying the Batchelor scale conjecture for specific forced modes on 2D torus.


<details>
  <summary>Details</summary>
Motivation: To understand mixing properties in advection-diffusion systems and verify the Batchelor scale conjecture, which relates dissipation rates to velocity field properties as diffusivity approaches zero.

Method: Analyze solutions to advection-diffusion equation with white-in-time velocity field on 2D torus with four forced modes, study exponential dissipation rates as diffusivity parameter goes to zero, and extend analysis to three-dimensional velocity fields.

Result: Show that almost-sure exponential dissipation rate stays bounded from below as diffusivity goes to zero, complementing existing upper bounds to verify Batchelor scale conjecture, and characterize exponential mixing rate without diffusion.

Conclusion: The paper provides a concrete example where Batchelor scale conjecture can be verified, demonstrates bounded dissipation rates in the zero-diffusivity limit, and extends results to three-dimensional velocity fields with similar properties.

Abstract: We consider the mixing properties of solutions to the advection-diffusion equation of a white-in-time velocity field on the 2-dimensional torus with four forced modes. As the diffusivity parameter goes to zero, we show that the almost-sure exponential dissipation rate stays bounded from below. Together with the corresponding upper bound established by Gess and Yaroslavtsev, this constitutes an example of a velocity field for which the Batchelor scale conjecture can be verified. In addition, we characterize the exponential mixing rate without diffusion of this system. Our results are not restricted to two dimensions, and we construct a three-dimensional white-in-time velocity field with the same properties.

</details>


### [50] [Homogenizationof non-divergence form operators in i.i.d. random environments](https://arxiv.org/abs/2512.04410)
*Xiaoqin Guo,Timo Sprekeler,Hung V. Tran*

Main category: math.PR

TL;DR: Improved convergence rates for homogenization of Dirichlet problems in random walks on random environments in higher dimensions.


<details>
  <summary>Details</summary>
Motivation: Previous results gave O(R^{-1}) convergence rates for homogenization in random environments with finite range dependence, but the authors believe better rates are achievable in higher dimensions (d≥3).

Method: Study random walks in balanced, i.i.d. random environments in ℤ^d for d≥3, analyzing homogenization of Dirichlet problems associated with non-divergence form difference operators.

Result: Achieved improved convergence rates: O(R^{-3/2}) for d=3 and O(R^{-2}log R) for d≥4, surpassing the previously expected optimal rate of O(R^{-1}).

Conclusion: Better convergence rates are possible for homogenization in random environments in higher dimensions, with dimension-dependent improvements that go beyond the finite-range dependence expectation.

Abstract: We study random walks in a balanced, i.i.d. random environment in $\mathbb Z^d$ for $d\geq 3$. We establish improved convergence rates for the homogenization of the Dirichlet problem associated with the corresponding non-divergence form difference operators, surpassing the $O(R^{-1})$ rate, which is expected to be optimal for environments with a finite range of dependence. In particular, the improved rates are $O(R^{-3/2})$ when $d=3$, and $O(R^{-2}\log R)$ when $d\geq 4$.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [51] [Amortized Inference of Multi-Modal Posteriors using Likelihood-Weighted Normalizing Flows](https://arxiv.org/abs/2512.04954)
*Rajneil Baruah*

Main category: cs.LG

TL;DR: Amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling, with improved multi-modal reconstruction using Gaussian Mixture Model base distributions.


<details>
  <summary>Details</summary>
Motivation: Efficient inference of theoretical parameters in high-dimensional inverse problems without requiring posterior training samples, addressing limitations of standard unimodal base distributions in capturing disconnected support.

Method: Normalizing Flows trained with likelihood-weighted importance sampling, initialized with Gaussian Mixture Model base distributions that match the cardinality of target modes.

Result: Standard unimodal base distributions fail to capture disconnected support, creating spurious probability bridges between modes. Gaussian Mixture Model initialization significantly improves reconstruction fidelity as measured by distance and divergence metrics.

Conclusion: The topology of base distributions critically impacts modeled posteriors; using Gaussian Mixture Models that match target mode cardinality enables better capture of multi-modal posterior distributions.

Abstract: We present a novel technique for amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling. This approach allows for the efficient inference of theoretical parameters in high-dimensional inverse problems without the need for posterior training samples. We implement the method on multi-modal benchmark tasks in 2D and 3D to check for the efficacy. A critical observation of our study is the impact of the topology of the base distributions on the modelled posteriors. We find that standard unimodal base distributions fail to capture disconnected support, resulting in spurious probability bridges between modes. We demonstrate that initializing the flow with a Gaussian Mixture Model that matches the cardinality of the target modes significantly improves reconstruction fidelity, as measured by some distance and divergence metrics.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [52] [A hybrid Green-Kubo (hGK) framework for calculating viscosity from short MD simulations](https://arxiv.org/abs/2512.04546)
*Akash K. Meel,Santosh Mogurampelly*

Main category: cond-mat.soft

TL;DR: Hybrid Green-Kubo (hGK) framework partitions stress autocorrelation function into short-time ballistic (from MD) and long-time relaxation (analytical tail fitting) components, enabling efficient viscosity calculation with orders-of-magnitude computational savings.


<details>
  <summary>Details</summary>
Motivation: Traditional Green-Kubo viscosity calculations require extensive phase space sampling and suffer from poor convergence, making them computationally demanding for soft matter and polymer systems.

Method: Hybrid Green-Kubo framework partitions stress autocorrelation function into: (1) short-time ballistic component from short MD simulations, and (2) long-time relaxation tail using analytically motivated functions fitted to short trajectories.

Result: Excellent agreement with established results for SPC/E water; successful application to challenging electrolyte systems (EC-LiTFSI and PEO-LiTFSI) where traditional GK fails; computational savings of several orders of magnitude without compromising accuracy.

Conclusion: hGK provides conceptually simple, broadly applicable, and computationally efficient route for viscosity prediction in molecular liquids, polymer melts, and ionically conducting soft materials, with clear avenues for refinement.

Abstract: Viscosity calculation from equilibrium molecular dynamics (MD) simulations relies on the traditional Green-Kubo (GK) framework, which integrates the stress autocorrelation function (SACF) over time. While the formalism is exact in the linear response regime, the traditional approach often suffers from poor convergence and requires extensive phase space sampling, which is computationally demanding for soft matter and polymer systems. In this Letter, we introduce a hybrid Green-Kubo (hGK) framework that alleviates these limitations by partitioning the SACF into two physically meaningful regimes: (i) a short time ballistic component extracted directly from short MD simulations, and (ii) a long time relaxation tail represented using analytically motivated functions, $φ(τ)$, fitted only to short trajectories. This strategy bypasses the need for extensive sampling while preserving physical rigor. Benchmarking against SPC/E water confirms excellent agreement with established results, and we further demonstrate the efficacy of the method for challenging electrolyte systems (EC-LiTFSI and PEO-LiTFSI), for which the GK framework fails to converge. The computational savings are substantial, with reductions of several orders of magnitude in required sampling, achieved without compromising predictive accuracy. We also discuss the limitations of the hGK framework and outline clear avenues for refinement, including optimal tail selection and robust identification of relaxation regimes in noisy stress data. The hGK framework presented in this Letter provides a conceptually simple, broadly applicable, and computationally efficient route for viscosity prediction in molecular liquids, polymer melts, and ionically conducting soft materials.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [53] [Degrees of universality in wave turbulence](https://arxiv.org/abs/2512.04866)
*Jiasheng Liu,Vladimir Rosenhaus,Gregory Falkovich*

Main category: cond-mat.stat-mech

TL;DR: The paper studies how inverse turbulent cascades transition from weak to strong turbulence, finding that spectral nonlocality enhances nonlinearity and causes spin-wave turbulence to become strong even when local interactions are weak.


<details>
  <summary>Details</summary>
Motivation: To understand the universality and transition mechanisms in wave turbulence, particularly how inverse cascades evolve from weak to strong turbulence regimes, and to contrast different models (spin waves, NSE, MMT-like models).

Method: Analyzes turbulence of spin waves in ferromagnets, turbulent cascades in Nonlinear Schrödinger Equation (NSE), and MMT-like models in higher dimensions with multiplicative interaction vertices. Uses one-loop corrections and vertex renormalization techniques to study spectral nonlocality effects.

Result: Spectral nonlocality causes spin-wave turbulence to become strong far from pumping scales even when local wave interactions are weak. Vertex renormalization leads to pumping-scale dependence in spin-wave turbulence but not in NSE models. Strong spin-wave turbulence resembles focusing NSE turbulence with critical-balance states, and UV nonlocality causes large-scale turbulence levels to decrease with increased pumping.

Conclusion: Nonlocality enhances nonlinearity in wave turbulence, creating new types of universality in strong turbulence regimes. Spin-wave turbulence transitions to strong turbulence due to spectral nonlocality, exhibiting critical-balance behavior similar to focusing NSE, with pumping-independent states emerging at large scales.

Abstract: Turbulence of weakly interacting waves displays a great deal of universality: independence of the details of the interaction and of the pumping and dissipation scales. Here we study how inverse turbulent cascades (from small to large scales) transition from weak to strong. We find that while one-loop corrections can be dependent on excitation and dissipation scales, new types of universality appear in strong turbulence. We contrast turbulence of spin waves in ferromagnets with turbulent cascades in the Nonlinear Schrödinger Equation (NSE) and in an MMT-like model in higher dimensions having a multiplicative interaction vertex: vertex renormalization gives rise to dependence on the pumping (UV scale) in the former but not in the latter. As a result of this spectral nonlocality, spin-wave turbulence stops being weak if one is sufficiently far from the pumping scale, even when the interaction of waves with comparable wavenumbers is weak. We paraphrase this as: nonlocality enhances nonlinearity.
  We then describe strong turbulence in a multi-component version of these models with a large number of components. We argue that strong spin-wave turbulence is similar to turbulence of the focusing NSE, as it realizes a critical-balance state. However, UV nonlocality causes the level of spin-wave turbulence at large scales to decrease with increasing pumping level, culminating in a state that is independent of the level of pumping.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [54] [Fermionic neural Gibbs states](https://arxiv.org/abs/2512.04663)
*Jannes Nys,Juan Carrasquilla*

Main category: quant-ph

TL;DR: fNGS is a neural network framework for modeling finite-temperature properties of strongly interacting fermions, achieving accurate results for doped Fermi-Hubbard model beyond exact methods.


<details>
  <summary>Details</summary>
Motivation: There's a need for scalable methods to study finite-temperature properties of strongly correlated fermionic systems beyond one dimension, as existing exact methods have limitations in system size and complexity.

Method: fNGS starts from a reference mean-field thermofield-double state and uses neural-network transformations combined with imaginary-time evolution to systematically build strong correlations.

Result: Applied to the doped Fermi-Hubbard model, fNGS accurately reproduces thermal energies over broad temperature ranges, interaction strengths, and large dopings, for system sizes beyond exact methods.

Conclusion: fNGS demonstrates a scalable route to studying finite-temperature properties of strongly correlated fermionic systems beyond one dimension using neural-network quantum state representations.

Abstract: We introduce fermionic neural Gibbs states (fNGS), a variational framework for modeling finite-temperature properties of strongly interacting fermions. fNGS starts from a reference mean-field thermofield-double state and uses neural-network transformations together with imaginary-time evolution to systematically build strong correlations. Applied to the doped Fermi-Hubbard model, a minimal lattice model capturing essential features of strong electronic correlations, fNGS accurately reproduces thermal energies over a broad range of temperatures, interaction strengths, even at large dopings, for system sizes beyond the reach of exact methods. These results demonstrate a scalable route to studying finite-temperature properties of strongly correlated fermionic systems beyond one dimension with neural-network representations of quantum states.

</details>


### [55] [Convergence of sample-based quantum diagonalization on a variable-length cuprate chain](https://arxiv.org/abs/2512.04962)
*L. Andrew Wray,Cheng-Ju Lin,Vincent Su,Hrant Gharibyan*

Main category: quant-ph

TL;DR: SQD algorithm for quantum chemistry faces convergence issues; study explores scaling with copper oxide plaquettes, identifies connectivity, expansion order, and orbital basis as key factors, and finds quantum noise can surprisingly improve convergence.


<details>
  <summary>Details</summary>
Motivation: Sample-based quantum diagonalization (SQD) is promising for NISQ-era quantum chemistry but suffers from convergence problems on practical timescales, requiring investigation of scaling and optimization strategies.

Method: Study scaling of SQD algorithm on variable-length copper oxide plaquettes (2-6 units) with minimal molecular orbital basis, testing effects of all-to-all connectivity, higher expansion order, non-Hartree-Fock basis, and real quantum hardware noise.

Result: All-to-all connectivity, higher SQD expansion order, and non-Hartree-Fock basis help overcome sampling bottlenecks but involve tradeoffs. Surprisingly, noise on Quantinuum H2 trapped ion device improved energy convergence beyond noise-free simulations.

Conclusion: SQD convergence can be improved through hardware connectivity, algorithmic expansion, and orbital basis selection, with quantum noise potentially beneficial rather than purely detrimental, requiring careful balancing of tradeoffs for NISQ applications.

Abstract: Sample-based quantum diagonalization (SQD) is an algorithm for hybrid quantum-classical molecular simulation that has been of broad interest for application with noisy intermediate scale quantum (NISQ) devices. However, SQD does not always converge on a practical timescale. Here, we explore scaling of the algorithm for a variable-length molecule made up of 2 to 6 copper oxide plaquettes with a minimal molecular orbital basis. The results demonstrate that enabling all-to-all connectivity, instituting a higher expansion order for the SQD algorithm, and adopting a non-Hartree-Fock molecular orbital basis can all play significant roles in overcoming sampling bottlenecks, though with tradeoffs that need to be weighed against the capabilities of quantum and classical hardware. Additionally, we find that noise on a real quantum computer, the Quantinuum H2 trapped ion device, can improve energy convergence beyond expectations based on noise-free statevector simulations.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [56] [Effective permeabilities for flow through anisotropic microscopic geometries](https://arxiv.org/abs/2512.04133)
*Loïc Balazi,Fabian Holzberger,Stephan B. Lunowa,Malte A. Peter,Daniel Peterseim,Barbara Wohlmuth*

Main category: physics.flu-dyn

TL;DR: Framework for computing anisotropic permeability in fibrous microstructures, validated for coiled aneurysm domains, showing significant directional flow effects missed by isotropic models.


<details>
  <summary>Details</summary>
Motivation: Need to accurately model flow in coiled aneurysm domains with dense, fibre-like obstacles, requiring better understanding of anisotropic permeability effects.

Method: Combines homogenisation theory with fully resolved simulations in Representative Elementary Volumes (REVs), validates Boutin's permeability model, and develops systematic methodology for capturing directional variations from fibre orientation.

Result: Anisotropy significantly impacts local flow direction and magnitude, creating directional permeability contrasts that isotropic approximations cannot reproduce; framework enables more realistic assessment of post-treatment aneurysm flow.

Conclusion: Proposed approach integrates coil-induced microstructural effects into continuum-scale hemodynamic models for better clinical assessment, with broader applicability to other fibrous porous systems.

Abstract: This work develops a computational and theoretical framework for determining effective permeabilities in anisotropic microscopic geometries containing dense, fibre-like obstacles, motivated by the need to model flow in coiled aneurysm domains accurately. Building on homogenisation theory and fully resolved simulations in Representative Elementary Volumes (REVs), we validate the permeability model introduced in [C. Boutin, Study of permeability by periodic and self-consistent homogenisation. Eur. J. Mech. A Solids, 19(4):603-632, 2000] and propose a systematic methodology for capturing the directional variations induced by fibre orientation. The resulting permeability tensors are incorporated into macroscopic flow simulations based on the Darcy equation, enabling direct comparison of anisotropic and isotropic permeability models across several benchmark configurations. Our findings show that anisotropy has a significant impact on local flow direction and magnitude, generating directional permeability contrasts which cannot be reproduced by classical isotropic approximations. By integrating coil-induced microstructural effects into continuum-scale hemodynamic models, the proposed approach enables more realistic assessment of post-treatment aneurysm flow behaviour. Beyond this clinical application, the framework is broadly applicable to other biomedical and engineering systems involving fibrous or filamentous porous microstructures.

</details>


### [57] [Can Explicit Subgrid Models Enhance Implicit LES Simulations? A GPU-Oriented High-Order-Solver Perspective](https://arxiv.org/abs/2512.04574)
*Gonzalo Rubio,Gerasimos Ntoukas,Miguel Chávez-Módena,Oscar Mariño,Bernat Font,Oriol Lehmkuhl,Eusebio Valero,Esteban Ferrer*

Main category: physics.flu-dyn

TL;DR: High-order DG methods on GPUs excel at turbulence simulation but face under-resolution issues at high Re. Study shows split-form DG schemes provide sufficient stability without SGS models when resolution is adequate, while SGS models only help when resolution is insufficient to complement numerical dissipation.


<details>
  <summary>Details</summary>
Motivation: High-order DG methods on GPU architectures offer excellent accuracy for turbulent flow simulations, but even with high-order discretizations, high Reynolds number simulations require under-resolution, making them sensitive to numerical dissipation and aliasing effects. The paper investigates the interplay between intrinsic DG dissipation mechanisms (split forms and Riemann solvers) and explicit subgrid-scale models in LES.

Method: The study evaluates kinetic energy dissipation, spectral accuracy, and numerical stability using the three-dimensional Taylor-Green vortex at Re = 1600 and an inviscid case (Re → ∞). It examines how split-form (energy- or entropy-stable) DG schemes interact with explicit subgrid-scale LES models under different resolution conditions.

Result: When stability is ensured through split-form schemes, SGS models are not strictly necessary for under-resolved turbulence. At moderate Re with sufficient resolution, SGS models don't improve accuracy due to overlap with inherent DG dissipation. At high Re with insufficient resolution, SGS models complement numerical dissipation and enhance accuracy by removing excess energy that numerical fluxes alone cannot dissipate.

Conclusion: The findings provide practical guidance for choosing numerical strategies in high-order turbulence simulations: split-form DG schemes alone are sufficient when resolution is adequate, while explicit SGS models are only beneficial when resolution is insufficient to complement the numerical dissipation at high Reynolds numbers.

Abstract: High-order Discontinuous Galerkin (DG) methods offer excellent accuracy for turbulent flow simulations, especially when implemented on GPU-oriented architectures that favor very high polynomial orders. On modern GPUs, high-order polynomial evaluations cost roughly the same as low-order ones, provided the DG degrees of freedom fit within device memory. However, even with high-order discretizations, simulations at high Reynolds numbers still require some level of under-resolution, leaving them sensitive to numerical dissipation and aliasing effects. Here, we investigate the interplay between intrinsic DG dissipation mechanisms (implicit dissipation) -- in particular split forms and Riemann solvers -- and explicit subgrid-scale models in Large Eddy Simulations (LES). Using the three-dimensional Taylor--Green vortex at $Re = 1600$ and an inviscid case ($Re \to \infty$), we evaluate kinetic energy dissipation, spectral accuracy, and numerical stability.
  Our results show that when stability for under-resolved turbulence is ensured through split-forms (energy- or entropy-stable) schemes, subgrid-scale (SGS) LES models are not strictly necessary. At moderate Reynolds numbers, when the spatial resolution is sufficient to capture the relevant turbulence scales (i.e., in well-resolved LES), adding SGS models does not improve accuracy because the wavenumber range where they act overlaps with the inherent numerical dissipation of the DG scheme. In contrast, when the resolution is insufficient, as is typical at high Reynolds numbers, explicit subgrid-scale models complement the numerical dissipation and enhance accuracy by removing the excess energy that numerical fluxes alone cannot dissipate.
  These findings provide practical guidance for choosing numerical strategies in high-order turbulence simulations.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [58] [Cumulant expansions of operator groups of quantum many-particle systems](https://arxiv.org/abs/2512.05036)
*V. I. Gerasimenko,I. V. Gapyak*

Main category: math-ph

TL;DR: The paper develops cluster expansion methods for operator groups in quantum many-body systems to construct generating operators for nonperturbative solutions to evolution equations.


<details>
  <summary>Details</summary>
Motivation: To address the Cauchy problem for hierarchies of evolution equations in many-particle quantum systems using nonperturbative approaches, moving beyond traditional perturbation methods.

Method: Cluster expansions for groups of operators associated with von Neumann equations (for states) and Heisenberg equations (for observables), constructing generating operators for nonperturbative solutions.

Result: Development of a method to construct generating operators that provide nonperturbative solutions to the Cauchy problem for hierarchies of evolution equations in quantum many-body systems.

Conclusion: The cluster expansion approach enables nonperturbative treatment of quantum many-particle evolution equations, offering a systematic framework for solving hierarchies of evolution equations beyond perturbation theory.

Abstract: The article presents a method of cluster expansions for groups of operators associated with the von Neumann equations for states and the Heisenberg equations for observables, aiming to construct generating operators for nonperturbative solutions to the Cauchy problem for hierarchies of evolution equations of many-particle quantum systems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [59] [Double Perovskites K2NbTaO6 and Rb2NbTaO6 from First-Principles: Towards Efficient Materials for Green Energy](https://arxiv.org/abs/2512.04134)
*Ouendadji Salima,Aissani Ali,El Haj Hassan Fouad,Benahmedi Lakhdar*

Main category: cond-mat.mtrl-sci

TL;DR: First-principles study shows K2NbTaO6 and Rb2NbTaO6 double perovskites are mechanically stable, brittle semiconductors with band gaps ~2.6-2.8 eV and UV absorption, but limited ductility and thermoelectric performance.


<details>
  <summary>Details</summary>
Motivation: Double perovskite oxides are attractive for coupled optical, mechanical, and thermal applications due to their structural flexibility and multifunctional nature. The study aims to investigate the stability and properties of K2NbTaO6 and Rb2NbTaO6 for potential optoelectronic and photocatalytic applications.

Method: First-principles computations were used to examine structural, electronic, elastic, optical, and thermoelectric properties. The Born stability criteria and Pugh's ratio were applied to assess mechanical stability and ductility. Band structure analysis determined electronic properties and energy gaps.

Result: Both compounds form cubic double perovskite structures with ordered Nb5+ and Ta5+ cations. They are mechanically stable but brittle (limited ductility). They exhibit semiconducting behavior with band gaps of 2.79 eV (K2NbTaO6) and 2.63 eV (Rb2NbTaO6). Optical spectra show noticeable UV absorption, relevant for optoelectronic and photocatalytic studies. Thermoelectric performance appears limited.

Conclusion: K2NbTaO6 and Rb2NbTaO6 are promising for theoretical studies of optoelectronic and photocatalytic processes due to their structural stability, semiconductor nature, and UV absorption properties. However, their brittle behavior and limited thermoelectric performance suggest practical device applications may be constrained.

Abstract: The structural flexibility and multifunctional nature of double perovskite oxides make them attractive for applications requiring coupled optical, mechanical, and thermal performance. Using first-principles computations, this study examines the structural, electronic, elastic, optical, and thermoelectric stability of K2NbTaO6 and Rb2NbTaO6. The two compounds combine to form a cubic double perovskite structure with ordered Nb$^{5+}$ and Ta$^{5+}$ cations. The calculated elastic constants satisfy the Born stability criteria, confirming mechanical stability; however, both K2NbTaO6 and Rb2NbTaO6 exhibit brittle behavior according to Pugh's ratio, reflecting limited ductility. Semiconducting behavior is revealed by band structure analysis with energy gaps of 2.79 eV for K2NbTaO6 and 2.63 eV for Rb2NbTaO6. Optical spectra show noticeable absorption in the high-energy region near the UV, indicating relevance for theoretical studies of optoelectronic and photocatalytic processes, without implying practical device efficiency. Therm

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [60] [An all-optical convolutional neural network for image identification](https://arxiv.org/abs/2512.04569)
*Wei-Wei Fu,Dong Zhao,Qing-Hong Rao,Heng-Yi Wang,Ben-Li Yu,Zhi-Jia Hu,Fang-Wen Sun,Kun Huang*

Main category: physics.optics

TL;DR: All-optical CNN using spatial-differentiation convolution with directional kernels and diffractive fully-connected layer achieves high-speed, energy-efficient image classification without explicit optical nonlinearities.


<details>
  <summary>Details</summary>
Motivation: Electronic CNNs face speed and energy efficiency bottlenecks due to resistive/capacitive losses. Photonic alternatives are promising but struggle with optical nonlinearities needed for end-to-end classification.

Method: Single spatial-differentiation convolutional stage with 24 directional kernels (360°) plus mean-filtering kernel, followed by diffractive fully-connected layer. Uses weak inherent optical diffraction nonlinearity instead of explicit nonlinear activations.

Result: 86.8% accuracy on MNIST, 94.8% on ten-class gesture dataset. Computational throughput: 1.13×10^5 TOPS, energy efficiency: 1.51×10^3 TOPS/W (highest reported for CNN hardware). Potential for 5-6 orders magnitude improvement with nanosecond detectors.

Conclusion: Demonstrates scalable pathway for ultralow-latency, ultralow-energy vision processing using all-optical CNN architecture that bypasses need for explicit optical nonlinearities.

Abstract: In modern artificial intelligence, convolutional neural networks (CNNs) have become a cornerstone for visual and perceptual tasks. However, their implementation on conventional electronic hardware faces fundamental bottlenecks in speed and energy efficiency due to resistive and capacitive losses. Photonic alternatives offer a promising route, yet the difficulty of realizing optical nonlinearities has prevented the realization of all-optical CNNs capable of end-to-end image classification. Here, we demonstrate an all-optical CNN that bypasses the need for explicit optical nonlinear activations. Our architecture comprises a single spatial-differentiation convolutional stage--using 24 directional kernels spanning 360°, along with a mean-filtering kernel--followed by a diffractive fully-connected layer. The directional convolution enhances feature selectivity, suppresses noise and crosstalk, and simplifies the classification task, allowing the weak nonlinearity inherent in optical diffraction to achieve high accuracy. We report experimentally classification accuracies of 86.8% on handwritten digits (MNIST) and 94.8% on a ten-class gesture dataset. The system delivers a computational throughput of 1.13X10^5 tera-operations per second (TOPS) and an energy efficiency of 1.51X10^3 TOPS/W--the highest reported among CNN hardware--with the potential to improve by a further 5-6 orders of magnitude using nanosecond-scale detectors. This work establishes a scalable pathway toward ultralow-latency, ultralow-energy vision processing for real-time intelligent systems.

</details>


### [61] [Structured Light at the Extreme: Harnessing Spatiotemporal Control for High-Field Laser-Matter Interactions](https://arxiv.org/abs/2512.05042)
*Sergio Carbajo,Seung-Whan Bahk,Justin Baker,Andrea Bertozzi,Abhimanyu Borthakur,Antonino Di Piazza,Andrew Forbes,Spencer Gessner,Jack Hirschman,Franz Kärtner,Maciej Lewenstein,Yuhang Li,Inhyuk Nam,Eileen Otte,Aydogan Ozcan,James Rozensweig,Yijie Shen,Liwei Song,Ye Tian,Yu Wang,Yuntian Wang,Logan Wright,Xiaojun Wu,Hao Zhang*

Main category: physics.optics

TL;DR: This review explores intelligent structured light for controlling high-field laser-matter interactions, covering advanced electromagnetic tools, AI-driven optimization, and transformative applications like programmable electron beams and compact accelerators.


<details>
  <summary>Details</summary>
Motivation: The motivation is to move beyond merely observing light-matter interactions at extreme intensities to actively commanding them through precise spatiotemporal and vectorial control of light, which represents a critical degree of freedom for advancing high-field laser physics.

Method: The review outlines a three-pillar framework: 1) Advanced electromagnetic toolkit including static optics and plasma light modulators, 2) Optimization engines using physics-informed digital twins and AI-driven inverse design, and 3) Exploration of applications enabled by integrated structured light control.

Result: The review identifies promising applications including programmable electron beams, orbital-angular-momentum-carrying γ-rays, compact THz accelerators, and robust communications systems, while highlighting challenges in material science, real-time adaptive control, and quantum extensions.

Conclusion: The review calls for interdisciplinary collaboration to overcome challenges in material science, MHz-rate adaptive control, and quantum extensions, aiming to transform from passive observation to active command of extreme light-matter interactions.

Abstract: This review charts the emerging paradigm of intelligent structured light for high-field laser-matter interactions, where the precise spatiotemporal and vectorial control of light is a critical degree of freedom. We outline a transformative framework built upon three synergistic pillars. First, we survey the advanced electromagnetic toolkit, moving beyond conventional spatial light modulators to include robust static optics and the promising frontier of plasma light modulators. Second, we detail the optimization engine for this high-dimensional design space, focusing on physics-informed digital twins and AI-driven inverse design to automate the discovery of optimal light structures. Finally, we explore the groundbreaking applications enabled by this integrated approach, including programmable electron beams, orbital-angular-momentum-carrying γ-rays, compact THz accelerators, and robust communications. The path forward necessitates overcoming grand challenges in material science, real-time adaptive control at MHz rates, and the extension of these principles to the quantum realm. This review serves as a call to action for a coordinated, interdisciplinary effort to command, rather than merely observe, light-matter interactions at the extreme.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [62] [Phase mixing and the Vlasov equation in cosmology](https://arxiv.org/abs/2512.04214)
*Martin Taylor,Renato Velozo Ruiz*

Main category: gr-qc

TL;DR: The paper studies Vlasov equation on expanding cosmological spacetimes, showing density decay rates depend on expansion rate and initial data regularity, with phase mixing effects enhancing decay.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time behavior of collisionless matter (Vlasov equation) in expanding cosmological spacetimes (FLRW models), particularly how expansion rates affect density decay and how phase mixing effects enhance this decay depending on initial data regularity.

Method: Uses a collection of commuting vector fields and combinatorial properties of associated differential operators. The vector fields are not explicit but have good properties for large t relative to momentum support. Employs physical space dyadic localization to handle non-compactly supported (including analytic) solutions.

Result: For expansion rate t^q (0<q<1/2), spatial density decays at rate t^{-6q}, with enhanced decay when spatial average is removed: polynomial enhancement for Sobolev data, super-polynomial but sub-exponential for analytic data. For borderline rate t^{1/2} (radiation-filled universe), logarithmic enhancement for Sobolev data and super-logarithmic enhancement (exp(-μ(log t)^ε)) for analytic data.

Conclusion: The decay of matter density in expanding universes depends crucially on both the expansion rate and the regularity of initial data, with phase mixing effects providing significant enhancement, particularly for analytic initial data. The borderline radiation-filled universe exhibits different, logarithmic-type enhancements.

Abstract: We consider the Vlasov equation on slowly expanding isotropic homogeneous tori, described by the Friedmann--Lemaître--Robertson--Walker cosmological spacetimes. For expansion rate $t^q$, with $0< q<\frac{1}{2}$ (excluding certain exceptional values), we show that the spatial density decays at the rate $t^{-6q}$ and that, when the spatial average is removed, the density decays at an enhanced rate due to a phase mixing effect. This enhancement is polynomial for Sobolev initial data and super-polynomial, but sub-exponential, for real analytic initial data. We further show that, when the expansion rate is the borderline $t^{\frac{1}{2}}$ -- the rate which describes a radiation filled universe -- a degenerate phase mixing effect results in a logarithmic enhancement for Sobolev initial data and a super-logarithmic enhancement (in fact, a gain of $\exp(-μ(\log t)^ε)$ for some $μ,ε>0$) for analytic initial data. The proof is based on a collection of commuting vector fields, and certain combinatorial properties of an associated collection of differential operators. The vector fields are not explicit, but are shown to have good properties when $t$ is large with respect to the momentum support of the solution. A physical space dyadic localisation is employed to treat non-compactly supported (in particular, non-trivial real analytic) but suitably decaying solutions.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [63] [The Dirichlet heat trace for domains with curved corners](https://arxiv.org/abs/2512.04422)
*Shi Zhuo Looi,David Sher*

Main category: math.SP

TL;DR: The paper studies short-time heat trace asymptotics on curvilinear polygons, deriving explicit formulas for the t^{1/2} coefficient involving boundary curvature integrals and corner contributions.


<details>
  <summary>Details</summary>
Motivation: To understand the spectral geometry of curvilinear polygons and develop precise asymptotic expansions for Dirichlet heat traces, which has applications to isospectrality problems.

Method: Uses conformal models and parametrix construction on sector heat spaces, expressing corner contributions via Hadamard finite parts of explicit traces over exact sectors.

Result: Derives that the t^{1/2} coefficient splits into boundary integral of κ^2 plus local corner contributions depending on interior angles and limiting curvatures. Computes c_{1/2}(π/2)=1/(16√π) for right-angled corners.

Conclusion: The results extend previous work by showing that any Dirichlet isospectral curvilinear polygon must be a polygon with straight sides, establishing a spectral rigidity property.

Abstract: We study the short-time asymptotics of the Dirichlet heat trace on planar curvilinear polygons. For such domains we show that the coefficient of $t^{1/2}$ in the expansion splits into a boundary integral of $κ^2$ and a sum of local corner contributions, one for each vertex. Each curved corner contribution depends only on the interior angle $α$ and on the limiting curvatures $κ_{\pm}$ on the adjacent sides. Using a conformal model and a parametrix construction on the sector heat space, we express this contribution in the form $c_{1/2}(α)\,r_0(α,κ_+, κ_-)$, where $c_{1/2}(α)$ is given by a Hadamard finite part of an explicit trace over the exact sector. For right-angled corners we compute $c_{1/2}(π/2)=1/(16\sqrtπ)$ and obtain a closed formula for the $t^{1/2}$ coefficient. As an application we extend a previous result in the literature by showing that any admissible curvilinear polygon that is Dirichlet isospectral to a polygon must itself be a polygon with straight sides.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [64] [Magnetic Field Amplification and Particle Acceleration in Weakly Magnetized Trans-relativistic Electron-ion Shocks](https://arxiv.org/abs/2512.03169)
*Taiki Jikei,Daniel Groselj,Lorenzo Sironi*

Main category: astro-ph.HE

TL;DR: 2D PIC simulations show quasi-parallel trans-relativistic shocks exhibit two regimes: Bell-dominated at higher magnetizations (σ≳10⁻³) efficiently accelerating ions, while Weibel-dominated at lower magnetizations (σ≲10⁻⁴) accelerates both ions and electrons equally.


<details>
  <summary>Details</summary>
Motivation: To understand the physics of quasi-parallel trans-relativistic shocks in weakly magnetized plasmas, particularly the competition between Bell and Weibel instabilities in shaping shock precursors and particle acceleration mechanisms.

Method: Long-duration two-dimensional particle-in-cell (PIC) simulations of quasi-parallel trans-relativistic shocks propagating in weakly magnetized plasmas.

Result: Bell-dominated shocks (σ≳10⁻³) convert ~20% of upstream flow energy into nonthermal ions with Bohm scaling (E_max ∝ t), while Weibel-dominated shocks (σ≲10⁻⁴) accelerate both ions and electrons equally (~10% each) with slower scaling (E_max ∝ t¹/²).

Conclusion: The competition between Bell and Weibel instabilities determines shock structure and particle acceleration efficiency in trans-relativistic shocks, with implications for astrophysical phenomena like extragalactic jet termination shocks, GRB afterglows, and fast blue optical transients.

Abstract: We investigate the physics of quasi-parallel trans-relativistic shocks propagating in weakly magnetized plasmas by means of long-duration two-dimensional particle-in-cell simulations. The structure of the shock precursor is shaped by a competition between the Bell instability and the Weibel instability. The Bell instability is dominant at relatively high magnetizations $(σ\gtrsim10^{-3})$, whereas the Weibel instability prevails at lower magnetizations $(σ\lesssim10^{-4})$. Bell-dominated shocks efficiently accelerate ions, converting a fraction $\varepsilon_{\mathrm{i}}\sim0.2$ of the upstream flow energy into downstream nonthermal ion energy. The maximum energy of nonthermal ions exhibits a Bohm scaling in time, as $E_{\max}\propto t$. A much smaller fraction $\varepsilon_{\mathrm{e}}\ll0.1$ of the upstream flow energy goes into downstream nonthermal electrons in the Bell-dominated regime. On the other hand, Weibel-dominated shocks efficiently generate both nonthermal ions and electrons with $\varepsilon_{\mathrm{i}}\sim\varepsilon_{\mathrm{e}}\sim0.1$, albeit with a slower scaling for the maximum energy, $E_{\mathrm{max}}\propto t^{1/2}$. Our results are applicable to a wide range of trans-relativistic shocks, including the termination shocks of extragalactic jets, the late stages of gamma-ray burst afterglows, and shocks in fast blue optical transients.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [65] [A Unified Low-rank ADI Framework with Shared Linear Solves for Simultaneously Solving Multiple Lyapunov, Sylvester, and Riccati Equations](https://arxiv.org/abs/2512.04676)
*Umair Zulfiqar,Zhong-Yi Huang*

Main category: eess.SY

TL;DR: Unified ADI framework solves multiple Lyapunov, Sylvester, and Riccati equations simultaneously using shared linear solves, while also producing reduced-order models that preserve system properties.


<details>
  <summary>Details</summary>
Motivation: Existing ADI methods for different matrix equations (Lyapunov, Sylvester, Riccati) share computational structure but are typically solved separately. The authors aim to unify these methods to share expensive linear solves across multiple equations, improving computational efficiency.

Method: Proposes a unified ADI framework that recognizes all three equation types as Petrov-Galerkin projection algorithms differing only in pole placement. The method shares shifted linear solves (the main computational cost) across equations, while performing inexpensive small-scale operations for pole placement and solution extraction.

Result: The framework can simultaneously solve 6 Lyapunov, 1 Sylvester, and 10 Riccati equations with only 2 shifted linear solves per iteration, dramatically reducing computational cost. Additionally produces reduced-order models that interpolate transfer functions at ADI shift mirror images while preserving stability, minimum-phase, positive-realness, bounded-realness, and passivity.

Conclusion: The unified ADI framework provides substantial computational savings by sharing expensive linear solves across multiple matrix equations while also serving as an interpolation-based model order reduction method that preserves important system properties in reduced models.

Abstract: It is known in the literature that the low-rank ADI method for Lyapunov equations is a Petrov-Galerkin projection algorithm that implicitly performs model order reduction. In this paper, we show that the low-rank ADI methods for Sylvester and Riccati equations are also Petrov-Galerkin projection algorithms that implicitly perform model order reduction. By observing that the ADI methods for Lyapunov, Sylvester, and Riccati equations differ only in pole placement and not in their interpolatory nature, we show that the shifted linear solves-which constitute the bulk of the computational cost-can be shared. The pole-placement step involves only small-scale operations and is therefore inexpensive. We propose a unified ADI framework that requires only two shifted linear solves per iteration to simultaneously solve six Lyapunov equations, one Sylvester equation, and ten Riccati equations, thus substantially increasing the return on investment for the computational cost spent on the linear solves. All operations needed to extract the individual solutions from these shared linear solves are small-scale and inexpensive.
  Since all ADI methods implicitly perform model order reduction when solving these linear matrix equations, we show that the resulting reduced-order models can be obtained as an additional byproduct. These models not only interpolate the original transfer function at the mirror images of the ADI shifts but also preserve important system properties such as stability, minimum-phase property, positive-realness, bounded-realness, and passivity. Consequently, the proposed unified ADI framework also serves as a recursive, interpolation-based model order reduction method, which can preserve several important properties of the original model in the reduced-order model.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [66] [High-repetition-rate, all-reflective optical guiding and electron acceleration in helium using an off-axis axicon](https://arxiv.org/abs/2512.04788)
*Jiří Šišma,Michal Nevrkla,Filip Vitha,Sebastian Lorenz,Illia Zymak,Alžběta Špádová,Andrea Kollárová,Matěj Jech,Alexandr Jančárek,Davorin Peceli,Carlo M. Lazzarini,Leonardo V. N. Goncalves,Gabriele M. Grittani,Sergei V. Bulanov,Jaron E. Shrock,Ela Rockafellow,Ari J. Sloss,Bo Miao,Scott W. Hancock,Howard M. Milchberg*

Main category: physics.acc-ph

TL;DR: High-power laser wakefield acceleration using self-waveguiding in plasma channels achieves stable 5 GeV electron beams at 0.2 Hz with a novel all-reflective optical setup.


<details>
  <summary>Details</summary>
Motivation: To develop a stable, high-repetition-rate laser wakefield acceleration system that doesn't require modifications to existing laser systems, making the technology more accessible for user facilities.

Method: Used the L3-HAPLS laser system (13 J, 30 fs, 0.2 Hz) with self-waveguiding in a 20 cm plasma channel in helium. Implemented a novel all-reflective optical setup including an off-axis reflective axicon for efficient acceleration and guiding.

Result: Achieved stable acceleration of electron beams to energies approaching 5 GeV at 0.2 Hz, with guiding demonstrated at repetition rates up to 3.3 Hz. The system stabilizes electron pointing and enhances energy gain.

Conclusion: This compact single laser, single compressor implementation of plasma channels for electron acceleration paves the way for broader adoption of LWFA technology across user facilities without requiring laser system modifications.

Abstract: We present recent results on high-power guiding and laser wakefield acceleration (LWFA) in the ELBA beamline at ELI Beamlines, using the L3-HAPLS laser system (13 J, 30 fs, 0.2 Hz). By employing self-waveguiding in a 20 cm plasma channel in helium, we achieved stable acceleration of electron beams to energies approaching 5 GeV. A novel all-reflective optical setup, including an off-axis reflective axicon, enabled efficient acceleration at 0.2 Hz and guiding at repetition rates up to 3.3 Hz. This compact single laser, single compressor implementation of plasma channels for electron acceleration stabilizes electron pointing and enhances energy gain without requiring modifications to the laser system, paving the way for broader adoption of the technology across user facilities.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [67] [NORi: An ML-Augmented Ocean Boundary Layer Parameterization](https://arxiv.org/abs/2512.04452)
*Xin Kai Lee,Ali Ramadhan,Andre Souza,Gregory LeClaire Wagner,Simone Silvestri,John Marshall,Raffaele Ferrari*

Main category: physics.ao-ph

TL;DR: NORi is a physics-based ML parameterization for ocean boundary layer turbulence that combines Richardson number closure with neural ODEs to capture entrainment dynamics, trained on large-eddy simulations and demonstrating excellent generalization and long-term stability.


<details>
  <summary>Details</summary>
Motivation: Traditional local diffusive closures cannot adequately represent entrainment through the base of the ocean boundary layer, which is crucial for accurate climate modeling. There's a need for parameterizations that can capture this complex physics while maintaining numerical stability and generalization across different ocean conditions.

Method: NORi combines a physics-based Richardson number-dependent diffusivity/viscosity closure with neural ordinary differential equations (NODEs). It's trained "a posteriori" using large-eddy simulations with a loss function that depends on time-integrated variables rather than noisy instantaneous subgrid fluxes. The method is designed for seawater's nonlinear equation of state.

Result: NORi demonstrates excellent prediction and generalization across different convective strengths, oceanic stratifications, rotation strengths, and surface wind forcings. It remains numerically stable for at least 100 years of integration despite being trained on only 2-day horizons, and can run with time steps as long as one hour.

Conclusion: The combination of expressive neural networks with physically-rigorous base closure provides a robust paradigm for climate model parameterizations, reducing data requirements, enabling direct optimization of inference performance, and implicitly encouraging numerical stability during training.

Abstract: NORi is a machine-learned (ML) parameterization of ocean boundary layer turbulence that is physics-based and augmented with neural networks. NORi stands for neural ordinary differential equations (NODEs) Richardson number (Ri) closure. The physical parameterization is controlled by a Richardson number-dependent diffusivity and viscosity. The NODEs are trained to capture the entrainment through the base of the boundary layer, which cannot be represented with a local diffusive closure. The parameterization is trained using large-eddy simulations in an "a posteriori" fashion, where parameters are calibrated with a loss function that explicitly depends on the actual time-integrated variables of interest rather than the instantaneous subgrid fluxes, which are inherently noisy. NORi is designed for the realistic nonlinear equation of state of seawater and demonstrates excellent prediction and generalization capabilities in capturing entrainment dynamics under different convective strengths, oceanic background stratifications, rotation strengths, and surface wind forcings. NORi is numerically stable for at least 100 years of integration time in large-scale simulations, despite only being trained on 2-day horizons, and can be run with time steps as long as one hour. The highly expressive neural networks, combined with a physically-rigorous base closure, prove to be a robust paradigm for designing parameterizations for climate models where data requirements are drastically reduced, inference performance can be directly targeted and optimized, and numerical stability is implicitly encouraged during training.

</details>
