<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 11]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math.OC](#math.OC) [Total: 3]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Do you precondition on the left or on the right?](https://arxiv.org/abs/2512.05160)
*Nicole Spillane,Pierre Matalon,Daniel B Szyld*

Main category: math.NA

TL;DR: This paper reports results from a social experiment asking researchers about left vs right preconditioning preferences, provides context on preconditioning methods, and shows examples where convergence bounds can be misleading.


<details>
  <summary>Details</summary>
Motivation: To understand researcher preferences and practices regarding left vs right preconditioning in numerical linear algebra, and to provide educational context about different preconditioning approaches.

Method: Conducted a social experiment survey at DD29 conference asking "Do you precondition on the left or on the right?", performed literature review on preconditioning methods, and analyzed convergence bounds with illustrative examples.

Result: Survey results showing researcher preferences, contextual analysis of left/right/split preconditioning approaches, and demonstration that convergence bounds can sometimes lead to misleading conclusions about algorithm performance.

Conclusion: The paper provides insights into preconditioning practices in the research community, educational context about different preconditioning methods, and cautions about over-reliance on convergence bounds for algorithm evaluation.

Abstract: This work is a follow-up to a poster that was presented at the DD29 conference. Participants were asked the question: ``Do you precondition on the left or on the right?''. Here we report on the results of this social experiment. We also provide context on left, right and split preconditioning, share our literature review on the topic, and analyze some of the finer points. Two examples illustrate that convergence bounds can sometimes lead to misleading conclusions.

</details>


### [2] [Calculation of Univariate Pade Approximants for solutions of the Michaelis-Menten equation with first order input using the Tau method](https://arxiv.org/abs/2512.05258)
*Gareth Hegarty*

Main category: math.NA

TL;DR: Jacobi formula generates diagonal Pade approximants via Tau method for Michaelis-Menten equation with first-order input.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient recursive algorithm for generating accurate rational approximations (Pade approximants) to solutions of the Michaelis-Menten equation with first-order input, which is important in enzyme kinetics modeling.

Method: Uses Jacobi formula within the Tau method framework, postulating specific forms for Jacobi coefficients and error terms, with the algorithm maintaining these forms through systematic cancellation patterns.

Result: Recursive generation of diagonal univariate Pade approximants for the Michaelis-Menten equation solution, with the algorithm preserving the postulated coefficient and error term structures.

Conclusion: The Jacobi formula combined with Tau method provides an effective recursive approach for constructing Pade approximants to Michaelis-Menten kinetics, with structured coefficient patterns enabling systematic computation.

Abstract: In this paper the Jacobi formula is used to recursively generate (diagonal) univariate Pade approximants using the Tau method for solutions Michaelis-Menten equation with first order input. In the algorithm the Jacobi coefficients and error terms in the Tau method are postulated to have a particular form, and this form is maintained by specific patterns of cancellations.

</details>


### [3] [Stability analysis of very high order minimization-based and Taylor-based embedded boundary treatments of discontinuous Galerkin for hyperbolic equations](https://arxiv.org/abs/2512.05278)
*Mirco Ciallella*

Main category: math.NA

TL;DR: The paper presents a unified stability analysis of high-order embedded boundary methods (ROD and SB) for linear advection, showing both methods can be formulated as polynomial corrections, enabling rigorous stability study and simpler implementation.


<details>
  <summary>Details</summary>
Motivation: To analyze the stability of very high order embedded boundary methods (ROD and SB) for linear advection in unfitted configurations, where these methods impose modified boundary conditions on computational boundaries.

Method: Proves that ROD minimization problem admits a polynomial correction analogous to SB method, enabling unified perspective. Uses eigenspectrum visualization for stability analysis of discretized operators up to degree 6. Investigates coupling with explicit and implicit time integration.

Result: Shows ROD can be simplified by eliminating linear system inversions through polynomial correction formulation. Provides side-by-side stability analysis of ROD and SB methods. Numerical experiments confirm stability findings.

Conclusion: The unified polynomial correction perspective benefits both algorithmic implementation (simplifying ROD) and mathematical analysis (enabling rigorous stability study) of high-order embedded boundary methods for hyperbolic problems.

Abstract: In this paper, we present a stability analysis of very high order embedded boundary methods, specifically the Reconstruction for Off-site Data (ROD) and Shifted Boundary (SB) methods, coupled with a discontinuous Galerkin discretization for the linear advection equation. In unfitted configurations, these methods impose consistent modified boundary conditions on the computational boundary. Due to the high algebraic complexity of very high order schemes, the stability is studied by visualizing the eigenspectrum of the discretized operators. A recent study on the SB method demonstrated that its Taylor expansion can be formulated as a direct polynomial correction. In this work, we prove that the ROD minimization problem admits an analogous polynomial correction. This unified perspective provides significant benefits: algorithmically, it greatly simplifies the implementation of ROD by eliminating the need for linear system inversions at each iteration; mathematically, it enables a rigorous stability study. For completeness, a side-by-side stability analysis of the ROD and SB methods is presented for polynomials up to degree 6. Furthermore, due to the stability restrictions of embedded methods for hyperbolic problems, a coupling with both explicit and implicit time integration is investigated. A set of numerical experiments confirms the findings of the stability study.

</details>


### [4] [Randomized Algorithms for Low-Rank Matrix and Tensor Decompositions](https://arxiv.org/abs/2512.05286)
*Katherine J. Pearce,Per-Gunnar Martinsson*

Main category: math.NA

TL;DR: Survey of randomized algorithms for low-rank matrix and tensor decompositions, covering accelerated classical methods and recent advances in dimensionality reduction techniques.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of randomized algorithms that enable efficient computation of low-rank decompositions for large-scale matrices and tensors, addressing computational challenges in numerical linear algebra.

Method: Survey approach covering: 1) Classical matrix algorithms accelerated by randomized dimensionality reduction (SVD, ID, CUR), 2) Recent advances in fast matrix sketching and sampling techniques, 3) Extension of randomized methods to tensor decompositions (CP and Tucker formats including higher-order SVD, ID, CUR).

Result: Systematic review demonstrating how randomized algorithms provide computational efficiency improvements for low-rank approximations of both matrices and tensors through dimensionality reduction techniques.

Conclusion: Randomized algorithms offer powerful tools for accelerating low-rank decompositions in numerical linear algebra, with applications extending from matrices to tensors, enabling efficient computation for large-scale data analysis problems.

Abstract: This paper surveys randomized algorithms in numerical linear algebra for low-rank decompositions of matrices and tensors. The survey begins with a review of classical matrix algorithms that can be accelerated by randomized dimensionality reduction, such as the singular value decomposition (SVD) or interpolative (ID) and CUR decompositions. Recent advances in randomized dimensionality reduction are discussed, including new methods of fast matrix sketching and sampling techniques, which are incorporated into classical matrix algorithms for fast low-rank matrix approximations. The extension of randomized matrix algorithms to tensors is then explored for several low-rank tensor decompositions in the CP and Tucker formats, including the higher-order SVD, ID, and CUR decomposition.

</details>


### [5] [A new class of general linear method with inherent quadratic stability for solving stiff differential systems](https://arxiv.org/abs/2512.05486)
*Sakshi Gautam,Ram K. Pandey*

Main category: math.NA

TL;DR: The paper proposes new A- and L-stable general linear methods (GLMs) with p=q and r=s=p+1, constructed using order conditions and error minimization under A-stability constraints, achieving orders up to 4 and demonstrating competitive performance on real-world problems.


<details>
  <summary>Details</summary>
Motivation: To develop improved time integration schemes that combine A-stability, L-stability, and inherent quadratic stability (IQS) criteria for solving stiff differential equations more effectively than existing GLMs.

Method: Construction of implicit GLMs with p=q and s=r using order conditions and error minimization subject to A-stability constraints, with Nordsieck input vector assumption. Methods are tested on van der Pol oscillator, Burgers' equation, and Gray-Scott model.

Result: Successfully constructed A- and L-stable GLMs with IQS criteria up to order 4. Numerical tests show competitive performance with existing GLMs, confirmed through observed order computation and error vs step size plots.

Conclusion: The proposed GLMs provide effective alternative time integration schemes that combine multiple stability properties and demonstrate practical applicability to stiff real-world problems including oscillators and PDEs.

Abstract: This article proposes a new class of general linear method with $p=q$ and $r=s=p+1$. The construction of the present method is carried out using order conditions and error minimization subject to $A$- stability constraints. The proposed time integration schemes are $A$- and $L$-stable general linear methods (GLMs) equipped with inherent quadratic stability (IQS) criteria. We construct implicit GLMs of orders up to four with $p = q$ and $s = r$ along with the Nordsieck input vector assumption. Further, we test these schemes on three real-world problems: the van der Pol oscillator and two partial differential equations consisting of diffusion (Burgers' equation and the Gray-Scott model), and numerical results are presented. Computational results confirm that our proposed schemes are competitive with the existing GLMs and can be recognized as an alternative time integration scheme. We demonstrate the order of accuracy and convergence for the proposed schemes through observed order computation and error versus step size plots.

</details>


### [6] [Long-time stability analysis of an explicit exponential Runge-Kutta scheme for Cahn-Hilliard equations](https://arxiv.org/abs/2512.05608)
*Jing Guo*

Main category: math.NA

TL;DR: Second-order exponential Runge-Kutta method for Cahn-Hilliard equation with proven long-time stability, uniform boundedness, and optimal error estimates without typical boundedness assumptions.


<details>
  <summary>Details</summary>
Motivation: Previous energy-stability analyses for Cahn-Hilliard equation numerical methods typically require boundedness assumptions. This paper aims to develop a fully discrete scheme that preserves energy dissipation without such assumptions, providing rigorous long-time stability analysis.

Method: Fourier spectral collocation in space combined with a two-stage explicit exponential Runge-Kutta (ERK2) method in time. The analysis proves uniform-in-time boundedness in discrete H¹ and H² norms under mild time-step conditions, with ℓ∞ bounds via discrete Sobolev embedding.

Result: The method preserves original energy dissipation property, establishes unconditional energy dissipation for fully discrete scheme, removes typical boundedness assumptions, and derives optimal-order error estimate in ℓ² norm based on uniform boundedness.

Conclusion: The analytical framework successfully proves long-time stability of ERK2 for Cahn-Hilliard equation, establishes unconditional energy dissipation, and provides optimal error estimates. The framework is generalizable to higher-order exponential integrators for broader phase-field models.

Abstract: In this paper, we present a comprehensive long-time stability analysis of a second-order explicit exponential Runge--Kutta (ERK2) method for the Cahn--Hilliard (CH) equation. By employing Fourier spectral collocation in space and a two-stage ERK2 scheme in time, we construct a fully discrete numerical method that preserves the original energy dissipation property. The uniform-in-time boundedness of the numerical solution is rigorously proven in the discrete $H^1$ and $H^2$ norms under a mild time-step condition, and an $\ell^\infty$ bound is derived via a discrete Sobolev embedding. These results remove the typical boundedness assumption required in previous energy-stability analyses, thereby establishing unconditional energy dissipation for the fully discrete scheme. Building on this uniform boundedness, we derive an optimal-order error estimate in the $\ell^2$ norm. The analytical framework developed herein is general and can be extended to higher-order exponential integrators for a broader class of phase-field models.

</details>


### [7] [Divergence-free decoupled finite element methods for incompressible flow problems](https://arxiv.org/abs/2512.05642)
*Volker John,Xu Li,Christian Merdon*

Main category: math.NA

TL;DR: This paper proposes divergence-free finite element methods for incompressible flows that decouple velocity and pressure using H(div)-conforming elements, building on previous work by John et al.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient numerical methods for incompressible flow problems by decoupling velocity and pressure computations, which are typically coupled in traditional approaches, thereby reducing computational complexity.

Method: The authors propose H(div)-conforming finite element methods that construct divergence-free basis functions, allowing velocity and pressure to be decoupled. They address algorithmic issues including basis computation and non-homogeneous Dirichlet boundary condition imposition.

Result: Numerical studies on 2D and 3D Stokes problems demonstrate the efficiency of the proposed methods compared to the coupled methods from the referenced paper by John et al.

Conclusion: The proposed divergence-free finite element methods successfully decouple velocity and pressure computations while maintaining efficiency, offering a practical alternative to traditional coupled approaches for incompressible flow problems.

Abstract: Incompressible flows are modeled by a coupled system of partial differential equations for velocity and pressure, Starting from a divergence-free mixed method proposed in [John, Li, Merdon and Rui, Math. Models Methods Appl. Sci. 34(05):919--949, 2024], this paper proposes $\vecb{H}(\mathrm{div})$-conforming finite element methods which decouple the velocity and pressure by constructing divergence-free basis functions. Algorithmic issues like the computation of this basis and the imposition of non-homogeneous Dirichlet boundary conditions are discussed. Numerical studies at two- and three-dimensional Stokes problems compare the efficiency of the proposed methods with methods from the above mentioned paper.

</details>


### [8] [Inexact Uzawa-Double Deep Ritz Method for Weak Adversarial Neural Networks](https://arxiv.org/abs/2512.05673)
*Emin Benny-Chacko,Ignacio Brevis,Luis Espath,Kristoffer G. van der Zee*

Main category: math.NA

TL;DR: The paper introduces Uzawa Deep Double Ritz method, a mesh-free deep PDE solver using neural networks for both trial and test functions with inexact Uzawa iterations and convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: To address stability issues in weak adversarial neural network PDE solvers where residual minimization in dual norms leads to saddle-point problems whose stability depends on iterative schemes.

Method: Develops an inexact Uzawa methodology where both trial and test functions are represented by neural networks and updated approximately. Introduces Uzawa Deep Double Ritz method with continuous level convergence analysis.

Result: Theoretical analysis shows overall iteration remains stable and convergent if inexact inner updates move in correct descent direction. Numerical experiments validate theoretical findings and demonstrate practical robustness and accuracy.

Conclusion: The proposed mesh-free deep PDE solver provides stable and convergent solution method for PDEs using neural networks with theoretical guarantees and practical effectiveness.

Abstract: The emergence of deep learning has stimulated a new class of PDE solvers in which the unknown solution is represented by a neural network. Within this framework, residual minimization in dual norms -- central to weak adversarial neural network approaches -- naturally leads to saddle-point problems whose stability depends on the underlying iterative scheme. Motivated by this structure, we develop an inexact Uzawa methodology in which both trial and test functions are represented by neural networks and updated only approximately. We introduce the Uzawa Deep Double Ritz method, a mesh-free deep PDE solver equipped with a continuous level convergence showing that the overall iteration remains stable and convergent provided the inexact inner updates move in the correct descent direction. Numerical experiments validate the theoretical findings and demonstrate the practical robustness and accuracy of the proposed approach.

</details>


### [9] [Optimal Time-Adaptivity for Parabolic Problems with applications to Model Order Reduction](https://arxiv.org/abs/2512.05676)
*Michael Feischl,Fernando Henríquez,David Niederkofler*

Main category: math.NA

TL;DR: First provably rate-optimal adaptive time stepping method for non-stationary PDEs using Radau IIA method and reduced basis approach with Laplace transform.


<details>
  <summary>Details</summary>
Motivation: Previous optimality proofs for adaptive mesh refinement were limited to stationary PDEs due to lack of coercive structure in time-dependent problems. Recent work showed adaptive Crank-Nicolson for heat equation is optimal under severe restrictions, motivating a more general solution.

Method: Combines new quasi-orthogonality approach (via inf-sup stability equivalence) with Radau IIA time integration method (blending Crank-Nicolson and implicit Euler advantages). Uses reduced basis method with Laplace transform to create tailored low-dimensional subspaces.

Result: First adaptive time stepping method for non-stationary PDEs with provable rate optimality in terms of number of time steps vs. approximation error. Achieves efficiency through reduced basis approach.

Conclusion: Breakthrough in adaptive time stepping theory: achieves provable optimality for non-stationary PDEs by combining novel quasi-orthogonality analysis with Radau IIA method and Laplace-transform-based model reduction.

Abstract: Since the first optimality proofs for adaptive mesh refinement algorithms in the early 2000s, the theory of optimal mesh refinement for PDEs was inherently limited to stationary problems. The reason for this is that time-dependent problems usually do not exhibit the necessary coercive structure that is used in optimality proofs to show a certain quasi-orthogonality, which is crucial for the theory. Recently, by using a new equivalence between quasi-orthogonality and inf-sup stability of the underlying problem, it was shown that an adaptive Crank-Nicolson scheme for the heat equation is optimal under a severe step size restriction. In this work, we use this new approach towards quasi-orthogonality together with a Radau IIA method that combines the advantages of the Crank-Nicolson and implicit Euler schemes. We obtain the first adaptive time stepping method for non-stationary PDEs that is provably rate optimal with respect to number of time steps vs. approximation error. Together with a reduced basis method that leverages the Laplace transform for building tailored subspaces of reduced dimension, we obtain a very efficient method.

</details>


### [10] [Interpretation of a Discrete de Rham method as a Finite Element System](https://arxiv.org/abs/2512.05912)
*Snorre H. Christiansen,Francesca Rapetti*

Main category: math.NA

TL;DR: DDR method can be interpreted as defining a computable consistent discrete L² product on conforming FES defined by PDEs, providing alternative analysis approach without modifying the method itself.


<details>
  <summary>Details</summary>
Motivation: To provide an alternative analytical perspective on the DDR (Discrete De Rham) method by interpreting it through the lens of finite element systems (FES) and discrete L² products, potentially strengthening conformity and consistency properties.

Method: Interpret DDR method as defining a computable consistent discrete L² product on conforming FES defined by PDEs. This approach allows analyzing DDR without modifying the numerical method itself, leveraging general FES framework.

Result: Proves stronger conformity and consistency properties than previously shown. Demonstrates ability to recover existing DDR results from general FES context. Extends discussion to include Virtual Element Method (VEM) connections.

Conclusion: The FES-based interpretation of DDR provides a powerful alternative analytical framework that yields stronger theoretical results while maintaining the same numerical implementation, and reveals connections with VEM methodology.

Abstract: We show that the DDR method can be interpreted as defining a computable consistent discrete $\mathrm{L}^2$ product on a conforming FES defined by PDEs. Without modifying the numerical method itself, this point of view provides an alternative approach to the analysis. The conformity and consistency properties we prove are stronger than those previously shown. We can also recover some of the other results that have been proved about DDR, from those that have already been proved, in principle, in the general context of FES. We also bring VEM, the Virtual Element Method, into the discussion.

</details>


### [11] [A Discontinuous Galerkin Consistent Splitting Method for the Incompressible Navier-Stokes Equations](https://arxiv.org/abs/2512.05919)
*Dominik Still,Natalia Nebulishvili,Richard Schussnig,Katharina Kormann,Martin Kronbichler*

Main category: math.NA

TL;DR: A discontinuous Galerkin method for incompressible Navier-Stokes equations using consistent splitting scheme that enforces divergence-free constraint implicitly, eliminates pressure boundary layers, and achieves optimal convergence with decoupled velocity-pressure solution.


<details>
  <summary>Details</summary>
Motivation: To develop a DG method that removes velocity-pressure compatibility conditions, eliminates pressure boundary layers, allows consistent boundary conditions for open/traction boundaries, and avoids splitting error limitations on temporal accuracy.

Method: Discontinuous Galerkin discretization of Liu's consistent splitting scheme with symmetric interior penalty method for spatial derivatives, semi-implicit convective treatment, Leray projection with divergence/normal continuity penalties, and appropriate flux selection for divergence operators.

Result: The method achieves optimal convergence rates in both space and time, is compatible with higher-order time integration, and successfully handles practical flow problems including 2D cylinder flow and 3D Taylor-Green vortex.

Conclusion: The proposed DG method provides an efficient, accurate approach for incompressible flows with implicit divergence enforcement, decoupled velocity-pressure solution, and applicability to practical flow problems without splitting error limitations.

Abstract: This work presents the discontinuous Galerkin discretization of the consistent splitting scheme proposed by Liu [J. Liu, J. Comp. Phys., 228(19), 2009]. The method enforces the divergence-free constraint implicitly, removing velocity--pressure compatibility conditions and eliminating pressure boundary layers. Consistent boundary conditions are imposed, also for settings with open and traction boundaries. Hence, accuracy in time is no longer limited by a splitting error.
  The symmetric interior penalty Galerkin method is used for second spatial derivatives. The convective term is treated in a semi-implicit manner, which relaxes the CFL restriction of explicit schemes while avoiding the need to solve nonlinear systems required by fully implicit formulations. For improved mass conservation, Leray projection is combined with divergence and normal continuity penalty terms.
  By selecting appropriate fluxes for both the divergence of the velocity field and the divergence of the convective operator, the consistent pressure boundary condition can be shown to reduce to contributions arising solely from the acceleration and the viscous term for the $L^2$ discretization. Per time step, the decoupled nature of the scheme with respect to the velocity and pressure fields leads to a single pressure Poisson equation followed by a single vector-valued convection-diffusion-reaction equation. We verify optimal convergence rates of the method in both space and time and demonstrate compatibility with higher-order time integration schemes. A series of numerical experiments, including the two-dimensional flow around a cylinder benchmark and the three-dimensional Taylor--Green vortex problem, verify the applicability to practically relevant flow problems.

</details>


### [12] [Qualitative and Quantitative Analysis of Riemannian Optimization Methods for Ground States of Rotating Multicomponent Bose-Einstein Condensates](https://arxiv.org/abs/2512.05939)
*Martin Hermann,Tatjana Stykel,Mahima Yadav*

Main category: math.NA

TL;DR: Riemannian optimization methods for computing ground states of rotating multicomponent Bose-Einstein condensates, with convergence analysis for gradient descent on quotient manifolds.


<details>
  <summary>Details</summary>
Motivation: Need to compute ground states of rotating multicomponent Bose-Einstein condensates (minimizers of Gross-Pitaevskii energy functional) while handling phase invariance non-uniqueness issues.

Method: Develop Riemannian optimization on quotient manifolds to resolve phase invariance; introduce auxiliary phase-aligned iteration; analyze Riemannian gradient descent with two specialized metrics (energy-adaptive and Lagrangian-based).

Result: Established unified local convergence framework with explicit rates; energy-adaptive method shows monotone energy decay and global convergence; Lagrangian-based method achieves faster local convergence by incorporating second-order information.

Conclusion: Riemannian optimization effectively computes ground states of rotating BECs; Lagrangian-based method offers superior local convergence despite lacking global convergence guarantees; numerical experiments validate theoretical analysis.

Abstract: We develop and analyze Riemannian optimization methods for computing ground states of rotating multicomponent Bose-Einstein condensates, defined as minimizers of the Gross-Pitaevskii energy functional. To resolve the non-uniqueness of ground states induced by phase invariance, we work on a quotient manifold endowed with a general Riemannian metric. By introducing an auxiliary phase-aligned iteration and employing fixed-point convergence theory, we establish a unified local convergence framework for Riemannian gradient descent methods and derive explicit convergence rates. Specializing this framework to two metrics tailored to the energy landscape, we study the energy-adaptive and Lagrangian-based Riemannian gradient descent methods. While monotone energy decay and global convergence are established only for the former, a quantified local convergence analysis is provided for both methods. Numerical experiments confirm the theoretical results and demonstrate that the Lagrangian-based method, which incorporates second-order information on the energy functional and mass constraints, achieves faster local convergence than the energy-adaptive scheme.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [The modified Camassa-Holm equation on the half line: a Riemann--Hilbert approach](https://arxiv.org/abs/2512.05274)
*Iryna Karpenko,Dmitry Shepelsky*

Main category: math.AP

TL;DR: The paper analyzes the initial-boundary value problem for the modified Camassa-Holm equation on the half-line using Riemann-Hilbert factorization approach.


<details>
  <summary>Details</summary>
Motivation: To solve the modified Camassa-Holm (mCH) equation on a half-line domain with both initial and boundary conditions, which requires developing appropriate mathematical techniques for such boundary value problems.

Method: Uses Riemann-Hilbert factorization approach - characterizes the solution in terms of a matrix Riemann-Hilbert problem in the complex spectral parameter plane. The RH problem data are determined by spectral functions associated with initial and boundary values.

Result: Provides a complete characterization of the solution to the IBV problem through the Riemann-Hilbert formulation, including compatibility conditions for the spectral functions derived from initial and boundary data.

Conclusion: The Riemann-Hilbert approach successfully solves the initial-boundary value problem for the modified Camassa-Holm equation on the half-line, with spectral compatibility conditions ensuring well-posedness.

Abstract: We consider the initial-boundary value (IBV) problem for the modified Camassa--Holm (mCH) equation $
\tilde m_t+\left((\tilde u^2-\tilde u_x^2+2\tilde u)\tilde m\right)_x = 0$, $\tilde m:=\tilde u-\tilde u_{xx}+1$ on the half line
  $x \ge 0$. We provide a characterization of the solution of the IBV
  problem in terms of the solution of a matrix Riemann--Hilbert (RH) factorization
  problem in the complex plane of the spectral parameter. The data of this RH problem
  are determined in terms of spectral functions associated with the initial and boundary values
  of the solution, whose compatibility is characterized in spectral terms.

</details>


### [14] [Blow-up suppression of the Patlak-Keller-Segel-Navier-Stokes system via Taylor-Couette flow](https://arxiv.org/abs/2512.05344)
*Shikun Cui,Lili Wang,Wendong Wang*

Main category: math.AP

TL;DR: The paper analyzes how strong Taylor-Couette flow prevents blow-up in a biological transport system modeling platelet accumulation and thrombosis in medical devices.


<details>
  <summary>Details</summary>
Motivation: Motivated by Taylor-Couette flow in extracorporeal circulation devices where vortex flow reduces platelet adhesion and protein adsorption, preventing thrombosis analogous to mathematical blow-up phenomena.

Method: Mathematical analysis of the 2D Patlak-Keller-Segel-Navier-Stokes system in an annular domain with Taylor-Couette flow, proving global boundedness of solutions without smallness restrictions when flow strength parameter A is large.

Result: Proves that sufficiently strong Taylor-Couette flow prevents blow-up, ensuring globally bounded solutions regardless of initial cell mass or velocity magnitude when flow parameter A is sufficiently large.

Conclusion: Strong flow can stabilize biological transport systems and prevent blow-up phenomena, providing mathematical justification for the observed thrombosis prevention in medical devices using Taylor-Couette flow.

Abstract: Motivated by the use of Taylor-Couette flow in extracorporeal circulation devices [K$\ddot{\rm o}$rfer et al., 2003, 26(4): 331-338], where it leads to an accumulation of platelets and plasma proteins in the vortex center and therefore to a decreased probability of contact between platelets and material surfaces and its protein adsorption per square unit is significantly lower than laminar flow. Increased platelet adhesion or protein adsorption on the device surface can induce platelet aggregation or thrombosis, which is analogous to the ``blow-up phenomenon" in mathematical modeling. Here we mathematically analyze this stability mechanism and demonstrate that sufficiently strong flow can prevent blow-up from occurring. In details, we investigate the two-dimensional Patlak-Keller-Segel-Navier-Stokes system in an annular domain around a Taylor-Couette flow $U(r,θ)=A\big(r+\frac{1}{r} \big)(-\sinθ, \cosθ)^{T}$ with $(r,θ)\in[1,R]\times\mathbb{S}^{1}$, and prove that the solutions are globally bounded without any smallness restriction on the initial cell mass or velocity when $A$ is large.

</details>


### [15] [Green functions, Hitchin's formula and curvature equations on tori II: Rectangular torus](https://arxiv.org/abs/2512.05360)
*Zhijie Chen,Erjuan Fu,Chang-Shou Lin*

Main category: math.AP

TL;DR: The paper studies critical points of the sum of two Green functions on rectangular tori, showing they have either 0 or 2 nontrivial critical points depending on the parameter p, with applications to Painlevé VI and curvature equations.


<details>
  <summary>Details</summary>
Motivation: To understand the critical point structure of the sum of two Green functions on rectangular tori, building on previous work by Lin and Wang on single Green functions and extending the analysis from Part I to the rectangular case.

Method: Develops a completely different approach from Part I, using Weierstrass ℘-function to parameterize the problem and establishing threshold values d₁,...,d₈ that determine the existence of nontrivial critical points.

Result: For rectangular tori (τ=ib), there exist 8 threshold values d₁<...<d₈ such that: if ℘(p) lies in certain intervals, G_p(z) has no nontrivial critical points; if ℘(p) lies in complementary intervals, G_p(z) has exactly one pair of nontrivial critical points that are non-degenerate saddle points.

Conclusion: The paper provides a complete characterization of critical points for the sum of two Green functions on rectangular tori, with applications to Painlevé VI equations and curvature problems, establishing precise conditions for existence and uniqueness of nontrivial critical points.

Abstract: Let $G(z)$ be the Green function on the flat torus $E_τ=\mathbb{C}/(\mathbb{Z}+\mathbb{Z}τ)$ with the singularity at $0$. Lin and Wang (Ann. Math. 2010) proved that $G(z)$ has either $3$ or $5$ critical points (depending on the choice of $τ$). Here we study the sum of two Green functions which can be reduced to $G_p(z):=\frac12(G(z+p)+G(z-p))$. In Part I \cite{CFL}, we proved that for any $p$ satisfying $p\neq -p$ in $E_τ$, the number of critical points of $G_p(z)$ belongs to $\{4,6,8,10\}$ (depending on the choice of $(τ, p)$) and each number really occurs.
  In the Part II of this series, we study the important case $τ=ib$ with $b>0$, i.e. $E_τ$ is a rectangular torus. By developing a completely different approach from Part I, we show the existence of $8$ real values $d_1<d_2<\cdots<d_7<d_8$ such that if $$\wp(p)\in (-\infty, d_1]\cup [d_2, d_3]\cup [d_4, d_5]\cup [d_6, d_7]\cup [d_8,+\infty),$$ then $G_p(z)$ has no nontrivial critical points; if $$\wp(p)\in (d_1, d_2)\cup (d_3, d_4)\cup (d_5, d_6)\cup (d_7, d_8),$$ then $G_p(z)$ has a unique pair of nontrivial critical points that are always non-degenerate saddle points. This allows us to study the possible distribution of the numbers of critical points of $G_p(z)$ for generic $p$. Applications to the Painlevé VI equation and the curvature equation are also given.

</details>


### [16] [A new proof of the Théorème de Structure related to a weak solution to the Navier-Stokes equations](https://arxiv.org/abs/2512.05598)
*Paolo Maremonti,Filippo Palma*

Main category: math.AP

TL;DR: New proof of the Théorème de Structure for Leray's weak solutions to Navier-Stokes equations using a priori estimates on approximating sequences, extending to more general settings including Hopf's weak solutions.


<details>
  <summary>Details</summary>
Motivation: To provide a new, more general proof of the partial regularity result (Théorème de Structure) for Leray's weak solutions to Navier-Stokes equations that covers broader settings, including Hopf's weak solutions for IBVP in bounded domains without requiring strong energy inequalities.

Method: Develops a new proof based on a priori estimates for a suitable approximating sequence, specifically employing a priori estimates on the Galerkin approximation rather than requiring energy inequalities in strong form.

Result: Achieves the Théorème de Structure (partial regularity result) through the new proof approach, extending the result to more general settings including weak solutions furnished by Hopf for IBVP in bounded domains.

Conclusion: The new proof based on a priori estimates provides a more general framework for establishing the Théorème de Structure, covering cases where traditional strong energy inequality approaches may not apply, particularly for Hopf's weak solutions in bounded domains.

Abstract: It is well known that a Leray's weak solution to the Navier-Stokes Cauchy problem enjoys a partial regularity which is known in the literature as the Théorème de Structure of a Leray's weak solution. As well, this result has been extended by some authors to the case of the IBVP. In this note, we achieve the Théorème de Structure by means of a new proof. Our proof is based on a priori estimates for a suitable approximating sequence. In this way our result covers a more general setting in the sense that, e.g., we can also include the case of the weak solutions furnished by Hopf for an IBVP in bounded domains without requiring an energy inequality in a strong form, but just employing a priori estimates on the Galerkin approximation.

</details>


### [17] [Resolvent trace asymptotics for operators in the Shubin class](https://arxiv.org/abs/2512.05689)
*Jörg Seiler*

Main category: math.AP

TL;DR: New Shubin-type pseudodifferential calculus with parameter-dependent operators; resolvents constructed and trace expansion derived.


<details>
  <summary>Details</summary>
Motivation: To develop a more general pseudodifferential calculus that includes both parameter-dependent and parameter-independent operators of Shubin type, enabling analysis of resolvents and their spectral properties.

Method: Introduces a new pseudodifferential calculus of Shubin type that incorporates operators depending on a non-negative real parameter alongside traditional parameter-independent operators. Constructs resolvents of these Shubin-type pseudodifferential operators.

Result: Successfully constructs resolvents for Shubin-type pseudodifferential operators and obtains their trace expansion, providing spectral analysis tools for this extended calculus.

Conclusion: The new calculus extends Shubin-type pseudodifferential operator theory to include parameter-dependent operators, with resolvent construction and trace expansion establishing fundamental spectral analysis capabilities.

Abstract: A new pseudodifferential calculus of Shubin type is introduced. The calculus contains operators depending on a non negative real parameter as well as operators independent of the parameter. Resolvents of Shubin type pseudodifferential operators are constructed and their trace expansion is obtained.

</details>


### [18] [Asymptotic Behavior of Rupture Solutions for the Elliptic MEMS Equation with Hénon-Type Term](https://arxiv.org/abs/2512.05743)
*Yunxiao Li*

Main category: math.AP

TL;DR: The paper studies rupture solutions for an elliptic MEMS equation with Hénon-type term, focusing on solutions that vanish at the origin, analyzing their asymptotic behavior near the rupture point, and proving existence of such solutions.


<details>
  <summary>Details</summary>
Motivation: To understand rupture solutions in elliptic MEMS equations with Hénon-type terms, particularly solutions that vanish at specific points (rupture points), which is important for analyzing failure modes and singular behavior in micro-electromechanical systems.

Method: Analyzes asymptotic behavior near the rupture point (origin) for different Hénon-type exponents α, derives full asymptotic expansions of arbitrary order, and proves existence of rupture solutions through constructive methods for both radial solutions and non-radial solutions with asymptotic radial conditions.

Result: Obtains complete asymptotic expansions for rupture solutions near the origin, characterizes behavior based on Hénon exponent α, and establishes existence of rupture solutions through constructive proofs for both radial and asymptotically radial cases.

Conclusion: The paper provides comprehensive analysis of rupture solutions in elliptic MEMS equations with Hénon-type terms, including detailed asymptotic behavior near rupture points and existence proofs, contributing to understanding of singular solutions in MEMS applications.

Abstract: For an elliptic MEMS equation with Hénon-type term, $Δu = λ|x|^α/u^{p} + F$, we study rupture solutions, i.e. solutions for which $u(x_0)=0$ at some point $x_0$, also $x_0$ is called a rupture point. In this paper we focus on the special case where the rupture occurs at the origin. According to the different Hénon-type exponents $α$, we analyze the asymptotic behavior of such solutions near the origin and derive a full asymptotic expansion of arbitrary order in a neighborhood of the origin. Moreover, for both radial solutions and non-radial solutions with asymptotic radial condition, we prove the existence of rupture solutions near the rupture point by constructing it.

</details>


### [19] [Bifurcation from bubbles in nonconvex cones](https://arxiv.org/abs/2512.05766)
*Filomena Pacella,Camilla Chiara Polvara,Luigi Provenzano*

Main category: math.AP

TL;DR: The paper studies symmetry breaking in critical semilinear elliptic equations in nonconvex cones, proving existence of nonradial solutions bifurcating from standard bubbles when a Neumann eigenvalue threshold is crossed.


<details>
  <summary>Details</summary>
Motivation: To understand symmetry breaking phenomena in critical semilinear elliptic equations in nonconvex cones, where radial solutions (standard bubbles) may lose stability and give rise to nonradial solutions.

Method: Construct a one-parameter family of spherical domains whose first Neumann eigenvalue crosses a critical threshold, then use Crandall-Rabinowitz bifurcation theorem (assuming eigenvalue simplicity) to prove existence of nonradial solution branches bifurcating from standard bubbles.

Result: Proves existence of a branch of nonradial solutions bifurcating from standard bubbles in nonconvex cones when the first nonzero Neumann eigenvalue of the Laplace-Beltrami operator crosses a stability threshold, and shows the bifurcation is global.

Conclusion: Symmetry breaking occurs in nonconvex cones for critical semilinear elliptic equations, with nonradial solutions emerging via bifurcation from standard bubbles when appropriate eigenvalue conditions are met, extending understanding beyond convex cones where only radial solutions exist.

Abstract: We investigate the Neumann problem for the critical semilinear elliptic equation in cones. The standard bubble provides a family of radial solutions, which are known to be the only positive solutions in convex cones. For nonconvex cones, symmetry breaking may occur and the symmetry breaking is related to the first nonzero Neumann eigenvalue of the Laplace Beltrami operator on the domain $D\subset§^{N-1}$, that spans the cone. We construct a one-parameter family of domains on the sphere whose first eigenvalue crosses the threshold at which the bubble loses stability. Under the assumption that this eigenvalue is simple, we prove, via the Crandall Rabinowitz bifurcation theorem, the existence of a branch of nonradial solutions bifurcating from the standard bubble. Moreover we show that the bifurcation is global.

</details>


### [20] [On the formation of microstructure and the occurrence of vortices in a singularly perturbed energy related to helimagnetism: a scaling law result](https://arxiv.org/abs/2512.05821)
*Janusz Ginster*

Main category: math.AP

TL;DR: Analysis of singularly perturbed energies in discrete J1-J3 models showing scaling laws for minimal energy and demonstrating that minimizers develop vortices in certain parameter regimes.


<details>
  <summary>Details</summary>
Motivation: To study singularly perturbed energies in discrete J1-J3 models where admissible fields are not necessarily gradient fields, and their curl is linked to topological singularities (vortices), addressing incompatible boundary conditions and lack of rigidity in existing models.

Method: Analysis of energies with non-convex bulk term and higher-order regularizing term under incompatible boundary conditions. Development of modified ball-construction technique that simultaneously considers both bulk energy and regularizing term due to lack of rigidity.

Result: Derived scaling law for minimal energy with respect to three parameters: incompatibility of boundary conditions, strength of regularizing term, and interatomic distance. Showed that in certain parameter regimes, minimizers necessarily develop vortices.

Conclusion: The work establishes fundamental scaling laws for singularly perturbed energies in discrete J1-J3 models and demonstrates the emergence of vortices as necessary features of minimizers in specific parameter regimes, providing insights into topological singularities in such systems.

Abstract: In this work, singularly perturbed energies arising from discrete $J_1$-$J_3$-models are studied. The energies under consideration consist of a non-convex bulk term and a higher-order regularizing term and are subject to incompatible boundary conditions. In contrast to existing results in the literature, in this work, admissible fields are not necessarily gradient fields, instead their curl is linked to topological singularities, so-called vortices, in the discrete $J_1$-$J_3$-model. The main result of this work is a scaling law for the minimal energy with respect to three parameters: one measuring the incompatibility of the boundary conditions, the second measuring the strength of the regularizing term, and the third being related to the interatomic distance in the discrete model. The shown result implies in particular that in certain parameter regimes, minimizers necessarily develop vortices. A key tool in the analysis is a careful modification of the celebrated ball-construction technique that, due to a lack of rigidity, considers simultaneously both the bulk energy and the regularizing term.

</details>


### [21] [Entropy and Fisher information in non-convex domains: one chain to rule them all](https://arxiv.org/abs/2512.05826)
*Jean-Baptiste Casteras,Marco Flaim,Léonard Monsaingeon*

Main category: math.AP

TL;DR: The paper proves Fisher information is a strong Wasserstein upper gradient for entropy on non-convex Riemannian domains, eliminating need for λ-displacement convexity arguments.


<details>
  <summary>Details</summary>
Motivation: To fill a gap in the literature by establishing Fisher information as a strong Wasserstein upper gradient of entropy without requiring λ-displacement convexity, which has been a limitation in previous approaches.

Method: The authors prove the Fisher information functional is a strong Wasserstein upper gradient on non-convex Riemannian domains. They establish novel quantitative short-time control of Fisher information along Neumann heat flow and develop an exact chain rule under AC₂ assumptions satisfied by JKO scheme limits.

Result: Successfully proves Fisher information is a strong Wasserstein upper gradient for entropy on non-convex Riemannian domains, enabling complete dispensing of λ-displacement convexity arguments in gradient flow analysis.

Conclusion: This work provides a more general framework for analyzing gradient flows in Wasserstein spaces by removing the convexity requirement, with applications to JKO schemes and heat flow analysis on Riemannian domains.

Abstract: We prove that the (square root) Fisher information functional is a strong Wasserstein upper gradient of the entropy on non-convex Riemannian domains. This fills a gap in the literature by allowing one to completely dispense from $λ$-displacement convexity arguments. Along the way we establish a novel quantitative short-time control of the Fisher information along the Neumann heat flow, and establish an exact chain rule under stronger $AC_2$ assumptions typically satisfied by curves of measures obtained as limits of JKO schemes.

</details>


### [22] [Higher-order diffusion and Cahn-Hilliard-type models revisited on the half-line](https://arxiv.org/abs/2512.05829)
*A. Chatziafratis,A. Miranville,G. Karali,A. S. Fokas,E. C. Aifantis*

Main category: math.AP

TL;DR: The paper develops rigorous solution methods for fourth-order diffusion-type PDEs using the Fokas unified transform method, analyzing well-posedness, regularity, and asymptotic properties.


<details>
  <summary>Details</summary>
Motivation: To address the need for rigorous solution methods for fourth-order variations of diffusion equations and linearized Cahn-Hilliard models, which have applications in heat-mass transfer, solid-fluid dynamics, and applied sciences.

Method: Combines the Fokas unified transform method with a new rigorous analysis framework for linear evolution PDEs on semi-infinite strips, extending previous work from third-order to fourth-order equations.

Result: Derived explicit solution representations with rigorous justification, analyzed solution regularity and asymptotic properties near boundaries, demonstrated uniform convergence and eventual periodicity, and constructed a new counter-example for solution uniqueness.

Conclusion: The developed solution formulae provide rigorous foundations for studying fourth-order diffusion-type PDEs and are expected to be useful for well-posedness studies of nonlinear counterparts.

Abstract: In this paper, we solve explicitly and analyze rigorously inhomogeneous initial-boundary-value problems (IBVP) for several fourth-order variations of the traditional diffusion equation and the associated linearized Cahn-Hilliard (C-H) model (also Kuramoto-Sivashinsky equation), formulated in the spatiotemporal quarter-plane. Such models are of relevance to heat-mass transfer phenomena, solid-fluid dynamics and the applied sciences. In particular, we derive formally effective solution representations, justifying a posteriori their validity. This includes the reconstruction of the prescribed initial and boundary data, which requires careful analysis of the various integral terms appearing in the formulae, proving that they converge in a strictly defined sense. In each IBVP, the novel formula is utilized to rigorously deduce the solution's regularity and asymptotic properties near the boundaries of the domain, including uniform convergence, eventual (long-time) periodicity under (eventually) periodic boundary conditions, and null non-controllability. Importantly, this analysis is indispensable for exploring the (non)uniqueness of the problem's solution and a new counter-example is constructed. Our work is based on the synergy between: (i) the well-known Fokas unified transform method and (ii) a new approach recently introduced for the rigorous analysis of the Fokas method and for investigating qualitative properties of linear evolution partial differential equations (PDE) on semi-infinite strips. Since only up to third-order evolution PDE have been investigated within this novel framework to date, we present our analysis and results in an illustrative manner and in order of progressively greater complexity, for the convenience of readers. The solution formulae established herein are expected to find utility in well-posedness studies for nonlinear counterparts too.

</details>


### [23] [Group Classification (1+2)-dimensional Linear Equation of Asian Options Pricing](https://arxiv.org/abs/2512.05963)
*Stanislav V. Spichak,Valeriy I. Stogniy,Inna M. Kopas*

Main category: math.AP

TL;DR: Group classification of (1+2)-dimensional linear PDEs for Asian options pricing, identifying maximum 8-dimensional Lie algebra symmetry and transformation to Kolmogorov equation.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze symmetry properties of PDEs used in Asian options pricing models, enabling transformation to simpler forms and construction of exact solutions.

Method: Lie group classification approach applied to class of (1+2)-dimensional linear PDEs; identification of maximum 8-dimensional Lie invariance algebra; use of point transformations to map equations to linear Kolmogorov equation; symmetry reduction using invariance algebra operators.

Result: Maximum dimension Lie invariance algebra is eight-dimensional; equations with such algebra can be transformed to linear Kolmogorov equation via point transformations; invariant exact solutions constructed for some equations through symmetry reduction.

Conclusion: Lie symmetry analysis provides systematic framework for studying Asian options pricing PDEs, enabling transformation to canonical forms and construction of exact solutions through symmetry reduction techniques.

Abstract: We consider a class of (1+2)-dimensional linear partial differential of Asian options pricing. Special cases have been used to models of financial mathematics. We carry out group classification of a class equations. In particular, the maximum dimension Lie invariance algebra within the above class is eight-dimensional. It is shown that an equation with such an algebra can be transformed into the linear Kolmogorov equation with the help of the point transformations of variables. Using the operators of invariance algebra symmetry reduction is carried out and invariant exact solutions are constructed for some equations.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [24] [A Conservative Discontinuous Galerkin Algorithm for Particle Kinetics on Smooth Manifolds](https://arxiv.org/abs/2512.05298)
*Grant Johnson,Ammar Hakim,James Juno*

Main category: physics.comp-ph

TL;DR: A conservative discontinuous Galerkin method for particle kinetics on manifolds using Hamiltonian formulations, with exact conservation properties and BGK collision operator.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical scheme for simulating particle kinetics on curved manifolds that conserves key physical quantities (density, energy) exactly, with applications to astrophysical and relativistic problems.

Method: Discontinuous Galerkin algorithm using both canonical and non-canonical Hamiltonian formulations for particle motion on manifolds, coupled with BGK collision operator and iterative scheme to preserve collisional invariants.

Result: The method conserves particle density and energy exactly in canonical formulation, handles manifold rotation via modified Hamiltonian, and successfully tests on Sod-shock, Kelvin-Helmholtz instability on sphere and paraboloid surfaces.

Conclusion: The approach provides a robust framework for kinetic simulations on manifolds with exact conservation properties, with potential extension to kinetic theory in general relativity.

Abstract: A novel, conservative discontinuous Galerkin algorithm is presented for particle kinetics on manifolds. The motion of particles on the manifold is represented using using both canonical and non-canonical Hamiltonian formulations. Our schemes apply to either formulations, but the canonical formulation results in a particularly efficient scheme that also conserves particle density and energy exactly. The collisionless update is coupled to a Bhatnagar-Gross-Krook (BGK) collision operator that provides a simplified model for relaxation to local thermodynamic equilibrium. An iterative scheme is constructed to ensure collisional invariants (density, momentum and energy) are preserved numerically. Rotation of the manifold is incorporated by modifying the Hamiltonian while ensuring a canonical formulation. Several test problems, including a kinetic version of the classical Sod-shock problem, Kelvin-Helmholtz instability on the surfaces of a sphere and a paraboloid, with and without rotations, is presented. A prospectus for further development of this approach to simulation of kinetic theory in general relativity is presented.

</details>


### [25] [Hypothesis-Based Particle Detection for Accurate Nanoparticle Counting and Digital Diagnostics](https://arxiv.org/abs/2512.05346)
*Neil H. Kim,Xiao-Liu Chu,Joseph B. DeGrandchamp,Matthew R. Foreman*

Main category: physics.comp-ph

TL;DR: A statistical particle counting algorithm for nanoparticle imaging assays that uses multiple-hypothesis testing without training data, validated on SARS-CoV-2 DNA biomarker detection.


<details>
  <summary>Details</summary>
Motivation: Digital assays enable precise detection of low-abundance analytes for early disease diagnosis, but need robust particle counting methods that don't require training data or empirical tuning while maintaining interpretability.

Method: Formulated as a multiple-hypothesis statistical test under an explicit image-formation model, using penalized likelihood rule. No training data or empirical parameter tuning required.

Result: Robust count accuracy across weak signals, variable backgrounds, magnification changes, and moderate PSF mismatch. Applied to SARS-CoV-2 DNA biomarker detection, showing statistically significant differences between control and positive samples with consistent over-dispersion.

Conclusion: The method establishes a reliable framework for nanoparticle-based detection assays in digital molecular diagnostics, providing interpretable outputs through direct links to imaging physics and statistical decision theory.

Abstract: Digital assays represent a shift from traditional diagnostics and enable the precise detection of low-abundance analytes, critical for early disease diagnosis and personalized medicine, through discrete counting of biomolecular reporters. Within this paradigm, we present a particle counting algorithm for nanoparticle based imaging assays, formulated as a multiple-hypothesis statistical test under an explicit image-formation model and evaluated using a penalized likelihood rule. In contrast to thresholding or machine learning methods, this approach requires no training data or empirical parameter tuning, and its outputs remain interpretable through direct links to imaging physics and statistical decision theory.
  Through numerical simulations we demonstrate robust count accuracy across weak signals, variable backgrounds, magnification changes and moderate PSF mismatch. Particle resolvability tests further reveal characteristic error modes, including under-counting at very small separations and localized over-counting near the resolution limit. Practically, we also confirm the algorithm's utility, through application to experimental dark-field images comprising a nanoparticle-based assay for detection of DNA biomarkers derived from SARS-CoV-2. Statistically significant differences in particle count distributions are observed between control and positive samples. Full count statistics obtained further exhibit consistent over-dispersion, and provide insight into non-specific and target-induced particle aggregation. These results establish our method as a reliable framework for nanoparticle-based detection assays in digital molecular diagnostics.

</details>


### [26] [Beyond Adam: Disentangling Optimizer Effects in the Fine-Tuning of Atomistic Foundation Models](https://arxiv.org/abs/2512.05489)
*Xiaoqing Liu,Yangshuai Wang,Teng Zhao*

Main category: physics.comp-ph

TL;DR: Benchmark study of 7 optimization algorithms for fine-tuning atomistic foundation models, finding AdamW and ScheduleFree perform best across molecular, crystalline, and liquid systems.


<details>
  <summary>Details</summary>
Motivation: While fine-tuning is essential for adapting universal interatomic potentials to specific systems, the influence of optimization algorithms on this process remains insufficiently characterized, creating a gap in understanding how different optimizers affect model performance and physical property predictions.

Method: Rigorous benchmark of seven first-order optimizers (Adam, AdamW, RAdam, SGD, LAMB, Ranger, ScheduleFree) for fine-tuning foundation models across molecular, crystalline, and liquid regimes. Evaluation based on energy/force accuracy for in-distribution and out-of-distribution configurations, plus downstream physical properties (elastic moduli, phonon spectra, interfacial dynamics). Analysis through preconditioning framework viewing optimizers as data-dependent linear gradient transformations.

Result: AdamW and ScheduleFree achieve superior curvature conditioning and force accuracy across all regimes. SGD exhibits slow convergence and instability. A brief second-order refinement stage reduces residual anisotropy in loss landscape and enhances physical observable fidelity without increasing inference costs.

Conclusion: The study provides conceptual insight and practical guidance for optimizer selection in fine-tuning universal interatomic potentials, demonstrating how different update rules impose specific spectral filters on the effective loss Hessian and highlighting the importance of optimizer choice for stable and efficient adaptation of foundation models.

Abstract: Atomistic foundation models constitute a paradigm shift in computational materials science by providing universal machine-learned interatomic potentials with broad transferability across chemical spaces. Although fine-tuning is essential for adapting these pretrained models to specific target systems, the influence of the optimization algorithm on this process remains insufficiently characterized. In this work, we perform a rigorous benchmark of seven first-order optimizers, including Adam, AdamW, RAdam, SGD, LAMB, Ranger, and ScheduleFree, for the fine-tuning of foundation models across molecular, crystalline, and liquid regimes. We evaluate these algorithms based on energy and force accuracy for both in-distribution and out-of-distribution configurations, as well as their impact on downstream physical properties such as elastic moduli, phonon spectra, and interfacial dynamics. We interpret these empirical results through a preconditioning framework that views each optimizer as a data-dependent linear transformation of the gradient. This analysis clarifies how different update rules impose specific spectral filters on the effective loss Hessian. Across all regimes, AdamW and ScheduleFree achieve superior curvature conditioning and force accuracy, whereas stochastic gradient descent exhibits slow convergence and instability. Furthermore, we demonstrate that a brief second-order refinement stage reduces residual anisotropy in the loss landscape and enhances the fidelity of physical observables without increasing inference costs. These findings provide conceptual insight and practical guidance for selecting and designing optimizers to ensure the stable and efficient fine-tuning of universal interatomic potentials.

</details>


### [27] [A Continuous Nonlinear Optimization Perspective on the Spin Glass Problem](https://arxiv.org/abs/2512.05852)
*Phil Duxbury,Carlile Lavor,Luiz Leduino de Salles-Neto*

Main category: physics.comp-ph

TL;DR: Continuous nonlinear optimization model for Spin Glass Problem with theoretical guarantee that continuous relaxation yields optimal discrete solutions.


<details>
  <summary>Details</summary>
Motivation: To provide a direct, conceptually transparent continuous formulation for the Spin Glass Problem that can leverage modern global optimization software, bridging statistical physics and combinatorial optimization.

Method: Builds on Rosenberg's classical result about multilinear polynomial problems, showing that for SGP, optimal values of continuous relaxation and discrete model coincide. Provides problem-specific argument to convert any optimal continuous solution to optimal discrete spin configuration.

Result: Computational experiments on standard benchmarks show this approach matches or surpasses recent integer programming linearization techniques, making it a practical complementary tool.

Conclusion: The continuous nonlinear optimization model offers an effective alternative to integer programming methods for Spin Glass Problem, maintaining problem hardness but providing transparent formulation usable with modern optimization software.

Abstract: We present a continuous nonlinear optimization model for the Spin Glass Problem (SGP), building on a classical result by Rosenberg (1972), which shows that for a class of multilinear polynomial problems the optimal values of the continuous relaxation and the corresponding discrete model coincide. Using the SGP as a case study, we provide a simple, problem-specific argument showing how any optimal solution returned by a continuous solver can be converted into an optimal discrete spin configuration, even when the solver outputs non-integer values. The relaxed model remains nonconvex and does not alter the inherent computational hardness of the problem, but it offers a direct and conceptually transparent continuous formulation that can be handled by modern global optimization software. Computational experiments on standard benchmark instances indicate that this approach can match, and in several cases surpass, recent integer programming linearization techniques, making it a practical and complementary tool for researchers working at the interface between statistical physics and combinatorial optimization.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [28] [Comparison of filament properties in real-size GBS simulations and experiments of TCV-X21](https://arxiv.org/abs/2512.05434)
*Y. Wang,C. Wüthrich,C. Theiler,S. García Herreros,D. S. Oliveira,D. Mancini,T. Golfinopoulos,P. Ricci,T. Body,the TCV team*

Main category: physics.plasm-ph

TL;DR: Direct comparison of SOL filament properties between GBS fluid turbulence simulations and TCV tokamak experiments shows good agreement on velocities but discrepancies in sizes and fluctuation levels.


<details>
  <summary>Details</summary>
Motivation: To perform quantitative validation of scrape-off layer (SOL) filament properties from first-principles simulations against experimental measurements, enabling better understanding of turbulent transport in tokamak boundary plasmas.

Method: Extended TCV-X21 dataset with 2D turbulence measurements from Gas Puff Imaging (GPI), analyzed GBS simulations using synthetic GPI diagnostic modeling neutral helium-plasma interaction and line-integration effects.

Result: Good agreement on poloidal/radial filament velocities between simulations and experiments, but simulations overestimate filament sizes and underestimate fluctuation levels. Filaments are predominantly density perturbations, and poloidal velocity direction matches E×B direction at midplane/X-point but not in divertor leg.

Conclusion: The study provides new insights into turbulent filament behavior and guides future improvements in first-principles boundary plasma simulations, highlighting the importance of instantaneous E×B velocity components.

Abstract: A direct quantitative comparison of Scrape-Off Layer (SOL) filament properties from fluid turbulence simulations using the GBS code and from experiments on the TCV tokamak is performed within the TCV-X21 validation case. This comparison is made possible by extending the open TCV-X21 dataset with 2D turbulence measurements obtained with Gas Puff Imaging (GPI), providing critical information on the size, velocity, and other key characteristics of turbulent filaments at the outboard midplane and in the divertor region. For the comparison, GBS simulations of TCV-X21 are analyzed using a dedicated synthetic GPI diagnostic that models the neutral helium-plasma interaction and emission processes and accounts for line-integration effects. Poloidal and radial filament velocities are found to be in good agreement between simulations and experiments, while the simulations overestimate the filament radial and poloidal sizes and underestimate the relative fluctuation levels. The simulations further indicate that filaments in the SOL are predominantly represented by density perturbations rather than temperature perturbations, consistent with previous assumptions in experimental analyses of cross-field turbulent transport from GPI data. The poloidal velocity direction of the filaments agrees with the time-averaged $\boldsymbol{E}\times\boldsymbol{B}$ direction at the outboard midplane and X-point region, but not in the divertor leg. Possible explanations are proposed and discussed, highlighting the influence of the instantaneous $\boldsymbol{E}\times\boldsymbol{B}$ velocity components in both poloidal and radial directions. This study provides new insights into turbulent filament behavior and contributes to guiding future efforts to improve first-principles simulations of the boundary plasma.

</details>


### [29] [Fast electrostatic microinstability evaluation in arbitrary toroidal magnetic geometry using a variational approach](https://arxiv.org/abs/2512.05678)
*M. C. L. Morren,P. Mulholland,J. H. E. Proll,M. J. Pueschel,L. Podavini,D. D. Kiszkiel,J. A. Schuurmans,A. Zocco*

Main category: physics.plasm-ph

TL;DR: Developed a semi-analytical dispersion relation for ITG and TEM instabilities in fusion plasmas using asymptotic expansion, valid for arbitrary toroidal geometry with non-local effects and variational properties.


<details>
  <summary>Details</summary>
Motivation: Small-scale turbulence from microinstabilities limits energy confinement in magnetic fusion devices. Understanding and modeling these instabilities (ITG and TEM) is crucial for predicting and improving plasma confinement.

Method: Developed a semi-analytical dispersion relation based on lowest-order solutions to gyrokinetic equations using asymptotic expansion in frequency ratios. The model accounts for magnetic ion and bounce-averaged electron drifts, non-local effects, arbitrary growth rates and curvature, and satisfies variational properties. Used Padé approximation for FLR effects to reduce computational costs.

Result: The model shows remarkable agreement with baseline dispersion relation at reduced computational costs. Verified against high-fidelity linear gyrokinetic simulations, showing good quantitative agreement for ITGs and TEMs in shaped tokamaks and low-magnetic-shear stellarators.

Conclusion: The developed semi-analytical dispersion relation provides an efficient and accurate tool for studying ITG and TEM instabilities in various magnetic confinement geometries, enabling better understanding and prediction of turbulence-driven transport in fusion plasmas.

Abstract: Small-scale turbulence originating from microinstabilities limits the energy confinement time in magnetic confinement fusion. Here we develop a semi-analytical dispersion relation based on lowest-order solutions to the gyrokinetic equations in an asymptotic expansion in the ratio of transit (bounce) frequency to the mode frequency for ions (electrons), capable of describing two common instabilities: the ion temperature gradient (ITG) mode and trapped-electron mode (TEM), in the electrostatic limit. The dispersion relation, which is valid in arbitrary toroidal geometry, takes into account resonances with the magnetic ion and bounce-averaged electron drifts, incorporates non-local effects along the magnetic field line, is valid for arbitrary sign of the growth rate and magnetic curvature, and is shown to satisfy a variational property. Several common approximation models are introduced for both the magnetic drift and finite Larmor radius (FLR) damping, with the Padé approximation for FLR effect in particular resulting in remarkable agreement with the baseline dispersion relation model at significantly reduced costs. The baseline model is verified by comparing solutions of the dispersion relation model to high-fidelity linear gyrokinetic simulations, where the exact eigenfunction of the electrostatic potential from simulations is used as a trial function, showing good quantitative agreement for ITGs and TEMs in (shaped) tokamaks as well as low-magnetic-shear stellarators.

</details>


### [30] [Modeling the effect of MHD activity on runaway electron generation during SPARC disruptions](https://arxiv.org/abs/2512.05709)
*R Datta,C Clauser,N Ferraro,R Sweeney,R A Tinguely*

Main category: physics.plasm-ph

TL;DR: Self-consistent modeling of runaway electron interactions with MHD instabilities, material injection, and VDEs in SPARC tokamak disruptions shows RE generation increases initially with MHD growth, then plateaus form with Ne-only injection, while D₂+Ne injection prevents steady plateaus via cold VDE termination.


<details>
  <summary>Details</summary>
Motivation: MHD instabilities and runaway electrons interact in complex ways during tokamak disruptions, making self-consistent modeling crucial for accurate RE prediction and mitigation strategy design (like massive gas injection) in high-current devices like SPARC.

Method: Used M3D-C1 extended MHD code with RE fluid model to investigate effects of 3-D nonlinear MHD activity, material injection (Ne and D₂ combinations), and 2-D axisymmetric vertical displacement events on RE evolution during SPARC disruptions.

Result: Ne-only injection (2-5×10²¹ atoms) produces large RE plateaus (>5 MA), while combined D₂+Ne injection yields lower RE current (<2 MA) with post-thermal-quench "cold" VDE terminating RE beam. Self-consistent coupling reveals initial RE increase from MHD growth, decreased saturation energies of m/n=1/1 mode, RE losses in stochastic fields, and subsequent confinement from flux surface re-healing.

Conclusion: First SPARC disruption simulations coupling REs, 3-D MHD instabilities, MGI, and axisymmetric VDEs demonstrate crucial insights into RE generation and mitigation in high-current devices, showing material injection composition significantly affects RE plateau formation and termination.

Abstract: Magnetohydrodynamic (MHD) instabilities and runaway electrons (REs) interact in several ways, making it important to self-consistently model these interactions for accurate predictions of RE generation and the design of mitigation strategies, such as massive gas injection (MGI). Using M3D-C1 - an extended MHD code with a RE fluid model - we investigate the effects of 3-D nonlinear MHD activity, material injection, and 2-D axisymmetric vertical displacement events (VDEs) on RE evolution during disruptions on SPARC - a high-field, high-current tokamak designed to achieve a fusion gain Q > 1. Several cases, comprising different combinations of neon (Ne) and deuterium ($\text{D}_2$) injection, are considered. Our results demonstrate key effects that arise from the self-consistent RE + MHD coupling, such as an initial increase in RE generation due to MHD instability growth, decreased saturation energies of the m/n = 1/1 mode driving sawteeth-like activity, RE losses in stochastic magnetic fields, and subsequent RE confinement and plateau formation due to re-healing of flux surfaces. Large RE plateaus (>5 MA) are obtained with Ne-only injection ($2-5 \times 10^{21}$ atoms), while combined $\text{D}_2$ + Ne injection ($2 \times 10^{21}$ Ne atoms; $1.8 \times 10^{22} \, \text{D}_2$ molecules) produces a lower RE current (<2 MA). With $\text{D}_2$ + Ne injection, a post thermal quench "cold" VDE terminates the RE beam, preventing a steady plateau. These simulations couple REs, 3-D MHD instabilities, MGI, and axisymmetric VDEs for the first time in SPARC disruption simulations and represent a crucial step in understanding RE generation and mitigation in high-current devices like SPARC.

</details>


### [31] [Computer simulations of the Stark effect in the helium-beta complex of krypton in ICF conditions](https://arxiv.org/abs/2512.05903)
*G. Pérez-Callejo,E. Stambulchik,R. Florido,M. A. Gigosos*

Main category: physics.plasm-ph

TL;DR: CSM codes with same physics produce identical Stark profiles for krypton He-beta line in ICF conditions, enabling reliable spectroscopy for plasma diagnostics.


<details>
  <summary>Details</summary>
Motivation: Spectroscopy with dopants like krypton provides crucial temperature/density data in ICF experiments, but CSM calculations for complex Stark profiles under extreme conditions are computationally challenging.

Method: Applied multiple computer simulation model (CSM) realizations to calculate Stark profiles of krypton He-beta line and satellites at ICF-relevant conditions (ne = 1e24-1e25 cm-3, Te = 3keV).

Result: Codes with identical underlying physics but different numerical approaches yield identical Stark profile results, and analysis shows how various physical effects influence line shapes.

Conclusion: CSM codes provide reliable Stark profile calculations for ICF spectroscopy, enabling accurate plasma diagnostics through krypton dopant lineshape analysis.

Abstract: There is an ongoing interest in using spectroscopy in inertial confinement fusion (ICF) experiments, where dopants such as krypton can provide vital information about the temperature and density of the imploding plasma. While the most advanced tools for calculating Stark profiles are computer simulation models (CSMs), their application to complex lineshapes under the extreme conditions of ICF experiments is computationally challenging. In this manuscript, we present results of several CSM realizations applied to the Stark shape of the krypton He-beta line and its satellites at ICF-relevant conditions (ne = 1e24 to 1e25 cm-3, Te = 3keV). We demonstrate that codes with the same underlying physics but different numerical approaches yield identical results and analyze the differences in the line profile caused by various physical effects.

</details>


### [32] [Transverse envelope dynamics of beam slices in a uniform charged ellipsoidal model of the plasma bubble regime](https://arxiv.org/abs/2512.05904)
*Abdul Mannan,Alessio Del Dotto,Massimo Ferrario*

Main category: physics.plasm-ph

TL;DR: Analysis of electron bunch propagation in plasma wakefields using ellipsoidal ion cavity model, studying energy spread and emittance degradation through sliced envelope equations.


<details>
  <summary>Details</summary>
Motivation: To understand and optimize electron bunch transport and acceleration in plasma wakefield accelerators by analyzing field distributions and beam quality degradation in ion cavity configurations.

Method: Model plasma wakefield as ellipsoidal ion cavity with linear fields, slice electron bunch into cylinders, solve envelope equations for each slice, analyze transverse envelope oscillations and matching conditions.

Result: Developed analytical framework for studying energy spread and emittance degradation in plasma wakefield acceleration, identified matching conditions for optimal transport and acceleration.

Conclusion: The ellipsoidal ion cavity model provides effective framework for analyzing beam quality in plasma wakefield accelerators, with envelope equations enabling optimization of matching conditions for minimal degradation.

Abstract: We consider a pair of driver/witness electron bunches propagating in an ionized gas background a configuration similar to the one produced in a capillary discharge where a plasma oscillation has been excited by a driving pulse. We assume as in the plasma nonlinear regime that the plasma electrons behind the driver are completely expelled and an ellipsoidal cavity filled with ions only is formed. The fields are linear in both longitudinal and transverse directions, at least in the region of interest for particle acceleration, as the one produced by a uniform ion distribution within a uniformly charged ellipsoidal volume. The fields produced by the ions and experienced by a witness electron beam are purely electrostatic, being the ions at rest in the laboratory frame on the time scale of interest and it can be represented with the field distribution produced by a 3D charged ellipsoidal. The energy spread and emittance degradation has been studied by slicing the bunch in an array of cylinders and solving envelope equations for each bunch slice. The properties of transverse envelope and emittance oscillations and energy spread degradation have been analyzed together with the related matching conditions for optimal transport and acceleration.

</details>


### [33] [Thermodynamics of Shear Equilibration During Magnetic Reconnection Onset in Mixed-Equilibrium Current Sheets](https://arxiv.org/abs/2512.05921)
*Dominic Payne,Marc Swisdak,James Drake,Tak Chu Li*

Main category: physics.plasm-ph

TL;DR: 2D PIC simulation study of magnetic shear effects on reconnection onset, examining guide field interactions with thermodynamics in depleted thermal energy regions.


<details>
  <summary>Details</summary>
Motivation: To understand how magnetic shear across polarity inversion lines affects reconnection onset and current sheet equilibration, particularly the interaction between guide fields and thermodynamic variables in energy-depleted regions.

Method: Used 2D Particle-in-Cell (PIC) simulations to study local interactions between reconnection guide fields and thermodynamic variables during reconnection onset in regions with initially depleted thermal energy and enhanced magnetic energy.

Result: Identified critical stages of equilibration process, characterized intervals based on whether pressure evolution is driven by density or temperature changes, and analyzed implications for local heat and work density evolution.

Conclusion: Examined power densities from electromagnetic field evolution and energy transfer, comparing them to thermodynamic changes to understand energy conversion mechanisms during reconnection onset.

Abstract: Magnetic shear across the polarity inversion line (PIL) plays an important role in the explosive nature of reconnection onset and in the equilibration of current sheets, acting as a source of free energy that can enhance or inhibit the onset process under certain conditions. In this study, we use a 2D PIC simulation to examine the local interaction between the reconnection guide field and thermodynamic variables during reconnection onset in a region of initially depleted thermal energy and enhanced magnetic energy in a large guide field background. We identify critical stages of the equilibration process, characterize intervals based on whether the pressure evolution is driven by changes in density or temperature, and discuss what these intervals imply about the evolution of local heat and work density. Finally, we examine power densities associated with electromagnetic field time evolution and electromagnetic energy transfer and compare to those related to thermodynamic changes.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [34] [The CUBE Virtual Reality Immersion](https://arxiv.org/abs/2512.05280)
*Laura Estridge,Joel Franklin*

Main category: physics.ed-ph

TL;DR: CUBE is a VR tool for visualizing electromagnetic radiation fields in advanced physics education


<details>
  <summary>Details</summary>
Motivation: Students struggle to visualize electromagnetic radiation fields in upper-level physics courses, especially the less familiar radiation fields that are abstract and difficult to conceptualize through traditional teaching methods

Method: Developed a virtual reality immersion called CUBE that allows students to visualize electromagnetic fields in 3D space, with pedagogical considerations guiding the software's features and design

Result: Created a functional VR tool (CUBE) that provides immersive visualization of electromagnetic radiation fields, with specific features designed to address pedagogical challenges in teaching these concepts

Conclusion: CUBE offers a promising approach to teaching electromagnetic radiation fields through immersive VR visualization, potentially improving student understanding of these abstract concepts in advanced physics education

Abstract: The purpose of this note is to introduce the CUBE, a virtual reality immersion that was developed to help visualize electromagnetic fields, particularly the less familiar radiation fields students typically encounter in upper level physics courses. We discuss the pedagogical motivation for different features found in the software, and provide a brief overview of its use.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [CFO: Learning Continuous-Time PDE Dynamics via Flow-Matched Neural Operators](https://arxiv.org/abs/2512.05297)
*Xianglong Hou,Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: CFO is a continuous-time neural operator framework for PDEs that uses flow matching to learn PDE dynamics directly, avoiding autoregressive error accumulation and enabling arbitrary temporal resolution.


<details>
  <summary>Details</summary>
Motivation: Autoregressive neural operators for time-dependent PDEs accumulate errors over long rollouts and require uniform temporal discretization. There's a need for continuous-time approaches that avoid these limitations while remaining computationally efficient.

Method: CFO repurposes flow matching to learn PDE right-hand sides directly without ODE solver backpropagation. It fits temporal splines to trajectory data, uses finite-difference estimates at knots to construct probability paths, and trains a neural operator via flow matching to predict analytic velocity fields.

Result: CFO demonstrates superior long-horizon stability and remarkable data efficiency across four benchmarks (Lorenz, 1D Burgers, 2D diffusion-reaction, 2D shallow water). It outperforms autoregressive baselines with up to 87% relative error reduction, even when trained on only 25% of irregularly subsampled time points.

Conclusion: CFO provides a time-resolution invariant framework for continuous-time PDE learning that avoids autoregressive error accumulation, enables arbitrary temporal querying and reverse-time inference, and achieves competitive efficiency with fewer function evaluations than autoregressive baselines.

Abstract: Neural operator surrogates for time-dependent partial differential equations (PDEs) conventionally employ autoregressive prediction schemes, which accumulate error over long rollouts and require uniform temporal discretization. We introduce the Continuous Flow Operator (CFO), a framework that learns continuous-time PDE dynamics without the computational burden of standard continuous approaches, e.g., neural ODE. The key insight is repurposing flow matching to directly learn the right-hand side of PDEs without backpropagating through ODE solvers. CFO fits temporal splines to trajectory data, using finite-difference estimates of time derivatives at knots to construct probability paths whose velocities closely approximate the true PDE dynamics. A neural operator is then trained via flow matching to predict these analytic velocity fields. This approach is inherently time-resolution invariant: training accepts trajectories sampled on arbitrary, non-uniform time grids while inference queries solutions at any temporal resolution through ODE integration. Across four benchmarks (Lorenz, 1D Burgers, 2D diffusion-reaction, 2D shallow water), CFO demonstrates superior long-horizon stability and remarkable data efficiency. CFO trained on only 25% of irregularly subsampled time points outperforms autoregressive baselines trained on complete data, with relative error reductions up to 87%. Despite requiring numerical integration at inference, CFO achieves competitive efficiency, outperforming autoregressive baselines using only 50% of their function evaluations, while uniquely enabling reverse-time inference and arbitrary temporal querying.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [36] [Sharp gradient estimates and monotonicity in positive Ricci curvature](https://arxiv.org/abs/2512.05467)
*Cosmin Manea*

Main category: math.DG

TL;DR: Sharp gradient estimate for Green's function on closed manifolds with positive Ricci curvature, extending Colding-Minicozzi results, with applications including new proof of Bishop's theorem in 4D.


<details>
  <summary>Details</summary>
Motivation: Extend gradient estimates for Green's functions from open manifolds with non-negative Ricci curvature (Colding-Minicozzi) to closed manifolds with positive Ricci curvature, and explore geometric applications.

Method: Prove sharp gradient estimate for natural Green's function on closed manifolds with positive Ricci curvature, establish connection to family of monotonicity formulae, extend previous results.

Result: Obtained sharp gradient estimate for Green's function on closed manifolds with positive Ricci curvature, showed relationship to monotonicity formulae, extended Colding-Minicozzi results, derived geometric applications including new proof of Bishop's volume comparison theorem in dimension four.

Conclusion: The paper establishes fundamental gradient estimates for Green's functions on closed manifolds with positive Ricci curvature, connects them to monotonicity formulae, and demonstrates significant geometric applications including a novel approach to Bishop's theorem in 4D.

Abstract: We prove a sharp gradient estimate for the natural Green's function of a closed manifold with positive Ricci curvature. We also show that this estimate is closely related to a family of monotonicity formulae. These results extend those previously obtained by Colding and Minicozzi for open manifolds with non-negative Ricci curvature. We further obtain several geometric applications, including a new proof of Bishop's volume comparison theorem in dimension four.

</details>


### [37] [Integral Formulas for Differential Forms on Weighted Manifolds and Applications](https://arxiv.org/abs/2512.05793)
*Fida El Chami,Ola Makhoul*

Main category: math.DG

TL;DR: Derived Reilly formula for differential forms on weighted manifolds with boundary, proved Poincaré-type inequality, presented weighted boundary value problems, and obtained new eigenvalue estimates.


<details>
  <summary>Details</summary>
Motivation: To extend classical results about differential forms and eigenvalue estimates to the setting of weighted manifolds with nonempty boundary, generalizing previous work that was limited to unweighted or boundaryless cases.

Method: Derived a Reilly formula specifically for differential forms on weighted manifolds with boundary, then used this formula as a tool to prove a Poincaré-type inequality and analyze weighted boundary value problems.

Result: Successfully obtained new eigenvalue estimates that extend previously known results, established a Poincaré-type inequality for weighted manifolds with boundary, and presented weighted versions of boundary value problems.

Conclusion: The Reilly formula for differential forms on weighted manifolds with boundary provides a powerful tool that yields several important applications including Poincaré-type inequalities and improved eigenvalue estimates, extending the classical theory to weighted settings.

Abstract: In this paper, we derive a Reilly formula for differential forms on weighted manifolds with nonempty boundary. As an application of this formula, we prove a Poincaré-type inequality in the same context and explore several of its consequences. We also present weighted versions of some boundary value problems and obtain new eigenvalue estimates that extend previously known results.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [38] [Gravitational aggregation regimes: critical dissipation threshold, optimal rigidity and fractal transition](https://arxiv.org/abs/2512.05130)
*Yohann Trivino*

Main category: cond-mat.soft

TL;DR: 3D DEM study reveals how contact mechanics and self-gravity govern aggregation in cold granular assemblies, identifying three regimes based on dissipation and stiffness parameters.


<details>
  <summary>Details</summary>
Motivation: To understand how microscopic contact laws control both kinetics and microstructure of gravity-driven aggregation, providing insights for planetesimal formation and calibration of DEM models against experiments.

Method: Three-dimensional Discrete Element Method coupling direct Newtonian attraction between particle pairs with linear visco-elastic normal contact law. Particles are non-cohesive spheres with restitution-coefficient parameterized normal force. Rotations integrated using quaternions to avoid singularities. Systematic parameter campaigns over dissipation (gamma) and normalized stiffness (ktilde = kn/kstar).

Result: Three aggregation regimes identified: 1) low gamma: particles remain dispersive; 2) above critical gamma (~5e2): aggregation accelerates until plateau; 3) optimal stiffness (ktilde ~1e6): aggregation time reaches minimum. Cluster fraction shows non-monotonic dependence on stiffness with optimal cohesion at intermediate rigidity. Fractal dimension mapping reveals transitions from compact (F~3) to ramified structures (F<2).

Conclusion: Microscopic contact laws quantitatively govern both kinetics and microstructure of gravity-driven aggregation, providing predictive framework for planetesimal formation and calibration of DEM models against laboratory and micro-gravity experiments.

Abstract: I present a three-dimensional Discrete Element Method study of self-gravitation and contact mechanics in cold granular assemblies. The model couples direct Newtonian attraction between every particle pair with a linear visco-elastic normal contact law. Particles are treated as non-cohesive spheres; the normal force is parameterized to reproduce a prescribed restitution coefficient. Rotations are integrated using quaternions to avoid singularities. By normalizing the stiffness kn by kstar = G*m^2/R^3 and time by the free-fall time t_ff, I perform systematic parameter campaigns over dissipation (gamma) and normalized stiffness ktilde = kn/kstar. Results reveal three aggregation regimes. For low gamma the particles remain largely dispersive; above a critical gamma of about 5e2 aggregation accelerates until plateaus are reached in the aggregation time T_agg divided by t_ff. For stiffness ktilde on the order of 1e6 the aggregation time reaches a clear minimum. The cluster fraction C/Ntot shows a non-monotonic dependence on ktilde, with optimal cohesion at intermediate rigidity and peripheral isolation at extreme stiffness. Mapping the fractal dimension F across (gamma, ktilde) demonstrates transitions from compact structures (F about 3) to ramified structures (F below 2). These findings quantify how microscopic contact laws govern both the kinetics and microstructure of gravity-driven aggregation, providing a predictive framework for planetesimal formation and for calibrating DEM models against laboratory and micro-gravity experiments.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [39] [Convolution-FFT for option pricing in the Heston model](https://arxiv.org/abs/2512.05326)
*Xiang Gao,Cody Hyndman*

Main category: q-fin.CP

TL;DR: A convolution-FFT method for Heston model option pricing with analytical error bounds and stable integrand via continuously differentiable characteristic function representation.


<details>
  <summary>Details</summary>
Motivation: Existing Fourier-based methods for Heston model option pricing suffer from instability due to branch-cut issues and empirically tuned damping parameters, lacking rigorous error analysis.

Method: Convolution-FFT method using continuously differentiable representation of joint characteristic function, with fully analytical error bounds for truncation and discretization errors.

Result: Stable integrand under large frequency oscillations, explicit closed-form error estimates, robust high-accuracy pricing at modest computational cost with confirmed theoretical rates.

Conclusion: First work providing explicit error bounds for FFT-based convolution method in Heston model, offering stable and accurate option pricing with rigorous error quantification.

Abstract: We propose a convolution-FFT method for pricing European options under the Heston model that leverages a continuously differentiable representation of the joint characteristic function. Unlike existing Fourier-based methods that rely on branch-cut adjustments or empirically tuned damping parameters, our approach yields a stable integrand even under large frequency oscillations. Crucially, we derive fully analytical error bounds that quantify both truncation error and discretization error in terms of model parameters and grid settings. To the best of our knowledge, this is the first work to provide such explicit, closed-form error estimates for an FFT-based convolution method specialized to the Heston model. Numerical experiments confirm the theoretical rates and illustrate robust, high-accuracy option pricing at modest computational cost.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [40] [Feedback stabilization of some fourth-order nonlinear parabolic equations with saturated controlsEQUATIONS WITH SATURATED CONTROLS](https://arxiv.org/abs/2512.05606)
*Patricio Guzmán,Felipe Labra,Hugo Parada*

Main category: eess.SY

TL;DR: The paper analyzes stabilization of Cahn-Hilliard and Kuramoto-Sivashinsky equations using saturated feedback control via spectral analysis and LMI-based design.


<details>
  <summary>Details</summary>
Motivation: To develop effective stabilization methods for Cahn-Hilliard and Kuramoto-Sivashinsky equations, which are important in pattern formation and phase transition modeling, using practical saturated feedback control that accounts for actuator limitations.

Method: Spectral analysis of linear operator to identify unstable eigenvalues, modal decomposition, linear matrix inequalities (LMIs) design, and geometric conditions on saturation function for stabilization strategy.

Result: Achieves local exponential stabilization in H² space (Sobolev space of functions with square-integrable second derivatives) for both equations under saturated feedback control.

Conclusion: The proposed spectral analysis and LMI-based approach with geometric saturation conditions provides an effective framework for stabilizing nonlinear PDEs like Cahn-Hilliard and Kuramoto-Sivashinsky equations with practical actuator constraints.

Abstract: In this work, we analyze the internal and boundary stabilization of the Cahn-Hilliard and Kuramoto-Sivashinsky equations under saturated feedback control. We conduct our study through the spectral analysis of the associated linear operator. We identify a finite number of eigenvalues related to the unstable part of the system and then design a stabilization strategy based on modal decomposition, linear matrix inequalities (LMIs), and geometric conditions on the saturation function. Local exponential stabilization in $H^{2}$ is established.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [41] [Mechanical Stability of 2D Ti2COx MXenes Under Compression Using Reactive Molecular Dynamics](https://arxiv.org/abs/2512.05166)
*Hossein Darban*

Main category: cond-mat.mtrl-sci

TL;DR: Large-scale reactive MD simulations reveal Ti2C and Ti2CO2 MXene nanosheets' compressive and post-buckling behavior, showing classical continuum mechanics overestimates buckling strains, oxygen termination increases buckling stress, and different loading conditions produce distinct buckling modes.


<details>
  <summary>Details</summary>
Motivation: To understand the compressive and post-buckling behavior of Ti2C and Ti2CO2 MXene nanosheets under various loading conditions (uniaxial, biaxial, shear) and investigate how factors like atomic defects, lateral confinement, and surface termination affect their mechanical response.

Method: Large-scale reactive molecular dynamics simulations of Ti2C and Ti2CO2 MXene nanosheets subjected to uniaxial, biaxial, and shear loads to study buckling modes, atomic-level deformation mechanisms, and failure characteristics.

Result: Classical continuum mechanics significantly overestimates buckling strains; nanosheets show higher buckling resistance along armchair direction; atomic defects reduce buckling stress but don't alter global buckling modes; lateral confinement pressure increases buckling stress; oxygen termination increases buckling stress from ~1 GPa to 3.5 GPa and reduces directional anisotropy; Ti2CO2 fractures under large compressive strains while Ti2C remains intact at strains >0.35; biaxial compression produces dome-like buckling while shear loads create elliptical deflection modes.

Conclusion: The study provides fundamental insights into MXene nanosheet buckling behavior, revealing significant differences from continuum predictions and highlighting the effects of surface termination and loading conditions. These findings stimulate future research on MXene morphological transformations for applications like nanotube, nanoscroll, and folded architectures.

Abstract: The compressive and post-buckling behavior of Ti2C and Ti2CO2 MXene nanosheets is studied using large-scale reactive molecular dynamics simulations. Nanosheets are subjected to uniaxial, biaxial, and shear loads to investigate their buckling modes, atomic-level deformation mechanisms, and failure characteristics. The results indicate that classical continuum mechanics significantly overestimates the buckling strains. Nanosheets exhibit higher resistance to buckling along the armchair direction than along the zigzag direction. Although atomic-scale defects reduce the buckling stress, they influence deformation only locally and do not alter the global buckling mode shapes. Lateral confinement pressure, such as that caused by polymerization-induced shrinkage in MXene-polymer composites, substantially increases the buckling stress. Oxygen surface termination increases the buckling stress from approximately 1 GPa to 3.5 GPa and reduces directional anisotropy in the elastic response. Under large compressive strains, Ti2CO2 nanosheets fracture, whereas Ti2C nanosheets retain structural integrity at strains exceeding 0.35. Atomistic analysis reveals opposite stress states in the top and bottom Ti layers due to curvature-induced strain gradients. Under biaxial compression, the nanosheet buckles in a dome-like shape, whereas shear loads produce elliptical deflection modes. The presented findings stimulate future studies on MXene morphological transformations, such as the development of nanotube, nanoscroll, and folded architectures.

</details>


### [42] [Mapping vacancy and bonding electron distributions around aluminium nanovoids](https://arxiv.org/abs/2512.05296)
*Philip N. H. Nakashima,Yu-Tsun Shao,Zezhong Zhang,Andrew E. Smith,Tianyu Liu,Nikhil V. Medhekar,Joanne Etheridge,Laure Bourgeois,Jian-Min Zuo*

Main category: cond-mat.mtrl-sci

TL;DR: First experimental method to measure bonding electron distributions around defects and nanostructures, applied to nanovoids in aluminum with 3% vacancy volume precision and 3D vacancy mapping.


<details>
  <summary>Details</summary>
Motivation: Defects and nanostructures disrupt chemical bonding which determines materials properties, but no experimental measurements of bonding electron distributions around such features have been possible until now.

Method: Developed a novel experimental method enabling depth-resolved, vacancy-sensitive measurements of bonding electron densities around defects and nanostructures, validated against density functional theory.

Result: Measured vacancy volume with 3% uncertainty, mapped vacancy concentrations around nanovoids with nanometer 3D resolution (previously only 2D), discovered radiation-damaged voids can "heal."

Conclusion: This breakthrough opens bonding electron density measurements to inhomogeneous nanostructured materials, enabling study of electronic origins of phenomena like strengthening, weakening, interface functionality, diffusion, and phase transformations.

Abstract: All materials have defects and many contain nanostructures, both of which disrupt chemical bonding - the basis of materials properties. No experimental measurements of bonding electron distributions associated with defects and nanostructures have ever been possible. We present a method enabling such measurements and interrogate nanovoids surrounded by vacancies - the most fundamental of nanostructures and defects - in aluminium. We measure the volume of a vacancy with 3% uncertainty and map vacancy concentrations surrounding nanovoids with nanometre resolution in three dimensions where previously only two-dimensional mapping was possible. We discover that radiation-damaged voids can "heal". Our bonding measurements are depth-resolved, vacancy-sensitive, and agree with density functional theory. This work opens bonding electron density measurements to inhomogeneous nanostructured multi-phased materials so that the electronic origins of phenomena such as strengthening, weakening, interface functionality, solute diffusion and phase transformations within them may be revealed.

</details>


### [43] [Magnetic and moiré Proximity Effects in WSe2/WSe2/CrI3 Trilayers](https://arxiv.org/abs/2512.05917)
*Junyi Liu,Xu Zhang,Gang Lu*

Main category: cond-mat.mtrl-sci

TL;DR: Magnetic proximity and moiré proximity effects in WSe2/WSe2/CrI3 trilayers create enhanced valley splitting and novel topological states through superposition of Umklapp excitons and imprinted moiré potentials.


<details>
  <summary>Details</summary>
Motivation: To study the interplay between magnetic proximity effects and moiré superlattices in van der Waals heterostructures, which is important for both scientific understanding and technological applications in spintronics and valleytronics.

Method: First-principles calculations on WSe2/WSe2/CrI3 trilayers with different stackings and twist angles, analyzing valley splitting, electric-field dependence, and moiré superlattice effects.

Result: Large valley splitting observed due to exciton charge redistribution via super-exchange-like mechanism; valley splitting amplified in moiré superlattices through superposition of Umklapp excitons; imprinted moiré potential on CrI3 layer and feedback to WSe2 bilayers demonstrated; cooperation yields novel topological and correlated states.

Conclusion: The combination of magnetic proximity and moiré proximity effects in trilayer heterostructures enables enhanced valley splitting and creates opportunities for novel topological and correlated electronic states through the interplay of direct and imprinted moiré potentials.

Abstract: Integrating magnetic order to moiré superlattices is of significant scientific and technological interest. Based on first-principles calculations, we study the interplay of magnetic proximity and moiré proximity in WSe2/WSe2/CrI3 trilayers with different stackings and twist angles. Large valley splitting is observed due to redistribution of the exciton charge density across layers via a super-exchange-like mechanism, and its electric-field dependence bears similarity to electrically tunable and valley-selective Feshbach resonances. The valley splitting can be magnified in moiré superlattices owing to the superposition of Umklapp excitons folded from moiré minibands, yielding spatially modulated and enhanced magnetic proximity. The moiré proximity effect is demonstrated via an imprinted moiré potential on CrI3 layer and its feedback to the direct moiré potential on WSe2 bilayers is observed. The cooperation between the direct and imprinted moiré potentials is shown to yield novel topological and correlated states.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [44] [Lagrangian versus Eulerian Methods for Toroidally-Magnetized Isothermal Disks](https://arxiv.org/abs/2512.05194)
*Yashvardhan Tomar,Philip F. Hopkins*

Main category: astro-ph.HE

TL;DR: Lagrangian methods reproduce high-resolution Eulerian results for toroidally-magnetized accretion disks, showing flux loss at all resolutions, unlike Eulerian methods which show no evolution at low resolution.


<details>
  <summary>Details</summary>
Motivation: To investigate whether the sustained midplane toroidal magnetic fields seen in recent Lagrangian simulations are a numerical resolution artifact, by comparing Lagrangian and Eulerian methods on the Guo et al. 2025 test problem.

Method: Reran the G25 test problem using two Lagrangian methods (meshless finite-mass and meshless finite-volume) and compared results with Eulerian static-mesh methods at different resolutions (high: Δx ≪ H_thermal, low: Δx ≫ H_thermal).

Result: Lagrangian methods reproduce high-resolution Eulerian results at all resolutions, showing flux loss and evolution toward converged solutions, while Eulerian methods show no evolution at low resolution. Lagrangian methods can follow flows to arbitrarily thin midplane layers.

Conclusion: Sustained midplane toroidal fields in recent Lagrangian simulations are not a numerical resolution effect; physical differences between those simulations and the G25 test problem must explain the different behaviors.

Abstract: A number of simulations have seen the emergence of strongly-toroidally-magnetized accretion disks from interstellar medium inflows. Recently, Guo et al. 2025 (G25) studied an idealized test problem of toroidally-magnetized disks in isothermal ideal MHD with an Eulerian static-mesh method, and argued the midplane behavior changes qualitatively (with a significant loss of toroidal magnetic flux) when the the thermal scale-length is resolved ($Δx < H_{\rm thermal}$). We rerun the G25 test problem with two Lagrangian methods: meshless finite-mass, and meshless finite-volume. We show that Lagrangian methods reproduce the high-resolution ($Δx \ll H_{\rm thermal}$) Eulerian G25 results. At low resolution ($Δx \gg H_{\rm thermal}$), behaviors differ: Lagrangian methods still lose flux and evolve 'as close as possible' to the converged solution, while Eulerian methods show no evolution. We argue this difference in convergence behavior is related to the ability of Lagrangian codes to follow flows to an arbitrarily thin midplane layer, analogous to the well-studied difference in Jeans fragmentation problems. This and results from other higher-resolution simulations and different codes suggest that the sustained midplane toroidal fields seen in recent Lagrangian multi-scale, multi-physics simulations cannot be a numerical resolution effect, and some physical difference between those simulations and the G25 test problem explains their different behaviors.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [45] [A Frenet frame analysis of protein geometry: hints for secondary structure assignments](https://arxiv.org/abs/2512.05660)
*M. Prados,M. D. Hernández de la Torre,F. de Soto*

Main category: physics.bio-ph

TL;DR: The paper analyzes protein secondary structure using Frenet frames to compute curvature and torsion of α-carbon chains, showing these metrics can identify secondary/supersecondary structures and supporting a U(1) gauge model description of proteins.


<details>
  <summary>Details</summary>
Motivation: To develop a mathematical framework for analyzing protein secondary structure using differential geometry concepts, specifically curvature and torsion of protein backbone chains, to better understand protein folding and structure.

Method: Uses Frenet frames to describe curvature and torsion of discrete curves formed by protein α-carbons. Applies a simple criterion based on curvature/torsion evaluation to identify secondary and supersecondary structures. Tests the approach on a large dataset from the Protein Data Bank.

Result: Curvature and torsion metrics successfully pinpoint secondary and supersecondary structures in proteins. The observed curvature/torsion patterns strongly support describing proteins as fixed points of an effective action inspired by a U(1) gauge model.

Conclusion: Frenet frame analysis provides a useful mathematical framework for protein structure analysis, with curvature and torsion serving as effective descriptors for identifying structural features and supporting theoretical models of protein folding.

Abstract: This paper deepens into the analysis of the protein secondary structure using Frenet frame to describe the curvature and torsion of the discrete curve formed by the protein $α$-carbons. We show how a simple criterion based on the evaluation of the curvature and torsion of the discrete curve can be useful to pinpoint the presence of some secondary and supersecondary structures in proteins. Moreover, the description of proteins as fixed points of an effective action inspired by an $U(1)$ gauge model is strongly supported by the curvature and torsion observed over a large dataset of proteins in the Protein Data Bank.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [46] [FieldSeer I: Physics-Guided World Models for Long-Horizon Electromagnetic Dynamics under Partial Observability](https://arxiv.org/abs/2512.05361)
*Ziheng Guo,Fang Wu,Maoxiong Zhao,Chaoqun Fang,Yang Bu*

Main category: physics.optics

TL;DR: FieldSeer I is a geometry-aware world model that predicts electromagnetic field dynamics in 2-D waveguides from partial observations, enabling interactive digital twins for photonic design.


<details>
  <summary>Details</summary>
Motivation: To create practical interactive digital twins for photonic design by developing a model that can forecast electromagnetic field dynamics from partial observations, allowing for geometry modifications without re-assimilation.

Method: A geometry-aware world model that assimilates short prefix of observed fields, conditions on scalar source action and structure/material map, generates closed-loop rollouts in physical domain, and trains in symmetric-log domain for numerical stability.

Result: Outperforms GRU and deterministic baselines on FDTD benchmark (200 unique simulations) across three settings: software-in-the-loop filtering, offline single-file rollouts, and offline multi-structure rollouts. Enables edit-after-prefix geometry modifications without re-assimilation.

Conclusion: Geometry-conditioned world models provide a practical path toward interactive digital twins for photonic design, demonstrating the feasibility of forecasting electromagnetic field dynamics from partial observations.

Abstract: We introduce FieldSeer I, a geometry-aware world model that forecasts electromagnetic field dynamics from partial observations in 2-D TE waveguides. The model assimilates a short prefix of observed fields, conditions on a scalar source action and structure/material map, and generates closed-loop rollouts in the physical domain. Training in a symmetric-log domain ensures numerical stability. Evaluated on a reproducible FDTD benchmark (200 unique simulations, structure-wise split), FieldSeer I achieves higher suffix fidelity than GRU and deterministic baselines across three practical settings: (i) software-in-the-loop filtering (64x64, P=80->Q=80), (ii) offline single-file rollouts (80x140, P=240->Q=40), and (iii) offline multi-structure rollouts (80x140, P=180->Q=100). Crucially, it enables edit-after-prefix geometry modifications without re-assimilation. Results demonstrate that geometry-conditioned world models provide a practical path toward interactive digital twins for photonic design.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [47] [OpenSQP: A Reconfigurable Open-Source SQP Algorithm in Python for Nonlinear Optimization](https://arxiv.org/abs/2512.05392)
*Anugrah Jo Joshy,John T. Hwang*

Main category: math.OC

TL;DR: OpenSQP is a modular Python implementation of sequential quadratic programming that allows easy customization of components while maintaining competitive performance with established optimization algorithms.


<details>
  <summary>Details</summary>
Motivation: Existing SQP implementations lack transparency and modularity, making it difficult to adapt them for specific applications or create new optimizers by swapping different modules.

Method: Developed OpenSQP as a modular, reconfigurable SQP algorithm in Python that allows users to easily modify or replace components like merit functions, line search procedures, Hessian approximations, and QP solvers.

Result: OpenSQP's standard configuration (using smooth augmented Lagrangian merit function and BFGS Hessian approximation) demonstrates competitive performance on CUTEst test problems compared to SLSQP, SNOPT, and IPOPT.

Conclusion: OpenSQP provides a flexible, transparent SQP framework that enables customization while maintaining robust performance comparable to leading optimization algorithms.

Abstract: Sequential quadratic programming (SQP) methods have been remarkably successful in solving a broad range of nonlinear optimization problems. These methods iteratively construct and solve quadratic programming (QP) subproblems to compute directions that converge to a local minimum. While numerous open-source and commercial SQP algorithms are available, their implementations lack the transparency and modularity necessary to adapt and fine-tune them for specific applications or to swap out different modules to create a new optimizer. To address this gap, we present OpenSQP, a modular and reconfigurable SQP algorithm implemented in Python that achieves robust performance comparable to leading algorithms. We implement OpenSQP in a manner that allows users to easily modify or replace components such as merit functions, line search procedures, Hessian approximations, and QP solvers. This flexibility enables the creation of tailored variants of the algorithm for specific needs. To demonstrate reliability, we present numerical results using the standard configuration of OpenSQP that employs a smooth augmented Lagrangian merit function for the line search and a quasi-Newton BFGS method for approximating the Hessians. We benchmark this configuration on a comprehensive set of problems from the CUTEst test suite. The results demonstrate performance that is competitive with proven nonlinear optimization algorithms such as SLSQP, SNOPT, and IPOPT.

</details>


### [48] [Stochastic Zeroth-Order Method for Computing Generalized Rayleigh Quotients](https://arxiv.org/abs/2512.05520)
*Jonas Bresch,Oleh Melnyk,Martin Schoen,Gabriele Steidl*

Main category: math.OC

TL;DR: Stochastic zeroth-order Riemannian algorithm for maximizing generalized Rayleigh quotient without adjoint or matrix inverse computations, with theoretical convergence guarantees and superior performance compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Conventional algorithms for maximizing generalized Rayleigh quotient rely on matrix-adjoint products, making them sensitive to errors from adjoint mismatches. There's a need for algorithms that avoid these computations while maintaining convergence guarantees.

Method: A stochastic zeroth-order Riemannian algorithm that maximizes the generalized Rayleigh quotient without requiring adjoint or matrix inverse computations. The method operates on Riemannian manifolds and uses stochastic zeroth-order optimization techniques.

Result: Theoretical convergence guarantees show that iterates converge to the set of global maximizers of the generalized Rayleigh quotient at a sublinear rate with probability one. Numerical experiments demonstrate excellent performance compared to state-of-the-art algorithms.

Conclusion: The proposed stochastic zeroth-order Riemannian algorithm effectively addresses adjoint mismatch issues in generalized Rayleigh quotient maximization, providing robust convergence guarantees and superior practical performance without requiring adjoint or matrix inverse computations.

Abstract: The maximization of the (generalized) Rayleigh quotient is a central problem in numerical linear algebra. Conventional algorithms for its computation typically rely on matrix-adjoint products, making them sensitive to errors arising from adjoint mismatches. To address this issue, we introduce a stochastic zeroth-order Riemannian algorithm that maximizes the generalized Rayleigh quotient without requiring adjoint or matrix inverse computations. We provide theoretical convergence guarantees showing that the iterates converge to the set of global maximizers of the (generalized) Rayleigh quotient at a sublinear rate with probability one. Our theoretical results are supported by numerical experiments, which demonstrate the excellent performance of the proposed method compared to state-of-the-art algorithms.

</details>


### [49] [Taylor Approximation Variance Reduction for Approximation Errors in PDE-constrained Bayesian Inverse Problems](https://arxiv.org/abs/2512.05723)
*Ruanui Nicholson,Radoslav Vuchkov,Umberto Villa,Noemi Petra*

Main category: math.OC

TL;DR: Scalable computational approach to reduce sampling costs in Bayesian approximation error method for PDE-based inverse problems using Taylor expansions as control variates.


<details>
  <summary>Details</summary>
Motivation: Surrogate models in PDE-based inverse problems introduce approximation errors. Bayesian approximation error (BAE) approach accounts for these errors but requires expensive Monte Carlo sampling that becomes a computational bottleneck, especially for high-dimensional problems.

Method: Develops scalable approach using Taylor expansions of accurate and surrogate forward models as control variates for variance reduction. Proposes efficient methods for evaluating mean and covariance of Taylor approximations based on linear(-ized) PDE solves. Approach is independent of uncertain parameter dimension, depending instead on intrinsic data dimension.

Result: Demonstrated benefits on two high-dimensional PDE inverse problems: 1) estimation of distributed Robin boundary coefficient in linear diffusion problem, 2) coefficient estimation in nonlinear diffusion problem. Approach shows scalability to high-dimensional problems.

Conclusion: Proposed method provides computationally efficient alternative to standard Monte Carlo sampling for BAE approach, enabling scalable uncertainty quantification for high-dimensional PDE-based inverse problems while maintaining accuracy.

Abstract: In numerous applications, surrogate models are used as a replacement for accurate parameter-to-observable mappings when solving large-scale inverse problems governed by partial differential equations (PDEs). The surrogate model may be a computationally cheaper alternative to the accurate parameter-to-observable mappings and/or may ignore additional unknowns or sources of uncertainty. The Bayesian approximation error (BAE) approach provides a means to account for the induced uncertainties and approximation errors (between the accurate parameter-to-observable mapping and the surrogate). The statistics of these errors are in general unknown a priori, and are thus calculated using Monte Carlo sampling. Although the sampling is typically carried out offline the process can still represent a computational bottleneck. In this work, we develop a scalable computational approach for reducing the costs associated with the sampling stage of the BAE approach. Specifically, we consider the Taylor expansion of the accurate and surrogate forward models with respect to the uncertain parameter fields either as a control variate for variance reduction or as a means to efficiently approximate the mean and covariance of the approximation errors. We propose efficient methods for evaluating the expressions for the mean and covariance of the Taylor approximations based on linear(-ized) PDE solves. Furthermore, the proposed approach is independent of the dimension of the uncertain parameter, depending instead on the intrinsic dimension of the data, ensuring scalability to high-dimensional problems. The potential benefits of the proposed approach are demonstrated for two high-dimensional inverse problems governed by PDE examples, namely for the estimation of a distributed Robin boundary coefficient in a linear diffusion problem, and for a coefficient estimation problem governed by a nonlinear diffusion problem.

</details>
