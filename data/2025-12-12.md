<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 11]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [math.KT](#math.KT) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [math-ph](#math-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Hybrid Finite Element and Least Squares Support Vector Regression Method for solving Partial Differential Equations with Legendre Polynomial Kernels](https://arxiv.org/abs/2512.09967)
*Maryam Babaei,Peter Rucz,Manfred Kaltenbacher,Stefan Schoder*

Main category: math.NA

TL;DR: Hybrid FEM-LSSVR method combines finite element nodal solutions with Legendre polynomial kernel regression to create analytical super-resolution solutions, achieving higher accuracy than base FEM while maintaining boundary consistency.


<details>
  <summary>Details</summary>
Motivation: To enhance low-order FEM solutions by providing closed-form analytical interpolation between nodes without requiring extensive code modifications, enabling super-resolution of numerical solutions and experimental data.

Method: Integrates FEM (provides nodal solutions) with LSSVR using higher-order Legendre polynomial kernels for element-wise enhancement. Uses localized kernel refinement and parallel computation to adapt existing low-order FEM codes.

Result: Achieves significantly higher accuracy than base FEM solutions, comparable to standalone FEM with same polynomial order. Successfully demonstrated on four elliptic boundary value problems with convergence studies.

Conclusion: The hybrid FEM-LSSVR approach provides an effective plug-and-play method for super-resolving low-order numerical solvers and experimental data, offering high-resolution accuracy while maintaining computational efficiency.

Abstract: A hybrid computational approach that integrates the finite element method (FEM) with least squares support vector regression (LSSVR) is introduced to solve partial differential equations. The method combines FEM's ability to provide the nodal solutions and LSSVR with higher-order Legendre polynomial kernels to deliver a closed-form analytical solution for interpolation between the nodes. The hybrid approach implements element-wise enhancement (super-resolution) of a given numerical solution, resulting in high resolution accuracy, while maintaining consistency with FEM nodal values at element boundaries. It can adapt any low-order FEM code to obtain high-order resolution by leveraging localized kernel refinement and parallel computation without additional implementation overhead. Therefore, effective inference/post-processing of the obtained super-resolved solution is possible. Evaluation results show that the hybrid FEM-LSSVR approach can achieve significantly higher accuracy compared to the base FEM solution. Comparable accuracy is a achieved when comparing the hybrid solution with a standalone FEM result with the same polynomial basis function order. The convergence studies were conducted for four elliptic boundary value problems to demonstrate the method's ability, accuracy, and reliability. Finally, the algorithm can be directly used as a plug-and-play method for super-resolving low-order numerical solvers and for super-resolution of expensive/under-resolved experimental data.

</details>


### [2] [A Mass Preserving Numerical Scheme for Kinetic Equations that Model Social Phenomena](https://arxiv.org/abs/2512.10027)
*Yassin Bahid,Eduardo Corona,Nancy Rodriguez*

Main category: math.NA

TL;DR: The paper presents a deterministic numerical scheme called the Mass Preserving Collocation Method for solving kinetic equations with Dirac delta transition rates, enabling efficient simulation of multi-subsystem social phenomena models while preserving mass and outperforming stochastic methods.


<details>
  <summary>Details</summary>
Motivation: Kinetic equations with Dirac delta transition rates are used to model social phenomena with sudden state changes, but existing stochastic methods (agent-based, Tau-leaping, hybrid) suffer from computational inefficiency, variability, and hyperparameter tuning issues.

Method: Developed a fully deterministic Mass Preserving Collocation Method for kinetic equations with transition rates of the form T(x,y,u) = δ_{φ(x,y) - u}. The method preserves mass numerically and enables efficient simulation of models with multiple subsystems.

Result: Established global existence and uniqueness of solutions for these systems. Validated the solver's accuracy, efficiency, and consistency on models with up to five subsystems. The method outperforms Tau-leaping and hybrid methods by resolving subsystem distributions while requiring significantly less computational time/resources, avoiding variability and hyperparameter tuning.

Conclusion: The Mass Preserving Collocation Method provides an efficient, deterministic alternative to stochastic approaches for simulating kinetic models with Dirac delta transition rates, offering computational advantages while maintaining accuracy for multi-subsystem social phenomena modeling.

Abstract: In recent years, kinetic equations have been used to model many social phenomena. A key feature of these models is that transition rate kernels involve Dirac delta functions, which capture sudden, discontinuous state changes. Here, we study kinetic equations with transition rates of the form $$ T(x,y,u) = δ_{φ(x,y) - u}. $$ We establish the global existence and uniqueness of solutions for these systems and introduce a fully deterministic scheme, the \emph{Mass Preserving Collocation Method}, which enables efficient, high fidelity simulation of models with multiple subsystems. We validate the accuracy, efficiency, and consistency of the solver on models with up to five subsystems, and compare its performance against two state-of-the-art agent-based methods: Tau-leaping and hybrid methods. Our scheme resolves subsystem distributions captured by these stochastic approaches while preserving mass numerically, requiring significantly less computational time and resources, and avoiding variability and hyperparameter tuning characteristic of these methods.

</details>


### [3] [Efficient Boys function evaluation using minimax approximation](https://arxiv.org/abs/2512.10059)
*Rasmus Vikhamar-Sandberg,Michal Repisky*

Main category: math.NA

TL;DR: Efficient GPU-optimized algorithm for evaluating Boys functions using rational minimax approximations with recurrence relations, avoiding lookup tables for better performance on modern hardware.


<details>
  <summary>Details</summary>
Motivation: Modern computing architectures like GPUs have high throughput but costly data movement, requiring algorithms that avoid irregular memory access patterns like lookup tables for efficient Boys function evaluation.

Method: Combines rational minimax approximations with upward/downward recurrence relations, partitioning the real axis into three regions: A and B use rational minimax approximations, C uses asymptotic approximation. Coefficients generated via rational Remez algorithm.

Result: Provides approximation regions and coefficients for Boys functions F₀ to F₃₂ with target maximum absolute error of 5×10⁻¹⁴, enabling efficient GPU implementation without lookup tables.

Conclusion: The algorithm is well-suited for hardware with high throughput and low latency by avoiding irregular memory access, making it particularly effective for GPU-based computation of Boys functions.

Abstract: We present an algorithm for efficient evaluation of Boys functions $F_0,\dots,F_{k_\mathrm{max}}$ tailored to modern computing architectures, in particular graphical processing units (GPUs), where maximum throughput is high and data movement is costly. The method combines rational minimax approximations with upward and downward recurrence relations. The non-negative real axis is partitioned into three regions, $[0,\infty\rangle = A\cup B\cup C$, where regions $A$ and $B$ are treated using rational minimax approximations and region $C$ by an asymptotic approximation. This formulation avoids lookup tables and irregular memory access, making it well suited hardware with high maximum throughput and low latency. The rational minimax coefficients are generated using the rational Remez algorithm. For a target maximum absolute error of $\varepsilon_\mathrm{tol} = 5\cdot10^{-14}$, the corresponding approximation regions and coefficients for Boys functions $F_0,\dots,F_{32}$ are provided in the appendix.

</details>


### [4] [Metric-driven numerical methods](https://arxiv.org/abs/2512.10083)
*Patrick Henning,Laura Huynh,Daniel Peterseim*

Main category: math.NA

TL;DR: The paper introduces metric-driven numerical methods using Riemannian gradient techniques to solve multiscale PDEs and eigenvalue problems, showing how metric choice leads to efficient iterative schemes and enhanced approximation spaces like LOD.


<details>
  <summary>Details</summary>
Motivation: To develop powerful numerical methods for solving multiscale partial differential equations and eigenvalue problems with nonlinearities, particularly in low-regularity regimes or when solutions exhibit heterogeneous multiscale features.

Method: Metric-driven methods using Riemannian gradient techniques, where gradients are represented in different metrics (Sobolev gradients) to accelerate convergence. The approach leads to specific iterative schemes and induces approximation spaces with enhanced properties.

Result: The method recovers the well-known Localized Orthogonal Decomposition (LOD) multiscale spaces from a new perspective. The approach is demonstrated for a model problem and applied to simulating ground states of spin-orbit-coupled Bose-Einstein condensates.

Conclusion: Metric-driven numerical methods provide a powerful framework for solving multiscale PDEs and eigenvalue problems, offering both efficient iterative schemes and enhanced approximation spaces that can handle low-regularity and heterogeneous multiscale features.

Abstract: In this paper, we explore the concept of metric-driven numerical methods as a powerful tool for solving various types of multiscale partial differential equations. Our focus is on computing constrained minimizers of functionals - or, equivalently, by considering the associated Euler-Lagrange equations - the solution of a class of eigenvalue problems that may involve nonlinearities in the eigenfunctions. We introduce metric-driven methods for such problems via Riemannian gradient techniques, leveraging the idea that gradients can be represented in different metrics (so-called Sobolev gradients) to accelerate convergence. We show that the choice of metric not only leads to specific metric-driven iterative schemes, but also induces approximation spaces with enhanced properties, particularly in low-regularity regimes or when the solution exhibits heterogeneous multiscale features. In fact, we recover a well-known class of multiscale spaces based on the Localized Orthogonal Decomposition (LOD), now derived from a new perspective. Alongside a discussion of the metric-driven approach for a model problem, we also demonstrate its application to simulating the ground states of spin-orbit-coupled Bose-Einstein condensates.

</details>


### [5] [Numerical approximation of the first $p$-Laplace eigenpair](https://arxiv.org/abs/2512.10122)
*Hannah Potgieter,Razvan C. Fetecau,Steven J. Ruuth*

Main category: math.NA

TL;DR: A numerical method for approximating the first Dirichlet eigenpair of the p-Laplace operator for large p values, connecting to the p→∞ limit and geometry, using surface finite elements with Newton inverse-power iteration and domain rescaling.


<details>
  <summary>Details</summary>
Motivation: To study the p→∞ limit of the p-Laplace operator's first Dirichlet eigenpair and understand its connection to domain geometry, while overcoming numerical challenges of large p computations.

Method: Surface finite element numerical scheme combining Newton inverse-power iteration with a novel domain rescaling strategy for stable computations at large p values.

Result: Accurate and robust computations demonstrated in 1D, planar domains, and surfaces in ℝ³, showing convergence to p→∞ limiting behavior.

Conclusion: The proposed method successfully handles large p values, reveals geometric connections in the p→∞ limit, and provides stable numerical approximations for p-Laplace eigenproblems on various domains.

Abstract: We approximate the first Dirichlet eigenpair of the $p$-Laplace operator for $2 \leq p < \infty$ on both Euclidean and surface domains. We emphasize large $p$ values and discuss how the $p \to \infty$ limit connects to the underlying geometry of our domain. Working with large $p$ values introduces significant numerical challenges. We present a surface finite element numerical scheme that combines a Newton inverse-power iteration with a new domain rescaling strategy, which enables stable computations for large $p$. Numerical experiments in $1$D, planar domains, and surfaces embedded in $\mathbb{R}^3$ demonstrate the accuracy and robustness of our approach and show convergence towards the $p \to \infty$ limiting behavior.

</details>


### [6] [A robust fully-mixed finite element method with skew-symmetry penalization for low-frequency poroelasticity](https://arxiv.org/abs/2512.10192)
*Stefano Bonetti,Michele Botti,Patrick Vega*

Main category: math.NA

TL;DR: A fully-mixed finite element method for dynamic poroelasticity in low-frequency regime using four-field hyperbolic formulation with penalized stress symmetry.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for simulating wave propagation in porous materials, particularly addressing the dynamic poroelasticity problem in low-frequency applications where accurate modeling of coupled solid-fluid interactions is crucial.

Method: Four-field, first-order hyperbolic system formulation with stress symmetry imposed via penalization; uses stable mixed finite elements for spatial discretization and implicit time integration schemes; perturbation approach to handle stress symmetry in saddle point system.

Result: The method demonstrates stability analysis robust to degenerate model parameters; numerical tests validate convergence, robustness, and performance for wave propagation simulation in porous materials.

Conclusion: The proposed fully-mixed finite element scheme provides an effective and robust approach for simulating dynamic poroelasticity problems, particularly suitable for wave propagation analysis in porous materials with guaranteed stability under various parameter regimes.

Abstract: In this work, we present and analyze a fully-mixed finite element scheme for the dynamic poroelasticity problem in the low-frequency regime. We write the problem as a four-field, first-order, hyperbolic system of equations where the symmetry constraint on the stress field is imposed via penalization. This strategy is equivalent to adding a perturbation to the saddle point system arising when the stress symmetry is weakly-imposed. The coupling of solid and fluid phases is discretized by means of stable mixed elements in space and implicit time advancing schemes. The presented stability analysis is fully robust with respect to meaningful cases of degenerate model parameters. Numerical tests validate the convergence and robustness and assess the performances of the method for the simulation of wave propagation phenomena in porous materials.

</details>


### [7] [Variational-hemivariational inequalities: A brief survey on mathematical theory and numerical analysis](https://arxiv.org/abs/2512.10204)
*Weimin Han*

Main category: math.NA

TL;DR: Survey paper on variational-hemivariational inequalities covering well-posedness analysis, numerical methods, and applications in mechanics and engineering.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive survey of recent developments in variational-hemivariational inequalities, which extend variational inequalities and are valuable for modeling non-smooth, set-valued relations in physical sciences and engineering.

Method: Presents theoretical results for abstract stationary variational-hemivariational inequalities with accessible existence/uniqueness proofs, compares three mechanical problems (variational equation, inequality, and hemivariational inequality), and discusses mixed formulations and numerical solutions for nonstationary/history-dependent cases.

Result: Systematic overview of well-posedness analysis and numerical methods for variational-hemivariational inequalities, with mechanical examples illustrating different mathematical formulations and applications in fluid mechanics.

Conclusion: Variational-hemivariational inequalities represent a significant extension of variational inequalities with broad applications in engineering and physics, requiring specialized mathematical tools for analysis and numerical solution of problems involving non-smooth, set-valued relations.

Abstract: Variational-hemivariational inequalities are an area full of interesting and challenging mathematical problems. The area can be viewed as a natural extension of that of variational inequalities. Variational-hemivariational inequalities are valuable for application problems from physical sciences and engineering that involve non-smooth and even set-valued relations, monotone or non-monotone, among physical quantities. In the recent years, there has been substantial growth of research interest in modeling, well-posedness analysis, development of numerical methods and numerical algorithms of variational-hemivariational inequalities. This survey paper is devoted to a brief account of well-posedness and numerical analysis results for variational-hemivariational inequalities. The theoretical results are presented for a family of abstract stationary variational-hemivariational inequalities and the main idea is explained for an accessible proof of existence and uniqueness. To better appreciate the distinguished feature of variational-hemivariational inequalities, for comparison, three mechanical problems are introduced leading to a variational equation, a variational inequality, and a variational-hemivariational inequality, respectively. The paper also comments on mixed variational-hemivariational inequalities, with examples from applications in fluid mechanics, and on results concerning the numerical solution of other types (nonstationary, history dependent) of variational-hemivariational inequalities.

</details>


### [8] [Convergence analysis of contrast source inversion type methods for acoustic inverse medium scattering problems](https://arxiv.org/abs/2512.10260)
*Qiao Hu,Bo Zhang,Haiwen Zhang*

Main category: math.NA

TL;DR: The paper proposes two new iteratively regularized CSI-type methods (IRCSI and IRSOM) with ℓ₁ proximal terms, proving their global convergence for nonlinear inverse scattering problems - the first such convergence result for fixed-frequency iterative methods.


<details>
  <summary>Details</summary>
Motivation: CSI-type methods (CSI and SOM) are efficient for inverse medium scattering but lack rigorous convergence proofs. The authors aim to develop regularized versions with provable convergence while maintaining similar computational complexity.

Method: Two new algorithms: IRCSI (iteratively regularized CSI) and IRSOM (iteratively regularized SOM) with novel ℓ₁ proximal terms as regularization. These maintain similar computational complexity to original CSI/SOM methods while adding regularization.

Result: The authors prove global convergence of both IRCSI and IRSOM under natural, weak conditions on the objective function. This is the first convergence result for iterative methods solving nonlinear inverse scattering problems with fixed frequency.

Conclusion: The proposed IRCSI-type methods combine efficiency of CSI-type approaches with provable convergence through ℓ₁ regularization, validated by numerical experiments showing both convergence and performance.

Abstract: The contrast source inversion (CSI) method and the subspace-based optimization method (SOM) are first proposed in 1997 and 2009, respectively, and subsequently modified. The two methods and their variants share several properties and thus are called the CSI-type methods. The CSI-type methods are efficient and popular methods for solving inverse medium scattering problems, but their rigorous convergence remains an open problem. In this paper, we propose two iteratively regularized CSI-type (IRCSI-type) methods with a novel $\ell_1$ proximal term as the iteratively regularized term: the iteratively regularized CSI (IRCSI) method and the iteratively regularized SOM (IRSOM) method, which have a similar computation complexity to the original CSI and SOM methods, respectively, and prove their global convergence under natural and weak conditions on the original objective function. To the best of our knowledge, this is the first convergence result for iterative methods of solving nonlinear inverse scattering problems with a fixed frequency. The convergence and performance of the two IRCSI-type algorithms are illustrated by numerical experiments.

</details>


### [9] [Matrix approach to the fractional calculus](https://arxiv.org/abs/2512.10330)
*V. N. Kolokoltsov,E. L. Shishkina*

Main category: math.NA

TL;DR: New matrix-based construction of fractional derivatives/integrals with respect to functions using operator discretization and semigroup theory.


<details>
  <summary>Details</summary>
Motivation: To develop a powerful analytical and numerical tool for fractional calculus by creating a matrix approximation approach that preserves semigroup structure, enabling efficient computation of fractional powers of operators.

Method: 1) Start with differential operator w.r.t. a function generating a semigroup. 2) Discretize operator to get matrix approximation that also approximates the semigroup. 3) Apply Balakrishnan's representations of fractional powers using semigroups. 4) Use semigroup norm estimates and operator-matrix difference bounds to derive convergence rates. 5) Derive explicit formula for arbitrary powers of two-band matrices.

Result: 1) Developed matrix approximation method for fractional calculus operators. 2) Derived convergence rates for approximating fractional powers of operators using matrix approximations. 3) Obtained explicit formula for calculating arbitrary powers of two-band matrices, crucial for numerical solutions of fractional differential/integral equations.

Conclusion: The matrix-based approach provides a powerful framework for both analytical and numerical fractional calculus, with proven convergence properties and practical computational formulas for solving fractional equations.

Abstract: In this paper, we introduce the new construction of fractional derivatives and integrals with respect to a function, based on a matrix approach. We believe that this is a powerful tool in both analytical and numerical calculations. We begin with the differential operator with respect to a function that generates a semigroup. By discretizing this operator, we obtain a matrix approximation. Importantly, this discretization provides not only an approximating operator but also an approximating semigroup. This point motivates our approach, as we then apply Balakrishnan's representations of fractional powers of operators, which are based on semigroups. Using estimates of the semigroup norm and the norm of the difference between the operator and its matrix approximation, we derive the convergence rate for the approximation of the fractional power of operators with the fractional power of correspondings matrix operators. In addition, an explicit formula for calculating an arbitrary power of a two-band matrix is obtained, which is indispensable in the numerical solution of fractional differential and integral equations.

</details>


### [10] [Second order reduced model via incremental projection for Navier Stokes](https://arxiv.org/abs/2512.10473)
*Mejdi Azaïez,Yayu Guo,Carlos Núñez Fernández,Samuele Rubino,Chuanju Xu*

Main category: math.NA

TL;DR: A reduced-order modeling approach using incremental projection schemes for Stokes equations with POD, achieving second-order time convergence and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Numerical simulation of incompressible flows is challenging due to tight velocity-pressure coupling; projection methods offer effective decoupling for large-scale computations.

Method: Reduced-order modeling using incremental projection schemes for Stokes equations with BDF2 time discretization and finite elements in space, employing POD for model reduction.

Result: Method enables explicit computation of reduced velocity and pressure while preserving accuracy; stability analysis and error estimates show second-order convergence in time.

Conclusion: The proposed POD-based reduced-order model with incremental projection schemes is effective for Stokes equations, validated through numerical experiments showing computational efficiency.

Abstract: The numerical simulation of incompressible flows is challenging due to the tight coupling of velocity and pressure. Projection methods offer an effective solution by decoupling these variables, making them suitable for large-scale computations. This work focuses on reduced-order modeling using incremental projection schemes for the Stokes equations. We present both semi-discrete and fully discrete formulations, employing BDF2 in time and finite elements in space. A proper orthogonal decomposition (POD) approach is adopted to construct a reduced-order model for the Stokes problem. The method enables explicit computation of reduced velocity and pressure while preserving accuracy. We provide a detailed stability analysis and derive error estimates, showing second-order convergence in time. Numerical experiments are conducted to validate the theoretical results and demonstrate computational efficiency.

</details>


### [11] [Analysis of discrete energy-decay preserving schemes for Maxwell's equations in Cole-Cole dispersive medium](https://arxiv.org/abs/2512.10560)
*Guoyu Zhang,Ziming Dong,Baoli Yin,Yang Liu,Hong Li*

Main category: math.NA

TL;DR: Energy-decay preserving numerical scheme for Maxwell's equations in Cole-Cole dispersive media with θ-parameterized temporal discretization that maintains physical energy dissipation properties.


<details>
  <summary>Details</summary>
Motivation: To develop numerical schemes for Maxwell's equations in Cole-Cole dispersive media that preserve the physical energy-decay property, which is crucial for accurate long-time simulations and maintaining physical fidelity.

Method: First establishes continuous energy-decay law through modified energy functional, then proposes novel θ-scheme for temporal discretization. The scheme preserves discrete energy dissipation when θ∈[α/2, 1/2] and achieves different convergence rates based on θ value.

Result: The SFTR-θ scheme demonstrates superior performance in maintaining monotonic energy decay compared to alternative 2nd-order fractional backward difference formula, especially in long-time simulations. Convergence rates: first-order for θ≠0.5, second-order for θ=0.5.

Conclusion: The proposed θ-scheme successfully preserves energy-decay properties for Maxwell's equations in Cole-Cole media, offering robust and physically faithful numerical simulation with tunable accuracy through θ parameter.

Abstract: This work investigates the design and analysis of energy-decay preserving numerical schemes for Maxwell's equations in a Cole-Cole (C-C) dispersive medium. A continuous energy-decay law is first established for the C-C model through a modified energy functional. Subsequently, a novel \(θ\)-scheme is proposed for temporal discretization, which is rigorously proven to preserve a discrete energy dissipation property under the condition \(θ\in [\fracα{2}, \frac{1}{2}]\). The temporal convergence rate of the scheme is shown to be first-order for \(θ\neq 0.5\) and second-order for \(θ= 0.5\). Extensive numerical experiments validate the theoretical findings, including convergence tests and energy-decay comparisons. The proposed SFTR-\(θ\) scheme demonstrates superior performance in maintaining monotonic energy decay compared to an alternative 2nd-order fractional backward difference formula, particularly in long-time simulations, highlighting its robustness and physical fidelity.

</details>


### [12] [Dynamically consistent finite volume scheme for a bimonomeric simplified model with inflammation processes for Alzheimer's disease](https://arxiv.org/abs/2512.10716)
*Juan Barajas-Calonge,Mauricio A. Sepulveda Cortes,Nicolas Torres,Luis Miguel Villada*

Main category: math.NA

TL;DR: A finite volume scheme is developed for an Alzheimer's disease progression model, with convergence to weak solutions and dynamic consistency with the spatially homogeneous version.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for simulating Alzheimer's disease progression that captures complex biological interactions while maintaining mathematical properties like non-negativity and convergence.

Method: Developed a finite volume scheme for a convection-diffusion-reaction system (4 PDEs + 1 ODE) modeling AD progression, with semi-implicit discretization of reaction terms and nonstandard discretization for spatial homogeneity.

Result: Proved existence of discrete solutions, non-negativity preservation, a priori bounds, convergence to admissible weak solutions, and dynamic consistency with the spatially homogeneous model.

Conclusion: The finite volume scheme provides a mathematically sound and biologically consistent numerical method for simulating Alzheimer's disease progression, validated through numerical experiments.

Abstract: A model of progression of Alzheimer's disease (AD) incorporating the interactions of A$β$-monomers, oligomers, microglial cells and interleukins with neurons is considered. The resulting convection-diffusion-reaction system consists of four partial differential equations (PDEs) and one ordinary differential equation (ODE). We develop a finite volume (FV) scheme for this system, together with non-negativity and a priori bounds for the discrete solution, so that we establish the existence of a discrete solution to the FV scheme. It is shown that the scheme converges to an admissible weak solution of the model. The reaction terms of the system are discretized using a semi-implicit strategy that coincides with a nonstandard discretization of the spatially homogeneous (SH) model. This construction enables us to prove that the FV scheme is dynamically consistent with respect to the spatially homogeneous version of the model. Finally, numerical experiments are presented to illustrate the model and to assess the behavior of the FV scheme.

</details>


### [13] [A Stabilized Finite Element Method for Morpho-Visco-Poroelastic Model](https://arxiv.org/abs/2512.10718)
*Sabia Asghar,Duncan den Bakker,Etelvina Javierre,Qiyao Peng,Fred J. Vermolen*

Main category: math.NA

TL;DR: A mathematical model combining elastic, viscous, porous effects with growth/shrinkage is analyzed for stability and numerical stabilization to avoid oscillations.


<details>
  <summary>Details</summary>
Motivation: To model tissue/tumor growth and dermal contraction phenomena where microstructural changes cause growth or shrinkage, requiring analysis of combined elastic, viscous, and porous effects.

Method: Developed a mathematical model combining elastic, viscous, and porous effects with growth/shrinkage, analyzed stability of equilibria for continuous and semi-discrete versions, derived numerical monotonicity condition, and proposed stabilization method to avoid spurious oscillations.

Result: Stability analysis of equilibria performed, numerical stabilization method derived to prevent oscillations, confirmed by computer simulations, and total variation evaluated as function of stabilization parameter for quantitative assessment.

Conclusion: The model successfully captures complex tissue growth phenomena, with stability analysis and numerical stabilization techniques ensuring reliable computational solutions without spurious oscillations.

Abstract: We propose a mathematical model that combines elastic, viscous and porous effects with growth or shrinkage due to microstructural changes. This phenomenon is important in tissue or tumor growth, as well as in dermal contraction. Although existence results of the solution to the problem are not given, the current study assesses stability of the equilibria for both the continuous and semi-discrete versions of the model. Furthermore, a numerical condition for monotonicity of the numerical solution is described, as well as a way to stabilize the numerical solution so that spurious oscillations are avoided. The derived stabilization result is confirmed by computer simulations. In order to have a more quantitative picture, the total variation has been evaluated as a function of the stabilization parameter.

</details>


### [14] [Physics-Informed Learning of Microvascular Flow Models using Graph Neural Networks](https://arxiv.org/abs/2512.10792)
*Paolo Botta,Piermario Vitullo,Thomas Ventimiglia,Andreas Linninger,Paolo Zunino*

Main category: math.NA

TL;DR: A deep learning approach using Graph Neural Networks (GNNs) to create efficient surrogate models for simulating blood flow in complex microvascular networks, achieving accurate results with significant computational speedup.


<details>
  <summary>Details</summary>
Motivation: Simulating blood flow in realistic microvascular networks is challenging due to multiscale nature and topological complexity of capillary networks, requiring computationally expensive full-order solvers.

Method: Proposes a GNN-based reduced-order modeling strategy trained on synthetic microvascular graphs with physics-informed training that integrates graph topology and local flow dynamics, enforcing mass conservation and rheological constraints through physics-informed loss functions.

Result: The GNN architecture demonstrates robust generalization across diverse network configurations, accurately reconstructs pressure and velocity fields for linear/nonlinear rheology problems, and shows substantial computational gains over full-order solvers, validated on mouse cerebral cortex data.

Conclusion: Establishes a new class of physics-grounded graph-based surrogate models for microvascular flow with inductive biases for mass conservation and rheology, enabling real-time inference for vascular modeling and biomedical applications.

Abstract: The simulation of microcirculatory blood flow in realistic vascular architectures poses significant challenges due to the multiscale nature of the problem and the topological complexity of capillary networks. In this work, we propose a novel deep learning-based reduced-order modeling strategy, leveraging Graph Neural Networks (GNNs) trained on synthetic microvascular graphs to approximate hemodynamic quantities on anatomically realistic domains. Our method combines algorithms for synthetic vascular generation with a physics-informed training procedure that integrates graph topological information and local flow dynamics. To ensure the physical reliability of the learned surrogates, we incorporate a physics-informed loss functional derived from the governing equations, allowing enforcement of mass conservation and rheological constraints. The resulting GNN architecture demonstrates robust generalization capabilities across diverse network configurations. The GNN formulation is validated on benchmark problems with linear and nonlinear rheology, showing accurate pressure and velocity field reconstruction with substantial computational gains over full-order solvers. The methodology showcases significant generalization capabilities with respect to vascular complexity, as highlighted by tests on data from the mouse cerebral cortex. This work establishes a new class of graph-based surrogate models for microvascular flow, grounded in physical laws and equipped with inductive biases that mirror mass conservation and rheological models, opening new directions for real-time inference in vascular modeling and biomedical applications.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Regularity of the free boundary for the supercooled Stefan problem in arbitrary dimensions](https://arxiv.org/abs/2512.10136)
*Max Engelstein,Inwon Kim,Sebastian Munoz*

Main category: math.AP

TL;DR: The paper provides the first free boundary regularity theory for the supercooled Stefan problem, showing the free boundary decomposes into regular, singular, and jump components with controlled dimensions, and proves it's a continuously differentiable graph where singularities always occur with infinite speed.


<details>
  <summary>Details</summary>
Motivation: The supercooled Stefan problem models solidification below freezing temperature, where physical experiments suggest fractal freezing sets, infinite-speed propagation, and nucleation phenomena. Despite these complex behaviors, there was no rigorous free boundary regularity theory for this problem in arbitrary dimensions.

Method: The authors decompose the free boundary into three components: (1) regular part advancing with finite speed, (2) singular part with controlled space-time dimension (≤ d-1 parabolic dimension) where infinite speed or nucleation occurs, and (3) jump component contained in a space-time smooth graph occurring at zero-dimensional times. They prove the free boundary is the graph of a continuously differentiable freezing time function.

Result: The free boundary has robust structure: it's a continuously differentiable graph t=s(x), singular set coincides with critical set of s, and singularities always occur with infinite speed. Each of the three components (regular, singular, jump) can be nonempty, as shown by examples.

Conclusion: This work establishes the first free boundary regularity theory for the supercooled Stefan problem in arbitrary dimensions, providing a comprehensive structural decomposition of the free boundary and proving that singularities are characterized by infinite speed propagation.

Abstract: We study the free boundary in the supercooled Stefan problem, a classical model for the solidification of water below its freezing temperature. In contrast with the melting problem, physical experiments and heuristics indicate that the water--ice interface in the supercooled problem may exhibit fractal freezing sets, infinite-speed propagation of the frozen front, and nucleation (the spontaneous appearance of ice). Despite this, we show that the free boundary has a robust structure.
  We decompose the free boundary into three parts: (1) a regular part that advances with finite speed in time; (2) a singular part consisting of points where the front attains infinite speed or nucleates, but with controlled space-time (i.e., $\leq d-1$ parabolic) dimension; and (3) a jump component, which can have large dimension in a time slice, but which is contained in a space-time smooth graph and occurs only at a zero-dimensional set of times. Examples show that each of these parts can be nonempty.
  Furthermore, we prove that the free boundary is the graph $t=s(x)$ of a continuously differentiable freezing time $s$, and the singular set coincides with the critical set of $s$, proving that singularities in supercooled freezing always occur with infinite speed.
  These results provide the first free boundary regularity theory for the supercooled Stefan problem in arbitrary dimensions.

</details>


### [16] [The supercooled Stefan problem: fractal freezing and the fine structure of maximal solutions](https://arxiv.org/abs/2512.10138)
*Raymond Chu,Inwon Kim,Sebastian Munoz*

Main category: math.AP

TL;DR: The paper analyzes the supercooled Stefan problem in arbitrary dimensions, studying general solutions with fractal freezing patterns and establishing regularity properties of maximal solutions obtained by optimizing average freezing time.


<details>
  <summary>Details</summary>
Motivation: To understand the mathematical properties of the supercooled Stefan problem, particularly the irregularities in general solutions and the regularity of maximal solutions, bridging theoretical analysis with physical observations of freezing phenomena.

Method: Uses a novel Markovian gluing principle to analyze general solutions, then studies maximal solutions by maximizing average freezing time, applies obstacle problem theory for regularity analysis, and examines radial and one-dimensional cases.

Result: General solutions show generic fractal freezing and nucleation, while maximal solutions have transition zones that are open modulo low-dimensional sets, allowing finer regularity analysis. Maximal solutions are generally non-universal but become universal and minimize nucleation in radial and 1D settings.

Conclusion: The maximal solution approach provides a framework for understanding regularity in the supercooled Stefan problem, with radial and one-dimensional cases showing universal behavior that aligns with physical observations of minimized nucleation.

Abstract: We study the supercooled Stefan problem in arbitrary dimensions. First, we study general solutions and their irregularities, showing generic fractal freezing and nucleation, based on a novel Markovian gluing principle. In contrast, we then establish regularity properties of maximal solutions, which are obtained by maximizing a suitable notion of "average" freezing time. Unexpectedly, we show that maximal solutions have a transition zone that is open modulo a low-dimensional set: this allows us to apply obstacle problem theory for a finer regularity analysis. We further show that maximal solutions are in general non-universal, and we obtain sharp stability results under perturbation of each maximal solution. Lastly, we study maximal solutions in both the radial and the one-dimensional setting. We show that in these cases the maximal solution is universal and minimizes nucleation, in agreement with phenomena observed in the physics literature.

</details>


### [17] [Parabolic Frequency on Gaussian Spaces and Unique Continuation](https://arxiv.org/abs/2512.10139)
*Jin Sun,Kui Wang*

Main category: math.AP

TL;DR: The paper establishes an almost-monotonicity formula for parabolic frequency on Gaussian spaces for solutions of the Ornstein-Uhlenbeck heat equation with lower-order terms, requiring only bounded b and linear growth c, extending previous results and deriving strong unique continuation principles.


<details>
  <summary>Details</summary>
Motivation: Classical monotonicity formulas for parabolic equations typically require bounded coefficients, but many natural problems involve unbounded coefficients. The authors aim to extend these results to Gaussian spaces with weaker assumptions on coefficients, particularly allowing linear growth in the potential term c.

Method: Develops a weighted L² framework using the backward Mehler kernel as a weight, which naturally encodes the Gaussian measure and compensates for unbounded coefficients. This approach allows handling solutions with at most exponential quadratic growth while only requiring bounded b and linear growth c.

Result: Establishes an almost-monotonicity formula for parabolic frequency on Gaussian spaces under weaker conditions than classical results. Derives the strong unique continuation principle from this monotonicity, extending Poon's results and complementing Colding and Minicozzi's geometric generalizations.

Conclusion: The weighted L² framework with backward Mehler kernel provides a robust method for handling unbounded coefficients in Gaussian spaces, enabling unique continuation results for equations with potentials exhibiting quadratic growth or certain singularities, significantly extending the applicability of monotonicity formulas.

Abstract: We establish an almost-monotonicity formula for a parabolic frequency on Gaussian spaces for solutions of the Ornstein-Uhlenbeck heat equation with lower-order terms: $$\partial_t u = L_γu + b(x,t) \cdot \nabla u + c(x,t)u, $$ where $L_γ= Δ- x \cdot \nabla$ is the Ornstein-Uhlenbeck operator. In contrast to classical results that require $b$ and $c$ to be bounded, we only assume that $b$ is bounded and $c$ satisfies a linear growth condition, while the solution $u$ is allowed to have at most exponential quadratic growth. The key innovation is a weighted $L^2$ framework that uses the backward Mehler kernel as a weight, which naturally encodes the underlying measure and compensates for the unbounded coefficients. From the frequency monotonicity, we derive the strong unique continuation principle. This extends Poon's seminal results and complements recent geometric generalizations by Colding and Minicozzi in the context of Gaussian measure spaces. We further apply our framework to establish unique continuation for equations with potentials exhibiting quadratic growth or certain singularities.

</details>


### [18] [Topological degree for negative fractional Kazdan--Warner equation on finite graphs](https://arxiv.org/abs/2512.10295)
*Yang Liu,Liang Shan,Mengjie Zhang*

Main category: math.AP

TL;DR: Extends fractional Kazdan-Warner equations to graphs, using topological degree theory to prove existence/multiplicity of solutions in negative case.


<details>
  <summary>Details</summary>
Motivation: Fractional Kazdan-Warner equations on graphs are underexplored compared to classical case, creating a research gap that needs addressing.

Method: Uses topological degree theory to investigate fractional Kazdan-Warner equation in negative case on connected finite graphs.

Result: Proves existence and multiplicity of solutions, extends Liu & Yang (2020) to fractional setting, provides concise proof for Shan & Liu (2025).

Conclusion: Successfully bridges gap in fractional Kazdan-Warner equations on graphs, offering both theoretical extension and improved proof techniques.

Abstract: Studies on Kazdan--Warner equations on graphs have grown steadily, yet the fractional case remains insufficiently explored. Using topological degree theory, this work investigates the fractional Kazdan--Warner equation in the negative case on connected finite graphs, focusing on the existence and multiplicity of solutions. This work not only extends the earlier result of S. Liu and Yang (2020) to the fractional setting, but also provides a concise proof for the work of Shan and Y. Liu (2025).

</details>


### [19] [Vortex atmospheres of traveling vortices: rigorous definition, existence, and topological classification](https://arxiv.org/abs/2512.10412)
*Kyudong Choi,In-Jee Jeong,Young-Jin Sim*

Main category: math.AP

TL;DR: Rigorous mathematical definition and proof of vortex atmosphere existence/uniqueness, comparing 2D dipole vs 3D ring atmospheres with topological distinctions.


<details>
  <summary>Details</summary>
Motivation: Vortex atmosphere phenomenon recognized since 19th century but lacked rigorous mathematical definitions and proofs; previous studies relied on approximations rather than exact analysis.

Method: Define vortex atmosphere rigorously, prove existence/uniqueness, characterize atmosphere as specific superlevel set of corresponding stream function, compare planar (2D dipole) vs axisymmetric (3D ring) configurations.

Result: Established rigorous definition of vortex atmosphere, proved existence and uniqueness, demonstrated topological distinctions: 2D dipole atmosphere forms oval-shaped region, 3D ring atmospheres can be spheroidal or toroidal.

Conclusion: Provides first rigorous mathematical foundation for vortex atmosphere concept, confirms historical observations by Hicks about topological differences between 2D and 3D vortex atmospheres.

Abstract: In incompressible and inviscid fluids, the vortex atmosphere refers to the collection of fluid particles outside the support of a traveling vortex that are nevertheless carried along with it. This phenomenon has been recognized since the nineteenth century, e.g., in the classical works of O. Reynolds [Nature, 1876] and O. Lodge [Lond. Edinb. Dubl. Phil. Mag., 1885], yet rigorous mathematical definitions and proofs have remained largely undeveloped, with most subsequent studies relying on thin-core approximations or asymptotic analyses. In this paper, we give a rigorous definition of a vortex atmosphere and establish its existence and uniqueness. We further compare the planar atmosphere surrounding a 2D vortex dipole with the axisymmetric atmosphere surrounding a 3D vortex ring. In particular, we emphasize and prove the topological distinctions observed by W. Hicks [Lond. Edinb. Dubl. Phil. Mag., 1919]: under natural assumptions, every 2D dipole with its atmosphere forms an oval-shaped region, whereas for 3D rings, both spheroidal and toroidal configurations may occur. Our proof is based on showing that each atmosphere can be characterized precisely as a specific superlevel set of its corresponding stream function.

</details>


### [20] [Asymptotic Sphere Concentration at Infinity for NLS with L^2 Constraint](https://arxiv.org/abs/2512.10512)
*Qing Guo,Chongyang Tian*

Main category: math.AP

TL;DR: Existence of normalized solutions to NLS with L²-mass concentrating on spheres with diverging radii for all n≥2, p>1, extending previous 2D results.


<details>
  <summary>Details</summary>
Motivation: Study attractive Bose-Einstein condensates modeled by nonlinear Schrödinger equation with L²-normalization constraint, focusing on concentration phenomena that escape to infinity rather than remaining on compact hypersurfaces.

Method: Combines tailored finite-dimensional reduction with blow-up analysis based on Pohozaev identities, developing new approximation scheme and functional setting adapted to high-dimensional sphere-at-infinity concentration regime.

Result: Proves existence of normalized solutions whose L²-mass concentrates on spheres with radii diverging to infinity for all dimensions n≥2 and exponents p>1.

Conclusion: Establishes qualitatively different concentration regime from classical point-concentration and unconstrained problems, extending previous 2D results to higher dimensions with new techniques.

Abstract: We consider the nonlinear Schrödinger equation$$-Δu + V(x)\,u = a\,u^p + μu \quad \text{in }\mathbb{R}^n,\qquad \int_{\mathbb{R}^n} u^2 = 1,$$modeling attractive Bose--Einstein condensates. For all dimensions $n\ge 2$ and all exponents $p>1$, we prove the existence of normalized solutions whose $L^2$-mass concentrates on spheres with radii diverging to infinity. In particular, the concentration set escapes to infinity rather than remaining on a fixed compact hypersurface, which makes our regime qualitatively different both from classical point-concentration phenomena and from concentrating profiles in unconstrained problems. Our approach combines a tailored finite-dimensional reduction with a blow-up analysis based on Pohozaev identities and, in this way, extends the two-dimensional mass-critical result for $(n,p)=(2,3)$ obtained in Guo--Tian--Zhou (Calc.\ Var.\ Partial Differential Equations, 2022). The proof in that paper relies in an essential way on the two-dimensional structure and does not directly apply in higher dimensions, whereas here we develop a different approximation scheme and functional setting adapted to the high-dimensional sphere-at-infinity concentration regime.

</details>


### [21] [Asymptotic analysis of fractional Sobolev spaces on thin films in the low-integrability regime](https://arxiv.org/abs/2512.10620)
*Andrea Braides,Andrea Pinamonti,Margherita Solci*

Main category: math.AP

TL;DR: Fractional Sobolev spaces on thin films converge to higher-order Sobolev spaces on the base domain as thickness vanishes, with asymptotic behavior at boundary regularity limits.


<details>
  <summary>Details</summary>
Motivation: To understand the limiting behavior of fractional Sobolev spaces on thin domains (thin films) as the thickness goes to zero, connecting thin-film geometry with dimension reduction in fractional calculus.

Method: Use dimension-reduction convergence framework with scaled Gagliardo seminorms to prove equicoerciveness of functionals; analyze asymptotic limits as s approaches 0 and 1/2.

Result: Fractional Sobolev spaces H^s(Ω_ε) on thin films converge to H^{s+1/2}(ω) as ε→0; asymptotic results established for s→0+ and s→1/2-.

Conclusion: Thin-film geometry induces a gain of 1/2 derivative in the limiting fractional Sobolev space, with precise asymptotic behavior at the regularity boundaries.

Abstract: We study the behaviour of fractional Sobolev spaces $H^s(Ω_\varepsilon)$ with $s\in(0,1/2)$ defined on ``thin films'' $Ω_\varepsilon=ω\times (0,\varepsilon)$ in $\mathbb R^d$, and prove that they tend to the space $H^{s+\frac12}(ω)$ as $\varepsilon\to 0$. This is made precise by using a notion of dimension-reduction convergence, with respect to which suitably scaled Gagliardo seminorms define equicoercive functionals. Asymptotic results are proved for $s\to 0^+$ and $s\to 1/2^-$.

</details>


### [22] [The $\ell^p$-boundedness of wave operators for the fourth order Schrödinger operators on the lattice $\mathbb{Z}$](https://arxiv.org/abs/2512.10649)
*Sisi Huang,Xiaohua Yao*

Main category: math.AP

TL;DR: The paper proves ℓ^p boundedness of wave operators for discrete fourth-order Schrödinger operators on ℤ and derives sharp ℓ^p-ℓ^{p'} decay estimates for discrete beam equations.


<details>
  <summary>Details</summary>
Motivation: To establish functional analytic properties of wave operators associated with discrete higher-order Schrödinger operators, which are important for understanding scattering theory and dispersive estimates on lattices.

Method: Uses asymptotic expansions of the resolvent near thresholds (0 and 16), discrete singular integral theory, and analysis of zero resonance types under decay assumptions on the potential V.

Result: Wave operators W_±(H,Δ²) are bounded on ℓ^p(ℤ) for all 1<p<∞, but not on endpoint spaces ℓ¹ or ℓ^∞ when thresholds are regular. Sharp ℓ^p-ℓ^{p'} decay estimates for discrete beam equations are derived.

Conclusion: The paper establishes comprehensive ℓ^p boundedness results for wave operators of discrete fourth-order Schrödinger operators and applies these to obtain optimal dispersive decay estimates for discrete beam equations.

Abstract: This paper investigates the $\ell^p$ boundedness of wave operators $W_\pm(H,Δ^2)$ associated with discrete fourth-order Schrödinger operators $H = Δ^2 + V$ on the lattice $\mathbb{Z}$, where $$(Δφ)(n)=φ(n+1)+φ(n-1)-2φ(n),\quad n\in\mathbb{Z},$$ and $V(n)$ is a real-valued potential on $\mathbb{Z}$. Under suitable decay assumptions on $V$ (depending on the types of zero resonance of $H$), we show that the wave operators $W_{\pm}(H, Δ^2)$ are bounded on $\ell^p(\mathbb{Z})$ for all $1 < p < \infty$: $$ \|W_{\pm}(H, Δ^2) f\|_{\ell^p(\mathbb{Z})} \lesssim \|f\|_{\ell^p(\mathbb{Z})}. $$ In particular, if both thresholds $0$ and $16$ are regular points of $H$, we prove that $W_{\pm}(H, Δ^2)$ are neither bounded on the endpoint space $\ell^1(\mathbb{Z})$ nor on $\ell^\infty(\mathbb{Z})$. We remark that the proof of these bounds relies fundamentally on the asymptotic expansions of the resolvent of $H$ near the thresholds $0$ and $16$, and on the theory of {\it discrete singular integrals} on the lattice.
  As applications, we derive the following sharp $\ell^p-\ell^{p'}$ decay estimates for solutions to the discrete beam equation with a parameter $a\in \mathbb{R}$ on the lattice $\mathbb{Z}$: $$ \|{\rm cos}(t\sqrt {H+a^2})P_{ac}(H)\|_{\ell^p\rightarrow\ell^{p'}}+\left\|\frac{{\rm sin}(t\sqrt {H+a^2})}{t\sqrt {H+a^2}}P_{ac}(H)\right\|_{\ell^p\rightarrow\ell^{p'}}\lesssim|t|^{-\frac{1}{3}(\frac{1}{p}-\frac{1}{p'})},\quad t\neq0, $$ where $1<p\le 2$, ${p'}$ is the conjugated index of $p$ and $P_{ac}(H)$ denotes the spectral projection onto the absolutely continuous spectrum space of $H$.

</details>


### [23] [On the ground state of the nonlinear Schr{ö}dinger equation: asymptotic behavior at the endpoint powers](https://arxiv.org/abs/2512.10690)
*Rémi Carles,Quentin Chauleur,Guillaume Ferriere,Dmitry Pelinovsky*

Main category: math.AP

TL;DR: Analysis of ground state behaviors in nonlinear Schrödinger equation at endpoint nonlinearity powers, showing convergence to Gaussian (Gausson) and algebraic soliton limits with explicit bounds and asymptotics.


<details>
  <summary>Details</summary>
Motivation: To understand the limiting behaviors of ground states in the nonlinear Schrödinger equation at extreme values of the nonlinearity parameter, particularly how they approach known special solutions like the Gausson and algebraic solitons.

Method: Mathematical analysis of radially symmetric, exponentially decaying solutions with appropriate rescaling to obtain non-trivial limits. Proves strong convergence with explicit bounds and provides detailed asymptotic expansions for both limiting cases.

Result: Demonstrates that ground states converge to Gaussian functions (Gaussons) at one endpoint and to Aubin-Talenti algebraic solitons (for dimensions ≥3) at the other endpoint, with rigorous convergence bounds and asymptotic descriptions.

Conclusion: The paper establishes precise limiting behaviors of nonlinear Schrödinger ground states at extreme nonlinearity parameters, connecting them to fundamental special solutions and providing both theoretical proofs and numerical verification.

Abstract: We consider the ground states of the nonlinear Schr{ö}dinger equation, which stand for radially symmetric and exponentially decaying solutions on the full space. We investigate their behaviors at both endpoint powers of the nonlinearity, up to some rescaling to infer non-trivial limits. One case corresponds to the limit towards a Gaussian function called Gausson, which is the ground state of the stationary logarithmic Schr{ö}dinger equation. The other case, for dimension at least three, corresponds to the limit towards the Aubin-Talenti algebraic soliton. We prove strong convergence with explicit bounds for both cases, and provide detailed asymptotics. These theoretical results are illustrated with numerical approximations.

</details>


### [24] [$Φ^4\_2$ theory limit of a many-body bosonic free energy](https://arxiv.org/abs/2512.10704)
*Lucas Jougla,Nicolas Rougerie*

Main category: math.AP

TL;DR: The paper proves convergence of quantum Gibbs free energy for interacting Bose gas to Φ⁴₂ NLS Gibbs measure in 2D, streamlining recent results with improved methods.


<details>
  <summary>Details</summary>
Motivation: To rigorously connect quantum statistical mechanics of interacting Bose gases with classical field theory descriptions, specifically establishing the Φ⁴₂ non-linear Schrödinger-Gibbs measure as the correct limiting object.

Method: Combines variational method of Lewin-Nam-Rougerie to connect quantum free energy to classical Hartree-Gibbs with smeared non-linearity, then uses arguments of Fröhlich-Knowles-Schlein-Sohinger for convergence to Φ⁴₂ free energy.

Result: Proves that free-energy of interacting Bose gas (relative to non-interacting one) converges to free energy of Φ⁴₂ non-linear Schrödinger-Gibbs measure in appropriate scaling limits.

Conclusion: The work provides a streamlined proof connecting quantum Bose gases to classical field theory, revisiting and simplifying recent results while establishing rigorous convergence to the Φ⁴₂ measure.

Abstract: We consider the quantum Gibbs state of an interacting Bose gas on the 2D torus. We set temperature, chemical potential and coupling constant in a regime where classical field theory gives leading order asymptotics. In the same limit, the repulsive interaction potential is set to be short-range: it converges to a Dirac delta function with a rate depending polynomially on the other scaling parameters. We prove that the free-energy of the interacting Bose gas (counted relatively to the non-interacting one) converges to the free energy of the $Φ^4\_2$ non-linear Schr{ö}dinger-Gibbs measure, thereby revisiting recent results and streamlining proofs thereof. We combine the variational method of Lewin-Nam-Rougerie to connect, with controled error, the quantum free energy to a classical Hartree-Gibbs one with smeared non-linearity. The convergence of the latter to the $Φ^4\_2$ free energy then follows from arguments of Fr{ö}hlich-Knowles-Schlein-Sohinger. This derivation parallels recent results of Nam-Zhu-Zhu.

</details>


### [25] [Observability inequality for the von Neumann equation in crystals](https://arxiv.org/abs/2512.10897)
*Thomas Borsoni,Virginie Ehrlacher*

Main category: math.AP

TL;DR: The paper establishes a uniform quantitative observability inequality for the von Neumann equation in periodic crystal settings, extending previous non-periodic results to quantum systems with lattice periodicity.


<details>
  <summary>Details</summary>
Motivation: To extend quantum observability results from non-periodic to periodic crystal settings, addressing the need for uniform control in small ħ regimes for quantum systems with lattice periodicity.

Method: Adapts Golse and Paul's (2022) stability argument between quantum (von Neumann) and classical (Liouville) dynamics using optimal transport-like pseudo-distance. Key adaptations include Bloch decomposition, periodic Schrödinger coherent states, periodic Töplitz operators, and periodic Husimi densities for the crystal setting.

Result: Successfully provides a quantitative observability inequality for the von Neumann equation on ℝ^d in crystal settings that remains uniform for small ħ values.

Conclusion: The framework for quantum-classical correspondence in observability can be extended to periodic systems by adapting mathematical tools to handle lattice periodicity while maintaining uniformity in the semiclassical limit.

Abstract: We provide a quantitative observability inequality for the von Neumann equation on $\mathbb{R}^d$ in the crystal setting, uniform in small $\hbar$. Following the method of Golse and Paul (2022) proving this result in the non-crystal setting, the method relies on a stability argument between the quantum (von Neumann) and classical (Liouville) dynamics and uses an optimal transport-like pseudo-distance between quantum and classical densities. Our contribution yields in the adaptation of all the required tools to the periodic setting, relying on the Bloch decomposition, notions of periodic Schrödinger coherent state, periodic Töplitz operator and periodic Husimi densities.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [26] [A Model-Guided Neural Network Method for the Inverse Scattering Problem](https://arxiv.org/abs/2512.10123)
*Olivia Tsang,Owen Melia,Vasileios Charisopoulos,Jeremy Hoskins,Yuehaw Khoo,Rebecca Willett*

Main category: physics.comp-ph

TL;DR: A physics-informed machine learning method for inverse medium scattering that incorporates explicit physics knowledge via a differentiable solver and uses progressive frequency refinement for stable reconstruction.


<details>
  <summary>Details</summary>
Motivation: Inverse medium scattering is ill-posed and nonlinear, with ML methods offering speed but struggling with highly nonlinear regimes and lacking explicit physics incorporation, while classical optimization approaches are computationally expensive.

Method: Proposes a machine learning framework with explicit physics knowledge through a differentiable solver representing the forward model, using progressive refinement of reconstructions with increasing wave frequencies to stabilize recovery.

Result: Empirically achieves high-quality reconstructions at a fraction of the computational or sampling costs compared to competing approaches.

Conclusion: The method successfully combines machine learning efficiency with explicit physics knowledge, providing stable and cost-effective solutions to inverse medium scattering problems.

Abstract: Inverse medium scattering is an ill-posed, nonlinear wave-based imaging problem arising in medical imaging, remote sensing, and non-destructive testing. Machine learning (ML) methods offer increased inference speed and flexibility in capturing prior knowledge of imaging targets relative to classical optimization-based approaches; however, they perform poorly in regimes where the scattering behavior is highly nonlinear. A key limitation is that ML methods struggle to incorporate the physics governing the scattering process, which are typically inferred implicitly from the training data or loosely enforced via architectural design. In this paper, we present a method that endows a machine learning framework with explicit knowledge of problem physics, in the form of a differentiable solver representing the forward model. The proposed method progressively refines reconstructions of the scattering potential using measurements at increasing wave frequencies, following a classical strategy to stabilize recovery. Empirically, we find that our method provides high-quality reconstructions at a fraction of the computational or sampling costs of competing approaches.

</details>


### [27] [Generative Modeling of Entangled Polymers with a Distance-Based Variational Autoencoder](https://arxiv.org/abs/2512.10131)
*Pietro Chiarantoni,Oscar Serra,Mohammad Erfan Mowlaei,Venkata Surya Kumar Choutipalli,Mark DelloStritto,Xinghua Shi,Micheal L. Klein,Vincenzo Carnevale*

Main category: physics.comp-ph

TL;DR: VAE framework learns and generates structured polymer globule configurations from distance matrices using MD-sampled polyethylene data.


<details>
  <summary>Details</summary>
Motivation: To develop a deep learning approach for learning and generating physically meaningful polymer globule configurations that can capture structural patterns and reproduce key physical observables.

Method: Combines coarse-grained molecular dynamics for sampling polyethylene structures, VAE with convolution and attention layers for encoding distance matrices into invariant latent space, and post-processing with multidimensional scaling and short MD for physical recovery.

Result: Model successfully encodes structural patterns into organized latent space, generates configurations that reproduce key observables (energy, size, entanglement) despite minor decoder discrepancies.

Conclusion: VAE framework with appropriate architecture and post-processing enables effective learning and generation of physically meaningful polymer globule configurations from distance matrices.

Abstract: We present a variational autoencoder framework for learning and generating configurations of structured polymer globules from distance matrices. We used coarse-grained molecular dynamics to sample polyethylene structures, which we used as the training set for our deep learning model. By combining convolution and attention layers, the model encodes the structural patterns of distance matrices into an organized and roto-translationally invariant latent space of lower dimensionality. The generative capability of the variational autoencoder, coupled with a post-processing pipeline based on multidimensional scaling and short molecular dynamics, enables the recovery of physically meaningful configurations. The reconstructed and generated samples reproduce key observables, including energy, size, and entanglement, despite minor discrepancies in the raw decoder output.

</details>


### [28] [Flow-priority optimization of additively manufactured variable-TPMS lattice heat exchanger based on macroscopic analysis](https://arxiv.org/abs/2512.10207)
*Kazutaka Yanagihara,Jun Iwasaki,Kiyoto Saso,Taichi Yamashita,Shomu Murakoshi,Akihiro Takezawa*

Main category: physics.comp-ph

TL;DR: Researchers optimized TPMS lattice heat exchangers by varying lattice density using macroscopic modeling, achieving 28.7% performance improvement over uniform lattices.


<details>
  <summary>Details</summary>
Motivation: While TPMS lattice structures improve heat transfer through uniform flow distribution and boundary layer disruption, uniform lattice configurations may not be optimal for macroscopic flow patterns. There's a need to optimize lattice distribution for better heat-exchange efficiency.

Method: Developed macroscopic modeling approach using Darcy-Forchheimer theory for flow analysis and volumetric heat-transfer coefficient for heat transfer. Used Primitive lattice isosurface threshold as design variable to optimize lattice distribution in a U-shaped planar heat exchanger, validated through geometric analysis and metal LPBF experiments.

Result: The optimized lattice distribution showed clear performance advantage over uniform lattice, with experimental results demonstrating 28.7% average enhancement in heat transfer performance.

Conclusion: Macroscopic modeling approach successfully optimizes TPMS lattice distribution in heat exchangers, significantly improving performance over uniform configurations, validating the effectiveness of the proposed optimization methodology.

Abstract: Heat exchangers incorporating triply periodic minimal surface (TPMS) lattice structures have attracted considerable research interest because they promote uniform flow distribution, disrupt boundary layers, and improve convective heat-transfer performance. However, from the perspective of forming a macroscopic flow pattern optimized for heat-exchange efficiency, a uniform lattice is not necessarily the optimal configuration. This study initially presents a macroscopic modeling approach for a two-fluid heat exchanger equipped with a TPMS Primitive lattice. The macroscopic flow analysis is conducted based on the Darcy--Forchheimer theory. Under the assumption that heat is transferred solely at the interface between the fluid and the TPMS walls, a macroscopic heat-transfer model is developed using a volumetric heat-transfer coefficient, which serves as an artificial property characterizing the unit-volume heat-transfer capability. To regulate the relative dominance of the hot and cold flows-effectively, the channel widths-within the heat exchanger, we adopt the isosurface threshold of the Primitive lattice as the design variable and construct an optimization scheme for the lattice distribution using the previously described macroscopic model. The optimization is subsequently carried out for a planar heat exchanger where the hot and cold fluids each follow U-shaped flow trajectories. The optimal solution was verified, and its validity was examined through detailed geometric analysis and experiments conducted using metal LPBF. The optimal solution derived from the macroscopic model also demonstrated a clear performance advantage over the uniform lattice in the experimental results. The optimal solution obtained from the macroscopic model also demonstrated a clear performance improvement over the uniform lattice, with an average enhancement of 28.7% in the experimental results.

</details>


### [29] [Ultra-Fast Muon Transport via Histogram Sampling on GPUs](https://arxiv.org/abs/2512.10520)
*Luis Felipe P. Cattelan,Shah Rukh Qasim,Patrick H. Owen,Nicola Serra*

Main category: physics.comp-ph

TL;DR: GPU-accelerated muon transport using histogram sampling achieves orders of magnitude speedup over CPU-based Geant4 simulations while preserving physical accuracy.


<details>
  <summary>Details</summary>
Motivation: To dramatically accelerate muon transport simulations by leveraging GPU parallel processing capabilities, overcoming the computational limitations of traditional CPU-based approaches like Geant4.

Method: Uses precomputed histograms of momentum loss and scattering from Geant4 simulations, implemented as a CUDA kernel with parallel algorithm for concurrent simulation of tens of thousands of particles, incorporating complex geometry and magnetic field integration via fourth-order Runge-Kutta method.

Result: Achieves speedups of several orders of magnitude compared to CPU-based Geant4 simulations, even when using large CPU farms with over a thousand cores, while preserving key physical features through validation against Geant4 in various geometries.

Conclusion: GPU-based implementations show significant potential for particle transport applications, with extensibility to neutrino propagation and future inclusion of discrete processes like particle decay.

Abstract: We present a GPU-accelerated method for muon transport based on histogram sampling that delivers orders of magnitude faster performance than CPU-based Geant4 simulation. Our method employs precomputed histograms of momentum loss and scattering, derived from detailed Geant4 simulations, to statistically reproduce all the non-decaying physics processes during muon traversal through matter. Implemented as a CUDA kernel, the parallel algorithm enables the concurrent simulation of tens of thousands of particles on a single GPU whilst taking into account a complex geometry and a magnetic field force integrated using a fourth-order Runge-Kutta method. Validation against Geant4 in both simple and realistic detector geometries shows that the approach preserves key physical features while achieving speedups of several orders of magnitude, even compared to CPU-based simulations on a large CPU farm with over a thousand cores. This work highlights the significant potential of GPU-based implementations for particle transport, with applicability extending to neutrino propagation and future implementations including discrete processes such as particle decay.

</details>


### [30] [MULE - A Co-Generation Fission Power Plant Concept to Support Lunar In-Situ Resource Utilisation](https://arxiv.org/abs/2512.10705)
*Julius Mercz,Philipp Reiss,Christian Reiter*

Main category: physics.comp-ph

TL;DR: A concept for a lunar fission power plant that directly heats molten salt electrolysis (MSE) plants for in-situ resource utilization while providing surplus electrical power for lunar bases.


<details>
  <summary>Details</summary>
Motivation: To enable sustained human presence on the Moon by developing robust in-situ resource utilization supply chains for consumables and propellant, requiring high-temperature processes like MSE (900°C+). Fission reactors are ideal for lunar power generation due to independence from solar irradiance, especially during the 14-day lunar night.

Method: Proposed a co-generation fission power plant concept using a ceramic core, gas-cooled very-high-temperature microreactor design. Used Serpent 2 neutron transport code to model the reactor and estimate lifetime through burnup simulation in hot conditions with integrated step-wise criticality search.

Result: Calculations show neutronically feasible operation time of at least 10 years at 100kW thermal power. The reactor can directly heat MSE plants to required temperatures while providing surplus electrical energy for lunar bases.

Conclusion: The concept demonstrates technical feasibility for lunar co-generation systems, with obtained power distributions providing basis for further thermal-hydraulic studies on reactor design and power plant implementation.

Abstract: For a sustained human presence on the Moon, robust in-situ resource utilisation supply chains to provide consumables and propellant are necessary. A promising process is molten salt electrolysis, which typically requires temperatures in excess of 900°C. Fission reactors do not depend on solar irradiance and are thus well suited for power generation on the Moon, especially during the 14-day lunar night. As of now, fission reactors have only been considered for electric power generation, but the reactor coolant could also be used directly to heat those processes to their required temperatures. In this work, a concept for a co-generation fission power plant on the Moon that can directly heat a MSE plant to the required temperatures and provide a surplus of electrical energy for the lunar base is presented. The neutron transport code Serpent 2 is used to model a ceramic core, gas-cooled very-high-temperature microreactor design and estimate its lifetime with a burnup simulation in hot conditions with an integrated step-wise criticality search. Calculations show a neutronically feasible operation time of at least 10 years at 100kW thermal power. The obtained power distributions lay a basis for further thermal-hydraulic studies on the technical feasibility of the reactor design and the power plant.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [31] [Two-dimensional PIC simulation of collective Thomson scattering in a beam-plasma system](https://arxiv.org/abs/2512.09949)
*Yuma Sato,Shuichi Matsukiyo*

Main category: physics.plasm-ph

TL;DR: 2D PIC simulations reproduce collective Thomson scattering in beam-plasma systems, showing asymmetric scattered wave spectra due to beam-plasma instabilities.


<details>
  <summary>Details</summary>
Motivation: To understand and characterize collective Thomson scattering (CTS) in beam-plasma systems, particularly for interpreting radar observations of ionospheric plasmas and laboratory CTS measurements.

Method: Used 2D PIC simulations to reproduce CTS, formulated geometric shape of scattered wave spectrum in wave number space, analyzed spectra in specific directions, and conducted additional simulations for weak linearly stable beam-plasma systems.

Result: Scattered wave spectrum becomes asymmetric in 2D wave number space; electron/ion features amplify and become asymmetric/distorted during Buneman/ion acoustic instabilities; asymmetric features also appear in stable hot beam systems.

Conclusion: The asymmetric characteristics of scattered wave spectra provide insights applicable to interpreting radar observations of ionospheric plasmas and CTS measurements in laboratory plasmas.

Abstract: Collective Thomson scattering (CTS) in a beam-plasma system is reproduced by using 2D PIC simulations and the characteristics of the scattered wave spectrum are examined. By formulating the geometric shape of the scattered wave spectrum in wave number space, where the velocity vector of the beam component and the wave vectors of the incident and scattered waves are arbitrary, it is demonstrated that the spectrum in 2D wave number space becomes asymmetric. The spectrum of scattered waves propagating in a specific direction is presented as a function of wavelength to show that the electron (ion) feature is amplified and becomes asymmetric or distorted when Buneman (ion acoustic) instability occurs. An additional simulation is conducted for a weak, linearly stable beam-plasma system with a hot beam, and confirmed that the obtained scattered wave spectrum shows asymmetric feature. The results are expected to be applicable to the interpretation of radar observations of ionospheric plasmas as well as CTS measurements in laboratory plasmas.

</details>


### [32] [A DC discharge plasma experiment for undergraduate laboratories](https://arxiv.org/abs/2512.10259)
*You-Hsuan Chen,Ting-An Wang,Pisin Chen*

Main category: physics.plasm-ph

TL;DR: Undergraduate students built a DC glow-discharge plasma chamber for junior-level curriculum, enabling systematic study of plasma phenomena including Paschen breakdown, Langmuir probe measurements, spectroscopy, and magnetic focusing experiments.


<details>
  <summary>Details</summary>
Motivation: Plasma physics offers fundamental phenomena ideal for undergraduate laboratory instruction, providing hands-on learning opportunities for junior-level students to explore plasma behavior through practical experiments.

Method: Designed and constructed a 1-meter-long quartz tube plasma chamber with movable electrode. Used it to characterize Paschen breakdown and voltage-current characteristics. Developed Langmuir probes for electron temperature/density mapping, Boltzmann plot spectroscopy for excitation temperatures, and custom Helmholtz coils for magnetic focusing experiments with Runge-Kutta simulations.

Result: Successfully created a versatile plasma chamber platform that enabled systematic exploration of plasma behavior under varying conditions (pressure, voltage, geometry). Characterized fundamental plasma properties and demonstrated magnetic focusing of electrons with trajectory simulations.

Conclusion: The plasma chamber provides an effective educational platform for investigating fundamental plasma phenomena and offers potential for future student-driven investigations, including microwave-plasma interactions and other advanced studies.

Abstract: Plasma physics offers a wide range of fundamental phenomena, making it an excellent subject for undergraduate laboratory instruction. In this work, we present the design, construction, and characterization of a DC glow-discharge plasma chamber developed for the junior-level curriculum, a project carried out by two undergraduate students. The apparatus consists of a 1-meter-long quartz tube with a movable electrode, enabling systematic exploration of plasma behavior under varying pressure, voltage, and geometry. Using this platform, we characterized the Paschen breakdown relation and the voltage-current characteristics of the plasma. We then developed Langmuir probes to map spatial distributions of electron temperature and density, and used Boltzmann plot spectroscopy to measure excitation temperatures across different plasma regions. Finally, with custom Helmholtz coils, we demonstrated magnetic focusing of electrons. We performed Runge-Kutta simulations of particle trajectories and analyzed the electron drift velocity by comparing the focal lengths. Overall, this plasma chamber provides a versatile platform for investigating fundamental plasma phenomena and offers potential for future studies, including microwave-plasma interactions and other student-driven investigations.

</details>


### [33] [Optimized matching conditions for self-guided laser wakefield accelerators](https://arxiv.org/abs/2512.10728)
*P. Valenta,K. G. Miller,B. K. Russell,M. Lamač,M. Jech,G. M. Grittani,S. V. Bulanov*

Main category: physics.plasm-ph

TL;DR: Bayesian optimization refines laser-plasma matching conditions to maximize electron energy in self-guided laser wakefield acceleration, showing wide parameter tolerance for experimental flexibility.


<details>
  <summary>Details</summary>
Motivation: To maximize electron energy production in self-guided laser wakefield acceleration by refining matching conditions and reducing the need for precise parameter tuning in experiments.

Method: Combines Bayesian optimization with particle-in-cell simulations in quasi-3D geometry and Lorentz-boosted frame to identify optimal laser-plasma matching conditions.

Result: Identifies maximum achievable electron energy for given laser energy and corresponding acceleration distance, demonstrating wide parameter tolerance without precise tuning.

Conclusion: Provides substantial experimental flexibility and relaxes operational constraints for self-guided laser wakefield accelerators through robust parameter optimization.

Abstract: We revisit the matching conditions for self-guided laser pulse propagation in plasma and refine their formulation to maximize the energy of electrons produced via laser wakefield acceleration. Bayesian optimization, combined with particle-in-cell simulations carried out in a quasi-three-dimensional geometry and a Lorentz-boosted frame, is employed. The optimization identifies the maximum electron energy that a self-guided laser wakefield accelerator, driven by a laser of a given energy, can produce, together with the corresponding acceleration distance. Our results further demonstrate that electrons with energies close to the maximum value can be obtained across a relatively wide range of input parameters and without the need for their precise tuning. This provides substantial flexibility for experimental implementation and significantly relaxes the operational constraints associated with self-guided laser wakefield accelerators.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [34] [Quantum Monte Carlo in Classical Phase Space with the Wigner-Kirkwood Commutation Function. Results for the Saturation Liquid Density of $^4$He](https://arxiv.org/abs/2512.09948)
*Phil Attard*

Main category: cond-mat.stat-mech

TL;DR: Metropolis Monte Carlo algorithm for complex phase space weights in quantum statistical mechanics, applied to Lennard-Jones helium near lambda transition with Wigner-Kirkwood expansion.


<details>
  <summary>Details</summary>
Motivation: To develop computational methods for quantum statistical mechanics systems with complex phase space weights, particularly for studying quantum fluids like helium near phase transitions.

Method: Metropolis Monte Carlo algorithm adapted for complex phase space weights, applied to Lennard-Jones model of helium with Wigner-Kirkwood commutation function expansion to third order.

Result: Simulations produce saturation liquid density values that agree with experimental measurements for helium near the lambda transition.

Conclusion: The developed Monte Carlo method successfully handles complex phase space weights in quantum statistical mechanics and provides accurate results for quantum fluid systems.

Abstract: A Metropolis Monte Carlo algorithm is given for the case of a complex phase space weight, which applies generally in quantum statistical mechanics. Computer simulations using Lennard-Jones $^4$He near the $λ$-transition, including an expansion to third order of the Wigner-Kirkwood commutation function, give a saturation liquid density in agreement with measured values.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [35] [Generation of proton beams at switchback boundary-like rotational discontinuities in the solar wind](https://arxiv.org/abs/2512.10406)
*Rong Lin,Fabio Bacchini,Jiansen He,Luca Pezzini,Jingyu Peng*

Main category: physics.space-ph

TL;DR: Proton trapping at switchback boundaries creates secondary beams with temperature anisotropy, exciting ion cyclotron waves in solar wind.


<details>
  <summary>Details</summary>
Motivation: To investigate how Alfvénic rotational discontinuities (RDs) at switchback boundaries affect proton kinetics in the inner heliosphere.

Method: Used hybrid Particle-In-Cell (PIC) approach to model a pair of switchback-boundary-like RDs in a 2D system.

Result: Found significant proton trapping at one boundary RD, creating secondary beam-like component with temperature anisotropy T⊥/T∥≳4 that excites ion cyclotron waves in the downstream transition layer.

Conclusion: Switchback boundaries could create proton beams in heliosphere; highlights need to investigate RD sub-structures and high-resolution observations of solar wind velocity distributions around RDs.

Abstract: Alfvénic rotational discontinuities (RDs) are abundant in the inner heliosphere and can be used to model the boundary of switchbacks, i.e. Alfvénic magnetic kinks. To investigate the effects of RDs on proton kinetics, we model a pair of switchback-boundary-like RDs with a hybrid Particle-In-Cell (PIC) approach in a 2D system. We find that, at one of the boundary RDs, a significant population of protons remains trapped over long times, creating a secondary beam-like component with temperature anisotropy $T_\perp/T_\|\gtrsim4$ in the proton velocity distribution function that excites ion cyclotron waves within the downstream portion of the transition layer. Further analysis suggests that the static electric field in the vicinity of the RD is the key factor in trapping the protons. This work indicates that switchback boundaries could represent a viable environment for the creation of proton beams in the heliosphere; it also highlights the need to investigate RD sub-structures, especially the embedded current systems of interplanetary RDs. Finally, this paper underscores the importance of high-resolution observations of the solar wind velocity distributions around RDs.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [36] [Excitation energies and UV-Vis absorption spectra from INDO/s+ML](https://arxiv.org/abs/2512.10397)
*Ezekiel Oyeniyi,Omololu Akin-Ojo*

Main category: cond-mat.mes-hall

TL;DR: ML models correct INDO/s excitation energies, reducing error from 1.1 eV to 0.2 eV relative to TDDFT with negligible computational overhead.


<details>
  <summary>Details</summary>
Motivation: INDO/s method is computationally efficient for studying excitation energies in large systems but has low accuracy compared to methods like TDDFT.

Method: Develop machine learning models that correct INDO/s results, combining the computational efficiency of INDO/s with ML-based accuracy improvements.

Result: ML corrections reduce INDO/s excitation energy error from ~1.1 eV to 0.2 eV relative to TDDFT, and produce UV-Vis absorption spectra in good agreement with TDDFT predictions.

Conclusion: ML-enhanced INDO/s provides accurate excitation energies and absorption spectra comparable to TDDFT while maintaining computational efficiency for large systems.

Abstract: The semi-empirical INDO/s method is popular for studies of excitation energies and absorption of molecules due to its low computational requirement, making it possible to make predictions for large systems. However, its accuracy is generally low, particularly, when compared with the typical accuracy of other methods such as time-dependent density functional theory (TDDFT). Here, we present machine learning (ML) models that correct the INDO/s results with negligible increases in the amount of computing resources needed. While INDO/s excitations energies have an average error of about 1.1 eV relative to TDDFT energies, the added ML corrections reduce the error to 0.2 eV. Furthermore, this combination of INDO/s and ML produces UV-Vis absorption spectra that are in good agreement with the TDDFT predictions.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [37] [Spectral Theory of the Weighted Fourier Transform with respect to a Function in $\mathbb{R}^n$: Uncertainty Principle and Diffusion-Wave Applications](https://arxiv.org/abs/2512.10880)
*Gustavo Dorrego,Luciano Luque*

Main category: math.CA

TL;DR: Generalizes weighted Fourier transform to n-dimensions, develops spectral theory, defines weighted fractional Laplacian, and solves fractional diffusion-wave equations with Fox H-function solutions.


<details>
  <summary>Details</summary>
Motivation: Extend the one-dimensional weighted Fourier transform with respect to a function to n-dimensional Euclidean space, enabling analysis of geometric deformations and fractional operators in higher dimensions.

Method: Generalize weighted Fourier transform to ℝⁿ, develop spectral theory on weighted Hilbert space, establish Plancherel identity, inversion formula, convolution theorem, Heisenberg uncertainty principle, define weighted fractional Laplacian (-Δ_{φ,ω})^s, and apply to solve generalized time-space fractional diffusion-wave equations.

Result: Complete spectral theory for n-dimensional weighted Fourier transform, rigorous definition of weighted fractional Laplacian, explicit solution of fractional diffusion-wave equations expressed in terms of Fox H-functions connected to generalized Mellin transform.

Conclusion: Successfully extended one-dimensional weighted Fourier transform framework to n-dimensions, providing comprehensive spectral tools for analyzing geometric deformations and solving fractional PDEs with explicit Fox H-function representations.

Abstract: In this paper, we generalize the weighted Fourier transform with respect to a function, originally proposed for the one-dimensional case in \cite{Dorrego}, to the $n$-dimensional Euclidean space $\mathbb{R}^{n}$. We develop a comprehensive spectral theory on a weighted Hilbert space, establishing the Plancherel identity, the inversion formula, the convolution theorem, and a Heisenberg-type uncertainty principle depending on the geometric deformation. Furthermore, we utilize this framework to rigorously define the weighted fractional Laplacian with respect to a function, denoted by $(-Δ_{φ,ω})^{s}$. Finally, we apply these tools to solve the generalized time-space fractional diffusion-wave equation, demonstrating that the fundamental solution can be expressed in terms of the Fox H-function, intrinsically related to the generalized $ω$-Mellin transform introduced in \cite{Dorrego}. In this paper, we generalize the weighted Fourier transform with respect to a function, originally proposed for the one-dimensional case, to the n-dimensional Euclidean space $\mathbb{R}^n$. We develop a comprehensive spectral theory on a weighted Hilbert space, establishing the Plancherel identity, the inversion formula, the convolution theorem, and a Heisenberg-type uncertainty principle depending on the geometric deformation. Furthermore, we utilize this framework to rigorously define the weighted fractional Laplacian with respect to a function, denoted by $(-Δ_{φ,ω})^s$. Finally, we apply these tools to solve the generalized time-space fractional diffusion-wave equation involving the weighted Hilfer derivative. We demonstrate that the fundamental solution can be explicitly expressed in terms of the Fox H-function, revealing an intrinsic connection with the generalized Mellin transform.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [38] [Finding core subgraphs of directed graphs via discrete Ricci curvature flow](https://arxiv.org/abs/2512.07899)
*Juan Zhao,Jicheng Ma,Yunyan Yang,Liang Zhao*

Main category: cs.SI

TL;DR: The paper introduces Ricci curvature and curvature flow for directed graphs, enabling detection of strongly connected subgraphs from weakly connected directed graphs without requiring strong connectivity.


<details>
  <summary>Details</summary>
Motivation: Existing research on Ricci curvature focuses heavily on undirected graphs for applications like community detection and core extraction, with relatively less attention on directed graphs. There's a need for geometric methods that work with directed networks.

Method: 1. Define Ricci curvature and accompanying curvature flow for directed graphs. 2. For strongly connected directed graphs, the flow admits a unique global solution. 3. To handle weakly connected graphs, transform them into strongly connected ones by adding edges with very large artificial weights. 4. Apply Ricci curvature flow, where the artificially weighted edges are automatically discarded during final iterations.

Result: The method successfully detects strongly connected subgraphs from weakly connected directed graphs. For core evaluation, the approach consistently surpasses traditional methods, achieving better results on at least two out of three key metrics.

Conclusion: The paper presents a novel geometric approach using Ricci curvature flow for directed graphs that can handle weakly connected networks by transforming them into strongly connected ones with artificial weights, enabling effective core subgraph detection without requiring strong connectivity as a precondition.

Abstract: Ricci curvature and its associated flow offer powerful geometric methods for analyzing complex networks. While existing research heavily focuses on applications for undirected graphs such as community detection and core extraction, there have been relatively less attention on directed graphs.
  In this paper, we introduce a definition of Ricci curvature and an accompanying curvature flow for directed graphs. Crucially, for strongly connected directed graphs, this flow admits a unique global solution. We then apply this flow to detect strongly connected subgraphs from weakly connected directed graphs. (A weakly connected graph is connected overall but not necessarily strongly connected). Unlike prior work requiring graphs to be strongly connected, our method loosens this requirement. We transform a weakly connected graph into a strongly connected one by adding edges with very large artificial weights. This modification does not compromise our core subgraph detection. Due to their extreme weight, these added edges are automatically discarded during the final iteration of the Ricci curvature flow.
  For core evaluation, our approach consistently surpasses traditional methods, achieving better results on at least two out of three key metrics. The implementation code is publicly available at https://github.com/12tangze12/Finding-core-subgraphs-on-directed-graphs.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [39] [Engineering Multifunctional Response in Monolayer Fe3O4 via Zr Adsorption: From Half-Metallicity to Enhanced Piezoelectricity](https://arxiv.org/abs/2512.10434)
*Sikander Azam,Qaiser Rafiq,Rajwali Khan,Hamdy Khamees Thabet*

Main category: cond-mat.mtrl-sci

TL;DR: Zr adsorption on Fe3O4 monolayer enhances optical, dielectric, and piezoelectric properties while maintaining magnetic functionality.


<details>
  <summary>Details</summary>
Motivation: To explore how Zr adsorption can tune the multifunctional properties of 2D magnetic Fe3O4 monolayers for spintronics, optoelectronics, and energy conversion applications.

Method: First-principles study using spin-polarized DFT with GGA+U method, examining Zr adsorption at two sites (on-top Fe and bridge between Fe atoms) and analyzing structural, electronic, magnetic, optical, elastic, and piezoelectric properties.

Result: Zr adsorption causes lattice distortions, orbital hybridization, reduced bandgap, increased optical absorption, enhanced dielectric response and optical conductivity, tripled piezoelectric coefficient e11 (especially at bridge site), mechanical softening, and shifted plasmon resonance.

Conclusion: Zr adsorption provides a controllable, non-destructive method to tune spin-charge-lattice interactions in Fe3O4 monolayers, enabling integration of magnetic, optical, and piezoelectric functionalities in a single 2D platform.

Abstract: Two-dimensional (2D) magnetic oxides are increasingly studied for their multifunctional potential in fields like spintronics, optoelectronics, and energy conversion. In this research, we conduct a detailed first-principles study of pure monolayer Fe3O4 and its modification through Zr adsorption at two sites: on top of an Fe atom and at the bridge between Fe atoms. Using spin-polarized density functional theory with the GGA plus U method, we examine how adsorption affects structure, electronic, magnetic, optical, elastic, and piezoelectric properties. The original monolayer shows half-metallicity, strong spin polarization, and a moderate in-plane piezoelectric effect. Zr adsorption causes local lattice distortions and orbital hybridization, resulting in intermediate electronic states, a reduced bandgap, and increased optical absorption in both spin channels. Notably, Zr at the bridge site greatly enhances dielectric response, optical conductivity, and piezoelectric coefficients, tripling e11 compared to the pristine layer. Elastic constants indicate mechanical softening after functionalization, and energy loss spectra display shifts in plasmon resonance. These findings suggest Zr adsorption offers a controllable, non-destructive way to tune spin, charge, and lattice interactions in Fe4O4 monolayers, connecting magnetic, optical, and piezoelectric functionalities within a single 2D material platform.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [40] [Large deviations for invariant measure of stochastic Allen-Cahn equation with inhomogeneous boundary conditions and multiplicative noise](https://arxiv.org/abs/2512.10536)
*Rui Bai,Chunrong Feng,Huaizhong Zhao*

Main category: math.PR

TL;DR: The paper proves a large deviation principle for invariant measures of the 1D stochastic Allen-Cahn equation with inhomogeneous Dirichlet boundary conditions and multiplicative noise, showing exponential concentration around the unique energy minimizer.


<details>
  <summary>Details</summary>
Motivation: The motivation is to establish rigorous large deviation results for invariant measures in stochastic PDEs with non-strongly dissipative dynamics and unbounded multiplicative noise, particularly for the Allen-Cahn equation with boundary conditions.

Method: The method uses L. Simon's convergence theorem to analyze the noiseless system's long-time behavior, establishes estimates for invariant measures on bounded sets in Sobolev spaces, and proves a small noise large deviation principle despite the lack of strong dissipativity.

Result: The main result proves the validity of a small noise large deviation principle for the family of invariant measures, showing that these measures concentrate exponentially fast around the unique minimizer of the Ginzburg-Landau energy functional when the noise parameter is sufficiently small.

Conclusion: The paper successfully establishes large deviation principles for invariant measures in a challenging setting with non-strongly dissipative dynamics and unbounded multiplicative noise, demonstrating exponential concentration around equilibrium states defined by boundary conditions.

Abstract: We prove the validity of a small noise large deviation principle for the family of invariant measures $\{μ_ε\}_{ε>0} $ associated to the one dimensional stochastic Allen-Cahn equation with inhomogeneous Dirichlet boundary conditions, perturbed by unbounded multiplicative noise. The main difficulty is that the system is not strongly dissipative. Using L. Simon's convergence theorem, we show that the dynamics of the noiseless system converge in large time to the minimizer of the Ginzburg-Landau energy functional, which is unique due to the boundary condition. We obtain an estimate of the invariant measure on the bounded set in the Sobolev space $W^{k^\star,p^\star} $, where $k^\star p^\star>1$, and $p^\star$ is large. As a corollary of the main result, we show that $μ_ε$ concentrates around the unique minimizer with such boundary conditions exponentially fast when $ε$ is sufficiently small.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [41] [Indirect methods in optimal control on Banach spaces](https://arxiv.org/abs/2512.10831)
*Roman Chertovskih,Nikolay Pogodaev,Maxim Staritsyn,A. Pedro Aguiar*

Main category: math.OC

TL;DR: Indirect descent methods for optimal control of nonlinear ODEs in Banach spaces, with new method using exact cost-increment formulas and finite-difference probes showing stable monotone convergence.


<details>
  <summary>Details</summary>
Motivation: Classical schemes based on Pontryagin's maximum principle are sensitive to local convexity and lack monotone convergence, motivating development of more robust methods.

Method: Alternative method using exact cost-increment formulas and finite-difference probes of the terminal cost, applied to optimal control problems governed by nonlinear ODEs in Banach spaces.

Result: The new method exhibits stable monotone convergence in numerical analysis of an Amari-type neural field control problem.

Conclusion: The proposed indirect descent method provides a more stable alternative to classical Pontryagin-based schemes, with demonstrated monotone convergence in neural field control applications.

Abstract: This work focuses on indirect descent methods for optimal control problems governed by nonlinear ordinary differential equations in Banach spaces, viewed as abstract models of distributed dynamics. As a reference line, we revisit the classical schemes, rooted in Pontryagin's maximum principle, and highlight their sensitivity to local convexity and lack of monotone convergence. We then develop an alternative method based on exact cost-increment formulas and finite-difference probes of the terminal cost. We show that our method exhibits stable monotone convergence in numerical analysis of an Amari-type neural field control problem.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [42] [Phase structure of the one-dimensional $\mathbb{Z}_2$ lattice gauge theory with second nearest-neighbor interactions](https://arxiv.org/abs/2512.10755)
*Yeimer Zambrano,Aleksey Alekseev,Konrad J. Kapcia,Krzysztof Cichy,Agnieszka Cichy*

Main category: cond-mat.str-el

TL;DR: Study of 1D Z₂ lattice gauge theory with hard-core bosons at half-filling, including second nearest-neighbor interactions, revealing rich phase transitions between Luttinger liquid, Mott insulator, and charge-ordered insulator phases.


<details>
  <summary>Details</summary>
Motivation: To extend previous studies of 1D Z₂ lattice gauge theory models by incorporating second nearest-neighbor interactions, investigating how extended interactions affect the ground-state phase diagram and the interplay between gauge fields, confinement, and charge ordering.

Method: Using matrix product state techniques within the density matrix renormalization group (DMRG) framework to compute charge gap, static structure factor, and pair-pair correlation functions for various interaction strengths and field parameters.

Result: For small nearest-neighbor interactions (V₁), increasing second nearest-neighbor repulsion (V₂) causes direct transition from Luttinger liquid to charge-ordered insulator. For large V₁, the system transitions from Mott insulator through an intermediate Luttinger liquid region before reaching charge-ordered insulator. Second nearest-neighbor interactions enhance charge order and suppress pair coherence.

Conclusion: The study extends the phase structure of 1D Z₂ lattice gauge theory models and demonstrates the complex interplay between gauge fields, confinement, and extended interactions, revealing rich phase transitions and the significant impact of second nearest-neighbor couplings on charge ordering and pair coherence.

Abstract: We investigate the ground-state phase diagram of a one-dimensional $\mathbb{Z}_2$ lattice gauge theory (LGT) model with hard-core bosons at half-filling, extending previous studies by including second nearest-neighbor (2NN) interactions. Using matrix product state techniques within the density matrix renormalization group, we compute charge gap, static structure factor, and pair-pair correlation functions for various interaction strengths and field parameters. We analyze two representative neatest-neighbor interaction strengths ($V_1$) that correspond to the Luttinger liquid (LL) and Mott insulator (MI) phases in the absence of the 2NN interactions. We introduce the 2NN coupling $V_2$ and investigate its impact on the system. Our results reveal very rich behavior. As the 2NN repulsion increases, in the case of small $V_1$, we observe a direct transition from the LL phase to a charge-ordered insulator (COI) phase, whereas for large $V_1$, we observe a transition from the MI phase (previously found with only $V_1$ included), going through an intermediate LL region, and finally reaching the COI regime. Additionally, the inclusion of 2NN interactions enhances charge order and suppresses pair coherence, evidenced by sharp peaks in the structure factor and rapid decay in pair-pair correlators. Our work extends the well-studied phase structure of 1D $\mathbb{Z}_2$ LGT models and demonstrates the interplay between gauge fields, confinement, and extended interactions.

</details>


<div id='math.KT'></div>

# math.KT [[Back]](#toc)

### [43] [Sharp mapping properties of Poisson transforms and the Baum-Connes conjecture](https://arxiv.org/abs/2512.10018)
*Heiko Gimperlein,Magnus Goffeng*

Main category: math.KT

TL;DR: Sharp quantitative analogue of Helgason's conjecture: Poisson transforms map Sobolev spaces boundedly with closed range to L² spaces for semisimple Lie groups of real rank one.


<details>
  <summary>Details</summary>
Motivation: To prove a quantitative version of Helgason's conjecture at the distribution level, and to complete Julg's program establishing the Baum-Connes conjecture for closed subgroups of semisimple Lie groups of real rank one.

Method: Uses the Poisson transform (Szegö map) studied by Knapp-Wallach, with Sobolev spaces defined using van Erp-Yuncken's Heisenberg calculus. Proves boundedness with closed range and generalizes to show compactness of commutators with smooth functions on Furstenberg compactification.

Result: Proves the sharp quantitative analogue of Helgason's conjecture, establishing that Poisson transforms map Sobolev spaces boundedly with closed range to L² spaces. Also proves compactness of commutators, completing the remaining open conjecture in Julg's Baum-Connes program.

Conclusion: The paper successfully proves both the quantitative Helgason conjecture and completes Julg's program for establishing the Baum-Connes conjecture for closed subgroups of semisimple Lie groups of real rank one, resolving important open problems in harmonic analysis and operator algebras.

Abstract: We prove a sharp, quantitative analogue of Helgason's conjecture at the level of distributions: For a semisimple Lie group $G$ of real rank one, Poisson transforms map a Sobolev space on $P\backslash G$ boundedly with closed range to an $L^2$-space on $K\backslash G$. The result is obtained for the Poisson transform studied by Knapp-Wallach under the name Szegö map, and the appropriate Sobolev spaces are defined using van Erp-Yuncken's Heisenberg calculus. The proof generalizes to show that commutators of this Poisson transform with smooth functions on the Furstenberg compactification are compact. This proves the remaining open conjecture in Julg's seminal program to establish the Baum-Connes conjecture for closed subgroups of semisimple Lie groups of real rank one.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [44] [Error Analysis of Generalized Langevin Equations with Approximated Memory Kernels](https://arxiv.org/abs/2512.10256)
*Quanjun Lang,Jianfeng Lu*

Main category: stat.ML

TL;DR: Analysis shows prediction error in stochastic dynamical systems with memory decays at rate determined by memory kernel decay, bounded by kernel estimation error in weighted norm.


<details>
  <summary>Details</summary>
Motivation: To understand how prediction accuracy in stochastic dynamical systems with memory (modeled by generalized Langevin equations) depends on memory kernel estimation quality, and to establish quantitative bounds linking kernel estimation error to trajectory prediction error.

Method: Uses synchronized noise coupling with Volterra comparison theorem for both subexponential and exponential kernel classes. For first-order models: resolvent estimates in weighted spaces. For second-order models with confining potentials: hypocoercive Lyapunov-type distance for contraction and stability analysis under kernel perturbations.

Result: Trajectory discrepancies decay at rate determined by memory kernel decay, quantitatively bounded by kernel estimation error in weighted norm. Framework accommodates non-translation-invariant kernels and white-noise forcing, explicitly linking improved kernel estimation to enhanced trajectory prediction.

Conclusion: Theoretical framework establishes rigorous connection between memory kernel estimation accuracy and prediction error in stochastic dynamical systems, validated by numerical examples, providing tools for analyzing prediction stability in systems with memory.

Abstract: We analyze prediction error in stochastic dynamical systems with memory, focusing on generalized Langevin equations (GLEs) formulated as stochastic Volterra equations. We establish that, under a strongly convex potential, trajectory discrepancies decay at a rate determined by the decay of the memory kernel and are quantitatively bounded by the estimation error of the kernel in a weighted norm. Our analysis integrates synchronized noise coupling with a Volterra comparison theorem, encompassing both subexponential and exponential kernel classes. For first-order models, we derive moment and perturbation bounds using resolvent estimates in weighted spaces. For second-order models with confining potentials, we prove contraction and stability under kernel perturbations using a hypocoercive Lyapunov-type distance. This framework accommodates non-translation-invariant kernels and white-noise forcing, explicitly linking improved kernel estimation to enhanced trajectory prediction. Numerical examples validate these theoretical findings.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [45] [The Radon Transform-Based Sampling Methods for Biharmonic Sources from the Scattered Fields](https://arxiv.org/abs/2512.10332)
*Xiaodong Liu,Qingxiang Shi,Jing Wang*

Main category: math-ph

TL;DR: Three sampling methods for reconstructing extended sources in biharmonic wave equations using scattered field data, with varying requirements for measurements and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop efficient quantitative sampling methods for reconstructing extended sources in biharmonic wave equations using scattered field measurements, addressing limitations of existing approaches that require derivative data.

Method: 1) First method uses indicator function based only on scattered fields on single circle (no Laplacian/derivative data needed). 2) Second method uses simplified double integral formula requiring Δu^s measurements for better efficiency. 3) Third method reconstructs source boundary from scattered fields at finite sensors by analyzing boundary singularities.

Result: Methods achieve high-resolution imaging of source support or source function itself. First method provides uniqueness proof via explicit formula. Third method establishes uniqueness for annulus and polygon-shaped sources.

Conclusion: Three novel sampling methods offer practical solutions for source reconstruction with varying measurement requirements, connecting scattered fields to Radon transform of source function and demonstrating effectiveness through numerical experiments.

Abstract: This paper presents three quantitative sampling methods for reconstructing extended sources of the biharmonic wave equation using scattered field data. The first method employs an indicator function that solely relies on scattered fields $ u^s$ measured on a single circle, eliminating the need for Laplacian or derivative data. Its theoretical foundation lies in an explicit formula for the source function, which also serves as a constructive proof of uniqueness. To improve computational efficiency, we introduce a simplified double integral formula for the source function, at the cost of requiring additional measurements $Δu^s$. This advancement motivates the second indicator function, which outperforms the first method in both computational speed and reconstruction accuracy. The third indicator function is proposed to reconstruct the support boundary of extended sources from the scattered fields $ u^s$ at a finite number of sensors. By analyzing singularities induced by the source boundary, we establish the uniqueness of annulus and polygon-shaped sources. A key characteristic of the first and third indicator functions is their link between scattered fields and the Radon transform of the source function. Numerical experiments demonstrate that the proposed sampling methods achieve high-resolution imaging of the source support or the source function itself.

</details>
