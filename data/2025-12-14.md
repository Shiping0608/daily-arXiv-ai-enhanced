<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 11]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [math-ph](#math-ph) [Total: 1]
- [math.KT](#math.KT) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Hybrid Finite Element and Least Squares Support Vector Regression Method for solving Partial Differential Equations with Legendre Polynomial Kernels](https://arxiv.org/abs/2512.09967)
*Maryam Babaei,Peter Rucz,Manfred Kaltenbacher,Stefan Schoder*

Main category: math.NA

TL;DR: Hybrid FEM-LSSVR method combines finite element nodal solutions with Legendre polynomial kernel regression to create analytical super-resolution solutions from low-order FEM outputs.


<details>
  <summary>Details</summary>
Motivation: To enhance low-order finite element solutions by providing high-resolution analytical solutions without modifying existing FEM codes, enabling super-resolution of numerical and experimental data.

Method: Integrates FEM nodal solutions with LSSVR using higher-order Legendre polynomial kernels for element-wise enhancement, maintaining boundary consistency and enabling parallel computation.

Result: Achieves significantly higher accuracy than base FEM, comparable to standalone high-order FEM, with demonstrated convergence for elliptic boundary value problems.

Conclusion: The hybrid approach provides a plug-and-play super-resolution method for enhancing low-order numerical solvers and experimental data without implementation overhead.

Abstract: A hybrid computational approach that integrates the finite element method (FEM) with least squares support vector regression (LSSVR) is introduced to solve partial differential equations. The method combines FEM's ability to provide the nodal solutions and LSSVR with higher-order Legendre polynomial kernels to deliver a closed-form analytical solution for interpolation between the nodes. The hybrid approach implements element-wise enhancement (super-resolution) of a given numerical solution, resulting in high resolution accuracy, while maintaining consistency with FEM nodal values at element boundaries. It can adapt any low-order FEM code to obtain high-order resolution by leveraging localized kernel refinement and parallel computation without additional implementation overhead. Therefore, effective inference/post-processing of the obtained super-resolved solution is possible. Evaluation results show that the hybrid FEM-LSSVR approach can achieve significantly higher accuracy compared to the base FEM solution. Comparable accuracy is a achieved when comparing the hybrid solution with a standalone FEM result with the same polynomial basis function order. The convergence studies were conducted for four elliptic boundary value problems to demonstrate the method's ability, accuracy, and reliability. Finally, the algorithm can be directly used as a plug-and-play method for super-resolving low-order numerical solvers and for super-resolution of expensive/under-resolved experimental data.

</details>


### [2] [A Mass Preserving Numerical Scheme for Kinetic Equations that Model Social Phenomena](https://arxiv.org/abs/2512.10027)
*Yassin Bahid,Eduardo Corona,Nancy Rodriguez*

Main category: math.NA

TL;DR: Global existence/uniqueness results and a deterministic Mass Preserving Collocation Method for kinetic equations with Dirac delta transition rates, outperforming stochastic agent-based methods in efficiency and avoiding variability.


<details>
  <summary>Details</summary>
Motivation: Kinetic equations model social phenomena with discontinuous state changes via Dirac delta functions in transition rates, but efficient deterministic solvers are needed to avoid the variability and computational costs of stochastic methods.

Method: Established global existence/uniqueness for kinetic equations with delta-function transition rates, then developed a fully deterministic Mass Preserving Collocation Method for efficient simulation of multi-subsystem models.

Result: Validated solver accuracy/efficiency on models with up to 5 subsystems, showing it resolves subsystem distributions while preserving mass numerically, requiring significantly less computational time/resources than Tau-leaping and hybrid methods.

Conclusion: The deterministic scheme provides high-fidelity simulation of kinetic models with Dirac delta transitions, avoiding variability and hyperparameter tuning of stochastic methods while being computationally superior.

Abstract: In recent years, kinetic equations have been used to model many social phenomena. A key feature of these models is that transition rate kernels involve Dirac delta functions, which capture sudden, discontinuous state changes. Here, we study kinetic equations with transition rates of the form $$ T(x,y,u) = δ_{φ(x,y) - u}. $$ We establish the global existence and uniqueness of solutions for these systems and introduce a fully deterministic scheme, the \emph{Mass Preserving Collocation Method}, which enables efficient, high fidelity simulation of models with multiple subsystems. We validate the accuracy, efficiency, and consistency of the solver on models with up to five subsystems, and compare its performance against two state-of-the-art agent-based methods: Tau-leaping and hybrid methods. Our scheme resolves subsystem distributions captured by these stochastic approaches while preserving mass numerically, requiring significantly less computational time and resources, and avoiding variability and hyperparameter tuning characteristic of these methods.

</details>


### [3] [Efficient Boys function evaluation using minimax approximation](https://arxiv.org/abs/2512.10059)
*Rasmus Vikhamar-Sandberg,Michal Repisky*

Main category: math.NA

TL;DR: Efficient GPU-optimized algorithm for evaluating Boys functions using rational minimax approximations and recurrence relations, avoiding lookup tables for better memory access patterns.


<details>
  <summary>Details</summary>
Motivation: Need efficient evaluation of Boys functions on modern GPU architectures where maximum throughput is high but data movement is costly, requiring algorithms that avoid irregular memory access patterns.

Method: Partition real axis into three regions (A, B, C), use rational minimax approximations in regions A and B, asymptotic approximation in region C, combine with upward/downward recurrence relations, generate coefficients using rational Remez algorithm.

Result: Algorithm achieves target maximum absolute error of ε_tol = 5×10^-14, provides approximation regions and coefficients for Boys functions F_0 to F_32 in appendix, optimized for GPU execution.

Conclusion: The method provides an efficient, hardware-friendly approach for Boys function evaluation on GPUs by avoiding lookup tables and irregular memory access, making it suitable for high-throughput computing architectures.

Abstract: We present an algorithm for efficient evaluation of Boys functions $F_0,\dots,F_{k_\mathrm{max}}$ tailored to modern computing architectures, in particular graphical processing units (GPUs), where maximum throughput is high and data movement is costly. The method combines rational minimax approximations with upward and downward recurrence relations. The non-negative real axis is partitioned into three regions, $[0,\infty\rangle = A\cup B\cup C$, where regions $A$ and $B$ are treated using rational minimax approximations and region $C$ by an asymptotic approximation. This formulation avoids lookup tables and irregular memory access, making it well suited hardware with high maximum throughput and low latency. The rational minimax coefficients are generated using the rational Remez algorithm. For a target maximum absolute error of $\varepsilon_\mathrm{tol} = 5\cdot10^{-14}$, the corresponding approximation regions and coefficients for Boys functions $F_0,\dots,F_{32}$ are provided in the appendix.

</details>


### [4] [Metric-driven numerical methods](https://arxiv.org/abs/2512.10083)
*Patrick Henning,Laura Huynh,Daniel Peterseim*

Main category: math.NA

TL;DR: The paper introduces metric-driven numerical methods using Riemannian gradient techniques to solve multiscale PDEs and eigenvalue problems, showing how metric choice leads to accelerated convergence and enhanced approximation spaces, with applications to Bose-Einstein condensates.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for solving multiscale partial differential equations and eigenvalue problems with nonlinearities, particularly in low-regularity regimes or when solutions exhibit heterogeneous multiscale features.

Method: Introduces metric-driven methods via Riemannian gradient techniques, using different metrics (Sobolev gradients) to accelerate convergence. The approach leads to specific iterative schemes and induces approximation spaces with enhanced properties, recovering Localized Orthogonal Decomposition (LOD) spaces from a new perspective.

Result: The metric-driven approach not only accelerates convergence through appropriate metric choices but also induces approximation spaces with improved properties for multiscale problems. The method is demonstrated on a model problem and applied to simulate ground states of spin-orbit-coupled Bose-Einstein condensates.

Conclusion: Metric-driven numerical methods provide a powerful framework for solving multiscale PDEs and eigenvalue problems, offering both accelerated convergence through Riemannian gradient techniques and enhanced approximation spaces, with practical applications to complex physical systems like Bose-Einstein condensates.

Abstract: In this paper, we explore the concept of metric-driven numerical methods as a powerful tool for solving various types of multiscale partial differential equations. Our focus is on computing constrained minimizers of functionals - or, equivalently, by considering the associated Euler-Lagrange equations - the solution of a class of eigenvalue problems that may involve nonlinearities in the eigenfunctions. We introduce metric-driven methods for such problems via Riemannian gradient techniques, leveraging the idea that gradients can be represented in different metrics (so-called Sobolev gradients) to accelerate convergence. We show that the choice of metric not only leads to specific metric-driven iterative schemes, but also induces approximation spaces with enhanced properties, particularly in low-regularity regimes or when the solution exhibits heterogeneous multiscale features. In fact, we recover a well-known class of multiscale spaces based on the Localized Orthogonal Decomposition (LOD), now derived from a new perspective. Alongside a discussion of the metric-driven approach for a model problem, we also demonstrate its application to simulating the ground states of spin-orbit-coupled Bose-Einstein condensates.

</details>


### [5] [Numerical approximation of the first $p$-Laplace eigenpair](https://arxiv.org/abs/2512.10122)
*Hannah Potgieter,Razvan C. Fetecau,Steven J. Ruuth*

Main category: math.NA

TL;DR: A numerical method for approximating the first Dirichlet eigenpair of p-Laplace operators for large p (2 ≤ p < ∞) on Euclidean and surface domains, connecting to p→∞ geometry.


<details>
  <summary>Details</summary>
Motivation: The p-Laplace operator's first Dirichlet eigenpair is important for understanding geometric properties, especially as p→∞ where it connects to domain geometry. Large p values present significant numerical challenges that need to be addressed.

Method: A surface finite element scheme combining Newton inverse-power iteration with a novel domain rescaling strategy to enable stable computations for large p values.

Result: Numerical experiments in 1D, planar domains, and surfaces in ℝ³ demonstrate the method's accuracy and robustness, showing convergence to p→∞ limiting behavior.

Conclusion: The proposed numerical scheme successfully handles the challenges of large p computations and reveals the geometric connections in the p→∞ limit for both Euclidean and surface domains.

Abstract: We approximate the first Dirichlet eigenpair of the $p$-Laplace operator for $2 \leq p < \infty$ on both Euclidean and surface domains. We emphasize large $p$ values and discuss how the $p \to \infty$ limit connects to the underlying geometry of our domain. Working with large $p$ values introduces significant numerical challenges. We present a surface finite element numerical scheme that combines a Newton inverse-power iteration with a new domain rescaling strategy, which enables stable computations for large $p$. Numerical experiments in $1$D, planar domains, and surfaces embedded in $\mathbb{R}^3$ demonstrate the accuracy and robustness of our approach and show convergence towards the $p \to \infty$ limiting behavior.

</details>


### [6] [A robust fully-mixed finite element method with skew-symmetry penalization for low-frequency poroelasticity](https://arxiv.org/abs/2512.10192)
*Stefano Bonetti,Michele Botti,Patrick Vega*

Main category: math.NA

TL;DR: A fully-mixed finite element method for dynamic poroelasticity in low-frequency regime using four-field hyperbolic formulation with stress symmetry penalization.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for simulating wave propagation in porous materials under low-frequency conditions, addressing challenges in poroelasticity modeling.

Method: Four-field first-order hyperbolic system with stress symmetry imposed via penalization; stable mixed finite elements in space with implicit time integration; perturbation added to saddle point system.

Result: Stability analysis shows robustness with respect to degenerate model parameters; numerical tests validate convergence and demonstrate good performance for wave propagation simulations.

Conclusion: The proposed fully-mixed finite element scheme provides a robust and effective approach for dynamic poroelasticity problems, particularly suitable for low-frequency wave propagation in porous materials.

Abstract: In this work, we present and analyze a fully-mixed finite element scheme for the dynamic poroelasticity problem in the low-frequency regime. We write the problem as a four-field, first-order, hyperbolic system of equations where the symmetry constraint on the stress field is imposed via penalization. This strategy is equivalent to adding a perturbation to the saddle point system arising when the stress symmetry is weakly-imposed. The coupling of solid and fluid phases is discretized by means of stable mixed elements in space and implicit time advancing schemes. The presented stability analysis is fully robust with respect to meaningful cases of degenerate model parameters. Numerical tests validate the convergence and robustness and assess the performances of the method for the simulation of wave propagation phenomena in porous materials.

</details>


### [7] [Variational-hemivariational inequalities: A brief survey on mathematical theory and numerical analysis](https://arxiv.org/abs/2512.10204)
*Weimin Han*

Main category: math.NA

TL;DR: Survey paper on variational-hemivariational inequalities covering well-posedness analysis, numerical methods, and applications to mechanical problems with non-smooth relations.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive survey of recent developments in variational-hemivariational inequalities, which extend variational inequalities and are valuable for modeling physical problems with non-smooth, set-valued, monotone or non-monotone relations in engineering and physical sciences.

Method: Presents theoretical results for abstract stationary variational-hemivariational inequalities, explains accessible proofs of existence and uniqueness, compares three mechanical problems (variational equation, variational inequality, and variational-hemivariational inequality), and discusses mixed formulations and numerical solutions for nonstationary/history-dependent cases.

Result: The paper surveys well-posedness analysis and numerical methods for variational-hemivariational inequalities, demonstrating their application to mechanical problems and highlighting their distinctive features compared to variational equations and inequalities.

Conclusion: Variational-hemivariational inequalities represent a significant extension of variational inequalities with important applications in engineering and physical sciences, and substantial progress has been made in their theoretical analysis and numerical solution methods.

Abstract: Variational-hemivariational inequalities are an area full of interesting and challenging mathematical problems. The area can be viewed as a natural extension of that of variational inequalities. Variational-hemivariational inequalities are valuable for application problems from physical sciences and engineering that involve non-smooth and even set-valued relations, monotone or non-monotone, among physical quantities. In the recent years, there has been substantial growth of research interest in modeling, well-posedness analysis, development of numerical methods and numerical algorithms of variational-hemivariational inequalities. This survey paper is devoted to a brief account of well-posedness and numerical analysis results for variational-hemivariational inequalities. The theoretical results are presented for a family of abstract stationary variational-hemivariational inequalities and the main idea is explained for an accessible proof of existence and uniqueness. To better appreciate the distinguished feature of variational-hemivariational inequalities, for comparison, three mechanical problems are introduced leading to a variational equation, a variational inequality, and a variational-hemivariational inequality, respectively. The paper also comments on mixed variational-hemivariational inequalities, with examples from applications in fluid mechanics, and on results concerning the numerical solution of other types (nonstationary, history dependent) of variational-hemivariational inequalities.

</details>


### [8] [Convergence analysis of contrast source inversion type methods for acoustic inverse medium scattering problems](https://arxiv.org/abs/2512.10260)
*Qiao Hu,Bo Zhang,Haiwen Zhang*

Main category: math.NA

TL;DR: Proposes two new iteratively regularized CSI-type methods (IRCSI and IRSOM) with ℓ₁ proximal terms that guarantee global convergence for inverse scattering problems, addressing a long-standing open problem.


<details>
  <summary>Details</summary>
Motivation: CSI-type methods (CSI and SOM) are popular for solving inverse medium scattering problems but lack rigorous convergence proofs, which has been an open problem since their introduction in 1997 and 2009.

Method: Develops two new algorithms: IRCSI (iteratively regularized CSI) and IRSOM (iteratively regularized SOM) that incorporate novel ℓ₁ proximal terms as regularization while maintaining similar computational complexity to original methods.

Result: Proves global convergence under natural and weak conditions on the objective function - the first convergence result for iterative methods solving nonlinear inverse scattering problems with fixed frequency.

Conclusion: The proposed IRCSI-type methods successfully address the convergence problem while maintaining efficiency, with numerical experiments demonstrating their convergence and performance.

Abstract: The contrast source inversion (CSI) method and the subspace-based optimization method (SOM) are first proposed in 1997 and 2009, respectively, and subsequently modified. The two methods and their variants share several properties and thus are called the CSI-type methods. The CSI-type methods are efficient and popular methods for solving inverse medium scattering problems, but their rigorous convergence remains an open problem. In this paper, we propose two iteratively regularized CSI-type (IRCSI-type) methods with a novel $\ell_1$ proximal term as the iteratively regularized term: the iteratively regularized CSI (IRCSI) method and the iteratively regularized SOM (IRSOM) method, which have a similar computation complexity to the original CSI and SOM methods, respectively, and prove their global convergence under natural and weak conditions on the original objective function. To the best of our knowledge, this is the first convergence result for iterative methods of solving nonlinear inverse scattering problems with a fixed frequency. The convergence and performance of the two IRCSI-type algorithms are illustrated by numerical experiments.

</details>


### [9] [Matrix approach to the fractional calculus](https://arxiv.org/abs/2512.10330)
*V. N. Kolokoltsov,E. L. Shishkina*

Main category: math.NA

TL;DR: A matrix-based construction of fractional derivatives/integrals with respect to functions, using operator semigroups and Balakrishnan's representations to derive convergence rates for numerical approximations.


<details>
  <summary>Details</summary>
Motivation: To develop a powerful analytical and numerical tool for fractional calculus by creating a new matrix-based construction of fractional derivatives and integrals with respect to functions.

Method: Start with differential operators generating semigroups, discretize to obtain matrix approximations, apply Balakrishnan's representations of fractional powers based on semigroups, use semigroup norm estimates to derive convergence rates for fractional operator approximations.

Result: Derived convergence rates for approximating fractional powers of operators with matrix approximations, and obtained explicit formula for calculating arbitrary powers of two-band matrices for numerical solutions of fractional differential/integral equations.

Conclusion: The matrix approach provides an effective framework for both analytical and numerical calculations in fractional calculus, with proven convergence properties and practical computational formulas.

Abstract: In this paper, we introduce the new construction of fractional derivatives and integrals with respect to a function, based on a matrix approach. We believe that this is a powerful tool in both analytical and numerical calculations. We begin with the differential operator with respect to a function that generates a semigroup. By discretizing this operator, we obtain a matrix approximation. Importantly, this discretization provides not only an approximating operator but also an approximating semigroup. This point motivates our approach, as we then apply Balakrishnan's representations of fractional powers of operators, which are based on semigroups. Using estimates of the semigroup norm and the norm of the difference between the operator and its matrix approximation, we derive the convergence rate for the approximation of the fractional power of operators with the fractional power of correspondings matrix operators. In addition, an explicit formula for calculating an arbitrary power of a two-band matrix is obtained, which is indispensable in the numerical solution of fractional differential and integral equations.

</details>


### [10] [Second order reduced model via incremental projection for Navier Stokes](https://arxiv.org/abs/2512.10473)
*Mejdi Azaïez,Yayu Guo,Carlos Núñez Fernández,Samuele Rubino,Chuanju Xu*

Main category: math.NA

TL;DR: Presents a reduced-order modeling approach using incremental projection methods for Stokes equations with POD, achieving second-order time convergence and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Numerical simulation of incompressible flows is challenging due to velocity-pressure coupling; projection methods decouple these variables for large-scale computations, but reduced-order modeling can further improve efficiency.

Method: Uses incremental projection schemes for Stokes equations with semi-discrete and fully discrete formulations (BDF2 in time, finite elements in space), constructs reduced-order model via proper orthogonal decomposition (POD), enabling explicit computation of reduced velocity and pressure.

Result: Provides detailed stability analysis and error estimates showing second-order convergence in time; numerical experiments validate theoretical results and demonstrate computational efficiency of the reduced-order model.

Conclusion: The proposed reduced-order modeling approach using incremental projection schemes with POD effectively addresses incompressible flow simulation challenges, maintaining accuracy while improving computational efficiency for Stokes equations.

Abstract: The numerical simulation of incompressible flows is challenging due to the tight coupling of velocity and pressure. Projection methods offer an effective solution by decoupling these variables, making them suitable for large-scale computations. This work focuses on reduced-order modeling using incremental projection schemes for the Stokes equations. We present both semi-discrete and fully discrete formulations, employing BDF2 in time and finite elements in space. A proper orthogonal decomposition (POD) approach is adopted to construct a reduced-order model for the Stokes problem. The method enables explicit computation of reduced velocity and pressure while preserving accuracy. We provide a detailed stability analysis and derive error estimates, showing second-order convergence in time. Numerical experiments are conducted to validate the theoretical results and demonstrate computational efficiency.

</details>


### [11] [Analysis of discrete energy-decay preserving schemes for Maxwell's equations in Cole-Cole dispersive medium](https://arxiv.org/abs/2512.10560)
*Guoyu Zhang,Ziming Dong,Baoli Yin,Yang Liu,Hong Li*

Main category: math.NA

TL;DR: Energy-decay preserving numerical schemes for Maxwell's equations in Cole-Cole dispersive media with novel θ-scheme that maintains discrete energy dissipation.


<details>
  <summary>Details</summary>
Motivation: To develop numerical schemes that preserve the physical energy-decay property of Maxwell's equations in Cole-Cole dispersive media, ensuring long-term simulation stability and physical fidelity.

Method: Established continuous energy-decay law via modified energy functional, then proposed novel θ-scheme for temporal discretization with rigorous proof of discrete energy dissipation preservation under θ∈[α/2, 1/2].

Result: The SFTR-θ scheme achieves first-order convergence for θ≠0.5 and second-order for θ=0.5, demonstrates superior energy-decay preservation compared to alternative 2nd-order fractional backward difference formula, especially in long-time simulations.

Conclusion: The proposed θ-scheme provides robust, physically faithful numerical method for Maxwell's equations in Cole-Cole media with guaranteed energy-decay preservation, offering better long-term stability than existing alternatives.

Abstract: This work investigates the design and analysis of energy-decay preserving numerical schemes for Maxwell's equations in a Cole-Cole (C-C) dispersive medium. A continuous energy-decay law is first established for the C-C model through a modified energy functional. Subsequently, a novel \(θ\)-scheme is proposed for temporal discretization, which is rigorously proven to preserve a discrete energy dissipation property under the condition \(θ\in [\fracα{2}, \frac{1}{2}]\). The temporal convergence rate of the scheme is shown to be first-order for \(θ\neq 0.5\) and second-order for \(θ= 0.5\). Extensive numerical experiments validate the theoretical findings, including convergence tests and energy-decay comparisons. The proposed SFTR-\(θ\) scheme demonstrates superior performance in maintaining monotonic energy decay compared to an alternative 2nd-order fractional backward difference formula, particularly in long-time simulations, highlighting its robustness and physical fidelity.

</details>


### [12] [Dynamically consistent finite volume scheme for a bimonomeric simplified model with inflammation processes for Alzheimer's disease](https://arxiv.org/abs/2512.10716)
*Juan Barajas-Calonge,Mauricio A. Sepulveda Cortes,Nicolas Torres,Luis Miguel Villada*

Main category: math.NA

TL;DR: A finite volume scheme is developed for an Alzheimer's disease progression model with PDEs and ODEs, proving non-negativity, a priori bounds, convergence to weak solutions, and dynamic consistency with the spatially homogeneous version.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for simulating Alzheimer's disease progression that maintains important biological properties like non-negativity and boundedness, while ensuring mathematical consistency with the underlying model.

Method: A finite volume scheme for a convection-diffusion-reaction system of four PDEs and one ODE, using semi-implicit discretization of reaction terms that coincides with nonstandard discretization of the spatially homogeneous model.

Result: The scheme preserves non-negativity and a priori bounds, converges to admissible weak solutions, and is dynamically consistent with the spatially homogeneous version of the model. Numerical experiments illustrate the model behavior.

Conclusion: The developed finite volume scheme provides a mathematically rigorous and biologically consistent numerical method for simulating Alzheimer's disease progression, with proven properties that ensure reliable simulations.

Abstract: A model of progression of Alzheimer's disease (AD) incorporating the interactions of A$β$-monomers, oligomers, microglial cells and interleukins with neurons is considered. The resulting convection-diffusion-reaction system consists of four partial differential equations (PDEs) and one ordinary differential equation (ODE). We develop a finite volume (FV) scheme for this system, together with non-negativity and a priori bounds for the discrete solution, so that we establish the existence of a discrete solution to the FV scheme. It is shown that the scheme converges to an admissible weak solution of the model. The reaction terms of the system are discretized using a semi-implicit strategy that coincides with a nonstandard discretization of the spatially homogeneous (SH) model. This construction enables us to prove that the FV scheme is dynamically consistent with respect to the spatially homogeneous version of the model. Finally, numerical experiments are presented to illustrate the model and to assess the behavior of the FV scheme.

</details>


### [13] [A Stabilized Finite Element Method for Morpho-Visco-Poroelastic Model](https://arxiv.org/abs/2512.10718)
*Sabia Asghar,Duncan den Bakker,Etelvina Javierre,Qiyao Peng,Fred J. Vermolen*

Main category: math.NA

TL;DR: A mathematical model combining elastic, viscous, porous effects with growth/shrinkage is analyzed for stability and numerical stabilization.


<details>
  <summary>Details</summary>
Motivation: To model tissue/tumor growth and dermal contraction phenomena where microstructural changes cause growth or shrinkage, requiring combination of elastic, viscous, and porous effects.

Method: Developed a mathematical model combining elastic, viscous, and porous effects with growth/shrinkage; analyzed stability of equilibria for both continuous and semi-discrete versions; derived numerical condition for monotonicity and stabilization method to avoid spurious oscillations.

Result: Stability analysis of equilibria performed; numerical stabilization condition derived; stabilization effectiveness confirmed through computer simulations; total variation evaluated as function of stabilization parameter for quantitative assessment.

Conclusion: The study provides stability analysis and numerical stabilization methods for a complex biomechanical model, with simulations confirming the effectiveness of the stabilization approach in preventing spurious oscillations.

Abstract: We propose a mathematical model that combines elastic, viscous and porous effects with growth or shrinkage due to microstructural changes. This phenomenon is important in tissue or tumor growth, as well as in dermal contraction. Although existence results of the solution to the problem are not given, the current study assesses stability of the equilibria for both the continuous and semi-discrete versions of the model. Furthermore, a numerical condition for monotonicity of the numerical solution is described, as well as a way to stabilize the numerical solution so that spurious oscillations are avoided. The derived stabilization result is confirmed by computer simulations. In order to have a more quantitative picture, the total variation has been evaluated as a function of the stabilization parameter.

</details>


### [14] [Physics-Informed Learning of Microvascular Flow Models using Graph Neural Networks](https://arxiv.org/abs/2512.10792)
*Paolo Botta,Piermario Vitullo,Thomas Ventimiglia,Andreas Linninger,Paolo Zunino*

Main category: math.NA

TL;DR: A deep learning approach using Graph Neural Networks (GNNs) to create fast, physics-informed surrogate models for simulating blood flow in complex microvascular networks, validated on mouse cerebral cortex data.


<details>
  <summary>Details</summary>
Motivation: Simulating microcirculatory blood flow is challenging due to multiscale nature and topological complexity of capillary networks. Traditional methods are computationally expensive, creating need for efficient reduced-order models.

Method: Proposes GNN-based reduced-order modeling trained on synthetic microvascular graphs. Combines synthetic vascular generation algorithms with physics-informed training that integrates graph topology and local flow dynamics. Uses physics-informed loss functional derived from governing equations to enforce mass conservation and rheological constraints.

Result: GNN architecture demonstrates robust generalization across diverse network configurations. Validated on benchmark problems with linear/nonlinear rheology, showing accurate pressure/velocity reconstruction with substantial computational gains over full-order solvers. Successfully tested on mouse cerebral cortex data.

Conclusion: Establishes new class of graph-based surrogate models for microvascular flow grounded in physical laws, with inductive biases mirroring mass conservation and rheological models. Opens new directions for real-time inference in vascular modeling and biomedical applications.

Abstract: The simulation of microcirculatory blood flow in realistic vascular architectures poses significant challenges due to the multiscale nature of the problem and the topological complexity of capillary networks. In this work, we propose a novel deep learning-based reduced-order modeling strategy, leveraging Graph Neural Networks (GNNs) trained on synthetic microvascular graphs to approximate hemodynamic quantities on anatomically realistic domains. Our method combines algorithms for synthetic vascular generation with a physics-informed training procedure that integrates graph topological information and local flow dynamics. To ensure the physical reliability of the learned surrogates, we incorporate a physics-informed loss functional derived from the governing equations, allowing enforcement of mass conservation and rheological constraints. The resulting GNN architecture demonstrates robust generalization capabilities across diverse network configurations. The GNN formulation is validated on benchmark problems with linear and nonlinear rheology, showing accurate pressure and velocity field reconstruction with substantial computational gains over full-order solvers. The methodology showcases significant generalization capabilities with respect to vascular complexity, as highlighted by tests on data from the mouse cerebral cortex. This work establishes a new class of graph-based surrogate models for microvascular flow, grounded in physical laws and equipped with inductive biases that mirror mass conservation and rheological models, opening new directions for real-time inference in vascular modeling and biomedical applications.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Regularity of the free boundary for the supercooled Stefan problem in arbitrary dimensions](https://arxiv.org/abs/2512.10136)
*Max Engelstein,Inwon Kim,Sebastian Munoz*

Main category: math.AP

TL;DR: The paper establishes a robust structure theory for the free boundary in the supercooled Stefan problem, showing it decomposes into regular, singular, and jump components with controlled dimensions, and proves the free boundary is a continuously differentiable graph.


<details>
  <summary>Details</summary>
Motivation: The supercooled Stefan problem models solidification below freezing temperature, where physical experiments suggest fractal freezing sets, infinite-speed propagation, and nucleation phenomena. Despite these complex behaviors, there was no comprehensive regularity theory for the free boundary in arbitrary dimensions.

Method: The authors decompose the free boundary into three components: (1) regular part advancing with finite speed, (2) singular part with infinite speed or nucleation points having controlled space-time dimension, and (3) jump component contained in a space-time smooth graph occurring at zero-dimensional times. They prove the free boundary is the graph of a continuously differentiable freezing time function.

Result: The free boundary has a robust structure with three distinct components that can all be nonempty. The singular set coincides with the critical set of the freezing time function, proving singularities always occur with infinite speed. This provides the first free boundary regularity theory for the supercooled Stefan problem in arbitrary dimensions.

Conclusion: Despite complex phenomena like fractal freezing and infinite-speed propagation in supercooled freezing, the free boundary exhibits a well-structured decomposition with controlled dimensional properties, and singularities are characterized by infinite speed at critical points of the freezing time function.

Abstract: We study the free boundary in the supercooled Stefan problem, a classical model for the solidification of water below its freezing temperature. In contrast with the melting problem, physical experiments and heuristics indicate that the water--ice interface in the supercooled problem may exhibit fractal freezing sets, infinite-speed propagation of the frozen front, and nucleation (the spontaneous appearance of ice). Despite this, we show that the free boundary has a robust structure.
  We decompose the free boundary into three parts: (1) a regular part that advances with finite speed in time; (2) a singular part consisting of points where the front attains infinite speed or nucleates, but with controlled space-time (i.e., $\leq d-1$ parabolic) dimension; and (3) a jump component, which can have large dimension in a time slice, but which is contained in a space-time smooth graph and occurs only at a zero-dimensional set of times. Examples show that each of these parts can be nonempty.
  Furthermore, we prove that the free boundary is the graph $t=s(x)$ of a continuously differentiable freezing time $s$, and the singular set coincides with the critical set of $s$, proving that singularities in supercooled freezing always occur with infinite speed.
  These results provide the first free boundary regularity theory for the supercooled Stefan problem in arbitrary dimensions.

</details>


### [16] [The supercooled Stefan problem: fractal freezing and the fine structure of maximal solutions](https://arxiv.org/abs/2512.10138)
*Raymond Chu,Inwon Kim,Sebastian Munoz*

Main category: math.AP

TL;DR: The paper studies the supercooled Stefan problem in arbitrary dimensions, analyzing general solutions with fractal freezing patterns versus maximal solutions with better regularity properties, and examines their universality and stability.


<details>
  <summary>Details</summary>
Motivation: To understand the mathematical properties of the supercooled Stefan problem, particularly the irregularities in general solutions (fractal freezing/nucleation) versus the regularity of maximal solutions, and to investigate universality and stability of these solutions.

Method: Uses a novel Markovian gluing principle to analyze general solutions, then studies maximal solutions obtained by maximizing average freezing time, applies obstacle problem theory for regularity analysis, and examines radial/one-dimensional cases.

Result: General solutions show generic fractal freezing and nucleation; maximal solutions have transition zones open modulo low-dimensional sets allowing obstacle problem analysis; maximal solutions are non-universal but have sharp stability; radial/1D maximal solutions are universal and minimize nucleation.

Conclusion: The supercooled Stefan problem exhibits complex behavior with fractal patterns in general solutions but better regularity in maximal solutions, with interesting differences in universality between general and special (radial/1D) cases that align with physical observations.

Abstract: We study the supercooled Stefan problem in arbitrary dimensions. First, we study general solutions and their irregularities, showing generic fractal freezing and nucleation, based on a novel Markovian gluing principle. In contrast, we then establish regularity properties of maximal solutions, which are obtained by maximizing a suitable notion of "average" freezing time. Unexpectedly, we show that maximal solutions have a transition zone that is open modulo a low-dimensional set: this allows us to apply obstacle problem theory for a finer regularity analysis. We further show that maximal solutions are in general non-universal, and we obtain sharp stability results under perturbation of each maximal solution. Lastly, we study maximal solutions in both the radial and the one-dimensional setting. We show that in these cases the maximal solution is universal and minimizes nucleation, in agreement with phenomena observed in the physics literature.

</details>


### [17] [Parabolic Frequency on Gaussian Spaces and Unique Continuation](https://arxiv.org/abs/2512.10139)
*Jin Sun,Kui Wang*

Main category: math.AP

TL;DR: Establishes an almost-monotonicity formula for parabolic frequency on Gaussian spaces for Ornstein-Uhlenbeck heat equations with lower-order terms under weaker growth conditions than classical results.


<details>
  <summary>Details</summary>
Motivation: Extend frequency monotonicity results to solutions of Ornstein-Uhlenbeck heat equations with unbounded coefficients, going beyond classical boundedness assumptions. This addresses limitations in existing theory where coefficients must be bounded.

Method: Develops a weighted L² framework using the backward Mehler kernel as weight to handle unbounded coefficients. This weight naturally encodes the underlying Gaussian measure and compensates for the growth of coefficients.

Result: Proves almost-monotonicity formula for parabolic frequency under weaker conditions: only b bounded and c with linear growth, while solutions can have exponential quadratic growth. Derives strong unique continuation principle from frequency monotonicity.

Conclusion: Extends Poon's seminal results and complements Colding-Minicozzi's geometric generalizations in Gaussian measure spaces. Framework also applies to establish unique continuation for equations with quadratic growth potentials or certain singularities.

Abstract: We establish an almost-monotonicity formula for a parabolic frequency on Gaussian spaces for solutions of the Ornstein-Uhlenbeck heat equation with lower-order terms: $$\partial_t u = L_γu + b(x,t) \cdot \nabla u + c(x,t)u, $$ where $L_γ= Δ- x \cdot \nabla$ is the Ornstein-Uhlenbeck operator. In contrast to classical results that require $b$ and $c$ to be bounded, we only assume that $b$ is bounded and $c$ satisfies a linear growth condition, while the solution $u$ is allowed to have at most exponential quadratic growth. The key innovation is a weighted $L^2$ framework that uses the backward Mehler kernel as a weight, which naturally encodes the underlying measure and compensates for the unbounded coefficients. From the frequency monotonicity, we derive the strong unique continuation principle. This extends Poon's seminal results and complements recent geometric generalizations by Colding and Minicozzi in the context of Gaussian measure spaces. We further apply our framework to establish unique continuation for equations with potentials exhibiting quadratic growth or certain singularities.

</details>


### [18] [Topological degree for negative fractional Kazdan--Warner equation on finite graphs](https://arxiv.org/abs/2512.10295)
*Yang Liu,Liang Shan,Mengjie Zhang*

Main category: math.AP

TL;DR: Extends fractional Kazdan-Warner equations to graphs using topological degree theory, proving existence and multiplicity of solutions in negative case.


<details>
  <summary>Details</summary>
Motivation: Fractional Kazdan-Warner equations on graphs remain insufficiently explored despite growing interest in graph-based studies, creating a research gap.

Method: Uses topological degree theory to investigate fractional Kazdan-Warner equation in negative case on connected finite graphs.

Result: Proves existence and multiplicity of solutions, extends Liu & Yang (2020) to fractional setting, provides concise proof for Shan & Liu (2025).

Conclusion: Successfully extends fractional analysis to graph Kazdan-Warner equations with topological methods, bridging classical and fractional results.

Abstract: Studies on Kazdan--Warner equations on graphs have grown steadily, yet the fractional case remains insufficiently explored. Using topological degree theory, this work investigates the fractional Kazdan--Warner equation in the negative case on connected finite graphs, focusing on the existence and multiplicity of solutions. This work not only extends the earlier result of S. Liu and Yang (2020) to the fractional setting, but also provides a concise proof for the work of Shan and Y. Liu (2025).

</details>


### [19] [Vortex atmospheres of traveling vortices: rigorous definition, existence, and topological classification](https://arxiv.org/abs/2512.10412)
*Kyudong Choi,In-Jee Jeong,Young-Jin Sim*

Main category: math.AP

TL;DR: Rigorous mathematical definition and proof of vortex atmosphere existence/uniqueness, comparing 2D dipole vs 3D ring atmospheres with topological distinctions.


<details>
  <summary>Details</summary>
Motivation: The vortex atmosphere phenomenon has been recognized since the 19th century but lacked rigorous mathematical definitions and proofs, with most studies relying on approximations rather than exact analysis.

Method: Develop rigorous definition of vortex atmosphere, prove existence and uniqueness. Characterize atmosphere as specific superlevel set of corresponding stream function. Compare planar atmosphere of 2D vortex dipole with axisymmetric atmosphere of 3D vortex ring.

Result: Established existence and uniqueness of vortex atmospheres. Proved topological distinctions: 2D dipole atmospheres form oval-shaped regions, while 3D ring atmospheres can be either spheroidal or toroidal configurations.

Conclusion: Provides first rigorous mathematical foundation for vortex atmosphere concept, confirming historical observations with precise proofs and revealing fundamental topological differences between 2D and 3D cases.

Abstract: In incompressible and inviscid fluids, the vortex atmosphere refers to the collection of fluid particles outside the support of a traveling vortex that are nevertheless carried along with it. This phenomenon has been recognized since the nineteenth century, e.g., in the classical works of O. Reynolds [Nature, 1876] and O. Lodge [Lond. Edinb. Dubl. Phil. Mag., 1885], yet rigorous mathematical definitions and proofs have remained largely undeveloped, with most subsequent studies relying on thin-core approximations or asymptotic analyses. In this paper, we give a rigorous definition of a vortex atmosphere and establish its existence and uniqueness. We further compare the planar atmosphere surrounding a 2D vortex dipole with the axisymmetric atmosphere surrounding a 3D vortex ring. In particular, we emphasize and prove the topological distinctions observed by W. Hicks [Lond. Edinb. Dubl. Phil. Mag., 1919]: under natural assumptions, every 2D dipole with its atmosphere forms an oval-shaped region, whereas for 3D rings, both spheroidal and toroidal configurations may occur. Our proof is based on showing that each atmosphere can be characterized precisely as a specific superlevel set of its corresponding stream function.

</details>


### [20] [Asymptotic Sphere Concentration at Infinity for NLS with L^2 Constraint](https://arxiv.org/abs/2512.10512)
*Qing Guo,Chongyang Tian*

Main category: math.AP

TL;DR: Existence of normalized solutions to NLS with L²-mass concentrating on spheres diverging to infinity, extending 2D results to all dimensions n≥2.


<details>
  <summary>Details</summary>
Motivation: Study attractive Bose-Einstein condensates modeled by nonlinear Schrödinger equation with L²-normalization constraint, seeking solutions where mass concentration escapes to infinity rather than remaining on compact sets.

Method: Combines tailored finite-dimensional reduction with blow-up analysis based on Pohozaev identities, developing new approximation scheme and functional setting adapted to high-dimensional sphere-at-infinity concentration.

Result: Proves existence of normalized solutions for all dimensions n≥2 and exponents p>1, with L²-mass concentrating on spheres whose radii diverge to infinity.

Conclusion: Extends previous 2D mass-critical results to higher dimensions, establishing qualitatively different concentration regime where concentration set escapes to infinity rather than remaining on fixed compact hypersurfaces.

Abstract: We consider the nonlinear Schrödinger equation$$-Δu + V(x)\,u = a\,u^p + μu \quad \text{in }\mathbb{R}^n,\qquad \int_{\mathbb{R}^n} u^2 = 1,$$modeling attractive Bose--Einstein condensates. For all dimensions $n\ge 2$ and all exponents $p>1$, we prove the existence of normalized solutions whose $L^2$-mass concentrates on spheres with radii diverging to infinity. In particular, the concentration set escapes to infinity rather than remaining on a fixed compact hypersurface, which makes our regime qualitatively different both from classical point-concentration phenomena and from concentrating profiles in unconstrained problems. Our approach combines a tailored finite-dimensional reduction with a blow-up analysis based on Pohozaev identities and, in this way, extends the two-dimensional mass-critical result for $(n,p)=(2,3)$ obtained in Guo--Tian--Zhou (Calc.\ Var.\ Partial Differential Equations, 2022). The proof in that paper relies in an essential way on the two-dimensional structure and does not directly apply in higher dimensions, whereas here we develop a different approximation scheme and functional setting adapted to the high-dimensional sphere-at-infinity concentration regime.

</details>


### [21] [Asymptotic analysis of fractional Sobolev spaces on thin films in the low-integrability regime](https://arxiv.org/abs/2512.10620)
*Andrea Braides,Andrea Pinamonti,Margherita Solci*

Main category: math.AP

TL;DR: Fractional Sobolev spaces on thin films converge to higher-order Sobolev spaces on the base domain as thickness vanishes, with asymptotic behavior at boundary parameter values.


<details>
  <summary>Details</summary>
Motivation: To understand how fractional Sobolev spaces behave on thin domains and establish rigorous convergence results as domain thickness approaches zero, connecting thin-film analysis with dimension reduction.

Method: Study fractional Sobolev spaces H^s(Ω_ε) on thin films Ω_ε = ω × (0,ε) in ℝ^d, use dimension-reduction convergence concept, analyze scaled Gagliardo seminorms for equicoercive functionals, examine asymptotic limits as s→0+ and s→1/2-.

Result: H^s(Ω_ε) converges to H^{s+½}(ω) as ε→0, establishing precise dimension-reduction convergence with equicoercive functionals, with asymptotic results for boundary parameter values.

Conclusion: Thin-film fractional Sobolev spaces exhibit systematic dimension reduction, gaining half-order regularity when passing to the limit base domain, with well-defined asymptotic behavior at parameter boundaries.

Abstract: We study the behaviour of fractional Sobolev spaces $H^s(Ω_\varepsilon)$ with $s\in(0,1/2)$ defined on ``thin films'' $Ω_\varepsilon=ω\times (0,\varepsilon)$ in $\mathbb R^d$, and prove that they tend to the space $H^{s+\frac12}(ω)$ as $\varepsilon\to 0$. This is made precise by using a notion of dimension-reduction convergence, with respect to which suitably scaled Gagliardo seminorms define equicoercive functionals. Asymptotic results are proved for $s\to 0^+$ and $s\to 1/2^-$.

</details>


### [22] [The $\ell^p$-boundedness of wave operators for the fourth order Schrödinger operators on the lattice $\mathbb{Z}$](https://arxiv.org/abs/2512.10649)
*Sisi Huang,Xiaohua Yao*

Main category: math.AP

TL;DR: The paper proves ℓ^p boundedness of wave operators for discrete fourth-order Schrödinger operators and derives sharp ℓ^p-ℓ^{p'} decay estimates for discrete beam equations.


<details>
  <summary>Details</summary>
Motivation: To establish functional analytic properties of wave operators associated with discrete fourth-order Schrödinger operators, which are important for understanding scattering theory and dispersive estimates on lattices.

Method: Uses asymptotic expansions of the resolvent near thresholds (0 and 16), discrete singular integrals theory, and analysis of zero resonance types of the operator H = Δ² + V.

Result: Wave operators W_±(H, Δ²) are bounded on ℓ^p(ℤ) for all 1 < p < ∞ under suitable decay assumptions on V. They are not bounded on endpoint spaces ℓ¹ and ℓ^∞ when thresholds are regular. Sharp ℓ^p-ℓ^{p'} decay estimates for discrete beam equations are derived.

Conclusion: The paper establishes comprehensive ℓ^p boundedness results for wave operators of discrete fourth-order Schrödinger operators and applies these to obtain optimal dispersive estimates for discrete beam equations, advancing scattering theory on lattices.

Abstract: This paper investigates the $\ell^p$ boundedness of wave operators $W_\pm(H,Δ^2)$ associated with discrete fourth-order Schrödinger operators $H = Δ^2 + V$ on the lattice $\mathbb{Z}$, where $$(Δφ)(n)=φ(n+1)+φ(n-1)-2φ(n),\quad n\in\mathbb{Z},$$ and $V(n)$ is a real-valued potential on $\mathbb{Z}$. Under suitable decay assumptions on $V$ (depending on the types of zero resonance of $H$), we show that the wave operators $W_{\pm}(H, Δ^2)$ are bounded on $\ell^p(\mathbb{Z})$ for all $1 < p < \infty$: $$ \|W_{\pm}(H, Δ^2) f\|_{\ell^p(\mathbb{Z})} \lesssim \|f\|_{\ell^p(\mathbb{Z})}. $$ In particular, if both thresholds $0$ and $16$ are regular points of $H$, we prove that $W_{\pm}(H, Δ^2)$ are neither bounded on the endpoint space $\ell^1(\mathbb{Z})$ nor on $\ell^\infty(\mathbb{Z})$. We remark that the proof of these bounds relies fundamentally on the asymptotic expansions of the resolvent of $H$ near the thresholds $0$ and $16$, and on the theory of {\it discrete singular integrals} on the lattice.
  As applications, we derive the following sharp $\ell^p-\ell^{p'}$ decay estimates for solutions to the discrete beam equation with a parameter $a\in \mathbb{R}$ on the lattice $\mathbb{Z}$: $$ \|{\rm cos}(t\sqrt {H+a^2})P_{ac}(H)\|_{\ell^p\rightarrow\ell^{p'}}+\left\|\frac{{\rm sin}(t\sqrt {H+a^2})}{t\sqrt {H+a^2}}P_{ac}(H)\right\|_{\ell^p\rightarrow\ell^{p'}}\lesssim|t|^{-\frac{1}{3}(\frac{1}{p}-\frac{1}{p'})},\quad t\neq0, $$ where $1<p\le 2$, ${p'}$ is the conjugated index of $p$ and $P_{ac}(H)$ denotes the spectral projection onto the absolutely continuous spectrum space of $H$.

</details>


### [23] [On the ground state of the nonlinear Schr{ö}dinger equation: asymptotic behavior at the endpoint powers](https://arxiv.org/abs/2512.10690)
*Rémi Carles,Quentin Chauleur,Guillaume Ferriere,Dmitry Pelinovsky*

Main category: math.AP

TL;DR: Analysis of ground states in nonlinear Schrödinger equation at endpoint nonlinearity limits, showing convergence to Gaussian (Gausson) and algebraic soliton (Aubin-Talenti) with explicit bounds and asymptotics.


<details>
  <summary>Details</summary>
Motivation: To understand the limiting behaviors of ground states in the nonlinear Schrödinger equation at extreme values of the nonlinearity parameter, particularly how they approach known special solutions (Gausson and Aubin-Talenti soliton) through appropriate scaling.

Method: Mathematical analysis of radially symmetric, exponentially decaying solutions on full space. Uses rescaling techniques to study limits at endpoint powers of nonlinearity. Combines theoretical proofs of strong convergence with explicit error bounds and detailed asymptotic expansions, supplemented by numerical verification.

Result: Proves strong convergence of ground states to Gaussian function (Gausson) in one limit case, and to algebraic Aubin-Talenti soliton (for dimensions ≥3) in the other limit case. Provides explicit convergence bounds and detailed asymptotics for both limits, with numerical illustrations confirming theoretical results.

Conclusion: Ground states of nonlinear Schrödinger equation exhibit well-defined limiting behaviors at endpoint nonlinearities: they converge to Gaussian (logarithmic Schrödinger ground state) and algebraic soliton solutions with quantifiable convergence rates, providing complete mathematical characterization of these limits.

Abstract: We consider the ground states of the nonlinear Schr{ö}dinger equation, which stand for radially symmetric and exponentially decaying solutions on the full space. We investigate their behaviors at both endpoint powers of the nonlinearity, up to some rescaling to infer non-trivial limits. One case corresponds to the limit towards a Gaussian function called Gausson, which is the ground state of the stationary logarithmic Schr{ö}dinger equation. The other case, for dimension at least three, corresponds to the limit towards the Aubin-Talenti algebraic soliton. We prove strong convergence with explicit bounds for both cases, and provide detailed asymptotics. These theoretical results are illustrated with numerical approximations.

</details>


### [24] [$Φ^4_2$ theory limit of a many-body bosonic free energy](https://arxiv.org/abs/2512.10704)
*Lucas Jougla,Nicolas Rougerie*

Main category: math.AP

TL;DR: The paper proves convergence of quantum Gibbs state free energy for interacting Bose gas to Φ⁴₂ NLS-Gibbs measure free energy in 2D scaling limit.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous connection between quantum many-body systems and classical field theory, specifically showing that the free energy of an interacting Bose gas converges to that of the Φ⁴₂ non-linear Schrödinger-Gibbs measure in appropriate scaling limits.

Method: Combines variational method of Lewin-Nam-Rougerie to connect quantum free energy to classical Hartree-Gibbs free energy with smeared non-linearity, then uses arguments of Fröhlich-Knowles-Schlein-Sohinger to show convergence to Φ⁴₂ free energy.

Result: Proves convergence of free energy of interacting Bose gas (relative to non-interacting one) to free energy of Φ⁴₂ NLS-Gibbs measure in 2D torus scaling limit with short-range repulsive interaction potential converging to Dirac delta.

Conclusion: Provides streamlined proof connecting quantum many-body systems to classical field theory, revisiting and simplifying recent results on convergence of Bose gas free energy to Φ⁴₂ measure free energy.

Abstract: We consider the quantum Gibbs state of an interacting Bose gas on the 2D torus. We set temperature, chemical potential and coupling constant in a regime where classical field theory gives leading order asymptotics. In the same limit, the repulsive interaction potential is set to be short-range: it converges to a Dirac delta function with a rate depending polynomially on the other scaling parameters. We prove that the free-energy of the interacting Bose gas (counted relatively to the non-interacting one) converges to the free energy of the $Φ^4_2$ non-linear Schr{ö}dinger-Gibbs measure, thereby revisiting recent results and streamlining proofs thereof. We combine the variational method of Lewin-Nam-Rougerie to connect, with controled error, the quantum free energy to a classical Hartree-Gibbs one with smeared non-linearity. The convergence of the latter to the $Φ^4_2$ free energy then follows from arguments of Fr{ö}hlich-Knowles-Schlein-Sohinger. This derivation parallels recent results of Nam-Zhu-Zhu.

</details>


### [25] [Observability inequality for the von Neumann equation in crystals](https://arxiv.org/abs/2512.10897)
*Thomas Borsoni,Virginie Ehrlacher*

Main category: math.AP

TL;DR: Quantitative observability inequality for von Neumann equation on ℝ^d in crystal setting, uniform in small ħ, adapting Golse & Paul's method to periodic case.


<details>
  <summary>Details</summary>
Motivation: Extend quantum-classical observability results to periodic/crystal settings, which is important for solid state physics and materials science where periodic potentials are fundamental.

Method: Adapt Golse & Paul's (2022) stability argument between quantum (von Neumann) and classical (Liouville) dynamics using optimal transport-like pseudo-distance. Key adaptations: Bloch decomposition, periodic Schrödinger coherent states, periodic Töplitz operators, and periodic Husimi densities.

Result: Successfully provides quantitative observability inequality for von Neumann equation in crystal setting that remains uniform as ħ→0, establishing quantum-classical correspondence in periodic systems.

Conclusion: The method successfully extends to periodic settings, providing tools for analyzing quantum observability in crystals and establishing uniform quantum-classical correspondence in solid state systems.

Abstract: We provide a quantitative observability inequality for the von Neumann equation on $\mathbb{R}^d$ in the crystal setting, uniform in small $\hbar$. Following the method of Golse and Paul (2022) proving this result in the non-crystal setting, the method relies on a stability argument between the quantum (von Neumann) and classical (Liouville) dynamics and uses an optimal transport-like pseudo-distance between quantum and classical densities. Our contribution yields in the adaptation of all the required tools to the periodic setting, relying on the Bloch decomposition, notions of periodic Schrödinger coherent state, periodic Töplitz operator and periodic Husimi densities.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [26] [A Model-Guided Neural Network Method for the Inverse Scattering Problem](https://arxiv.org/abs/2512.10123)
*Olivia Tsang,Owen Melia,Vasileios Charisopoulos,Jeremy Hoskins,Yuehaw Khoo,Rebecca Willett*

Main category: physics.comp-ph

TL;DR: A physics-informed machine learning method for inverse medium scattering that incorporates explicit physics knowledge via a differentiable solver and uses progressive frequency refinement for stable reconstruction.


<details>
  <summary>Details</summary>
Motivation: Inverse medium scattering is ill-posed and nonlinear, with ML methods offering speed but struggling in highly nonlinear regimes due to poor physics incorporation. Current ML approaches either implicitly infer physics from data or loosely enforce it through architecture.

Method: The method endows ML with explicit physics knowledge using a differentiable solver representing the forward model. It progressively refines reconstructions using measurements at increasing wave frequencies, following classical stabilization strategies.

Result: Empirically, the method provides high-quality reconstructions at a fraction of the computational or sampling costs of competing approaches.

Conclusion: The proposed physics-informed ML framework successfully addresses limitations of traditional ML methods by explicitly incorporating problem physics, achieving efficient and stable reconstruction for inverse scattering problems.

Abstract: Inverse medium scattering is an ill-posed, nonlinear wave-based imaging problem arising in medical imaging, remote sensing, and non-destructive testing. Machine learning (ML) methods offer increased inference speed and flexibility in capturing prior knowledge of imaging targets relative to classical optimization-based approaches; however, they perform poorly in regimes where the scattering behavior is highly nonlinear. A key limitation is that ML methods struggle to incorporate the physics governing the scattering process, which are typically inferred implicitly from the training data or loosely enforced via architectural design. In this paper, we present a method that endows a machine learning framework with explicit knowledge of problem physics, in the form of a differentiable solver representing the forward model. The proposed method progressively refines reconstructions of the scattering potential using measurements at increasing wave frequencies, following a classical strategy to stabilize recovery. Empirically, we find that our method provides high-quality reconstructions at a fraction of the computational or sampling costs of competing approaches.

</details>


### [27] [Generative Modeling of Entangled Polymers with a Distance-Based Variational Autoencoder](https://arxiv.org/abs/2512.10131)
*Pietro Chiarantoni,Oscar Serra,Mohammad Erfan Mowlaei,Venkata Surya Kumar Choutipalli,Mark DelloStritto,Xinghua Shi,Micheal L. Klein,Vincenzo Carnevale*

Main category: physics.comp-ph

TL;DR: VAE framework learns and generates structured polymer globule configurations from distance matrices using MD-sampled polyethylene data.


<details>
  <summary>Details</summary>
Motivation: To develop a deep learning approach for learning and generating physically meaningful configurations of structured polymer globules, which are complex molecular systems with important structural patterns.

Method: Combined coarse-grained molecular dynamics to sample polyethylene structures as training data, then used a variational autoencoder with convolution and attention layers to encode distance matrices into a roto-translationally invariant latent space. Added post-processing with multidimensional scaling and short MD simulations.

Result: The model successfully reconstructs and generates polymer configurations that reproduce key physical observables including energy, size, and entanglement, despite minor discrepancies in the raw decoder output.

Conclusion: The VAE framework with appropriate architectural choices and post-processing enables effective learning and generation of physically meaningful polymer globule configurations from distance matrices.

Abstract: We present a variational autoencoder framework for learning and generating configurations of structured polymer globules from distance matrices. We used coarse-grained molecular dynamics to sample polyethylene structures, which we used as the training set for our deep learning model. By combining convolution and attention layers, the model encodes the structural patterns of distance matrices into an organized and roto-translationally invariant latent space of lower dimensionality. The generative capability of the variational autoencoder, coupled with a post-processing pipeline based on multidimensional scaling and short molecular dynamics, enables the recovery of physically meaningful configurations. The reconstructed and generated samples reproduce key observables, including energy, size, and entanglement, despite minor discrepancies in the raw decoder output.

</details>


### [28] [Flow-priority optimization of additively manufactured variable-TPMS lattice heat exchanger based on macroscopic analysis](https://arxiv.org/abs/2512.10207)
*Kazutaka Yanagihara,Jun Iwasaki,Kiyoto Saso,Taichi Yamashita,Shomu Murakoshi,Akihiro Takezawa*

Main category: physics.comp-ph

TL;DR: Optimizing TPMS lattice distribution in heat exchangers using macroscopic modeling improves heat transfer by 28.7% compared to uniform lattice designs.


<details>
  <summary>Details</summary>
Motivation: While TPMS lattice structures enhance heat transfer through uniform flow distribution and boundary layer disruption, uniform lattice configurations may not be optimal for macroscopic flow patterns. There's a need to optimize lattice distribution for better heat-exchange efficiency.

Method: Developed macroscopic modeling approach using Darcy-Forchheimer theory for flow analysis and volumetric heat-transfer coefficient for heat transfer modeling. Used Primitive lattice isosurface threshold as design variable to optimize lattice distribution. Applied optimization to planar heat exchanger with U-shaped flow trajectories and validated through geometric analysis and metal LPBF experiments.

Result: The optimized lattice distribution demonstrated 28.7% average performance improvement over uniform lattice in experimental results, showing clear advantage in heat-transfer efficiency.

Conclusion: Optimizing TPMS lattice distribution using macroscopic modeling significantly enhances heat exchanger performance compared to uniform designs, validating the effectiveness of the proposed optimization approach.

Abstract: Heat exchangers incorporating triply periodic minimal surface (TPMS) lattice structures have attracted considerable research interest because they promote uniform flow distribution, disrupt boundary layers, and improve convective heat-transfer performance. However, from the perspective of forming a macroscopic flow pattern optimized for heat-exchange efficiency, a uniform lattice is not necessarily the optimal configuration. This study initially presents a macroscopic modeling approach for a two-fluid heat exchanger equipped with a TPMS Primitive lattice. The macroscopic flow analysis is conducted based on the Darcy--Forchheimer theory. Under the assumption that heat is transferred solely at the interface between the fluid and the TPMS walls, a macroscopic heat-transfer model is developed using a volumetric heat-transfer coefficient, which serves as an artificial property characterizing the unit-volume heat-transfer capability. To regulate the relative dominance of the hot and cold flows-effectively, the channel widths-within the heat exchanger, we adopt the isosurface threshold of the Primitive lattice as the design variable and construct an optimization scheme for the lattice distribution using the previously described macroscopic model. The optimization is subsequently carried out for a planar heat exchanger where the hot and cold fluids each follow U-shaped flow trajectories. The optimal solution was verified, and its validity was examined through detailed geometric analysis and experiments conducted using metal LPBF. The optimal solution derived from the macroscopic model also demonstrated a clear performance advantage over the uniform lattice in the experimental results. The optimal solution obtained from the macroscopic model also demonstrated a clear performance improvement over the uniform lattice, with an average enhancement of 28.7% in the experimental results.

</details>


### [29] [Ultra-Fast Muon Transport via Histogram Sampling on GPUs](https://arxiv.org/abs/2512.10520)
*Luis Felipe P. Cattelan,Shah Rukh Qasim,Patrick H. Owen,Nicola Serra*

Main category: physics.comp-ph

TL;DR: GPU-accelerated muon transport using histogram sampling achieves orders of magnitude speedup over CPU-based Geant4 simulations by precomputing physics processes and running parallel simulations on a single GPU.


<details>
  <summary>Details</summary>
Motivation: To dramatically accelerate muon transport simulations by leveraging GPU parallelization, overcoming the computational limitations of traditional CPU-based methods like Geant4.

Method: Precomputed histograms of momentum loss and scattering from Geant4 simulations are used to statistically reproduce physics processes. Implemented as a CUDA kernel with parallel algorithm for concurrent simulation of tens of thousands of particles, handling complex geometry and magnetic fields using fourth-order Runge-Kutta integration.

Result: Achieves speedups of several orders of magnitude compared to CPU-based Geant4 simulations, even when running on a large CPU farm with over a thousand cores. Validation shows preservation of key physical features in both simple and realistic detector geometries.

Conclusion: GPU-based implementations have significant potential for particle transport simulations, with applicability extending to neutrino propagation and future implementations including discrete processes like particle decay.

Abstract: We present a GPU-accelerated method for muon transport based on histogram sampling that delivers orders of magnitude faster performance than CPU-based Geant4 simulation. Our method employs precomputed histograms of momentum loss and scattering, derived from detailed Geant4 simulations, to statistically reproduce all the non-decaying physics processes during muon traversal through matter. Implemented as a CUDA kernel, the parallel algorithm enables the concurrent simulation of tens of thousands of particles on a single GPU whilst taking into account a complex geometry and a magnetic field force integrated using a fourth-order Runge-Kutta method. Validation against Geant4 in both simple and realistic detector geometries shows that the approach preserves key physical features while achieving speedups of several orders of magnitude, even compared to CPU-based simulations on a large CPU farm with over a thousand cores. This work highlights the significant potential of GPU-based implementations for particle transport, with applicability extending to neutrino propagation and future implementations including discrete processes such as particle decay.

</details>


### [30] [MULE -- A Co-Generation Fission Power Plant Concept to Support Lunar In-Situ Resource Utilisation](https://arxiv.org/abs/2512.10705)
*Julius Mercz,Philipp Reiss,Christian Reiter*

Main category: physics.comp-ph

TL;DR: A co-generation fission power plant concept for the Moon that directly heats molten salt electrolysis plants while providing surplus electrical power for lunar bases.


<details>
  <summary>Details</summary>
Motivation: For sustained human presence on the Moon, robust in-situ resource utilization supply chains are needed. Fission reactors can provide continuous power during lunar nights and their coolant heat can be used directly for high-temperature industrial processes like molten salt electrolysis.

Method: Used neutron transport code Serpent 2 to model a ceramic core, gas-cooled very-high-temperature microreactor design. Conducted burnup simulation in hot conditions with integrated step-wise criticality search to estimate reactor lifetime.

Result: Calculations show neutronically feasible operation time of at least 10 years at 100kW thermal power. The power distributions provide basis for further thermal-hydraulic studies.

Conclusion: A co-generation fission power plant concept is presented that can directly heat MSE plants to required temperatures while providing surplus electrical energy, demonstrating neutronically feasible long-term operation for lunar resource utilization.

Abstract: For a sustained human presence on the Moon, robust in-situ resource utilisation supply chains to provide consumables and propellant are necessary. A promising process is molten salt electrolysis, which typically requires temperatures in excess of 900°C. Fission reactors do not depend on solar irradiance and are thus well suited for power generation on the Moon, especially during the 14-day lunar night. As of now, fission reactors have only been considered for electric power generation, but the reactor coolant could also be used directly to heat those processes to their required temperatures. In this work, a concept for a co-generation fission power plant on the Moon that can directly heat a MSE plant to the required temperatures and provide a surplus of electrical energy for the lunar base is presented. The neutron transport code Serpent 2 is used to model a ceramic core, gas-cooled very-high-temperature microreactor design and estimate its lifetime with a burnup simulation in hot conditions with an integrated step-wise criticality search. Calculations show a neutronically feasible operation time of at least 10 years at 100kW thermal power. The obtained power distributions lay a basis for further thermal-hydraulic studies on the technical feasibility of the reactor design and the power plant.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [31] [Two-dimensional PIC simulation of collective Thomson scattering in a beam-plasma system](https://arxiv.org/abs/2512.09949)
*Yuma Sato,Shuichi Matsukiyo*

Main category: physics.plasm-ph

TL;DR: 2D PIC simulations reproduce collective Thomson scattering in beam-plasma systems, showing asymmetric scattered wave spectra that become distorted during instabilities.


<details>
  <summary>Details</summary>
Motivation: To understand and characterize collective Thomson scattering (CTS) in beam-plasma systems, particularly how scattered wave spectra become asymmetric and distorted during plasma instabilities, for applications in interpreting radar observations of ionospheric plasmas and laboratory CTS measurements.

Method: Used 2D particle-in-cell (PIC) simulations to reproduce CTS in beam-plasma systems. Formulated geometric shape of scattered wave spectrum in wave number space with arbitrary beam velocity and wave vectors. Conducted simulations for both unstable (Buneman and ion acoustic instability) and stable (weak, linearly stable beam-plasma with hot beam) systems.

Result: Scattered wave spectrum in 2D wave number space becomes asymmetric. Electron feature amplifies and becomes asymmetric/distorted during Buneman instability; ion feature amplifies and distorts during ion acoustic instability. Even in stable systems, scattered wave spectrum shows asymmetric features. The spectrum's direction-dependent wavelength function reveals these distortion patterns.

Conclusion: CTS in beam-plasma systems produces asymmetric scattered wave spectra that become distorted during plasma instabilities. These findings provide a framework for interpreting radar observations of ionospheric plasmas and laboratory CTS measurements, with potential applications in plasma diagnostics.

Abstract: Collective Thomson scattering (CTS) in a beam-plasma system is reproduced by using 2D PIC simulations and the characteristics of the scattered wave spectrum are examined. By formulating the geometric shape of the scattered wave spectrum in wave number space, where the velocity vector of the beam component and the wave vectors of the incident and scattered waves are arbitrary, it is demonstrated that the spectrum in 2D wave number space becomes asymmetric. The spectrum of scattered waves propagating in a specific direction is presented as a function of wavelength to show that the electron (ion) feature is amplified and becomes asymmetric or distorted when Buneman (ion acoustic) instability occurs. An additional simulation is conducted for a weak, linearly stable beam-plasma system with a hot beam, and confirmed that the obtained scattered wave spectrum shows asymmetric feature. The results are expected to be applicable to the interpretation of radar observations of ionospheric plasmas as well as CTS measurements in laboratory plasmas.

</details>


### [32] [A DC discharge plasma experiment for undergraduate laboratories](https://arxiv.org/abs/2512.10259)
*You-Hsuan Chen,Ting-An Wang,Pisin Chen*

Main category: physics.plasm-ph

TL;DR: Undergraduate students built a versatile DC glow-discharge plasma chamber to study fundamental plasma phenomena including Paschen breakdown, Langmuir probe measurements, spectroscopy, and magnetic focusing effects.


<details>
  <summary>Details</summary>
Motivation: Plasma physics provides rich fundamental phenomena ideal for undergraduate laboratory education, but hands-on plasma experiments are often complex and expensive. The authors aimed to create an accessible, versatile plasma chamber for junior-level curriculum that allows systematic exploration of basic plasma physics concepts.

Method: Designed and constructed a 1-meter-long quartz tube plasma chamber with movable electrode. Characterized Paschen breakdown and voltage-current characteristics. Developed Langmuir probes to measure electron temperature and density spatial distributions. Used Boltzmann plot spectroscopy for excitation temperature measurements. Added custom Helmholtz coils for magnetic focusing experiments with Runge-Kutta simulations of particle trajectories.

Result: Successfully created a functional plasma chamber platform that enables systematic investigation of plasma behavior under varying pressure, voltage, and geometry. Demonstrated magnetic focusing of electrons and analyzed electron drift velocity by comparing focal lengths. The apparatus provides comprehensive plasma diagnostics capabilities.

Conclusion: The plasma chamber offers a versatile, accessible platform for undergraduate plasma physics education, enabling hands-on investigation of fundamental phenomena. It has potential for future studies including microwave-plasma interactions and other student-driven research projects.

Abstract: Plasma physics offers a wide range of fundamental phenomena, making it an excellent subject for undergraduate laboratory instruction. In this work, we present the design, construction, and characterization of a DC glow-discharge plasma chamber developed for the junior-level curriculum, a project carried out by two undergraduate students. The apparatus consists of a 1-meter-long quartz tube with a movable electrode, enabling systematic exploration of plasma behavior under varying pressure, voltage, and geometry. Using this platform, we characterized the Paschen breakdown relation and the voltage-current characteristics of the plasma. We then developed Langmuir probes to map spatial distributions of electron temperature and density, and used Boltzmann plot spectroscopy to measure excitation temperatures across different plasma regions. Finally, with custom Helmholtz coils, we demonstrated magnetic focusing of electrons. We performed Runge-Kutta simulations of particle trajectories and analyzed the electron drift velocity by comparing the focal lengths. Overall, this plasma chamber provides a versatile platform for investigating fundamental plasma phenomena and offers potential for future studies, including microwave-plasma interactions and other student-driven investigations.

</details>


### [33] [Optimized matching conditions for self-guided laser wakefield accelerators](https://arxiv.org/abs/2512.10728)
*P. Valenta,K. G. Miller,B. K. Russell,M. Lamač,M. Jech,G. M. Grittani,S. V. Bulanov*

Main category: physics.plasm-ph

TL;DR: Bayesian optimization refines self-guided laser wakefield acceleration matching conditions to maximize electron energy, showing wide parameter tolerance for experimental flexibility.


<details>
  <summary>Details</summary>
Motivation: To maximize electron energy in laser wakefield acceleration by refining self-guided laser pulse propagation matching conditions and reducing experimental constraints.

Method: Bayesian optimization combined with quasi-3D particle-in-cell simulations in Lorentz-boosted frames to identify optimal parameters for maximum electron energy.

Result: Identified maximum achievable electron energy for given laser energy and corresponding acceleration distance, with wide parameter tolerance eliminating need for precise tuning.

Conclusion: Self-guided laser wakefield accelerators can produce near-maximum energy electrons across wide parameter ranges, providing substantial experimental flexibility and relaxing operational constraints.

Abstract: We revisit the matching conditions for self-guided laser pulse propagation in plasma and refine their formulation to maximize the energy of electrons produced via laser wakefield acceleration. Bayesian optimization, combined with particle-in-cell simulations carried out in a quasi-three-dimensional geometry and a Lorentz-boosted frame, is employed. The optimization identifies the maximum electron energy that a self-guided laser wakefield accelerator, driven by a laser of a given energy, can produce, together with the corresponding acceleration distance. Our results further demonstrate that electrons with energies close to the maximum value can be obtained across a relatively wide range of input parameters and without the need for their precise tuning. This provides substantial flexibility for experimental implementation and significantly relaxes the operational constraints associated with self-guided laser wakefield accelerators.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [34] [The Radon Transform-Based Sampling Methods for Biharmonic Sources from the Scattered Fields](https://arxiv.org/abs/2512.10332)
*Xiaodong Liu,Qingxiang Shi,Jing Wang*

Main category: math-ph

TL;DR: Three sampling methods for reconstructing extended sources in biharmonic wave equations using scattered field data, with varying requirements for measurements and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop quantitative sampling methods for reconstructing extended sources in biharmonic wave equations using scattered field measurements, addressing limitations of existing methods that require derivative data or have computational inefficiencies.

Method: Three indicator functions: 1) Uses only scattered fields on a single circle without derivative data, based on explicit source function formula; 2) Simplified double integral formula requiring Δu^s measurements for better efficiency; 3) Reconstructs source boundary from scattered fields at finite sensors using singularity analysis.

Result: The methods achieve high-resolution imaging of source support or source function itself, with the second method outperforming the first in computational speed and reconstruction accuracy, and uniqueness established for annulus and polygon-shaped sources.

Conclusion: The proposed sampling methods provide effective reconstruction of extended sources in biharmonic wave equations with varying measurement requirements, establishing connections between scattered fields and Radon transform of source functions for practical applications.

Abstract: This paper presents three quantitative sampling methods for reconstructing extended sources of the biharmonic wave equation using scattered field data. The first method employs an indicator function that solely relies on scattered fields $ u^s$ measured on a single circle, eliminating the need for Laplacian or derivative data. Its theoretical foundation lies in an explicit formula for the source function, which also serves as a constructive proof of uniqueness. To improve computational efficiency, we introduce a simplified double integral formula for the source function, at the cost of requiring additional measurements $Δu^s$. This advancement motivates the second indicator function, which outperforms the first method in both computational speed and reconstruction accuracy. The third indicator function is proposed to reconstruct the support boundary of extended sources from the scattered fields $ u^s$ at a finite number of sensors. By analyzing singularities induced by the source boundary, we establish the uniqueness of annulus and polygon-shaped sources. A key characteristic of the first and third indicator functions is their link between scattered fields and the Radon transform of the source function. Numerical experiments demonstrate that the proposed sampling methods achieve high-resolution imaging of the source support or the source function itself.

</details>


<div id='math.KT'></div>

# math.KT [[Back]](#toc)

### [35] [Sharp mapping properties of Poisson transforms and the Baum-Connes conjecture](https://arxiv.org/abs/2512.10018)
*Heiko Gimperlein,Magnus Goffeng*

Main category: math.KT

TL;DR: Quantitative analogue of Helgason's conjecture: Poisson transforms map Sobolev spaces to L² spaces with closed range for semisimple Lie groups of real rank one.


<details>
  <summary>Details</summary>
Motivation: The paper aims to prove a quantitative version of Helgason's conjecture at the distribution level, which is the remaining open conjecture in Julg's program to establish the Baum-Connes conjecture for closed subgroups of semisimple Lie groups of real rank one.

Method: Uses the Poisson transform (Szegö map) studied by Knapp-Wallach, with Sobolev spaces defined using van Erp-Yuncken's Heisenberg calculus. The proof shows boundedness with closed range and generalizes to show compactness of commutators with smooth functions on the Furstenberg compactification.

Result: Proves that Poisson transforms map a Sobolev space on P\G boundedly with closed range to an L²-space on K\G for semisimple Lie groups of real rank one, and shows commutators with smooth functions are compact.

Conclusion: Completes the proof of the remaining open conjecture in Julg's program, establishing the Baum-Connes conjecture for closed subgroups of semisimple Lie groups of real rank one through this quantitative distribution-level analogue of Helgason's conjecture.

Abstract: We prove a sharp, quantitative analogue of Helgason's conjecture at the level of distributions: For a semisimple Lie group $G$ of real rank one, Poisson transforms map a Sobolev space on $P\backslash G$ boundedly with closed range to an $L^2$-space on $K\backslash G$. The result is obtained for the Poisson transform studied by Knapp-Wallach under the name Szegö map, and the appropriate Sobolev spaces are defined using van Erp-Yuncken's Heisenberg calculus. The proof generalizes to show that commutators of this Poisson transform with smooth functions on the Furstenberg compactification are compact. This proves the remaining open conjecture in Julg's seminal program to establish the Baum-Connes conjecture for closed subgroups of semisimple Lie groups of real rank one.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [36] [Spectral Theory of the Weighted Fourier Transform with respect to a Function in $\mathbb{R}^n$: Uncertainty Principle and Diffusion-Wave Applications](https://arxiv.org/abs/2512.10880)
*Gustavo Dorrego,Luciano Luque*

Main category: math.CA

TL;DR: Generalizes weighted Fourier transform to n-dimensions, develops spectral theory, defines weighted fractional Laplacian, and solves fractional diffusion-wave equations using Fox H-functions.


<details>
  <summary>Details</summary>
Motivation: Extend the one-dimensional weighted Fourier transform with respect to a function to higher dimensions, enabling more comprehensive spectral analysis and applications to fractional differential equations.

Method: Generalize weighted Fourier transform to ℝⁿ, develop spectral theory on weighted Hilbert space, establish Plancherel identity, inversion formula, convolution theorem, uncertainty principle, define weighted fractional Laplacian (-Δ_{φ,ω})^s.

Result: Complete spectral theory established, fundamental solution of generalized time-space fractional diffusion-wave equation expressed in terms of Fox H-function, connection with generalized Mellin transform revealed.

Conclusion: Successfully extended weighted Fourier transform framework to n-dimensions, providing powerful tools for analyzing fractional differential equations with geometric deformations through spectral methods.

Abstract: In this paper, we generalize the weighted Fourier transform with respect to a function, originally proposed for the one-dimensional case in \cite{Dorrego}, to the $n$-dimensional Euclidean space $\mathbb{R}^{n}$. We develop a comprehensive spectral theory on a weighted Hilbert space, establishing the Plancherel identity, the inversion formula, the convolution theorem, and a Heisenberg-type uncertainty principle depending on the geometric deformation. Furthermore, we utilize this framework to rigorously define the weighted fractional Laplacian with respect to a function, denoted by $(-Δ_{φ,ω})^{s}$. Finally, we apply these tools to solve the generalized time-space fractional diffusion-wave equation, demonstrating that the fundamental solution can be expressed in terms of the Fox H-function, intrinsically related to the generalized $ω$-Mellin transform introduced in \cite{Dorrego}. In this paper, we generalize the weighted Fourier transform with respect to a function, originally proposed for the one-dimensional case, to the n-dimensional Euclidean space $\mathbb{R}^n$. We develop a comprehensive spectral theory on a weighted Hilbert space, establishing the Plancherel identity, the inversion formula, the convolution theorem, and a Heisenberg-type uncertainty principle depending on the geometric deformation. Furthermore, we utilize this framework to rigorously define the weighted fractional Laplacian with respect to a function, denoted by $(-Δ_{φ,ω})^s$. Finally, we apply these tools to solve the generalized time-space fractional diffusion-wave equation involving the weighted Hilfer derivative. We demonstrate that the fundamental solution can be explicitly expressed in terms of the Fox H-function, revealing an intrinsic connection with the generalized Mellin transform.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [37] [Error Analysis of Generalized Langevin Equations with Approximated Memory Kernels](https://arxiv.org/abs/2512.10256)
*Quanjun Lang,Jianfeng Lu*

Main category: stat.ML

TL;DR: Analysis of prediction error in stochastic dynamical systems with memory, showing trajectory discrepancies decay at rates determined by memory kernel decay and bounded by kernel estimation error.


<details>
  <summary>Details</summary>
Motivation: To understand how prediction errors in stochastic dynamical systems with memory (modeled by generalized Langevin equations) depend on memory kernel properties and estimation accuracy, providing theoretical foundations for improved trajectory prediction.

Method: Uses stochastic Volterra equations to model GLEs, integrates synchronized noise coupling with Volterra comparison theorem, employs resolvent estimates in weighted spaces for first-order models, and develops hypocoercive Lyapunov-type distance for second-order models with confining potentials.

Result: Established that trajectory discrepancies decay at rates determined by memory kernel decay, quantitatively bounded by kernel estimation error in weighted norm. Framework accommodates non-translation-invariant kernels and white-noise forcing, explicitly linking improved kernel estimation to enhanced trajectory prediction.

Conclusion: Theoretical framework provides explicit connection between memory kernel estimation accuracy and trajectory prediction quality in stochastic dynamical systems, validated by numerical examples, with applications to both first-order and second-order models under strongly convex potentials.

Abstract: We analyze prediction error in stochastic dynamical systems with memory, focusing on generalized Langevin equations (GLEs) formulated as stochastic Volterra equations. We establish that, under a strongly convex potential, trajectory discrepancies decay at a rate determined by the decay of the memory kernel and are quantitatively bounded by the estimation error of the kernel in a weighted norm. Our analysis integrates synchronized noise coupling with a Volterra comparison theorem, encompassing both subexponential and exponential kernel classes. For first-order models, we derive moment and perturbation bounds using resolvent estimates in weighted spaces. For second-order models with confining potentials, we prove contraction and stability under kernel perturbations using a hypocoercive Lyapunov-type distance. This framework accommodates non-translation-invariant kernels and white-noise forcing, explicitly linking improved kernel estimation to enhanced trajectory prediction. Numerical examples validate these theoretical findings.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [38] [Finding core subgraphs of directed graphs via discrete Ricci curvature flow](https://arxiv.org/abs/2512.07899)
*Juan Zhao,Jicheng Ma,Yunyan Yang,Liang Zhao*

Main category: cs.SI

TL;DR: The paper introduces Ricci curvature and curvature flow for directed graphs, enabling detection of strongly connected subgraphs from weakly connected graphs without requiring strong connectivity.


<details>
  <summary>Details</summary>
Motivation: Existing Ricci curvature methods focus heavily on undirected graphs for community detection and core extraction, with relatively less attention on directed graphs. There's a need for geometric methods that work with directed networks.

Method: Define Ricci curvature and curvature flow for directed graphs. For weakly connected graphs, transform them into strongly connected ones by adding edges with very large artificial weights. The curvature flow uniquely solves for strongly connected graphs, and the artificial edges are automatically discarded during final iterations due to their extreme weights.

Result: The method consistently surpasses traditional approaches for core evaluation, achieving better results on at least two out of three key metrics. The flow admits unique global solutions for strongly connected directed graphs.

Conclusion: The proposed Ricci curvature flow provides an effective geometric method for analyzing directed graphs, particularly for detecting strongly connected subgraphs from weakly connected graphs, overcoming previous limitations requiring strong connectivity.

Abstract: Ricci curvature and its associated flow offer powerful geometric methods for analyzing complex networks. While existing research heavily focuses on applications for undirected graphs such as community detection and core extraction, there have been relatively less attention on directed graphs.
  In this paper, we introduce a definition of Ricci curvature and an accompanying curvature flow for directed graphs. Crucially, for strongly connected directed graphs, this flow admits a unique global solution. We then apply this flow to detect strongly connected subgraphs from weakly connected directed graphs. (A weakly connected graph is connected overall but not necessarily strongly connected). Unlike prior work requiring graphs to be strongly connected, our method loosens this requirement. We transform a weakly connected graph into a strongly connected one by adding edges with very large artificial weights. This modification does not compromise our core subgraph detection. Due to their extreme weight, these added edges are automatically discarded during the final iteration of the Ricci curvature flow.
  For core evaluation, our approach consistently surpasses traditional methods, achieving better results on at least two out of three key metrics. The implementation code is publicly available at https://github.com/12tangze12/Finding-core-subgraphs-on-directed-graphs.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [39] [Indirect methods in optimal control on Banach spaces](https://arxiv.org/abs/2512.10831)
*Roman Chertovskih,Nikolay Pogodaev,Maxim Staritsyn,A. Pedro Aguiar*

Main category: math.OC

TL;DR: Novel indirect descent method for optimal control of nonlinear ODEs in Banach spaces with monotone convergence, tested on neural field control.


<details>
  <summary>Details</summary>
Motivation: Classical indirect methods based on Pontryagin's maximum principle are sensitive to local convexity and lack monotone convergence, motivating development of more stable alternatives.

Method: Develops an alternative indirect descent method using exact cost-increment formulas and finite-difference probes of the terminal cost, applied to optimal control of nonlinear ODEs in Banach spaces.

Result: The proposed method exhibits stable monotone convergence in numerical analysis of an Amari-type neural field control problem, outperforming classical schemes.

Conclusion: The new indirect descent method provides a more robust alternative to classical Pontryagin-based approaches, offering monotone convergence for optimal control problems in distributed dynamics.

Abstract: This work focuses on indirect descent methods for optimal control problems governed by nonlinear ordinary differential equations in Banach spaces, viewed as abstract models of distributed dynamics. As a reference line, we revisit the classical schemes, rooted in Pontryagin's maximum principle, and highlight their sensitivity to local convexity and lack of monotone convergence. We then develop an alternative method based on exact cost-increment formulas and finite-difference probes of the terminal cost. We show that our method exhibits stable monotone convergence in numerical analysis of an Amari-type neural field control problem.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [40] [Generation of proton beams at switchback boundary-like rotational discontinuities in the solar wind](https://arxiv.org/abs/2512.10406)
*Rong Lin,Fabio Bacchini,Jiansen He,Luca Pezzini,Jingyu Peng*

Main category: physics.space-ph

TL;DR: Proton trapping at switchback boundary rotational discontinuities creates beam-like populations with temperature anisotropy that excite ion cyclotron waves, suggesting these boundaries are viable proton beam generation sites in the heliosphere.


<details>
  <summary>Details</summary>
Motivation: To investigate how Alfvénic rotational discontinuities (RDs) at switchback boundaries affect proton kinetics, since these structures are abundant in the inner heliosphere and can model magnetic kink boundaries.

Method: Used hybrid Particle-In-Cell (PIC) approach in a 2D system to model a pair of switchback-boundary-like rotational discontinuities and analyze proton behavior.

Result: Found significant proton trapping at one boundary RD, creating a secondary beam-like component with temperature anisotropy T⊥/T∥ ≳ 4 that excites ion cyclotron waves in the downstream transition layer. Static electric field near RD is key to proton trapping.

Conclusion: Switchback boundaries could represent viable environments for proton beam creation in the heliosphere, highlighting the need to investigate RD substructures (especially embedded current systems) and obtain high-resolution solar wind velocity distribution observations around RDs.

Abstract: Alfvénic rotational discontinuities (RDs) are abundant in the inner heliosphere and can be used to model the boundary of switchbacks, i.e. Alfvénic magnetic kinks. To investigate the effects of RDs on proton kinetics, we model a pair of switchback-boundary-like RDs with a hybrid Particle-In-Cell (PIC) approach in a 2D system. We find that, at one of the boundary RDs, a significant population of protons remains trapped over long times, creating a secondary beam-like component with temperature anisotropy $T_\perp/T_\|\gtrsim4$ in the proton velocity distribution function that excites ion cyclotron waves within the downstream portion of the transition layer. Further analysis suggests that the static electric field in the vicinity of the RD is the key factor in trapping the protons. This work indicates that switchback boundaries could represent a viable environment for the creation of proton beams in the heliosphere; it also highlights the need to investigate RD sub-structures, especially the embedded current systems of interplanetary RDs. Finally, this paper underscores the importance of high-resolution observations of the solar wind velocity distributions around RDs.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [41] [Engineering Multifunctional Response in Monolayer Fe3O4 via Zr Adsorption: From Half-Metallicity to Enhanced Piezoelectricity](https://arxiv.org/abs/2512.10434)
*Sikander Azam,Qaiser Rafiq,Rajwali Khan,Hamdy Khamees Thabet*

Main category: cond-mat.mtrl-sci

TL;DR: Zr adsorption on monolayer Fe3O4 enhances optical, dielectric, and piezoelectric properties while maintaining half-metallicity, offering a tunable platform for multifunctional 2D magnetic oxides.


<details>
  <summary>Details</summary>
Motivation: To explore how Zr adsorption can controllably tune the multifunctional properties of 2D magnetic Fe3O4 monolayers for applications in spintronics, optoelectronics, and energy conversion.

Method: First-principles study using spin-polarized DFT with GGA+U method to analyze pure monolayer Fe3O4 and Zr adsorption at two sites (on-top Fe and bridge between Fe atoms), examining structural, electronic, magnetic, optical, elastic, and piezoelectric properties.

Result: Zr adsorption causes lattice distortions and orbital hybridization, reducing bandgap and increasing optical absorption. Bridge-site adsorption triples piezoelectric coefficient e11, enhances dielectric response and optical conductivity, while maintaining half-metallicity. Elastic constants show mechanical softening after functionalization.

Conclusion: Zr adsorption provides a non-destructive, controllable method to tune spin, charge, and lattice interactions in Fe3O4 monolayers, integrating magnetic, optical, and piezoelectric functionalities in a single 2D material platform.

Abstract: Two-dimensional (2D) magnetic oxides are increasingly studied for their multifunctional potential in fields like spintronics, optoelectronics, and energy conversion. In this research, we conduct a detailed first-principles study of pure monolayer Fe3O4 and its modification through Zr adsorption at two sites: on top of an Fe atom and at the bridge between Fe atoms. Using spin-polarized density functional theory with the GGA plus U method, we examine how adsorption affects structure, electronic, magnetic, optical, elastic, and piezoelectric properties. The original monolayer shows half-metallicity, strong spin polarization, and a moderate in-plane piezoelectric effect. Zr adsorption causes local lattice distortions and orbital hybridization, resulting in intermediate electronic states, a reduced bandgap, and increased optical absorption in both spin channels. Notably, Zr at the bridge site greatly enhances dielectric response, optical conductivity, and piezoelectric coefficients, tripling e11 compared to the pristine layer. Elastic constants indicate mechanical softening after functionalization, and energy loss spectra display shifts in plasmon resonance. These findings suggest Zr adsorption offers a controllable, non-destructive way to tune spin, charge, and lattice interactions in Fe4O4 monolayers, connecting magnetic, optical, and piezoelectric functionalities within a single 2D material platform.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [42] [Large deviations for invariant measure of stochastic Allen-Cahn equation with inhomogeneous boundary conditions and multiplicative noise](https://arxiv.org/abs/2512.10536)
*Rui Bai,Chunrong Feng,Huaizhong Zhao*

Main category: math.PR

TL;DR: The paper proves a large deviation principle for invariant measures of the 1D stochastic Allen-Cahn equation with inhomogeneous Dirichlet boundary conditions and multiplicative noise, showing concentration around the unique energy minimizer.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous large deviation results for stochastic Allen-Cahn equations with inhomogeneous boundary conditions and unbounded multiplicative noise, addressing challenges from non-strong dissipativity.

Method: Uses L. Simon's convergence theorem to analyze noiseless dynamics, obtains estimates on invariant measures in Sobolev spaces W^{k*,p*} with k*p*>1, and proves large deviation principle via small noise analysis.

Result: Proves validity of small noise large deviation principle for invariant measures, shows exponential concentration around unique Ginzburg-Landau energy minimizer for sufficiently small noise parameter ε.

Conclusion: The invariant measures concentrate exponentially fast around the unique minimizer of the Ginzburg-Landau energy functional with given boundary conditions, establishing rigorous large deviation behavior for this class of stochastic PDEs.

Abstract: We prove the validity of a small noise large deviation principle for the family of invariant measures $\{μ_ε\}_{ε>0} $ associated to the one dimensional stochastic Allen-Cahn equation with inhomogeneous Dirichlet boundary conditions, perturbed by unbounded multiplicative noise. The main difficulty is that the system is not strongly dissipative. Using L. Simon's convergence theorem, we show that the dynamics of the noiseless system converge in large time to the minimizer of the Ginzburg-Landau energy functional, which is unique due to the boundary condition. We obtain an estimate of the invariant measure on the bounded set in the Sobolev space $W^{k^\star,p^\star} $, where $k^\star p^\star>1$, and $p^\star$ is large. As a corollary of the main result, we show that $μ_ε$ concentrates around the unique minimizer with such boundary conditions exponentially fast when $ε$ is sufficiently small.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [43] [Excitation energies and UV-Vis absorption spectra from INDO/s+ML](https://arxiv.org/abs/2512.10397)
*Ezekiel Oyeniyi,Omololu Akin-Ojo*

Main category: cond-mat.mes-hall

TL;DR: ML models correct INDO/s excitation energies, reducing error from 1.1 eV to 0.2 eV relative to TDDFT while maintaining low computational cost.


<details>
  <summary>Details</summary>
Motivation: INDO/s method is computationally efficient for studying excitation energies in large molecular systems but suffers from low accuracy compared to methods like TDDFT.

Method: Develop machine learning models that correct INDO/s results, combining the computational efficiency of INDO/s with ML-based accuracy improvements.

Result: ML corrections reduce average error from 1.1 eV to 0.2 eV relative to TDDFT, and produce UV-Vis absorption spectra in good agreement with TDDFT predictions.

Conclusion: The INDO/s+ML approach provides accurate excitation energies and absorption spectra with negligible computational overhead, bridging the gap between efficiency and accuracy.

Abstract: The semi-empirical INDO/s method is popular for studies of excitation energies and absorption of molecules due to its low computational requirement, making it possible to make predictions for large systems. However, its accuracy is generally low, particularly, when compared with the typical accuracy of other methods such as time-dependent density functional theory (TDDFT). Here, we present machine learning (ML) models that correct the INDO/s results with negligible increases in the amount of computing resources needed. While INDO/s excitations energies have an average error of about 1.1 eV relative to TDDFT energies, the added ML corrections reduce the error to 0.2 eV. Furthermore, this combination of INDO/s and ML produces UV-Vis absorption spectra that are in good agreement with the TDDFT predictions.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [44] [Phase structure of the one-dimensional $\mathbb{Z}_2$ lattice gauge theory with second nearest-neighbor interactions](https://arxiv.org/abs/2512.10755)
*Yeimer Zambrano,Aleksey Alekseev,Konrad J. Kapcia,Krzysztof Cichy,Agnieszka Cichy*

Main category: cond-mat.str-el

TL;DR: Study of 1D Z2 lattice gauge theory with hard-core bosons including second nearest-neighbor interactions reveals rich phase transitions between Luttinger liquid, Mott insulator, and charge-ordered insulator phases.


<details>
  <summary>Details</summary>
Motivation: To extend previous studies of 1D Z2 lattice gauge theory models by including second nearest-neighbor interactions and investigate their impact on the ground-state phase diagram, particularly examining how extended interactions affect confinement and charge ordering.

Method: Used matrix product state techniques within density matrix renormalization group (DMRG) to compute charge gap, static structure factor, and pair-pair correlation functions for various interaction strengths and field parameters. Analyzed two representative nearest-neighbor interaction strengths (V1) corresponding to Luttinger liquid and Mott insulator phases, then introduced second nearest-neighbor coupling (V2) to study its effects.

Result: Rich phase behavior emerges: For small V1, increasing V2 causes direct transition from Luttinger liquid to charge-ordered insulator. For large V1, transition goes from Mott insulator through intermediate Luttinger liquid region to charge-ordered insulator. Second nearest-neighbor interactions enhance charge order and suppress pair coherence, evidenced by sharp peaks in structure factor and rapid decay in pair-pair correlators.

Conclusion: The work extends understanding of 1D Z2 lattice gauge theory phase structure and demonstrates complex interplay between gauge fields, confinement, and extended interactions, showing how second nearest-neighbor couplings can dramatically alter phase transitions and ordering phenomena.

Abstract: We investigate the ground-state phase diagram of a one-dimensional $\mathbb{Z}_2$ lattice gauge theory (LGT) model with hard-core bosons at half-filling, extending previous studies by including second nearest-neighbor (2NN) interactions. Using matrix product state techniques within the density matrix renormalization group, we compute charge gap, static structure factor, and pair-pair correlation functions for various interaction strengths and field parameters. We analyze two representative neatest-neighbor interaction strengths ($V_1$) that correspond to the Luttinger liquid (LL) and Mott insulator (MI) phases in the absence of the 2NN interactions. We introduce the 2NN coupling $V_2$ and investigate its impact on the system. Our results reveal very rich behavior. As the 2NN repulsion increases, in the case of small $V_1$, we observe a direct transition from the LL phase to a charge-ordered insulator (COI) phase, whereas for large $V_1$, we observe a transition from the MI phase (previously found with only $V_1$ included), going through an intermediate LL region, and finally reaching the COI regime. Additionally, the inclusion of 2NN interactions enhances charge order and suppresses pair coherence, evidenced by sharp peaks in the structure factor and rapid decay in pair-pair correlators. Our work extends the well-studied phase structure of 1D $\mathbb{Z}_2$ LGT models and demonstrates the interplay between gauge fields, confinement, and extended interactions.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [45] [Quantum Monte Carlo in Classical Phase Space with the Wigner-Kirkwood Commutation Function. Results for the Saturation Liquid Density of $^4$He](https://arxiv.org/abs/2512.09948)
*Phil Attard*

Main category: cond-mat.stat-mech

TL;DR: A Monte Carlo method for complex phase space weights in quantum statistical mechanics, tested on liquid helium near lambda transition.


<details>
  <summary>Details</summary>
Motivation: Quantum statistical mechanics often involves complex phase space weights that are challenging for traditional Monte Carlo methods, requiring new computational approaches.

Method: Metropolis Monte Carlo algorithm adapted for complex phase space weights, applied to Lennard-Jones helium near lambda transition with Wigner-Kirkwood expansion to third order.

Result: Simulations produce saturation liquid density values that agree with experimental measurements.

Conclusion: The developed Monte Carlo method successfully handles complex phase space weights in quantum systems and accurately predicts thermodynamic properties like liquid density.

Abstract: A Metropolis Monte Carlo algorithm is given for the case of a complex phase space weight, which applies generally in quantum statistical mechanics. Computer simulations using Lennard-Jones $^4$He near the $λ$-transition, including an expansion to third order of the Wigner-Kirkwood commutation function, give a saturation liquid density in agreement with measured values.

</details>
