<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 17]
- [math.AP](#math.AP) [Total: 12]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [math.DG](#math.DG) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [math.SP](#math.SP) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [math.PR](#math.PR) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Enhancing polynomial approximation of continuous functions by composition with homeomorphisms](https://arxiv.org/abs/2512.13740)
*Álvaro Fernández Corral,Yahya Saleh*

Main category: math.NA

TL;DR: Polynomials composed with homeomorphisms enhance approximation capabilities while maintaining density in continuous function spaces, enabling accurate approximations for functions with finite local extrema.


<details>
  <summary>Details</summary>
Motivation: Standard numerical methods for multivariate approximation often suffer from the curse of dimensionality. The paper aims to enhance polynomial approximation capabilities by composing them with homeomorphisms to achieve better accuracy while maintaining theoretical guarantees.

Method: Compose algebraic polynomials with homeomorphisms to create families of functions. Prove that for univariate continuous functions with finite local extrema, there exists a polynomial of finite degree and a homeomorphism whose composition can approximate the target function arbitrarily well. Use invertible neural networks to parametrize the homeomorphism in practical applications.

Result: Theoretical proof that polynomial-homeomorphism compositions can approximate functions with finite local extrema to arbitrary accuracy. Numerical experiments on regression tasks and molecular potential-energy surfaces show strong agreement with theoretical analysis, demonstrating practical effectiveness.

Conclusion: Composing polynomials with homeomorphisms enhances approximation capabilities while maintaining theoretical density properties. This approach is particularly valuable for multivariate problems where traditional methods face the curse of dimensionality, with practical applications validated through numerical experiments.

Abstract: We enhance the approximation capabilities of algebraic polynomials by composing them with homeomorphisms. This composition yields families of functions that remain dense in the space of continuous functions, while enabling more accurate approximations. For univariate continuous functions exhibiting a finite number of local extrema, we prove that there exist a polynomial of finite degree and a homeomorphism whose composition approximates the target function to arbitrary accuracy. The construction is especially relevant for multivariate approximation problems, where standard numerical methods often suffer from the curse of dimensionality. To support our theoretical results, we investigate both regression tasks and the construction of molecular potential-energy surfaces, parametrizing the underlying homeomorphism using invertible neural networks. The numerical experiments show strong agreement with our theoretical analysis.

</details>


### [2] [Offline Maximizing Minimally Invasive Proper Orthogonal Decomposition for Reduced Order Modeling of $S_n$ Radiation Transport](https://arxiv.org/abs/2512.13963)
*Quincy Huhn,Jean Ragusa,Youngsoo Choi*

Main category: math.NA

TL;DR: OMMI-POD: A novel ROM approach for Sn transport equation using offline sweeps and interpolation to achieve 1600x speedup with low error.


<details>
  <summary>Details</summary>
Motivation: Deterministic solutions to Sn transport equation are computationally expensive. Need efficient Reduced Order Models to approximate Full Order Model solutions while maintaining accuracy.

Method: Offline Maximizing Minimally Invasive POD (OMMI-POD) extends Minimally Invasive POD by performing transport sweeps offline. Generates library of reduced systems from training set, then interpolates in online stage for rapid approximate solutions.

Result: Evaluated on multigroup 2-D test problem, demonstrates low error and 1600-fold speedup over full order model.

Conclusion: OMMI-POD provides an efficient ROM approach for Sn transport equation with significant computational speedup while maintaining accuracy.

Abstract: Deterministic solutions to the Sn transport equation can be computationally expensive to calculate. Reduced Order Models (ROMs) provide an efficient means of approximating the Full Order Model (FOM) solution. We propose a novel approach for constructing ROMs of the Sn radiation transport equation, Offline Maximizing Minimally Invasive (OMMI) Proper Orthogonal Decomposition (POD). POD uses snapshot data to build a reduced basis, which is then used to project the FOM. Minimally Invasive POD leverages the sweep infrastructure within deterministic Sn transport solvers to construct the reduced linear system, even though the FOM linear system is never directly assembled. OMMI-POD extends Minimally Invasive POD by performing transport sweeps offline, thereby maximizing the potential speedup. It achieves this by generating a library of reduced systems from a training set, which is then interpolated in the online stage to provide a rapid approximate solution to the Sn transport equation. The model's performance is evaluated on a multigroup 2-D test problem, demonstrating low error and a 1600-fold speedup over the full order model.

</details>


### [3] [An inverse problem for the one-phase Stefan problem with varying melting temperature](https://arxiv.org/abs/2512.13975)
*Marc Dambrine,Helmut Harbrecht*

Main category: math.NA

TL;DR: Numerical solution of forward and backward Stefan problems with time-varying melting temperature using moving mesh finite element methods.


<details>
  <summary>Details</summary>
Motivation: To solve practical problems where melting temperature varies over time, such as when external pressure changes, requiring both forward prediction and backward reconstruction capabilities.

Method: Moving mesh finite element method for both forward (computing domain evolution) and backward (reconstructing melting temperature) problems with time-dependent melting temperature.

Result: Development of numerical algorithms and provision of numerical simulations for both forward and backward Stefan problems with time-varying boundary conditions.

Conclusion: Successfully developed computational framework for solving transient one-phase Stefan problems with time-dependent melting temperature, enabling both forward prediction and inverse reconstruction.

Abstract: The present article is dedicated to the forward and backward solution of a transient one-phase Stefan problem. In the forward problem, we compute the evolution of the initial domain for a Stefan problem where the melting temperature varies over time. This occurs in practice, for example, when the pressure in the external space changes in time. In the corresponding backward problem, we then reconstruct the time-dependent melting temperature from the knowledge of the evolving geometry. We develop respective numerical algorithms using a moving mesh finite element method and provide numerical simulations.

</details>


### [4] [Multiple Scale Methods For Optimization Of Discretized Continuous Functions](https://arxiv.org/abs/2512.13993)
*Nicholas J. E. Richardson,Noah Marusenko,Michael P. Friedlander*

Main category: math.NA

TL;DR: Multiscale optimization framework for Lipschitz functions using coarse-to-fine grid refinement with warm-starting, achieving tighter error bounds and order-of-magnitude speedups.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient optimization framework for problems over Lipschitz continuous functions that can achieve provably better error bounds with lower computational cost than single-scale approaches.

Method: Multiscale approach solving coarse-grid discretization first, then using linear interpolation to warm-start projected gradient descent on progressively finer grids. Includes greedy and lazy variants with constraint modification techniques to preserve feasibility across scales.

Result: Theoretical convergence guarantees show tighter error bounds at lower computational cost than single-scale optimization. Numerical experiments on probability density estimation problems (including geological data) demonstrate speedups of an order of magnitude or better.

Conclusion: The multiscale optimization framework provides a computationally efficient approach for Lipschitz function optimization with provable convergence guarantees and significant practical speedups, extending to any base algorithm with fixed-rate iterate convergence.

Abstract: A multiscale optimization framework for problems over a space of Lipschitz continuous functions is developed. The method solves a coarse-grid discretization followed by linear interpolation to warm-start project gradient descent on progressively finer grids. Greedy and lazy variants are analyzed and convergence guarantees are derived that show the multiscale approach achieves provably tighter error bounds at lower computational cost than single-scale optimization. The analysis extends to any base algorithm with iterate convergence at a fixed rate. Constraint modification techniques preserve feasibility across scales. Numerical experiments on probability density estimation problems, including geological data, demonstrate speedups of an order of magnitude or better.

</details>


### [5] [Adaptive Wavelet-Galerkin Modelling of Heat Conduction in Heterogeneous Composite Materials](https://arxiv.org/abs/2512.14089)
*Taylan Demir,Atakan Koçyiğit*

Main category: math.NA

TL;DR: Adaptive wavelet Galerkin method for transient heat conduction in composites uses wavelet-based refinement to efficiently resolve sharp temperature gradients near interfaces.


<details>
  <summary>Details</summary>
Motivation: Need efficient numerical methods for transient heat conduction in heterogeneous composite materials where sharp temperature gradients occur near material interfaces and boundary layers, requiring high resolution without excessive computational cost.

Method: Combines multiresolution wavelet bases with implicit time discretization; uses adaptive refinement driven by wavelet coefficients to selectively refine regions with sharp gradients; reduces degrees of freedom compared to uniform discretizations.

Result: Numerical examples demonstrate accurate resolution of layered, inclusion-based, and functionally graded composites with improved computational efficiency; wavelet-based adaptivity significantly reduces required degrees of freedom.

Conclusion: The adaptive wavelet Galerkin method provides an efficient approach for transient heat conduction in composites, effectively resolving sharp gradients near interfaces while maintaining computational efficiency through wavelet-based adaptive refinement.

Abstract: We present an adaptive wavelet Galerkin method for transient heat conduction in heterogeneous composite materials. The approach combines multiresolution wavelet bases with an implicit time discretization to efficiently resolve sharp temperature gradients near material interfaces and boundary layers. Adaptive refinement is driven by wavelet coefficients, significantly reducing the number of degrees of freedom compared to uniform discretizations. Numerical examples demonstrate accurate resolution of layered, inclusion-based, and functionally graded composites with improved computational efficiency.

</details>


### [6] [Weighted Group Lasso for a static EEG problem](https://arxiv.org/abs/2512.14163)
*Ole Løseth Elvetun,Bjørn Fredrik Nielsen,Niranjana Sudheer*

Main category: math.NA

TL;DR: Weighted Group Lasso for EEG source reconstruction reduces depth/orientation bias, with theoretical guarantees and practical weighting strategies improving localization accuracy.


<details>
  <summary>Details</summary>
Motivation: The static inverse EEG problem suffers from depth bias and orientation bias when reconstructing neuronal sources from scalp voltage measurements. Existing methods need improvement in spatial accuracy and physiological realism.

Method: Weighted Group Lasso formulation that models three orthogonal dipole components at each location as a single coherent group. Uses regularization framework with specific weighting strategies, including truncated Moore-Penrose pseudoinverse for the weighting matrix.

Result: The method effectively mitigates depth and orientation bias. Theoretical recovery guarantees provided for single and multiple group sources. Numerical experiments show practical reconstruction quality depends on weighting strategy, with truncated Moore-Penrose pseudoinverse giving small Dipole Localization Error (DLE).

Conclusion: Weighted Group Lasso offers a robust approach for inverse EEG problems, enabling improved spatial accuracy and more physiologically realistic reconstruction of neural activity.

Abstract: We investigate the weighted Group Lasso formulation for the static inverse electroencephalography (EEG) problem, aiming at reconstructing the unknown underlying neuronal sources from voltage measurements on the scalp. By modelling the three orthogonal dipole components at each location as a single coherent group, we demonstrate that depth bias and orientation bias can be effectively mitigated through the proposed regularization framework. On the theoretical front, we provide concise recovery guarantees for both single and multiple group sources. Our numerical experiments highlight that while theoretical bounds hold for a broad range of weight definitions, the practical reconstruction quality, for cases not covered by the theory, depends significantly on the specific weighting strategy employed. Specifically, employing a truncated Moore-Penrose pseudoinverse for the involved weighting matrix gives a small Dipole Localization Error (DLE). The proposed method offers a robust approach for inverse EEG problems, enabling improved spatial accuracy and a more physiologically realistic reconstruction of neural activity.

</details>


### [7] [Efficient LU factorization exploiting direct-indirect Burton-Miller equation for Helmholtz transmission problems](https://arxiv.org/abs/2512.14193)
*Yasuhiro Matsumoto,Kei Matsushima*

Main category: math.NA

TL;DR: A direct-indirect mixed Burton-Miller boundary integral equation for Helmholtz scattering with transmissive scatterers that enables faster LU-factorization-based solvers and maintains well-posedness.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient boundary integral formulation for Helmholtz scattering problems with transmissive scatterers that allows for faster direct solvers while maintaining mathematical well-posedness.

Method: Proposes a direct-indirect mixed Burton-Miller boundary integral equation with three unknowns, exploits sparse alignment of boundary integral operators for efficient LU factorization, and develops proxy method with weak admissibility low-rank approximation for fast direct solvers.

Result: The proposed formulation enables approximately 40% faster direct solvers compared to ordinary formulations when using LU-factorization-based solvers, effectively accelerates nonlinear eigenvalue finding, and maintains well-posedness for C² boundaries.

Conclusion: The direct-indirect mixed Burton-Miller formulation provides significant computational advantages for Helmholtz scattering problems while establishing mathematical rigor for well-posedness on smooth boundaries.

Abstract: This paper proposes a direct-indirect mixed Burton-Miller boundary integral equation for solving Helmholtz scattering problems with transmissive scatterers. The proposed formulation has three unknowns, one more than the number of unknowns for the ordinary formulation. However, we can construct efficient numerical solvers based on LU factorization by exploiting the sparse alignment of the boundary integral operators of the proposed formulation. Numerical examples demonstrate that the direct solver based on the proposed formulation is approximately 40% faster than the ordinary formulation when the LU-factorization-based solver is used. In addition, the proposed formulation is applied to a fast direct solver employing LU factorization in its algorithm. In the application to the fast direct solver, the proxy method with a weak admissibility low-rank approximation is developed. The speedup achieved using the proposed formulation is also shown to be effective in finding nonlinear eigenvalues, which are related to the uniqueness of the solution, in boundary value problems. Furthermore, the well-posedness of the proposed boundary integral equation is established for scatterers with boundaries of class $C^2$, using the mapping property of boundary integral operators in Hölder space.

</details>


### [8] [Analysis of a finite element method for second order uniformly elliptic PDEs in non-divergence form](https://arxiv.org/abs/2512.14219)
*Weifeng Qiu*

Main category: math.NA

TL;DR: A finite element method for second-order linear elliptic PDEs in non-divergence form and elliptic Hamilton-Jacobi-Bellman equations, with optimal convergence in discrete W^{2,p}-norm for convex polyhedra and non-convex polygons.


<details>
  <summary>Details</summary>
Motivation: To develop a unified finite element method that can handle both linear elliptic PDEs in non-divergence form and elliptic HJB equations, addressing challenges with discontinuous coefficients and extending applicability to non-convex domains.

Method: A finite element method that works for both problem types, considering two scenarios for the coefficient matrix A: uniformly continuous A, and discontinuous A where γA is dominated by the identity matrix with positive weight function γ.

Result: Proves optimal convergence in discrete W^{2,p}-norm for 1<p≤2 on convex polyhedra in ℝ^d (d=2,3), and for p in a neighborhood of 4/3 on 2D non-convex polygons. Also proves well-posedness of strong solutions in W^{2,p}(Ω) for both problem types under these conditions.

Conclusion: The proposed finite element method successfully handles both linear elliptic PDEs in non-divergence form and HJB equations, with proven convergence results for convex and non-convex domains, while relaxing continuity assumptions on HJB coefficients.

Abstract: We propose one finite element method for both second order linear uniformly elliptic PDE in non-divergence form and the elliptic Hamilton-Jacobi-Bellman (HJB) equation. For the linear elliptic PDE in non-divergence form, we consider two scenarios of the matrix coefficient matrix $A$. One is $A$ is uniformly continuous. The other is $A$ is discontinuous but $γA$ is dominated by $I_{d}$ where $γ$ is a positive weight function.
  We prove that optimal convergence in discrete $W^{2,p}$-norm of the numerical approximation to the strong solution for $1<p\leq 2$ on convex polyhedra in $\mathbb{R}^{d}$ ($d=2,3$). If the domain is a two dimensional non-convex polygon, $p$ is valid in a neighbourhood of $\frac{4}{3}$. We also prove the well-posedness of strong solution in $W^{2,p}(Ω)$ for both linear elliptic PDE in non-divergence form and the HJB equation for $1< p \leq 2$ on convex polyhedra in $\mathbb{R}^{d}$ ($d=2,3$) and for $p$ in an open interval starting from $1$ and including $\frac{4}{3}$ on two dimensional non-convex polygon. Furthermore, we relax the assumptions on the continuity of coefficients of the HJB equation, which have been widely used in literature.

</details>


### [9] [Structure-preserving Variational Multiscale Stabilization of the Incompressible Navier-Stokes Equations](https://arxiv.org/abs/2512.14231)
*Kevin Dijkstra,Deepesh Toshniwal*

Main category: math.NA

TL;DR: A VMS formulation for incompressible Navier-Stokes using FEEC framework that preserves geometric structure, ensures stability, optimal convergence, and efficient parallel computation.


<details>
  <summary>Details</summary>
Motivation: To develop a stabilized discretization for Navier-Stokes equations that preserves the geometric and topological structure of continuous spaces while ensuring stability and convergence, addressing limitations of traditional unstabilized methods.

Method: Uses FEEC framework with vorticity-velocity-pressure formulation, introduces fine-scale governing equations for unresolved scales discretized using FEEC, preserves de Rham complex structure, enables parallel fine-scale solving, and allows elimination during matrix assembly.

Result: Formulation is residual-based, energetically stable, optimally convergent, computationally efficient (parallel fine-scale solving), applicable to both low- and high-regularity discretizations, and validated through numerical experiments showing improved solution quality.

Conclusion: The FEEC-based VMS formulation successfully preserves continuous structure, ensures stability and optimal convergence, provides computational efficiency through parallelization, and works across various discretization types with asymptotic stabilization turning off upon mesh refinement.

Abstract: This paper introduces a Variational Multiscale Stabilization (VMS) formulation of the incompressible Navier--Stokes equations that utilizes the Finite Element Exterior Calculus (FEEC) framework. The FEEC framework preserves the geometric and topological structure of continuous spaces and PDEs in the discrete spaces and model, and helps build stable and convergent discretizations. For the Navier-Stokes equations, this structure is encoded in the de Rham complex. In this work, we consider the vorticity-velocity-pressure formulation discretized within the FEEC framework. We model the effect of the unresolved scales on the finite-dimensional solution by introducing appropriate fine-scale governing equations, which we also discretize using the FEEC approach. This preserves the structure of the continuous problem in both the coarse- and fine-scale solutions; for instance, both the coarse- and fine-scale velocities are pointwise incompressible. We demonstrate that the resulting formulation is residual-based, energetically stable, and optimally convergent. Moreover, our fine-scale model provides an efficient computational approach: by decoupling fine-scale problems across elements, they can be solved in parallel. In fact, the fine-scale equations can be eliminated during matrix assembly, leading to a VMS formulation in which the problem size is governed solely by the coarse-scale discretization. Finally, the proposed formulation applies to both the lowest regularity discretizations of the de Rham complex and high-regularity isogeometric discretizations. We validate our theoretical results through numerical experiments, simulating both steady-, unsteady-, viscous-, and inviscid-flow problems. These tests show that the stabilized solutions are qualitatively better than the unstabilized ones, converge at optimal rates, and, as the mesh is refined, the stabilization is asymptotically turned off.

</details>


### [10] [SPINNs -- Deep learning framework for approximation of stochastic differential equations](https://arxiv.org/abs/2512.14258)
*Marcin Baranek,Paweł Przybyłowicz*

Main category: math.NA

TL;DR: SPINNs provide a neural network framework for solving stochastic differential equations with Lévy noise.


<details>
  <summary>Details</summary>
Motivation: There's a need for systematic approaches to solve SDEs driven by Lévy noise using neural networks, as traditional methods may be computationally expensive or limited.

Method: Stochastic physics-informed neural networks (SPINNs) are introduced as a mathematical framework that combines neural networks with stochastic calculus to approximate solutions of SDEs with Lévy noise.

Result: The paper presents a systematic framework for approximating SDE solutions using neural networks, though specific numerical results aren't mentioned in the abstract.

Conclusion: SPINNs offer a promising approach for solving stochastic differential equations with Lévy noise using neural networks, providing a systematic mathematical framework for this challenging problem.

Abstract: In this paper, we introduce the SPINNs (stochastic physics-informed neural networks) in a systematic manner. This provides a mathematical framework for approximating the solution of stochastic differential equations (SDEs) driven by Levy noise using artificial neural networks.

</details>


### [11] [An Additively Preconditioned Trust Region Strategy for Machine Learning](https://arxiv.org/abs/2512.14286)
*Samuel Cruz Alegría,Bindi Çapriqi,Shega Likaj,Ken Trotti,Rolf Krause*

Main category: math.NA

TL;DR: APTS: A nonlinearly preconditioned Trust-Region method using additive Schwarz corrections to accelerate convergence for large-scale nonconvex optimization in deep learning.


<details>
  <summary>Details</summary>
Motivation: Modern deep learning involves solving large-scale, highly nonconvex optimization problems with rough objective landscapes. Inspired by parallel preconditioners in Krylov methods for linear systems, the authors aim to develop a method that accelerates convergence while reducing hyperparameter tuning needs.

Method: APTS combines right-preconditioned additive Schwarz framework with classical Trust-Region algorithm. It decomposes parameter space into sub-domains, solves local nonlinear sub-problems in parallel, and assembles corrections additively.

Result: The method demonstrates fast convergence and largely eliminates the need for hyperparameter tuning due to the underlying Trust-Region strategy.

Conclusion: APTS provides an effective approach for large-scale nonconvex optimization in deep learning by leveraging parallel preconditioning techniques from linear algebra within a Trust-Region framework.

Abstract: Modern machine learning, especially the training of deep neural networks, depends on solving large-scale, highly nonconvex optimization problems, whose objective function exhibit a rough landscape. Motivated by the success of parallel preconditioners in the context of Krylov methods for large scale linear systems, we introduce a novel nonlinearly preconditioned Trust-Region method that makes use of an additive Schwarz correction at each minimization step, thereby accelerating convergence.
  More precisely, we propose a variant of the Additively Preconditioned Trust-Region Strategy (APTS), which combines a right-preconditioned additive Schwarz framework with a classical Trust-Region algorithm. By decomposing the parameter space into sub-domains, APTS solves local non-linear sub-problems in parallel and assembles their corrections additively. The resulting method not only shows fast convergence; due to the underlying Trust-Region strategy, it furthermore largely obviates the need for hyperparameter tuning.

</details>


### [12] [Separation-free exponential fitting with structured noise, with applications to inverse problems in parabolic PDEs](https://arxiv.org/abs/2512.14301)
*Rami Katz,Dmitry Batenkov,Giulia Giordano*

Main category: math.NA

TL;DR: Prony's method achieves super-exponential accuracy for recovering eigenvalues and amplitudes from noisy measurements when noise is structured as subsequent eigenvalues, even in separation-free regimes.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the extremely ill-conditioned inverse problem of recovering exponents and amplitudes of exponential sums from noisy measurements. While arbitrary noise makes recovery impossible, structured noise (using subsequent eigenvalues) surprisingly eliminates this ill-conditioning, enabling accurate recovery.

Method: The authors use Prony's method and leverage recent super-resolution theory to handle the "separation-free" regime where eigenvalues diverge to infinity. They prove theoretical error bounds and demonstrate numerical implementation.

Result: The paper shows that with structured noise, exponents and amplitudes can be recovered with super-exponential accuracy. The classical Prony's method achieves optimal error decay even in separation-free regimes, extending its applicability.

Conclusion: Structured noise transforms an extremely ill-conditioned problem into one with super-exponential accuracy. This enables practical applications like recovering unknown potentials in reaction-diffusion equations from discrete solution traces.

Abstract: We investigate the recovery of exponents and amplitudes of an exponential sum, where the exponents $\left\{λ_n \right\}_{n=1}^{N_1}$ are the first $N_1$ eigenvalues of a Sturm-Liouville operator, from finitely many measurements subject to measurement noise. This inverse problem is extremely ill-conditioned when the noise is arbitrary and unstructured. Surprisingly, however, the extreme ill-conditioning exhibited by this problem disappears when considering a \emph{structured} noise term, taken as an exponential sum with exponents given by the subsequent eigenvalues $\left\{λ_n \right\}_{n=N_1+1}^{N_1+N_2}$ of the Sturm-Liouville operator, multiplied by a noise magnitude parameter $\varepsilon>0$. In this case, we rigorously show that the exponents and amplitudes can be recovered with super-exponential accuracy: we both prove the theoretical result and show that it can be achieved numerically by a specific algorithm. By leveraging recent results on the mathematical theory of super-resolution, we show in this paper that the classical Prony's method attains the analytic optimal error decay also in the ``separation-free'' regime where $λ_n \to \infty$ as $n \to \infty$, thereby extending the applicability of Prony's method to new settings. As an application of our theoretical analysis, we show that the approximated eigenvalues obtained by our method can be used to recover an unknown potential in a linear reaction-diffusion equation from discrete solution traces.

</details>


### [13] [Reducing Training Complexity in Empirical Quadrature-Based Model Reduction via Structured Compression](https://arxiv.org/abs/2512.14416)
*Björn Liljegren-Sailer*

Main category: math.NA

TL;DR: A preprocessing method that compresses training data to reduce offline computational costs for complexity reduction in nonlinear model order reduction, achieving order-of-magnitude savings while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing offline training algorithms for complexity reduction techniques (like empirical quadrature and cell-based empirical cubature) are prohibitively expensive because they operate on raw snapshot data of all nonlinear integrands, which scales with both the number of snapshots and reduced model dimension.

Method: Introduces a preprocessing approach based on structured compression of training data that scales only with the number of collected snapshots, not with the reduced model dimension, enabling more efficient offline training.

Result: Achieves roughly an order-of-magnitude reduction in offline computational cost and memory requirements, enabling application to larger-scale problems while preserving accuracy as demonstrated through error analysis and numerical examples.

Conclusion: The proposed preprocessing method makes complexity reduction techniques for nonlinear model order reduction more computationally feasible for large-scale problems by significantly reducing offline costs without sacrificing accuracy.

Abstract: Model order reduction seeks to approximate large-scale dynamical systems by lower-dimensional reduced models. For linear systems, a small reduced dimension directly translates into low computational cost, ensuring online efficiency. This property does not generally hold for nonlinear systems, where an additional approximation of nonlinear terms -- known as complexity reduction -- is required. To achieve online efficiency, empirical quadrature and cell-based empirical cubature are among the most effective complexity reduction techniques. However, existing offline training algorithms can be prohibitively expensive because they operate on raw snapshot data of all nonlinear integrands associated with the reduced model. In this paper, we introduce a preprocessing approach based on a specific structured compression of the training data. Its key feature is that it scales only with the number of collected snapshots, rather than additionally with the reduced model dimension. Overall, this yields roughly an order-of-magnitude reduction in offline computational cost and memory requirements, thereby enabling the application of the complexity reduction methods to larger-scale problems. Accuracy is preserved, as indicated by our error analysis and demonstrated through numerical examples.

</details>


### [14] [Semi-robust equal-order hybridized discontinuous methods](https://arxiv.org/abs/2512.14419)
*Xiaoqi Ma,Jin Zhang*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces a unified analysis framework of equal-order hybridized discontinuous finite element (HDG) methods. The general framework covers standard HDG, embedded discontinuous finite element, and embedded-hybridized discontinuous finite element methods.

</details>


### [15] [Ensemble Parameter Estimation for the LPLSP Framework: A Rapid Approach to Reduced-Order Modeling for Transient Thermal Systems](https://arxiv.org/abs/2512.14467)
*Neelakantan Padmanabhan*

Main category: math.NA

TL;DR: Ensemble parameter estimation framework enables LPLSP method to generate reduced order thermal models from single transient dataset, eliminating need for multiple parametric simulations.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of earlier LPLSP implementations that required multiple parametric simulations to excite each heat source independently, which was computationally expensive and time-consuming.

Method: Developed ensemble parameter estimation framework with two strategies: rank-reduction and two-stage decomposition. Simultaneously identifies all model coefficients using fully transient excitations from a single dataset.

Result: ROMs achieve mean temperature-prediction errors within 5% of CFD simulations while reducing model-development times to O(10^0 s)-O(10^1 s). ROM evaluation of new transient conditions takes O(10^0 s).

Conclusion: The framework enables rapid thermal analysis and automated generation of digital twins for both simulated and physical systems by dramatically reducing computational costs and improving scalability.

Abstract: This work introduces an ensemble parameter estimation framework that enables the Lumped Parameter Linear Superposition (LPLSP) method to generate reduced order thermal models from a single transient dataset. Unlike earlier implementations that relied on multiple parametric simulations to excite each heat source independently, the proposed approach simultaneously identifies all model coefficients using fully transient excitations. Two estimation strategies namely rank-reduction and two-stage decomposition are developed to further reduce computational cost and improve scalability for larger systems. The proposed strategies yield ROMs with mean temperature-prediction errors within 5% of CFD simulations while reducing model-development times to O(10^0 s)-O(10^1 s). Once constructed, the ROM evaluates new transient operating conditions in O(10^0 s), enabling rapid thermal analysis and enabling automated generation of digital twins for both simulated and physical systems.

</details>


### [16] [On the constants in inverse trace inequalities for polynomials orthogonal to lower-order subspaces](https://arxiv.org/abs/2512.14570)
*Zhaonan Dong,Tanvi Wadhawan*

Main category: math.NA

TL;DR: Sharp explicit constants derived for inverse trace inequalities of polynomials orthogonal to lower-order subspaces on simplices.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for precise constants in inverse trace inequalities for polynomial functions on simplices, which are crucial for the hp-analysis of hybrid Galerkin methods like hybridizable discontinuous Galerkin and hybrid high-order methods.

Method: Uses orthogonal polynomial expansions on reference simplices and analyzes eigenvalues of relevant blocks of face mass matrices, following arguments from previous work [9].

Result: Derives sharp, explicit constants for inverse trace inequalities for polynomial functions in ℙₚ(T) orthogonal to lower-order subspace ℙₙ(T) (n ≤ p) on d-dimensional simplices.

Conclusion: The derived sharp constants are valuable for hp-analysis of hybrid Galerkin methods, providing precise tools for analyzing methods like hybridizable discontinuous Galerkin and hybrid high-order methods.

Abstract: We derive sharp, explicit constants in inverse trace inequalities for polynomial functions belonging to $\mathbb{P}_p(T)$ (polynomial space with total degree $p$) that are orthogonal to the lower-order subspace $\mathbb{P}_n(T)$, $n\leq p$, where $T$ denotes a $d$-dimensional simplex. The proofs rely on orthogonal polynomial expansions on reference simplices and on a careful analysis of the eigenvalues of the relevant blocks of the face mass matrices, following the arguments developed in [9]. These results are very useful in the $hp$-analysis of the hybrid Galerkin methods, e.g. hybridizable discontinuous Galerkin methods, hybrid high-order methods, etc.

</details>


### [17] [Inverse obstacle scattering regularized by the tangent-point energy](https://arxiv.org/abs/2512.14590)
*Henrik Schumacher,Jannik Rönsch,Thorsten Hohage,Max Wardetzky*

Main category: math.NA

TL;DR: Using tangent-point energy as Tikhonov regularizer for 3D inverse scattering problems to handle ill-conditioning and produce high-quality reconstructions.


<details>
  <summary>Details</summary>
Motivation: Inverse scattering problems in 3D are ill-conditioned and require effective regularization to produce stable and accurate reconstructions. The authors seek a regularization approach that can handle both self-avoidance (prevent surface self-intersection) and surface roughness while having good mathematical properties.

Method: The authors employ the tangent-point energy as a Tikhonov regularizer for 3D inverse scattering problems. This functional acts on embedded surfaces, providing self-avoidance properties while also penalizing surface roughness. They develop a reconstruction algorithm based on iteratively regularized Gauss-Newton method.

Result: The method demonstrates well-posedness of regularized problems and convergence of solutions to the true solution as noise vanishes. Numerical experiments show the approach is computationally feasible and produces reconstructions of unprecedented quality compared to existing methods.

Conclusion: The tangent-point energy is an effective Tikhonov regularizer for 3D inverse scattering problems, offering both mathematical guarantees (well-posedness, convergence) and practical performance (computational feasibility, high-quality reconstructions).

Abstract: We employ the so-called tangent-point energy as Tikhonov regularizer for ill-conditioned inverse scattering problems in 3D. The tangent-point energy is a self-avoiding functional on the space of embedded surfaces that also penalizes surface roughness. Moreover, it features nice compactness and continuity properties. These allow us to show the well-posedness of the regularized problems and the convergence of the regularized solutions to the true solution in the limit of vanishing noise level. We also provide a reconstruction algorithm of iteratively regularized Gauss-Newton type. Our numerical experiments demonstrate that our method is numerically feasible and effective in producing reconstructions of unprecedented quality.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [18] [On compressible fluid flows of Forchheimer-type in rotating heterogeneous porous media](https://arxiv.org/abs/2512.13959)
*Emine Celik,Luan Hoang,Thinh Kieu*

Main category: math.AP

TL;DR: The paper analyzes compressible fluid dynamics in rotating heterogeneous porous media with Forchheimer-type flow, establishing L^α and L^∞ estimates for solutions to a degenerate/singular pseudo-pressure equation.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of compressible fluids in rotating heterogeneous porous media, which is important for applications like geophysics and reservoir engineering, especially when dealing with Forchheimer-type flow (non-Darcy flow) and complex boundary conditions.

Method: Reduces governing equations to a nonlinear PDE for pseudo-pressure, develops new elliptic and parabolic Sobolev inequalities with multiple weights suitable for the equation's nonlinear structure, and proves trace theorems to handle mixed boundary conditions.

Result: Establishes L^α-estimates for solutions (for any positive α) in terms of initial/boundary data and angular speed, and obtains L^∞-estimates without requiring L^∞-norm conditions on weights or data.

Conclusion: The paper successfully develops analytical tools to handle degenerate/singular parabolic equations arising from compressible fluid flow in rotating porous media, providing rigorous estimates that don't require restrictive assumptions on data norms.

Abstract: We study the dynamics of compressible fluids in rotating heterogeneous porous media. The fluid flow is of {F}orchheimer-type and is subject to a mixed mass and volumetric flux boundary condition. The governing equations are reduced to a nonlinear partial differential equation for the pseudo-pressure. This parabolic-typed equation can be degenerate and/or singular in the spatial variables, the unknown and its gradient. We establish the $L^α$-estimate for the solutions, for any positive number $α$, in terms of the initial and boundary data and the angular speed of rotation. It requires new elliptic and parabolic Sobolev inequalities and trace theorem with multiple weights that are suitable to the nonlinear structure of the equation. The $L^\infty$-estimate is then obtained without imposing any conditions on the $L^\infty$-norms of the weights and the initial and boundary data.

</details>


### [19] [Existence, scaling, and spectral gap for traveling fronts in the 2D renormalized Allen--Cahn equation](https://arxiv.org/abs/2512.14245)
*Gideon Chiusole,Christian Kuehn*

Main category: math.AP

TL;DR: Construct monotone traveling wave fronts for renormalized stochastic Allen-Cahn equation in 2D, analyze their asymptotic profile and speed, and establish spectral gap growing linearly with renormalization constant.


<details>
  <summary>Details</summary>
Motivation: Study the deterministic skeleton of renormalized stochastic Allen-Cahn equation in 2D to understand traveling wave solutions and their stability properties in the small regularization parameter regime.

Method: Construct monotone traveling wave front solutions connecting renormalized equilibria, derive small-δ asymptotic descriptions of profile and speed, linearize about the wave, and analyze spectrum in weighted space.

Result: For sufficiently small δ>0, monotone traveling wave fronts exist, their asymptotic profile and speed are characterized, and spectral gap between symmetry-induced eigenvalue 0 and rest of spectrum grows linearly with renormalization constant as δ→0.

Conclusion: The deterministic skeleton of renormalized stochastic Allen-Cahn equation in 2D exhibits stable traveling wave fronts with quantifiable spectral properties that scale linearly with the renormalization parameter.

Abstract: We study the deterministic skeleton of the renormalized stochastic Allen--Cahn equation in spatial dimension $2$. For all sufficiently small regularization parameters $δ>0$, we construct monotone traveling wave front solutions connecting the renormalized equilibria, derive a small-$δ$ asymptotic description of their profile and speed, and identify the leading-order contributions. Linearizing about the wave and working in a naturally chosen weighted space, we show that there exists a spectral gap between the symmetry induced eigenvalue $0$ and the rest of the spectrum. The spectral gap grows linearly in the renormalization constant as $δ\downarrow 0$.

</details>


### [20] [Modeling of a non-Newtonian thin film passing a thin porous medium](https://arxiv.org/abs/2512.14275)
*María Anguiano,Francisco J. Suárez-Grau*

Main category: math.AP

TL;DR: The paper studies asymptotic behavior of non-Newtonian fluid flow coupling between a thin film and adjacent thin porous medium, identifying critical scaling regimes where effective flow is described by coupled 1D Darcy and Reynolds laws.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of fluid flow coupling between thin films and porous media with microstructure, particularly for non-Newtonian fluids, which is important for applications in filtration, lubrication, and porous media flows.

Method: Theoretical asymptotic analysis using small parameters: ε (pore size), h_ε (porous medium thickness), and η_ε (film thickness). Considers non-Newtonian Stokes equations with power law viscosity (flow index r). Derives critical scaling regime where h_ε ≈ η_ε^{(2r-1)/(r-1)} ε^{-r/(r-1)}.

Result: Identifies critical regime where the effective flow as ε→0 is described by a coupled system: 1D Darcy law (for porous medium) coupled with 1D Reynolds law (for thin film). This provides the asymptotic limit behavior for the fluid-porous system.

Conclusion: The study establishes rigorous asymptotic analysis for non-Newtonian fluid coupling between thin films and porous media, revealing critical scaling relationships that lead to simplified effective equations combining Darcy and Reynolds laws.

Abstract: This theoretical study deals with asymptotic behavior of a coupling between a thin film of fluid and an adjacent thin porous medium. We assume that the size of the microstructure of the porous medium is given by a small parameter $0<\ep\ll 1$, the thickness of the thin porous medium is defined by a parameter $0<h_\ep\ll 1$, and the thickness of the thin film is defined by a small parameter $0<η_\ep\ll 1$, where $h_\ep$ and $η_\ep$ are devoted to tend to zero when $\ep\to 0$. In this paper, we consider the case of a non-Newtonian fluid governed by the incompressible Stokes equations with power law viscosity of flow index $r\in (1, +\infty)$, and we prove that there exists a critical regime, which depends on $r$, between $\ep$, $η_\ep$ and $h_\ep$. More precisely, in this critical regime given by $h_\ep\approx η_\ep^{2r-1\over r-1}\ep^{-{r\over r-1}}$, we prove that the effective flow when $\ep\to 0$ is described by a 1D Darcy law coupled with a 1D Reynolds law.

</details>


### [21] [Asymptotic analysis of the Navier-Stokes equations in a thin domain with power law slip boundary conditions](https://arxiv.org/abs/2512.14303)
*María Anguiano,Francisco J. Suárez-Grau*

Main category: math.AP

TL;DR: This paper analyzes Navier-Stokes equations in 3D thin domains with anisotropic power law slip boundary conditions, identifying a critical exponent γ_s^*=3-2s that determines three different asymptotic limit behaviors as domain thickness ε→0.


<details>
  <summary>Details</summary>
Motivation: To understand how anisotropic power law slip boundary conditions (generalizing Navier slip conditions) influence fluid behavior in thin 3D domains, and to determine how different scaling exponents γ affect the asymptotic limit as domain thickness approaches zero.

Method: Theoretical asymptotic analysis of Navier-Stokes equations in thin domains with thickness ε, using power law slip boundary conditions with anisotropic tensor scaling as ε^(γ/s). Analysis examines different regimes based on parameter γ relative to critical value γ_s^*=3-2s.

Result: Identifies critical exponent γ_s^*=3-2s that separates three distinct asymptotic regimes: 1) γ=γ_s^* yields power law slip limit, 2) γ>γ_s^* yields perfect slip limit, 3) γ<γ_s^* yields no-slip limit. Shows how boundary condition scaling determines effective boundary behavior in thin domain limit.

Conclusion: The scaling exponent γ in anisotropic power law slip boundary conditions crucially determines the effective boundary condition in the thin domain limit, with a sharp transition at γ_s^*=3-2s between no-slip, power law slip, and perfect slip regimes.

Abstract: This theoretical study deals with the Navier-Stokes equations posed in a 3D thin domain with thickness $0<\ep\ll 1$, assuming power law slip boundary conditions, with an anisotropic tensor, on the bottom. This condition, introduced in (Djoko {\it et al.} {\it Comput. Math. Appl.} 128 (2022) 198--213), represents a generalization of the Navier slip boundary condition. The goal is to study the influence of the power law slip boundary conditions with an anisotropic tensor of order $\ep^{γ\over s}$, with $γ\in \mathbb{R}$ and flow index $1<s<2$, on the behavior of the fluid with thickness $\ep$ by using asymptotic analysis when $\ep\to 0$, depending on the values of $γ$. As a result, we deduce the existence of a critical value of $γ$ given by $γ_s^*=3-2s$ and so, three different limit boundary conditions are derived. The critical case $γ=γ_s^*$ corresponds to a limit condition of type power law slip. The supercritical case $γ>γ_s^*$ corresponds to a limit boundary condition of type perfect slip. The subcritical case $γ<γ_s^*$ corresponds to a limit boundary condition of type no-slip.

</details>


### [22] [Estimates for the distances between solutions to Kolmogorov equations with diffusion matrices of low regularity](https://arxiv.org/abs/2512.14362)
*Vladimir I. Bogachev,Stanislav V. Shaposhnikov*

Main category: math.AP

TL;DR: Estimates for weighted L¹-norm difference between probability solutions of Kolmogorov equations in terms of diffusion matrix and drift differences, without requiring Sobolev derivatives.


<details>
  <summary>Details</summary>
Motivation: Previous results for comparing solutions to Kolmogorov equations required Sobolev derivatives of solutions and coefficients. The authors aim to develop estimates that avoid these requirements, making the results more applicable to broader classes of solutions.

Method: The paper establishes estimates for the weighted L¹-norm difference between two probability solutions to Kolmogorov equations. The approach works with diffusion matrices that are non-singular, bounded, and satisfy the Dini mean oscillation condition. The estimates directly relate the solution difference to differences in diffusion matrices and drifts.

Result: The authors obtain explicit estimates that bound the weighted L¹-norm difference of solutions in terms of the differences between diffusion matrices and drifts. These estimates do not involve Sobolev derivatives, unlike previous results, and apply under the specified regularity conditions on the diffusion matrices.

Conclusion: The paper provides new quantitative stability estimates for Kolmogorov equations that are more general than previous results, as they don't require Sobolev regularity assumptions. This extends the applicability of comparison results to broader classes of solutions and coefficients.

Abstract: We obtain estimates for the weighted $L^1$-norm of the difference of two probability solutions to Kolmogorov equations in terms of the difference of the diffusion matrices and the drifts. Unlike the previously known results, our estimate does not involve Sobolev derivatives of solutions and coefficients. The diffusion matrices are supposed to be non-singular, bounded and satisfy the Dini mean oscillation condition.

</details>


### [23] [Homogenization of the random Neumann sieve problem under minimal assumptions on the size of the perforations](https://arxiv.org/abs/2512.14384)
*Mert Baştuğ*

Main category: math.AP

TL;DR: Random Neumann sieve problem for Poisson equation with clustered holes: determines optimal stochastic integrability for random radii to achieve homogenization.


<details>
  <summary>Details</summary>
Motivation: Study homogenization limits for Poisson equation with Neumann boundary conditions on randomly distributed perforations (sieve holes) that may cluster, which challenges standard homogenization theory.

Method: Analyzes Neumann sieve problem with holes distributed via stationary marked point process. Investigates limit behavior as hole density increases, focusing on conditions for stochastic homogenization despite clustering.

Result: Determines optimal stochastic integrability conditions for random hole radii that ensure homogenization occurs even with clustered hole distributions.

Conclusion: Identifies precise conditions on random hole sizes that guarantee stochastic homogenization for Poisson equation with Neumann boundary conditions on clustered perforations.

Abstract: We study the limit behavior of the solutions to the Neumann sieve problem for the Poisson equation when the sieve-holes are randomly distributed according to a stationary marked point process. We determine the optimal stochastic integrability for the random radii of the perforations for which stochastic homogenization takes place despite the presence of clustering holes.

</details>


### [24] [Qualitative properties of blowing-up solutions of nonlinear elliptic equations with critical Sobolev exponent](https://arxiv.org/abs/2512.14401)
*Minbo Yang,Shunneng Zhao*

Main category: math.AP

TL;DR: This paper studies critical elliptic equations with singular perturbation, analyzing eigenvalue estimates, Morse indices of bubble solutions, and asymptotic behavior under nondegeneracy conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the qualitative properties of solutions to critical elliptic equations with singular perturbations, particularly focusing on eigenvalue problems, Morse indices of single-bubble solutions, and asymptotic behavior under various conditions on the domain and perturbation function.

Method: Uses identities of Green's function derivatives, rescaled functions, blow-up analysis to estimate first (N+2)-eigenvalues and eigenfunctions. Analyzes eigenvalue problem of the elliptic equation, studies Morse index of single-bubble solutions, and examines asymptotic behavior under nondegeneracy conditions.

Result: Provides estimates on eigenvalues and eigenfunctions, proves Morse index of single-bubble solution is N+1 when Hessian matrix of Robin function is nondegenerate at blow-up point. Shows asymptotic behavior and nondegeneracy of solutions under mixture conditions involving both κ(x) matrix and Robin function.

Conclusion: The paper establishes important relationships between blow-up analysis, eigenvalue problems, and Morse indices in critical elliptic equations with perturbations, providing tools to understand solution behavior and stability properties in singularly perturbed settings.

Abstract: In this paper, we are concerned with the critical elliptic equation \begin{equation}\label{kx} \left\lbrace\begin{aligned}
  &-Δu=u^{p}+εκ(x)u^{q}\quad\hspace{2mm} \mbox{in}~~Ω,
  \\&u>0\quad \quad\quad\quad\quad\quad\quad\quad\hspace{1mm}\hspace{0.5mm}~\mbox{in}~~Ω
  \\&u=0\quad \quad\quad\quad\quad\quad\quad\quad\hspace{1mm}\hspace{0.5mm}~\mbox{on}~\partialΩ,
  \end{aligned} \right. \end{equation} where $Ω$ is a smooth bounded domain in $\mathbb{R}^N$ for $N\geq3$, $p=(N+2)/(N-2)$, $1<q<p$, $ε>0$ is a small parameter. If $κ(x)=1$, by applying the various identities of derivatives of Green's function and the rescaled functions, with blow-up analysis, we first provide a number of estimates on the first $(N+2)$-eigenvalues and their corresponding eigenfunctions, and prove the qualitative behavior of the eigenpairs $(λ_{i,ε}, v_{i,ε})$ to the eigenvalue problem of the elliptic equation \eqref{kx} for $i=1,\cdots,N+2$. As a consequence, we have that the Morse index of a single-bubble solution is $N+1$ if the Hessian matrix of the Robin function is nondegenerate at a blow-up point. Moreover, if $κ(x)\in C^2(\overlineΩ)$, we show that, for $ε>0$ small, the asymptotic behavior of the solutions and nondegeneracy of the solutions for the problem \eqref{kx} under a nondegeneracy condition on the blow-up point of a "mixture" of both the matrix $κ(x)$ and Robin function.

</details>


### [25] [Parabolic free boundary phase transition and mean curvature flow](https://arxiv.org/abs/2512.14437)
*Jingeon An,Kiichi Tashiro*

Main category: math.AP

TL;DR: The paper establishes a connection between parabolic Allen-Cahn equations and mean curvature flow, deriving a forced mean curvature flow satisfied by level surfaces of solutions to nonlinear parabolic equations, and shows convergence of parabolic free boundary Allen-Cahn equation to mean curvature flow.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between parabolic Allen-Cahn equations and mean curvature flow, providing a rigorous mathematical framework that connects these important geometric evolution equations and free boundary problems.

Method: Derives a forced mean curvature flow equation satisfied by level surfaces of solutions to ∂ₜu = Δu - f(u). Introduces the concept of inner gradient flow to unify parabolic free boundary problems. Analyzes the parabolic free boundary Allen-Cahn equation with specific boundary conditions and studies convergence properties as ε → 0.

Result: Shows that the forcing term ∂ᵥlog|∇u| converges to zero at an algebraic rate as ε → 0, uniformly in time. This implies that the parabolic free boundary Allen-Cahn equation converges to mean curvature flow uniformly in both ε and time in the C²,α sense.

Conclusion: The paper provides a rigorous connection between parabolic Allen-Cahn equations and mean curvature flow, establishing convergence results that justify viewing Allen-Cahn equations as diffused versions of mean curvature flow, with applications to free boundary problems.

Abstract: It is known that there is a strong relation between the parabolic Allen--Cahn equation and the mean curvature flow, in the sense that the parabolic Allen--Cahn equation can be considered as a ``diffused" mean curvature flow. In this work, we derive a forced mean curvature flow
  \[
  v=-H-\partial_ν\log |\nabla u|+f(u)/|\nabla u|,
  \]
  satisfied by level surfaces of any solution to the nonlinear parabolic equation
  \[
  \partial_tu=Δu-f(u).
  \]
  Moreover, we introduce the notion of the inner gradient flow, and unify parabolic free boundary problems in the gradient flow framework. Finally, we consider the parabolic free boundary Allen--Cahn equation
  \[
  \left\{
  \begin{alignedat}{2}
  \partial_tu&=Δu\quad&&\text{in}\quad\{|u|<1\}
  |\nabla u|&=1/ε\quad&&\text{on}\quad\partial\{|u|<1\},
  \end{alignedat}
  \right.
  \]
  and confirm that under reasonable assumptions, the $C^α$ norm of the forcing term $\partial_ν\log|\nabla u|$ converges to zero at an algebraic rate as $ε\rightarrow 0$, uniformly in time. This implies that the parabolic free boundary Allen--Cahn equation converges to the mean curvature flow, uniformly (in $ε$ and in time) in the $C^{2,α}$ sense.

</details>


### [26] [On Viscosity Solutions of Hamilton-Jacobi Equations in the Wasserstein space and the Vanishing Viscosity Limit](https://arxiv.org/abs/2512.14568)
*Giacomo Ceccherini Silberstein,Daniela Tonon*

Main category: math.AP

TL;DR: Develops unified framework for viscosity solutions to first-order and semilinear Hamilton-Jacobi equations with idiosyncratic operator, establishes vanishing-viscosity limit with optimal convergence rate.


<details>
  <summary>Details</summary>
Motivation: To create a unified mathematical framework for studying both first-order and semilinear Hamilton-Jacobi equations involving the idiosyncratic operator, and to extend vanishing-viscosity convergence results beyond classical control theory settings.

Method: Develops a unified viscosity solution framework, uses vanishing-viscosity limit analysis with optimal convergence rate, employs Hopf-Lax representation for existence theorems, and characterizes idiosyncratic operator action on geodesically convex functions.

Result: Establishes convergence of semilinear Hamilton-Jacobi equation solutions to corresponding first-order solutions as idiosyncratic noise vanishes, with optimal convergence rate. Provides existence theorems for first-order equations and characterization of idiosyncratic operator on convex functions.

Conclusion: The paper successfully creates a unified framework for viscosity solutions to Hamilton-Jacobi equations with idiosyncratic operator, extends vanishing-viscosity convergence beyond classical settings with optimal rate, and provides useful mathematical tools including existence results and operator characterizations.

Abstract: The aim of this article is twofold. First, we develop a unified framework for viscosity solutions to both first-order Hamilton-Jacobi equations and semilinear Hamilton-Jacobi equations driven by the idiosyncratic operator. Second, we establish a vanishing-viscosity limit-extending beyond the classical control-theoretic setting-for solutions of semilinear Hamilton-Jacobi equations, proving their convergence to the corresponding first-order solution as the idiosyncratic noise vanishes. Our approach provides an optimal convergence rate.
  We also present some results of independent interest. These include existence theorems for the first-order equation, obtained through an appropriate Hopf-Lax representation, and a useful description of the action of the idiosyncratic operator on geodesically convex functions.

</details>


### [27] [Computation and analysis of global solution curves for super-critical equations](https://arxiv.org/abs/2512.14577)
*Philip Korman,Dieter S. Schmidt*

Main category: math.AP

TL;DR: Study of Dirichlet problem on unit ball with supercritical nonlinearities, focusing on solution curves, ground states, singular solutions, and computational challenges.


<details>
  <summary>Details</summary>
Motivation: To understand analytical and computational aspects of supercritical elliptic equations on the unit ball, particularly focusing on the special role of Lin-Ni equation and overcoming computational difficulties for large parameter values.

Method: Analytical study of solution curves separated by ground state solutions, investigation of singular solutions, and development of computational techniques using Mathematica software to handle large parameter values and turning points.

Result: The Lin-Ni equation plays a special role in both ground state solutions and singular solutions. The paper develops new results on singular solutions and computational methods to overcome challenges in supercritical regimes.

Conclusion: The study provides insights into solution structure of supercritical elliptic equations, demonstrates special significance of Lin-Ni equation, and develops computational techniques for handling challenging parameter regimes.

Abstract: We study analytical and computational aspects for Dirichlet problem on the unit ball $B$: $|x|<1$ in $R^n$, modeled on the equation \[ Δu +λ\left(u^p+u^q \right)=0, \;\; \mbox{in $B$}, \;\; u=0 \s \mbox{on $\partial B$}, \] with a positive parameter $λ$, and $1<p<\frac{n+2}{n-2}<q$, where $\frac{n+2}{n-2}$ is the critical power. It turns out that a special role is played by the Lin-Ni equation [18], where $q=2p-1$ and $p>\frac{n}{n+2}$. This was already observed by I. Flores [6], who proved the existence of infinitely many ground state solutions. We study properties of infinitely many solution curves of this problem that are separated by these ground state solutions. We also study singular solutions (where $u(0)=\infty$), and again the Lin-Ni equation plays a special role. \medskip
  Super-critical equations are very challenging computationally: solutions exist only for very large $λ$, and curves of positive solutions make turns at very large values of $u(0)=||u||_{L^{\infty}}$. We overcome these difficulties by developing new results on singular solutions, and by using some delicate capabilities of {\em Mathematica} software.

</details>


### [28] [Proper solutions of the $1/H$-flow and the Green kernel of the $p$-Laplacian](https://arxiv.org/abs/2512.14591)
*Luca Benatti,Luciano Mari,Marco Rigoli,Alberto G. Setti,Kai Xu*

Main category: math.AP

TL;DR: Existence and optimal growth estimates for weak inverse mean curvature flow from a point on manifolds with specific curvature/isoperimetric conditions, plus new p-Laplacian Green kernel decay estimates and convergence of p-capacitary potentials to inverse mean curvature flow with outer obstacle.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for inverse mean curvature flow starting from a singular point on certain manifolds, addressing gaps in existing literature on p-Laplacian Green kernel estimates and connecting p-capacitary potentials to geometric flows.

Method: Develop new decay estimates for the Green kernel of the p-Laplacian to fix literature gaps, then apply these to prove existence and optimal growth estimates for weak inverse mean curvature flow from a point on manifolds with specific curvature and isoperimetric conditions.

Result: Proved existence and optimal growth estimates for weak inverse mean curvature flow issuing from a point on suitable manifolds, established new decay estimates for p-Laplacian Green kernels, and demonstrated convergence of p-capacitary potentials to inverse mean curvature flow with outer obstacle.

Conclusion: The paper provides rigorous existence and growth results for singular inverse mean curvature flows while contributing new analytical tools for p-Laplacian Green kernels and establishing connections between p-capacitary potentials and geometric flows.

Abstract: We show existence and optimal growth estimate for the weak inverse mean curvature flow issuing from a point, on manifolds with certain curvature and isoperimetric conditions. Some of the results are obtained by proving new decay estimates for the Green kernel of the $p$-Laplacian which fix a gap in the literature. Additionally, we address the convergence of $p$-capacitary potentials to the inverse mean curvature flow with outer obstacle.

</details>


### [29] [Existence and regularity for perturbed Stokes system with critical drift in 2D](https://arxiv.org/abs/2512.14627)
*Misha Chernobai,Tai-Peng Tsai*

Main category: math.AP

TL;DR: Extension of perturbed Stokes system analysis to 2D case with critical divergence-free drift, proving unique existence of q-weak solutions for large drift in weak L² space and W¹² solutions for L² drift.


<details>
  <summary>Details</summary>
Motivation: Extend previous work on perturbed Stokes systems from ℝⁿ (n≥3) to the two-dimensional case, addressing the unique challenges of 2D fluid dynamics with critical divergence-free drifts.

Method: Analyze perturbed Stokes system with critical divergence-free drift in bounded Lipschitz domains in ℝ², using weak L² space framework and establishing existence results through analytical methods similar to previous higher-dimensional work.

Result: Proved unique existence of q-weak solutions for force in L^q (q close to 2) with large drift in weak L² space, and unique existence of W¹² solutions for drift in L²(ℝ²) with arbitrarily large Lipschitz constant L.

Conclusion: Successfully extended perturbed Stokes system results to 2D, establishing well-posedness for critical divergence-free drifts in both weak L² and L² spaces, with analogous results also proven for scalar equations with similar drift conditions.

Abstract: We consider a perturbed Stokes system with critical divergence-free drift in a bounded Lipschitz domain in $R^2$, with sufficiently small Lipschitz constant L. It extends our previous work in $\Bbb R^n, n\ge 3$, to two-dimensional case. For large drift in weak $L^2$ space, we prove unique existence of q-weak solutions for force in $L^q$ with q close to 2. Moreover, for drift in $L^2(\Bbb R^2)$ we prove the unique existence of $W^{1,2}$ solutions for arbitrarily large L. Using similar methods we can also prove analogous results for scalar equations with divergence-free drifts in weak $L^2$ space.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [30] [Generative Monte Carlo Sampling for Constant-Cost Particle Transport](https://arxiv.org/abs/2512.13965)
*Joseph A. Farmer,Aidan Murray,Johannes Krotz,Ryan G. McClarren*

Main category: physics.comp-ph

TL;DR: GMC integrates generative AI into particle transport simulation by training neural networks with conditional flow matching to sample particle exit states directly, achieving constant computational cost per cell transmission and significant speedups in optically thick regimes.


<details>
  <summary>Details</summary>
Motivation: Traditional Monte Carlo methods for particle transport simulation scale linearly with optical thickness in diffusive limits, becoming computationally expensive in optically thick regimes. There's a need to align particle transport methods with modern computing architectures optimized for neural network inference.

Method: Reformulates cell-transmission as conditional generation task, trains neural networks using conditional flow matching to sample particle exit states (position, direction, path length) without simulating scattering histories. Uses optical coordinate scaling for material generalization.

Result: GMC preserves statistical fidelity of standard Monte Carlo with expected 1/√N convergence rate and accurate scalar flux profiles. Achieves constant O(1) cost per cell transmission vs. linear scaling with optical thickness, yielding order-of-magnitude speedups in optically thick regimes.

Conclusion: GMC provides a novel paradigm that strategically aligns particle transport with modern AI-optimized computing architectures, enabling transport codes to leverage ongoing advances in AI hardware and algorithms while maintaining Monte Carlo statistical fidelity.

Abstract: We present Generative Monte Carlo (GMC), a novel paradigm for particle transport simulation that integrates generative artificial intelligence directly into the stochastic solution of the linear Boltzmann equation. By reformulating the cell-transmission problem as a conditional generation task, we train neural networks using conditional flow matching to sample particle exit states, including position, direction, and path length, without simulating scattering histories. The method employs optical coordinate scaling, enabling a single trained model to generalize across any material. We validate GMC on two canonical benchmarks, namely a heterogeneous lattice problem characteristic of nuclear reactor cores and a linearized hohlraum geometry representative of high-energy density radiative transfer. Results demonstrate that GMC preserves the statistical fidelity of standard Monte Carlo, exhibiting the expected $1/\sqrt{N}$ convergence rate while maintaining accurate scalar flux profiles. While standard Monte Carlo computational cost scales linearly with optical thickness in the diffusive limit, GMC achieves constant $O(1)$ cost per cell transmission, yielding order-of-magnitude speedups in optically thick regimes. This framework strategically aligns particle transport with modern computing architectures optimized for neural network inference, positioning transport codes to leverage ongoing advances in AI hardware and algorithms.

</details>


### [31] [Physics-Informed Machine Learning for Two-Phase Moving-Interface and Stefan Problems](https://arxiv.org/abs/2512.14010)
*Che-Chia Chang,Te-Sheng Lin,Ming-Chih Lai*

Main category: physics.comp-ph

TL;DR: A physics-informed neural network framework solves two-phase Stefan problems by explicitly tracking the moving interface and enforcing temperature gradient discontinuities using two neural networks.


<details>
  <summary>Details</summary>
Motivation: The Stefan problem models phase-change processes but poses computational challenges due to moving interfaces and nonlinear temperature-phase coupling, requiring new computational approaches.

Method: Uses two neural networks: one for the moving interface and another for the temperature field. The interface network categorizes thermal diffusivity to select training points, while the temperature network uses a modified zero-level set function to capture gradient jumps across the interface.

Result: Numerical experiments demonstrate superior accuracy compared to other neural network methods, showing robust performance for phase-change problems and ability to capture unstable interface evolution like Mullins-Sekerka instability.

Conclusion: The proposed physics-informed neural network framework offers a robust and flexible alternative to traditional numerical methods for solving Stefan problems with moving boundaries.

Abstract: The Stefan problem is a classical free-boundary problem that models phase-change processes and poses computational challenges due to its moving interface and nonlinear temperature-phase coupling. In this work, we develop a physics-informed neural network framework for solving two-phase Stefan problems. The proposed method explicitly tracks the interface motion and enforces the discontinuity in the temperature gradient across the interface while maintaining global consistency of the temperature field. Our approach employs two neural networks: one representing the moving interface and the other for the temperature field. The interface network allows rapid categorization of thermal diffusivity in the spatial domain, which is a crucial step for selecting training points for the temperature network. The temperature network's input is augmented with a modified zero-level set function to accurately capture the jump in its normal derivative across the interface. Numerical experiments on two-phase dynamical Stefan problems demonstrate the superior accuracy and effectiveness of our proposed method compared with the ones obtained by other neural network methodology in literature. The results indicate that the proposed framework offers a robust and flexible alternative to traditional numerical methods for solving phase-change problems governed by moving boundaries. In addition, the proposed method can capture an unstable interface evolution associated with the Mullins-Sekerka instability.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [32] [Diagnosing symplecticity in simulations of high-dimensional Hamiltonian systems](https://arxiv.org/abs/2512.13951)
*William Barham,J. W. Burby*

Main category: physics.plasm-ph

TL;DR: Numerical diagnostic tool using Poincaré integral invariant to check symplecticity preservation in particle-in-cell codes reveals linear interpolation breaks symplecticity, requiring at least quadratic interpolation.


<details>
  <summary>Details</summary>
Motivation: To develop a computable diagnostic tool for monitoring symplecticity conservation in numerical integration of Hamiltonian systems, specifically for particle-in-cell (PIC) kinetic plasma simulations.

Method: Use integrals of the Liouville 1-form (first Poincaré integral invariant) approximated with spectral convergence to create a numerical diagnostic for checking symplecticity preservation in PIC codes.

Result: Symplectic electrostatic PIC methods fail to be symplectic when using linear shape functions for charge interpolation; at least quadratic interpolation is required for true symplecticity preservation.

Conclusion: The diagnostic tool successfully identifies that common linear interpolation in PIC methods breaks symplecticity, and higher-order (quadratic) interpolation is necessary for structure-preserving symplectic PIC methods.

Abstract: Integrals of the Liouville $1$-form, known as the first Poincaré integral invariant, provide a computable figure of merit for monitoring the conservation of symplecticity in the numerical integration of Hamiltonian systems. These integrals may be approximated with spectral convergence in the number of sample points, limited only by the regularity of the Hamiltonian. We devise a numerical integral invariant diagnostic for checking preservation of symplecticity in particle-in-cell (PIC) kinetic plasma simulation codes. As a first application of this diagnostic tool, we check the preservation of symplecticity in symplectic electrostatic particle-in-cell (PIC) methods. Surprisingly, such PIC methods fail to have symplectic time-advance maps if the charge is interpolated to the grid using linear shape functions, as is commonly done in practice. It is found that at least quadratic interpolation is needed for a structure-preserving PIC method to truly be symplectic.

</details>


### [33] [Influence of ion motion in a resonantly driven wakefield accelerator](https://arxiv.org/abs/2512.14476)
*Erwin Walter,John P. Farmer,Marlene Turner,Frank Jenko*

Main category: physics.plasm-ph

TL;DR: Ion motion in plasma wakefield acceleration suppresses wakefield excitation through two effects: loss of resonance between drive beam and plasma wave, and phase mixing due to transverse wavebreaking.


<details>
  <summary>Details</summary>
Motivation: Plasma wakefield acceleration schemes using driver trains rely on resonant excitation of plasma waves that must survive many periods. Ion motion can significantly impact beam-plasma interactions, especially for long beam self-modulation experiments.

Method: Simulations are used to study the impact of ion motion on the development of self-modulation of a long beam, directly applicable to recent experiments.

Result: Two distinct but related effects contribute to wakefield excitation suppression: loss of resonance between drive beam and excited plasma wave, and phase mixing due to transverse wavebreaking. Both effects follow the same scaling with ion mass.

Conclusion: Ion motion significantly affects plasma wakefield acceleration, with two previously unrecognized effects (resonance loss and phase mixing) both contributing to wakefield suppression, challenging previous understanding that only wavebreaking was important.

Abstract: Several different schemes for plasma wakefield acceleration using a train of drivers have been pursued, based on the resonant excitation of a plasma wave. Since these schemes rely on the plasma electron wave surviving for many periods, the motion of the plasma ions can have a significant impact on the beam--plasma interaction. In this work, simulations are used to study the impact of this ion motion on the development of the self-modulation of a long beam, directly applicable to recent experiments. It is shown that two related but distinct effects contribute to the suppression of the wakefield excitation: the loss of resonance between the drive beam and the plasma wave it excites, and phase mixing due to transverse wavebreaking. Although only the latter has previously been investigated, we show that the two effects follow the same scaling with ion mass.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [34] [Generalized relativistic second order magnetohydrodynamics: A correlation function approach using Zubarev's nonequilibrium statistical operator](https://arxiv.org/abs/2512.13824)
*Abhishek Tiwari,Binoy Krishna Patra*

Main category: physics.flu-dyn

TL;DR: The paper develops second-order relativistic magnetohydrodynamics using Zubarev's non-equilibrium statistical operator framework, incorporating energy-momentum conservation and magnetic-flux conservation for magnetized plasmas with parity and charge-conjugation symmetry.


<details>
  <summary>Details</summary>
Motivation: To construct a comprehensive framework for second-order relativistic magnetohydrodynamics that properly accounts for dissipative effects in magnetized plasmas while maintaining fundamental conservation laws and symmetries.

Method: Uses Zubarev's non-equilibrium statistical operator (NESO) framework, incorporating total energy-momentum conservation and Bianchi identity (magnetic-flux conservation). Focuses on relativistic magnetized plasmas preserving parity and charge-conjugation symmetry, and extends the formalism to include nonlocal contributions systematically.

Result: Derives all dissipative tensors for the medium and provides Kubo formulas for all transport coefficients that arise at second order. Successfully extends the NESO formalism to account for nonlocal contributions.

Conclusion: The paper establishes a complete theoretical framework for second-order relativistic magnetohydrodynamics in magnetized plasmas, providing both the dissipative structure and computational tools (Kubo formulas) for transport coefficients, with extensions to nonlocal effects.

Abstract: We use total energy-momentum conservation and the Bianchi identity (magnetic-flux conservation) to construct second-order relativistic magnetohydrodynamics in a Zubarev's non-equilibrium statistical operator (NESO) framework. We obtain all dissipative tensors in the medium by focusing on a relativistic magnetized plasma that preserves parity and is symmetric to charge-conjugation. We also provide Kubo formulas for all transport coefficients that arise at second order. Moreover, we extend the NESO formalism to systematically take into account for nonlocal contributions.

</details>


### [35] [Self-adaptive physics-informed neural network for forward and inverse problems in heterogeneous porous flow](https://arxiv.org/abs/2512.14610)
*Md. Abdul Aziz,Thilo Strauss,Muhammad Mohebujjaman,Taufiquar Khan*

Main category: physics.flu-dyn

TL;DR: Self-adaptive PINN framework for Darcy flow forward modeling and permeability inversion in heterogeneous porous media with region-aware parameterization and automatic loss balancing.


<details>
  <summary>Details</summary>
Motivation: Standard PINNs struggle with discontinuous permeability fields and require manual loss weight tuning, limiting their reliability for forward modeling and inverse problems in heterogeneous porous media.

Method: Region-aware permeability parameterization with binary spatial masks to preserve sharp jumps; self-learned loss weights for automatic balancing of PDE residuals, boundary constraints, and data mismatch; interleaved AdamW-L-BFGS optimization strategy.

Result: Accurate forward surrogates and reliable inverse permeability recovery for heterogeneous porous media with discontinuous permeability fields, eliminating smoothing artifacts and manual tuning requirements.

Conclusion: The framework establishes an effective mesh-free solver and data-driven inversion tool for porous-media systems governed by PDEs, with improved robustness and convergence stability.

Abstract: We develop a self-adaptive physics-informed neural network (PINN) framework that reliably solves forward Darcy flow and performs accurate permeability inversion in heterogeneous porous media. In the forward setting, the PINN predicts velocity and pressure for discontinuous, piecewise-constant permeability; in the inverse setting, it identifies spatially varying permeability directly from indirect flow observations. Both models use a region-aware permeability parameterization with binary spatial masks, which preserves sharp permeability jumps and avoids the smoothing artifacts common in standard PINNs. To stabilize training, we introduce self-learned loss weights that automatically balance PDE residuals, boundary constraints, and data mismatch, eliminating manual tuning and improving robustness, particularly for inverse problems. An interleaved AdamW-L-BFGS optimization strategy further accelerates and stabilizes convergence. Numerical results demonstrate accurate forward surrogates and reliable inverse permeability recovery, establishing the method as an effective mesh-free solver and data-driven inversion tool for porous-media systems governed by partial differential equations.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [36] [Optical Downlink Modeling for LEO and MEO Satellites under Atmospheric Turbulence with a Quantum State Tomography Use Case](https://arxiv.org/abs/2512.13828)
*Artur Czerwinski,Jakub J. Borkowski,Saeed Haddadi*

Main category: quant-ph

TL;DR: This paper analyzes link budgets for free-space optical systems between LEO/MEO satellites and ground stations, and proposes a quantum state tomography scheme for satellite-based quantum networks.


<details>
  <summary>Details</summary>
Motivation: To support the design and evaluation of space-based optical links by providing a comprehensive analysis of the link budget for satellite-to-ground optical communications, accounting for atmospheric effects and turbulence.

Method: Developed a detailed model of satellite-to-ground optical channel accounting for atmospheric absorption/scattering, free-space diffraction, and turbulence fluctuations. Introduced a general method for computing transmittance along slant paths incorporating zenith angle, slant range, and altitude-dependent attenuation.

Result: Provides numerical estimates of losses under typical operational conditions including aperture averaging effects. The framework serves as a critical tool for defining technical specifications in satellite communication demonstrators and simulations.

Conclusion: The proposed framework supports optical link design and evaluation. Additionally, introduces a satellite-based quantum use case for quantum state tomography to verify quantum resource quality for quantum information networks.

Abstract: This paper presents a comprehensive analysis of the link budget for free-space optical systems involving Low Earth Orbit (LEO) and Medium Earth Orbit (MEO) satellites. We develop a detailed model of the satellite-to-ground channel that accounts for the primary physical processes affecting transmittance: atmospheric absorption and scattering, free-space diffraction, and turbulence-induced fluctuations. The study introduces a general method for computing transmittance along a slant path between a satellite and an optical ground station, incorporating zenith angle, slant range, and altitude-dependent attenuation. The proposed framework is intended to support the design and evaluation of space-based optical links and serves as a critical tool for defining technical specifications in satellite communication demonstrators and simulations. Numerical estimates are provided to illustrate the magnitude of losses under typical operational conditions, including the role of aperture averaging. In addition to the link budget analysis, we introduce a satellite-based quantum use case. We propose a scheme for quantum state tomography performed on states generated by an onboard photon source on an LEO or MEO satellite and transmitted to the optical ground station. This approach enables continuous verification of the quality of quantum resources that can be used to perform quantum protocols within quantum information networks.

</details>


### [37] [Frozen Gaussian sampling algorithms for simulating Markovian open quantum systems in the semiclassical regime](https://arxiv.org/abs/2512.14015)
*Limin Xu,Zhen Huang,Zhennan Zhou*

Main category: quant-ph

TL;DR: Frozen Gaussian Sampling algorithm breaks computational barriers for simulating Markovian open quantum systems in semiclassical regime with ε-independent sampling error and eliminates boundary instabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional grid-based methods fail for simulating Markovian open quantum systems in semiclassical regime due to prohibitive resolution requirements from highly oscillatory dynamics and boundary-induced instabilities in long-time simulations.

Method: Introduces Frozen Gaussian Sampling (FGS) algorithm based on Wigner-Fokker-Planck phase-space formulation - a mesh-free method using Gaussian sampling in phase space.

Result: FGS achieves two transformative advantages: 1) sampling error independent of semiclassical parameter ε, breaking computational scaling barriers; 2) eliminates boundary-induced instabilities. Provides numerical evidence for steady states in strongly non-harmonic potentials.

Conclusion: FGS algorithm serves as powerful investigatory tool for exploring long-time behavior of open quantum systems, enabling study of regimes where analytical results are lacking, particularly steady states in strongly non-harmonic potentials.

Abstract: Simulating Markovian open quantum systems in the semiclassical regime poses a grand challenge for computational physics, as the highly oscillatory nature of the dynamics imposes prohibitive resolution requirements on traditional grid-based methods. To overcome this barrier, this paper introduces an efficient Frozen Gaussian Sampling (FGS) algorithm based on the Wigner-Fokker-Planck phase-space formulation. The proposed algorithm exhibits two transformative advantages. First, for the computation of physical observables, its sampling error is independent of the semiclassical parameter $\varepsilon$, thus fundamentally breaking the prohibitive computational scaling faced by grid methods in the semiclassical limit. Second, its mesh-free nature entirely eliminates the boundary-induced instabilities that constrain long-time grid-based simulations. Leveraging these capabilities, the FGS algorithm serves as a powerful investigatory tool for exploring the long-time behavior of open quantum systems. Specifically, we provide compelling numerical evidence for the existence of steady states in strongly non-harmonic potentials-a regime where rigorous analytical results are currently lacking.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [38] [Multimode Jahn-Teller Effect in Negatively Charged Nitrogen-Vacancy Center in Diamond](https://arxiv.org/abs/2512.14495)
*Jianhua Zhang,Jun Liu,Z. Z. Zhu,K. M. Ho,V. V. Dobrovitski,C. Z. Wang*

Main category: cond-mat.mtrl-sci

TL;DR: First-principles DFT study reveals multimode Jahn-Teller effect in NV- center excited state, identifying dominant vibrational modes that match phonon sideband observations and provide insights for quantum applications.


<details>
  <summary>Details</summary>
Motivation: To understand the multimode Jahn-Teller effect in negatively charged nitrogen-vacancy (NV) centers in their excited state, which is crucial for applications in quantum information, magnetometry, and sensing.

Method: First-principles calculations based on density functional theory (DFT), analysis of activation pathways of JT distortions, and comparison with ab initio molecular dynamics (AIMD) simulations and two-dimensional electronic spectroscopy (2DES) observations.

Result: Identified dominant vibrational modes in JT distortions that are closely related to phonon sideband observed in 2DES, consistent with AIMD simulations. Quantified contributions of different vibrational modes to the JT effect.

Conclusion: Provides new understanding of vibronic coupling origin and mechanism in NV centers, offering insights into dephasing, relaxation, and optically driven quantum effects critical for quantum applications.

Abstract: Multimode Jahn-Teller (JT) effect in a negatively charged nitrogen-vacancy (NV) center in its excited state is studied by first-principles calculations based on density function theory (DFT). The activation pathways of the JT distortions are analyzed to elucidate and quantify the contribution of different vibrational modes. The results show that the dominant vibrational modes in the JT distortions are closely related to the phonon sideband observed in two-dimensional electronic spectroscopy (2DES), consistent with ab initio molecular dynamics (AIMD) simulation results. Our calculations provide a new way to understand the origin and the mechanism of the vibronic coupling of the system. The obtained dominant vibrational modes coupled to the NV centre and their interactions with electronic states provides new insights into dephasing, relaxation and optically driven quantum effects, and are critical for the application to quantum information, magnetometry and sensing.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [39] [From STLS to Projection-based Dictionary Selection in Sparse Regression for System Identification](https://arxiv.org/abs/2512.14404)
*Hangjun Cho,Fabio V. G. Amaral,Andrei A. Klishin,Cassio M. Oishi,Steven L. Brunton*

Main category: stat.ML

TL;DR: Proposes score-guided library selection for SINDy algorithms to improve sparse regression in data-driven modeling of dynamical systems.


<details>
  <summary>Details</summary>
Motivation: To provide practical guidance for data-driven modeling, particularly for SINDy-type algorithms, by improving dictionary selection in sparse regression to enhance accuracy and interpretability in dynamical system identification.

Method: Revisits Sequential Threshold Least Squares (STLS) and proposes score-guided library selection that uses projected reconstruction errors (scores) and mutual coherence of dictionary terms to refine the dictionary. Includes theoretical analysis of score and dictionary-selection strategy in both original and weak SINDy regimes.

Result: Numerical experiments on ordinary and partial differential equations demonstrate that score-based screening improves both accuracy and interpretability in dynamical system identification.

Conclusion: Integrating score-guided methods to refine the dictionary can help SINDy users enhance robustness for data-driven discovery of governing equations in some cases.

Abstract: In this work, we revisit dictionary-based sparse regression, in particular, Sequential Threshold Least Squares (STLS), and propose a score-guided library selection to provide practical guidance for data-driven modeling, with emphasis on SINDy-type algorithms. STLS is an algorithm to solve the $\ell_0$ sparse least-squares problem, which relies on splitting to efficiently solve the least-squares portion while handling the sparse term via proximal methods. It produces coefficient vectors whose components depend on both the projected reconstruction errors, here referred to as the scores, and the mutual coherence of dictionary terms. The first contribution of this work is a theoretical analysis of the score and dictionary-selection strategy. This could be understood in both the original and weak SINDy regime. Second, numerical experiments on ordinary and partial differential equations highlight the effectiveness of score-based screening, improving both accuracy and interpretability in dynamical system identification. These results suggest that integrating score-guided methods to refine the dictionary more accurately may help SINDy users in some cases to enhance their robustness for data-driven discovery of governing equations.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [40] [Electrically tunable spin qubits in strain-engineered graphene p-n junctions](https://arxiv.org/abs/2512.14508)
*Myung-Chul Jung,Nojoon Myoung*

Main category: cond-mat.mes-hall

TL;DR: Strain engineering in graphene enables spin-qubit operation through Rashba spin-orbit coupling and Zeeman fields, with tunable quantum dots showing distinct spin-conserving and spin-flip avoided crossings for coherent spin manipulation.


<details>
  <summary>Details</summary>
Motivation: To extend strain-induced charge-qubit architectures in graphene by incorporating spin degrees of freedom, enabling spin-qubit operation in single-layer graphene while maintaining its intrinsic mobility and spin coherence.

Method: Using strain-induced nanobubbles in graphene p-n junctions to generate pseudo-magnetic fields that form double quantum dots with gate-tunable level hybridization. Analyzed through tight-binding quantum transport simulations and a four-band model.

Result: Two distinct avoided crossings observed: spin-conserving gaps at zero detuning and spin-flip gaps at finite detuning. Spin-flip gaps increase with SOC strength while spin-conserving gaps decrease. Time-domain simulations confirm detuning-dependent Rabi oscillations corresponding to these operational regimes.

Conclusion: Strain-induced confinement combined with tunable spin-orbit coupling provides a viable mechanism for coherent spin manipulation in pristine graphene, positioning strained single-layer graphene as a promising platform for scalable spin-based quantum technologies.

Abstract: Strain engineering enables quantum confinement in pristine graphene without degrading its intrinsic mobility and spin coherence. Here, we extend previously proposed strain-induced charge-qubit architectures by incorporating spin degrees of freedom through Rashba spin-orbit coupling (RSOC) and Zeeman fields, enabling spin-qubit operation in single-layer graphene (SLG). In a graphene p-n junction, a strain-induced nanobubble generates a pseudo-magnetic field that forms double quantum dots with gate-tunable level hybridization. Tight-binding quantum transport simulations and a four-band model reveal two distinct avoided crossings: spin-conserving gaps at zero detuning and spin-flip gaps at finite detuning, the latter increasing with SOC strength while the former decreases. Time-domain simulations confirm detuning-dependent Rabi oscillations corresponding to these two operational regimes. These results demonstrate that strain-induced confinement combined with tunable SOC provides a viable mechanism for coherent spin manipulation in pristine graphene, positioning strained SLG as a promising platform for scalable spin-based quantum technologies.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [41] [Macular: a multi-scale simulation platform for the retina and the primary visual system](https://arxiv.org/abs/2512.13052)
*Bruno Cessac,Erwan Demairy,Jérôme Emonet,Evgenia Kartsaki,Thibaud Kloczko,Côme Le Breton,Nicolas Niclausse,Selma Souihel,Jean-Luc Szpyrka,Julien Wintz*

Main category: q-bio.NC

TL;DR: Macular is a GUI-based simulation platform for creating in silico experiments of the retina and primary visual system, allowing users to build 3D neural structures, simulate with arbitrary video inputs, and modify parameters without programming.


<details>
  <summary>Details</summary>
Motivation: To enable neurobiologists and visual system modelers to test hypotheses in silico without requiring programming skills, allowing simulation of both natural and altered conditions (pharmacology, pathology, development).

Method: Provides a graphical interface for creating 3D structures with interconnected layers representing retinal/visual cortex cells. Users can use built-in cells/synapses or create custom ones by entering equations in text format (e.g., LaTeX). The platform automatically generates and compiles C++ code and creates simulation interfaces.

Result: Developed Macular platform with example scenarios including published retino-cortical models. The tool allows visualization of input videos, 3D structures, cell/synapse activity, and real-time parameter adjustment through the interface.

Conclusion: Macular provides an accessible simulation platform for visual system researchers to conduct in silico experiments without programming expertise, supporting both standard research scenarios and studies of altered physiological conditions.

Abstract: We developed Macular, a simulation platform with a graphical interface, designed to produce in silico experiment scenarios for the retina and the primary visual system. A scenario consists of generating a three-dimensional structure with interconnected layers, each layer corresponding to a type of 'cell' in the retina or visual cortex. The cells can correspond to neurons or more complex structures (such as cortical columns). The inputs are arbitrary videos. The user can use the cells and synapses provided with the software, or create their own using a graphical interface where they enter the constituent equations in text format (e.g., LaTeX). They also create the three-dimensional structure via the graphical interface. Macular then automatically generates and compiles the C++ code and generates the simulation interface. This allows the user to view the input video and the three-dimensional structure in layers. It also allows the user to select cells and synapses in each layer and view the activity of their state variables. Finally, the user can adjust the phenomenological parameters of the cells or synapses via the interface. We provide several example scenarios, corresponding to published articles, including an example of a retino-cortical model. Macular was designed for neurobiologists and modelers, specialists in the primary visual system, who want to test hypotheses in silico without the need for programming. By design, this tool allows natural or altered conditions (pharmacology, pathology, development) to be simulated.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [42] [Complete Characterizations of Well-Posedness in Parametric Composite Optimization](https://arxiv.org/abs/2512.14124)
*Boris S. Mordukhovich,Peipei Tang,Chengjing Wang*

Main category: math.OC

TL;DR: Complete characterization of well-posedness for KKT systems in perturbed composite optimization using parabolic regularity and second-order analysis.


<details>
  <summary>Details</summary>
Motivation: To establish a unified framework for analyzing stability and sensitivity of solutions to composite optimization problems, and to provide rigorous foundations for numerical algorithm design.

Method: Leverages parabolic regularity for composite models, introduces second-order variational functions, develops strong second-order sufficient condition (SSOSC), characterizes second-order qualification condition (SOQC), and analyzes Lipschitz-like/Aubin properties of KKT systems.

Result: Shows SSOSC extends classical second-order conditions, obtains equivalent characterizations of SOQC, proves equivalence between Lipschitz-like property and strong regularity under C²-cone reducibility, and establishes equivalence between Lipschitz-like property and generalized Jacobian nonsingularity under verifiable assumptions.

Conclusion: Provides comprehensive characterization of KKT system well-posedness, offering unified framework for stability analysis, sensitivity studies, and numerical algorithm justification in composite optimization.

Abstract: This paper provides complete characterization of well-posedness for Karush-Kuhn-Tucker (KKT) systems associated with general problems of perturbed composite optimization. Leveraging the property of parabolic regularity for composite models, we show that the second-order subderivative of the cost function reduces to the novel second-order variational function playing a crucial role in the subsequent analysis. This foundational result implies that the strong second-order sufficient condition (SSOSC) introduced in this work for the general class of composite optimization problems naturally extends the classical second-order sufficient condition in nonlinear programming. Then we obtain several equivalent characterizations of the second-order qualification condition (SOQC) and highlight its equivalence to the constraint nondegeneracy condition under the $\mathcal{C}^{2}$-cone reducibility assumption. These insights lead us to multiple equivalent conditions for the major Lipschitz-like/Aubin property of KKT systems, including the SOQC combined with the new second-order subdifferential condition and the SOQC combined with tilt stability of local minimizers. Furthermore, under $\mathcal{C}^{2}$-cone reducibility, we prove that the Lipschitz-like property of the reference KKT system is equivalent to its strong regularity. Finally, we demonstrate that the Lipschitz-like property is equivalent to the nonsingularity of the generalized Jacobian associated with the KKT system under a certain verifiable assumption. These results provide a unified and rigorous framework for analyzing stability and sensitivity of solutions to composite optimization problems, as well as for the design and justification of numerical algorithms.

</details>


### [43] [A preconditioned second-order convex splitting algorithm with extrapolation](https://arxiv.org/abs/2512.14468)
*Xinhua Shen,Hongpeng Sun*

Main category: math.OC

TL;DR: Extrapolation-enhanced preconditioned second-order convex splitting algorithms for nonconvex optimization, combining BDF2 with extrapolation for efficiency without heavy computation.


<details>
  <summary>Details</summary>
Motivation: Nonconvex optimization problems are prevalent in machine learning and data science, but existing methods may be computationally expensive or inefficient for these complex problems.

Method: Introduce extrapolation strategy into preconditioned second-order convex splitting algorithms, combining BDF2 with extrapolation method, using implicit-explicit scheme with preconditioned process to simplify subproblems.

Result: Algorithms efficiently solve nonconvex problems without significant computational overhead, with theoretical global convergence established using Kurdyka-Łojasiewicz properties. Numerical experiments on benchmark problems, SCAD-regularized least squares, and image segmentation show reduced solution times and competitive performance.

Conclusion: The proposed extrapolation-enhanced preconditioned second-order convex splitting algorithms are highly efficient for nonconvex optimization, achieving both theoretical convergence guarantees and practical performance improvements across various applications.

Abstract: Nonconvex optimization problems are widespread in modern machine learning and data science. We introduce an extrapolation strategy into a class of preconditioned second-order convex splitting algorithms for nonconvex optimization problems. The proposed algorithms combine second-order backward differentiation formulas (BDF2) with an extrapolation method. Meanwhile, the implicit-explicit scheme simplifies the subproblem through a preconditioned process. As a result, our approach solves nonconvex problems efficiently without significant computational overhead. Theoretical analysis establishes global convergence of the algorithms using Kurdyka-Łojasiewicz properties. Numerical experiments include a benchmark problem, the least squares problem with SCAD regularization, and an image segmentation problem. These results demonstrate that our algorithms are highly efficient, as they achieve reduced solution times and competitive performance.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [44] [Ancient Solutions to the Biharmonic Heat Equation](https://arxiv.org/abs/2512.13952)
*Alexander McWeeney*

Main category: math.DG

TL;DR: Ancient solutions to biharmonic heat equation on complete manifolds with polynomial growth are bounded by dimensions of biharmonic functions.


<details>
  <summary>Details</summary>
Motivation: Generalize Colding and Minicozzi's work on ancient caloric functions to the biharmonic heat equation setting.

Method: Analyze polynomially bounded ancient solutions to biharmonic heat equation on complete manifolds with polynomial volume growth.

Result: Space of polynomially bounded ancient solutions is bounded by dimensions of spaces of polynomially bounded biharmonic functions.

Conclusion: Successfully extends Colding-Minicozzi's caloric function results to biharmonic heat equation, establishing dimension bounds for ancient solutions.

Abstract: We show that the space of polynomially bounded ancient solutions to the biharmonic heat equation on a complete manifold with polynomial volume growth is bounded by the dimensions of spaces of polynomially bounded biharmonic functions. This generalizes the work of Colding and Minicozzi in [6] for ancient caloric functions.

</details>


### [45] [A parabolic flow for the large volume heterotic $G_2$ system](https://arxiv.org/abs/2512.14317)
*Mario Garcia-Fernandez,Andres J. Moreno,Alec Payne,Jeffrey Streets*

Main category: math.DG

TL;DR: The paper introduces a geometric flow for conformally coclosed G₂-structures that converges to torsion-free G₂-structures, solving a long-standing question in special holonomy theory.


<details>
  <summary>Details</summary>
Motivation: To address a folklore question in the special holonomy community about the existence of a well-posed flow for coclosed G₂-structures whose fixed points correspond to torsion-free G₂-structures (metrics with holonomy contained in G₂).

Method: Introduces a geometric flow of conformally coclosed G₂-structures that, after conformal rescaling, becomes a flow of coclosed G₂-structures coupled to a dilaton function flow. The flow is related to Grigorian's modified G₂ coflow.

Result: Establishes short-time existence, Shi-type smoothing properties, classification of fixed points, monotonicity formula for the G₂-dilaton functional, convergence of nonsingular solutions, and reveals connections to flows for SU(3)-structures through dimension reduction.

Conclusion: The paper provides an affirmative answer to the folklore question by constructing a well-posed flow for coclosed G₂-structures whose fixed points are precisely torsion-free G₂-structures, with applications in string theory and connections to other geometric flows.

Abstract: We introduce a geometric flow of conformally coclosed $G_2$-structures, whose fixed points are large volume solutions of the heterotic $G_2$ system, with vanishing scalar torsion class $τ_0 = 0$. After conformal rescaling, it becomes a flow of coclosed $G_2$-structures, related to Grigorian's modified $G_2$ coflow, which is coupled to a flow for a dilaton function. Our main results establish fundamental short-time existence and Shi-type smoothing properties of this flow, as well as a classification of its fixed points. By a classical rigidity result in the string theory literature, the fixed points on a compact manifold correspond to torsion-free $G_2$-structures, that is, to metrics with holonomy contained in $G_2$. Thus, we establish in the affirmative a folklore question in the special holonomy community, about the existence of a well-posed flow for coclosed $G_2$-structures with fixed points given by torsion-free $G_2$-structures. The flow also satisfies a monotonicity formula for the $G_2$-dilaton functional (volume scale in string theory), which allows us to strengthen the rigidity result with an alternative proof. The monotonicity of the $G_2$-dilaton functional, combined with the Shi-type estimates, leads to a general result on the convergence of nonsingular solutions. A dimension reduction analysis reveals an interesting link with natural flows for $SU(3)$-structures, previously introduced in the literature.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [46] [Co-simulation errors due to step size changes](https://arxiv.org/abs/2512.13845)
*Lars T. Kyllingstad*

Main category: cs.CE

TL;DR: In co-simulation with continuous-time units, state discrepancies from extrapolation errors can paradoxically increase when reducing macro time steps under certain conditions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses a counterintuitive phenomenon in continuous-time co-simulation where connected simulation units maintain internal states representing time integrals of shared variables. Normally, reducing macro time steps should decrease extrapolation errors between these states, but the authors identify conditions where this assumption fails.

Method: The authors analyze the mathematical behavior of extrapolation errors in co-simulation systems where simulation units have internal states representing time integrals of shared variables. They examine how these errors evolve when macro time step sizes are changed, particularly investigating the conditions under which reducing step sizes can paradoxically increase state discrepancies.

Result: The paper demonstrates that under certain circumstances, reducing macro time step sizes in continuous-time co-simulation can actually increase discrepancies between internal states of connected simulation units, contrary to the expected behavior where smaller steps should reduce extrapolation errors.

Conclusion: The findings reveal a non-intuitive behavior in co-simulation systems where step size reduction doesn't always guarantee error reduction, highlighting the need for careful consideration of step size adaptation strategies in co-simulation frameworks to avoid unexpected error amplification.

Abstract: When two simulation units in a continuous-time co-simulation are connected via some variable $q$, and both simulation units have an internal state which represents the time integral of $q$, there will generally be a discrepancy between those states due to extrapolation errors. Normally, such extrapolation errors diminish if the macro time step size is reduced. Here we show that, under certain circumstances, step size changes can cause such discrepancies to increase even when the change is towards smaller steps.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [47] [Traveling chimeras and collective coordination in beta-cell networks](https://arxiv.org/abs/2512.13984)
*Carine Simo,Venceslas Nguefoue Meli,Patrick Louodop,Samuel Bowong,Thierry Njougouo*

Main category: physics.bio-ph

TL;DR: Computational study reveals that pancreatic β-cells in a nonlocally coupled ring network exhibit synchronization, traveling waves, and traveling chimera states, providing insights into insulin secretion regulation and diabetes mechanisms.


<details>
  <summary>Details</summary>
Motivation: Pancreatic β-cells coordinate pulsatile insulin secretion through intercellular interactions, and disruptions in this coordination are implicated in type I and type II diabetes. Understanding collective dynamics could reveal mechanisms underlying these metabolic disorders.

Method: Employed computational framework with network of coupled β-cells in nonlocally coupled ring topology incorporating both electrical and metabolic coupling pathways to capture short- and long-range islet interactions.

Result: Numerical simulations revealed emergent behaviors including synchronization, traveling waves, and traveling chimera states (coherent and incoherent domains coexisting and propagating across network).

Conclusion: The work provides new insights into coordinated β-cell activity regulation and pulsatile insulin secretion, clarifying how coupling structure and intercellular communication shape islet-wide dynamics and contribute to understanding diabetes dysfunctions.

Abstract: Pancreatic $β$-cells play a central role in maintaining glucose homeostasis through the pulsatile secretion of insulin. This essential function relies not only on intracellular regulatory mechanisms but also on coordinated interactions among $β$-cells within the islets of Langerhans. Disruptions in this intercellular coordination are increasingly implicated in metabolic disorders such as type~I and type~II diabetes. In this work, we employ a computational framework to investigate the collective dynamics of a network of coupled $β$-cells interacting through a nonlocally coupled ring topology that incorporates both electrical and metabolic coupling pathways. This topology captures short- and long-range interactions known to shape islet communication. Numerical simulations reveal a variety of emergent behaviors, including synchronization, traveling waves, and traveling chimera states, in which coherent and incoherent domains coexist and propagate across the network. These findings provide new insight into the mechanisms governing coordinated $β$-cell activity and the regulation of pulsatile insulin secretion. By clarifying how coupling structure and intercellular communication shape islet-wide dynamics, this work contributes to a deeper understanding of the dysfunctions underlying diabetes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Physically consistent model learning for reaction-diffusion systems](https://arxiv.org/abs/2512.14240)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: This paper develops methods to learn physically consistent reaction-diffusion systems from data by enforcing mass conservation and quasipositivity constraints during model learning, ensuring well-posedness and alignment with fundamental physical laws.


<details>
  <summary>Details</summary>
Motivation: To create interpretable and reliable data-driven models for reaction-diffusion systems that inherently satisfy key physical properties like mass conservation and quasipositivity, ensuring the learned models are physically consistent and well-posed.

Method: Uses a regularization-based framework for structured model learning with systematic modifications to parameterized reaction terms to inherently satisfy mass conservation and quasipositivity. Extends theoretical results to reaction-diffusion systems with these physically consistent terms.

Result: Proves that solutions to the learning problem converge to a unique, regularization-minimizing solution even when conservation laws and quasipositivity are enforced. Provides approximation results for quasipositive functions essential for constructing physically consistent parameterizations.

Conclusion: The approach advances interpretable and reliable data-driven models for reaction-diffusion systems that align with fundamental physical laws by ensuring physical consistency and well-posedness through systematic constraint enforcement during learning.

Abstract: This paper addresses the problem of learning reaction-diffusion (RD) systems from data while ensuring physical consistency and well-posedness of the learned models. Building on a regularization-based framework for structured model learning, we focus on learning parameterized reaction terms and investigate how to incorporate key physical properties, such as mass conservation and quasipositivity, directly into the learning process. Our main contributions are twofold: First, we propose techniques to systematically modify a given class of parameterized reaction terms such that the resulting terms inherently satisfy mass conservation and quasipositivity, ensuring that the learned RD systems preserve non-negativity and adhere to physical principles. These modifications also guarantee well-posedness of the resulting PDEs under additional regularity and growth conditions. Second, we extend existing theoretical results on regularization-based model learning to RD systems using these physically consistent reaction terms. Specifically, we prove that solutions to the learning problem converge to a unique, regularization-minimizing solution of a limit system even when conservation laws and quasipositivity are enforced. In addition, we provide approximation results for quasipositive functions, essential for constructing physically consistent parameterizations. These results advance the development of interpretable and reliable data-driven models for RD systems that align with fundamental physical laws.

</details>


### [49] [Adaptive digital twins for predictive decision-making: Online Bayesian learning of transition dynamics](https://arxiv.org/abs/2512.13919)
*Eugenio Varetti,Matteo Torzoni,Marco Tezzele,Andrea Manzoni*

Main category: cs.LG

TL;DR: Adaptive digital twins using probabilistic graphical models with hierarchical Bayesian learning for enhanced value in civil engineering applications.


<details>
  <summary>Details</summary>
Motivation: To enhance value realization of digital twins in civil engineering by making them adaptive, improving personalization, robustness, and cost-effectiveness.

Method: Uses probabilistic graphical models (dynamic Bayesian networks) with state transition probabilities as random variables with conjugate priors for hierarchical online learning. Solves parametric Markov decision processes through reinforcement learning for dynamic policy computation.

Result: Provides mathematical framework for larger class of distributions than current literature. Enables effortless Bayesian updates for transition dynamics learning.

Conclusion: Proposed adaptive digital twin framework offers enhanced personalization, increased robustness, and improved cost-effectiveness, demonstrated on structural health monitoring and maintenance planning of a railway bridge.

Abstract: This work shows how adaptivity can enhance value realization of digital twins in civil engineering. We focus on adapting the state transition models within digital twins represented through probabilistic graphical models. The bi-directional interaction between the physical and virtual domains is modeled using dynamic Bayesian networks. By treating state transition probabilities as random variables endowed with conjugate priors, we enable hierarchical online learning of transition dynamics from a state to another through effortless Bayesian updates. We provide the mathematical framework to account for a larger class of distributions with respect to the current literature. To compute dynamic policies with precision updates we solve parametric Markov decision processes through reinforcement learning. The proposed adaptive digital twin framework enjoys enhanced personalization, increased robustness, and improved cost-effectiveness. We assess our approach on a case study involving structural health monitoring and maintenance planning of a railway bridge.

</details>


### [50] [Derivative-Informed Fourier Neural Operator: Universal Approximation and Applications to PDE-Constrained Optimization](https://arxiv.org/abs/2512.14086)
*Boyuan Yao,Dingcheng Luo,Lianghao Cao,Nikola Kovachki,Thomas O'Leary-Roseberry,Omar Ghattas*

Main category: cs.LG

TL;DR: DIFNOs are Fourier neural operators trained with derivative information for accurate PDE-constrained optimization, offering superior sample efficiency compared to conventional FNOs.


<details>
  <summary>Details</summary>
Motivation: Accurate surrogate-driven PDE-constrained optimization requires accurate surrogate Fréchet derivatives, which conventional FNOs may not provide. The paper aims to develop derivative-informed neural operators that can closely emulate both operator responses and sensitivities for more effective PDE-constrained optimization.

Method: Developed DIFNOs (derivative-informed Fourier neural operators) trained by minimizing prediction error jointly on output and Fréchet derivative samples. Established theoretical approximation results for FNOs and their derivatives. Created efficient training schemes using dimension reduction and multi-resolution techniques to reduce computational costs.

Result: Proved simultaneous universal approximation of FNOs and their Fréchet derivatives, and universal approximation in weighted Sobolev spaces. Numerical experiments on diffusion-reaction, Helmholtz, and Navier-Stokes equations show DIFNOs achieve superior sample complexity for operator learning and solving infinite-dimensional PDE-constrained inverse problems.

Conclusion: DIFNOs provide accurate derivative-informed operator learning and enable efficient solution of PDE-constrained optimization problems with high accuracy at low training sample sizes, making them superior to conventional FNOs for surrogate modeling in optimization contexts.

Abstract: We present approximation theories and efficient training methods for derivative-informed Fourier neural operators (DIFNOs) with applications to PDE-constrained optimization. A DIFNO is an FNO trained by minimizing its prediction error jointly on output and Fréchet derivative samples of a high-fidelity operator (e.g., a parametric PDE solution operator). As a result, a DIFNO can closely emulate not only the high-fidelity operator's response but also its sensitivities. To motivate the use of DIFNOs instead of conventional FNOs as surrogate models, we show that accurate surrogate-driven PDE-constrained optimization requires accurate surrogate Fréchet derivatives. Then, for continuously differentiable operators, we establish (i) simultaneous universal approximation of FNOs and their Fréchet derivatives on compact sets, and (ii) universal approximation of FNOs in weighted Sobolev spaces with input measures that have unbounded supports. Our theoretical results certify the capability of FNOs for accurate derivative-informed operator learning and accurate solution of PDE-constrained optimization. Furthermore, we develop efficient training schemes using dimension reduction and multi-resolution techniques that significantly reduce memory and computational costs for Fréchet derivative learning. Numerical examples on nonlinear diffusion--reaction, Helmholtz, and Navier--Stokes equations demonstrate that DIFNOs are superior in sample complexity for operator learning and solving infinite-dimensional PDE-constrained inverse problems, achieving high accuracy at low training sample sizes.

</details>


### [51] [Hybrid Iterative Solvers with Geometry-Aware Neural Preconditioners for Parametric PDEs](https://arxiv.org/abs/2512.14596)
*Youngkyu Lee,Francesc Levrero Florencio,Jay Pathak,George Em Karniadakis*

Main category: cs.LG

TL;DR: Geo-DeepONet is a geometry-aware deep operator network that enables accurate operator learning across arbitrary unstructured meshes without retraining, leading to robust hybrid preconditioned iterative solvers for parametric PDEs.


<details>
  <summary>Details</summary>
Motivation: Classical iterative solvers for parametric PDEs are highly sensitive to domain geometry and discretization, and previous neural operator hybrids under-perform on unseen geometries.

Method: Develop Geo-DeepONet that incorporates domain information from finite element discretizations, then couple it with traditional methods (relaxation schemes, Krylov subspace algorithms) to create geometry-aware hybrid preconditioned iterative solvers.

Result: Numerical experiments on parametric PDEs over diverse unstructured domains demonstrate enhanced robustness and efficiency of the proposed hybrid solvers for multiple real-world applications.

Conclusion: Geo-DeepONet enables geometry-aware operator learning across arbitrary meshes without retraining, and when combined with classical solvers, creates robust hybrid methods that overcome limitations of both purely classical and previous neural operator approaches.

Abstract: The convergence behavior of classical iterative solvers for parametric partial differential equations (PDEs) is often highly sensitive to the domain and specific discretization of PDEs. Previously, we introduced hybrid solvers by combining the classical solvers with neural operators for a specific geometry 1, but they tend to under-perform in geometries not encountered during training. To address this challenge, we introduce Geo-DeepONet, a geometry-aware deep operator network that incorporates domain information extracted from finite element discretizations. Geo-DeepONet enables accurate operator learning across arbitrary unstructured meshes without requiring retraining. Building on this, we develop a class of geometry-aware hybrid preconditioned iterative solvers by coupling Geo-DeepONet with traditional methods such as relaxation schemes and Krylov subspace algorithms. Through numerical experiments on parametric PDEs posed over diverse unstructured domains, we demonstrate the enhanced robustness and efficiency of the proposed hybrid solvers for multiple real-world applications.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [52] [Eigenvalue asymptotics for strong $δ$-interactions supported on curves with corners](https://arxiv.org/abs/2512.14393)
*Badreddine Benhellal,Noah Körner,Konstantin Pankrashkin*

Main category: math.SP

TL;DR: Study of eigenvalue asymptotics for 2D Schrödinger operator with singular delta potential on piecewise smooth closed curves with corners as coupling parameter α→∞.


<details>
  <summary>Details</summary>
Motivation: To understand how corners affect eigenvalue behavior in singular Schrödinger operators, particularly distinguishing between eigenvalues influenced by corner geometry versus those behaving like smooth curves.

Method: Analyze two-dimensional Schrödinger operator -Δ-αδ_Γ with Dirac δ-distribution supported by piecewise smooth closed curve Γ, study asymptotic behavior of individual eigenvalues as α→∞ using spectral analysis techniques.

Result: First few eigenvalues determined by corner opening angles only, while other eigenvalues have same leading asymptotics as smooth curves. With additional corner assumptions (no acute corners), detailed asymptotics established via one-dimensional effective operator on boundary.

Conclusion: Corner geometry significantly influences leading eigenvalue asymptotics, with acute corners requiring special treatment; effective one-dimensional operator captures detailed asymptotic behavior for non-acute corners.

Abstract: Let $Γ\subset\mathbb{R}^2$ be a piecewise smooth closed curve with corners. We discuss the asymptotic behavior of the individual eigenvalues of the two-dimensional Schrödinger operator $-Δ-αδ_Γ$ for $α\to\infty$, where $δ_Γ$ is the Dirac $δ$-distribution supported by $Γ$. It is shown that the asymptotics of several first eigenvalues is determined by the corner opening only, while the main term in the asymptotic expansion for the other eigenvalues is the same as for smooth curves. Under an additional assumption on the corners of $Γ$ (which is satisfied, in particular, if $Γ$ has no acute corners), a more detailed eigenvalue asymptotics is established in terms of a one-dimensional effective operator on the boundary.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [53] [Causal character of imaginary Killing spinors and spinorial slicings](https://arxiv.org/abs/2512.14569)
*Sven Hirsch,Yiyue Zhang*

Main category: gr-qc

TL;DR: Characterizes asymptotically AdS spin initial data sets saturating BPS bound, establishing dimension thresholds for gravitational waves and rotating black holes, with key results on imaginary Killing spinors and codimension-2 slicing construction.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which asymptotically Anti-de Sitter (AdS) spin initial data sets saturate the Bogomol'nyi-Prasad-Sommerfield (BPS) bound, particularly for gravitational waves and rotating black holes in higher dimensions, and to establish precise dimension thresholds for these phenomena.

Method: Uses spinor techniques to analyze asymptotically AdS initial data sets, develops a theorem for replacing imaginary Killing spinors of mixed causal type with strictly timelike or null ones, and constructs codimension-2 slicing analogous to minimal surface methods.

Result: Establishes sharp dimension thresholds for gravitational waves and rotating black holes saturating BPS bound in asymptotically AdS spacetimes, provides criterion for replacing mixed causal type Killing spinors, and demonstrates construction of codimension-2 slicing using spinors.

Conclusion: Spinor methods provide powerful tools for characterizing BPS-saturating initial data in asymptotically AdS spacetimes, with dimension thresholds revealing fundamental constraints on gravitational waves and rotating black holes, while the codimension-2 slicing construction extends geometric techniques to spinor-based approaches.

Abstract: We characterize spin initial data sets that saturate the BPS bound in the asymptotically AdS setting. This includes both gravitational waves and rotating black holes in higher dimensions, and we establish a sharp dimension threshold in each case. A key ingredient in our argument is a theorem providing a general criterion for when an imaginary Killing spinor of mixed causal type can be replaced by one that is strictly timelike or null. Moreover, in analogy with the minimal surface method, we demonstrate that spinors can be used to construct a codimension-$2$ slicing.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [54] [Five lectures on regularity structures and SPDEs](https://arxiv.org/abs/2512.14264)
*I. Bailleul*

Main category: math.PR

TL;DR: Introduction to regularity structures for studying singular stochastic PDEs through five lectures with two appendices providing technical details and contextual background.


<details>
  <summary>Details</summary>
Motivation: To provide an accessible introduction to the mathematical framework of regularity structures, which is essential for analyzing and solving singular stochastic partial differential equations that arise in various fields of mathematics and physics.

Method: Five structured lectures covering the fundamentals of regularity structures, supplemented by two appendices containing technical results and deeper contextual information to support the main content.

Result: A comprehensive educational resource that introduces the theory of regularity structures and demonstrates their application to singular stochastic PDEs, making this advanced mathematical framework more accessible to learners.

Conclusion: The lectures successfully provide an introductory pathway into the complex field of regularity structures, offering both foundational knowledge and technical depth needed for working with singular stochastic PDEs.

Abstract: This set of five lectures provides an introduction to regularity structures and their use for the study of singular stochastic partial differential equations. Two appendices provide some additional informations that enter in the main text either as some technical results or as some results that deepen the context within which we set these lectures.

</details>
