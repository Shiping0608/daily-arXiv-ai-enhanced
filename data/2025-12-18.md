<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 20]
- [math.AP](#math.AP) [Total: 29]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [math.DG](#math.DG) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [math.FA](#math.FA) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [math.PR](#math.PR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [math.DS](#math.DS) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Accuracy of the Yee FDTD Scheme for Normal Incidence of Plane Waves on Dielectric and Magnetic Interfaces](https://arxiv.org/abs/2512.14863)
*Pavel A. Makarov,Vladimir I. Shcheglov*

Main category: math.NA

TL;DR: Analyzes accuracy of Yee FDTD scheme for simulating plane wave incidence on planar interfaces between lossless media, quantifying errors from staggered-grid discretization of material discontinuities.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify systematic errors in standard Yee FDTD simulations when modeling wave incidence on material interfaces, particularly due to how staggered grids discretize material discontinuities over transition layers.

Method: Analyzes two common FDTD interface models with different staggered-grid placements of material parameters. Derives discrete analogs of Fresnel coefficients by formulating effective boundary conditions from Yee update equations. Uses transition-layer model to quantify errors and examines role of Courant number.

Result: Reveals that staggered grid implicitly spreads material discontinuity over one spatial step transition layer, causing systematic deviations from exact theory. Provides qualitative criteria predicting deviation direction/nature and rigorous error estimates for weak and strong impedance contrasts. Shows Courant number modulates errors and conditions where numerical dispersion and interface discretization jointly influence accuracy.

Conclusion: The Yee FDTD scheme introduces systematic interface modeling errors due to staggered-grid discretization, which can be quantified and predicted. Understanding these errors is crucial for accurate simulation of wave interactions with material interfaces, with Courant number playing a modulating role.

Abstract: This paper analyzes the accuracy of the standard Yee finite-difference time-domain (FDTD) scheme for simulating normal incidence of harmonic plane waves on planar interfaces between lossless, linear, homogeneous, isotropic media. We consider two common FDTD interface models based on different staggered-grid placements of material parameters. For each, we derive discrete analogs of the Fresnel reflection and transmission coefficients by formulating effective boundary conditions that emerge from the Yee update equations. A key insight is that the staggered grid implicitly spreads the material discontinuity over a transition layer of one spatial step, leading to systematic deviations from exact theory. We quantify these errors via a transition-layer model and provide (i) qualitative criteria predicting the direction and nature of deviations, and (ii) rigorous error estimates for both weak and strong impedance contrasts. Finally, we examine the role of the Courant number in modulating these errors, revealing conditions under which numerical dispersion and interface discretization jointly influence accuracy.

</details>


### [2] [Extending the explicit constraint force method to inverse problems](https://arxiv.org/abs/2512.14877)
*Conor Rowan*

Main category: math.NA

TL;DR: The paper extends the Explicit Constraint Force Method (ECFM) from solution reconstruction to inverse problems, showing it can handle dynamic systems, noisy data, stochastic models, and even boundary/geometry recovery.


<details>
  <summary>Details</summary>
Motivation: ECFM was originally developed for solution reconstruction with missing physics, but inverse problems have similar mathematical formulations. The authors want to explore whether ECFM can serve as an alternative strategy for inverse analysis across various challenging scenarios.

Method: Extends ECFM framework to inverse problems through: 1) Comparison with standard approaches using numerical examples, 2) Extension to dynamic problems, 3) Novel approach for noisy measurement data, 4) Combination with polynomial chaos expansion for stochastic models, 5) Application to boundary condition and domain geometry recovery.

Result: ECFM proves viable for inverse problems, successfully handling dynamic systems, noisy data, stochastic components, and recovering missing boundary conditions and domain geometries from measurement data.

Conclusion: ECFM provides a principled mathematical framework that can be effectively extended to solve various inverse problems, offering an alternative strategy for inverse analysis with demonstrated capabilities across multiple challenging scenarios.

Abstract: Recently, the explicit constraint force method (ECFM) was introduced as a principled approach to solution reconstruction in the presence of missing physics. In solution reconstruction, parameters of a physical model are estimated from sparse measurement data as a means to obtain the full solution field. In contrast, inverse problems target the missing parameters and estimate the solution along the way. Noting the similarity of the mathematical formulations of these two tasks, we investigate the use of ECFM to solve inverse problems. First, we compare the ECFM formulation of the inverse problem to a standard approach using two numerical examples. The first example provides an extension of ECFM to dynamic problems, and the second offers a novel approach to treat noisy measurement data. Next, we introduce a method to solve inverse problems for which the parameterized model has stochastic components. This approach is based on constraint forces and the polynomial chaos expansion, and is illustrated with another numerical example. Finally, we discuss extensions of ECFM to recover missing boundary conditions and domain geometries from measurement data, which are shown to be special cases of problems treated previously in the literature. The purpose of this work is to extend the mathematical framework of ECFM to novel applications and to gauge the method's viability as an alternative strategy for inverse analysis.

</details>


### [3] [Optimization of gridding algorithms for FFT by vector optimization](https://arxiv.org/abs/2512.14914)
*Federico Achini,Paola Causin,Sara Vanini,Ke Chen,Simone Scacchi*

Main category: math.NA

TL;DR: This paper proposes a new framework for designing optimal gridding kernels for FFT applications using vector optimization theory, outperforming traditional PSWF and state-of-the-art methods in specific regions with orders-of-magnitude error improvements.


<details>
  <summary>Details</summary>
Motivation: FFT requires gridding for non-uniformly sampled data, and current methods rely on PSWF as the optimal kernel. However, there's a need for a more rigorous framework to define kernel optimality and design kernels tailored to specific application requirements and error profiles.

Method: The paper redefines kernel optimality through vector optimization theory, characterizing optimal kernels as Pareto-efficient solutions of an error shape operator. It establishes mathematical properties (continuity, existence), proposes a methodology to construct kernels for desired target error functions, and implements this via interior-point optimization.

Result: The proposed kernels outperform both traditional PSWF and state-of-the-art MIRT-NUFFT methods in specific regions of interest, achieving orders-of-magnitude improvements in mean absolute errors. The VO-based approach provides customized accuracy profiles aligned with application requirements.

Conclusion: Vector optimization provides a powerful framework for designing tailored gridding kernels with superior performance. Future work includes extending to multidimensional cases, relative error minimization, and potentially integrating machine learning for adaptive target error selection.

Abstract: The Fast Fourier Transform (FFT) is widely used in applications such as MRI, CT, and interferometry; however, because of its dependence on uniformly sampled data, it requires the use of gridding techniques for practical implementation. The performance of these algorithms strongly depends on the choice of the gridding kernel, with the first prolate spheroidal wave function (PSWF) regarded as optimal. This work redefines kernel optimality through the lens of vector optimization (VO), introducing a rigorous framework that characterizes optimal kernels as Pareto-efficient solutions of an error shape operator. We establish the continuity of such operator, study the existence of solutions, and propose a novel methodology to construct kernels tailored to a desired target error function. The approach is implemented numerically via interior-point optimization. Comparative experiments demonstrate that the proposed kernels outperform both the PSWF and the state-of-the-art methods (MIRT-NUFFT) in specific regions of interest, achieving orders-of-magnitude improvements in mean absolute errors. These results confirm the potential of VO-based kernel design to provide customized accuracy profiles aligned with application-specific requirements. Future research will extend this framework to multidimensional cases and relative error minimization, with potential integration of machine learning for adaptive target error selection.

</details>


### [4] [Boundary condition enforcement with PINNs: a comparative study and verification on 3D geometries](https://arxiv.org/abs/2512.14941)
*Conor Rowan,Kai Hampleman,Kurt Maute,Alireza Doostan*

Main category: math.NA

TL;DR: Systematic comparison of boundary condition enforcement techniques for PINNs on complex 3D geometries, proposing a general framework and verifying on linear/nonlinear test problems.


<details>
  <summary>Details</summary>
Motivation: PINNs lack comprehensive studies on complex 3D geometries due to BC enforcement challenges, with limited side-by-side comparisons of existing techniques and their efficacy on geometrically complex problems.

Method: Systematic comparison of BC enforcement techniques for PINNs, proposing a general solution framework for arbitrary 3D geometries that is agnostic to PDE, geometry, and BC types, requiring minimal hyperparameter tuning.

Result: Verification of methodology on 3D linear and nonlinear test problems with combinations of Dirichlet, Neumann, and Robin boundaries, demonstrating capability to handle complex geometries.

Conclusion: This work advances PINNs toward becoming a mature numerical method competitive with established techniques like finite element method, addressing key BC enforcement challenges in complex 3D domains.

Abstract: Since their advent nearly a decade ago, physics-informed neural networks (PINNs) have been studied extensively as a novel technique for solving forward and inverse problems in physics and engineering. The neural network discretization of the solution field is naturally adaptive and avoids meshing the computational domain, which can both improve the accuracy of the numerical solution and streamline implementation. However, there have been limited studies of PINNs on complex three-dimensional geometries, as the lack of mesh and the reliance on the strong form of the partial differential equation (PDE) make boundary condition (BC) enforcement challenging. Techniques to enforce BCs with PINNs have proliferated in the literature, but a comprehensive side-by-side comparison of these techniques and a study of their efficacy on geometrically complex three-dimensional test problems are lacking. In this work, we i) systematically compare BC enforcement techniques for PINNs, ii) propose a general solution framework for arbitrary three-dimensional geometries, and iii) verify the methodology on three-dimensional, linear and nonlinear test problems with combinations of Dirichlet, Neumann, and Robin boundaries. Our approach is agnostic to the underlying PDE, the geometry of the computational domain, and the nature of the BCs, while requiring minimal hyperparameter tuning. This work represents a step in the direction of establishing PINNs as a mature numerical method, capable of competing head-to-head with incumbents such as the finite element method.

</details>


### [5] [Dynamical Tensor Train Approximation for Kinetic Equations](https://arxiv.org/abs/2512.14950)
*Geshuo Wang,Jingwei Hu*

Main category: math.NA

TL;DR: Dynamical low-rank method using tensor-train format for kinetic equations reduces computational cost by exploiting low-rank structure in velocity space.


<details>
  <summary>Details</summary>
Motivation: Kinetic equations are computationally challenging due to high-dimensional phase space (position + velocity). Traditional methods suffer from the curse of dimensionality, requiring efficient approaches to handle the velocity variable's three-dimensional structure.

Method: Develops a dynamical low-rank method based on projector-splitting integrator in tensor-train (TT) format. Treats spatial variable as parameter while discretizing 3D velocity using tensor trains. Uses sweeping procedure to update tensor cores instead of standard step-and-truncate, enabling smaller TT-ranks.

Result: Method achieves substantial reductions in memory usage and computational cost compared to traditional approaches. Demonstrates effectiveness on several representative kinetic equations.

Conclusion: The tensor-train based dynamical low-rank method provides an efficient framework for solving kinetic equations by exploiting low-rank structure in velocity space, overcoming computational challenges of high-dimensional phase space.

Abstract: The numerical solution of kinetic equations is challenging due to the high dimensionality of the underlying phase space. In this paper, we develop a dynamical low-rank method based on the projector-splitting integrator in tensor-train (TT) format. The key idea is to discretize the three-dimensional velocity variable using tensor trains while treating the spatial variable as a parameter, thereby exploiting the low-rank structure of the distribution function in velocity space. In contrast to the standard step-and-truncate approach, this method updates the tensor cores through a sweeping procedure, allowing the use of relatively small TT-ranks and leading to substantial reductions in memory usage and computational cost. We demonstrate the effectiveness of the proposed approach on several representative kinetic equations.

</details>


### [6] [Determinant-Based Error Bounds for CUR Matrix Approximation: Oversampling and Volume Sampling](https://arxiv.org/abs/2512.15102)
*Frank de Hoog,Markus Hegland*

Main category: math.NA

TL;DR: CUR matrix approximation error bounds using determinant methods, connecting local projection errors to global quality with probabilistic volume sampling framework.


<details>
  <summary>Details</summary>
Motivation: To establish theoretical foundations for CUR matrix approximation by connecting local projection errors to global approximation quality, providing geometric insight into approximation degradation and quantifying benefits of oversampling.

Method: Determinant-based methods with bordered Gramian matrix identities, probabilistic framework using volume sampling, and interpolation-type error bounds analysis for both CUR decomposition and Nyström method.

Result: Derived error bounds showing expected squared error is bounded by interpolation factor (transitioning from (k+1)² to (k+1) with oversampling) times best rank-k approximation error, providing unified theoretical foundation.

Conclusion: Determinant-based methods provide interpretable local error components and geometric insight into CUR approximation, with probabilistic framework establishing direct connection between CUR quality and optimal low-rank approximation.

Abstract: We derive error bounds for CUR matrix approximation using determinant-based methods that relate local projection errors to global approximation quality. For general matrices, we establish determinant identities for bordered Gramian matrices that decompose CUR approximation errors into interpretable local components. These identities connect projection errors onto submatrix column spaces directly to determinants, providing geometric insight into approximation degradation. We develop a probabilistic framework based on volume sampling that yields interpolation-type error bounds quantifying the benefits of oversampling: when $r > k$ rows are selected for $k$ columns, the expected error factor transitions linearly from $(k+1)^2$ (no oversampling) to $(k+1)$ (full oversampling). Our analysis establishes that the expected squared error is bounded by this interpolation factor times the squared error of the best rank-$k$ approximation, directly connecting CUR approximation quality to the optimal low-rank approximation. The framework applies to both CUR decomposition for general matrices and the Nyström method for symmetric positive semi-definite matrices, providing a unified theoretical foundation for determinant-based low-rank approximation analysis.

</details>


### [7] [A geometrically informed algebraic multigrid preconditioned iterative approach for solving high-order finite element systems](https://arxiv.org/abs/2512.15121)
*Songzhe Xu,Majid Rasouli,Robert M. Kirby,David Moxey,Hari Sundar*

Main category: math.NA

TL;DR: GIAMG: A geometrically informed algebraic multigrid method that incorporates p-coarsening for high-order finite element problems, achieving mesh-independent convergence and excellent parallel scalability.


<details>
  <summary>Details</summary>
Motivation: High-order finite element discretizations lack efficient solvers for large-scale real-world applications. Conventional AMG is applied agnostically without using available geometric information, which could be beneficial especially for high-order problems where p-coarsening before h-coarsening is known to improve scalability in geometric multigrid but is non-trivial to implement in AMG.

Method: Developed GIAMG (geometrically informed algebraic multigrid) method that uses minimal geometric information from users to set up a grid hierarchy including p-coarsening at top grids. The approach combines geometric information with algebraic techniques to create an efficient multigrid solver for high-order problems.

Result: Extensive evaluation on 3D Helmholtz and incompressible flow problems demonstrates mesh-independent convergence and excellent parallel scalability. GIAMG outperforms existing AMG packages like Hypre and ML, with the added benefit of increased sparsification of coarse grid operators through p-coarsening.

Conclusion: Incorporating geometric information to assist AMG hierarchy setup, particularly for high-order finite element problems, is beneficial. GIAMG successfully implements p-coarsening within an AMG framework, providing an efficient open-source solution for large-scale high-order problems that outperforms existing AMG packages.

Abstract: Algebraic multigrid (AMG) is conventionally applied in a black-box fashion, agnostic to the underlying geometry. In this work, we propose that using geometric information -- when available -- to assist with setting up the AMG hierarchy is beneficial, especially for solving linear systems resulting from high-order finite element discretizations. High-order problems draw considerable interest to both the scientific and engineering communities, but lack efficient solvers, at least open-source codes, tailored for unstructured high-order discretizations targeting large-scale, real-world applications. For geometric multigrid, it is known that using p-coarsening before h-coarsening can provide better scalability, but setting up p-coarsening is non-trivial in AMG. We develop a geometrically informed algebraic multigrid (GIAMG) method, as well as an associated high-performance computing program, which is able to set up a grid hierarchy that includes p-coarsening at the top grids with minimal information of the geometry from the user. A major advantage of using p-coarsening with AMG -- beyond the benefits known in the context of geometric multigrid (GMG) -- is the increased sparsification of coarse grid operators. We extensively evaluate GIAMG by testing on the 3D Helmholtz and incompressible flow problems, and demonstrate mesh-independent convergence, and excellent parallel scalability. We also compare the performance of GIAMG with existing AMG packages, including Hypre and ML.

</details>


### [8] [A New Fast Finite Difference Scheme for Tempered Time Fractional Advection-Dispersion Equation with a Weak Singularity at Initial Time](https://arxiv.org/abs/2512.15141)
*Liangcai Huang,Shujuan Lü*

Main category: math.NA

TL;DR: A second-order fast finite difference scheme for solving Tempered Time Fractional Advection-Dispersion Equation with nonsmooth initial conditions, achieving second-order convergence in both time and space.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical scheme for solving the Tempered Time Fractional Advection-Dispersion Equation, particularly addressing the challenge of nonsmooth initial conditions which commonly occur in practical applications.

Method: Proposes a new second-order fast finite difference scheme in time for the Tempered Time Fractional Advection-Dispersion Equation. The scheme is analyzed under the assumption of nonsmooth initial conditions, with theoretical investigation of uniqueness, stability, and convergence properties.

Result: The scheme achieves second-order convergence in both time and space dimensions. Theoretical analysis confirms the uniqueness, stability, and convergence of the proposed scheme. Numerical examples validate the theoretical results.

Conclusion: The proposed fast finite difference scheme is effective for solving the Tempered Time Fractional Advection-Dispersion Equation with nonsmooth initial conditions, providing second-order accuracy in both temporal and spatial dimensions with proven theoretical guarantees.

Abstract: In this paper, we propose a new second-order fast finite difference scheme in time for solving the Tempered Time Fractional Advection-Dispersion Equation. Under the assumption that the solution is nonsmooth at the initial time, we investigate the uniqueness, stability, and convergence of the scheme. Furthermore, we prove that the scheme achieves second-order convergence in both time and space. Finally, corresponding numerical examples are provided.

</details>


### [9] [Efficient high-order two-derivative DIRK methods with optimized phase errors](https://arxiv.org/abs/2512.15227)
*Julius Ehigie,Vu Thai Luan*

Main category: math.NA

TL;DR: New high-order two-derivative diagonally implicit Runge-Kutta (TDDIRK) schemes with optimized phase errors are constructed and analyzed, showing improved accuracy and efficiency over existing DIRK methods.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient high-order numerical integration schemes for ODEs and PDEs by optimizing phase errors in two-derivative diagonally implicit Runge-Kutta methods, improving upon existing DIRK schemes.

Method: Constructs new TDDIRK schemes with optimized phase errors, provides convergence analysis, investigates phase errors and linear stability, and derives specific 2-stage 4th-order, 2-stage 5th-order, and 3-stage 5th-order TDDIRK families.

Result: Successfully developed new families of high-order TDDIRK schemes with optimized phase errors, demonstrated through numerical experiments at both ODE and PDE levels that these schemes offer improved accuracy and efficiency compared to known DIRK schemes.

Conclusion: The new TDDIRK schemes with optimized phase errors provide superior performance over existing DIRK methods, making them valuable for high-order numerical integration in both ODE and PDE applications.

Abstract: This work constructs and analyzes new efficient high-order two-derivative diagonally implicit Runge--Kutta (TDDIRK) schemes with optimized phase errors. Specifically, we present a convergence result for TDDIRK methods and investigate their optimized phase errors and linear stability analysis. Based on these, we derive new families of 2-stage fourth-order, 2-stage fifth-order, and 3-stage fifth-order TDDIRK schemes. Finally, we provide numerical experiments at both the ODE and PDE levels to demonstrate the accuracy and efficiency of these new schemes compared to known DIRK schemes in the literature.

</details>


### [10] [From flocking to jamming in collective cell dynamics: a Vicsek-like model including contact forces](https://arxiv.org/abs/2512.15305)
*Laurent Navoret,Roxana Sublet,Marcela Szopos*

Main category: math.NA

TL;DR: Agent-based model combining Vicsek-like polarity alignment with contact forces and velocity feedback, showing order-disorder transitions and jamming effects in collective cell dynamics.


<details>
  <summary>Details</summary>
Motivation: To develop a theoretical framework that improves understanding of complex collective cell dynamics and emerging tissue flows by combining different modeling approaches.

Method: Agent-based model combining classical Vicsek-like polarity alignments with contact forces from Maury and Venel (2011), plus velocity feedback on polarity and soft attraction-repulsion interactions. Includes well-posedness analysis, discretization, and extensive numerical experiments.

Result: The model recovers order-disorder phase transitions in flocks and jamming effects in high density regimes, demonstrating capability to capture key collective behaviors.

Conclusion: The developed framework is a promising theoretical tool for understanding complex collective cell dynamics and tissue flows, successfully integrating multiple modeling ingredients.

Abstract: The goal of the present work is to propose an agent-based model that originally combines classical Vicsek-like polarity alignments and contact forces, as implemented in the framework developed by Maury and Venel in [Maury, Venel, 2011]. The description additionally incorporates velocity feedback on polarity and soft attraction-repulsion interactions. After carefully studying the well posedness of the model, we introduce a suitable discretization and perform an extensive range of numerical experiments to assess the impact of different modeling ingredients. The dynamical system is capable of recovering the order-disorder phase transition of the flock, as well as the jamming effect in high density regimes. As such, the developed framework can be seen as a promising theoretical tool that could contribute to improving the understanding of complex collective cell dynamics and emerging tissue flows.

</details>


### [11] [Continuous Finite Element Method For Maxwell Eigenvalue Problems With Regular Decomposition Technique](https://arxiv.org/abs/2512.15314)
*Feiyi Liao,Haochen Liu,Hehu Xie*

Main category: math.NA

TL;DR: A novel numerical method using standard high order Lagrange finite elements for Maxwell eigenvalue problems, based on high order regular decomposition of H₀ˢ(curl; Ω) space.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for Maxwell eigenvalue problems using standard high order Lagrange finite elements, which typically face challenges due to the curl-conforming nature of the problem.

Method: Uses regular decomposition technique to decompose H₀ˢ(curl; Ω) space into vector potential space plus gradient of scalar space with higher regularity. Based on this decomposition, designs a novel numerical method employing standard high order Lagrange finite elements.

Result: Proves full convergence orders for eigenpair approximations. Provides numerical examples validating the scheme and confirming theoretical convergence results.

Conclusion: The proposed method successfully enables use of standard high order Lagrange finite elements for Maxwell eigenvalue problems with proven convergence, validated by numerical experiments.

Abstract: With the regular decomposition technique, we decompose the space $\mathbf{H}_0^s(\mathbf{curl}; Ω)$ into the sum of a vector potential space and the gradient of a scalar space, both possessing higher regularity. Based on this new high order regular decomposition, a novel numerical method using standard high order Lagrange finite elements is designed for solving Maxwell eigenvalue problems. Specifically, the full convergence orders of the eigenpair approximations are proved for the proposed numerical method. Finally, numerical examples are provided to validate the proposed scheme and confirm the theoretical convergence results.

</details>


### [12] [Hyperbolic trigonometric functions as approximation kernels and their properties II: Wavelets](https://arxiv.org/abs/2512.15317)
*M. Buhmann,J. Jódar,M. Rodríguez*

Main category: math.NA

TL;DR: Extension of previous radial basis function work to create prewavelet approximations using hyperbolic radial basis functions for localized time-frequency analysis and filtering.


<details>
  <summary>Details</summary>
Motivation: To develop new methods for function approximation using prewavelets constructed from hyperbolic radial basis functions, enabling highly localized time-frequency decompositions suitable for analysis and filtering applications.

Method: Extends previous radial basis function work to construct prewavelets from hyperbolic radial basis functions. The approach creates approximations using spaced spanned functions and is general enough to apply to other radial basis functions like multiquadrics, thin-plate splines, and polynomial splines.

Result: New prewavelet construction methods that provide highly localized time-frequency decompositions suitable for analysis and filtering, with broad applicability to various radial basis function classes.

Conclusion: The paper successfully extends radial basis function theory to create versatile prewavelet approximations that offer localized time-frequency analysis capabilities applicable to multiple function classes.

Abstract: In a previous paper we have introduced a new class of radial basis functions that are powerful means to approximate functions by quasi-interpolation. In this article we extend the results to create new ways of approximating functions by prewavelets that are constructed from spaced spanned of the new hyperbolic radial basis functions. They consist of highly localised time-frequency decompositions that are suitable for analysis and filtering. The construction is sufficiently general to apply for large classes of other radial basis functions too - such as multiquadrics and their generalisations and thin-plate splines -, as well as, for example, polynomial splines.

</details>


### [13] [A moment model of shallow granular flows with variable friction laws](https://arxiv.org/abs/2512.15332)
*Julio Careaga,Qian Huang,Julian Koellermeier*

Main category: math.NA

TL;DR: Develops a granular flow modeling framework using shallow water moment equations with polynomial velocity expansions, incorporating various friction laws and a path-conservative finite volume scheme for stiff source terms.


<details>
  <summary>Details</summary>
Motivation: To create a more accurate modeling framework for granular flows that extends classical shallow water equations to account for vertically variable velocity profiles and incorporates realistic friction effects from different rheological models.

Method: Uses shallow water moment equations on inclined planes with polynomial expansion of velocity field. Incorporates friction through strain-rate tensor with bulk and bottom friction terms. Develops a path-conservative finite volume scheme based on polynomial viscosity matrix method to handle stiff source terms.

Result: Successfully develops a comprehensive modeling framework that can incorporate various friction laws (Manning, Coulomb, Savage-Hutter, μ(I)-rheology) and handles wet-dry fronts. The numerical scheme effectively manages the stiffness of source terms.

Conclusion: The proposed framework provides a flexible and accurate approach for modeling granular flows with vertically variable velocity profiles and diverse friction laws, with robust numerical implementation capable of handling challenging scenarios like wet-dry fronts.

Abstract: In this work, we develop a modelling framework for granular flows based on the shallow water moment equations on inclined planes. Under the assumption of a polynomial expansion of the velocity field, the model extends the classical shallow water equations to vertically variable velocity profiles. The friction effects, which are captured through the strain-rate tensor, are incorporated into the model in two terms, the bulk and bottom friction. We propose a modelling procedure to incorporate general friction laws into our framework and exemplify this combining the Manning, Coulomb, Savage-Hutter, and $μ(I)$-rheology friction models in our modeling framework. Moreover, we develop a path-conservative finite volume numerical scheme based on the polynomial viscosity matrix method to properly handle the stiffness of the source terms. Numerical simulations are presented for different models of friction, including the case of wet-dry fronts.

</details>


### [14] [Consistent Parametric Model Order Reduction by Matrix Interpolation for Varying Underlying Meshes](https://arxiv.org/abs/2512.15373)
*Sebastian Resch-Schopper,Romain Rumpler,Gerhard Müller*

Main category: math.NA

TL;DR: A novel framework for parametric model order reduction (pMOR) via matrix interpolation that handles varying underlying meshes by treating reduced bases as continuous displacement fields and using mesh morphing techniques.


<details>
  <summary>Details</summary>
Motivation: Existing pMOR methods for geometric parameters require identical mesh topology and degrees of freedom across all parameter configurations, which is difficult or impossible for large parameter ranges or when using automatic meshing.

Method: Treat sampled reduced bases as continuous displacement fields that can be represented in different discretizations. Use mesh morphing (spring analogy with elastic hardening and radial basis function morphing) and basis interpolation to represent all sampled bases in terms of one reference mesh, enabling matrix interpolation and subspace comparison.

Result: Numerical experiments on beam-shaped plate and plate with hole for 1D and 2D parameter spaces show high accuracy for both morphing methods, significantly outperforming two existing approaches for pMOR with varying meshes.

Conclusion: The proposed framework successfully enables pMOR by matrix interpolation for varying underlying meshes, overcoming a major limitation of existing methods while maintaining accuracy and enabling subspace comparison for consistency checking.

Abstract: Parametric model order reduction (pMOR) is a powerful tool for accelerating finite element (FE) simulations while maintaining parametric dependencies. For geometric parameters, pMOR by matrix interpolation is a well-suited approach because it does not require an affine representation of the parametric dependency, which is often not available for geometric parameters. However, the method requires that the underlying FE mesh has the same number of degrees of freedom and the same topology for all parameter configurations. This requirement can be difficult or even impossible to achieve for large parameter ranges or when automatic meshing is used. In this work, we propose a novel framework for pMOR by matrix interpolation for varying underlying meshes. The key idea is to understand the sampled reduced bases as continuous displacement fields that can be represented in different discretizations. By using mesh morphing and basis interpolation, the sampled reduced bases described in varying meshes can all be represented in terms of one reference mesh. This not only allows for performing pMOR by matrix interpolation, but also enables comparing the subspaces that the reduced bases span, which is important to detect strong changes that could lead to inconsistencies in the reduced operators. For mesh morphing, two strategies, namely morphing by spring analogy with elastic hardening and radial basis function morphing, were implemented and tested. Numerical experiments on a beam-shaped plate and a plate with a hole for one- and two-dimensional parameter spaces show that the proposed framework achieves high accuracy for both morphing methods and performs significantly better than two existing approaches for pMOR by matrix interpolation for varying underlying meshes.

</details>


### [15] [Randomized orthogonalization and Krylov subspace methods: principles and algorithms](https://arxiv.org/abs/2512.15455)
*Jean-Guillaume de Damas,Laura Grigori,Igor Simunec,Edouard Timsit*

Main category: math.NA

TL;DR: Review of randomized orthogonalization techniques that construct well-conditioned bases with orthonormal sketches, reducing computational/communication costs while maintaining numerical stability for large-scale linear algebra problems.


<details>
  <summary>Details</summary>
Motivation: To reduce computational and communication costs of traditional orthogonalization procedures on parallel architectures while preserving or improving numerical stability, especially for large-scale linear algebra problems.

Method: Randomized orthogonalization techniques including randomized Gram-Schmidt and Householder QR algorithms, applied within Krylov subspace methods to create randomized Arnoldi relations.

Result: Emergence of randomized orthogonalization as a powerful paradigm that mitigates orthogonalization costs in Krylov methods while maintaining numerical stability.

Conclusion: Randomized orthogonalization offers efficient alternatives to traditional methods for solving large-scale linear algebra problems including linear systems, eigenvalue problems, matrix functions, and matrix equations.

Abstract: We present an overview of randomized orthogonalization techniques that construct a well-conditioned basis whose sketch is orthonormal. Randomized orthogonalization has recently emerged as a powerful paradigm for reducing the computational and communication cost of state-of-the-art orthogonalization procedures on parallel architectures, while preserving, and in some cases improving, their numerical stability. This approach can be employed within Krylov subspace methods to mitigate the cost of orthogonalization, yielding a randomized Arnoldi relation. We review the main variants of the randomized Gram--Schmidt and Householder QR algorithms, and discuss their application to Krylov methods for the solution of large-scale linear algebra problems, such as linear systems of equations, eigenvalue problems, the evaluation of matrix functions, and matrix equations.

</details>


### [16] [Robust a posteriori error analysis of the stochastic Cahn-Hilliard equation with rough noise](https://arxiv.org/abs/2512.15495)
*Lubomir Banas,Jean Daniel Mukam*

Main category: math.NA

TL;DR: A posteriori error estimates for adaptive FEM of stochastic Cahn-Hilliard equation with rough noise, robust to interfacial width and noise regularization parameters.


<details>
  <summary>Details</summary>
Motivation: Need reliable error estimation for stochastic Cahn-Hilliard equation with rough noise to enable adaptive finite element methods that can handle challenging noise characteristics.

Method: Derived a posteriori error estimates for fully discrete adaptive FEM approximation of stochastic Cahn-Hilliard equation with spatially regularized white noise. The method is robust to both interfacial width and noise regularization parameters.

Result: Developed robust a posteriori error estimates and proposed practical adaptive algorithm. Numerical simulations demonstrate the theoretical findings.

Conclusion: The paper provides reliable error estimation framework for adaptive FEM of stochastic Cahn-Hilliard equations with rough noise, enabling practical numerical simulations with theoretical guarantees.

Abstract: We derive a posteriori error estimate for a fully discrete adaptive finite element approximation of the stochastic Cahn-Hilliard equation with rough noise. The considered model is derived from the stochastic Cahn-Hilliard equation with additive space-time white noise through suitable spatial regularization of the white noise. The a posteriori estimate is robust with respect to the interfacial width parameter as well as the noise regularization parameter. We propose a practical adaptive algorithm for the considered problem and perform numerical simulations to illustrate the theoretical findings.

</details>


### [17] [Space-Time Spectral Collocation Tensor-Network Approach for Maxwell's Equations](https://arxiv.org/abs/2512.15631)
*Dibyendu Adak,Rujeko Chinomona,Duc P. Truong,Oleg Korobkin,Kim Ø. Rasmussen,Boian S. Alexandrov*

Main category: math.NA

TL;DR: Space-time Chebyshev spectral collocation method for 3D Maxwell's equations combined with Tensor-Train format for efficient low-rank approximation.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for solving three-dimensional Maxwell's equations by combining global space-time discretization with tensor-network techniques to handle the large linear systems that arise.

Method: Uses staggered Chebyshev spectral collocation in space and time for the electric field wave equation, preserves divergence-free constraint, recovers magnetic field via discrete Faraday's law, and approximates the large linear system in low-rank TT-format without requiring separable forcing.

Result: Derived condition-number bounds for the operator, proved spectral convergence for both electric and magnetic fields, and demonstrated through numerical experiments that the TT-based solver maintains accuracy with approximately linear complexity in grid points.

Conclusion: The combination of space-time spectral methods with tensor-network techniques provides an efficient and accurate approach for solving 3D Maxwell's equations with theoretical guarantees and practical computational benefits.

Abstract: In this work, we develop a space--time Chebyshev spectral collocation method for three-dimensional Maxwell's equations and combine it with tensor-network techniques in Tensor-Train (TT) format. Under constant material parameters, the Maxwell system is reduced to a vector wave equation for the electric field, which we discretize globally in space and time using a staggered spectral collocation scheme. The staggered polynomial spaces are designed so that the discrete curl and divergence operators preserve the divergence-free constraint on the magnetic field. The magnetic field is then recovered in a space--time post-processing step via a discrete version of Faraday's law. The global space--time formulation yields a large but highly structured linear system, which we approximate in low-rank TT-format directly from the operator and data, without assuming that the forcing is separable in space and time. We derive condition-number bounds for the resulting operator and prove spectral convergence estimates for both the electric and magnetic fields. Numerical experiments for three-dimensional electromagnetic test problems confirm the theoretical convergence rates and show that the TT-based solver maintains accuracy with approximately linear complexity in the number of grid points in space and time.

</details>


### [18] [Reduced Basis Methods for Parametric Steady-State Radiative Transfer Equation](https://arxiv.org/abs/2512.15640)
*Kimberly Matsuda,Yanlai Chen,Yingda Cheng,Fengyan Li*

Main category: math.NA

TL;DR: This paper presents the first systematic investigation of projection-based reduced order models (ROMs) for the parametric steady-state radiative transfer equation, achieving 4-6 orders of magnitude speedup compared to full order models.


<details>
  <summary>Details</summary>
Motivation: The radiative transfer equation (RTE) is computationally expensive due to its high dimensionality. Deterministic methods are accurate but costly. The authors aim to develop efficient reduced order models to overcome these computational challenges while maintaining accuracy.

Method: Four ROMs based on reduced basis method framework: Galerkin or least-squares Petrov-Galerkin projection with either L1 or residual-based error indicators. Implementation strategies focus on efficiency, accuracy, conditioning, and numerical robustness under affine parameter dependence.

Result: Two ROMs are certified for positively bounded absorption cross sections. Computational complexities derived for offline training and online prediction. Numerical experiments show 4-6 orders of magnitude speedup for 2D2v examples compared to full order models.

Conclusion: The proposed ROMs provide efficient, accurate, and robust reduced surrogate solvers for parametric steady-state RTE with isotropic scattering, offering significant computational speedup while maintaining solution fidelity.

Abstract: The radiative transfer equation (RTE) is a fundamental mathematical model to describe physical phenomena involving the propagation of radiation and its interactions with the host medium. Deterministic methods can produce accurate solutions without any statistical noise, yet often at a price of expensive computational costs originating from the intrinsic high dimensionality of the model. With this work, we present the first systematic investigation of projection-based reduced order models (ROMs) following the reduced basis method (RBM) framework to simulate the parametric steady-state RTE with isotropic scattering and one energy group. Four ROMs are designed, with each defining a nested family of reduced surrogate solvers of different resolution/fidelity. They are based on either a Galerkin or least-squares Petrov-Galerkin projection and utilize either an $L_1$ or residual-based importance/error indicator. Two of the proposed ROMs are certified in the setting when the absorption cross section is positively bounded below uniformly. One technical focus and contribution lie in the proposed implementation strategies under the affine assumption of the parameter dependence of the model. These well-crafted broadly applicable strategies not only ensure the efficiency and accuracy of the offline training stage and the online prediction of reduced surrogate solvers, they also take into account the conditioning of the reduced systems as well as the stagnation-free residual evaluation for numerical robustness. Computational complexities are derived for both the offline training and online prediction stages of the proposed model order reduction strategies, and they are demonstrated numerically along with the accuracy and robustness of the reduced surrogate solvers. Numerically we observe four to six orders of magnitude speedup of our ROMs compared to full order models for some 2D2v examples.

</details>


### [19] [Material data identification in generalized continua](https://arxiv.org/abs/2512.15646)
*Jacinto Ulloa,Laurent Stainier*

Main category: math.NA

TL;DR: Data-driven framework extracts material behavior from full-field measurements in generalized continua without constitutive assumptions, using non-classical balance laws and clustering to create material datasets for calibration or model-free simulations.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches for identifying material behavior in generalized continua rely on constitutive assumptions or homogenization schemes, which may not capture complex non-classical stress states. There's a need for a more direct, data-driven method that can extract material behavior from experimental measurements without these limitations.

Method: The framework uses full-field kinematics and force measurements to extract generalized stress-strain data by enforcing non-classical balance laws and compatibility relations on boundary value problems. It infers generalized stresses and constructs representative material datasets through clustering in a non-classical phase space.

Result: The method reliably extracts non-symmetric and higher-order local stress states, creating material data suitable for model calibration or model-free data-driven simulations. Validation with synthetic data and application to mechanical metamaterials demonstrate practical utility for characterizing microstructured solids.

Conclusion: This data-driven framework provides a practical route for material characterization of generalized continua, enabling direct extraction of material behavior from experimental measurements without relying on traditional constitutive assumptions or homogenization schemes.

Abstract: We introduce a data-driven framework for identifying material behavior from full-field kinematics and force measurements in generalized (micromorphic) continua. Unlike traditional approaches that rely on constitutive assumptions or homogenization schemes, our method extracts generalized stress--strain data by enforcing non-classical balance laws and compatibility relations on full-field boundary value problems. Specifically, the approach infers the associated generalized stresses and constructs representative material datasets via clustering in a non-classical phase space. We show that the proposed method reliably extracts non-symmetric and higher-order local stress states, providing material data suitable for either model calibration or model-free data-driven simulations of generalized continua. These capabilities are demonstrated in validation simulations with synthetic data and in an application to mechanical metamaterials, suggesting a practical route for material characterization of microstructured solids.

</details>


### [20] [Time integration of quantized tensor trains using the interpolative dynamical low-rank approximation](https://arxiv.org/abs/2512.15703)
*Erika Ye,Chao Yang*

Main category: math.NA

TL;DR: Interpolative dynamical low-rank approximation (DLRA) using interpolation points and polynomials in quantized tensor trains (QTTs) framework, showing advantages over orthogonal DLRA for nonlinear systems and upwind schemes.


<details>
  <summary>Details</summary>
Motivation: Traditional DLRA uses orthogonal projectors that are only well-suited for linear systems, and most DLRA research has focused on non-quantized tensor trains rather than the efficient QTT framework for high-dimensional data.

Method: Investigates interpolative DLRA schemes where the low-rank manifold is constructed from carefully chosen interpolation points and interpolating polynomials within the quantized tensor trains framework.

Result: Through various examples, shows that interpolative DLRA outperforms orthogonal DLRA for nonlinear systems and time integrators requiring nonlinear element-wise operations like upwind schemes.

Conclusion: Interpolative DLRA is suitable for nonlinear systems and complex time integration schemes within the efficient QTT framework, offering advantages over traditional orthogonal projection methods.

Abstract: Quantized tensor trains (QTTs) are a low-rank and multiscale framework that allows for efficient approximation and manipulation of multi-dimensional, high resolution data. One area of active research is their use in numerical simulation of hyperbolic systems such as the Navier-Stokes equations and the Vlasov equations. One popular time integration scheme is the dynamical low-rank approximation (DLRA), in which the time integration is constrained to a low-rank manifold. However, until recently, DLRA has typically used orthogonal projectors to project the original dynamical system into a reduced space, which is only well-suited for linear systems. DLRA has also mostly been investigated in the context of non-quantized tensor trains. This work investigates interpolative DLRA schemes in which the low-rank manifold is constructed from aptly chosen interpolation points and interpolating polynomials, in the context of QTTs. Through various examples, its performance is compared to its orthogonal counterpart. This work demonstrates how interpolative DLRA is suitable for nonlinear systems and time integrators requiring nonlinear element-wise operations, such as upwind time integration schemes.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [21] [Chemotaxis models with signal-dependent sensitivity and a logistic-type source, I: Boundedness and global existence](https://arxiv.org/abs/2512.14858)
*Le Chen,Ian Ruau,Wenxian Shen*

Main category: math.AP

TL;DR: This paper studies boundedness and global existence of solutions to a parabolic-elliptic chemotaxis system with signal-dependent sensitivity and logistic source, analyzing three mechanisms: negative chemotaxis, nonlinear cross-diffusion strength, and logistic damping strength.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the boundedness and global existence of solutions to chemotaxis systems with signal-dependent sensitivity and logistic growth, which are important in modeling biological phenomena like cell migration and pattern formation. The decay of chemotactic sensitivity for large chemical concentrations introduces analytical challenges that need to be addressed.

Method: The authors analyze the chemotaxis system using three different viewpoints: negative chemotaxis (χ₀<0), the strength of nonlinear cross-diffusion rate u^m/(1+v)^β, and the strength of logistic-type damping u(a-bu^α). They derive explicit conditions under which all positive classical solutions remain bounded, and show that when m≥1, boundedness implies global existence.

Result: The main results establish explicit conditions for boundedness of all positive classical solutions. For example, when m=1, global existence is guaranteed if β > max{1, 1/2 + χ₀/4 max{2, γN}}. Several known results for special cases are recovered. The paper also mentions that Part II will study asymptotic behavior including uniform persistence, stability, and bifurcation of equilibria.

Conclusion: The paper successfully establishes conditions for boundedness and global existence in chemotaxis systems with signal-dependent sensitivity and logistic source, addressing analytical difficulties introduced by the decay of chemotactic sensitivity. The results provide a comprehensive understanding of how different mechanisms (negative chemotaxis, cross-diffusion strength, logistic damping) contribute to solution boundedness.

Abstract: We study, in Part I of this series, boundedness and global existence of positive classical solutions to a parabolic-elliptic chemotaxis system with signal-dependent sensitivity and a logistic-type source on a bounded smooth domain $Ω\subset\mathbb{R}^N$: \begin{equation*}
  \begin{cases}
  \displaystyle u_t=Δu-χ_0\nabla\cdot\left(\frac{u^m}{(1+v)^β}\nabla v\right)+au-bu^{1+α}, & x\inΩ, \cr
  \displaystyle 0=Δv-μv+νu^γ, & x\inΩ, \cr
  \displaystyle \frac{\partial u}{\partial n}=\frac{\partial v}{\partial n}=0, & x\in\partialΩ.
  \end{cases} \end{equation*} Here, $u$ denotes the population density and $v$ the chemical concentration. The parameters $α,γ,m,μ,ν$ are positive, $χ_0$ is real, and $a,b,β$ are nonnegative. We analyze boundedness from three viewpoints: negative chemotaxis ($χ_0<0$), the strength of the nonlinear cross diffusion rate $\frac{u^m}{(1+v)^β}$, and the strength of the logistic-type damping $u(a-bu^α)$. Under explicit conditions reflecting these mechanisms, all positive classical solutions remain bounded. Moreover, when $m\ge 1$, boundedness implies global existence. Although the decay of $χ(v) = \dfrac{χ_0}{(1+v)^β}$ for large $v$ has a damping effect, it also introduces new analytical difficulties; our techniques yield, for example, global existence for $m=1$ provided that \begin{equation*}
  β>\max\left\{1,\frac12+\frac{χ_0}{4}\max\{2,γN\}\right\}. \end{equation*} Several known results for special cases are recovered. Part II is devoted to the asymptotic behavior of globally defined solutions, including uniform persistence as well as stability and bifurcation of positive constant equilibria.

</details>


### [22] [Relations between principal eigenvalue and torsional rigidity with Robin boundary conditions](https://arxiv.org/abs/2512.14927)
*Giuseppe Buttazzo,Simone Cito,Francesco Solombrino*

Main category: math.AP

TL;DR: Paper studies bounds for products of torsional rigidity and principal eigenvalue of Laplace operator with Dirichlet/Robin boundary conditions in Lipschitz domains, showing different threshold exponents.


<details>
  <summary>Details</summary>
Motivation: To establish mathematical bounds for combined quantities involving torsional rigidity and principal eigenvalues under different boundary conditions, comparing Dirichlet vs Robin cases.

Method: Mathematical analysis of Laplace operator with Dirichlet and Robin boundary conditions on Lipschitz domains, deriving upper and lower bounds for products of torsional rigidity and principal eigenvalue powers.

Result: Explicit recovery of threshold exponent for Robin case, showing it's strictly smaller than in Dirichlet case, establishing bounds for these combined quantities.

Conclusion: Different boundary conditions lead to different threshold exponents when combining torsional rigidity and principal eigenvalue, with Robin case having smaller exponent than Dirichlet case.

Abstract: We consider the torsional rigidity and the principal eigenvalue related to the Laplace operator with Dirichlet and Robin boundary conditions. The goal is to find upper and lower bounds to products of suitable powers of the quantities above in the class of Lipschitz domains. The threshold exponent for the Robin case is explicitly recovered and shown to be strictly smaller than in the Dirichlet one.

</details>


### [23] [Moisture dynamics with phase changes coupled to heat-conducting, compressible fluids](https://arxiv.org/abs/2512.14931)
*Felix Brandt,Matthias Hieber,Lin Ma,Tarek Zöchling*

Main category: math.AP

TL;DR: Local strong well-posedness for large data and global well-posedness for small data of a coupled compressible Navier-Stokes-moisture model with phase changes.


<details>
  <summary>Details</summary>
Motivation: To establish well-posedness results for a model coupling heat-conducting compressible Navier-Stokes equations with moisture dynamics including phase changes, which is important for atmospheric modeling but mathematically challenging due to strong coupling from latent heat effects.

Method: Lagrangian approach to handle hyperbolicity in continuity equation, optimal L^p-L^q estimates for linearized system, fixed point argument with nonlinear estimates for local result, and refined analysis near equilibria with delicate a priori bounds for global result.

Result: First result showing local strong well-posedness for large data in suitable function spaces and strong well-posedness on [0,τ] for every τ>0 for small initial data for this coupled moisture-thermodynamics model.

Conclusion: The paper successfully establishes well-posedness for a physically important coupled moisture-Navier-Stokes model with phase changes, overcoming challenges from latent heat coupling and hyperbolicity through sophisticated analytical techniques.

Abstract: It is shown that a model coupling the heat-conducting compressible Navier-Stokes equations to a micro-physics model of moisture in air is locally strongly well-posed for large data in suitable function spaces and strongly well-posed on $[0,τ]$ for every $τ> 0$ for small initial data. This seems to be the first result on $[0,τ]$ for arbitrary $τ> 0$ for a model coupling moisture dynamics to heat-conducting, compressible Navier-Stokes equations. A key feature of the micro-physics model is that it also includes phase changes of water in moist air. These phase changes are associated with large amounts of latent heat and thus result in a strong coupling to the thermodynamic equation. The well-posedness results are obtained by means of a Lagrangian approach, which allows to treat the hyperbolicity in the continuity equation. More precisely, optimal $\mathrm{L}^p$-$\mathrm{L}^q$ estimates are shown for the linearized system, leading to the local well-posedness result by a fixed point argument and suitable nonlinear estimates. For the well-posedness result on $[0,τ]$ for arbitrary $τ> 0$, a refined analysis of the linearized problem close to equilibria is carried out, and the roughness of the source term, induced by the phase changes, requires to establish delicate a priori bounds.

</details>


### [24] [Asymptotic formulas for $L^2$ bifurcation curves of nonlocal logistic equation of population dynamics](https://arxiv.org/abs/2512.14955)
*Tetsutaro Shibata*

Main category: math.AP

TL;DR: Study of 1D nonlocal Kirchhoff type bifurcation problems from population dynamics, focusing on asymptotic shapes of L² bifurcation curves as amplitude approaches infinity.


<details>
  <summary>Details</summary>
Motivation: The paper investigates bifurcation problems arising from logistic population dynamics equations, specifically focusing on nonlocal Kirchhoff type problems. These models are important for understanding population dynamics and pattern formation in biological systems.

Method: The authors analyze one-dimensional nonlocal Kirchhoff type bifurcation problems derived from logistic equations. They study the asymptotic behavior of L² bifurcation curves λ = λ(α) as α → ∞, where α represents the L² norm of the solution u_λ.

Result: The main result is obtaining precise asymptotic shapes of the L² bifurcation curves as the amplitude parameter α approaches infinity. This provides detailed understanding of how solutions behave in the large-amplitude regime.

Conclusion: The paper successfully characterizes the asymptotic behavior of bifurcation curves for nonlocal Kirchhoff type problems in population dynamics, contributing to the mathematical understanding of pattern formation in biological systems.

Abstract: The one-dimensional nonlocal Kirchhoff type bifurcation problems which are derived from logistic equation of population dynamics are studied. We obtain the precise asymptotic shapes of $L^2$ bifurcation curves $λ= λ(α)$ as $α\to \infty$, where $α:= \Vert u_λ\Vert_2$.

</details>


### [25] [Unbounded Branches of Non-Radial Solutions to Semilinear Elliptic Systems on a Unit Ball in $\mathbb R^3$ and Their Patterns](https://arxiv.org/abs/2512.14957)
*Casey Crane,Ziad Ghanem*

Main category: math.AP

TL;DR: The paper develops a mathematical framework for analyzing symmetry-breaking phenomena in semilinear elliptic systems on the 3D unit ball, focusing on non-radial solution branches with specific spatial and internal symmetries.


<details>
  <summary>Details</summary>
Motivation: To extend previous scalar results to systems and understand how symmetries (spatial and internal) interact to produce complex patterns in high-dimensional elliptic systems, particularly in applications like coupled spherical oscillators.

Method: Uses G-equivariant Leray-Schauder degree theory and Burnside ring techniques, where G = O(3) × Γ × ℤ₂ (Γ is a finite group for coupling symmetries). The approach handles non-simple eigenvalue multiplicities and provides explicit bifurcation conditions based on spectral resonance between coupling eigenvalues and spherical Laplacian modes.

Result: Derives computable criteria for existence of unbounded branches of non-radial solutions, classifies their isotropy types, and establishes a general method for detecting and characterizing symmetry-breaking in high-dimensional elliptic systems.

Conclusion: The framework successfully analyzes symmetry-breaking in elliptic systems, showing how Platonic symmetries and internal permutations interact to produce complex patterns, with applications to coupled spherical oscillators demonstrating the method's effectiveness.

Abstract: We investigate symmetry-breaking phenomena in semilinear elliptic systems on the unit ball in $\mathbb{R}^3$, focusing on the emergence of non-radial solution branches with prescribed spatial and internal symmetries. Extending previous scalar results, we develop a framework for systems equivariant under $G := O(3) \times Γ\times \mathbb{Z}_2$, where $Γ$ is a finite group encoding coupling symmetries. Using the $G$-equivariant Leray--Schauder degree and Burnside ring techniques, we derive computable criteria for the existence of unbounded branches of non-radial solutions and classify their isotropy types. Our approach accommodates non-simple eigenvalue multiplicities and provides explicit bifurcation conditions in terms of spectral resonance between coupling eigenvalues and spherical Laplacian modes. Applications to coupled spherical oscillators illustrate how Platonic symmetries and internal permutations interact to produce complex patterns. These results establish a general method for detecting and characterizing symmetry-breaking in high-dimensional elliptic systems.

</details>


### [26] [Exact solution structures on some nonlocal overdetermined problems](https://arxiv.org/abs/2512.14987)
*Kazuki Sato,Futoshi Takahashi*

Main category: math.AP

TL;DR: Serrin-type overdetermined problems with Kirchhoff nonlocal terms have solution structures determined by transcendental equations; explicit solutions derived from nonlocal-free counterparts.


<details>
  <summary>Details</summary>
Motivation: To understand solution structures of Serrin-type overdetermined problems when Kirchhoff-type nonlocal terms are introduced, extending classical results to nonlocal settings.

Method: Prove solution count matches transcendental equations defined by nonlocal terms; derive explicit solutions using unique solutions of corresponding problems without nonlocal terms.

Result: Exact number of solutions equals those of specific transcendental equations; explicit solution forms obtained via transformation from nonlocal-free solutions.

Conclusion: Kirchhoff nonlocal terms in Serrin-type overdetermined problems yield solution structures governed by transcendental equations, with explicit solutions constructible from classical counterparts.

Abstract: In this paper, we study the solution structures of Serrin-type overdetermined problems with Kirchhoff-type nonlocal terms. We prove that the exact number of solutions is the same as those of some transcendental equations defined by the nonlocal terms. We also obtain the explicit form of solutions by using the unique solutions of the overdetermined problems without the nonlocal terms.

</details>


### [27] [On self-similar singular solutions to a vorticity stretching equation](https://arxiv.org/abs/2512.15017)
*Dapeng Du,Jingyu Li,Xinyue Shi*

Main category: math.AP

TL;DR: Existence of self-similar singular solutions and finite-time blowup for a model equation involving Calderon-Zygmond operator Z₁₁.


<details>
  <summary>Details</summary>
Motivation: To study singular solution behavior and blowup phenomena in a model equation with a degenerate Calderon-Zygmond operator, which presents mathematical challenges due to operator degeneracy.

Method: Uses spectral uncertainty principle to overcome degeneracy of operator Z₁₁ = ∂₁₁Δ⁻¹. Analyzes self-similar singular solutions with special form and studies blowup for compactly supported initial data with positive integral.

Result: Proves existence of self-similar singular solutions. Shows finite-time blowup occurs when initial data is compactly supported and has positive integral.

Conclusion: The model equation exhibits both self-similar singular solutions and finite-time blowup behavior, with spectral uncertainty principle being key to handling operator degeneracy.

Abstract: We consider the following model equation: \begin{equation}
  ω_{t} = Z_{11}ω\,ω, \end{equation} where \begin{equation}
  Z_{11} = \partial_{11}Δ^{-1} \end{equation} is a Calderon-Zygmond operator. We get the existence of self-similar singular solutions with a special form. The main difficulty is the degeneracy of the operator $Z_{11}$ that is overcome by the spectral uncertainty principle. We also show that the solution to this model blows up in finite time if the initial datum is compactly supported and has a positive integral.

</details>


### [28] [Global well-posedness of the three-dimensional non-isentropic compressible magnetohydrodynamic equations under a scaling-invariant smallness condition](https://arxiv.org/abs/2512.15021)
*Lin Xu,Xin Zhong*

Main category: math.AP

TL;DR: Global existence and uniqueness of strong solutions for 3D non-isentropic compressible magnetohydrodynamic equations with vacuum, under smallness of a new scaling-invariant quantity involving initial data.


<details>
  <summary>Details</summary>
Motivation: To establish global well-posedness for compressible magnetohydrodynamic equations with vacuum in 3D, improving previous results by removing the artificial viscosity condition 3μ>λ and introducing a better scaling-invariant quantity.

Method: Derive delicate energy estimates and exploit the intrinsic structure of the system to prove global existence and uniqueness of strong solutions.

Result: Global strong solutions exist uniquely when the new scaling-invariant quantity (involving initial density supremum, L³ norm of density, L² norms of velocity and magnetic field, and various Sobolev norms) is sufficiently small.

Conclusion: The paper improves previous work by removing the viscosity restriction and providing a more natural scaling-invariant condition for global well-posedness of compressible MHD with vacuum.

Abstract: We consider the Cauchy problem of the non-isentropic compressible magnetohydrodynamic equations in $\mathbb{R}^3$ with far-field vacuum. By deriving delicate energy estimates and exploiting the intrinsic structure of the system, we establish the global existence and uniqueness of strong solutions provided that the scaling-invariant quantity \begin{align*} (1+\barρ+\tfrac{1}{\barρ}) [\|ρ_{0}\|_{L^{3}}+ ( \barρ^{2}+\barρ)( \| \sqrt{ρ_{0}}u_{0}\|_{L^{2}}^{2}+\| b_{0}\|_{L^{2}}^{2}) ] [\|\nabla u_{0}\|_{L^{2}}^{2}+(\barρ+1)\|\sqrt{ρ_{0}} θ_{0}\|_{L^{2}}^{2}+\| \nabla b_{0}\|_{L^{2}}^{2}+\| b_{0}\|_{L^{4}}^{4} ] \end{align*} is sufficiently small, where $\barρ$ denotes the essential supremum of the initial density. Our result may be regarded as an improved version compared with that of Liu and the second author (J. Differential Equations 336 (2022), pp. 456--478) in the sense that an artificial condition $3μ>λ$ on the viscosity coefficients is removed. In particular, we provide a new scaling-invariant quantity regarding the initial data.

</details>


### [29] [On global classical and weak solutions with arbitrary large initial data to the multi-dimensional viscous Saint-Venant system and compressible Navier-Stokes equations subject to the BD entropy condition under spherical symmetry](https://arxiv.org/abs/2512.15029)
*Xiangdi Huanga,Weili Meng,Xueyao Zhang*

Main category: math.AP

TL;DR: Global existence of smooth solutions for 2D viscous shallow water equations with arbitrary initial data under radial symmetry, extending to compressible Navier-Stokes with BD entropy conditions.


<details>
  <summary>Details</summary>
Motivation: Since 1871, the global existence of smooth solutions for 2D shallow water equations with arbitrarily large initial data has been a long-standing open problem. The paper aims to resolve this for the viscous case under radial symmetry.

Method: Treats viscous Saint-Venant system as endpoint case of BD entropy condition for compressible Navier-Stokes equations. Uses radial symmetry framework and addresses critical embedding issues specific to 2D spatial dimension.

Result: Establishes global existence of smooth solutions for 2D radially symmetric viscous shallow water equations with arbitrary smooth initial data. Extends result to 2D and 3D compressible Navier-Stokes equations satisfying BD entropy conditions (excluding endpoint case).

Conclusion: Resolves the long-standing open problem for viscous shallow water equations under radial symmetry, with shallow water equations representing the endpoint case of compressible Navier-Stokes with BD entropy conditions.

Abstract: In 1871, Saint-Venant introduced the renowned shallow water equations. Since then, for the two-dimensional viscous or inviscid shallow water equations, the global existence of smooth solutions with arbitrarily large initial data has remained a challenging and long-standing open problem. In this paper, we provide an affirmative resolution to the viscous problem under the assumption of two-dimensional radial symmetry. Specifically, we establish the global existence of smooth solutions for the two-dimensional radially symmetric viscous shallow water equations with arbitrary smooth initial data. To achieve this goal, our approach relies crucially on overcoming two major obstacles: first, treating the viscous Saint-Venant system as the endpoint case of the BD entropy condition for the compressible Navier-Stokes equations; and second, addressing the critical embedding imposed by the spatial dimension, which currently holds only in two dimensions. However, the same result can be extended to three dimension for the compressible Navier-Stokes equations satisfying general BD entropy conditions excluding the endpoint case. Indeed, under the same symmtric framework, we also prove the global existence of smooth solutions for arbitrarily large initial data for both the two- and three-dimensional compressible Navier-Stokes equations subject to the BD entropy condition. It is particularly noteworthy that the aforementioned shallow water equations precisely correspond to the endpoint case of the compressible Navier-Stokes equations satisfying the BD entropy condition.

</details>


### [30] [Nonlinear asymptotic stability and optimal decay rate around the three-dimensional Oseen vortex filament](https://arxiv.org/abs/2512.15040)
*Te Li,Ping Zhang,Yibin Zhang*

Main category: math.AP

TL;DR: The paper proves nonlinear asymptotic stability of 3D Oseen vortex filaments at high Reynolds numbers, showing all non-axisymmetric perturbations decay at optimal rate t^{-κ|α|^{1/2}}, resolving a conjecture about spectral scaling laws.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time dynamics of 3D incompressible Navier-Stokes equations near vortex filaments at high Reynolds numbers, where vortex stretching, shearing, and mixing generate small spatial scales that amplify viscous effects.

Method: Adopts an anisotropic self-similar coordinate system adapted to filament geometry, establishes spectral estimates for the nonlocal Oseen operator L_⊥ - αΛ_⊥, proves sharp spectral lower bound Σ(α) ∼ |α|^{1/2}, and identifies explicit spectral point attaining optimal bound.

Result: Proves nonlinear asymptotic stability of Oseen vortex filament, shows all non-axisymmetric perturbations decay at optimal rate t^{-κ|α|^{1/2}}, fully resolves conjecture about spectral and pseudospectral bounds Σ(α) and Ψ(α) scaling laws.

Conclusion: Provides rigorous mathematical explanation for shear-mixing mechanism near 3D Oseen vortex filaments, establishing optimal decay rates and spectral properties that explain how vortex stretching and mixing interact with viscous effects.

Abstract: In the high-Reynolds-number regime, this work investigates the long-time dynamics of the three-dimensional incompressible Navier-Stokes equations near the Oseen vortex filament. The flow exhibits a strong interplay between vortex stretching, shearing, and mixing, which generates ever-smaller spatial scales and thereby significantly amplifies viscous effects. By adopting an anisotropic self-similar coordinate system adapted to the filament geometry, we establish the nonlinear asymptotic stability of the Oseen vortex filament. All non-axisymmetric perturbations are shown to decay at the optimal rate $t^{-κ|α|^{1/2}}$. At the linear level, this decay mechanism corresponds to a sharp spectral lower bound $Σ(α) \sim |α|^{1/2}$ for the nonlocal Oseen operator $L_\perp - αΛ_\perp$, and we identify an explicit spectral point attaining this optimal bound. Combined with the spectral estimates obtained in \cite{LWZ}, our analysis fully resolves the conjecture proposed in \cite{GM} concerning the asymptotic scaling laws for the spectral and pseudospectral bounds $Σ(α)$ and $Ψ(α)$. These results provide a rigorous mathematical explanation for the shear-mixing mechanism in the vicinity of the 3D Oseen vortex filament.

</details>


### [31] [Well-Posedness of Pseudo-Parabolic Gradient Systems with State-Dependent Dynamics](https://arxiv.org/abs/2512.15164)
*Harbir Antil,Daiki Mizuno,Ken Shirakawa,Naotaka Ukai*

Main category: math.AP

TL;DR: A mathematical framework for pseudo-parabolic gradient systems with state-dependent dynamics, establishing well-posedness theory and applying it to anisotropic image denoising and grain boundary motion models.


<details>
  <summary>Details</summary>
Motivation: To develop a general mathematical framework for pseudo-parabolic gradient systems where dynamics depend on the state itself, which arises naturally in scientific models like grain boundary motion with state-dependent mobilities and anisotropic image denoising with variable orientation-adaptation operators.

Method: Develops a general mathematical framework for pseudo-parabolic gradient systems with state-dependent dynamics induced by variable coefficient fields in the governing energy functional. Establishes existence of energy-dissipating solutions and uniqueness/continuous dependence on initial data.

Result: Establishes two main results: existence of energy-dissipating solutions, and uniqueness and continuous dependence on initial data. The framework yields a general well-posedness theory for a broad class of nonlinear evolutionary systems driven by state-dependent operators.

Conclusion: The proposed framework provides a general well-posedness theory applicable to various nonlinear evolutionary systems. Demonstrates applicability through two concrete models: anisotropic image-denoising and a new pseudo-parabolic KWC-type model for anisotropic grain boundary motion, both fitting naturally within the abstract structure.

Abstract: This paper develops a general mathematical framework for pseudo-parabolic gradient systems with state-dependent dynamics. The state dependence is induced by variable coefficient fields in the governing energy functional. Such coefficients arise naturally in scientific and technological models, including state-dependent mobilities in KWC-type grain boundary motion and variable orientation-adaptation operators in anisotropic image denoising. We establish two main results: the existence of energy-dissipating solutions, and the uniqueness and continuous dependence on initial data. The proposed framework yields a general well-posedness theory for a broad class of nonlinear evolutionary systems driven by state-dependent operators. As illustrative applications, we present an anisotropic image-denoising model and a new pseudo-parabolic KWC-type model for anisotropic grain boundary motion, and prove that both fit naturally within the abstract structure of $(\mathrm{S})_ν$.

</details>


### [32] [Modeling of a micropolar thin film flow with rapidly varying thickness and non-standard boundary conditions](https://arxiv.org/abs/2512.15202)
*María Anguiano,Francisco J. Suárez-Grau*

Main category: math.AP

TL;DR: Derivation of generalized Reynolds equation for micropolar fluid flow in thin domain with oscillating rough boundary, showing roughness effects on pressure and providing expressions for average velocity and microrotation.


<details>
  <summary>Details</summary>
Motivation: To understand how surface roughness affects micropolar fluid flow in thin domains, particularly in the "Reynolds roughness regime" where domain thickness is much smaller than roughness wavelength.

Method: Mathematical analysis of micropolar fluid flow through thin domain with zero Dirichlet boundary condition on rapidly oscillating top boundary and non-standard conditions on flat bottom, using asymptotic methods in Reynolds roughness regime.

Result: Rigorous derivation of generalized Reynolds equation for pressure showing roughness-induced effects, along with explicit expressions for average velocity and microrotation.

Conclusion: Surface roughness significantly influences micropolar fluid flow in thin domains, and the derived generalized Reynolds equation captures these roughness effects mathematically, providing tools for analyzing such systems.

Abstract: In this paper, we study the asymptotic behavior of the micropolar fluid flow through a thin domain assuming zero Dirichlet boundary condition on the top boundary, which is rapidly oscillating, and non-standard boundary conditions on the flat bottom. Assuming ``Reynolds roughness regime", in which the thickness of the domain is very small compared to the wavelenth of the roughness (i.e. a very slight roughness), we rigorously derive a generalized Reynolds equation for pressure clearly showing the roughness-induced effects. Moreover, we give expressions for the average velocity and microrotation.

</details>


### [33] [A multiscale framework integrating within-host infection kinetics with airborne transmission dynamics](https://arxiv.org/abs/2512.15209)
*Andrew Omame,Sarafa Iyaniwura*

Main category: math.AP

TL;DR: Novel multiscale framework integrates within-host infection kinetics with airborne transmission using ODE-PDE coupling, enabling study of both individual infection dynamics and population-level spread in indoor environments.


<details>
  <summary>Details</summary>
Motivation: There's a need to couple within-host infection dynamics with population-level transmission for airborne pathogens, especially in indoor settings. Existing models treat these scales separately, lacking integrated frameworks that capture both individual infection kinetics and between-host transmission.

Method: Developed a multiscale mathematical framework that integrates ODEs for within-host infection kinetics with a diffusion-based PDE for airborne pathogen movement. Hosts are represented as patches, with scales linked through boundary conditions for viral shedding and inhalation. Used matched asymptotic analysis to derive a nonlinear ODE model from the coupled system.

Result: Successfully derived a reduced nonlinear ODE model that retains spatial heterogeneity through Neumann Green's functions. Established existence, uniqueness, and boundedness of solutions. Analyzed within-host infection kinetics as functions of diffusion rate and host spatial configuration. In well-mixed limit, recovers classical target cell limited viral dynamics.

Conclusion: The multiscale modeling approach provides a tractable yet biologically grounded framework for simultaneously studying transient within-host infection dynamics and population-level disease spread in indoor environments, addressing a major challenge in infectious disease modeling.

Abstract: Coupling within-host infection dynamics with population-level transmission remains a major challenge in infectious disease modeling, especially for airborne pathogens with potential to spread indoor. The frequent emergence of such diseases highlight the need for integrated frameworks that capture both individual-level infection kinetics and between-host transmission. While analytical models for each scale exist, tractable approaches that link them remain limited. In this study, we present a novel multiscale mathematical framework that integrates within-host infection kinetics with airborne transmission dynamics. The model represents each host as a patch and couples a system of ordinary differential equations (ODEs) describing in-host infection kinetics with a diffusion-based partial differential equation (PDE) for airborne pathogen movement in enclosed spaces. These scales are linked through boundary conditions on each patch boundary, representing viral shedding and inhalation. Using matched asymptotic analysis in the regime of intermediate diffusivity, we derived a nonlinear ODE model from the coupled ODE-PDE system that retains spatial heterogeneity through Neumann Green's functions. We established the existence, uniqueness, and boundedness of solutions to the reduced model and analyzed within-host infection kinetics as functions of the airborne pathogen diffusion rate and host spatial configuration. In the well-mixed limit, the model recovers the classical target cell limited viral dynamics framework. Overall, the proposed multiscale modeling approach enables the simultaneous study of transient within-host infection dynamics and population-level disease spread, providing a tractable yet biologically grounded framework for investigating airborne disease transmission in indoor environments.

</details>


### [34] [Strichartz estimates in Wiener amalgam spaces for Schrödinger equations with at most quadratic potentials](https://arxiv.org/abs/2512.15218)
*Shun Takizawa*

Main category: math.AP

TL;DR: Generalization of Strichartz estimates to Wiener amalgam spaces for Schrödinger equations with quadratically growing potentials


<details>
  <summary>Details</summary>
Motivation: Classical Strichartz estimates in Lebesgue spaces provide limited local-in-space regularity recovery. Wiener amalgam spaces offer stronger local regularity recovery, but existing results only cover specific potentials (V(x)=0, ±|x|²/2). Need to extend to broader class of quadratically growing potentials.

Method: Prove Strichartz estimates in Wiener amalgam spaces for Schrödinger equations with potentials growing at most quadratically at spatial infinity. Generalizes Cordero and Nicola's work from specific potentials to broader class.

Result: Successfully establish Strichartz estimates in Wiener amalgam spaces for Schrödinger equations with quadratically bounded potentials. Provides stronger local-in-space regularity recovery than classical Lebesgue space estimates.

Conclusion: Wiener amalgam spaces offer superior local regularity recovery for Schrödinger equations with quadratically growing potentials, extending previous limited results to broader potential classes.

Abstract: For Schrödinger equations with potentials which grow at most quadratically at spatial infinity, we prove Strichartz estimates in Wiener amalgam spaces. These estimates provide a stronger recovery of local-in-space regularity than the classical Strichartz estimates in Lebesgue spaces. Our result is a generalization of the results on Strichartz estimates in Wiener amalgam spaces by Cordero and Nicola, which are stated for the potentials $V(x) = 0,|x|^2/2, -|x|^2/2$.

</details>


### [35] [Linear systems, determinants and solutions of the Kadomtsev-Petviashvili equation](https://arxiv.org/abs/2512.15245)
*Gordon Blower,Simon J. Malham*

Main category: math.AP

TL;DR: The paper connects linear system theory with integrable systems, showing how Fredholm determinants of Hankel operators from scattering functions solve the KP equation.


<details>
  <summary>Details</summary>
Motivation: To establish connections between linear system theory (scattering functions, Hankel operators) and integrable systems theory (KP equation), and to provide effective numerical methods for solving KP via Fredholm determinants.

Method: Uses linear systems (-A,B,C) to define scattering functions φ(x)(t), constructs Hankel operators Γ_φ(x), studies algebras including R_x = ∫_x^∞ e^{-tA}BCe^{-tA}dt, identifies Pöppe's operators with shift actions on kernels, and expresses Pöppe's bracket via Fedosov product.

Result: Shows that Fredholm determinant τ(x) = det(I+Γ_φ(x)) solves KP equation, establishes properties of operator algebras, and demonstrates that det(I+R_x) provides effective numerical computation of KP solutions.

Conclusion: The paper successfully bridges linear system theory with integrable systems, providing both theoretical connections and practical computational methods for solving the KP equation via Fredholm determinants of Hankel operators.

Abstract: Let $(-A,B,C)$ be a linear system in continuous time $t>0$ with input and output space ${\mathbb C}$ and state space $H$. The scattering (or impulse response) functions $φ_{(x)}(t)=Ce^{-(t+2x)A}B$ determines a Hankel integral operator $Γ_{φ_{(x)}}$; if $Γ_{φ_{(x)}}$ is trace class, then the Fredholm determinant $τ(x)=\det (I+Γ_{φ_{(x)}})$ determines the tau function of $(-A,B,C)$. The paper establishes properties of algebras including $R_x = \int_x^\infty e^{-tA}BCe^{-tA}\,dt$ on $H$, and obtains solutions of the Kadomtsev-Petviashvili PDE. Pöppe's semi-additive operators are identified with orbits of a shift action on integral kernels, and Pöppe's bracket operation is expressed in terms of the Fedosov product. The paper shows that the Fredholm determinant $\det (I+R_x)$ gives an effective method for numerical computation of solutions of $KP$.

</details>


### [36] [Very weak solutions to degenerate parabolic double-phase systems](https://arxiv.org/abs/2512.15255)
*Wontae Kim,Lauri Särkiö*

Main category: math.AP

TL;DR: Local self-improving property proven for gradient of very weak solutions to degenerate parabolic double-phase systems via reverse Hölder inequality with solution-independent constants.


<details>
  <summary>Details</summary>
Motivation: To establish regularity properties for solutions to degenerate parabolic double-phase systems, which are important in PDE theory and applications involving materials with varying properties.

Method: Developed a new phase analysis method and used a reverse Hölder inequality with constants independent of the solution, avoiding self-referential arguments through delicate techniques.

Result: Proved a local self-improving property for the gradient of very weak solutions to degenerate parabolic double-phase systems.

Conclusion: The paper successfully establishes gradient regularity for solutions to complex degenerate parabolic systems through innovative phase analysis and careful avoidance of circular reasoning.

Abstract: We prove a local self-improving property for the gradient of very weak solutions to degenerate parabolic double-phase systems. The result is based on a reverse Hölder inequality with constants that are independent of the solution. Delicate methods are required to avoid a self-referential argument. In particular, we develop a new phase analysis method.

</details>


### [37] [Carleson-type removability for $p$-parabolic equations](https://arxiv.org/abs/2512.15277)
*Michał Borowski,Theo Elenius,Leah Schätzler,David Stolnicki*

Main category: math.AP

TL;DR: Characterization of removable sets for Hölder continuous solutions to degenerate parabolic p-growth equations using intrinsic parabolic Hausdorff measures.


<details>
  <summary>Details</summary>
Motivation: To understand which sets can be removed without affecting Hölder continuous solutions to degenerate parabolic equations, providing a complete characterization for this important class of PDEs.

Method: Develops a new method using fundamental properties of obstacle problems and supersolutions to prove sufficiency, and establishes Hölder continuity of solutions with measure data for necessity. Uses intrinsic parabolic Hausdorff measures that depend on Hölder exponent.

Result: Provides a sufficient and necessary condition for removable sets in terms of intrinsic parabolic Hausdorff measure. The method applies to general operators and offers new insights even for the classical case p=2.

Conclusion: The paper gives a complete characterization of removable sets for Hölder continuous solutions to degenerate parabolic p-growth equations, with a novel approach that simplifies proofs and provides new perspectives on the problem.

Abstract: We characterize removable sets for Hölder continuous solutions to degenerate parabolic equations of $p$-growth. A sufficient and necessary condition for a set to be removable is given in terms of an intrinsic parabolic Hausdorff measure, which depends on the considered Hölder exponent. We present a new method to prove the sufficient condition, which relies only on fundamental properties of the obstacle problem and supersolutions, and applies to a general class of operators. For the necessity of the condition, we establish the Hölder continuity of solutions with measure data, provided the measure satisfies a suitable decay property. The techniques developed in this article provide a new point of view even in the case $p=2$.

</details>


### [38] [Weak Error on the densities for the Euler scheme of stable additive SDEs with Besov drift](https://arxiv.org/abs/2512.15299)
*Mathis Fitoussi,Elena Issoglio,Stéphane Menozzi*

Main category: math.AP

TL;DR: Analysis of Euler-Maruyama discretization for SDEs with stable Lévy noise and distributional drift, establishing weak convergence rates for densities.


<details>
  <summary>Details</summary>
Motivation: To study numerical approximation of SDEs driven by symmetric isotropic α-stable processes (α∈(1,2)) with distributional drift coefficients, which arise in various applications but present analytical challenges due to the combination of Lévy noise and irregular drift.

Method: Define an appropriate Euler scheme for the SDE dX_t = b(t,X_t)dt + dZ_t, where Z is symmetric isotropic d-dimensional α-stable process and b belongs to a mixed Lebesgue-Besov space. Analyze weak error convergence rates on densities under parameter constraints that guarantee weak well-posedness.

Result: Obtain convergence rate for weak error on densities of the Euler-Maruyama discretization. The rate depends on the parameters of the problem (α, dimension d, and regularity parameters of the drift b).

Conclusion: The Euler-Maruyama scheme provides a viable numerical method for SDEs with stable Lévy noise and distributional drift, with quantifiable convergence rates under appropriate parameter constraints that ensure weak well-posedness.

Abstract: We are interested in the Euler-Maruyama dicretization of the formal SDE, $dX_t=b(t,X_t)dt+dZ_t$, where $Z$ is a symmetric isotropic d dimensional stable process of index $α\in (1,2)$, and $b$ is distributional. It belongs to a mix Lebesgue-Besov space. The associated parameters satisfy some constraints which guarantee weak-well posedness. Defining an appropriate Euler scheme, we obtain a convergence rate for the weak error on the densities. The rate depends on the parameters.

</details>


### [39] [A Nonhomogeneous Boundary-Value Problem For The Nonlinear KdV Equation on Star Graphs](https://arxiv.org/abs/2512.15307)
*Roberto de A. Capistrano Filho,Hugo Parada,Jandeilson Santos da Silva*

Main category: math.AP

TL;DR: Develops well-posedness theory for KdV equations on star-graph structures using s-compatibility framework, extending classical single-equation results to N-coupled equations.


<details>
  <summary>Details</summary>
Motivation: To extend classical KdV analysis from single equations to complex graph configurations, addressing the lack of comprehensive well-posedness theory for KdV equations with coupled boundary conditions on graphs.

Method: Introduces s-compatibility framework generalizing classical compatibility conditions to star-shaped graphs, combines analytical techniques with fixed-point arguments to establish global well-posedness at H^s level.

Result: Establishes sharp global well-posedness for both linear and nonlinear KdV problems on star-graphs, providing first comprehensive theory for such coupled boundary conditions on graphs.

Conclusion: Successfully extends classical KdV analysis to star-shaped graphs, solving several open problems and laying foundation for future control studies on graph structures.

Abstract: This paper investigates a boundary-value problem for the Korteweg-de Vries (KdV) equation on a star-graph structure. We develop a unified framework introducing the notion of $s$-compatibility, which generalizes classical compatibility conditions to star-shaped and more complex graph configurations, inspired by the works of Bona, Sun, and Zhang [14]. By combining analytical techniques with a fixed-point argument, we establish sharp global well-posedness for both the linear and nonlinear problems at the $H^s$ level. In this setting, our results extend the classical analysis for a single KdV equation [14] to star-shaped graphs composed of $N$ equations. These results provide the first comprehensive well-posedness theory for KdV equations with coupled boundary conditions on graphs. Although control issues are not treated in this article, the analytic results obtained here address several open problems, which will be addressed in a forthcoming

</details>


### [40] [Weak curvature conditions on metric graphs](https://arxiv.org/abs/2512.15329)
*Juliane Krautz*

Main category: math.AP

TL;DR: The paper establishes equivalence between three weak characterizations of lower curvature bounds on metric graphs using gradient estimates, regularization techniques, and Cheeger energy representation.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive understanding of weak lower curvature bounds on metric graphs by connecting different mathematical characterizations that have been studied separately in different contexts.

Method: Starting from pointwise gradient estimates for the heat semigroup, the authors prove equivalence between: 1) weak Bakry-Émery curvature condition, 2) weak Evolutionary Variational Inequality, and 3) weak geodesic convexity. The proof uses careful regularization of absolutely continuous curves and explicit representation of Cheeger energy.

Result: Established equivalence between three different weak characterizations of lower curvature bounds on metric graphs, providing a unified framework for understanding curvature properties in this setting.

Conclusion: The paper successfully connects different weak curvature characterizations on metric graphs, with potential applications to the Schrödinger bridge problem, demonstrating the utility of these unified characterizations in geometric analysis on metric graphs.

Abstract: Starting from pointwise gradient estimates for the heat semigroup, we study three characterizations of weak lower curvature bounds on metric graphs. More precisely, we prove the equivalence between a weak notion of the Bakry-Émery curvature condition, a weak Evolutionary Variational Inequality and a weak form of geodesic convexity. The proof is based on a careful regularization of absolutely continuous curves together with an explicit representation of the Cheeger energy. We conclude with a brief discussion on possible applications to the Schrödinger bridge problem on metric graphs.

</details>


### [41] [Trace theory for parabolic boundary value problems with rough boundary conditions](https://arxiv.org/abs/2512.15382)
*Robert Denk,Floris B. Roodenburg*

Main category: math.AP

TL;DR: Characterization of trace spaces for weighted Sobolev spaces with boundary distance weights, applied to prove well-posedness for heat equation with rough boundary data in C^{1,κ} domains.


<details>
  <summary>Details</summary>
Motivation: Boundary value problems often involve solutions whose derivatives blow up at the boundary. Weighted Sobolev spaces with boundary distance weights are suitable for such problems, but their trace theory needs development for applications to PDEs with rough boundary data.

Method: Characterize trace spaces arising from intersections of weighted, vector-valued Sobolev spaces where weights are powers of distance to boundary. Apply this trace theory to analyze the heat equation with inhomogeneous boundary data in domains of regularity C^{1,κ}.

Result: Complete characterization of trace spaces for these weighted function spaces. Proof of well-posedness for heat equation with rough inhomogeneous boundary data in Sobolev spaces of higher regularity in C^{1,κ} domains.

Conclusion: The developed trace theory for weighted Sobolev spaces enables rigorous treatment of boundary value problems with singular solutions and rough boundary data, particularly for the heat equation in domains with limited regularity.

Abstract: We characterise the trace spaces arising from intersections of weighted, vector-valued Sobolev spaces, where the weights are powers of the distance to the boundary. These weighted function spaces are particularly suitable for treating boundary value problems where derivatives of the solution blow up at the boundary. As an application of our trace theory, we prove well-posedness for the heat equation with rough inhomogeneous boundary data in Sobolev spaces of higher regularity in domains of fixed regularity $C^{1,κ}$, with $κ\in [0,1)$.

</details>


### [42] [On a relaxed Cahn-Hilliard tumour growth model with single-well potential and degenerate mobility](https://arxiv.org/abs/2512.15426)
*Cecilia Cavaterra,Matteo Fornoni,Maurizio Grasselli,Benoît Perthame*

Main category: math.AP

TL;DR: Existence of weak solutions for a phase-field tumor growth model with degenerate mobility and singular potential, via elliptic regularization.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical existence results for a complex tumor growth model that combines Cahn-Hilliard dynamics with nutrient transport, proliferation, consumption, and chemotaxis.

Method: Introduce elliptic regularization with parameter δ>0 to the chemical potential equation, prove existence for the regularized system, then take δ→0 limit to obtain existence for original degenerate system.

Result: Proved existence of weak solutions for both the regularized system and the original tumor growth model through limiting arguments.

Conclusion: The elliptic regularization technique successfully establishes existence of weak solutions for the challenging tumor growth model with degenerate mobility and singular potential.

Abstract: We consider a phase-field system modelling solid tumour growth. This system consists of a Cahn-Hilliard equation coupled with a nutrient equation. The former is characterised by a degenerate mobility and a singular potential. Both equations are subject to suitable reaction terms which model proliferation and nutrient consumption. Chemotactic effects are also taken into account. Adding an elliptic regularisation, depending on a relaxation parameter $δ>0$, in the equation for the chemical potential, we prove the existence of a weak solution to an initial and boundary value problem for the relaxed system. Then, we let $δ$ go to zero, and we recover the existence of a weak solution to the original system.

</details>


### [43] [Capillary $L_p$-Christoffel-Minkowski problem](https://arxiv.org/abs/2512.15464)
*Yingxiang Hu,Mohammad N. Ivaki*

Main category: math.AP

TL;DR: The paper solves the capillary L_p-Christoffel-Minkowski problem for even hypersurfaces in half-space when 1<p<k+1, extending previous capillary Christoffel-Minkowski results.


<details>
  <summary>Details</summary>
Motivation: To extend the capillary Christoffel-Minkowski problem to the L_p setting for hypersurfaces in half-space, building on previous work by HIS25.

Method: Uses a non-collapsing estimate approach that provides lower bounds for both height and capillary support function, working within the class of even hypersurfaces.

Result: Successfully solves the capillary L_p-Christoffel-Minkowski problem in half-space for the parameter range 1<p<k+1 for even hypersurfaces.

Conclusion: The paper extends capillary Christoffel-Minkowski existence results to the L_p setting, with the non-collapsing estimate being a key technical tool for the proof.

Abstract: We solve the capillary $L_p$-Christoffel--Minkowski problem in the half-space for $1<p<k+1$ in the class of even hypersurfaces. A crucial ingredient is a non-collapsing estimate that yields lower bounds for both the height and the capillary support function. Our result extends the capillary Christoffel--Minkowski existence result of \cite{HIS25}.

</details>


### [44] [A Nonlinear elliptic PDE with curve singularity on the boundary](https://arxiv.org/abs/2512.15475)
*Mamadou Ciss,Abdourahmane Diatta,El Hadji Abdoulaye Thiam*

Main category: math.AP

TL;DR: This paper studies the existence of positive H¹-solutions to a Hardy-Sobolev trace type equation on bounded domains with smooth boundary, where the nonlinear boundary condition involves a distance function to a submanifold and critical Hardy-Sobolev exponent.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how the existence of solutions to this type of Hardy-Sobolev trace equation depends on the local geometry of the domain boundary and the submanifold Σ, particularly near the point 0. This extends previous work on Hardy-Sobolev problems to the trace setting with geometric dependencies.

Method: The authors study a PDE with Laplace operator in the domain and a nonlinear boundary condition involving the distance function ρ_Σ to a closed submanifold Σ. They consider the critical Hardy-Sobolev exponent q_s = 2(N-s)/(N-1) for 0 ≤ s < 1. The analysis likely involves variational methods, geometric analysis, and possibly concentration-compactness arguments to handle the critical exponent.

Result: The paper establishes conditions for existence of positive H¹(Ω)-solutions, showing that existence depends on the local geometry of ∂Ω and Σ at 0, as well as the shapes of Ω and its boundary ∂Ω. The specific geometric conditions that guarantee existence are the main results.

Conclusion: The existence of solutions to Hardy-Sobolev trace type equations is sensitive to geometric features of the domain boundary and the submanifold Σ. The local geometry at specific points (like 0) plays a crucial role in determining whether positive solutions exist.

Abstract: Let $Ω$ be a bounded domain of $\mathbb{R}^{N+1}$ ($N \geq 3$) with smooth boundary $\partial Ω$ and $Σ$ be a closed submanifold contained on $\partial Ω$ and containing $0$. We are interesting in the existence of positive $H^1(Ω)$-solution of the following Hardy-Sobolev trace type equation \begin{equation*} \begin{cases} -Δu+u=0 \qquad & \textrm{ in $Ω$}\\\\ \displaystyle\frac{\partial u}{\partial ν}= ρ_Σ^{-s} u^{q_s-1} \qquad & \textrm{ on $\partial Ω$}, \end{cases} \end{equation*} where $ν$ is the unit outer normal of $\partial Ω$, $ρ_Σ: \partial Ω\to \mathbb{R}$ is the distance function in $\partial Ω$ to the curve $Σ$: $$ ρ_Σ(x):= \inf_{y \in Σ} d_{\tilde{g}}(x, y) $$ and for $0\leq s <1$, $q_s:=\frac{2(N-s)}{N-1}$ is the critical Hardy-Sobolev exponent. The existence of solution may depend on the local geometry of the boundary $\partial Ω$ and $Σ$ at $0$ or in the shapes of the domain $Ω$ and its boundary $\partial Ω$.

</details>


### [45] [A plethora of fully localised solitary waves for the full-dispersion Kadomtsev-Petviashvili equation](https://arxiv.org/abs/2512.15487)
*Mats Ehrnström,Mark D. Groves*

Main category: math.AP

TL;DR: The paper proves existence of fully localized solitary waves (lumps) for the full-dispersion KP-I equation using perturbation methods from the KP-I equation.


<details>
  <summary>Details</summary>
Motivation: The KP-I equation models gravity-capillary waves with strong surface tension (Bond number β>1/3) and has known lump solitary waves. The full-dispersion KP-I (FDKP-I) equation retains exact dispersion from water-wave problems, and its existence of similar localized solutions needs investigation.

Method: The authors treat FDKP-I as a perturbation of the KP-I equation and apply a suitable variant of the implicit-function theorem to prove existence of symmetric fully localized solitary waves.

Result: The paper establishes that the FDKP-I equation also possesses a family of symmetric fully localized solitary waves (lumps), extending the known results from KP-I to the more physically accurate full-dispersion version.

Conclusion: The full-dispersion KP-I equation inherits the existence of localized lump solutions from the KP-I equation, confirming that these physically important wave structures persist when exact dispersion is retained in the model.

Abstract: The KP-I equation arises as a weakly nonlinear model equation for gravity-capillary waves with Bond number $β>1/3$, also called strong surface tension. This equation has recently been shown to have a family of nondegenerate, symmetric `fully localised' or `lump' solitary waves which decay to zero in all spatial directions. The full-dispersion KP-I equation is obtained by retaining the exact dispersion relation in the modelling from the water-wave problem. In this paper we show that the FDKP-I equation also has a family of symmetric fullly localised solitary waves which are obtained by casting it as a perturbation of the KP-I equation and applying a suitable variant of the implicit-function theorem.

</details>


### [46] [Effective Equations for a Compressible Liquid-Vapor Flow Model with Highly Oscillating Initial Density](https://arxiv.org/abs/2512.15535)
*Christian Rohde,Florian Wendt*

Main category: math.AP

TL;DR: Derivation of effective model for compressible viscous liquid-vapor flow with many phase boundaries using Young measure convergence from Navier-Stokes-Korteweg system.


<details>
  <summary>Details</summary>
Motivation: Need effective models for spray-like flows with many phase boundaries where direct simulation of all interfaces is computationally infeasible.

Method: Start from parabolic relaxation of Navier-Stokes-Korteweg system, consider sequence of oscillating initial densities mimicking many phase boundaries, analyze convergence of weak solutions using Young measures.

Result: Effective model consists of deterministic hydrodynamic equations plus kinetic equation for Young measure describing mixing dynamics, reformulated via cumulative distribution function.

Conclusion: Derived mathematically justified effective model for multi-phase flows with many interfaces using Young measure theory, making problem tractable for numerical approximation.

Abstract: We derive and justify a new effective model for a compressible viscous liquid-vapor flow on a spray-like scale, i.e., for settings with a large number of phase boundaries. As a model on the detailed scale, we start from a parabolic relaxation of the Navier-Stokes-Korteweg system. We consider a sequence of initial data where the sequence of initial densities is assumed to be highly oscillating mimicking the high number of phase boundaries initially. Then, we consider a sequence of finite energy weak solutions corresponding to the sequence of initial data. Anticipating that the effective equations are found in the limit of infinitely many initial phase changes, we interpret the densities as Young measures and prove the convergence of the sequence of solutions to the effective model. The effective model consists of a deterministic part for the fluid's hydrodynamic quantities and a kinetic equation for the limit Young measure encoding the mixing dynamics. By characterizing the Young measure with the corresponding cumulative distribution function, we rewrite the kinetic equation for the Young measure into a kinetic equation for the cumulative distribution function such that the resulting equations are accessible by standard approximation methods.

</details>


### [47] [Vanishing viscosity limit for $n\times n$ hyperbolic system of conservation laws in 1-d with nonlinear viscosity: Part-I Uniform BV estimates](https://arxiv.org/abs/2512.15620)
*Boris Haspot,Animesh Jana*

Main category: math.AP

TL;DR: Proves global uniform BV bounds for parabolic approximations of hyperbolic conservation laws with commuting viscosity and flux matrices.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous stability results for parabolic regularizations of hyperbolic conservation laws, which are important for numerical analysis and understanding viscous approximations.

Method: Analyzes 1-D parabolic system with non-singular viscosity matrix B(u) and strictly hyperbolic flux A(u), requiring commuting condition A(u)B(u) = B(u)A(u). Uses BV space techniques and small initial data assumption.

Result: Proves global in time uniform BV bounds for solutions when ε>0, provided initial data is small in BV and matrices A(u) and B(u) commute.

Conclusion: The commuting condition between flux and viscosity matrices enables global BV stability for parabolic approximations of hyperbolic conservation laws with small initial data.

Abstract: We consider the following parabolic approximation for hyperbolic system of conservation laws in 1-D with non-singular viscosity matrix $B(u)$ and $A(u)$ strictly hyperbolic, \[u_t+A(u)u_x = \varepsilon(B(u)u_x)_x.\] We prove global in time uniform $BV$ bound for solution to this parabolic system when $\varepsilon>0$ provided that the initial data is small in $BV$ and the matrix $A(u)$ and $B(u)$ commutate.

</details>


### [48] [What does it mean for a 3D star-shaped scatterer to be small in the time domain?](https://arxiv.org/abs/2512.15629)
*Maryna Kachanovska,Adrian Savchuk*

Main category: math.AP

TL;DR: Time-domain wave scattering by small obstacles can be modeled without temporal error deterioration, contrary to recent suggestions.


<details>
  <summary>Details</summary>
Motivation: Recent works suggest time-domain scattering by small obstacles involves an additional temporal scale, but this paper argues this isn't necessary and aims to construct asymptotic models with time-stable errors.

Method: Focuses on sound-soft scattering problem by star-shaped obstacles in 3D, constructing asymptotic models that maintain accuracy without temporal error growth.

Result: Demonstrates possibility of constructing asymptotic models where error does not deteriorate over time, at least for the specific case studied.

Conclusion: Time-domain scattering by small obstacles can be modeled without the additional temporal scale suggested by recent works, enabling time-stable asymptotic approximations.

Abstract: In the frequency domain wave scattering problems, obstacles can be effectively replaced by point scatterers as soon as the wavelength of the incident wave exceeds significantly their diameter. The situation is less clear in the time domain, where recent works suggest the presence of an additional temporal scale that quantifies the smallness of the obstacle. In this paper we argue that this is not necessarily the case, and that it is possible to construct asymptotic models with an error that does not deteriorate in time, at least in the case of a sound-soft scattering problem by a star-shaped obstacle in 3D.

</details>


### [49] [Variational solutions of the Dirichlet problem, Lebesgue's cusp and non-local properties](https://arxiv.org/abs/2512.15682)
*Wolfgang Arendt,Daniel Daners,Manfred Sauter*

Main category: math.AP

TL;DR: The paper analyzes variational solutions for Dirichlet problems, showing they equal Perron solutions, and examines limitations in determining classical solution existence from boundary regularity.


<details>
  <summary>Details</summary>
Motivation: To understand the properties of variational solutions for Dirichlet problems with continuous boundary data, and to investigate the relationship between boundary regularity and existence of classical solutions.

Method: Establishes properties of variational solutions, proves equivalence with Perron solutions, analyzes Lebesgue's cusp/domain (associated with thin rod potential), and examines generic non-local properties of Perron solution continuity.

Result: Variational solutions coincide with Perron solutions; they can be characterized by boundary functions when finite energy, but boundary regularity alone cannot determine classical solution existence; non-continuity of Perron solutions at singular points is generic and non-local.

Conclusion: Variational solutions provide an equivalent framework to Perron solutions for Dirichlet problems, but there are fundamental limitations in predicting classical solution existence from boundary function regularity alone.

Abstract: A recent result from [AtES24] allows one to define variational solutions of the Dirichlet problem for general continuous boundary data. We establish basic properties of this notion of solution and show that it coincides with the Perron solution. Variational solutions can elegantly be characterised in terms of the given boundary function when the variational solution has finite energy. However, it is impossible to decide in terms of the regularity of the given boundary function when a classical solution exists. We demonstrate this by analysing Lebesgue's cusp, and more precisely Lebesgue's domain which is associated with the potential of a thin rod with mass density going to zero at one end. We also show that the non-continuity of the Perron solution at a singular point is a generic and non-local property.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [50] [Design of a specimen to train path-dependent deep learning material models from a single uniaxial test: eliciting strain diversity via automatically differentiable elastoplastic topology optimization](https://arxiv.org/abs/2512.14963)
*Shunyu Yin,Bernardo P. Ferreira,Gawel Kus,Miguel A. Bessa*

Main category: physics.comp-ph

TL;DR: Neural networks for material modeling can be trained from a single topology-optimized specimen under simple uniaxial loading instead of requiring many complex experiments.


<details>
  <summary>Details</summary>
Motivation: Training neural networks for nonlinear, path-dependent material behavior typically requires large, diverse datasets from synthetic simulations or many complex experiments, which hinders practical adoption in real-world material testing.

Method: Use automatically differentiable elastoplastic topology optimization to design a single specimen that generates diverse stress-strain states under simple uniaxial loading, then employ automatically differentiable model updating (ADiMU) to train neural network surrogates.

Result: The approach successfully trains large neural networks from a single topology-optimized specimen, substantially reducing the experimental burden for data-driven material modeling.

Conclusion: Topology-optimized specimens under simple loading can effectively train neural networks for material behavior, making data-driven material modeling more practical by reducing experimental requirements.

Abstract: Artificial neural networks accurately learn nonlinear, path-dependent material behavior. However, training them typically requires large, diverse datasets, often created via synthetic unit cell simulations. This hinders practical adoption because physical experiments on standardized specimens with simple geometries fail to generate sufficiently diverse stress-strain trajectories. Consequently, an unreasonably large number of experiments or complex multi-axial tests would be needed. This work shows that such networks can be trained from a single specimen subjected to simple uniaxial loading, by designing the specimen using a novel automatically differentiable elastoplastic topology optimization method. Our strategy diversifies the stress-strain states observed in a single test involving plastic deformation. We then employ the automatically differentiable model updating (ADiMU) method to train the neural network surrogates. This work demonstrates that topology-optimized specimens under simple loading can train large neural networks, thereby substantially reducing the experimental burden associated with data-driven material modeling.

</details>


### [51] [UGKS and UGKWP Methods for Multiscale Simulation of Electrostatic Plasma in Quasineutral and Hydrodynamic Limits](https://arxiv.org/abs/2512.15406)
*Zhigang Pu,Kun Xu*

Main category: physics.comp-ph

TL;DR: Extends UGKS and UGKWP methods for electrostatic plasma modeling to handle both Debye length and mean free path asymptotic limits, removing resolution constraints in hydrodynamic and quasineutral regimes.


<details>
  <summary>Details</summary>
Motivation: Existing plasma modeling methods face constraints from both Debye length and mean free path, limiting their efficiency and applicability across different plasma regimes. The standard Poisson equation becomes inefficient in quasineutral regimes.

Method: Extends Unified Gas-Kinetic Scheme (UGKS) and Unified Gas-Kinetic Wave-Particle (UGKWP) methods by coupling collision and transport processes within numerical flux, and introduces reformulated Poisson equation coupled with macroscopic moment equations.

Result: Methods accurately capture plasma dynamics across hydrodynamic and quasineutral regimes without resolution constraints from Debye length or mean free path, verified through benchmark tests including linear/nonlinear Landau damping and bump-on-tail instability.

Conclusion: The proposed schemes provide robust, asymptotically consistent plasma modeling that overcomes traditional resolution constraints, enabling efficient simulation across different plasma regimes while maintaining accuracy.

Abstract: This study extends the Unified Gas-Kinetic Scheme (UGKS) and the Unified Gas-Kinetic Wave-Particle (UGKWP) method for electrostatic plasma modeling, ensuring the correct asymptotic limits with respect to both the Debye length and the mean free path. By coupling collision and transport processes within the numerical flux, the proposed approach effectively removes the hydrodynamic-limit constraint associated with the mean free path. In addition, a reformulated Poisson equation, coupled with the macroscopic moment equations, is introduced to overcome the inefficiency of the standard Poisson formulation in the quasineutral regime. The accuracy and asymptotic consistency of the proposed schemes are verified through several benchmark tests, including linear and nonlinear Landau damping and the bump-on-tail instability. The results demonstrate that the methods robustly capture plasma dynamics across hydrodynamic and quasineutral regimes, without resolution constraints imposed by either the Debye length or the mean free path.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [52] [Permanent magnet optimization of stellarators with coupling from finite permeability and demagnetization effects](https://arxiv.org/abs/2512.14997)
*Armin Ulrich,Mason Haberle,Alan A. Kaptanoglu*

Main category: physics.plasm-ph

TL;DR: GPMOmr extends greedy permanent magnet optimization with macromagnetic refinement to account for finite-permeability effects and magnet interactions in stellarator designs, achieving similar performance to classical GPMO while producing more realistic magnetization patterns.


<details>
  <summary>Details</summary>
Motivation: Previous GPMO methods assumed ideal rigid-remanence models, but real permanent magnets have finite permeability and exhibit magnet-magnet/magnet-coil coupling effects that can significantly impact magnetic field accuracy in stellarator designs.

Method: GPMOmr introduces a block-level macromagnetic model that accounts for finite permeability and demagnetizing interactions. This model is embedded in the greedy optimization loop to refine permanent magnet arrays while considering realistic magnetic coupling effects.

Result: Finite-permeability effects cause degree-scale tilts and few-percent magnitude changes in individual magnets, increasing the standard squared-flux objective by more than a factor of two for fixed layouts. GPMOmr achieves flux error histories and final errors within a few percent of classical GPMO while producing more nonuniform magnetization patterns.

Conclusion: GPMOmr provides a fast, practical tool for quantifying and incorporating finite-permeability effects in permanent-magnet stellarator designs, offering a framework for extending optimization to higher field strengths and materials with stronger macromagnetic coupling.

Abstract: Permanent magnets provide an attractive path for shaping university-scale stellarator magnetic fields. Previous work has shown that greedy permanent magnet optimization (GPMO) can produce sparse, grid-aligned arrays that match target surfaces with high accuracy under an ideal rigid-remanence model. Here we extend this approach to a greedy permanent magnet optimization with macromagnetic refinement (GPMOmr) by introducing a block-level macromagnetic model that accounts for magnet-magnet and magnet-coil coupling from finite permeability and demagnetizing interactions, and apply it to the published magnet grid from the MUSE stellarator design. Finite-permeability effects produce degree-scale tilts and few-percent magnitude changes in individual magnets and modify the surface-normal field $\mathbf B\cdot\mathbf n$ only at the percent level, yet for a fixed layout they increase the standard squared-flux objective by more than a factor of two. When the same model is embedded in the greedy loop, GPMOmr achieves $f_B$ histories and final errors within a few percent of classical GPMO while producing visibly more nonuniform magnetization patterns. Our formulation provides a fast and practical tool for quantifying and incorporating finite-permeability effects in permanent-magnet stellarator designs, and offers a framework for extending permanent-magnet optimization to higher field strengths and to materials with stronger macromagnetic coupling.

</details>


### [53] [Effects of Unequal Electron-Ion Plasma Beta on Pressure-Strain Interaction in Turbulent Plasmas](https://arxiv.org/abs/2512.15025)
*M. Hasan Barbhuiya,Subash Adhikari*

Main category: physics.plasm-ph

TL;DR: The paper investigates how initial electron-ion temperature imbalance affects turbulent heating mechanisms in space plasmas through pressure-strain interaction analysis in kinetic simulations.


<details>
  <summary>Details</summary>
Motivation: To understand how unequal electron-ion temperatures in weakly collisional space plasmas modify the pathways through which turbulence increases each species' internal energy, which is largely unexplored despite being common in space environments.

Method: Five fully kinetic two-dimensional particle-in-cell simulations of undriven decaying turbulence with varying initial electron-to-ion temperature ratios, analyzing species' internal energy density alongside decomposition of pressure-strain term (focusing on volume-preserving deformation with normal and shear contributions).

Result: Electron internal energy changes are governed primarily by shear deformation power density concentrated in electron-scale current sheets, while ion shear and normal deformation components cancel, yielding smaller net deformation power density peaking around electron-scale structures. Initial temperature imbalance modifies energy conversion channels but preserves qualitative trends.

Conclusion: Thermal disequilibrium shapes species-dependent turbulent heating rates measured via pressure-strain interaction, with shear deformation part providing good approximation, offering framework for interpreting energy evolution in space plasmas with unequal species temperatures.

Abstract: A common occurrence in weakly collisional space plasmas is the unequal electron-ion temperatures. The pressure-strain interaction provides a mechanism-agnostic pathway for increasing plasma internal energy through spatiotemporally local isotropic compression and volume preserving deformation, yet its behavior under thermal disequilibrium is largely unexplored. We investigate this using five fully kinetic two-dimensional particle-in-cell simulations of undriven decaying turbulence by varying the initial electron-to-ion temperature ratio. By analyzing the species' internal energy density alongside a decomposition of the pressure-strain term, with a focus on the volume-preserving deformation that contains normal and shear contributions, we quantify how the initial temperature imbalance modifies the channels through which turbulence increases each species' internal energy density. The cumulative pressure-strain interaction tracks the change in internal energy for both electrons and ions, with the total deformation channel of energy conversion dominating. We discover that local changes to electron internal energy density are governed primarily by the shear deformation power density, concentrated in electron-scale current sheets, while the ion shear and normal deformation components cancel, yielding a much smaller net deformation power density that peaks around, rather than within, those electron-scale current structures. We find that the amplitudes and localization of deformation change, but preserve these qualitative trends. Together, these results show how thermal disequilibrium could shape species-dependent turbulent "heating rate", measured via pressure-strain interaction and approximated via only its shear deformation part, and provide a framework for interpreting energy evolution and conversion in space plasmas where unequal species temperature is the norm.

</details>


### [54] [MHD Simulation Study on Impurity Assimilation Efficiency and Disruption Dynamics during Shattered Pellet Injection](https://arxiv.org/abs/2512.15271)
*Jinqiang Mao,Ping Zhu,Shiyong Zeng*

Main category: physics.plasm-ph

TL;DR: 3D MHD simulations of Shattered Pellet Injection in J-TEXT-like tokamak reveal optimal parameters for disruption mitigation: slower fragment velocity, finer fragments, mixed D-Ne composition, and higher thermal conductivity improve impurity assimilation and cooling efficiency.


<details>
  <summary>Details</summary>
Motivation: SPI is crucial for mitigating plasma disruptions in fusion devices, but optimizing its efficiency requires understanding the complex interaction between impurity dynamics and MHD response, which current knowledge lacks.

Method: Performed 3D nonlinear MHD simulations of SPI-induced disruption in a J-TEXT-like tokamak using NIMROD code, systematically examining key parameters: fragment velocity/fineness, injection quantity, impurity composition, injection location, multiple injectors, resistivity, and parallel thermal conductivity.

Result: Slower fragment velocity enhances impurity assimilation and amplifies MHD activity; finer fragments increase ablation and cooling efficiency; mixed D-Ne pellets elevate electron density without compromising radiative cooling; higher thermal conductivity reduces radiation asymmetry; higher resistivity leads to stronger magnetic perturbations and current spikes.

Conclusion: The findings provide physical bases for optimizing SPI schemes in future tokamak devices by identifying key parameter effects on impurity assimilation, cooling efficiency, and MHD response during disruption mitigation.

Abstract: Shattered Pellet Injection (SPI) has become a critical technique for mitigating plasma disruptions in fusion devices, yet optimizing its efficiency demands a proper understanding of the interaction between impurity dynamics and MHD response. We perform 3D nonlinear MHD simulations of SPI-induced disruption in a J-TEXT-like tokamak using the NIMROD code, systematically examining key parameters: fragment velocity and fineness, injection quantity, impurity composition, injection location and multiple injectors, resistivity, and parallel thermal conductivity. We find that slower fragment velocity enhances impurity assimilation and amplifies MHD activity. Finer fragments significantly increase impurity ablation and cooling efficiency. Mixed deuterium-neon pellets effectively elevate electron density without compromising radiative cooling efficiency. Plasma poloidal rotation affects ablation and cooling efficiency, whereas toroidally uniform multi-pellet injection enhances impurity ablation by nearly a factor equal to the number of pellets and lowers radiation asymmetry. Higher plasma parallel thermal conductivity results in higher radiation cooling efficiency in parallel directions, enhances impurity transport, and reduces toroidal peaking factor (TPF) of radiation. Variations in resistivity significantly influence Ohmic heating, impurity deposition and current dynamics after TQ, with higher resistivity leading to stronger magnetic perturbations and more pronounced current spikes. These findings provide physical bases for optimizing SPI schemes in future tokamak devices.

</details>


### [55] [First implementation of AXUV-based analysis and macro-instability diagnostics on WHAM](https://arxiv.org/abs/2512.15360)
*K. Shih,D. Endrizzi,D. A. Sutherland,J. Anderson,D. Bindl,E. L. Claveau,C. Everson,J. Eickman,S. J. Frank,E. Marriott,E. Penne,J. Pizzo,T. Qian,J. Viola,C. B. Forest,D. Yakovlev*

Main category: physics.plasm-ph

TL;DR: First implementation of AXUV diode array analysis on WHAM mirror plasma, enabling real-time assessment of macroscopic instabilities via statistical moments and instability parameter χ.


<details>
  <summary>Details</summary>
Motivation: AXUV diode arrays are widely used in fusion experiments for plasma radiation measurements, but their potential for real-time macroscopic instability assessment in magnetic mirror plasmas hasn't been demonstrated.

Method: Used a single 20-channel calibrated AXUV assembly with 100 kHz temporal resolution and ~1 cm spatial accuracy to measure line-integrated plasma emission. Processed data to obtain statistical moments (centroid displacement Φ(t) and effective radius R(t)), then defined macroscopic instability parameter χ(t) from joint covariance.

Result: Successfully implemented AXUV-based analysis framework on WHAM. Parameter χ serves as compact indicator of global macroscopic instability, decreasing with increasing end-plate bias and showing strong anti-correlation with diamagnetic flux during confinement transitions.

Conclusion: A single AXUV array can provide quantitative, real-time assessment of macroscopic plasma instabilities, representing first demonstration of such capability in magnetic mirror plasma. Future work with multiple arrays will enhance spatial coverage and enable full-mode tracking.

Abstract: Absolute extreme ultraviolet (AXUV) diode arrays are widely used in fusion experiments for time-resolved measurements of plasma radiation. We report the first implementation of an AXUV-based analysis framework on the Wisconsin High-Temperature Superconducting (HTS) Axisymmetric Mirror (WHAM). A single, precisely calibrated 20-channel AXUV assembly measures line-integrated plasma emission with $ 100~\mathrm{kHz}$ temporal resolution and $\sim1~\mathrm{cm}$ spatial accuracy across the mid-plane. The data were processed to obtain plasma's statistical moments, yielding time-resolved measurement of the centroid displacement $Φ(t)$ and effective radius $R(t)$. From the joint covariance of these quantities, we define a macroscopic instability parameter $χ(t)$, that quantifies large-scale plasma motion and profile evolution directly from AXUV observables. The parameter $χ$ serves as a compact indicator of global macroscopic instability, decreasing with increasing end-plate bias and exhibiting strong anti-correlation with diamagnetic flux during confinement transitions. These results demonstrate that a single AXUV array can provide quantitative, real-time assessment of macroscopic plasma instabilities, constituting the first demonstration of such capability in a magnetic mirror plasma. Future extensions to multiple arrays will further enhance spatial coverage and enable full-mode tracking in axisymmetric mirror configurations and related fusion devices.

</details>


### [56] [Radial electric field and density fluctuations measured by Doppler reflectometry during the post-pellet enhanced confinement phase in W7-X](https://arxiv.org/abs/2512.15576)
*T. Estrada,D. Carralero,T. Windisch,E. Sánchez,J. M. García-Regaña,J. Martínez-Fernández,A. de la Peña,J. L. Velasco,J. A. Alonso,M. Beurskens,S. Bozhenkov,H. Damm,G. Fuchert,R. Kleiber,N. Pablant,E. Pasch*

Main category: physics.plasm-ph

TL;DR: Radial electric field profiles and density fluctuations measured in W7-X stellarator during post-pellet enhanced confinement phase show strong E_r wells and reduced turbulence, with good agreement to neoclassical simulations and gyrokinetic analysis pointing to profile evolution and E_r stabilization as key factors.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind enhanced confinement in W7-X stellarator after pellet injection, particularly the role of radial electric field profiles and turbulence suppression in different magnetic configurations and heating conditions.

Method: Used Doppler reflectometry to measure radial profiles of density fluctuations and E_r field during post-pellet phase. Compared experimental E_r profiles with neoclassical simulations (DKES, KNOSOS codes). Conducted gyrokinetic simulations (stella, EUTERPE codes) to analyze turbulence stabilization mechanisms.

Result: Measured pronounced E_r wells up to -40 kV/m at ρ~0.7-0.8, scaling with plasma density and ECH power. Density fluctuations decrease more in post-pellet phase than gas-fuelled plasmas, and lower in high iota configuration. Simulations show good E_r profile agreement with neoclassical predictions and identify profile evolution and E_r stabilization as dominant turbulence suppression factors.

Conclusion: Enhanced confinement in W7-X post-pellet phase is driven by strong radial electric field wells that suppress turbulence, with the effect enhanced in high iota magnetic configurations. The stabilization results from both plasma profile evolution after pellet injection and the direct stabilizing effect of the E_r profile itself.

Abstract: Radial profiles of density fluctuations and radial electric field, $E_r$, have been measured using Doppler reflectometry during the post-pellet enhanced confinement phase achieved, under different heating power levels and magnetic configurations, along the 2018 W7-X experimental campaign. A pronounced $E_r$-well is measured with local values as high as -40 kV/m in the radial range $ρ\sim 0.7-0.8$ during the post-pellet enhanced confinement phase. The maximum $E_r$ intensity scales with both plasma density and Electron Cyclotron Heating (ECH) power level following a similar trend as the plasma energy content. A good agreement is found when the experimental $E_r$ profiles are compared to simulations carried out using the neoclassical codes DKES and KNOSOS. The density fluctuation level decreases from the plasma edge toward the plasma core and the drop is more pronounced in the post-pellet enhanced confinement phase than in reference gas fuelled plasmas. Besides, in the post-pellet phase, the density fluctuation level is lower in the high iota magnetic configuration than in the standard one. In order to discriminate whether this difference is related to the differences in the plasma profiles or in the stability properties of the two configurations, gyrokinetic simulations have been carried out using the codes \texttt{stella} and EUTERPE. The simulation results point to the plasma profile evolution after the pellet injection and the stabilization effect of the radial electric field profile as the dominant players in the stabilization of the plasma turbulence.

</details>


### [57] [Impact of magnetic islands on plasma flow and turbulence in W7-X](https://arxiv.org/abs/2512.15588)
*T. Estrada,E. Maragkoudakis,D. Carralero,T. Windisch,J. L Velasco,C. Killer,T. Andreeva,J. Geiger,A. Dinklage,A. Krämer-Flecken,G. A. Wurden,M. Beurskens,S. Bozhenkov,H. Damm,G. Fuchert,E. Pasch*

Main category: physics.plasm-ph

TL;DR: Experimental study shows magnetic islands in W7-X stellarator affect plasma flow profiles and reduce turbulence near island O-points.


<details>
  <summary>Details</summary>
Motivation: To understand how magnetic islands influence plasma flow and turbulence in stellarator configurations, particularly in the W7-X device.

Method: Used V-band Doppler reflectometer to measure perpendicular plasma flow and density fluctuations with good spatial resolution in W7-X configurations with 5/5 magnetic islands at plasma edge.

Result: Clear signature of 5/5 magnetic island detected in perpendicular flow profile; island contribution to flow maximum at island boundaries and near zero at O-point; density fluctuation reduction found near island O-point.

Conclusion: Magnetic islands significantly modify plasma flow profiles and can reduce turbulence, with observations consistent with findings from other devices and gyrokinetic simulations.

Abstract: The effect of magnetic islands on plasma flow and turbulence has been experimentally investigated in the stellarator W7-X. Magnetic configurations with the 5/5 magnetic island positioned at the plasma edge, inside the last closed flux surface, are studied. The main diagnostic used in the present work is a V-band Doppler reflectometer that allows the measurement of the perpendicular plasma flow and density fluctuations with good spatial resolution. A characteristic signature of the 5/5 magnetic island is clearly detected in the perpendicular flow profile. The comparison of the experimental flow and the neoclassically driven $E\times B$ flow indicates that the island contribution to the flow is maximum at the island boundaries and close to zero at the island O-point. Besides, a reduction in the density fluctuation level is found nearby the island O-point. The similarities between these observations and those found in other devices and in gyrokinetic simulations are discussed.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [58] [Fully non-linear elliptic equations on noncompact complex manifolds](https://arxiv.org/abs/2512.15350)
*Hanzhang Yin*

Main category: math.DG

TL;DR: A priori estimates and existence results for fully non-linear equations on noncompact Kähler/Hermitian manifolds, with applications to constructing Kähler metrics with prescribed volume forms and complete Chern-Einstein metrics.


<details>
  <summary>Details</summary>
Motivation: To develop analytical tools for solving fully non-linear equations on noncompact complex manifolds, which has important geometric applications in constructing special metrics like Kähler metrics with prescribed volume forms and Chern-Einstein metrics.

Method: Obtain a priori estimates and prove existence results for solutions of a general class of fully non-linear equations on noncompact Kähler and Hermitian manifolds using analytical techniques.

Result: Successfully established a priori estimates and existence theorems, enabling construction of Kähler metrics with prescribed volume forms on strictly pseudoconvex domains, and complete Chern-Einstein metrics on Hermitian manifolds with bounded geometry.

Conclusion: The developed analytical framework provides powerful tools for solving fully non-linear equations on noncompact complex manifolds, with significant geometric applications in metric construction problems.

Abstract: In this paper, we obtain the a priori estimates and the existence results for solutions of a general class of fully non-linear equations on noncompact Kähler and Hermitian manifolds. As geometric applications, we can construct Kähler metrics with some prescribed volume forms on strictly pseudoconvex domains; and we can construct complete Chern-Einstein metrics on Hermitian manifolds with bounded geometry.

</details>


### [59] [A modified Bakry-Émery $Γ_2$ criterion inequality and the monotonicity of the Tsallis entropy](https://arxiv.org/abs/2512.15525)
*Xiaohan Cai,Xiaodong Wang*

Main category: math.DG

TL;DR: The paper proves weighted versions of the Bakry-Émery Γ₂ criterion that yield improved logarithmic Sobolev constants and sharp Sobolev inequalities via Tsallis entropy monotonicity.


<details>
  <summary>Details</summary>
Motivation: The Bakry-Émery Γ₂ criterion is a powerful tool for establishing logarithmic Sobolev inequalities, but there's room for improving the constants and developing weighted versions that connect to Tsallis entropy and yield sharp Sobolev inequalities.

Method: The authors prove a one-parameter family of weighted Bakry-Émery Γ₂ criterion inequalities and study their limiting behavior. They also establish a modified weighted Γ₂ criterion inequality that relates to Tsallis entropy monotonicity under heat flow.

Result: The weighted Γ₂ criterion yields improved constants matching Ji's result in the limit case. The modified criterion provides a family of sharp Sobolev inequalities through Tsallis entropy monotonicity.

Conclusion: Weighted Bakry-Émery Γ₂ criteria provide a flexible framework for obtaining improved logarithmic Sobolev constants and sharp Sobolev inequalities, with connections to Tsallis entropy and heat flow monotonicity.

Abstract: The Bakry-Émery $Γ_2$ criterion inequality provides a method for establishing the logarithmic Sobolev inequality. We prove a one-parameter family of weighted Bakry-Émery $Γ_2$ criterion inequalities which in the limit case yields the improved constant due to Ji \cite{Ji24}. Furthermore, we establish a modified weighted $Γ_2$ criterion inequality which could be interpreted as a monotonicity of the Tsallis entropy under the heat flow and yields a family of sharp Sobolev inequalities.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [60] [Bidirectional Fourier-Enhanced Deep Operator Network for Spatio-Temporal Propagation in Multi-Mode Fibers](https://arxiv.org/abs/2512.15474)
*Dinesh Kumar Murugan,Nithyanandan Kanagaraj*

Main category: physics.optics

TL;DR: Operator learning framework for bidirectional forward/inverse modeling of ultrashort-pulse propagation in graded-index multimode fibers, enabling orders-of-magnitude speedup over conventional numerical solvers.


<details>
  <summary>Details</summary>
Motivation: Conventional numerical solvers for ultrashort-pulse propagation in multimode fibers are computationally expensive, limiting real-time prediction, parameter exploration, experimental feedback, and inverse retrieval of input fields from measured outputs.

Method: Operator learning framework combining spectral filters for spatio-temporal representations with Fourier-embedded conditioning on physical parameters, learning both forward and inverse propagation operators within a single unified architecture.

Result: The model achieves orders-of-magnitude speedup over numerical solvers while accurately predicting complex field evolution on unseen cases, enabling real-time beam diagnostics and data-driven design of complex input fields.

Conclusion: This represents one of the first bidirectional operator-learning frameworks for ultrashort-pulse multimode fiber propagation, with potential applications to various wave systems exhibiting nonlinear and dispersive effects in optics and beyond.

Abstract: Ultrashort-pulse propagation in graded-index multimode fibers is a highly nonlinear phenomenon driven by several physical processes. Although conventional numerical solvers can reproduce this behavior with high fidelity, their computational cost limits real-time prediction, rapid parameter exploration, experimental feedback, and especially inverse retrieval of input fields from measured outputs. In this work, we introduce an operator learning framework that learns both the forward and inverse propagation operators within a single unified architecture. By combining spectral filters for spatio-temporal representations with Fourier-embedded conditioning on physical parameters, the model functions as a fast surrogate capable of accurately predicting complex field evolution on previously unseen cases. To our knowledge, this represents one of the first demonstrations of a bidirectional operator-learning framework applied to ultrashort-pulse multimode fiber propagation. The resulting architecture enables orders-of-magnitude speedup over numerical solvers, paving the way for real-time beam diagnostics, data-driven design of complex input fields, and closed-loop spatio-temporal control. Moreover, the same framework can potentially be applied to a wide variety of wave systems exhibiting analogous nonlinear and dispersive effects in optics and beyond.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [61] [Nonparametric Stochastic Subspaces via the Bootstrap for Characterizing Model Error](https://arxiv.org/abs/2512.15624)
*Akash Yadav,Ruda Zhang*

Main category: cs.CE

TL;DR: A bootstrap-based stochastic subspace model for characterizing model error in reduced-order modeling that leverages empirical data distribution to quantify uncertainties in computational mechanics.


<details>
  <summary>Details</summary>
Motivation: Reliable uncertainty quantification in engineering requires accounting for both aleatory and epistemic uncertainties, particularly model error which dominates prediction errors and influences engineering decisions. Current approaches struggle with characterizing model error effectively in computational mechanics.

Method: Introduces a bootstrap-based stochastic subspace model that uses the empirical data distribution to induce a sampling distribution over principal subspaces for reduced-order modeling. The method enforces linear constraints by construction, requires only one hyperparameter, and is straightforward to implement.

Result: The method enables improved characterization of model error compared to existing approaches. It offers advantages: assumption-free leveraging of empirical data distribution, enforcement of linear constraints, simplified training with one hyperparameter, and straightforward implementation.

Conclusion: The proposed bootstrap-based stochastic subspace model provides an effective approach for characterizing model error in computational mechanics, addressing the challenge of uncertainty quantification in reduced-order modeling with practical advantages over existing methods.

Abstract: Reliable forward uncertainty quantification in engineering requires methods that account for aleatory and epistemic uncertainties. In many applications, epistemic effects arising from uncertain parameters and model form dominate prediction error and strongly influence engineering decisions. Because distinguishing and representing each source separately is often infeasible, their combined effect is typically analyzed using a unified model-error framework. Model error directly affects model credibility and predictive reliability; yet its characterization remains challenging. To address this need, we introduce a bootstrap-based stochastic subspace model for characterizing model error in the stochastic reduced-order modeling framework. Given a snapshot matrix of state vectors, the method leverages the empirical data distribution to induce a sampling distribution over principal subspaces for reduced order modeling. The resulting stochastic model enables improved characterization of model error in computational mechanics compared with existing approaches. The method offers several advantages: (1) it is assumption-free and leverages the empirical data distribution; (2) it enforces linear constraints (such as boundary conditions) by construction; (3) it requires only one hyperparameter, significantly simplifying the training process; and (4) its algorithm is straightforward to implement. We evaluate the method's performance against existing approaches using numerical examples in computational mechanics and structural dynamics.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [62] [Spatial Approximation for Evolutionary Equations](https://arxiv.org/abs/2512.15354)
*Andreas Buchinger,Christian Seifert,Sascha Trostorff,Marcus Waurick*

Main category: math.FA

TL;DR: Develops a general approximation theory for evolutionary equations (Picard 2009) as theoretical foundation for numerical analysis, demonstrated with spectral methods for heat equation.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous mathematical foundation for numerical approximation methods applied to evolutionary equations, addressing the need for theoretical underpinning in numerical analysis of such equations.

Method: Develops general approximation theory for evolutionary equations, then applies it to spatial discretization of heat equation using spectral methods as a concrete demonstration.

Result: Provides a general framework for approximation of evolutionary equations that serves as theoretical foundation for numerical analysis, with successful application to heat equation discretization via spectral methods.

Conclusion: The developed approximation theory offers a solid mathematical foundation for numerical analysis of evolutionary equations, demonstrated through practical application to heat equation with spectral methods.

Abstract: We consider evolutionary equations as introduced by R.\ Picard in 2009 and develop a general theory for approximation which can be seen as a theoretical foundation for numerical analysis for evolutionary equations. To demonstrate the approximation result, we apply it to a spatial discretisation of the heat equation using spectral methods.

</details>


### [63] [A cheap way to closed operator sums](https://arxiv.org/abs/2512.15594)
*Bernhard H. Haak,Peer Christian Kunstmann*

Main category: math.FA

TL;DR: A unified approach using Littlewood-Paley norms and interpolation theory to study closedness of operator sums A+B, providing simplified proofs for known results and new results in ℓ^q-interpolation spaces.


<details>
  <summary>Details</summary>
Motivation: To develop a simple, common framework for analyzing closedness of operator sums A+B in Banach spaces, unifying existing results and extending them to new settings.

Method: Uses Littlewood-Paley type norms and tools from several interpolation theories to study sectorial operators A and B with ω_A+ω_B<π.

Result: Provides simplified proofs for Da Prato-Grisvard and Kalton-Weis results, proves new results in ℓ^q-interpolation spaces with applications to maximal regularity, and gives new proof for Dore-Venni result.

Conclusion: The Littlewood-Paley/interpolation approach offers a unified, simpler framework for studying closedness of operator sums, connecting various known results and enabling new extensions.

Abstract: Let $A$ and $B$ be sectorial operators in a Banach space $X$ of angles $ω_A$ and $ω_B$, respectively, where $ω_A+ω_B<π$. We present a simple and common approach to results on closedness of the operator sum $A+B$, based on Littlewood-Paley type norms and tools from several interpolation theories. This allows us to give short proofs for the well-known results due to Da~Prato-Grisvard and Kalton-Weis. We prove a new result in $\ell^q$-interpolation spaces and illustrate it with a maximal regularity result for abstract parabolic equations. Our approach also yields a new proof for the Dore-Venni result.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [64] [The study of coherent Rayleigh-Brillouin scattering in multiple flow regimes using unified gas-kinetic scheme](https://arxiv.org/abs/2512.15357)
*Xiaozhe Xi,Junzhe Cao,Kun Xu*

Main category: physics.flu-dyn

TL;DR: UGKS extended to simulate coherent Rayleigh-Brillouin scattering (CRBS) using BGK-Shakhov model, validated against argon experiments, enabling study of high-intensity multiscale gas-kinetic phenomena.


<details>
  <summary>Details</summary>
Motivation: CRBS is promising for gas characterization and kinetic process investigation, but its multiscale nature requires methods that can handle different flow regimes without constraints on molecular mean free path or collision time.

Method: Extended unified gas-kinetic scheme (UGKS) with BGK-Shakhov model governing equation; developed second-order accurate numerical algorithm using Strang splitting method to handle perturbation source term.

Result: Model validated against argon CRBS experiments with excellent agreement; systematically examined impact of incident signal intensity on CRBS spectra across Knudsen numbers with physical mechanism analysis.

Conclusion: UGKS successfully broadens applicability of CFD-based CRBS simulations and provides reliable numerical foundation for exploring high-intensity multiscale gas-kinetic phenomena.

Abstract: Coherent Rayleigh-Brillouin scattering (CRBS) holds great promise for the characterization of gas properties and the investigation of gas kinetic processes. The CRBS spectrum exhibits a strong dependence on the Knudsen number (Kn), revealing its inherently multiscale nature. In the unified gas-kinetic scheme (UGKS), collisions are intrinsically coupled with free transport during flux construction, endowing the method with distinct multiscale capabilities. Specifically, the UGKS reduces to a Boltzmann solver when the relaxation time is greater than or equal to the time step, and to the gas-kinetic scheme (GKS)-a Navier-Stokes solver-when the relaxation time is much smaller than the time step, thereby accommodating flow regimes without constraints on the molecular mean free path or collision time. In this study, the UGKS is extended to simulate CRBS phenomena, with the governing equation formulated based on the BGK-Shakhov model. Detailed derivations are provided. To account for the additional perturbation source term, a second-order accurate numerical algorithm is developed using the Strang splitting method within the UGKS framework. The proposed model is validated against argon CRBS experiments, demonstrating excellent agreement. Building on this validated framework, the impact of incident signal intensity on CRBS spectra across a range of Knudsen numbers is systematically examined, accompanied by an in-depth analysis of the underlying physical mechanisms. This work broadens the applicability of CFD-based CRBS simulations and provides a reliable numerical foundation for exploring high-intensity, multiscale gas-kinetic phenomena in future research.

</details>


### [65] [On bifurcations and traction forces on an obstacle in incompressible flow](https://arxiv.org/abs/2512.15424)
*Jakub Cach,Karel Tůma,Jan Blechta,Sebastian Schwarzacher*

Main category: physics.flu-dyn

TL;DR: Numerical study shows correspondence between traction profile changes in steady Navier-Stokes equations and bifurcations in unsteady flow past confined cylinder, offering computationally cheaper way to detect critical Reynolds numbers.


<details>
  <summary>Details</summary>
Motivation: To develop a computationally inexpensive strategy for detecting critical Reynolds numbers in confined cylinder flow by exploring relationships between steady-state traction profiles and unsteady flow bifurcations.

Method: Systematic numerical investigation using duality method for traction profiles, deflation methods, linear stability analysis, and direct numerical simulation of unsteady equations for Reynolds numbers up to 1000 in planar Schäfer-Turek benchmark.

Result: Found qualitative correspondence between changes in steady-state traction profiles and bifurcations in long-time unsteady flow behavior, including symmetry breaking, oscillations, and multiple steady solutions.

Conclusion: The relationship between traction profile changes and flow bifurcations suggests a computationally cheaper alternative to direct numerical simulation for detecting critical Reynolds numbers in confined cylinder flows.

Abstract: We present a systematic numerical investigation of bifurcations in the two-dimensional incompressible Navier-Stokes flow past a confined circular cylinder. The results indicate that there is a qualitative correspondence between changes in the traction profiles of the steady Navier-Stokes equations and bifurcations of the long-time behavior of the unsteady Navier-Stokes equations. The bifurcations include the appearance of symmetry breaking, oscillations, and multiple steady solutions. The well-known planar Schäfer-Turek benchmark is considered with Reynolds numbers up to 1000. For the analysis of bifurcations and traction profiles, several numerical strategies are applied, including a duality method for computing traction profiles, deflation methods, and linear stability analysis. Long-time flow behavior is often explored through direct numerical simulation of the unsteady equations; an approach that is computationally demanding. The relations presented here indicate the possibility of a computationally inexpensive strategy to detect critical Reynolds numbers.

</details>


### [66] [Estimates for the 2D Navier-Stokes equations: the effects of forcing](https://arxiv.org/abs/2512.15188)
*Ritwik Mukherjee,John D. Gibbon,Dario Vincenzi*

Main category: physics.flu-dyn

TL;DR: Extends mathematical estimates for 2D Navier-Stokes equations to forcings with varying regularity, identifying three distinct regimes based on forcing smoothness.


<details>
  <summary>Details</summary>
Motivation: Traditional mathematical estimates use Grashof number, but turbulence experiments use Reynolds number. Need conversion for meaningful comparison. Previous work assumed square-integrable second derivative forcing, but turbulence phenomenology depends on forcing regularity.

Method: Extend estimates for energy and enstrophy dissipation rates and attractor dimension to forcings in Sobolev space of order s, where Fourier coefficients decay faster than k^{-s-1}. Consider range -1 ≤ s ≤ 2, covering from minimal regularity for weak solutions to smooth forcing.

Result: Main finding: existence of three distinct regimes as a function of forcing regularity. The transition between regimes depends on the smoothness parameter s of the forcing.

Conclusion: Forcing regularity significantly affects turbulence estimates, with three distinct mathematical regimes emerging. This bridges mathematical analysis with physical turbulence measurements by accounting for forcing smoothness.

Abstract: Mathematical estimates for the Navier-Stokes equations are traditionally expressed in terms of the Grashof number, which is a dimensionless measure of the magnitude of the forcing and hence a control parameter of the system. However, experimental measurements and statistical theories of turbulence are based on the Reynolds number. Thus, a meaningful comparison between mathematical and physical results requires a conversion of the mathematical estimates to a Reynolds-dependent form. In two dimensions, this was achieved under the assumption that the second derivative of the forcing is square integrable. Nonetheless, numerical simulations have shown that the phenomenology of turbulence is sensitive to the degree of regularity of the forcing. Therefore, we extend the available estimates for the energy and enstrophy dissipation rates as well as the attractor dimension to forcings in the Sobolev space of order $s$; i.e. forcings whose Fourier coefficients decay with the wavenumber $k$ faster than $k^{-s-1}$. We consider the range $-1\leqslant s\leqslant 2$, where $s=2$ corresponds to the known estimates, and $s=-1$ is the smallest value of $s$ for which weak solutions are known to exist. The main result is the existence of three distinct regimes as a function of the regularity of the forcing.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [67] [Continuous data assimilation for 2D stochastic Navier-Stokes equations](https://arxiv.org/abs/2512.15184)
*Hakima Bessaih,Benedetta Ferrario,Oussama Landoulsi,Margherita Zanella*

Main category: math.PR

TL;DR: Extends deterministic nudging data assimilation to stochastic Navier-Stokes equations, proving convergence conditions for additive/multiplicative noise.


<details>
  <summary>Details</summary>
Motivation: The AOT nudging algorithm works well for deterministic systems but needs extension to stochastic regimes for practical applications where random forcing (noise) is present in fluid systems.

Method: Extends the deterministic nudging framework to 2D incompressible Navier-Stokes equations with additive or multiplicative noise, analyzing convergence conditions for nudging parameters and observation scales.

Result: For multiplicative noise: convergence in expectation with exponential/polynomial rates depending on noise covariance growth. For additive noise: exponential convergence both in expectation and pathwise.

Conclusion: Provides stochastic generalization of AOT theory, showing how random forcing, viscous dissipation, and feedback control interact to govern synchronization in stochastic fluid systems.

Abstract: Continuous data assimilation methods, such as the nudging algorithm introduced by Azouani, Olson, and Titi (AOT) [2], are known to be highly effective in deterministic settings for asymptotically synchronizing approximate solutions with observed dynamics. In this work, we extend this framework to a stochastic regime by considering the two-dimensional incompressible Navier-Stokes equations subject to either additive or multiplicative noise. We establish sufficient conditions on the nudging parameter and the spatial observation scale that guarantee convergence of the nudged solution to the true stochastic flow.
  In the case of multiplicative noise, convergence holds in expectation, with exponential or polynomial rates depending on the growth of the noise covariance. For additive noise, we obtain the exponential convergence both in expectation and pathwise. These results yield a stochastic generalization of the AOT theory, demonstrating how the interplay between random forcing, viscous dissipation and feedback control governs synchronization in stochastic fluid systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [68] [PIP$^2$ Net: Physics-informed Partition Penalty Deep Operator Network](https://arxiv.org/abs/2512.15086)
*Hongjin Mi,Huiqiang Lun,Changhong Mou,Yeyu Zhang*

Main category: cs.LG

TL;DR: PIP² Net introduces partition-of-unity regularization to DeepONet for more stable and accurate operator learning of PDEs, outperforming existing methods on nonlinear PDE benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing operator learning architectures like DeepONet and FNO require large datasets, lack physical structure, and suffer from trunk-network instability (mode imbalance/collapse), motivating the use of classical partition-of-unity methods for regularization.

Method: Developed PIP² Net (Physics-informed Partition Penalty Deep Operator Network) with a simplified, principled partition penalty based on partition-of-unity regularization, improving trunk network coordination and expressiveness while maintaining DeepONet's flexibility.

Result: PIP² Net consistently outperformed DeepONet, PI-DeepONet, and POU-DeepONet in prediction accuracy and robustness across three nonlinear PDEs: viscous Burgers equation, Allen-Cahn equation, and a diffusion-reaction system.

Conclusion: Partition-of-unity regularization provides effective stabilization for operator learning, addressing trunk network instability while improving accuracy and robustness for solving parameterized PDEs.

Abstract: Operator learning has become a powerful tool for accelerating the solution of parameterized partial differential equations (PDEs), enabling rapid prediction of full spatiotemporal fields for new initial conditions or forcing functions. Existing architectures such as DeepONet and the Fourier Neural Operator (FNO) show strong empirical performance but often require large training datasets, lack explicit physical structure, and may suffer from instability in their trunk-network features, where mode imbalance or collapse can hinder accurate operator approximation. Motivated by the stability and locality of classical partition-of-unity (PoU) methods, we investigate PoU-based regularization techniques for operator learning and develop a revised formulation of the existing POU--PI--DeepONet framework. The resulting \emph{P}hysics-\emph{i}nformed \emph{P}artition \emph{P}enalty Deep Operator Network (PIP$^{2}$ Net) introduces a simplified and more principled partition penalty that improved the coordinated trunk outputs that leads to more expressiveness without sacrificing the flexibility of DeepONet. We evaluate PIP$^{2}$ Net on three nonlinear PDEs: the viscous Burgers equation, the Allen--Cahn equation, and a diffusion--reaction system. The results show that it consistently outperforms DeepONet, PI-DeepONet, and POU-DeepONet in prediction accuracy and robustness.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [69] [Discussion on the vanishment of solar atmospheric structures during magnetic reconnection](https://arxiv.org/abs/2512.15159)
*Jun Zhang,Tao Ding,Yulei Wang*

Main category: astro-ph.SR

TL;DR: During solar magnetic reconnection, reconnecting structures suddenly shorten by tens of Mm as partial structures vanish observationally.


<details>
  <summary>Details</summary>
Motivation: While magnetic reconnection in solar atmosphere is known to alter topological connectivity and release energy, the length changes of reconnecting structures have rarely been reported, creating a gap in understanding the observational evolution of these topological structures.

Method: Researchers identified reconnection events using 3 criteria: explicit X-type configuration with two independent atmospheric structures, clearly observed reconnection process, and ability to track topological connectivity from at least 5 minutes before to 5 minutes after reconnection. Three qualifying events were selected and studied.

Result: During reconnection, the total length of two topological structures in each event shortened suddenly: Event 1: 47 Mm, Event 2: 3.7 Mm, Event 3: 8.2 Mm. This indicates partial structures vanish observationally during magnetic reconnection.

Conclusion: The observed sudden shortening of reconnecting structures suggests partial structure disappearance during magnetic reconnection. Possible explanations include magnetic tension-induced shrinkage, bizarre changes in the third dimension, or magnetic field annihilation.

Abstract: In solar atmosphere, magnetic reconnection alters the topological connectivity, and magnetic energy is released. However, the length change of the reconnecting structures has rarely been reported. To identify the evolution of the topological structures, we search for reconnection events which should satisfy 3 criteria. (1) Each event displays an explicit X-type configuration, and the configuration consists of two sets of independent atmospheric structures, (2) the reconnection process is clearly observed, and (3) the topological connectivity of the structures can be tracked from at least 5 minutes prior to the occurrence of magnetic reconnection to 5 minutes after the reconnection. In this work, 3 events are selected and studied. During the reconnection moment, the total length of the two topological structures in each event shortens suddenly, and the decrements for events 1--3 are 47 Mm, 3.7 Mm, and 8.2 Mm, respectively, implying that partial structures vanish observationally during magnetic reconnection process. Several possibilities about the vanishment, e.g. the shrinkage of the reconnecting structures due to magnetic tension, the bizarre change in the third dimension, and magnetic field annihilation, have been discussed.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [70] [Energy Inference of Black-Box Quantum Computers Using Quantum Speed Limit](https://arxiv.org/abs/2512.15472)
*Nobumasa Ishida,Yoshihiko Hasegawa*

Main category: quant-ph

TL;DR: A method to infer energy scales of gate Hamiltonians in black-box quantum computers using only user-accessible data and quantum speed limits, applied to IBM's superconducting quantum processor.


<details>
  <summary>Details</summary>
Motivation: Cloud-based quantum computers don't provide hardware-level information like underlying Hamiltonians, obstructing characterization of physical properties. Users need ways to infer these properties without direct hardware access.

Method: Reinterpret Margolus-Levitin and Mandelstam-Tamm quantum speed limit bounds as estimators of energy expectation value and variance. Relate these to shortest time for processor to orthogonalize quantum state. Infer shortest gate time from job execution times using gate-time amplification. Apply to IBM's superconducting quantum processor.

Result: Estimated energy scales for single-, two-, and three-qubit gates on IBM's processor. Order of estimated energy consistent with typical drive energies in superconducting qubit systems, suggesting current gate operations approach quantum speed limit.

Conclusion: Fundamental energetic properties of black-box quantum computers can be quantitatively accessed through operational time measurements, reflecting the conjugate relationship between time and energy imposed by the uncertainty principle.

Abstract: Cloud-based quantum computers do not provide users with access to hardware-level information such as the underlying Hamiltonians, which obstructs the characterization of their physical properties. We propose a method to infer the energy scales of gate Hamiltonians in such black-box quantum processors using only user-accessible data, by exploiting quantum speed limits. Specifically, we reinterpret the Margolus-Levitin and Mandelstam-Tamm bounds as estimators of the energy expectation value and variance, respectively, and relate them to the shortest time for the processor to orthogonalize a quantum state. This shortest gate time, expected to lie on the nanosecond scale, is inferred from job execution times measured in seconds by employing gate-time amplification. We apply the method to IBM's superconducting quantum processor and estimate the energy scales associated with single-, two-, and three-qubit gates. The order of estimated energy is consistent with typical drive energies in superconducting qubit systems, suggesting that current gate operations approach the quantum speed limit. Our results demonstrate that fundamental energetic properties of black-box quantum computers can be quantitatively accessed through operational time measurements, reflecting the conjugate relationship between time and energy imposed by the uncertainty principle.

</details>


### [71] [QuantGraph: A Receding-Horizon Quantum Graph Solver](https://arxiv.org/abs/2512.15476)
*Pranav Vaidhyanathan,Aristotelis Papatheodorou,David R. M. Arvidsson-Shukur,Mark T. Mitchison,Natalia Ares,Ioannis Havoutis*

Main category: quant-ph

TL;DR: QuantGraph is a quantum-enhanced framework that combines local optimization with global refinement using Grover-adaptive-search algorithms, achieving 60% search space reduction and 2x precision improvement while maintaining quantum speedup.


<details>
  <summary>Details</summary>
Motivation: Dynamic programming for graph optimization scales poorly with problem size, creating a need for more efficient approaches that can handle larger problems while maintaining solution quality.

Method: Two-stage quantum-enhanced framework: 1) Local stage finds optimal transitions to establish cost threshold for pruning (60% reduction), 2) Global stage refines solution using Grover-adaptive-search. Embedded in receding-horizon MPC for stability and guidance.

Result: QuantGraph achieves 60% search space reduction, 2x increase in control-discretization precision for fixed query budget, robust closed-loop behavior, lower overall complexity, and maintains Grover-search's quadratic speedup over classical methods.

Conclusion: The integration of quantum search algorithms with classical control theory principles creates a scalable, robust framework that significantly improves graph optimization performance while preserving quantum computational advantages.

Abstract: Dynamic programming is a cornerstone of graph-based optimization. While effective, it scales unfavorably with problem size. In this work, we present QuantGraph, a two-stage quantum-enhanced framework that casts local and global graph-optimization problems as quantum searches over discrete trajectory spaces. The solver is designed to operate efficiently by first finding a sequence of locally optimal transitions in the graph (local stage), without considering full trajectories. The accumulated cost of these transitions acts as a threshold that prunes the search space (up to 60% reduction for certain examples). The subsequent global stage, based on this threshold, refines the solution. Both stages utilize variants of the Grover-adaptive-search algorithm. To achieve scalability and robustness, we draw on principles from control theory and embed QuantGraph's global stage within a receding-horizon model-predictive-control scheme. This classical layer stabilizes and guides the quantum search, improving precision and reducing computational burden. In practice, the resulting closed-loop system exhibits robust behavior and lower overall complexity. Notably, for a fixed query budget, QuantGraph attains a 2x increase in control-discretization precision while still benefiting from Grover-search's inherent quadratic speedup compared to classical methods.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [72] [Nonlinear oscillators at resonance with periodic forcing](https://arxiv.org/abs/2512.14940)
*Philip Korman,Yi Li*

Main category: math.DS

TL;DR: This paper unifies previous results on periodic oscillations and unbounded solutions for nonlinear equations with linear part at resonance and periodic forcing, providing existence/non-existence conditions and detailed dynamics description when periodic solutions don't exist.


<details>
  <summary>Details</summary>
Motivation: To unify and extend previous research by Lazer, Frederickson, Leach, Alonso, Ortega, Korman, and Li on nonlinear equations with resonant linear parts and periodic forcing, creating a comprehensive framework for understanding periodic oscillations and unbounded solutions.

Method: The paper synthesizes and generalizes results from multiple previous studies, providing unified conditions for existence and non-existence of periodic solutions, and develops a detailed description of the dynamics when periodic solutions are absent.

Result: The paper establishes unified conditions for existence/non-existence of periodic solutions in resonant nonlinear equations with periodic forcing, and provides comprehensive characterization of solution dynamics when periodic solutions don't exist.

Conclusion: The research successfully unifies previous work on resonant nonlinear oscillations, offering a complete theoretical framework that clarifies the conditions for periodic solutions and describes the alternative dynamics when such solutions are absent.

Abstract: In this note we unify the results of A.C. Lazer and P.O. Frederickson [3], A.C. Lazer [6], A.C. Lazer and D.E. Leach [7], J.M. Alonso and R. Ortega [1], and P. Korman and Y. Li [4] on periodic oscillations and unbounded solutions of nonlinear equations with linear part at resonance and periodic forcing. We give conditions for the existence and non-existence of periodic solutions, and obtain a rather detailed description of the dynamics for nonlinear oscillations at resonance, in case periodic solutions do not exist.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [73] [Magnetised turbulent plasmas as high-energy particle accelerators](https://arxiv.org/abs/2512.15239)
*M. Lemoine*

Main category: astro-ph.HE

TL;DR: Theoretical modeling shows that magnetized turbulent plasmas with large-amplitude relativistic turbulence can act as extreme particle accelerators through a generalized Fermi mechanism involving magnetic field line bends and velocity compression regions.


<details>
  <summary>Details</summary>
Motivation: To understand particle acceleration mechanisms in magnetized turbulent plasmas, particularly in the context of large-amplitude, semi to fully relativistic turbulence observed in astrophysical environments.

Method: Theoretical modeling based on fully kinetic numerical simulations of turbulent plasmas, analyzed within a "generalized Fermi" framework of stochastic acceleration.

Result: Acceleration primarily occurs through particle interactions with sharp magnetic field line bends and velocity compression regions. The acceleration rate is spatially inhomogeneous with a broken power-law distribution extending to large values, making relativistic turbulence an extreme accelerator.

Conclusion: Relativistic, large-amplitude turbulence serves as an efficient particle accelerator through stochastic Fermi-like processes, with implications for particle transport and energy spectrum evolution in astrophysical systems with radiative losses over long timescales.

Abstract: This proceedings paper reports on the theoretical modelling of particle acceleration in magnetised turbulent plasmas. It briefly reviews some recent findings obtained from fully kinetic numerical simulations of large-amplitude, semi to fully relativistic turbulence. The paper then argues that these findings can be understood within the framework of a ``generalised Fermi'' picture of stochastic acceleration, which it summarises. The dominant contributions to acceleration appear to arise from particle interactions with sharp, dynamic bends of the magnetic field lines and regions of velocity compression. Interestingly, the acceleration rate is spatially inhomogeneous and its probability distribution follows a broken power law extending up to large values. This makes relativistic, large-amplitude turbulence an extreme particle accelerator. Some implications for particle transport and the shape of the particle energy spectrum in the presence of radiative losses and over long timescales are also discussed.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [74] [Thermal Stabilization of Defect Charge States and Finite-Temperature Charge Transition Levels](https://arxiv.org/abs/2512.15463)
*Tobias Hainer,Ethan Berger,Esmée Berger,Olof Hildeberg,Paul Erhart,Julia Wiktor*

Main category: cond-mat.mtrl-sci

TL;DR: Machine learning potentials enable efficient calculation of temperature-dependent charge transition levels in materials, revealing significant shifts and new stable charge states that static zero-Kelvin methods miss.


<details>
  <summary>Details</summary>
Motivation: Point defects create localized electronic states that influence carrier behavior in materials, and their charge transition levels depend on temperature. Current static, zero-Kelvin defect formalisms fail to capture these temperature effects, potentially missing important quantitative shifts and qualitative changes in stable charge states.

Method: Use machine-learned interatomic potentials to efficiently compute temperature-dependent charge transition levels. Apply thermodynamic integration to quantify free-energy differences between charge states and calculate vibrational entropy contributions at finite temperatures. Test the approach on vacancies in MgO, LiF, and CsSnBr3.

Result: Charge transition levels shift with temperature in all three materials due to both entropy and electronic contributions. In CsSnBr3, a neutral charge state becomes thermodynamically stable above 60 K, creating a temperature-dependent Fermi-level window that doesn't exist at 0 K.

Conclusion: The widely used static, zero-Kelvin defect formalism can miss both quantitative shifts in charge transition levels and the qualitative emergence of new stable charge states at finite temperatures. Machine learning potentials combined with thermodynamic integration provide an efficient way to capture these important temperature effects.

Abstract: Point defects introduce localized electronic states that critically affect carrier trapping, recombination, and transport in functional materials. The associated charge transition levels (CTLs) can depend on temperature, requiring accurate treatment of vibrational and electronic free-energy contributions. In this work, we use machine-learned interatomic potentials to efficiently compute temperature-dependent CTLs for vacancies in MgO, LiF, and CsSnBr3. Using thermodynamic integration, we quantify free-energy differences between charge states and calculate the vibrational entropy contributions at finite temperatures. We find that CTLs shift with temperature in MgO, LiF and CsSnBr3 from both entropy and electronic contributions. Notably, in CsSnBr3 a neutral charge state becomes thermodynamically stable above 60 K, introducing a temperature-dependent Fermi-level window absent at 0 K. We show that the widely used static, zero-kelvin defect formalism can miss both quantitative CTL shifts and the qualitative emergence of new stable charge states.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [75] [Characterising injection signatures in Jupiter's ultraviolet aurora using Juno observations](https://arxiv.org/abs/2512.15172)
*Linus Head,Denis Grodent,Bertrand Bonfond,Alessandro Moirano,Guillaume Sicorello,Julie Vinesse,Alyssa Mouton,Maïté Dumont,Thomas Greathouse,Vincent Hue,Ali Sulaiman,Barry Mauk,Zhonghua Yao,Ruilong Guo,Jinyan Zhao*

Main category: astro-ph.EP

TL;DR: Jupiter's UV aurora discrete features are analyzed using Juno data, showing magnetodisc scattering explains most electron precipitation in injection signatures, which can be classified into dawn-storm and non-dawn-storm types.


<details>
  <summary>Details</summary>
Motivation: There's ambiguity about whether magnetodisc scattering or high-latitude Alfvenic acceleration best explains Jupiter's UV aurora injection signatures, and uncertainty about the relationship between arcs in outer emission and injections.

Method: Used automatic detection of discrete features in Jupiter's UV aurora alongside Juno-UVS data and in-situ measurements from other Juno instruments.

Result: Magnetodisc scattering likely accounts for most electron precipitation associated with injection signatures. Injection signatures can be classified into two types: dawn-storm and non-dawn-storm. Arc-like features in outer emission show similar properties to blob-like injection signatures.

Conclusion: Scattering is the dominant mechanism for electron precipitation in Jupiter's injection signatures, with evidence for two distinct types of injection events, and arcs in outer emission may be sequences of injection signatures broadened by energy-dependent electron drift.

Abstract: Discrete features in Jupiter's ultraviolet aurora have been interpreted as signatures of plasma injections in the middle magnetosphere. There exists some ambiguity whether magnetodisc scattering or high-latitude Alfvenic acceleration best describes the observed properties of these injection signatures, and also to what extent arcs in the outer emission are related to injections. Many injection signatures are the result of the evolution of dawn storms; there is, however, limited evidence that non-dawn-storm injection signatures are sometimes present in the aurora. We use automatic detection of these discrete features, alongside data from Juno-UVS and in-situ measurements by other Juno instruments, to show that scattering likely accounts for most of the electron precipitation associated with injection signatures. Additionally, there is evidence that injection signatures can be classified into two types: dawn-storm and non-dawn-storm. Arc-like features in the outer emission show very similar properties to traditional blob-like injection signatures and may consist of sequences of injection signatures that have broadened into an arc via energy-dependent electron drift.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [76] [Multiscale modeling of blood circulation with cerebral autoregulation and network pathway analysis for hemodynamic redistribution in the vascular network with anatomical variations and stenosis conditions](https://arxiv.org/abs/2512.15482)
*Jiawei Liu,Atsushi Kanoke,Hidenori Endo,Kuniyasu Niizuma,Hiroshi Suito*

Main category: physics.med-ph

TL;DR: Multiscale circulation model couples systemic hemodynamics with cerebral arterial network to simulate flow redistribution in Circle of Willis under normal, varied anatomical, and stenotic conditions, revealing collateral activation patterns.


<details>
  <summary>Details</summary>
Motivation: To understand how the Circle of Willis regulates cerebral hemodynamics through flow redistribution under anatomical variations and vascular stenosis, which is crucial for cerebrovascular disease diagnosis and treatment planning.

Method: Developed a multiscale circulation model coupling systemic hemodynamic framework with cerebral arterial network reconstructed from medical imaging, incorporating cerebral autoregulation mechanism (CAM).

Result: Baseline simulations match physiological flow distributions; anatomical variations show ACoA as earliest/most sensitive collateral while PCoAs show structure-dependent engagement; stenosis simulations reveal transition to fetal-type PCA configuration with ACoA flow reversal and PCoA activation.

Conclusion: The framework provides a physiologically interpretable, image-informed tool for investigating cerebral flow regulation through functional collaterals within the Circle of Willis, with potential applications in cerebrovascular disease diagnosis and treatment planning.

Abstract: Cerebral hemodynamics is fundamentally regulated by the Circle of Willis (CoW), which redistributes flow through communicating arteries to stabilize perfusion under anatomical variations and vascular stenosis. In this study, we develop a multiscale circulation model by coupling a systemic hemodynamic framework with a cerebral arterial network reconstructed from medical imaging. The model incorporates a cerebral autoregulation mechanism (CAM) and enables quantitative simulation of flow redistribution within the CoW under normal, anatomically varied, and stenotic conditions. Baseline simulations reproduce physiological flow distributions in which communicating arteries remain nearly inactive, showing negligible cross-flow and agreement with clinical measurements. In contrast, anatomical variations reveal distinct collateral activation patterns: the anterior communicating artery (ACoA) emerges as the earliest and most sensitive functional collateral, whereas the posterior communicating arteries (PCoAs) exhibit structure-dependent engagement. Progressive stenosis simulations further demonstrate a transition from a complete CoW to a fetal-type posterior cerebral artery (PCA) configuration, characterized by early ACoA flow reversal followed by ipsilateral PCoA activation, consistent with experimental and transcranial Doppler observations. Finally, a path-based quantitative analysis is introduced to illustrate how the cerebral vascular network dynamically reconfigures collateral pathways in response to structural changes. Overall, the proposed framework provides a physiologically interpretable, image-informed tool for investigating cerebral flow regulation through functional collaterals within the CoW, with potential applications in the diagnosis and treatment planning of cerebrovascular diseases.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [77] [The role of the exchange-Coulomb potential in two-dimensional electron transport](https://arxiv.org/abs/2512.15456)
*J. L. Figueiredo,J. T. Mendonça,H. Terças*

Main category: cond-mat.mes-hall

TL;DR: Quantum kinetic theory for 2D electron gases with self-consistent Hartree-Fock exchange, leading to exchange-corrected fluid models, plasmonic instabilities, and enhanced Coulomb drag matching experiments.


<details>
  <summary>Details</summary>
Motivation: To develop a quantum kinetic theory that properly incorporates exchange interactions at the Hartree-Fock level for 2D electron gases, going beyond classical Vlasov and Boltzmann models which miss important quantum exchange effects.

Method: Derived a Hartree-Fock-Wigner equation for electronic Wigner function starting from Coulomb Hamiltonian, obtaining closed fluid models with exchange-corrected pressure, force, and current. Applied to single layers, coupled layers, and Coulomb drag problems.

Result: Exchange renormalizes Fermi velocity and drives long-wavelength plasmonic instability at low densities in single layers. In coupled layers, predicts acoustic-optical mode coupling and instability forming long-lived charge-imbalance patterns. Exchange substantially enhances Coulomb drag resistivity in dilute GaAs double wells, quantitatively matching experiments.

Conclusion: The self-consistent Hartree-Fock treatment of exchange in quantum kinetic theory reveals important physical effects missed by classical models, including plasmonic instabilities, mode coupling, and enhanced Coulomb drag that matches experimental observations in 2D electron systems.

Abstract: We develop a quantum kinetic theory of two-dimensional electron gases in which exchange is treated self-consistently at the Hartree-Fock level and enters as a nonlocal, momentum-dependent field in phase space. By starting from the Coulomb Hamiltonian, we derive a Hartree-Fock-Wigner equation for the electronic Wigner function and obtain a closed fluid model with exchange-corrected pressure, force, and current. For a single layer, we show that exchange renormalizes the Fermi velocity and can drive a long-wavelength plasmonic instability at low densities. In coupled layers, the same framework predicts acoustic-optical mode coupling, and an instability forming long-lived charge-imbalance patterns that are not predicted by classical Vlasov and Boltzmann models. Finally, we apply the kinetic model to the Coulomb drag problem and show how exchange substantially enhances the drag resistivity in dilute GaAs double wells, quantitatively matching experimental observations.

</details>
