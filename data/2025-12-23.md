<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 24]
- [math.AP](#math.AP) [Total: 49]
- [physics.comp-ph](#physics.comp-ph) [Total: 6]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 11]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [hep-th](#hep-th) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [math.DG](#math.DG) [Total: 3]
- [math.OC](#math.OC) [Total: 3]
- [nlin.CD](#nlin.CD) [Total: 1]
- [math.PR](#math.PR) [Total: 4]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 2]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 2]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [quant-ph](#quant-ph) [Total: 5]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 5]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [math-ph](#math-ph) [Total: 2]
- [cond-mat.supr-con](#cond-mat.supr-con) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Voronoi integration of the rendering equation](https://arxiv.org/abs/2512.17974)
*Nicolas Chenavier,Samuel Delepoulle,Christophe Renaud,Franck Vandewièle*

Main category: math.NA

TL;DR: Proposes Voronoi tessellation reweighting with Poisson point process sampling to reduce variance in photorealistic image rendering compared to traditional Monte Carlo methods.


<details>
  <summary>Details</summary>
Motivation: Traditional Monte Carlo methods for rendering equation integration face challenges in controlling variance, resulting in noisy visual artifacts in difficult-to-render regions.

Method: Introduces a Voronoi tessellation reweighting scheme combined with a Poisson point process sampling strategy for integration of the rendering equation.

Result: Theoretically shows that variance induced by Poisson-Voronoi tessellation is smaller than Monte Carlo method when intensity of underlying process is arbitrarily large and function satisfies Hölder continuity condition.

Conclusion: The proposed approach addresses limitations of standard Monte Carlo methods in photorealistic image rendering by providing better variance control through geometric sampling strategies.

Abstract: In photorealistic image rendering, Monte Carlo methods form the foundation for the integration of the rendering equation in modern approaches. However, despite their effectiveness, traditional Monte Carlo methods often face challenges in controlling variance, resulting in noisy visual artifacts in regions that are difficult to render. In this work, we propose a new approach to the integration of the rendering equation by introducing a Voronoi tessellation reweighting scheme combined with a Poisson point process sampling strategy to address some of the limitations of standard Monte Carlo methods. From a theoretical point of view, we show that the variance induced by a Poisson-Voronoi tessellation is smaller than that of the Monte Carlo method when the intensity of the underlying process is arbitrarily large and when the function to be integrated satisfies a Holder continuity condition.

</details>


### [2] [A local Fortin projection for the Scott-Vogelius elements on general meshes](https://arxiv.org/abs/2512.18033)
*Franziska Eickmann,Johnny Guzmán,Michael Neilan,L. Ridgway Scott,Tabea Tscherpel*

Main category: math.NA

TL;DR: Constructed a local Fortin projection for Scott-Vogelius finite element pair (k≥4) on 2D shape-regular triangulations, including singular vertices, with divergence preservation and local stability.


<details>
  <summary>Details</summary>
Motivation: The Scott-Vogelius finite element pair is important for incompressible flow simulations but requires stable projections. Existing projections may not handle singular vertices or preserve boundary data properly on general triangulations.

Method: Constructed a local Fortin projection specifically designed for the Scott-Vogelius finite element pair with polynomial degree k≥4 on shape-regular 2D triangulations. The method handles singular vertices and preserves both divergence in the dual pressure space and discrete boundary data.

Result: Successfully developed a projection that satisfies local stability estimates while preserving the divergence property and boundary conditions, even on triangulations containing singular vertices.

Conclusion: The constructed local Fortin projection provides a stable and practical tool for Scott-Vogelius elements in 2D, overcoming previous limitations with singular vertices and boundary data preservation.

Abstract: We construct a local Fortin projection for the Scott-Vogelius finite element pair for polynomial degree $k \ge 4$ on general shape-regular triangulations in two dimensions. In particular, the triangulation may contain singular vertices. In addition to preserving the divergence in the dual of the pressure space, the projection preserves discrete boundary data and satisfies local stability estimates.

</details>


### [3] [Approximation and learning with compositional tensor trains](https://arxiv.org/abs/2512.18059)
*Martin Eigel,Charles Miranda,Anthony Nouy,David Sommer*

Main category: math.NA

TL;DR: CTTs combine tensor-train decomposition with compositional functions for efficient multivariate function approximation, offering controlled compression and specialized optimization algorithms.


<details>
  <summary>Details</summary>
Motivation: To create a scalable alternative to deep neural networks that combines the expressivity of compositional models with the algorithmic efficiency of tensor algebra, enabling controlled compression and efficient optimization.

Method: Compositional tensor trains (CTTs) - composing low-rank functions in tensor-train format, enabling encoding of various approximation tools. Optimization uses layerwise algorithm inspired by natural gradient descent with low-rank Gram matrix estimation and tensor structured random sketching, plus optimal control-inspired methods.

Result: Numerical experiments on regression tasks demonstrate CTTs' expressivity and the relevance of the proposed optimization algorithms. The format offers a scalable alternative to DNNs with controlled compression capabilities.

Conclusion: CTTs successfully combine compositional model expressivity with tensor algebra efficiency, providing a promising scalable alternative to standard deep neural networks with structured compression and specialized optimization.

Abstract: We introduce compositional tensor trains (CTTs) for the approximation of multivariate functions, a class of models obtained by composing low-rank functions in the tensor-train format. This format can encode standard approximation tools, such as (sparse) polynomials, deep neural networks (DNNs) with fixed width, or tensor networks with arbitrary permutation of the inputs, or more general affine coordinate transformations, with similar complexities. This format can be viewed as a DNN with width exponential in the input dimension and structured weights matrices. Compared to DNNs, this format enables controlled compression at the layer level using efficient tensor algebra. On the optimization side, we derive a layerwise algorithm inspired by natural gradient descent, allowing to exploit efficient low-rank tensor algebra. This relies on low-rank estimations of Gram matrices, and tensor structured random sketching. Viewing the format as a discrete dynamical system, we also derive an optimization algorithm inspired by numerical methods in optimal control. Numerical experiments on regression tasks demonstrate the expressivity of the new format and the relevance of the proposed optimization algorithms. Overall, CTTs combine the expressivity of compositional models with the algorithmic efficiency of tensor algebra, offering a scalable alternative to standard deep neural networks.

</details>


### [4] [A Domain Decomposition Deep Neural Network Method with Multi-Activation Functions for Solving Elliptic and Parabolic Interface Problems](https://arxiv.org/abs/2512.18178)
*Qijia Zhai*

Main category: math.NA

TL;DR: A domain decomposition deep learning method using two independent neural networks with multi-activation functions for solving high-dimensional elliptic and parabolic interface problems with discontinuous coefficients.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient deep learning approach for solving challenging interface problems with discontinuous coefficients in high dimensions (2D-10D), where traditional methods face difficulties with complex interface geometries.

Method: Domain decomposition with two independent neural networks (one per subdomain), coupled through interface conditions in the loss function. Features a multi-activation mechanism that adaptively blends multiple activation functions (tanh and Gaussian-type) with interface-aware weighting to enhance learning near interfaces.

Result: Proved conditional error bounds relating solution accuracy to trained loss values and quadrature errors. Numerical experiments on elliptic and parabolic interface problems with various interface geometries in 2D-10D demonstrate effectiveness and accuracy.

Conclusion: The MAF approach provides an effective deep learning solution for high-dimensional interface problems, with adaptive activation functions improving learning efficiency near interfaces where coupling constraints are most demanding.

Abstract: We present a domain decomposition-based deep learning method for solving elliptic and parabolic interface problems with discontinuous coefficients in two to ten dimensions. Our Multi-Activation Function (MAF) approach employs two independent neural networks, one for each subdomain, coupled through interface conditions in the loss function. The key innovation is a multi-activation mechanism within each subdomain network that adaptively blends multiple activation functions (e.g., $\tanh$ and Gaussian-type) with interface-aware weighting, enhancing learning efficiency near interfaces where coupling constraints are most demanding. We prove conditional error bounds relating solution accuracy to trained loss values and quadrature errors. Numerical experiments on elliptic and parabolic interface problems with various interface geometries (2D--10D) validate the effectiveness and accuracy of the proposed method.

</details>


### [5] [A Singularity Guided Nyström Method for Elastostatics on Two Dimensional Domains with Corners](https://arxiv.org/abs/2512.18208)
*Baoling Xie,Jun Lai*

Main category: math.NA

TL;DR: Developed analytical framework and singularity-guided Nyström method for 2D Lamé boundary integral equations on cornered domains, achieving higher-order accuracy than uniform methods.


<details>
  <summary>Details</summary>
Motivation: Need to solve 2D Lamé system boundary integral equations on domains with corners, where singularities arise at corners affecting solution accuracy and numerical stability.

Method: Local Mellin analysis on wedges to obtain factorizable characteristic equation for singular exponents; proved Fredholm well-posedness in weighted Sobolev spaces; constructed explicit density-to-Taylor mapping; proposed singularity-guided Nyström scheme using corner exponents and Legendre-tail indicator for panel refinement.

Result: Obtained higher-order accuracy than uniform Nyström method; identified crowding-limited regime for domains with re-entrant angles; proved invertibility of density-to-Taylor mapping for all but countable set of angles.

Conclusion: The singularity-guided Nyström method effectively handles corner singularities in 2D Lamé BIEs, providing superior accuracy through analytical understanding of corner behavior and adaptive refinement strategies.

Abstract: We develop a comprehensive analytical and numerical framework for boundary integral equations (BIEs) of the 2D Lamé system on cornered domains. By applying local Mellin analysis on a wedge, we obtain a factorizable characteristic equation for the singular exponents of the boundary densities, and clarify their dependence on boundary conditions. The Fredholm well-posedness of the BIEs on cornered domains is proved in weighted Sobolev spaces. We further construct an explicit density-to-Taylor mapping for the BIE and show its invertibility for all but a countable set of angles. Based on these analytical results, we propose a singularity guided Nyström (SGN) scheme for the numerical solution of BIEs on cornered domains. The SGN uses the computed corner exponents and a Legendre-tail indicator to drive panel refinement. An error analysis that combines this refinement strategy with an exponentially accurate far-field quadrature rule is provided. Numerical experiments across various cornered geometries demonstrate that SGN obtains higher order accuracy than uniform Nyström method and reveal a crowding-limited regime for domains with re-entrant angles.

</details>


### [6] [Hybrid multiscale method for polymer melts: analysis and simulations](https://arxiv.org/abs/2512.18272)
*Ranajay Datta,Mária Lukáčová-Medviďová,Andreas Schömer,Peter Virnau*

Main category: math.NA

TL;DR: A hybrid multiscale approach models dense ring polymer melt flow near walls using MD simulations for microscopic stress, coupled with a macroscopic Cahn-Hilliard-Navier-Stokes system with FEM implementation.


<details>
  <summary>Details</summary>
Motivation: To understand and model the complex flow behavior of dense melts of flexible and semiflexible ring polymers in confined geometries with walls, bridging microscopic polymer dynamics to macroscopic flow phenomena.

Method: Hybrid multiscale approach combining molecular dynamics simulations with Irving-Kirkwood formula for stress tensor, coupled to macroscopic Cahn-Hilliard-Navier-Stokes system with dynamic/no-slip boundary conditions, solved using finite element method with proven solvability and energy stability.

Result: The macroscopic model can replicate phase segregation between flexible and semiflexible rings under flow (observed in microscopic simulations) by introducing effective attractive forces, with numerical scheme demonstrating solvability and energy stability.

Conclusion: The developed multiscale framework successfully bridges microscopic polymer dynamics to macroscopic flow modeling, enabling simulation of complex phase segregation phenomena in dense ring polymer melts near walls with mathematically rigorous numerical implementation.

Abstract: We model the flow behaviour of dense melts of flexible and semiflexible ring polymers in the presence of walls using a hybrid multiscale approach. Specifically, we perform molecular dynamics simulations and apply the Irving-Kirkwood formula to determine an averaged stress tensor for a macroscopic model. For the latter, we choose a Cahn-Hilliard-Navier-Stokes system with dynamic and no-slip boundary conditions. We present numerical simulations of the macroscopic flow that are based on a finite element method. In particular, we present detailed proofs of the solvability and the energy stability of our numerical scheme. Phase segregation under flow between flexible and semiflexible rings, as observed in the microscopic simulations, can be replicated in the macroscopic model by introducing effective attractive forces.

</details>


### [7] [Convexification Numerical Method for Imaging of Moving Targets](https://arxiv.org/abs/2512.18361)
*Michael V. Klibanov,Jingzhi Li,Vladimir G. Romanov,Zhipeng Yang*

Main category: math.NA

TL;DR: A convexification method for imaging moving targets using hyperbolic PDEs with lateral Cauchy data and Fourier series approximation.


<details>
  <summary>Details</summary>
Motivation: To develop a stable and globally convergent numerical method for imaging moving targets, which is formulated as a coefficient inverse problem for hyperbolic equations with time-varying coefficients.

Method: Uses truncated Fourier series with special orthonormal basis, obtains Lipschitz stability estimate, then develops convexification method based on Carleman estimate for global convergence.

Result: Achieves Lipschitz stability, develops globally convergent convexification method, and presents successful numerical experiment results.

Conclusion: The convexification method provides a stable, globally convergent approach for imaging moving targets using hyperbolic PDEs with lateral Cauchy data.

Abstract: The problem of imaging of a moving target is formulated as a Coefficient Inverse Problem for a hyperbolic equation with its coefficient depending on all three spatial variables and time. As the initial condition, the point source running along a straight line is used. Lateral Cauchy data are known for each position of the point source. A truncated Fourier series with respect to a special orthonormal basis is used. First, Lipschitz stability estimate is obtained. Next, a globally convergent numerical method, the so-called convexification method, is developed and its convergence analysis is carried out. The convexification method is based on a Carleman estimate. Results of numerical experiments are presented.

</details>


### [8] [Sixth-order explicit one-step methods for stiff ODEs via hybrid deferred correction involving RK2 and RK4: Application to reaction-diffusion equations](https://arxiv.org/abs/2512.18377)
*Saint Cyr E. R. Koyaguerebo-Imé*

Main category: math.NA

TL;DR: A 6th-order explicit one-step method (DC6RK2/4) is created by applying RK4 deferred correction to the explicit midpoint rule, offering improved stability for stiff ODEs with complex eigenvalues.


<details>
  <summary>Details</summary>
Motivation: To develop an explicit method that can handle stiff ODEs with complex eigenvalues without requiring extremely small time steps, particularly for problems where Jacobian matrices have eigenvalues with larger imaginary parts than real parts.

Method: Apply fourth-order explicit Runge-Kutta (RK4) deferred correction to the explicit midpoint rule, creating DC6RK2/4 method. Prove convergence and accuracy order through deferred correction condition satisfied by RK4.

Result: DC6RK2/4 achieves 6th-order accuracy with stability region containing that of RK6, tangent to [-5.626,0[x[-4.730,4.730] in complex plane. Performs well on stiff nonlinear problems and long-term integration, outperforming standard implicit methods for problems with complex eigenvalues where imaginary parts dominate real parts.

Conclusion: DC6RK2/4 is an effective explicit method for stiff ODEs with complex eigenvalues, offering high-order accuracy without requiring extremely small time steps, and shows advantages over traditional implicit methods for certain stiff problem classes.

Abstract: In this paper, the fourth-order explicit Runge-Kutta method (RK4) is used to make a Deferred Correction (DC) on the explicit midpoint rule, resulting in an explicit one-step method of order six of accuracy, denoted DC6RK2/4. Convergence and order of accuracy of DC6RK2/4 are proven through a deferred correction condition satisfied by the RK4. The region of absolute stability of this method contains that of a RK6 and is tangent to the region [-5.626,0[x[-4.730,4.730] of the complex plane, containing a significant part of the imaginary axis. Numerical experiments with standard test problems for stiff systems of ODEs show that DC6RK2/4 performs well on problems regarding strong non-linearity and long-term integration, and this method does not require extremely small time steps for accurate numerical solutions of stiff problems. Moreover, this method is better than standard implicit methods like the Backward Differentiation Formulae and the DC methods for the implicit midpoint rule on stiff problems for which Jacobian matrices along the solution curve have complex eigenvalues where imaginary parts have larger magnitudes than real parts. An application of DC6RK2/4 to a class of test problems for reaction-diffusion equations in one dimensional is also carried out.

</details>


### [9] [A least-squares meshfree method for the incompressible Navier-Stokes equations: A satisfactory solenoidal velocity field via a staggered-variable arrangement](https://arxiv.org/abs/2512.18422)
*Takeharu Matsuda,Satoshi Ii*

Main category: math.NA

TL;DR: A meshfree incompressible flow solver using local primal-dual grid to ensure solenoidal velocity fields and avoid pressure-velocity decoupling issues.


<details>
  <summary>Details</summary>
Motivation: Meshfree methods for incompressible flow face challenges: difficulty satisfying incompressibility at discrete level, pressure-velocity decoupling (zero-energy mode), and inconsistency in discrete operators for Poisson equation in projection methods.

Method: Introduces local primal-dual grid into mesh-constrained discrete point method, creating virtual dual cells based on local node connectivity. Uses time evolution converting to evolve velocity on cell interfaces for consistent pressure-velocity coupling under primal-dual arrangement.

Result: Method satisfies solenoidal velocity field condition at discrete level, achieves expected spatial convergence order, and accurately reproduces flow features across wide Reynolds number range. Linear acoustic equation validation confirms staggered-variable arrangement effectiveness.

Conclusion: The proposed meshfree method with local primal-dual grid successfully addresses key challenges in incompressible flow simulation, providing consistent discrete operators and preventing pressure-velocity decoupling while maintaining truly meshfree nature.

Abstract: Incompressible flow solvers based on strong-form meshfree methods represent arbitrary geometries without the need for a global mesh system. However, their local evaluations make it difficult to satisfy incompressibility at the discrete level. Moreover, the collocated arrangement of velocity and pressure variables tends to induce a zero-energy mode, leading to decoupling between the two variables. In projection-based approaches, a spatial discretization scheme based on a conventional node-based moving least-squares method for the pressure causes inconsistency between the discrete operators on both sides of the Poisson equation. Thus, a solenoidal velocity field cannot be ensured numerically. In this study, a numerical method for the incompressible Navier-Stokes equations is developed by introducing a local primal-dual grid into the mesh-constrained discrete point method, enabling consistent discrete operators. The \textit{virtual} dual cell constructed is based on the local connectivity among nodes, and therefore our method remains truly meshfree. To achieve a consistent coupling between velocity and pressure variables under the primal-dual arrangement, time evolution converting is applied to evolve the velocity on cell interfaces. For numerical validation, a linear acoustic equation is solved to confirm the effectiveness of the staggered-variable arrangement based on the local primal-dual grid. Then, incompressible Navier-Stokes equations are solved, and the proposed method is demonstrated to satisfy the condition of a solenoidal velocity field at the discrete level, achieve the expected spatial convergence order, and accurately reproduce flow features over a wide range of Reynolds numbers.

</details>


### [10] [Overcoming Spectral Bias via Cross-Attention](https://arxiv.org/abs/2512.18586)
*Xiaodong Feng,Tao Tang,Xiaoliang Wan,Tao Zhou*

Main category: math.NA

TL;DR: Proposed cross-attention architecture with learnable scaling factors accelerates high-frequency convergence in neural networks by adaptively reweighting multiscale random Fourier features, with extension to PDE learning via specialized sub-networks.


<details>
  <summary>Details</summary>
Motivation: To address spectral bias where high-frequency components converge much slower than low-frequency ones during neural network training, which limits performance on problems with high-frequency or oscillatory patterns.

Method: Cross-attention-based architecture with learnable scaling factors that adaptively reweights a scaled multiscale random Fourier feature bank. Features incremental spectral enrichment by appending dominant Fourier modes from intermediate approximations. Extended to PDE learning via linear combination of specialized high-frequency and low-frequency sub-networks with learnable mixing factor.

Result: Demonstrated effectiveness on high-frequency/discontinuous regression, image reconstruction, and PDE examples. Accelerates high-frequency convergence relative to comparable baselines while maintaining robustness.

Conclusion: The proposed adaptive multiscale Fourier feature architecture with cross-attention effectively mitigates spectral bias, enabling faster convergence of high-frequency components and improved performance on oscillatory problems.

Abstract: Spectral bias implies an imbalance in training dynamics, whereby high-frequency components may converge substantially more slowly than low-frequency ones. To alleviate this issue, we propose a cross-attention-based architecture that adaptively reweights a scaled multiscale random Fourier feature bank with learnable scaling factors. The learnable scaling adjusts the amplitudes of the multiscale random Fourier features, while the cross-attention residual structure provides an input-dependent mechanism to emphasize the most informative scales. As a result, the proposed design accelerates high-frequency convergence relative to comparable baselines built on the same multiscale bank. Moreover, the attention module supports incremental spectral enrichment: dominant Fourier modes extracted from intermediate approximations via discrete Fourier analysis can be appended to the feature bank and used in subsequent training, without modifying the backbone architecture.
  We further extend this framework to PDE learning by introducing a linear combination of two sub-networks: one specialized in capturing high-frequency components of the PDE solution and the other in capturing low-frequency components, with a learnable (or optimally chosen) mixing factor to balance the two contributions and improve training efficiency in oscillatory regimes. Numerical experiments on high-frequency and discontinuous regression problems, image reconstruction tasks, as well as representative PDE examples, demonstrate the effectiveness and robustness of the proposed method.

</details>


### [11] [Generalized Chebyshev acceleration on the unit disc](https://arxiv.org/abs/2512.18848)
*Nurgül Gökgöz*

Main category: math.NA

TL;DR: Relaxing restrictive eigenvalue conditions for generalized Chebyshev acceleration by introducing an alternative iterative scheme that converges to the same solution.


<details>
  <summary>Details</summary>
Motivation: Generalized Chebyshev acceleration requires highly restrictive eigenvalue inclusion conditions for the iteration matrix, limiting its applicability to many practical problems.

Method: Introduce an alternative iterative scheme that relaxes the restrictive eigenvalue requirements while still converging to the same solution as the original method.

Result: The proposed approach is examined and shown to be effective when applied to large-scale sparse normal matrices.

Conclusion: The new method successfully relaxes the restrictive eigenvalue conditions of generalized Chebyshev acceleration while maintaining convergence to the correct solution, making it applicable to a broader range of problems including large-scale sparse systems.

Abstract: Generalized Chebyshev acceleration is a semi-iterative technique applicable to a basic iterative method only when the eigenvalues of the iteration matrix satisfy a highly restrictive inclusion condition. In this work, we relax this requirement by introducing an alternative iterative scheme that converges to the same solution. The effectiveness of the proposed approach is examined through its application to a large-scale sparse normal matrix.

</details>


### [12] [Weak Galerkin finite element methods for elliptic interface problems on nonconvex polygonal partitions](https://arxiv.org/abs/2512.18905)
*Chunmei Wang,Shangyou Zhang*

Main category: math.NA

TL;DR: A weak Galerkin finite element method for elliptic interface problems on nonconvex polygonal partitions with built-in stabilizer and optimal-order error estimates.


<details>
  <summary>Details</summary>
Motivation: To develop an effective numerical method for solving elliptic interface problems on complex nonconvex polygonal domains, which are challenging due to interface discontinuities and domain irregularities.

Method: A weak Galerkin finite element method with built-in stabilizer that maintains a simple, symmetric, and positive definite formulation on nonconvex polygonal partitions.

Result: Optimal-order error estimates in discrete H¹ norm are rigorously derived, and numerical experiments confirm theoretical results, demonstrating robustness and effectiveness.

Conclusion: The proposed weak Galerkin method successfully handles elliptic interface problems on nonconvex polygonal domains with optimal convergence and numerical stability.

Abstract: This paper proposes a weak Galerkin (WG) finite element method for elliptic interface problems defined on nonconvex polygonal partitions. The method features a built-in stabilizer and retains a simple, symmetric, and positive definite formulation. An optimal-order error estimate is rigorously derived in the discrete $H^1$ norm. Furthermore, a series of numerical experiments are provided to verify the theoretical results and to demonstrate the robustness and effectiveness of the proposed WG method for elliptic interface problems.

</details>


### [13] [An asymptotically compatible unfitted finite element methods for nonlocal elliptic Interfaces: local limits and sharp error estimates](https://arxiv.org/abs/2512.18939)
*Haixia Dong,Ziqing Xie,Jiwei Zhang*

Main category: math.NA

TL;DR: An asymptotically compatible unfitted finite element method for 1D nonlocal elliptic interface problems with optimal error estimates and second-order convergence.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for nonlocal elliptic interface problems that can handle interfaces without requiring body-fitted meshes, while maintaining asymptotic compatibility with local counterparts.

Method: An unfitted finite element method with three key components: 1) extended maximum principle and asymptotic consistency analysis of flux operator, 2) Nitsche-type formulation incorporating nonlocal jump conditions directly into weak form, 3) rigorous proof using nonlocal maximum principle, flux consistency, and newly derived nonlocal Poincare inequality.

Result: Achieves optimal error estimates in both energy and L2 norms, demonstrates second-order convergence of nonlocal solutions to local counterparts in maximum norm, and shows robustness and efficiency through numerical experiments.

Conclusion: The proposed method successfully solves 1D nonlocal elliptic interface problems with high accuracy without body-fitted meshes, provides theoretical guarantees of optimal convergence, and establishes a foundation for extension to higher-dimensional problems.

Abstract: This paper presents the development and analysis of an asymptotically compatible (AC) unfitted finite element method for one-dimensional nonlocal elliptic interface problems. The proposed method achieves optimal error estimates through three principal contributions: (i) an extended maximum principle, coupled with an asymptotic consistency analysis of the flux operator, which establishes second-order convergence of nonlocal solutions to their local counterparts in the maximum norm; (ii) a Nitsche-type formulation that directly incorporates nonlocal jump conditions into the weak form, enabling high accuracy without body-fitted meshes; and (iii) a rigorous proof of optimal convergence rates in both the energy and L2 norms via the nonlocal maximum principle, flux consistency, and a newly derived nonlocal Poincare inequality. Numerical experiments confirm the theoretical findings and demonstrate the robustness and efficiency of the proposed approach, thereby providing a foundation for extensions to higher dimensions.

</details>


### [14] [A TraceFEM $C^0$ Interior Penalty Method for the Surface Biharmonic Equation](https://arxiv.org/abs/2512.18949)
*Michael Neilan,Hongzhi Wan*

Main category: math.NA

TL;DR: TraceFEM for surface biharmonic problem using quadratic Lagrange elements with C0 interior penalty formulation achieves optimal first-order convergence in H2 norm and quadratic convergence in L2 norm.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate and stable finite element method for solving surface biharmonic problems, which are important in applications like thin shell mechanics, surface fluid dynamics, and geometric flows.

Method: Trace Finite Element Method (TraceFEM) using standard quadratic Lagrange finite element spaces on 3D background mesh with symmetric C0 interior penalty formulation on polyhedral surface approximation. Stability achieved through surface edge penalties and bulk-facet penalization of gradient and Hessian jumps.

Result: Proved optimal first-order convergence in discrete H2 norm and quadratic convergence in L2 norm, demonstrating the method's accuracy and stability properties.

Conclusion: The proposed TraceFEM discretization provides an effective numerical approach for surface biharmonic problems with proven convergence rates, combining background mesh flexibility with surface-specific stabilization techniques.

Abstract: We construct and analyze a TraceFEM discretization for the surface biharmonic problem. The method utilizes standard quadratic Lagrange finite element spaces defined on a three-dimensional background mesh and a symmetric $C^0$ interior penalty formulation posed on a second-order polyhedral approximation of the surface. Stability is achieved through a combination of surface edge penalties and bulk-facet penalization of gradient and Hessian jumps. We prove optimal first-order convergence in a discrete $H^2$ norm and quadratic convergence in the $L^2$ norm.

</details>


### [15] [A Spectral Low-Mode Reduced Method for Elliptic Problems](https://arxiv.org/abs/2512.18955)
*Prosper Torsu*

Main category: math.NA

TL;DR: Spectral low-mode reduced solver for elliptic PDEs using Dirichlet Laplacian eigenmodes as coarse basis, providing analytic reduced models without training data.


<details>
  <summary>Details</summary>
Motivation: Need efficient solvers for second-order elliptic boundary value problems with spatially varying diffusion coefficients, aiming to reduce computational cost while preserving coefficient heterogeneity.

Method: Project standard finite difference/element discretizations onto coarse space spanned by lowest Dirichlet Laplacian eigenmodes using exact Galerkin projection, creating analytic reduced model requiring no training data.

Result: Reduced solution is energy-optimal in selected subspace; truncation error decays as √log M/M in H₀¹ norm for H²-regular solutions; projected operator well-conditioned; numerical experiments show accuracy and speedups over sparse direct solvers, favorable vs multigrid/deflation-based Krylov methods.

Conclusion: Spectral low-mode reduction provides efficient, accurate solver for heterogeneous elliptic problems with analytic reduced models, optimal energy properties, and practical computational advantages.

Abstract: We develop a spectral low-mode reduced solver for second-order elliptic boundary value problems with spatially varying diffusion coefficients. The approach projects standard finite difference or finite element discretization onto a global coarse space spanned by the lowest Dirichlet Laplacian eigenmodes, yielding an analytic reduced model that requires no training data and preserves coefficient heterogeneity through an exact Galerkin projection. The reduced solution is energy-optimal in the selected subspace and, for $H^2$-regular solutions, the truncation error associated with discarded modes satisfies a $\sqrt{\log M}/M$ decay in the $H_0^1$ norm. For uniformly stable reduced bases, the projected operator is well conditioned with respect to mesh refinement, and numerical experiments corroborate the predicted accuracy and demonstrate meaningful speedups over sparse direct solvers, with favorable performance relative to multigrid and deflation-based Krylov methods for heterogeneous coefficients in the tested setups.

</details>


### [16] [Randomized time stepping of nonlinearly parametrized solutions of evolution problems](https://arxiv.org/abs/2512.19009)
*Yijun Dong,Paul Schwerdtner,Benjamin Peherstorfer*

Main category: math.NA

TL;DR: Randomized time stepping via sketching improves conditioning and efficiency for Dirac-Frenkel variational principle problems.


<details>
  <summary>Details</summary>
Motivation: The Dirac-Frenkel variational principle leads to poorly conditioned time-dependent least-squares problems when using nonlinear parametrizations for model reduction and PDE solving.

Method: Introduces randomized time stepping with sketching: at each time step, solves a low-dimensional random projection of the parameter vector via sketching, which regularizes the problem and reduces unknowns.

Result: Numerical experiments show competitive accuracy and better runtime efficiency compared to standard regularization methods.

Conclusion: Randomized time stepping via sketching effectively addresses conditioning issues in Dirac-Frenkel variational problems while improving computational efficiency.

Abstract: The Dirac-Frenkel variational principle is a widely used building block for using nonlinear parametrizations in the context of model reduction and numerically solving partial differential equations; however, it typically leads to time-dependent least-squares problems that are poorly conditioned. This work introduces a randomized time stepping scheme that solves at each time step a low-dimensional, random projection of the parameter vector via sketching. The sketching has a regularization effect that leads to better conditioned least-squares problems and at the same time reduces the number of unknowns that need to be solved for at each time step. Numerical experiments with benchmark examples demonstrate that randomized time stepping via sketching achieves competitive accuracy and outperforms standard regularization in terms of runtime efficiency.

</details>


### [17] [Newton's method in adaptive iteratively linearized FEM](https://arxiv.org/abs/2512.19357)
*Philipp Bringmann,Maximilian Brunner,Dirk Praetorius*

Main category: math.NA

TL;DR: This paper presents an adaptive FEM that incorporates Newton's method with adaptive damping for solving nonlinear PDEs, featuring novel convergence analysis using discrete dual norms instead of energy-based arguments.


<details>
  <summary>Details</summary>
Motivation: To develop an adaptive finite element method for nonlinear PDEs that combines Newton's method with adaptive damping, addressing the need for convergence analysis beyond energy-minimization problems and providing computable error measures.

Method: Incorporates Newton's method into adaptive FEM with adaptive damping parameter selection on each discretization level. Uses discrete dual norm of residual as computable measure for linearization error, analyzing strongly monotone operators with locally Lipschitz continuous Fréchet derivatives.

Result: Provides first convergence analysis with optimal rates for adaptive iteratively linearized FEM beyond energy-minimization problems. Theory applies to strongly monotone operators, demonstrated with semilinear PDE examples and numerical experiments.

Conclusion: The novel approach using discrete dual norms enables rigorous convergence analysis for adaptive Newton-FEM methods, extending applicability beyond energy-based problems to a broader class of nonlinear PDEs with strong monotonicity properties.

Abstract: This paper concerns the inclusion of Newton's method into an adaptive finite element method (FEM) for the solution of nonlinear partial differential equations (PDEs). It features an adaptive choice of the damping parameter in the Newton iteration for the discretized nonlinear problems on each level ensuring both global linear and local quadratic convergence. In contrast to energy-based arguments in the literature, a novel approach in the analysis considers the discrete dual norm of the residual as a computable measure for the linearization error. As a consequence, this paper provides the first convergence analysis with optimal rates of an adaptive iteratively linearized FEM beyond energy-minimization problems. The presented theory applies to strongly monotone operators with locally Lipschitz continuous Fréchet derivative. We present a class of semilinear PDEs fitting into this framework and provide numerical experiments to underline the theoretical results.

</details>


### [18] [A Cartesian Cut-Cell Two-Fluid Method for Two-Phase Diffusion Problems](https://arxiv.org/abs/2512.19407)
*Louis Libat,Can Selçuk,Eric Chénier,Vincent Le Chenadec*

Main category: math.NA

TL;DR: A Cartesian cut-cell finite-volume method for sharp-interface two-phase diffusion problems using a two-fluid approach with embedded interface conditions and geometric information based on low-order moments.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for solving two-phase diffusion problems with sharp interfaces in static geometries, handling complex embedded boundaries and various boundary conditions while maintaining conservation properties.

Method: Uses a Cartesian cut-cell finite-volume approach with a two-fluid formulation: independent diffusion equations discretized in each phase on fixed staggered Cartesian grids, coupled through embedded interface conditions enforcing continuity of normal flux and jump laws. Cut cells are treated by integrating equations over phase-restricted control volumes, using only low-order geometric moments (trimmed volumes, apertures, interface measures/centroids) without constructing explicit cut-cell polytopes.

Result: The method demonstrates expected convergence behavior, sharp enforcement of interfacial laws, and excellent conservation properties across 1D, 2D, and 3D benchmarks with curved embedded boundaries, Robin conditions, and strong property/jump contrasts.

Conclusion: The proposed Cartesian cut-cell finite-volume method provides an effective framework for sharp-interface two-phase diffusion problems, with natural extensions to moving interfaces and Stefan-type free-boundary problems.

Abstract: We present a Cartesian cut-cell finite-volume method for sharp-interface two-phase diffusion problems in static geometries. The formulation follows a two-fluid approach: independent diffusion equations are discretized in each phase on a fixed staggered Cartesian grid, while the phases are coupled through embedded interface conditions enforcing continuity of normal flux and a general jump law. Cut cells are treated by integrating the governing equations over phase-restricted control volumes and faces, yielding discrete divergence and gradient operators that are locally conservative within each phase. Interface coupling is achieved by introducing a small set of interfacial unknowns per cut cell on the embedded boundary; the resulting algebraic system involves only bulk and interfacial averages. A key feature of the method is the use of a reduced set of geometric information based solely on low-order moments (trimmed volumes, apertures and interface measures/centroids), allowing robust implementation without constructing explicitly cut-cell polytopes. The method supports steady (Poisson) and unsteady (diffusion) regimes and incorporates Dirichlet, Neumann, Robin boundary conditions and general jumps. We validate the scheme on one-, two- and three-dimensional mono- and diphasic benchmarks, including curved embedded boundaries, Robin conditions and strong property/jump contrasts. The results demonstrate the expected convergence behavior, sharp enforcement of interfacial laws and excellent conservation properties. Extensions to moving interfaces and Stefan-type free-boundary problems are natural perspectives of this framework.

</details>


### [19] [Mixed formulation and structure-preserving discretization of Cosserat rod dynamics in a port-Hamiltonian framework](https://arxiv.org/abs/2512.19408)
*Philipp L. Kinon,Simon R. Eugster,Peter Betsch*

Main category: math.NA

TL;DR: Energy-based modeling framework for nonlinear Cosserat rod dynamics with large displacements/rotations using port-Hamiltonian formulation and structure-preserving discretization.


<details>
  <summary>Details</summary>
Motivation: To develop a robust computational framework for spatial Cosserat rods undergoing large deformations that is objective, locking-free, and preserves energy-momentum consistency while handling finite rotations without singularities.

Method: Mixed formulation with independent displacement, velocity, and stress variables; director formulation for finite rotations; port-Hamiltonian system with quadratic energy; time-differentiated compliance for constraints; structure-preserving finite element discretization; energy-momentum consistent integration.

Result: Developed a novel framework that yields constant mass matrix, avoids singularities, naturally integrates dissipative behavior (generalized-Maxwell) and non-standard actuation (pneumatic/tendon), and maintains port-Hamiltonian structure after discretization.

Conclusion: The framework establishes a new approach to energy-momentum consistent formulations in computational mechanics involving finite rotations, as validated by numerical examples.

Abstract: An energy-based modeling framework for the nonlinear dynamics of spatial Cosserat rods undergoing large displacements and rotations is proposed. The mixed formulation features independent displacement, velocity and stress variables and is further objective and locking-free. Finite rotations are represented using a director formulation that avoids singularities and yields a constant mass matrix. This results in an infinite-dimensional nonlinear port-Hamiltonian (PH) system governed by partial differential-algebraic equations with a quadratic energy functional. Using a time-differentiated compliance form of the stress-strain relations allows for the imposition of kinematic constraints, such as inextensibility or shear-rigidity. A structure-preserving finite element discretization leads to a finite-dimensional system with PH structure, thus facilitating the design of an energy-momentum consistent integration scheme. Dissipative material behavior (via the generalized-Maxwell model) and non-standard actuation approaches (via pneumatic chambers or tendons) integrate naturally into the framework. As illustrated by selected numerical examples, the present framework establishes a new approach to energy-momentum consistent formulations in computational mechanics involving finite rotations.

</details>


### [20] [A massively parallel non-overlapping Schwarz preconditioner for PolyDG methods in brain electrophysiology](https://arxiv.org/abs/2512.19536)
*Caterina B. Leimer Saglio,Stefano Pagani,Paola F. Antonietti*

Main category: math.NA

TL;DR: Non-overlapping Schwarz preconditioners for high-order PolyDG discretizations of brain electrophysiology models, showing improved solver efficiency and parallel scalability.


<details>
  <summary>Details</summary>
Motivation: To develop efficient parallel solvers for large-scale brain electrophysiology simulations using high-order discretizations of coupled monodomain and Barreto-Cressman models.

Method: Uses high-order Polytopal Discontinuous Galerkin (PolyDG) spatial discretization with Crank-Nicolson time scheme and explicit ion term extrapolation. Implements additive Schwarz preconditioners combining local subdomain solvers with coarse-grid correction.

Result: Numerical experiments show robustness with respect to discretization parameters and significant reduction in iteration counts compared to unpreconditioned solvers.

Conclusion: The proposed Schwarz preconditioned PolyDG approach is well-suited for parallel large-scale brain electrophysiology simulations due to its efficiency and scalability.

Abstract: We investigate non-overlapping Schwarz preconditioners for the algebraic systems stemming from high-order discretizations of the coupled monodomain and Barreto-Cressman models, with applications to brain electrophysiology. The spatial discretization is based on a high-order Polytopal Discontinuous Galerkin (PolyDG) method, coupled with the Crank-Nicolson time discretization scheme with explicit extrapolation of the ion term. To improve solver efficiency, we consider additive Schwarz preconditioners within the PolyDG framework, which combines (massively parallel) local subdomain solvers with a coarse-grid correction. Numerical experiments demonstrate robustness with respect to the discretization parameters, as well as a significant reduction in iteration counts compared to the unpreconditioned solver. These features make the proposed approach well-suited for parallel large-scale simulations in brain electrophysiology.

</details>


### [21] [Convergence analysis for non-iterative sequential schemes for Biot's model](https://arxiv.org/abs/2512.19579)
*Xiaozhe Hu,Francisco J. Gaspar,Carmen Rodrigo*

Main category: math.NA

TL;DR: First convergence analysis of explicit fixed-stress split scheme for Biot's equations, proving optimal convergence with inf-sup condition, plus new stabilized decoupled algorithm for linear elements.


<details>
  <summary>Details</summary>
Motivation: Explicit sequential coupling methods for poroelasticity are computationally attractive but lack theoretical analysis. This work addresses the gap by providing the first convergence analysis for the explicit fixed-stress split scheme.

Method: Explicit fixed-stress split scheme: solve flow problem first with time-lagged displacement, then mechanics problem. Also proposes new stabilized decoupled algorithm using piecewise linear finite elements based on recent stabilization techniques.

Result: Proves the explicit fixed-stress split scheme is optimally convergent when finite element discretization satisfies inf-sup condition. Demonstrates the new stabilized decoupled algorithm for linear elements is also optimally convergent.

Conclusion: Provides rigorous theoretical foundation for explicit sequential coupling methods in poroelasticity, making them more reliable for practical applications while maintaining computational efficiency advantages.

Abstract: An alternative to the fully implicit or monolithic methods used for the solution of the coupling of fluid flow and deformation in porous media is a sequential approach in which the fully coupled system is broken into subproblems (flow and mechanics problems) that are solved one after the other. This fully explicit coupling approach is a very simple scheme which allows much flexibility in the implementation and has a lower computational cost, making it quite attractive in practice since smaller linear systems need to be solved in order to obtain the solution for the whole coupled poroelastic system. Due to the appealing advantages of these methods, intensive research is currently being carried out in this direction, as in the present work. Although the application of this type of method is very common in practice, there exist only a few works devoted to their theoretical analysis. In this work, we consider the so-called explicit fixed-stress split scheme, which consists of solving the flow problem first with time-lagging the displacement term, followed by the solution of the mechanics problem. To the best of our knowledge, we provide the first convergence analysis of the explicit fixed-stress split scheme for Biot's equations. In particular, we prove that this algorithm is optimally convergent if the considered finite element discretization satisfies an inf-sup condition. In addition, with the aim of designing the simplest scheme for solving Biot's model, we also propose a similar decoupled algorithm for piecewise linear finite elements for both variables which arises from the novel stabilization recently proposed in \cite{Pe2025}, and is demonstrated to be optimally convergent.

</details>


### [22] [A modified Brinkman penalization fictitious domain method for the unsteady Navier-Stokes equations](https://arxiv.org/abs/2512.19580)
*Zhanybek Baitulenov,Maxim Olshanskii,Almas Temirbekov,Nurlan Temirbekov,Syrym Kasenov*

Main category: math.NA

TL;DR: A modified fictitious domain method with continuation for unsteady Navier-Stokes equations, enabling solution-dependent parameter choice, with existence/convergence proofs and improved convergence rates for strong solutions.


<details>
  <summary>Details</summary>
Motivation: To develop an improved fictitious domain method for unsteady Navier-Stokes equations that allows solution-dependent parameter selection, provides rigorous mathematical foundations, and enables strongly mass-conservative discretization.

Method: Modified fictitious domain method with continuation in lower-order coefficients, enabling solution-dependent critical parameter choice. Uses pointwise divergence-free finite element method for discretization.

Result: Proved global-in-time existence and convergence of weak solutions, local-in-time existence and convergence of unique strong solutions, obtained new higher-order convergence rate estimates for strong solutions, and demonstrated strongly mass-conservative discrete method.

Conclusion: The modified framework successfully enables solution-dependent parameter selection, provides rigorous mathematical guarantees, achieves improved convergence rates, and allows strongly mass-conservative discretization for unsteady Navier-Stokes equations.

Abstract: This paper investigates a modification of the fictitious domain method with continuation in the lower-order coefficients for the unsteady Navier-Stokes equations governing the motion of an incompressible homogeneous fluid in a bounded 2D or 3D domain. The modification enables {a solution-dependent} choice of the critical parameter. Global-in-time existence and convergence of a weak solution to the auxiliary problem are proved, and local-in-time existence and convergence of a unique strong solution are established. For the strong solution, a new higher-order convergence rate estimate in the penalization parameter is obtained. The introduced framework allows us to apply a pointwise divergence free finite element method as a discretization technique, leading to strongly mass conservative discrete fictitious domain method. A numerical example illustrates the performance of the method.

</details>


### [23] [Static size-effects meet the dynamic scattering properties of finite-sized mechanical metamaterials: a relaxed micromorphic study with parameter identification via two-stage static-dynamic optimization](https://arxiv.org/abs/2512.19604)
*Mohammad Sarhil,Leonardo Andres Perez Ramirez,Max Jendrik Voss,Angela Madeo*

Main category: math.NA

TL;DR: The paper investigates the correlation between static size-effects and dynamic scattering in mechanical metamaterials using the relaxed micromorphic model with a two-stage parameter optimization approach.


<details>
  <summary>Details</summary>
Motivation: To understand whether the static size-effects (where "smaller is stiffer") and dynamic reflection/transmission patterns in mechanical metamaterials are correlated, since both phenomena occur when deformation wavelengths become comparable to unit-cell sizes.

Method: Uses the relaxed micromorphic model with a two-stage optimization: first stage fits static parameters via least squares based on total energy from static size-effects; second stage fits dynamic parameters by matching dispersion curves of the model to those of the fully discretized microstructure, comparing results for one vs. two propagation directions and for models with/without curvature.

Result: The paper presents a systematic parameter identification procedure that connects static and dynamic behaviors, allowing assessment of how well different relaxed micromorphic model variants capture both size-effects and wave scattering phenomena.

Conclusion: The proposed two-stage optimization framework enables investigation of correlations between static size-effects and dynamic scattering in mechanical metamaterials, providing insights into microstructure-related phenomena across different loading regimes.

Abstract: Mechanical metamaterials exhibit size-effects when a few unit-cells are subjected to static loading because no clear micro-macro scale separation holds and the characteristic length of the deformation becomes comparable to the unit-cell size. These size-effects typically manifest themselves as a strengthening of the response in a form summarized as "smaller is stiffer". Moreover, the dynamical behavior of mechanical metamaterials is very remarkable, featuring unique phenomena such as dispersive behavior and band-gaps where elastic waves cannot propagate over specific frequency ranges. In these frequency ranges, the wavelength becomes gradually comparable to the unit-cell size, giving rise to microstructure related phenomena which become particularly visible in the reflection/transmission patterns where an incident wave hits the metamaterial's interfaces. This raises the question of whether the static size-effects and dynamic reflection/transmission patterns are correlated.
  In this work, we investigate the interaction of the static size-effects and the dynamic scattering response of mechanical metamaterials by employing the relaxed micromorphic model. We introduce a two-stage optimization procedure to identify the material parameters. In the first stage, the static material parameters are identified by exploiting the static size-effects through a least squares fitting procedure based on the total energy. The dynamic parameters are determined in the second stage by fitting the dispersion curves of the relaxed micromorphic model to those of the fully discretized microstructure. At this second stage, we assess the results obtained by fitting the dispersion curves in one and in two propagation directions, for both the relaxed micromorphic model (RMM) with curvature and its reduced counterpart (RRMM) without curvature.
  The full abstract is presented in the paper.

</details>


### [24] [Milstein-type Schemes for Hyperbolic SPDEs](https://arxiv.org/abs/2512.19647)
*Felix Kastner,Katharina Klioba*

Main category: math.NA

TL;DR: This paper analyzes Milstein-type schemes for hyperbolic semilinear stochastic PDEs with multiplicative Gaussian noise, establishing optimal convergence rates for pathwise uniform strong error.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend convergence analysis from parabolic to hyperbolic stochastic PDEs and from exponential to rational Milstein schemes, while strengthening error estimates from root-mean-square to pathwise uniform bounds.

Method: The study employs Milstein-type schemes (both rational and exponential variants) for approximating hyperbolic semilinear stochastic evolution equations with multiplicative Gaussian noise. The analysis considers mild solutions on Hilbert spaces and derives convergence rates using semigroup theory and stochastic analysis techniques.

Result: For sufficiently regular nonlinearity and noise, the paper establishes strong convergence of order one: rational Milstein schemes achieve $E_h^\infty\lesssim h\sqrt{\log(T/h)}$ and exponential Milstein schemes achieve $E_h^\infty \lesssim h$. Numerical experiments validate these rates for the stochastic Schrödinger equation, with applications to Maxwell's and transport equations.

Conclusion: The research successfully extends Milstein scheme analysis to hyperbolic SPDEs, demonstrates optimal convergence rates for both rational and exponential variants, and strengthens error estimates to pathwise uniform bounds, with practical validation through numerical experiments.

Abstract: This article studies the temporal approximation of hyperbolic semilinear stochastic evolution equations with multiplicative Gaussian noise by Milstein-type schemes. We take the term hyperbolic to mean that the leading operator generates a contractive, not necessarily analytic $C_0$-semigroup. Optimal convergence rates are derived for the pathwise uniform strong error \[
  E_h^\infty := \Big(\mathbb{E}\max_{1\le j \le M}\|U_{t_j}-u_j\|_X^p\Big)^{1/p} \] on a Hilbert space $X$ for $p\in [2,\infty)$. Here, $U$ is the mild solution and $u_j$ its Milstein approximation at time $t_j=jh$ with step size $h>0$ and final time $T=Mh>0$. For sufficiently regular nonlinearity and noise, we establish strong convergence of order one, with the error satisfying $E_h^\infty\lesssim h\sqrt{\log(T/h)}$ for rational Milstein schemes and $E_h^\infty \lesssim h$ for exponential Milstein schemes. This extends previous results from parabolic to hyperbolic SPDEs and from exponential to rational Milstein schemes. Root-mean-square error estimates are strengthened to pathwise uniform estimates. Numerical experiments validate the convergence rates for the stochastic Schrödinger equation. Further applications to Maxwell's and transport equations are included.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [25] [Incompressible limits at large Mach number for a reduced compressible MHD system](https://arxiv.org/abs/2512.18078)
*Francesco Fanelli,Young-Sam Kwon,Aneta Wróblewska-Kamińska*

Main category: math.AP

TL;DR: Singular limit analysis of a reduced compressible non-resistive MHD model, performing incompressible limit with Mach number O(1) via magnetic pressure rescaling.


<details>
  <summary>Details</summary>
Motivation: To study the singular limit problem for a reduced compressible non-resistive MHD model (related to two-fluid models) by performing incompressible limit while keeping Mach number of order O(1), extending previous work to ill-prepared initial data and including Coriolis effects.

Method: Uses global finite energy weak solutions framework with ill-prepared initial data. Employs compensated compactness argument and leverages transport equation underlying the dynamics (known from two-fluid models) to identify asymptotics of O(ε) terms.

Result: Successfully performs incompressible limit while maintaining Mach number O(1), identifies asymptotics of O(ε) terms and characterizes their dynamics, which is crucial for obtaining closed system in the limit.

Conclusion: The approach using transport equation and compensated compactness enables rigorous analysis of singular limit for compressible non-resistive MHD, providing new insights into asymptotic behavior and dynamics of O(ε) terms that previous studies couldn't capture.

Abstract: This paper studies a singular limit problem for a reduced model for compressible non-resistive MHD which was first introduced in \cite{Li-Sun_JDE, Li-Sun} in a two-dimensional setting. This system can also be related to a certain class of two-fluid models. By a suitable rescaling of the magnetic pressure in terms of some parameter $\varepsilon>0$, by letting $\varepsilon\to 0$ we perform the incompressible limit while keeping the Mach number of order $O(1)$.
  The study is conducted in the framework of global in time finite energy weak solutions and for ill-prepared initial data. We also consider a similar problem in presence of a strong Coriolis term. The key ingredient of the proof, based on a compensated compactness argument, is the use of the transport equation (well-known in the context of two-fluid models) underlying the dynamics. Thanks to it, and differently from previous studies about the incompressible limit, we are able to identify the asymptotics of the terms of order $O(\varepsilon)$ and to characterise their dynamics; such an information is in fact crucial to obtain a closed system in the limit.

</details>


### [26] [On well-posedness of the s-Schrödinger maps in the subcritical regime](https://arxiv.org/abs/2512.18170)
*Ahmed Dughayshim*

Main category: math.AP

TL;DR: Local well-posedness of s-Schrödinger map equation in n≥3 dimensions for small initial data in Besov space B^σ_{2,1} with σ≥(n+1)/2.


<details>
  <summary>Details</summary>
Motivation: Study well-posedness of s-Schrödinger map equation in higher dimensions (n≥3) in subcritical regime to establish existence and uniqueness of solutions.

Method: Use Besov space framework B^σ_{2,1} with regularity σ≥(n+1)/2 and smallness condition on initial data norm to prove local well-posedness.

Result: Established local well-posedness for s-Schrödinger map equation when initial data u₀∈B^σ_{2,1} with σ≥(n+1)/2 and ||u₀||_{B^σ_{2,1}}≪1.

Conclusion: The s-Schrödinger map equation is locally well-posed in dimension n≥3 for sufficiently small initial data in sufficiently regular Besov spaces.

Abstract: We study well-posedness of the $s$-Schrödinger map equation in dimension $n \geq 3$ in the subcritical regime, more precisely we establish a local well-posedness result when the initial data is $u_{0} \in B^σ_{2,1}$ with $ σ\geq \frac{n+1}{2}$ and $ \Vert u_{0} \Vert_{B^σ_{2,1}} \ll 1.$

</details>


### [27] [On scattering behavior of corner domains with anisotropic inhomogeneities: part II](https://arxiv.org/abs/2512.18175)
*Pu-Zhao Kow,Mikko Salo,Henrik Shahgholian*

Main category: math.AP

TL;DR: The paper proves that anisotropic inhomogeneous Lipschitz media with certain geometric features (corners in 2D, edge points in higher dimensions) always produce nontrivial scattering for any incoming wave, with results categorized into two cases based on angle properties.


<details>
  <summary>Details</summary>
Motivation: To understand scattering behavior of anisotropic inhomogeneous Lipschitz media with geometric singularities (corners, edges), building on previous work and using free boundary techniques.

Method: Uses free boundary techniques from cited references to analyze scattering of anisotropic inhomogeneous Lipschitz media at fixed wave number, focusing on media with piecewise C¹ or convex penetrable obstacles with corners in 2D and edge points in higher dimensions.

Result: Two main results: 1) In 2D, piecewise C¹ or convex penetrable obstacles with corners, and in higher dimensions, obstacles with edge points always induce nontrivial scattering for any incoming wave. 2) Piecewise C¹ obstacles with corners in 2D (and edge points in higher dimensions) with angles not in πℚ always produce nontrivial scattering.

Conclusion: Geometric singularities (corners, edges) in anisotropic inhomogeneous Lipschitz media guarantee nontrivial scattering regardless of incoming wave, with angle properties playing a role in the scattering behavior.

Abstract: We study the scattering behavior of an anisotropic inhomogeneous Lipschitz medium at a fixed wave number, continuing our previous work [SIAM J. Math. Anal., 56(4):4834-4853, 2024] and using free boundary techniques from [arXiv:2506.22328]. Our main results can be categorized into two distinct cases. In the first case, we show that in two dimensions, piecewise $C^{1}$ or convex penetrable obstacles with corners, and in higher dimensions, obstacles with edge points, always induce nontrivial scattering for any incoming wave. In the second case, we prove that piecewise $C^{1}$ obstacles with corners in two dimensions (and with edge points in higher dimensions) with angles $\notinπ\mathbb{Q}$ always produce nontrivial scattering for any incoming wave.

</details>


### [28] [Boundary Stabilization of a Degenerate Euler-Bernoulli Beam under Axial Force and Time Delay](https://arxiv.org/abs/2512.18179)
*Ben Bakary Junior Siriki,Adama Coulibaly*

Main category: math.AP

TL;DR: Analysis of non-uniform Euler-Bernoulli beam with degenerate flexural rigidity under axial force and delayed boundary control, establishing uniform exponential energy decay.


<details>
  <summary>Details</summary>
Motivation: Extend previous work by Salhi et al. and Siriki et al. by incorporating axial force and generalized control laws (including rotational velocity control) for more complex distributed systems.

Method: Reformulate system as abstract evolution problem in augmented Hilbert space with weighted Sobolev spaces using semigroup theory. Apply energy multiplier method and non-standard Lyapunov functional with weighted integral terms.

Result: Established well-posedness and uniform exponential energy decay with precise decay rate estimate for the beam system with time delay.

Conclusion: Proposed framework provides robust approach for analyzing complex distributed systems with degenerate rigidity, axial forces, and delayed boundary control.

Abstract: This paper provides a qualitative analysis of a non-uniform Euler-Bernoulli beam with degenerate flexural rigidity, subjected to axial force and boundary control with time delay $τ> 0$. By reformulating the system as an abstract evolution problem in an augmented Hilbert space incorporating weighted Sobolev spaces, we employ semigroup theory to ensure well-posedness. Using the energy multiplier method and a non-standard Lyapunov functional featuring weighted integral terms, we establish uniform exponential energy decay and provide a precise decay rate estimate. This work extends the results of Salhi et al. \cite{salhi2025} and Siriki et al. \cite{siriki2025} by incorporating axial force and generalized control laws, including rotational velocity control. The proposed framework offers a robust approach for analyzing complex distributed systems.

</details>


### [29] [Decay estimates for one Aharonov-Bohm solenoid in a uniform magnetic field III: Product cones](https://arxiv.org/abs/2512.18183)
*Haoran Wang*

Main category: math.AP

TL;DR: Extends Euclidean models to conically singular spaces, proving weighted dispersive inequality for Schrödinger equation and dispersive estimate for wave equation with Aharonov-Bohm solenoid in magnetic field on product cone.


<details>
  <summary>Details</summary>
Motivation: To generalize Euclidean models to conically singular spaces, specifically studying quantum mechanical systems with Aharonov-Bohm effects in curved geometries with conical singularities.

Method: Analyzes Schrödinger and wave equations with Aharonov-Bohm solenoid in uniform magnetic field on product cone X = C(S^1_σ) = (0,∞)_r × S^1_σ with flat metric g = dr² + r²dθ², using abstract Keel-Tao argument for Strichartz estimates.

Result: Proves weighted dispersive inequality for Schrödinger equation and dispersive estimate for wave equation, with corresponding Strichartz estimates as byproducts.

Conclusion: Successfully extends Euclidean models to conically singular spaces, establishing fundamental dispersive estimates for quantum equations with Aharonov-Bohm effects in conical geometries.

Abstract: The goal of a recently launched project is to extend the Euclidean models in \cite{Wang24,WZZ25-AHP,WZZ25-JDE} to a more general setting of conically singular spaces. In this paper, the main results include a weighted dispersive inequality for the Schrödinger equation and a dispersive estimate for the wave equation both with one Aharonov-Bohm solenoid in a uniform magnetic field on the product cone $X=\mathcal{C}(\mathbb{S}_σ^1)=(0,+\infty)_r\times\mathbb{S}_σ^1$ endowed with the flat metric $g=dr^2+r^2dθ^2$, where $\mathbb{S}_σ^1\simeq\mathbb{R}/2πσ\mathbb{Z}$ denotes the circle of radius $σ\geq1$ in the Euclidean plane $\mathbb{R}^2$. As a byproduct, we also give the corresponding Strichartz estimates for these equations via the abstract argument of Keel-Tao.

</details>


### [30] [Stability of inverse boundary value problem for the fourth-order Schrödinger equation](https://arxiv.org/abs/2512.18212)
*Yang Liu,Yixian Gao*

Main category: math.AP

TL;DR: Stability analysis for inverse boundary value problem of perturbed fourth-order Schrödinger equation using boundary measurements.


<details>
  <summary>Details</summary>
Motivation: To establish stability results for recovering perturbed potentials from boundary measurements in inverse boundary value problems, which is important for applications in medical imaging, geophysics, and non-destructive testing where internal properties need to be reconstructed from surface measurements.

Method: Uses complex geometric optics solution method and Fourier analysis to analyze the stability of the inverse problem, with estimates depending on a priori information about regularity and support of the inhomogeneity.

Result: Establishes stability results for the perturbed potential based on boundary measurements, showing how the stability estimates depend on various a priori assumptions about the unknown potential's regularity and support.

Conclusion: The paper successfully demonstrates stability for the inverse boundary value problem of perturbed fourth-order Schrödinger equations, providing quantitative estimates that depend on available a priori information about the unknown potential.

Abstract: This paper is concerned with the stability of the inverse boundary value problem for the perturbed fourth-order Schrödinger equation in a bounded domain with Cauchy data. We establish stability results for the perturbed potential relying on boundary measurements. The estimates depend on various a priori information regarding the regularity and the support of the inhomogeneity. The proof primarily utilizes the complex geometric optics solution method and Fourier analysis.

</details>


### [31] [Well-posedness of the Euler system of gas dynamics](https://arxiv.org/abs/2512.18267)
*Eduard Feireisl,Maria Lukacova-Medvidova*

Main category: math.AP

TL;DR: Proposes a two-step selection criterion for dissipative measure-valued solutions of Euler equations, maximizing entropy production then energy defect to identify turbulent solutions.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic selection criterion for measure-valued solutions of the Euler system that can distinguish between classical weak solutions and truly turbulent solutions, addressing the non-uniqueness problem in compressible fluid dynamics.

Method: Two-step maximization process: first maximize entropy production rate, then maximize total energy defect (turbulent energy). This selects solutions that are Borel-measurable in initial data dependence.

Result: The criterion identifies weak solutions in first step if they exist; otherwise selects turbulent solutions in second step. Turbulent solutions have energy defect that vanishes over time, and selected solutions have Borel-measurable dependence on initial data with almost continuous dependence.

Conclusion: The proposed selection criterion provides a systematic way to identify both classical weak solutions and turbulent measure-valued solutions of the Euler system, with desirable mathematical properties including measurable dependence on initial data and vanishing energy defect for turbulent solutions over time.

Abstract: We propose a new two-step selection criterion applicable to the dissipative measure--valued solutions of the Euler system of gas dynamics. The process consists of a successive maximisation of the entropy production rate and the total energy defect, i.e. maximisation of the turbulent energy. If the selected solution is a weak solution of the Euler system, then it is identified in the first step. Solutions selected in the second step are truly measure--valued maximising the energy defect. Accordingly, they are called turbulent solutions. The energy defect of turbulent solutions vanishes with growing time. The selected solutions depend in a Borel--measurable way on the initial data. In particular, they are almost continuously dependent on the initial data.

</details>


### [32] [On the sharp multi-bubble stability for fractional Hardy-Sobolev equations -- A quantitative approach in low dimensions](https://arxiv.org/abs/2512.18350)
*Souptik Chakraborty,Utsab Sarkar*

Main category: math.AP

TL;DR: Sharp quantitative multi-bubble stability for non-sign-changing critical points of fractional Hardy-Sobolev inequality in low dimensions, with linear control of distance to bubble manifold by Euler-Lagrange deficit.


<details>
  <summary>Details</summary>
Motivation: To establish quantitative stability results for multi-bubble configurations in fractional Hardy-Sobolev inequalities, particularly understanding how functions with energy close to bubble superpositions behave and how their distance to the bubble manifold relates to the Euler-Lagrange deficit.

Method: Combines localization scheme adapted to Hardy weight, weighted fractional Kato-Ponce commutator estimates, bubble-wise spectral gap inequality, and sharp interaction analysis. Also constructs matching counterexample to show optimality.

Result: Proves that Euler-Lagrange deficit controls linearly the distance to multi-bubble manifold in homogeneous fractional Sobolev norm, recovers precise bubble configuration, yields quantitative rigidity under arbitrary finite weak interactions, and shows linear rate is optimal.

Conclusion: Establishes sharp quantitative multi-bubble stability for fractional Hardy-Sobolev inequality in low dimensions, providing complete understanding of bubble interactions and optimal stability rates.

Abstract: We establish sharp quantitative multi-bubble stability for non-sign-changing critical points of the fractional Hardy-Sobolev inequality in the low-dimensional regime $2s<N<6s-2t$. For functions whose energy is close to that of a finite superposition of bubbles, we prove that the Euler-Lagrange deficit controls linearly the distance, in the homogeneous fractional Sobolev norm, to the multi-bubble manifold, and we recover the precise bubble configuration. This yields quantitative rigidity under arbitrary finite weak interactions. The proof combines a localization scheme adapted to the Hardy weight, weighted fractional Kato-Ponce commutator estimates, a bubble-wise spectral gap inequality, and a sharp interaction analysis. We also show that the linear rate is optimal by constructing a matching counterexample.

</details>


### [33] [Ground states and phase transitions for an aggregation model with fast diffusion on sphere](https://arxiv.org/abs/2512.18358)
*Razvan C. Fetecau,Hansol Park*

Main category: math.AP

TL;DR: Study of free energy on sphere with nonlinear fast diffusion entropy competing with nonlocal attractive interactions, identifying phase transitions and Dirac mass concentrations in ground states.


<details>
  <summary>Details</summary>
Motivation: Generalize Onsager free energy with dipolar potential used for polymer orientation studies to understand competition between spreading (entropy) and concentration (interaction) effects.

Method: Analyze global energy minimizers of functional combining nonlinear fast diffusion entropy and nonlocal interaction energy, studying phase transitions with respect to interaction strength.

Result: Identified various regimes of fast diffusion exponent and interaction strength producing qualitatively different equilibria and ground states, including Dirac mass concentrations due to reduced diffusion at high densities.

Conclusion: The competition between nonlinear diffusion and nonlocal attraction leads to rich phase behavior with different types of ground states, supported by numerical evidence.

Abstract: We consider a free energy on the sphere that contains an entropy associated to nonlinear fast diffusion, and a nonlocal interaction energy. The two components of the free energy compete with each other, as one favours spreading and the other promotes concentration, respectively. The model is a generalization of the Onsager free energy with dipolar potential, used to study polymer orientation. We study the global energy minimizers of the energy functional, and in particular the various phase transitions that occur with respect to the strength of the nonlocal attractive interactions. In the considered regime, diffusion reduces as the density increases, for which reason the global energy minimizers can contain Dirac mass concentrations. We identify various ranges of the fast diffusion exponent and of the interaction strength, which give qualitatively different equilibria and ground states. The theoretical results are supported by numerical illustrations.

</details>


### [34] [Kuznecov formulae for fractal measures](https://arxiv.org/abs/2512.18379)
*Yakun Xi*

Main category: math.AP

TL;DR: The paper proves an asymptotic formula for Kuznecov sums of singular/fractal measures on Riemannian manifolds, extending classical results from smooth submanifolds to Ahlfors regular measures.


<details>
  <summary>Details</summary>
Motivation: Classical Kuznecov formulae by Zelditch give asymptotic expansions for sums of eigenfunction integrals over smooth submanifolds. The authors aim to extend these results to singular and fractal measures, which are important in geometric analysis and quantum chaos.

Method: The authors work with s-Ahlfors regular measures and introduce an averaged s-density condition. They prove the asymptotic formula using spectral theory and geometric analysis techniques, showing that the remainder term is optimal and cannot generally be improved to power-saving error.

Result: They prove that for s-Ahlfors regular measures with averaged s-density constant A_μ, the Kuznecov sum has asymptotic expansion N_μ(λ) = (2π)^{-(n-s)} vol(B^{n-s}) A_μ λ^{n-s} + o(λ^{n-s}) as λ→∞, where n is manifold dimension and s is measure regularity.

Conclusion: The paper successfully extends Kuznecov formulae to singular/fractal measures, showing that s-Ahlfors regularity and averaged s-density conditions are essentially optimal for one-term asymptotics, with the remainder term being generally optimal.

Abstract: Let $(M,g)$ be a compact, connected Riemannian manifold of dimension $n\ge 2$, and let $\{e_j\}_{j=0}^\infty$ be an orthonormal basis of Laplace eigenfunctions $-Δ_g e_j=λ_j^2 e_j$. Given a finite Borel measure $μ$ on $M$, consider the Kuznecov sum \[
  N_μ(λ):=\sum_{λ_j\le λ}\Bigl|\int_M e_j\,dμ\Bigr|^2. \] Assume that $μ$ is $s$-Ahlfors regular for some $s\in(0,n)$ and admits an averaged $s$-density constant $A_μ$. We prove that \[
  N_μ(λ)
  = (2π)^{-(n-s)}\,{\rm vol}\,(B^{\,n-s})\,A_μ\,λ^{n-s}
  + o(λ^{n-s})
  \qquad (λ\to\infty). \] The hypotheses of $s$-Ahlfors regularity and the averaged $s$-density condition are essentially optimal for such a one-term asymptotic, and in general the remainder $o(λ^{n-s})$ cannot be improved uniformly to a power-saving error term. This extends the classical Kuznecov formulae of Zelditch for smooth submanifold measures to a broad class of singular and fractal measures.

</details>


### [35] [Imaging nonlinearity coefficient and sound speed with the JMGT equation in frequency domain](https://arxiv.org/abs/2512.18431)
*Barbara Kaltenbacher*

Main category: math.AP

TL;DR: Proves uniqueness and stability for reconstructing two coefficients (sound speed and nonlinearity parameter) in the JMGT equation using only two sources, with multiharmonic expansion as key tool, and shows regularization property as relaxation time tends to zero.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inverse problem of reconstructing physical parameters (sound speed and nonlinearity parameter) in nonlinear acoustics from limited observations. This is important for medical imaging, non-destructive testing, and other applications where only limited measurements are available.

Method: Uses multiharmonic expansion of the PDE solution to work in frequency domain, which captures higher harmonics appearing due to nonlinearity. The approach relies on observations from only two sources and analyzes the reconstruction problem as the relaxation time tends to zero, connecting JMGT to the classical Westervelt equation.

Result: Proves uniqueness and stability of reconstruction for two coefficients in the JMGT equation using only two sources. Also demonstrates a regularization property when the relaxation time tends to zero, providing a connection to reconstruction from the Westervelt equation.

Conclusion: The paper establishes theoretical foundations for parameter reconstruction in nonlinear acoustics with limited data, showing that multiharmonic analysis enables stable reconstruction from minimal sources, and bridges JMGT and Westervelt equation approaches through regularization analysis.

Abstract: In this paper we prove uniqueness and stability of reconstruction of two coefficients (sound speed and nonlinearity parameter) in the Jordan-Moore-Gibson-Thompson JMGT equation of nonlinear acoustics, relying on observations resulting from only two sources. A key tool for this purpose is a multiharmonic expansion of the PDE solution, which reflects the physical phenomenon of higher harmonics appearing due to nonlinearity and allows us to work in frequency domain. Based on this result, we derive a regularization property of reconstruction with JMGT as the relexation time tends to zero (in the spirit of a quasi reversibility method) for reconstruction from the classical Westervelt equation.

</details>


### [36] [On the Lavrentiev gap for manifold-valued maps](https://arxiv.org/abs/2512.18447)
*Carlo Alberto Antonini,Filomena De Filippis,Cintia Pacchiano Camacho*

Main category: math.AP

TL;DR: The paper investigates when smooth maps on compact manifolds have modular density properties and when these properties fail.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which smooth maps on compact manifolds exhibit modular density properties, which is important for functional analysis and geometric analysis applications.

Method: Theoretical analysis of modular density properties for smooth maps on compact manifolds, likely using functional analysis techniques and geometric measure theory.

Result: Identifies conditions for validity and failure of modular density properties for smooth maps on compact manifolds.

Conclusion: The paper establishes precise criteria determining when modular density holds or fails for smooth maps on compact manifolds, contributing to functional analysis on manifolds.

Abstract: We investigate the validity and the failure of modular density of smooth maps on compact manifolds.

</details>


### [37] [Stochastic homogenization of coarse-grained elliptic equations](https://arxiv.org/abs/2512.18469)
*Aidan Lau*

Main category: math.AP

TL;DR: The paper proves quenched stochastic homogenization for divergence-form elliptic equations under stationary, ergodic, integrable coefficients with coarse-grained ellipticity, and recovers a sufficient joint integrability condition.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous homogenization results for elliptic equations with random coefficients under minimal regularity assumptions, particularly focusing on quenched (almost sure) convergence rather than annealed (in expectation) results.

Method: The authors prove quenched stochastic homogenization using probabilistic methods, assuming coefficients are stationary, ergodic, integrable, and satisfy a coarse-grained ellipticity condition that requires coefficients remain bounded in a negative regularity sense on large scales.

Result: The main result is a proof of quenched stochastic homogenization for divergence-form elliptic equations under the stated assumptions. As a corollary, they recover a sufficient joint integrability condition on both symmetric and skew-symmetric parts of the coefficient field.

Conclusion: The paper provides a rigorous framework for quenched homogenization with minimal regularity requirements, establishing that coarse-grained ellipticity is sufficient for homogenization and yielding practical integrability conditions for applications.

Abstract: We prove quenched stochastic homogenization for divergence-form elliptic equations, under the assumption that the coefficients are stationary, ergodic, integrable, and satisfy a coarse-grained ellipticity assumption. The ellipticity assumption requires that the coefficients remain bounded in a negative regularity sense on large scales. As a corollary, we recover a sufficient joint integrability condition on the symmetric and skew-symmetric parts of the coefficient field.

</details>


### [38] [Global Regular Solutions of the Multidimensional Degenerate Compressible Navier-Stokes Equations with Large Initial Data of Spherical Symmetry](https://arxiv.org/abs/2512.18545)
*Gui-Qiang G. Chen,Jiawen Zhang,Shengguo Zhu*

Main category: math.AP

TL;DR: For compressible Navier-Stokes with degenerate density-dependent viscosity, spherically symmetric flows remain globally regular and cannot develop singularities like cavitation or implosion, unlike the constant viscosity case.


<details>
  <summary>Details</summary>
Motivation: Address fundamental open problem: whether regular spherically symmetric compressible Navier-Stokes flows can develop singularities (cavitation/implosion) in finite time. Challenge: overcoming coordinate singularity at origin to control density bounds.

Method: Carefully designed weighted radial estimates via region segmentation method to obtain uniform control over both density and effective velocity. This methodology handles difficulties from degenerate density-dependent viscosity.

Result: For general large spherically symmetric initial data with bounded positive density, solutions remain globally regular and cannot undergo cavitation or implosion in 2D and 3D. Results hold for all γ∈(1,∞) in 2D and physical γ∈(1,3) in 3D, without restriction on initial data size.

Conclusion: Situation fundamentally different from constant viscosity case - with degenerate density-dependent viscosity, implosions do not occur. Methodology developed should be useful for solving other related nonlinear PDEs with similar difficulties.

Abstract: A fundamental open problem in the theory of the compressible Navier-Stokes equations is whether regular spherically symmetric flows can develop singularities -- such as cavitation or implosion -- in finite time. A formidable challenge lies in how the well-known coordinate singularity at the origin can be overcome to control the lower or upper bound of the density. For the barotropic Navier-Stokes system with constant viscosity coefficients, recent striking results have shown that such implosions do indeed occur. In this paper, we show that the situation is fundamentally different when the viscosity coefficients are degenerately density-dependent (as in the shallow water equations). We prove that, for general large spherically symmetric initial data with bounded positive density, solutions remain globally regular and cannot undergo cavitation or implosion in two and three spatial dimensions. Our results hold for all adiabatic exponents $γ\in (1,\infty)$ in two dimensions, and for physical adiabatic exponents $γ\in (1, 3)$ in three dimensions, without any restriction on the size of the initial data. To achieve these results, we make carefully designed weighted radial estimates via a region segmentation method, which is the key for obtaining uniform control over both the density and the effective velocity. The methodology developed here should also be useful for solving other related nonlinear partial differential equations involving similar difficulties.

</details>


### [39] [Infinitely many solutions for a class of resonant problems](https://arxiv.org/abs/2512.18562)
*Philip Korman*

Main category: math.AP

TL;DR: The paper studies radially symmetric solutions for resonant elliptic problems on a unit ball, where the nonlinearity is periodic with mean zero. The number of solutions (infinite vs. finite) depends on space dimension n, with different behavior in cases: 1≤n≤3, n=4, n=5, n=6, and n≥7.


<details>
  <summary>Details</summary>
Motivation: To understand the solution structure of resonant elliptic boundary value problems with periodic nonlinearities, particularly how the dimension of the space affects the existence and multiplicity of radially symmetric solutions.

Method: Analysis of radially symmetric solutions for the resonant elliptic PDE on a unit ball, using variational methods and bifurcation theory to study the interplay between the principal eigenvalue and the periodic nonlinearity.

Result: The problem exhibits different solution multiplicities depending on space dimension: infinite solutions for some dimensions, finite solutions for others, with distinct behavior in five different dimensional regimes.

Conclusion: The dimension n plays a crucial role in determining solution multiplicity for resonant problems with periodic nonlinearities, revealing a rich dimensional dependence structure with five distinct regimes.

Abstract: We consider radially symmetric solutions for a class of resonant problems on a unit ball $B \subset R^n$ around the origin \[ Δu+\la _1 u +g(u)=f(r) \s \mbox{for $x \in B$}, \s u=0 \s \mbox{on $\partial B$} \,. \] Here the function $g(u)$ is periodic of mean zero, $x \in R^n$, $r=|x|$, $\la _1$ is the principal eigenvalue of $Δ$ on $B$. The problem has either infinitely many or finitely many solutions depending on the space dimension $n$. The situation turns out to be different for each of the following cases: $1 \leq n \leq 3$, $n=4$, $n=5$, $n=6$, and $n \geq 7$.

</details>


### [40] [Sharp criteria for a degenerate diffusion-aggregation system with the intermediate exponent](https://arxiv.org/abs/2512.18576)
*Tiantian Zhou,Li Chen,Yutian Lei*

Main category: math.AP

TL;DR: The paper establishes two equivalent sharp criteria for global existence vs. finite-time blow-up in multi-dimensional nonlocal degenerate diffusion-aggregation equations with singular potentials.


<details>
  <summary>Details</summary>
Motivation: To understand the critical thresholds governing solution behavior in nonlocal degenerate diffusion-aggregation equations with singular potentials, where existing results require L∞ boundedness of initial data.

Method: Analyzes the equation with diffusion exponent m in intermediate range and singular aggregation potential |x|^{-γ}. Uses two different initial data assumptions to establish two criteria based on L-norms vs. total mass/extremal functions. Employs moment estimates and Lions-Aubin Lemma for compactness arguments on whole space.

Result: Proves two sharp criteria (Theorems 1.1 and 1.2) for global existence and finite-time blow-up. Shows the two initial free energy conditions are equivalent, and consequently the two criteria themselves are equivalent, unifying classification results from different approaches.

Conclusion: The paper successfully establishes and unifies sharp criteria for solution behavior in nonlocal diffusion-aggregation equations, removing the L∞ boundedness requirement and providing a comprehensive classification framework.

Abstract: In this paper, we investigate a multi-dimensional nonlocal degenerate diffusion-aggregation equation with a diffusion exponent $m$ in the intermediate range $\frac{2d}{2d-γ}<m<\frac{d+γ}{d}$, where the nonlocal aggregation term is given by singular potential $|x|^{-γ}$, $0<γ\leq d-2$. Under two different assumptions on the initial data, we establish two sharp criteria (i.e., the critical thresholds in Theorem 1.1 and Theorem 1.2) governing the global existence and finite-time blow-up of solutions. Once the initial free energy is less than a constant that depends on the total mass (or depends on the extremum function of the Hardy-Littlewood-Sobolev inequality), the first criterion depends on the relationship between the $L^{\frac{2d}{2d-γ}}$-norm of initial data and total mass, while the second relies on the relationship between the $L^m$-norm of initial data and extremal function.
  In the discussion of the second criterion, we do not require $L^\infty(\mathbb{R}^d)$ boundedness of the initial data, which is necessary in reference \cite{B}. Furthermore, with the help of moment estimate, we manage to prove the compactness argument on the whole space by using the Lions-Aubin Lemma.
  Importantly, we demonstrate that the two initial free energy conditions on which two criteria are based are equivalent. Building on this, we further prove that the two sharp criteria themselves are also equivalent, thereby unifying the classification results obtained from two different approaches.

</details>


### [41] [Boundary regularity of a fourth order Alt-Caffarelli problem and applications to the minimization of the critical buckling load](https://arxiv.org/abs/2512.18626)
*Jimmy Lamboley,Mickaël Nahon*

Main category: math.AP

TL;DR: The paper studies a higher-order analogue of the Alt-Caffarelli functional related to shape optimization problems, particularly minimizing the critical buckling load of clamped plates. It establishes boundary regularity results in 2D, proving full analytic regularity except at certain angles.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand shape optimization problems involving higher-order functionals, specifically the minimization of critical buckling loads for clamped plates with fixed area. This extends classical Alt-Caffarelli theory to higher-order settings.

Method: The method uses a monotonicity formula discovered by Dipierro, Karakhanyan, and Valdinoci, enhanced with a new epiperimetric inequality. This combination allows for proving regularity results up to the boundary in two dimensions.

Result: The main results are: 1) Several boundary regularity results in two dimensions, 2) Full analytic regularity of the boundary (outside angles of approximately 1.43π) near points of density less than 1 in optimal shapes.

Conclusion: The paper successfully extends regularity theory to higher-order shape optimization problems, providing analytic boundary regularity except at specific critical angles, advancing understanding of plate buckling optimization.

Abstract: We study a higher order analogue to the Alt-Caffarelli functional that arises in several shape optimization problems, among which the minimization of the critical buckling load of a clamped plate of fixed area. We obtain several regularity results up to the boundary in two dimensions, in particular we prove the full regularity of the boundary (analytic outside angles of opening $\approx 1.43π$) near any point of density less than 1 of the optimal shape. These results are based on the monotonicity formula discovered by Dipierro, Karakhanyan, and Valdinoci, which we improve with a new epiperimetric inequality.

</details>


### [42] [Inverse problems with integral conditions for the generalized Korteweg-de Vries equation](https://arxiv.org/abs/2512.18649)
*Oleg S. Balashov,Andrei V. Faminskii*

Main category: math.AP

TL;DR: The paper establishes well-posedness results for three inverse problems involving the generalized KdV equation with integral conditions, without restrictions on nonlinearity growth rate, using various control strategies.


<details>
  <summary>Details</summary>
Motivation: To address inverse problems for the generalized KdV equation where either the equation's right-hand side, boundary data, or both serve as controls, particularly focusing on cases without limitations on nonlinearity growth rates.

Method: Analysis of three inverse problems with integral conditions on bounded intervals for the generalized KdV equation, considering regular time-dependent solutions and using either the equation's right-hand side, boundary data, or both as control mechanisms.

Result: Established well-posedness results for all three inverse problems, requiring either smallness of input data or smallness of the time interval for the analysis to hold.

Conclusion: The paper successfully demonstrates well-posedness for multiple inverse problem formulations of the generalized KdV equation with integral conditions, providing a framework that accommodates unrestricted nonlinearity growth rates under certain smallness conditions.

Abstract: Results on well-posedness of three inverse problems with integral conditions on a bounded interval for the generalized Korteweg-de Vries equation without any restrictions on the growth rate of nonlinearity are established. Either the right-hand side of equation or the boundary data, or both are chosen as controls. The considered solutions are regular with respect to the time variable. Assumptions on smallness of the input data or smallness of a time interval are required.

</details>


### [43] [Qualitative analysis of multi-peak solutions for Nonlinear Schrödinger equations with nearly critical Sobolev exponents](https://arxiv.org/abs/2512.18663)
*Zhongyuan Liu,Shuying Tian,Huafei Xie,Pingping Yang*

Main category: math.AP

TL;DR: The paper establishes local uniqueness and Morse index for multi-peak solutions of nonlinear Schrödinger equations with critical exponent when the potential has non-degenerate critical points.


<details>
  <summary>Details</summary>
Motivation: Previous work by Cao et al. (2025) proved existence of multi-peak solutions for nonlinear Schrödinger equations with critical exponent. The current paper aims to further characterize these solutions by establishing their local uniqueness and Morse index, providing deeper qualitative understanding of solution properties.

Method: Uses blow-up analysis based on Pohozaev identities to analyze multi-peak solutions. The approach leverages the assumption that the potential function V(x) possesses k non-degenerate critical points.

Result: Establishes local uniqueness of multi-peak solutions and determines their Morse index for the nonlinear Schrödinger equation with critical exponent p = (N+2)/(N-2) in dimensions N ≥ 6.

Conclusion: The paper successfully characterizes qualitative properties of multi-peak solutions, showing they are locally unique and have computable Morse index when the potential has non-degenerate critical points, extending the existence results from previous work.

Abstract: In this paper, we are concerned with qualitative properties of multi-peak solutions of the following nonlinear Schrödinger equations \begin{equation*} -Δu+V(x)u= u^{p-\varepsilon},\,\,\,u>0,\,\,\,\text{in}\,\,\,\mathbb{R}^N, \end{equation*} where $V(x)$ is a nonnegative continuous function, $\varepsilon>0$, $p=\frac{N+2}{N-2}$, $N\geq6$. The existence of multi-peak solutions has been obtained by Cao et al. (Calc. Var. Partial Differential Equations, 64: 139, 2025). The main objective in this paper is to establish the local uniqueness and Morse index of the multi-peak solutions in \cite{CLl1} provided that $V(x)$ possesses $k$ non-degenerate critical points by using the blow-up analysis based on Pohozaev identities.

</details>


### [44] [Best constants for Hardy inequalities in Triebel--Lizorkin spaces](https://arxiv.org/abs/2512.18688)
*Michał Kijaczko*

Main category: math.AP

TL;DR: Sharp constants found for fractional Hardy inequalities in weighted Triebel-Lizorkin seminorms on whole space and half-spaces, generalizing previous weighted fractional Hardy inequalities for Gagliardo seminorms.


<details>
  <summary>Details</summary>
Motivation: To extend and generalize existing weighted fractional Hardy inequalities from Gagliardo seminorms to the broader class of Triebel-Lizorkin seminorms, establishing sharp constants in these inequalities.

Method: Mathematical analysis approach to find sharp constants in fractional Hardy inequalities for weighted Triebel-Lizorkin seminorms, working on both whole space and half-space domains.

Result: Successfully determined sharp constants for fractional Hardy inequalities in weighted Triebel-Lizorkin seminorms, with results that are new even for the unweighted case and generalize previous work on Gagliardo seminorms.

Conclusion: The paper establishes sharp fractional Hardy inequalities for Triebel-Lizorkin seminorms, providing a significant generalization of existing results and new insights even in the unweighted setting.

Abstract: We find sharp constants in fractional Hardy inequalities for weighted Triebel--Lizorkin seminorms on the whole space and half-spaces. Our results generalize recently obtained weighted fractional Hardy inequalities for Gagliardo seminorms, but are new even for the unweighted case.

</details>


### [45] [Multiscale homogenization of non-local energies of convolution-type](https://arxiv.org/abs/2512.18697)
*Giuseppe Cosma Brusca*

Main category: math.AP

TL;DR: The paper analyzes non-local convolution-type functionals with two small parameters (ε for localization, δ for oscillation) and computes Γ-limits for different scaling regimes based on λ = ε/δ.


<details>
  <summary>Details</summary>
Motivation: To understand the interplay between localization effects (from non-local interactions) and homogenization effects (from periodic microstructures) when both occur simultaneously at different scales.

Method: Analyze a family of non-local integral functionals with two small parameters ε and δ, where ε controls localization length-scale and δ controls oscillation scale. Study the asymptotic behavior as both parameters tend to zero, focusing on the limit λ = lim(ε/δ). Use Γ-convergence techniques to compute limits in strong L^p-topology.

Result: Three different regimes emerge depending on λ: 1) λ = 0 (localization dominates), 2) λ ∈ (0,∞) (critical regime with balanced scales), 3) λ = ∞ (homogenization dominates). The Γ-limit is computed for each regime, showing how the interplay between localization and homogenization depends on the relative scaling.

Conclusion: The separation of scales between localization and homogenization is determined by the parameter λ = ε/δ. The critical regime occurs when λ ∈ (0,∞), where both effects interact nontrivially. The analysis provides a complete picture of how non-local functionals behave under simultaneous localization and homogenization.

Abstract: We analyze a family of non-local integral functionals of convolution-type depending on two small positive parameters $\varepsilon,δ$: the first rules the length-scale of the non-local interactions and produces a `localization' effect as it tends to $0$, the second is the scale of oscillation of a finely inhomogeneous periodic structure in the domain. We prove that a separation of the two scales occurs and that the interplay between the localization and homogenization effects in the asymptotic analysis is determined by the parameter $λ$ defined as the limit of the ratio $\varepsilon/δ$. We compute the $Γ$-limit of the functionals with respect to the strong $L^p$-topology for each possible value of $λ$ and detect three different regimes, the critical scale being obtained when $λ\in(0,+\infty)$.

</details>


### [46] [Rigidity for homogeneous solutions to the two-dimensional Euler equations in sector-type domains](https://arxiv.org/abs/2512.18700)
*Li Li,Xukai Yan,Zhibo Yang*

Main category: math.AP

TL;DR: The paper proves rigidity results for homogeneous solutions to 2D stationary Euler equations in sector domains, showing that under certain boundary homogeneity and non-vanishing conditions, solutions must be homogeneous throughout the domain.


<details>
  <summary>Details</summary>
Motivation: To understand the rigidity properties of homogeneous solutions to the stationary Euler equations in sector-type domains, which is important for characterizing solution structures in fluid dynamics.

Method: Analyzes $(-α)$-homogeneous solutions in sector domains $Ω_{a,b,θ_0}$, considering different boundary cases (inner radius a=0 or a>0, outer radius b=∞ or b<∞), and imposes boundary homogeneity assumptions plus non-vanishing conditions on velocity components.

Result: For each type of domain configuration, if a solution satisfies boundary homogeneity assumptions and either the radial or angular velocity component doesn't vanish in the domain (excluding origin), then the solution must be homogeneous throughout the entire domain.

Conclusion: The paper establishes strong rigidity properties for homogeneous solutions to 2D stationary Euler equations in sector domains, showing that local homogeneity conditions combined with non-vanishing velocity components force global homogeneity.

Abstract: We study the rigidity problem for $(-α)$-homogeneous solutions to the two-dimensional incompressible stationary Euler equations in sector-type domains $Ω_{a, b, θ_0}:= \{(r,θ): a<r<b, \ 0<θ<θ_0\}$, where $α\in\mathbb{R}$, $0\leqslant a < b \leqslant +\infty$ and $0< θ_0 \leqslant 2π$. For each type of domains, depending on whether $a = 0$ or $a > 0$, and $b = +\infty$ or $b < +\infty$, we show that if a solution satisfies some homogeneity assumptions on the boundary of $Ω_{a, b, θ_0}$ and if the radial or angular component of the velocity does not vanish in $\overline{Ω_{a, b, θ_0}}\setminus\{\bm{0}\}$, then it must be homogeneous throughout $\overline{Ω_{a, b, θ_0}}\setminus\{\bm{0}\}$.

</details>


### [47] [Nonlocal conservation laws with p-norm, the singular limit problem and applications to traffic flow](https://arxiv.org/abs/2512.18701)
*Felisia Angela Chiarello,Alexander Keimer,Lukas Pflug*

Main category: math.AP

TL;DR: Study of scalar nonlocal conservation laws with p-norm integration, extending beyond L¹ to L^p spaces including p∈(0,1), proving existence/uniqueness, singular limits to local conservation laws, and convergence as p→0.


<details>
  <summary>Details</summary>
Motivation: Previous nonlocal conservation laws typically use L¹ integration, but this paper extends to L^p norms including fractional p∈(0,1), investigating how different integration metrics affect solutions and their convergence to local conservation laws.

Method: Mathematical analysis of scalar nonlocal conservation laws with p-norm integration, establishing existence/uniqueness theorems for bounded initial data, studying singular limits as kernel approaches Dirac distribution, analyzing monotonicity properties, and proving convergence as p→0.

Result: Proved existence/uniqueness of weak solutions for initial data bounded away from zero; established singular limit convergence to local entropy solutions for exponential kernel for all p∈(0,∞); showed convergence as p→0 results in different nonlocal conservation law; demonstrated monotonicity preservation.

Conclusion: The p-norm generalization significantly extends nonlocal conservation law theory beyond L¹ case, showing robustness of nonlocal approximations to local conservation laws across different integration metrics, with interesting convergence behaviors as p→0.

Abstract: In this contribution, we study scalar nonlocal conservation laws with the $p$-norm. Here, 'nonlocal' means that the velocity of the conservation law depends on an integral term in space. Typically, the nonlocal term consists of integrating the solution in $L^{1}$, whereas here we will study the case when the solution is integrated in the $L^{p}$-norm. We consider even the case of the $L^{p}$ metric when $p\in (0,1)$ and establish, for an initial datum which is uniformly bounded away from zero, the existence and uniqueness of weak solutions. We then demonstrate that there are also solutions to the initial datum being zero under more restrictive assumptions. Furthermore, we investigate the singular limit, i.e., what happens when the nonlocal kernel converges to a Dirac distribution. Indeed, for the one-sided exponential kernel, we recover the (entropy) solution of the corresponding local conservation law for all $p\in(0,\infty)$ with further restrictions for $p\in(0,1)$. This generalizes the celebrated singular limit result for nonlocal conservation laws for $p=1$ significantly and showcases the robustness of the approximation of local conservation laws by nonlocal ones. We investigate also the monotonicity of the solution when assuming that the initial datum is monotone. Finally, we prove the convergence of solutions for $p\rightarrow 0$ on a small time horizon, resulting in a different kind of nonlocal conservation law. Numerical studies showcasing the effect of $p$ on the singular limit convergence and more conclude the contribution.

</details>


### [48] [Non-homogeneous conormal derivative problem for quasilinear elliptic equations with Morrey data](https://arxiv.org/abs/2512.18742)
*Dian K. Palagachev,Lubomira G. Softova*

Main category: math.AP

TL;DR: The paper proves global essential boundedness for weak solutions to quasilinear elliptic equations with non-homogeneous conormal derivative conditions, generalizing classical L^p results to Morrey space framework.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend classical boundedness results for solutions to elliptic equations (like those by Ladyzhenskaya and Ural'tseva) from the L^p framework to more general Morrey space settings, allowing for more irregular coefficient behavior.

Method: The authors consider quasilinear divergence form elliptic equations modeled on the m-Laplacian with Carathéodory nonlinear terms satisfying controlled growth conditions. They use Morrey space theory to control the x-behavior of coefficients and prove global essential boundedness for weak solutions.

Result: The main result is the proof of global essential boundedness for weak solutions to the non-homogeneous conormal derivative problem, generalizing classical L^p results to the Morrey space framework.

Conclusion: The paper successfully extends the classical boundedness theory for elliptic equations to the more general setting of Morrey spaces, providing a framework that accommodates more irregular coefficient behavior while maintaining essential boundedness of solutions.

Abstract: A non-homogeneous conormal derivative problem is considered for quasilinear divergence form elliptic equations modeled on the $m$-Laplacian operator. The nonlinear terms are given by Carathéodory functions and satisfy controlled growth structure conditions with respect to the solution and its gradient, while their $x$-behaviour is controlled in terms of suitable Morrey spaces.
  Global essential boundedness is proved for the weak solutions, generalizing thus the classical $L^p$-result of Ladyzhenskaya and Ural'tseva to the framework of the Morrey scales.

</details>


### [49] [Sharp Fractional Sobolev Embeddings on Closed Manifolds](https://arxiv.org/abs/2512.18770)
*Hao Tan,Zetian Yan,Zhipeng Yang*

Main category: math.AP

TL;DR: The paper develops a fractional Sobolev framework on Riemannian manifolds using heat kernels, analyzes critical fractional Sobolev embeddings, determines optimal coefficients for lower-order terms, proves limitations on sharp inequalities, establishes almost sharp inequalities, and derives improved inequalities under orthogonality constraints.


<details>
  <summary>Details</summary>
Motivation: To extend fractional Sobolev theory to Riemannian manifolds using intrinsic heat-kernel methods, addressing the critical embedding problem and understanding the limitations and optimal forms of Sobolev inequalities in this geometric setting.

Method: Develops an intrinsic fractional Sobolev framework on closed Riemannian manifolds based on heat kernels. Studies critical fractional Sobolev embedding, determines optimal coefficients for lower-order L^p terms, analyzes sharpness of inequalities, establishes almost sharp inequalities with near-Euclidean constants, and derives improved inequalities under orthogonality constraints with sign-changing test functions.

Result: 1) Determined optimal coefficient of lower-order L^p term; 2) Proved fully sharp p-power inequality cannot hold globally in superquadratic range; 3) Established almost sharp inequality with leading constant arbitrarily close to Euclidean best constant; 4) Derived improved inequalities under finitely many orthogonality constraints with respect to sign-changing test families.

Conclusion: The paper provides a comprehensive fractional Sobolev framework on Riemannian manifolds, characterizes optimal inequalities, establishes limitations on sharpness, and shows how orthogonality constraints can lead to improved inequalities, advancing the understanding of functional inequalities in geometric settings.

Abstract: We develop an intrinsic, heat-kernel based fractional Sobolev framework on closed Riemannian manifolds and study the critical fractional Sobolev embedding. We determine the optimal coefficient of the lower-order $L^{p}$ term and prove that the fully sharp $p$-power inequality cannot hold globally in the superquadratic range. We further establish an almost sharp inequality whose leading constant is arbitrarily close to the Euclidean best constant, and we derive improved inequalities under finitely many orthogonality constraints with respect to sign-changing test families.

</details>


### [50] [On the effects of surface roughness in non-isothermal porous medium flow](https://arxiv.org/abs/2512.18787)
*María Anguiano,Igor Pažanin,Francisco J. Suárez-Grau*

Main category: math.AP

TL;DR: Asymptotic analysis of non-isothermal Darcy-Brinkman thin-film flow with oscillating boundary and viscous dissipation, deriving effective models via periodic unfolding method.


<details>
  <summary>Details</summary>
Motivation: To understand heat transfer and flow behavior in thin films with rough boundaries where viscous dissipation acts as a heat source, which is important for applications involving lubrication, microfluidics, and porous media flows.

Method: Combined asymptotic analysis and periodic unfolding method to analyze convergence as film thickness and roughness period tend to zero; derived limit problems with coupled elliptic systems including Reynolds-type equations, Darcy-Brinkman cell problems, and reduced energy equation.

Result: Established convergence of velocity, pressure, and temperature fields; derived effective models that depend on relative scaling of roughness wavelength; found strong coupling induced by oscillatory geometry in critical roughness regime that doesn't occur in smooth-boundary cases.

Conclusion: The oscillatory boundary geometry in thin-film flows creates unique coupling effects in the critical roughness regime, leading to more complex effective models than smooth-boundary cases, with implications for heat transfer and flow control in applications with surface roughness.

Abstract: We analyze a non-isothermal Darcy-Brinkman thin-film flow with a periodically oscillating boundary and viscous dissipation acting as a heat source. Using asymptotic analysis and the periodic unfolding method, we establish the convergence of velocity, pressure, and temperature fields as the small parameter (related to the film thickness and the period of the roughness) tends to zero. The limit problems depend on the relative scaling of the roughness wavelength and consist of coupled elliptic systems combining Reynolds-type equations with Darcy-Brinkman cell problems and reduced energy equation. In the critical roughness regime, the effective model exhibits a strong coupling induced by the oscillatory geometry, which does not occur in a smooth-boundary case.

</details>


### [51] [Positivity and long-term behaviour of a diffusion model with measure-valued nonlocal reaction term](https://arxiv.org/abs/2512.18799)
*Xiao Yang,Qiyao Peng,Sander C. Hille*

Main category: math.AP

TL;DR: Analysis of a diffusion equation with nonlocal singular reaction (Dirac source/sink at origin) showing positivity preservation conditions and convergence to constant steady states outside observation region.


<details>
  <summary>Details</summary>
Motivation: To understand behavior of diffusion systems with nonlocal sensing and localized intervention, modeling control systems that sense concentration at distance but act at specific location (origin).

Method: Extensive use of Laplace transform arguments to analyze the diffusion equation with Dirac source/sink reaction term.

Result: Identified parameter regime and conditions on positive initial conditions (monotonicity and symmetry) that guarantee solution positivity for all time. Also provided conditions ensuring convergence to constant steady state outside observation region.

Conclusion: The paper establishes mathematical conditions under which positivity is preserved and convergence occurs in diffusion systems with nonlocal singular reactions, providing insights for control system modeling.

Abstract: The behaviour is investigated of solutions to a diffusion equation on the real line with nonlocal and singular reaction term, i.e., given by a Dirac source or sink at the origin. It gives a simplified representation of for example a control system that senses concentration at a distance, but "intervenes" at the origin. Positivity of solutions (for positive initial conditions) cannot be guaranteed for all parameter settings in the model. We determine a parameter regime and conditions on the positive initial condition in terms of monotonicity and symmetry, that do allow us to conclude the positivity of the solution for all time. In addition, we provide conditions that ensure convergence of the system to a constant steady state (pointwise), outside the region of observation. Technically, we extensively use Laplace transform arguments to achieve these results.

</details>


### [52] [Continuous in time bubbling and Soliton Resolution for Non-negative Solutions of the Energy-Critical Heat Flow](https://arxiv.org/abs/2512.18840)
*Shrey Aryan*

Main category: math.AP

TL;DR: The paper proves soliton resolution for energy-critical nonlinear heat flow in dimensions d≥3, showing solutions decompose into solitons, radiation, and vanishing error.


<details>
  <summary>Details</summary>
Motivation: To establish the Soliton Resolution Conjecture for energy-critical nonlinear heat flow, which describes how solutions asymptotically decompose into fundamental components as time evolves.

Method: Analysis of finite energy solutions to the energy-critical nonlinear heat flow equation in dimensions d≥3, likely using PDE techniques, energy methods, and soliton decomposition theory.

Result: Proves that any finite energy solution asymptotically resolves into a sum of possibly time-dependent solitons, radiation, and an error term vanishing in the energy space. Settles the Soliton Resolution Conjecture for non-negative initial data in all dimensions d≥3.

Conclusion: The Soliton Resolution Conjecture is established for energy-critical nonlinear heat flow in dimensions d≥3, providing complete asymptotic decomposition of solutions into solitons and radiation components.

Abstract: We show that any finite energy solution of the energy-critical nonlinear heat flow in dimensions $d\geq 3$ asymptotically resolves into a sum of possibly time-dependent solitons, a radiation term, and an error term that vanishes in the energy space. As a consequence, when the initial data has finite energy and is non-negative, we settle the Soliton Resolution Conjecture for all dimensions $d\geq 3.$

</details>


### [53] [The lifespan of strong solutions to the compressible MHD equations with entropy transport in the presence of vacuum](https://arxiv.org/abs/2512.18911)
*Yongteng Gu,Xiangdi Huang*

Main category: math.AP

TL;DR: Finite-time blow-up of strong solutions to compressible MHD system without magnetic diffusion, with lifespan upper bound estimation.


<details>
  <summary>Details</summary>
Motivation: Investigate singularity formation in compressible MHD systems without magnetic diffusion, extending previous results from isentropic MHD to systems with entropy transport and free boundary problems.

Method: Establish local well-posedness for bounded domains, analyze singularity formation in 2D radially symmetric and 3D cylindrically symmetric cases, prove blow-up when initial density vanishes in interior region with non-trivial magnetic field, extend to free boundary problem with boundary expansion analysis.

Result: Prove finite-time blow-up occurs when initial density vanishes in interior region containing origin with non-trivial magnetic field in vacuum region, derive explicit upper bound for solution lifespan, generalize previous results to entropy transport systems and free boundary problems.

Conclusion: Compressible MHD systems without magnetic diffusion exhibit finite-time blow-up under specific vacuum and magnetic field conditions, with explicit lifespan estimates, extending and improving previous isentropic MHD results to more general systems.

Abstract: In this paper, we investigate the finite time blow-up of strong solutions to the compressible magnetohydrodynamic (MHD) system (without magnetic diffusion) coupled with entropy transport, and derive an upper bound for the lifespan of such solutions. We first establish the local well-posedness of strong solutions for bounded domains and study the mechanism of finite-time singularity formation in the 2D radially symmetric case and 3D cylindrically symmetric case. We prove that if the initial density vanishes in an interior region containing the origin and the magnetic field is non-trivial within this vacuum region, the strong solution must blow up in finite time. These results generalize and improve the previous results of Huang-Xin-Yan [Math. Ann. 392 (2025) 2365-2394] for the compressible isentropic MHD equations. Significantly, we extend this blow-up result to the free boundary problem. Our analysis of the boundary's expansion allows us to explicitly estimate the maximum lifespan of the solution.

</details>


### [54] [Global strong solutions and asymptotic behavior for arbitrarily large initial data of the 2D compressible Navier-Stokes equations with transport entropy](https://arxiv.org/abs/2512.18976)
*Jie Fan,Xiangdi Huang*

Main category: math.AP

TL;DR: First global existence of strong solutions for arbitrarily large initial data in 2D non-isentropic compressible Navier-Stokes with variable entropy, extending Kazhikhov-Vaigant results beyond isentropic case.


<details>
  <summary>Details</summary>
Motivation: Previous global existence results for compressible Navier-Stokes with variable viscosity were limited to isentropic equations and couldn't be generalized to non-isentropic (heat-conductive) systems. The authors aim to extend these results to a broader class of compressible fluids with variable entropy.

Method: Study 2D compressible Navier-Stokes equations with variable entropy where pressure depends nonlinearly on density and entropy, and entropy evolves through transport equation. Use constant shear viscosity and power-law bulk viscosity λ(ρ)=ρ^β with β>4/3. For bounded domains, derive new commutator estimates compatible with Navier-slip boundary conditions.

Result: Prove global existence of strong solutions for arbitrarily large initial data on 2D periodic domains and bounded domains with Navier-slip boundary conditions. Solutions allow initial vacuum and require no smallness assumptions. Density remains uniformly bounded for all time, and solutions converge to equilibrium as t→∞.

Conclusion: Successfully extend Kazhikhov-Vaigant type results to non-isentropic compressible fluids with variable entropy, overcoming limitations of previous isentropic-only approaches. The work provides first global existence theory for this class of equations with large data.

Abstract: In 1995, Kazhikhov and Vaigant introduced a particular class of isentropic compressible Navier-Stokes equations with variable viscosity coefficients and, for the first time, established the existence of global smooth solutions for arbitrarily large initial data in bounded two-dimensional domains. This result was subsequently extended and refined to accommodate more general constraints on the viscosity coefficients. However, because the proofs in this line of work [17,14,15,8] rely heavily on the structure of the isentropic equations, they could not be generalized to the broader setting of multidimensional compressible heat-conductive Navier-Stokes-Fourier systems. In this paper, we consider a special class of non-isentropic compressible fluids governed by the two-dimensional compressible Navier-Stokes equations with variable entropy. In this system, the pressure depends nonlinearly on both density and entropy, and the entropy evolves solely through a transport equation-a feature that distinguishes it from the standard Navier-Stokes-Fourier model. We establish, for the first time, the global existence of strong solutions for arbitrarily large initial data on both two-dimensional periodic domains and bounded domains endowed with Navier-slip boundary conditions. For the bounded-domain case, a key step in our analysis is the derivation of new commutator estimates compatible with the slip condition. Our results hold even when the initial density may contain vacuum and require no smallness assumption on the initial data, provided the shear viscosity is constant and the bulk viscosity follows a power-law form $λ(ρ)=ρ^β$ with $β> 4/3$. Moreover, we demonstrate that the density remains uniformly bounded for all time. Consequently, the solution converges to an equilibrium state as time tends to infinity.

</details>


### [55] [Global well-posedness for the generalized intermediate NLS with a nonvanishing condition at infinity](https://arxiv.org/abs/2512.18998)
*Takafumi Akahori,Rana Badreddine,Slim Ibrahim,Nobu Kishimoto*

Main category: math.AP

TL;DR: First well-posedness result for Intermediate Nonlinear Schrödinger equation with dark solitons in functional space adapted to nonvanishing boundary conditions.


<details>
  <summary>Details</summary>
Motivation: The Intermediate Nonlinear Schrödinger equation models internal waves and admits dark solitons with non-zero boundary conditions, but these solutions fall outside existing well-posedness theories.

Method: Establish local and global well-posedness in a Zhidkov-type space naturally suited to non-trivial boundary conditions, extending results to generalized defocusing equation.

Result: First well-posedness result for the equation in a functional setting adapted to its dark soliton structure.

Conclusion: Successfully developed well-posedness theory for dark soliton solutions by using appropriate functional spaces that accommodate nonvanishing boundary conditions.

Abstract: The Intermediate Nonlinear Schrödinger equation models quasi-harmonic internal waves in two-fluid layer system, and admits dark solitons, that is, solutions with nonvanishing boundary conditions at spatial infinity. These solutions fall outside existing well-posedness theories. We establish local and global well-posedness in a Zhidkov-type space naturally suited to such non-trivial boundary conditions, and extend these results to a generalized defocusing equation. This appears to be the first well-posedness result for the equation in a functional setting adapted to its dark soliton structure.

</details>


### [56] [On the basin of attraction for the free boundary free elastic flow](https://arxiv.org/abs/2512.19015)
*Klaus Deckelnick,Hans-Christoph Grunau,Robert Nürnberg,Glen Wheeler,Valentina-Mira Wheeler*

Main category: math.AP

TL;DR: The paper analyzes the basin of attraction for the straight line in free boundary elastic flow, proving attraction to level 1.9615π and suggesting the conjectured 2π threshold may be false.


<details>
  <summary>Details</summary>
Motivation: To understand the stability and basin of attraction of the straight line configuration in free boundary elastic flow, which is important for understanding the dynamics of elastic curves meeting boundaries perpendicularly.

Method: The authors use analytical methods to prove the basin of attraction result, show theoretical limitations of their approach, and provide numerical evidence to support their findings.

Result: Proved that the straight line has a basin of attraction at least to level 1.9615π in Euler's scale-invariant bending energy, and provided evidence that the previously conjectured 2π threshold may not hold.

Conclusion: The straight line configuration is stable within a certain energy range (1.9615π), but the commonly conjectured 2π threshold appears to be mathematically unattainable and potentially incorrect based on both analytical limitations and numerical evidence.

Abstract: The free boundary free elastic flow is the steepest descent gradient flow for the elastic energy of curves meeting parallel lines perpendicularly. In this article we prove that the straight line has, measured in Euler's scale-invariant bending energy, a basin of attraction at least to the level $1.9615\, π$. We show that our method of proof cannot be pushed to the previously conjectured level $2π$, and in addition present numerical evidence that this conjecture may in fact be false.

</details>


### [57] [Upper-semicontinuity of uniform attractors for the non-autonomous viscoelastic Kirchhoff plate equation with memory](https://arxiv.org/abs/2512.19079)
*Yuming Qin,Hongli Wang*

Main category: math.AP

TL;DR: Analysis of long-time dynamics for non-autonomous viscoelastic Kirchhoff plate equation with memory effects, establishing global existence, uniform attractors for subcritical/critical growth, and upper semicontinuity as perturbation parameter ε→0⁺.


<details>
  <summary>Details</summary>
Motivation: Extend classical attractor theory to more general non-autonomous viscoelastic systems with memory effects, and resolve open questions about limiting behavior of attractors in presence of both memory and critical nonlinearity.

Method: Establish global existence of weak solution inducing continuous process, then demonstrate existence of uniform attractors using operator techniques and innovative analytical approach, finally prove upper semicontinuity of attractor family as ε→0⁺ through delicate energy estimates and contradiction argument.

Result: Global existence of weak solution established, uniform attractors proven for both subcritical and critical growth scenarios, and upper semicontinuity of attractor family as ε→0⁺ demonstrated.

Conclusion: Results extend classical attractor theory to non-autonomous viscoelastic systems with memory and resolve open questions about attractor limiting behavior with memory and critical nonlinearity, providing comprehensive dynamical analysis framework.

Abstract: This paper delves into the long-time dynamics of a non-autonomous viscoelastic Kirchhoff plate equation with memory effects, described by
  $$
  u_{t t}-Δu_{t t}+a_ε(t) u_t+αΔ^2 u-\int_0^{\infty} μ(s) Δ^2 u(t-s) \mathrm{d} s-Δu_t+f(u)=g(x,t),
  $$
  in bounded domain $Ω\subset \mathbb{R}^N$ with smooth boundary and nonlinear terms. Initially, the global existence of a weak solution that induces a continuous process is established. Subsequently, the existence of a uniform attractor is demonstrated in both subcritical and critical growth scenarios, utilizing operator techniques and an innovative analytical approach. Finally, the upper semicontinuity of the family of uniform attractors as the pert parameterurbation $ε\to 0^+$ is proven through delicate energy estimates and a contradiction argument. Our results not only extend classical attractor theory to more general non-autonomous viscoelastic systems but also resolve open questions regarding the limiting behavior of attractors in the presence of both memory and critical nonlinearity.

</details>


### [58] [Minimizing movements for quasilinear Keller--Segel systems with nonlinear mobility in weighted Wasserstein metrics](https://arxiv.org/abs/2512.19137)
*Kyogo Murai*

Main category: math.AP

TL;DR: Global existence of weak solutions for quasilinear Keller-Segel systems with nonlinear mobility using minimizing movements in weighted Wasserstein and L² spaces.


<details>
  <summary>Details</summary>
Motivation: Keller-Segel systems with nonlinear mobility require different mathematical treatment than those with linear mobility. The nonlinearity of mobility necessitates working in weighted Wasserstein spaces rather than standard Wasserstein spaces, and non-Lipschitz mobility functions require approximation techniques.

Method: Minimizing movements (JKO scheme) in the product space of weighted Wasserstein space and L² space. First approximate the non-Lipschitz mobility with Lipschitz functions, solve the approximated systems, then establish uniform estimates and convergences to obtain solutions for the original system.

Result: Proved global existence of weak solutions to quasilinear Keller-Segel systems with nonlinear mobility using the minimizing movements approach with appropriate functional analytic framework.

Conclusion: The minimizing movements method can be successfully adapted to handle quasilinear Keller-Segel systems with nonlinear mobility by using weighted Wasserstein spaces and approximation techniques for non-Lipschitz mobility functions.

Abstract: We prove the global existence of weak solutions to quasilinear Keller--Segel systems with nonlinear mobility by minimizing movements (JKO scheme) in the product space of the weighted Wasserstein space and $L^2$ space. While minimizing movements for Keller--Segel systems with linear mobility are adapted in the product space of the Wasserstein space and $L^2$ space, due to the nonlinearity of mobility, we need to use the weighted Wasserstein space instead of the Wasserstein space. Moreover, since the mobility function is not Lipschitz, we first find solutions to the Keller--Segel systems whose mobility is apporoximated by a Lipschitz function, and then we establish additional uniform estimates and convergences to derive solutions to the Keller--Segel systems.

</details>


### [59] [Optimal stabilization rate for the wave equation with hyperbolic boundary condition](https://arxiv.org/abs/2512.19167)
*Hugo Parada,Nicolas Vanspranghe*

Main category: math.AP

TL;DR: The paper proves that energy of classical solutions to wave equations with hyperbolic boundary conditions and boundary damping decays at rate 1/t, which is shown to be optimal.


<details>
  <summary>Details</summary>
Motivation: To establish precise decay rates for wave equations with dynamic Wentzell boundary conditions and boundary damping, particularly understanding how energy dissipates over time in such systems.

Method: Uses resolvent estimates derived from studying high-frequency quasimodes to analyze the energy decay behavior of classical solutions.

Result: Proves that energy decays like 1/t for wave equations with hyperbolic boundary conditions and boundary damping, and shows this rate is sharp (optimal). Also handles mixed boundary conditions where part of boundary may be kept at rest if dynamic part satisfies geometric control condition.

Conclusion: The 1/t decay rate is established as the precise optimal energy dissipation rate for wave equations with dynamic Wentzell boundary conditions and boundary damping, with results extending to mixed boundary configurations under appropriate geometric conditions.

Abstract: We show that the energy of classical solutions to the wave equation with hyperbolic boundary condition (i.e., dynamic Wentzell boundary condition) and damping on the boundary decays like 1/t. In fact we allow mixed boundary conditions: a possibly empty, disjoint part of the boundary may be kept at rest provided that the dynamic part satisfies the geometric control condition. We also prove that this decay rate is sharp. Our results follow from resolvent estimates, which we establish by studying high-frequency quasimodes.

</details>


### [60] [Existence of positive solutions for a class of almost critical problems on an annulus](https://arxiv.org/abs/2512.19209)
*Gabriele Mancini,Giuseppe Mario Rago,Giusi Vaira*

Main category: math.AP

TL;DR: The paper studies multi-peak positive solutions for slightly subcritical/supercritical elliptic problems on annuli, showing that annulus geometry changes with bump count: thinner annuli for subcritical, smaller holes for supercritical.


<details>
  <summary>Details</summary>
Motivation: To understand how the geometry of an annulus affects the existence and properties of multi-peak positive solutions for elliptic problems near criticality, particularly how the domain shape relates to solution patterns.

Method: Uses explicit forms of Green function and Robin function on annuli to analyze solution behavior. Studies how annulus dimensions change with increasing number of bumps in solutions for both subcritical and supercritical cases.

Result: Shows that for slightly subcritical problems, the annulus becomes thinner as the number of bumps increases. For slightly supercritical problems, the hole of the annulus becomes very small.

Conclusion: The geometry of the annulus domain significantly influences multi-peak solution patterns near criticality, with distinct dimensional changes for subcritical vs supercritical regimes.

Abstract: In this paper we will consider multi-peaks positive solutions for a class of slightly subcritical or slightly supercritical elliptic problems on an annulus with Dirichlet boundary conditions. By using the explicit form of the Green function and of the Robin function on the annulus, we prove that the annulus becomes thinner and thinner when the number of bumps increases for the slightly subcritical case, while the hole of the annulus is very small for the slightly supercritical case.

</details>


### [61] [Global boundedness of weak solutions with finite energy to a general class of Dirichlet problems](https://arxiv.org/abs/2512.19224)
*Giovanni Cupini,Paolo Marcellini*

Main category: math.AP

TL;DR: The paper presents global boundedness results for weak solutions of nonlinear elliptic equations in divergence form under general growth conditions, extending beyond minimizers of integral functionals.


<details>
  <summary>Details</summary>
Motivation: To establish boundedness of weak solutions for general classes of elliptic equations in divergence form, which is a classic tool for achieving higher regularity. The authors aim to develop results applicable to the extensive literature on PDEs under general growth conditions, moving beyond the limitation to minimizers of integral functionals.

Method: The authors propose the class of weak solutions with finite energy as the appropriate framework for studying regularity. They develop global boundedness results under general assumptions that can be applied to various cases of nonlinear elliptic equations in divergence form, specifically focusing on Dirichlet problems.

Result: The paper establishes global boundedness results for weak solutions to general nonlinear elliptic equations in divergence form under general growth conditions, working within the class of weak solutions with finite energy.

Conclusion: The authors successfully extend regularity theory beyond minimizers of integral functionals to the broader class of weak solutions for Dirichlet problems of nonlinear elliptic equations in divergence form, providing a framework applicable to various cases in the general growth literature.

Abstract: As explained in detail in the prologue to this manuscript, boundedness of weak solutions for general classes of elliptic equations in divergence form is a classic tool for achieving higher regularity. We propose here some global boundedness results under general assumptions that can be applied to several cases studied in the recent and extensive literature on partial differential equations \textit{under general growth}. In particular, we propose the class of \textit{weak solutions with finite energy} in which to search for solutions and in which regularity can be studied and achieved. We emphasize that we are not limited to minimizers of certain integral functionals, as often considered recently in this context of general growth, but to the broader class of weak solutions to Dirichlet problems for general nonlinear elliptic equations in divergence form.

</details>


### [62] [Obstacle problems for the fractional $p$-Laplacian on fractal domains: well-posedness and asymptotics](https://arxiv.org/abs/2512.19252)
*Simone Creo,Salvatore Fragapane*

Main category: math.AP

TL;DR: Study of obstacle problems for regional fractional p-Laplacian in domains with Koch snowflake fractal boundaries, including well-posedness, equivalent formulations, and asymptotic analysis of approximating problems.


<details>
  <summary>Details</summary>
Motivation: To understand obstacle problems involving fractional p-Laplacian operators in domains with fractal boundaries, specifically the Koch snowflake, which presents mathematical challenges due to its non-smooth nature and requires careful approximation approaches.

Method: 1. Prove well-posedness for obstacle problems with regional fractional p-Laplacian in Koch snowflake domains. 2. Establish two equivalent formulations. 3. Construct approximating problems in domains with pre-fractal boundaries (n-th approximations of Koch snowflake). 4. Perform asymptotic analysis as n→∞ (approaching fractal) and p→∞ (approaching infinity Laplacian).

Result: Successfully proved well-posedness for both the original obstacle problem and its approximating versions. Established equivalent formulations and performed complete asymptotic analysis showing convergence properties as both the domain approximation (n→∞) and operator parameter (p→∞) approach their limits.

Conclusion: The paper provides a rigorous mathematical framework for studying obstacle problems involving fractional p-Laplacian operators in domains with fractal boundaries, with applications to free boundary problems and variational inequalities in non-smooth domains.

Abstract: We study obstacle problems for the regional fractional $p$-Laplacian in a domain $Ω\subset\mathbb{R}^2$ having as fractal boundary the Koch snowflake. We prove well-posedness results for the solution of the obstacle problem, as well as two equivalent formulations. Moreover, we study corresponding approximating obstacle problems in a sequence of domains $Ω_n\subset\mathbb{R}^2$ having as boundary the $n$-th pre-fractal approximation of the Koch snowflake, for $n\in\mathbb{N}$. After proving the well-posedness of the approximating obstacle problems, we perform the asymptotic analysis for both $n\to+\infty$ and $p\to+\infty$.

</details>


### [63] [On the large time behavior of the 2D inhomogeneous incompressible viscous flows](https://arxiv.org/abs/2512.19281)
*Song Jiang,Quan Wang*

Main category: math.AP

TL;DR: The paper analyzes 2D inhomogeneous Navier-Stokes equations for stratified flows in bounded domains under gravity, characterizing steady states, proving relaxation to hydrostatic equilibrium despite Rayleigh-Taylor instabilities, identifying conditions for convergence to linear density profiles, and establishing improved regularity estimates.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time behavior of stratified viscous fluids under gravity in bounded domains, particularly characterizing equilibrium states and convergence properties despite potential transient instabilities like Rayleigh-Taylor mechanisms.

Method: Rigorous mathematical analysis including characterization of steady states under Dirichlet boundary conditions, perturbative analysis around arbitrary hydrostatic profiles, identification of necessary/sufficient conditions for convergence to linear density profiles, and establishment of improved regularity estimates in Sobolev spaces.

Result: 1) All admissible equilibria are hydrostatic satisfying ∇p_s = -ρ_s∇f; 2) System always relaxes to hydrostatic equilibrium despite transient Rayleigh-Taylor growth; 3) Identified condition for convergence to linear hydrostatic density profile ρ_s = -γf + β; 4) Improved regularity estimates for H^3 initial data.

Conclusion: Stratified viscous flows in bounded domains under gravity exhibit robust relaxation to hydrostatic equilibrium regardless of transient instabilities, with specific conditions determining convergence to particular linear density profiles, supported by enhanced regularity results for strong solutions.

Abstract: This paper studies the two-dimensional inhomogeneous Navier--Stokes equations governing stratified flows in a bounded domain under a gravitational potential \(f\). Our main results are as follows. First, we provide a rigorous characterization of steady states, proving that under the Dirichlet condition \(\mathbf{u}|_{\partial Ω} = \mathbf{0}\), all admissible equilibria are hydrostatic and satisfy \(\nabla p_s = -ρ_s \nabla f\). Second, through a perturbative analysis around arbitrary hydrostatic profiles, we show that despite possible transient growth induced by the Rayleigh--Taylor mechanism, the system always relaxes to a hydrostatic equilibrium. Third, we identify a necessary and sufficient condition on the initial density perturbation for convergence to a linear hydrostatic density profile of the form \(ρ_s = -γf + β\), with \(γ> 0\) and \(β> 0\). Finally, we establish improved regularity estimates for strong solutions corresponding to initial data in the Sobolev space \(H^3(Ω)\).

</details>


### [64] [An overdetermined problem related to the p-Laplacian on Riemannian manifolds](https://arxiv.org/abs/2512.19329)
*Guangyue Huang,Chunlei Luo,Hongru Song*

Main category: math.AP

TL;DR: Study of overdetermined p-Laplacian problems on positively curved Riemannian manifolds using new P-functions to derive integral identities, leading to Heintze-Karcher type inequality and Soap Bubble Theorem.


<details>
  <summary>Details</summary>
Motivation: To investigate overdetermined boundary value problems for p-Laplacian equations on compact Riemannian manifolds with positive Ricci curvature, extending classical results to nonlinear elliptic operators.

Method: Introduce a new P-function related to the first nonzero eigenvalue of p-Laplacian, derive integral identities through analytical techniques on Riemannian manifolds with positive curvature.

Result: Obtain integral identities that lead to proofs of Heintze-Karcher type inequality and the Soap Bubble Theorem for p-Laplacian equations on positively curved manifolds.

Conclusion: The new P-function approach successfully extends classical geometric inequalities and characterization results to nonlinear p-Laplacian operators on Riemannian manifolds with positive Ricci curvature.

Abstract: In this paper, we study the overdetermined problem for the p-Laplacian equation on a compact Riemannian manifold with positive Ricci curvature. By introducing a new P-function which is related to the first nonzero eigenvalue for p-Laplacian, we obtain some integral identities. As their applications, the Heintze-Karcher type inequality and the Soap Bubble Theorem have been achieved.

</details>


### [65] [The Neumann Green function of the annulus](https://arxiv.org/abs/2512.19397)
*Giuseppe Mario Rago*

Main category: math.AP

TL;DR: Explicit representation formula for Neumann Green function in annulus using Gegenbauer polynomials and zonal harmonics


<details>
  <summary>Details</summary>
Motivation: To develop an explicit analytical representation for the Green function with Neumann boundary conditions in annular domains, which is important for solving boundary value problems in annular geometries

Method: Utilizes Gegenbauer polynomials and zonal harmonic functions to construct the representation formula mathematically

Result: Successfully builds an explicit representation formula for the Neumann Green function in the annulus

Conclusion: Provides a useful analytical tool for solving Neumann boundary value problems in annular domains using special functions

Abstract: Using Gegenbauer polynomials and the zonal harmonic functions we build an explicit representation formula for the Green function with Neumann boundary conditions in the annulus.

</details>


### [66] [Hölder regularity of doubly nonlinear nonlocal quasilinear parabolic equations in some mixed singular-degenerate regime](https://arxiv.org/abs/2512.19421)
*Karthik Adimurthi,Mitesh Modasiya*

Main category: math.AP

TL;DR: The paper proves Hölder regularity for bounded weak solutions of nonlocal quasilinear equations in the mixed singular-degenerate parameter range, using a new intrinsic scaling approach that explicitly leverages the nonlocal nature of the problem.


<details>
  <summary>Details</summary>
Motivation: Previous Hölder regularity results for nonlocal quasilinear equations were limited to specific parameter regimes: translation invariant (q=2), scale invariant (q=p), and purely degenerate (2<p, q<p) cases. The purely singular case (1<p<2, p<q) was expected to follow similar strategies. However, the mixed singular-degenerate range remained open and challenging, with the analogous problem in the local setting still unresolved.

Method: The authors adapt recent ideas in nonlocal regularity theory and develop a new intrinsic scaling approach specifically tailored for the mixed singular-degenerate parameter range. The method explicitly utilizes the nonlocal structure of the problem, and the estimates are not stable as s→0, highlighting the nonlocal nature of the approach.

Result: The main result establishes Hölder continuity for bounded weak solutions in the parameter range max{p,q,2} < min{q + (p-1)/(1+n/sp), 2 + (p-1)/(1+n/sp)}. This covers the previously open mixed singular-degenerate case for nonlocal quasilinear equations.

Conclusion: The paper successfully extends Hölder regularity theory to the mixed singular-degenerate parameter range for nonlocal quasilinear equations, using a novel intrinsic scaling approach that exploits the nonlocal structure. The result is significant as the analogous problem remains open in the local setting, demonstrating the power of nonlocal methods in this context.

Abstract: We study local Hölder regularity of bounded, weak solutions for the nonlocal quasilinear equations of the form \[ (|u|^{q-2}u)_t + \text{P.V.} \int_{\mathbb{R}^n} \frac{|u(x,t) - u(y,t)|^{p-2}(u(x,t)-u(y,t))}{|x-y|^{n+sp}} dy = 0, \] with $p\in (1,\infty)$, $q\in (1,\infty)$ and $s \in (0,1)$. Analogous Hölder continuity result in the local case is known in the purely singular case $\{1<p<2, p<q\}$, purely degenerate case $\{2<p, q<p\}$, scale invariant case $\{p=q\}$ and translation invariant case $\{q=2,1<p<\infty\}$. In the nonlocal setting, Hölder regularity is known when the equation is either translation invariant $\{q=2, 1<p<\infty\}$ or scale invariant $\{q=p, 1<p<\infty\}$ or purely degenerate case $\{2<p, q<p\}$. Similar strategy can be used to obtain Hölder regularity in the purely singular case $\{1<p<2, p<q\}$.
  In this paper, we adapt several ideas developed over the past few years and combine it with a new intrinsic scaling to prove Hölder regularity in the mixed singular-degenerate range $\max\{p,q,2\} < \min\left\{q + \tfrac{p-1}{1+\frac{n}{sp}}, 2 + \tfrac{p-1}{1+\frac{n}{sp}}\right\}$. The proof explicitly makes use of the nonlocal nature of the problem and as a consequence, our estimates are not stable at $s \rightarrow 0$. We note that the analogous regularity in the local problem remains open.

</details>


### [67] [A critical threshold for the cosmological Euler-Poisson system](https://arxiv.org/abs/2512.19454)
*David Fajman,Maciej Maliborski,Maximilian Ofner,Todd Oliynyk,Zoe Wyatt*

Main category: math.AP

TL;DR: The paper analyzes the gravitational Euler-Poisson system in an expanding Universe, finding a critical stability threshold at expansion rate α=2/3, with global stability for α>2/3 and shock formation for α≤2/3.


<details>
  <summary>Details</summary>
Motivation: To understand the stability behavior of classical barotropic fluids in an expanding cosmological background, particularly how expansion affects fluid dynamics and whether there exists critical thresholds for stability.

Method: Study the gravitational Euler-Poisson system with linear equation of state in expanding Universe with scale factor a(t)=t^α. Use both analytical proofs for α>2/3 regime and numerical simulations for α≤2/3 regime to investigate solution behavior from small initial data.

Result: Found critical threshold at α=2/3: For α>2/3, fluid variables remain global and small; for α≤2/3, numerical evidence shows shock formation for arbitrarily small initial data. The threshold is independent of fluid's speed of sound, unlike relativistic case.

Conclusion: There exists a critical stability threshold for barotropic fluids in expanding domains at α=2/3, with fundamentally different behavior in non-relativistic vs relativistic regimes. The expansion rate determines whether fluids remain smooth or develop shocks.

Abstract: We consider the gravitational Euler-Poisson system with a linear equation of state on an expanding cosmological model of the Universe. The expansion of the spatial sections introduces an additional dissipating effect in the Euler equation. We prescribe the expansion rate of space by a scale factor $a(t)=t^α$ with $α\in(0,1)$, which describes the growth of length scales over time. This model is regularly applied in cosmology to study classical fluids in an expanding Universe. We study the behaviour of solutions to this system arising from small, near-homogeneous initial data and discover a \emph{critical} change of behaviour near the expansion rate $α=2/3$, which corresponds to the matter-dominated regime in cosmology. In particular, we prove that for $α>2/3$ the fluid variables are global in time and remain small provided they are sufficiently small in a suitable norm initially. In the complementary regime $α\leq2/3$, we present numerical evidence for shock formation of solutions to the Euler equation for arbitrarily small initial data. In combination, this establishes the existence of a critical stability threshold for barotropic fluids in expanding domains. In contrast to our previous work on the corresponding relativistic system, the threshold in the classical system considered here is independent of the speed of sound of the fluid. This establishes that fluids in cosmology behave fundamentally different in the non-relativistic regime than in the relativistic one.

</details>


### [68] [Computing multiple solutions from knowledge of the critical set](https://arxiv.org/abs/2512.19499)
*Otavio Kaminski,Diego S. Monteiro,Carlos Tomei*

Main category: math.AP

TL;DR: A geometric model for functions between spaces of same dimension, combining analysis and topology, applicable to semilinear elliptic operators and useful for numerical continuation methods.


<details>
  <summary>Details</summary>
Motivation: To develop a geometric framework that combines analytical and topological approaches for understanding functions between spaces, particularly useful for solving equations F(x)=y through continuation methods and applicable to semilinear elliptic differential operators.

Method: Proposes a geometric model requiring functions to be proper on bounded sets, with Jacobians as Fredholm operators of index zero in infinite dimensions. Uses continuation methods informed by global geometry of F, demonstrated through three classes: 2D Euclidean functions, 15D discretized nonlinear Sturm-Liouville problems, and semilinear elliptic equations.

Result: The model provides a unified framework combining analysis and topology, successfully applied to various problems including visualization in 2D, solving discretized Sturm-Liouville problems with abundant solutions, and computing multiple solutions of semilinear elliptic equations.

Conclusion: The geometric model offers a powerful approach for studying functions between spaces, particularly valuable for numerical continuation methods and applicable to a broad class of semilinear elliptic differential operators, with demonstrated effectiveness across different dimensional contexts.

Abstract: {We explore a simple {\it geometric model} for functions between spaces of the same dimension (in infinite dimensions, we require that Jacobians be Fredholm operators of index zero). The model combines standard results in analysis and topology associated with familiar global and local aspects. Functions are supposed to be proper on bounded sets. The model is valid for a large class of semilinear elliptic differential operators.
  It also provides a fruitful context for numerical analysis. 
  For a function $F: X \to Y$ between real Banach spaces, continuation methods to solve $F(x) = y$ may improve from considerations about the global geometry of $F$.
  We consider three classes of examples. First we handle functions from the Euclidean plane to itself, for which the reasoning behind the techniques is visualizable. The second, between spaces of dimension 15, is obtained by discretizing a nonlinear Sturm-Liouville problem for which special right hand sides admit abundant solutions. Finally, we compute the six solutions of a semilinear elliptic equation $-Δu - f(u) = g$ studied by Solimini.}

</details>


### [69] [Well-posedness and long time dynamics for a quasi-geostrophic ocean-atmosphere model with radiation balance](https://arxiv.org/abs/2512.19556)
*Federico Fornasaro,Tobias Kuna,Giulia Carigi*

Main category: math.AP

TL;DR: The paper establishes mathematical foundations for a coupled atmosphere-ocean quasi-geostrophic model, proving solution existence/uniqueness, continuous dependence on parameters, finite-dimensional global attractor, and that ocean temperature can be reconstructed from velocity observations.


<details>
  <summary>Details</summary>
Motivation: To develop a mathematically rigorous framework for coupled atmosphere-ocean models in mid-latitudes that incorporates both mechanical and thermodynamical interactions, bridging quasi-geostrophic dynamics with energy balance model physics.

Method: Combines multilayer quasi-geostrophic dynamical framework with temperature equations including long- and short-wave radiative forcing. Uses functional analysis to establish mathematical properties within a suitable functional framework.

Result: Proves existence and uniqueness of solutions, continuous dependence on radiation parameters, finite-dimensional global attractor for long-time dynamics, and existence of finite set of determining modes. Shows ocean temperature can be reconstructed from velocity field observations.

Conclusion: The coupled atmosphere-ocean model has robust mathematical properties including well-posedness and finite-dimensional asymptotic behavior, with practical implications for reconstructing ocean temperature from velocity measurements.

Abstract: We investigate a coupled atmosphere-ocean model including the mechanical and thermodynamical interaction between the two fluids for the mid-latitudes. The formulation combines a multilayer quasi-geostrophic dynamical framework with temperature equations incorporating long- and short-wave radiative forcing, as in energy balance models. Within a suitable functional framework, we establish the existence and uniqueness of solutions, and their continuous dependence on the radiation parameters. We also prove that the long-time dynamics are described by a finite-dimensional global attractor and, moreover, that the system possesses a finite set of determining modes that governs its asymptotic behaviour. In particular, we show that the long-term evolution of the ocean's temperature can be reconstructed solely from observations of the velocity fields across the model's layers.

</details>


### [70] [Schr{ö}dinger maps to a K{ä}hler manifold in two dimensions](https://arxiv.org/abs/2512.19559)
*Benjamin Dodson,Jeremy L. Marzuola*

Main category: math.AP

TL;DR: Global well-posedness and scattering for Schrödinger maps to Kähler manifolds with small initial data in Besov spaces.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for Schrödinger maps to general Kähler manifolds, extending existing results and providing global existence and scattering properties for small initial data.

Method: Uses Besov space framework and small data analysis techniques to prove global well-posedness and scattering for Schrödinger maps to general Kähler manifolds.

Result: Proves global well-posedness and scattering for Schrödinger maps to general Kähler manifolds with small initial data in Besov spaces.

Conclusion: Schrödinger maps to Kähler manifolds exhibit global well-posedness and scattering behavior for sufficiently small initial data in appropriate Besov spaces.

Abstract: We prove a global well--posedness and scattering result for Schr{ö}dinger maps to a general K{ä}hler manifold with small initial data in a Besov space.

</details>


### [71] [Ground state solutions to the nonlinear Born-Infeld problem](https://arxiv.org/abs/2512.19566)
*Bartosz Bieganowski,Norihisa Ikoma,Jarosław Mederski*

Main category: math.AP

TL;DR: Existence of ground state solutions for nonlinear Born-Infeld equation in ℝᴺ with zero/positive mass cases, new Sobolev-type inequality proof, and existence of nonradial solutions for N≥4.


<details>
  <summary>Details</summary>
Motivation: Previous approaches to the nonlinear Born-Infeld problem relied on approximation schemes and symmetry assumptions, which could yield solutions not at the ground state energy level. The paper aims to develop a more direct approach without these limitations.

Method: New direct variational approach combining minimization over a Pohožaev manifold with profile decomposition techniques, avoiding approximation arguments and symmetry assumptions.

Result: Existence of ground state solutions in zero and positive mass cases, new proof of Sobolev-type inequality with optimal constant characterization, and existence of nonradial solutions for N≥4 (settling open problem for N=5).

Conclusion: The paper provides a more robust framework for studying Born-Infeld equations, establishes existence of nonradial solutions, and offers improved analytical tools through the new variational approach and inequality proof.

Abstract: In the paper we show the existence of ground state solutions to the nonlinear Born-Infeld problem \[ \mathrm{div}\, \left( \frac{\nabla u}{\sqrt{1-|\nabla u|^2}} \right) + f(u) = 0, \quad x \in \mathbb{R}^N \] in the zero and positive mass cases. Moreover, we find a new proof of the Sobolev-type inequality \[ \int_{\mathbb{R}^N} \left(1 - \sqrt{1-|\nabla u|^2}\right) \, dx \geq C_{N,p} \left( \int_{\mathbb{R}^N} |u|^p \, dx \right)^{\frac{N}{N+p}}, \] for $p > 2^*$ as well as the characterization of the optimal constant $C_{N,p}$ in terms of the ground state energy level. Previous approaches relied on approximation schemes and/or symmetry assumptions, which typically yield to compact embeddings and may lead to solutions that are not at the ground state energy level. In contrast, neither approximation arguments nor symmetry assumptions are employed in the paper to obtain a ground state solution. Instead, we develop a new direct variational approach based on minimization over a Pohožaev manifold combined with profile decomposition techniques. Finally, we show that nonradial solutions exist whenever $N \geq 4$; in particular, this settles a previously open problem in the case $N=5$.

</details>


### [72] [The Lorentzian Calderón problem on vector bundles](https://arxiv.org/abs/2512.19601)
*Sean Gomes,Lauri Oksanen*

Main category: math.AP

TL;DR: The paper studies a Lorentzian version of the Calderón problem, proving unique determination of connections and potentials on Hermitian vector bundles over Lorentzian manifolds from Dirichlet-to-Neumann maps, up to natural gauge transformations.


<details>
  <summary>Details</summary>
Motivation: To extend the Calderón problem (inverse problem for determining material properties from boundary measurements) from Riemannian to Lorentzian geometry, which is relevant for wave equations in spacetime physics.

Method: Extends techniques from earlier scalar works (arXiv:2008.07508, arXiv:2112.01663) to vector bundles. Considers Lorentzian manifolds satisfying curvature bounds, including perturbations of Minkowski space over strictly convex domains.

Result: For the specified class of Lorentzian manifolds, the connection and potential are uniquely determined by the Dirichlet-to-Neumann map of the connection wave operator, modulo natural gauge transformations.

Conclusion: The Calderón problem can be solved in Lorentzian geometry for certain spacetime manifolds, establishing unique identifiability of gauge fields from boundary measurements of wave propagation.

Abstract: In this paper we study a Lorentzian version of the Calderón problem, which is concerned with the determination of a connection and potential on a Hermitian vector bundle over a Lorentzian manifold from the Dirichlet-to-Neumann map of the associated connection wave operator. For a class of Lorentzian manifolds satisfying a curvature bound, including perturbations of Minkowski space over strictly convex domains, the connection and potential is shown to be uniquely determined up to the natural gauge transformations of the problem. The proof is based on ideas from the earlier works arXiv:2008.07508, arXiv:2112.01663 of the second author in the scalar setting.

</details>


### [73] [Global bifurcation of hollow vortex streets](https://arxiv.org/abs/2512.19619)
*Vasileios N. Oikonomou,Samuel Walsh*

Main category: math.AP

TL;DR: The paper develops a method to desingularize periodic point vortex configurations into hollow vortex streets using analytic global bifurcation theory.


<details>
  <summary>Details</summary>
Motivation: To study hollow vortex streets - periodic configurations of vortices with constant pressure cores in irrotational flow - and develop a systematic way to transform point vortex configurations into physically meaningful hollow vortex solutions.

Method: Uses analytic global bifurcation theory and adapts the desingularization technique of Chen, Walsh, and Wheeler to periodic settings. The approach transforms non-degenerate singly-periodic point vortex configurations into hollow vortex streets.

Result: Proves that any non-degenerate singly-periodic point vortex configuration can be desingularized to create a global curve of solutions to the steady hollow vortex street problem. Characterizes singular behavior that develops along the curve. Applies method to von Kármán vortex streets, translating vortex arrays, and two-pair (2P) configurations.

Conclusion: Provides a rigorous framework for constructing hollow vortex streets from point vortex configurations, with applications to classical vortex patterns and characterization of solution behavior at extremes of the bifurcation curve.

Abstract: Vortex streets are periodic configurations of vortices propagating through an irrotational flow. In this paper, we study streets of hollow vortices, which are solutions to the free boundary $2$-d irrotational incompressible Euler equations. Each vortex core is a region of constant pressure in the complement of the fluid domain with a nonzero circulation around it. We prove that any non-degenerate singly-periodic point vortex configuration can be ``desingularized'' to create a global curve of solutions to the steady hollow vortex street problem, and we further characterize the types of singular behavior that can develop as one transverses the curve to its extreme. As specific examples, we study von Kármán vortex streets, translating vortex arrays, and a two-pair (2P) configuration. Our method is based on analytic global bifurcation theory and adapts the desingularization technique of Chen, Walsh, and Wheeler to the periodic setting.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [74] [BattMo -- Battery Modelling Toolbox](https://arxiv.org/abs/2512.17933)
*Xavier Raynaud,Halvor Møll Nilsen,August Johansson,Eibar Flores,Lorena Hendrix,Francesca Watson,Sridevi Krishnamurthi,Olav Møyner,Simon Clark*

Main category: physics.comp-ph

TL;DR: BattMo is a MATLAB-based finite volume continuum modeling toolbox for simulating various battery chemistries and 3D designs, featuring thermal coupling, degradation modeling, and gradient-based parameter optimization.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible, modular framework for battery performance simulation that supports semantic interoperability through BattINFO standards and enables efficient parameter calibration from experimental data.

Method: Uses Doyle-Fuller-Newman (DFN) approach as base model with finite volume continuum modeling. Models are structured hierarchically as computational graphs (variables as nodes, functional relationships as edges). Supports automatic differentiation and adjoint computation for gradient-based optimization.

Result: Developed BattMo toolbox that can quickly setup and solve models for various battery chemistries and 3D designs (cylindrical/prismatic cells), with thermal coupling, degradation mechanisms (SEI growth), and composite materials.

Conclusion: BattMo provides a flexible, modular battery modeling framework with semantic interoperability support, enabling efficient simulation and parameter calibration for diverse battery designs and chemistries.

Abstract: This paper presents the Battery Modelling Toolbox (BattMo), a flexible finite volume continuum modelling framework in MATLAB\textsuperscript{\textregistered} (\citeproc{ref-MATLAB}{The MathWorks Inc., 2025}) for simulating the performance of electro-chemical cells. BattMo can quickly setup and solve models for a variety of battery chemistries, even considering 3D designs such as cylindrical and prismatic cells. The simulation input parameters, including the material parameters and geometric descriptions, are specified through JSON schemas. In this respect, we follow the guidelines of the Battery Interface Ontology (BattINFO) to support semantic interoperability in accordance with the FAIR principles (\citeproc{ref-fair}{Wilkinson et al., 2016}). The Doyle-Fuller-Newman (DFN) (\citeproc{ref-Doyle1993ModelingCell}{Doyle et al., 1993}) approach is used as a base model. We include fully coupled thermal simulations. It is possible to include degradation mechanisms such as SEI layer growth, and the use of composite material, such as a mixture of Silicon and graphite. The models are setup in a hierarchical way, for clarity and modularity. Each model corresponds to a computational graph, which introduces a set of variables (the nodes) and functional relationship (the edges). This design enables the flexibility for changing and designing new models. The solver in BattMo uses automatic differentiation and support adjoint computation. We can therefore compute the derivative of objective functions with respect to all parameters efficiently. Gradient-based optimization routines can be used to calibrate parameters from experimental data.

</details>


### [75] [Origins of phase-field crack widening in dynamic fragmentation explained](https://arxiv.org/abs/2512.18022)
*Shad Durussel,Gergely Molnár,Jean-François Molinari*

Main category: physics.comp-ph

TL;DR: Phase-field fracture method shows spurious damage diffusion in dynamics due to trapped elastic waves; mass erosion technique reduces this artifact while maintaining convergence to linear elastic fracture mechanics predictions.


<details>
  <summary>Details</summary>
Motivation: To investigate dynamic crack propagation and fragmentation using phase-field fracture approach, which is mesh-independent due to damage regularization, but suffers from unphysical effects in dynamic simulations.

Method: Phase-field fracture approach with damage regularization zone; analysis of dynamic crack propagation with and without mass erosion technique that conserves elastic wave speed in damaged regions.

Result: Dynamic simulations show progressive widening of damage zone due to trapped elastic waves; mass erosion effectively reduces spurious damage diffusion; both methods converge to linear elastic fracture mechanics predictions when regularization length is small enough and mesh sufficiently fine.

Conclusion: Phase-field fracture method with proper regularization and mesh resolution converges to classical fracture mechanics predictions in dynamics; mass erosion technique helps mitigate unphysical damage diffusion caused by wave interactions.

Abstract: We investigate dynamic crack propagation and fragmentation with the phase-field fracture approach. The method was chosen for its ability to yield crack paths that are independent of the underlying mesh, thanks to the damage regularization zone. In dynamics, we observe a progressive widening of this regularization zone and attribute it to an unphysical trapping of elastic waves. We show that the damage zones do not represent free boundaries accurately and that wave interactions induce additional damage. We reveal how mass erosion, by conserving the elastic wave speed in the damaged regions, can be used to efficiently reduce the spurious diffusion of damage. Furthermore, we provide numerical evidence that dynamically propagating cracks in the phase-field formulation, both with and without mass erosion, converge to the predictions of linear elastic fracture mechanics. For vanishing regularization length, the crack speed and energy release rate become independent of the phase-field regularization length, provided that this length scale is small enough and the mesh fine enough to resolve the process zone.

</details>


### [76] [Long-range electrostatics for machine learning interatomic potentials is easier than we thought](https://arxiv.org/abs/2512.18029)
*Dongjin Kim,Bingqing Cheng*

Main category: physics.comp-ph

TL;DR: The paper introduces Latent Ewald Summation (LES) framework - a simple approach to incorporate long-range electrostatics into machine learning interatomic potentials using Coulomb functional form with environment-dependent charges, without requiring explicit DFT partial charge training.


<details>
  <summary>Details</summary>
Motivation: Current MLIPs lack long-range electrostatics, limiting their applications to interfaces, charge-transfer reactions, polar/ionic materials, and biomolecules where electrostatic interactions are crucial.

Method: Proposes two design principles: (1) use Coulomb functional form with environment-dependent charges to capture electrostatic interactions, (2) avoid explicit training on ambiguous DFT partial charges. The LES framework can augment any short-range MLIP and can incorporate charge equilibration schemes, dipoles, Born effective charges, and other features.

Result: The framework enables capturing long-range interactions, charges, and electrical response using only standard energy and force training data. It provides substantial flexibility while maintaining simplicity.

Conclusion: Incorporating long-range electrostatics into MLIPs is simpler and more broadly applicable than commonly assumed, with minimal physics-guided design rules that can overcome current limitations in the field.

Abstract: The lack of long-range electrostatics is a key limitation of modern machine learning interatomic potentials (MLIPs), hindering reliable applications to interfaces, charge-transfer reactions, polar and ionic materials, and biomolecules. In this Perspective, we distill two design principles behind the Latent Ewald Summation (LES) framework, which can capture long-range interactions, charges, and electrical response just by learning from standard energy and force training data: (i) use a Coulomb functional form with environment-dependent charges to capture electrostatic interactions, and (ii) avoid explicit training on ambiguous density functional theory (DFT) partial charges. When both principles are satisfied, substantial flexibility remains: essentially any short-range MLIP can be augmented; charge equilibration schemes can be added when desired; dipoles and Born effective charges can be inferred or finetuned; and charge/spin-state embeddings or tensorial targets can be further incorporated. We also discuss current limitations and open challenges. Together, these minimal, physics-guided design rules suggest that incorporating long-range electrostatics into MLIPs is simpler and perhaps more broadly applicable than is commonly assumed.

</details>


### [77] [Renormalization-Group Geometry of Homeostatically Regulated Reentry Networks](https://arxiv.org/abs/2512.19086)
*Byung Gyu Chae*

Main category: physics.comp-ph

TL;DR: A minimal continuous-time formulation of homeostatically regulated reentrant networks (FHRN) shows population dynamics reduce to 1D radial flow, revealing fixed threshold for sustained reflective activity and enabling complete RG analysis of reentry-homeostasis interaction.


<details>
  <summary>Details</summary>
Motivation: Reentrant computation (recursive self-coupling where networks continuously reinject and reinterpret their own internal state) plays central role in biological cognition but remains poorly characterized in neural network architectures.

Method: Introduce minimal continuous-time formulation of homeostatically regulated reentrant network (FHRN); show population dynamics admit exact reduction to one-dimensional radial flow; perform complete renormalization-group (RG) analysis of reentry-homeostasis interaction.

Result: Derive closed RG system for parameters governing structural gain, homeostatic stiffness, and reentrant amplification; show all trajectories attracted to critical surface γρ=1 where intrinsic leak and reentrant drive exactly balance; identify phase structure with quenched, reactive, and reflective regimes exhibiting mean-field critical onset with universal scaling.

Conclusion: Results provide RG-theoretic characterization of reflective computation and demonstrate how homeostatic fields stabilize deep reentrant transformations through scale-dependent self-regulation.

Abstract: Reentrant computation-recursive self-coupling in which a network continuously reinjects and reinterprets its own internal state-plays a central role in biological cognition but remains poorly characterized in neural network architectures. We introduce a minimal continuous-time formulation of a homeostatically regulated reentrant network (FHRN) and show that its population dynamics admit an exact reduction to a one-dimensional radial flow. This reduction reveals a dynamically fixed threshold for sustained reflective activity and enables a complete renormalization-group (RG) analysis of the reentry-homeostasis interaction. We derive a closed RG system for the parameters governing structural gain, homeostatic stiffness, and reentrant amplification, and show that all trajectories are attracted to a critical surface defined by $γρ=1$, where intrinsic leak and reentrant drive exactly balance. The resulting phase structure comprises quenched, reactive, and reflective regimes and exhibits a mean-field critical onset with universal scaling. Our results provide an RG-theoretic characterization of reflective computation and demonstrate how homeostatic fields stabilize deep reentrant transformations through scale-dependent self-regulation.

</details>


### [78] [Self-Consistent Probability Flow for High-Dimensional Fokker-Planck Equations](https://arxiv.org/abs/2512.19196)
*Xiaolong Wu,Qifeng Liao*

Main category: physics.comp-ph

TL;DR: SCPF method solves high-dimensional Fokker-Planck equations by reformulating them as first-order PF-ODE constraints, using CNF with HTE to achieve linear O(D) complexity and O(1) wall-clock time on GPUs.


<details>
  <summary>Details</summary>
Motivation: High-dimensional Fokker-Planck equations suffer from curse of dimensionality and computational bottlenecks in evaluating second-order diffusion terms. Existing methods like PINNs face O(D²) complexity from automatic differentiation, while probability flow approaches have serial operations or sampling efficiency issues.

Method: Reformulate second-order FP equation into equivalent first-order deterministic Probability Flow ODE constraint. Minimize residual of PF-ODE continuity equation instead of explicit Hessian computation. Use Continuous Normalizing Flows with Hutchinson Trace Estimator to reduce training complexity to O(D). Apply generative adaptive sampling strategy to address data sparsity.

Result: SCPF effectively mitigates curse of dimensionality, maintaining high accuracy and constant computational cost for problems up to 100 dimensions. Achieves linear O(D) complexity and effective O(1) wall-clock time on GPUs across diverse benchmarks including anisotropic OU processes, high-dimensional Brownian motions, and Geometric OU processes.

Conclusion: SCPF provides an efficient solution for high-dimensional Fokker-Planck equations by avoiding explicit second-order derivative computation through PF-ODE reformulation and leveraging CNF with HTE, achieving scalable performance while maintaining accuracy.

Abstract: Solving high-dimensional Fokker-Planck (FP) equations is a challenge in computational physics and stochastic dynamics, due to the curse of dimensionality (CoD) and the bottleneck of evaluating second-order diffusion terms. Existing deep learning approaches, such as Physics-Informed Neural Networks (PINNs), face computational challenges as dimensionality increases, driven by the $O(D^2)$ complexity of automatic differentiation for second-order derivatives. While recent probability flow approaches bypass this by learning score functions or matching velocity fields, they often involve serial computational operations or depend on sampling efficiency in complex distributions. To address these issues, we propose the Self-Consistent Probability Flow (SCPF) method. We reformulate the second-order FP equation into an equivalent first-order deterministic Probability Flow ODE (PF-ODE) constraint. Unlike score matching or velocity matching, SCPF solves this problem by minimizing the residual of the PF-ODE continuity equation, which avoids explicit Hessian computation. We leverage Continuous Normalizing Flows (CNF) combined with the Hutchinson Trace Estimator (HTE) to reduce the training complexity to linear scale $O(D)$, achieving an effective $O(1)$ wall-clock time on GPUs. To address data sparsity in high dimensions, we apply a generative adaptive sampling strategy and theoretically prove that dynamically aligning collocation points with the evolving probability mass is a necessary condition to bound the approximation error. Experiments on diverse benchmarks -- ranging from anisotropic Ornstein-Uhlenbeck (OU) processes and high-dimensional Brownian motions with time-varying diffusion terms, to Geometric OU processes featuring non-Gaussian solutions -- demonstrate that SCPF effectively mitigates the CoD, maintaining high accuracy and constant computational cost for problems up to 100 dimensions.

</details>


### [79] [Optimal Uncertainty Quantification under General Moment Constraints on Input Subdomains](https://arxiv.org/abs/2512.19572)
*Rong Jin,Xingsheng Sun*

Main category: physics.comp-ph

TL;DR: Optimal uncertainty quantification framework for systems with truncated moment constraints over subdomains, providing rigorous probability of failure bounds for safety certification with efficient computational methods.


<details>
  <summary>Details</summary>
Motivation: Need for rigorous safety certification of systems with uncertain inputs characterized by partial information (truncated moment constraints over subdomains) rather than full probability distributions.

Method: Develop OUQ framework with subdomain moment constraints, transform infinite-dimensional optimization to finite-dimensional unconstrained problems using canonical moments, incorporate inverse transform sampling for high-dimensional efficiency, and connect zeroth-order constraints to evidence theory.

Result: Increasing subdomains or moment order systematically tightens bound intervals; ITS reduces computational costs by up to two orders of magnitude while maintaining <1% relative error; identifies regimes where bounds are sensitive to partitioning or higher-order moments.

Conclusion: The framework provides principled safety certification with rigorous probability of failure bounds, offers computational efficiency for high-dimensional problems, and guides uncertainty reduction strategies for system safety.

Abstract: We present an optimal uncertainty quantification (OUQ) framework for systems whose uncertain inputs are characterized by truncated moment constraints defined over subdomains. Based on this partial information, rigorous optimal upper and lower bounds on the probability of failure (PoF) are derived over the admissible set of probability measures, providing a principled basis for system safety certification. We formulate the OUQ problem under general subdomain moment constraints and develop a high-performance computational framework to compute the optimal bounds. This approach transforms the original infinite-dimensional optimization problems into finite-dimensional unconstrained ones parameterized solely by free canonical moments. To address the prohibitive cost of PoF evaluation in high-dimensional settings, we incorporate inverse transform sampling (ITS), enabling efficient and accurate PoF estimation within the OUQ optimization. We also demonstrate that constraining inputs only by zeroth-order moments over subdomains yields a formulation equivalent to evidence theory. Three groups of numerical examples demonstrate the framework's effectiveness and scalability. Results show that increasing the number of subdomains or the moment order systematically tightens the bound interval. For high-dimensional problems, the ITS strategy reduces computational costs by up to two orders of magnitude while maintaining relative error below 1%. Furthermore, we identify regimes where optimal bounds are sensitive to subdomain partitioning or higher-order moments, guiding uncertainty reduction efforts for safety certification.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [80] [Divertor Detachment Characterization in Negative Triangularity Discharges in DIII-D via 2D Edge-Plasma Transport Modeling](https://arxiv.org/abs/2512.18052)
*Menglong Zhao,Filippo Scotti,Thomas Rognlien,Marvin Rensink,Alessandro Marinoni,Dinh Truong,Huiqian Wang,Kathreen Thome,Carlos Paz-Soldan*

Main category: physics.plasm-ph

TL;DR: Edge fluid modeling shows negative triangularity requires ~40% higher density for detachment onset than positive triangularity, with detachment more difficult due to shorter connection lengths and reduced divertor leg length.


<details>
  <summary>Details</summary>
Motivation: To understand the physics of divertor-plasma detachment in negative triangularity discharges on DIII-D and compare detachment requirements between negative and positive triangularity configurations.

Method: Used 2D multi-fluid code UEDGE with cross-field particle drifts to perform density scans, reproducing experimental roll-over of outer-target ion saturation current for both forward and reverse toroidal magnetic field configurations.

Result: Negative triangularity requires ~40% higher density for detachment onset compared to forward BT, and deep detachment is not achieved for reverse BT. Negative triangularity needs substantially higher densities (at/above Greenwald limit) compared to positive triangularity Ohmic discharges.

Conclusion: Detachment is more difficult in negative triangularity due to shorter midplane-to-target connection length, reduced outer divertor leg length, and lower cross-field transport compared to positive triangularity configurations.

Abstract: Edge fluid modeling of the first divertor-plasma detachment experiments in negative triangularity discharges on DIII-D is presented using the 2D multi-fluid code UEDGE, including cross-field particle drifts. Density scans are performed to reproduce the experimental roll-over of the outer-target ion saturation current and to investigate detachment physics for both forward and reverse toroidal magnetic field configurations. Consistent with experiments, the simulations show that approximately 40% higher density is required to reach detachment onset for forward BT compared to reverse BT, and that deep detachment is not achieved for reverse BT. Comparisons with positive triangularity Ohmic discharges further demonstrate that negative triangularity requires substantially higher densities, at or above the Greenwald limit, to access detachment. The modeling indicates that the increased difficulty of achieving detachment in negative triangularity arises from a shorter midplane-to-target connection length, a reduced outer divertor leg length, and lower cross-field transport compared to positive triangularity configurations.

</details>


### [81] [MCPlas, a MATLAB toolbox for reproducible plasma modelling with COMSOL](https://arxiv.org/abs/2512.18091)
*Marjan N. Stankov,Daan Boer,Wouter Graef,Kevin van 't Veer,Aleksandar P. Jovanović,Florian Sigeneger,Detlef Loffhagen,Jan van Dijk,Markus M. Becker*

Main category: physics.plasm-ph

TL;DR: MCPlas is a MATLAB toolbox for automated generation of fluid-Poisson models for non-thermal plasmas in COMSOL, using structured JSON input data and offering advanced electron transport treatment.


<details>
  <summary>Details</summary>
Motivation: To provide a transparent, reproducible workflow for non-thermal plasma simulation in COMSOL with structured data input, advanced electron transport modeling, and cross-platform reusability.

Method: Developed MATLAB toolbox that generates equation-based fluid-Poisson models using structured JSON input data (compatible with LXCat platform), supports 1D/2D geometries with various coordinate systems, and includes advanced electron transport description beyond common approaches.

Result: Successfully tested on DC- and RF-driven low-pressure argon glow discharges; verified reliability by comparison with COMSOL's Plasma Module; demonstrated significance of electron transport treatment and boundary conditions; showed easy handling of complex reaction kinetics and JSON data reusability across platforms.

Conclusion: MCPlas provides a transparent, reproducible workflow for non-thermal plasma simulation in COMSOL with structured JSON input, advanced electron transport modeling, and cross-platform data reusability, validated against established commercial tools.

Abstract: The MCPlas toolbox represents a collection of MATLAB functions for the automated generation of an equation-based fluid-Poisson model for non-thermal plasmas in the multiphysics simulation software COMSOL. Following the development of the new generation of the LXCat platform, all input data are prepared in a structured and interoperable JSON format and can be supplied and validated using existing JSON schemas. The toolbox includes fully transparent, editable MATLAB source code and offers an advanced description of electron transport in addition to commonly used approaches in the plasma modelling community. It supports one-dimensional and two-dimensional modelling geometries employing Cartesian, polar and cylindrical coordinate systems. MCPlas is tested on two reference cases: DC- and RF-driven low-pressure glow discharges in argon. Comparison of MCPlas results with results obtained by employing COMSOL's Plasma Module verifies the reliability of the plasma model implemented by MCPlas and demonstrates the significance of electron transport treatment and boundary conditions applied in the toolbox. Using the same examples, the easy handling of complex reaction kinetic models in MCPlas and the reusability of its JSON input data across different modelling platforms are illustrated. This demonstrates that MCPlas provides a transparent and reproducible workflow for the simulation of non-thermal plasmas using COMSOL.

</details>


### [82] [Electron Density Depletion in Re-Entry Plasma Flows Using Pulsed Electric Fields](https://arxiv.org/abs/2512.18163)
*Felipe Martin Rodriguez Fuentes,Bernard Parent*

Main category: physics.plasm-ph

TL;DR: First fully-coupled simulation of high-voltage pulsed discharges in hypersonic flow shows electric fields can create plasma depletion windows that reduce communication blackout by orders of magnitude with manageable power requirements.


<details>
  <summary>Details</summary>
Motivation: Communication blackout during re-entry due to plasma layer creates critical telemetry gaps for re-entry vehicles, requiring mitigation strategies.

Method: Fully-coupled simulation of high-voltage pulsed discharges interacting with Mach 24 flowfield using drift-diffusion model, with sensitivity analysis of ion and electron mobility models.

Result: Electric field generates non-neutral plasma sheath near cathode, depleting electron density by several orders of magnitude, reducing 4 GHz signal attenuation from 60% to 4% with 66 W/cm² power requirement. Sheath topology governed by ion kinetics, insensitive to electron mobility.

Conclusion: Drift-diffusion model provides conservative lower bound for mitigation performance; kinetic approach accounting for ballistic ion transport and non-local ionization would likely predict better performance with thicker sheaths and lower attenuation.

Abstract: Communication blackout due to the plasma layer creates a critical telemetry gap for re-entry vehicles. To mitigate this, we present the first fully-coupled simulation of high-voltage pulsed discharges interacting with a Mach 24 flowfield. The results demonstrate that the applied electric field generates a large, non-neutral plasma sheath near the cathode, depleting electron density by several orders of magnitude over a distance commensurate with the height of the shock layer. This depletion window effectively reduces the attenuation of a 4 GHz signal from 60% to 4% with a manageable power requirement of 66 W per cm$^2$ of exposed cathode surface. A sensitivity analysis reveals that the sheath topology is governed principally by ion kinetics; specifically, corrections to ion mobility at high reduced electric fields lead to enhanced space-charge shielding and a subsequent contraction of the sheath. Conversely, the sheath structure is largely insensitive to the electron mobility model. Finally, we argue that the present drift-diffusion model likely yields a conservative lower bound for mitigation performance. A kinetic approach accounting for ballistic ion transport and non-local ionization would likely predict thicker sheaths and lower attenuation for equivalent power deposition.

</details>


### [83] [Transfer Learning for Analysis of Collective and Non-Collective Thomson Scattering Spectra](https://arxiv.org/abs/2512.18173)
*T. Van Hoomissen,J. Alhuthali,A. M. Ortiz,D. A. Mariscal,R. S. Dorst,S. Eisenbach,H. Zhang,J. J. Pilgram,C. G. Constantin,L. Rovige,C. Niemann,D. B. Schaeffer*

Main category: physics.plasm-ph

TL;DR: Transfer learning improves deep neural network performance for Thomson scattering diagnostics when training data is limited (<200 spectra), enabling accurate electron density and temperature estimation even in noisy conditions.


<details>
  <summary>Details</summary>
Motivation: Conventional fitting algorithms for Thomson scattering diagnostics can fail when spectra are noisy or when fast real-time analysis is needed. Deep neural networks offer a solution but typically require large training datasets, which may not be available for experimental measurements.

Method: Developed five architecturally diverse deep neural networks pre-trained on synthetic Thomson scattering data, then adapted via transfer learning to experimental data. Evaluated performance with and without transfer learning across different training set sizes in both collective and non-collective scattering regimes.

Result: Transfer learning significantly reduces model error when training sets contain approximately 200 or fewer experimentally measured spectra. The approach enables accurate estimation of electron density (n_e) and electron temperature (T_e) even with limited experimental data.

Conclusion: Transfer learning is an effective strategy for improving deep neural network performance in Thomson scattering diagnostics when experimental training data is limited, making accurate plasma parameter estimation feasible for real-time operation and noisy conditions.

Abstract: Thomson scattering (TS) diagnostics provide reliable, minimally perturbative measurements of fundamental plasma parameters, such as electron density ($n_e$) and electron temperature ($T_e$). Deep neural networks can provide accurate estimates of $n_e$ and $T_e$ when conventional fitting algorithms may fail, such as when TS spectra are dominated by noise, or when fast analysis is required for real-time operation. Although deep neural networks typically require large training sets, transfer learning can improve model performance on a target task with limited data by leveraging pre-trained models from related source tasks, where select hidden layers are further trained using target data. We present five architecturally diverse deep neural networks, pre-trained on synthetic TS data and adapted for experimentally measured TS data, to evaluate the efficacy of transfer learning in estimating $n_e$ and $T_e$ in both the collective and non-collective scattering regimes. We evaluate errors in $n_e$ and $T_e$ estimates as a function of training set size for models trained with and without transfer learning, and we observe decreases in model error from transfer learning when the training set contains $\lessapprox$ 200 experimentally measured spectra.

</details>


### [84] [Growth of Phaseolus vulgaris in Response to Seed Priming by Plasma-Activated Water in Laboratory Screening and Outdoor Pot Trial](https://arxiv.org/abs/2512.18285)
*Mustafa Ghulam,Ramin Mehrabifard,Adriana Mišúthová,Zuzana Lukačová,Pratik Doshi,Zdenko Machala,Božena Šerá*

Main category: physics.plasm-ph

TL;DR: PAW treatments (priming, spraying, combination) enhanced Common bean growth in pot trials with increased seedling length, biomass, and antioxidant enzyme activity, though no germination improvement was observed in lab tests.


<details>
  <summary>Details</summary>
Motivation: To investigate the effects of plasma-activated water (PAW) on Common bean growth and physiology, exploring different application methods (priming, spraying, combination) for potential agricultural applications.

Method: Conducted laboratory germination trials and pot trials with three PAW treatments: PAW priming, PAW spraying, and their combination. Measured germination rates, seedling length, biomass, and antioxidant enzyme activity (SOD, G-POX, CAT, APX, GR).

Result: No germination improvement in lab trials. Pot trials showed significant increases in seedling length, biomass, and antioxidant enzyme activity. Enzymes SOD, G-POX, CAT, APX, and GR exhibited significantly higher activity in PAW-treated plants.

Conclusion: PAW enhances Common bean growth and physiology through reactive oxygen and nitrogen species, supporting its potential application in field farming despite no germination benefits.

Abstract: This study explores plasma-activated water (PAW) effects on Common bean growth in laboratory and pot trials. Three treatments were assessed: PAW priming, spraying, and their combination. Laboratory trials showed no germination improvement. However, pot trials revealed notable increases in seedling length, biomass, and antioxidant enzyme activity. Enzymes SOD, G-POX, CAT, APX, and GR showed significantly higher activity in PAW-treated plants. These effects were linked to reactive oxygen and nitrogen species in PAW. Findings suggest PAW enhances bean growth and physiology, supporting field farming applications.

</details>


### [85] [Nondiffusive transport of inertial heavy impurities in drift-wave turbulence](https://arxiv.org/abs/2512.18394)
*Zetao Lin,Benjamin Kadoch,Sadruddin Benkadda,Kai Schneider*

Main category: physics.plasm-ph

TL;DR: Tungsten impurities with inertia in drift-wave turbulence show non-diffusive transport and core accumulation, unlike tracer models.


<details>
  <summary>Details</summary>
Motivation: To understand impurity transport in fusion plasmas, particularly how tungsten impurities with finite inertia behave in drift-wave turbulence, which differs from previous tracer-based approaches.

Method: Used the Hasegawa-Wakatani model to simulate tungsten impurity transport with finite inertia effects, comparing to traditional tracer-based models.

Result: Simulations reveal a transition to non-diffusive dynamics for certain charge states, showing a turbulence-driven mechanism for core impurity accumulation.

Conclusion: Particle inertia plays a nontrivial role in impurity dynamics, with important implications for impurity control in future fusion devices.

Abstract: We investigate the transport behavior of tungsten impurities with finite inertia in drift-wave turbulence using the Hasegawa-Wakatani model. Unlike previous tracer-based models, our simulations reveal a transition to non-diffusive dynamics for a range of charge states. This novel mechanism offers a turbulence-driven route to core impurity accumulation. This finding underscores the nontrivial role of particle inertia in impurity dynamics and has strong implications for impurity control in future fusion devices.

</details>


### [86] [Study of the impact of fast ions on core turbulence at rational surfaces via global gyrokinetic simulations](https://arxiv.org/abs/2512.18639)
*D. Brioschi,A. Di Siena,R. Bilato,A. Bottino,T. Hayward-Schneider,A. Mishchenko,E. Poli,A. Zocco,F. Jenko*

Main category: physics.plasm-ph

TL;DR: Fast ions enhance shearing structures that suppress turbulence by 90% in some regions, and fishbone modes can further reduce turbulence without flattening thermal profiles.


<details>
  <summary>Details</summary>
Motivation: To study how fast ions interact with safety factor rational surfaces in turbulent plasmas, specifically examining their effects on turbulence suppression through various mechanisms.

Method: Global nonlinear gyrokinetic simulations analyzing fast particles-induced enhancement of shearing structures, considering competition with thermal profile dilution and quasi-resonant interaction.

Result: Fast ions reduce destabilization threshold for zonal modes, creating regions with 90% turbulence reduction. An n=m=1 fishbone mode further reduces turbulence when it doesn't flatten thermal profiles.

Conclusion: Fast ions effectively suppress turbulence through enhanced shearing structures and fishbone-driven zonal structures, offering efficient turbulence control mechanisms in plasmas.

Abstract: In this work, the interplay between fast ions and safety factor rational surfaces is studied in a turbulent plasma via global nonlinear gyrokinetic simulations. Initially, the fast particles-induced enhancement of shearing structures from turbulence self-interaction is analyzed. Our study takes into account the competition between this mechanism and other fast ions effects, i.e. thermal profiles dilution and quasi-resonant interaction. We find the fast ions-induced reduction of destabilization threshold for the zonal modes to be a very efficient way to suppress turbulence. Indeed, it leads to the formation of regions where turbulent transport is reduced by 90\% of its original value. Furthermore, an $n=m=1$ fishbone is driven unstable inside the plasma and its interaction with turbulence is studied. We find the beat-driven zonal structure generate by this mode to further reduce turbulence when its presence does not drastically flatten the thermal profiles.

</details>


### [87] [Non-Inductive Current Start-Up Using Multi-Harmonic Electron Cyclotron Wave and Current Ramp-Up Through Combined Electron Cyclotron Wave and Ohmic Heating in EXL-50U Spherical Torus](https://arxiv.org/abs/2512.18941)
*Xinchen Jiang,Yuejiang Shi,Yueng-Kay Martin Peng,Shaodong Song,Wenjun Liu,Xianming Song,Xiang Gu,Ji Qi,Dong Guo,Debabrata Banerjee,Lili Dong,Zhenxing Wang,Chunyan Li,Junquan Lin,Pingwei Zheng,Haojie MA,Huasheng Xie,Jiaqi Dong,Qingwei Yang,Yunfeng Liang,Baoshan Yuan,Xianmei Zhang,Minsheng Liu,EXL-50U team*

Main category: physics.plasm-ph

TL;DR: Multi-harmonic ECW current drive in EXL-50U spherical torus shows enhanced current generation with more resonance layers, achieving 1 MA plasma current with 70% non-inductive fraction.


<details>
  <summary>Details</summary>
Motivation: To investigate and demonstrate the effectiveness of multi-harmonic electron cyclotron wave (ECW) for non-inductive current start-up in spherical torus devices, particularly exploring how multiple resonance layers enhance driven current.

Method: Systematic experiments on EXL-50U spherical torus varying number of harmonic resonance layers by adjusting magnetic field or plasma cross section; using multi-harmonic ECW heating with X-mode or electron Bernstein wave; combining ECW with Ohmic heating during current ramp-up phase.

Result: Significant current enhancement with increasing resonance layers; first experimental verification of multi-harmonic ECW's critical role; first confirmation of first harmonic extraordinary mode current drive capacity; achieved 1 MA plasma current with 70% non-inductively driven current fraction through ECW-Ohmic heating synergy.

Conclusion: Multi-harmonic ECW effectively drives non-inductive current in spherical torus, with current drive efficiency enhanced by multiple resonance layers and synergistic combination with Ohmic heating, demonstrating practical potential for steady-state fusion reactor operation.

Abstract: The non-inductive current start-up by multi-harmonic electron cyclotron wave has been systematically investigated in the EXL-50U spherical torus. Significant enhancements of the driven current with increasing number of resonance layers have been demonstrated by variation of the number of harmonic resonance layers of the ECW through adjustment of the magnetic field or plasma cross section. The critical role of multi-harmonic ECW in enhancing the driven current has been experimentally verified for the first time. To explain the related experimental observations, a physical mechanism involving multi-harmonic heating, multiple reflections, and multi-pass absorption - leading to the generation of high-energy electrons via X-mode wave or electron Bernstein wave has been proposed. The current drive capacity of the first harmonic extraordinary mode of the ECW has also been experimentally confirmed for the first time. After the application of Ohmic heating during the current ramp-up phase, the current drive efficiency of ECW is further enhanced. Leveraging the synergistic effect between ECW and Ohmic heating, EXL-50U achieved a plasma current of 1 MA, with the non-inductively driven current fraction reaching 70%.

</details>


### [88] [Magnetically confined charged particles: From steep density profiles to the breaking of the adiabatic invariant](https://arxiv.org/abs/2512.19211)
*Aurélien Cordonnier,Yohann Lebouazda,Xavier Leoncini,Guilhem Dif-Pradalier*

Main category: physics.plasm-ph

TL;DR: Maximum entropy Vlasov equilibria for magnetically confined plasmas show overall stability but exhibit chaotic dynamics under burning plasma conditions, potentially undermining gyrokinetic modeling assumptions and causing unaccounted transport losses.


<details>
  <summary>Details</summary>
Motivation: To examine the stability of Vlasov equilibrium solutions derived through maximum entropy principle for magnetically confined plasmas, particularly investigating how these equilibria behave under burning plasma conditions where chaotic dynamics may emerge.

Method: Treating the toroidal limit as a perturbation from analytical cylindrical solutions, using aspect ratio as perturbation parameter, and computing particle trajectories sampled from kinetic equilibrium distribution to analyze stability.

Result: Equilibria align well with inviscid MHD description and show overall stability, but under burning plasma conditions, chaotic dynamics emerge for supra-thermal and thermal particles, destroying adiabatic invariance of magnetic moment.

Conclusion: Chaotic dynamics in burning plasmas could undermine gyrokinetic modeling assumptions and lead to unaccounted transport losses, warranting further investigation of turbulence-energetic particle interplay with Hamiltonian chaos.

Abstract: This study examines the stability of Vlasov equilibrium solutions for magnetically confined plasmas, derived through the principle of maximum entropy. By treating the toroidal limit as a perturbation from an analytical cylindrical solution, we demonstrate that these equilibria align well with the inviscid magnetohydrodynamic (MHD) description. Using the aspect ratio as a perturbation parameter, we compute particle trajectories sampled from the kinetic equilibrium distribution, confirming the overall stability of the solutions. However, under burning plasma conditions, chaotic dynamics emerge for particles with supra-thermal and even thermal energies. This destroys the adiabatic invariance of the magnetic moment. The exact consequences are unclear, but they could undermine the foundational assumptions of gyrokinetic modelling in burning plasmas. Nevertheless, these results suggest the possibility of unaccounted transport losses in future burning plasma operations. The interplay between turbulence and energetic particles in the presence of Hamiltonian chaos certainly warrants further investigation.

</details>


### [89] [Generation of near GeV protons by tightly focused laser interacting with down-ramp density plasma](https://arxiv.org/abs/2512.19341)
*Guanqi Qiu,Qianyi Ma,Dongchi Cai,Deji Liu,Yinren Shou,Zheng Gong,Jinqing Yu,Xueqing Yan*

Main category: physics.plasm-ph

TL;DR: Proton energy enhancement achieved by reducing laser focal spot size, with 83.5% energy increase at 0.8μm spot vs 3μm conventional spot, plus 60% additional improvement with optimal electron density profile.


<details>
  <summary>Details</summary>
Motivation: Enhancing proton energy for applications like cancer therapy while reducing dependence on high laser energy and large-scale laser facilities.

Method: Particle-in-cell simulations and theoretical modeling to study proton acceleration with reduced focal spot sizes, plus analytical derivation of optimal electron density profile.

Result: 83.5% proton energy enhancement at 0.8μm spot size compared to 3μm conventional spot; additional 60% improvement with optimal density profile; robust across parameter variations.

Conclusion: Advanced focusing techniques and optimal plasma profiles can reduce laser energy requirements for proton acceleration, potentially enabling medical and scientific applications without large-scale laser facilities.

Abstract: Enhancing proton energy is of great importance in laser-driven proton acceleration with finite laser energy for applications such as cancer therapy. We demonstrate an unusual acceleration scheme that achieves higher proton energies at lower laser energies by reducing the focal spot size. Through particle-in-cell simulations and theoretical modeling, we find that at small spot sizes (0.8 μm), the proton energy is enhanced by 83.5%, much higher than that under conventional spot sizes (3 μm). This is because the proton acceleration is dominated by electrons driven by an enhanced ponderomotive force at small spot sizes, generating stronger charge-separation fields that propagate faster. To further improve the proton energy, we analytically derive an optimal electron density profile, which enables phase-stable proton acceleration with an energy increased by 60%. These results are robust across parameter variations, suggesting that advanced focusing techniques and optimal plasma profiles could loose the requirement of laser energy, potentially reducing the dependence on large-scale laser facilities for medical and scientific applications.

</details>


### [90] [Optimization of the characteristics of a relativistic electron beam based on laser wake-field acceleration using a non-symmetric gas target profile](https://arxiv.org/abs/2512.19431)
*D. Mancelli,G. Andrianaki,I. Tazes,C. Vlachos,I. Fitilis,I. Nikolos,M. Bakarezos,E. P. Benis,V. Dimitriou,N. A. Papadogiannis,M. Tatarakis*

Main category: physics.plasm-ph

TL;DR: Researchers developed a high-energy, high-charge electron source using a novel gaseous target and ultra-intense femtosecond laser, achieving order-of-magnitude charge increase and doubled maximum energy via Laser Wake-Field Acceleration with ionization and downramp injection mechanisms.


<details>
  <summary>Details</summary>
Motivation: To develop an improved electron source for Very High Energy Electrons applications like radiotherapy by enhancing both the total charge and maximum energy of laser-accelerated electron beams.

Method: Used a novel gaseous target with a nonsymmetrical nozzle irradiated by an ultra-intense femtosecond laser pulse, employing Laser Wake-Field Acceleration mechanism. Particle-in-cell simulations were used to analyze electron injection via ionization and downramp injection mechanisms.

Result: Achieved at least an order-of-magnitude increase in total electron beam charge compared to previous symmetrical nozzle experiments, and doubled the maximum energy of accelerated electrons.

Conclusion: The demonstrated electron source is a promising candidate for high-dose Very High Energy Electrons applications such as radiotherapy, showing significant improvements in both charge and energy performance.

Abstract: We demonstrate a high-energy, high-charge, electron source produced by the irradiation of a novel gaseous target by an ultra-intense femtosecond laser pulse. By exploiting a nonsymmetrical nozzle, we increased the total charge of the electron beam by at least an order of magnitude with respect to our previous experiments using symmetrical nozzles. In addition, the maximum energy of the accelerated electrons was enhanced by a factor of two. The electrons are accelerated via the Laser Wake-Field Acceleration mechanism. Particle-in-cell simulations indicate that electrons are injected via the ionization and the downramp injection mechanisms. Our measurements indicate that the demonstrated electron source is a considerable candidate for high dose, Very High Energy Electrons applications, such as radiotherapy.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [91] [Is the active suspension in a complex viscoelastic fluid more chaotic or more ordered?](https://arxiv.org/abs/2512.18580)
*Yuan Zhou,Qingzhi Zou,Ignacio Pagonabarraga,Kaihuan Zhang,Kai Qi*

Main category: cond-mat.soft

TL;DR: Polymers in active swimmer suspensions enhance orientational order through hydrodynamic feedback mechanisms, increasing polarization by up to 26x for neutral squirmers and 5x for pullers.


<details>
  <summary>Details</summary>
Motivation: To understand whether polymers in complex viscoelastic environments enhance chaotic motion or promote orientational order in active swimmer suspensions, addressing a fundamental question about collective behavior in complex biological fluids.

Method: Lattice Boltzmann simulations of squirmer suspensions in polymer solutions, comparing to Newtonian counterparts, with analysis of hydrodynamic feedback mechanisms and polymer-swimmer alignment correlations.

Result: Polymers significantly enhance polarization (up to 26x for neutral squirmers, 5x for pullers) at intermediate swimmer volume fractions, creating orientational order through hydrodynamic feedback where squirmers stretch/align polymers which reinforce swimmer orientation.

Conclusion: Polymers promote orientational order in active suspensions through hydrodynamic feedback mechanisms, establishing a framework for understanding collective motion in complex fluids and suggesting strategies for controlling active systems via polymer-mediated interactions.

Abstract: The habitat of microorganisms is typically complex and viscoelastic. A natural question arises: Do polymers in a suspension of active swimmers enhance chaotic motion or promote orientational order? We address this issue by performing lattice Boltzmann simulations of squirmer suspensions in polymer solutions. At intermediate swimmer volume fractions, comparing to the Newtonian counterpart, polymers enhance polarization by up to a factor of 26 for neutral squirmers and 5 for pullers, thereby notably increasing orientational order. This effect arises from hydrodynamic feedback mechanism: squirmers stretch and align polymers, which in turn reinforce swimmer orientation and enhance polarization via hydrodynamic and steric interactions. The mechanism is validated by a positive correlation between polarization and a defined polymer-swimmer alignment parameter. Our findings establish a framework for understanding collective motion in complex fluids and suggest strategies for controlling active systems via polymer-mediated interactions.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [92] [Multiresolution analysis of quantum theories using Daubechies wavelet basis](https://arxiv.org/abs/2512.18372)
*Mrinmoy Basak*

Main category: hep-th

TL;DR: Thesis develops wavelet-based flow equation method (Similarity Renormalization Group) for 2D scalar field theory, achieving systematic block-diagonalization of Hamiltonian by resolution scale and improved truncation compared to previous approaches.


<details>
  <summary>Details</summary>
Motivation: To address multiscale problems where multiple length/energy scales contribute simultaneously, and to develop improved methods for scale separation in field theories using wavelet-based framework.

Method: Formulates flow equation method (Similarity Renormalization Group) within wavelet-based framework and applies it to 2D scalar field theory. Uses model of two real scalar fields coupled through quadratic interaction to demonstrate resolution separation.

Result: Flow systematically block-diagonalizes Hamiltonian with respect to wavelet resolution, achieving improved truncation. Effectively suppresses couplings between low- and high-resolution degrees of freedom, providing clear mechanism for isolating low-resolution physics.

Conclusion: Wavelet-based flow equation approach offers effective method for constructing effective Hamiltonians by separating scales, with potential applications to multiscale problems in field theory and beyond.

Abstract: Flow equation methods, more generally known as Similarity Renormalization Group (SRG) techniques, were developed to address multiscale problems where multiple length or energy scales contribute simultaneously. In this Thesis, we formulate the flow equation method within a wavelet-based framework and apply it to study scale (resolution) separation in a two-dimensional scalar field theory. We demonstrate that the flow systematically block-diagonalizes the Hamiltonian with respect to wavelet resolution, achieving improved truncation compared to earlier studies. Using a model of two real scalar fields coupled through a quadratic interaction, we show that the flow equations effectively suppress couplings between low- and high-resolution degrees of freedom. This provides a clear mechanism for isolating low-resolution physics and offers insight into the construction of effective Hamiltonians using a wavelet-based flow equation approac

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [93] [Gap-free Information Transfer in 4D-STEM via Fusion of Complementary Scattering Channels](https://arxiv.org/abs/2512.19460)
*Shengbo You,Georgios Varnavides,Sagar Khavnekar,Nikita Palatkin,Sihan Shao,Mingjian Wu,Daniel Stroppa,Darya Chernikova,Baixu Zhu,Ricardo Egoavil,Stefano Vespucci,Xingchen Ye,Florian K. M. Schur,Erdmann Spiecker,Philipp Pelz*

Main category: physics.optics

TL;DR: FF-STEM combines ptychographic phase reconstruction with dark-field imaging in a single 4D-STEM acquisition to eliminate low-frequency contrast gaps while maintaining dose efficiency and quantitative fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing linear phase-contrast STEM techniques suppress low-frequency contrast transfer, creating a "contrast gap" that limits quantitative imaging. Dark-field STEM captures low-frequency information but is dose-inefficient due to discarding much of the scattered signal.

Method: Fused Full-field STEM (FF-STEM) combines bright-field data for ptychographic phase reconstruction (estimating probe aberrations) with tilt-corrected dark-field imaging in a single 4D-STEM acquisition. The two channels are optimally fused in Fourier space using minimum-variance weighting based on spectral signal-to-noise ratio.

Result: FF-STEM produces transfer-gap-free images with high contrast and quantitative fidelity, preserving ptychography's upsampling and depth-sectioning capabilities while adding robust low-frequency contrast from dark-field imaging.

Conclusion: FF-STEM overcomes limitations of existing phase-contrast STEM techniques by providing dose-efficient, near-real-time reconstruction with complete frequency coverage, enabling quantitative imaging without experimental complexity.

Abstract: Linear phase-contrast scanning transmission electron microscopy (STEM) techniques compatible with high-throughput 4D-STEM acquisition are widely used to enhance phase contrast in weakly scattering and beam-sensitive materials. In these modalities, contrast transfer is often suppressed at low spatial frequencies, resulting in a characteristic contrast gap that limits quantitative imaging. Approaches that retain low-frequency phase contrast exist but typically require substantially increased experimental complexity, restricting routine use. Dark-field STEM imaging captures this missing low-frequency information through electrons scattered outside the bright-field disk, but discards a large fraction of the scattered signal and is therefore dose-inefficient. Fused Full-field STEM (FF-STEM) is introduced as a 4D-STEM imaging modality that overcomes this limitation by combining ptychographic phase reconstruction with tilt-corrected dark-field imaging within a single acquisition. Bright-field data are used to estimate probe aberrations and reconstruct a high-resolution phase image, while dark-field data provide complementary low-frequency contrast. The two channels are optimally fused in Fourier space using minimum-variance weighting based on the spectral signal-to-noise ratio, yielding transfer-gap-free images with high contrast and quantitative fidelity. FF-STEM preserves the upsampling and depth-sectioning capabilities of ptychography, adds robust low-frequency contrast characteristic of dark-field imaging, and enables dose-efficient, near-real-time reconstruction.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [94] [Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture](https://arxiv.org/abs/2512.19367)
*Christian Hägg,Kathlén Kohn,Giovanni Luca Marchetti,Boris Shapiro*

Main category: cs.LG

TL;DR: Sprecher Networks (SNs) are neural architectures inspired by classical KAS theory, using shared learnable splines with shift parameters and mixing weights for parameter and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: To create more parameter and memory-efficient neural networks by leveraging classical function approximation theory (Kolmogorov-Arnold-Sprecher construction) while addressing limitations of MLPs and KANs.

Method: SNs use shared learnable splines (monotonic/general) in structured blocks with explicit shift parameters and mixing weights. They implement Sprecher's 1965 sum of shifted splines formula in single-layer variant and extend to deeper multi-layer compositions, with optional lateral mixing connections for intra-block communication.

Result: SNs achieve O(LN + LG) parameter scaling vs MLPs' O(LN^2), reduce peak forward-intermediate memory from O(N^2) to O(N), enabling wider architectures under memory constraints. They demonstrate high parameter and memory efficiency in empirical evaluations.

Conclusion: Sprecher Networks provide a theoretically motivated, parameter and memory-efficient alternative to MLPs and KANs, leveraging classical function approximation theory with practical architectural innovations for modern deep learning applications.

Abstract: We present Sprecher Networks (SNs), a family of trainable neural architectures inspired by the classical Kolmogorov-Arnold-Sprecher (KAS) construction for approximating multivariate continuous functions. Distinct from Multi-Layer Perceptrons (MLPs) with fixed node activations and Kolmogorov-Arnold Networks (KANs) featuring learnable edge activations, SNs utilize shared, learnable splines (monotonic and general) within structured blocks incorporating explicit shift parameters and mixing weights. Our approach directly realizes Sprecher's specific 1965 sum of shifted splines formula in its single-layer variant and extends it to deeper, multi-layer compositions. We further enhance the architecture with optional lateral mixing connections that enable intra-block communication between output dimensions, providing a parameter-efficient alternative to full attention mechanisms. Beyond parameter efficiency with $O(LN + LG)$ scaling (where $G$ is the knot count of the shared splines) versus MLPs' $O(LN^2)$, SNs admit a sequential evaluation strategy that reduces peak forward-intermediate memory from $O(N^2)$ to $O(N)$ (treating batch size as constant), making much wider architectures feasible under memory constraints. We demonstrate empirically that composing these blocks into deep networks leads to highly parameter and memory-efficient models, discuss theoretical motivations, and compare SNs with related architectures (MLPs, KANs, and networks with learnable node activations).

</details>


### [95] [Initialization of a Polyharmonic Cascade, Launch and Testing](https://arxiv.org/abs/2512.19524)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: The paper presents a universal initialization method for polyharmonic cascade architectures using symmetric hyperoctahedral constellations, enabling stable training of very deep networks (up to 500 layers) without skip connections while simplifying computations to 2D operations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of training very deep neural networks without skip connections by developing a theoretically-grounded initialization procedure that ensures stability and simplifies computations.

Method: Proposes symmetric constellation initialization in the form of hyperoctahedra with a central point for polyharmonic cascade architectures, reducing all linear algebra to efficient 2D operations executable on GPUs.

Result: Demonstrates scalability and robustness: MNIST (98.3% without convolutions/augmentations), HIGGS (AUC ~0.885 on 11M examples), Epsilon (AUC ~0.963 with 2000 features). Enables stable training of cascades up to 500 layers without skip connections.

Conclusion: The proposed universal initialization method successfully enables stable training of extremely deep polyharmonic cascade architectures while simplifying computations, with reproducible results provided in public repositories.

Abstract: This paper concludes a series of studies on the polyharmonic cascade, a deep machine learning architecture theoretically derived from indifference principles and the theory of random functions. A universal initialization procedure is proposed, based on symmetric constellations in the form of hyperoctahedra with a central point. This initialization not only ensures stable training of cascades with tens and hundreds of layers (up to 500 layers without skip connections), but also radically simplifies the computations. Scalability and robustness are demonstrated on MNIST (98.3% without convolutions or augmentations), HIGGS (AUC approximately 0.885 on 11M examples), and Epsilon (AUC approximately 0.963 with 2000 features). All linear algebra is reduced to 2D operations and is efficiently executed on GPUs. A public repository and an archived snapshot are provided for full reproducibility.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [96] [On potentials for sub-Laplacians and geometric applications](https://arxiv.org/abs/2512.18221)
*Shiguang Ma,Jie Qing*

Main category: math.DG

TL;DR: Extends potential theory from Euclidean spaces to homogeneous Carnot groups, using geometric completeness to estimate Hausdorff dimension of polar sets for sub-Laplacians, with applications to various CR geometries.


<details>
  <summary>Details</summary>
Motivation: To extend classical potential theory and its geometric applications from Euclidean spaces to the more general setting of homogeneous Carnot groups, which are important in sub-Riemannian geometry and analysis.

Method: Introduces a new approach using geometric completeness to estimate Hausdorff dimension of polar sets. Relies on inequalities analogous to classic Riesz potential integral inequalities, extends geometric measure theory to homogeneous Carnot groups, and uses polar coordinates with horizontal radial curves constructed by Balogh and Tyson for polarizable Carnot groups.

Result: Develops potential theory for sub-Laplacians in homogeneous Carnot groups, establishing estimates for Hausdorff dimension of polar sets through geometric completeness approach.

Conclusion: The framework enables applications of potentials for sub-Laplacians in various CR geometries including CR geometry, quaternionic CR geometry, and octonionic CR geometry, extending classical results to these non-Euclidean settings.

Abstract: In this paper we extend the research on potential theory and its geometric applications from Euclidean spaces to homogeneous Carnot groups. We introduce a new approach to use the geometric completeness to estimate the Hausdorff dimension of polar sets of potentials of nonnegative Radon measures for sub-Laplacians in homogeneous Carnot groups. Our approach relies on inequalities that are analogous to the classic integral inequalities about Riesz potentials in Euclidean spaces. Our approach also uses extensions of some of geometric measure theory to homogeneous Carnot groups and the polar coordinates with horizontal radial curves constructed by Balogh and Tyson for polarizable Carnot groups. As consequences, we develop applications of potentials for sub-Laplacians in CR geometry, quaternionic CR geometry, and octonionic CR geometry.

</details>


### [97] [Explicit harmonic and wave maps into variable-curvature surfaces](https://arxiv.org/abs/2512.18376)
*Anestis Fotiadis,Giannis Polychrou*

Main category: math.DG

TL;DR: Explicit harmonic and wave maps constructed between pseudo-Riemannian surfaces with variable curvature using integrable ODE reduction.


<details>
  <summary>Details</summary>
Motivation: Extend explicit solution construction beyond classical constant-curvature and symmetric-space settings to include variable curvature surfaces like ellipsoids.

Method: Use natural ansatz to reduce harmonic and wave map equations to integrable ordinary differential equations for broad class of target metrics.

Result: Obtained explicit solutions for harmonic and wave maps between pseudo-Riemannian surfaces including nonconstant curvature surfaces.

Conclusion: Method works uniformly for both elliptic and hyperbolic regimes, providing explicit constructions beyond traditional symmetric settings.

Abstract: We construct explicit harmonic and wave maps between pseudo-Riemannian surfaces of variable curvature. For a broad class of target metrics, including nonconstant curvature surfaces such as ellipsoids, the harmonic and wave map equations admit a reduction to integrable ordinary differential equations under a natural ansatz. This yields explicit solutions beyond the classical constant-curvature and symmetric-space settings. The method applies uniformly in both elliptic and hyperbolic regimes.

</details>


### [98] [The space-time-Grassmann measure of the Brakke flow](https://arxiv.org/abs/2512.19227)
*Yu Tong Liu,Myles Workman*

Main category: math.DG

TL;DR: The paper introduces a new definition of Brakke flows using space-time-Grassmann measures, establishing equivalence with classical definitions and proving measurability of key geometric quantities.


<details>
  <summary>Details</summary>
Motivation: To provide a more robust and canonical framework for studying Brakke flows by introducing space-time measures that capture the flow's geometric evolution in a unified way.

Method: Construct a canonical space-time-Grassmann measure λ over J × G_k(U) for k-dimensional Brakke flows, characterize flows with respect to this measure's space-time weight, and define Brakke flows as space-time measures satisfying the Brakke inequality distributionally.

Result: Proved existence of canonical space-time-Grassmann measure λ, established equivalence between classical Brakke flow definitions and the new measure-based definition, and showed measurability of mean curvature vector, density, and tangent map with respect to the space-time weight measure.

Conclusion: The new space-time measure approach provides a canonical, unified framework for Brakke flows that is equivalent to classical definitions while offering improved measurability properties for key geometric quantities.

Abstract: For a $k$-dimensional Brakke flow on an open subset $U \subset \mathbf{R}^{n}$, over an open time interval $J$, we prove the existence of a canonical space-time-Grassmann measure $λ$, over $J \times \mathbf{G}_{k} (U)$, and give a characterisation of the flow with respect to the space-time weight of this measure. This results in a new definition of the Brakke flow, as that of a space-time measure which satisfies the Brakke inequality in a distributional sense. Each such space-time measure corresponds to a class of equivalent (classical) Brakke flows, thus yielding an equivalence between the classical definitions of the Brakke flow, and this new definition. Moreover, we prove that the mean curvature vector, density, and tangent map along the flow, are all measurable with respect to this space-time weight measure.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [99] [A perturbed preconditioned gradient descent method for the unconstrained minimization of composite objectives](https://arxiv.org/abs/2512.19532)
*Jea-Hyun Park,Abner J. Salgado,Steven M. Wise*

Main category: math.OC

TL;DR: PPGD method for strongly convex optimization with approximate gradient information, applied to Cahn-Hilliard equations with variable mobility.


<details>
  <summary>Details</summary>
Motivation: Need efficient optimization methods for problems where gradient information is only approximately known, particularly for complex PDE problems like the Cahn-Hilliard equations with variable mobility.

Method: Perturbed preconditioned gradient descent (PPGD) method for unconstrained minimization of strongly convex objectives with locally Lipschitz continuous gradient, where gradient of one component (F) is only known approximately. Analysis conducted in infinite dimensions with preconditioner.

Result: Proves linear rate of convergence up to an error term dependent on gradient approximation. Successfully applies PPGD to stationary Cahn-Hilliard equations with variable mobility under periodic boundary conditions.

Conclusion: PPGD is effective for optimization with approximate gradients, validated by numerical experiments showing theoretical convergence rates and exploration of mobility effects on computation.

Abstract: We introduce a perturbed preconditioned gradient descent (PPGD) method for the unconstrained minimization of a strongly convex objective $G$ with a locally Lipschitz continuous gradient. We assume that $G(v)=E(v)+F(v)$ and that the gradient of $F$ is only known approximately. Our analysis is conducted in infinite dimensions with a preconditioner built into the framework. We prove a linear rate of convergence, up to an error term dependent on the gradient approximation. We apply the PPGD to the stationary Cahn-Hilliard equations with variable mobility under periodic boundary conditions. Numerical experiments are presented to validate the theoretical convergence rates and explore how the mobility affects the computation.

</details>


### [100] [Studies on the Rao-Nakra Sandwich Beam: Well-Posedness, Dynamics, and Controllability](https://arxiv.org/abs/2512.18381)
*George J. Bautista,Roberto de A. Capistrano-Filho,Boumediene Chentouf,Oscar Sierra Fonseca,Juan Límaco*

Main category: math.OC

TL;DR: Analysis of well-posedness, stabilization, and boundary controllability for a three-equation coupled Rao-Nakra sandwich beam system with dynamic boundary conditions, addressing both time-delayed damping and boundary control problems.


<details>
  <summary>Details</summary>
Motivation: The paper investigates a complex three-equation coupled system representing a Rao-Nakra type sandwich beam, which presents significant mathematical challenges due to dynamic boundary conditions and coupling between longitudinal and transverse displacements. The motivation is to establish rigorous mathematical foundations for such systems, addressing both stabilization (with time-varying delays) and controllability problems that have practical applications in structural engineering and control theory.

Method: Two main approaches: 1) For the damped system with time-dependent weights and delays, uses semigroup theory and Kato's classical result for well-posedness, and Lyapunov functionals for exponential energy decay. 2) For boundary controllability, employs the Hilbert Uniqueness Method (HUM) with observability inequalities for the adjoint system. Both approaches handle the full three-equation coupling and dynamic boundary conditions through carefully constructed mathematical techniques.

Result: Proves existence and uniqueness of solutions for both damped and controlled systems. Demonstrates exponential energy decay despite time-varying weights and delays. Establishes null controllability of the boundary control system through observability inequalities and HUM application.

Conclusion: Successfully addresses the significant mathematical challenges of the full three-equation coupled Rao-Nakra sandwich beam system with dynamic boundary conditions. The work provides comprehensive results on well-posedness, stabilization with exponential decay despite delays, and boundary null controllability, representing important contributions to the mathematical analysis of coupled structural systems.

Abstract: In this work, we investigate the well-posedness, stabilization, and boundary controllability of a linear Rao-Nakra type sandwich beam. The system consists of three coupled equations that represent the longitudinal displacements of the outer layers and the transverse displacement of the composite beam, all of which are coupled with dynamical boundary conditions. In the first problem, time-dependent weights and delays are considered. Then, we establish the existence and uniqueness of solutions for the Cauchy problem associated with the damped system using semigroup theory and a classical result by Kato. Furthermore, employing a Lyapunov-based approach, we prove that the system's energy decays exponentially, despite the presence of time-varying weights and delays. In the second problem, we consider a boundary linear control system and prove its well-posedness. By deriving an observability inequality for the adjoint system and applying the Hilbert Uniqueness Method (HUM), we show that the system is null controllable. A key contribution of this work lies in handling the full three-equation coupled system, which involves significant difficulty due to the dynamic boundary conditions, resolved via appropriately constructed Lyapunov functionals and intermediate observability inequalities.

</details>


### [101] [An alternative approach to well-posedness of McKean-Vlasov equations arising in Consensus-Based Optimization](https://arxiv.org/abs/2512.19446)
*Alessandro Baldi*

Main category: math.OC

TL;DR: Proves well-posedness of mean-field CBO equations using truncation arguments to handle non-Lipschitz fields, extending uniqueness results.


<details>
  <summary>Details</summary>
Motivation: Consensus-Based Optimization (CBO) is a derivative-free particle optimization method, but its mean-field description involves non-local SDEs with non-globally Lipschitz continuous fields, requiring rigorous well-posedness analysis.

Method: Introduces a novel truncation approach using a cut-off function defined on probability measure space to handle non-Lipschitz fields, allowing analysis within Sznitman's classical framework for McKean-Vlasov equations.

Result: Establishes existence of strong solutions and extends pathwise uniqueness to a broader class of solutions for mean-field CBO equations through the truncation argument.

Conclusion: The truncation method provides a robust framework for proving well-posedness of mean-field CBO equations with non-Lipschitz fields, advancing theoretical understanding of CBO methods.

Abstract: In this work we study the mean-field description of Consensus-Based Optimization (CBO), a derivative-free particle optimization method. Such a description is provided by a non-local SDE of McKean-Vlasov type, whose fields lack of global Lipschitz continuity. We propose a novel approach to prove the well-posedness of the mean-field CBO equation based on a truncation argument. The latter is performed through the introduction of a cut-off function, defined on the space of probability measures, acting on the fields. This procedure allows us to study the well-posedness problem in the classical framework of Sznitman. Through this argument, we recover the established result on the existence of strong solutions, and we extend the class of solutions for which pathwise uniqueness holds.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [102] [Investigating Hamiltonian Dynamics by the Method of Covariant Lyapunov Vectors](https://arxiv.org/abs/2512.17962)
*Jean-Jacq du Plessis*

Main category: nlin.CD

TL;DR: This thesis develops methods for analyzing Hamiltonian systems using Lyapunov exponents and covariant Lyapunov vectors (CLVs), with applications to the Hénon-Heiles system and DNA bubble dynamics.


<details>
  <summary>Details</summary>
Motivation: To improve numerical methods for computing covariant Lyapunov vectors in Hamiltonian systems and apply these techniques to understand chaotic dynamics in physical systems like the Hénon-Heiles model and DNA thermal openings.

Method: Uses the Ginelli & collaborators algorithm for CLV computation, develops convergence measurement methods for vectors/subspaces, applies to Hénon-Heiles system for center subspace analysis, and introduces instantaneous Lyapunov vectors (ILVs) for DNA bubble dynamics in the Peyrard-Bishop-Dauxois model.

Result: Developed methods to determine appropriate transient times for CLV computation, improved center subspace accuracy in Hénon-Heiles, observed splitting subspaces become almost tangent during sticky regimes, and found distinct relationships between DNA bubble sizes and ILV distributions (but not with CLVs).

Conclusion: The thesis successfully advances numerical methods for CLV analysis in Hamiltonian systems, providing insights into chaotic dynamics and demonstrating the utility of ILVs for studying transient phenomena like DNA bubble formation.

Abstract: In this thesis, we review the theory of Lyapunov exponents and covariant Lyapunov vectors (CLVs) and use these objects to numerically investigate the dynamics of several autonomous Hamiltonian systems. The algorithm which we use for computing CLVs is the one developed by Ginelli and collaborators (G&C), which is quite efficient and has been used previously in many numerical investigations. Using two low-dimensional Hamiltonian systems as toy models, we develop a method for measuring the convergence rates of vectors and subspaces computed via the G&C algorithm, and we use the time it takes for this convergence to occur to determine the appropriate transient time lengths needed when applying this algorithm to compute CLVs. The tangent dynamics of the centre subspace of the Hénon-Heiles system is investigated numerically through the use of CLVs, and we propose a method that improves the accuracy of the centre subspace computed with the G&C algorithm. As another application of the method of CLVs to the Hénon-Heiles system, we find that the splitting subspaces (which form a splitting of the tangent space and define the CLVs) become almost tangent during sticky regimes of motion, an observation which is related to the hyperbolicity of the system. Additionally, we investigate the dynamics of bubbles (i.e. thermal openings between base pairs) in homogeneous DNA sequences using the Peyrard-Bishop-Dauxois lattice model of DNA. For the purpose of studying short-lived bubbles in DNA, the notions of instantaneous Lyapunov vectors (ILVs) are introduced in the context of Hamiltonian dynamics. While we find that the size of the opening between base pairs has no clear relationship with the spatial distribution of the first CLV at that site, we do observe a distinct relationship with various ILV distributions.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [103] [Hybrid Stochastic Functional Differential Equations with Infinite Delay: Approximations and Numerics](https://arxiv.org/abs/2512.18990)
*Guozhen Li,Xiaoyue Li,Xuerong Mao,Guoting Song*

Main category: math.PR

TL;DR: Hybrid SFDEs with infinite delay can be approximated by corresponding finite delay equations, enabling numerical approximation.


<details>
  <summary>Details</summary>
Motivation: To investigate whether solutions of hybrid stochastic functional differential equations (SFDEs) with infinite delay can be approximated by solutions of corresponding equations with finite delay, enabling numerical approximation methods.

Method: Theoretical analysis establishing approximation results for a large class of highly nonlinear hybrid SFDEs with infinite delay by studying their finite delay counterparts.

Result: A positive result is established showing that hybrid SFDEs with infinite delay can be approximated by corresponding hybrid SFDEs with finite delay.

Conclusion: The new theory enables numerical approximation of solutions for hybrid SFDEs with infinite delay via numerical solutions of corresponding finite delay equations.

Abstract: This paper is to investigate if the solution of a hybrid stochastic functional differential equation (SFDE) with infinite delay can be approximated by the solution of the corresponding hybrid SFDE with finite delay. A positive result is established for a large class of highly nonlinear hybrid SFDEs with infinite delay. Our new theory makes it possible to numerically approximate the solution of the hybrid SFDE with infinite delay, via the numerical solution of the corresponding hybrid SFDE with finite delay.

</details>


### [104] [A note on stochastic semilinear dissipative evolution equations](https://arxiv.org/abs/2512.18398)
*Carlo Marinelli*

Main category: math.PR

TL;DR: Proves existence and uniqueness of mild solutions for semilinear stochastic evolution equations with additive noise, where the linear drift generates a compact contraction semigroup and the nonlinear part is a decreasing superposition operator.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for semilinear stochastic evolution equations with specific structural assumptions, which are important in modeling various physical and engineering systems with random perturbations.

Method: Uses functional analysis and stochastic calculus techniques, leveraging properties of compact contraction semigroups and decreasing superposition operators to prove existence and uniqueness results for mild solutions.

Result: Successfully proves the existence and uniqueness of mild solutions to the specified class of semilinear stochastic evolution equations with additive noise under the given assumptions.

Conclusion: The paper establishes rigorous mathematical results for a specific class of semilinear stochastic evolution equations, providing a solid theoretical foundation for further analysis and applications in stochastic partial differential equations.

Abstract: Existence and uniqueness of mild solutions to a class of semilinear stochastic evolution equations with additive noise is proved. The linear part of the drift term is the generator of a compact semigroup of contractions, while the nonlinear part is only assumed to be the superposition operator associated to a decreasing function.

</details>


### [105] [Large deviations for stochastic evolution equations beyond the coercive case](https://arxiv.org/abs/2512.19501)
*Esmée Theewis*

Main category: math.PR

TL;DR: The paper establishes a small-noise large deviation principle (LDP) for stochastic evolution equations in L² settings without requiring coercivity, accommodating non-coercive coefficients, modified criticality conditions, and vanishing drift perturbations.


<details>
  <summary>Details</summary>
Motivation: Existing LDP frameworks typically require coercive coefficients or variational settings, limiting their applicability. The authors aim to develop a more general framework that works with non-coercive coefficients and accommodates a broader class of stochastic evolution equations.

Method: The authors replace coercivity requirements with only well-posedness of the stochastic evolution equation and two concrete, verifiable a priori estimates. They accommodate drift nonlinearities satisfying a modified criticality condition and allow for vanishing drift perturbations (enabling treatment of both Itô and Stratonovich interpretations).

Result: The paper proves the small-noise LDP for stochastic evolution equations in L² settings without coercivity. The framework has been applied to the 3D primitive equations with full transport noise in another paper, and in this paper is applied to a non-coercive reaction-diffusion system. Even in coercive cases, new LDP results are obtained for equations with critical nonlinearities.

Conclusion: The developed framework significantly expands the scope of LDP theory for stochastic evolution equations by eliminating coercivity requirements, accommodating various noise interpretations, and handling critical nonlinearities. The versatility is demonstrated through applications to physically relevant systems like the 3D primitive equations and reaction-diffusion systems.

Abstract: We prove the small-noise large deviation principle (LDP) for stochastic evolution equations in an $L^2$-setting. As the coefficients are allowed to be non-coercive, our framework encompasses a much broader scope than variational settings. To replace coercivity, we require only well-posedness of the stochastic evolution equation and two concrete, verifiable a priori estimates. Furthermore, we accommodate drift nonlinearities satisfying a modified criticality condition, and we allow for vanishing drift perturbations. The latter permits the inclusion of Itô--Stratonovich correction terms, enabling the treatment of both noise interpretations. In another paper, our results have been applied to the 3D primitive equations with full transport noise. In the current paper, we give an application to a reaction-diffusion system which lacks coercivity, further demonstrating the versatility of the framework. Finally, we show that even in the coercive case, we obtain new LDP results for equations with critical nonlinearities that rely on our modified criticality condition, including the stochastic 2D Allen--Cahn equation in the weak setting.

</details>


### [106] [The large deviation principle for the stochastic 3D primitive equations with transport noise](https://arxiv.org/abs/2512.19541)
*Antonio Agresti,Esmée Theewis*

Main category: math.PR

TL;DR: Proves small-noise large deviation principle for 3D primitive equations with transport noise and turbulent pressure, treating both Stratonovich and Itô noise.


<details>
  <summary>Details</summary>
Motivation: Transport noise is important for geophysical fluid dynamics as it models small-scale effects on large-scale dynamics. The mathematical challenge involves transport noise acting on full horizontal velocity, creating non-trivial turbulent pressure requiring complex energy analysis.

Method: Proves small-noise large deviation principle for three-dimensional primitive equations with transport noise and turbulent pressure. The analysis involves obtaining necessary energy bounds for the system where transport noise acts on the full horizontal velocity.

Result: Successfully establishes the large deviation principle for both Stratonovich and Itô noise formulations of the 3D primitive equations with transport noise and turbulent pressure.

Conclusion: The paper provides rigorous mathematical foundation for understanding small-noise behavior in geophysical fluid dynamics models with transport noise, addressing the challenging case where noise affects full horizontal velocity and generates turbulent pressure.

Abstract: We prove the small-noise large deviation principle for the three-dimensional primitive equations with transport noise and turbulent pressure. Transport noise is important for geophysical fluid dynamics applications, as it takes into account the effect of small scales on the large scale dynamics. The main mathematical challenge is that we allow for the transport noise to act on the full horizontal velocity, therefore leading to a non-trivial turbulent pressure, which requires an involved analysis to obtain the necessary energy bounds. Both Stratonovich and Itô noise are treated.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [107] [Super-Poissonian Squeezed Light in the Ground State of Strongly Coupled Light-matter Systems](https://arxiv.org/abs/2512.18242)
*Cankut Tasci,Mohammad Hassan,Leon Orlov-Sullivan,Leonardo A. Cunha,Johannes Flick*

Main category: physics.chem-ph

TL;DR: QEDFT with pMBD functional reveals nonclassical photonic features (squeezing, super-Poissonian statistics) in ground states of strongly coupled molecular systems, showing importance of many-body electron-photon correlations.


<details>
  <summary>Details</summary>
Motivation: While strong light-matter coupling creates hybrid states with correlated photonic/electronic degrees of freedom, the impact of many-body effects on quantum-optical observables remains largely unexplored, despite known reshaping of electronic properties.

Method: Use quantum electrodynamical density-functional theory (QEDFT) combined with the recently developed photon-many-body dispersion (pMBD) functional to capture higher-order electron-photon correlations and multi-photon processes.

Result: Ground-state photonic observables (photon number fluctuations, second-order correlations, quadrature variances) show squeezing and super-Poissonian photon statistics emerging from light-matter interactions in strong coupling regime.

Conclusion: Capturing the full hierarchy of many-body, electron-photon and multi-photon correlations is essential for consistent description of quantum-optical properties in strongly coupled molecular systems, establishing QEDFT as a first-principles framework for predicting nonclassical photonic features.

Abstract: Strong light-matter coupling enables hybrid states in which photonic and electronic degrees of freedom become correlated even in the ground state. While many-body effects in long-range dispersion interactions are known to reshape electronic properties under such conditions, their impact on quantum-optical observables remains largely unexplored. Here, we address this problem using quantum electrodynamical density-functional theory (QEDFT) combined with the recently developed photon-many-body dispersion (pMBD) functional, which can capture higher-order electron-photon correlations and multi-photon processes. We compute ground-state photonic observables including photon number fluctuations, second-order correlations, and quadrature variances, and find squeezing and super-Poissonian photon statistics emerging from light-matter interactions in the strong coupling regime. Our results demonstrate that capturing the full hierarchy of many-body, electron-photon and multi-photon correlations is essential for a consistent description of quantum-optical properties in strongly coupled molecular systems, establishing QEDFT as a first-principles framework for predicting nonclassical photonic features in the ground state of complex systems.

</details>


### [108] [A Fixed-Volume Variant of Gibbs-Ensemble Monte Carlo Yields Significant Speedup in Binodal Calculation](https://arxiv.org/abs/2512.18899)
*Sanbo Qin,Huan-Xiang Zhou*

Main category: physics.chem-ph

TL;DR: Fixed-volume GEMC eliminates volume exchange to dramatically speed up gas-liquid binodal calculations for complex systems like proteins.


<details>
  <summary>Details</summary>
Motivation: Standard Gibbs-ensemble Monte Carlo (GEMC) is computationally too demanding for realistic protein models due to slow volume exchange, limiting its practical application.

Method: Developed a variant GEMC without volume exchange by determining an appropriate initial density, maintaining the core GEMC approach but eliminating the slowest step.

Result: Testing on Lennard-Jones and patchy particles shows enormous speedup without any loss of accuracy in predicted binodals compared to standard GEMC.

Conclusion: Fixed-volume GEMC promises many applications due to its fast speed while maintaining accuracy, making it practical for complex systems like proteins.

Abstract: Gibbs-ensemble Monte Carlo (GEMC) is a powerful method for calculating the gas-liquid binodals of simple models and small molecules, but is too demanding computationally for realistic models of proteins. Here we discover that the main reason for long simulations is that volume exchange is very slow to achieve, and develop a variant GEMC without volume exchange. The key is to determine an appropriate initial density. Test of this fixed-volume GEMC method on Lennard-Jones and patchy particles shows enormous speedup without any loss of accuracy in predicted binodals. The fast speed of fixed-volume GEMC promises many applications.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [109] [Rapid stabilization of the heat equation with localized disturbance](https://arxiv.org/abs/2512.19160)
*Patricio Guzmán,Hugo Parada,Christian Calle-Cárdenas*

Main category: eess.SY

TL;DR: Novel multivalued feedback control for rapid stabilization of multidimensional heat equation with unknown localized disturbances, combining frequency Lyapunov method with sign operator for robustness.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of stabilizing heat equations with unknown spatially localized disturbances without requiring explicit disturbance modeling or estimation.

Method: Proposes a multivalued feedback control strategy that synthesizes frequency Lyapunov method with sign multivalued operator, connecting Lyapunov-based stability analysis with spectral inequalities.

Result: Proves well-posedness of the closed-loop system via maximal monotone operator theory and guarantees exponential stabilization despite unknown disturbances.

Conclusion: The approach successfully achieves robust exponential stabilization of multidimensional heat equations with unknown localized disturbances without explicit disturbance modeling or estimation.

Abstract: This paper studies the rapid stabilization of a multidimensional heat equation in the presence of an unknown spatially localized disturbance. A novel multivalued feedback control strategy is proposed, which synthesizes the frequency Lyapunov method (introduced by Xiang [41]) with the sign multivalued operator. This methodology connects Lyapunov-based stability analysis with spectral inequalities, while the inclusion of the sign operator ensures robustness against the disturbance. The closed-loop system is governed by a differential inclusion, for which well-posedness is proved via the theory of maximal monotone operators. This approach not only guarantees exponential stabilization but also circumvents the need for explicit disturbance modeling or estimation.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [110] [Estimating Solvation Free Energies with Boltzmann Generators](https://arxiv.org/abs/2512.18147)
*Maximilian Schebek,Nikolas M. Froböse,Bettina G. Keller,Jutta Rogal*

Main category: cond-mat.stat-mech

TL;DR: Normalizing flows map solvent configurations between different solute sizes, improving free energy calculations compared to traditional alchemical methods.


<details>
  <summary>Details</summary>
Motivation: Traditional solvation free energy calculations require extensive sampling and many intermediate states to ensure sufficient phase-space overlap between gas and solution states, making them computationally expensive.

Method: Developed a computational framework using normalizing flows that directly maps solvent configurations between solutes of different sizes, enabling direct estimation of free energy differences without multiple intermediate steps.

Result: The flow-based approach yields acceptable accuracy for challenging transformations (solute growth, increased solute-solute separation) that typically require multiple intermediate simulation steps. Analysis shows the flow generates physically meaningful solvent rearrangements and substantially enhances configurational overlap between states.

Conclusion: Flow-based models represent a promising alternative to traditional free energy estimation methods, potentially reducing computational cost while maintaining accuracy for solvation free energy calculations.

Abstract: Accurate calculations of solvation free energies remain a central challenge in molecular simulations, often requiring extensive sampling and numerous alchemical intermediates to ensure sufficient overlap between phase-space distributions of a solute in the gas phase and in solution. Here, we introduce a computational framework based on normalizing flows that directly maps solvent configurations between solutes of different sizes, and compare the accuracy and efficiency to conventional free energy estimates. For a Lennard-Jones solvent, we demonstrate that this approach yields acceptable accuracy in estimating free energy differences for challenging transformations, such as solute growth or increased solute-solute separation, which typically demand multiple intermediate simulation steps along the transformation. Analysis of radial distribution functions indicates that the flow generates physically meaningful solvent rearrangements, substantially enhancing configurational overlap between states in configuration space. These results suggest flow-based models as a promising alternative to traditional free energy estimation methods.

</details>


### [111] [Escape from heterogeneous diffusion](https://arxiv.org/abs/2512.19646)
*Hwai-Ray Tung,Sean D Lawley*

Main category: cond-mat.stat-mech

TL;DR: Analytical determination of mean escape times and splitting probabilities for heterogeneous diffusion in 3D domains with small targets, revealing complex dependence on diffusion heterogeneity and interpretation (Itô vs Stratonovich).


<details>
  <summary>Details</summary>
Motivation: While mean first passage times for homogeneous diffusion are well-understood, little is known for heterogeneous diffusion where diffusivity depends on particle location. There's ambiguity in interpreting such stochastic processes (Itô vs Stratonovich controversy), and understanding search times in such systems is important for physical applications.

Method: Developed analytical methods to determine mean escape time and splitting probabilities for arbitrary heterogeneous diffusion in arbitrary three-dimensional domains with small targets (perfectly or imperfectly absorbing). The analysis considers different interpretations of the stochastic process (Itô, Stratonovich, kinetic).

Result: Revealed general principles for how search depends on heterogeneous diffusion and its interpretation. Found intricate behavior where increasing diffusivity can have opposite effects: decrease, not affect, or even increase escape time depending on the system. Results provide framework to determine appropriate interpretation for specific physical systems.

Conclusion: The study provides analytical solutions for heterogeneous diffusion problems, resolving ambiguity in stochastic process interpretation and revealing counterintuitive effects of diffusivity changes on search times. The results have practical applications for determining appropriate mathematical models for specific physical systems.

Abstract: Many physical processes depend on the time it takes a diffusing particle to find a target. Though this classical quantity is now well-understood in various scenarios, little is known if the diffusivity depends on the location of the particle. For such heterogeneous diffusion, an ambiguity arises in interpreting the stochastic process, which reflects the well-known Itô versus Stratonovich controversy. Here we analytically determine the mean escape time and splitting probabilities for an arbitrary heterogeneous diffusion in an arbitrary three-dimensional domain with small targets that can be perfectly or imperfectly absorbing. Our analysis reveals general principles for how search depends on heterogeneous diffusion and its interpretation (e.g. Itô, Stratonovich, or kinetic). An intricate picture emerges in which, for instance, increasing the diffusivity can decrease, not affect, or even increase the escape time. Our results could be used to determine the appropriate interpretation for specific physical systems.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [112] [Momentum-resolved spectral functions of super-moiré systems using tensor networks](https://arxiv.org/abs/2512.18397)
*Anouar Moustaj,Yitao Sun,Tiago V. C. Antao,Jose L. Lado*

Main category: cond-mat.str-el

TL;DR: Tensor network method computes momentum-resolved spectral functions for large non-periodic super-moiré systems, enabling atomistic modeling of twisted van der Waals heterostructures.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with computing spectral functions in large, non-periodic super-moiré systems due to their exceptional size, limiting direct modeling of experimental observations in twisted van der Waals heterostructures.

Method: Encode exponentially large tight-binding problems as auxiliary quantum many-body problems, solved using many-body kernel polynomial tensor network algorithm combined with quantum Fourier transform tensor network.

Result: Successfully demonstrated for 1D/2D super-moiré systems including non-uniform strain, mean-field interactions, and quasicrystalline patterns; enables position-dependent electronic structure imaging and minigap analysis.

Conclusion: Establishes powerful methodology for computing momentum-resolved spectral functions in exceptionally large super-moiré systems, providing direct tool for modeling scanning twisting microscope tunneling experiments.

Abstract: Computing spectral functions in large, non-periodic super-moiré systems remains an open problem due to the exceptionally large system size that must be considered. Here, we establish a tensor network methodology that allows computing momentum-resolved spectral functions of non-interacting and interacting super-moiré systems at an atomistic level. Our methodology relies on encoding an exponentially large tight-binding problem as an auxiliary quantum many-body problem, solved with a many-body kernel polynomial tensor network algorithm combined with a quantum Fourier transform tensor network. We demonstrate the method for one and two-dimensional super-moiré systems, including super-moiré with non-uniform strain, interactions treated at the mean-field level, and quasicrystalline super-moiré patterns. Furthermore, we demonstrate that our methodology allows us to compute momentum-resolved spectral functions restricted to selected regions of a super-moiré, enabling direct imaging of position-dependent electronic structure and minigaps in super-moiré systems with non-uniform strain. Our results establish a powerful methodology to compute momentum-resolved spectral functions in exceptionally large super-moiré systems, providing a tool to directly model scanning twisting microscope tunneling experiments in twisted van der Waals heterostructures.

</details>


### [113] [2D coherent spectroscopy signatures of exciton condensation in Ta$_2$NiSe$_5$](https://arxiv.org/abs/2512.19689)
*Jiyu Chen,Jernej Mravlje,Denis Golež,Philipp Werner*

Main category: cond-mat.str-el

TL;DR: 2D coherent spectroscopy can distinguish between excitonic and lattice-driven orders in materials like Ta2NiSe5, with excitonic regimes showing strong 2DCS enhancement from condensate modes while phonon-dominated regimes show weak signals.


<details>
  <summary>Details</summary>
Motivation: To develop a method that can discriminate between excitonic and lattice-driven orders in quantum materials, which is challenging with conventional linear optical spectroscopy due to overlapping contributions from single-particle and collective modes.

Method: Used time-dependent Hartree-Fock approach to analyze third-order 2D coherent spectroscopy (2DCS) signals in a realistic model of Ta2NiSe5, examining both excitonic and electron-phonon coupling regimes.

Result: In excitonic regime, 2DCS signals are strongly enhanced by condensate's amplitude and phase modes with negligible single-particle contributions. In phonon-dominated regime, amplitude mode contribution drops rapidly. 2DCS can detect massive relative phase mode (Leggett analog).

Conclusion: 2DCS is a powerful tool for tracking symmetry-broken states and distinguishing between Coulomb-driven (excitonic) and phonon-driven orders, overcoming limitations of linear optical spectroscopy.

Abstract: We show that the nonlinear optical response probed by two-dimensional coherent spectroscopy (2DCS) can discriminate between excitonic and lattice driven order. In the excitonic regime of a realistic model of Ta$_2$NiSe$_5$, the third order 2DCS signals are strongly enhanced by the condensate's amplitude and phase modes, with negligible contributions from single-particle excitations. In the linear optical response, in contrast, single-particle and collective-mode contributions overlap. With increasing electron-phonon coupling, the amplitude mode contribution to 2DCS initially remains robust, but then drops rapidly and remains small in the phonon-dominated regime -- even in systems with large order parameter. 2DCS also aids the detection of the massive relative phase mode, which is analogous to the Leggett mode in superconductors. Our analysis, based on the time-dependent Hartree-Fock approach, demonstrates that 2DCS can track the emergence of the symmetry-broken state and the crossover from Coulomb-driven to phonon-driven order.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [114] [Energetically-dominant Sunward-Propagating Alfvén Waves Near 1 au and Their Relation to Large-scale Magnetic Switchbacks](https://arxiv.org/abs/2512.18806)
*Nickolas Giardetti,Sofiane Bourouaine,Jean C. Perez*

Main category: physics.space-ph

TL;DR: Analysis of 20+ years of Wind spacecraft data shows sunward-propagating Alfvén waves (SAWs) occur 1-14% of time, with 17.5% of SAW intervals associated with magnetic field switchbacks, suggesting switchbacks may be a source of SAWs.


<details>
  <summary>Details</summary>
Motivation: To investigate the population and characteristics of sunward-propagating Alfvén waves (SAWs) in the solar wind near 1 au, and to explore their potential relationship with large-scale magnetic field switchbacks.

Method: Used over 20 years of Wind spacecraft data near 1 au. Identified SAWs using normalized cross helicity, plasma incompressibility, and magnetic incompressibility parameters. Incorporated heliospheric magnetic field polarity to determine propagation direction. Employed pitch angle distributions of suprathermal electron strahl to identify inverted magnetic field topology for switchback detection.

Result: SAW occurrence rates vary from 1% to 14% depending on time scale and solar wind stream type. For 1-hour time scales, 17.5% of 1636 SAW intervals are associated with magnetic field switchbacks occurring at scales larger than one hour.

Conclusion: The analysis supports the idea that magnetic field switchbacks are one of the candidate sources for a portion of the sunward-propagating Alfvén wave population in the solar wind.

Abstract: In this letter, we investigate the population of energetically-dominant sunward-propagating Alfvén waves (SAWs) using more than 20 years of data provided by the Wind spacecraft near 1 au. We refer to SAWs as energetically-dominant sunward-propagating Alfvén waves within inertial range scales. Key parameters such as normalized cross helicity, plasma incompressibility, and magnetic incompressibility are used to determine the SAWs. Incorporating the polarity of the heliospheric magnetic field, AW modes are identified, which enables the determination of the propagation direction. Occurrence rates of SAWs vary from 1% to 14% depending on the time scale and solar wind stream type considered. Particularly, the relationship between large-scale magnetic field switchbacks (SBs) and SAWs (for a 1-hour long time scale) is investigated. A methodology utilizing pitch angle distributions of suprathermal electron strahl is employed to identify inverted magnetic field topology. The intervals containing SAWs are cross-referenced and examined with intervals identified as SBs. For a sample of 1636 1-hour SAW intervals, 17.5% are associated with magnetic field switchbacks occurring at scales larger than one hour. The analysis lends support to the idea of switchbacks as one of the candidate sources for a portion of the SAW population.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [115] [Achieving angular-momentum conservation with physics-informed neural networks in computational relativistic spin hydrodynamics](https://arxiv.org/abs/2512.17971)
*Hidefumi Matsuda,Koichi Hattori,Koichi Murase*

Main category: physics.flu-dyn

TL;DR: PINNs used as numerical solver for relativistic spin hydrodynamics, demonstrating accurate conservation of total angular momentum and enabling study of spin-orbit angular momentum conversion.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical framework for studying relativistic spin hydrodynamics that can accurately conserve total angular momentum and enable controlled investigation of spin-orbit angular momentum conversion, which is a central feature of relativistic spin hydrodynamics driven by rotational viscous effects.

Method: Physics-informed neural networks (PINNs) are used as a numerical solver, where the conservation law of total angular momentum is directly imposed in the loss function as a training target. The method is applied to two physical scenarios with rotating fluid in a cylindrical container.

Result: The PINNs-based framework accurately conserves total angular momentum throughout fluid evolution and provides the first numerical evidence for spin-orbit angular momentum conversion with fully nonlinear computational relativistic spin hydrodynamics. Two conversion processes are demonstrated: orbital-to-spin conversion (Barnett effect analogy) and spin-to-orbital conversion (Einstein-de Haas effect analogy).

Conclusion: PINNs provide an effective numerical framework for relativistic spin hydrodynamics that ensures accurate conservation of total angular momentum and enables controlled studies of spin-orbit conversion processes, offering new capabilities for investigating rotational viscous effects in relativistic fluid dynamics.

Abstract: We propose physics-informed neural networks (PINNs) as a numerical solver for relativistic spin hydrodynamics and demonstrate that the total angular momentum, i.e., the sum of orbital and spin angular momentum, is accurately conserved throughout the fluid evolution by imposing the conservation law directly in the loss function as a training target. This enables controlled numerical studies of the mutual conversion between spin and orbital angular momentum, a central feature of relativistic spin hydrodynamics driven by the rotational viscous effect. We present two physical scenarios with a rotating fluid confined in a cylindrical container: one case in which initial orbital angular momentum is converted into spin angular momentum in analogy with the Barnett effect, and the opposite case in which initial spin angular momentum is converted into orbital angular momentum in analogy with the Einstein-de Haas effect. We investigate these conversion processes governed by the rotational viscous effect by analyzing the spacetime profiles of thermal vorticity and spin potential. Our PINNs-based framework provides the first numerical evidence for spin-orbit angular momentum conversion with fully nonlinear computational relativistic spin hydrodynamics.

</details>


### [116] [Energy dissipation mechanisms in an acoustically-driven slit](https://arxiv.org/abs/2512.19507)
*Haocheng Yu,Tianyi Chu,Spencer H. Bryngelson*

Main category: physics.flu-dyn

TL;DR: Acoustic energy conversion to vortical motion and viscous dissipation in slit geometries is quantified, revealing optimal damping occurs when acoustic displacement matches slit thickness (KC~1), with vortex shedding dominating absorption and viscous losses accounting for 20-60% of kinetic energy.


<details>
  <summary>Details</summary>
Motivation: To understand how incident acoustic energy converts to vortical motion and viscous dissipation in slit geometries, and to identify parameter regimes that enhance or suppress acoustic damping for acoustic-based design applications.

Method: Direct numerical simulations across parameter space (ISPL, St, Re), spectral proper orthogonal decomposition (SPOD) for energy-ranked coherent structures, mode-by-mode analysis of spectral kinetic energy and viscous loss components.

Result: Maximum acoustic-hydrodynamic conversion at ISPL=150dB when acoustic displacement equals slit thickness (KC~1), with periodic boundary layer separation and vortex shedding dominating damping. VL contributes 20-60% of KE. Higher frequencies produce X-shaped modes reducing energy input by ~50%. Re effects depend on amplitude: at 150dB, larger Re suppresses broadband fluctuations; at 120dB, absorption scales monotonically with viscosity. Over 99% of VL confined near slit mouth.

Conclusion: KE-VL spectra provide physically interpretable basis for acoustic design, identifying parameter regimes that enhance or suppress acoustic damping in slit geometries through vortex shedding and viscous dissipation mechanisms.

Abstract: We quantify how incident acoustic energy is converted into vortical motion and viscous dissipation for a two-dimensional plane-wave passing through a slit geometry. We perform direct numerical simulations over a broad parameter space in incident sound pressure level (ISPL), Strouhal number (St), and Reynolds number (Re). Spectral proper orthogonal decomposition (SPOD) yields energy-ranked coherent structures at each frequency, from which we construct mode-by-mode fields for spectral kinetic energy (KE) and viscous loss (VL) components to examine the mechanisms of acoustic absorption. At ISPL=150dB, the acoustic-hydrodynamic energy conversion is highest when the acoustic displacement amplitude is comparable to the slit thickness, corresponding to a Keulegan-Carpenter number of order unity. In this regime, the oscillatory boundary layer undergoes periodic separation, resulting in vortex shedding that dominates acoustic damping. VL accounts for 20-60% of the KE contribution. For higher acoustic frequencies, the confinement of the Stokes layer produces X-shaped near-slit modes, reducing the total energy input by approximately 50%. The influence of Re depends on amplitude. At ISPL=150dB, larger Re values correspond to suppressed broadband fluctuations and sharpened harmonic peaks. At ISPL = 120dB, the boundary layers remain attached, vortex shedding is weak, absorption monotonically scales with viscosity, and the Re- and St-dependencies become comparable. Across all conditions, more than 99% of the VL is confined to a compact region surrounding the slit mouth. The KE-VL spectra describe parameter regimes that enhance or suppress acoustic damping in slit geometries, providing a physically interpretable basis for acoustic-based design.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [117] [Size-Consistent Quantum Chemistry on Quantum Computers](https://arxiv.org/abs/2512.18395)
*Noah Garrett,Michael Rose,David A. Mazziotti*

Main category: quant-ph

TL;DR: Quantum hardware preserves size consistency for up to 118 H₂ molecules, supporting scalable quantum chemistry simulations despite noise.


<details>
  <summary>Details</summary>
Motivation: Size consistency is crucial for scalable quantum chemistry, but quantum device noise may degrade this property by coupling independent subsystems.

Method: Systematically evaluate size consistency on quantum hardware by simulating increasing numbers of non-interacting H₂ molecules using optimally shallow unitary circuits with one- and two-qubit designs.

Result: Molecular energies remain size-consistent within chemical accuracy for estimated 118 subsystems (one-qubit) and 71 subsystems (two-qubit), demonstrating noise-resilient preservation of size consistency.

Conclusion: Current quantum devices preserve size consistency over chemically relevant system sizes, supporting feasibility of scalable, noise-resilient simulation of strongly correlated molecules and materials.

Abstract: Hybrid quantum-classical algorithms have begun to leverage quantum devices to efficiently represent many-electron wavefunctions, enabling early demonstrations of molecular simulations on real hardware. A key prerequisite for scalable quantum chemistry, however, is size consistency: the energy of non-interacting subsystems must scale linearly with system size. While many algorithms are theoretically size-consistent, noise on quantum devices may couple nominally independent subsystems and degrade this fundamental property. Here, we systematically evaluate size consistency on quantum hardware by simulating systems composed of increasing numbers of non-interacting H$_{2}$ molecules using optimally shallow unitary circuits. We find that molecular energies remain size-consistent within chemical accuracy for an estimated 118 and 71 H$_{2}$ subsystems for one- and two-qubit unitary designs, respectively, demonstrating that current quantum devices preserve size consistency over chemically relevant system sizes and supporting the feasibility of scalable, noise-resilient simulation of strongly correlated molecules and materials.

</details>


### [118] [El Agente Cuántico: Automating quantum simulations](https://arxiv.org/abs/2512.18847)
*Ignacio Gustin,Luis Mantilla Calderón,Juan B. Pérez-Sánchez,Jérôme F. Gonthier,Yuma Nakamura,Karthik Panicker,Manav Ramprasad,Zijian Zhang,Yunheng Zou,Varinia Bernales,Alán Aspuru-Guzik*

Main category: quant-ph

TL;DR: Quantum simulation faces computational and software complexity barriers. El Agente Cuántico is an AI multi-agent system that automates quantum simulation workflows by translating natural language into executable computations across quantum software frameworks.


<details>
  <summary>Details</summary>
Motivation: Quantum simulation is crucial for understanding quantum systems but faces barriers: exponential Hilbert space growth and complex software tools create accessibility challenges for researchers.

Method: Developed a multi-agent AI system that reasons over library documentation and APIs to dynamically assemble end-to-end quantum simulations. It translates natural-language scientific intent into executed computations across heterogeneous quantum-software frameworks.

Result: The system automates workflows spanning state preparation, closed/open-system dynamics, tensor-network methods, quantum control, error correction, and resource estimation. It unifies traditionally distinct simulation paradigms behind a single natural-language interface.

Conclusion: This approach reduces technical barriers and enables scalable, adaptive, autonomous quantum simulation for faster exploration of physical models, rapid hypothesis testing, and closer integration between theory, simulation, and quantum hardware.

Abstract: Quantum simulation is central to understanding and designing quantum systems across physics and chemistry. Yet it has barriers to access from both computational complexity and computational perspectives, due to the exponential growth of Hilbert space and the complexity of modern software tools. Here we introduce{\cinzel El Agente Cuántico}, a multi-agent AI system that automates quantum-simulation workflows by translating natural-language scientific intent into executed and validated computations across heterogeneous quantum-software frameworks. By reasoning directly over library documentation and APIs, our agentic system dynamically assembles end-to-end simulations spanning state preparation, closed- and open-system dynamics, tensor-network methods, quantum control, quantum error correction, and quantum resource estimation. The developed system unifies traditionally distinct simulation paradigms behind a single natural-language interface. Beyond reducing technical barriers, this approach opens a path toward scalable, adaptive, and increasingly autonomous quantum simulation, enabling faster exploration of physical models, rapid hypothesis testing, and closer integration between theory, simulation, and emerging quantum hardware.

</details>


### [119] [Correcting quantum errors one gradient step at a time](https://arxiv.org/abs/2512.18061)
*Manav Seksaria,Anil Prabhakar*

Main category: quant-ph

TL;DR: Gradient-based optimization of quantum error correction codewords for specific noise channels using Wirtinger gradients with soft orthonormalization penalties.


<details>
  <summary>Details</summary>
Motivation: To develop a general method for optimizing quantum error correction codewords tailored to specific noise channels, improving fidelity beyond standard code designs.

Method: Differentiate fidelity and use finite-difference Wirtinger gradients with soft penalties to promote orthonormalization, optimizing complex coefficients of codewords for given noise channels with fixed recovery operations.

Result: Validated on XXX/ZZZ repetition codes and [[5,1,3]] code; achieved substantial fidelity gains under isotropic Pauli noise with Petz recovery (0.783→0.915 for noise strength 0.05 in 100 steps).

Conclusion: The method is deterministic, highly parallelizable, scalable, and effectively optimizes quantum error correction codewords for specific noise environments.

Abstract: In this work, we introduce a general, gradient-based method that optimises codewords for a given noise channel and fixed recovery. We do so by differentiating fidelity and descending on the complex coefficients using finite-difference Wirtinger gradients with soft penalties to promote orthonormalisation. We validate the gradients on symmetry checks (XXX/ZZZ repetition codes) and the $[[5, 1, 3]]$ code, then demonstrate substantial gains under isotropic Pauli noise with Petz recovery: fidelity improves from 0.783 to 0.915 in 100 steps for an isotropic Pauli noise of strength 0.05. The procedure is deterministic, highly parallelisable, and highly scalable.

</details>


### [120] [Structure-Preserving Optimal Control of Open Quantum Systems via a Discrete Contact PMP](https://arxiv.org/abs/2512.18879)
*Leonardo Colombo*

Main category: quant-ph

TL;DR: Developed discrete Pontryagin Maximum Principle for quantum control with Lindblad dynamics and contact Lie-group variational integrator preserving CPTP structure and contact geometry.


<details>
  <summary>Details</summary>
Motivation: Need geometric numerical methods for optimal control of dissipative quantum systems that preserve physical structure (CPTP) and geometric properties, avoiding drift and instability in conventional discretizations.

Method: Developed discrete PMP for Lindblad dynamics, created contact Lie-group variational integrator using type-II discrete contact generating function to preserve CPTP structure and contact geometry exactly.

Result: Contact LGVI maintains exact CPTP structure and discrete contact geometry, enabling stable PMP shooting iterations even under strong dissipation/long horizons, unlike RK2 which suffers geometric drift.

Conclusion: Geometric-preserving integrators (contact LGVI) are essential for stable, physically consistent optimal control of dissipative quantum systems, outperforming conventional methods that lose structure.

Abstract: We develop a discrete Pontryagin Maximum Principle (PMP) for controlled open quantum systems governed by Lindblad dynamics, and introduce a second--order \emph{contact Lie--group variational integrator} (contact LGVI) that preserves both the CPTP (completely positive and trace--preserving) structure of the Lindblad flow and the contact geometry underlying the discrete PMP. A type--II discrete contact generating function produces a strict discrete contactomorphism under which the state, costate, and cost propagate in exact agreement with the variational structure of the discrete contact PMP.
  We apply this framework to the optimal control of a dissipative qubit and compare it with a non--geometric explicit RK2 discretization of the Lindblad equation. Although both schemes have the same formal order, the RK2 method accumulates geometric drift (loss of trace, positivity violations, and breakdown of the discrete contact form) that destabilizes PMP shooting iterations, especially under strong dissipation or long horizons. In contrast, the contact LGVI maintains exact CPTP structure and discrete contact geometry step by step, yielding stable, physically consistent, and geometrically faithful optimal control trajectories.

</details>


### [121] [Partition Function Estimation Using Analog Quantum Processors](https://arxiv.org/abs/2512.19685)
*Thinh Le,Elijah Pelofske*

Main category: quant-ph

TL;DR: Using D-Wave quantum annealers to approximate Ising model partition functions via quantum annealing sampling methods, achieving results comparable to classical Monte Carlo methods with very fast sampling times.


<details>
  <summary>Details</summary>
Motivation: To explore whether programmable superconducting flux qubit quantum annealers (specifically D-Wave processors) can effectively approximate partition functions of Ising models, potentially offering advantages over classical Monte Carlo methods in terms of speed and efficiency.

Method: Two quantum annealer sampling approaches: 1) chains of Monte Carlo-like reverse quantum anneals, and 2) standard linear-ramp quantum annealing. Control parameters include effective analog energy scale of J coupling, total annealing time, and anneal-pause for reverse annealing. Core technique samples across energy spectrum to obtain density of states estimates, enabling partition function computation with sampling error.

Result: On a 25-spin ±J hardware graph native Ising model, found parameter regimes providing comparable quality to classical Monte Carlo methods (Multiple Histogram Reweighting and Wang-Landau). Fast quench-like anneals quickly generate ensemble distributions that are very good estimates of true partition function. On Pegasus graph-structured QPU: logarithmic relative error of 7.6×10⁻⁶ from 171,000 samples using 0.2 seconds QPU time with 8 nanoseconds anneal time per sample.

Conclusion: Quantum annealers can effectively approximate Ising model partition functions with performance comparable to classical Monte Carlo methods, and remarkably fast quench-like anneals can produce high-quality estimates within superconducting qubit closed system dynamics timescales, suggesting potential advantages for thermodynamic quantity computation.

Abstract: We evaluate using programmable superconducting flux qubit D-Wave quantum annealers to approximate the partition function of Ising models. We propose the use of two distinct quantum annealer sampling methods: chains of Monte Carlo-like reverse quantum anneals, and standard linear-ramp quantum annealing. The control parameters used to attenuate the quality of the simulations are the effective analog energy scale of the J coupling, the total annealing time, and for the case of reverse annealing the anneal-pause. The core estimation technique is to sample across the energy spectrum of the classical Hamiltonian of interest, and therefore obtain a density of states estimate for each energy level, which in turn can be used to compute an estimate of the partition function with some sampling error. This estimation technique is powerful because once the distribution is sampled it allows thermodynamic quantity computation at arbitrary temperatures. On a $25$ spin $\pm J$ hardware graph native Ising model we find parameter regimes of the D-Wave processors that provide comparable result quality to two standard classical Monte Carlo methods, Multiple Histogram Reweighting and Wang-Landau. Remarkably, we find that fast quench-like anneals can quickly generate ensemble distributions that are very good estimates of the true partition function of the classical Ising model; on a Pegasus graph-structured QPU we report a logarithmic relative error of $7.6 \times 10^{-6}$, from $171,000$ samples generated using $0.2$ seconds of QPU time with an anneal time of $8$ nanoseconds per sample which is interestingly within the closed system dynamics timescale of the superconducting qubits.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [122] [Symmetry breaking transforms strong to normal correlation and false metals to true insulators](https://arxiv.org/abs/2512.18236)
*Alex Zunger,Jia-Xin Xiong,John P. Perdew*

Main category: cond-mat.mtrl-sci

TL;DR: DFT's false metal predictions for transition-metal oxides can be corrected by allowing symmetry breaking (structural, magnetic, dipolar) that lowers energy, converting false metals to real insulators without needing strong correlation treatment.


<details>
  <summary>Details</summary>
Motivation: To resolve the long-standing division between material scientists and condensed matter physicists about why DFT often incorrectly predicts open-shell transition-metal oxides as metals when they are actually insulators, and to address the over-reliance on strong correlation as the solution.

Method: Allow DFT calculations to lower energy by breaking structural, magnetic, or dipolar symmetries, using observed local symmetry-breaking motifs as input. Total energy calculations identify systems that support energy-lowering symmetry breaking versus those that don't.

Result: When symmetry-breaking motifs are used as input, false metals are converted into real insulators without strong correlation treatment. DFT calculations with symmetry breaking correct most cases where advanced exchange-correlation functionals previously failed to distinguish metals from insulators.

Conclusion: Symmetry breaking transforms strong correlation to normal correlation and false metals to true insulators by removing degeneracies, providing a unified perspective that reconciles the historic Mott vs. Slater controversy.

Abstract: Material scientists and condensed matter physicists have long been divided on the issue of choosing the conceptual framework for explaining why open-shell transition-metal oxides tend to be insulators, whereas otherwise successful theories such as DFT often predict them to be (false) metals. Strong correlation becomes the recommended medicine. We point out that strong correlation can be mitigated by allowing DFT to lower the energy by breaking structural, magnetic or dipolar symmetries. Such local motifs are observed experimentally by local probes beyond the 'average structure' determined by X-Ray diffraction. Observed broken symmetries can arise from slow fluctuations that persist over the observation time or longer. The surprising fact is that when symmetry breaking motifs are used as input to electronic structure calculations, false metals are converted into real insulators without the recommended medicine of strong correlation. Consistently, DFT calculations that show energy lowering symmetry breaking correct most cases where DFT, even with advanced exchange-correlation functionals, previously missed the correct metal vs insulator designation. Total energy calculations distinguish systems that support energy-lowering symmetry breaking from those that do not. This approach distinguishes between paramagnetic insulating and metallic phases and shows mass enhancement in Mott metals. The reason is that symmetry breaking removes many of the degeneracies that exist in a symmetry-unbroken system, reducing significantly the need for strong correlation. If one chooses to ignore symmetry breaking, the persistent degeneracies often call for strong correlation treatment. Thus, symmetry breaking transforms strong to normal correlation and false metals to true insulators. This view sheds light on the historic controversy between Mott and Slater that still reverberates today.

</details>


### [123] [CrystalFormer-CSP: Thinking Fast and Slow for Crystal Structure Prediction](https://arxiv.org/abs/2512.18251)
*Zhendong Cao,Shigang Ou,Lei Wang*

Main category: cond-mat.mtrl-sci

TL;DR: CrystalFormer-CSP is a unified framework combining data-driven and physics-based approaches for efficient crystal structure prediction using pretrained generative models and ML force fields.


<details>
  <summary>Details</summary>
Motivation: Crystal structure prediction is a fundamental challenge in materials science that requires accurate and efficient methods to determine stable atomic arrangements for given chemical compositions.

Method: The framework unifies data-driven heuristic and physics-driven optimization approaches, using pretrained generative models for space-group-informed structure generation and universal machine learning force fields for energy minimization, with optional reinforcement fine-tuning.

Result: The approach demonstrates effectiveness on benchmark problems and is made accessible through web interface and language model integration.

Conclusion: CrystalFormer-CSP provides an efficient, unified solution for crystal structure prediction that bridges data-driven and physics-based approaches, with practical accessibility through modern interfaces.

Abstract: Crystal structure prediction is a fundamental problem in materials science. We present CrystalFormer-CSP, an efficient framework that unifies data-driven heuristic and physics-driven optimization approaches to predict stable crystal structures for given chemical compositions. The approach combines pretrained generative models for space-group-informed structure generation and a universal machine learning force field for energy minimization. Reinforcement fine-tuning can be employed to further boost the accuracy of the framework. We demonstrate the effectiveness of CrystalFormer-CSP on benchmark problems and showcase its usage via web interface and language model integration.

</details>


### [124] [Multi-Functional Properties of Manganese Pnictides: A First-Principles Study on Magneto-Optics and Magnetocaloric Properties](https://arxiv.org/abs/2512.18253)
*Jayendran S,Abhishek K G,Suresh R,Helmer Fjellvåg,Ravindran P*

Main category: cond-mat.mtrl-sci

TL;DR: Systematic computational study of MnX (X = N, P, As, Sb, Bi) pnictides reveals their magnetocaloric and magneto-optical properties, with MnBi showing strongest Kerr effect, providing framework for magnetic refrigeration material design.


<details>
  <summary>Details</summary>
Motivation: Magnetic refrigeration offers energy-efficient, environmentally friendly cooling alternative to vapor-compression systems, requiring optimal magnetocaloric materials with significant magnetic entropy changes under moderate fields.

Method: Combined density functional theory and Monte-Carlo simulations to study exchange interactions, magnetic anisotropy, and magnetocaloric properties; all-electron, fully relativistic, full-potential linearized muffin-tin orbital method for magneto-optical Kerr and Faraday spectra.

Result: MnBi shows largest Kerr effect due to maximal Mn 3d exchange splitting and Bi's large spin-orbit coupling; magnetic transition temperatures from simulations match experimental values; site-projected moments agree with XMCD and experimental data.

Conclusion: Provides unified microscopic understanding of magnetocaloric performance and magneto-optical activity in Mn-based pnictides, establishing reliable computational framework for designing next-generation magnetic refrigeration materials.

Abstract: Magnetic refrigeration presents an energy-efficient and environmentally benign alternative to traditional vapour-compression cooling technologies. It relies on the magnetocaloric effect, in which the temperature of a magnetic material changes in response to variations in an applied magnetic field. Optimal magnetocaloric materials are characterized by a significant change in magnetic entropy under moderate magnetic field. In this study, we systematically investigated the inter-atomic exchange interactions, magnetic anisotropy energy and magnetocaloric properties of MnX (X = N, P, As, Sb, Bi) using a combination of density functional theory and Monte-Carlo simulations. Additionally, the magneto-optical Kerr and Faraday spectra were computed using the all-electron, fully relativistic, full-potential linearized muffin-tin orbital method. The largest Kerr effect observed in MnBi can be inferred as a combined effect of maximal exchange splitting of Mn 3d states and the large spin-orbit coupling of Bi. To extract site-projected spin and orbital moments, spin-orbit coupling and orbital polarization correction are accounted in the present calculation, which shows good agreement between the moment obtained from the X-ray magnetic circular dichroism sum rule analysis, spin-polarized calculation, and experimental studies. The magnetic transition temperatures predicted through Monte-Carlo simulations were in good agreement with the corresponding experimental values. Our results provide a unified microscopic understanding of magnetocaloric performance and magneto-optical activity in Mn-based pnictides and establish a reliable computational framework for designing next-generation magnetic refrigeration materials.

</details>


### [125] [An interface crack in 1d piezoelectric quasicrystal under antiplane mechanical loading and electric field](https://arxiv.org/abs/2512.17981)
*Mohammed Altoumaimi,V. V. Loboda*

Main category: cond-mat.mtrl-sci

TL;DR: Analysis of a mode III interface crack in 1D piezoelectric quasicrystal under combined electromechanical loading, solving for stress fields and electric characteristics with both impermeable and permeable crack face conditions.


<details>
  <summary>Details</summary>
Motivation: To understand fracture behavior in piezoelectric quasicrystals under complex electromechanical loading, particularly for interface cracks where both phonon and phason fields interact with electric fields, which is important for advanced material applications.

Method: Complex function approach to represent electromechanical parameters through vector-functions analytic everywhere except crack region. Formulated and solved vector Hilbert problem for impermeable crack faces, and derived quadratic equation for electric flux in permeable case.

Result: Obtained analytical solutions for phonon and phason stresses, displacement jumps, and electric characteristics along material interface. Derived formulas for stress intensity factors for each field. Conducted numerical computations for three loading variants with visualized field distributions.

Conclusion: Successfully developed analytical framework for analyzing mode III interface cracks in piezoelectric quasicrystals under combined loading, providing solutions for both crack face boundary conditions and enabling quantitative fracture analysis.

Abstract: The present study provides the consideration of a mode III interface crack in one-dimentional (1D) piezoelectric quasicrystal under antiplane phonon and phason loading and inplane electric field. Due to complex function approach all required electromechanical parameters are presented through vector-functions analytic in the whole complex plane except the crack region. The cases of electrically impermeable (insulated) and electrically limited permeable conditions on the crack faces are considered. In the first case a vector Hilbert problem in the complex plane is formulated and solved exactly and in the second one the quadratic equation with respect to the electric flux through the crack region is obtained additionally. Its solution permits to find phonon and phason stresses, displacement jumps (sliding) and also electric characteristics along the material interface. Analytical formulas are also obtained for the corresponding stress intensity factors related to each field. The numerical computations for three selected variants of the loading conditions was conducted and the resulting field distributions are visualised on the crack continuation beyond the crack and also inside of the crack region.

</details>


### [126] [Computational Design of Metal-Free Porphyrin Dyes for Sustainable Dye-Sensitized Solar Cells Informing Energy Informatics and Decision Support](https://arxiv.org/abs/2512.19529)
*Md Mahmudul Hasan,Chiara Bordin,Fairuz Islam,Tamanna Tasnim,Md. Athar Ishtiyaq,Md. Tasin Nur Rahim,Dhrubo Roy*

Main category: cond-mat.mtrl-sci

TL;DR: Computational screening of 15 porphyrin-based organic dyes for DSSCs identifies N1 dye with 14.37% PCE as optimal candidate.


<details>
  <summary>Details</summary>
Motivation: To develop novel organic dyes for dye-sensitized solar cells (DSSCs) through computational design and screening, enabling predictive materials discovery and generating performance indicators for energy informatics.

Method: Designed 15 dyes by introducing 3 acceptor/anchoring groups and 5 donor groups to porphyrin base structure. Used DFT (B3LYP/6-311G(d,p)) for ground state optimization and TD-DFT with PCM (THF solvent) for excited state properties. Calculated HOMO-LUMO levels, absorption spectra, charge transfer free energies, and photovoltaic metrics.

Result: N1 dye (triphenylamine donor, p-ethynylbenzoic acid acceptor) showed best performance: ΔG_reg = -9.73 eV, ΔG_inj = 7.18 eV, VOC = 1.47 V, JSC = 15.03 mA/cm², PCE = 14.37%. Absorption maxima ranged 690.64-975.55 nm. Most dyes had suitable HOMO/LUMO alignment for DSSC operation.

Conclusion: The N1 dye is highly promising for DSSC applications with enhanced PCE. The study demonstrates the value of computational screening for accelerating dye discovery and generating data for energy informatics and system modeling.

Abstract: This study aims to evaluate the optoelectronic properties of metal free porphyrin-based D-$π$-A dyes via in-silico performance investigation notifying energy informatics and decision support. To develop novel organic dyes, three acceptor/anchoring groups and five donating groups were introduced to strategic positions of the base porphyrin structure, resulting in a total of fifteen dyes. The singlet ground state geometries of the dyes were optimized utilizing density functional theory (DFT) with B3LYP and the excited state optical properties were explored through time-dependent DFT (TD-DFT) using the PCM model with tetrahydrofuran (THF) as solvent. Both DFT and TD-DFT calculations were carried out using the 6-311G(d,p) basis set. The HOMO energy levels of almost all the modified dyes are lower than the redox potential of I$^-$/I$3^-$ and LUMO energy levels are higher than the conduction band of TiO$2$. The absorption maxima values ranged from 690.64 to 975.55 nm. The dye N1 using triphenylamine group as donor and p-ethynylbenzoic acid group as acceptor, showed optimum optoelectronic properties ($ΔG{reg}=-9.73$ eV, $ΔG{inj}=7.18$ eV, $V_{OC}=1.47$ V and $J_{SC}=15.03$ mA/cm$^2$) with highest PCE 14.37%, making it the best studied dye. This newly modified organic dye with enhanced PCE is remarkably effective for the dye-sensitized solar cells (DSSC) industry. Beyond materials discovery, this study highlights the role of high-performance computing in enabling predictive screening of dye candidates and generating performance indicators (HOMO-LUMO gaps, absorption spectra, charge transfer free energies, photovoltaic metrics). These outputs can serve as key parameters for energy informatics and system modelling.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [127] [Source quantification by mobile gamma-ray spectrometry systems: A Bayesian approach](https://arxiv.org/abs/2512.18769)
*David Breitenmoser,Alberto Stabilini,Malgorzata Magdalena Kasprzak,Sabine Mayer*

Main category: physics.ins-det

TL;DR: A Bayesian inference framework for mobile gamma-ray spectrometry that improves source quantification accuracy by an order of magnitude through Monte Carlo templates and Bayesian inversion.


<details>
  <summary>Details</summary>
Motivation: Accurate quantification of gamma-ray sources from mobile surveys has been a long-standing, elusive inverse problem in nuclear and computational physics, requiring better solutions for practical applications.

Method: Full-spectrum Bayesian inference combining high-fidelity, platform-dynamic Monte Carlo template generation with Bayesian inversion, benchmarked against laboratory and in-situ ground truths.

Result: Demonstrates accurate and robust quantification of both natural and anthropogenic radionuclides under field conditions, improving activity estimates by an order of magnitude with principled uncertainty quantification.

Conclusion: The framework enables a more statistically rigorous and physics-informed era of mobile gamma-ray spectrometry, enhancing capabilities in emergency response, environmental monitoring, nuclear security, and planetary exploration.

Abstract: Accurately quantifying gamma-ray sources from mobile gamma-ray spectrometry surveys has remained a fundamentally elusive, long-standing inverse problem at the interface of nuclear and computational physics. Here, we present a full-spectrum Bayesian inference framework that resolves this inverse problem by combining high-fidelity, platform-dynamic Monte Carlo template generation with Bayesian inversion. Applying this methodology to airborne measurements benchmarked against laboratory and in-situ ground truths, we demonstrate accurate and robust quantification of both natural and anthropogenic radionuclides under field conditions. By improving activity estimates by an order of magnitude, providing principled uncertainty quantification, and rigorously accounting for overdispersion, this framework opens the way to a more statistically rigorous and physics-informed era of mobile gamma-ray spectrometry, unlocking enhanced inference capabilities in emergency response, environmental monitoring, nuclear security, and planetary exploration.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [128] [A Phase Space Representation of the Metaplectic Group](https://arxiv.org/abs/2512.18415)
*Maurice de Gosson*

Main category: math-ph

TL;DR: The paper constructs an extension of the metaplectic group Mp(n) that acts on functions defined on phase space rather than just configuration space.


<details>
  <summary>Details</summary>
Motivation: The symplectic group Sp(n) acts on phase space, and its double cover Mp(n) acts on configuration space functions. The authors want to extend this action to phase space functions to bridge the gap between classical symplectic geometry and quantum mechanics in phase space representation.

Method: The construction uses the authors' previous results on twisted Weyl symbols of metaplectic operators and Bopp pseudodifferential operators, which are phase space extensions of standard Weyl operators.

Result: The authors successfully construct an extension Mp(n) of the metaplectic group Mp(n) that acts on square integrable functions on phase space.

Conclusion: This work connects symplectic geometry with quantum mechanics in phase space by extending the action of metaplectic operators from configuration space to phase space functions.

Abstract: The symplectic group Sp(n) acts on phase space while the unitary representation of its double cover, Mp(n), the metaplectic group, acts on functions defined on configuration space. We will construct an extension Mp(n) of Mp(n) acting on square integrable functions on phase space. This is performed using previous results of ours involving explicit expressions of the twisted Weyl symbols of metaplectic operators and Bopp pseudodifferential operators, which are phase space extensions of the usual Weyl operators.

</details>


### [129] [Finite time energy cascade for mixed $3-$ and $4-$wave kinetic equations](https://arxiv.org/abs/2512.19531)
*Gigliola Staffilani,Minh-Binh Tran*

Main category: math-ph

TL;DR: Study of wave kinetic equation for trapped Bose gas showing energy cascades to arbitrarily large frequencies, with some cases exhibiting finite-time energy transfer to infinity.


<details>
  <summary>Details</summary>
Motivation: To understand energy transfer mechanisms in finite temperature trapped Bose gases through wave kinetic equations that combine 3-wave and 4-wave interaction processes.

Method: Analysis of a kinetic equation with collision operator comprising three distinct wave interaction mechanisms: one 3-wave process and two 4-wave processes, describing the temporal evolution of density function of thermal cloud in trapped Bose gas.

Result: For broad class of initial data, solutions exhibit immediate cascade of energy towards arbitrarily large frequencies. For other classes of initial conditions, energy is transferred to infinity in finite time.

Conclusion: The wave kinetic equation for trapped Bose gases demonstrates robust energy cascade phenomena with different temporal behaviors depending on initial conditions, revealing fundamental energy transfer mechanisms in quantum systems.

Abstract: In this work we study a kinetic equation whose collision operator comprises three distinct wave interaction mechanisms: one representing a 3-wave process, and two corresponding to 4-wave processes. This wave kinetic equation describes the temporal evolution of the density function of the thermal cloud of a finite temperature trapped Bose gas. We establish that, for a broad class of initial data, solutions exhibit an immediate cascade of energy towards arbitrarily large frequencies. Furthermore, for other classes of initial conditions, we demonstrate that the energy is transferred to infinity in finite time.

</details>


<div id='cond-mat.supr-con'></div>

# cond-mat.supr-con [[Back]](#toc)

### [130] [Influence of Magnetic Order on Proximity-Induced Superconductivity in Mn Layers on Nb(110) from First Principles](https://arxiv.org/abs/2512.19634)
*Sohair ElMeligy,Balázs Újfalussy,Kyungwha Park*

Main category: cond-mat.supr-con

TL;DR: First-principles study shows magnetic order (FM vs AFM) significantly affects proximity-induced superconductivity in Mn-Nb heterostructures, with AFM favoring stronger singlet pairing and revealing magnetic-order-dependent in-gap states.


<details>
  <summary>Details</summary>
Motivation: To understand how magnetic order influences proximity-induced superconductivity in magnetic/superconducting heterostructures, specifically examining the interplay between magnetism and superconductivity at interfaces.

Method: Used first-principles calculations with Bogoliubov-de Gennes solver within multiple scattering theory and screened Korringa-Kohn-Rostoker Green's function method to study Mn-Nb(110) heterostructures with single and double Mn layers.

Result: Found magnetic-order-dependent features: bands crossing Fermi level within SC gap, secondary gaps, plateau-like regions, and V-shaped in-gap states. AFM ordering produces ~10x stronger induced singlet SC order parameter in Mn layers compared to FM, though still only 4.44% of bulk Nb value. Negligible but comparable triplet order parameters indicate singlet-triplet mixing.

Conclusion: Magnetic order strongly modulates proximity-induced superconductivity in Mn layers, with antiferromagnetic alignment significantly enhancing singlet pairing compared to ferromagnetic ordering, revealing complex magnetic-order-dependent superconducting properties at interfaces.

Abstract: We investigate the influence of magnetic order on the proximity-induced superconducting state in the Mn layers of a Mn-Nb(110) heterostructure by using a first-principles method. For this study, we use the recently developed Bogoliubov-de Gennes (BdG) solver for superconducting heterostructures [Csire et al., Phys. Rev. B 97, 024514 (2018)] within the first-principles calculations based on multiple scattering theory and the screened Korringa-Kohn-Rostoker (SKKR) Green's function method. In our calculations, we first study the normal-state density of states (DOS) in the single- and double-Mn-layer heterostructures, and calculate the induced magnetic moments in the Nb layers. Next, we compute the momentum-resolved spectral functions in the superconducting state for the heterostructure with a single Mn layer, and find bands crossing the Fermi level within the superconducting (SC) gap. We also study the SC state DOS in the single- and double-Mn-layer heterostructures and compare some of our results with experimental findings, revealing secondary gaps, plateau-like regions, and central V-shaped in-gap states within the bulk SC Nb gap that are magnetic-order-dependent. Finally, we compute the singlet and internally antisymmetric triplet (IAT) order parameters for each layer for both heterostructures, and find an order of magnitude difference in the induced singlet part of the SC order parameter in the Mn layer/s between the FM and AFM cases in favor of the AFM pairing with the maximum still being only 4.44% of the bulk Nb singlet order parameter value. We also find a negligible induced triplet part, yet comparable to the induced singlet values, indicating some singlet-triplet mixing in the Mn layer/s.

</details>
