<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 9]
- [math.AP](#math.AP) [Total: 17]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [gr-qc](#gr-qc) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [math-ph](#math-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A dichotomy of finite element spaces and its application to an energy-conservative scheme for the regularized long wave equation](https://arxiv.org/abs/2512.20737)
*Dimitrios Antonopoulos,Dimitrios Mitsotakis*

Main category: math.NA

TL;DR: Odd-degree finite elements show optimal convergence for nonlinear dispersive wave equations, while even-degree elements have reduced accuracy due to L²-projection properties.


<details>
  <summary>Details</summary>
Motivation: The paper investigates why certain energy-conservative Galerkin discretizations for nonlinear dispersive wave equations exhibit unusual convergence behavior: optimal convergence with odd polynomial degree finite elements but reduced accuracy with even-degree elements.

Method: The study connects this behavior to the structure of finite element spaces, specifically showing it's related to the standard L²-projection of derivatives which has a super-approximation property only for odd polynomial degrees. The analysis is applied to an energy-conservative Galerkin approximation of the regularized long-wave equation with cubic energy functional.

Result: The work demonstrates that the convergence pattern is intrinsic to finite element space structure. For the regularized long-wave equation scheme, it conserves both mass and energy, approximates impulse with high accuracy, and establishes a priori error bounds for the semi-discrete formulation.

Conclusion: The unusual convergence behavior in energy-conservative Galerkin discretizations is fundamentally linked to the mathematical properties of finite element spaces, particularly the L²-projection behavior that differs between odd and even polynomial degrees, with important implications for numerical methods in nonlinear dispersive wave equations.

Abstract: Certain energy-conservative Galerkin discretizations for nonlinear dispersive wave equations have revealed an unusual convergence behavior: optimal convergence is attained when continuous Lagrange finite element spaces of odd polynomial degree are employed, whereas the use of even-degree polynomials leads to reduced accuracy. The present work demonstrates that this behavior is intrinsic to the structure of the finite element spaces themselves. In particular, it is shown to be closely connected to the standard $L^2$-projection of derivatives, which possesses a super-approximation property exclusively for odd polynomial degrees. We also examine the implications of this feature for an energy-conservative Galerkin approximation of the regularized long-wave equation where the energy is a cubic functional. Although the resulting scheme conserves both mass and energy, we further show that the impulse is approximated with high accuracy, and we establish {\em a priori} error bounds for the associated semi-discrete formulation.

</details>


### [2] [On stability of Weak Greedy Algorithm in the presence of noise](https://arxiv.org/abs/2512.20750)
*V. N. Temlyakov*

Main category: math.NA

TL;DR: Theoretical study of greedy algorithm stability, focusing on how small data perturbations affect algorithm outcomes.


<details>
  <summary>Details</summary>
Motivation: While greedy algorithms are typically studied for convergence and convergence rate, stability (resilience to small perturbations) is another crucial property that needs theoretical investigation, particularly for noisy data scenarios.

Method: Theoretical analysis of greedy algorithms with focus on stability properties, examining how algorithms behave when input data contains small perturbations or noise.

Result: Presents theoretical results demonstrating that certain greedy algorithms maintain stability, meaning small data perturbations don't cause large changes in algorithm outcomes.

Conclusion: Stability is an important property of greedy algorithms alongside convergence and convergence rate, and theoretical analysis shows these algorithms can be resilient to noisy data perturbations.

Abstract: This paper is devoted to the theoretical study of the efficiency, namely, stability of some greedy algorithms. In the greedy approximation theory researchers are mostly interested in the following two important properties of an algorithm -- convergence and rate of convergence. In this paper we present some results on one more important property of an algorithm -- stability. Stability means that small perturbations do not result in a large change in the outcome of the algorithm. In this paper we discuss one kind of perturbations -- noisy data.

</details>


### [3] [Streamfunction-vorticity formulation for incompressible viscid and inviscid flows on general surfaces](https://arxiv.org/abs/2512.20763)
*Tim Brüers,Christoph Lehrenfeld,Max Wardetzky*

Main category: math.NA

TL;DR: Streamfunction-vorticity formulation for Navier-Stokes/Euler equations on general surfaces including non-simply connected ones, ensuring exactly tangential and incompressible velocity fields with pressure robustness.


<details>
  <summary>Details</summary>
Motivation: Traditional velocity-pressure formulations struggle to guarantee tangential and incompressible velocity fields on general surfaces without increasing computational costs, especially on non-simply connected surfaces where harmonic velocity components are important.

Method: Develops a streamfunction-vorticity formulation that relies only on scalar and finite-dimensional quantities, specifically designed for general surfaces including non-simply connected ones, capturing harmonic velocity components essential for dynamics.

Result: The formulation ensures exactly tangential and incompressible velocity fields while being pressure robust. It's proven equivalent to velocity-pressure formulation under reasonable regularity assumptions, with numerical examples demonstrating applicability.

Conclusion: The streamfunction-vorticity approach provides a key advantage over traditional methods by guaranteeing structural properties (tangential, incompressible velocity) without computational cost increases, validated through theoretical equivalence proofs and numerical demonstrations.

Abstract: This paper presents a streamfunction-vorticity formulation for the Navier--Stokes and Euler equations on general surfaces. Notably, this includes non-simply connected surfaces, on which the harmonic components of the velocity field play a fundamental role in the dynamics. By relying only on scalar and finite-dimensional quantities, our formulation ensures that the resulting methods give exactly tangential and incompressible velocity fields, while also being pressure robust. Compared to traditional methods based on velocity-pressure formulations, where one can only guarantee these structural properties by increasing the computational costs, this is a key advantage. We rigorously validate our formulation by proving its equivalence to the well understood velocity-pressure formulation under reasonable regularity assumptions. Furthermore, we demonstrate the applicability of the approach with numerical examples.

</details>


### [4] [Computing nonlinear Schrödinger equations with Hermite functions beyond harmonic traps](https://arxiv.org/abs/2512.20840)
*Valeria Banica,Georg Maierhofer,Katharina Schratz*

Main category: math.NA

TL;DR: Hermite basis functions, originally used for Schrödinger equations with harmonic potential, are shown to be stable and effective for simulating Schrödinger equations without potential, making them suitable for nonlinear dispersive equations on unbounded domains.


<details>
  <summary>Details</summary>
Motivation: To extend the application of Hermite basis functions beyond their traditional use in Schrödinger equations with harmonic potential, and to establish their suitability for more general nonlinear dispersive equations on unbounded domains.

Method: The paper demonstrates that the stability properties of Hermite basis functions, which are well-established for spatial discretisation of Schrödinger equations with harmonic potential, also extend to Schrödinger equations without potential.

Result: Hermite basis functions maintain their stability properties when applied to Schrödinger equations without potential, positioning them as a natural basis choice for computational approaches to nonlinear dispersive equations on unbounded domains.

Conclusion: Hermite basis functions are a versatile and stable computational tool that can be effectively used for simulating not only Schrödinger equations with harmonic potential but also more general nonlinear dispersive equations on unbounded domains.

Abstract: Hermite basis functions are a powerful tool for spatial discretisation of Schrödinger equations with harmonic potential. In this work we show that their stability properties extend to the simulation of Schrödinger equations without potential, thus leading them as a natural basis for computation of nonlinear dispersive equations on unbounded domains.

</details>


### [5] [Parameter-free inexact block Schur complement preconditioning for linear poroelasticity under a hybrid Bernardi-Raugel and weak Galerkin finite element discretization](https://arxiv.org/abs/2512.20844)
*Weizhang Huang,Zhuoran Wang*

Main category: math.NA

TL;DR: Inexact block Schur complement preconditioning for linear poroelasticity with hybrid discretization, addressing locking issues through regularization for pure Dirichlet boundary conditions, achieving mesh- and parameter-independent convergence.


<details>
  <summary>Details</summary>
Motivation: Linear poroelasticity problems discretized with hybrid elements (Bernardi-Raugel for displacement, weak Galerkin for pressure) face numerical challenges: with pure Dirichlet boundary conditions on displacement, the leading block becomes almost singular in the nearly incompressible (locking) regime, hindering efficient iterative solution.

Method: Reformulate the system as a three-field problem with inherent regularization that maintains the original solution while ensuring nonsingularity. Apply inexact block diagonal and triangular Schur complement preconditioners with MINRES and GMRES methods. Analyze both pure Dirichlet and mixed boundary conditions scenarios.

Result: Theoretical analysis shows MINRES and GMRES with the proposed preconditioners achieve convergence independent of mesh size and locking parameter for the regularized system. Similar results hold for mixed boundary conditions even without regularization. Numerical experiments in 2D/3D confirm robustness with respect to mesh size and locking parameter in both boundary scenarios.

Conclusion: The proposed regularization effectively addresses locking issues in pure Dirichlet boundary conditions, while the inexact block Schur complement preconditioners provide robust, parameter-independent convergence for linear poroelasticity problems. The method's effectiveness is further demonstrated in a complex spinal cord simulation with discontinuous material parameters.

Abstract: This work investigates inexact block Schur complement preconditioning for linear poroelasticity problems discretized using a hybrid approach: Bernardi-Raugel elements for solid displacement and lowest-order weak Galerkin elements for fluid pressure. When pure Dirichlet boundary conditions are applied to the displacement, the leading block of the resulting algebraic system becomes almost singular in the nearly incompressible (locking) regime, hindering efficient iterative solution. To overcome this, the system is reformulated as a three-field problem with an inherent regularization that maintains the original solution while ensuring nonsingularity. Analysis shows that both the minimal residual (MINRES) and generalized minimal residual (GMRES) methods, when preconditioned with inexact block diagonal and triangular Schur complement preconditioners, achieve convergence independent of mesh size and the locking parameter for the regularized system. Similar theoretical results are established for the situation with displacement subject to mixed boundary conditions, even without regularization. Numerical experiments in 2D and 3D confirm the benefits of regularization under pure Dirichlet conditions and the robustness of the preconditioners with respect to mesh size and the locking parameter in both boundary condition scenarios. Finally, a spinal cord simulation with discontinuous material parameters further illustrates the effectiveness and robustness of the proposed iterative solvers.

</details>


### [6] [Mixed Precision General Alternating-Direction Implicit Method for Solving Large Sparse Linear Systems](https://arxiv.org/abs/2512.21164)
*Jifeng Ge,Bastien Vieublé,Juan Zhang*

Main category: math.NA

TL;DR: Mixed precision GADI method accelerates large sparse linear systems by solving subsystems in low precision while maintaining high precision residuals, achieving 1.7-3.1× speedups on GPU.


<details>
  <summary>Details</summary>
Motivation: To reduce execution time for solving large-scale sparse linear systems Ax=b while maintaining high accuracy, by leveraging mixed precision computing where appropriate parts of the algorithm can use lower precision.

Method: Three-precision GADI framework that solves subsystems in low precision (Bfloat16/FP32) while computing residuals and solution updates in high precision. Includes rounding error analysis, convergence conditions, and Gaussian Process Regression for parameter selection.

Result: Speedups of 2.6×, 1.7×, and 3.1× over full double precision GADI on large-scale 2D, 3D convection-diffusion and complex reaction-diffusion problems (up to 1.3×10⁸ unknowns) using NVIDIA A100 GPU.

Conclusion: Mixed precision GADI successfully accelerates large-scale sparse linear solvers while maintaining high accuracy, with systematic parameter selection and proven convergence guarantees under specific conditions.

Abstract: In this article, we introduce a three-precision formulation of the General Alternating-Direction Implicit method (GADI) designed to accelerate the solution of large-scale sparse linear systems $Ax=b$. GADI is a framework that can represent many existing Alternating-Direction Implicit (ADI) methods. These methods are a class of linear solvers based on a splitting of $A$ such that the solution of the original linear system can be decomposed into the successive computation of easy-to-solve structured subsystems. Our proposed mixed precision scheme for GADI solves these subsystems in low precision to reduce the overall execution time while computing the residual and solution update in high precision to enable the solution to converge to high accuracy. We develop a rounding error analysis of mixed precision GADI that establishes the rates of convergence of the forward and backward errors to certain limiting accuracies. Our analysis also highlights the conditions on the splitting matrices under which mixed precision GADI is guaranteed to converge for a given set of precisions. We then discuss a systematic and robust strategy for selecting the GADI regularization parameter $α$, whose adjustment is critical for performance. Specifically, our proposed strategy makes use of a Gaussian Process Regression (GPR) model trained on a dataset of low-dimensional problems to initialize $α$. Finally, we proceed to a performance analysis of mixed precision GADI on an NVIDIA A100 GPU to validate our approach. Using low precision (Bfloat16 or FP32) to solve the subsystems, we obtain speedups of $2.6\times$, $1.7\times$, and $3.1\times$ over a full double precision GADI implementation on large-scale 2D, 3D convection-diffusion and complex reaction-diffusion problems (up to $1.3\times 10^{8}$ unknowns), respectively.

</details>


### [7] [A mixed finite element method for the stochastic Boussinesq equations with multiplicative noise](https://arxiv.org/abs/2512.21297)
*Liet Vo*

Main category: math.NA

TL;DR: A fully discrete mixed finite element method for stochastic Boussinesq system with multiplicative noise, using spatial mixed FEM and temporal semi-implicit Euler-Maruyama scheme, with error bounds and convergence proofs.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze a fully discrete numerical method for solving stochastic Boussinesq systems driven by multiplicative noise, which are important in fluid dynamics with thermal effects and random forcing.

Method: Combines standard mixed finite element method for spatial discretization with semi-implicit Euler-Maruyama scheme for temporal discretization. Uses localization technique with high-moment stability estimates for error analysis.

Result: Established error bounds for velocity, pressure, and temperature approximations. Proved convergence in probability for the fully discrete method in both L² and H¹-type norms. Numerical experiments validate theoretical estimates.

Conclusion: The proposed fully discrete mixed finite element method is effective for stochastic Boussinesq systems with multiplicative noise, with proven convergence properties and validated by numerical experiments.

Abstract: This work investigates a fully discrete mixed finite element method for the stochastic Boussinesq system driven by multiplicative noise. The spatial discretization is performed using a standard mixed finite element method, while the temporal discretization is based on a semi-implicit Euler-Maruyama scheme. By combining a localization technique with high-moment stability estimates, we establish error bounds for the velocity, pressure, and temperature approximations. As a direct consequence, we prove convergence in probability for the fully discrete method in both $L^2$ and $H^1$-type norms. Several numerical experiments are presented to validate the theoretical error estimates and demonstrate the effectiveness of the proposed scheme.

</details>


### [8] [FORCE-$α$ Numerical Fluxes within the Arbitrary High Order Semidiscrete WENO-DeC Framework: A Competitive Alternative to Upwind Fluxes](https://arxiv.org/abs/2512.21306)
*Lorenzo Micalizzi,Eleuterio Toro*

Main category: math.NA

TL;DR: FORCE-α centered numerical fluxes perform competitively with upwind fluxes in high-order FV-WENO-DeC schemes for Euler equations up to 7th order.


<details>
  <summary>Details</summary>
Motivation: Centered fluxes like FORCE-α offer flexibility for complex hyperbolic systems where Riemann solvers are difficult/expensive to construct, unlike upwind fluxes that require Riemann problem structure.

Method: Systematic investigation using arbitrary high-order semidiscrete FV framework with WENO spatial reconstruction and DeC time discretization, applied to ideal Euler equations in 1D and 2D up to order 7.

Result: FORCE-α numerical fluxes are shown to be a competitive alternative to classical upwind fluxes (Rusanov, HLL, exact Riemann solver) within high-order frameworks.

Conclusion: FORCE-α centered fluxes provide a viable and flexible option for high-order numerical schemes for hyperbolic PDEs, especially for complex systems where Riemann solvers are impractical.

Abstract: This work systematically investigates the performance of FORCE--$α$ numerical fluxes within an arbitrary high order semidiscrete finite volume (FV) framework for hyperbolic partial differential equations (PDEs). Such numerical fluxes have been recently introduced by Toro, Saggiorato, Tokareva, and Hidalgo (Journal of Computational Physics, 416, 2020), and constitute a family of centred fluxes obtained from a suitable modification of First--Order Centred (FORCE) numerical fluxes. In contrast with upwind fluxes, such as Rusanov, Harten--Lax--van Leer (HLL) or the exact Riemann solver (RS) numerical flux, centred ones do not consider in any way the structure of the Riemann problem at cell interfaces. Adopting centred numerical fluxes leads to a high level of flexibility of the resulting numerical schemes, for example in the context of complicated hyperbolic systems, for which RSs may be impossible to construct or computationally expensive.
  The baseline framework adopted in this investigation is a FV semidiscrete approach with Weighted Essentially Non--Oscillatory (WENO) spatial reconstruction and Deferred Correction (DeC) time discretization, and results are reported up to order 7. Previous investigations involving the same framework have established that increasing the order of accuracy tends to decrease the differences in the results obtained through different numerical fluxes. The goal of this paper is to show that the employment of FORCE--$α$ numerical fluxes within such a framework is a competitive alternative to the adoption of more classical upwind fluxes. The hyperbolic system considered for this investigation is the ideal Euler equations in one and two space dimensions.

</details>


### [9] [Variationally correct operator learning: Reduced basis neural operator with a posteriori error estimation](https://arxiv.org/abs/2512.21319)
*Yuan Qiu,Wolfgang Dahmen,Peng Chen*

Main category: math.NA

TL;DR: A variationally correct operator learning framework using FOSLS objectives with provable error equivalence, implemented via Reduced Basis Neural Operators for stability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard PDE-residual losses in neural operators lack variational correctness - small residuals don't guarantee small solution errors due to non-compliant norms and inconsistent boundary condition penalties.

Method: Develops FOSLS objectives with provable equivalence to solution error in PDE-induced norms. Uses Reduced Basis Neural Operator (RBNO) to predict coefficients for pre-computed conforming reduced basis, ensuring variational stability by design. Incorporates mixed boundary conditions via variational lifts.

Result: Rigorous convergence analysis bounding total error by discretization bias, basis truncation error, neural network approximation error, and statistical estimation errors. Numerical benchmarks show superior accuracy in PDE-compliant norms compared to baselines, with residual loss serving as reliable a posteriori error estimator.

Conclusion: The framework provides a theoretically sound approach to operator learning with guaranteed error control, combining FOSLS objectives with reduced basis methods to ensure variational correctness while maintaining computational efficiency.

Abstract: Minimizing PDE-residual losses is a common strategy to promote physical consistency in neural operators. However, standard formulations often lack variational correctness, meaning that small residuals do not guarantee small solution errors due to the use of non-compliant norms or ad hoc penalty terms for boundary conditions. This work develops a variationally correct operator learning framework by constructing first-order system least-squares (FOSLS) objectives whose values are provably equivalent to the solution error in PDE-induced norms. We demonstrate this framework on stationary diffusion and linear elasticity, incorporating mixed Dirichlet-Neumann boundary conditions via variational lifts to preserve norm equivalence without inconsistent penalties. To ensure the function space conformity required by the FOSLS loss, we propose a Reduced Basis Neural Operator (RBNO). The RBNO predicts coefficients for a pre-computed, conforming reduced basis, thereby ensuring variational stability by design while enabling efficient training. We provide a rigorous convergence analysis that bounds the total error by the sum of finite element discretization bias, reduced basis truncation error, neural network approximation error, and statistical estimation errors arising from finite sampling and optimization. Numerical benchmarks validate these theoretical bounds and demonstrate that the proposed approach achieves superior accuracy in PDE-compliant norms compared to standard baselines, while the residual loss serves as a reliable, computable a posteriori error estimator.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [10] [Dispersive decay for the Inter-critical nonlinear Schrödinger equation in $\mathbb{R}^3$](https://arxiv.org/abs/2512.20683)
*Boyu Jiang,Jiawei Shen,Kexue Li*

Main category: math.AP

TL;DR: The paper establishes uniform decay estimates for long-time dynamics of 3D mass-supercritical, energy-subcritical NLS solutions in critical homogeneous Sobolev spaces.


<details>
  <summary>Details</summary>
Motivation: To extend previous results on long-time behavior of NLS solutions by obtaining uniform decay estimates for the critical regime where initial data lies in the critical homogeneous Sobolev space.

Method: Investigates the Cauchy problem for 3D nonlinear Schrödinger equation in mass-supercritical, energy-subcritical regime. Uses critical homogeneous Sobolev space framework with scaling-critical regularity s_c = 5/6.

Result: Obtains uniform decay estimate for long-time dynamics of solutions with initial data in $\dot{H}^{s_c}(\mathbb{R}^3)$, extending previous results.

Conclusion: The work successfully establishes decay estimates for NLS solutions in the critical regime, advancing understanding of long-time behavior in mass-supercritical, energy-subcritical settings.

Abstract: This paper investigates the Cauchy problem for the nonlinear Schrödinger equation (NLS) in the mass-supercritical and energy-subcritical regime within three spatial dimensions. For initial data in the critical homogeneous Sobolev space $\dot{H}^{s_c}(\mathbb{R}^3)$ (where $s_c = \frac{5}{6}$), we get a uniform decay estimate for the long-time dynamics of solutions, which extends the previous results.

</details>


### [11] [On a Hamilton-Jacobi PDE theory for hydrodynamic limit of action minimizing collective dynamics](https://arxiv.org/abs/2512.20809)
*Jin Feng*

Main category: math.AP

TL;DR: Multi-scale convergence theory for Hamilton-Jacobi PDEs in probability measure spaces, derived from hydrodynamic limits of N-particle Lagrangian dynamics, using weak K.A.M. theory and new viscosity solution techniques.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous convergence theory for Hamilton-Jacobi PDEs in probability measure spaces that arise from hydrodynamic limits of deterministic N-particle Lagrangian dynamics, addressing the infinite dimensional singular averaging structure in these equations.

Method: Develops an indirect variational approach applying finite dimensional weak K.A.M. theory to infinite dimensional setting; uses new viscosity solution techniques including projection with submetry structure, multi-scale convergence in metric spaces, and comparison principles in probability measure spaces; treats weakly interacting particle systems.

Result: Establishes rigorous convergence results for solutions of nonlinear PDEs in probability measure spaces by identifying limiting Hamiltonian through averaging steps; develops general techniques applicable to Alexandrov metric spaces with curvature bounded from below.

Conclusion: Successfully extends weak K.A.M. theory to infinite dimensional Hamilton-Jacobi equations in probability measure spaces, providing new mathematical tools for analyzing hydrodynamic limits and multi-scale convergence in measure-valued dynamics.

Abstract: We establish multi-scale convergence theory for a class of Hamilton-Jacobi PDEs in space of probability measures. They arise from context of hydrodynamic limit of N-particle deterministic action minimizing (global) Lagrangian dynamics.
  From a Lagrangian point of view, this can also be viewed as a limit result on two scale convergence of action minimizing probability-measure-valued paths. However, we focus on the Hamiltonian formulation here mostly. We derive and study convergence of the associated abstract but scalar Hamilton-Jacobi equations, defined in space of probability measures. There is an infinite dimensional singular averaging structure within these equations. We develop an indirect variational approach to apply finite dimensional weak K.A.M. theory to such infinite dimensional setting here. With a weakly interacting particle assumption, the averaging step only involves that of individual particles, which is implicitly but rigorously treated using the weak K.A.M. theory. Consequently, we can close the above mentioned averaging step by identifying limiting Hamiltonian, and arrive at a rigorous convergence result on solutions of the nonlinear PDEs in space of probability measures.
  In technical development parts of the paper, we devise new viscosity solution techniques regarding projection of equations with a submetry structure in state space, multi-scale convergence for certain abstract Hamilton-Jacobi equations in metric spaces, as well as comparison principles for equations in space of probability measures. The space of probability measure we consider is a special case of Alexandrov metric space with curvature bounded from below. Since some results are better explained in such metric space setting, we also develop some techniques in the general settings which are of independent interests.

</details>


### [12] [Infinitely many solutions and asymptotics for resonant oscillatory problems](https://arxiv.org/abs/2512.20816)
*Philip Korman,Dieter S. Schmidt*

Main category: math.AP

TL;DR: The paper proves existence of infinitely many solutions for oscillatory resonant PDEs on balls/rectangles, develops asymptotic formulas, and validates with numerical computations.


<details>
  <summary>Details</summary>
Motivation: To study oscillatory resonant problems for semilinear PDEs where the first harmonic of the right-hand side is not required to be zero or small, addressing gaps in existing theory.

Method: Analytical approach for proving existence of infinitely many solutions on balls and rectangles in R^n, derivation of asymptotic formulas in terms of first harmonic, and detailed numerical method for validation.

Result: Existence of infinitely many solutions proven, global solution set studied, asymptotic formulas derived, and numerical computations demonstrate accuracy of asymptotic approximations.

Conclusion: The paper successfully addresses oscillatory resonant problems without requiring small first harmonic, provides theoretical existence results with asymptotic analysis, and validates findings numerically.

Abstract: For a class of oscillatory resonant problems, involving Dirichlet problems for semilinear PDE's on balls and rectangles in $R^n$, we show the existence of infinitely many solutions, and study the global solution set. The first harmonic of the right hand side is not required to be zero, or small. We also derive asymptotic formulas in terms of the first harmonic of solutions, and illustrate their accuracy by numerical computations. The numerical method is explained in detail.

</details>


### [13] [Uniqueness for the Homogeneous Landau-Coulomb Equation in $L^{3/2}$](https://arxiv.org/abs/2512.20899)
*Maria Pia Gualdani,Weiran Sun*

Main category: math.AP

TL;DR: Proves uniqueness of H-solutions to homogeneous Landau-Coulomb equation in critical space L^{3/2}, completing global well-posedness theory.


<details>
  <summary>Details</summary>
Motivation: To establish uniqueness of rough solutions to nonlinear kinetic equations, specifically completing the global well-posedness theory for the homogeneous Landau-Coulomb equation in the critical space L^{3/2}(ℝ³).

Method: Uses the M-operator technique developed in previous works, applying it to the space-homogeneous case where the M-operator can be taken as a Bessel potential operator.

Result: Proves uniqueness of H-solutions satisfying specific regularity conditions (⟨v⟩^{k₀}f ∈ C([0,T]; L^{3/2}) and ⟨v⟩^{-3/2}∇_v((⟨v⟩^{k₀}f)^{3/4}) ∈ L²), showing solutions constructed in previous work are unique.

Conclusion: Completes the global well-posedness theory for the homogeneous Landau-Coulomb equation in the critical space L^{3/2}(ℝ³), demonstrating the effectiveness of the M-operator technique for establishing uniqueness of rough solutions to nonlinear kinetic equations.

Abstract: We prove the uniqueness of $H$-solutions to the homogeneous Landau-Coulomb equation satisfying $\langle v \rangle^{k_0} f \in C([0, T]; L^{3/2}(\mathbb{R}^3))$ and $\langle v \rangle^{-3/2} \nabla_v ((\langle v \rangle^{k_0} f)^{3/4}) \in L^2((0, T) \times \mathbb{R}^3)$ for any $k_0 \geq 5$. In particular, this shows that the solutions constructed in~\cite{GGL25} are unique. The present work thus completes the global well-posedness theory in the critical space $L^{3/2}(\mathbb{R}^3)$. Our proof is part of a broader effort to use the $\mathcal{M}$-operator technique developed in~\cite{AGS2025, AMSY2020} to establish the uniqueness of rough solutions to nonlinear kinetic equations. When applied to the space-homogeneous case, the $\mathbb{M}$-operator can be taken simply as a Bessel potential operator.

</details>


### [14] [Quantitative bounds for Hölder exponents in the Krylov--Safonov and Evans--Krylov theories](https://arxiv.org/abs/2512.21025)
*Jongmyeong Kim,Se-Chan Lee*

Main category: math.AP

TL;DR: Quantitative bounds for Hölder exponents in Krylov-Safonov and Evans-Krylov theories when ellipticity ratio is close to one.


<details>
  <summary>Details</summary>
Motivation: To establish precise quantitative bounds for Hölder regularity exponents in two important PDE theories when the ellipticity ratio approaches one, providing more refined regularity estimates.

Method: Uses Ishii-Lions method for Krylov-Safonov theory and a Schauder-type perturbation argument for Evans-Krylov theory.

Result: Establishes quantitative bounds for Hölder exponents, showing how regularity improves as ellipticity ratio approaches one.

Conclusion: The paper provides quantitative regularity estimates that bridge the gap between fully nonlinear elliptic equations and their linear counterparts when ellipticity is nearly constant.

Abstract: We establish quantitative bounds for Hölder exponents in the Krylov--Safonov and Evans--Krylov theories when the ellipticity ratio is close to one. Our analysis relies on the Ishii--Lions method for the Krylov--Safonov theory and a Schauder-type perturbation argument for the Evans--Krylov theory.

</details>


### [15] [Calderón-Zygmund gradient estimates for $p$-Laplace systems with BMO complex coefficients](https://arxiv.org/abs/2512.21036)
*Van-Chuong Quach,Thanh-Nhan Nguyen,Minh-Phuong Tran*

Main category: math.AP

TL;DR: The paper establishes global gradient bounds and Calderón-Zygmund type estimates for weak solutions to degenerate elliptic systems with complex-valued coefficients, requiring only small BMO coefficients rather than the stronger VMO condition.


<details>
  <summary>Details</summary>
Motivation: To extend regularity theory for elliptic systems with complex coefficients by establishing gradient bounds under weaker conditions than previously required (small BMO instead of VMO), building on recent work that established existence and uniqueness under less restrictive assumptions.

Method: The authors prove a global Calderón-Zygmund type estimate for weak solutions to divergence-form degenerate elliptic systems with complex-valued coefficients, where the leading coefficients are only required to be sufficiently small in BMO space.

Result: Establishes global gradient bounds and Calderón-Zygmund type estimates for weak solutions, from which Morrey-space regularity follows as a consequence, under the condition that complex-valued coefficients are sufficiently small in BMO.

Conclusion: This work contributes to better understanding solution behavior for elliptic systems with complex coefficients and extends regularity theory by working under weaker coefficient conditions (small BMO) than previously studied (VMO).

Abstract: This work is concerned with global gradient bounds for a class of divergence-form degenerate elliptic systems with complex-valued coefficients. Notably, the leading coefficients are merely required to be sufficiently small in BMO, which is strictly weaker than the VMO condition. In the complex setting, the well-posedness of this problem was recently investigated in [W. Kim, M. Vestberg, Existence, uniqueness and regularity for elliptic $p$-Laplace systems with complex coefficients,arXiv:2503.18932], where the authors established a strong accretivity condition on the leading coefficients, and this structural condition allows them to derive Schauder-type estimates for weak solutions. In our study, it has already been observed that gaining existence and uniqueness of weak solutions is possible under a natural and less restrictive assumption on the complex-valued coefficients. Following this direction, we prove a global Caderón-Zygmund-type estimate for weak solutions, from which the Morrey-space regularity follows as a consequence. This paper is a contribution to the better understanding of solution behavior and may be viewed as part of a series of works aimed at extending regularity theory in the complex-valued setting.

</details>


### [16] [A Unified Truncation Method for Infinitely Many Solutions Without Symmetry](https://arxiv.org/abs/2512.21119)
*Anouar Bahrouni*

Main category: math.AP

TL;DR: The paper introduces a refined variational truncation method that proves existence of infinitely many solutions for nonlinear problems without symmetry, covering semilinear elliptic PDEs, nonvariational elliptic PDEs with gradient dependence, and periodic Hamiltonian systems.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in proving multiplicity of solutions for nonlinear problems that lack symmetry, particularly for nonvariational elliptic PDEs with gradient dependence and periodic Hamiltonian systems where previous methods failed.

Method: A refined variational truncation method that systematically separates solutions, combined with an iterative scheme for nonvariational PDEs, and a careful limiting process for Hamiltonian systems to prevent solution collapse.

Result: Three major advances: 1) infinite sequences of positive/negative solutions for semilinear elliptic PDEs, 2) first proof of infinitely many solutions for nonvariational elliptic PDEs with gradient dependence, 3) multiple distinct solutions for periodic Hamiltonian systems on the whole real line without collapse.

Conclusion: The unified truncation methodology provides a robust and versatile tool for addressing multiplicity problems across variational/non-variational PDEs and infinite dimensional dynamical systems in the absence of symmetry.

Abstract: This paper establishes the existence of infinitely many solutions for nonlinear problems without any symmetry, achieving three major advances. First, in the setting of semilinear elliptic PDEs, we introduce a refined variational truncation method that yields infinite sequences of positive as well as negative solutions. Second and most notably, we resolve a long-standing and difficult problem for nonvariational elliptic PDEs with gradient dependence. By combining our truncation method with an iterative scheme, we prove, for the first time, the existence of infinitely many solutions for this class of PDEs. Third, we overcome a central difficulty for periodic Hamiltonian systems on the real line: we show that the multiplicity of solutions, constructed on a sequence of finite intervals, survives in the limit; in other words, no collapse occurs, and we obtain multiple distinct solutions on the whole real line.
  The core novelty lies in a carefully designed truncation methodology that systematically separates solutions and remains effective across variational and non-variational PDEs as well as infinite dimensional dynamical systems. This unified perspective provides a robust and versatile tool for addressing multiplicity problems in the absence of symmetry.

</details>


### [17] [Equilibrium Configurations and their Uniqueness in a Fluid-Solid Interaction Problem](https://arxiv.org/abs/2512.21130)
*D. Bonheure,G. P. Galdi,C. Patriarca*

Main category: math.AP

TL;DR: Existence of large and uniqueness of small equilibrium configurations for Navier-Stokes fluid coupled with rigid body under spring forces and restoring moments, driven by uniform velocity field at infinity.


<details>
  <summary>Details</summary>
Motivation: Study equilibrium states in fluid-structure interaction problems where a rigid body (with spring forces/restoring moments) interacts with Navier-Stokes fluid, driven by uniform velocity at large distances.

Method: Mathematical analysis of coupled Navier-Stokes equations with rigid body dynamics, addressing rotational degrees of freedom that create nonlinearity.

Result: Proves existence of equilibrium configurations in the "large" (global existence) and uniqueness in the "small" (local uniqueness) for the coupled system.

Conclusion: Successfully establishes fundamental mathematical properties of equilibrium states in this complex fluid-structure interaction problem despite nonlinear challenges from body rotation.

Abstract: We demonstrate existence in the ``large" and uniqueness in the ``small" of equilibrium configurations for the coupled system consisting of a Navier-Stokes fluid interacting with a rigid body subjected to spring forces and restoring moments. The driving mechanism is a uniform, given velocity field of the fluid at large spatial distances from the body. The main difficulty in the proof of the above properties arises from the fact that the body can rotate around a given axis, which produces a highly nonlinear problem.

</details>


### [18] [Existence and non-existence phenomena for nonlinear elliptic equations with $L^1$ data and singular reactions](https://arxiv.org/abs/2512.21131)
*Francescantonio Oliva,Francesco Petitta,Matheus F. Stapenhorst*

Main category: math.AP

TL;DR: The paper studies existence and non-existence of solutions for singular elliptic boundary value problems with p-Laplacian operator and singular term a(x)/u^γ, showing solvability for large μ > μ₀ and no finite energy solutions for small μ < μ₀*.


<details>
  <summary>Details</summary>
Motivation: Extend the celebrated work of Diaz, Morel and Oswald (1997) on singular elliptic problems from the Laplacian case (p=2) to the p-Laplacian case (p≠2), and provide new results even for p=2 when the singular term has critical growth near zero (γ=1).

Method: Study singular elliptic boundary value problems with p-Laplacian operator, singular term a(x)/u^γ, and positive forcing term μf(x). Use analytical techniques to establish threshold behavior: existence of solutions for large μ and non-existence for small μ.

Result: For any positive f ∈ L¹(Ω), problem (1) is solvable for any μ > μ₀ (with μ₀ large enough), while no finite energy solution exists if 0 < μ < μ₀* (with μ₀* small). This extends previous results to p≠2 and provides new insights for p=2 with critical singular growth.

Conclusion: The paper successfully extends the classical results on singular elliptic problems to the p-Laplacian framework, establishing threshold behavior for solution existence based on the parameter μ, with complete characterization of existence/non-existence regimes.

Abstract: We study existence and non-existence of solutions for singular elliptic boundary value problems as \begin{equation}\label{eintro}\begin{cases}\tag{1}
  \displaystyle -Δ_p u+ \frac{a(x)}{u^γ}=μf(x) \ &\text{ in }Ω, \newline
  u>0&\text{ in }Ω, \newline
  u = 0 \ &\text{ on }
  \partialΩ,
  \end{cases} \end{equation} where $Ω$ is a smooth bounded open subset of $\mathbb{R}^N$ ($N\ge 2$), $Δ_p u$ is the $p$-Laplacian with $p>1$, $0<γ\leq 1$, and $a\geq0$ is bounded and non-trivial. For any positive $ f\in L^{1}(Ω)$ we show that problem \eqref{eintro} is solvable for any $μ>μ_0>0$, for some $μ_0$ large enough. As a reciprocal outcome we also show that no finite energy solution exists if $0<μ<μ_{0*}$, for some small $μ_{0*}$.
  This paper extends the celebrated one of J. I. Diaz, J. M. Morel and L. Oswald ([16]) to the case $p\neq2$. Our result is also new for $p=2$ provided the singular term has a critical growth near zero (i.e. $γ=1$).

</details>


### [19] [Well-posedness and the Łojasiewicz-Simon inequality in the asymptotic analysis of a nonlinear heat equation with constraints of finite codimension](https://arxiv.org/abs/2512.21158)
*Ashish Bawalia,Zdzisław Brzeźniak,Manil T. Mohan,Piotr Rybka*

Main category: math.AP

TL;DR: Global well-posedness and asymptotic convergence of norm-preserving nonlinear heat equation on Poincaré domains with L² constraint.


<details>
  <summary>Details</summary>
Motivation: To establish global existence and analyze long-term behavior of strong solutions to nonlinear heat equations with L²-norm preserving constraints, which arise in constrained gradient flows and geometric PDEs.

Method: Modify nonlinearity, use m-accretive evolution equation theory for existence, apply resolvent ideas and Yosida approximation for regularity, and employ Łojasiewicz-Simon gradient inequality on Hilbert submanifolds for asymptotic analysis.

Result: Proves global existence of strong solutions for p satisfying: 2≤p<∞ (d≤4) and 2≤p≤(2d-4)/(d-4) (d≥5). Shows convergence to stationary states in W^{2,q}∩W^{1,q}_0 spaces for bounded domains with even p and d≤3.

Conclusion: Provides alternative approach for global existence and long-term behavior analysis of norm-preserving nonlinear heat equations, establishing convergence to equilibrium using geometric analytic tools.

Abstract: We establish the global well-posedness of the $D(A)-$valued strong solution to a nonlinear heat equation with constraints on a \textit{Poincaré domain} $\bO\subset \R^d$ whose boundary is of class $C^2$. Consider the following nonlinear heat equation
  \begin{align*}
  \frac{\partial u}{\partial t} - Δu + |u|^{p-2}u = 0,
  \end{align*}
  projected onto the tangent space $T_u\bM$, where
  $\mathcal{M}:=\left\{u\in L^2(\bO):\|u\|_{L^2(\bO)}=1\right\}$ is a submanifold of $L^2(\bO)$. The nonlinearity exponent satisfies $2\le p < \infty$ for $1\leq d\leq 4$ and $2 \le p \le \frac{2d-4}{d-4}$ for $d \ge 5$. The solution is constrained to lie within $\mathcal{M}$ which encodes the norm-preserving constraint. By modifying the nonlinearity and exploiting the abstract theory for \textit{$m-$accretive }evolution equations, we prove the existence of a global strong solution.
  Using {resolvent-idea } and the \textit{Yosida approximation} method, we derive regularity results. In the asymptotic analysis, $\bO$ is restricted to bounded domains with even $p$
  and $1\le d \le 3$. For any initial data in $D(A) \cap \mathcal{M}$, we apply the \textit{Łojasiewicz-Simon gradient inequality} on a Hilbert submanifold [F. Rupp, \textit{J. Funct. Anal.}, 279(8), 2020], to demonstrate that the unique global strong solution converges in $W^{2,q}(\bO) \cap W^{1,q}_0(\bO)$ to a stationary state, where $2 \le q < \frac{2d}{d + 4 - 4β}$ and $1 < β< \frac{3}{2}$.
  This work proposes an alternative method for establishing the global existence and analyzing long-term behavior of the unique strong solution to an $L^2-$norm preserving nonlinear heat equation.

</details>


### [20] [Optimal Hardy-weights for the Finsler $p$-Dirichlet integral with a potential](https://arxiv.org/abs/2512.21162)
*Yongjun Hou*

Main category: math.AP

TL;DR: The paper constructs optimal Hardy-weights for Finsler p-Dirichlet integrals with and without potentials, involving local Morrey space potentials.


<details>
  <summary>Details</summary>
Motivation: To establish optimal Hardy-type inequalities for Finsler p-Dirichlet operators on domains with singularities (punctured domains) and with potentials from local Morrey spaces, extending classical Hardy inequalities to more general geometric settings.

Method: Constructs optimal Hardy-weights for two operators: 1) Finsler p-Dirichlet integral on punctured domains Ω* = Ω\{x̂}, and 2) Same operator with potential V from local Morrey space on Ω. Uses Finsler geometry framework where H(x,·) is a family of norms parameterized by x.

Result: Successfully constructs optimal Hardy-weights for both operators under specified conditions, providing Hardy-type inequalities for Finsler p-Laplacian operators with singularities and Morrey space potentials.

Conclusion: Extends Hardy inequality theory to Finsler p-Dirichlet operators with singular domains and potentials in local Morrey spaces, establishing optimal weights that generalize classical results to anisotropic geometric settings.

Abstract: Fix an integer $n\geq 2$, an exponent $1<p<\infty$, and a domain $Ω\subseteq\mathbb{R}^{n}$. Let $Ω^{*}\triangleqΩ\setminus\{\hat{x}\}$ where $\hat{x}\inΩ$. Under some further conditions, we construct optimal Hardy-weights for the Finsler $p$-Dirichlet integral $$Q_{0}[φ;Ω^{*}]\triangleq\int_{Ω^{*}}H(x,\nabla φ)^{p}\,\mathrm{d}x\quad \mbox{on}\quad C^{\infty}_{c}(Ω^{*}),$$ and the Finsler $p$-Dirichlet integral with a potential $$Q_{V}[φ;Ω]\triangleq\int_Ω\left(H(x,\nabla φ)^{p}+ V|φ|^{p}\right)\,\mathrm{d}x\quad \mbox{on}\quad C^{\infty}_{c}(Ω),$$where $H(x,\cdot)$ is a family of norms on $\mathbb{R}^{n}$ parameterized by $x\inΩ^{*}$ or $x\inΩ$, respectively, and the potential $V$ lies in a subspace $\widehat{M}^{q}_{\rm loc}(p;Ω)$ of a local Morrey space $M^{q}_{\rm loc}(p;Ω)$.

</details>


### [21] [Navier-Stokes-Cahn-Hilliard system in a $3$D perforated domain with free slip and source term: Existence and homogenization](https://arxiv.org/abs/2512.21171)
*Amartya Chakrabortty,Haradhan Dutta,Hari Shankar Mahato*

Main category: math.AP

TL;DR: Analysis of a diffuse-interface model for binary mixtures in porous media using Navier-Stokes-Cahn-Hilliard equations with homogenization to derive effective macroscopic models.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of binary incompressible mixtures in periodically perforated porous media at microscopic scales and derive effective macroscopic models through homogenization.

Method: Two-part analysis: 1) Prove existence of weak solutions with uniform ε-estimates for fixed ε; 2) Perform periodic homogenization (ε→0) with different capillarity strength regimes.

Result: Two distinct effective models emerge: 1) Stokes-Cahn-Hilliard system without macroscopic convection for vanishing capillarity; 2) Full Navier-Stokes-Cahn-Hilliard system with nonlinear convection for balanced capillarity.

Conclusion: The homogenization successfully bridges microscopic pore-scale dynamics to macroscopic continuum models, with capillarity strength determining the presence of macroscopic convective effects.

Abstract: We study a diffuse-interface model for a binary incompressible mixture in a periodically perforated porous medium, described by a time-dependent Navier-Stokes-Cahn-Hilliard (NSCH) system posed on the pore domain $Ω_p^\varepsilon\subset\mathbb{R}^3$. The microscopic model involves a variable viscosity tensor, a non-conservative source term in the Cahn--Hilliard equation, and mixed boundary conditions: no-slip on the outer boundary and Navier slip with zero tangential stress on the surfaces of the solid inclusions. The capillarity strength $λ^\varepsilon>0$ depends on the microscopic scale $\varepsilon>0$.
  The analysis consists of two main parts. First, for each fixed $\varepsilon>0$, we prove the existence of a weak solution on a finite time interval $(0,T)$ and derive a priori estimates that are uniform with respect to $\varepsilon$ (and $λ^\varepsilon$). Second, we perform the periodic homogenization for the perforated setting, a limit $\varepsilon\to0$. Depending on the limit value $λ$ of the capillarity strength $λ^\varepsilon$, we obtain two distinct effective models: (i) in the vanishing capillarity regime $λ=0$, the limit system is of Stokes-Cahn-Hilliard type, with no macroscopic convection or advection; (ii) in the balanced regime $λ\in(0,+\infty)$, we derive a Navier-Stokes-Cahn-Hilliard system with nonlinear convection and advective transport of the phase field at the macroscopic scale. Finally, we establish the convergence of the microscopic free energy to a homogenized energy functional satisfying an analogous dissipation law.

</details>


### [22] [Long-Time Existence and Behavior of Solutions to the Inhomogeneous Kinetic FPU Equation](https://arxiv.org/abs/2512.21187)
*Haoling Xiang*

Main category: math.AP

TL;DR: The paper analyzes the inhomogeneous kinetic Fermi-Pasta-Ulam equation, showing that spatial dispersion effects extend solution lifespan from quadratic to quartic time scales.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the long-time behavior of phonon density distributions in nonlinear transport equations with structural degeneracies, particularly how spatial transport interacts with momentum-space collisions.

Method: Developed a functional framework capturing spatial transport and collision operator degeneracies, used dispersive estimates for transport flow to quantify decay effects, and applied these to obtain improved bounds for the nonlinear collision operator.

Result: Small solutions near vacuum can be propagated on significantly longer time scales than conservation laws alone would predict, extending classical quadratic lifespan to quartic time scale.

Conclusion: Spatial dispersion provides a crucial mechanism for extending solution lifespan in degenerate kinetic equations, demonstrating that transport effects can overcome limitations imposed by conservation laws.

Abstract: We study the inhomogeneous kinetic Fermi-Pasta-Ulam (FPU) equation, a nonlinear transport equation describing the evolution of phonon density distributions with four-phonon interactions. The equation combines free transport in physical space with a nonlinear collision operator acting in momentum space and exhibiting structural degeneracies. We develop a functional framework that captures the interplay between spatial transport and the degeneracies arising in the collision operator. A key ingredient of the analysis is a dispersive estimate for the transport flow, which quantifies decay effects generated by spatial propagation. Using this dispersive mechanism, we obtain improved bounds for the nonlinear collision operator and show that small solutions near the vacuum can be propagated on time scales significantly longer than those dictated by conservation laws alone. In particular, dispersion allows one to extend the classical quadratic lifespan to a quartic time scale.

</details>


### [23] [Green's Function and Solution Representation for a Boundary Value Problem Involving the Prabhakar Fractional Derivative](https://arxiv.org/abs/2512.21259)
*Erkinjon Karimov,Doniyor Usmonov,Maftuna Mirzaeva*

Main category: math.AP

TL;DR: The paper develops Green's function methods for boundary value problems involving Prabhakar fractional derivatives, providing explicit solution representations and proving existence/uniqueness.


<details>
  <summary>Details</summary>
Motivation: To extend classical Green-function techniques to Prabhakar fractional differential equations and provide analytical tools for studying boundary and inverse problems in this more general fractional framework.

Method: Using structural properties of Prabhakar kernels and generalized Mittag-Leffler functions to reduce the boundary value problem to a Volterra integral equation, then explicitly constructing the Green's function and deriving a closed-form integral solution representation.

Result: Successfully constructed the Green's function for Prabhakar fractional boundary value problems, derived explicit integral representation of solutions, and proved existence and uniqueness of solutions.

Conclusion: The work extends classical Green-function methods to Prabhakar fractional operators, providing powerful analytical tools for further research on boundary and inverse problems in this generalized fractional calculus framework.

Abstract: We investigate a first boundary value problem for a second-order partial differential equation involving the Prabhakar fractional derivative in time. Using structural properties of the Prabhakar kernel and generalized Mittag-Leffler functions, we reduce the problem to a Volterra type integral equation. This reduction enables the explicit construction of the corresponding Green's function. Based on the obtained Green's function, we derive a closed-form integral representation of the solution and prove its existence and uniqueness. The results extend classical Green-function techniques to a wider class of fractional operators and provide analytical tools for further study of boundary and inverse problems associated with Prabhakar-type fractional differential equations.

</details>


### [24] [Operational Calculus for the nth-Level Prabhakar Type Fractional Derivative with Applications](https://arxiv.org/abs/2512.21273)
*Imtiaz Waheed,Erkinjon Karimov,Mujeeb ur Rehman*

Main category: math.AP

TL;DR: Study investigates nth-level Prabhakar fractional derivative, establishes its properties, develops operational calculus, and applies it to solve fractional differential equations.


<details>
  <summary>Details</summary>
Motivation: The Prabhakar fractional derivative is a generalization that encompasses several well-known fractional derivatives, making it important for developing a unified framework for fractional calculus and solving complex fractional differential equations.

Method: Established fundamental properties of nth-level Prabhakar fractional derivative, particularly its relationship with Prabhakar fractional integral. Developed Mikusinski-type operational calculus for this derivative to create a framework for solving differential equations involving this operator.

Result: Successfully developed operational calculus framework and presented analytical solutions for two problems: a fractional order ordinary differential equation and the time fractional heat equation, both containing the nth-level Prabhakar derivative.

Conclusion: The study provides a comprehensive mathematical framework for the nth-level Prabhakar fractional derivative, demonstrating its practical utility through successful applications to fractional differential equations, thereby advancing fractional calculus theory and applications.

Abstract: This study investigates the nth-level Prabhakar fractional derivative, a generalization encompassing some well-known fractional derivatives. We establish its fundamental properties, particularly its relationship with the corresponding Prabhakar fractional integral. Furthermore, we develop Mikusinski-type operational calculus for this derivative, providing a framework for solving differential equations involving this operator. To illustrate its application, we present analytical solutions of two problems: a fractional order ordinary differential equation and the time fractional heat equation, both of which include the nth-level Prabhakar derivative.

</details>


### [25] [Non-Algebraic Decay for Solutions to the Navier-Stokes Equations](https://arxiv.org/abs/2512.21312)
*Lorenzo Brandolese,Matthieu Pageard,Cilon F. Perusato*

Main category: math.AP

TL;DR: The paper addresses a gap in Wiegner's theorem for 2D Navier-Stokes solutions with non-algebraic decay rates, showing these solutions behave asymptotically like heat equation solutions.


<details>
  <summary>Details</summary>
Motivation: Michael Wiegner's seminal work provided sharp algebraic decay rates for Navier-Stokes solutions, showing asymptotic similarity to heat equation solutions. However, there's a gap in the conclusion for 2D solutions with non-algebraic decay rates that needs to be addressed.

Method: The paper likely employs mathematical analysis techniques to examine the asymptotic behavior of 2D Navier-Stokes solutions, particularly focusing on the gap in Wiegner's theorem for non-algebraic decay cases. The approach would involve rigorous analysis of decay rates and asymptotic behavior.

Result: The paper closes the identified gap in Wiegner's theorem for 2D Navier-Stokes solutions with non-algebraic decay rates, providing a complete understanding of how these solutions behave asymptotically like heat equation solutions.

Conclusion: The research completes Wiegner's theorem by addressing the missing case for 2D Navier-Stokes solutions with non-algebraic decay, establishing a comprehensive understanding of asymptotic behavior between Navier-Stokes and heat equation solutions.

Abstract: Around forty years ago, Michael Wiegner provided, in a seminal paper, sharp algebraic decay rates for solutions of the Navier--Stokes equations, showing that these solutions behave asymptotically like the solutions of the heat equation with the same data as $t\to+\infty$, in the $L^2$-norm, up to some critical decay rate. In the present paper, we close a gap that appears in the conclusion of Wiegner's theorem in the 2D case, for solutions with non-algebraic decay rate.

</details>


### [26] [Large time behavior of the solution to the Cauchy problem for the discrete p-Laplacian with density on infinite graphs](https://arxiv.org/abs/2512.21321)
*Alan A. Tedeev*

Main category: math.AP

TL;DR: Analysis of Cauchy problem for nonstationary discrete p-Laplacian on infinite graphs with inhomogeneous density, proving precise stabilization rates for p>2 and universal bounds for fast-decaying densities.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time behavior of solutions to the nonstationary discrete p-Laplacian equation on infinite graphs with inhomogeneous density, particularly establishing precise stabilization rates and universal bounds.

Method: Uses energy inequalities and a new embedding result to analyze the Cauchy problem for the discrete p-Laplacian on infinite graphs supporting Sobolev inequality.

Result: For p>2 and nonnegative solutions with non-power function density, proves precise stabilization rate in time. For p>2 with fast-decaying density, establishes universal bound.

Conclusion: The paper provides rigorous analysis of stabilization behavior for discrete p-Laplacian on infinite graphs with inhomogeneous density, demonstrating how density properties affect long-time solution behavior.

Abstract: We consider the Cauchy problem for the nonstationary discrete p-Laplacian with inhomogeneous density \r{ho}(x) on an infinite graph which supports the Sobolev inequality. For nonnegative solutions when p > 2, we prove the precise rate of stabilization in time, provided \r{ho}(x) is a non-power function. When p > 2 and \r{ho}(x) goes to zero fast enough, we prove the universal bound. Our technique relies on suitable energy inequalities and a new embedding result.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [Quantum Origin of Classical Background Fields from Coherent States: A First-Principles Formulation in QED](https://arxiv.org/abs/2512.21122)
*Keita Seto*

Main category: physics.plasm-ph

TL;DR: The paper provides a first-principles derivation showing how classical background fields in QED emerge from coherent states of the electromagnetic field, establishing a unified framework that connects operator-based and functional approaches.


<details>
  <summary>Details</summary>
Motivation: To clarify the quantum origin of classical background electromagnetic fields used in QED for describing phenomena like laser-matter interactions and strong-field physics, and to provide a systematic foundation that goes beyond fixed background approximations.

Method: Starting from the operator formulation of QED, the authors show how scattering amplitudes between coherent states lead to an effective description using background fields. They maintain separation between coherent laser modes and other quantized photons, and demonstrate how conventional generating functionals with prescribed background fields emerge as limiting cases.

Result: The framework consistently incorporates effects beyond fixed background approximations (depletion and backreaction) without assuming specific field strengths or intensity regimes. It shows how standard formulations of strong-field QED arise as well-defined special cases within this general approach.

Conclusion: The work establishes a general, intensity-independent foundation for QED with coherent background fields, providing a unified view of operator-based and functional approaches that clarifies the quantum origins of classical background fields in QED.

Abstract: Classical background electromagnetic fields are routinely employed in quantum electrodynamics to describe a wide range of physical situations, from laser-matter interactions to strong-field phenomena. In this work, we present a first-principles formulation that clarifies the quantum origin of such classical background fields in QED by systematically deriving them from coherent states of the electromagnetic field.
  Abstract Starting from the operator formulation of QED, we show how scattering amplitudes between coherent states naturally lead to an effective description in terms of background fields, while maintaining a clear separation between the coherent laser mode and other quantized photon degrees of freedom. This framework allows one to consistently incorporate effects beyond the fixed background approximation, such as depletion and backreaction, without assuming any particular field strength or intensity regime.
  Abstract We further demonstrate how the conventional generating functional with a prescribed background field emerges as a limiting case, corresponding to fixed coherent state boundary conditions. The path integral representation is then obtained as a reformulation of the same underlying Heisenberg picture amplitudes, providing a unified view of operator-based and functional approaches.
  Abstract Our results establish a general and intensity-independent foundation for QED with coherent background fields, within which the standard formulations of strong-field QED arise as well-defined special cases.

</details>


### [28] [Multivariate scaling of proton and ion energies, divergence, and charge states in Target Normal Sheath Acceleration](https://arxiv.org/abs/2512.21279)
*Vasiliki E. Alexopoulou*

Main category: physics.plasm-ph

TL;DR: Researchers developed predictive scaling laws for laser-driven ion acceleration using a unified multiphysics model, enabling optimization of proton and ion beams for medical and scientific applications.


<details>
  <summary>Details</summary>
Motivation: Despite active research on laser-driven ion beams for applications like proton therapy and materials science, there's a lack of predictive correlations between laser/target parameters and resulting ion-beam properties due to the complex, multiphysics nature of laser-plasma interactions.

Method: Used a unified multiphysics model with >95% accuracy across pulse conditions to derive statistically validated scaling laws. Applied multivariate regression with cross-validation for continuous beam properties (cutoff energies, divergences) and classification/regression tree (CART) methods for discrete ionization states.

Result: Generated predictive scaling relations, probability maps, and contour plots correlating proton, carbon, and oxygen ion properties to laser/target parameters including pulse duration, power, spot size, target thickness, contrast, wavelength, and polarization.

Conclusion: The framework provides physically interpretable predictions for optimizing laser-driven ion sources across broad parameter spaces, advancing applications in proton therapy, materials modification, and high-energy-density physics.

Abstract: The interaction of an intense laser pulse with a solid target produces energetic proton and ion beams through the Target Normal Sheath Acceleration (TNSA) mechanism. Such beams are under active investigation for applications in proton beam therapy, materials modification, and nuclear and high-energy-density physics. Despite extensive experimental and theoretical effort, predictive correlations between laser and target parameters and the resulting ion-beam properties remain an open research question, owing to the intrinsically multiphysics and strongly coupled nature of laser-plasma interactions. Here, we employ our unified multiphysics model that reproduces laser-solid interaction dynamics with accuracy exceeding 95% over a broad range of short- and ultrashort-pulse conditions. Using this model, we derive statistically validated scaling laws and probability maps that correlate proton, carbon, and oxygen ion cutoff energies, beam divergences, and ionization states to a wide set of laser and target parameters, including pulse duration, laser power, laser beam spot, target thickness, prepulse-main pulse interval, contrast, laser wavelength, and polarization. Continuous beam properties (cutoff energies and beam divergences) are described using multivariate regression with cross-validation, while discrete ionization states are analyzed using classification and regression tree (CART) methods, enabling nonlinear and threshold-dependent behavior to be captured. The resulting scaling relations, contour maps, and box plots elucidate the coupled roles of laser pulse, and target geometry in governing TNSA ion acceleration and charge-state formation. These results provide a predictive and physically interpretable framework for understanding and optimizing laser-driven ion sources across a wide parameter space.

</details>


### [29] [Impurity peaking of SPARC H-modes: a sensitivity study on physics and engineering assumptions](https://arxiv.org/abs/2512.21286)
*Marco Muraca,Pablo Rodriguez-Fernandez,Joe Hall,Nathaniel T. Howard,Daniel Fajardo,Giovanni Tardini,Benedikt Zimmermann,Thomas Body*

Main category: physics.plasm-ph

TL;DR: SPARC tokamak impurity transport simulations show turbulent transport dominates neoclassical, predicting low tungsten accumulation in H-mode plasmas with minimal impact from rotation and pedestal impurity variations.


<details>
  <summary>Details</summary>
Motivation: To predict impurity transport behavior in the upcoming SPARC tokamak, particularly for tungsten accumulation which can degrade plasma performance, and to understand how transport mechanisms compare to ITER predictions.

Method: Used ASTRA+STRAHL framework with FACIT for neoclassical transport, TGLF-SAT2 for turbulent transport, and neural network trained on EPED simulations for self-consistent pedestal calculations. Performed simulations for three H-mode plasmas with sensitivity studies on impurity concentrations, rotation, and DT fuel composition.

Result: Turbulent impurity transport dominates neoclassical component; predictions are insensitive to tungsten pedestal concentration; argon variations show small effects; rotation has minimal impact; maximum fusion power occurs at 55-45% DT composition; low tungsten accumulation predicted for SPARC's low collisionality regime.

Conclusion: SPARC will experience low tungsten accumulation due to prevailing turbulent transport over neoclassical, similar to ITER predictions, with impurity transport showing robustness to various modeling uncertainties and plasma conditions.

Abstract: In this paper, an overview of the impurity transport for three H-mode plasmas in the upcoming SPARC tokamak has been provided. The simulations have been performed within the ASTRA+STRAHL framework, using FACIT and TGLF-SAT2 to predict, respectively, neoclassical and turbulent core transport, while a neural network trained on EPED simulations has been employed to calculate the pedestal height and width self-consistently. A benchmark with previous simulations at constant impurity fraction has been provided for three H-modes, spanning different plasma current and magnetic field values. For a scenario, additional simulations have been performed to account for uncertainties in the modeling assumptions. The predictions are nearly insensitive to changes in the top of pedestal W concentrations. Varying the Ar pedestal concentration has shown a small effect on the impurity peaking and nearly constant fusion gain values, due to multiple effects on pedestal pressure, main ion dilution and density peaking. The inclusion of rotation in ASTRA simulations has shown minimal impact on confinement and impurity transport predictions. An exploratory study has been provided with a first set of simulations treating D and T separately, experiencing a maximum fusion power at 55-45% DT fuel composition, and an asymmetric distribution with respect to the D concentration. All the results, including sensitivity scans of toroidal velocity and ion temperature and density gradients, highlighted that turbulent impurity transport prevails on the neoclassical component, aligning with previous ITER predictions, and suggesting that next generation devices like SPARC, operating at low collisionality, will experience low W accumulation.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [30] [The space spinor formalism and estimates for spinor fields](https://arxiv.org/abs/2512.20768)
*Mariem Magdy,Juan A. Valiente Kroon*

Main category: gr-qc

TL;DR: The paper adapts the positive commutator method from second-order hyperbolic equations to first-order spinor equations using space spinor formalism.


<details>
  <summary>Details</summary>
Motivation: To develop systematic methods for constructing estimates for spinor fields satisfying first-order equations, bridging the gap between techniques for second-order hyperbolic equations and spinor field analysis.

Method: Uses space spinor formalism for 2-component spinors to adapt the method of positive commutators (originally for second-order hyperbolic equations) to first-order spinor equations.

Result: Provides a framework for constructing estimates for spinor fields and recasts hyperbolicity concepts in the context of spinor equations, connecting this approach with other estimation strategies.

Conclusion: The space spinor formalism enables effective adaptation of positive commutator methods to first-order spinor equations, offering new tools for analyzing spinor field estimates and hyperbolicity.

Abstract: We show how the space spinor formalism for 2-component spinors can be used to construct estimates for spinor fields satisfying first order equations. We discuss the connection of the approach presented in this article with other strategies for the construction of estimates. In addition, we recast several concepts related to the notion of hyperbolicity in the context of spinor equations. The approach described in this article can be regarded as an adaptation to first order equations of the method of positive commutators for second order hyperbolic equations.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [31] [Formal O(N3) scaling GW calculations by block tensor decomposition for large molecule systems](https://arxiv.org/abs/2512.21022)
*Yueyang Zhang,Wei Wu,Peifeng Su*

Main category: physics.chem-ph

TL;DR: BTD-based GW algorithm achieves O(N²) scaling for large molecular systems, enabling calculations with over 3000 basis functions.


<details>
  <summary>Details</summary>
Motivation: The GW approximation is crucial for quasiparticle energies but has high computational cost and poor scaling for large systems.

Method: Extends block tensor decomposition (BTD) algorithm with imaginary-time GW formalism and real space screening for polarizability.

Result: Achieves observed O(N²) scaling, BTD-RPA also shows O(N²) scaling, enables eigenvalue-self-consistent GW for >3000 basis functions.

Conclusion: BTD establishes an efficient and scalable approach for large-scale GW calculations in molecular systems.

Abstract: Within the framework of many-body perturbation theory based on Green's functions, the $GW$ approximation has emerged as a pivotal method for computing quasiparticle energies and excitation spectra. However, its high computational cost and steep scaling present significant challenges for applications to large molecular systems. In this work, we extend the block tensor decomposition (BTD) algorithm, recently developed in our previous work [J. Chem. Phys. 163, 174109 (2025)] for low-rank tensor compression, to enable a formally $O(N^3)$-scaling $GW$ algorithm. By integrating BTD with an imaginary-time $GW$ formalism and introducing a real space screening strategy for the polarizability, we achieve an observed scaling of approximately $O(N^2)$ in test systems. Key parameters of the algorithm are optimized on the S66 dataset using the JADE algorithm, ensuring a balanced compromise between accuracy and efficiency. Our BTD-based random phase approximation also exhibits $O(N^2)$ scaling, and eigenvalue-self-consistent $GW$ calculations become feasible for systems with over 3000 basis functions. This work establishes BTD as an efficient and scalable approach for large-scale $GW$ calculations in molecular systems.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [32] [Implicit Numerical Scheme for the Hamilton-Jacobi-Bellman Quasi-Variational Inequality in the Optimal Market-Making Problem with Alpha Signal](https://arxiv.org/abs/2512.20850)
*Alexey Meteykin*

Main category: q-fin.MF

TL;DR: The paper develops a numerical method for solving combined stochastic and impulse control problems in limit order book market making using an implicit time-discretization scheme with policy iteration.


<details>
  <summary>Details</summary>
Motivation: Market makers face complex control problems in limit order books that involve both stochastic and impulse control elements, requiring stable numerical methods to solve the resulting Hamilton-Jacobi-Bellman quasi-variational inequalities.

Method: Implicit time-discretization scheme coupled with a policy iteration algorithm, which removes time-step restrictions of explicit methods and ensures unconditional stability.

Result: The proposed method achieves unconditional stability and convergence to the unique viscosity solution by verifying monotonicity, stability, and consistency conditions and applying the comparison principle.

Conclusion: The implicit scheme with policy iteration provides an effective numerical approach for solving market maker control problems in limit order books, overcoming limitations of explicit methods while guaranteeing convergence.

Abstract: We address the problem of combined stochastic and impulse control for a market maker operating in a limit order book. The problem is formulated as a Hamilton-Jacobi-Bellman quasi-variational inequality (HJBQVI). We propose an implicit time-discretization scheme coupled with a policy iteration algorithm. This approach removes time-step restrictions typical of explicit methods and ensures unconditional stability. Convergence to the unique viscosity solution is established by verifying monotonicity, stability, and consistency conditions and applying the comparison principle.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [33] [Mathematical Analysis of Symmetry-Protected Bound States in the Continuum in Waveguide Arrays](https://arxiv.org/abs/2512.20895)
*Xin Feng,Wei Wu*

Main category: physics.optics

TL;DR: Rigorous mathematical analysis of symmetry-based Bound States in the Continuum (BICs) in optical waveguide arrays using nonorthogonal coupled-mode equations and Bessel function addition theorems.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive mathematical foundation for symmetry-protected BICs in waveguide arrays, moving beyond approximations like tight-binding or orthogonal coupled-mode equations to enable precise design of BIC-based devices.

Method: Transforms wave propagation into Nonorthogonal Coupled-Mode Equations (NCME) using Bessel function addition theorems for exact overlap integrals and coupling coefficients. Analyzes finite and infinite waveguide arrays using harmonic analysis, then proves BIC existence in symmetric vertical waveguide configurations.

Result: Derived exact expressions for overlap integrals and coupling coefficients, characterized dispersion relations and continuum for infinite arrays, proved existence of BICs in symmetric waveguide systems, and demonstrated transition from perfect BIC to leaky modes under symmetry-breaking perturbations.

Conclusion: Provides a rigorous mathematical framework for symmetry-protected BICs in waveguide arrays, offering an efficient and precise computational model for designing BIC-based optical devices with controlled radiation properties.

Abstract: This paper presents a rigorous mathematical analysis for symmetry-based Bound States in the Continuum (BICs) in optical waveguide arrays. Different from existing research, we consider a finite system of horizontally and equidistantly aligned waveguides and transform the wave propagation problem into Nonorthogonal Coupled-Mode Equations (NCME), rather than adopting the tight-binding approximation or orthogonal coupled-mode equations. We derive the exact expressions of the overlap integrals and coupling coefficients by utilizing the addition theorems of Bessel functions. We then generalize the discussion to an infinite waveguide array and rigorously characterize the dispersion relation and continuum with the help of theories in harmonic analysis. In the second part of the paper, we give a strict proof of the existence of BICs in the aforementioned waveguide system with two additional identical vertical waveguides aligned symmetrically above and below the horizontal waveguide array. We further numerically demonstrate the transition from a perfect BIC to a leaky mode by introducing a symmetry-breaking refractive index perturbation and quantitatively analyze the resulting radiation losses. This work gives a comprehensive study of symmetry-protected BICs and provides an efficient and precise computational model for designing such BICs devices.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [34] [Ab initio Approach to Collective Excitations in Excitonic Insulators](https://arxiv.org/abs/2512.20969)
*Fengyuan Xuan,Jiexi Song,Zhiyuan Sun*

Main category: cond-mat.mtrl-sci

TL;DR: Ab initio method for studying collective excitations in symmetry-broken systems like excitonic insulators, density waves, and superconductors using Bethe-Salpeter equation in quasiparticle representation.


<details>
  <summary>Details</summary>
Motivation: To develop a first-principles approach for quantitatively predicting excited state phenomena in electronic systems with spontaneous symmetry breaking, which is challenging with existing methods.

Method: Derive Bethe-Salpeter equation for particle-hole excitations in quasiparticle representation, solve for collective excited states, and compute order parameter fluctuations from first principles.

Result: Successfully applied to excitonic insulating phases in biased WSe2-MoSe2 bilayer, revealing gapless phase-mode, subgap Bardasis-Schrieffer modes, and above-gap scattering states.

Conclusion: This ab initio approach enables quantitative predictions of excited state phenomena in symmetry-broken electronic systems, paving the way for first-principles studies of collective excitations.

Abstract: An ab initio approach is presented for studying the collective excitations in excitonic insulators, charge/spin density waves and superconductors. We derive the Bethe-Salpeter-Equation for the particle-hole excitations in the quasiparticle representation, from which the collective excited states are solved and the corresponding order parameter fluctuations are computed. This method is demonstrated numerically for the excitonic insulating phases of the biased WSe2-MoSe2 bilayer. It reveals the gapless phase-mode, the subgap Bardasis-Schrieffer modes and the above-gap scattering states. Our work paves the way for quantitative predictions of excited state phenomena from first-principles calculations in electronic systems with spontaneous symmetry breaking.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [35] [Prediction Air Temperature in Geothermal Heat Exchangers Using Pseudorandom Numbers: The New DARL Model](https://arxiv.org/abs/2512.19976)
*C. Ramírez-Dolores,J. C. Zamora-Luria,J. A. Altamirano-Acosta,L. Sarao-Cruz,P. Jiménez-Palma,J. Moreno-Falconi*

Main category: cs.CY

TL;DR: DARL model uses pseudo-random numbers with Fermat prime seeds to predict air temperature distribution in Earth-Air-Water Heat Exchangers, reducing sensor dependency with <6.2% error.


<details>
  <summary>Details</summary>
Motivation: Current methods for characterizing thermal air distribution in EAWHEs require dense sensor networks, leading to high instrumentation, data acquisition, and computational costs. There's a need for alternative approaches that minimize sensor dependency while maintaining accuracy.

Method: DARL model integrates experimental boundary condition data with simulations using pseudo-random numbers (PRNs). PRNs are generated using Fermat's prime numbers as seeds to initialize the generator. The model combines these with ordinary linear regressions for temperature prediction.

Result: The model can estimate thermal air distribution at different lengths with relative error less than 6.2%. Statistical validations including Shapiro-Wilk test and root mean square error demonstrate the model's efficiency and predictive capacity.

Conclusion: DARL represents a significant methodological advance that reduces dependence on dense sensor networks in EAWHE experimental systems while maintaining accurate temperature prediction capabilities, offering potential cost savings and practical implementation benefits.

Abstract: The use of Earth-Air-Water Heat Exchangers (EAWHE) for sustainable air conditioning has not been widely studied. Due to their experimental nature, methods of characterizing internal thermal air distribution impose high dependence on instrumentation by sensors and entail data acquisition and computational costs. This document presents an alternative method that estimates air temperature distribution while minimizing the need for a dense network of sensors in the experimental system. The proposed model, DARL (Data of Air and Random Length), can predict the temperature of air circulating inside EAWHEs. DARL is a significant methodological advance that integrates experimental data from boundary conditions with simulations based on pseudo-random numbers (PRNs). These PRNs are generated using Fermat's prime numbers as seeds to initialize the generator. Ordinary linear regressions and robust statistical validations, including the Shapiro-Wilk test and root mean square error, have demonstrated that the model can estimate the thermal distribution of air at different lengths with a relative error of less than 6.2%. These results demonstrate the model's efficiency, predictive capacity, and potential to reduce dependence on sensors.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [36] [Minijets and Broken Stationarity in a Blazar : Novel Insights into the Origin of $γ$-ray Variability in CTA 102](https://arxiv.org/abs/2512.21240)
*Agniva Roychowdhury*

Main category: astro-ph.HE

TL;DR: CTA 102's 18-year Fermi-LAT light curves show a transition from log-normal to non-log-normal flux distributions after a major 2017 flare, indicating a state change from frequent flaring to plateaued behavior, explained by magnetic relaxation and a modified minijets-in-a-jet model.


<details>
  <summary>Details</summary>
Motivation: To understand why high-energy blazar light curves (particularly CTA 102's GeV emission) transition from log-normal to non-log-normal flux distributions after major flares, and to identify the physical mechanisms driving this state change.

Method: Analyzed 18-year Fermi-LAT archival data (0.1-100 GeV) of CTA 102; performed statistical analyses of flux distributions; developed Monte Carlo simulations of a modified minijets-in-a-jet model where GeV flares occur when maximum minijets align toward the broad line region and line of sight within an external Compton framework.

Result: Neither pre- nor post-flare light curves follow strictly log-normal distributions; significant reduction in skewness from pre- to post-flare indicates transition from energetic flaring state to plateaued state; magnetic relaxation explains state transition; simulated flux distributions match observed data using modified log-normal power-law distribution.

Conclusion: CTA 102 underwent a state transition after the 2017 flare due to magnetic relaxation, with the modified minijets-in-a-jet model successfully reproducing both the GeV flares and their flux distributions, explaining the observed departure from log-normal statistics.

Abstract: High-energy blazar light curves, in X-rays and beyond, have historically preferred a log-normal flux distribution, signifying multiplicative processes either in the jet itself or due to connection(s) with accretion. Here we present 18 year archival Fermi-LAT light curves (0.1-100 GeV) of the flat spectrum radio quasar (FSRQ) CTA 102 from August 2008 to November 2025, which underwent a huge flare in 2017, with a $\sim$ factor of 100 jump in $γ$-ray flux, along with similar flaring in X-rays. Our statistical analyses confirm that neither the pre nor the post-flare total GeV light curves follow a strictly log-normal distribution. Instead, we observe a statistically significant reduction in skewness from the pre to the post-flare light curves, which implies the blazar transitioned from an energetic state with frequent flaring to a more plateaued state with occasional flaring. We further find that this state transition can be explained through magnetic relaxation, where many reconnection events caused the 2017 flare, after which the magnetic field was ordered and its energy reached a minimum. To explain this further, we use a Monte Carlo simulation of a modified minijets-in-a-jet model where GeV flares are produced only when a maximum number of minijets move toward the broad line region and towards the line of sight, in the context of an external Compton model. The flux distributions (both observed and simulated) could be fit by a modified log-normal power-law distribution, implying our minijets model can reproduce the GeV flares in CTA 102 as well as their flux distributions.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [37] [Regularity of Einstein 5-manifolds via 4-dimensional gap theorems](https://arxiv.org/abs/2512.21317)
*Yiqi Huang,Tristan Ozuch*

Main category: math.DG

TL;DR: The paper refines regularity results for noncollapsed limits of 5D manifolds with bounded Ricci curvature, proving unique tangent cones, structure of singular sets, and orbifold regularity for Einstein limits.


<details>
  <summary>Details</summary>
Motivation: To improve understanding of the geometric structure and regularity properties of noncollapsed limits in 5-dimensional Riemannian geometry, particularly for Einstein manifolds with bounded Ricci curvature.

Method: Uses geometric analysis techniques including tangent cone analysis, singular set characterization, and relies crucially on a new independent result about isolation of spherical/hyperbolic 4-orbifolds among Einstein 4-orbifolds in Gromov-Hausdorff sense.

Result: Proves five main results: (1) unique tangent cones of form ℝ×ℝ⁴/Γ on top stratum, (2) singular set contained in countable union of Lipschitz curves/points, (3) Lipschitz curves are smooth geodesics away from nowhere dense subset, (4) geodesic interiors are removable leading to real-analytic orbifolds, (5) uniqueness of tangent cones at infinity for asymptotically Ricci-flat manifolds with Euclidean volume growth.

Conclusion: The results establish refined regularity for 5D Einstein limits and suggest the broader question of whether similar orbifold regularity holds for noncollapsed limits of Einstein manifolds off a codimension 5 set in arbitrary dimensions.

Abstract: We refine the regularity of noncollapsed limits of 5-dimensional manifolds with bounded Ricci curvature. In particular, for noncollapsed limits of Einstein 5-manifolds, we prove that
  (1) tangent cones are unique of the form $\mathbb{R}\times\mathbb{R}^4/Γ$ on the top stratum, hence outside a countable set of points,
  (2) the singular set is entirely contained in a countable union of Lipschitz curves and points,
  (3) away from a nowhere dense subset, these Lipschitz curves consist of smooth geodesics,
  (4) the interior of any geodesic is removable: limits of Einstein manifolds are real-analytic orbifolds with singularities along geodesic and bounded curvature away from their extreme points, and
  (5) if an asymptotically Ricci-flat 5-manifold with Euclidean volume growth has one tangent cone at infinity that splits off a line, then it is the unique tangent cone at infinity.
  These results prompt the question of the orbifold regularity of noncollapsed limits of Einstein manifolds off a codimension 5 set in arbitrary dimension.
  The proofs rely on a new result of independent interest: all spherical and hyperbolic 4-orbifolds are isolated among Einstein 4-orbifolds in the Gromov-Hausdorff sense. This yields various gap theorems for Einstein 4-orbifolds, which do not extend to higher dimensions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [38] [Forecasting N-Body Dynamics: A Comparative Study of Neural Ordinary Differential Equations and Universal Differential Equations](https://arxiv.org/abs/2512.20643)
*Suriya R S,Prathamesh Dinesh Joshi,Rajat Dandekar,Raj Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: Scientific ML approach using Neural ODEs and UDEs for n-body problem forecasting, with UDEs showing superior data efficiency (20% vs 90% training data needed).


<details>
  <summary>Details</summary>
Motivation: Traditional ML models for n-body trajectory prediction are data-intensive black boxes that ignore physical laws and lack interpretability. Scientific ML embeds known physical laws into ML frameworks for better performance and interpretability.

Method: Uses Scientific ML frameworks in Julia: Neural ODEs and Universal Differential Equations (UDEs) to predict n-body system dynamics. Employs synthetically created noisy data to simulate real-world observational limitations. Determines forecasting breakdown point - the minimum training data needed for accurate predictions.

Result: UDE model is much more data efficient, requiring only 20% of data for correct forecasting, while Neural ODE requires 90% of data.

Conclusion: Scientific ML approaches, particularly UDEs, offer significant advantages for n-body problem forecasting by embedding physical laws, improving data efficiency, and providing interpretability compared to traditional black-box ML models.

Abstract: The n body problem, fundamental to astrophysics, simulates the motion of n bodies acting under the effect of their own mutual gravitational interactions. Traditional machine learning models that are used for predicting and forecasting trajectories are often data intensive black box models, which ignore the physical laws, thereby lacking interpretability. Whereas Scientific Machine Learning ( Scientific ML ) directly embeds the known physical laws into the machine learning framework. Through robust modelling in the Julia programming language, our method uses the Scientific ML frameworks: Neural ordinary differential equations (NODEs) and Universal differential equations (UDEs) to predict and forecast the system dynamics. In addition, an essential component of our analysis involves determining the forecasting breakdown point, which is the smallest possible amount of training data our models need to predict future, unseen data accurately. We employ synthetically created noisy data to simulate real-world observational limitations. Our findings indicate that the UDE model is much more data efficient, needing only 20% of data for a correct forecast, whereas the Neural ODE requires 90%.

</details>


### [39] [Improving Matrix Exponential for Generative AI Flows: A Taylor-Based Approach Beyond Paterson--Stockmeyer](https://arxiv.org/abs/2512.20777)
*Jorge Sastre,Daniel Faronbi,José Miguel Alonso,Peter Traver,Javier Ibáñez,Nuria Lloret*

Main category: cs.LG

TL;DR: Optimized Taylor-based algorithm for matrix exponential with dynamic parameter selection, designed for high-throughput generative AI applications.


<details>
  <summary>Details</summary>
Motivation: Matrix exponential is fundamental for scientific computing and system simulation, with applications in control theory, quantum mechanics, and generative AI. While Padé approximants with scaling/squaring have been standard, recent Taylor-based methods offer superior accuracy and reduced computational complexity, especially for high-throughput generative AI flows.

Method: Developed an optimized Taylor-based algorithm for matrix exponential with rigorous error analysis and dynamic selection strategy for Taylor order and scaling factor to minimize computational effort under prescribed error tolerance.

Result: Extensive numerical experiments show significant acceleration and high numerical stability compared to state-of-the-art implementations, establishing the method as highly efficient for large-scale generative modeling.

Conclusion: The proposed Taylor-based algorithm provides an efficient, accurate, and stable solution for matrix exponential computation, particularly well-suited for high-throughput generative AI applications.

Abstract: The matrix exponential is a fundamental operator in scientific computing and system simulation, with applications ranging from control theory and quantum mechanics to modern generative machine learning. While Padé approximants combined with scaling and squaring have long served as the standard, recent Taylor-based methods, which utilize polynomial evaluation schemes that surpass the classical Paterson--Stockmeyer technique, offer superior accuracy and reduced computational complexity. This paper presents an optimized Taylor-based algorithm for the matrix exponential, specifically designed for the high-throughput requirements of generative AI flows. We provide a rigorous error analysis and develop a dynamic selection strategy for the Taylor order and scaling factor to minimize computational effort under a prescribed error tolerance. Extensive numerical experiments demonstrate that our approach provides significant acceleration and maintains high numerical stability compared to existing state-of-the-art implementations. These results establish the proposed method as a highly efficient tool for large-scale generative modeling.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [40] [Propagation Estimates for the Boson Star Equation](https://arxiv.org/abs/2512.20718)
*Sébastien Breteaux,Jérémy Faupin,Viviana Grasselli*

Main category: math-ph

TL;DR: The paper analyzes boson star equations with general two-body potentials, proving finite speed of propagation bounds and asymptotic phase-space estimates for global solutions.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical properties of boson star equations with general interaction potentials, particularly propagation speed limits and long-time asymptotic behavior, which are important for understanding relativistic quantum systems.

Method: Mathematical analysis of the boson star equation with general two-body potential w, assuming w decomposes as a finite signed measure plus bounded function. Uses Sobolev space framework and scattering theory techniques to prove propagation bounds and asymptotic estimates.

Result: 1) Local solutions cannot propagate faster than light speed, up to exponentially small remainder. 2) For short-range potentials and sufficiently regular/small initial data, global solutions satisfy asymptotic phase-space propagation estimates and minimal velocity estimates tied to scattering state momentum.

Conclusion: The boson star equation exhibits relativistic propagation constraints even with general interaction potentials, and for suitable potentials/initial data, solutions display controlled asymptotic behavior connected to scattering theory.

Abstract: We consider the boson star equation with a general two-body interaction potential $w$ and initial data $ψ_0$ in a Sobolev space. Under general assumptions on $w$, namely that $w$ decomposes as a sum of a finite, signed measure and an essentially bounded function, we prove that the (local in time) solution cannot propagate faster than the speed of light, up to a sharp exponentially small remainder term. If $w$ is short-range and $ψ_0$ is regular and small enough, we prove in addition asymptotic phase-space propagation estimates and minimal velocity estimates for the (global in time) solution, depending on the momentum of the scattering state associated to $ψ_0$.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [41] [The Dynamical Anatomy of Anderson Acceleration:From Adaptive Momentum to Variable-Mass ODEs](https://arxiv.org/abs/2512.21269)
*Kewang Chen,Yongqiu Jiang,Kees Vuik*

Main category: math.OC

TL;DR: The paper provides a rigorous analysis of Anderson Acceleration (AA) through high-resolution ODEs, revealing AA's instability mechanism and proposing an improved Energy-Guarded AA algorithm with better convergence properties.


<details>
  <summary>Details</summary>
Motivation: To bridge the theoretical gap in understanding Anderson Acceleration (introduced in 1965) by connecting discrete acceleration algorithms with continuous dynamical systems, and to address fundamental questions about AA's physical nature and instability mechanisms.

Method: Using High-Resolution ODEs to analyze AA, proving AA can be rewritten as an adaptive momentum method, conducting Lyapunov energy analysis to identify instability mechanisms, and proposing Energy-Guarded Anderson Acceleration (EG-AA) with thermodynamic consistency constraints.

Result: Theoretical analysis reveals AA's instability comes from unchecked growth in effective mass acting as negative damping, while high-resolution analysis identifies implicit Hessian-driven stabilization. EG-AA is proven to be no worse than standard AA and numerical experiments show improved convergence stability and rates in ill-conditioned problems.

Conclusion: The paper provides fundamental theoretical insights into Anderson Acceleration's dynamics, identifies its instability mechanism, and proposes EG-AA as a thermodynamically consistent improvement that maintains acceleration benefits while suppressing instability through energy guarding.

Abstract: This paper provides a rigorous derivation and analysis of accelerated optimization algorithms through the lens of High-Resolution Ordinary Differential Equations (ODEs). While classical Nesterov acceleration is well-understood via asymptotic vanishing damping, the dynamics of Anderson Acceleration (AA) remain less transparent. This work makes significant theoretical contributions to AA by bridging discrete acceleration algorithms with continuous dynamical systems, while also providing practical algorithmic innovations. Our work addresses fundamental questions about the physical nature of Anderson Acceleration that have remained unanswered since its introduction in 1965. Firstly, we prove that AA can be exactly rewritten as an adaptive momentum method and, in the high-resolution limit, converges to a second-order ODE with Variable Effective Mass. Through a Lyapunov energy analysis, we reveal the specific instability mechanism of standard AA: unchecked growth in effective mass acts as negative damping, physically injecting energy into the system and violating dissipation constraints. Conversely, high-resolution analysis identifies an implicit Hessian-driven damping term that provides stabilization in stiff regimes. Leveraging these dynamical insights, we then propose Energy-Guarded Anderson Acceleration (EG-AA), an algorithm that acts as an inertial governor to enforce thermodynamic consistency. Morevoer, our convergence analysis, formulated via the Acceleration Gain Factor, proves that EG-AA improves upon gradient descent by maximizing the geometric contraction of the linear subspace projection while actively suppressing nonlinear approximation errors. Theoretical bounds confirm that EG-AA is no worse than standard AA, and numerical experiments demonstrate strictly improved convergence stability and rates in ill-conditioned convex composite problems compared to standard Anderson mixing.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [42] [On the Relationship Between Nanoflare Energy and Delay in the Closed Solar Corona](https://arxiv.org/abs/2512.20875)
*Shanwlee Sow Mondal,James A. Klimchuk,Craig D. Johnston,Lars K. S. Daldorff*

Main category: astro-ph.SR

TL;DR: No correlation found between nanoflare energies and their delays, suggesting onset not determined solely by critical magnetic stress but may involve triggering by other events.


<details>
  <summary>Details</summary>
Motivation: To understand the physical mechanism of nanoflares and plasma response by determining the relationship between nanoflare energies and their delays.

Method: Used 3D multi-strand simulation with prescribed photospheric motions to generate nanoflares self-consistently. Quantified energies and durations using three distinct methods, then investigated correlation using two non-parametric, rank-based statistical tests.

Result: Consistently found little to no correlation between nanoflare energies and delays across all methods. Distribution of exponent α in E ∝ τ_D^α peaks near zero, with broad delay distributions within fixed energy bins. Results hold for both preceding and subsequent event correlations, and for high-energy nanoflares subset.

Conclusion: Absence of correlation suggests nanoflare onset is not solely determined by critical magnetic stress value and may involve triggering by other events, possibly related to locally complex topology.

Abstract: Determining the relationship between nanoflare energies and their delays is the key for understanding the physical mechanism of the events and the plasma response. Nanoflares analyzed in this study were generated self-consistently via prescribed photospheric motions in a 3D multi-strand simulation of a subset of active region magnetic flux. Energies and durations were quantified using three distinct methods. In this study, we investigated the correlation between nanoflare energies (E) and delays ($τ_D$) using two non-parametric, rank-based statistical tests. Across all methods, results consistently show little to no correlation. This is further supported by the distribution of the exponent $α$ in the assumed relation $E \propto τ_D^α$, which peaks near zero, and by broad delay distributions within fixed energy bins. These findings are irrespective of whether delays are correlated with the energy of the preceding or subsequent event. They also hold for a subset of high-energy nanoflares. The absence of correlation suggests that nanoflare onset is not solely determined by a critical value of magnetic stress and may involve triggering by other events, perhaps related to a locally complex topology.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [43] [Flow Gym](https://arxiv.org/abs/2512.20642)
*Francesco Banelli,Antonio Terpin,Alan Bonomi,Raffaello D'Andrea*

Main category: physics.flu-dyn

TL;DR: Flow Gym is a toolkit for flow-field quantification research with synthetic image generation, unified algorithm interface, and JAX implementations.


<details>
  <summary>Details</summary>
Motivation: To create a standardized research environment for flow-field quantification methods, addressing the need for consistent testing, training, and deployment of algorithms that analyze tracer particle images.

Method: Builds on OpenAI Gym/Stable-Baselines3 framework, uses SynthPix for synthetic image generation, provides unified interface for algorithm testing/training, and includes JAX implementations of existing algorithms.

Result: A comprehensive toolkit that enables standardized evaluation and development of flow-field quantification algorithms with synthetic data generation capabilities.

Conclusion: Flow Gym provides an essential research infrastructure for advancing flow-field quantification methods through standardized benchmarking and algorithm development.

Abstract: Flow Gym is a toolkit for research and deployment of flow-field quantification methods inspired by OpenAI Gym and Stable-Baselines3. It uses SynthPix as synthetic image generation engine and provides a unified interface for the testing, deployment and training of (learning-based) algorithms for flow-field quantification from a number of consecutive images of tracer particles. It also contains a growing number of integrations of existing algorithms and stable (re-)implementations in JAX.

</details>


### [44] [Velocity dip in turbulent mixed convection of an open Poiseuille-Rayleigh-Bénard channel](https://arxiv.org/abs/2512.20977)
*Ben-Rui Xu,Ao Xu,Heng-Dong Xi*

Main category: physics.flu-dyn

TL;DR: Velocity-dip phenomenon emerges in turbulent mixed convection in open PRB channels due to large-scale rolls transporting low-speed fluid upward, causing maximum velocity to occur below the free-slip boundary.


<details>
  <summary>Details</summary>
Motivation: To understand the emergence of velocity-dip phenomenon in turbulent mixed convection in open Poiseuille-Rayleigh-Bénard channels with free-slip upper boundary, which is important for understanding flow physics in mixed convection systems.

Method: Three-dimensional direct numerical simulations (DNS) for Rayleigh numbers 10^5 ≤ Ra ≤ 10^8 at fixed Pr = 0.71 and Re_b = 2850, analyzing flow regimes and proposing a model based on balance between buoyancy and shear production with dissipation.

Result: Three flow regimes identified: shear-dominated (small-scale streaks), mixed (large-scale rolls spanning channel height), and buoyancy-dominated (roll fragmentation). Velocity dip occurs due to large-scale rolls transporting low-speed fluid upward, creating negative viscous contribution and velocity gradient reversal near surface.

Conclusion: Velocity-dip phenomenon in mixed convection is caused by large-scale roll structures that transport low-speed fluid upward, and the proposed model accurately reproduces DNS mean velocity profiles across the studied Ra range.

Abstract: We study the emergence of a velocity-dip phenomenon in turbulent mixed convection in open Poiseuille-Rayleigh-Bénard (PRB) channels with a free-slip upper boundary. Three-dimensional direct numerical simulations (DNS) are performed for Rayleigh numbers in the range $10^5 \leq Ra \leq 10^8$, at a fixed Prandtl number $Pr = 0.71$ and a bulk Reynolds number $Re_b = 2850$. In the shear-dominated regime, the flow is characterised by small-scale structures such as near-wall streaks. As buoyancy becomes comparable to shear, streamwise-oriented large-scale rolls emerge and span the full channel height. At higher Rayleigh numbers, buoyancy dominates and the rolls fragment, giving rise to a convection-cell-dominated regime. Short-time-averaged flow fields show that streamwise rolls transport low-speed fluid from the bottom wall towards the upper boundary, forming laterally extended low-speed regions, while roll fragmentation induces upstream low-speed regions near the upper boundary. Both mechanisms locally reduce the near-surface mean velocity, leading to a velocity dip in which the maximum mean streamwise velocity is located below the upper boundary. Consistent with the mean momentum budget, the near-surface region exhibits a large-scale Reynolds shear stress that exceeds the local total shear stress, implying a negative viscous contribution and a reversal of the mean velocity gradient. To model this behaviour, we propose a model based on a balance between buoyancy and shear production with dissipation, incorporating a linear wall-normal profile for the Reynolds shear stress, a wall-normal-independent buoyancy-production term, and a decomposition of the dissipation into shear-induced and buoyancy-induced contributions. Our model accurately reproduces the DNS mean velocity profiles across the explored $Ra$ range.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [45] [Quantum Homotopy Algorithm for Solving Nonlinear PDEs and Flow Problems](https://arxiv.org/abs/2512.21033)
*Sachin S. Bharadwaj,Balasubramanya Nadiga,Stephan Eidenbenz,Katepalli R. Sreenivasan*

Main category: quant-ph

TL;DR: A near-optimal quantum algorithm for solving time-dependent, dissipative, nonlinear PDEs using quantum homotopy analysis and finite-difference methods with improved complexity scaling.


<details>
  <summary>Details</summary>
Motivation: Quantum algorithms for nonlinear PDEs governing flow problems are challenging to develop but crucial for enhancing the practical usefulness of quantum computing in simulating complex physical phenomena.

Method: Embeds PDEs in truncated high-dimensional linear space using quantum homotopy analysis, discretizes linearized system with finite-difference methods using compact quantum algorithm, adapts to nonlinearity and physics, provides embedding strategy and complexity bounds.

Result: Complexity estimates improve existing approaches in matrix operator norms, condition number, simulation time, and accuracy; introduces physically motivated nonlinearity measure connected to flow Reynolds number; demonstrated with 1D Burgers problem simulations.

Conclusion: Shows potential of hybrid quantum algorithms for simulating practical nonlinear phenomena on near-term and fault-tolerant quantum devices with improved efficiency and robustness.

Abstract: Quantum algorithms to integrate nonlinear PDEs governing flow problems are challenging to discover but critical to enhancing the practical usefulness of quantum computing. We present here a near-optimal, robust, and end-to-end quantum algorithm to solve time-dependent, dissipative, and nonlinear PDEs. We embed the PDEs in a truncated, high dimensional linear space on the basis of quantum homotopy analysis. The linearized system is discretized and integrated using finite-difference methods that use a compact quantum algorithm. The present approach can adapt its input to the nature of nonlinearity and underlying physics. The complexity estimates improve existing approaches in terms of scaling of matrix operator norms, condition number, simulation time, and accuracy. We provide a general embedding strategy, bounds on stability criteria, accuracy, gate counts and query complexity. A physically motivated measure of nonlinearity is connected to a parameter that is similar to the flow Reynolds number $Re_{\textrm{H}}$, whose inverse marks the allowed integration window, for given accuracy and complexity. We illustrate the embedding scheme with numerical simulations of a one-dimensional Burgers problem. This work shows the potential of the hybrid quantum algorithm for simulating practical and nonlinear phenomena on near-term and fault-tolerant quantum devices.

</details>
