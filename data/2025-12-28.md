<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 9]
- [math.AP](#math.AP) [Total: 17]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [math-ph](#math-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A dichotomy of finite element spaces and its application to an energy-conservative scheme for the regularized long wave equation](https://arxiv.org/abs/2512.20737)
*Dimitrios Antonopoulos,Dimitrios Mitsotakis*

Main category: math.NA

TL;DR: Odd-degree finite elements achieve optimal convergence for nonlinear dispersive wave equations, while even-degree elements show reduced accuracy due to super-approximation properties of L²-projections.


<details>
  <summary>Details</summary>
Motivation: To explain the unusual convergence behavior observed in energy-conservative Galerkin discretizations for nonlinear dispersive wave equations, where odd-degree polynomial finite elements achieve optimal convergence while even-degree elements show reduced accuracy.

Method: Analysis connecting the convergence behavior to the structure of finite element spaces, specifically examining the standard L²-projection of derivatives and its super-approximation property for odd polynomial degrees. Application to energy-conservative Galerkin approximation of the regularized long-wave equation with cubic energy functional.

Result: The reduced accuracy for even-degree polynomials is intrinsic to finite element space structure, linked to super-approximation properties of L²-projections that only occur for odd degrees. For the regularized long-wave equation, the scheme conserves mass and energy, approximates impulse with high accuracy, and has established a priori error bounds.

Conclusion: The convergence disparity between odd and even-degree finite elements is fundamental to the mathematical structure of these spaces, with important implications for designing efficient numerical schemes for nonlinear dispersive wave equations.

Abstract: Certain energy-conservative Galerkin discretizations for nonlinear dispersive wave equations have revealed an unusual convergence behavior: optimal convergence is attained when continuous Lagrange finite element spaces of odd polynomial degree are employed, whereas the use of even-degree polynomials leads to reduced accuracy. The present work demonstrates that this behavior is intrinsic to the structure of the finite element spaces themselves. In particular, it is shown to be closely connected to the standard $L^2$-projection of derivatives, which possesses a super-approximation property exclusively for odd polynomial degrees. We also examine the implications of this feature for an energy-conservative Galerkin approximation of the regularized long-wave equation where the energy is a cubic functional. Although the resulting scheme conserves both mass and energy, we further show that the impulse is approximated with high accuracy, and we establish {\em a priori} error bounds for the associated semi-discrete formulation.

</details>


### [2] [On stability of Weak Greedy Algorithm in the presence of noise](https://arxiv.org/abs/2512.20750)
*V. N. Temlyakov*

Main category: math.NA

TL;DR: Theoretical study of greedy algorithm stability, focusing on how small data perturbations affect algorithm outcomes


<details>
  <summary>Details</summary>
Motivation: While convergence and convergence rate are well-studied in greedy approximation theory, stability (robustness to perturbations) is an important but less explored property that needs theoretical investigation

Method: Theoretical analysis of greedy algorithms, specifically examining stability properties when dealing with noisy data perturbations

Result: Presents theoretical results demonstrating that greedy algorithms maintain stability under small data perturbations, showing that minor changes in input don't cause large changes in output

Conclusion: Stability is a crucial property of greedy algorithms alongside convergence and convergence rate, and theoretical analysis shows these algorithms can be robust to noisy data perturbations

Abstract: This paper is devoted to the theoretical study of the efficiency, namely, stability of some greedy algorithms. In the greedy approximation theory researchers are mostly interested in the following two important properties of an algorithm -- convergence and rate of convergence. In this paper we present some results on one more important property of an algorithm -- stability. Stability means that small perturbations do not result in a large change in the outcome of the algorithm. In this paper we discuss one kind of perturbations -- noisy data.

</details>


### [3] [Streamfunction-vorticity formulation for incompressible viscid and inviscid flows on general surfaces](https://arxiv.org/abs/2512.20763)
*Tim Brüers,Christoph Lehrenfeld,Max Wardetzky*

Main category: math.NA

TL;DR: Streamfunction-vorticity formulation for Navier-Stokes/Euler equations on general surfaces including non-simply connected ones, ensuring exactly tangential and incompressible velocity fields with pressure robustness.


<details>
  <summary>Details</summary>
Motivation: Traditional velocity-pressure formulations struggle to guarantee tangential and incompressible velocity fields on general surfaces without increasing computational costs. The harmonic components of velocity on non-simply connected surfaces play a fundamental role that needs proper treatment.

Method: Develops a streamfunction-vorticity formulation that relies only on scalar and finite-dimensional quantities. This approach naturally ensures exactly tangential and incompressible velocity fields while being pressure robust. The formulation is validated through rigorous mathematical equivalence proofs.

Result: The formulation is proven equivalent to the well-understood velocity-pressure formulation under reasonable regularity assumptions. Numerical examples demonstrate the applicability of the approach on general surfaces.

Conclusion: The streamfunction-vorticity formulation provides a key advantage over traditional methods by guaranteeing structural properties (tangential, incompressible velocity fields, pressure robustness) without increasing computational costs, especially important for non-simply connected surfaces.

Abstract: This paper presents a streamfunction-vorticity formulation for the Navier--Stokes and Euler equations on general surfaces. Notably, this includes non-simply connected surfaces, on which the harmonic components of the velocity field play a fundamental role in the dynamics. By relying only on scalar and finite-dimensional quantities, our formulation ensures that the resulting methods give exactly tangential and incompressible velocity fields, while also being pressure robust. Compared to traditional methods based on velocity-pressure formulations, where one can only guarantee these structural properties by increasing the computational costs, this is a key advantage. We rigorously validate our formulation by proving its equivalence to the well understood velocity-pressure formulation under reasonable regularity assumptions. Furthermore, we demonstrate the applicability of the approach with numerical examples.

</details>


### [4] [Computing nonlinear Schrödinger equations with Hermite functions beyond harmonic traps](https://arxiv.org/abs/2512.20840)
*Valeria Banica,Georg Maierhofer,Katharina Schratz*

Main category: math.NA

TL;DR: Hermite basis functions extend from Schrödinger equations with harmonic potential to those without potential, making them suitable for nonlinear dispersive equations on unbounded domains.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that Hermite basis functions, known for their effectiveness in spatial discretization of Schrödinger equations with harmonic potential, maintain their stability properties for Schrödinger equations without potential, enabling their use as a natural basis for nonlinear dispersive equations on unbounded domains.

Method: Extending the application of Hermite basis functions from Schrödinger equations with harmonic potential to those without potential, analyzing their stability properties in this new context.

Result: Hermite basis functions maintain their stability properties when applied to Schrödinger equations without potential, making them suitable for computational approaches to nonlinear dispersive equations on unbounded domains.

Conclusion: Hermite basis functions serve as a natural and stable computational basis for solving nonlinear dispersive equations on unbounded domains, extending their proven utility from harmonic potential Schrödinger equations to more general cases.

Abstract: Hermite basis functions are a powerful tool for spatial discretisation of Schrödinger equations with harmonic potential. In this work we show that their stability properties extend to the simulation of Schrödinger equations without potential, thus leading them as a natural basis for computation of nonlinear dispersive equations on unbounded domains.

</details>


### [5] [Parameter-free inexact block Schur complement preconditioning for linear poroelasticity under a hybrid Bernardi-Raugel and weak Galerkin finite element discretization](https://arxiv.org/abs/2512.20844)
*Weizhang Huang,Zhuoran Wang*

Main category: math.NA

TL;DR: Inexact block Schur complement preconditioners for hybrid discretization of poroelasticity problems achieve mesh- and locking-parameter-independent convergence for both MINRES and GMRES methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of solving linear poroelasticity problems discretized with hybrid methods (Bernardi-Raugel elements for displacement, weak Galerkin for pressure). Under pure Dirichlet boundary conditions for displacement, the system becomes almost singular in the nearly incompressible (locking) regime, hindering efficient iterative solution.

Method: The system is reformulated as a three-field problem with inherent regularization to maintain the original solution while ensuring nonsingularity. The paper analyzes MINRES and GMRES methods preconditioned with inexact block diagonal and triangular Schur complement preconditioners. Both pure Dirichlet and mixed boundary conditions are considered.

Result: Theoretical analysis shows mesh- and locking-parameter-independent convergence for both MINRES and GMRES with the proposed preconditioners for the regularized system. Similar results hold for mixed boundary conditions even without regularization. Numerical experiments in 2D and 3D confirm robustness, and a spinal cord simulation with discontinuous material parameters demonstrates effectiveness.

Conclusion: Inexact block Schur complement preconditioning provides robust, efficient iterative solvers for hybrid-discretized poroelasticity problems, overcoming locking issues and achieving parameter-independent convergence for various boundary conditions.

Abstract: This work investigates inexact block Schur complement preconditioning for linear poroelasticity problems discretized using a hybrid approach: Bernardi-Raugel elements for solid displacement and lowest-order weak Galerkin elements for fluid pressure. When pure Dirichlet boundary conditions are applied to the displacement, the leading block of the resulting algebraic system becomes almost singular in the nearly incompressible (locking) regime, hindering efficient iterative solution. To overcome this, the system is reformulated as a three-field problem with an inherent regularization that maintains the original solution while ensuring nonsingularity. Analysis shows that both the minimal residual (MINRES) and generalized minimal residual (GMRES) methods, when preconditioned with inexact block diagonal and triangular Schur complement preconditioners, achieve convergence independent of mesh size and the locking parameter for the regularized system. Similar theoretical results are established for the situation with displacement subject to mixed boundary conditions, even without regularization. Numerical experiments in 2D and 3D confirm the benefits of regularization under pure Dirichlet conditions and the robustness of the preconditioners with respect to mesh size and the locking parameter in both boundary condition scenarios. Finally, a spinal cord simulation with discontinuous material parameters further illustrates the effectiveness and robustness of the proposed iterative solvers.

</details>


### [6] [Mixed Precision General Alternating-Direction Implicit Method for Solving Large Sparse Linear Systems](https://arxiv.org/abs/2512.21164)
*Jifeng Ge,Bastien Vieublé,Juan Zhang*

Main category: math.NA

TL;DR: Mixed precision GADI method accelerates large sparse linear systems by solving subsystems in low precision while maintaining high precision for residuals and updates, achieving 1.7-3.1× speedups on GPU.


<details>
  <summary>Details</summary>
Motivation: To accelerate solution of large-scale sparse linear systems by reducing computational cost while maintaining high accuracy, leveraging mixed precision computing on modern hardware.

Method: Three-precision GADI formulation with subsystems solved in low precision (Bfloat16/FP32), residuals/solution updates in high precision, error analysis, GPR-based parameter selection, and GPU implementation.

Result: Achieved 2.6×, 1.7×, and 3.1× speedups over full double precision GADI on 2D/3D convection-diffusion and reaction-diffusion problems with up to 1.3×10⁸ unknowns.

Conclusion: Mixed precision GADI effectively accelerates large-scale linear solvers while maintaining accuracy, with systematic parameter selection and proven convergence guarantees.

Abstract: In this article, we introduce a three-precision formulation of the General Alternating-Direction Implicit method (GADI) designed to accelerate the solution of large-scale sparse linear systems $Ax=b$. GADI is a framework that can represent many existing Alternating-Direction Implicit (ADI) methods. These methods are a class of linear solvers based on a splitting of $A$ such that the solution of the original linear system can be decomposed into the successive computation of easy-to-solve structured subsystems. Our proposed mixed precision scheme for GADI solves these subsystems in low precision to reduce the overall execution time while computing the residual and solution update in high precision to enable the solution to converge to high accuracy. We develop a rounding error analysis of mixed precision GADI that establishes the rates of convergence of the forward and backward errors to certain limiting accuracies. Our analysis also highlights the conditions on the splitting matrices under which mixed precision GADI is guaranteed to converge for a given set of precisions. We then discuss a systematic and robust strategy for selecting the GADI regularization parameter $α$, whose adjustment is critical for performance. Specifically, our proposed strategy makes use of a Gaussian Process Regression (GPR) model trained on a dataset of low-dimensional problems to initialize $α$. Finally, we proceed to a performance analysis of mixed precision GADI on an NVIDIA A100 GPU to validate our approach. Using low precision (Bfloat16 or FP32) to solve the subsystems, we obtain speedups of $2.6\times$, $1.7\times$, and $3.1\times$ over a full double precision GADI implementation on large-scale 2D, 3D convection-diffusion and complex reaction-diffusion problems (up to $1.3\times 10^{8}$ unknowns), respectively.

</details>


### [7] [A mixed finite element method for the stochastic Boussinesq equations with multiplicative noise](https://arxiv.org/abs/2512.21297)
*Liet Vo*

Main category: math.NA

TL;DR: A fully discrete mixed finite element method for stochastic Boussinesq system with multiplicative noise, using spatial mixed FEM and temporal semi-implicit Euler-Maruyama scheme, with error bounds and convergence proofs.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze a fully discrete numerical method for solving stochastic Boussinesq systems driven by multiplicative noise, which are important in modeling fluid dynamics with thermal effects under random perturbations.

Method: Combines standard mixed finite element method for spatial discretization with semi-implicit Euler-Maruyama scheme for temporal discretization. Uses localization technique with high-moment stability estimates to analyze errors.

Result: Established error bounds for velocity, pressure, and temperature approximations. Proved convergence in probability for the fully discrete method in both L² and H¹-type norms. Numerical experiments validate theoretical estimates.

Conclusion: The proposed fully discrete mixed finite element method is effective for stochastic Boussinesq systems with multiplicative noise, with proven convergence properties and validated by numerical experiments.

Abstract: This work investigates a fully discrete mixed finite element method for the stochastic Boussinesq system driven by multiplicative noise. The spatial discretization is performed using a standard mixed finite element method, while the temporal discretization is based on a semi-implicit Euler-Maruyama scheme. By combining a localization technique with high-moment stability estimates, we establish error bounds for the velocity, pressure, and temperature approximations. As a direct consequence, we prove convergence in probability for the fully discrete method in both $L^2$ and $H^1$-type norms. Several numerical experiments are presented to validate the theoretical error estimates and demonstrate the effectiveness of the proposed scheme.

</details>


### [8] [FORCE-$α$ Numerical Fluxes within the Arbitrary High Order Semidiscrete WENO-DeC Framework: A Competitive Alternative to Upwind Fluxes](https://arxiv.org/abs/2512.21306)
*Lorenzo Micalizzi,Eleuterio Toro*

Main category: math.NA

TL;DR: FORCE-α numerical fluxes are systematically evaluated in high-order finite volume schemes for hyperbolic PDEs, showing they're competitive alternatives to upwind fluxes despite ignoring Riemann problem structure.


<details>
  <summary>Details</summary>
Motivation: Centered numerical fluxes like FORCE-α offer flexibility for complicated hyperbolic systems where exact Riemann solvers may be impossible to construct or computationally expensive, providing an alternative to classical upwind fluxes.

Method: Uses a semidiscrete finite volume framework with WENO spatial reconstruction and Deferred Correction time discretization up to order 7, applying FORCE-α numerical fluxes to the ideal Euler equations in 1D and 2D.

Result: FORCE-α numerical fluxes perform competitively within high-order frameworks, with increasing order of accuracy reducing differences between results from different numerical fluxes.

Conclusion: FORCE-α numerical fluxes are a viable and competitive alternative to classical upwind fluxes in high-order finite volume schemes for hyperbolic PDEs, offering flexibility for complex systems.

Abstract: This work systematically investigates the performance of FORCE--$α$ numerical fluxes within an arbitrary high order semidiscrete finite volume (FV) framework for hyperbolic partial differential equations (PDEs). Such numerical fluxes have been recently introduced by Toro, Saggiorato, Tokareva, and Hidalgo (Journal of Computational Physics, 416, 2020), and constitute a family of centred fluxes obtained from a suitable modification of First--Order Centred (FORCE) numerical fluxes. In contrast with upwind fluxes, such as Rusanov, Harten--Lax--van Leer (HLL) or the exact Riemann solver (RS) numerical flux, centred ones do not consider in any way the structure of the Riemann problem at cell interfaces. Adopting centred numerical fluxes leads to a high level of flexibility of the resulting numerical schemes, for example in the context of complicated hyperbolic systems, for which RSs may be impossible to construct or computationally expensive.
  The baseline framework adopted in this investigation is a FV semidiscrete approach with Weighted Essentially Non--Oscillatory (WENO) spatial reconstruction and Deferred Correction (DeC) time discretization, and results are reported up to order 7. Previous investigations involving the same framework have established that increasing the order of accuracy tends to decrease the differences in the results obtained through different numerical fluxes. The goal of this paper is to show that the employment of FORCE--$α$ numerical fluxes within such a framework is a competitive alternative to the adoption of more classical upwind fluxes. The hyperbolic system considered for this investigation is the ideal Euler equations in one and two space dimensions.

</details>


### [9] [Variationally correct operator learning: Reduced basis neural operator with a posteriori error estimation](https://arxiv.org/abs/2512.21319)
*Yuan Qiu,Wolfgang Dahmen,Peng Chen*

Main category: math.NA

TL;DR: This paper develops a variationally correct operator learning framework using FOSLS objectives that guarantee solution error equivalence, proposes Reduced Basis Neural Operators for function space conformity, and provides rigorous convergence analysis with superior accuracy in PDE-compliant norms.


<details>
  <summary>Details</summary>
Motivation: Standard PDE-residual losses in neural operators often lack variational correctness - small residuals don't guarantee small solution errors due to non-compliant norms or ad hoc boundary condition penalties. There's a need for a mathematically rigorous framework that ensures norm equivalence between residuals and solution errors.

Method: Develops a variationally correct operator learning framework using first-order system least-squares (FOSLS) objectives with provable equivalence to solution errors. Incorporates mixed boundary conditions via variational lifts. Proposes Reduced Basis Neural Operator (RBNO) that predicts coefficients for pre-computed conforming reduced basis to ensure variational stability by design.

Result: Provides rigorous convergence analysis bounding total error by finite element discretization bias, reduced basis truncation error, neural network approximation error, and statistical estimation errors. Numerical benchmarks validate theoretical bounds and show superior accuracy in PDE-compliant norms compared to standard baselines. The residual loss serves as a reliable computable a posteriori error estimator.

Conclusion: The proposed framework addresses the variational correctness gap in neural operator learning by ensuring norm equivalence between residuals and solution errors through FOSLS objectives and RBNO architecture, providing both theoretical guarantees and practical computational advantages with reliable error estimation.

Abstract: Minimizing PDE-residual losses is a common strategy to promote physical consistency in neural operators. However, standard formulations often lack variational correctness, meaning that small residuals do not guarantee small solution errors due to the use of non-compliant norms or ad hoc penalty terms for boundary conditions. This work develops a variationally correct operator learning framework by constructing first-order system least-squares (FOSLS) objectives whose values are provably equivalent to the solution error in PDE-induced norms. We demonstrate this framework on stationary diffusion and linear elasticity, incorporating mixed Dirichlet-Neumann boundary conditions via variational lifts to preserve norm equivalence without inconsistent penalties. To ensure the function space conformity required by the FOSLS loss, we propose a Reduced Basis Neural Operator (RBNO). The RBNO predicts coefficients for a pre-computed, conforming reduced basis, thereby ensuring variational stability by design while enabling efficient training. We provide a rigorous convergence analysis that bounds the total error by the sum of finite element discretization bias, reduced basis truncation error, neural network approximation error, and statistical estimation errors arising from finite sampling and optimization. Numerical benchmarks validate these theoretical bounds and demonstrate that the proposed approach achieves superior accuracy in PDE-compliant norms compared to standard baselines, while the residual loss serves as a reliable, computable a posteriori error estimator.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [10] [Dispersive decay for the Inter-critical nonlinear Schrödinger equation in $\mathbb{R}^3$](https://arxiv.org/abs/2512.20683)
*Boyu Jiang,Jiawei Shen,Kexue Li*

Main category: math.AP

TL;DR: The paper establishes uniform decay estimates for long-time dynamics of 3D NLS solutions in mass-supercritical, energy-subcritical regime with initial data in critical homogeneous Sobolev space.


<details>
  <summary>Details</summary>
Motivation: To extend previous results on the Cauchy problem for nonlinear Schrödinger equations by obtaining uniform decay estimates for long-time dynamics in the challenging mass-supercritical, energy-subcritical regime in three dimensions.

Method: Analysis of the Cauchy problem for NLS in 3D with initial data in the critical homogeneous Sobolev space $\dot{H}^{s_c}(\mathbb{R}^3)$ where $s_c = \frac{5}{6}$, focusing on establishing uniform decay estimates for long-time dynamics.

Result: Obtained uniform decay estimate for long-time dynamics of solutions, extending previous results in the field.

Conclusion: The paper successfully extends the understanding of long-time behavior for NLS in the mass-supercritical, energy-subcritical regime by establishing uniform decay estimates for solutions with initial data in the critical Sobolev space.

Abstract: This paper investigates the Cauchy problem for the nonlinear Schrödinger equation (NLS) in the mass-supercritical and energy-subcritical regime within three spatial dimensions. For initial data in the critical homogeneous Sobolev space $\dot{H}^{s_c}(\mathbb{R}^3)$ (where $s_c = \frac{5}{6}$), we get a uniform decay estimate for the long-time dynamics of solutions, which extends the previous results.

</details>


### [11] [On a Hamilton-Jacobi PDE theory for hydrodynamic limit of action minimizing collective dynamics](https://arxiv.org/abs/2512.20809)
*Jin Feng*

Main category: math.AP

TL;DR: Multi-scale convergence theory for Hamilton-Jacobi PDEs in probability measure spaces, derived from hydrodynamic limits of N-particle Lagrangian dynamics, using variational approaches and weak K.A.M. theory.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous convergence results for Hamilton-Jacobi PDEs in probability measure spaces that arise from hydrodynamic limits of deterministic particle systems, addressing infinite-dimensional singular averaging structures.

Method: Develops indirect variational approach applying finite-dimensional weak K.A.M. theory to infinite-dimensional setting; uses viscosity solution techniques, projection methods for equations with submetry structure, multi-scale convergence in metric spaces, and comparison principles in probability measure spaces.

Result: Establishes multi-scale convergence theory for Hamilton-Jacobi PDEs in probability measure spaces, identifies limiting Hamiltonian, and proves rigorous convergence of solutions for weakly interacting particle systems.

Conclusion: Successfully extends finite-dimensional weak K.A.M. theory to infinite-dimensional probability measure spaces, providing new analytical tools for studying hydrodynamic limits and Hamilton-Jacobi equations in metric spaces with curvature bounds.

Abstract: We establish multi-scale convergence theory for a class of Hamilton-Jacobi PDEs in space of probability measures. They arise from context of hydrodynamic limit of N-particle deterministic action minimizing (global) Lagrangian dynamics.
  From a Lagrangian point of view, this can also be viewed as a limit result on two scale convergence of action minimizing probability-measure-valued paths. However, we focus on the Hamiltonian formulation here mostly. We derive and study convergence of the associated abstract but scalar Hamilton-Jacobi equations, defined in space of probability measures. There is an infinite dimensional singular averaging structure within these equations. We develop an indirect variational approach to apply finite dimensional weak K.A.M. theory to such infinite dimensional setting here. With a weakly interacting particle assumption, the averaging step only involves that of individual particles, which is implicitly but rigorously treated using the weak K.A.M. theory. Consequently, we can close the above mentioned averaging step by identifying limiting Hamiltonian, and arrive at a rigorous convergence result on solutions of the nonlinear PDEs in space of probability measures.
  In technical development parts of the paper, we devise new viscosity solution techniques regarding projection of equations with a submetry structure in state space, multi-scale convergence for certain abstract Hamilton-Jacobi equations in metric spaces, as well as comparison principles for equations in space of probability measures. The space of probability measure we consider is a special case of Alexandrov metric space with curvature bounded from below. Since some results are better explained in such metric space setting, we also develop some techniques in the general settings which are of independent interests.

</details>


### [12] [Infinitely many solutions and asymptotics for resonant oscillatory problems](https://arxiv.org/abs/2512.20816)
*Philip Korman,Dieter S. Schmidt*

Main category: math.AP

TL;DR: The paper proves existence of infinitely many solutions for oscillatory resonant PDEs on balls/rectangles, develops asymptotic formulas, and validates with numerical computations.


<details>
  <summary>Details</summary>
Motivation: To study oscillatory resonant problems for semilinear PDEs where the first harmonic of the right-hand side is not required to be zero or small, addressing existence and structure of solutions.

Method: Analytical approach for Dirichlet problems on balls and rectangles in R^n, proving existence of infinitely many solutions, deriving asymptotic formulas in terms of first harmonic, and implementing numerical validation.

Result: Existence of infinitely many solutions is established, asymptotic formulas are derived, and numerical computations demonstrate accuracy of the asymptotic approximations.

Conclusion: The paper successfully addresses oscillatory resonant PDE problems without smallness assumptions on the first harmonic, providing both theoretical existence results and practical computational methods with verified accuracy.

Abstract: For a class of oscillatory resonant problems, involving Dirichlet problems for semilinear PDE's on balls and rectangles in $R^n$, we show the existence of infinitely many solutions, and study the global solution set. The first harmonic of the right hand side is not required to be zero, or small. We also derive asymptotic formulas in terms of the first harmonic of solutions, and illustrate their accuracy by numerical computations. The numerical method is explained in detail.

</details>


### [13] [Uniqueness for the Homogeneous Landau-Coulomb Equation in $L^{3/2}$](https://arxiv.org/abs/2512.20899)
*Maria Pia Gualdani,Weiran Sun*

Main category: math.AP

TL;DR: Proves uniqueness of H-solutions to homogeneous Landau-Coulomb equation in critical space L^{3/2}, completing global well-posedness theory.


<details>
  <summary>Details</summary>
Motivation: To establish uniqueness of rough solutions to nonlinear kinetic equations, specifically completing the global well-posedness theory for the homogeneous Landau-Coulomb equation in the critical space L^{3/2}(ℝ³).

Method: Uses the M-operator technique (developed in previous works) applied to the space-homogeneous case, where the M-operator is taken as a Bessel potential operator. Proves uniqueness for solutions satisfying specific regularity conditions.

Result: Proves uniqueness of H-solutions satisfying ⟨v⟩^{k₀}f ∈ C([0,T]; L^{3/2}(ℝ³)) and ⟨v⟩^{-3/2}∇_v((⟨v⟩^{k₀}f)^{3/4}) ∈ L²((0,T)×ℝ³) for any k₀ ≥ 5. Shows solutions constructed in previous work (GGL25) are unique.

Conclusion: Completes the global well-posedness theory for the homogeneous Landau-Coulomb equation in the critical space L^{3/2}(ℝ³), demonstrating the effectiveness of the M-operator technique for establishing uniqueness of rough solutions to nonlinear kinetic equations.

Abstract: We prove the uniqueness of $H$-solutions to the homogeneous Landau-Coulomb equation satisfying $\langle v \rangle^{k_0} f \in C([0, T]; L^{3/2}(\mathbb{R}^3))$ and $\langle v \rangle^{-3/2} \nabla_v ((\langle v \rangle^{k_0} f)^{3/4}) \in L^2((0, T) \times \mathbb{R}^3)$ for any $k_0 \geq 5$. In particular, this shows that the solutions constructed in~\cite{GGL25} are unique. The present work thus completes the global well-posedness theory in the critical space $L^{3/2}(\mathbb{R}^3)$. Our proof is part of a broader effort to use the $\mathcal{M}$-operator technique developed in~\cite{AGS2025, AMSY2020} to establish the uniqueness of rough solutions to nonlinear kinetic equations. When applied to the space-homogeneous case, the $\mathbb{M}$-operator can be taken simply as a Bessel potential operator.

</details>


### [14] [Quantitative bounds for Hölder exponents in the Krylov--Safonov and Evans--Krylov theories](https://arxiv.org/abs/2512.21025)
*Jongmyeong Kim,Se-Chan Lee*

Main category: math.AP

TL;DR: Quantitative bounds for Hölder exponents in Krylov-Safonov and Evans-Krylov theories when ellipticity ratio is close to one.


<details>
  <summary>Details</summary>
Motivation: To establish precise quantitative bounds for Hölder regularity exponents in two important PDE theories (Krylov-Safonov and Evans-Krylov) when the ellipticity ratio approaches one, which corresponds to nearly uniformly elliptic equations.

Method: Uses Ishii-Lions method for Krylov-Safonov theory and a Schauder-type perturbation argument for Evans-Krylov theory.

Result: Establishes quantitative bounds for Hölder exponents in both theories when ellipticity ratio is close to one.

Conclusion: Provides precise quantitative control over Hölder regularity in near-uniformly elliptic settings using different analytical approaches for the two classical theories.

Abstract: We establish quantitative bounds for Hölder exponents in the Krylov--Safonov and Evans--Krylov theories when the ellipticity ratio is close to one. Our analysis relies on the Ishii--Lions method for the Krylov--Safonov theory and a Schauder-type perturbation argument for the Evans--Krylov theory.

</details>


### [15] [Calderón-Zygmund gradient estimates for $p$-Laplace systems with BMO complex coefficients](https://arxiv.org/abs/2512.21036)
*Van-Chuong Quach,Thanh-Nhan Nguyen,Minh-Phuong Tran*

Main category: math.AP

TL;DR: Global gradient bounds for degenerate elliptic systems with complex coefficients having small BMO coefficients, extending regularity theory beyond VMO conditions.


<details>
  <summary>Details</summary>
Motivation: To establish global gradient bounds for divergence-form degenerate elliptic systems with complex-valued coefficients, where the leading coefficients are only required to be sufficiently small in BMO (weaker than VMO). This extends previous work on existence and uniqueness to include regularity estimates.

Method: Proves a global Calderón-Zygmund-type estimate for weak solutions under natural, less restrictive assumptions on complex-valued coefficients. The approach builds on recent work that established existence and uniqueness under similar conditions, now extending to regularity theory.

Result: Establishes global gradient bounds (Calderón-Zygmund-type estimates) for weak solutions, from which Morrey-space regularity follows as a consequence. This provides regularity results under weaker coefficient conditions (small BMO rather than VMO).

Conclusion: The paper contributes to better understanding solution behavior for complex-valued elliptic systems and extends regularity theory beyond traditional VMO conditions, representing progress in the series of works on complex-valued PDEs.

Abstract: This work is concerned with global gradient bounds for a class of divergence-form degenerate elliptic systems with complex-valued coefficients. Notably, the leading coefficients are merely required to be sufficiently small in BMO, which is strictly weaker than the VMO condition. In the complex setting, the well-posedness of this problem was recently investigated in [W. Kim, M. Vestberg, Existence, uniqueness and regularity for elliptic $p$-Laplace systems with complex coefficients,arXiv:2503.18932], where the authors established a strong accretivity condition on the leading coefficients, and this structural condition allows them to derive Schauder-type estimates for weak solutions. In our study, it has already been observed that gaining existence and uniqueness of weak solutions is possible under a natural and less restrictive assumption on the complex-valued coefficients. Following this direction, we prove a global Caderón-Zygmund-type estimate for weak solutions, from which the Morrey-space regularity follows as a consequence. This paper is a contribution to the better understanding of solution behavior and may be viewed as part of a series of works aimed at extending regularity theory in the complex-valued setting.

</details>


### [16] [A Unified Truncation Method for Infinitely Many Solutions Without Symmetry](https://arxiv.org/abs/2512.21119)
*Anouar Bahrouni*

Main category: math.AP

TL;DR: The paper introduces a refined variational truncation method that proves existence of infinitely many solutions for nonlinear problems without symmetry, covering semilinear elliptic PDEs, nonvariational elliptic PDEs with gradient dependence, and periodic Hamiltonian systems on the real line.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in proving multiplicity of solutions for nonlinear problems without symmetry assumptions, particularly addressing long-standing difficulties in nonvariational elliptic PDEs with gradient dependence and the collapse problem in periodic Hamiltonian systems on the real line.

Method: A carefully designed truncation methodology that systematically separates solutions, combined with variational truncation for semilinear elliptic PDEs, truncation with iterative scheme for nonvariational elliptic PDEs, and limiting procedures for periodic Hamiltonian systems to prevent solution collapse.

Result: Three major advances: 1) Infinite sequences of positive and negative solutions for semilinear elliptic PDEs, 2) First proof of infinitely many solutions for nonvariational elliptic PDEs with gradient dependence, 3) Multiple distinct solutions for periodic Hamiltonian systems on the whole real line without collapse.

Conclusion: The unified truncation methodology provides a robust and versatile tool for addressing multiplicity problems across variational and non-variational PDEs as well as infinite dimensional dynamical systems in the absence of symmetry.

Abstract: This paper establishes the existence of infinitely many solutions for nonlinear problems without any symmetry, achieving three major advances. First, in the setting of semilinear elliptic PDEs, we introduce a refined variational truncation method that yields infinite sequences of positive as well as negative solutions. Second and most notably, we resolve a long-standing and difficult problem for nonvariational elliptic PDEs with gradient dependence. By combining our truncation method with an iterative scheme, we prove, for the first time, the existence of infinitely many solutions for this class of PDEs. Third, we overcome a central difficulty for periodic Hamiltonian systems on the real line: we show that the multiplicity of solutions, constructed on a sequence of finite intervals, survives in the limit; in other words, no collapse occurs, and we obtain multiple distinct solutions on the whole real line.
  The core novelty lies in a carefully designed truncation methodology that systematically separates solutions and remains effective across variational and non-variational PDEs as well as infinite dimensional dynamical systems. This unified perspective provides a robust and versatile tool for addressing multiplicity problems in the absence of symmetry.

</details>


### [17] [Equilibrium Configurations and their Uniqueness in a Fluid-Solid Interaction Problem](https://arxiv.org/abs/2512.21130)
*D. Bonheure,G. P. Galdi,C. Patriarca*

Main category: math.AP

TL;DR: Existence of large-scale equilibrium configurations and uniqueness of small-scale solutions for Navier-Stokes fluid interacting with a rigid body under spring forces, driven by uniform far-field fluid velocity.


<details>
  <summary>Details</summary>
Motivation: To study the equilibrium behavior of coupled fluid-structure interaction systems where a rigid body with rotational freedom interacts with a Navier-Stokes fluid, particularly when driven by uniform far-field flow conditions.

Method: Mathematical analysis of the coupled Navier-Stokes equations interacting with rigid body dynamics under spring forces and restoring moments, addressing the nonlinearity introduced by body rotation around a given axis.

Result: Demonstrated existence of equilibrium configurations in the "large" (global existence) and uniqueness in the "small" (local uniqueness) for this coupled fluid-body system.

Conclusion: The coupled Navier-Stokes fluid-rigid body system with spring forces admits equilibrium solutions, with global existence and local uniqueness established despite the nonlinear challenges posed by body rotation.

Abstract: We demonstrate existence in the ``large" and uniqueness in the ``small" of equilibrium configurations for the coupled system consisting of a Navier-Stokes fluid interacting with a rigid body subjected to spring forces and restoring moments. The driving mechanism is a uniform, given velocity field of the fluid at large spatial distances from the body. The main difficulty in the proof of the above properties arises from the fact that the body can rotate around a given axis, which produces a highly nonlinear problem.

</details>


### [18] [Existence and non-existence phenomena for nonlinear elliptic equations with $L^1$ data and singular reactions](https://arxiv.org/abs/2512.21131)
*Francescantonio Oliva,Francesco Petitta,Matheus F. Stapenhorst*

Main category: math.AP

TL;DR: The paper studies existence and non-existence of solutions for singular elliptic boundary value problems with p-Laplacian operator and singular term, establishing threshold values μ₀ and μ₀* for solution existence.


<details>
  <summary>Details</summary>
Motivation: To extend the celebrated work of Diaz, Morel and Oswald (1997) on singular elliptic problems from the Laplacian case (p=2) to the p-Laplacian case (p≠2), and to provide new results even for p=2 when the singular term has critical growth near zero (γ=1).

Method: Analysis of singular elliptic boundary value problems with p-Laplacian operator, using techniques from nonlinear PDEs and variational methods to establish threshold existence results based on the parameter μ.

Result: For any positive f∈L¹(Ω), problem (1) is solvable for any μ>μ₀>0 (with μ₀ large enough), and no finite energy solution exists if 0<μ<μ₀* (with μ₀* small enough).

Conclusion: The paper successfully extends previous results to the p-Laplacian case and provides new insights for critical singular growth, establishing clear threshold behavior for solution existence in singular elliptic problems.

Abstract: We study existence and non-existence of solutions for singular elliptic boundary value problems as \begin{equation}\label{eintro}\begin{cases}\tag{1}
  \displaystyle -Δ_p u+ \frac{a(x)}{u^γ}=μf(x) \ &\text{ in }Ω, \newline
  u>0&\text{ in }Ω, \newline
  u = 0 \ &\text{ on }
  \partialΩ,
  \end{cases} \end{equation} where $Ω$ is a smooth bounded open subset of $\mathbb{R}^N$ ($N\ge 2$), $Δ_p u$ is the $p$-Laplacian with $p>1$, $0<γ\leq 1$, and $a\geq0$ is bounded and non-trivial. For any positive $ f\in L^{1}(Ω)$ we show that problem \eqref{eintro} is solvable for any $μ>μ_0>0$, for some $μ_0$ large enough. As a reciprocal outcome we also show that no finite energy solution exists if $0<μ<μ_{0*}$, for some small $μ_{0*}$.
  This paper extends the celebrated one of J. I. Diaz, J. M. Morel and L. Oswald ([16]) to the case $p\neq2$. Our result is also new for $p=2$ provided the singular term has a critical growth near zero (i.e. $γ=1$).

</details>


### [19] [Well-posedness and the Łojasiewicz-Simon inequality in the asymptotic analysis of a nonlinear heat equation with constraints of finite codimension](https://arxiv.org/abs/2512.21158)
*Ashish Bawalia,Zdzisław Brzeźniak,Manil T. Mohan,Piotr Rybka*

Main category: math.AP

TL;DR: Global well-posedness and asymptotic convergence of norm-preserving nonlinear heat equation on Poincaré domains using m-accretive evolution theory and Łojasiewicz-Simon inequality.


<details>
  <summary>Details</summary>
Motivation: To establish global existence and analyze long-term behavior of strong solutions to an L²-norm preserving nonlinear heat equation with constraints, providing an alternative approach to such constrained evolution problems.

Method: Modifies nonlinearity and uses abstract theory for m-accretive evolution equations to prove global strong solution existence. Employs resolvent ideas and Yosida approximation for regularity. Uses Łojasiewicz-Simon gradient inequality on Hilbert submanifold for asymptotic analysis.

Result: Proves existence of global strong solution for nonlinear heat equation with norm constraint. Shows solution converges to stationary state in W^{2,q} ∩ W^{1,q}_0 spaces for bounded domains with specific parameter ranges.

Conclusion: Provides alternative method for establishing global existence and analyzing long-term behavior of unique strong solutions to L²-norm preserving nonlinear heat equations, demonstrating convergence to equilibrium using geometric analysis on constraint manifolds.

Abstract: We establish the global well-posedness of the $D(A)-$valued strong solution to a nonlinear heat equation with constraints on a \textit{Poincaré domain} $\bO\subset \R^d$ whose boundary is of class $C^2$. Consider the following nonlinear heat equation
  \begin{align*}
  \frac{\partial u}{\partial t} - Δu + |u|^{p-2}u = 0,
  \end{align*}
  projected onto the tangent space $T_u\bM$, where
  $\mathcal{M}:=\left\{u\in L^2(\bO):\|u\|_{L^2(\bO)}=1\right\}$ is a submanifold of $L^2(\bO)$. The nonlinearity exponent satisfies $2\le p < \infty$ for $1\leq d\leq 4$ and $2 \le p \le \frac{2d-4}{d-4}$ for $d \ge 5$. The solution is constrained to lie within $\mathcal{M}$ which encodes the norm-preserving constraint. By modifying the nonlinearity and exploiting the abstract theory for \textit{$m-$accretive }evolution equations, we prove the existence of a global strong solution.
  Using {resolvent-idea } and the \textit{Yosida approximation} method, we derive regularity results. In the asymptotic analysis, $\bO$ is restricted to bounded domains with even $p$
  and $1\le d \le 3$. For any initial data in $D(A) \cap \mathcal{M}$, we apply the \textit{Łojasiewicz-Simon gradient inequality} on a Hilbert submanifold [F. Rupp, \textit{J. Funct. Anal.}, 279(8), 2020], to demonstrate that the unique global strong solution converges in $W^{2,q}(\bO) \cap W^{1,q}_0(\bO)$ to a stationary state, where $2 \le q < \frac{2d}{d + 4 - 4β}$ and $1 < β< \frac{3}{2}$.
  This work proposes an alternative method for establishing the global existence and analyzing long-term behavior of the unique strong solution to an $L^2-$norm preserving nonlinear heat equation.

</details>


### [20] [Optimal Hardy-weights for the Finsler $p$-Dirichlet integral with a potential](https://arxiv.org/abs/2512.21162)
*Yongjun Hou*

Main category: math.AP

TL;DR: The paper constructs optimal Hardy-weights for Finsler p-Dirichlet integrals with and without potentials on domains with a removed point.


<details>
  <summary>Details</summary>
Motivation: Hardy inequalities are fundamental in analysis and PDEs, but constructing optimal Hardy-weights (weights that make the inequality sharp) for Finsler p-Dirichlet integrals with potentials is challenging, especially on domains with singularities like removed points.

Method: The authors work with Finsler p-Dirichlet integrals on domains Ω* = Ω \ {x̂} and Ω, where H(x,·) are parameterized norms. They construct optimal Hardy-weights using techniques from potential theory and functional analysis, working within a subspace of local Morrey spaces for the potential V.

Result: The main result is the construction of optimal Hardy-weights for both the Finsler p-Dirichlet integral on Ω* and the version with potential V on Ω, under specified conditions on n, p, Ω, and V.

Conclusion: The paper provides explicit constructions of optimal Hardy-weights for Finsler p-Dirichlet operators with and without potentials, extending classical Hardy inequality theory to more general Finsler settings and domains with singularities.

Abstract: Fix an integer $n\geq 2$, an exponent $1<p<\infty$, and a domain $Ω\subseteq\mathbb{R}^{n}$. Let $Ω^{*}\triangleqΩ\setminus\{\hat{x}\}$ where $\hat{x}\inΩ$. Under some further conditions, we construct optimal Hardy-weights for the Finsler $p$-Dirichlet integral $$Q_{0}[φ;Ω^{*}]\triangleq\int_{Ω^{*}}H(x,\nabla φ)^{p}\,\mathrm{d}x\quad \mbox{on}\quad C^{\infty}_{c}(Ω^{*}),$$ and the Finsler $p$-Dirichlet integral with a potential $$Q_{V}[φ;Ω]\triangleq\int_Ω\left(H(x,\nabla φ)^{p}+ V|φ|^{p}\right)\,\mathrm{d}x\quad \mbox{on}\quad C^{\infty}_{c}(Ω),$$where $H(x,\cdot)$ is a family of norms on $\mathbb{R}^{n}$ parameterized by $x\inΩ^{*}$ or $x\inΩ$, respectively, and the potential $V$ lies in a subspace $\widehat{M}^{q}_{\rm loc}(p;Ω)$ of a local Morrey space $M^{q}_{\rm loc}(p;Ω)$.

</details>


### [21] [Navier-Stokes-Cahn-Hilliard system in a $3$D perforated domain with free slip and source term: Existence and homogenization](https://arxiv.org/abs/2512.21171)
*Amartya Chakrabortty,Haradhan Dutta,Hari Shankar Mahato*

Main category: math.AP

TL;DR: Analysis of a diffuse-interface model for binary mixtures in perforated porous media, proving existence of weak solutions and deriving two distinct homogenized models depending on capillarity strength scaling.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of binary incompressible mixtures in periodically perforated porous media, particularly how microscopic flow and phase separation interact and how this scales up to macroscopic models.

Method: Two-part analysis: (1) Prove existence of weak solutions to the microscopic Navier-Stokes-Cahn-Hilliard system with variable viscosity, non-conservative source term, and mixed boundary conditions; (2) Perform periodic homogenization as ε→0 to derive effective macroscopic models.

Result: Two distinct homogenized models emerge depending on capillarity strength scaling: (i) Stokes-Cahn-Hilliard system without macroscopic convection for vanishing capillarity (λ=0); (ii) Full Navier-Stokes-Cahn-Hilliard system with nonlinear convection and advective transport for balanced capillarity (λ∈(0,∞)). Also establishes convergence of microscopic free energy to homogenized energy functional.

Conclusion: The scaling of capillarity strength relative to pore size critically determines the macroscopic behavior, leading to fundamentally different effective models that either preserve or suppress macroscopic fluid convection in porous media.

Abstract: We study a diffuse-interface model for a binary incompressible mixture in a periodically perforated porous medium, described by a time-dependent Navier-Stokes-Cahn-Hilliard (NSCH) system posed on the pore domain $Ω_p^\varepsilon\subset\mathbb{R}^3$. The microscopic model involves a variable viscosity tensor, a non-conservative source term in the Cahn--Hilliard equation, and mixed boundary conditions: no-slip on the outer boundary and Navier slip with zero tangential stress on the surfaces of the solid inclusions. The capillarity strength $λ^\varepsilon>0$ depends on the microscopic scale $\varepsilon>0$.
  The analysis consists of two main parts. First, for each fixed $\varepsilon>0$, we prove the existence of a weak solution on a finite time interval $(0,T)$ and derive a priori estimates that are uniform with respect to $\varepsilon$ (and $λ^\varepsilon$). Second, we perform the periodic homogenization for the perforated setting, a limit $\varepsilon\to0$. Depending on the limit value $λ$ of the capillarity strength $λ^\varepsilon$, we obtain two distinct effective models: (i) in the vanishing capillarity regime $λ=0$, the limit system is of Stokes-Cahn-Hilliard type, with no macroscopic convection or advection; (ii) in the balanced regime $λ\in(0,+\infty)$, we derive a Navier-Stokes-Cahn-Hilliard system with nonlinear convection and advective transport of the phase field at the macroscopic scale. Finally, we establish the convergence of the microscopic free energy to a homogenized energy functional satisfying an analogous dissipation law.

</details>


### [22] [Long-Time Existence and Behavior of Solutions to the Inhomogeneous Kinetic FPU Equation](https://arxiv.org/abs/2512.21187)
*Haoling Xiang*

Main category: math.AP

TL;DR: The paper develops a functional framework to study the inhomogeneous kinetic Fermi-Pasta-Ulam equation, showing that dispersion effects from spatial transport extend solution lifespan from quadratic to quartic time scales.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the interplay between spatial transport and degeneracies in the collision operator of the kinetic FPU equation, which describes phonon density distributions with four-phonon interactions.

Method: Develops a functional framework capturing spatial transport and collision operator degeneracies, uses dispersive estimates for transport flow to quantify decay effects, and obtains improved bounds for the nonlinear collision operator.

Result: Shows that small solutions near vacuum can be propagated on significantly longer time scales than conservation laws alone would predict, extending classical quadratic lifespan to quartic time scale through dispersion effects.

Conclusion: Dispersion generated by spatial propagation provides a mechanism to overcome degeneracies in the collision operator, enabling longer existence times for solutions to the inhomogeneous kinetic FPU equation.

Abstract: We study the inhomogeneous kinetic Fermi-Pasta-Ulam (FPU) equation, a nonlinear transport equation describing the evolution of phonon density distributions with four-phonon interactions. The equation combines free transport in physical space with a nonlinear collision operator acting in momentum space and exhibiting structural degeneracies. We develop a functional framework that captures the interplay between spatial transport and the degeneracies arising in the collision operator. A key ingredient of the analysis is a dispersive estimate for the transport flow, which quantifies decay effects generated by spatial propagation. Using this dispersive mechanism, we obtain improved bounds for the nonlinear collision operator and show that small solutions near the vacuum can be propagated on time scales significantly longer than those dictated by conservation laws alone. In particular, dispersion allows one to extend the classical quadratic lifespan to a quartic time scale.

</details>


### [23] [Green's Function and Solution Representation for a Boundary Value Problem Involving the Prabhakar Fractional Derivative](https://arxiv.org/abs/2512.21259)
*Erkinjon Karimov,Doniyor Usmonov,Maftuna Mirzaeva*

Main category: math.AP

TL;DR: The paper develops Green's function methods for boundary value problems involving Prabhakar fractional derivatives, providing explicit solution representations and proving existence/uniqueness.


<details>
  <summary>Details</summary>
Motivation: To extend classical Green-function techniques to Prabhakar-type fractional differential equations, which represent a wider class of fractional operators beyond traditional fractional calculus.

Method: Uses structural properties of Prabhakar kernel and generalized Mittag-Leffler functions to reduce the boundary value problem to a Volterra integral equation, enabling explicit construction of Green's function and closed-form solution representation.

Result: Successfully constructed explicit Green's function for Prabhakar fractional boundary value problems, derived closed-form integral solution representation, and proved existence and uniqueness of solutions.

Conclusion: The work extends classical analytical tools to Prabhakar fractional operators, providing foundation for studying boundary and inverse problems in this more general fractional calculus framework.

Abstract: We investigate a first boundary value problem for a second-order partial differential equation involving the Prabhakar fractional derivative in time. Using structural properties of the Prabhakar kernel and generalized Mittag-Leffler functions, we reduce the problem to a Volterra type integral equation. This reduction enables the explicit construction of the corresponding Green's function. Based on the obtained Green's function, we derive a closed-form integral representation of the solution and prove its existence and uniqueness. The results extend classical Green-function techniques to a wider class of fractional operators and provide analytical tools for further study of boundary and inverse problems associated with Prabhakar-type fractional differential equations.

</details>


### [24] [Operational Calculus for the nth-Level Prabhakar Type Fractional Derivative with Applications](https://arxiv.org/abs/2512.21273)
*Imtiaz Waheed,Erkinjon Karimov,Mujeeb ur Rehman*

Main category: math.AP

TL;DR: Study of nth-level Prabhakar fractional derivative: establishes properties, develops operational calculus, and applies to solve fractional differential equations.


<details>
  <summary>Details</summary>
Motivation: To investigate a generalized fractional derivative (nth-level Prabhakar) that encompasses several well-known fractional derivatives, and to develop mathematical tools for working with this operator.

Method: 1. Establish fundamental properties of the nth-level Prabhakar fractional derivative, particularly its relationship with the corresponding Prabhakar fractional integral. 2. Develop Mikusinski-type operational calculus for this derivative. 3. Apply the framework to solve differential equations involving this operator.

Result: 1. Established properties of the nth-level Prabhakar fractional derivative. 2. Developed operational calculus framework. 3. Provided analytical solutions for two problems: a fractional order ordinary differential equation and the time fractional heat equation, both containing the nth-level Prabhakar derivative.

Conclusion: The nth-level Prabhakar fractional derivative is a useful generalization that encompasses several fractional derivatives, and the developed operational calculus provides an effective framework for solving differential equations involving this operator.

Abstract: This study investigates the nth-level Prabhakar fractional derivative, a generalization encompassing some well-known fractional derivatives. We establish its fundamental properties, particularly its relationship with the corresponding Prabhakar fractional integral. Furthermore, we develop Mikusinski-type operational calculus for this derivative, providing a framework for solving differential equations involving this operator. To illustrate its application, we present analytical solutions of two problems: a fractional order ordinary differential equation and the time fractional heat equation, both of which include the nth-level Prabhakar derivative.

</details>


### [25] [Non-Algebraic Decay for Solutions to the Navier-Stokes Equations](https://arxiv.org/abs/2512.21312)
*Lorenzo Brandolese,Matthieu Pageard,Cilon F. Perusato*

Main category: math.AP

TL;DR: The paper addresses a gap in Wiegner's theorem about Navier-Stokes solution decay rates in 2D for non-algebraic decay cases.


<details>
  <summary>Details</summary>
Motivation: Wiegner's seminal work established sharp algebraic decay rates for Navier-Stokes solutions, showing they behave asymptotically like heat equation solutions. However, there's a gap in the conclusion for 2D solutions with non-algebraic decay rates that needs to be addressed.

Method: The paper likely involves mathematical analysis of Navier-Stokes equations in 2D, focusing on asymptotic behavior and decay rates, particularly addressing the specific gap in Wiegner's theorem for non-algebraic decay cases.

Result: The paper closes the identified gap in Wiegner's theorem for 2D Navier-Stokes solutions with non-algebraic decay rates, providing a complete understanding of asymptotic behavior.

Conclusion: The work completes Wiegner's theorem by addressing the missing case in 2D for non-algebraic decay, providing a comprehensive understanding of Navier-Stokes solution asymptotics in relation to heat equation behavior.

Abstract: Around forty years ago, Michael Wiegner provided, in a seminal paper, sharp algebraic decay rates for solutions of the Navier--Stokes equations, showing that these solutions behave asymptotically like the solutions of the heat equation with the same data as $t\to+\infty$, in the $L^2$-norm, up to some critical decay rate. In the present paper, we close a gap that appears in the conclusion of Wiegner's theorem in the 2D case, for solutions with non-algebraic decay rate.

</details>


### [26] [Large time behavior of the solution to the Cauchy problem for the discrete p-Laplacian with density on infinite graphs](https://arxiv.org/abs/2512.21321)
*Alan A. Tedeev*

Main category: math.AP

TL;DR: Study of Cauchy problem for nonstationary discrete p-Laplacian with inhomogeneous density on infinite graphs, proving precise stabilization rates and universal bounds for nonnegative solutions when p>2.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time behavior of solutions to the discrete p-Laplacian equation with variable density on infinite graphs, particularly how solutions stabilize over time and establish universal bounds under certain conditions.

Method: Uses energy inequalities and develops a new embedding result to analyze the Cauchy problem for the nonstationary discrete p-Laplacian with inhomogeneous density on graphs supporting Sobolev inequality.

Result: For p>2 and nonnegative solutions: 1) Prove precise stabilization rate in time when density is non-power function; 2) Prove universal bound when density decays sufficiently fast.

Conclusion: The paper establishes rigorous quantitative results about long-time behavior of discrete p-Laplacian equations on infinite graphs with variable density, providing both stabilization rates and universal bounds under specific conditions.

Abstract: We consider the Cauchy problem for the nonstationary discrete p-Laplacian with inhomogeneous density \r{ho}(x) on an infinite graph which supports the Sobolev inequality. For nonnegative solutions when p > 2, we prove the precise rate of stabilization in time, provided \r{ho}(x) is a non-power function. When p > 2 and \r{ho}(x) goes to zero fast enough, we prove the universal bound. Our technique relies on suitable energy inequalities and a new embedding result.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [Quantum Origin of Classical Background Fields from Coherent States: A First-Principles Formulation in QED](https://arxiv.org/abs/2512.21122)
*Keita Seto*

Main category: physics.plasm-ph

TL;DR: The paper provides a first-principles derivation showing how classical background fields in QED emerge from coherent states of the electromagnetic field, establishing a unified framework that connects operator-based and functional approaches.


<details>
  <summary>Details</summary>
Motivation: Classical background electromagnetic fields are widely used in QED for various physical applications, but their quantum origin needs clarification. The paper aims to systematically derive these classical backgrounds from coherent states to provide a rigorous foundation.

Method: Starting from the operator formulation of QED, the authors show how scattering amplitudes between coherent states lead to effective background field descriptions. They maintain separation between coherent laser modes and other quantized photon degrees of freedom, and derive both generating functional and path integral representations from Heisenberg picture amplitudes.

Result: The framework consistently incorporates effects beyond fixed background approximation (depletion, backreaction) without assuming specific field strengths. Conventional generating functionals with prescribed background fields emerge as limiting cases with fixed coherent state boundary conditions.

Conclusion: The work establishes a general, intensity-independent foundation for QED with coherent background fields, showing that standard strong-field QED formulations arise as well-defined special cases within this unified framework.

Abstract: Classical background electromagnetic fields are routinely employed in quantum electrodynamics to describe a wide range of physical situations, from laser-matter interactions to strong-field phenomena. In this work, we present a first-principles formulation that clarifies the quantum origin of such classical background fields in QED by systematically deriving them from coherent states of the electromagnetic field.
  Abstract Starting from the operator formulation of QED, we show how scattering amplitudes between coherent states naturally lead to an effective description in terms of background fields, while maintaining a clear separation between the coherent laser mode and other quantized photon degrees of freedom. This framework allows one to consistently incorporate effects beyond the fixed background approximation, such as depletion and backreaction, without assuming any particular field strength or intensity regime.
  Abstract We further demonstrate how the conventional generating functional with a prescribed background field emerges as a limiting case, corresponding to fixed coherent state boundary conditions. The path integral representation is then obtained as a reformulation of the same underlying Heisenberg picture amplitudes, providing a unified view of operator-based and functional approaches.
  Abstract Our results establish a general and intensity-independent foundation for QED with coherent background fields, within which the standard formulations of strong-field QED arise as well-defined special cases.

</details>


### [28] [Multivariate scaling of proton and ion energies, divergence, and charge states in Target Normal Sheath Acceleration](https://arxiv.org/abs/2512.21279)
*Vasiliki E. Alexopoulou*

Main category: physics.plasm-ph

TL;DR: The paper develops predictive scaling laws for laser-driven ion acceleration using a unified multiphysics model, enabling optimization of proton and ion beam properties for various applications.


<details>
  <summary>Details</summary>
Motivation: Despite extensive research on laser-driven ion acceleration via TNSA, there are no reliable predictive correlations between laser/target parameters and resulting ion-beam properties due to the complex, multiphysics nature of laser-plasma interactions.

Method: Used a unified multiphysics model with >95% accuracy to analyze laser-solid interactions. Applied multivariate regression with cross-validation for continuous beam properties (cutoff energies, divergences) and classification/regression tree (CART) methods for discrete ionization states across multiple laser/target parameters.

Result: Developed statistically validated scaling laws, probability maps, contour plots, and box plots that correlate proton, carbon, and oxygen ion properties to laser/target parameters, capturing nonlinear and threshold-dependent behaviors in TNSA acceleration.

Conclusion: The framework provides predictive and physically interpretable tools for understanding and optimizing laser-driven ion sources across a wide parameter space, advancing applications in proton therapy, materials modification, and high-energy-density physics.

Abstract: The interaction of an intense laser pulse with a solid target produces energetic proton and ion beams through the Target Normal Sheath Acceleration (TNSA) mechanism. Such beams are under active investigation for applications in proton beam therapy, materials modification, and nuclear and high-energy-density physics. Despite extensive experimental and theoretical effort, predictive correlations between laser and target parameters and the resulting ion-beam properties remain an open research question, owing to the intrinsically multiphysics and strongly coupled nature of laser-plasma interactions. Here, we employ our unified multiphysics model that reproduces laser-solid interaction dynamics with accuracy exceeding 95% over a broad range of short- and ultrashort-pulse conditions. Using this model, we derive statistically validated scaling laws and probability maps that correlate proton, carbon, and oxygen ion cutoff energies, beam divergences, and ionization states to a wide set of laser and target parameters, including pulse duration, laser power, laser beam spot, target thickness, prepulse-main pulse interval, contrast, laser wavelength, and polarization. Continuous beam properties (cutoff energies and beam divergences) are described using multivariate regression with cross-validation, while discrete ionization states are analyzed using classification and regression tree (CART) methods, enabling nonlinear and threshold-dependent behavior to be captured. The resulting scaling relations, contour maps, and box plots elucidate the coupled roles of laser pulse, and target geometry in governing TNSA ion acceleration and charge-state formation. These results provide a predictive and physically interpretable framework for understanding and optimizing laser-driven ion sources across a wide parameter space.

</details>


### [29] [Impurity peaking of SPARC H-modes: a sensitivity study on physics and engineering assumptions](https://arxiv.org/abs/2512.21286)
*Marco Muraca,Pablo Rodriguez-Fernandez,Joe Hall,Nathaniel T. Howard,Daniel Fajardo,Giovanni Tardini,Benedikt Zimmermann,Thomas Body*

Main category: physics.plasm-ph

TL;DR: SPARC tokamak impurity transport simulations show turbulent transport dominates over neoclassical, predicting low tungsten accumulation in H-mode plasmas with minimal impact from rotation and pedestal concentration variations.


<details>
  <summary>Details</summary>
Motivation: To predict impurity transport behavior in the upcoming SPARC tokamak, particularly for tungsten accumulation which can degrade plasma performance, and to understand how transport mechanisms compare to previous ITER predictions.

Method: Used ASTRA+STRAHL framework with FACIT for neoclassical transport, TGLF-SAT2 for turbulent transport, and neural network from EPED simulations for self-consistent pedestal calculations. Benchmarked three H-mode scenarios with varying plasma parameters and performed sensitivity studies on impurity concentrations, rotation, and fuel composition.

Result: Turbulent impurity transport dominates neoclassical transport in SPARC. Predictions are insensitive to tungsten pedestal concentration, minimally affected by argon pedestal concentration and rotation. Maximum fusion power occurs at 55-45% DT fuel composition with asymmetric distribution. Low tungsten accumulation predicted due to low collisionality operation.

Conclusion: SPARC will experience low tungsten accumulation similar to ITER predictions, with turbulent transport prevailing over neoclassical. The device's low collisionality operation favors this outcome, and modeling assumptions show minimal sensitivity to various parameters including rotation and impurity concentrations.

Abstract: In this paper, an overview of the impurity transport for three H-mode plasmas in the upcoming SPARC tokamak has been provided. The simulations have been performed within the ASTRA+STRAHL framework, using FACIT and TGLF-SAT2 to predict, respectively, neoclassical and turbulent core transport, while a neural network trained on EPED simulations has been employed to calculate the pedestal height and width self-consistently. A benchmark with previous simulations at constant impurity fraction has been provided for three H-modes, spanning different plasma current and magnetic field values. For a scenario, additional simulations have been performed to account for uncertainties in the modeling assumptions. The predictions are nearly insensitive to changes in the top of pedestal W concentrations. Varying the Ar pedestal concentration has shown a small effect on the impurity peaking and nearly constant fusion gain values, due to multiple effects on pedestal pressure, main ion dilution and density peaking. The inclusion of rotation in ASTRA simulations has shown minimal impact on confinement and impurity transport predictions. An exploratory study has been provided with a first set of simulations treating D and T separately, experiencing a maximum fusion power at 55-45% DT fuel composition, and an asymmetric distribution with respect to the D concentration. All the results, including sensitivity scans of toroidal velocity and ion temperature and density gradients, highlighted that turbulent impurity transport prevails on the neoclassical component, aligning with previous ITER predictions, and suggesting that next generation devices like SPARC, operating at low collisionality, will experience low W accumulation.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [30] [Propagation Estimates for the Boson Star Equation](https://arxiv.org/abs/2512.20718)
*Sébastien Breteaux,Jérémy Faupin,Viviana Grasselli*

Main category: math-ph

TL;DR: The paper analyzes boson star equations with general two-body potentials, proving finite speed of propagation bounds and asymptotic phase-space estimates for solutions.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical properties of boson star equations with general interaction potentials, particularly propagation speed limitations and long-time asymptotic behavior, which are important for understanding the physical behavior of bosonic systems.

Method: The authors consider boson star equations with general two-body potentials that decompose as sums of finite signed measures and bounded functions. They use Sobolev space analysis and scattering theory to prove propagation bounds and asymptotic estimates.

Result: For general potentials, they prove that solutions cannot propagate faster than light speed (up to exponentially small remainders). For short-range potentials with sufficiently regular/small initial data, they establish asymptotic phase-space propagation estimates and minimal velocity bounds tied to scattering state momentum.

Conclusion: The work provides rigorous mathematical foundations for boson star dynamics, establishing causality-preserving propagation bounds and detailed asymptotic behavior for solutions with appropriate potentials and initial conditions.

Abstract: We consider the boson star equation with a general two-body interaction potential $w$ and initial data $ψ_0$ in a Sobolev space. Under general assumptions on $w$, namely that $w$ decomposes as a sum of a finite, signed measure and an essentially bounded function, we prove that the (local in time) solution cannot propagate faster than the speed of light, up to a sharp exponentially small remainder term. If $w$ is short-range and $ψ_0$ is regular and small enough, we prove in addition asymptotic phase-space propagation estimates and minimal velocity estimates for the (global in time) solution, depending on the momentum of the scattering state associated to $ψ_0$.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [31] [Quantum Homotopy Algorithm for Solving Nonlinear PDEs and Flow Problems](https://arxiv.org/abs/2512.21033)
*Sachin S. Bharadwaj,Balasubramanya Nadiga,Stephan Eidenbenz,Katepalli R. Sreenivasan*

Main category: quant-ph

TL;DR: A near-optimal quantum algorithm for solving time-dependent, dissipative nonlinear PDEs using quantum homotopy analysis and compact finite-difference methods, with improved complexity scaling and practical applications.


<details>
  <summary>Details</summary>
Motivation: Quantum algorithms for nonlinear PDEs governing flow problems are challenging but critical for enhancing the practical usefulness of quantum computing in simulating real-world physical phenomena.

Method: Embeds PDEs in truncated high-dimensional linear space using quantum homotopy analysis, discretizes linearized system with finite-difference methods using compact quantum algorithm, adapts to nonlinearity nature and physics.

Result: Complexity estimates improve existing approaches in matrix operator norms, condition number, simulation time, and accuracy; provides general embedding strategy, stability bounds, accuracy, gate counts, and query complexity; connects nonlinearity measure to Reynolds-like parameter.

Conclusion: Demonstrates potential of hybrid quantum algorithm for simulating practical nonlinear phenomena on near-term and fault-tolerant quantum devices, illustrated with 1D Burgers problem simulations.

Abstract: Quantum algorithms to integrate nonlinear PDEs governing flow problems are challenging to discover but critical to enhancing the practical usefulness of quantum computing. We present here a near-optimal, robust, and end-to-end quantum algorithm to solve time-dependent, dissipative, and nonlinear PDEs. We embed the PDEs in a truncated, high dimensional linear space on the basis of quantum homotopy analysis. The linearized system is discretized and integrated using finite-difference methods that use a compact quantum algorithm. The present approach can adapt its input to the nature of nonlinearity and underlying physics. The complexity estimates improve existing approaches in terms of scaling of matrix operator norms, condition number, simulation time, and accuracy. We provide a general embedding strategy, bounds on stability criteria, accuracy, gate counts and query complexity. A physically motivated measure of nonlinearity is connected to a parameter that is similar to the flow Reynolds number $Re_{\textrm{H}}$, whose inverse marks the allowed integration window, for given accuracy and complexity. We illustrate the embedding scheme with numerical simulations of a one-dimensional Burgers problem. This work shows the potential of the hybrid quantum algorithm for simulating practical and nonlinear phenomena on near-term and fault-tolerant quantum devices.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [32] [Regularity of Einstein 5-manifolds via 4-dimensional gap theorems](https://arxiv.org/abs/2512.21317)
*Yiqi Huang,Tristan Ozuch*

Main category: math.DG

TL;DR: This paper refines regularity results for noncollapsed limits of 5-dimensional manifolds with bounded Ricci curvature, particularly Einstein 5-manifolds, establishing unique tangent cones, structure of singular sets, and orbifold regularity.


<details>
  <summary>Details</summary>
Motivation: To improve understanding of the regularity and structure of noncollapsed limits in 5-dimensional Riemannian geometry, particularly for Einstein manifolds, where previous results were less refined.

Method: The proofs rely on a new result showing that all spherical and hyperbolic 4-orbifolds are isolated among Einstein 4-orbifolds in the Gromov-Hausdorff sense, which yields gap theorems for Einstein 4-orbifolds.

Result: Five main results: (1) unique tangent cones of form ℝ×ℝ⁴/Γ on top stratum, (2) singular set contained in countable union of Lipschitz curves and points, (3) Lipschitz curves are smooth geodesics away from nowhere dense subset, (4) interior of geodesics removable leading to real-analytic orbifolds, (5) uniqueness of tangent cones at infinity for asymptotically Ricci-flat manifolds.

Conclusion: These refined regularity results for 5-dimensional limits raise the question about orbifold regularity of noncollapsed limits of Einstein manifolds off a codimension 5 set in arbitrary dimension, with the key technical tool being the isolation property of 4-dimensional orbifolds.

Abstract: We refine the regularity of noncollapsed limits of 5-dimensional manifolds with bounded Ricci curvature. In particular, for noncollapsed limits of Einstein 5-manifolds, we prove that
  (1) tangent cones are unique of the form $\mathbb{R}\times\mathbb{R}^4/Γ$ on the top stratum, hence outside a countable set of points,
  (2) the singular set is entirely contained in a countable union of Lipschitz curves and points,
  (3) away from a nowhere dense subset, these Lipschitz curves consist of smooth geodesics,
  (4) the interior of any geodesic is removable: limits of Einstein manifolds are real-analytic orbifolds with singularities along geodesic and bounded curvature away from their extreme points, and
  (5) if an asymptotically Ricci-flat 5-manifold with Euclidean volume growth has one tangent cone at infinity that splits off a line, then it is the unique tangent cone at infinity.
  These results prompt the question of the orbifold regularity of noncollapsed limits of Einstein manifolds off a codimension 5 set in arbitrary dimension.
  The proofs rely on a new result of independent interest: all spherical and hyperbolic 4-orbifolds are isolated among Einstein 4-orbifolds in the Gromov-Hausdorff sense. This yields various gap theorems for Einstein 4-orbifolds, which do not extend to higher dimensions.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [33] [The space spinor formalism and estimates for spinor fields](https://arxiv.org/abs/2512.20768)
*Mariem Magdy,Juan A. Valiente Kroon*

Main category: gr-qc

TL;DR: The paper adapts the space spinor formalism to construct estimates for spinor fields satisfying first-order equations, connecting this approach to existing strategies and recasting hyperbolicity concepts in spinor contexts.


<details>
  <summary>Details</summary>
Motivation: To develop systematic methods for constructing estimates for spinor fields satisfying first-order equations, bridging the gap between existing techniques for second-order hyperbolic equations and spinor field analysis.

Method: Uses the space spinor formalism for 2-component spinors to construct estimates, adapting the method of positive commutators (originally for second-order hyperbolic equations) to first-order spinor equations.

Result: Develops a framework for constructing estimates for spinor fields, establishes connections with other estimation strategies, and reformulates hyperbolicity concepts in the context of spinor equations.

Conclusion: The space spinor formalism provides an effective approach for constructing estimates for first-order spinor equations, serving as a spinor analog to the method of positive commutators for second-order hyperbolic equations.

Abstract: We show how the space spinor formalism for 2-component spinors can be used to construct estimates for spinor fields satisfying first order equations. We discuss the connection of the approach presented in this article with other strategies for the construction of estimates. In addition, we recast several concepts related to the notion of hyperbolicity in the context of spinor equations. The approach described in this article can be regarded as an adaptation to first order equations of the method of positive commutators for second order hyperbolic equations.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [34] [Mathematical Analysis of Symmetry-Protected Bound States in the Continuum in Waveguide Arrays](https://arxiv.org/abs/2512.20895)
*Xin Feng,Wei Wu*

Main category: physics.optics

TL;DR: Rigorous mathematical analysis of symmetry-based Bound States in the Continuum (BICs) in optical waveguide arrays using nonorthogonal coupled-mode equations and Bessel function addition theorems.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive mathematical framework for analyzing symmetry-protected BICs in optical waveguide arrays, moving beyond existing approximations like tight-binding or orthogonal coupled-mode equations to achieve more precise computational models for device design.

Method: Transformed wave propagation into Nonorthogonal Coupled-Mode Equations (NCME), derived exact expressions using Bessel function addition theorems, generalized to infinite arrays using harmonic analysis, and proved BIC existence in symmetric waveguide configurations with numerical verification of symmetry-breaking effects.

Result: Successfully derived exact expressions for overlap integrals and coupling coefficients, rigorously characterized dispersion relations, proved existence of symmetry-protected BICs in specific waveguide configurations, and numerically demonstrated transition from perfect BICs to leaky modes with symmetry-breaking perturbations.

Conclusion: The work provides a comprehensive mathematical foundation for symmetry-protected BICs in waveguide arrays, offering an efficient and precise computational model for designing BIC-based optical devices with rigorous analytical backing.

Abstract: This paper presents a rigorous mathematical analysis for symmetry-based Bound States in the Continuum (BICs) in optical waveguide arrays. Different from existing research, we consider a finite system of horizontally and equidistantly aligned waveguides and transform the wave propagation problem into Nonorthogonal Coupled-Mode Equations (NCME), rather than adopting the tight-binding approximation or orthogonal coupled-mode equations. We derive the exact expressions of the overlap integrals and coupling coefficients by utilizing the addition theorems of Bessel functions. We then generalize the discussion to an infinite waveguide array and rigorously characterize the dispersion relation and continuum with the help of theories in harmonic analysis. In the second part of the paper, we give a strict proof of the existence of BICs in the aforementioned waveguide system with two additional identical vertical waveguides aligned symmetrically above and below the horizontal waveguide array. We further numerically demonstrate the transition from a perfect BIC to a leaky mode by introducing a symmetry-breaking refractive index perturbation and quantitatively analyze the resulting radiation losses. This work gives a comprehensive study of symmetry-protected BICs and provides an efficient and precise computational model for designing such BICs devices.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [35] [Ab initio Approach to Collective Excitations in Excitonic Insulators](https://arxiv.org/abs/2512.20969)
*Fengyuan Xuan,Jiexi Song,Zhiyuan Sun*

Main category: cond-mat.mtrl-sci

TL;DR: Ab initio method for studying collective excitations in symmetry-broken electronic systems using Bethe-Salpeter equation in quasiparticle representation.


<details>
  <summary>Details</summary>
Motivation: To develop a first-principles approach for quantitatively predicting excited state phenomena in electronic systems with spontaneous symmetry breaking (excitonic insulators, density waves, superconductors).

Method: Derive Bethe-Salpeter equation for particle-hole excitations in quasiparticle representation, solve for collective excited states, compute order parameter fluctuations, demonstrate numerically on biased WSe2-MoSe2 bilayer excitonic insulator.

Result: Reveals gapless phase-mode, subgap Bardasis-Schrieffer modes, and above-gap scattering states in excitonic insulating phases of biased WSe2-MoSe2 bilayer.

Conclusion: Provides quantitative first-principles framework for excited state predictions in symmetry-broken electronic systems.

Abstract: An ab initio approach is presented for studying the collective excitations in excitonic insulators, charge/spin density waves and superconductors. We derive the Bethe-Salpeter-Equation for the particle-hole excitations in the quasiparticle representation, from which the collective excited states are solved and the corresponding order parameter fluctuations are computed. This method is demonstrated numerically for the excitonic insulating phases of the biased WSe2-MoSe2 bilayer. It reveals the gapless phase-mode, the subgap Bardasis-Schrieffer modes and the above-gap scattering states. Our work paves the way for quantitative predictions of excited state phenomena from first-principles calculations in electronic systems with spontaneous symmetry breaking.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [36] [Minijets and Broken Stationarity in a Blazar : Novel Insights into the Origin of $γ$-ray Variability in CTA 102](https://arxiv.org/abs/2512.21240)
*Agniva Roychowdhury*

Main category: astro-ph.HE

TL;DR: Analysis of 18-year Fermi-LAT data shows CTA 102's GeV light curves don't follow strict log-normal distributions, with skewness decreasing after the 2017 flare, indicating a state transition from frequent to occasional flaring explained by magnetic relaxation and minijets-in-a-jet model.


<details>
  <summary>Details</summary>
Motivation: To understand why high-energy blazar light curves (particularly CTA 102's GeV emission) deviate from expected log-normal distributions and to explain the state transition observed after the 2017 flare through physical mechanisms.

Method: Statistical analysis of 18-year Fermi-LAT archival data (0.1-100 GeV) from 2008-2025, comparing pre- and post-flare light curves. Used Monte Carlo simulations of a modified minijets-in-a-jet model within an external Compton framework, where GeV flares occur when maximum minijets align toward the broad line region and line of sight.

Result: Neither pre- nor post-flare GeV light curves follow strict log-normal distributions. Significant reduction in skewness from pre- to post-flare indicates transition from energetic state with frequent flaring to more plateaued state with occasional flaring. Observed and simulated flux distributions fit modified log-normal power-law distribution, supporting the minijets model.

Conclusion: CTA 102's state transition can be explained by magnetic relaxation where many reconnection events caused the 2017 flare, after which the magnetic field became ordered and reached minimum energy. The modified minijets-in-a-jet model successfully reproduces both GeV flares and their flux distributions.

Abstract: High-energy blazar light curves, in X-rays and beyond, have historically preferred a log-normal flux distribution, signifying multiplicative processes either in the jet itself or due to connection(s) with accretion. Here we present 18 year archival Fermi-LAT light curves (0.1-100 GeV) of the flat spectrum radio quasar (FSRQ) CTA 102 from August 2008 to November 2025, which underwent a huge flare in 2017, with a $\sim$ factor of 100 jump in $γ$-ray flux, along with similar flaring in X-rays. Our statistical analyses confirm that neither the pre nor the post-flare total GeV light curves follow a strictly log-normal distribution. Instead, we observe a statistically significant reduction in skewness from the pre to the post-flare light curves, which implies the blazar transitioned from an energetic state with frequent flaring to a more plateaued state with occasional flaring. We further find that this state transition can be explained through magnetic relaxation, where many reconnection events caused the 2017 flare, after which the magnetic field was ordered and its energy reached a minimum. To explain this further, we use a Monte Carlo simulation of a modified minijets-in-a-jet model where GeV flares are produced only when a maximum number of minijets move toward the broad line region and towards the line of sight, in the context of an external Compton model. The flux distributions (both observed and simulated) could be fit by a modified log-normal power-law distribution, implying our minijets model can reproduce the GeV flares in CTA 102 as well as their flux distributions.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [37] [Formal O(N3) scaling GW calculations by block tensor decomposition for large molecule systems](https://arxiv.org/abs/2512.21022)
*Yueyang Zhang,Wei Wu,Peifeng Su*

Main category: physics.chem-ph

TL;DR: BTD-based GW algorithm achieves O(N²) scaling for large molecular systems, enabling calculations with over 3000 basis functions.


<details>
  <summary>Details</summary>
Motivation: The GW approximation is crucial for quasiparticle energies but has high computational cost and poor scaling for large systems.

Method: Extends block tensor decomposition (BTD) algorithm with imaginary-time GW formalism and real-space screening for polarizability, optimized on S66 dataset using JADE.

Result: Achieves O(N²) scaling in test systems, BTD-based RPA also shows O(N²) scaling, enables eigenvalue-self-consistent GW for >3000 basis functions.

Conclusion: BTD establishes an efficient and scalable approach for large-scale GW calculations in molecular systems.

Abstract: Within the framework of many-body perturbation theory based on Green's functions, the $GW$ approximation has emerged as a pivotal method for computing quasiparticle energies and excitation spectra. However, its high computational cost and steep scaling present significant challenges for applications to large molecular systems. In this work, we extend the block tensor decomposition (BTD) algorithm, recently developed in our previous work [J. Chem. Phys. 163, 174109 (2025)] for low-rank tensor compression, to enable a formally $O(N^3)$-scaling $GW$ algorithm. By integrating BTD with an imaginary-time $GW$ formalism and introducing a real space screening strategy for the polarizability, we achieve an observed scaling of approximately $O(N^2)$ in test systems. Key parameters of the algorithm are optimized on the S66 dataset using the JADE algorithm, ensuring a balanced compromise between accuracy and efficiency. Our BTD-based random phase approximation also exhibits $O(N^2)$ scaling, and eigenvalue-self-consistent $GW$ calculations become feasible for systems with over 3000 basis functions. This work establishes BTD as an efficient and scalable approach for large-scale $GW$ calculations in molecular systems.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [38] [Implicit Numerical Scheme for the Hamilton-Jacobi-Bellman Quasi-Variational Inequality in the Optimal Market-Making Problem with Alpha Signal](https://arxiv.org/abs/2512.20850)
*Alexey Meteykin*

Main category: q-fin.MF

TL;DR: Market maker control problem solved via HJBQVI with implicit time-discretization and policy iteration for unconditional stability.


<details>
  <summary>Details</summary>
Motivation: Address the combined stochastic and impulse control problem for market makers operating in limit order books, overcoming time-step restrictions of explicit methods.

Method: Formulate as Hamilton-Jacobi-Bellman quasi-variational inequality (HJBQVI), use implicit time-discretization scheme coupled with policy iteration algorithm.

Result: Unconditional stability achieved, convergence to unique viscosity solution established by verifying monotonicity, stability, and consistency conditions with comparison principle.

Conclusion: The proposed implicit scheme with policy iteration provides stable solution for market maker control problems without time-step restrictions.

Abstract: We address the problem of combined stochastic and impulse control for a market maker operating in a limit order book. The problem is formulated as a Hamilton-Jacobi-Bellman quasi-variational inequality (HJBQVI). We propose an implicit time-discretization scheme coupled with a policy iteration algorithm. This approach removes time-step restrictions typical of explicit methods and ensures unconditional stability. Convergence to the unique viscosity solution is established by verifying monotonicity, stability, and consistency conditions and applying the comparison principle.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [39] [Prediction Air Temperature in Geothermal Heat Exchangers Using Pseudorandom Numbers: The New DARL Model](https://arxiv.org/abs/2512.19976)
*C. Ramírez-Dolores,J. C. Zamora-Luria,J. A. Altamirano-Acosta,L. Sarao-Cruz,P. Jiménez-Palma,J. Moreno-Falconi*

Main category: cs.CY

TL;DR: DARL model predicts air temperature distribution in Earth-Air-Water Heat Exchangers using experimental data and pseudo-random numbers, reducing sensor dependency with <6.2% error.


<details>
  <summary>Details</summary>
Motivation: EAWHE systems for sustainable air conditioning lack comprehensive study, and current characterization methods require dense sensor networks with high instrumentation, data acquisition, and computational costs.

Method: DARL model integrates experimental boundary condition data with simulations using pseudo-random numbers generated from Fermat's prime seeds, applying ordinary linear regressions and robust statistical validation.

Result: The model estimates thermal air distribution at different lengths with relative error less than 6.2%, validated by Shapiro-Wilk test and root mean square error analysis.

Conclusion: DARL represents a significant methodological advance that demonstrates efficiency, predictive capacity, and potential to reduce sensor dependency in EAWHE characterization.

Abstract: The use of Earth-Air-Water Heat Exchangers (EAWHE) for sustainable air conditioning has not been widely studied. Due to their experimental nature, methods of characterizing internal thermal air distribution impose high dependence on instrumentation by sensors and entail data acquisition and computational costs. This document presents an alternative method that estimates air temperature distribution while minimizing the need for a dense network of sensors in the experimental system. The proposed model, DARL (Data of Air and Random Length), can predict the temperature of air circulating inside EAWHEs. DARL is a significant methodological advance that integrates experimental data from boundary conditions with simulations based on pseudo-random numbers (PRNs). These PRNs are generated using Fermat's prime numbers as seeds to initialize the generator. Ordinary linear regressions and robust statistical validations, including the Shapiro-Wilk test and root mean square error, have demonstrated that the model can estimate the thermal distribution of air at different lengths with a relative error of less than 6.2%. These results demonstrate the model's efficiency, predictive capacity, and potential to reduce dependence on sensors.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [40] [On the Relationship Between Nanoflare Energy and Delay in the Closed Solar Corona](https://arxiv.org/abs/2512.20875)
*Shanwlee Sow Mondal,James A. Klimchuk,Craig D. Johnston,Lars K. S. Daldorff*

Main category: astro-ph.SR

TL;DR: No correlation found between nanoflare energies and their delays, suggesting nanoflare onset is not determined solely by magnetic stress but may involve other triggering mechanisms.


<details>
  <summary>Details</summary>
Motivation: Understanding the relationship between nanoflare energies and delays is crucial for revealing the physical mechanisms behind these events and plasma response in solar active regions.

Method: Used 3D multi-strand simulations with prescribed photospheric motions to generate nanoflares self-consistently. Quantified energies and durations using three distinct methods, then investigated correlations using two non-parametric, rank-based statistical tests.

Result: Consistently found little to no correlation between nanoflare energies and delays across all methods. The exponent α in E ∝ τ_D^α distribution peaks near zero, and delay distributions within fixed energy bins are broad. Results hold for both preceding and subsequent event correlations and for high-energy nanoflares.

Conclusion: The absence of correlation suggests nanoflare onset is not solely determined by a critical value of magnetic stress, but may involve triggering by other events, possibly related to locally complex magnetic topology.

Abstract: Determining the relationship between nanoflare energies and their delays is the key for understanding the physical mechanism of the events and the plasma response. Nanoflares analyzed in this study were generated self-consistently via prescribed photospheric motions in a 3D multi-strand simulation of a subset of active region magnetic flux. Energies and durations were quantified using three distinct methods. In this study, we investigated the correlation between nanoflare energies (E) and delays ($τ_D$) using two non-parametric, rank-based statistical tests. Across all methods, results consistently show little to no correlation. This is further supported by the distribution of the exponent $α$ in the assumed relation $E \propto τ_D^α$, which peaks near zero, and by broad delay distributions within fixed energy bins. These findings are irrespective of whether delays are correlated with the energy of the preceding or subsequent event. They also hold for a subset of high-energy nanoflares. The absence of correlation suggests that nanoflare onset is not solely determined by a critical value of magnetic stress and may involve triggering by other events, perhaps related to a locally complex topology.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [41] [The Dynamical Anatomy of Anderson Acceleration:From Adaptive Momentum to Variable-Mass ODEs](https://arxiv.org/abs/2512.21269)
*Kewang Chen,Yongqiu Jiang,Kees Vuik*

Main category: math.OC

TL;DR: The paper provides a rigorous analysis of Anderson Acceleration (AA) through High-Resolution ODEs, revealing AA's instability mechanism and proposing Energy-Guarded AA (EG-AA) with improved convergence stability.


<details>
  <summary>Details</summary>
Motivation: To bridge discrete acceleration algorithms with continuous dynamical systems, address fundamental questions about the physical nature of Anderson Acceleration that have remained unanswered since 1965, and understand why standard AA can be unstable.

Method: Uses High-Resolution Ordinary Differential Equations to analyze Anderson Acceleration, proves AA can be rewritten as an adaptive momentum method, conducts Lyapunov energy analysis to identify instability mechanisms, and proposes Energy-Guarded Anderson Acceleration (EG-AA) with thermodynamic consistency enforcement.

Result: Reveals that standard AA's instability comes from unchecked growth in effective mass acting as negative damping, identifies implicit Hessian-driven damping for stabilization, proves EG-AA improves upon gradient descent via Acceleration Gain Factor analysis, and shows EG-AA is no worse than standard AA with strictly improved convergence stability in numerical experiments.

Conclusion: The paper provides fundamental theoretical insights into Anderson Acceleration through dynamical systems analysis, identifies its instability mechanism, and proposes EG-AA as a thermodynamically consistent algorithm that improves convergence stability while maintaining acceleration benefits.

Abstract: This paper provides a rigorous derivation and analysis of accelerated optimization algorithms through the lens of High-Resolution Ordinary Differential Equations (ODEs). While classical Nesterov acceleration is well-understood via asymptotic vanishing damping, the dynamics of Anderson Acceleration (AA) remain less transparent. This work makes significant theoretical contributions to AA by bridging discrete acceleration algorithms with continuous dynamical systems, while also providing practical algorithmic innovations. Our work addresses fundamental questions about the physical nature of Anderson Acceleration that have remained unanswered since its introduction in 1965. Firstly, we prove that AA can be exactly rewritten as an adaptive momentum method and, in the high-resolution limit, converges to a second-order ODE with Variable Effective Mass. Through a Lyapunov energy analysis, we reveal the specific instability mechanism of standard AA: unchecked growth in effective mass acts as negative damping, physically injecting energy into the system and violating dissipation constraints. Conversely, high-resolution analysis identifies an implicit Hessian-driven damping term that provides stabilization in stiff regimes. Leveraging these dynamical insights, we then propose Energy-Guarded Anderson Acceleration (EG-AA), an algorithm that acts as an inertial governor to enforce thermodynamic consistency. Morevoer, our convergence analysis, formulated via the Acceleration Gain Factor, proves that EG-AA improves upon gradient descent by maximizing the geometric contraction of the linear subspace projection while actively suppressing nonlinear approximation errors. Theoretical bounds confirm that EG-AA is no worse than standard AA, and numerical experiments demonstrate strictly improved convergence stability and rates in ill-conditioned convex composite problems compared to standard Anderson mixing.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Forecasting N-Body Dynamics: A Comparative Study of Neural Ordinary Differential Equations and Universal Differential Equations](https://arxiv.org/abs/2512.20643)
*Suriya R S,Prathamesh Dinesh Joshi,Rajat Dandekar,Raj Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: Scientific ML approach using Neural ODEs and UDEs for n-body problem forecasting, with UDEs being much more data-efficient (20% vs 90% data needed).


<details>
  <summary>Details</summary>
Motivation: Traditional ML models for n-body trajectory prediction are data-intensive black boxes that ignore physical laws and lack interpretability. Scientific ML embeds known physical laws into ML frameworks for better physics-informed predictions.

Method: Uses Scientific ML frameworks in Julia: Neural ODEs and Universal Differential Equations (UDEs) to predict n-body system dynamics. Employs synthetically created noisy data to simulate real-world observational limitations. Determines forecasting breakdown point - the minimum training data needed for accurate predictions.

Result: UDE model is significantly more data-efficient, requiring only 20% of data for correct forecasting, while Neural ODE requires 90% of data.

Conclusion: Scientific ML approaches, particularly UDEs, offer more interpretable and data-efficient solutions for n-body problem forecasting compared to traditional black-box ML models, with UDEs demonstrating superior data efficiency.

Abstract: The n body problem, fundamental to astrophysics, simulates the motion of n bodies acting under the effect of their own mutual gravitational interactions. Traditional machine learning models that are used for predicting and forecasting trajectories are often data intensive black box models, which ignore the physical laws, thereby lacking interpretability. Whereas Scientific Machine Learning ( Scientific ML ) directly embeds the known physical laws into the machine learning framework. Through robust modelling in the Julia programming language, our method uses the Scientific ML frameworks: Neural ordinary differential equations (NODEs) and Universal differential equations (UDEs) to predict and forecast the system dynamics. In addition, an essential component of our analysis involves determining the forecasting breakdown point, which is the smallest possible amount of training data our models need to predict future, unseen data accurately. We employ synthetically created noisy data to simulate real-world observational limitations. Our findings indicate that the UDE model is much more data efficient, needing only 20% of data for a correct forecast, whereas the Neural ODE requires 90%.

</details>


### [43] [Improving Matrix Exponential for Generative AI Flows: A Taylor-Based Approach Beyond Paterson--Stockmeyer](https://arxiv.org/abs/2512.20777)
*Jorge Sastre,Daniel Faronbi,José Miguel Alonso,Peter Traver,Javier Ibáñez,Nuria Lloret*

Main category: cs.LG

TL;DR: Optimized Taylor-based algorithm for matrix exponential with dynamic parameter selection, achieving significant acceleration for generative AI applications while maintaining high numerical stability.


<details>
  <summary>Details</summary>
Motivation: Matrix exponential is fundamental for scientific computing and system simulation, with growing importance in generative AI. While Padé approximants with scaling and squaring have been standard, recent Taylor-based methods offer superior accuracy and reduced computational complexity, especially for high-throughput generative AI flows.

Method: Developed an optimized Taylor-based algorithm for matrix exponential using polynomial evaluation schemes that surpass classical Paterson-Stockmeyer technique. Includes rigorous error analysis and dynamic selection strategy for Taylor order and scaling factor to minimize computational effort under prescribed error tolerance.

Result: Extensive numerical experiments show significant acceleration and high numerical stability compared to existing state-of-the-art implementations. The method proves highly efficient for large-scale generative modeling.

Conclusion: The proposed Taylor-based algorithm establishes itself as a highly efficient tool for matrix exponential computation in generative AI applications, offering both performance improvements and numerical reliability over traditional methods.

Abstract: The matrix exponential is a fundamental operator in scientific computing and system simulation, with applications ranging from control theory and quantum mechanics to modern generative machine learning. While Padé approximants combined with scaling and squaring have long served as the standard, recent Taylor-based methods, which utilize polynomial evaluation schemes that surpass the classical Paterson--Stockmeyer technique, offer superior accuracy and reduced computational complexity. This paper presents an optimized Taylor-based algorithm for the matrix exponential, specifically designed for the high-throughput requirements of generative AI flows. We provide a rigorous error analysis and develop a dynamic selection strategy for the Taylor order and scaling factor to minimize computational effort under a prescribed error tolerance. Extensive numerical experiments demonstrate that our approach provides significant acceleration and maintains high numerical stability compared to existing state-of-the-art implementations. These results establish the proposed method as a highly efficient tool for large-scale generative modeling.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [44] [Flow Gym](https://arxiv.org/abs/2512.20642)
*Francesco Banelli,Antonio Terpin,Alan Bonomi,Raffaello D'Andrea*

Main category: physics.flu-dyn

TL;DR: Flow Gym is a toolkit for flow-field quantification research, providing synthetic data generation, algorithm testing/training interfaces, and JAX implementations of existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for a standardized, unified platform for developing and testing flow-field quantification algorithms, similar to how OpenAI Gym standardized reinforcement learning environments.

Method: The toolkit uses SynthPix for synthetic image generation, provides a unified interface for algorithm testing/deployment/training, and includes JAX implementations of existing algorithms for flow-field quantification from tracer particle images.

Result: Flow Gym offers a comprehensive toolkit that enables researchers to work with synthetic flow data, test algorithms in standardized environments, and leverage efficient JAX implementations for flow-field quantification tasks.

Conclusion: Flow Gym provides an essential research infrastructure for the flow-field quantification community, facilitating algorithm development, comparison, and deployment through its standardized framework and growing ecosystem of integrated methods.

Abstract: Flow Gym is a toolkit for research and deployment of flow-field quantification methods inspired by OpenAI Gym and Stable-Baselines3. It uses SynthPix as synthetic image generation engine and provides a unified interface for the testing, deployment and training of (learning-based) algorithms for flow-field quantification from a number of consecutive images of tracer particles. It also contains a growing number of integrations of existing algorithms and stable (re-)implementations in JAX.

</details>


### [45] [Velocity dip in turbulent mixed convection of an open Poiseuille-Rayleigh-Bénard channel](https://arxiv.org/abs/2512.20977)
*Ben-Rui Xu,Ao Xu,Heng-Dong Xi*

Main category: physics.flu-dyn

TL;DR: Velocity-dip phenomenon emerges in turbulent mixed convection in open channels with free-slip upper boundary due to large-scale rolls transporting low-speed fluid upward and roll fragmentation creating near-surface low-speed regions.


<details>
  <summary>Details</summary>
Motivation: To understand the emergence of velocity-dip phenomenon in turbulent mixed convection where maximum mean streamwise velocity occurs below the upper boundary rather than at the surface.

Method: Three-dimensional direct numerical simulations (DNS) for Rayleigh numbers 10^5 to 10^8 at fixed Pr=0.71 and Re_b=2850, analyzing flow structures, momentum budgets, and developing a model based on buoyancy-shear production balance.

Result: Three flow regimes identified: shear-dominated (small-scale streaks), transitional (large-scale rolls spanning channel height), and buoyancy-dominated (fragmented convection cells). Velocity dip occurs due to large-scale rolls transporting low-speed fluid upward and roll fragmentation creating near-surface low-speed regions.

Conclusion: Velocity-dip phenomenon in mixed convection is caused by large-scale flow structures that locally reduce near-surface mean velocity. The proposed model accurately reproduces DNS mean velocity profiles across the studied parameter range.

Abstract: We study the emergence of a velocity-dip phenomenon in turbulent mixed convection in open Poiseuille-Rayleigh-Bénard (PRB) channels with a free-slip upper boundary. Three-dimensional direct numerical simulations (DNS) are performed for Rayleigh numbers in the range $10^5 \leq Ra \leq 10^8$, at a fixed Prandtl number $Pr = 0.71$ and a bulk Reynolds number $Re_b = 2850$. In the shear-dominated regime, the flow is characterised by small-scale structures such as near-wall streaks. As buoyancy becomes comparable to shear, streamwise-oriented large-scale rolls emerge and span the full channel height. At higher Rayleigh numbers, buoyancy dominates and the rolls fragment, giving rise to a convection-cell-dominated regime. Short-time-averaged flow fields show that streamwise rolls transport low-speed fluid from the bottom wall towards the upper boundary, forming laterally extended low-speed regions, while roll fragmentation induces upstream low-speed regions near the upper boundary. Both mechanisms locally reduce the near-surface mean velocity, leading to a velocity dip in which the maximum mean streamwise velocity is located below the upper boundary. Consistent with the mean momentum budget, the near-surface region exhibits a large-scale Reynolds shear stress that exceeds the local total shear stress, implying a negative viscous contribution and a reversal of the mean velocity gradient. To model this behaviour, we propose a model based on a balance between buoyancy and shear production with dissipation, incorporating a linear wall-normal profile for the Reynolds shear stress, a wall-normal-independent buoyancy-production term, and a decomposition of the dissipation into shear-induced and buoyancy-induced contributions. Our model accurately reproduces the DNS mean velocity profiles across the explored $Ra$ range.

</details>
