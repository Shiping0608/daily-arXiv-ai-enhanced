<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 15]
- [math.AP](#math.AP) [Total: 26]
- [physics.comp-ph](#physics.comp-ph) [Total: 11]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 8]
- [math.OC](#math.OC) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [nlin.PS](#nlin.PS) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 6]
- [cs.LG](#cs.LG) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Differentiable Inverse Modeling with Physics-Constrained Latent Diffusion for Heterogeneous Subsurface Parameter Fields](https://arxiv.org/abs/2512.22421)
*Zihan Lin,QiZhi He*

Main category: math.NA

TL;DR: LD-DIM: A latent diffusion-based differentiable inversion method for PDE-constrained inverse problems that combines pretrained diffusion priors with differentiable numerical solvers to reconstruct high-dimensional parameter fields in a low-dimensional manifold.


<details>
  <summary>Details</summary>
Motivation: PDE-constrained inverse problems with high-dimensional spatially distributed coefficients suffer from ill-conditioning and instability under sparse observations. Existing methods like PINNs and physics-embedded VAEs have limitations in numerical stability and reconstruction accuracy.

Method: Couples pretrained latent diffusion model (LDM) with differentiable finite-volume PDE solver. Uses adjoint-based gradients with reverse-mode automatic differentiation. Performs inversion directly in latent space to suppress ill-conditioned degrees of freedom while preserving structural modes.

Result: Demonstrated on porous media flow inverse problem. Achieves improved numerical stability and reconstruction accuracy for both Gaussian random fields and bimaterial distributions compared to PINNs and physics-embedded VAEs. Maintains sharp discontinuities and reduces sensitivity to initialization.

Conclusion: LD-DIM provides a robust framework for PDE-constrained inverse problems by combining diffusion priors with differentiable solvers, enabling stable gradient-based optimization and accurate reconstruction of heterogeneous parameter fields from sparse observations.

Abstract: We present a latent diffusion-based differentiable inversion method (LD-DIM) for PDE-constrained inverse problems involving high-dimensional spatially distributed coefficients. LD-DIM couples a pretrained latent diffusion prior with an end-to-end differentiable numerical solver to reconstruct unknown heterogeneous parameter fields in a low-dimensional nonlinear manifold, improving numerical conditioning and enabling stable gradient-based optimization under sparse observations. The proposed framework integrates a latent diffusion model (LDM), trained in a compact latent space, with a differentiable finite-volume discretization of the forward PDE. Sensitivities are propagated through the discretization using adjoint-based gradients combined with reverse-mode automatic differentiation. Inversion is performed directly in latent space, which implicitly suppresses ill-conditioned degrees of freedom while preserving dominant structural modes, including sharp material interfaces. The effectiveness of LD-DIM is demonstrated using a representative inverse problem for flow in porous media, where heterogeneous conductivity fields are reconstructed from spatially sparse hydraulic head measurements. Numerical experiments assess convergence behavior and reconstruction quality for both Gaussian random fields and bimaterial coefficient distributions. The results show that LD-DIM achieves consistently improved numerical stability and reconstruction accuracy of both parameter fields and corresponding PDE solutions compared with physics-informed neural networks (PINNs) and physics-embedded variational autoencoder (VAE) baselines, while maintaining sharp discontinuities and reducing sensitivity to initialization.

</details>


### [2] [ROM for Viscous, Incompressible Flow in Polygons -- exponential $n$-width bounds and convergence rate](https://arxiv.org/abs/2512.22567)
*Francesco Romor,Federico Pichi,Giovanni Stabile,Gianluigi Rozza,Christoph Schwab*

Main category: math.NA

TL;DR: Exponential convergence of ROM approximations for stationary Navier-Stokes equations with mixed boundary conditions in polygonal domains, validated by numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To establish theoretical foundations for exponential convergence rates of Reduced Order Models (ROMs) for complex fluid flow problems with mixed boundary conditions, which are common in practical engineering applications but challenging to analyze theoretically.

Method: Leverages recent results on corner-weighted analytic regularity of velocity/pressure fields to prove exponential convergence of mixed hp-Finite Element Methods. Uses these bounds to infer exponential bounds for Kolmogorov n-widths, which then imply exponential convergence rates for POD Galerkin methods based on low-order, divergence-stable mixed FEM truth solutions.

Result: Proves exponential convergence of ROM approximations for stationary Navier-Stokes equations with mixed boundary conditions in polygonal domains. Numerical experiments confirm the theoretical exponential convergence rates.

Conclusion: The paper establishes rigorous theoretical foundations for exponential convergence of ROMs in complex fluid flow problems with mixed boundary conditions, bridging analytic regularity theory with practical reduced-order modeling, with numerical validation supporting the theoretical results.

Abstract: We demonstrate exponential convergence of Reduced Order Model (ROM) approximations for mixed boundary value problems of the stationary, incompressible Navier-Stokes equations in plane, polygonal domains $Ω$. Admissible boundary conditions comprise mixed BCs, no-slip, slip and open boundary conditions, subject to corner-weighted analytic boundary data and volume forcing. The small data hypothesis is assumed to ensure existence of a unique weak solution in the sense of Leray-Hopf. Recent results on corner-weighted, analytic regularity of velocity and pressure fields in $Ω$, imply exponential convergence rates of so-called mixed $hp$-Finite Element Methods in $H^1(Ω)^2\times L^2(Ω)$ on sequences of geometric partitions of $Ω$, with corner-refinement. Based on these exponential convergence rate bounds, we infer exponential bounds for the Kolmogorov $n$-widths of solution sets for analytic forcing and boundary data. This implies corresponding exponential convergence rates of POD Galerkin methods that are based on truth solutions which are obtained offline from low-order, divergence stable mixed Finite Element discretizations. Numerical experiments confirm the exponential rates and the theoretical results.

</details>


### [3] [A high-order method for the numerical approximation of fractional nonlinear Schrödinger equations](https://arxiv.org/abs/2512.22708)
*A. Durán,N. Reguera*

Main category: math.NA

TL;DR: Spectral Galerkin + implicit Runge-Kutta methods for fractional NLS equation with periodic boundary conditions, with theoretical error analysis and numerical validation.


<details>
  <summary>Details</summary>
Motivation: To develop efficient and accurate numerical methods for solving the fractional nonlinear Schrödinger equation with periodic boundary conditions, which has applications in quantum mechanics and nonlinear optics.

Method: Fourier spectral Galerkin method for spatial discretization, combined with diagonally implicit high-order Runge-Kutta schemes (based on composition with implicit midpoint rule) for temporal discretization.

Result: Proved properties and error estimates for both semidiscretization (space only) and full discretization (space-time), with numerical experiments demonstrating convergence and performance.

Conclusion: The proposed spectral Galerkin + implicit Runge-Kutta scheme provides an effective numerical method for fractional NLS equations with rigorous error analysis and good computational performance.

Abstract: In this paper, the periodic initial-value problem for the fractional nonlinear Schrödinger (fNLS) equation is discretized in space by a Fourier spectral Galerkin method and in time by diagonally implicit, high-order Runge-Kutta schemes, based on the composition with the implicit midpoint rule (IMR). Some properties and error estimates for the semidiscretization in space and for the full discretization are proved. The convergence results and the general performance of the scheme are illustrated with several numerical experiments.

</details>


### [4] [Convergent numerical schemes for the viscoelastic Giesekus model in two dimensions](https://arxiv.org/abs/2512.22831)
*Endre Süli,Dennis Trautwein*

Main category: math.NA

TL;DR: Develops stable and convergent numerical methods for the viscoelastic Giesekus model in 2D, proving convergence to weak solutions without regularization.


<details>
  <summary>Details</summary>
Motivation: Existing numerical schemes for viscoelastic Giesekus models often suffer from accuracy limitations and convergence problems due to lack of rigorous existence results or discretization limitations.

Method: Develops a class of stable and convergent numerical methods for the 2D viscoelastic Giesekus model, which couples incompressible Navier-Stokes equations with evolution equation for elastic stress tensor in terms of deformation gradient.

Result: Proves subsequence convergence of the numerical method to large-data global weak solutions in 2D without cut-offs or regularization, providing alternative proof of existence result by Bulíček et al. Numerical experiments verify practicality.

Conclusion: The proposed numerical methods are stable, convergent, and practical for solving viscoelastic Giesekus model in 2D, addressing limitations of existing approaches while providing rigorous convergence proofs.

Abstract: In this work, we develop a class of stable and convergent numerical methods for the approximate solution of the viscoelastic Giesekus model in two space dimensions. The model couples the incompressible Navier--Stokes equations with an evolution equation for an additional stress tensor accounting for elastic effects. This coupled evolution equation is stated here in terms of the elastic deformation gradient and models transport and nonlinear relaxation effects. In the existing literature, numerical schemes for such models often suffer from accuracy limitations and convergence problems, usually due to the lack of rigorous existence results or inherent limitations of the discretization. Therefore, our main goal is to prove the (subsequence) convergence of the proposed numerical method to a large-data global weak solution in two dimensions, without relying on cut-offs or additional regularization. This also provides an alternative proof of the recent existence result by Bulíček et al.~(Nonlinearity, 2022). Finally, we verify the practicality of the proposed method through numerical experiments, including convergence studies and typical benchmark problems.

</details>


### [5] [A Two-Stage Finite Element Approach for High-precision Guaranteed Lower Eigenvalue Bounds](https://arxiv.org/abs/2512.23182)
*Xuefeng Liu,Michael Plum*

Main category: math.NA

TL;DR: New two-stage algorithm using high-order FEM on graded meshes produces rigorous lower eigenvalue bounds as sharp as high-order upper bounds, solving long-standing difficulty in eigenvalue computation.


<details>
  <summary>Details</summary>
Motivation: There's a persistent gap between accurate upper eigenvalue bounds (easy with standard high-order FEM) and equally sharp rigorous lower bounds. Existing approaches using low-order elements or non-standard FEM are limited or technically involved, making the problem demanding and competitive.

Method: Proposes a new two-stage rigorous algorithm that employs high-order FEM on graded meshes to produce rigorous lower eigenvalue bounds. The method works particularly well on graded or highly nonuniform meshes.

Result: Numerical experiments for Laplacian and Steklov eigenvalue problems on square and dumbbell domains demonstrate the accuracy and efficiency of the method. The algorithm produces lower bounds as sharp as corresponding high-order upper bounds.

Conclusion: The proposed approach provides a practical and competitive solution to the long-standing difficulty of obtaining sharp, reliable lower eigenvalue bounds, closing the gap between accurate upper and lower bounds in eigenvalue computation.

Abstract: Obtaining high-precision guaranteed lower eigenvalue bounds remains difficult, even though the standard high-order conforming finite element (FEM) easily yields extremely sharp upper bounds. Recently developed rigorous approaches using such as Crouzeix--Raviart or linear conforming elements do not extend well to high-order FEM. Some non-standard FEM approaches can provide sharp eigenvalue bounds but are technically involved. This persistent gap between accurate upper bounds and equally sharp rigorous lower bounds via standard high-order conforming FEMs makes the problem technically demanding and highly competitive. In this paper, we propose a new two-stage rigorous algorithm that closes this gap by employing high-order FEM on graded meshes and producing rigorous lower eigenvalue bounds as sharp as the corresponding high-order upper bounds, as demonstrated in our numerical examples. Numerical experiments for the Laplacian and Steklov eigenvalue problems on square and dumbbell domains show the accuracy and efficiency of the method, particularly on graded or highly nonuniform meshes. These results confirm that the proposed approach provides a practical and competitive solution to the long-standing difficulty of obtaining sharp, reliable lower eigenvalue bounds.

</details>


### [6] [Frenet Immersed Finite Element Spaces on Triangular Meshes](https://arxiv.org/abs/2512.23238)
*Tao Lin,Yuanhui Lin,Xu Zhang*

Main category: math.NA

TL;DR: Develop geometry-conforming immersed finite element spaces on triangular meshes for elliptic interface problems using Frenet-Serret mapping to transform interface curves to straight lines for exact jump condition imposition.


<details>
  <summary>Details</summary>
Motivation: Extend existing immersed finite element framework from rectangular to triangular meshes for elliptic interface problems, enabling more flexible mesh generation while maintaining exact imposition of interface jump conditions.

Method: Construct GC-IFE spaces via Frenet-Serret mapping that transforms interface curves to straight lines. Three construction procedures: initial construction using monomial bases, generalized construction using orthogonal polynomials, and reconstruction methods to improve mass matrix conditioning.

Result: Demonstrated optimal approximation capability of proposed IFE spaces through numerical examples. Achieved optimal convergence rates in H¹ and L² norms when incorporated into interior penalty discontinuous Galerkin methods for elliptic interface problems.

Conclusion: Successfully extended geometry-conforming IFE framework to triangular meshes with three construction methods, showing optimal approximation and convergence properties for elliptic interface problems.

Abstract: In this paper, we develop geometry-conforming immersed finite element (GC-IFE) spaces on triangular meshes for elliptic interface problems. These IFE spaces are constructed via a Frenet-Serret mapping that transforms the interface curve into a straight line, allowing the interface jump conditions to be imposed exactly. Extending the framework of [7,8] from rectangular to triangular meshes, we introduce three procedures for constructing high-degree Frenet-IFE spaces: an initial construction based on monomial bases, a generalized construction using orthogonal polynomials, and reconstruction methods aimed at improving the conditioning of the associated mass matrix. The optimal approximation capability of the proposed IFE spaces is demonstrated through numerical examples. We further incorporate these spaces into interior penalty discontinuous Galerkin methods for elliptic interface problems and observe optimal convergence rates in the $H^1$ and $L^2$ norms.

</details>


### [7] [$L^2$ and $L^\infty$ rational approximation](https://arxiv.org/abs/2512.23357)
*Michael S. Ackermann,Sean Reiter,Lloyd N. Trefethen*

Main category: math.NA

TL;DR: Computational comparison of best L² and L∞ rational approximations for analytic functions on the unit disk using new barycentric TF-IRKA formulation.


<details>
  <summary>Details</summary>
Motivation: While theoretical foundations for L² and L∞ rational approximations exist for decades, there's a lack of computational studies comparing these approaches for analytic functions on the unit disk.

Method: Develops new barycentric formulation of TF-IRKA (Tangential Frequency Interpolation-based Rational Krylov Algorithm) to compute L² best approximations, and compares with L∞ approximations.

Result: First computational study comparing L² and L∞ rational approximations for analytic functions on the unit disk, enabled by new algorithmic developments.

Conclusion: The paper provides computational insights into rational approximation theory by implementing and comparing L² and L∞ methods using novel barycentric TF-IRKA formulation.

Abstract: Using recently developed algorithms, we compute and compare best $L^2$ and $L^\infty$ rational approximations of analytic functions on the unit disk. Although there is some theory for these problems going back decades, this may be the first computational study. To compute the $L^2$ best approximations, we employ a new formulation of TF-IRKA in barycentric form.

</details>


### [8] [A Data-Driven Approach to Solving First-Kind Fredholm Integral Equations and Their Convergence Analysis](https://arxiv.org/abs/2512.23362)
*Duan-Peng Ling,Wenlong Zhang*

Main category: math.NA

TL;DR: Statistical recovery of solutions to Fredholm integral equations from noisy scattered measurements with optimal error bounds and explicit regularization parameter rules.


<details>
  <summary>Details</summary>
Motivation: To develop statistical methods for solving ill-posed Fredholm integral equations from discrete, scattered, and noisy pointwise measurements, which is a common problem in inverse problems and statistical learning.

Method: Analyze Fredholm integral equations with forward operators whose range belongs to Sobolev space of order m, leading to algebraic singular-value decay. Derive optimal reconstruction error bounds in weak topology with a priori regularization parameter choice. Establish mean-square error rates for bounded-variance noise and exponential concentration bounds for sub-Gaussian noise.

Result: Obtain optimal upper bounds for reconstruction error with explicit dependence on sample size n, noise level σ, and smoothness index m. Develop both a priori and a posteriori rules for regularization parameter selection. Numerical experiments validate theoretical results and demonstrate practical efficiency.

Conclusion: The paper provides a complete statistical framework for solving Fredholm integral equations from noisy measurements with optimal error rates and practical regularization parameter selection rules that work well in numerical experiments.

Abstract: We investigate the statistical recovery of solutions to first-kind Fredholm integral equations with discrete, scattered, and noisy pointwise measurements. Assuming the forward operator's range belongs to the Sobolev space of order $m$, which implies algebraic singular-value decay $s_j\le Cj^{-m}$, we derive optimal upper bounds for the reconstruction error in the weak topology under an a priori choice of the regularization parameter. For bounded-variance noise, we establish mean-square error rates that explicitly quantify the dependence on sample size $n$, noise level $σ$, and smoothness index $m$; under sub-Gaussian noise, we strengthen these to exponential concentration bounds. The analysis yields an explicit a priori and a posteriori rule for the regularization parameter. Numerical experiments validate the theoretical results and demonstrate the efficiency of our practical parameter choice.

</details>


### [9] [High-order implicit Runge-Kutta time integrators for component-based model reduction of FSI problems](https://arxiv.org/abs/2512.23363)
*Tommaso Taddei,Xuejun Xu,Lei Zhang*

Main category: math.NA

TL;DR: A model order reduction framework for incompressible fluid-structure interaction using high-order implicit Runge-Kutta methods with bubble-port decomposition and supremizer enrichment.


<details>
  <summary>Details</summary>
Motivation: To develop stable and accurate reduced-order models for strongly-coupled parametric FSI problems that can handle long-time integration while preserving energy balance and avoiding additional interface enrichment.

Method: Uses separate reduced spaces for fluid velocity, pressure, and solid displacement; enriches velocity space with supremizer modes for inf-sup stability; employs bubble-port decomposition to satisfy interface kinematic conditions; applies Galerkin projection and Radau-IIA IRK method with static condensation of interface DOFs.

Result: The reduced-order model preserves semi-discrete energy balance from the full-order model and yields stable, accurate results for long-time integration of strongly-coupled parametric FSI problems without needing additional interface enrichment.

Conclusion: The combination of high-order IRK schemes with bubble-port decoupling provides an effective framework for stable and accurate reduced-order modeling of parametric FSI problems, particularly suitable for long-time integration scenarios.

Abstract: We propose a model order reduction framework for incompressible fluid-structure interaction (FSI) problems based on high-order implicit Runge-Kutta (IRK) methods. We consider separate reduced spaces for fluid velocity, fluid pressure and solid displacement; we enrich the velocity space with supremizer modes to ensure the inf-sup stability of the fluid subproblem; we consider bubble-port decomposition of fluid velocity and solid displacement to satisfy the kinematic conditions at the fluid structure interface. We resort to Galerkin projection to define the semi-discrete reduced-order model and we consider a Radau-IIA IRK method for time integration: the resulting algebraic system is solved using static condensation of the interface degrees of freedom. The reduced-order model preserves a semi-discrete energy balance inherited from the full-order model, and avoids the need for additional interface enrichment. Numerical experiments demonstrate that the proposed combination of high-order IRK schemes with bubble-port decoupling of velocity and displacement degrees of freedom yields stable and accurate reduced-order model for long-time integration of strongly-coupled parametric FSI problems.

</details>


### [10] [Sensitivity Analysis on the Sphere and a Spherical ANOVA Decomposition](https://arxiv.org/abs/2512.23476)
*Laura Weidensager*

Main category: math.NA

TL;DR: Novel sensitivity analysis on spheres using ANOVA-like decomposition with parity parameterization for high-dimensional functions with low-dimensional interactions.


<details>
  <summary>Details</summary>
Motivation: To develop sensitivity analysis methods for functions on spheres that account for the natural geometry and dependencies between input variables, enabling efficient modeling of high-dimensional functions.

Method: Decompose functions on spheres into terms with subset indices and parity parameters, using orthogonal basis functions to capture low-dimensional variable interactions.

Result: Formulas for function decomposition on spheres that incorporate both variable subset dependencies and parity properties, enabling efficient approximation of high-dimensional functions.

Conclusion: The proposed sensitivity analysis framework on spheres provides an effective approach for modeling complex high-dimensional functions by leveraging geometric properties and low-dimensional interactions.

Abstract: We establish sensitivity analysis on the sphere. We present formulas that allow us to decompose a function $f\colon \mathbb S^d\rightarrow \mathbb R$ into a sum of terms $f_{\boldsymbol u,\boldsymbol ξ}$. The index $\boldsymbol u$ is a subset of $\{1,2,\ldots,d+1\}$, where each term $f_{\boldsymbol u,\boldsymbol ξ}$ depends only on the variables with indices in $\boldsymbol u$. In contrast to the classical analysis of variance (ANOVA) decomposition, we additionally use the decomposition of a function into functions with different parity, which adds the additional parameter $\boldsymbol ξ$. The natural geometry on the sphere naturally leads to the dependencies between the input variables. Using certain orthogonal basis functions for the function approximation, we are able to model high-dimensional functions with low-dimensional variable interactions.

</details>


### [11] [Error Estimates for Gauss--Christoffel Quadrature under Reduced Regularity Conditions](https://arxiv.org/abs/2512.23540)
*Mehdi Hamzehnejad,Abbas Salemi*

Main category: math.NA

TL;DR: Improved error bounds for Gauss-Christoffel quadrature using weaker regularity assumptions than classical weighted bounded variation conditions.


<details>
  <summary>Details</summary>
Motivation: Classical error estimates for Gauss-Christoffel quadrature rely on restrictive weighted bounded variation assumptions with singular weight (1-x²)^{-1/2}, which may be too strong for functions with limited endpoint regularity.

Method: Develops new identity for higher-order derivatives of Chebyshev polynomials to establish improved decay estimates for Chebyshev coefficients, replacing classical weighted condition V_r with weaker condition U_r.

Result: Obtains less restrictive error estimate for Gauss-Christoffel quadrature rule, extends approach to Gauss-Gegenbauer case, and provides numerical validation.

Conclusion: The paper presents significant theoretical improvement by weakening regularity requirements for Gauss-Christoffel quadrature error bounds, making them applicable to broader function classes.

Abstract: Gauss--Christoffel quadrature is a fundamental method for numerical integration, and its convergence analysis is closely related to the decay of Chebyshev expansion coefficients. Classical estimates, including those due to Trefethen, are based on weighted bounded variation assumptions involving the singular weight $(1-x^{2})^{-1/2}$, which may be too restrictive for functions with limited regularity at the endpoints.
  In this paper, we establish a new error bound for Gauss--Christoffel quadrature under weakened regularity assumptions. The analysis relies on a new identity for higher-order derivatives of Chebyshev polynomials. As a consequence, we obtain an improved decay estimate for Chebyshev coefficients, where the classical weighted condition \[ V_{r}=\int_{-1}^{1}\frac{|f^{(r+1)}(x)|}{\sqrt{1-x^{2}}}\,dx \] is replaced by the weaker condition \[ U_{r}=\int_{-1}^{1}|f^{(r+1)}(x)|\,dx. \]
  This result leads to a corresponding error estimate for the Gauss--Christoffel quadrature rule, which is less restrictive than previous bounds. The approach is also extended to the Gauss--Gegenbauer case. Numerical experiments are provided to illustrate the theoretical results.

</details>


### [12] [Analysis of kinetic-diffusion Monte Carlo simulation and source term estimation scheme in nuclear fusion applications](https://arxiv.org/abs/2512.23580)
*Zhirui Tang,Julian Koellermeier,Emil Løvbak,Giovanni Samaey*

Main category: math.NA

TL;DR: KDMC with fluid estimation accelerates plasma edge simulations, achieving lower error than fluid methods and significant speedup over kinetic MC, validated through theoretical bounds and numerical verification.


<details>
  <summary>Details</summary>
Motivation: Monte Carlo methods for Boltzmann-BGK equations in plasma edge simulations become computationally expensive for large reactors like ITER/DEMO due to high collision rates, requiring acceleration techniques.

Method: Asymptotic-preserving kinetic-diffusion Monte Carlo (KDMC) simulation combined with fluid estimation technique; comprehensive convergence analysis with theoretical upper bounds and numerical verification.

Result: KDMC with fluid estimation achieves lower error than purely fluid-based methods (even one order of magnitude lower in fusion-relevant cases) and shows significant speedup compared to reference kinetic MC.

Conclusion: The analysis confirms KDMC with associated fluid estimation is effective for nuclear fusion applications, providing accurate and computationally efficient plasma edge simulations.

Abstract: In plasma edge simulations, the behavior of neutral particles is often described by a Boltzmann--BGK equation. Solving this kinetic equation and estimating the moments of its solution are essential tasks, typically carried out using Monte Carlo (MC) methods. However, for large-sized reactors, like ITER and DEMO, high collision rates lead to a substantial computational cost. To accelerate the calculation, an asymptotic-preserving kinetic-diffusion Monte Carlo (KDMC) simulation method (Mortier et al., SIAM J. Sci. Comput., 2022) and a corresponding fluid estimation technique (Mortier et al., Contrib. Plasma Phys., 2022) have recently been proposed. In this work, we present a comprehensive analysis of the convergence of KDMC combined with the associated fluid estimation. The analysis consists of proving theoretical upper bounds for both KDMC and the fluid estimation, and numerical verifications of these bounds. In addition, we compare the analyzed algorithm with a purely fluid-based method using the fully kinetic MC method as a reference. The algorithm consistently achieves lower error than the fluid-based method, and even one order of magnitude lower in a fusion-relevant test case. Moreover, the algorithm exhibits a significant speedup compared to the reference kinetic MC method. Overall, our analysis confirms the effectiveness of KDMC with the associated fluid estimation in nuclear fusion applications.

</details>


### [13] [Learning Lévy density via adaptive RKHS regression with bi-level optimization](https://arxiv.org/abs/2512.23621)
*Luxuan Yang,Fei Lu,Ting Gao,Wei Wei,Jinqiao Duan*

Main category: math.NA

TL;DR: A nonparametric method to learn Lévy densities from probability density data using adaptive RKHS regularization and bilevel optimization for parameter selection.


<details>
  <summary>Details</summary>
Motivation: Learning Lévy densities from probability density data governed by nonlocal Fokker-Planck equations is an ill-posed inverse problem that requires effective regularization and parameter selection methods.

Method: Recast as kernel identification in nonlocal integral operator; construct adaptive RKHS with data-built kernel; use GSVD-based bilevel optimization for regularization parameter selection; prove theoretical convergence rates.

Result: Reconstruction error decays at near optimal rate; bilevel RKHS method outperforms classical L-curve and generalized cross-validation; adaptive RKHS norm is more accurate and robust than L² and ℓ² regularization.

Conclusion: The proposed adaptive RKHS method with bilevel optimization provides an effective, accurate, and robust approach for learning Lévy densities from various data types, with theoretical guarantees and practical advantages over classical methods.

Abstract: We propose a nonparametric method to learn the Lévy density from probability density data governed by a nonlocal Fokker-Planck equation. We recast the problem as identifying the kernel in a nonlocal integral operator from discrete data, which leads to an ill-posed inverse problem. To regularize it, we construct an adaptive reproducing kernel Hilbert space (RKHS) whose kernel is built directly from the data. Under standard source and spectral decay conditions, we show that the reconstruction error decays in the mesh size at a near optimal rate. Importantly, we develop a generalized singular value decomposition (GSVD)-based bilevel optimization algorithm to choose the regularization parameter, leading to efficient and robust computation of the regularized estimator. Numerical experiments for several Lévy densities, drift fields and data types (PDE-based densities and sample ensemble-based KDE reconstructions) demonstrate that our bilevel RKHS method outperforms classical L-curve and generalized cross-validation strategies and that the adaptive RKHS norm is more accurate and robust than $L^2_ρ$- and $\ell^2$-based regularization.

</details>


### [14] [Simultaneous Approximation of the Score Function and Its Derivatives by Deep Neural Networks](https://arxiv.org/abs/2512.23643)
*Konstantin Yakovlev,Nikita Puchkin*

Main category: math.NA

TL;DR: Theory for approximating score functions and their derivatives with error bounds free from curse of dimensionality, handling low-dimensional data with unbounded support.


<details>
  <summary>Details</summary>
Motivation: Existing methods for score function approximation typically require bounded data support and suffer from curse of dimensionality. There's a need for theoretical guarantees that work with realistic data distributions having low-dimensional structure and unbounded support.

Method: Develops a theoretical framework for simultaneous approximation of score functions and their derivatives. The approach relaxes the usual bounded support requirement and provides approximation guarantees for derivatives of any prescribed order.

Result: Achieves approximation error bounds matching literature results while eliminating curse of dimensionality. The theory handles data with low-dimensional structure and unbounded support, and extends to higher-order derivatives beyond first-order settings.

Conclusion: The proposed theory provides a more general and practical framework for score function approximation, overcoming limitations of existing methods regarding data support constraints and dimensionality issues, while enabling higher-order derivative approximations.

Abstract: We present a theory for simultaneous approximation of the score function and its derivatives, enabling the handling of data distributions with low-dimensional structure and unbounded support. Our approximation error bounds match those in the literature while relying on assumptions that relax the usual bounded support requirement. Crucially, our bounds are free from the curse of dimensionality. Moreover, we establish approximation guarantees for derivatives of any prescribed order, extending beyond the commonly considered first-order setting.

</details>


### [15] [A High-Order Spectral Element Solver for Steady-State Free Surface Flows](https://arxiv.org/abs/2512.23648)
*Simone Minniti,Jens Visbech,Claes Eskilsson,Nicola Parolini,Allan Peter Engsig-Karup*

Main category: math.NA

TL;DR: A high-order spectral element solver for free-surface Navier-Stokes flows using Firedrake framework with curvilinear elements for accurate free-surface and body curvature representation.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient high-order numerical method for solving steady incompressible Navier-Stokes equations with free surfaces, addressing the challenge of determining unknown free surface profiles while maintaining geometric accuracy for curved boundaries.

Method: Spectral element method implemented in Firedrake framework using high-order polynomial basis on unstructured meshes. Uses iterative pseudo-time procedure to determine free surface profiles, incorporates curvature through curvilinear elements via transfinite linear blending, and employs weak formulations.

Result: The model achieves high-order accuracy confirmed through convergence studies, demonstrates substantial speed-up over low-order schemes, and successfully solves benchmark cases including lid-driven cavity, flows around cylinders/NACA airfoils, and free-surface flows over bathymetry bumps and submerged airfoils.

Conclusion: The developed spectral element solver provides an efficient, high-order accurate approach for free-surface Navier-Stokes problems, successfully handling complex geometries and demonstrating superior performance compared to low-order methods.

Abstract: We present a spectral element solver for the steady incompressible Navier-Stokes equations subject to a free surface. Utilizing the kinematic behaviour of the free surface boundary, an iterative pseudo-time procedure is proposed to determine the a priori unknown free surface profile. The numerical model is implemented in the open-source finite element framework Firedrake, which enables the use of a high-order polynomial basis on unstructured meshes through weak formulations. Additionally, the curvature of the free surface and submerged bodies is incorporated through curvilinear elements obtained via transfinite linear blending, which conserves the high-order convergent properties of the overall scheme. The model is applied to several benchmark cases in two spatial dimensions. Initially, it addresses fixed-domain problems, including the lid-driven cavity flow and flows around bodies such as a cylinder and a NACA airfoil. Subsequently, with the presence of a free surface, it is extended to determine the flow around a bathymetry bump and a submerged NACA airfoil. The results confirm the high-order accuracy of the model through convergence studies and demonstrate a substantial speed-up over low-order numerical schemes.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [16] [Time Reparametrization, Not Fractional Calculus: A Reassessment of the Conformable Derivative](https://arxiv.org/abs/2512.22366)
*Aziz El Ghazouani,Fouad Ibrahim Abdou Amir,Khoulane Mohamed,M'hamed Elomari*

Main category: math.AP

TL;DR: The conformable derivative is not a genuine fractional derivative but a classical derivative under nonlinear time reparametrization, lacking true nonlocal fractional effects.


<details>
  <summary>Details</summary>
Motivation: To critically reassess claims that the conformable derivative is a new fractional derivative operator and clarify its true mathematical nature.

Method: Theoretical analysis showing conformable derivative is equivalent to classical differentiation under nonlinear time scaling, transformation of problems to classical formulations, numerical simulations comparing conformable, classical, and Caputo fractional models.

Result: Conformable derivative is not a fractional operator but a computational tool for power-law time scaling; many "novel fractional" results can be reinterpreted classically; problems can be transformed to classical formulations and solved without nonlocal effects.

Conclusion: Classical derivatives and established fractional derivatives (like Caputo) provide more faithful frameworks for modeling memory-dependent phenomena; the conformable derivative misconception persists despite its limitations.

Abstract: The conformable derivative has been promoted in numerous publications as a new fractional derivative operator. This article provides a critical reassessment of this claim. We demonstrate that the conformable derivative is not a fractional operator but a useful computational tool for systems with power-law time scaling, equivalent to classical differentiation under a nonlinear time reparametrization. Several results presented in the literature as novel fractional contributions can be reinterpreted within a classical framework. We show that problems formulated using the conformable derivative can be transformed into classical formulations via a change of variable. The solution is derived classically and then transformed back, this reformulation highlights the absence of genuinely nonlocal fractional effects. We provide a theoretical analysis, numerical simulations comparing conformable, classical, and truly fractional (Caputo) models, and discuss the reasons why this misconception persists. Our results suggest that classical derivatives, as well as established fractional derivatives, offer a more faithful framework for modeling memory-dependent phenomena.

</details>


### [17] [Regularity of solutions of the Navier-Stokes-αβ equations with wall-eddy boundary conditions](https://arxiv.org/abs/2512.22436)
*Nella Rotundo,Gantumur Tsogtgerel*

Main category: math.AP

TL;DR: First complete analytical treatment of Navier-Stokes-αβ system with wall-eddy boundary conditions, proving global well-posedness, regularity, uniqueness, and stability.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous mathematical analysis of the wall-eddy boundary model proposed by Fried and Gurtin (2008), which models near-wall turbulence through tangential vorticity traction proportional to wall vorticity.

Method: Variational formulation of stationary fourth-order system, proving symmetry and Gårding inequality for bilinear form; verification of Douglis-Nirenberg ellipticity and Lopatinskii-Shapiro covering condition; derivation of hierarchy of energy estimates for nonlinear evolution equation.

Result: Established global well-posedness and regularity for Navier-Stokes-αβ system with wall-eddy boundary conditions; proved full Agmon-Douglis-Nirenberg regularity for coupled system; demonstrated global regularity, uniqueness, and stability.

Conclusion: Provides first complete analytical treatment of Fried and Gurtin's wall-eddy boundary model, establishing rigorous mathematical foundation for this continuum-mechanical model of near-wall turbulence.

Abstract: We establish global well-posedness and regularity for the Navier-Stokes-αβ system endowed with the wall-eddy boundary conditions proposed by Fried and Gurtin (2008). These conditions introduce a tangential vorticity traction proportional to wall vorticity and provide a continuum-mechanical model for near-wall turbulence. Our analysis begins with a variational formulation of the stationary fourth-order system, where we prove symmetry and a Gårding inequality for the associated bilinear form. We then verify Douglis-Nirenberg ellipticity and the Lopatinskii-Shapiro covering condition, establishing full Agmon-Douglis-Nirenberg regularity for the coupled system. Building on this framework, we derive a hierarchy of energy estimates for the nonlinear evolution equation, which yields global regularity, uniqueness, and stability. To our knowledge, this provides the first complete analytical treatment of the wall-eddy boundary model of Fried and Gurtin.

</details>


### [18] [CR Yamabe Equation on the Heisenberg Group via the method of moving spheres](https://arxiv.org/abs/2512.22458)
*Congwen Liu*

Main category: math.AP

TL;DR: All positive solutions to the CR Yamabe equation on the Heisenberg group are Jerison-Lee bubbles, without requiring finite-energy or symmetry assumptions.


<details>
  <summary>Details</summary>
Motivation: To classify positive solutions to the CR Yamabe equation on the Heisenberg group, analogous to the celebrated Caffarelli-Gidas-Spruck classification theorem in Euclidean space.

Method: Developed a systematic approach to implement the method of moving spheres in the setting of the Heisenberg group.

Result: Proved that all positive solutions to the CR Yamabe equation on the Heisenberg group are Jerison-Lee bubbles, without imposing finite-energy or a priori symmetry assumptions.

Conclusion: Established a complete classification theorem for the CR Yamabe equation on the Heisenberg group, analogous to the Caffarelli-Gidas-Spruck result in Euclidean space.

Abstract: In this paper, we classify positive solutions to the CR Yamabe equation on the Heisenberg group $\mathbb{H}^n$. We show that all such solutions are Jerison-Lee bubbles, without imposing any finite-energy or a priori symmetry assumptions. This result can be regarded as an analogue for $\mathbb{H}^n$ of the celebrated Caffarelli-Gidas-Spruck classification theorem in $\mathbb{R}^n$. To establish this, we develop a systematic approach to implement the method of moving spheres in the setting of the Heisenberg group.

</details>


### [19] [Finite propagation and saturation in reaction-diffusion-advection equations governed by p-Laplacian operator](https://arxiv.org/abs/2512.22493)
*Cristina Marcelli*

Main category: math.AP

TL;DR: Analysis of traveling wave solutions for a mono-stable reaction-diffusion-advection equation with nonlinear diffusion and advection terms, focusing on existence, non-existence, and classification of solution types.


<details>
  <summary>Details</summary>
Motivation: To study front propagation phenomena in a generalized reaction-diffusion-advection equation with nonlinear diffusion (p-Laplacian type) and advection terms, which extends classical models to more complex scenarios.

Method: Mathematical analysis of traveling wave solutions, including existence and non-existence proofs, and classification criteria based on whether solutions attain equilibrium states at finite time and their regularity properties.

Result: Provides criteria to determine if traveling wave solutions reach one or both equilibria at finite time, and whether they are continuable as C¹-solutions or are sharp solutions.

Conclusion: The paper establishes a comprehensive classification framework for traveling wave solutions in generalized reaction-diffusion-advection equations, characterizing their behavior near equilibrium states and regularity properties.

Abstract: The paper concerns front propagation for the following mono-stable reaction-diffusion-advection equation \[f(u)u_x + g(u)u_τ= [d(u)|u_x|^{p-2} u_x]_x+ ρ(u), \quad (x,τ)\in \R\times [0,+\infty).\] Besides existence and non-existence results for traveling wave solutions, the main focus is their classification: we provide criteria to establish if they attain one or both the equilibria at a finite time and in this case, if they are continuable as $C^1$-solutions or if they are sharp solutions.

</details>


### [20] [Wave dynamics governing vortex breakdown in smooth Euler flows](https://arxiv.org/abs/2512.22543)
*Tsuyoshi Yoneda*

Main category: math.AP

TL;DR: Vortex breakdown in 3D Euler equations is governed by wave dynamics from underlying transport flow, using Lagrangian coordinates to eliminate Lie bracket.


<details>
  <summary>Details</summary>
Motivation: To understand vortex breakdown in 3D incompressible Euler equations when a vortex is transported by a prescribed Lagrangian flow, avoiding singular integral representations of pressure.

Method: Construct an effective Lagrangian coordinate system where the associated Lie bracket vanishes identically, avoiding singular integral representation of pressure term.

Result: Shows that vortex breakdown is governed by wave dynamics generated by the underlying transport flow.

Conclusion: Vortex breakdown in 3D Euler equations can be understood through wave dynamics from transport flow using Lagrangian coordinates that eliminate Lie bracket issues.

Abstract: We consider the three-dimensional incompressible Euler equations in a setting where a vortex is transported by a prescribed Lagrangian flow. We show that vortex breakdown is governed by wave dynamics generated by the underlying transport flow. The key idea is to avoid any singular integral representation of the pressure term and instead construct an effective Lagrangian coordinate system in which the associated Lie bracket vanishes identically.

</details>


### [21] [The fibre operators in the Bloch-Floquet decomposition of periodic magnetic pseudo-differential operators](https://arxiv.org/abs/2512.22547)
*Horia D. Cornean,Bernard Helffer,Radu Purice*

Main category: math.AP

TL;DR: The paper studies fiber operators for periodic magnetic pseudo-differential operators, deriving explicit distribution kernel formulas and proving these fiber operators are toroidal pseudo-differential operators.


<details>
  <summary>Details</summary>
Motivation: To understand the structure of fiber operators corresponding to periodic magnetic pseudo-differential operators with periodic magnetic potentials, which is important for spectral analysis and mathematical physics applications.

Method: Analyzes fiber operators using two perspectives: as operators on d-dimensional torus, and as infinite matrices on discrete ℓ² space via discrete Fourier transform. Derives explicit formulas for their distribution kernels.

Result: Obtains explicit formulas for distribution kernels of fiber operators in both representations. Proves that these fiber operators are toroidal pseudo-differential operators using the derived distribution kernels.

Conclusion: The fiber operators of periodic magnetic pseudo-differential operators with periodic magnetic potentials have explicit distribution kernels and are themselves toroidal pseudo-differential operators, providing important structural insights.

Abstract: We study the structure of the fibre operators corresponding to periodic magnetic pseudo-differential operators having periodic magnetic potentials. We obtain explicit formulas for their distribution kernel, both when these fibres are seen as operators on the $d$-dimensional torus, and also when they are seen as infinite matrices acting on a discrete $\ell^2$ space via a discrete Fourier transform. Moreover, using these distribution kernels we prove that the fibre operators are toroidal pseudo-differential operators.

</details>


### [22] [On a Thermodynamically Consistent Diffuse-Interface Model for Incompressible Two-Phase Flows with Chemotaxis and Mass Transport](https://arxiv.org/abs/2512.22585)
*Andrea Giorgini,Jingning He,Hao Wu*

Main category: math.AP

TL;DR: Global existence and regularity results for a thermodynamically consistent Navier-Stokes/Cahn-Hilliard model of two-phase flow with chemotaxis effects and unmatched densities.


<details>
  <summary>Details</summary>
Motivation: To study a hydrodynamic system describing two-phase incompressible flows with unmatched densities coupled with soluble chemical species, incorporating chemotaxis effects and mass transport processes derived from Onsager's variational principle.

Method: Use approximation schemes with compactness methods to establish global finite energy and weak solutions in 2D. Analyze three decoupled subsystems with bootstrap arguments to prove global strong solution existence/uniqueness and regularity propagation.

Result: Proved global existence of finite energy solutions and weak solutions. Showed existence/uniqueness of global strong solutions for regular initial data. Demonstrated that chemical density stays bounded if initial data is bounded, preventing concentration singularities unlike Keller-Segel systems.

Conclusion: The thermodynamically consistent model exhibits better regularity properties than classical Keller-Segel systems, with chemical diffusion driven by potential gradients preventing singularity formation, offering a significant distinction in chemotaxis modeling.

Abstract: We investigate a hydrodynamic system of Navier--Stokes/Cahn--Hilliard type, which describes the motion of a two-phase flow of two incompressible fluids with unmatched densities coupled with a soluble chemical species. Derived from Onsager's variational principle, this thermodynamically consistent diffuse-interface model incorporates both the chemotaxis effects induced by the chemical species and the mass transport processes within the mixture. For the two-dimensional initial-boundary value problem, we establish the existence of global finite energy solutions and global weak solutions, using a suitable approximation scheme combined with compactness methods. Next, by carefully analyzing three decoupled subsystems and employing a bootstrap argument, we prove the existence and uniqueness of a global strong solution for sufficiently regular initial data, as well as the propagation of regularity for global weak solutions. In particular, we show that the density of the chemical substance stays bounded for all time if its initial datum is bounded. This implies a significant distinction from the classical Keller--Segel system: diffusion driven by the chemical potential gradient can prevent the formation of concentration singularities.

</details>


### [23] [Dispersive estimates for discrete Klein-Gordon equations on one-dimensional lattice with quasi-periodic potentials](https://arxiv.org/abs/2512.22613)
*Zhiqiang Wan,Heng Zhang*

Main category: math.AP

TL;DR: The paper proves dispersive estimates for discrete Klein-Gordon equation with quasi-periodic potentials, showing persistence of time-decay rate, and applies these to derive Strichartz estimates and establish global well-posedness for nonlinear versions.


<details>
  <summary>Details</summary>
Motivation: To understand how quasi-periodic potentials affect dispersive properties of discrete Klein-Gordon equations and whether the fundamental time-decay rate can be preserved despite the presence of such potentials.

Method: Proving ℓ¹→ℓ∞ dispersive estimates for discrete Klein-Gordon equation on ℤ with small real-analytic quasi-periodic potentials, showing the time-decay rate persists as (1/3)− (just below 1/3).

Result: Successfully established that the time-decay rate of (1/3)− persists for discrete Klein-Gordon equation with small quasi-periodic potentials, derived corresponding Strichartz estimates, and proved small-data global well-posedness for the associated nonlinear equation.

Conclusion: Small real-analytic quasi-periodic potentials do not destroy the fundamental dispersive properties of discrete Klein-Gordon equations, allowing for persistence of time-decay rates and enabling global well-posedness results for nonlinear versions.

Abstract: We prove $\ell^{1}\!\to\!\ell^{\infty}$ dispersive estimates for the discrete Klein--Gordon equation on $\mathbb Z$ with small real-analytic quasi-periodic potentials, showing that the time-decay rate persists as $(\tfrac13)^{-}$. As applications, we derive the corresponding Strichartz estimates and establish small-data global well-posedness for the associated nonlinear discrete Klein--Gordon equation.

</details>


### [24] [Ground states of the Schrödinger equation coupled with fourth-order gravitation -- Part 1: the case $K_{a, b} \leq 0$](https://arxiv.org/abs/2512.22619)
*Gustavo de Paula Ramos*

Main category: math.AP

TL;DR: This paper studies ground states of a normalized nonlocal semilinear problem in ℝ³ with a specific kernel K_{a,b} and singular potential V, arising from Schrödinger equations coupled with nonrelativistic gravitational potential in fourth-order gravity theories.


<details>
  <summary>Details</summary>
Motivation: The problem originates from seeking standing waves of the Schrödinger equation coupled with nonrelativistic gravitational potential in fourth-order gravity theories. The specific kernel K_{a,b} represents gravitational interactions, and understanding ground states is crucial for physical applications in quantum gravity models.

Method: The authors analyze the normalized nonlocal semilinear problem with kernel K_{a,b} and singular potential V. They use variational methods and asymptotic analysis to study existence/nonexistence of ground states for different parameter regimes of (a,b).

Result: (i) Complete existence/nonexistence picture for ground states of the autonomous problem across all K_{a,b} geometries; (ii) Conditions ensuring existence of ground states for the nonautonomous problem when K_{a,b} ≤ 0; (iii) Convergence results showing that as (a,b) approaches certain limits, ground states converge to solutions of Schrödinger equation, Choquard equation, or rescaled Choquard equation.

Conclusion: The paper provides comprehensive results on ground states for this class of nonlocal problems, establishing connections between different limiting cases and offering insights into the mathematical structure of gravitational interactions in fourth-order gravity theories.

Abstract: We are interested in the existence and asymptotic behavior of ground states of the following normalized nonlocal semilinear problem: \[ \begin{cases} - Δu + (V - ω) u + (K_{a, b} \ast u^2) u = 0 &\text{in} ~ \mathbb{R}^3; \\ \|u\|_{\mathscr{L}^2}^2 = μ, \end{cases} \] where \[ K_{a, b} (x) := \frac{1}{|x|} \left( \frac{4}{3} e^{- b |x|} - \frac{1}{3} e^{- a |x|} - 1 \right); \] $0 \leq a, b \leq \infty$; $V$ denotes a singular potential that vanishes at infinity and the unknowns are $ω\in \mathbb{R}$, $u \colon \mathbb{R}^3 \to \mathbb{R}$. This problem is obtained by looking for standing waves of the Schrödinger equation coupled with the nonrelativistic gravitational potential prescribed by a family of fourth-order gravity theories. In this paper, (i) we obtain a complete picture of the existence/nonexistence of ground states of the associated autonomous problem for every possible geometry of $K_{a, b}$, (ii) we obtain conditions that ensure the existence of ground states of the nonautonomous problem when $K_{a, b} \leq 0$ and (iii) we prove that as \[ (a, b) \to (A, B) \in \left\{(0, 0), (\infty, \infty), (0, \infty)\right\}, \] ground states of this problem respectively converge to a ground state of (1) the Schrödinger equation, (2) the Choquard equation and (3) a rescaling of the Choquard equation.

</details>


### [25] [Asymptotic behavior of a nonlinear shallow shell model when the shell becomes a plate](https://arxiv.org/abs/2512.22677)
*Trung Hieu Giang,Ngoc Quynh Nguyen*

Main category: math.AP

TL;DR: Analysis of asymptotic behavior of minimizing solutions for nonlinear shallow shell model under general applied forces, extending previous work with more restrictive force assumptions.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of minimizing solutions for the Donnell-Vlasov-Mushtari-Galimov-Koiter nonlinear shallow shell model under general applied forces, extending beyond previous work that only considered vanishing tangential force components.

Method: Mathematical analysis of the nonlinear shallow shell model, studying the asymptotic behavior of minimizing solutions through theoretical methods and extending previous results.

Result: Established results on asymptotic behavior of minimizing solutions that apply to general applied forces, substantially extending previous work that required vanishing tangential force components.

Conclusion: The paper successfully extends the understanding of asymptotic behavior in nonlinear shallow shell models to more general force conditions, providing a broader theoretical framework for analyzing these mechanical systems.

Abstract: This paper studies a nonlinear shallow shell model proposed by Donnell, Vlasov, Mushtari, Galimov, and Koiter. More specifically, we address the question concerning the asymptotic behavior of minimizing solutions. Our result can be applied to general applied forces. Thus, it substantially extends the one given in \cite{oana2} whereby the tangential components of the applied forces are assumed to vanish.

</details>


### [26] [Global Martingale Entropy Solutions to the Stochastic Isentropic Euler Equations](https://arxiv.org/abs/2512.22719)
*Gui-Qiang G. Chen,Feimin Huang,Danli Wang*

Main category: math.AP

TL;DR: Existence and compactness of global martingale entropy solutions for stochastically forced isentropic Euler equations with general pressure law, using stochastic compensated compactness framework.


<details>
  <summary>Details</summary>
Motivation: To establish existence and compactness results for stochastic isentropic Euler equations with finite relative-energy solutions, overcoming the lack of uniform L^∞ bounds due to stochastic forcing.

Method: Develop stochastic compensated compactness framework in L^p, use vanishing viscosity method with careful uniform estimates of stochastic approximate solutions, derive higher-order relative energy estimates.

Result: Proved existence of global martingale entropy solutions with finite relative-energy, established local mechanical energy inequality for polytropic pressure case without requiring higher moment estimates for entropy.

Conclusion: The developed stochastic compensated compactness framework and uniform estimate techniques provide a foundation for studying similar stochastic PDE problems.

Abstract: We establish the existence and compactness of global martingale entropy solutions with finite relative-energy for the stochastically forced system of isentropic Euler equations governed by a general pressure law. To achieve these, a stochastic compensated compactness framework in $L^p$ is developed to overcome the difficulty that the uniform $L^{\infty}$ bound for the stochastic approximate solutions is unavailable, owing to the stochastic forcing term. The convergence of the vanishing viscosity method is established by employing the stochastic compactness framework, along with careful uniform estimates of the stochastic approximate solutions, to obtain the existence of global martingale entropy solutions with finite relative-energy. In particular, in the polytropic pressure case for all adiabatic exponents, we prove that the global solutions satisfy the local mechanical energy inequality when the initial data are only required to have finite relative-energy (while the higher moment estimates for entropy are not required here, as needed in the earlier work). Higher-order relative energy estimates for approximate solutions are also derived to establish the entropy inequality for more convex entropy pairs and to then prove the compactness of solutions to the stochastic isentropic Euler system. The stochastic compensated compactness framework and the uniform estimate techniques for approximate solutions developed in this paper should be useful in the study of other similar problems.

</details>


### [27] [Blowup rate for rotational NLS with a repulsive potential](https://arxiv.org/abs/2512.22821)
*Yi Hu,Yongki Lee,Shijun Zheng*

Main category: math.AP

TL;DR: Analytical proof of log-log blowup rate for mass-critical rotating NLS with repulsive harmonic potential, showing that increasing repulsive potential strength can prevent blowup and yield global solutions.


<details>
  <summary>Details</summary>
Motivation: To understand blowup dynamics in mass-critical nonlinear Schrödinger equations with rotation and repulsive harmonic potential, particularly how repulsive potentials affect blowup rates and can prevent singularity formation.

Method: Uses virial identity and R_γ-transform (pseudo-conformal transform in this setting) for analytical proof, plus numerical simulations with dynamic rescaling and adaptive mesh refinement.

Result: Proves log-log blowup rate for initial data with mass slightly above ground state Q, obtains mass concentration description near blowup time, and shows that increasing repulsive potential strength can yield global solutions.

Conclusion: Repulsive harmonic potentials can suppress blowup in focusing rotating NLS, contrasting with attractive potentials, with analytical and numerical evidence supporting log-log blowup rate and mass concentration behavior.

Abstract: In this paper we give an analytical proof of the ``$\log$-$\log$'' blowup rate for mass-critical nonlinear Schrödinger equation (NLS) with a rotation ($Ω\neq 0$) and a repulsive harmonic potential $V_γ(x) = \textrm{sgn}(γ) γ^2 |x|^2$, $γ< 0$ when the initial data has a mass slightly above that of $Q$, the ground state solution to the free NLS. The proof is based on a virial identity and an $\mathcal{R}_γ$-transform, a pseudo-conformal transform in this setting. Further, we obtain a limiting behavior description concerning the mass concentration near blowup time. A remarkable finding is that increasing the value $|γ|$ for the repulsive potential $V_γ$ can give rise to global in time solution for the focusing RNLS, which is in contrast to the case where $γ$ is positive. This kind of phenomenon was earlier observed in the non-rotational case $Ω= 0$ in Carles' work. In addition, we provide numerical simulations to partially illustrate the blowup profile along with the blowup rate using dynamic rescaling and adaptive mesh refinement method.

</details>


### [28] [Crystalline Motion of discrete interfaces in the Blume-Emery-Griffiths Model: partial wetting](https://arxiv.org/abs/2512.22870)
*Marco Cicalese,Giuliana Fusco,Giovanni Savaré*

Main category: math.AP

TL;DR: The paper extends variational analysis of lattice systems modeling two immiscible phases with surfactant from complete to partial wetting regimes, deriving continuum evolution that shows new phenomena like moving/pinned facet coexistence and metastable states.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between microscopic lattice models and experimentally observed surfactant-induced pinning phenomena in immiscible systems, particularly extending previous work on completely wetted crystals to the more realistic regime of partial wetting where surfactant occupies only portions of the interface.

Method: Using a minimizing-movements scheme within variational framework to rigorously derive continuum evolution from discrete lattice systems of Blume-Emery-Griffith type, analyzing the coupling between interfacial motion and surfactant redistribution in partial wetting regime.

Result: The derived continuum evolution reveals complex coupling between interfacial motion and surfactant redistribution, exhibiting new features including coexistence of moving and pinned facets, emergence of long-lived metastable states, and surfactant-induced pinning phenomena absent in fully wetted case.

Conclusion: This provides the first discrete-to-continuum variational description of partially wetted crystalline interfaces, successfully connecting microscopic lattice models with experimental observations of surfactant effects in immiscible systems.

Abstract: We continue the variational study of the discrete-to-continuum evolution of lattice systems of Blume-Emery-Griffith type which model two immiscible phases in the presence of a surfactant. In our previous work \cite{CFS}, we analyzed the case of a completely wetted crystal and described how the interplay between surfactant evaporation and mass conservation leads to a transition between crystalline mean curvature flow and pinned evolutions. In the present paper, we extend the analysis to the regime of partial wetting, where the surfactant occupies only a portion of the interface. Within the minimizing-movements scheme, we rigorously derive the continuum evolution and show how partial wetting introduces a complex coupling between interfacial motion and redistribution of surfactant. The resulting evolution exhibits new features absent in the fully wetted case, including the coexistence of moving and pinned facets or the emergence and long-lived metastable states. This provides, to our knowledge, the first discrete-to-continuum variational description of partially wetted crystalline interfaces, bridging the gap between microscopic lattice models and experimentally observed surfactant-induced pinning phenomena in immiscible systems.

</details>


### [29] [Refined Limiting Profiles of the Principal Eigenvalue Problems with Large Advection](https://arxiv.org/abs/2512.22918)
*Yujin Guo,Yuan Lou,Hongfei Zhang*

Main category: math.AP

TL;DR: The paper analyzes the refined limiting profiles of principal eigenpairs for an eigenvalue problem with advection term as the advection coefficient α→∞, showing the visible effect of large advection on eigenvalues and eigenfunctions.


<details>
  <summary>Details</summary>
Motivation: To understand how large advection affects the principal eigenpair (λ, φ) in eigenvalue problems with advection terms, which is important for applications where advection dominates diffusion.

Method: Analyzes the refined limiting profiles of the principal eigenpair (λ, φ) for the eigenvalue problem with advection term as α→∞, using asymptotic analysis techniques on the PDE with diffusive and advection coefficients.

Result: Obtains refined expansions showing the visible effect of large advection on the principal eigenpair, with the argument being applicable to general principal eigenvalue problems.

Conclusion: The analysis provides detailed understanding of how large advection influences eigenvalue problems, with the methodology applicable to broader classes of eigenvalue problems with advection terms.

Abstract: In this paper, we are concerned with the following eigenvalue problem with an advection term: \begin{equation}\label{0.1} \left\{ \begin{split} -εΔφ-2α\nabla m(x)\cdot\nabla φ+V(x)φ&=λφ \ \text{in}\ \ Ω,\\ φ&=0\ \ \hbox{on}\ \ \partialΩ, ~~~\text{(0.1)} \end{split} \right. \end{equation} where $Ω\subset\mathbb{R}^N~(N\geq1)$ satisfying $\partialΩ\in C^{2}$ is a bounded domain and contains the origin as an interior point, the constants $ε>0$ and $α>0$ are the diffusive and advection coefficients, respectively, and $m(x)\in C^{2}(\barΩ)$, $V (x)\in C^γ(\barΩ)~(0<γ<1)$ are given functions. We analyze the refined limiting profiles of the principal eigenpair $(λ, φ)$ for (0.1) as $α\rightarrow\infty$, which display the visible effect of the large advection on $(λ, φ)$. It expects that our argument is applicable to investigating the refined expansions of the general principal eigenvalue problems.

</details>


### [30] [Diffusion wave phenomena and optimal time decay for incompressible viscoelastic flows](https://arxiv.org/abs/2512.22921)
*Shenghan Li,Yong Wang*

Main category: math.AP

TL;DR: The paper establishes L^p decay estimates for 3D incompressible viscoelastic flows using wave equation representation and stationary phase methods, revealing hyperbolic nature.


<details>
  <summary>Details</summary>
Motivation: Motivated by D. Hoff and K. Zumbrun's work on diffusion wave phenomena, the authors investigate similar phenomena in three-dimensional incompressible viscoelastic flows.

Method: Employ representation formula of the wave equation and stationary phase methods on the sphere S^{d-1} to analyze the solution behavior.

Result: Establish L^p decay estimates for the solution over the entire range 1 ≤ p ≤ ∞, demonstrating hyperbolic characteristics of incompressible viscoelastic flows.

Conclusion: The analysis reveals the hyperbolic nature of incompressible viscoelastic flows through comprehensive decay estimates obtained via wave equation techniques.

Abstract: Motivated by the work of D. Hoff and K. Zumbrun (Indiana Univ. Math. J. 44: 603-676, 1995), we investigate the diffusion wave phenomena in three-dimensional incompressible viscoelastic flows. By employing the representation formula of the wave equation and the stationary phase methods on the sphere $\mathbb{S}^{d-1}$, we establish $L^p$ decay estimates for the solution over the whole range $1\leq p \leq \infty$, which reveals the hyperbolic nature of the incompressible viscoelastic flows.

</details>


### [31] [Determining habitat anomalies in cross-diffusion predator-prey chemotaxis models](https://arxiv.org/abs/2512.22946)
*Yuhan Li,Hongyu Liu,Catharine W. K. Lo*

Main category: math.AP

TL;DR: Unique identification of spatial anomalies (habitat degradation zones) and ecological parameters in predator-prey systems using only boundary measurements.


<details>
  <summary>Details</summary>
Motivation: Addresses the inverse problem at the interface of mathematical analysis and spatial ecology: identifying unknown spatial anomalies and ecological parameters from limited boundary data, bridging ecological sensing with quantitative inference of internal habitat heterogeneity.

Method: Formulates problem as simultaneous recovery of unknown interior subdomain and discontinuous ecological interaction rules across its boundary. Develops unified theoretical framework for unique determination of anomaly geometry and discontinuous coefficients characterizing altered interactions within degraded region.

Result: Develops theoretical results covering smooth anomalies in time-dependent systems and extends to non-smooth polyhedral inclusions in stationary regimes. Provides mathematical basis for detecting and characterizing habitat degradation from limited external data.

Conclusion: Bridges gap between ecological sensing and quantitative inference of internal habitat heterogeneity, offering mathematical framework for detecting habitat degradation zones and altered ecological interactions using only boundary measurements.

Abstract: This paper addresses an open inverse problem at the interface of mathematical analysis and spatial ecology: the unique identification of unknown spatial anomalies -- interpreted as zones of habitat degradation -- and their associated ecological parameters in multi-species predator-prey systems with multiple chemical signals, using only boundary measurements. We formulate the problem as the simultaneous recovery of an unknown interior subdomain and discontinuous ecological interaction rules across its boundary. A unified theooretical framework is developed that unique determines both the anomaly's geometry and discontinuous coefficients characterizing the altered interactions within the degraded region. Our results cover smooth anomalies in time-dependent systems and are extended to non-smooth polyhedral inclusions in stationary regimes. This work bridges a gap between ecological sensing and the quantitative inference of internal habitat heterogeneity, offering a mathamtical basis for detecting and characterizing habitat degradation from limited external data.

</details>


### [32] [Standing waves of the Anderson-Gross-Pitaevskii equation](https://arxiv.org/abs/2512.22960)
*Samaël Mackowiak*

Main category: math.AP

TL;DR: Construction of standing wave solutions for the Anderson-Gross-Pitaevskii equation (nonlinear Schrödinger with confining potential and spatial white noise) in 1D and 2D using variational methods.


<details>
  <summary>Details</summary>
Motivation: Study standing waves for the Anderson-Gross-Pitaevskii equation, which combines nonlinear Schrödinger dynamics with confining potential and multiplicative spatial white noise, representing quantum systems with disorder.

Method: Variational methods to construct standing wave solutions by solving the associated nonlinear elliptic equation with spatial white noise potential.

Result: Successful construction of standing wave solutions with results on their regularity properties, spatial localization behavior, and stability characteristics.

Conclusion: Standing waves can be constructed for the Anderson-Gross-Pitaevskii equation in dimensions 1 and 2, with established regularity, localization, and stability properties using variational approaches.

Abstract: In this paper, we study standing waves for the Anderson-Gross-Pitaevskii equation in dimension 1 and 2. The Anderson-Gross-Pitaevskii equation is a nonlinear Schrödinger equation with a confining potential and a multiplicative spatial white noise. Standing waves are characterized by a profile which is invariant by the dynamic and solves a nonlinear elliptic equation with spatial white noise potential. We construct such solutions via variational methods and obtain some results on their regularity, localization and stability.

</details>


### [33] [Derivation of nonlinear time-dependent macroscopic conductivity for an electropermeabilization model via homogenization](https://arxiv.org/abs/2512.23007)
*Tobias Gebäck,Ioanna Motschan-Armen,Irina Pettersson*

Main category: math.AP

TL;DR: Mathematical analysis of electropermeabilization in biological tissue using homogenization theory to derive effective macroscopic conductivity with memory effects.


<details>
  <summary>Details</summary>
Motivation: To develop a rigorous mathematical model explaining experimentally observed nonlinear conductivity dynamics in biological tissues during electropermeabilization, where tissue conductivity varies with electric field strength in a sigmoid pattern.

Method: Uses homogenization theory with periodic microstructure, starting from cell-level model describing electric potential and porosity degree. Performs dimension analysis with small parameter ε (cell-to-tissue size ratio), proves well-posedness of microscopic problem, derives a priori estimates, obtains formal asymptotics, and rigorously justifies expansion using two-scale convergence with monotonicity arguments.

Result: Derives macroscopic model with memory effects and nonlinear time-dependent effective current. The model captures nontrivial evolution of effective conductivity including characteristic drop reflecting capacitive behavior of lipid bilayer. Numerical computations show tissue conductivity varies nonlinearly with electric field strength despite constant microscopic conductivity, exhibiting sigmoid trend.

Conclusion: Provides rigorous mathematical explanation for experimentally observed conductivity dynamics in electropermeabilization, demonstrating how homogenization theory can capture complex tissue-level behavior from cell-scale physics.

Abstract: We study a phenomenological electropermeabilization model in a periodic medium representing biological tissue. Starting from a cell-level model describing the electric potential and the degree of porosity, we perform dimension analysis to identify a relevant scaling in terms of a small parameter $\ve$ - the ratio between the cell and the tissue size. The electric potential satisfies electrostatic equations in the extra- and intracellular domains, while its jump across the cell membrane evolves according to a nonlinear law coupled with an ordinary differential equation for the porosity degree. We prove the well-posedness of the microscopic problem, derive a priori estimates, obtain formal asymptotics, and rigorously justify the expansion combining two-scale convergence with monotonicity arguments. The resulting macroscopic model exhibits memory effects and a nonlinear, time-dependent effective current. It captures the nontrivial evolution of effective conductivity, including a characteristic drop reflecting the capacitive behavior of the lipid bilayer, in agreement with experimental data. Numerical computations of the effective conductivity confirm that, although microscopic conductivity is constant, tissue conductivity varies nonlinearly with electric field strength, showing a sigmoid trend. This suggests a rigorous mathematical explanation for experimentally observed conductivity dynamics.

</details>


### [34] [Guillarmou's Normal Operator for Magnetic and Thermostat Flows](https://arxiv.org/abs/2512.23106)
*Sebastián Muñoz-Thon,Sean Richardson*

Main category: math.AP

TL;DR: Generalization of Guillarmou's normal operator to thermostat and magnetic flows, showing they're elliptic pseudodifferential operators of order -1, with application to stability estimate for magnetic X-ray transform.


<details>
  <summary>Details</summary>
Motivation: Extend the theory of normal operators from geodesic X-ray transforms to more general dynamical systems (thermostat and magnetic flows), building on Guillarmou's work for Anosov manifolds.

Method: Generalize Guillarmou's normal operator construction to thermostat flows and magnetic flows under certain dynamical assumptions, analyzing their properties as pseudodifferential operators.

Result: Proved that the generalized normal operators for both thermostat and magnetic flows are elliptic pseudodifferential operators of order -1.

Conclusion: Successfully extended the normal operator framework to broader classes of flows, enabling new applications including stability estimates for magnetic X-ray transforms.

Abstract: Guillarmou's normal operator over a closed Anosov manifold is analogous to the classical normal operator of the geodesic X-ray transform over manifolds with boundary. In this paper, we generalize this normal operator, under some dynamical assumptions, to thermostat flows as well as to the case of the magnetic flows. In particular, we show that these generalized normal operators are elliptic pseudodifferential operators of order -1 in each case. As an application, we prove a stability estimate for the magnetic X-ray transform.

</details>


### [35] [Qualitative analysis on the critical points of the Kirchhoff-Routh function](https://arxiv.org/abs/2512.23172)
*Francesca Gladiali,Massimo Grossi,Peng Luo,Shusen Yan*

Main category: math.AP

TL;DR: The paper studies critical points of the Kirchhoff-Routh function in domains with small holes, establishing exact number, location, nondegeneracy of critical points, and existence of multiple two-peak solutions for elliptic problems.


<details>
  <summary>Details</summary>
Motivation: The Kirchhoff-Routh function arises from concentration phenomena in nonlinear elliptic problems and de-singularization problems for steady Euler equations. Understanding its critical points is important for studying vortex dynamics and concentration phenomena in PDEs.

Method: Analyze the Kirchhoff-Routh function in domains with small holes using perturbation methods and asymptotic analysis. Study the exact number and location of critical points, prove their nondegeneracy, and examine how hole location affects critical points.

Result: Established exact number and location of critical points of Kirchhoff-Routh function in domains with small holes, proved their nondegeneracy, showed hole location plays crucial role, and demonstrated existence of multiple two-peak solutions for elliptic problems.

Conclusion: The study provides complete characterization of critical points for Kirchhoff-Routh function in domains with holes, revealing important geometric effects and establishing existence results for multiple solutions in related elliptic problems.

Abstract: In this paper, we study the number of critical points of the Kirchhoff-Routh function \begin{equation*} \mathcal{KR}_D(x,y)=Λ_1^2\mathcal{R}_D(x)+Λ_2^2\mathcal{R}_D(y)-2Λ_1Λ_2G_D(x,y), \end{equation*} where $D$ is a bounded domain in $\mathbb{R}^2$, $x,y\in D$, $Λ_1,Λ_2>0$, $\mathcal{R}_D$ is the Robin function, and $G_D$ is the Green function of the operator $-Δ$ with $0$ Dirichlet boundary condition on $D$. This function arises from concentration phenomena in nonlinear elliptic problems and from the de-singularization problem for the steady Euler equation. For domains with a small hole, we establish not only the exact number and the location of the critical points of $\mathcal{KR}_D$, but also their nondegeneracy. We show that the location of the hole plays a crucial role. Finally in the context of elliptic problems, we establish the existence of multiple two-peak solutions.

</details>


### [36] [Local well-posedness of the Schrödinger flow into $\mathbb{S}^2$ with natural boundary conditions](https://arxiv.org/abs/2512.23201)
*Bo Chen,Youde Wang*

Main category: math.AP

TL;DR: New approximation scheme developed to resolve local well-posedness for Landau-Lifshitz equation (Schrödinger flow into S²) with natural boundary conditions


<details>
  <summary>Details</summary>
Motivation: The Landau-Lifshitz equation, which describes Schrödinger flow into the standard unit 2-sphere, has unresolved local well-posedness problems with natural boundary conditions that need to be addressed.

Method: Developed a new approximation scheme specifically designed to handle the Landau-Lifshitz equation with its natural boundary conditions, focusing on establishing local well-posedness.

Result: The paper presents a resolution to the local well-posedness problem for the Landau-Lifshitz equation with natural boundary conditions through the developed approximation scheme.

Conclusion: The new approximation scheme successfully resolves the local well-posedness problem for the Landau-Lifshitz equation with natural boundary conditions, providing a mathematical foundation for studying this important geometric PDE.

Abstract: In this paper, we develop a new approximation scheme to resolve the local well-posedness problem for the Landau-Lifshitz equation (i.e., the Schrödinger flow into the standard unit 2-sphere $\mathbb{S}^2\subset \mathbb{R}^3$) with natural boundary conditions.

</details>


### [37] [Ground States for the Nonlinear Schr{ö}dinger Equation on Open Books and Dimensional Reduction to Metric Graphs](https://arxiv.org/abs/2512.23286)
*Stefan Le Coz,Boris Shakarov*

Main category: math.AP

TL;DR: Study of dimensional reduction from 2D open books to metric graphs, showing critical width threshold for ground state dimensionality.


<details>
  <summary>Details</summary>
Motivation: To understand how stationary states on two-dimensional open book structures reduce to their counterparts on metric graphs in the shrinking limit, and to characterize the transition between one-dimensional and two-dimensional ground states.

Method: Develop functional-analytic framework for variational problems on open books, prove existence of solutions as constrained action minimizers, analyze graph-based open books (isomorphic to product of graph with interval), and establish critical transverse width threshold.

Result: Existence of sharp transition: below critical transverse width, all ground states coincide with graph ground states trivially extended; above critical width, ground states become genuinely two-dimensional.

Conclusion: Open books exhibit dimensional reduction to metric graphs with a clear threshold separating one-dimensional and two-dimensional ground state regimes, providing mathematical framework for understanding such transitions.

Abstract: In this work, we study the dimensional reduction of stationary states in the shrinking limit for a broad class of two-dimensional domains, called open books, to their counterparts on metric graphs. An open book is a two-dimensional structure formed by rectangular domains sharing common boundaries. We first develop a functional-analytic framework suited to variational problems on open books and establish the existence of solutions as constrained action minimizers. For graph-based open books (i.e., those isomorphic to the product of a graph with an interval) we prove the existence of a sharp transition in the dimensionality of ground states. Specifically, there exists a critical transverse width: below this threshold, all ground states coincide with the ground states on the underlying graph trivially extended in the transverse direction; above it, ground states become genuinely two-dimensional.

</details>


### [38] [Normalized solutions of nonlinear magnetic Schrödinger equations on metric graphs](https://arxiv.org/abs/2512.23321)
*Pietro d'Avenia,Zhentao He,Chao Ji*

Main category: math.AP

TL;DR: The paper establishes magnetic Sobolev space theory on metric graphs, proves self-adjointness of magnetic Schrödinger operators, and studies normalized solutions to nonlinear magnetic Schrödinger equations on both compact and noncompact metric graphs across different mass regimes.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive mathematical framework for studying magnetic Schrödinger equations on metric graphs, which are important models for quantum networks and nanostructures, and to understand the existence and multiplicity of normalized solutions in various settings.

Method: First establishes the theory of magnetic Sobolev spaces H^1_A(G,C) on metric graphs, proves self-adjointness of corresponding magnetic Schrödinger operators, then uses variational methods and functional analysis to study normalized solutions to nonlinear magnetic Schrödinger equations on both compact and noncompact metric graphs with different types of nonlinearities.

Result: Develops the mathematical foundation for magnetic Sobolev spaces on metric graphs, proves self-adjointness of magnetic Schrödinger operators, and obtains existence and multiplicity results for normalized solutions across mass-subcritical, mass-critical, and mass-supercritical cases in various graph settings.

Conclusion: The paper provides a complete theoretical framework for magnetic Schrödinger equations on metric graphs and establishes comprehensive results about normalized solutions, advancing the mathematical understanding of quantum phenomena on network structures.

Abstract: In this paper we first establish the theory of a magnetic Sobolev space $H^1_A(\mathcal{G},\mathbb{C})$ on metric graphs $\mathcal{G}$ and we prove the self-adjointness of its corresponding magnetic Schrödinger operator. Then, in this setting, we investigate the existence and multiplicity of normalized solutions to nonlinear magnetic Schrödinger equations on compact metric graphs and on noncompact metric graphs with localized nonlinearities or nonlinearities acting on whole metric graphs, covering the mass-subcritical, mass-critical, and mass-supercritical cases.

</details>


### [39] [Solutions of the singular Yamabe problem near singular boundaries](https://arxiv.org/abs/2512.23331)
*Weiming Shen,Zhehui Wang,Jiongduo Xie*

Main category: math.AP

TL;DR: Extends asymptotic analysis of singular Yamabe problem solutions near boundaries in non-flat metrics, showing local solutions approximate tangent cone solutions for Lipschitz domains with conical structure.


<details>
  <summary>Details</summary>
Motivation: Previous work on singular Yamabe problem focused on conformally flat metrics. Need to understand asymptotic behaviors near singular boundaries for more general non-flat background metrics, especially for Lipschitz domains with conical structure.

Method: Investigate asymptotic behaviors of solutions to singular Yamabe problem with negative constant scalar curvature. Analyze local positive solutions near singular boundaries in non-conformally-flat metrics. Study approximation of local solutions by positive solutions in tangent cones at singular boundary points.

Result: Derive optimal estimates for asymptotic behaviors. Show local positive solutions are well approximated by positive solutions in tangent cones at singular boundary points for Lipschitz domains with asymptotic conical structure. Extends results from previous papers [10, 12, 26].

Conclusion: Successfully extends asymptotic analysis of singular Yamabe problem to non-conformally-flat metrics, providing optimal estimates and establishing approximation results for Lipschitz domains with conical structure, generalizing previous work.

Abstract: In this paper, we investigate the asymptotic behaviors of solutions to the singular Yamabe problem with negative constant scalar curvature near singular boundaries and derive optimal estimates, where the background metrics are not assumed to be conformally flat. Specifically, we demonstrate that for a wide class of Lipschitz domains with asymptotic conical structure, the local positive solutions are well approximated by the positive solutions in the tangent cones at singular boundary points. This extends the results of [10, 12, 26].

</details>


### [40] [Regularity for mixed-order nonlinear fractional equations with degenerate coefficients](https://arxiv.org/abs/2512.23359)
*Ho-Sik Lee,Jihoon Ok,Kyeong Song*

Main category: math.AP

TL;DR: The paper studies regularity properties of weak solutions to nonlinear integro-differential equations involving weighted superposition of two fractional p-Laplacian operators with different fractional orders.


<details>
  <summary>Details</summary>
Motivation: To establish regularity theory for solutions to nonlinear integro-differential equations that combine two fractional p-Laplacian operators with different fractional orders, which appear in various applications including nonlocal continuum mechanics and image processing.

Method: The authors consider weak solutions to equations with leading operator being a superposition of $(-Δ_{p})^{s}$ and $(-Δ_{p})^{t}$ weighted by coefficients $a(\cdot,\cdot),b(\cdot,\cdot) \ge 0$. They use analytical techniques for nonlocal operators and establish regularity results through careful estimates and functional analysis methods.

Result: Proves local boundedness and Hölder regularity of weak solutions under natural assumptions on coefficients and parameters. Additionally, when $a(\cdot,\cdot) \equiv 1$, proves a Harnack inequality for weak solutions.

Conclusion: The paper establishes fundamental regularity properties for solutions to this class of nonlinear integro-differential equations, providing important tools for the analysis of such equations and extending known results for single fractional p-Laplacian operators to the superposition case.

Abstract: We consider a class of nonlinear integro-differential equations whose leading operator is obtained as a superposition of $(-Δ_{p})^{s}$ and $(-Δ_{p})^{t}$, where $0<s<t<1<p<\infty$, weighted via two possibly degenerate coefficients $a(\cdot,\cdot),b(\cdot,\cdot) \ge 0$. We prove local boundedness and Hölder regularity of its weak solutions under natural assumptions on the coefficients $a(\cdot,\cdot)$, $b(\cdot,\cdot)$ and the powers $s,t$, and $p$. Moreover, when $a(\cdot,\cdot) \equiv 1$, we also prove a Harnack inequality for weak solutions.

</details>


### [41] [The Time-Periodic Cahn-Hilliard-Gurtin System on the Half Space as a Mixed-Order System with General Boundary Conditions](https://arxiv.org/abs/2512.23582)
*Guillaume Neuttiens,Jonas Sauer*

Main category: math.AP

TL;DR: Proves well-posedness and maximal regularity for time-periodic Cahn-Hilliard-Gurtin system in half space using novel complementing boundary conditions that extend classical Lopatinskiĭ-Shapiro conditions to time-periodic mixed-order systems.


<details>
  <summary>Details</summary>
Motivation: To establish well-posedness and maximal regularity for time-periodic Cahn-Hilliard-Gurtin systems in half spaces, which requires developing new boundary condition theory since classical Lopatinskiĭ-Shapiro conditions are insufficient for mixed-order systems.

Method: Introduces novel class of complementing boundary conditions that extend classical Lopatinskiĭ-Shapiro conditions from elliptic/parabolic theory to time-periodic mixed-order systems with general boundary conditions.

Result: Proves well-posedness and maximal regularity for time-periodic Cahn-Hilliard-Gurtin system in half space, and demonstrates insufficiency of classical Lopatinskiĭ-Shapiro conditions for mixed-order systems.

Conclusion: New complementing boundary conditions are necessary for well-posedness of time-periodic mixed-order systems, providing proper extension of classical theory to handle complex boundary value problems like Cahn-Hilliard-Gurtin systems.

Abstract: A well-posedness and maximal regularity result for the time-periodic Cahn-Hilliard-Gurtin system in the half space is proved. For this purpose, we introduce a novel class of complementing boundary conditions, extending the classical Lopatinskiĭ-Shapiro conditions from elliptic and parabolic theory to time-periodic mixed-order systems with general boundary conditions. Moreover, we show that the classical Lopatinskiĭ-Shapiro conditions are in general insufficient for well-posedness of mixed-order systems.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [42] [The Solution of Potential-Driven, Steady-State Nonlinear Network Flow Equations via Graph Partitioning](https://arxiv.org/abs/2512.22124)
*Shriram Srinivasan,Kaarthik Sundar*

Main category: physics.comp-ph

TL;DR: A distributed algorithm for solving large-scale nonlinear flow networks by partitioning them into smaller subsystems, enabling local solutions while maintaining global convergence through interconnect coordination.


<details>
  <summary>Details</summary>
Motivation: Solving potential-driven steady-state flow in large networks (like natural gas or water pipelines) becomes increasingly challenging as network size grows, especially when networks are owned by different operators who need to maintain data privacy.

Method: Partition the network into tractable subsystems, solve each subsystem locally, and coordinate solutions at interconnects/transfer points using an algorithm related to the Schur complement method, allowing operators to use their preferred solution methods.

Result: The method successfully solves challenging test cases while enabling data sharing only at interconnects, preserving operator autonomy and data privacy.

Conclusion: The proposed distributed algorithm provides an effective approach for solving large nonlinear flow networks while respecting operator boundaries and data privacy requirements.

Abstract: The solution of potential-driven steady-state flow in large networks is required in various engineering applications, such as transport of natural gas or water through pipeline networks. The resultant system of nonlinear equations depends on the network topology, and its solution grows more challenging as the network size increases. We present an algorithm that utilizes a given partition of a network into tractable sizes to compute a global solution for the full nonlinear system through local solution of smaller subsystems induced by the partitions. When the partitions are induced by interconnects or transfer points corresponding to networks owned by different operators, the method ensures data is shared solely at the interconnects, leaving network operators free to solve the network flow system corresponding to their own domain in any manner of their choosing. The proposed method is shown to be connected to the Schur complement and the method's viability demonstrated on some challenging test cases.

</details>


### [43] [A Radiation Exchange Factor Transformation with Proven Convergence, Non-Negativity, and Energy Conservation](https://arxiv.org/abs/2512.22157)
*Nikolaj Maack Bielefeld*

Main category: physics.comp-ph

TL;DR: Matrix-based exchange factor transformation solves coupled mixed boundary radiative transfer problems with guaranteed convergence, non-negative radiation, and exact energy conservation.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing radiative transfer methods for general domains with mixed boundary conditions, particularly identifying and eliminating discrepancies in classical approaches like Hottel's zonal method.

Method: Uses a matrix-based exchange factor transformation that converts first-interaction exchange factor matrix F into absorption matrix A and multiple reflection-scattering matrix R through a Neumann series that analytically traces all reflection-scattering paths to steady state.

Result: The method establishes rigorous convergence conditions, guarantees non-negative radiation, achieves exact energy conservation to machine precision, eliminates discrepancies in classical methods, and validates against diffusion approximation and existing results for pure/partial scattering cases.

Conclusion: The proposed transformation provides a robust solution for medium-scale general reflecting-scattering problems and scales to large problems when matrix sparsity conditions are met, offering improvements over classical radiative transfer methods.

Abstract: This paper presents a matrix-based exchange factor transformation for solving coupled mixed boundary condition radiative transfer problems on general domains. The method applies to participating media ranging from transparent to absorbing, emitting, and scattering, with boundaries ranging from absorbing to reflecting. Given a first-interaction exchange factor matrix $\mathbf{F}$, the transformation produces an absorption matrix $\mathbf{A}$ and a multiple reflection-scattering matrix $\mathbf{R}$ through a Neumann series that analytically traces all reflection-scattering paths to steady state. The paper establishes rigorous conditions under which the method guarantees convergence, non-negative radiation, and exact energy conservation to machine precision. A comparison with Noble's matrix formulation of Hottel's zonal method reveals a previously unidentified discrepancy in that classical approach; the proposed transformation eliminates this discrepancy. The method is validated against the diffusion approximation in the high-extinction limit and against results of Crosbie and Schrenker for pure and partial scattering cases. The method is applicable to medium-scale general reflecting-scattering problems and scales to large problems when negligible reflection-scattering and high extinction ensure matrix sparsity.

</details>


### [44] [Integrating Wide and Deep Neural Networks with Squeeze-and-Excitation Blocks for Multi-Target Property Prediction in Additively Manufactured Fiber Reinforced Composites](https://arxiv.org/abs/2512.22397)
*Behzad Parvaresh,Rahmat K. Adesunkanmi,Adel Alaeddini*

Main category: physics.comp-ph

TL;DR: Data-efficient multi-target learning approach combining LHS-guided experimentation with SE-WDNN to predict mechanical and manufacturing properties of 3D-printed continuous fiber-reinforced composites.


<details>
  <summary>Details</summary>
Motivation: Continuous fiber-reinforced composites made by additive manufacturing have high specific strength but their performance is sensitive to process-material parameter interactions, making exhaustive experimental testing impractical.

Method: Latin Hypercube Sampling-guided experimentation with 155 specimens from 4,320 design space combinations, using squeeze-and-excitation wide and deep neural network (SE-WDNN) for multi-target prediction, compared against baseline ML models.

Result: SE-WDNN achieved lowest overall test error (MAPE = 12.33%) with statistically significant improvements over baseline wide and deep neural network. SHAP analysis revealed reinforcement strategy as major influence on mechanical performance.

Conclusion: Integration of LHS and SE-WDNN enables interpretable and sample-efficient multi-target predictions, guiding parameter selection in CFRC-AM with balance between mechanical behavior and manufacturing metrics.

Abstract: Continuous fiber-reinforced composite manufactured by additive manufacturing (CFRC-AM) offers opportunities for printing lightweight materials with high specific strength. However, their performance is sensitive to the interaction of process and material parameters, making exhaustive experimental testing impractical. In this study, we introduce a data-efficient, multi-input, multi-target learning approach that integrates Latin Hypercube Sampling (LHS)-guided experimentation with a squeeze-and-excitation wide and deep neural network (SE-WDNN) to jointly predict multiple mechanical and manufacturing properties of CFRC-AMs based on different manufacturing parameters. We printed and tested 155 specimens selected from a design space of 4,320 combinations using a Markforged Mark Two 3D printer. The processed data formed the input-output set for our proposed model. We compared the results with those from commonly used machine learning models, including feedforward neural networks, Kolmogorov-Arnold networks, XGBoost, CatBoost, and random forests. Our model achieved the lowest overall test error (MAPE = 12.33%) and showed statistically significant improvements over the baseline wide and deep neural network for several target variables (paired t-tests, p <= 0.05). SHapley Additive exPlanations (SHAP) analysis revealed that reinforcement strategy was the major influence on mechanical performance. Overall, this study demonstrates that the integration of LHS and SE-WDNN enables interpretable and sample-efficient multi-target predictions, guiding parameter selection in CFRC-AM with a balance between mechanical behavior and manufacturing metrics.

</details>


### [45] [A survey of interlayer interaction models for graphene and other 2D materials](https://arxiv.org/abs/2512.22670)
*Gourav Yadav,Shakti S. Gupta,Roger A. Sauer*

Main category: physics.comp-ph

TL;DR: Survey of mechanical models for van der Waals interactions in 2D materials, covering continuum and discrete approaches, contact phenomena, and computational strategies.


<details>
  <summary>Details</summary>
Motivation: To systematically review and organize mechanical models that describe van der Waals interactions in 2D materials, which are crucial for understanding various physical phenomena in these systems.

Method: Comprehensive survey approach examining both continuum elastomer-like models and discrete crystalline models, analyzing normal and tangential contact models, considering atomistic and continuum approaches, and evaluating computational strategies.

Result: Organized framework of mechanical models showing how van der Waals interactions lead to contact instabilities, Moiré patterns, surface reconstructions, and superlubricity, with analysis of external loading effects and length scale dependencies.

Conclusion: Provides a systematic overview of mechanical modeling approaches for van der Waals interactions in 2D materials, highlighting computational strategies for multiscale modeling and emphasizing the importance of these models for understanding complex interfacial phenomena.

Abstract: This work presents a survey of mechanical models describing van der Waals interactions between 2D materials, encompassing both continuous elastomer-like materials and discrete (crystalline) 2D materials such as graphene. These interactions give rise to a range of physical phenomena, including contact instabilities, Moiré patterns, surface reconstructions, and superlubricity. The underlying contact forces follow from the variation of an interfacial interaction potential. The presentation first discusses normal contact models, and then tangential contact models. Both atomistic and continuum approaches are considered. In addition, the influence of external loading and changes in length scale on the ground state configuration and frictional contact behavior are analyzed. A particular emphasis is placed on discussing strategies that reduce computational cost in multiscale modeling.

</details>


### [46] [Overcoming Computational Bottlenecks in Quantum Hydrodynamics: A Volume-Based Integral Formalism](https://arxiv.org/abs/2512.22920)
*Christos Mystilidis,Christos Tserkezis,Guy A. E. Vandenbosch,N. Asger Mortensen,Xuezhi Zheng*

Main category: physics.comp-ph

TL;DR: VIE method enables efficient simulation of quantum hydrodynamic nanoparticles by overcoming computational bottlenecks of mesoscopic material models like SC-HDM.


<details>
  <summary>Details</summary>
Motivation: Mesoscopic models for metal optical response face computational bottlenecks despite being more efficient than ab initio methods. The SC-HDM captures nonlocal electron dynamics and spill-out but requires demanding computations.

Method: Developed a Volume Integral Equation (VIE) method as an alternative to Differential Equation (DE)-based approaches for implementing the Self-Consistent Hydrodynamic Drude Model (SC-HDM). The method leverages inherent symmetries for spherical nanoparticles and can extract mesoscopic material-response functions directly.

Result: Achieved significant computational efficiency with similar performance for three increasingly complex material models. Breaks the assumption that increased material sophistication requires taxing simulations. The VIE approach serves as a methodological scaffold adaptable to various material models.

Conclusion: The VIE method opens new ways to model quantum hydrodynamic nanoparticles efficiently and will serve as an essential benchmarking tool for more complex geometries, circumventing lengthy microscopic calculations.

Abstract: Mesoscopic models of the optical response of metals have emerged as fundamental building blocks in quantum plasmonics, in principle overcoming the computational bottlenecks of ab initio techniques by implementing aspects of the atomistic description of the metal in otherwise classical calculations. Nonetheless, even these approaches are eventually hindered by demanding computations due to sophisticated material response. Here, this issue is addressed for the advanced Self-Consistent Hydrodynamic Drude Model (SC-HDM), which captures both nonlocal electron dynamics and electron spill-out, through a Volume Integral Equation (VIE) method. Adopting an IE-based method shifts perspective from the commonly employed Differential Equation (DE)-based ones, demonstrating significant computational efficiency. The VIE approach is a valuable methodological scaffold: It addresses SC-HDM and simpler models, but can also be adapted to more advanced ones. For spherical nanoparticles (NPs), using the inherent symmetries, similar performance for three increasingly complicated material models is achieved, breaking the taboo that increased sophistication in material response requires taxing simulations. Mesoscopic material-response functions can be readily extracted from the VIE implementation, thus circumventing the need for lengthy microscopic calculations. This method opens a new way of modeling quantum hydrodynamic NPs and will serve as essential benchmarking tool for recipes addressing more complicated geometries.

</details>


### [47] [Masgent: An AI-assisted Materials Simulation Agent](https://arxiv.org/abs/2512.23010)
*Guanghen Liu,Songge Yang,Yu Zhong*

Main category: physics.comp-ph

TL;DR: Masgent is an AI-assisted materials simulation agent that uses LLMs to enable natural-language interaction for DFT and MLP simulations, reducing setup time from hours to seconds.


<details>
  <summary>Details</summary>
Motivation: Current DFT and MLP simulations require extensive scripting, multi-step procedures, and HPC expertise, which hinders reproducibility and slows down materials discovery.

Method: Developed Masgent, an AI-assisted platform powered by large language models that unifies structure manipulation, automated VASP input generation, DFT workflow construction, MLP-based simulations, and ML utilities in a single platform.

Result: Enables researchers to perform complex simulation tasks through natural-language interaction, eliminating most manual scripting and reducing setup time from hours to seconds.

Conclusion: Masgent democratizes access to state-of-the-art computational methodologies, accelerating hypothesis testing, pre-screening, and exploratory research for both new and experienced practitioners.

Abstract: Density functional theory (DFT) and machine learning potentials (MLPs) are essential for predicting and understanding materials properties, yet preparing, executing, and analyzing these simulations typically requires extensive scripting, multi-step procedures, and significant high-performance computing (HPC) expertise. These challenges hinder reproducibility and slow down discovery. Here, we introduce Masgent, an AI-assisted materials simulation agent that unifies structure manipulation, automated VASP input generation, DFT workflow construction and analysis, fast MLP-based simulations, and lightweight machine learning (ML) utilities within a single platform. Powered by large language models (LLMs), Masgent enables researchers to perform complex simulation tasks through natural-language interaction, eliminating most manual scripting and reducing setup time from hours to seconds. By standardizing protocols and integrating advanced simulation and data-driven tools, Masgent democratizes access to state-of-the-art computational methodologies, accelerating hypothesis testing, pre-screening, and exploratory research for both new and experienced practitioners.

</details>


### [48] [Reconstructing Relativistic Magnetohydrodynamics with Physics-Informed Neural Networks](https://arxiv.org/abs/2512.23057)
*Corwin Cheung,Marcos Johnson-Noya,Michael Xiang,Dominic Chang,Alfredo Guevara*

Main category: physics.comp-ph

TL;DR: First PINN surrogates for relativistic magnetohydrodynamics using hybrid PDE/data-driven approach with primitive variables and divergence-free constraints.


<details>
  <summary>Details</summary>
Motivation: Need for efficient surrogates for relativistic magnetohydrodynamics (RMHD) simulations, which are computationally expensive. Physics-informed neural networks offer potential for faster predictions while maintaining physical consistency.

Method: Hybrid PDE and data-driven workflow using primitive variables instead of conservative form. Incorporates divergence-free condition directly without cleaning modes. Uses novel MUON optimizer and posterior residual-guided networks to reduce PDE violations.

Result: Baseline PINN trained on early-time snapshots can successfully extrapolate RMHD dynamics in 1D and 2D. Posterior residual-guided networks systematically reduce PDE violations, demonstrating improved accuracy.

Conclusion: Successfully constructed first PINN surrogates for RMHD that can extrapolate dynamics and reduce PDE violations, offering promising approach for efficient RMHD simulations.

Abstract: We construct the first physics-informed neural-network (PINN) surrogates for relativistic magnetohydrodynamics (RMHD) using a hybrid PDE and data-driven workflow. Instead of training for the conservative form of the equations, we work with Jacobians or PDE characteristics directly in terms of primitive variables. We further add to the trainable system the divergence-free condition, without the need of cleaning modes. Using a novel MUON optimizer implementation, we show that a baseline PINN trained on early-time snapshots can extrapolate RMHD dynamics in one and two spatial dimensions, and that posterior residual-guided networks can systematically reduce PDE violations.

</details>


### [49] [Exponential divided differences via Chebyshev polynomials](https://arxiv.org/abs/2512.23061)
*Itay Hen*

Main category: physics.comp-ph

TL;DR: A Chebyshev-polynomial-based algorithm for efficient and stable evaluation of high-order exponential divided differences with O(qN) cost and incremental updates for dynamic node sets.


<details>
  <summary>Details</summary>
Motivation: Exponential divided differences are important in numerical linear algebra, matrix-function evaluation, and quantum Monte Carlo simulations, but efficient and numerically stable evaluation for high-order cases with dynamically evolving node sets remains a significant computational challenge.

Method: Combines Chebyshev-Bessel expansion of the exponential function with a direct recurrence for Chebyshev divided differences. Also develops an incremental update scheme for dynamic node sets that enables insertion/removal of single nodes in O(N) time when the affine mapping interval is fixed.

Result: Achieves computational cost of O(qN), where q is divided-difference order and N is Chebyshev truncation length. N scales linearly with spectral width through decay of modified Bessel coefficients, while dependence on q enters only through structural polynomial constraints.

Conclusion: The algorithm provides an efficient and numerically stable solution for evaluating high-order exponential divided differences, with practical applications in computational mathematics and physics. A full C++ reference implementation is publicly available.

Abstract: Exponential divided differences arise in numerical linear algebra, matrix-function evaluation, and quantum Monte Carlo simulations, where they serve as kernel weights for time evolution and observable estimation. Efficient and numerically stable evaluation of high-order exponential divided differences for dynamically evolving node sets remains a significant computational challenge. We present a Chebyshev-polynomial-based algorithm that addresses this problem by combining the Chebyshev-Bessel expansion of the exponential function with a direct recurrence for Chebyshev divided differences. The method achieves a computational cost of ${\cal O}(qN)$, where $q$ is the divided-difference order and $N$ is the Chebyshev truncation length. We show that $N$ scales linearly with the spectral width through the decay of modified Bessel coefficients, while the dependence on $q$ enters only through structural polynomial constraints. We further develop an incremental update scheme for dynamic node sets that enables the insertion or removal of a single node in ${\cal O}(N)$ time when the affine mapping interval is held fixed. A full \texttt{C++} reference implementation of the algorithms described in this work is publicly available.

</details>


### [50] [A space-time extension of a conservative two-fluid cut-cell diffusion method for moving geometries](https://arxiv.org/abs/2512.23358)
*Louis Libat,Can Selçuk,Eric Chénier,Vincent Le Chenadec*

Main category: physics.comp-ph

TL;DR: Space-time extension of conservative Cartesian cut-cell method for two-phase diffusion in moving geometries, maintaining strict conservation and handling topology changes.


<details>
  <summary>Details</summary>
Motivation: Need for accurate simulation of two-phase diffusion in moving geometries while maintaining strict conservation properties, especially for applications like phase change problems.

Method: Two-fluid approach with separate scalar fields in each phase, coupled by sharp interface conditions. Uses space-time control volumes on fixed Cartesian grid with geometric moments (swept volumes, apertures) as weights in finite-volume operators to handle moving boundaries and fresh/dead-cell events.

Result: Demonstrates super-linear accuracy in space, robust behavior under repeated topology changes, and strict conservation across strong coefficient jumps and moving interfaces in 2D and 3D tests.

Conclusion: The space-time cut-cell framework provides a conservative building block for multiphase transport in evolving geometries and serves as foundation for future free-boundary extensions like Stefan-type phase change.

Abstract: We present a space-time extension of a conservative Cartesian cut-cell finite-volume method for two-phase diffusion in prescribed-motion geometries. The formulation follows a two-fluid approach: one scalar field is solved in each phase with discontinuous material properties, coupled by sharp interface conditions enforcing flux continuity and jump laws. To handle moving boundaries on a fixed Cartesian grid, the discrete balance is written over phase-restricted space-time control volumes, whose geometric moments (swept volumes and apertures) are used as weights in the finite-volume operators. This construction naturally accounts for the creation and destruction of cut cells (fresh/dead-cell events) and yields strict discrete conservation. The resulting scheme retains the algebraic structure of the static cut-cell formulation while incorporating motion through local geometric weights and interface coupling operators. A series of verification and validation tests in two and three dimensions demonstrate super-linear accuracy in space, robust behavior under repeated topology changes and conservation across strong coefficient jumps and moving interfaces. The proposed space-time cut-cell framework provides a conservative building block for multiphase transport in evolving geometries and a foundation for future free-boundary extensions such as Stefan-type phase change.

</details>


### [51] [PINNs for Electromagnetic Wave Propagation](https://arxiv.org/abs/2512.23396)
*Nilufer K. Bulut*

Main category: physics.comp-ph

TL;DR: Hybrid training strategies enable PINNs to achieve FDTD-level accuracy and energy conservation in electromagnetic wave propagation problems.


<details>
  <summary>Details</summary>
Motivation: PINNs offer mesh-free solutions and applicability to inverse problems in electromagnetism, but suffer from accuracy and energy conservation deficiencies compared to established methods like FDTD. The study aims to bridge this performance gap.

Method: Hybrid methodology combining: 1) time marching with causality-aware weighting to address causality collapse, 2) two-stage interface continuity loss to mitigate time marching discontinuities, and 3) local Poynting-based regularizer to suppress cumulative energy drift.

Result: Achieved 0.09% NRMSE and 1.01% L² error over time, with only 0.024% relative energy mismatch in 2D PEC cavity scenario. Training used only physics-based losses without labeled data, with FDTD used only for evaluation.

Conclusion: PINNs with hybrid training strategies can achieve competitive results with FDTD in canonical electromagnetic examples, making them a viable alternative for electromagnetic wave propagation problems.

Abstract: Physics-Informed Neural Networks (PINNs) are a methodology that aims to solve physical systems by directly embedding PDE constraints into the neural network training process. In electromagnetism, where well-established methodologies such as FDTD and FEM already exist, new methodologies are expected to provide clear advantages to be accepted. Despite their mesh-free nature and applicability to inverse problems, PINNs can exhibit deficiencies in terms of accuracy and energy metrics when compared to FDTD solutions. This study demonstrates hybrid training strategies can bring PINNs closer to FDTD-level accuracy and energy consistency.
  This study presents a hybrid methodology addressing common challenges in wave propagation scenarios. The causality collapse problem in time-dependent PINN training is addressed via time marching and causality-aware weighting. In order to mitigate the discontinuities that are introduced by time marching, a two-stage interface continuity loss is applied. In order to suppress loss accumulation, which is manifested as cumulative energy drift in electromagnetic waves, a local Poynting-based regularizer has been developed.
  In the developed PINN model, high field accuracy is achieved with an average 0.09\% $NRMSE$ and 1.01\% $L^2$ error over time. Energy conservation is achieved on the PINN side with only a 0.024\% relative energy mismatch in the 2D PEC cavity scenario. Training is performed without labeled field data, using only physics-based residual losses; FDTD is used solely for post-training evaluation. The results demonstrate that PINNs can achieve competitive results with FDTD in canonical electromagnetic examples and are a viable alternative.

</details>


### [52] [MultiAtomLiouvilleEquationGenerator: A Mathematica package for Liouville superoperators and master equations of multilevel atomic systems](https://arxiv.org/abs/2512.23591)
*Pablo Yanes-Thomas,Rocío Jáuregui-Renaud Santiago F. Caballero-Benítez,Daniel Sahagún Sánchez,Alejandro Kunold*

Main category: physics.comp-ph

TL;DR: MulAtoLEG is an open-source Mathematica package for generating Liouville superoperators and equations for multi-atom systems, supporting complex atomic transitions and general quantum master equations.


<details>
  <summary>Details</summary>
Motivation: There's a need for computational tools to handle complex multilevel atomic systems with arbitrary numbers of atoms, particularly for alkali atoms with intricate transition configurations. Existing theoretical frameworks exist but lack accessible computational implementations.

Method: The package extends Lehmberg's adjoint master equation approach for two-level emitters to multilevel systems, reformulated as a master equation by Genes. It generates exact Liouville superoperators and equations using Mathematica's vectorization and sparse linear algebra for efficiency.

Result: MulAtoLEG successfully generates exact Liouville equations for arbitrary multi-atom systems, supports dressed-state basis calculations, and can handle general Hamiltonians and Lindbladians beyond just atomic systems.

Conclusion: MulAtoLEG provides a powerful computational tool for quantum optics and atomic physics research, enabling exact equation generation for complex multilevel atomic systems while being limited only by available computational resources.

Abstract: MulAtoLEG (Multi-Atom Liouville Equation Generator) is an open-source Mathematica package for generating Liouville superoperators and Liouville equations, specialized for multilevel atomic systems comprising an arbitrary number of atoms. This scheme is based on an extension to multilevel atomic systems, originally developed by Lehmberg [R. H. Lehmberg, Phys. Rev. A 2, 883 (1970)] as an adjoint master equation for ensembles of two-level emitters and later reformulated by Genes [M. Reitz, C. Sommer and C. Genes, PRX Quantum 3, 010201 (2022)] as a master equation. The package facilitates the generation of equations for complex transition configurations in alkali atoms. Although primarily designed for atomic systems, it can also generate the master and adjoint master equations for general Hamiltonians and Lindbladians. In addition, it includes functionalities to construct the differential equations in the dressed-state basis, where, in many cases, the non-unitary evolution operator can be determined explicitly. To maximize computational efficiency, the package leverages Mathematica's vectorization and sparse linear algebra capabilities. Since MulAtoLEG produces exact equations without approximations, the feasible system size is naturally limited by the available computational resources.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [53] [On the accessibility of stable reactor operating regimes in quasi-symmetric stellarators](https://arxiv.org/abs/2512.22355)
*Adelle M. Wright,Benjamin J. Faber*

Main category: physics.plasm-ph

TL;DR: Low-beta regime in quasi-axisymmetric stellarators shows abrupt transition to deleterious transport, challenging reactor design strategies.


<details>
  <summary>Details</summary>
Motivation: To investigate accessibility of reactor-relevant low-beta regime in stellarators for achieving sustained burning plasma conditions while avoiding destructive instabilities.

Method: Used state-of-the-art high-fidelity macro- and microscopic simulation tools (including GENE code) to analyze a reactor-scale quasi-axisymmetric stellarator configuration with flattened core pressure profile.

Result: Linear and nonlinear calculations show abrupt transition to highly deleterious transport at low local plasma beta, despite favorable macroscopic and neoclassical properties.

Conclusion: The low-beta regime accessibility challenge has significant implications for stellarator optimization and impacts quasi-symmetric stellarator design strategies.

Abstract: Maximising particle and energy confinement is crucial for achieving the sustained burning plasma conditions necessary to realise fusion energy. For stellarator reactors, one proposed strategy for avoiding destructive instabilities is to operate at high-field but low(er) plasma pressure. In this work, we investigate the accessibility of such a reactor-relevant low-beta regime in a reactor-scale quasi-axisymmetric stellarator using state-of-the-art high-fidelity macro- and microscopic simulation tools. We consider a configuration with a flattened core pressure profile and favourable properties from the macroscopic and neoclassical perspectives. By contrast, linear and nonlinear calculations with the GENE code show an abrupt transition to a regime of highly deleterious transport at low (local) plasma beta. We describe the characterisation of these transport regimes as well as the confinement transition. We discuss the implications broadly for stellarator optimisation and highlight the impact on quasi-symmetric stellarator design strategies.

</details>


### [54] [Impact of fast ions on turbulent transport in high-\b{eta} HL-2A scenarios](https://arxiv.org/abs/2512.22611)
*Jingchun Li,Zhaoyang Lu,Jianqiang Xu,Wei Chen,Jiaqi Dong,Jingting Luo,Yong Liu*

Main category: physics.plasm-ph

TL;DR: Fast-ion pressure gradients can both stabilize ITG turbulence via thermal-ion dilution and destabilize FI-BAEs at high gradients, with dual effects on transport through zonal flow regulation.


<details>
  <summary>Details</summary>
Motivation: To understand the complex role of fast ions in regulating turbulent transport in high-beta fusion plasmas, which is crucial for achieving high-performance magnetic confinement fusion.

Method: Gyrokinetic simulations with linear analyses examining fast-ion effects on ITG modes, TEMs, and FI-BAEs, plus nonlinear simulations to study transport dynamics and zonal flow interactions.

Result: Fast ions strongly stabilize ITG modes via thermal-ion dilution but minimally affect TEMs; high FI gradients trigger FI-BAE transition; moderate FI suppresses transport via zonal flow shear, while strong FI drive enhances transport by destabilizing FI-BAEs.

Conclusion: Fast ions play a dual role in turbulence regulation - stabilizing at moderate levels but destabilizing at high gradients - providing insights into multiscale transport physics for high-performance fusion plasmas.

Abstract: The fast-ion (FI) on turbulent transport is one of the key topics of magnetic confinement fusion. This work focus on the impact of FI pressure gradients on turbulence in a high-\b{eta} plasma scenario using gyrokinetic simulations. Linear analyses reveal that FIs strongly stabilize ion temperature gradient (ITG) modes via the thermal-ion dilution, while their influence on trapped electron modes (TEMs) is minimal. At elevated FI pressure gradients, a transition to a FI-driven BAE (FI-BAE) regime occurs, as evidenced by mode structure and frequency alignment within the Alfvénic gap. Electron \b{eta} scans further demonstrate the emergence of kinetic ballooning modes (KBMs) at higher \b{eta}, whereas an ITG-TEM hybrid turbulence dominates near experimental \b{eta} values. Nonlinear simulations show that moderate FI pressure suppresses transport via zonal flow (ZF) shear, whereas strong FI drive weakens ZFs and enhances transport by destabilizing FI-BAEs. These results highlight the dual role of FIs in regulating turbulence and offer insight into multiscale transport physics relevant for high-performance plasmas.

</details>


### [55] [Treatment of sunflower seeds by cold atmospheric plasma enhances their tolerance to water stress during germination and early seedling development](https://arxiv.org/abs/2512.23034)
*L. Taras,C. Bailly,T. Dufour*

Main category: physics.plasm-ph

TL;DR: Cold atmospheric plasma treatment improves sunflower seed germination and seedling development under water stress conditions.


<details>
  <summary>Details</summary>
Motivation: To investigate how cold atmospheric plasma (CAP) treatment affects seed germination and early seedling development in sunflower under water stress conditions, potentially offering a solution for improving crop emergence in suboptimal environments.

Method: Used dielectric barrier device to generate CAP, detected excited molecular nitrogen and ozone via optical emission spectroscopy and mass spectrometry. Treated dry sunflower seeds (both dormant and non-dormant genotypes) with plasma under controlled gap accuracy. Conducted germination tests under water stress and greenhouse experiments for seedling development assessment.

Result: CAP significantly improved germination rates for both dormant and non-dormant sunflower genotypes under water stress. Gap accuracy was crucial for effective seed-plasma interaction. Plasma treatment also stimulated seedling development under water stress conditions in greenhouse experiments.

Conclusion: Cold atmospheric plasma treatment shows potential as an effective approach to promote the entire emergence process of crop species, particularly in alleviating seed dormancy and improving performance under water stress conditions.

Abstract: The aim of this study was to investigate the impact of ambient air plasma treatment on both sunflower seed germination and early steps of seedling development under water stress. Dry seeds were exposed to a cold atmospheric plasma (CAP) generated in a dielectric barrier device where excited molecular nitrogen and ozone were detected by optical emission spectroscopy and mass spectrometry respectively. Interestingly, we explain the crucial role of gap's accuracy when treating seeds with CAP, especially to improve interaction between seeds and plasma and therefore ensure an efficient treatment. CAP significantly improved the germination rates of seeds of dormant and non-dormant sunflower genotypes under water stress, demonstrating its efficiency in alleviating seed dormancy and in improving germination under suboptimal conditions. Furthermore, greenhouse experiments demonstrated that plasma treatment also stimulated seedling development under water stress conditions. These findings highlight the potential of CAP treatment as an effective approach to promote the whole process of emergence of crop species.

</details>


### [56] [Thermodynamically Consistent Vibrational-Electron Heating: Generalized Model for Multi-Quantum Transitions](https://arxiv.org/abs/2512.23072)
*Bernard Parent,Felipe Martin Rodriguez Fuented*

Main category: physics.plasm-ph

TL;DR: Generalized thermodynamically consistent model for electron-vibrational heating that includes multi-quantum overtone transitions, correcting systematic errors in previous single-quantum models for high-temperature plasma applications.


<details>
  <summary>Details</summary>
Motivation: Previous thermodynamically consistent models for electron-vibrational heating were limited to single-quantum transitions, restricting validity to low-temperature regimes (Te ≲ 1.5 eV). This limitation prevents accurate prediction of electron temperature in high-energy plasma applications like hypersonic flight and plasma-assisted combustion.

Method: Generalized the model to include multi-quantum overtone transitions. Derived formulation where total heating rate is summation of channel-specific cooling rates Q_e-v^(m) for each quantum jump m, scaled by thermodynamic factor exp(mθ_v/T_e - mθ_v/T_v). This preserves thermodynamic consistency by ensuring zero net energy transfer at equilibrium.

Result: Previous models neglecting hot-band transitions incur systematic heating error of exp(-θ_v/T_v), exceeding 40% when T_v > θ_v, effectively preventing thermal relaxation. The generalized model corrects this error and extends applicability to high-energy regimes while maintaining thermodynamic consistency.

Conclusion: The generalized multi-quantum model enables accurate electron temperature prediction in high-temperature plasma applications by correcting systematic errors in previous single-quantum approaches, ensuring proper thermal relaxation and thermodynamic consistency across wider temperature ranges.

Abstract: Accurate prediction of electron temperature ($T_{\rm e}$) is critical for non-equilibrium plasma applications ranging from hypersonic flight to plasma-assisted combustion. We recently proposed a thermodynamically consistent model for vibrational-electron heating [Phys. Fluids 37, 096141 (2025)] that enforces the convergence of $T_{\rm e}$ to the vibrational temperature ($T_{\rm v}$) at equilibrium. However, the original derivation was restricted to single-quantum transitions, limiting its validity to low-temperature regimes ($T_{\rm e} \lesssim 1.5$ eV). In this Letter, we generalize the model to include multi-quantum overtone transitions, extending its applicability to high-energy regimes. We demonstrate that previous models neglecting hot-band transitions incur a systematic heating error of $\exp(-θ_{\rm v}/T_{\rm v})$, where $θ_{\rm v}$ is the characteristic vibrational temperature. This error exceeds 40\% when $T_{\rm v}$ is greater than $θ_{\rm v}$, effectively preventing thermal relaxation. To correct this, we derive a formulation where the total heating rate is a summation of channel-specific cooling rates $Q_{\rm e-v}^{(m)}$, each associated with a quantum jump $m$, scaled by a thermodynamic factor $\exp(mθ_{\rm v}/T_{\rm e}-mθ_{\rm v}/T_{\rm v})$. This generalized model preserves thermodynamic consistency by ensuring zero net energy transfer at equilibrium.

</details>


### [57] [Breaking seed dormancy in Mediterranean Brassica rapa wild populations: is cold plasma treatment efficient?](https://arxiv.org/abs/2512.23114)
*M. H. Wagner,T. Dufour,A. Geraci,E. Oddo,G. R. Tarantino,F. Scafidi,C. Bailly,H. Hadj Arab,B. Boucenna,M. Tiret,C. Falentin,A. Dupont,S. Ducournau,A. M. Chèvre*

Main category: physics.plasm-ph

TL;DR: Cold plasma treatment effectively breaks dormancy in wild turnip seeds, improving germination rates and speed, but doesn't significantly affect seedling emergence.


<details>
  <summary>Details</summary>
Motivation: To evaluate dormancy and germination traits in wild Brassica rapa populations across the Mediterranean region and compare the effectiveness of different dormancy-breaking methods.

Method: Studied 61 wild Brassica rapa populations; compared three dormancy-breaking methods (gibberellic acid, scarification, cold plasma); assessed germination ability, T10, mean germination time, greenhouse emergence; used histological and SEM analysis of seed coats.

Result: Cold plasma was most effective: increased germination from 18% to 60%, reduced germination time by 24 hours, reduced mean germination time. However, seedling emergence remained around 55% for both treated and untreated seeds. Seed coat structure varied by geographical origin, with Sicilian populations showing deeper dormancy.

Conclusion: Cold plasma is an effective method for breaking embryo dormancy in wild Brassica rapa seeds, though it doesn't improve seedling establishment. Geographical origin influences seed dormancy through seed coat characteristics.

Abstract: Turnip (Brassica rapa) is a native species of the Mediterranean area, spread from northwest France to south Algeria. In this study, dormancy and germination traits were assessed for 61 wild Brassica rapa populations collected across the Mediterranean region. Seed dormancy is a key factor influencing germination and seedling establishment. Three dormancy-breaking methods were compared: gibberellic acid, scarification and cold plasma. The efficiency and selectivity were evaluated through germination ability, time to 10% germination (T10), mean germination time and greenhouse emergence. Five days after imbibition, germination was only 18% for the untreated seeds but 60% for the plasma-treated seeds. Germination also began 24 hours earlier and mean germination time was reduced across most populations. However, there was a limited effect on seedling emergence, which remained around 55% for both untreated and treated samples. Comparative analysis indicates that cold plasma was more effective in alleviating embryo dormancy. In addition, histological and scanning electron microscopy showed that the seed coat differed according to the geographical origin of the populations, with a deeper dormancy in seeds from Sicilian populations.

</details>


### [58] [Axisymmetric magnetic field effects on hollow cathode generated plasma column in APPEL-device](https://arxiv.org/abs/2512.23230)
*Y. Patil,S. K. Karkari*

Main category: physics.plasm-ph

TL;DR: Plasma column generated using hollow cathode discharge with magnetic field; energetic electron confinement guides plasma along axis; column length inversely related to electron-neutral collisions.


<details>
  <summary>Details</summary>
Motivation: To understand and demonstrate the generation and sustainment of elongated plasma columns using hollow cathode discharge in magnetic fields, and to investigate the role of energetic electron confinement in guiding plasma propagation.

Method: Experimental setup using linear plasma device with hollow cathode discharge and axisymmetric magnetic field. Diagnostics to measure electron distribution and plasma characteristics. Fluid simulations using COMSOL Multiphysics to reproduce experimental trends.

Result: Successfully generated elongated plasma column with energetic electrons concentrated peripherally near source, converging toward axis ~3.0m downstream. Plasma column length shows inverse relationship with electron-neutral collision frequency, indicating collisional damping importance. Simulations qualitatively match experimental trends and support theoretical model.

Conclusion: The study demonstrates successful plasma column generation via hollow cathode discharge with magnetic guidance, where energetic electron confinement plays crucial role. Experimental and simulation results validate theoretical model of energetic electron behavior in magnetically guided plasma columns.

Abstract: An elongated plasma column has been successfully generated and sustained in a linear plasma device using a hollow cathode discharge in the presence of an axisymmetric magnetic field. The confinement of cold energetic electrons produced near the hollow cathode plays a crucial role in guiding the plasma along the device axis. Experimental diagnostics reveal a high concentration of energetic electrons in the peripheral region near the source, which progressively converge toward the axis at a downstream location approximately 3.0 meters from the cathode. The length of the plasma column exhibits an inverse relationship with the electron-neutral collision frequency, indicating the significance of collisional damping in the propagation of energetic electrons. These observations are further supported by fluid simulations performed using COMSOL Multiphysics, which qualitatively reproduce the experimental trends. The results are consistent with a theoretical model previously proposed by the authors, reinforcing the understanding of energetic electron behaviour in magnetically guided plasma columns.

</details>


### [59] [Ab initio recombination in the expanding ultracold plasmas](https://arxiv.org/abs/2512.23433)
*Yurii V. Dumin,Ludmila M. Svirskaya*

Main category: physics.plasm-ph

TL;DR: First successful ab initio simulation of recombination in ultracold plasmas without auxiliary assumptions, achieving 20% recombination efficiency matching experimental results.


<details>
  <summary>Details</summary>
Motivation: Standard simulations of recombination in ultracold plasmas face scale problems, producing only "virtual" electron-ion pairs and requiring artificial criteria to identify recombination events.

Method: Used special algorithm with: (1) scalable reference frame co-moving with expanding plasma, (2) dynamic choice of mirror cells for Coulomb sums, (3) accurate treatment of singular interparticle interactions without truncation or softening of Coulomb forces.

Result: Successfully identified real recombination events via sharp equidistant energy peaks from captured electrons passing orbital pericenters, achieving 20% recombination efficiency matching experimental measurements.

Conclusion: First ab initio simulation successfully traces real recombination in ultracold plasmas without auxiliary assumptions, validating both experimental results and earlier semi-empirical simulations.

Abstract: The efficiency of recombination is of crucial importance for the existence of ultracold plasmas, particularly, the ones formed in the magneto-optical traps. Unfortunately, a straightforward simulation of the recombination encounters the problem of huge difference in the spatial and temporal scales for free and bound motion of the electrons. As a result, only the "virtual" electron-ion pairs are usually reproduced in such simulations, and it is necessary to employ some additional criteria to identify them with the recombined atoms (this might be a minimal number of revolutions of the electron about the nearest ion or a maximal distance between them). It is the aim of this paper to present the first successful ab initio simulation of the recombination without any auxiliary assumptions. We employed a special algorithm, which was based on: (i) using the "scalable" reference frame, co-moving with the expanding plasma, (ii) dynamical choice of the number of "mirror" cells, taking into account in calculation of the Coulomb sums, and (iii) accurate treatment of the singular interparticle interactions, without any truncation or "softening" of the Coulomb forces. Then, the recombination events are identified by a series of sharp equidistant peaks in the kinetic and/or potential energies for a sample of particles, which are caused by the captured electrons passing near the pericenters of their orbits; and this is confirmed by a detailed inspection of the particle trajectories. Thereby, we were able to trace formation of the real - rather than "virtual" - electron-ion pairs. The total efficiency of recombination for the realistic experimental conditions was found to be about 20%, which is in perfect agreement both with the laboratory measurements and with the earlier semi-empirical simulations.

</details>


### [60] [Predicting core transport in ITER baseline discharges with neon injections](https://arxiv.org/abs/2512.23682)
*Dmitri M Orlov,Joseph McClenaghan,Jeff Candy,Jeremy D Lore,Nathan T Howard,Francesco Sciortino,Christopher Holland*

Main category: physics.plasm-ph

TL;DR: Integrated modeling study finds narrow compatibility window (Z_eff ≈ 1.6-1.75, auxiliary heating 75-100% of nominal) where core transport predictions align with neon-seeded divertor protection targets for ITER 15 MA baseline.


<details>
  <summary>Details</summary>
Motivation: Achieving self-consistent performance predictions for ITER requires integrated modeling of core transport and divertor power exhaust under realistic impurity conditions, particularly for impurity control and auxiliary-heating scheduling in early ITER operation.

Method: Used OMFIT STEP workflow with TGYRO for stationary temperature/density profile predictions (Z_eff 1.5-2.5), evaluated power crossing separatrix (P_sep), conducted rotation-sensitivity studies, and performed AURORA modeling for charge-exchange radiation analysis.

Result: P_sep varies by factor >1.7 across Z_eff scan, matches ~100 MW SOLPS-ITER prediction when Z_eff ≈ 1.6 or auxiliary heating reduced to ~75% of nominal. Toroidal flow variations modify P_sep by ≤20%. Identified restricted compatibility window: Z_eff ≈ 1.6-1.75 and auxiliary heating factor 0.75-1.0.

Conclusion: The self-consistent, model-constrained framework provides actionable guidance for impurity control and auxiliary-heating scheduling in early ITER operation and supports future whole-device scenario optimization.

Abstract: Achieving self-consistent performance predictions for ITER requires integrated modeling of core transport and divertor power exhaust under realistic impurity conditions. We present results from the first systematic power-flow and impurity-content study for the ITER 15 MA baseline scenario constrained directly by existing SOLPS-ITER neon-seeded divertor solutions. Using the OMFIT STEP workflow, stationary temperature and density profiles are predicted with TGYRO for $1.5 \le Z_{\rm eff} \le 2.5$, and the corresponding power crossing the separatrix $P_{\rm sep}$ is evaluated. We find that $P_{\rm sep}$ varies by more than a factor of 1.7 across this scan and matches the $\sim 100$~MW SOLPS-ITER prediction when $Z_{\rm eff} \simeq 1.6$ or when auxiliary heating is reduced to $\sim 75\%$ of nominal. Rotation-sensitivity studies show that plausible variations in toroidal flow magnitude modify $P_{\rm sep}$ by $\lesssim 20\%$, while AURORA modeling confirms that charge-exchange radiation inside the separatrix is dynamically negligible under predicted ITER neutral densities. These results identify a restricted compatibility window, $Z_{\rm eff} \approx 1.6$--1.75 and $0.75 \lesssim f_{P_{\rm aux}} \le 1.0$, in which core transport predictions remain aligned with neon-seeded divertor protection targets. This self-consistent, model-constrained framework provides actionable guidance for impurity control and auxiliary-heating scheduling in early ITER operation and supports future whole-device scenario optimization.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [61] [A first-order method for nonconvex-strongly-concave constrained minimax optimization](https://arxiv.org/abs/2512.22909)
*Zhaosong Lu,Sanyou Mei*

Main category: math.OC

TL;DR: Proposed first-order augmented Lagrangian method for nonconvex-strongly-concave constrained minimax problems achieves O(ε^{-3.5}logε^{-1}) operation complexity for ε-KKT solutions, improving previous best by factor ε^{-0.5}.


<details>
  <summary>Details</summary>
Motivation: Need efficient algorithms for constrained minimax problems with nonconvex-strongly-concave structure, which appear in various machine learning and optimization applications. Existing methods have suboptimal complexity.

Method: First-order augmented Lagrangian method that transforms constrained problem into sequence of unconstrained nonconvex-strongly-concave minimax subproblems. These subproblems are solved using a specialized first-order method that exploits strong concavity structure.

Result: Achieves operation complexity of O(ε^{-3.5}logε^{-1}) for finding ε-KKT solution, improving previous best-known complexity by factor ε^{-0.5}. Complexity measured in terms of fundamental operations.

Conclusion: Proposed method provides improved computational efficiency for constrained nonconvex-strongly-concave minimax problems, with theoretical guarantees on operation complexity that advances the state-of-the-art.

Abstract: In this paper we study a nonconvex-strongly-concave constrained minimax problem. Specifically, we propose a first-order augmented Lagrangian method for solving it, whose subproblems are nonconvex-strongly-concave unconstrained minimax problems and suitably solved by a first-order method developed in this paper that leverages the strong concavity structure. Under suitable assumptions, the proposed method achieves an \emph{operation complexity} of $O(\varepsilon^{-3.5}\log\varepsilon^{-1})$, measured in terms of its fundamental operations, for finding an $\varepsilon$-KKT solution of the constrained minimax problem, which improves the previous best-known operation complexity by a factor of $\varepsilon^{-0.5}$.

</details>


### [62] [Small-time global controllability of a class of bilinear fourth-order parabolic equations](https://arxiv.org/abs/2512.23339)
*Subrata Majumdar,Debanjit Mondal*

Main category: math.OC

TL;DR: Fourth-order nonlinear parabolic equations with bilinear controls achieve small-time global controllability using geometric control methods and linearization techniques.


<details>
  <summary>Details</summary>
Motivation: To investigate controllability properties of fourth-order nonlinear parabolic equations with bilinear controls, addressing challenges in controlling higher-order PDEs with limited control inputs.

Method: 1. Use geometric control approach adapted to fourth-order setting with frequency-localized controls for approximate controllability. 2. Analyze null controllability of linearized fourth-order system and apply fixed-point argument with approximate control property for exact controllability.

Result: 1. Small-time global approximate controllability achieved with three scalar controls between states sharing same sign. 2. Small-time global exact controllability to non-zero constant states established.

Conclusion: Fourth-order nonlinear parabolic equations with bilinear controls exhibit both approximate and exact controllability properties in small time, demonstrating effectiveness of geometric control methods and linearization techniques for higher-order PDE control problems.

Abstract: In this work, we investigate the small-time global controllability properties of a class of fourth-order nonlinear parabolic equations driven by a bilinear control posed on the one-dimensional torus. The controls depend only on time and act through a prescribed family of spatial profiles. Our first result establishes the small-time global approximate controllability of the system using three scalar controls, between states that share the same sign. This property is obtained by adapting the geometric control approach to the fourth-order setting, using a finite family of frequency-localized controls. We then study the small-time global exact controllability to non-zero constant states for the concerned system. This second result is achieved by analyzing the null controllability of an appropriate linearized fourth-order system and by deducing the controllability of the nonlinear bilinear model through a fixed-point argument together with the small-time global approximate control property.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [63] [Variational quantum eigensolver for chemical molecules](https://arxiv.org/abs/2512.22572)
*Luca Ion,Adam Smith*

Main category: quant-ph

TL;DR: Quantum computing approach using VQE to compute ground states of He-H+ and H2O molecules, benchmarked against classical exact results.


<details>
  <summary>Details</summary>
Motivation: Solving interacting multi-particle systems is fundamental in quantum chemistry and condensed matter physics, but classically challenging. Quantum computing offers potential advantages for such calculations.

Method: Variational Quantum Eigensolver (VQE) implemented on both quantum computer simulator and IBM quantum device. H2O simulations performed on Nottingham's HPC facilities.

Result: Ground states and ground-state energies computed for He-H+ and H2O molecules. Results benchmarked against exact classical ground-state energies.

Conclusion: Demonstrates feasibility of quantum computing approaches for molecular ground state calculations, with validation against classical exact methods.

Abstract: Solving interacting multi-particle systems is a central challenge in quantum chemistry and condensed matter physics. In this work, we investigate the computation of ground states and ground-state energies for the He-H+ and H2O molecules using quantum computing techniques. We employ the variational quantum eigensolver (VQE), implemented both on a quantum computer simulator and on an IBM quantum device. The resulting energies are benchmarked against exact ground-state energies obtained via classical methods. Simulations of the H2O molecule were performed on Nottingham's High Performance Computing (HPC) facilities.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [64] [The Open Polymers 2026 (OPoly26) Dataset and Evaluations](https://arxiv.org/abs/2512.23117)
*Daniel S. Levine,Nicholas Liesen,Lauren Chua,James Diffenderfer,Helgi Ingolfsson,Matthew P. Kroonblawd,Nitesh Kumar,Amitesh Maiti,Supun S. Mohottalalage,Muhammed Shuaibi,Brian Van Essen,Brandon M. Wood,C. Lawrence Zitnick,Samuel M. Blau,Evan R. Antoniuk*

Main category: physics.chem-ph

TL;DR: The paper introduces OPoly26, a large dataset of 6.57 million DFT calculations on polymer-derived clusters, addressing the gap in ML datasets for polymers and showing improved ML model performance for polymer property predictions.


<details>
  <summary>Details</summary>
Motivation: Polymers are fundamental to biology and technology, but existing ML datasets focus on small molecules and materials, excluding polymers due to computational expense of high-quality electronic structure calculations on representative polymeric structures.

Method: Created the Open Polymers 2026 (OPoly26) dataset containing over 6.57 million DFT calculations on up to 360-atom clusters derived from polymeric systems, capturing chemical diversity including monomer composition, polymerization degree, chain architectures, and solvation environments.

Result: The dataset comprises over 1.2 billion total atoms and shows that augmenting ML model training with OPoly26 improves model performance for polymer prediction tasks.

Conclusion: OPoly26 addresses the polymer data gap in ML, enables better polymer property predictions, and is publicly released to advance ML models for polymers and contribute to universal atomistic models.

Abstract: Polymers-macromolecular systems composed of repeating chemical units-constitute the molecular foundation of living organisms, while their synthetic counterparts drive transformative advances across medicine, consumer products, and energy technologies. While machine learning (ML) models have been trained on millions of quantum chemical atomistic simulations for materials and/or small molecular structures to enable efficient, accurate, and transferable predictions of chemical properties, polymers have largely not been included in prior datasets due to the computational expense of high quality electronic structure calculations on representative polymeric structures. Here, we address this shortcoming with the creation of the Open Polymers 2026 (OPoly26) dataset, which contains more than 6.57 million density functional theory (DFT) calculations on up to 360 atom clusters derived from polymeric systems, comprising over 1.2 billion total atoms. OPoly26 captures the chemical diversity that makes polymers intrinsically tunable and versatile materials, encompassing variations in monomer composition, degree of polymerization, chain architectures, and solvation environments. We show that augmenting ML model training with the OPoly26 dataset improves model performance for polymer prediction tasks. We also publicly release the OPoly26 dataset to help further the development of ML models for polymers, and more broadly, strive towards universal atomistic models.

</details>


<div id='nlin.PS'></div>

# nlin.PS [[Back]](#toc)

### [65] [amangkurat: A Python Library for Symplectic Pseudo-Spectral Solution of the Idealized (1+1)D Nonlinear Klein-Gordon Equation](https://arxiv.org/abs/2512.22635)
*Sandy H. S. Herho,Siti N. Kaban*

Main category: nlin.PS

TL;DR: amangkurat is an open-source Python library for simulating relativistic scalar field dynamics using Fourier pseudo-spectral methods with symplectic time integration, featuring adaptive timestepping and JIT compilation for performance.


<details>
  <summary>Details</summary>
Motivation: To create a robust, efficient, and accessible computational platform for simulating nonlinear Klein-Gordon equations in (1+1)D spacetime, enabling exploratory research and education in relativistic scalar field dynamics.

Method: Hybrid computational strategy combining Fourier pseudo-spectral spatial discretization with symplectic Størmer-Verlet temporal integration, plus adaptive timestepping (CFL criteria) and Just-In-Time compilation for parallelized force computation.

Result: The library successfully simulates four canonical physical regimes (linear waves, phi-fourth kinks, sine-Gordon breathers, kink-antikink collisions) and demonstrates statistical distinguishability using information-theoretic entropy metrics, achieving high computational efficiency on standard hardware.

Conclusion: amangkurat provides a viable, efficient platform for numerical simulation of relativistic scalar field dynamics, validated across multiple physical regimes with comprehensive analysis tools, suitable for both research and educational applications.

Abstract: This study introduces amangkurat, an open-source Python library designed for the robust numerical simulation of relativistic scalar field dynamics governed by the nonlinear Klein-Gordon equation in $(1+1)$D spacetime. The software implements a hybrid computational strategy that couples Fourier pseudo-spectral spatial discretization with a symplectic Størmer-Verlet temporal integrator, ensuring both exponential spatial convergence for smooth solutions and long-term preservation of Hamiltonian structure. To optimize performance, the solver incorporates adaptive timestepping based on Courant-Friedrichs-Lewy (CFL) stability criteria and utilizes Just-In-Time (JIT) compilation for parallelized force computation. The library's capabilities are validated across four canonical physical regimes: dispersive linear wave propagation, static topological kink preservation in phi-fourth theory, integrable breather dynamics in the sine-Gordon model, and non-integrable kink-antikink collisions. Beyond standard numerical validation, this work establishes a multi-faceted analysis framework employing information-theoretic entropy metrics (Shannon, Rényi, and Tsallis), kernel density estimation, and phase space reconstruction to quantify the distinct phenomenological signatures of these regimes. Statistical hypothesis testing confirms that these scenarios represent statistically distinguishable dynamical populations. Benchmarks on standard workstation hardware demonstrate that the implementation achieves high computational efficiency, making it a viable platform for exploratory research and education in nonlinear field theory.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [66] [Scaling inequalities for Steklov eigenvalues in space forms and sharp eigenvalue estimates on warped product manifolds](https://arxiv.org/abs/2512.22885)
*Zongyi Lv,Changwei Xiong,Yuxun Zou*

Main category: math.DG

TL;DR: The paper studies monotonicity properties of Steklov eigenvalue spectra on geodesic disks and warped product manifolds, obtaining Escobar-type bounds and confirming a conjecture on fourth-order Steklov problems.


<details>
  <summary>Details</summary>
Motivation: To understand how Steklov eigenvalues (spectra of boundary value problems) behave on curved spaces, particularly geodesic disks in spheres/hyperbolic spaces and warped product manifolds, and to establish sharp bounds for these eigenvalues.

Method: Two-part approach: 1) Derive monotonicity of normalized spectra for second-order and fourth-order Steklov problems on 2D geodesic disks using four geometric normalizations; 2) Obtain sharp bounds for fourth-order Steklov spectra on warped product manifolds with non-negative Ricci curvature and convex boundaries.

Result: 1) Monotonicity results for normalized spectra on geodesic disks, leading to Escobar-type bounds for varying curvature; 2) Sharp bounds for fourth-order Steklov spectra on warped product manifolds, confirming Wang and Xia's 2018 conjecture for 3D cases.

Conclusion: The paper establishes fundamental monotonicity properties and sharp bounds for Steklov eigenvalues on curved spaces, advancing the understanding of spectral geometry and confirming an important conjecture in the field.

Abstract: In the first part, we derive monotonicity of the normalized spectra for the second-order Steklov problem and two fourth-order Steklov problems on the $2$-dimensional geodesic disks with respect to the geodesic radius in the sphere and the hyperbolic space. The normalizations are made using four natural geometric factors. As corollaries, we get Escobar-type bounds for Steklov eigenvalues on $2$-dimensional geodesic disks with varying curvature in space forms. We also get two monotonicity results for higher-dimensional cases. In the second part, we obtain some sharp bounds concerning the spectra of the two fourth-order Steklov problems on warped product manifolds with non-negative Ricci curvature and a strictly convex boundary. In particular, we confirm Qiaoling Wang and Changyu Xia's conjecture (2018) on the sharp lower bound of the first non-zero eigenvalue of a fourth-order Steklov problem in the case of $3$-dimensional warped product manifolds.

</details>


### [67] [Rotationally symmetric translating solitons of fully nonlinear extrinsic geometric flows: Classification and Applications](https://arxiv.org/abs/2512.23623)
*José Torres Santaella*

Main category: math.DG

TL;DR: Study of rotationally symmetric translators for nonlinear curvature flows, analyzing bowl-type and catenoidal-type solutions with asymptotics, plus rigidity results for graphical translators.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior and classification of rotationally symmetric translating solutions (translators) for fully nonlinear extrinsic geometric flows driven by curvature functions, which are important in studying mean curvature flow and related geometric evolution equations.

Method: Analysis of rotationally symmetric translators through establishing fine asymptotics for bowl-type evolutions, constructing and classifying catenoidal-type solutions when admissible, and proving rigidity/uniqueness results under structural and convexity assumptions.

Result: Established detailed asymptotics for bowl-type evolutions, constructed and classified catenoidal-type solutions with their asymptotic behavior, and proved rigidity and uniqueness results within appropriate classes of graphical translators under natural structural and convexity conditions.

Conclusion: The paper provides comprehensive analysis of rotationally symmetric translators for nonlinear curvature flows, offering classification results, asymptotic descriptions, and rigidity theorems that advance understanding of translating solutions in geometric flows.

Abstract: We study rotationally symmetric translators for fully nonlinear extrinsic geometric flows driven by a curvature function, and we establish the fine asymptotics of bowl-type evolutions and, when admissible, the construction and classification of catenoidal-type solutions, together with their asymptotic behavior. Under natural structural and convexity assumptions, we also prove rigidity and uniqueness results within appropriate classes of graphical translators of such curvature flows.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [68] [Active-Absorbing Phase Transitions in the Parallel Minority Game](https://arxiv.org/abs/2512.22826)
*Aryan Tyagi,Soumyajyoti Biswas,Anirban Chakraborti*

Main category: cond-mat.stat-mech

TL;DR: The Parallel Minority Game exhibits different critical behaviors depending on agent decision rules: instantaneous rules show mean-field directed percolation scaling, while threshold rules produce a distinct non-mean-field universality class.


<details>
  <summary>Details</summary>
Motivation: To understand how minimal cognitive features at the agent level fundamentally alter large-scale critical behavior in socio-economic and active systems, specifically examining how different microscopic decision rules affect phase transitions in adaptive multi-agent models.

Method: Comprehensive numerical study of the Parallel Minority Game under two families of microscopic decision rules: (1) instantaneous population-based updates, and (2) threshold-based activation that only triggers movement after overcrowding crosses a threshold. Measurements of time-dependent and steady-state activity A(t) and overcrowding fraction F(t) as functions of control parameter g=N/D.

Result: Instantaneous rules display mean-field directed-percolation scaling with β≈1.00, δ≈0.5, and ν∥≈2.0. Threshold rules produce a distinct non-mean-field universality class with β≈0.75 and systematic failure of MF-DP dynamical scaling, showing that thresholding acts as a relevant perturbation to DP.

Conclusion: Minimal cognitive features at the agent level (specifically threshold-based decision rules) fundamentally alter large-scale critical behavior, creating distinct universality classes that deviate from mean-field directed percolation predictions in socio-economic and active systems.

Abstract: The Parallel Minority Game (PMG) is a synchronous adaptive multi-agent model that exhibits active-absorbing transitions characteristic of non-equilibrium statistical systems. We perform a comprehensive numerical study of the PMG under two families of microscopic decision rules: (i) agents update their choices based on instantaneous population in their alternative choices, and (ii) threshold-based activation that activates agents movement only after overcrowding density crossing a threshold. We measure time-dependent and steady state limits of activity $A(t)$, overcrowding fraction $F(t)$ as functions of the control parameter $g=N/D$, where $N$ is the number of agents and $D$ is the total number of sites. Instantaneous rules display mean-field directed-percolation (MF-DP) scaling with $β\approx1.00$, $δ\approx0.5$, and $ν_{\parallel}\approx2.0$. Threshold rules, however, produce a distinct non-mean-field universality class with $β\approx0.75$ and a systematic failure of MF-DP dynamical scaling. We show that thresholding acts as a relevant perturbation to DP. The results highlight how minimal cognitive features at the agent level fundamentally alter large-scale critical behaviour in socio-economic and active systems.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [69] [Solving the constraint equation for general free data](https://arxiv.org/abs/2512.22704)
*Xuantao Chen,Sergiu Klainerman*

Main category: gr-qc

TL;DR: A new method for solving Einstein constraint equations that prescribes four scalar quantities representing full dynamical degrees of freedom, enabling construction of large classes of initial Cauchy data sets for black holes.


<details>
  <summary>Details</summary>
Motivation: To develop a flexible method for constructing initial data sets for Einstein's equations that can: 1) provide large classes of exterior solutions matching interior solutions via gluing techniques, 2) generalize trapped surface formation results, 3) construct data with various decay rates (fast/slow), and 4) test the sharpness of existing stability results by violating Shen's decay conditions.

Method: After choosing appropriate gauge conditions and freely specifying four scalars (modulo ℓ≤1 modes), the constraint equations are rewritten as a well-posed system of coupled transport and elliptic equations on 2-spheres, solved via an iteration procedure.

Result: The method produces a large class of exterior solutions of constraint equations that can be matched to given interior solutions using existing gluing techniques, enabling construction of initial Cauchy data sets evolving to black holes. The method is flexible enough to handle various decay rates (O(r^{-1-δ}) for metric, O(r^{-2-δ}) for extrinsic curvature) and can be adapted for arbitrarily fast decaying data.

Conclusion: This new approach provides a powerful framework for constructing initial data for Einstein's equations with prescribed properties, generalizing previous results and offering potential to test the sharpness of stability conditions in Minkowski space by constructing counterexamples that violate Shen's decay conditions.

Abstract: We revisit the problem of solving the Einstein constraint equations in vacuum by a new method, which allows us to prescribe four scalar quantities, representing the full dynamical degrees of freedom of the constraint system. We show that once appropriate gauge conditions have been chosen and four scalars freely specified (modulo $\ell\leq 1$ modes), we can rewrite the constraint equations as a well-posed system of coupled transport and elliptic equations on $2$-spheres, which we solve by an iteration procedure. Our method provides a large class of exterior solutions of the constraint equations that can be matched to given interior solutions, according to the existing gluing techniques. As such, it can be applied to provide a large class of initial Cauchy data sets evolving to black holes, generalizing the well-known result of the formation of trapped surfaces due to Li and Yu. Though in our main theorem, we only specify conditions consistent with $g-g_{Schw}=O(r^{-1-δ})$, $k=O(r^{-2-δ})$, the method is flexible enough to be applied in many other situations. It can, in particular, be easily adapted to construct arbitrarily fast decaying data. We expect, moreover, that our method can also be applied to construct data with slower decay, such as that used by Shen. In fact, an important motivation for developing our method is to show that the result of Shen is sharp, i.e., construct small, smooth initial data sets which violate Shen's decay conditions, and for which the stability of the Minkowski space result is wrong.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [70] [A front-tracking study of retinal detachment treatment by magnetic drop targeting](https://arxiv.org/abs/2512.22537)
*Mohammad Amin Amini,Gretar Tryggvason,Ehsan Amani*

Main category: physics.flu-dyn

TL;DR: Extended Front-Tracking Method for 3D ferrofluid drop targeting in retinal detachment treatment, analyzing magnetic field effects on drop dynamics and retinal coverage.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate computational model for Ferrofluid Drop Targeting (FDT) treatment of Retinal Detachment that considers real 3D eye geometry, magnet configurations, and viscoelastic vitreous humor properties.

Method: Extended Front-Tracking Method (FTM) adapted for 3D unstructured Eulerian grids with wall effects, including multi-region grid design, threshold distance definition, front smoothing, and volume correction algorithms near walls.

Result: Both magnetic Bond number and drop-to-VH magnetic permeability ratio significantly affect terminal shape parameters like retinal coverage. Increasing both parameters boosts total FDT force, coverage area, and stress concentration, while reducing drop-VH surface tension mitigates retinal stress.

Conclusion: The developed 3D FTM model successfully simulates ferrofluid drop targeting for retinal detachment treatment, revealing key design parameters that optimize retinal coverage while managing stress concentration on delicate retinal tissue.

Abstract: We investigate the Ferrofluid Drop Targeting (FDT) for the treatment of the Retinal Detachment (RD), considering, for the first time, the real 3D geometry of an eye and magnets configurations as well as the viscoelastic rheology of the medium, i.e., the Vitreous Humor (VH). A Front-Tracking Method (FTM) is extended to handle a general 3D unstructured Eulerian grid and strong wall effects. The challenges include the accuracy and robustness of the solver when the drop spreads on the retina under the effect of a magnetic field, which necessitates the design of a multi-region Eulerian grid and defining a threshold distance between the front and wall, along with the choice of an effective front smoothing and volume correction FTM sub-algorithms near the walls. After model validations, the effect of different design parameters on important objectives, such as the travel time, settling time, retinal coverage area, and impact compressive stress, are studied. The results reveal that, in addition to the magnetic Bond number, the ratio of the drop-to-VH magnetic permeabilities plays a key role in the terminal shape parameters, like the retinal coverage. Additionally, simultaneously increasing these two parameters, significantly increase the total FDT force, coverage area, and stress concentration, while decreasing the drop-VH surface tension can mitigate the stress concentration on the retina.

</details>


### [71] [Electrode Geometry Optimization in Vortex-Type Seawater Magnetohydrodynamic Generators](https://arxiv.org/abs/2512.22446)
*Arleen Natalie,Budiarso,Ridho Irwansyah*

Main category: physics.flu-dyn

TL;DR: Electrode geometry optimization in vortex-type seawater MHD generators shows whole-area electrodes increase power output by 155% compared to baseline partial electrodes.


<details>
  <summary>Details</summary>
Motivation: MHD generators offer clean energy conversion by transforming conductive fluids' kinetic energy into electricity. This study aims to improve performance through electrode geometry modifications to advance seawater-based MHD as sustainable energy systems.

Method: Three electrode designs (partial, whole-area, spiral) were analyzed using combined analytical and numerical simulations with COMSOL Multiphysics. Focused on internal resistance reduction, current density distribution, and power output.

Result: Electrode area and spacing are critical performance determinants. Whole-area electrode achieved highest output with 155% power increase over baseline. Spiral electrode reduced internal resistance but had lower open-circuit voltage. Simulations showed <4% deviation from theoretical models.

Conclusion: Geometric optimization of electrodes is crucial for advancing seawater-based MHD generators as sustainable and efficient energy conversion systems, with whole-area electrodes showing the most promising performance improvements.

Abstract: Magnetohydrodynamics (MHD) generators present a promising pathway for clean energy conversion by directly transforming conductive fluids' kinetic energy into electricity. This study investigates the impact of electrode geometry modifications on the performance of a vortex-type seawater MHD generator. Three electrode designs, partial, whole-area, and spiral, are analyzed through combined analytical and numerical simulations using COMSOL Multiphysics. The study focuses on internal resistance reduction, current density distribution, and overall power output. The results indicate that electrode area and spacing are critical determinants of performance. The whole-area electrode achieves the highest output, with a 155 percent increase in power compared to the baseline partial electrode. The spiral electrode demonstrates reduced internal resistance and improved current flow but exhibits lower open-circuit voltage due to reduced electrode spacing. The simulations show strong agreement with theoretical models, with deviations of less than 4 percent in open-circuit voltage predictions. These findings highlight the importance of geometric optimization for advancing seawater-based MHD generators as sustainable and efficient energy conversion systems.

</details>


### [72] [On the Reynolds-number scaling of Poisson solver complexity](https://arxiv.org/abs/2512.22644)
*F. Xavier Trias,Àdel Alsalti-Baldellou,Assensi Oliva*

Main category: physics.flu-dyn

TL;DR: The paper analyzes how Poisson equation solver complexity scales with Reynolds number in large incompressible flow simulations, finding it decreases for Navier-Stokes turbulence but increases for 1D Burgers equation.


<details>
  <summary>Details</summary>
Motivation: To understand whether solving the Poisson equation becomes easier or harder for very large simulations of incompressible flows at high Reynolds numbers, which is crucial for extreme-scale computational fluid dynamics.

Method: Combines physical and numerical arguments to derive power-law scalings, performs theoretical convergence analysis for Jacobi and multigrid solvers, defines a 2D phase space divided by solver iteration trends, and validates with numerical experiments.

Result: For Navier-Stokes turbulence, solver complexity decreases with increasing Reynolds number, while for the 1D Burgers equation it increases. The theoretical framework successfully predicts these opposite trends.

Conclusion: The proposed framework provides unified understanding of solver convergence scaling with Reynolds number and offers valuable guidance for developing next-generation preconditioning and multigrid strategies for extreme-scale simulations.

Abstract: We aim to answer the following question: is the complexity of numerically solving the Poisson equation increasing or decreasing for very large simulations of incompressible flows? Physical and numerical arguments are combined to derive power-law scalings at very high Reynolds numbers. A theoretical convergence analysis for both Jacobi and multigrid solvers defines a two-dimensional phase space divided into two regions depending on whether the number of solver iterations tends to decrease or increase with the Reynolds number. Numerical results indicate that, for Navier-Stokes turbulence, the complexity decreases with increasing Reynolds number, whereas for the one-dimensional Burgers equation it follows the opposite trend. The proposed theoretical framework thus provides a unified perspective on how solver convergence scales with the Reynolds number and offers valuable guidance for the development of next-generation preconditioning and multigrid strategies for extreme-scale simulations.

</details>


### [73] [A rational length scale for large-eddy simulation of turbulence on anisotropic grids](https://arxiv.org/abs/2512.22717)
*F. Xavier Trias,Jesús Ruano,Alexey Duben,Andrey Gorobets*

Main category: physics.flu-dyn

TL;DR: A new subgrid characteristic length scale for LES is proposed to address mesh anisotropy issues, derived from analyzing numerical discretization-filtering entanglement, showing improved performance in isotropic turbulence and channel flow simulations.


<details>
  <summary>Details</summary>
Motivation: Direct numerical simulation of turbulence is too expensive for real applications, requiring simplified LES models. Current eddy-viscosity models use subgrid characteristic lengths tied to local grid size, but these definitions fail for unstructured or anisotropic meshes (like pancake-like meshes for near-wall turbulence), with no consensus on proper formulation despite significant impact on LES performance.

Method: Introduces a novel subgrid characteristic length derived from analyzing the entanglement between numerical discretization and filtering in LES. The length scale has mathematical properties and simplicity designed to reduce mesh anisotropy impact on simulation accuracy.

Result: Demonstrates effectiveness through simulations of decaying isotropic turbulence and turbulent channel flow using different codes, showing improved performance in handling mesh anisotropies.

Conclusion: The proposed subgrid length scale provides a robust solution for LES models on anisotropic meshes, addressing a long-standing open problem in turbulence modeling and improving simulation accuracy for practical applications.

Abstract: Due to the prohibitive cost of resolving all relevant scales, direct numerical simulations of turbulence remain unfeasible for most real-world applications. Consequently, dynamically simplified formulations are needed for coarse-grained simulations. In this regard, eddy-viscosity models for Large-Eddy Simulation (LES) are widely used both in academia and industry. These models require a subgrid characteristic length, typically linked to the local grid size. While this length scale corresponds to the mesh step for isotropic grids, its definition for unstructured or anisotropic Cartesian meshes, such as the pancake-like meshes commonly used to capture near-wall turbulence or shear layers, remains an open question. Despite its significant influence on LES model performance, no consensus has been reached on its proper formulation. In this work, we introduce a novel subgrid characteristic length. This length scale is derived from the analysis of the entanglement between the numerical discretization and the filtering in LES. Its mathematical properties and simplicity make it a robust choice for reducing the impact of mesh anisotropies on simulation accuracy. The effectiveness of the proposed subgrid length is demonstrated through simulations of decaying isotropic turbulence and a turbulent channel flow using different codes.

</details>


### [74] [An efficient eigenvalue bounding method: CFL condition revisited](https://arxiv.org/abs/2512.22994)
*F. Xavier Trias,Xavier Álvarez-Farré,Àdel Alsalti-Baldellou,Andrey Gorobets,Assensi Oliva*

Main category: physics.flu-dyn

TL;DR: A new inexpensive method for computing eigenbounds of convective and diffusive matrices in CFD simulations without matrix reconstruction, enabling larger time-steps and better code portability.


<details>
  <summary>Details</summary>
Motivation: Explicit temporal schemes in turbulence simulations require small time-steps due to stability constraints, and traditional CFL-based approaches need expensive matrix reconstructions. Modern supercomputing heterogeneity also challenges code portability, requiring minimal kernel sets for maintenance.

Method: Proposes an inexpensive method that computes eigenbounds without reconstructing time-dependent matrices, relying only on sparse-matrix vector products where only vectors change over time.

Result: Method demonstrated effective and robust for different test cases on both structured Cartesian and unstructured meshes. When combined with self-adaptive temporal scheme, achieves significantly larger time-steps compared to conventional CFL approaches.

Conclusion: The proposed method provides an efficient alternative to traditional CFL-based eigenbound computations, enabling larger time-steps while maintaining code portability through minimal kernel requirements.

Abstract: Direct and large-eddy simulations of turbulence are often solved using explicit temporal schemes. However, this imposes very small time-steps because the eigenvalues of the (linearized) dynamical system, re-scaled by the time-step, must lie inside the stability region. In practice, fast and accurate estimations of the spectral radii of both the discrete convective and diffusive terms are therefore needed. This is virtually always done using the so-called CFL condition. On the other hand, the large heterogeneity and complexity of modern supercomputing systems are nowadays hindering the efficient cross-platform portability of CFD codes. In this regard, our leitmotiv reads: relying on a minimal set of (algebraic) kernels is crucial for code portability and maintenance! In this context, this work focuses on the computation of eigenbounds for the above-mentioned convective and diffusive matrices which are needed to determine the time-step à la CFL. To do so, a new inexpensive method, that does not require to re-construct these time-dependent matrices, is proposed and tested. It just relies on a sparse-matrix vector product where only vectors change on time. Hence, both implementation in existing codes and cross-platform portability are straightforward. The effectiveness and robustness of the method are demonstrated for different test cases on both structured Cartesian and unstructured meshes. Finally, the method is combined with a self-adaptive temporal scheme, leading to significantly larger time-steps compared with other more conventional CFL-based approaches.

</details>


### [75] [Phase-field modeling of multicomponent vesicles in viscoelastic fluid](https://arxiv.org/abs/2512.23315)
*Zuowei Wen,Navid Valizadeh,Timon Rabczuk,Xiaoying Zhuang*

Main category: physics.flu-dyn

TL;DR: Developed a CSF phase-field model to study multicomponent vesicles in viscoelastic fluids using advanced numerical methods including RBVMS, SUPG, and IGA.


<details>
  <summary>Details</summary>
Motivation: Multicomponent vesicles in viscoelastic fluids are important for understanding physiological processes, requiring accurate modeling of their hydrodynamics.

Method: Continuum surface force phase-field model coupling fluid field (Newtonian + Oldroyd-B), surface concentration field (Cahn-Hilliard), and membrane evolution (nonlinear advection-diffusion). Uses RBVMS for Navier-Stokes, SUPG for Oldroyd-B, standard Galerkin for other equations, implicit monolithic scheme with generalized-α time integration, and isogeometric analysis for spatial accuracy.

Result: Presented 2D numerical examples in shear and Poiseuille flows showing the influence of membrane composition and fluid viscoelasticity on vesicle hydrodynamics.

Conclusion: The developed model successfully captures complex interactions in multicomponent vesicle systems in viscoelastic flows, providing insights into physiological processes.

Abstract: Multicomponent vesicles suspended in viscoelastic fluids are crucial for understanding a variety of physiological processes. In this work, we develop a continuum surface force (CSF) phase-field model to investigate the hydrodynamics of inextensible multicomponent vesicles in viscoelastic fluid flows with inertial forces. Our model couples a fluid field comprising both Newtonian and Oldroyd-B fluids, a surface concentration field representing the multicomponent distribution on the vesicle membrane, and a phase-field variable governing the membrane evolution. The viscoelasticity effect of extra stress is well incorporated into the full Navier-Stokes equations in the fluid field. The surface concentration field is determined by Cahn-Hilliard equations, while the membrane evolution is governed by a nonlinear advection-diffusion equation. The membrane is coupled to the surrounding fluid through the continuum surface force (CSF) framework. To ensure stable numerical solutions of the highly nonlinear multi-field model, we employ a residual-based variational multiscale (RBVMS) method for the Navier-Stokes equations, a Streamline-Upwind Petrov-Galerkin (SUPG) method for the Oldroyd-B equations, and a standard Galerkin finite element framework for the remaining equations. The system of PDEs is solved using an implicit, monolithic scheme based on the generalized-$α$ time integration method. To enhance spatial accuracy, we employ isogeometric analysis (IGA). We present a series of two-dimensional numerical examples in shear and Poiseuille flows to elucidate the influence of membrane composition and fluid viscoelasticity on the hydrodynamics of multicomponent vesicles.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [76] [PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations](https://arxiv.org/abs/2512.23056)
*Min Zhu,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer,Lu Lu*

Main category: cs.LG

TL;DR: PI-MFM is a physics-informed multimodal foundation model framework that enforces governing PDE equations during training, enabling data-efficient learning of PDE solution operators across diverse equation families.


<details>
  <summary>Details</summary>
Motivation: Existing multi-operator learning approaches for PDEs are data-hungry and neglect physics during training, limiting their practical application and data efficiency.

Method: PI-MFM takes symbolic PDE representations as input and automatically assembles PDE residual losses via vectorized derivative computation, enabling unified physics-informed training across equation families.

Result: PI-MFM outperforms purely data-driven models, especially with sparse data, partial observations, or few labeled pairs. It improves robustness against noise and enables zero-shot physics-informed fine-tuning to unseen PDE families with ~1% test error.

Conclusion: PI-MFM provides a practical, scalable path toward data-efficient, transferable PDE solvers by integrating physics directly into multimodal foundation model training.

Abstract: Partial differential equations (PDEs) govern a wide range of physical systems, and recent multimodal foundation models have shown promise for learning PDE solution operators across diverse equation families. However, existing multi-operator learning approaches are data-hungry and neglect physics during training. Here, we propose a physics-informed multimodal foundation model (PI-MFM) framework that directly enforces governing equations during pretraining and adaptation. PI-MFM takes symbolic representations of PDEs as the input, and automatically assembles PDE residual losses from the input expression via a vectorized derivative computation. These designs enable any PDE-encoding multimodal foundation model to be trained or adapted with unified physics-informed objectives across equation families. On a benchmark of 13 parametric one-dimensional time-dependent PDE families, PI-MFM consistently outperforms purely data-driven counterparts, especially with sparse labeled spatiotemporal points, partially observed time domains, or few labeled function pairs. Physics losses further improve robustness against noise, and simple strategies such as resampling collocation points substantially improve accuracy. We also analyze the accuracy, precision, and computational cost of automatic differentiation and finite differences for derivative computation within PI-MFM. Finally, we demonstrate zero-shot physics-informed fine-tuning to unseen PDE families: starting from a physics-informed pretrained model, adapting using only PDE residuals and initial/boundary conditions, without any labeled solution data, rapidly reduces test errors to around 1% and clearly outperforms physics-only training from scratch. These results show that PI-MFM provides a practical and scalable path toward data-efficient, transferable PDE solvers.

</details>


### [77] [Spectral Analysis of Hard-Constraint PINNs: The Spatial Modulation Mechanism of Boundary Functions](https://arxiv.org/abs/2512.23295)
*Yuchen Xie,Honghang Chi,Haopeng Quan,Yahui Wang,Wei Wang,Yu Ma*

Main category: cs.LG

TL;DR: HC-PINNs with hard constraints alter learning dynamics via boundary functions acting as spectral filters, with effective rank predicting convergence better than condition numbers.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical mechanisms behind Physics-Informed Neural Networks with hard constraints (HC-PINNs), which have been empirically successful but lack theoretical analysis of their training dynamics compared to soft-constrained formulations.

Method: Established a rigorous Neural Tangent Kernel (NTK) framework for HC-PINNs, deriving explicit kernel composition laws and conducting spectral analysis to show how boundary functions act as spectral filters reshaping the neural network's kernel eigenspectrum.

Result: Boundary functions introduce multiplicative spatial modulation that fundamentally alters the learning landscape; effective rank of residual kernel serves as deterministic predictor of training convergence; common boundary functions can cause spectral collapse leading to optimization stagnation despite exact boundary satisfaction.

Conclusion: The framework transforms boundary function design from heuristic to principled spectral optimization, providing theoretical foundation for geometric hard constraints in scientific machine learning, validated across multi-dimensional benchmarks.

Abstract: Physics-Informed Neural Networks with hard constraints (HC-PINNs) are increasingly favored for their ability to strictly enforce boundary conditions via a trial function ansatz $\tilde{u} = A + B \cdot N$, yet the theoretical mechanisms governing their training dynamics have remained unexplored.
  Unlike soft-constrained formulations where boundary terms act as additive penalties, this work reveals that the boundary function $B$ introduces a multiplicative spatial modulation that fundamentally alters the learning landscape.
  A rigorous Neural Tangent Kernel (NTK) framework for HC-PINNs is established, deriving the explicit kernel composition law.
  This relationship demonstrates that the boundary function $B(\vec{x})$ functions as a spectral filter, reshaping the eigenspectrum of the neural network's native kernel.
  Through spectral analysis, the effective rank of the residual kernel is identified as a deterministic predictor of training convergence, superior to classical condition numbers.
  It is shown that widely used boundary functions can inadvertently induce spectral collapse, leading to optimization stagnation despite exact boundary satisfaction.
  Validated across multi-dimensional benchmarks, this framework transforms the design of boundary functions from a heuristic choice into a principled spectral optimization problem, providing a solid theoretical foundation for geometric hard constraints in scientific machine learning.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [78] [Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space](https://arxiv.org/abs/2512.22676)
*Sergey Salishev*

Main category: eess.SP

TL;DR: Develops low-power signal processing algorithms focusing on energy efficiency through optimal parallelism selection, memory-efficient function approximations, conflict-free FFT scheduling, and Toeplitz system analysis.


<details>
  <summary>Details</summary>
Motivation: To improve energy efficiency of low-power computing hardware by developing signal-processing algorithms that work under constraints of minimal parallelism and memory space, enabling efficient specialized accelerators.

Method: Four main approaches: (1) power/energy consumption model for CMOS logic to select optimal parallelism, (2) integer-friendly approximation methods using constrained piecewise-polynomial constructions to reduce lookup-table size, (3) provably conflict-free data placement and execution order for mixed-radix streaming FFT on memory-constrained systems, (4) parallelism/memory analysis of fast Schur algorithm for Toeplitz system solving.

Result: Provides constructive theorems, schedules, and design trade-offs that enable efficient specialized accelerators for low-power signal processing applications, with specific techniques for energy optimization, memory reduction, and conflict-free execution.

Conclusion: The thesis successfully develops a comprehensive framework for energy-efficient signal processing on low-power hardware through algorithmic innovations in parallelism optimization, memory-efficient approximations, and conflict-free scheduling, enabling practical implementation of specialized accelerators.

Abstract: This thesis develops signal-processing algorithms and implementation schemes under constraints of minimal parallelism and memory space, with the goal of improving energy efficiency of low-power computing hardware. We propose (i) a power/energy consumption model for clocked CMOS logic that supports selecting optimal parallelism, (ii) integer-friendly approximation methods for elementary functions that reduce lookup-table size via constrained piecewise-polynomial (quasi-spline) constructions with accuracy guarantees, (iii) provably conflict-free data placement and execution order for mixed-radix streaming FFT on multi-bank and single-port memories, including a self-sorting FFT variant, and (iv) a parallelism/memory analysis of the fast Schur algorithm for superfast Toeplitz system solving, motivated by echo-cancellation workloads. The results provide constructive theorems, schedules, and design trade-offs enabling efficient specialized accelerators.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [79] [Inverse scattering for waveguides in topological insulators](https://arxiv.org/abs/2512.22480)
*Guillaume Bal,Xixian Wang,Zhongjian Wang*

Main category: math-ph

TL;DR: Inverse scattering problem for topological waveguides: short-range perturbations can be reconstructed from scattering data in linearized/finite-dimensional settings with stability results, demonstrated numerically.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse scattering problem for topologically non-trivial waveguides separating 2D topological insulators, specifically focusing on Dirac systems, which are important for understanding and characterizing topological materials.

Method: Uses a Dirac system model, linearized setting, and finite-dimensional approach with smallness constraints. Employs theoretical analysis with stability results and implements numerical solution using standard adjoint method with numerical simulations.

Result: Shows that short-range perturbations can be fully reconstructed from scattering data in both linearized and finite-dimensional settings under smallness constraints. Provides stability results in appropriate topologies and validates findings with numerical simulations.

Conclusion: The inverse scattering problem for topological waveguides is solvable with perturbation reconstruction from scattering data, supported by theoretical stability guarantees and validated by numerical implementation.

Abstract: This paper concerns the inverse scattering problem of a topologically non-trivial waveguide separating two-dimensional topological insulators. We consider the specific model of a Dirac system. We show that a short-range perturbation can be fully reconstructed from scattering data in a linearized setting and in a finite-dimensional setting under a smallness constraint. We also provide a stability result in appropriate topologies. We then solve the problem numerically by means of a standard adjoint method and illustrate our theoretical findings with several numerical simulations.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [80] [Volume and Surface Area of two Orthogonal, Partially Intersecting Cylinders: A Generalization of the Steinmetz Solid](https://arxiv.org/abs/2512.22555)
*Fynn Jerome Aschmoneit,Bastiaan Cockx*

Main category: cs.CE

TL;DR: Exact integral expressions and empirical approximations for volume and surface area of intersecting orthogonal cylinders with arbitrary depth ratios.


<details>
  <summary>Details</summary>
Motivation: The intersection of orthogonal cylinders is important for engineering design, manufacturing, and numerical simulation. While analytical solutions exist for fully intersecting cylinders (Steinmetz solid), partial intersections with arbitrary depth ratios require numerical methods or approximations.

Method: Develops general integral expressions for both intersection volume and surface area as explicit functions of intersection depth. Also provides empirical approximation functions for closed-form evaluation.

Result: Empirical approximations achieve relative errors below 15% across the full range of intersection depth. Validation against Quasi-Monte Carlo simulation confirms accuracy of both analytical and approximate solutions.

Conclusion: Provides both exact formulations and practical approximations for calculating volume and surface area of partially intersecting orthogonal cylinders, addressing a gap in computational geometry with engineering applications.

Abstract: The intersection of two orthogonal cylinders represents a classical problem in computational geometry with direct applications to engineering design, manufacturing, and numerical simulation. While analytical solutions exist for the fully intersecting case, the Steinmetz solid, partial intersections with arbitrary depth ratios require numerical methods or approximations. This work presents general integral expressions for both the intersection volume and surface area as explicit functions of the intersection depth. Accompanying these exact formulations are empirical approximation functions, which provide closed-form evaluations with relative errors below 15% across the full range of intersection depth. Validation against Quasi-Monte Carlo simulation confirms the accuracy of both the analytical and approximate solutions.

</details>


### [81] [A Domain Decomposition-based Solver for Acoustic Wave propagation in Two-Dimensional Random Media](https://arxiv.org/abs/2512.23027)
*Sudhi Sharma Padillath Vasudevan*

Main category: cs.CE

TL;DR: Sampling-free intrusive stochastic Galerkin method solves acoustic wave propagation with log-normal random wave speed using PCE and domain decomposition with scalable preconditioned conjugate gradient solver.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve acoustic wave propagation problems with uncertain wave speed modeled as log-normal random field, addressing computational challenges from increasing mesh size, time steps, and random parameters.

Method: Intrusive stochastic Galerkin approach using polynomial chaos expansion (PCE) transforms stochastic PDE into deterministic PDEs, then linear system. Domain decomposition with two-level Neumann-Neumann preconditioner and conjugate gradient solver handles computational complexity.

Result: The method demonstrates efficient scalability for solving acoustic wave propagation with random wave speed, effectively managing computational costs from mesh refinement, time discretization, and increasing random parameters.

Conclusion: The intrusive stochastic Galerkin approach combined with domain decomposition and preconditioned iterative solver provides an efficient, scalable framework for uncertainty quantification in acoustic wave propagation problems with random material properties.

Abstract: An acoustic wave propagation problem with a log normal random field approximation for wave speed is solved using a sampling-free intrusive stochastic Galerkin approach. The stochastic partial differential equation with the inputs and outputs expanded using polynomial chaos expansion (PCE) is transformed into a set of deterministic PDEs and further to a system of linear equations. Domain decomposition (DD)-based solvers are utilized to handle the overwhelming computational cost for the resulting system with increasing mesh size, time step and number of random parameters. A conjugate gradient iterative solver with a two-level Neumann-Neumann preconditioner is applied here showing their efficient scalabilities.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [82] [Anisotropic Photostriction and Strain-modulated Carrier Lifetimes in Orthorhombic Semiconductors](https://arxiv.org/abs/2512.23187)
*Jianxin Yu,Kun Yang,Jiawen Li,Sheng Meng,Xinghua Shi,Jin Zhang*

Main category: cond-mat.mtrl-sci

TL;DR: Anisotropic photostriction in 2D orthorhombic semiconductors shows directional lattice deformation under photoexcitation, with expansion along armchair direction and contraction along zigzag direction, enabling control of carrier recombination lifetimes.


<details>
  <summary>Details</summary>
Motivation: To understand the microscopic origin of anisotropic photostriction in low-dimensional systems and establish quantitative links between photoexcited carriers and lattice deformation, enabling development of light-controllable, directionally sensitive optomechanical devices at atomic scale.

Method: Used time-dependent density functional theory to trace dynamics of photoexcited carriers in layered black phosphorus and germanium selenides, establishing quantitative relationship between carrier density and lattice deformation.

Result: Found significant anisotropic structural response with lattice expansion along armchair direction and contraction along zigzag direction, attributed to interplay between charge redistribution and intrinsic lattice anisotropy. Photostrictive strains can be tuned by photodoping densities, and photoinduced strains significantly increase carrier recombination lifetimes by suppressing nonradiative recombination through enlarged bandgap and weakened nonadiabatic coupling.

Conclusion: Provides microscopic insight into anisotropic photostriction in 2D semiconductors, demonstrating precise control over photoinduced response and establishing foundation for atomic-scale optomechanical devices with directional sensitivity.

Abstract: We demonstrate anisotropic photostriction in two-dimensional orthorhombic semiconductors using time-dependent density functional theory. By tracing the dynamics of photoexcited carriers, we establish a quantitative link between carrier density and lattice deformation in layered black phosphorus and germanium selenides. The structural response exhibits significant anisotropy, featuring lattice expansion along the armchair direction and contraction along the zigzag direction, which is attributed to the interplay between charge redistribution and intrinsic lattice anisotropy. Both the magnitude and orientation of the photostrictive strains can be tuned by photodoping densities, enabling precise control over the photoinduced response. Notably, the photoinduced strains significantly increase carrier recombination lifetimes by suppressing nonradiative recombination, primarily due to the enlarged bandgap and weakened nonadiabatic coupling. These results provide microscopic insight into the origin of anisotropic photostriction in low-dimensional systems and lay the groundwork for light-controllable, directionally sensitive optomechanical devices at the atomic scale.

</details>
