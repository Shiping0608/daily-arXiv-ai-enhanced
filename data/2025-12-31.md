<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 15]
- [math.AP](#math.AP) [Total: 30]
- [physics.comp-ph](#physics.comp-ph) [Total: 11]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 8]
- [math.OC](#math.OC) [Total: 3]
- [cs.CE](#cs.CE) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [nlin.PS](#nlin.PS) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [gr-qc](#gr-qc) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 6]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Differentiable Inverse Modeling with Physics-Constrained Latent Diffusion for Heterogeneous Subsurface Parameter Fields](https://arxiv.org/abs/2512.22421)
*Zihan Lin,QiZhi He*

Main category: math.NA

TL;DR: LD-DIM: Latent diffusion-based differentiable inversion method for PDE-constrained inverse problems that combines pretrained latent diffusion prior with differentiable numerical solver to reconstruct heterogeneous parameter fields in low-dimensional manifold.


<details>
  <summary>Details</summary>
Motivation: PDE-constrained inverse problems with high-dimensional spatially distributed coefficients are challenging due to ill-conditioning and sparse observations. Existing methods struggle with numerical stability and preserving sharp material interfaces.

Method: Couples pretrained latent diffusion model (LDM) with differentiable finite-volume PDE solver. Uses adjoint-based gradients with reverse-mode automatic differentiation. Inversion performed directly in latent space to suppress ill-conditioned degrees of freedom while preserving structural modes.

Result: LD-DIM achieves improved numerical stability and reconstruction accuracy for heterogeneous conductivity fields from sparse hydraulic head measurements. Outperforms PINNs and physics-embedded VAE baselines in preserving sharp discontinuities and reducing sensitivity to initialization.

Conclusion: LD-DIM provides a stable, accurate framework for PDE-constrained inverse problems by leveraging latent diffusion priors with differentiable solvers, enabling effective reconstruction of complex heterogeneous parameter fields under sparse observations.

Abstract: We present a latent diffusion-based differentiable inversion method (LD-DIM) for PDE-constrained inverse problems involving high-dimensional spatially distributed coefficients. LD-DIM couples a pretrained latent diffusion prior with an end-to-end differentiable numerical solver to reconstruct unknown heterogeneous parameter fields in a low-dimensional nonlinear manifold, improving numerical conditioning and enabling stable gradient-based optimization under sparse observations. The proposed framework integrates a latent diffusion model (LDM), trained in a compact latent space, with a differentiable finite-volume discretization of the forward PDE. Sensitivities are propagated through the discretization using adjoint-based gradients combined with reverse-mode automatic differentiation. Inversion is performed directly in latent space, which implicitly suppresses ill-conditioned degrees of freedom while preserving dominant structural modes, including sharp material interfaces. The effectiveness of LD-DIM is demonstrated using a representative inverse problem for flow in porous media, where heterogeneous conductivity fields are reconstructed from spatially sparse hydraulic head measurements. Numerical experiments assess convergence behavior and reconstruction quality for both Gaussian random fields and bimaterial coefficient distributions. The results show that LD-DIM achieves consistently improved numerical stability and reconstruction accuracy of both parameter fields and corresponding PDE solutions compared with physics-informed neural networks (PINNs) and physics-embedded variational autoencoder (VAE) baselines, while maintaining sharp discontinuities and reducing sensitivity to initialization.

</details>


### [2] [ROM for Viscous, Incompressible Flow in Polygons -- exponential $n$-width bounds and convergence rate](https://arxiv.org/abs/2512.22567)
*Francesco Romor,Federico Pichi,Giovanni Stabile,Gianluigi Rozza,Christoph Schwab*

Main category: math.NA

TL;DR: Exponential convergence of Reduced Order Models for stationary Navier-Stokes equations with mixed boundary conditions in polygonal domains, validated by numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To establish theoretical foundations for exponential convergence rates of ROM approximations for complex Navier-Stokes boundary value problems with mixed boundary conditions, which are important for efficient computational fluid dynamics simulations.

Method: Leverages recent results on corner-weighted analytic regularity of velocity/pressure fields to prove exponential convergence of mixed hp-FEM. Uses these bounds to infer exponential bounds for Kolmogorov n-widths, then applies to POD Galerkin methods based on truth solutions from low-order, divergence stable mixed FEM discretizations.

Result: Demonstrates exponential convergence of ROM approximations for mixed boundary value problems of stationary, incompressible Navier-Stokes equations. Numerical experiments confirm the exponential convergence rates and theoretical results.

Conclusion: ROM approximations for Navier-Stokes equations with mixed boundary conditions achieve exponential convergence, providing efficient computational methods for complex fluid dynamics problems in polygonal domains with various boundary condition types.

Abstract: We demonstrate exponential convergence of Reduced Order Model (ROM) approximations for mixed boundary value problems of the stationary, incompressible Navier-Stokes equations in plane, polygonal domains $Ω$. Admissible boundary conditions comprise mixed BCs, no-slip, slip and open boundary conditions, subject to corner-weighted analytic boundary data and volume forcing. The small data hypothesis is assumed to ensure existence of a unique weak solution in the sense of Leray-Hopf. Recent results on corner-weighted, analytic regularity of velocity and pressure fields in $Ω$, imply exponential convergence rates of so-called mixed $hp$-Finite Element Methods in $H^1(Ω)^2\times L^2(Ω)$ on sequences of geometric partitions of $Ω$, with corner-refinement. Based on these exponential convergence rate bounds, we infer exponential bounds for the Kolmogorov $n$-widths of solution sets for analytic forcing and boundary data. This implies corresponding exponential convergence rates of POD Galerkin methods that are based on truth solutions which are obtained offline from low-order, divergence stable mixed Finite Element discretizations. Numerical experiments confirm the exponential rates and the theoretical results.

</details>


### [3] [A high-order method for the numerical approximation of fractional nonlinear Schrödinger equations](https://arxiv.org/abs/2512.22708)
*A. Durán,N. Reguera*

Main category: math.NA

TL;DR: Fourier spectral Galerkin method for spatial discretization and diagonally implicit high-order Runge-Kutta schemes for temporal discretization of periodic fractional nonlinear Schrödinger equation.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate and efficient numerical scheme for solving the periodic initial-value problem of the fractional nonlinear Schrödinger equation, which combines spectral accuracy in space with high-order time integration.

Method: Spatial discretization using Fourier spectral Galerkin method; temporal discretization using diagonally implicit high-order Runge-Kutta schemes based on composition with implicit midpoint rule.

Result: Proved properties and error estimates for both semidiscretization in space and full discretization; demonstrated convergence and performance through numerical experiments.

Conclusion: The proposed numerical scheme provides an effective approach for solving fractional nonlinear Schrödinger equations with proven theoretical properties and demonstrated practical performance.

Abstract: In this paper, the periodic initial-value problem for the fractional nonlinear Schrödinger (fNLS) equation is discretized in space by a Fourier spectral Galerkin method and in time by diagonally implicit, high-order Runge-Kutta schemes, based on the composition with the implicit midpoint rule (IMR). Some properties and error estimates for the semidiscretization in space and for the full discretization are proved. The convergence results and the general performance of the scheme are illustrated with several numerical experiments.

</details>


### [4] [Convergent numerical schemes for the viscoelastic Giesekus model in two dimensions](https://arxiv.org/abs/2512.22831)
*Endre Süli,Dennis Trautwein*

Main category: math.NA

TL;DR: Developed stable, convergent numerical methods for 2D viscoelastic Giesekus model without cut-offs/regularization, proving convergence to weak solutions and providing alternative existence proof.


<details>
  <summary>Details</summary>
Motivation: Existing numerical schemes for viscoelastic Giesekus models suffer from accuracy limitations and convergence problems due to lack of rigorous existence results or discretization limitations.

Method: Developed a class of stable and convergent numerical methods for the 2D viscoelastic Giesekus model that couples incompressible Navier-Stokes equations with evolution equation for elastic stress tensor in terms of deformation gradient.

Result: Proved (subsequence) convergence of the numerical method to large-data global weak solution in 2D without cut-offs/regularization, providing alternative proof of existence result by Bulíček et al. (2022). Verified practicality through numerical experiments including convergence studies and benchmark problems.

Conclusion: The paper presents a rigorous numerical framework for viscoelastic Giesekus model with proven convergence properties, offering both theoretical guarantees and practical computational verification.

Abstract: In this work, we develop a class of stable and convergent numerical methods for the approximate solution of the viscoelastic Giesekus model in two space dimensions. The model couples the incompressible Navier--Stokes equations with an evolution equation for an additional stress tensor accounting for elastic effects. This coupled evolution equation is stated here in terms of the elastic deformation gradient and models transport and nonlinear relaxation effects. In the existing literature, numerical schemes for such models often suffer from accuracy limitations and convergence problems, usually due to the lack of rigorous existence results or inherent limitations of the discretization. Therefore, our main goal is to prove the (subsequence) convergence of the proposed numerical method to a large-data global weak solution in two dimensions, without relying on cut-offs or additional regularization. This also provides an alternative proof of the recent existence result by Bulíček et al.~(Nonlinearity, 2022). Finally, we verify the practicality of the proposed method through numerical experiments, including convergence studies and typical benchmark problems.

</details>


### [5] [A Two-Stage Finite Element Approach for High-precision Guaranteed Lower Eigenvalue Bounds](https://arxiv.org/abs/2512.23182)
*Xuefeng Liu,Michael Plum*

Main category: math.NA

TL;DR: New two-stage algorithm using high-order FEM on graded meshes produces rigorous lower eigenvalue bounds as sharp as high-order upper bounds, closing the gap between accurate upper bounds and reliable lower bounds.


<details>
  <summary>Details</summary>
Motivation: There's a persistent difficulty in obtaining high-precision guaranteed lower eigenvalue bounds while high-order conforming FEM easily yields extremely sharp upper bounds. Existing rigorous approaches don't extend well to high-order FEM, and non-standard approaches are technically involved, creating a competitive gap between upper and lower bounds.

Method: Proposes a new two-stage rigorous algorithm that employs high-order FEM on graded meshes to produce rigorous lower eigenvalue bounds. The method works particularly well on graded or highly nonuniform meshes.

Result: Numerical experiments for Laplacian and Steklov eigenvalue problems on square and dumbbell domains demonstrate the accuracy and efficiency of the method, showing it produces lower bounds as sharp as corresponding high-order upper bounds.

Conclusion: The proposed approach provides a practical and competitive solution to the long-standing difficulty of obtaining sharp, reliable lower eigenvalue bounds, closing the gap between accurate upper bounds and equally sharp rigorous lower bounds.

Abstract: Obtaining high-precision guaranteed lower eigenvalue bounds remains difficult, even though the standard high-order conforming finite element (FEM) easily yields extremely sharp upper bounds. Recently developed rigorous approaches using such as Crouzeix--Raviart or linear conforming elements do not extend well to high-order FEM. Some non-standard FEM approaches can provide sharp eigenvalue bounds but are technically involved. This persistent gap between accurate upper bounds and equally sharp rigorous lower bounds via standard high-order conforming FEMs makes the problem technically demanding and highly competitive. In this paper, we propose a new two-stage rigorous algorithm that closes this gap by employing high-order FEM on graded meshes and producing rigorous lower eigenvalue bounds as sharp as the corresponding high-order upper bounds, as demonstrated in our numerical examples. Numerical experiments for the Laplacian and Steklov eigenvalue problems on square and dumbbell domains show the accuracy and efficiency of the method, particularly on graded or highly nonuniform meshes. These results confirm that the proposed approach provides a practical and competitive solution to the long-standing difficulty of obtaining sharp, reliable lower eigenvalue bounds.

</details>


### [6] [Frenet Immersed Finite Element Spaces on Triangular Meshes](https://arxiv.org/abs/2512.23238)
*Tao Lin,Yuanhui Lin,Xu Zhang*

Main category: math.NA

TL;DR: Develop geometry-conforming immersed finite element spaces on triangular meshes for elliptic interface problems using Frenet-Serret mapping to transform interface curves to straight lines, enabling exact imposition of jump conditions.


<details>
  <summary>Details</summary>
Motivation: Extend existing immersed finite element framework from rectangular to triangular meshes for elliptic interface problems, enabling more flexible mesh generation and better handling of complex geometries.

Method: Construct high-degree Frenet-IFE spaces using three approaches: initial construction with monomial bases, generalized construction with orthogonal polynomials, and reconstruction methods to improve mass matrix conditioning. Incorporate these spaces into interior penalty discontinuous Galerkin methods.

Result: Demonstrated optimal approximation capability of proposed IFE spaces. Achieved optimal convergence rates in H¹ and L² norms when used in interior penalty discontinuous Galerkin methods for elliptic interface problems.

Conclusion: Successfully developed geometry-conforming IFE spaces on triangular meshes with exact imposition of interface jump conditions, extending previous rectangular mesh methods and achieving optimal convergence in numerical simulations.

Abstract: In this paper, we develop geometry-conforming immersed finite element (GC-IFE) spaces on triangular meshes for elliptic interface problems. These IFE spaces are constructed via a Frenet-Serret mapping that transforms the interface curve into a straight line, allowing the interface jump conditions to be imposed exactly. Extending the framework of [7,8] from rectangular to triangular meshes, we introduce three procedures for constructing high-degree Frenet-IFE spaces: an initial construction based on monomial bases, a generalized construction using orthogonal polynomials, and reconstruction methods aimed at improving the conditioning of the associated mass matrix. The optimal approximation capability of the proposed IFE spaces is demonstrated through numerical examples. We further incorporate these spaces into interior penalty discontinuous Galerkin methods for elliptic interface problems and observe optimal convergence rates in the $H^1$ and $L^2$ norms.

</details>


### [7] [$L^2$ and $L^\infty$ rational approximation](https://arxiv.org/abs/2512.23357)
*Michael S. Ackermann,Sean Reiter,Lloyd N. Trefethen*

Main category: math.NA

TL;DR: Computational comparison of L² and L∞ rational approximations for analytic functions on the unit disk using new barycentric TF-IRKA formulation for L² approximations.


<details>
  <summary>Details</summary>
Motivation: While theoretical foundations for L² and L∞ rational approximations on the unit disk exist for decades, there has been no computational study comparing these approaches in practice.

Method: Develops new barycentric formulation of TF-IRKA (Transfer Function Iterative Rational Krylov Algorithm) to compute L² best approximations, and compares results with L∞ approximations.

Result: First computational study comparing L² and L∞ rational approximations for analytic functions on the unit disk, providing practical insights beyond existing theory.

Conclusion: The paper presents the first computational comparison of L² and L∞ rational approximations, introducing a novel barycentric TF-IRKA formulation that enables practical computation of L² approximations.

Abstract: Using recently developed algorithms, we compute and compare best $L^2$ and $L^\infty$ rational approximations of analytic functions on the unit disk. Although there is some theory for these problems going back decades, this may be the first computational study. To compute the $L^2$ best approximations, we employ a new formulation of TF-IRKA in barycentric form.

</details>


### [8] [A Data-Driven Approach to Solving First-Kind Fredholm Integral Equations and Their Convergence Analysis](https://arxiv.org/abs/2512.23362)
*Duan-Peng Ling,Wenlong Zhang*

Main category: math.NA

TL;DR: Statistical recovery of Fredholm integral equations from noisy scattered measurements with optimal error bounds and practical regularization parameter selection.


<details>
  <summary>Details</summary>
Motivation: Need to solve ill-posed first-kind Fredholm integral equations from discrete, scattered, and noisy pointwise measurements, which is challenging due to ill-posedness and measurement noise.

Method: Assume forward operator's range in Sobolev space of order m (implying algebraic singular-value decay s_j ≤ Cj^{-m}). Derive optimal reconstruction error bounds in weak topology with a priori regularization parameter choice. Establish mean-square error rates for bounded-variance noise and exponential concentration bounds for sub-Gaussian noise. Provide explicit a priori and a posteriori rules for regularization parameter selection.

Result: Optimal upper bounds for reconstruction error in weak topology. Explicit mean-square error rates depending on sample size n, noise level σ, and smoothness m. Exponential concentration bounds for sub-Gaussian noise. Validated numerical experiments showing efficiency of practical parameter choice.

Conclusion: Theoretical framework provides optimal statistical recovery guarantees for ill-posed integral equations with practical regularization parameter selection rules that work well in numerical experiments.

Abstract: We investigate the statistical recovery of solutions to first-kind Fredholm integral equations with discrete, scattered, and noisy pointwise measurements. Assuming the forward operator's range belongs to the Sobolev space of order $m$, which implies algebraic singular-value decay $s_j\le Cj^{-m}$, we derive optimal upper bounds for the reconstruction error in the weak topology under an a priori choice of the regularization parameter. For bounded-variance noise, we establish mean-square error rates that explicitly quantify the dependence on sample size $n$, noise level $σ$, and smoothness index $m$; under sub-Gaussian noise, we strengthen these to exponential concentration bounds. The analysis yields an explicit a priori and a posteriori rule for the regularization parameter. Numerical experiments validate the theoretical results and demonstrate the efficiency of our practical parameter choice.

</details>


### [9] [High-order implicit Runge-Kutta time integrators for component-based model reduction of FSI problems](https://arxiv.org/abs/2512.23363)
*Tommaso Taddei,Xuejun Xu,Lei Zhang*

Main category: math.NA

TL;DR: A model order reduction framework for incompressible FSI problems using high-order implicit Runge-Kutta methods with separate reduced spaces, supremizer enrichment, and bubble-port decomposition for stable long-time integration.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and stable reduced-order model for strongly-coupled parametric fluid-structure interaction problems that can handle long-time integration while preserving stability properties from the full-order model.

Method: Uses high-order implicit Runge-Kutta (IRK) methods with separate reduced spaces for fluid velocity, pressure, and solid displacement. Enriches velocity space with supremizer modes for inf-sup stability, employs bubble-port decomposition to satisfy interface kinematic conditions, uses Galerkin projection for semi-discrete ROM, and Radau-IIA IRK for time integration with static condensation of interface DOFs.

Result: The ROM preserves semi-discrete energy balance from the full-order model, avoids need for additional interface enrichment, and numerical experiments show stable and accurate performance for long-time integration of strongly-coupled parametric FSI problems.

Conclusion: The combination of high-order IRK schemes with bubble-port decoupling yields effective reduced-order modeling for FSI problems, providing stability and accuracy for long-time simulations while maintaining computational efficiency through model reduction.

Abstract: We propose a model order reduction framework for incompressible fluid-structure interaction (FSI) problems based on high-order implicit Runge-Kutta (IRK) methods. We consider separate reduced spaces for fluid velocity, fluid pressure and solid displacement; we enrich the velocity space with supremizer modes to ensure the inf-sup stability of the fluid subproblem; we consider bubble-port decomposition of fluid velocity and solid displacement to satisfy the kinematic conditions at the fluid structure interface. We resort to Galerkin projection to define the semi-discrete reduced-order model and we consider a Radau-IIA IRK method for time integration: the resulting algebraic system is solved using static condensation of the interface degrees of freedom. The reduced-order model preserves a semi-discrete energy balance inherited from the full-order model, and avoids the need for additional interface enrichment. Numerical experiments demonstrate that the proposed combination of high-order IRK schemes with bubble-port decoupling of velocity and displacement degrees of freedom yields stable and accurate reduced-order model for long-time integration of strongly-coupled parametric FSI problems.

</details>


### [10] [Sensitivity Analysis on the Sphere and a Spherical ANOVA Decomposition](https://arxiv.org/abs/2512.23476)
*Laura Weidensager*

Main category: math.NA

TL;DR: The paper presents a sensitivity analysis method for functions on spheres using a novel decomposition that incorporates both variable subsets and parity properties.


<details>
  <summary>Details</summary>
Motivation: To develop sensitivity analysis techniques specifically for functions defined on spheres, addressing the limitations of classical ANOVA decomposition which doesn't account for the natural geometry and dependencies between variables on spherical domains.

Method: Decomposes functions on spheres into terms f_{u,ξ} where u is a subset of variables and ξ represents parity properties. Uses orthogonal basis functions for approximation, enabling modeling of high-dimensional functions with low-dimensional interactions.

Result: Establishes formulas for sphere sensitivity analysis that incorporate both variable interactions and parity decomposition, providing a framework for analyzing functions on spherical domains with inherent geometric dependencies.

Conclusion: The proposed method extends sensitivity analysis to spherical domains by combining variable subset decomposition with parity analysis, offering a more appropriate framework for functions defined on spheres compared to classical ANOVA.

Abstract: We establish sensitivity analysis on the sphere. We present formulas that allow us to decompose a function $f\colon \mathbb S^d\rightarrow \mathbb R$ into a sum of terms $f_{\boldsymbol u,\boldsymbol ξ}$. The index $\boldsymbol u$ is a subset of $\{1,2,\ldots,d+1\}$, where each term $f_{\boldsymbol u,\boldsymbol ξ}$ depends only on the variables with indices in $\boldsymbol u$. In contrast to the classical analysis of variance (ANOVA) decomposition, we additionally use the decomposition of a function into functions with different parity, which adds the additional parameter $\boldsymbol ξ$. The natural geometry on the sphere naturally leads to the dependencies between the input variables. Using certain orthogonal basis functions for the function approximation, we are able to model high-dimensional functions with low-dimensional variable interactions.

</details>


### [11] [Error Estimates for Gauss--Christoffel Quadrature under Reduced Regularity Conditions](https://arxiv.org/abs/2512.23540)
*Mehdi Hamzehnejad,Abbas Salemi*

Main category: math.NA

TL;DR: New error bound for Gauss-Christoffel quadrature with weaker regularity assumptions than classical weighted bounded variation conditions, using improved Chebyshev coefficient decay estimates.


<details>
  <summary>Details</summary>
Motivation: Classical error estimates for Gauss-Christoffel quadrature rely on restrictive weighted bounded variation assumptions with singular weight (1-x²)^{-1/2}, which may be too strong for functions with limited regularity at endpoints. Need more general error bounds under weaker conditions.

Method: Develops new identity for higher-order derivatives of Chebyshev polynomials to establish improved decay estimates for Chebyshev coefficients. Replaces classical weighted condition V_r = ∫|f^{(r+1)}(x)|/√(1-x²) dx with weaker condition U_r = ∫|f^{(r+1)}(x)| dx. Extends approach to Gauss-Gegenbauer case.

Result: Obtains new error bound for Gauss-Christoffel quadrature that is less restrictive than previous bounds. Provides improved decay estimate for Chebyshev coefficients under weaker regularity assumptions. Numerical experiments validate theoretical results.

Conclusion: Establishes more general convergence analysis for Gauss-Christoffel quadrature by weakening classical regularity requirements, providing practical error estimates for broader function classes including those with limited endpoint regularity.

Abstract: Gauss--Christoffel quadrature is a fundamental method for numerical integration, and its convergence analysis is closely related to the decay of Chebyshev expansion coefficients. Classical estimates, including those due to Trefethen, are based on weighted bounded variation assumptions involving the singular weight $(1-x^{2})^{-1/2}$, which may be too restrictive for functions with limited regularity at the endpoints.
  In this paper, we establish a new error bound for Gauss--Christoffel quadrature under weakened regularity assumptions. The analysis relies on a new identity for higher-order derivatives of Chebyshev polynomials. As a consequence, we obtain an improved decay estimate for Chebyshev coefficients, where the classical weighted condition \[ V_{r}=\int_{-1}^{1}\frac{|f^{(r+1)}(x)|}{\sqrt{1-x^{2}}}\,dx \] is replaced by the weaker condition \[ U_{r}=\int_{-1}^{1}|f^{(r+1)}(x)|\,dx. \]
  This result leads to a corresponding error estimate for the Gauss--Christoffel quadrature rule, which is less restrictive than previous bounds. The approach is also extended to the Gauss--Gegenbauer case. Numerical experiments are provided to illustrate the theoretical results.

</details>


### [12] [Analysis of kinetic-diffusion Monte Carlo simulation and source term estimation scheme in nuclear fusion applications](https://arxiv.org/abs/2512.23580)
*Zhirui Tang,Julian Koellermeier,Emil Løvbak,Giovanni Samaey*

Main category: math.NA

TL;DR: KDMC with fluid estimation accelerates plasma edge simulations by reducing computational cost while maintaining accuracy compared to kinetic MC methods.


<details>
  <summary>Details</summary>
Motivation: Monte Carlo methods for solving Boltzmann-BGK equations in plasma edge simulations become computationally expensive for large reactors like ITER and DEMO due to high collision rates, requiring acceleration techniques.

Method: Combines asymptotic-preserving kinetic-diffusion Monte Carlo (KDMC) simulation with fluid estimation technique, analyzes convergence through theoretical upper bounds and numerical verification, and compares with purely fluid-based methods.

Result: KDMC with fluid estimation achieves lower error than fluid-based methods (up to one order of magnitude in fusion-relevant cases) and significant speedup compared to reference kinetic MC methods.

Conclusion: The analysis confirms the effectiveness of KDMC with associated fluid estimation for nuclear fusion applications, providing both computational acceleration and maintained accuracy.

Abstract: In plasma edge simulations, the behavior of neutral particles is often described by a Boltzmann--BGK equation. Solving this kinetic equation and estimating the moments of its solution are essential tasks, typically carried out using Monte Carlo (MC) methods. However, for large-sized reactors, like ITER and DEMO, high collision rates lead to a substantial computational cost. To accelerate the calculation, an asymptotic-preserving kinetic-diffusion Monte Carlo (KDMC) simulation method (Mortier et al., SIAM J. Sci. Comput., 2022) and a corresponding fluid estimation technique (Mortier et al., Contrib. Plasma Phys., 2022) have recently been proposed. In this work, we present a comprehensive analysis of the convergence of KDMC combined with the associated fluid estimation. The analysis consists of proving theoretical upper bounds for both KDMC and the fluid estimation, and numerical verifications of these bounds. In addition, we compare the analyzed algorithm with a purely fluid-based method using the fully kinetic MC method as a reference. The algorithm consistently achieves lower error than the fluid-based method, and even one order of magnitude lower in a fusion-relevant test case. Moreover, the algorithm exhibits a significant speedup compared to the reference kinetic MC method. Overall, our analysis confirms the effectiveness of KDMC with the associated fluid estimation in nuclear fusion applications.

</details>


### [13] [Learning Lévy density via adaptive RKHS regression with bi-level optimization](https://arxiv.org/abs/2512.23621)
*Luxuan Yang,Fei Lu,Ting Gao,Wei Wei,Jinqiao Duan*

Main category: math.NA

TL;DR: Nonparametric method learns Lévy density from probability density data via adaptive RKHS regularization and bilevel optimization, outperforming classical regularization methods.


<details>
  <summary>Details</summary>
Motivation: Learning Lévy densities from probability density data governed by nonlocal Fokker-Planck equations is an ill-posed inverse problem requiring effective regularization strategies.

Method: Recast as kernel identification in nonlocal integral operator; construct adaptive RKHS with data-driven kernel; use GSVD-based bilevel optimization for regularization parameter selection.

Result: Reconstruction error decays at near optimal rate; bilevel RKHS method outperforms L-curve and generalized cross-validation; adaptive RKHS norm is more accurate and robust than L²ρ- and ℓ²-based regularization.

Conclusion: The proposed adaptive RKHS framework with bilevel optimization provides an effective, robust solution for learning Lévy densities from probability density data, with theoretical guarantees and superior practical performance.

Abstract: We propose a nonparametric method to learn the Lévy density from probability density data governed by a nonlocal Fokker-Planck equation. We recast the problem as identifying the kernel in a nonlocal integral operator from discrete data, which leads to an ill-posed inverse problem. To regularize it, we construct an adaptive reproducing kernel Hilbert space (RKHS) whose kernel is built directly from the data. Under standard source and spectral decay conditions, we show that the reconstruction error decays in the mesh size at a near optimal rate. Importantly, we develop a generalized singular value decomposition (GSVD)-based bilevel optimization algorithm to choose the regularization parameter, leading to efficient and robust computation of the regularized estimator. Numerical experiments for several Lévy densities, drift fields and data types (PDE-based densities and sample ensemble-based KDE reconstructions) demonstrate that our bilevel RKHS method outperforms classical L-curve and generalized cross-validation strategies and that the adaptive RKHS norm is more accurate and robust than $L^2_ρ$- and $\ell^2$-based regularization.

</details>


### [14] [Simultaneous Approximation of the Score Function and Its Derivatives by Deep Neural Networks](https://arxiv.org/abs/2512.23643)
*Konstantin Yakovlev,Nikita Puchkin*

Main category: math.NA

TL;DR: Novel theory for approximating score functions and their derivatives with error bounds that avoid the curse of dimensionality and handle unbounded support distributions.


<details>
  <summary>Details</summary>
Motivation: Existing methods for score function approximation typically require bounded data support and suffer from the curse of dimensionality. The paper aims to overcome these limitations by developing a theory that works with unbounded support distributions and low-dimensional structures.

Method: Develops a theoretical framework for simultaneous approximation of score functions and their derivatives. The approach relaxes the usual bounded support requirement and provides error bounds that are free from the curse of dimensionality.

Result: Achieves approximation error bounds matching literature results while using weaker assumptions (unbounded support). The bounds avoid the curse of dimensionality and extend to derivatives of any prescribed order, not just first-order.

Conclusion: The theory enables more flexible and practical score function approximation for real-world data distributions with low-dimensional structure and unbounded support, with guarantees for higher-order derivatives.

Abstract: We present a theory for simultaneous approximation of the score function and its derivatives, enabling the handling of data distributions with low-dimensional structure and unbounded support. Our approximation error bounds match those in the literature while relying on assumptions that relax the usual bounded support requirement. Crucially, our bounds are free from the curse of dimensionality. Moreover, we establish approximation guarantees for derivatives of any prescribed order, extending beyond the commonly considered first-order setting.

</details>


### [15] [A High-Order Spectral Element Solver for Steady-State Free Surface Flows](https://arxiv.org/abs/2512.23648)
*Simone Minniti,Jens Visbech,Claes Eskilsson,Nicola Parolini,Allan Peter Engsig-Karup*

Main category: math.NA

TL;DR: Spectral element solver for free-surface Navier-Stokes using pseudo-time iteration to determine unknown free surface profile, implemented in Firedrake with high-order accuracy and curvilinear elements.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient high-order numerical method for solving steady incompressible Navier-Stokes equations with free surfaces, which is challenging due to the a priori unknown free surface profile that must be determined as part of the solution.

Method: Spectral element solver implemented in Firedrake framework using high-order polynomial basis on unstructured meshes. Uses iterative pseudo-time procedure to determine free surface profile based on kinematic boundary conditions. Incorporates curvature through curvilinear elements via transfinite linear blending to maintain high-order convergence.

Result: Successfully applied to 2D benchmark cases including lid-driven cavity, flow around cylinder and NACA airfoil (fixed domains), and flow around bathymetry bump and submerged NACA airfoil (free surface). Demonstrated high-order accuracy through convergence studies and showed substantial speed-up over low-order schemes.

Conclusion: The developed spectral element solver effectively handles free-surface Navier-Stokes problems with high-order accuracy and computational efficiency, validated through various benchmark cases in both fixed and free-surface domains.

Abstract: We present a spectral element solver for the steady incompressible Navier-Stokes equations subject to a free surface. Utilizing the kinematic behaviour of the free surface boundary, an iterative pseudo-time procedure is proposed to determine the a priori unknown free surface profile. The numerical model is implemented in the open-source finite element framework Firedrake, which enables the use of a high-order polynomial basis on unstructured meshes through weak formulations. Additionally, the curvature of the free surface and submerged bodies is incorporated through curvilinear elements obtained via transfinite linear blending, which conserves the high-order convergent properties of the overall scheme. The model is applied to several benchmark cases in two spatial dimensions. Initially, it addresses fixed-domain problems, including the lid-driven cavity flow and flows around bodies such as a cylinder and a NACA airfoil. Subsequently, with the presence of a free surface, it is extended to determine the flow around a bathymetry bump and a submerged NACA airfoil. The results confirm the high-order accuracy of the model through convergence studies and demonstrate a substantial speed-up over low-order numerical schemes.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [16] [Time Reparametrization, Not Fractional Calculus: A Reassessment of the Conformable Derivative](https://arxiv.org/abs/2512.22366)
*Aziz El Ghazouani,Fouad Ibrahim Abdou Amir,Khoulane Mohamed,M'hamed Elomari*

Main category: math.AP

TL;DR: The conformable derivative is not a true fractional operator but a classical derivative under nonlinear time reparametrization, lacking genuine nonlocal fractional effects.


<details>
  <summary>Details</summary>
Motivation: To critically reassess claims that the conformable derivative is a new fractional derivative operator and clarify its true mathematical nature.

Method: Theoretical analysis showing conformable derivative is equivalent to classical differentiation under nonlinear time scaling; transformation of problems to classical formulations via change of variable; numerical simulations comparing conformable, classical, and Caputo fractional models.

Result: Conformable derivative is not a fractional operator but a computational tool for power-law time scaling; many "novel fractional" results can be reinterpreted classically; no genuine nonlocal effects present; classical and established fractional derivatives better model memory-dependent phenomena.

Conclusion: The conformable derivative misconception persists but it's not a true fractional operator; classical derivatives and established fractional derivatives (like Caputo) provide more faithful frameworks for modeling memory-dependent phenomena.

Abstract: The conformable derivative has been promoted in numerous publications as a new fractional derivative operator. This article provides a critical reassessment of this claim. We demonstrate that the conformable derivative is not a fractional operator but a useful computational tool for systems with power-law time scaling, equivalent to classical differentiation under a nonlinear time reparametrization. Several results presented in the literature as novel fractional contributions can be reinterpreted within a classical framework. We show that problems formulated using the conformable derivative can be transformed into classical formulations via a change of variable. The solution is derived classically and then transformed back, this reformulation highlights the absence of genuinely nonlocal fractional effects. We provide a theoretical analysis, numerical simulations comparing conformable, classical, and truly fractional (Caputo) models, and discuss the reasons why this misconception persists. Our results suggest that classical derivatives, as well as established fractional derivatives, offer a more faithful framework for modeling memory-dependent phenomena.

</details>


### [17] [Regularity of solutions of the Navier-Stokes-αβ equations with wall-eddy boundary conditions](https://arxiv.org/abs/2512.22436)
*Nella Rotundo,Gantumur Tsogtgerel*

Main category: math.AP

TL;DR: First complete analytical proof of global well-posedness and regularity for Navier-Stokes-αβ system with wall-eddy boundary conditions modeling near-wall turbulence.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous mathematical analysis for the wall-eddy boundary conditions proposed by Fried and Gurtin (2008), which model near-wall turbulence through tangential vorticity traction proportional to wall vorticity, addressing a gap in analytical treatment of this continuum-mechanical turbulence model.

Method: 1. Variational formulation of stationary fourth-order system with proof of symmetry and Gårding inequality for bilinear form. 2. Verification of Douglis-Nirenberg ellipticity and Lopatinskii-Shapiro covering condition for full Agmon-Douglis-Nirenberg regularity. 3. Derivation of hierarchy of energy estimates for nonlinear evolution equation.

Result: Established global well-posedness, regularity, uniqueness, and stability for Navier-Stokes-αβ system with wall-eddy boundary conditions. This represents the first complete analytical treatment of Fried and Gurtin's wall-eddy boundary model.

Conclusion: Successfully provided rigorous mathematical foundation for the wall-eddy boundary model of near-wall turbulence, demonstrating its well-posedness and regularity properties through systematic analysis of variational formulation, ellipticity conditions, and energy estimates.

Abstract: We establish global well-posedness and regularity for the Navier-Stokes-αβ system endowed with the wall-eddy boundary conditions proposed by Fried and Gurtin (2008). These conditions introduce a tangential vorticity traction proportional to wall vorticity and provide a continuum-mechanical model for near-wall turbulence. Our analysis begins with a variational formulation of the stationary fourth-order system, where we prove symmetry and a Gårding inequality for the associated bilinear form. We then verify Douglis-Nirenberg ellipticity and the Lopatinskii-Shapiro covering condition, establishing full Agmon-Douglis-Nirenberg regularity for the coupled system. Building on this framework, we derive a hierarchy of energy estimates for the nonlinear evolution equation, which yields global regularity, uniqueness, and stability. To our knowledge, this provides the first complete analytical treatment of the wall-eddy boundary model of Fried and Gurtin.

</details>


### [18] [CR Yamabe Equation on the Heisenberg Group via the method of moving spheres](https://arxiv.org/abs/2512.22458)
*Congwen Liu*

Main category: math.AP

TL;DR: All positive solutions to the CR Yamabe equation on the Heisenberg group are Jerison-Lee bubbles, without requiring finite-energy or symmetry assumptions.


<details>
  <summary>Details</summary>
Motivation: To establish a complete classification of positive solutions to the CR Yamabe equation on the Heisenberg group, analogous to the celebrated Caffarelli-Gidas-Spruck theorem in Euclidean space.

Method: Developed a systematic approach to implement the method of moving spheres in the setting of the Heisenberg group.

Result: Proved that all positive solutions to the CR Yamabe equation on the Heisenberg group are Jerison-Lee bubbles, without imposing any finite-energy or a priori symmetry assumptions.

Conclusion: This provides a complete classification theorem for the Heisenberg group analogous to the Caffarelli-Gidas-Spruck theorem in Euclidean space, achieved through adaptation of the moving spheres method to the Heisenberg group setting.

Abstract: In this paper, we classify positive solutions to the CR Yamabe equation on the Heisenberg group $\mathbb{H}^n$. We show that all such solutions are Jerison-Lee bubbles, without imposing any finite-energy or a priori symmetry assumptions. This result can be regarded as an analogue for $\mathbb{H}^n$ of the celebrated Caffarelli-Gidas-Spruck classification theorem in $\mathbb{R}^n$. To establish this, we develop a systematic approach to implement the method of moving spheres in the setting of the Heisenberg group.

</details>


### [19] [Finite propagation and saturation in reaction-diffusion-advection equations governed by p-Laplacian operator](https://arxiv.org/abs/2512.22493)
*Cristina Marcelli*

Main category: math.AP

TL;DR: The paper analyzes front propagation for a mono-stable reaction-diffusion-advection equation with non-standard diffusion, focusing on existence/non-existence and classification of traveling wave solutions.


<details>
  <summary>Details</summary>
Motivation: To understand front propagation phenomena in reaction-diffusion-advection equations with p-Laplacian type diffusion, which arise in various physical and biological applications where standard diffusion models are insufficient.

Method: Mathematical analysis of traveling wave solutions for the given PDE, including existence/non-existence proofs and classification criteria based on boundary behavior and regularity.

Result: Provides criteria to determine whether traveling wave solutions attain equilibria at finite time, and if so, whether they are continuable as C¹-solutions or sharp solutions.

Conclusion: The paper establishes a comprehensive classification framework for traveling wave solutions in mono-stable reaction-diffusion-advection equations with non-standard diffusion, characterizing their boundary behavior and regularity properties.

Abstract: The paper concerns front propagation for the following mono-stable reaction-diffusion-advection equation \[f(u)u_x + g(u)u_τ= [d(u)|u_x|^{p-2} u_x]_x+ ρ(u), \quad (x,τ)\in \R\times [0,+\infty).\] Besides existence and non-existence results for traveling wave solutions, the main focus is their classification: we provide criteria to establish if they attain one or both the equilibria at a finite time and in this case, if they are continuable as $C^1$-solutions or if they are sharp solutions.

</details>


### [20] [Wave dynamics governing vortex breakdown in smooth Euler flows](https://arxiv.org/abs/2512.22543)
*Tsuyoshi Yoneda*

Main category: math.AP

TL;DR: Vortex breakdown in 3D Euler equations is governed by wave dynamics from underlying transport flow, using Lagrangian coordinates to eliminate pressure singularities.


<details>
  <summary>Details</summary>
Motivation: To understand vortex breakdown in 3D incompressible Euler equations when a vortex is transported by a prescribed Lagrangian flow, avoiding singular integral representations of pressure.

Method: Construct an effective Lagrangian coordinate system where the associated Lie bracket vanishes identically, avoiding singular integral representation of pressure term.

Result: Shows that vortex breakdown is governed by wave dynamics generated by the underlying transport flow.

Conclusion: Vortex breakdown in 3D Euler equations can be analyzed through wave dynamics using Lagrangian coordinates that eliminate pressure singularities.

Abstract: We consider the three-dimensional incompressible Euler equations in a setting where a vortex is transported by a prescribed Lagrangian flow. We show that vortex breakdown is governed by wave dynamics generated by the underlying transport flow. The key idea is to avoid any singular integral representation of the pressure term and instead construct an effective Lagrangian coordinate system in which the associated Lie bracket vanishes identically.

</details>


### [21] [The fibre operators in the Bloch-Floquet decomposition of periodic magnetic pseudo-differential operators](https://arxiv.org/abs/2512.22547)
*Horia D. Cornean,Bernard Helffer,Radu Purice*

Main category: math.AP

TL;DR: The paper studies fiber operators for periodic magnetic pseudo-differential operators, deriving explicit kernel formulas and proving they are toroidal pseudo-differential operators.


<details>
  <summary>Details</summary>
Motivation: To understand the structure of fiber operators corresponding to periodic magnetic pseudo-differential operators with periodic magnetic potentials, which is important for analyzing their spectral properties and mathematical structure.

Method: The authors obtain explicit formulas for the distribution kernel of fiber operators in two representations: as operators on the d-dimensional torus, and as infinite matrices acting on discrete ℓ² space via discrete Fourier transform.

Result: The paper provides explicit formulas for the distribution kernels of fiber operators in both representations, and uses these kernels to prove that the fiber operators are toroidal pseudo-differential operators.

Conclusion: The fiber operators corresponding to periodic magnetic pseudo-differential operators with periodic magnetic potentials have explicit distribution kernels and are themselves toroidal pseudo-differential operators.

Abstract: We study the structure of the fibre operators corresponding to periodic magnetic pseudo-differential operators having periodic magnetic potentials. We obtain explicit formulas for their distribution kernel, both when these fibres are seen as operators on the $d$-dimensional torus, and also when they are seen as infinite matrices acting on a discrete $\ell^2$ space via a discrete Fourier transform. Moreover, using these distribution kernels we prove that the fibre operators are toroidal pseudo-differential operators.

</details>


### [22] [On a Thermodynamically Consistent Diffuse-Interface Model for Incompressible Two-Phase Flows with Chemotaxis and Mass Transport](https://arxiv.org/abs/2512.22585)
*Andrea Giorgini,Jingning He,Hao Wu*

Main category: math.AP

TL;DR: Global existence of weak and strong solutions for a Navier-Stokes/Cahn-Hilliard model of two-phase flows with unmatched densities and chemotaxis effects, showing boundedness of chemical concentration prevents blow-up unlike Keller-Segel systems.


<details>
  <summary>Details</summary>
Motivation: To study a thermodynamically consistent diffuse-interface model for two-phase flows with unmatched densities coupled with soluble chemical species, incorporating chemotaxis effects and mass transport processes derived from Onsager's variational principle.

Method: 1) Establish global finite energy and weak solutions in 2D using approximation schemes and compactness methods. 2) Prove existence/uniqueness of global strong solutions via analysis of three decoupled subsystems and bootstrap arguments. 3) Show propagation of regularity for weak solutions.

Result: 1) Global existence of finite energy and weak solutions in 2D. 2) Existence/uniqueness of global strong solutions for regular initial data. 3) Chemical density stays bounded if initial data is bounded, preventing concentration singularities unlike Keller-Segel systems.

Conclusion: The model's diffusion driven by chemical potential gradient prevents blow-up phenomena observed in classical Keller-Segel systems, establishing well-posedness and regularity properties for this complex two-phase flow with chemotaxis.

Abstract: We investigate a hydrodynamic system of Navier--Stokes/Cahn--Hilliard type, which describes the motion of a two-phase flow of two incompressible fluids with unmatched densities coupled with a soluble chemical species. Derived from Onsager's variational principle, this thermodynamically consistent diffuse-interface model incorporates both the chemotaxis effects induced by the chemical species and the mass transport processes within the mixture. For the two-dimensional initial-boundary value problem, we establish the existence of global finite energy solutions and global weak solutions, using a suitable approximation scheme combined with compactness methods. Next, by carefully analyzing three decoupled subsystems and employing a bootstrap argument, we prove the existence and uniqueness of a global strong solution for sufficiently regular initial data, as well as the propagation of regularity for global weak solutions. In particular, we show that the density of the chemical substance stays bounded for all time if its initial datum is bounded. This implies a significant distinction from the classical Keller--Segel system: diffusion driven by the chemical potential gradient can prevent the formation of concentration singularities.

</details>


### [23] [Dispersive estimates for discrete Klein-Gordon equations on one-dimensional lattice with quasi-periodic potentials](https://arxiv.org/abs/2512.22613)
*Zhiqiang Wan,Heng Zhang*

Main category: math.AP

TL;DR: Proves ℓ¹→ℓ∞ dispersive estimates for discrete Klein-Gordon equation with small quasi-periodic potentials, showing time-decay rate persists as (1/3)⁻. Applications include Strichartz estimates and small-data global well-posedness for nonlinear version.


<details>
  <summary>Details</summary>
Motivation: To understand how dispersive properties (time-decay rates) of the discrete Klein-Gordon equation are affected by the presence of quasi-periodic potentials, particularly whether the optimal decay rate can be preserved.

Method: Mathematical analysis proving ℓ¹→ℓ∞ dispersive estimates for the discrete Klein-Gordon equation on ℤ with small real-analytic quasi-periodic potentials, showing persistence of the (1/3)⁻ time-decay rate.

Result: Successfully proves that the optimal time-decay rate (1/3)⁻ persists for the discrete Klein-Gordon equation with small quasi-periodic potentials, and derives corresponding Strichartz estimates and establishes small-data global well-posedness for the nonlinear equation.

Conclusion: Small quasi-periodic potentials do not destroy the dispersive properties of the discrete Klein-Gordon equation, allowing preservation of optimal decay rates and enabling well-posedness results for nonlinear versions.

Abstract: We prove $\ell^{1}\!\to\!\ell^{\infty}$ dispersive estimates for the discrete Klein--Gordon equation on $\mathbb Z$ with small real-analytic quasi-periodic potentials, showing that the time-decay rate persists as $(\tfrac13)^{-}$. As applications, we derive the corresponding Strichartz estimates and establish small-data global well-posedness for the associated nonlinear discrete Klein--Gordon equation.

</details>


### [24] [Ground states of the Schrödinger equation coupled with fourth-order gravitation -- Part 1: the case $K_{a, b} \leq 0$](https://arxiv.org/abs/2512.22619)
*Gustavo de Paula Ramos*

Main category: math.AP

TL;DR: This paper studies ground states of a normalized nonlocal semilinear problem with singular potential and a family of fourth-order gravity kernels, providing existence/nonexistence results and asymptotic convergence to known equations.


<details>
  <summary>Details</summary>
Motivation: The problem arises from seeking standing waves of the Schrödinger equation coupled with nonrelativistic gravitational potential in fourth-order gravity theories. The kernel K_{a,b} represents different gravitational interaction models depending on parameters a and b.

Method: The authors analyze the normalized nonlocal semilinear problem using variational methods and asymptotic analysis. They study the associated autonomous problem, then extend to nonautonomous case with singular potential V, and examine limiting behaviors as parameters approach boundary values.

Result: (i) Complete existence/nonexistence picture for ground states of autonomous problem across all geometries of K_{a,b}; (ii) Existence conditions for ground states in nonautonomous case when K_{a,b} ≤ 0; (iii) Convergence results showing ground states approach solutions of Schrödinger equation, Choquard equation, and rescaled Choquard equation in different parameter limits.

Conclusion: The paper provides comprehensive understanding of ground state existence and behavior for this family of nonlocal problems, connecting them to classical equations through parameter limits and establishing rigorous convergence results.

Abstract: We are interested in the existence and asymptotic behavior of ground states of the following normalized nonlocal semilinear problem: \[ \begin{cases} - Δu + (V - ω) u + (K_{a, b} \ast u^2) u = 0 &\text{in} ~ \mathbb{R}^3; \\ \|u\|_{\mathscr{L}^2}^2 = μ, \end{cases} \] where \[ K_{a, b} (x) := \frac{1}{|x|} \left( \frac{4}{3} e^{- b |x|} - \frac{1}{3} e^{- a |x|} - 1 \right); \] $0 \leq a, b \leq \infty$; $V$ denotes a singular potential that vanishes at infinity and the unknowns are $ω\in \mathbb{R}$, $u \colon \mathbb{R}^3 \to \mathbb{R}$. This problem is obtained by looking for standing waves of the Schrödinger equation coupled with the nonrelativistic gravitational potential prescribed by a family of fourth-order gravity theories. In this paper, (i) we obtain a complete picture of the existence/nonexistence of ground states of the associated autonomous problem for every possible geometry of $K_{a, b}$, (ii) we obtain conditions that ensure the existence of ground states of the nonautonomous problem when $K_{a, b} \leq 0$ and (iii) we prove that as \[ (a, b) \to (A, B) \in \left\{(0, 0), (\infty, \infty), (0, \infty)\right\}, \] ground states of this problem respectively converge to a ground state of (1) the Schrödinger equation, (2) the Choquard equation and (3) a rescaling of the Choquard equation.

</details>


### [25] [Asymptotic behavior of a nonlinear shallow shell model when the shell becomes a plate](https://arxiv.org/abs/2512.22677)
*Trung Hieu Giang,Ngoc Quynh Nguyen*

Main category: math.AP

TL;DR: Analysis of asymptotic behavior of minimizing solutions for nonlinear shallow shell model under general applied forces, extending previous work with vanishing tangential forces.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of minimizing solutions for the Donnell-Vlasov-Mushtari-Galimov-Koiter nonlinear shallow shell model under more general loading conditions than previously studied.

Method: Mathematical analysis of the nonlinear shallow shell model, studying the asymptotic behavior of minimizing solutions through analytical methods and extending previous results.

Result: Established asymptotic behavior results for minimizing solutions that work with general applied forces, substantially extending previous work that only handled cases with vanishing tangential force components.

Conclusion: The analysis successfully extends the understanding of asymptotic behavior in nonlinear shallow shell models to more realistic loading conditions where tangential forces are present, providing a more comprehensive mathematical framework.

Abstract: This paper studies a nonlinear shallow shell model proposed by Donnell, Vlasov, Mushtari, Galimov, and Koiter. More specifically, we address the question concerning the asymptotic behavior of minimizing solutions. Our result can be applied to general applied forces. Thus, it substantially extends the one given in \cite{oana2} whereby the tangential components of the applied forces are assumed to vanish.

</details>


### [26] [Global Martingale Entropy Solutions to the Stochastic Isentropic Euler Equations](https://arxiv.org/abs/2512.22719)
*Gui-Qiang G. Chen,Feimin Huang,Danli Wang*

Main category: math.AP

TL;DR: The paper establishes existence and compactness of global martingale entropy solutions for stochastically forced isentropic Euler equations with general pressure law, developing a stochastic compensated compactness framework in L^p to handle lack of uniform L^∞ bounds.


<details>
  <summary>Details</summary>
Motivation: To study stochastically forced isentropic Euler equations with general pressure law, where stochastic forcing prevents uniform L^∞ bounds for approximate solutions, requiring new analytical tools.

Method: Develops a stochastic compensated compactness framework in L^p space; uses vanishing viscosity method with careful uniform estimates of stochastic approximate solutions; derives higher-order relative energy estimates.

Result: Proves existence of global martingale entropy solutions with finite relative-energy; establishes compactness of solutions; in polytropic pressure case, shows solutions satisfy local mechanical energy inequality without requiring higher moment estimates for entropy.

Conclusion: The stochastic compensated compactness framework and uniform estimate techniques developed are broadly applicable to similar stochastic PDE problems, providing new tools for handling stochastic forcing in conservation laws.

Abstract: We establish the existence and compactness of global martingale entropy solutions with finite relative-energy for the stochastically forced system of isentropic Euler equations governed by a general pressure law. To achieve these, a stochastic compensated compactness framework in $L^p$ is developed to overcome the difficulty that the uniform $L^{\infty}$ bound for the stochastic approximate solutions is unavailable, owing to the stochastic forcing term. The convergence of the vanishing viscosity method is established by employing the stochastic compactness framework, along with careful uniform estimates of the stochastic approximate solutions, to obtain the existence of global martingale entropy solutions with finite relative-energy. In particular, in the polytropic pressure case for all adiabatic exponents, we prove that the global solutions satisfy the local mechanical energy inequality when the initial data are only required to have finite relative-energy (while the higher moment estimates for entropy are not required here, as needed in the earlier work). Higher-order relative energy estimates for approximate solutions are also derived to establish the entropy inequality for more convex entropy pairs and to then prove the compactness of solutions to the stochastic isentropic Euler system. The stochastic compensated compactness framework and the uniform estimate techniques for approximate solutions developed in this paper should be useful in the study of other similar problems.

</details>


### [27] [Blowup rate for rotational NLS with a repulsive potential](https://arxiv.org/abs/2512.22821)
*Yi Hu,Yongki Lee,Shijun Zheng*

Main category: math.AP

TL;DR: Analytical proof of log-log blowup rate for mass-critical rotating NLS with repulsive harmonic potential, showing that increasing repulsive potential strength can prevent blowup and lead to global solutions.


<details>
  <summary>Details</summary>
Motivation: To understand blowup dynamics in mass-critical nonlinear Schrödinger equations with rotation and repulsive harmonic potential, particularly how repulsive potentials can prevent finite-time blowup even for initial data with mass above the ground state threshold.

Method: Uses virial identity and an R_γ-transform (pseudo-conformal transform adapted to this setting), along with numerical simulations using dynamic rescaling and adaptive mesh refinement methods.

Result: Proves log-log blowup rate for mass-critical NLS with rotation and repulsive potential, obtains limiting behavior description of mass concentration near blowup time, and shows that increasing repulsive potential strength can yield global solutions.

Conclusion: Repulsive harmonic potentials can suppress blowup in focusing rotating NLS, contrasting with attractive potentials, with increasing repulsive strength potentially leading to global existence even for supercritical mass initial data.

Abstract: In this paper we give an analytical proof of the ``$\log$-$\log$'' blowup rate for mass-critical nonlinear Schrödinger equation (NLS) with a rotation ($Ω\neq 0$) and a repulsive harmonic potential $V_γ(x) = \textrm{sgn}(γ) γ^2 |x|^2$, $γ< 0$ when the initial data has a mass slightly above that of $Q$, the ground state solution to the free NLS. The proof is based on a virial identity and an $\mathcal{R}_γ$-transform, a pseudo-conformal transform in this setting. Further, we obtain a limiting behavior description concerning the mass concentration near blowup time. A remarkable finding is that increasing the value $|γ|$ for the repulsive potential $V_γ$ can give rise to global in time solution for the focusing RNLS, which is in contrast to the case where $γ$ is positive. This kind of phenomenon was earlier observed in the non-rotational case $Ω= 0$ in Carles' work. In addition, we provide numerical simulations to partially illustrate the blowup profile along with the blowup rate using dynamic rescaling and adaptive mesh refinement method.

</details>


### [28] [Crystalline Motion of discrete interfaces in the Blume-Emery-Griffiths Model: partial wetting](https://arxiv.org/abs/2512.22870)
*Marco Cicalese,Giuliana Fusco,Giovanni Savaré*

Main category: math.AP

TL;DR: This paper extends variational analysis of lattice systems modeling two immiscible phases with surfactant from complete to partial wetting regimes, deriving continuum evolution that reveals new phenomena like moving/pinned facet coexistence and metastable states.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between microscopic lattice models and experimentally observed surfactant-induced pinning phenomena by extending previous work on completely wetted crystals to the more realistic regime of partial wetting, where surfactant occupies only portions of the interface.

Method: Using minimizing-movements scheme within variational framework to rigorously derive continuum evolution from discrete lattice systems of Blume-Emery-Griffith type, analyzing the coupling between interfacial motion and surfactant redistribution in partial wetting regime.

Result: The analysis reveals complex coupling between interfacial motion and surfactant redistribution in partial wetting, leading to new phenomena including coexistence of moving and pinned facets, emergence of long-lived metastable states, and different evolutionary behaviors compared to fully wetted case.

Conclusion: This provides the first discrete-to-continuum variational description of partially wetted crystalline interfaces, connecting microscopic lattice models with experimental observations of surfactant-induced pinning in immiscible systems.

Abstract: We continue the variational study of the discrete-to-continuum evolution of lattice systems of Blume-Emery-Griffith type which model two immiscible phases in the presence of a surfactant. In our previous work \cite{CFS}, we analyzed the case of a completely wetted crystal and described how the interplay between surfactant evaporation and mass conservation leads to a transition between crystalline mean curvature flow and pinned evolutions. In the present paper, we extend the analysis to the regime of partial wetting, where the surfactant occupies only a portion of the interface. Within the minimizing-movements scheme, we rigorously derive the continuum evolution and show how partial wetting introduces a complex coupling between interfacial motion and redistribution of surfactant. The resulting evolution exhibits new features absent in the fully wetted case, including the coexistence of moving and pinned facets or the emergence and long-lived metastable states. This provides, to our knowledge, the first discrete-to-continuum variational description of partially wetted crystalline interfaces, bridging the gap between microscopic lattice models and experimentally observed surfactant-induced pinning phenomena in immiscible systems.

</details>


### [29] [Refined Limiting Profiles of the Principal Eigenvalue Problems with Large Advection](https://arxiv.org/abs/2512.22918)
*Yujin Guo,Yuan Lou,Hongfei Zhang*

Main category: math.AP

TL;DR: The paper analyzes the limiting behavior of principal eigenpairs for an eigenvalue problem with advection as the advection coefficient becomes large.


<details>
  <summary>Details</summary>
Motivation: To understand how large advection affects the principal eigenvalue and eigenfunction of elliptic operators with advection terms, which has applications in various physical and biological models.

Method: Analyzes the refined limiting profiles of the principal eigenpair (λ, φ) as α→∞ for the eigenvalue problem -εΔφ - 2α∇m(x)·∇φ + V(x)φ = λφ with Dirichlet boundary conditions.

Result: Obtains refined expansions showing the visible effect of large advection on the principal eigenvalue and eigenfunction, with arguments applicable to general principal eigenvalue problems.

Conclusion: The analysis provides insights into how large advection influences spectral properties, with methodology extendable to broader eigenvalue problems.

Abstract: In this paper, we are concerned with the following eigenvalue problem with an advection term: \begin{equation}\label{0.1} \left\{ \begin{split} -εΔφ-2α\nabla m(x)\cdot\nabla φ+V(x)φ&=λφ \ \text{in}\ \ Ω,\\ φ&=0\ \ \hbox{on}\ \ \partialΩ, ~~~\text{(0.1)} \end{split} \right. \end{equation} where $Ω\subset\mathbb{R}^N~(N\geq1)$ satisfying $\partialΩ\in C^{2}$ is a bounded domain and contains the origin as an interior point, the constants $ε>0$ and $α>0$ are the diffusive and advection coefficients, respectively, and $m(x)\in C^{2}(\barΩ)$, $V (x)\in C^γ(\barΩ)~(0<γ<1)$ are given functions. We analyze the refined limiting profiles of the principal eigenpair $(λ, φ)$ for (0.1) as $α\rightarrow\infty$, which display the visible effect of the large advection on $(λ, φ)$. It expects that our argument is applicable to investigating the refined expansions of the general principal eigenvalue problems.

</details>


### [30] [Diffusion wave phenomena and optimal time decay for incompressible viscoelastic flows](https://arxiv.org/abs/2512.22921)
*Shenghan Li,Yong Wang*

Main category: math.AP

TL;DR: This paper studies diffusion wave phenomena in 3D incompressible viscoelastic flows, establishing L^p decay estimates across the full range 1≤p≤∞ to reveal hyperbolic nature.


<details>
  <summary>Details</summary>
Motivation: Motivated by Hoff and Zumbrun's 1995 work, the authors investigate diffusion wave phenomena in three-dimensional incompressible viscoelastic flows to understand their hyperbolic characteristics.

Method: The authors employ the representation formula of the wave equation and stationary phase methods on the sphere 𝕊^{d-1} to analyze the solution behavior.

Result: They establish L^p decay estimates for the solution over the entire range 1≤p≤∞, demonstrating the hyperbolic nature of incompressible viscoelastic flows.

Conclusion: The analysis reveals the hyperbolic characteristics of incompressible viscoelastic flows through comprehensive decay estimates, extending previous work on diffusion wave phenomena.

Abstract: Motivated by the work of D. Hoff and K. Zumbrun (Indiana Univ. Math. J. 44: 603-676, 1995), we investigate the diffusion wave phenomena in three-dimensional incompressible viscoelastic flows. By employing the representation formula of the wave equation and the stationary phase methods on the sphere $\mathbb{S}^{d-1}$, we establish $L^p$ decay estimates for the solution over the whole range $1\leq p \leq \infty$, which reveals the hyperbolic nature of the incompressible viscoelastic flows.

</details>


### [31] [Determining habitat anomalies in cross-diffusion predator-prey chemotaxis models](https://arxiv.org/abs/2512.22946)
*Yuhan Li,Hongyu Liu,Catharine W. K. Lo*

Main category: math.AP

TL;DR: This paper solves an inverse problem to uniquely identify unknown spatial anomalies (habitat degradation zones) and their ecological parameters in multi-species predator-prey systems using only boundary measurements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap between ecological sensing and quantitative inference of internal habitat heterogeneity. There's a need to detect and characterize habitat degradation from limited external data, addressing an open inverse problem at the interface of mathematical analysis and spatial ecology.

Method: The authors formulate the problem as simultaneous recovery of unknown interior subdomains and discontinuous ecological interaction rules across boundaries. They develop a unified theoretical framework that uniquely determines both the anomaly's geometry and discontinuous coefficients characterizing altered interactions within degraded regions.

Result: The results cover smooth anomalies in time-dependent systems and are extended to non-smooth polyhedral inclusions in stationary regimes. The framework successfully determines both the geometry of habitat degradation zones and the altered ecological interaction parameters.

Conclusion: This work provides a mathematical basis for detecting and characterizing habitat degradation from limited external data, bridging ecological sensing with quantitative inference of internal habitat heterogeneity in predator-prey systems with multiple chemical signals.

Abstract: This paper addresses an open inverse problem at the interface of mathematical analysis and spatial ecology: the unique identification of unknown spatial anomalies -- interpreted as zones of habitat degradation -- and their associated ecological parameters in multi-species predator-prey systems with multiple chemical signals, using only boundary measurements. We formulate the problem as the simultaneous recovery of an unknown interior subdomain and discontinuous ecological interaction rules across its boundary. A unified theooretical framework is developed that unique determines both the anomaly's geometry and discontinuous coefficients characterizing the altered interactions within the degraded region. Our results cover smooth anomalies in time-dependent systems and are extended to non-smooth polyhedral inclusions in stationary regimes. This work bridges a gap between ecological sensing and the quantitative inference of internal habitat heterogeneity, offering a mathamtical basis for detecting and characterizing habitat degradation from limited external data.

</details>


### [32] [Standing waves of the Anderson-Gross-Pitaevskii equation](https://arxiv.org/abs/2512.22960)
*Samaël Mackowiak*

Main category: math.AP

TL;DR: Construction of standing waves for Anderson-Gross-Pitaevskii equation in 1D and 2D using variational methods, with analysis of regularity, localization, and stability.


<details>
  <summary>Details</summary>
Motivation: Study standing waves for Anderson-Gross-Pitaevskii equation (nonlinear Schrödinger equation with confining potential and multiplicative spatial white noise) to understand wave dynamics in disordered media.

Method: Variational methods to construct standing wave solutions, characterized by profiles invariant under dynamics solving nonlinear elliptic equation with spatial white noise potential.

Result: Successfully constructed standing wave solutions and obtained results on their regularity properties, spatial localization characteristics, and stability behavior.

Conclusion: Standing waves can be constructed for Anderson-Gross-Pitaevskii equation in dimensions 1 and 2 using variational approaches, with established regularity, localization, and stability properties.

Abstract: In this paper, we study standing waves for the Anderson-Gross-Pitaevskii equation in dimension 1 and 2. The Anderson-Gross-Pitaevskii equation is a nonlinear Schrödinger equation with a confining potential and a multiplicative spatial white noise. Standing waves are characterized by a profile which is invariant by the dynamic and solves a nonlinear elliptic equation with spatial white noise potential. We construct such solutions via variational methods and obtain some results on their regularity, localization and stability.

</details>


### [33] [Derivation of nonlinear time-dependent macroscopic conductivity for an electropermeabilization model via homogenization](https://arxiv.org/abs/2512.23007)
*Tobias Gebäck,Ioanna Motschan-Armen,Irina Pettersson*

Main category: math.AP

TL;DR: Mathematical analysis of electropermeabilization in biological tissue using homogenization theory to derive macroscopic conductivity model with memory effects.


<details>
  <summary>Details</summary>
Motivation: To understand electropermeabilization (electroporation) at tissue scale by deriving effective macroscopic models from microscopic cell-level descriptions, explaining experimentally observed nonlinear conductivity changes.

Method: Use homogenization theory with periodic medium scaling (ε = cell/tissue size ratio), derive a priori estimates, formal asymptotics, and rigorously justify expansion using two-scale convergence and monotonicity arguments.

Result: Derived macroscopic model with memory effects and nonlinear time-dependent effective current that captures characteristic conductivity drop (reflecting capacitive lipid bilayer behavior) and sigmoid-shaped conductivity variation with electric field strength.

Conclusion: Provides rigorous mathematical foundation for experimentally observed electropermeabilization conductivity dynamics, showing how constant microscopic conductivity leads to nonlinear tissue-level conductivity changes through homogenization.

Abstract: We study a phenomenological electropermeabilization model in a periodic medium representing biological tissue. Starting from a cell-level model describing the electric potential and the degree of porosity, we perform dimension analysis to identify a relevant scaling in terms of a small parameter $\ve$ - the ratio between the cell and the tissue size. The electric potential satisfies electrostatic equations in the extra- and intracellular domains, while its jump across the cell membrane evolves according to a nonlinear law coupled with an ordinary differential equation for the porosity degree. We prove the well-posedness of the microscopic problem, derive a priori estimates, obtain formal asymptotics, and rigorously justify the expansion combining two-scale convergence with monotonicity arguments. The resulting macroscopic model exhibits memory effects and a nonlinear, time-dependent effective current. It captures the nontrivial evolution of effective conductivity, including a characteristic drop reflecting the capacitive behavior of the lipid bilayer, in agreement with experimental data. Numerical computations of the effective conductivity confirm that, although microscopic conductivity is constant, tissue conductivity varies nonlinearly with electric field strength, showing a sigmoid trend. This suggests a rigorous mathematical explanation for experimentally observed conductivity dynamics.

</details>


### [34] [Guillarmou's Normal Operator for Magnetic and Thermostat Flows](https://arxiv.org/abs/2512.23106)
*Sebastián Muñoz-Thon,Sean Richardson*

Main category: math.AP

TL;DR: Generalization of Guillarmou's normal operator to thermostat and magnetic flows, showing they're elliptic pseudodifferential operators of order -1, with application to stability estimate for magnetic X-ray transform.


<details>
  <summary>Details</summary>
Motivation: To extend the theory of normal operators from geodesic X-ray transforms to more general dynamical systems, specifically thermostat flows and magnetic flows, which have applications in inverse problems and tomography.

Method: Generalize Guillarmou's normal operator construction to thermostat flows and magnetic flows under certain dynamical assumptions, using pseudodifferential operator theory and analysis of these dynamical systems.

Result: The generalized normal operators for both thermostat flows and magnetic flows are shown to be elliptic pseudodifferential operators of order -1. A stability estimate is proved for the magnetic X-ray transform.

Conclusion: The paper successfully extends the normal operator framework to more general dynamical systems, providing analytical tools for studying inverse problems involving thermostat and magnetic flows, with practical applications in stability analysis.

Abstract: Guillarmou's normal operator over a closed Anosov manifold is analogous to the classical normal operator of the geodesic X-ray transform over manifolds with boundary. In this paper, we generalize this normal operator, under some dynamical assumptions, to thermostat flows as well as to the case of the magnetic flows. In particular, we show that these generalized normal operators are elliptic pseudodifferential operators of order -1 in each case. As an application, we prove a stability estimate for the magnetic X-ray transform.

</details>


### [35] [Qualitative analysis on the critical points of the Kirchhoff-Routh function](https://arxiv.org/abs/2512.23172)
*Francesca Gladiali,Massimo Grossi,Peng Luo,Shusen Yan*

Main category: math.AP

TL;DR: Study of critical points of Kirchhoff-Routh function in domains with small holes, establishing exact number, location, nondegeneracy, and connection to multiple two-peak solutions in elliptic problems.


<details>
  <summary>Details</summary>
Motivation: The Kirchhoff-Routh function arises from concentration phenomena in nonlinear elliptic problems and de-singularization problems for steady Euler equations. Understanding its critical points is important for analyzing vortex dynamics and singular perturbation problems in fluid dynamics and PDEs.

Method: Analyze the Kirchhoff-Routh function in bounded domains with small holes. Use properties of Robin function and Green function with Dirichlet boundary conditions. Study how hole location affects critical points and establish nondegeneracy conditions.

Result: For domains with small holes, establish exact number and location of critical points of Kirchhoff-Routh function, prove their nondegeneracy, and show hole location plays crucial role. Also establish existence of multiple two-peak solutions in elliptic problems context.

Conclusion: The study provides precise characterization of critical points in perturbed domains, demonstrating significant dependence on hole location, with applications to vortex dynamics and singular perturbation problems in PDEs.

Abstract: In this paper, we study the number of critical points of the Kirchhoff-Routh function \begin{equation*} \mathcal{KR}_D(x,y)=Λ_1^2\mathcal{R}_D(x)+Λ_2^2\mathcal{R}_D(y)-2Λ_1Λ_2G_D(x,y), \end{equation*} where $D$ is a bounded domain in $\mathbb{R}^2$, $x,y\in D$, $Λ_1,Λ_2>0$, $\mathcal{R}_D$ is the Robin function, and $G_D$ is the Green function of the operator $-Δ$ with $0$ Dirichlet boundary condition on $D$. This function arises from concentration phenomena in nonlinear elliptic problems and from the de-singularization problem for the steady Euler equation. For domains with a small hole, we establish not only the exact number and the location of the critical points of $\mathcal{KR}_D$, but also their nondegeneracy. We show that the location of the hole plays a crucial role. Finally in the context of elliptic problems, we establish the existence of multiple two-peak solutions.

</details>


### [36] [Global strong solutions for non-isothermal compressible nematic liquid crystal flows under a scaling-invariant smallness condition](https://arxiv.org/abs/2512.23197)
*Lin Xu,Xin Zhong*

Main category: math.AP

TL;DR: Global existence and uniqueness of strong solutions for 3D non-isothermal compressible nematic liquid crystal system with vacuum, under smallness of a new scaling-invariant quantity without extra viscosity restrictions.


<details>
  <summary>Details</summary>
Motivation: Previous work (Commun. Math. Sci. 2023) established results but with limitations. This paper aims to improve by identifying a new scaling-invariant quantity and removing additional restrictions on viscosity coefficients.

Method: Derive refined energy estimates and exploit the coupled structure of the equations to establish global existence and uniqueness of strong solutions.

Result: Proves global existence and uniqueness of strong solutions when the specified scaling-invariant quantity is sufficiently small, improving upon previous work.

Conclusion: The paper successfully establishes a global well-posedness theory for the 3D non-isothermal compressible nematic liquid crystal system with vacuum under minimal assumptions, identifying a new scaling-invariant quantity as the key condition.

Abstract: We study the three-dimensional Cauchy problem for a non-isothermal compressible nematic liquid crystal system with far-field vacuum. By deriving refined energy estimates and exploiting the coupled structure of the equations, we establish the global existence and uniqueness of strong solutions, provided that the following scaling-invariant quantity is sufficiently small:
  $$
  \big(1+\barρ+\tfrac{1}{\barρ}\big)
  \big[\|ρ_{0}\|_{L^{3}}+(\barρ^{2}+\barρ)\big(\|\sqrt{ρ_{0}}u_{0}\|_{L^{2}}^{2}+\|\nabla d_{0}\|_{L^{2}}^{2}\big)\big]
  \big[\|\nabla u_{0}\|_{L^{2}}^{2}+(\barρ+1)\|\sqrt{ρ_{0}}θ_{0}\|_{L^{2}}^{2}
  +\|\nabla^{2} d_{0}\|_{L^{2}}^{2}+\|\nabla d_{0}\|_{L^{4}}^{4}\big].
  $$
  In particular, our result identifies a new scaling-invariant quantity and does not impose additional restrictions on the viscosity coefficients, which improves previous work (Commun. Math. Sci. 21 (2023), 1455--1486).

</details>


### [37] [Local well-posedness of the Schrödinger flow into $\mathbb{S}^2$ with natural boundary conditions](https://arxiv.org/abs/2512.23201)
*Bo Chen,Youde Wang*

Main category: math.AP

TL;DR: New approximation scheme resolves local well-posedness for Landau-Lifshitz equation (Schrödinger flow into S²) with natural boundary conditions.


<details>
  <summary>Details</summary>
Motivation: The Landau-Lifshitz equation, which describes Schrödinger flow into the standard unit 2-sphere, has unresolved local well-posedness problems with natural boundary conditions that need to be addressed.

Method: Develops a new approximation scheme specifically designed to handle the Landau-Lifshitz equation with natural boundary conditions.

Result: Successfully resolves the local well-posedness problem for the Landau-Lifshitz equation with natural boundary conditions using the new approximation scheme.

Conclusion: The proposed approximation scheme provides an effective solution to the local well-posedness problem for the Landau-Lifshitz equation with natural boundary conditions, advancing the mathematical understanding of this important PDE.

Abstract: In this paper, we develop a new approximation scheme to resolve the local well-posedness problem for the Landau-Lifshitz equation (i.e., the Schrödinger flow into the standard unit 2-sphere $\mathbb{S}^2\subset \mathbb{R}^3$) with natural boundary conditions.

</details>


### [38] [Infinitely many positive solutions to nonlinear scalar field equation with nonsmooth nonlinearity](https://arxiv.org/abs/2512.23207)
*Tianhao Liu,Juncheng Wei,Wenming Zou*

Main category: math.AP

TL;DR: Existence of infinitely many positive multi-bump solutions for logarithmic scalar field equations with non-symmetric potentials using nonsmooth variational methods.


<details>
  <summary>Details</summary>
Motivation: Study physically relevant logarithmic scalar field equations that arise in quantum mechanics and field theory, addressing the challenge of nonsmooth logarithmic nonlinearity which prevents standard critical point theory application.

Method: Use nonsmooth critical point theory, localized variational methods, and max-min arguments to handle the nonsmooth nature of logarithmic nonlinearity where the energy functional is not C¹.

Result: Proved existence of multi-bump positive solutions: for problem (P) with infinitely many bumps, and for normalized problem (P_N) with finite number of bumps, under non-symmetric, non-periodic potential conditions.

Conclusion: First successful application of localized variational method to nonsmooth functionals, establishing existence of infinitely many positive solutions for logarithmic scalar field equations with challenging nonsmooth nonlinearities.

Abstract: This paper investigates the existence of infinitely many positive solutions for the logarithmic scalar field equation
  \begin{equation}
  \tag{$P$} \label{equ1}
  -Δu+ V(x) u= u\log u^2, \quad u\in H^1(\mathbb{R}^N),
  \end{equation}
  and its counterpart with prescribed $L^2$-norms
  \begin{align}\label{equ2} \tag{$P_N$}
  & -Δu+ V(x) u +λu= u\log u^2, \quad u\in H^1(\mathbb{R}^N),
  &\int_{\mathbb{R}^N} u^2 ~\mathrm{d}x=a^2>0,
  \end{align}
  which come from physically relevant situations. Here, $N\geq 2$, $V:\mathbb{R}^N\to \mathbb{R}$ is a non-symmetric and non-periodic potential satisfying certain decay conditions, $ a $ is prescribed constant, and $λ$ arises as an unknown Lagrange multipliers. For problem \eqref{equ1}, using purely variational methods, we establish the existence of multi-bump positive solutions with either finitely or infinitely many bumps. For normalized problem \eqref{equ2}, we prove the existence of normalized multi-bump positive solutions with a finite number of bumps.
  The main difficulty comes from the nonsmooth nature of logarithmic nonlinearity, which introduces some challenges to the variational framework.
  In particular, the corresponding energy functional is not of class $C^1$ on $H^1(\mathbb{R}^N)$, which prevents the direct application of standard critical point theory for $C^1$ functional or any reduction methods for $C^{1+σ}$ nonlinearity. The main ingredients in this paper are nonsmooth critical point theory, localized variational methods and a max-min argument.
  To the best of our knowledge, this paper appears to be the first successful application of the localized variational method to nonsmooth functionals.

</details>


### [39] [Global stability and asymptotic behavior for incompressible ideal MHD equations with velocity damping term](https://arxiv.org/abs/2512.23263)
*Hui Fang,Pingping Gui,Yanping Zhou*

Main category: math.AP

TL;DR: The paper studies stability and long-time behavior of a multi-dimensional incompressible MHD system with velocity damping, focusing on small perturbations near a steady-state magnetic field satisfying Diophantine conditions.


<details>
  <summary>Details</summary>
Motivation: To mathematically characterize the stabilizing effect of background magnetic fields and bridge gaps in understanding asymptotic time behavior in partially dissipative fluid models.

Method: Uses Fourier analysis and energy estimates to analyze the system, providing a versatile analytical framework applicable to other partially dissipative fluid models.

Result: Demonstrates that background magnetic fields exert stabilizing effects on the system, with rigorous mathematical characterization of stability and large time behavior.

Conclusion: The research provides a comprehensive analytical framework for studying partially dissipative fluid systems, establishing mathematical foundations for understanding magnetic field stabilization effects.

Abstract: In this article, we study the stability and large time behavior for an multi-dimensional incompressible magnetohydrodynamical system with a velocity damping term, for small perturbations near a steady-state of magnetic field fulfilling the Diophantine condition. Our results mathematically characterize the background magnetic field exerts the stabilizing effect, and bridge the gap left by previous work with respect to the asymptotic behavior in time. Our proof approach mainly relies on the Fourier analysis and energy estimates. In addition, we provide a versatile analytical framework applicable to many other partially dissipative fluid models.

</details>


### [40] [On blow-up rate for the Hénon parabolic equation with Sobolev supercritical nonlinearity](https://arxiv.org/abs/2512.23271)
*Kotaro Hisa,Yukihiro Seki*

Main category: math.AP

TL;DR: Analysis of blow-up solutions for the Hénon parabolic equation with supercritical Sobolev exponent, showing existence of solutions blowing up at origin despite vanishing potential term, with Type I blow-up classification for certain exponent ranges.


<details>
  <summary>Details</summary>
Motivation: To understand blow-up behavior of solutions to the Hénon parabolic equation with vanishing spatial potential term at origin, particularly when exponent is supercritical in Sobolev sense, and to determine blow-up rates and classification.

Method: Construct explicit solutions that blow up at origin, analyze blow-up rates, prove Type I blow-up classification for exponents below Joseph-Lundgren exponent, establish lower bounds for Type I rates, and provide classification of threshold solutions.

Result: Successfully constructed solutions blowing up at origin despite vanishing potential term, proved all blow-ups are Type I when p is below Joseph-Lundgren exponent, established lower bounds for Type I rates, and provided classification of threshold solutions for p > 1+σ/N.

Conclusion: The vanishing spatial potential term does not prevent blow-up at origin for supercritical exponents, and blow-up behavior follows Type I classification for certain exponent ranges, with complete analysis of blow-up rates and threshold solution classification.

Abstract: We discuss the Hénon parabolic equation $\partial_t u = Δu + |x|^σu^p$ in a finite ball in $\mathbb{R}^N$ under the Dirichlet boundary condition, where $N\ge1$, $p>1$, and $σ>0$. We assume that the exponent $p$ is supercritical in the Sobolev sense. Since the spatial potential term $|x|^σ$ vanishes at the origin, solutions seem less likely to blow up at the origin. We construct a solution that blows up at the origin and also carry out an analysis of blow-up rate of solutions. In particular, if $p$ is less than the Joseph--Lundgren exponent, all blow-ups are shown to be of Type I. The lower bound corresponding to Type I rate is also shown for some particular blow-up solutions. As by products, we present a basic result on classification to threshold solutions for every $p>1+σ/N$.

</details>


### [41] [Ground States for the Nonlinear Schr{ö}dinger Equation on Open Books and Dimensional Reduction to Metric Graphs](https://arxiv.org/abs/2512.23286)
*Stefan Le Coz,Boris Shakarov*

Main category: math.AP

TL;DR: Dimensional reduction of stationary states from 2D open books to metric graphs, with sharp transition in ground state dimensionality based on transverse width.


<details>
  <summary>Details</summary>
Motivation: To understand how stationary states on two-dimensional open book structures reduce to their counterparts on metric graphs in the shrinking limit, and to characterize the transition between one-dimensional and two-dimensional ground states.

Method: Develop functional-analytic framework for variational problems on open books, establish existence of solutions as constrained action minimizers, and analyze graph-based open books (isomorphic to product of graph with interval) to prove existence of critical transverse width threshold.

Result: Proved existence of sharp transition: below critical transverse width, all ground states coincide with graph ground states extended trivially; above critical width, ground states become genuinely two-dimensional.

Conclusion: Open books exhibit dimensional reduction to metric graphs with a clear threshold behavior where ground state dimensionality transitions from effectively one-dimensional to genuinely two-dimensional depending on transverse width.

Abstract: In this work, we study the dimensional reduction of stationary states in the shrinking limit for a broad class of two-dimensional domains, called open books, to their counterparts on metric graphs. An open book is a two-dimensional structure formed by rectangular domains sharing common boundaries. We first develop a functional-analytic framework suited to variational problems on open books and establish the existence of solutions as constrained action minimizers. For graph-based open books (i.e., those isomorphic to the product of a graph with an interval) we prove the existence of a sharp transition in the dimensionality of ground states. Specifically, there exists a critical transverse width: below this threshold, all ground states coincide with the ground states on the underlying graph trivially extended in the transverse direction; above it, ground states become genuinely two-dimensional.

</details>


### [42] [Normalized solutions of nonlinear magnetic Schrödinger equations on metric graphs](https://arxiv.org/abs/2512.23321)
*Pietro d'Avenia,Zhentao He,Chao Ji*

Main category: math.AP

TL;DR: The paper establishes magnetic Sobolev space theory on metric graphs, proves self-adjointness of magnetic Schrödinger operators, and studies normalized solutions to nonlinear magnetic Schrödinger equations on both compact and noncompact metric graphs across different mass regimes.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive mathematical framework for studying magnetic Schrödinger operators on metric graphs and investigate normalized solutions to nonlinear magnetic Schrödinger equations in this setting, which has applications in quantum graphs and condensed matter physics.

Method: First establishes the theory of magnetic Sobolev space H^1_A(G,C) on metric graphs and proves self-adjointness of corresponding magnetic Schrödinger operators. Then uses variational methods to study existence and multiplicity of normalized solutions to nonlinear magnetic Schrödinger equations on both compact and noncompact metric graphs with different nonlinearity configurations.

Result: Develops the mathematical foundation for magnetic Sobolev spaces on metric graphs and proves self-adjointness results. Provides existence and multiplicity results for normalized solutions across mass-subcritical, mass-critical, and mass-supercritical cases for both compact and noncompact metric graphs with localized or global nonlinearities.

Conclusion: The paper successfully establishes the theoretical framework for magnetic Schrödinger operators on metric graphs and provides comprehensive results on normalized solutions to nonlinear magnetic Schrödinger equations, covering various graph types and nonlinearity regimes.

Abstract: In this paper we first establish the theory of a magnetic Sobolev space $H^1_A(\mathcal{G},\mathbb{C})$ on metric graphs $\mathcal{G}$ and we prove the self-adjointness of its corresponding magnetic Schrödinger operator. Then, in this setting, we investigate the existence and multiplicity of normalized solutions to nonlinear magnetic Schrödinger equations on compact metric graphs and on noncompact metric graphs with localized nonlinearities or nonlinearities acting on whole metric graphs, covering the mass-subcritical, mass-critical, and mass-supercritical cases.

</details>


### [43] [Solutions of the singular Yamabe problem near singular boundaries](https://arxiv.org/abs/2512.23331)
*Weiming Shen,Zhehui Wang,Jiongduo Xie*

Main category: math.AP

TL;DR: Study of asymptotic behaviors of solutions to singular Yamabe problem with negative scalar curvature near singular boundaries, extending previous results to non-conformally flat metrics.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behaviors of solutions to the singular Yamabe problem near singular boundaries when background metrics are not conformally flat, extending previous work that assumed conformal flatness.

Method: Investigate solutions for Lipschitz domains with asymptotic conical structure, showing local positive solutions are well approximated by positive solutions in tangent cones at singular boundary points.

Result: Derived optimal estimates for asymptotic behaviors and demonstrated approximation results for a wide class of Lipschitz domains, extending previous results from [10, 12, 26].

Conclusion: The paper successfully extends understanding of singular Yamabe problem to non-conformally flat metrics, providing optimal estimates and approximation results for solutions near singular boundaries with conical structure.

Abstract: In this paper, we investigate the asymptotic behaviors of solutions to the singular Yamabe problem with negative constant scalar curvature near singular boundaries and derive optimal estimates, where the background metrics are not assumed to be conformally flat. Specifically, we demonstrate that for a wide class of Lipschitz domains with asymptotic conical structure, the local positive solutions are well approximated by the positive solutions in the tangent cones at singular boundary points. This extends the results of [10, 12, 26].

</details>


### [44] [Regularity for mixed-order nonlinear fractional equations with degenerate coefficients](https://arxiv.org/abs/2512.23359)
*Ho-Sik Lee,Jihoon Ok,Kyeong Song*

Main category: math.AP

TL;DR: The paper studies regularity properties of weak solutions to nonlinear integro-differential equations involving weighted superposition of fractional p-Laplacians with different fractional orders s and t.


<details>
  <summary>Details</summary>
Motivation: To establish regularity theory for a broader class of nonlinear integro-differential equations that combine different fractional p-Laplacian operators with degenerate coefficients, extending existing results for single fractional operators.

Method: The authors consider weak solutions to equations with leading operator being a superposition of $(-Δ_{p})^{s}$ and $(-Δ_{p})^{t}$ weighted by coefficients $a(\cdot,\cdot)$ and $b(\cdot,\cdot)$. They use techniques from nonlinear potential theory and regularity theory for fractional operators.

Result: Proved local boundedness and Hölder regularity of weak solutions under natural assumptions on coefficients and parameters. When $a(\cdot,\cdot) \equiv 1$, also established a Harnack inequality for weak solutions.

Conclusion: The paper successfully extends regularity results to a more general class of nonlinear integro-differential equations combining multiple fractional p-Laplacians with degenerate coefficients, providing a comprehensive regularity theory including local boundedness, Hölder continuity, and Harnack inequality.

Abstract: We consider a class of nonlinear integro-differential equations whose leading operator is obtained as a superposition of $(-Δ_{p})^{s}$ and $(-Δ_{p})^{t}$, where $0<s<t<1<p<\infty$, weighted via two possibly degenerate coefficients $a(\cdot,\cdot),b(\cdot,\cdot) \ge 0$. We prove local boundedness and Hölder regularity of its weak solutions under natural assumptions on the coefficients $a(\cdot,\cdot)$, $b(\cdot,\cdot)$ and the powers $s,t$, and $p$. Moreover, when $a(\cdot,\cdot) \equiv 1$, we also prove a Harnack inequality for weak solutions.

</details>


### [45] [The Time-Periodic Cahn-Hilliard-Gurtin System on the Half Space as a Mixed-Order System with General Boundary Conditions](https://arxiv.org/abs/2512.23582)
*Guillaume Neuttiens,Jonas Sauer*

Main category: math.AP

TL;DR: The paper establishes well-posedness and maximal regularity for time-periodic Cahn-Hilliard-Gurtin system in half space using novel complementing boundary conditions that extend Lopatinskiĭ-Shapiro conditions to mixed-order systems.


<details>
  <summary>Details</summary>
Motivation: Classical Lopatinskiĭ-Shapiro conditions are insufficient for well-posedness of mixed-order systems, necessitating new boundary condition theory for time-periodic mixed-order systems like Cahn-Hilliard-Gurtin.

Method: Introduces novel class of complementing boundary conditions that extend classical Lopatinskiĭ-Shapiro conditions from elliptic/parabolic theory to time-periodic mixed-order systems with general boundary conditions.

Result: Proves well-posedness and maximal regularity for time-periodic Cahn-Hilliard-Gurtin system in half space using the new complementing boundary conditions.

Conclusion: The new complementing boundary conditions are necessary for well-posedness of mixed-order systems, as classical Lopatinskiĭ-Shapiro conditions are generally insufficient for such systems.

Abstract: A well-posedness and maximal regularity result for the time-periodic Cahn-Hilliard-Gurtin system in the half space is proved. For this purpose, we introduce a novel class of complementing boundary conditions, extending the classical Lopatinskiĭ-Shapiro conditions from elliptic and parabolic theory to time-periodic mixed-order systems with general boundary conditions. Moreover, we show that the classical Lopatinskiĭ-Shapiro conditions are in general insufficient for well-posedness of mixed-order systems.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [46] [The Solution of Potential-Driven, Steady-State Nonlinear Network Flow Equations via Graph Partitioning](https://arxiv.org/abs/2512.22124)
*Shriram Srinivasan,Kaarthik Sundar*

Main category: physics.comp-ph

TL;DR: A distributed algorithm for solving large-scale nonlinear flow networks by partitioning them into smaller subsystems, enabling parallel computation while maintaining data privacy at interconnects.


<details>
  <summary>Details</summary>
Motivation: Solving potential-driven steady-state flow in large networks (like natural gas or water pipelines) becomes increasingly challenging as network size grows, requiring scalable methods that can handle the nonlinear equations while respecting operational boundaries between different network operators.

Method: The algorithm partitions the network into tractable subsystems based on natural boundaries (interconnects/transfer points). Each subsystem is solved locally, with data shared only at the interconnects. The global solution is computed through coordination of these local solutions, connected mathematically to the Schur complement method.

Result: The method successfully demonstrates viability on challenging test cases, showing it can compute global solutions for full nonlinear systems while allowing network operators to solve their own domains independently and maintain data privacy.

Conclusion: This distributed approach provides a scalable solution for large network flow problems, particularly valuable when networks span multiple operators, as it enables parallel computation while respecting operational boundaries and data privacy requirements.

Abstract: The solution of potential-driven steady-state flow in large networks is required in various engineering applications, such as transport of natural gas or water through pipeline networks. The resultant system of nonlinear equations depends on the network topology, and its solution grows more challenging as the network size increases. We present an algorithm that utilizes a given partition of a network into tractable sizes to compute a global solution for the full nonlinear system through local solution of smaller subsystems induced by the partitions. When the partitions are induced by interconnects or transfer points corresponding to networks owned by different operators, the method ensures data is shared solely at the interconnects, leaving network operators free to solve the network flow system corresponding to their own domain in any manner of their choosing. The proposed method is shown to be connected to the Schur complement and the method's viability demonstrated on some challenging test cases.

</details>


### [47] [A Radiation Exchange Factor Transformation with Proven Convergence, Non-Negativity, and Energy Conservation](https://arxiv.org/abs/2512.22157)
*Nikolaj Maack Bielefeld*

Main category: physics.comp-ph

TL;DR: Matrix-based exchange factor transformation for solving coupled mixed boundary radiative transfer problems with guaranteed convergence, non-negative radiation, and exact energy conservation.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing radiative transfer methods for general domains with mixed boundary conditions, particularly identifying and eliminating discrepancies in classical approaches like Noble's matrix formulation of Hottel's zonal method.

Method: Transforms first-interaction exchange factor matrix F into absorption matrix A and multiple reflection-scattering matrix R using Neumann series that analytically traces all reflection-scattering paths to steady state.

Result: Method guarantees convergence, non-negative radiation, and exact energy conservation to machine precision. Validated against diffusion approximation in high-extinction limit and Crosbie & Schrenker results for pure/partial scattering cases.

Conclusion: The transformation eliminates discrepancies in classical approaches and is applicable to medium-scale general reflecting-scattering problems, scaling to large problems when matrix sparsity conditions are met.

Abstract: This paper presents a matrix-based exchange factor transformation for solving coupled mixed boundary condition radiative transfer problems on general domains. The method applies to participating media ranging from transparent to absorbing, emitting, and scattering, with boundaries ranging from absorbing to reflecting. Given a first-interaction exchange factor matrix $\mathbf{F}$, the transformation produces an absorption matrix $\mathbf{A}$ and a multiple reflection-scattering matrix $\mathbf{R}$ through a Neumann series that analytically traces all reflection-scattering paths to steady state. The paper establishes rigorous conditions under which the method guarantees convergence, non-negative radiation, and exact energy conservation to machine precision. A comparison with Noble's matrix formulation of Hottel's zonal method reveals a previously unidentified discrepancy in that classical approach; the proposed transformation eliminates this discrepancy. The method is validated against the diffusion approximation in the high-extinction limit and against results of Crosbie and Schrenker for pure and partial scattering cases. The method is applicable to medium-scale general reflecting-scattering problems and scales to large problems when negligible reflection-scattering and high extinction ensure matrix sparsity.

</details>


### [48] [Integrating Wide and Deep Neural Networks with Squeeze-and-Excitation Blocks for Multi-Target Property Prediction in Additively Manufactured Fiber Reinforced Composites](https://arxiv.org/abs/2512.22397)
*Behzad Parvaresh,Rahmat K. Adesunkanmi,Adel Alaeddini*

Main category: physics.comp-ph

TL;DR: Data-efficient multi-target learning approach combining Latin Hypercube Sampling with squeeze-and-excitation wide & deep neural network to predict mechanical/manufacturing properties of 3D-printed continuous fiber-reinforced composites.


<details>
  <summary>Details</summary>
Motivation: Continuous fiber-reinforced composites via additive manufacturing offer lightweight high-strength materials, but their performance depends on complex process-material interactions that make exhaustive experimental testing impractical.

Method: Latin Hypercube Sampling-guided experimentation with 155 specimens from 4,320 design space combinations, using squeeze-and-excitation wide and deep neural network (SE-WDNN) for multi-input, multi-target prediction of mechanical/manufacturing properties.

Result: SE-WDNN achieved lowest overall test error (MAPE = 12.33%) and statistically significant improvements over baseline models; SHAP analysis showed reinforcement strategy as major influence on mechanical performance.

Conclusion: Integration of LHS and SE-WDNN enables interpretable, sample-efficient multi-target predictions, guiding parameter selection in CFRC-AM with balanced consideration of mechanical behavior and manufacturing metrics.

Abstract: Continuous fiber-reinforced composite manufactured by additive manufacturing (CFRC-AM) offers opportunities for printing lightweight materials with high specific strength. However, their performance is sensitive to the interaction of process and material parameters, making exhaustive experimental testing impractical. In this study, we introduce a data-efficient, multi-input, multi-target learning approach that integrates Latin Hypercube Sampling (LHS)-guided experimentation with a squeeze-and-excitation wide and deep neural network (SE-WDNN) to jointly predict multiple mechanical and manufacturing properties of CFRC-AMs based on different manufacturing parameters. We printed and tested 155 specimens selected from a design space of 4,320 combinations using a Markforged Mark Two 3D printer. The processed data formed the input-output set for our proposed model. We compared the results with those from commonly used machine learning models, including feedforward neural networks, Kolmogorov-Arnold networks, XGBoost, CatBoost, and random forests. Our model achieved the lowest overall test error (MAPE = 12.33%) and showed statistically significant improvements over the baseline wide and deep neural network for several target variables (paired t-tests, p <= 0.05). SHapley Additive exPlanations (SHAP) analysis revealed that reinforcement strategy was the major influence on mechanical performance. Overall, this study demonstrates that the integration of LHS and SE-WDNN enables interpretable and sample-efficient multi-target predictions, guiding parameter selection in CFRC-AM with a balance between mechanical behavior and manufacturing metrics.

</details>


### [49] [A survey of interlayer interaction models for graphene and other 2D materials](https://arxiv.org/abs/2512.22670)
*Gourav Yadav,Shakti S. Gupta,Roger A. Sauer*

Main category: physics.comp-ph

TL;DR: Survey of mechanical models for van der Waals interactions in 2D materials, covering continuum and discrete approaches, contact instabilities, Moiré patterns, and computational strategies.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of mechanical models that describe van der Waals interactions between 2D materials, which are crucial for understanding various physical phenomena in these materials.

Method: Systematic survey approach examining both normal and tangential contact models, considering atomistic and continuum approaches, analyzing effects of external loading and length scales, and discussing computational strategies.

Result: Comprehensive review of mechanical models for van der Waals interactions in 2D materials, covering contact instabilities, Moiré patterns, surface reconstructions, superlubricity, and computational cost reduction strategies.

Conclusion: The survey provides a unified framework for understanding van der Waals interactions in 2D materials, highlighting the importance of both continuum and discrete approaches, and offering computational strategies for efficient multiscale modeling.

Abstract: This work presents a survey of mechanical models describing van der Waals interactions between 2D materials, encompassing both continuous elastomer-like materials and discrete (crystalline) 2D materials such as graphene. These interactions give rise to a range of physical phenomena, including contact instabilities, Moiré patterns, surface reconstructions, and superlubricity. The underlying contact forces follow from the variation of an interfacial interaction potential. The presentation first discusses normal contact models, and then tangential contact models. Both atomistic and continuum approaches are considered. In addition, the influence of external loading and changes in length scale on the ground state configuration and frictional contact behavior are analyzed. A particular emphasis is placed on discussing strategies that reduce computational cost in multiscale modeling.

</details>


### [50] [Overcoming Computational Bottlenecks in Quantum Hydrodynamics: A Volume-Based Integral Formalism](https://arxiv.org/abs/2512.22920)
*Christos Mystilidis,Christos Tserkezis,Guy A. E. Vandenbosch,N. Asger Mortensen,Xuezhi Zheng*

Main category: physics.comp-ph

TL;DR: VIE method enables efficient simulation of quantum hydrodynamic nanoparticles by overcoming computational bottlenecks of mesoscopic material models like SC-HDM.


<details>
  <summary>Details</summary>
Motivation: Mesoscopic models for metal optical response face computational challenges despite being more efficient than ab initio methods. The SC-HDM captures nonlocal electron dynamics and spill-out but suffers from demanding computations due to sophisticated material response.

Method: Developed a Volume Integral Equation (VIE) method for the Self-Consistent Hydrodynamic Drude Model (SC-HDM), shifting from Differential Equation (DE)-based approaches. The method leverages inherent symmetries for spherical nanoparticles and can extract mesoscopic material-response functions directly.

Result: Achieved significant computational efficiency with similar performance for three increasingly complex material models. Broke the taboo that increased material sophistication requires taxing simulations. The method can extract material-response functions without lengthy microscopic calculations.

Conclusion: The VIE approach provides an efficient methodological framework for modeling quantum hydrodynamic nanoparticles, serving as an essential benchmarking tool for more complex geometries while overcoming computational bottlenecks of advanced material models.

Abstract: Mesoscopic models of the optical response of metals have emerged as fundamental building blocks in quantum plasmonics, in principle overcoming the computational bottlenecks of ab initio techniques by implementing aspects of the atomistic description of the metal in otherwise classical calculations. Nonetheless, even these approaches are eventually hindered by demanding computations due to sophisticated material response. Here, this issue is addressed for the advanced Self-Consistent Hydrodynamic Drude Model (SC-HDM), which captures both nonlocal electron dynamics and electron spill-out, through a Volume Integral Equation (VIE) method. Adopting an IE-based method shifts perspective from the commonly employed Differential Equation (DE)-based ones, demonstrating significant computational efficiency. The VIE approach is a valuable methodological scaffold: It addresses SC-HDM and simpler models, but can also be adapted to more advanced ones. For spherical nanoparticles (NPs), using the inherent symmetries, similar performance for three increasingly complicated material models is achieved, breaking the taboo that increased sophistication in material response requires taxing simulations. Mesoscopic material-response functions can be readily extracted from the VIE implementation, thus circumventing the need for lengthy microscopic calculations. This method opens a new way of modeling quantum hydrodynamic NPs and will serve as essential benchmarking tool for recipes addressing more complicated geometries.

</details>


### [51] [Masgent: An AI-assisted Materials Simulation Agent](https://arxiv.org/abs/2512.23010)
*Guanghen Liu,Songge Yang,Yu Zhong*

Main category: physics.comp-ph

TL;DR: Masgent is an AI-assisted materials simulation agent that uses LLMs to enable natural-language interaction for DFT and MLP simulations, reducing setup time from hours to seconds and democratizing access to advanced computational methods.


<details>
  <summary>Details</summary>
Motivation: Current DFT and MLP simulations require extensive scripting, multi-step procedures, and significant HPC expertise, which hinders reproducibility and slows down materials discovery. There's a need to simplify and accelerate these complex computational workflows.

Method: Masgent unifies structure manipulation, automated VASP input generation, DFT workflow construction and analysis, fast MLP-based simulations, and lightweight ML utilities within a single platform powered by large language models (LLMs).

Result: The platform enables researchers to perform complex simulation tasks through natural-language interaction, eliminating most manual scripting and reducing setup time from hours to seconds.

Conclusion: Masgent democratizes access to state-of-the-art computational methodologies by standardizing protocols and integrating advanced simulation and data-driven tools, accelerating hypothesis testing, pre-screening, and exploratory research for both new and experienced practitioners.

Abstract: Density functional theory (DFT) and machine learning potentials (MLPs) are essential for predicting and understanding materials properties, yet preparing, executing, and analyzing these simulations typically requires extensive scripting, multi-step procedures, and significant high-performance computing (HPC) expertise. These challenges hinder reproducibility and slow down discovery. Here, we introduce Masgent, an AI-assisted materials simulation agent that unifies structure manipulation, automated VASP input generation, DFT workflow construction and analysis, fast MLP-based simulations, and lightweight machine learning (ML) utilities within a single platform. Powered by large language models (LLMs), Masgent enables researchers to perform complex simulation tasks through natural-language interaction, eliminating most manual scripting and reducing setup time from hours to seconds. By standardizing protocols and integrating advanced simulation and data-driven tools, Masgent democratizes access to state-of-the-art computational methodologies, accelerating hypothesis testing, pre-screening, and exploratory research for both new and experienced practitioners.

</details>


### [52] [Reconstructing Relativistic Magnetohydrodynamics with Physics-Informed Neural Networks](https://arxiv.org/abs/2512.23057)
*Corwin Cheung,Marcos Johnson-Noya,Michael Xiang,Dominic Chang,Alfredo Guevara*

Main category: physics.comp-ph

TL;DR: First PINN surrogates for relativistic magnetohydrodynamics using hybrid PDE/data-driven approach with primitive variables and divergence-free constraint.


<details>
  <summary>Details</summary>
Motivation: To develop accurate neural network surrogates for relativistic magnetohydrodynamics (RMHD) that can efficiently simulate complex plasma dynamics without traditional numerical limitations.

Method: Hybrid PDE and data-driven workflow using physics-informed neural networks (PINNs) with primitive variables instead of conservative form, incorporating divergence-free condition directly, and using novel MUON optimizer implementation with residual-guided networks.

Result: Baseline PINN trained on early-time snapshots successfully extrapolates RMHD dynamics in 1D and 2D, and posterior residual-guided networks systematically reduce PDE violations.

Conclusion: The approach demonstrates that PINNs can effectively model RMHD systems, offering a promising alternative to traditional numerical methods with improved accuracy through residual-guided refinement.

Abstract: We construct the first physics-informed neural-network (PINN) surrogates for relativistic magnetohydrodynamics (RMHD) using a hybrid PDE and data-driven workflow. Instead of training for the conservative form of the equations, we work with Jacobians or PDE characteristics directly in terms of primitive variables. We further add to the trainable system the divergence-free condition, without the need of cleaning modes. Using a novel MUON optimizer implementation, we show that a baseline PINN trained on early-time snapshots can extrapolate RMHD dynamics in one and two spatial dimensions, and that posterior residual-guided networks can systematically reduce PDE violations.

</details>


### [53] [Exponential divided differences via Chebyshev polynomials](https://arxiv.org/abs/2512.23061)
*Itay Hen*

Main category: physics.comp-ph

TL;DR: A Chebyshev-polynomial-based algorithm for efficient evaluation of high-order exponential divided differences with O(qN) cost and incremental updates for dynamic node sets.


<details>
  <summary>Details</summary>
Motivation: Exponential divided differences are crucial in numerical linear algebra, matrix-function evaluation, and quantum Monte Carlo simulations, but efficient and numerically stable evaluation for high-order cases with dynamically evolving node sets remains computationally challenging.

Method: Combines Chebyshev-Bessel expansion of the exponential function with a direct recurrence for Chebyshev divided differences, achieving O(qN) computational cost where q is divided-difference order and N is Chebyshev truncation length. Also develops incremental update scheme for dynamic node sets enabling single node insertion/removal in O(N) time.

Result: Algorithm achieves linear scaling of N with spectral width through decay of modified Bessel coefficients, with q dependence entering only through structural polynomial constraints. Provides efficient solution for high-order exponential divided differences with dynamic node sets.

Conclusion: The Chebyshev-polynomial-based algorithm provides an efficient, numerically stable solution for evaluating high-order exponential divided differences with dynamic node sets, with publicly available C++ implementation.

Abstract: Exponential divided differences arise in numerical linear algebra, matrix-function evaluation, and quantum Monte Carlo simulations, where they serve as kernel weights for time evolution and observable estimation. Efficient and numerically stable evaluation of high-order exponential divided differences for dynamically evolving node sets remains a significant computational challenge. We present a Chebyshev-polynomial-based algorithm that addresses this problem by combining the Chebyshev-Bessel expansion of the exponential function with a direct recurrence for Chebyshev divided differences. The method achieves a computational cost of ${\cal O}(qN)$, where $q$ is the divided-difference order and $N$ is the Chebyshev truncation length. We show that $N$ scales linearly with the spectral width through the decay of modified Bessel coefficients, while the dependence on $q$ enters only through structural polynomial constraints. We further develop an incremental update scheme for dynamic node sets that enables the insertion or removal of a single node in ${\cal O}(N)$ time when the affine mapping interval is held fixed. A full \texttt{C++} reference implementation of the algorithms described in this work is publicly available.

</details>


### [54] [A space-time extension of a conservative two-fluid cut-cell diffusion method for moving geometries](https://arxiv.org/abs/2512.23358)
*Louis Libat,Can Selçuk,Eric Chénier,Vincent Le Chenadec*

Main category: physics.comp-ph

TL;DR: Space-time extension of conservative Cartesian cut-cell finite-volume method for two-phase diffusion in moving geometries, maintaining strict conservation and handling topology changes.


<details>
  <summary>Details</summary>
Motivation: Need to handle moving boundaries in two-phase diffusion problems on fixed Cartesian grids while maintaining conservation properties, especially for applications like phase change and evolving geometries.

Method: Two-fluid approach with separate scalar fields in each phase, using phase-restricted space-time control volumes with geometric moments (swept volumes and apertures) as weights in finite-volume operators to handle moving boundaries and fresh/dead-cell events.

Result: Demonstrates super-linear accuracy in space, robust behavior under repeated topology changes, and strict conservation across strong coefficient jumps and moving interfaces in 2D and 3D tests.

Conclusion: The space-time cut-cell framework provides a conservative building block for multiphase transport in evolving geometries and serves as foundation for free-boundary extensions like Stefan-type phase change.

Abstract: We present a space-time extension of a conservative Cartesian cut-cell finite-volume method for two-phase diffusion in prescribed-motion geometries. The formulation follows a two-fluid approach: one scalar field is solved in each phase with discontinuous material properties, coupled by sharp interface conditions enforcing flux continuity and jump laws. To handle moving boundaries on a fixed Cartesian grid, the discrete balance is written over phase-restricted space-time control volumes, whose geometric moments (swept volumes and apertures) are used as weights in the finite-volume operators. This construction naturally accounts for the creation and destruction of cut cells (fresh/dead-cell events) and yields strict discrete conservation. The resulting scheme retains the algebraic structure of the static cut-cell formulation while incorporating motion through local geometric weights and interface coupling operators. A series of verification and validation tests in two and three dimensions demonstrate super-linear accuracy in space, robust behavior under repeated topology changes and conservation across strong coefficient jumps and moving interfaces. The proposed space-time cut-cell framework provides a conservative building block for multiphase transport in evolving geometries and a foundation for future free-boundary extensions such as Stefan-type phase change.

</details>


### [55] [PINNs for Electromagnetic Wave Propagation](https://arxiv.org/abs/2512.23396)
*Nilufer K. Bulut*

Main category: physics.comp-ph

TL;DR: Hybrid training strategies enable PINNs to achieve FDTD-level accuracy and energy conservation in electromagnetic wave propagation problems.


<details>
  <summary>Details</summary>
Motivation: While PINNs offer mesh-free advantages for solving PDEs, they often fall short of traditional methods like FDTD in accuracy and energy metrics for electromagnetism. This study aims to bridge this gap by developing improved training strategies.

Method: A hybrid methodology combining: 1) time marching with causality-aware weighting to address causality collapse, 2) two-stage interface continuity loss to mitigate discontinuities from time marching, and 3) local Poynting-based regularizer to suppress cumulative energy drift.

Result: Achieved high field accuracy (0.09% NRMSE, 1.01% L² error) and excellent energy conservation (0.024% relative energy mismatch) in 2D PEC cavity scenario. Training used only physics-based losses without labeled data.

Conclusion: PINNs with hybrid training strategies can achieve competitive results with FDTD in canonical electromagnetic problems, making them a viable alternative to traditional methods.

Abstract: Physics-Informed Neural Networks (PINNs) are a methodology that aims to solve physical systems by directly embedding PDE constraints into the neural network training process. In electromagnetism, where well-established methodologies such as FDTD and FEM already exist, new methodologies are expected to provide clear advantages to be accepted. Despite their mesh-free nature and applicability to inverse problems, PINNs can exhibit deficiencies in terms of accuracy and energy metrics when compared to FDTD solutions. This study demonstrates hybrid training strategies can bring PINNs closer to FDTD-level accuracy and energy consistency.
  This study presents a hybrid methodology addressing common challenges in wave propagation scenarios. The causality collapse problem in time-dependent PINN training is addressed via time marching and causality-aware weighting. In order to mitigate the discontinuities that are introduced by time marching, a two-stage interface continuity loss is applied. In order to suppress loss accumulation, which is manifested as cumulative energy drift in electromagnetic waves, a local Poynting-based regularizer has been developed.
  In the developed PINN model, high field accuracy is achieved with an average 0.09\% $NRMSE$ and 1.01\% $L^2$ error over time. Energy conservation is achieved on the PINN side with only a 0.024\% relative energy mismatch in the 2D PEC cavity scenario. Training is performed without labeled field data, using only physics-based residual losses; FDTD is used solely for post-training evaluation. The results demonstrate that PINNs can achieve competitive results with FDTD in canonical electromagnetic examples and are a viable alternative.

</details>


### [56] [MultiAtomLiouvilleEquationGenerator: A Mathematica package for Liouville superoperators and master equations of multilevel atomic systems](https://arxiv.org/abs/2512.23591)
*Pablo Yanes-Thomas,Rocío Jáuregui-Renaud Santiago F. Caballero-Benítez,Daniel Sahagún Sánchez,Alejandro Kunold*

Main category: physics.comp-ph

TL;DR: MulAtoLEG is an open-source Mathematica package for generating Liouville superoperators and equations for multi-atom, multi-level atomic systems, supporting complex transition configurations and dressed-state basis calculations.


<details>
  <summary>Details</summary>
Motivation: There's a need for computational tools to handle complex multilevel atomic systems with arbitrary numbers of atoms, particularly for alkali atoms with intricate transition configurations. Existing approaches for two-level emitters need extension to multilevel systems.

Method: The package extends Lehmberg's adjoint master equation approach for two-level emitters to multilevel atomic systems, implementing exact equation generation without approximations. It leverages Mathematica's vectorization and sparse linear algebra for efficiency, and can work in both bare and dressed-state bases.

Result: MulAtoLEG successfully generates exact Liouville superoperators and equations for arbitrary multilevel atomic systems, handles complex transition configurations in alkali atoms, and can produce master/adjoint master equations for general Hamiltonians and Lindbladians.

Conclusion: MulAtoLEG provides a valuable computational tool for quantum optics and atomic physics researchers working with multilevel atomic systems, offering exact equation generation with computational efficiency limited only by available resources.

Abstract: MulAtoLEG (Multi-Atom Liouville Equation Generator) is an open-source Mathematica package for generating Liouville superoperators and Liouville equations, specialized for multilevel atomic systems comprising an arbitrary number of atoms. This scheme is based on an extension to multilevel atomic systems, originally developed by Lehmberg [R. H. Lehmberg, Phys. Rev. A 2, 883 (1970)] as an adjoint master equation for ensembles of two-level emitters and later reformulated by Genes [M. Reitz, C. Sommer and C. Genes, PRX Quantum 3, 010201 (2022)] as a master equation. The package facilitates the generation of equations for complex transition configurations in alkali atoms. Although primarily designed for atomic systems, it can also generate the master and adjoint master equations for general Hamiltonians and Lindbladians. In addition, it includes functionalities to construct the differential equations in the dressed-state basis, where, in many cases, the non-unitary evolution operator can be determined explicitly. To maximize computational efficiency, the package leverages Mathematica's vectorization and sparse linear algebra capabilities. Since MulAtoLEG produces exact equations without approximations, the feasible system size is naturally limited by the available computational resources.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [57] [On the accessibility of stable reactor operating regimes in quasi-symmetric stellarators](https://arxiv.org/abs/2512.22355)
*Adelle M. Wright,Benjamin J. Faber*

Main category: physics.plasm-ph

TL;DR: Stellarator fusion reactors aim for high-field, low-pressure operation to avoid instabilities, but simulations reveal an abrupt transition to highly deleterious transport at low plasma beta, challenging current stellarator design strategies.


<details>
  <summary>Details</summary>
Motivation: To achieve sustained burning plasma conditions for fusion energy, maximizing particle and energy confinement is crucial. For stellarator reactors, operating at high magnetic field but low plasma pressure is proposed to avoid destructive instabilities.

Method: Used state-of-the-art high-fidelity macro- and microscopic simulation tools to investigate low-beta regime accessibility in a reactor-scale quasi-axisymmetric stellarator. Considered configuration with flattened core pressure profile and favorable macroscopic/neoclassical properties, then performed linear and nonlinear calculations with the GENE code.

Result: Despite favorable macroscopic and neoclassical properties, linear and nonlinear calculations show an abrupt transition to a regime of highly deleterious transport at low (local) plasma beta, revealing unexpected confinement challenges.

Conclusion: The discovery of abrupt transport degradation at low beta has significant implications for stellarator optimization and highlights important impacts on quasi-symmetric stellarator design strategies, challenging the assumption that low-beta operation would be straightforward.

Abstract: Maximising particle and energy confinement is crucial for achieving the sustained burning plasma conditions necessary to realise fusion energy. For stellarator reactors, one proposed strategy for avoiding destructive instabilities is to operate at high-field but low(er) plasma pressure. In this work, we investigate the accessibility of such a reactor-relevant low-beta regime in a reactor-scale quasi-axisymmetric stellarator using state-of-the-art high-fidelity macro- and microscopic simulation tools. We consider a configuration with a flattened core pressure profile and favourable properties from the macroscopic and neoclassical perspectives. By contrast, linear and nonlinear calculations with the GENE code show an abrupt transition to a regime of highly deleterious transport at low (local) plasma beta. We describe the characterisation of these transport regimes as well as the confinement transition. We discuss the implications broadly for stellarator optimisation and highlight the impact on quasi-symmetric stellarator design strategies.

</details>


### [58] [Impact of fast ions on turbulent transport in high-\b{eta} HL-2A scenarios](https://arxiv.org/abs/2512.22611)
*Jingchun Li,Zhaoyang Lu,Jianqiang Xu,Wei Chen,Jiaqi Dong,Jingting Luo,Yong Liu*

Main category: physics.plasm-ph

TL;DR: Fast-ion pressure gradients in high-beta plasmas can either stabilize ITG turbulence via thermal-ion dilution or destabilize fast-ion-driven BAEs, with nonlinear effects showing dual transport regulation via zonal flow shear.


<details>
  <summary>Details</summary>
Motivation: To understand the complex role of fast ions in regulating turbulent transport in magnetic confinement fusion plasmas, particularly in high-beta scenarios relevant for fusion performance.

Method: Gyrokinetic simulations including linear analyses and nonlinear simulations, examining mode structures, frequencies, and transport dynamics with varying fast-ion pressure gradients and electron beta values.

Result: Fast ions strongly stabilize ITG modes via thermal-ion dilution but have minimal effect on TEMs. At high fast-ion pressure gradients, transition to fast-ion-driven BAEs occurs. Nonlinear simulations show moderate fast-ion pressure suppresses transport via zonal flow shear, while strong fast-ion drive weakens zonal flows and enhances transport by destabilizing FI-BAEs.

Conclusion: Fast ions play a dual role in turbulence regulation - stabilizing ITG modes but potentially destabilizing FI-BAEs at high pressure gradients, offering insights into multiscale transport physics for high-performance fusion plasmas.

Abstract: The fast-ion (FI) on turbulent transport is one of the key topics of magnetic confinement fusion. This work focus on the impact of FI pressure gradients on turbulence in a high-\b{eta} plasma scenario using gyrokinetic simulations. Linear analyses reveal that FIs strongly stabilize ion temperature gradient (ITG) modes via the thermal-ion dilution, while their influence on trapped electron modes (TEMs) is minimal. At elevated FI pressure gradients, a transition to a FI-driven BAE (FI-BAE) regime occurs, as evidenced by mode structure and frequency alignment within the Alfvénic gap. Electron \b{eta} scans further demonstrate the emergence of kinetic ballooning modes (KBMs) at higher \b{eta}, whereas an ITG-TEM hybrid turbulence dominates near experimental \b{eta} values. Nonlinear simulations show that moderate FI pressure suppresses transport via zonal flow (ZF) shear, whereas strong FI drive weakens ZFs and enhances transport by destabilizing FI-BAEs. These results highlight the dual role of FIs in regulating turbulence and offer insight into multiscale transport physics relevant for high-performance plasmas.

</details>


### [59] [Treatment of sunflower seeds by cold atmospheric plasma enhances their tolerance to water stress during germination and early seedling development](https://arxiv.org/abs/2512.23034)
*L. Taras,C. Bailly,T. Dufour*

Main category: physics.plasm-ph

TL;DR: Cold atmospheric plasma treatment improves sunflower seed germination and seedling development under water stress conditions.


<details>
  <summary>Details</summary>
Motivation: To investigate how ambient air plasma treatment affects sunflower seed germination and early seedling development under water stress conditions, particularly focusing on improving germination rates in dormant and non-dormant genotypes.

Method: Used cold atmospheric plasma (CAP) generated in a dielectric barrier device with optical emission spectroscopy and mass spectrometry to detect excited molecular nitrogen and ozone. Emphasized the importance of gap accuracy for effective seed-plasma interaction. Conducted greenhouse experiments to test effects on germination and seedling development under water stress.

Result: CAP significantly improved germination rates of both dormant and non-dormant sunflower genotypes under water stress, alleviating seed dormancy and improving germination under suboptimal conditions. Plasma treatment also stimulated seedling development under water stress in greenhouse experiments.

Conclusion: CAP treatment shows potential as an effective approach to promote the entire emergence process of crop species, particularly under challenging environmental conditions like water stress.

Abstract: The aim of this study was to investigate the impact of ambient air plasma treatment on both sunflower seed germination and early steps of seedling development under water stress. Dry seeds were exposed to a cold atmospheric plasma (CAP) generated in a dielectric barrier device where excited molecular nitrogen and ozone were detected by optical emission spectroscopy and mass spectrometry respectively. Interestingly, we explain the crucial role of gap's accuracy when treating seeds with CAP, especially to improve interaction between seeds and plasma and therefore ensure an efficient treatment. CAP significantly improved the germination rates of seeds of dormant and non-dormant sunflower genotypes under water stress, demonstrating its efficiency in alleviating seed dormancy and in improving germination under suboptimal conditions. Furthermore, greenhouse experiments demonstrated that plasma treatment also stimulated seedling development under water stress conditions. These findings highlight the potential of CAP treatment as an effective approach to promote the whole process of emergence of crop species.

</details>


### [60] [Thermodynamically Consistent Vibrational-Electron Heating: Generalized Model for Multi-Quantum Transitions](https://arxiv.org/abs/2512.23072)
*Bernard Parent,Felipe Martin Rodriguez Fuented*

Main category: physics.plasm-ph

TL;DR: Generalized thermodynamically consistent model for electron-vibrational heating including multi-quantum transitions, correcting systematic errors in previous single-quantum models.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of electron temperature is critical for non-equilibrium plasma applications. Previous single-quantum transition models were limited to low-temperature regimes (Te ≲ 1.5 eV) and neglected hot-band transitions, causing systematic heating errors that prevent thermal relaxation.

Method: Generalized the thermodynamically consistent model to include multi-quantum overtone transitions. Derived formulation where total heating rate is summation of channel-specific cooling rates Qe-v^(m) for each quantum jump m, scaled by thermodynamic factor exp(mθv/Te - mθv/Tv).

Result: Previous models neglecting hot-band transitions incur systematic heating error of exp(-θv/Tv), exceeding 40% when Tv > θv. The generalized model extends applicability to high-energy regimes while preserving thermodynamic consistency by ensuring zero net energy transfer at equilibrium.

Conclusion: The generalized multi-quantum transition model corrects systematic errors in electron-vibrational heating predictions, enabling accurate temperature predictions across broader temperature ranges for non-equilibrium plasma applications.

Abstract: Accurate prediction of electron temperature ($T_{\rm e}$) is critical for non-equilibrium plasma applications ranging from hypersonic flight to plasma-assisted combustion. We recently proposed a thermodynamically consistent model for vibrational-electron heating [Phys. Fluids 37, 096141 (2025)] that enforces the convergence of $T_{\rm e}$ to the vibrational temperature ($T_{\rm v}$) at equilibrium. However, the original derivation was restricted to single-quantum transitions, limiting its validity to low-temperature regimes ($T_{\rm e} \lesssim 1.5$ eV). In this Letter, we generalize the model to include multi-quantum overtone transitions, extending its applicability to high-energy regimes. We demonstrate that previous models neglecting hot-band transitions incur a systematic heating error of $\exp(-θ_{\rm v}/T_{\rm v})$, where $θ_{\rm v}$ is the characteristic vibrational temperature. This error exceeds 40\% when $T_{\rm v}$ is greater than $θ_{\rm v}$, effectively preventing thermal relaxation. To correct this, we derive a formulation where the total heating rate is a summation of channel-specific cooling rates $Q_{\rm e-v}^{(m)}$, each associated with a quantum jump $m$, scaled by a thermodynamic factor $\exp(mθ_{\rm v}/T_{\rm e}-mθ_{\rm v}/T_{\rm v})$. This generalized model preserves thermodynamic consistency by ensuring zero net energy transfer at equilibrium.

</details>


### [61] [Breaking seed dormancy in Mediterranean Brassica rapa wild populations: is cold plasma treatment efficient?](https://arxiv.org/abs/2512.23114)
*M. H. Wagner,T. Dufour,A. Geraci,E. Oddo,G. R. Tarantino,F. Scafidi,C. Bailly,H. Hadj Arab,B. Boucenna,M. Tiret,C. Falentin,A. Dupont,S. Ducournau,A. M. Chèvre*

Main category: physics.plasm-ph

TL;DR: Cold plasma treatment effectively breaks dormancy in wild turnip seeds, improving germination rates and speed, but doesn't significantly affect seedling emergence.


<details>
  <summary>Details</summary>
Motivation: To understand dormancy and germination traits in wild Brassica rapa populations across the Mediterranean region, and compare the effectiveness of different dormancy-breaking methods (gibberellic acid, scarification, cold plasma).

Method: Studied 61 wild Brassica rapa populations from Mediterranean region; compared three dormancy-breaking methods (gibberellic acid, scarification, cold plasma); evaluated germination ability, T10, mean germination time, and greenhouse emergence; used histological and SEM analysis to examine seed coat differences.

Result: Cold plasma was most effective: increased germination from 18% to 60% after 5 days, reduced germination time by 24 hours, and shortened mean germination time. However, seedling emergence remained around 55% for both treated and untreated seeds. Seed coat structure varied by geographical origin, with Sicilian populations showing deeper dormancy.

Conclusion: Cold plasma effectively alleviates embryo dormancy in wild turnip seeds, but geographical origin influences dormancy depth through seed coat characteristics. The treatment improves germination but not subsequent seedling establishment.

Abstract: Turnip (Brassica rapa) is a native species of the Mediterranean area, spread from northwest France to south Algeria. In this study, dormancy and germination traits were assessed for 61 wild Brassica rapa populations collected across the Mediterranean region. Seed dormancy is a key factor influencing germination and seedling establishment. Three dormancy-breaking methods were compared: gibberellic acid, scarification and cold plasma. The efficiency and selectivity were evaluated through germination ability, time to 10% germination (T10), mean germination time and greenhouse emergence. Five days after imbibition, germination was only 18% for the untreated seeds but 60% for the plasma-treated seeds. Germination also began 24 hours earlier and mean germination time was reduced across most populations. However, there was a limited effect on seedling emergence, which remained around 55% for both untreated and treated samples. Comparative analysis indicates that cold plasma was more effective in alleviating embryo dormancy. In addition, histological and scanning electron microscopy showed that the seed coat differed according to the geographical origin of the populations, with a deeper dormancy in seeds from Sicilian populations.

</details>


### [62] [Axisymmetric magnetic field effects on hollow cathode generated plasma column in APPEL-device](https://arxiv.org/abs/2512.23230)
*Y. Patil,S. K. Karkari*

Main category: physics.plasm-ph

TL;DR: Hollow cathode discharge creates elongated plasma column in magnetic field, with energetic electron confinement guiding plasma along axis.


<details>
  <summary>Details</summary>
Motivation: To understand how energetic electrons behave in magnetically guided plasma columns and investigate the role of electron confinement in plasma propagation.

Method: Experimental generation of plasma using hollow cathode discharge in linear device with axisymmetric magnetic field, combined with COMSOL Multiphysics fluid simulations for validation.

Result: Plasma column successfully generated with energetic electrons concentrated peripherally near source, converging toward axis 3.0m downstream. Column length inversely related to electron-neutral collision frequency.

Conclusion: Experimental and simulation results confirm theoretical model, showing collisional damping significantly affects energetic electron propagation in magnetically guided plasma columns.

Abstract: An elongated plasma column has been successfully generated and sustained in a linear plasma device using a hollow cathode discharge in the presence of an axisymmetric magnetic field. The confinement of cold energetic electrons produced near the hollow cathode plays a crucial role in guiding the plasma along the device axis. Experimental diagnostics reveal a high concentration of energetic electrons in the peripheral region near the source, which progressively converge toward the axis at a downstream location approximately 3.0 meters from the cathode. The length of the plasma column exhibits an inverse relationship with the electron-neutral collision frequency, indicating the significance of collisional damping in the propagation of energetic electrons. These observations are further supported by fluid simulations performed using COMSOL Multiphysics, which qualitatively reproduce the experimental trends. The results are consistent with a theoretical model previously proposed by the authors, reinforcing the understanding of energetic electron behaviour in magnetically guided plasma columns.

</details>


### [63] [Ab initio recombination in the expanding ultracold plasmas](https://arxiv.org/abs/2512.23433)
*Yurii V. Dumin,Ludmila M. Svirskaya*

Main category: physics.plasm-ph

TL;DR: First successful ab initio simulation of recombination in ultracold plasmas without auxiliary assumptions, achieving 20% recombination efficiency matching experimental data.


<details>
  <summary>Details</summary>
Motivation: Existing simulations of recombination in ultracold plasmas face challenges due to huge spatial/temporal scale differences between free and bound electron motion, requiring artificial criteria to identify recombination events.

Method: Developed special algorithm using: (1) scalable reference frame co-moving with expanding plasma, (2) dynamic choice of mirror cells for Coulomb sums, (3) accurate treatment of singular interparticle interactions without truncation or softening of Coulomb forces.

Result: Successfully identified real recombination events via sharp equidistant peaks in kinetic/potential energies from captured electrons passing near pericenters, achieving 20% recombination efficiency matching experimental measurements.

Conclusion: First ab initio simulation successfully traces formation of real electron-ion pairs in ultracold plasmas without auxiliary assumptions, validating both experimental measurements and earlier semi-empirical simulations.

Abstract: The efficiency of recombination is of crucial importance for the existence of ultracold plasmas, particularly, the ones formed in the magneto-optical traps. Unfortunately, a straightforward simulation of the recombination encounters the problem of huge difference in the spatial and temporal scales for free and bound motion of the electrons. As a result, only the "virtual" electron-ion pairs are usually reproduced in such simulations, and it is necessary to employ some additional criteria to identify them with the recombined atoms (this might be a minimal number of revolutions of the electron about the nearest ion or a maximal distance between them). It is the aim of this paper to present the first successful ab initio simulation of the recombination without any auxiliary assumptions. We employed a special algorithm, which was based on: (i) using the "scalable" reference frame, co-moving with the expanding plasma, (ii) dynamical choice of the number of "mirror" cells, taking into account in calculation of the Coulomb sums, and (iii) accurate treatment of the singular interparticle interactions, without any truncation or "softening" of the Coulomb forces. Then, the recombination events are identified by a series of sharp equidistant peaks in the kinetic and/or potential energies for a sample of particles, which are caused by the captured electrons passing near the pericenters of their orbits; and this is confirmed by a detailed inspection of the particle trajectories. Thereby, we were able to trace formation of the real - rather than "virtual" - electron-ion pairs. The total efficiency of recombination for the realistic experimental conditions was found to be about 20%, which is in perfect agreement both with the laboratory measurements and with the earlier semi-empirical simulations.

</details>


### [64] [Predicting core transport in ITER baseline discharges with neon injections](https://arxiv.org/abs/2512.23682)
*Dmitri M Orlov,Joseph McClenaghan,Jeff Candy,Jeremy D Lore,Nathan T Howard,Francesco Sciortino,Christopher Holland*

Main category: physics.plasm-ph

TL;DR: ITER performance predictions require integrated core transport and divertor modeling with impurities. Study finds narrow compatibility window (Z_eff ~1.6-1.75, auxiliary heating 75-100% of nominal) where core predictions align with neon-seeded divertor protection targets.


<details>
  <summary>Details</summary>
Motivation: Achieving self-consistent performance predictions for ITER requires integrated modeling of core transport and divertor power exhaust under realistic impurity conditions, particularly for impurity control and auxiliary-heating scheduling in early ITER operation.

Method: Used OMFIT STEP workflow with TGYRO for stationary temperature/density profile predictions across Z_eff range (1.5-2.5), evaluated power crossing separatrix (P_sep), compared with SOLPS-ITER neon-seeded divertor solutions, conducted rotation-sensitivity studies, and performed AURORA modeling for charge-exchange radiation analysis.

Result: P_sep varies by factor >1.7 across Z_eff scan, matches SOLPS-ITER prediction (~100 MW) when Z_eff ≈ 1.6 or auxiliary heating reduced to ~75% of nominal. Rotation variations modify P_sep by ≤20%. Charge-exchange radiation negligible under predicted ITER neutral densities. Identified restricted compatibility window: Z_eff ≈ 1.6-1.75 and auxiliary heating factor 0.75-1.0.

Conclusion: The study provides a self-consistent, model-constrained framework for impurity control and auxiliary-heating scheduling in early ITER operation, identifying a narrow parameter window where core transport predictions align with divertor protection targets, supporting future whole-device scenario optimization.

Abstract: Achieving self-consistent performance predictions for ITER requires integrated modeling of core transport and divertor power exhaust under realistic impurity conditions. We present results from the first systematic power-flow and impurity-content study for the ITER 15 MA baseline scenario constrained directly by existing SOLPS-ITER neon-seeded divertor solutions. Using the OMFIT STEP workflow, stationary temperature and density profiles are predicted with TGYRO for $1.5 \le Z_{\rm eff} \le 2.5$, and the corresponding power crossing the separatrix $P_{\rm sep}$ is evaluated. We find that $P_{\rm sep}$ varies by more than a factor of 1.7 across this scan and matches the $\sim 100$~MW SOLPS-ITER prediction when $Z_{\rm eff} \simeq 1.6$ or when auxiliary heating is reduced to $\sim 75\%$ of nominal. Rotation-sensitivity studies show that plausible variations in toroidal flow magnitude modify $P_{\rm sep}$ by $\lesssim 20\%$, while AURORA modeling confirms that charge-exchange radiation inside the separatrix is dynamically negligible under predicted ITER neutral densities. These results identify a restricted compatibility window, $Z_{\rm eff} \approx 1.6$--1.75 and $0.75 \lesssim f_{P_{\rm aux}} \le 1.0$, in which core transport predictions remain aligned with neon-seeded divertor protection targets. This self-consistent, model-constrained framework provides actionable guidance for impurity control and auxiliary-heating scheduling in early ITER operation and supports future whole-device scenario optimization.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [65] [A first-order method for nonconvex-strongly-concave constrained minimax optimization](https://arxiv.org/abs/2512.22909)
*Zhaosong Lu,Sanyou Mei*

Main category: math.OC

TL;DR: Proposed first-order augmented Lagrangian method for nonconvex-strongly-concave constrained minimax problems with improved O(ε^{-3.5}logε^{-1}) operation complexity.


<details>
  <summary>Details</summary>
Motivation: Address constrained minimax problems with nonconvex-strongly-concave structure, aiming to improve computational efficiency over existing methods.

Method: First-order augmented Lagrangian method with subproblems solved by specialized first-order method leveraging strong concavity structure.

Result: Achieves O(ε^{-3.5}logε^{-1}) operation complexity for finding ε-KKT solution, improving previous best by factor of ε^{-0.5}.

Conclusion: Proposed method provides significant computational improvement for constrained nonconvex-strongly-concave minimax problems.

Abstract: In this paper we study a nonconvex-strongly-concave constrained minimax problem. Specifically, we propose a first-order augmented Lagrangian method for solving it, whose subproblems are nonconvex-strongly-concave unconstrained minimax problems and suitably solved by a first-order method developed in this paper that leverages the strong concavity structure. Under suitable assumptions, the proposed method achieves an \emph{operation complexity} of $O(\varepsilon^{-3.5}\log\varepsilon^{-1})$, measured in terms of its fundamental operations, for finding an $\varepsilon$-KKT solution of the constrained minimax problem, which improves the previous best-known operation complexity by a factor of $\varepsilon^{-0.5}$.

</details>


### [66] [Small-time approximate controllability for the nonlinear complex Ginzburg-Landau equation with bilinear control](https://arxiv.org/abs/2512.22512)
*Xingwu Zeng,Can Zhang*

Main category: math.OC

TL;DR: The paper proves small-time global controllability for the complex Ginzburg-Landau equation with power-type nonlinearity on a torus using a multiplicative geometric control approach.


<details>
  <summary>Details</summary>
Motivation: To establish controllability results for the complex Ginzburg-Landau equation, which is a fundamental model in physics describing pattern formation and phase transitions, particularly in superconductivity and nonlinear optics.

Method: Develops a multiplicative version of the geometric control approach introduced by Agrachev and Sarychev, applied to the CGL equation with power-type nonlinearity on a torus of arbitrary dimension.

Result: Shows small-time global controllability of the CGL equation under a saturation hypothesis on the control operator, proving that the system can be steered between any states in arbitrarily small time.

Conclusion: The multiplicative geometric control method successfully establishes controllability for the complex Ginzburg-Landau equation with power-type nonlinearities, extending previous control theory results to this important physical model.

Abstract: In this paper, we consider the bilinear approximate controllability for the complex Ginzburg-Landau (CGL) equation with a power-type nonlinearity of any integer degree on a torus of arbitrary space dimension. Under a saturation hypothesis on the control operator, we show the small-time global controllability of the CGL equation. The proof is obtained by developing a multiplicative version of a geometric control approach, introduced by Agrachev and Sarychev in \cite{AS05,AS06}.

</details>


### [67] [Small-time global controllability of a class of bilinear fourth-order parabolic equations](https://arxiv.org/abs/2512.23339)
*Subrata Majumdar,Debanjit Mondal*

Main category: math.OC

TL;DR: Fourth-order nonlinear parabolic equations with bilinear controls achieve small-time global approximate controllability between same-sign states using three scalar controls, and exact controllability to non-zero constant states.


<details>
  <summary>Details</summary>
Motivation: To investigate controllability properties of fourth-order nonlinear parabolic equations with bilinear controls on one-dimensional torus, where controls depend only on time and act through prescribed spatial profiles.

Method: 1) For approximate controllability: adapt geometric control approach to fourth-order setting using finite family of frequency-localized controls. 2) For exact controllability: analyze null controllability of appropriate linearized fourth-order system, then use fixed-point argument with approximate control property.

Result: 1) Small-time global approximate controllability between states sharing same sign using three scalar controls. 2) Small-time global exact controllability to non-zero constant states.

Conclusion: The paper establishes both approximate and exact controllability results for fourth-order nonlinear parabolic equations with bilinear controls, demonstrating effective control strategies for this class of higher-order systems.

Abstract: In this work, we investigate the small-time global controllability properties of a class of fourth-order nonlinear parabolic equations driven by a bilinear control posed on the one-dimensional torus. The controls depend only on time and act through a prescribed family of spatial profiles. Our first result establishes the small-time global approximate controllability of the system using three scalar controls, between states that share the same sign. This property is obtained by adapting the geometric control approach to the fourth-order setting, using a finite family of frequency-localized controls. We then study the small-time global exact controllability to non-zero constant states for the concerned system. This second result is achieved by analyzing the null controllability of an appropriate linearized fourth-order system and by deducing the controllability of the nonlinear bilinear model through a fixed-point argument together with the small-time global approximate control property.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [68] [Volume and Surface Area of two Orthogonal, Partially Intersecting Cylinders: A Generalization of the Steinmetz Solid](https://arxiv.org/abs/2512.22555)
*Fynn Jerome Aschmoneit,Bastiaan Cockx*

Main category: cs.CE

TL;DR: Exact integral expressions and empirical approximations for volume and surface area of intersecting orthogonal cylinders with arbitrary depth ratios, validated against simulations.


<details>
  <summary>Details</summary>
Motivation: The intersection of orthogonal cylinders is important for engineering design, manufacturing, and numerical simulation. While analytical solutions exist for fully intersecting cylinders (Steinmetz solid), partial intersections with arbitrary depth ratios require numerical methods or approximations.

Method: Develops general integral expressions for intersection volume and surface area as explicit functions of intersection depth. Also provides empirical approximation functions for closed-form evaluation. Validates against Quasi-Monte Carlo simulation.

Result: Exact formulations for intersection volume and surface area, plus empirical approximations with relative errors below 15% across the full range of intersection depth. Validation confirms accuracy of both analytical and approximate solutions.

Conclusion: Provides both exact integral expressions and practical approximation functions for calculating volume and surface area of intersecting orthogonal cylinders with arbitrary intersection depths, bridging the gap between analytical solutions and numerical methods.

Abstract: The intersection of two orthogonal cylinders represents a classical problem in computational geometry with direct applications to engineering design, manufacturing, and numerical simulation. While analytical solutions exist for the fully intersecting case, the Steinmetz solid, partial intersections with arbitrary depth ratios require numerical methods or approximations. This work presents general integral expressions for both the intersection volume and surface area as explicit functions of the intersection depth. Accompanying these exact formulations are empirical approximation functions, which provide closed-form evaluations with relative errors below 15% across the full range of intersection depth. Validation against Quasi-Monte Carlo simulation confirms the accuracy of both the analytical and approximate solutions.

</details>


### [69] [A Domain Decomposition-based Solver for Acoustic Wave propagation in Two-Dimensional Random Media](https://arxiv.org/abs/2512.23027)
*Sudhi Sharma Padillath Vasudevan*

Main category: cs.CE

TL;DR: Sampling-free intrusive stochastic Galerkin method solves acoustic wave propagation with log-normal random wave speed using PCE, transformed to deterministic PDEs, solved efficiently with domain decomposition and two-level Neumann-Neumann preconditioned conjugate gradient.


<details>
  <summary>Details</summary>
Motivation: To solve acoustic wave propagation problems with uncertain wave speed (modeled as log-normal random field) efficiently, avoiding the computational cost issues from sampling methods when dealing with increasing mesh size, time steps, and random parameters.

Method: Intrusive stochastic Galerkin approach with polynomial chaos expansion (PCE) transforms stochastic PDE into deterministic PDEs, then to linear system. Uses domain decomposition-based solvers with conjugate gradient iterative solver and two-level Neumann-Neumann preconditioner for computational efficiency.

Result: The method demonstrates efficient scalability for handling the computational challenges of large-scale stochastic wave propagation problems with uncertain parameters.

Conclusion: The proposed sampling-free intrusive stochastic Galerkin approach with domain decomposition and two-level preconditioning provides an efficient and scalable solution for acoustic wave propagation with random field wave speed approximation.

Abstract: An acoustic wave propagation problem with a log normal random field approximation for wave speed is solved using a sampling-free intrusive stochastic Galerkin approach. The stochastic partial differential equation with the inputs and outputs expanded using polynomial chaos expansion (PCE) is transformed into a set of deterministic PDEs and further to a system of linear equations. Domain decomposition (DD)-based solvers are utilized to handle the overwhelming computational cost for the resulting system with increasing mesh size, time step and number of random parameters. A conjugate gradient iterative solver with a two-level Neumann-Neumann preconditioner is applied here showing their efficient scalabilities.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [70] [Variational quantum eigensolver for chemical molecules](https://arxiv.org/abs/2512.22572)
*Luca Ion,Adam Smith*

Main category: quant-ph

TL;DR: Quantum computing approach using VQE to compute ground states and energies for He-H+ and H2O molecules, benchmarked against classical exact results.


<details>
  <summary>Details</summary>
Motivation: Solving interacting multi-particle systems is fundamental in quantum chemistry and condensed matter physics, but computationally challenging for classical methods.

Method: Variational Quantum Eigensolver (VQE) implemented on both quantum computer simulator and IBM quantum device, with H2O simulations run on Nottingham's HPC facilities.

Result: Ground states and ground-state energies computed for He-H+ and H2O molecules, with results benchmarked against exact classical ground-state energies.

Conclusion: Quantum computing techniques (VQE) can be applied to molecular systems, providing a promising approach for quantum chemistry calculations with validation against classical benchmarks.

Abstract: Solving interacting multi-particle systems is a central challenge in quantum chemistry and condensed matter physics. In this work, we investigate the computation of ground states and ground-state energies for the He-H+ and H2O molecules using quantum computing techniques. We employ the variational quantum eigensolver (VQE), implemented both on a quantum computer simulator and on an IBM quantum device. The resulting energies are benchmarked against exact ground-state energies obtained via classical methods. Simulations of the H2O molecule were performed on Nottingham's High Performance Computing (HPC) facilities.

</details>


<div id='nlin.PS'></div>

# nlin.PS [[Back]](#toc)

### [71] [amangkurat: A Python Library for Symplectic Pseudo-Spectral Solution of the Idealized (1+1)D Nonlinear Klein-Gordon Equation](https://arxiv.org/abs/2512.22635)
*Sandy H. S. Herho,Siti N. Kaban*

Main category: nlin.PS

TL;DR: amangkurat is an open-source Python library for simulating relativistic scalar field dynamics using Fourier pseudo-spectral methods with symplectic time integration, validated across four physical regimes with information-theoretic analysis.


<details>
  <summary>Details</summary>
Motivation: To create a robust, efficient, and open-source computational tool for simulating relativistic scalar field dynamics governed by the nonlinear Klein-Gordon equation, enabling exploratory research and education in nonlinear field theory.

Method: Hybrid computational strategy combining Fourier pseudo-spectral spatial discretization with symplectic Størmer-Verlet temporal integration, featuring adaptive timestepping based on CFL criteria and JIT compilation for parallelized force computation.

Result: The library successfully simulates four canonical physical regimes (dispersive linear waves, static topological kinks, integrable breathers, and non-integrable kink-antikink collisions) and demonstrates statistical distinguishability between these regimes using information-theoretic entropy metrics and phase space analysis.

Conclusion: amangkurat provides an efficient, validated platform for numerical simulation of relativistic scalar field dynamics with high computational performance on standard hardware, making it suitable for both research and educational applications in nonlinear field theory.

Abstract: This study introduces amangkurat, an open-source Python library designed for the robust numerical simulation of relativistic scalar field dynamics governed by the nonlinear Klein-Gordon equation in $(1+1)$D spacetime. The software implements a hybrid computational strategy that couples Fourier pseudo-spectral spatial discretization with a symplectic Størmer-Verlet temporal integrator, ensuring both exponential spatial convergence for smooth solutions and long-term preservation of Hamiltonian structure. To optimize performance, the solver incorporates adaptive timestepping based on Courant-Friedrichs-Lewy (CFL) stability criteria and utilizes Just-In-Time (JIT) compilation for parallelized force computation. The library's capabilities are validated across four canonical physical regimes: dispersive linear wave propagation, static topological kink preservation in phi-fourth theory, integrable breather dynamics in the sine-Gordon model, and non-integrable kink-antikink collisions. Beyond standard numerical validation, this work establishes a multi-faceted analysis framework employing information-theoretic entropy metrics (Shannon, Rényi, and Tsallis), kernel density estimation, and phase space reconstruction to quantify the distinct phenomenological signatures of these regimes. Statistical hypothesis testing confirms that these scenarios represent statistically distinguishable dynamical populations. Benchmarks on standard workstation hardware demonstrate that the implementation achieves high computational efficiency, making it a viable platform for exploratory research and education in nonlinear field theory.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [72] [Anisotropic Photostriction and Strain-modulated Carrier Lifetimes in Orthorhombic Semiconductors](https://arxiv.org/abs/2512.23187)
*Jianxin Yu,Kun Yang,Jiawen Li,Sheng Meng,Xinghua Shi,Jin Zhang*

Main category: cond-mat.mtrl-sci

TL;DR: Anisotropic photostriction in 2D orthorhombic semiconductors shows lattice expansion along armchair direction and contraction along zigzag direction, with photostriction tuning via carrier density and enhanced carrier lifetimes.


<details>
  <summary>Details</summary>
Motivation: To understand the microscopic origin of anisotropic photostriction in low-dimensional systems and enable light-controllable, directionally sensitive optomechanical devices at atomic scale.

Method: Time-dependent density functional theory (TDDFT) to trace dynamics of photoexcited carriers and establish quantitative link between carrier density and lattice deformation in layered black phosphorus and germanium selenides.

Result: Photostriction exhibits significant anisotropy with lattice expansion along armchair direction and contraction along zigzag direction. Magnitude and orientation can be tuned by photodoping densities. Photoinduced strains increase carrier recombination lifetimes by suppressing nonradiative recombination due to enlarged bandgap and weakened nonadiabatic coupling.

Conclusion: The study provides microscopic insight into anisotropic photostriction in low-dimensional systems and lays groundwork for light-controllable, directionally sensitive optomechanical devices at atomic scale.

Abstract: We demonstrate anisotropic photostriction in two-dimensional orthorhombic semiconductors using time-dependent density functional theory. By tracing the dynamics of photoexcited carriers, we establish a quantitative link between carrier density and lattice deformation in layered black phosphorus and germanium selenides. The structural response exhibits significant anisotropy, featuring lattice expansion along the armchair direction and contraction along the zigzag direction, which is attributed to the interplay between charge redistribution and intrinsic lattice anisotropy. Both the magnitude and orientation of the photostrictive strains can be tuned by photodoping densities, enabling precise control over the photoinduced response. Notably, the photoinduced strains significantly increase carrier recombination lifetimes by suppressing nonradiative recombination, primarily due to the enlarged bandgap and weakened nonadiabatic coupling. These results provide microscopic insight into the origin of anisotropic photostriction in low-dimensional systems and lay the groundwork for light-controllable, directionally sensitive optomechanical devices at the atomic scale.

</details>


### [73] [The Fundamental Lemma of Altermagnetism: Emergence of Alterferrimagnetism](https://arxiv.org/abs/2512.23589)
*Chanchal K. Barman,Bishal Das,Alessio Filippetti,Aftab Alam,Fabio Bernardini*

Main category: cond-mat.mtrl-sci

TL;DR: The paper introduces the Fundamental Lemma of Altermagnetism (FLAM) to define exact conditions for altermagnetic phases based on site-symmetry groups, and proposes a new class of materials called Alterferrimagnets that generalize altermagnetism to include multiple magnetic species.


<details>
  <summary>Details</summary>
Motivation: To address the need for precise mathematical conditions to distinguish altermagnets from collinear antiferromagnets, and to extend the concept beyond traditional ferromagnetic/ferrimagnetic classifications that don't properly handle multiple magnetic species.

Method: Developed the Fundamental Lemma of Altermagnetism (FLAM) using site-symmetry groups and halving subgroups within crystallographic space groups. Applied spin group formalism to analyze magnetic materials and propose alterferrimagnets as a generalization.

Result: Established exact mathematical conditions for altermagnetic phase existence. Proposed and defined alterferrimagnets (AFiMs) as fully compensated ferrimagnets that show alternating momentum-dependent spin-polarized electronic bands, generalizing traditional altermagnetism to multiple magnetic species.

Conclusion: The FLAM provides rigorous criteria for identifying altermagnetic materials, while alterferrimagnets represent a new class of materials that combine features of altermagnetism with multiple magnetic species, potentially enabling new spintronic applications.

Abstract: Recent years have seen a proliferation in investigations on Altermagnetism due to its exciting prospects both from an applications perspective and theoretical standpoint. Traditionally, altermagnets are distinguished from collinear antiferromagnets using the central concept of halving subgroups within the spin space group formalism. In this work, we propose the Fundamental Lemma of Altermagnetism (FLAM) deriving the exact conditions required for the existence of altermagnetic phase in a magnetic material on the basis of site-symmetry groups and halving subgroups for a given crystallographic space group. The spin group formalism further clubs ferrimagnetism with ferromagnetism since the same-spin and opposite-spin sublattices lose their meaning in the presence of multiple magnetic species. As a consequence of FLAM, we further propose a class of fully compensated ferrimagnets, termed as Alterferrimagnets (AFiMs), which can show alternating momentum-dependent spin-polarized non-relativistic electronic bands within the first Brillouin zone. We show that alterferrimagnetism is a generalization of traditional collinear altermagnetism where multiple magnetic species are allowed to coexist forming fully compensated magnetic-sublattices, each with individual up-spin and down-spin sublattices.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [74] [Solving the constraint equation for general free data](https://arxiv.org/abs/2512.22704)
*Xuantao Chen,Sergiu Klainerman*

Main category: gr-qc

TL;DR: New method for solving Einstein constraint equations in vacuum by prescribing four scalar quantities representing dynamical degrees of freedom, enabling construction of large class of exterior solutions for black hole initial data.


<details>
  <summary>Details</summary>
Motivation: To develop a flexible method for constructing initial Cauchy data sets for black holes, generalize Li and Yu's trapped surface formation result, and potentially show sharpness of Shen's decay conditions for Minkowski space stability.

Method: Prescribe four scalar quantities (full dynamical degrees of freedom) modulo ℓ≤1 modes, choose appropriate gauge conditions, rewrite constraints as well-posed system of coupled transport and elliptic equations on 2-spheres, solve via iteration procedure.

Result: Method provides large class of exterior solutions that can be matched to interior solutions using existing gluing techniques, applicable to various decay conditions from O(r^{-1-δ}) to arbitrarily fast decaying data.

Conclusion: Flexible approach enables construction of diverse initial data sets for black holes, potentially demonstrating sharpness of Shen's decay conditions and advancing understanding of Minkowski space stability.

Abstract: We revisit the problem of solving the Einstein constraint equations in vacuum by a new method, which allows us to prescribe four scalar quantities, representing the full dynamical degrees of freedom of the constraint system. We show that once appropriate gauge conditions have been chosen and four scalars freely specified (modulo $\ell\leq 1$ modes), we can rewrite the constraint equations as a well-posed system of coupled transport and elliptic equations on $2$-spheres, which we solve by an iteration procedure. Our method provides a large class of exterior solutions of the constraint equations that can be matched to given interior solutions, according to the existing gluing techniques. As such, it can be applied to provide a large class of initial Cauchy data sets evolving to black holes, generalizing the well-known result of the formation of trapped surfaces due to Li and Yu. Though in our main theorem, we only specify conditions consistent with $g-g_{Schw}=O(r^{-1-δ})$, $k=O(r^{-2-δ})$, the method is flexible enough to be applied in many other situations. It can, in particular, be easily adapted to construct arbitrarily fast decaying data. We expect, moreover, that our method can also be applied to construct data with slower decay, such as that used by Shen. In fact, an important motivation for developing our method is to show that the result of Shen is sharp, i.e., construct small, smooth initial data sets which violate Shen's decay conditions, and for which the stability of the Minkowski space result is wrong.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [75] [Scaling inequalities for Steklov eigenvalues in space forms and sharp eigenvalue estimates on warped product manifolds](https://arxiv.org/abs/2512.22885)
*Zongyi Lv,Changwei Xiong,Yuxun Zou*

Main category: math.DG

TL;DR: This paper analyzes monotonicity properties and sharp bounds for Steklov eigenvalues on geodesic disks and warped product manifolds, with applications to Escobar-type bounds and confirmation of a 2018 conjecture.


<details>
  <summary>Details</summary>
Motivation: The motivation is to establish monotonicity properties of normalized spectra for second-order and fourth-order Steklov problems on geometric domains, and to obtain sharp bounds for eigenvalues on warped product manifolds, addressing open problems in spectral geometry.

Method: The paper uses geometric analysis techniques to derive monotonicity results for normalized spectra on 2D geodesic disks with respect to geodesic radius, employing four natural geometric factors for normalization. For warped product manifolds, the authors apply analytical methods to obtain sharp bounds for fourth-order Steklov eigenvalues.

Result: The authors prove: 1) monotonicity of normalized spectra for Steklov problems on geodesic disks in spheres and hyperbolic space; 2) Escobar-type bounds for Steklov eigenvalues on 2D geodesic disks with varying curvature; 3) monotonicity results for higher-dimensional cases; 4) sharp bounds for fourth-order Steklov spectra on warped product manifolds; 5) confirmation of Wang and Xia's 2018 conjecture for 3D warped product manifolds.

Conclusion: The paper successfully establishes fundamental monotonicity properties and sharp bounds for Steklov eigenvalues on various geometric domains, resolving important conjectures and advancing the understanding of spectral geometry for higher-order Steklov problems.

Abstract: In the first part, we derive monotonicity of the normalized spectra for the second-order Steklov problem and two fourth-order Steklov problems on the $2$-dimensional geodesic disks with respect to the geodesic radius in the sphere and the hyperbolic space. The normalizations are made using four natural geometric factors. As corollaries, we get Escobar-type bounds for Steklov eigenvalues on $2$-dimensional geodesic disks with varying curvature in space forms. We also get two monotonicity results for higher-dimensional cases. In the second part, we obtain some sharp bounds concerning the spectra of the two fourth-order Steklov problems on warped product manifolds with non-negative Ricci curvature and a strictly convex boundary. In particular, we confirm Qiaoling Wang and Changyu Xia's conjecture (2018) on the sharp lower bound of the first non-zero eigenvalue of a fourth-order Steklov problem in the case of $3$-dimensional warped product manifolds.

</details>


### [76] [Rotationally symmetric translating solitons of fully nonlinear extrinsic geometric flows: Classification and Applications](https://arxiv.org/abs/2512.23623)
*José Torres Santaella*

Main category: math.DG

TL;DR: Study of rotationally symmetric translators for nonlinear curvature flows, establishing asymptotics of bowl-type solutions and constructing/classifying catenoidal-type solutions with their asymptotic behavior.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of rotationally symmetric translators in fully nonlinear extrinsic geometric flows driven by curvature functions, which are important in geometric analysis and PDE theory.

Method: Analysis of rotationally symmetric translators using geometric PDE techniques, establishing fine asymptotics for bowl-type evolutions, and constructing/classifying catenoidal-type solutions under structural assumptions.

Result: Established asymptotic behavior of bowl-type solutions, constructed and classified catenoidal-type translators with their asymptotic properties, and proved rigidity/uniqueness results for graphical translators under convexity assumptions.

Conclusion: The paper provides comprehensive classification and asymptotic analysis of rotationally symmetric translators for nonlinear curvature flows, establishing both existence results and uniqueness/rigidity theorems under appropriate conditions.

Abstract: We study rotationally symmetric translators for fully nonlinear extrinsic geometric flows driven by a curvature function, and we establish the fine asymptotics of bowl-type evolutions and, when admissible, the construction and classification of catenoidal-type solutions, together with their asymptotic behavior. Under natural structural and convexity assumptions, we also prove rigidity and uniqueness results within appropriate classes of graphical translators of such curvature flows.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [77] [Electrode Geometry Optimization in Vortex-Type Seawater Magnetohydrodynamic Generators](https://arxiv.org/abs/2512.22446)
*Arleen Natalie,Budiarso,Ridho Irwansyah*

Main category: physics.flu-dyn

TL;DR: Study shows electrode geometry significantly affects seawater MHD generator performance, with whole-area electrodes providing 155% power increase over baseline partial electrodes.


<details>
  <summary>Details</summary>
Motivation: MHD generators offer clean energy conversion from conductive fluids, but electrode design optimization is needed to improve performance and efficiency for practical seawater applications.

Method: Combined analytical and numerical simulations using COMSOL Multiphysics to analyze three electrode designs (partial, whole-area, spiral) focusing on internal resistance, current density distribution, and power output.

Result: Whole-area electrode achieved highest output with 155% power increase over baseline; spiral electrode reduced internal resistance but had lower open-circuit voltage; simulations matched theory with <4% deviation.

Conclusion: Electrode area and spacing are critical performance determinants, and geometric optimization is essential for advancing efficient seawater-based MHD generators as sustainable energy systems.

Abstract: Magnetohydrodynamics (MHD) generators present a promising pathway for clean energy conversion by directly transforming conductive fluids' kinetic energy into electricity. This study investigates the impact of electrode geometry modifications on the performance of a vortex-type seawater MHD generator. Three electrode designs, partial, whole-area, and spiral, are analyzed through combined analytical and numerical simulations using COMSOL Multiphysics. The study focuses on internal resistance reduction, current density distribution, and overall power output. The results indicate that electrode area and spacing are critical determinants of performance. The whole-area electrode achieves the highest output, with a 155 percent increase in power compared to the baseline partial electrode. The spiral electrode demonstrates reduced internal resistance and improved current flow but exhibits lower open-circuit voltage due to reduced electrode spacing. The simulations show strong agreement with theoretical models, with deviations of less than 4 percent in open-circuit voltage predictions. These findings highlight the importance of geometric optimization for advancing seawater-based MHD generators as sustainable and efficient energy conversion systems.

</details>


### [78] [A front-tracking study of retinal detachment treatment by magnetic drop targeting](https://arxiv.org/abs/2512.22537)
*Mohammad Amin Amini,Gretar Tryggvason,Ehsan Amani*

Main category: physics.flu-dyn

TL;DR: A 3D computational study of ferrofluid drop targeting for retinal detachment treatment, incorporating realistic eye geometry, magnetic configurations, and vitreous humor viscoelasticity using an extended front-tracking method.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate computational model for ferrofluid drop targeting (FDT) therapy for retinal detachment that accounts for real 3D eye anatomy, magnetic field configurations, and the viscoelastic properties of vitreous humor.

Method: Extended Front-Tracking Method (FTM) adapted for 3D unstructured Eulerian grids with strong wall effects, including multi-region grid design, threshold distance between front and wall, front smoothing, and volume correction algorithms near walls.

Result: The ratio of drop-to-vitreous humor magnetic permeabilities significantly affects terminal shape parameters like retinal coverage. Increasing both magnetic Bond number and permeability ratio boosts total FDT force, coverage area, and stress concentration, while reducing drop-VH surface tension can mitigate retinal stress.

Conclusion: The developed 3D computational model successfully captures FDT dynamics for retinal detachment treatment, revealing key design parameters that influence therapeutic outcomes including travel time, settling time, coverage area, and retinal stress.

Abstract: We investigate the Ferrofluid Drop Targeting (FDT) for the treatment of the Retinal Detachment (RD), considering, for the first time, the real 3D geometry of an eye and magnets configurations as well as the viscoelastic rheology of the medium, i.e., the Vitreous Humor (VH). A Front-Tracking Method (FTM) is extended to handle a general 3D unstructured Eulerian grid and strong wall effects. The challenges include the accuracy and robustness of the solver when the drop spreads on the retina under the effect of a magnetic field, which necessitates the design of a multi-region Eulerian grid and defining a threshold distance between the front and wall, along with the choice of an effective front smoothing and volume correction FTM sub-algorithms near the walls. After model validations, the effect of different design parameters on important objectives, such as the travel time, settling time, retinal coverage area, and impact compressive stress, are studied. The results reveal that, in addition to the magnetic Bond number, the ratio of the drop-to-VH magnetic permeabilities plays a key role in the terminal shape parameters, like the retinal coverage. Additionally, simultaneously increasing these two parameters, significantly increase the total FDT force, coverage area, and stress concentration, while decreasing the drop-VH surface tension can mitigate the stress concentration on the retina.

</details>


### [79] [On the Reynolds-number scaling of Poisson solver complexity](https://arxiv.org/abs/2512.22644)
*F. Xavier Trias,Àdel Alsalti-Baldellou,Assensi Oliva*

Main category: physics.flu-dyn

TL;DR: The paper analyzes how Poisson equation solver complexity scales with Reynolds number in large incompressible flow simulations, finding decreasing complexity for Navier-Stokes turbulence but increasing complexity for Burgers equation.


<details>
  <summary>Details</summary>
Motivation: To understand whether solving the Poisson equation becomes more or less computationally expensive as Reynolds number increases in large-scale incompressible flow simulations, which is crucial for developing efficient solvers for extreme-scale simulations.

Method: Combines physical and numerical arguments to derive power-law scalings at high Reynolds numbers. Performs theoretical convergence analysis for Jacobi and multigrid solvers, defining a 2D phase space divided into regions based on solver iteration trends with Reynolds number.

Result: Numerical results show that for Navier-Stokes turbulence, solver complexity decreases with increasing Reynolds number, while for the 1D Burgers equation, complexity increases with Reynolds number.

Conclusion: The theoretical framework provides a unified perspective on solver convergence scaling with Reynolds number and offers valuable guidance for developing next-generation preconditioning and multigrid strategies for extreme-scale simulations.

Abstract: We aim to answer the following question: is the complexity of numerically solving the Poisson equation increasing or decreasing for very large simulations of incompressible flows? Physical and numerical arguments are combined to derive power-law scalings at very high Reynolds numbers. A theoretical convergence analysis for both Jacobi and multigrid solvers defines a two-dimensional phase space divided into two regions depending on whether the number of solver iterations tends to decrease or increase with the Reynolds number. Numerical results indicate that, for Navier-Stokes turbulence, the complexity decreases with increasing Reynolds number, whereas for the one-dimensional Burgers equation it follows the opposite trend. The proposed theoretical framework thus provides a unified perspective on how solver convergence scales with the Reynolds number and offers valuable guidance for the development of next-generation preconditioning and multigrid strategies for extreme-scale simulations.

</details>


### [80] [A rational length scale for large-eddy simulation of turbulence on anisotropic grids](https://arxiv.org/abs/2512.22717)
*F. Xavier Trias,Jesús Ruano,Alexey Duben,Andrey Gorobets*

Main category: physics.flu-dyn

TL;DR: Proposes a new subgrid characteristic length scale for LES that reduces mesh anisotropy effects by analyzing numerical discretization-filtering entanglement.


<details>
  <summary>Details</summary>
Motivation: Direct numerical simulation of turbulence is too expensive for real applications, requiring coarse-grained LES models. Current eddy-viscosity models need a subgrid length scale tied to local grid size, but defining this for unstructured or anisotropic meshes (common in near-wall turbulence) remains unresolved and significantly impacts LES performance.

Method: Introduces a novel subgrid characteristic length derived from analyzing the entanglement between numerical discretization and filtering in LES. The approach focuses on mathematical properties and simplicity to create a robust length scale that minimizes mesh anisotropy effects.

Result: Demonstrates effectiveness through simulations of decaying isotropic turbulence and turbulent channel flow using different codes, showing improved performance in handling mesh anisotropies.

Conclusion: The proposed subgrid length scale provides a robust solution for reducing mesh anisotropy effects in LES, addressing a long-standing open problem in turbulence modeling for unstructured and anisotropic meshes commonly used in practical applications.

Abstract: Due to the prohibitive cost of resolving all relevant scales, direct numerical simulations of turbulence remain unfeasible for most real-world applications. Consequently, dynamically simplified formulations are needed for coarse-grained simulations. In this regard, eddy-viscosity models for Large-Eddy Simulation (LES) are widely used both in academia and industry. These models require a subgrid characteristic length, typically linked to the local grid size. While this length scale corresponds to the mesh step for isotropic grids, its definition for unstructured or anisotropic Cartesian meshes, such as the pancake-like meshes commonly used to capture near-wall turbulence or shear layers, remains an open question. Despite its significant influence on LES model performance, no consensus has been reached on its proper formulation. In this work, we introduce a novel subgrid characteristic length. This length scale is derived from the analysis of the entanglement between the numerical discretization and the filtering in LES. Its mathematical properties and simplicity make it a robust choice for reducing the impact of mesh anisotropies on simulation accuracy. The effectiveness of the proposed subgrid length is demonstrated through simulations of decaying isotropic turbulence and a turbulent channel flow using different codes.

</details>


### [81] [An efficient eigenvalue bounding method: CFL condition revisited](https://arxiv.org/abs/2512.22994)
*F. Xavier Trias,Xavier Álvarez-Farré,Àdel Alsalti-Baldellou,Andrey Gorobets,Assensi Oliva*

Main category: physics.flu-dyn

TL;DR: Proposes a new inexpensive method for computing eigenbounds (spectral radii) of convective and diffusive matrices in turbulence simulations without reconstructing time-dependent matrices, enabling larger time-steps and better code portability.


<details>
  <summary>Details</summary>
Motivation: Explicit temporal schemes in turbulence simulations impose very small time-steps due to stability constraints. Current CFL-based approaches require expensive eigenvalue estimations, and modern heterogeneous supercomputing systems hinder code portability. The paper aims to develop a minimal, portable method for eigenbound computation.

Method: Develops an inexpensive method that computes eigenbounds without reconstructing time-dependent convective and diffusive matrices. The method relies only on sparse-matrix vector products where only vectors change over time, making implementation simple and portable across platforms.

Result: The method is demonstrated to be effective and robust for different test cases on both structured Cartesian and unstructured meshes. When combined with a self-adaptive temporal scheme, it leads to significantly larger time-steps compared to conventional CFL-based approaches.

Conclusion: The proposed method provides an efficient, portable solution for eigenbound computation in turbulence simulations, enabling larger time-steps while maintaining minimal implementation requirements for cross-platform compatibility.

Abstract: Direct and large-eddy simulations of turbulence are often solved using explicit temporal schemes. However, this imposes very small time-steps because the eigenvalues of the (linearized) dynamical system, re-scaled by the time-step, must lie inside the stability region. In practice, fast and accurate estimations of the spectral radii of both the discrete convective and diffusive terms are therefore needed. This is virtually always done using the so-called CFL condition. On the other hand, the large heterogeneity and complexity of modern supercomputing systems are nowadays hindering the efficient cross-platform portability of CFD codes. In this regard, our leitmotiv reads: relying on a minimal set of (algebraic) kernels is crucial for code portability and maintenance! In this context, this work focuses on the computation of eigenbounds for the above-mentioned convective and diffusive matrices which are needed to determine the time-step à la CFL. To do so, a new inexpensive method, that does not require to re-construct these time-dependent matrices, is proposed and tested. It just relies on a sparse-matrix vector product where only vectors change on time. Hence, both implementation in existing codes and cross-platform portability are straightforward. The effectiveness and robustness of the method are demonstrated for different test cases on both structured Cartesian and unstructured meshes. Finally, the method is combined with a self-adaptive temporal scheme, leading to significantly larger time-steps compared with other more conventional CFL-based approaches.

</details>


### [82] [Phase-field modeling of multicomponent vesicles in viscoelastic fluid](https://arxiv.org/abs/2512.23315)
*Zuowei Wen,Navid Valizadeh,Timon Rabczuk,Xiaoying Zhuang*

Main category: physics.flu-dyn

TL;DR: Developed a CSF phase-field model to study multicomponent vesicles in viscoelastic fluids using advanced numerical methods including RBVMS, SUPG, and IGA.


<details>
  <summary>Details</summary>
Motivation: Multicomponent vesicles in viscoelastic fluids are important for understanding physiological processes, requiring accurate modeling of their hydrodynamics with inertial forces.

Method: Continuum surface force (CSF) phase-field model coupling fluid field (Newtonian + Oldroyd-B), surface concentration field (Cahn-Hilliard), and membrane evolution (nonlinear advection-diffusion). Uses RBVMS for Navier-Stokes, SUPG for Oldroyd-B, standard Galerkin for other equations, implicit monolithic scheme with generalized-α time integration, and isogeometric analysis for spatial accuracy.

Result: Numerical examples in 2D shear and Poiseuille flows demonstrate the influence of membrane composition and fluid viscoelasticity on vesicle hydrodynamics.

Conclusion: The developed model successfully captures complex hydrodynamics of multicomponent vesicles in viscoelastic flows, providing insights into physiological processes through advanced numerical simulations.

Abstract: Multicomponent vesicles suspended in viscoelastic fluids are crucial for understanding a variety of physiological processes. In this work, we develop a continuum surface force (CSF) phase-field model to investigate the hydrodynamics of inextensible multicomponent vesicles in viscoelastic fluid flows with inertial forces. Our model couples a fluid field comprising both Newtonian and Oldroyd-B fluids, a surface concentration field representing the multicomponent distribution on the vesicle membrane, and a phase-field variable governing the membrane evolution. The viscoelasticity effect of extra stress is well incorporated into the full Navier-Stokes equations in the fluid field. The surface concentration field is determined by Cahn-Hilliard equations, while the membrane evolution is governed by a nonlinear advection-diffusion equation. The membrane is coupled to the surrounding fluid through the continuum surface force (CSF) framework. To ensure stable numerical solutions of the highly nonlinear multi-field model, we employ a residual-based variational multiscale (RBVMS) method for the Navier-Stokes equations, a Streamline-Upwind Petrov-Galerkin (SUPG) method for the Oldroyd-B equations, and a standard Galerkin finite element framework for the remaining equations. The system of PDEs is solved using an implicit, monolithic scheme based on the generalized-$α$ time integration method. To enhance spatial accuracy, we employ isogeometric analysis (IGA). We present a series of two-dimensional numerical examples in shear and Poiseuille flows to elucidate the influence of membrane composition and fluid viscoelasticity on the hydrodynamics of multicomponent vesicles.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [83] [The Open Polymers 2026 (OPoly26) Dataset and Evaluations](https://arxiv.org/abs/2512.23117)
*Daniel S. Levine,Nicholas Liesen,Lauren Chua,James Diffenderfer,Helgi Ingolfsson,Matthew P. Kroonblawd,Nitesh Kumar,Amitesh Maiti,Supun S. Mohottalalage,Muhammed Shuaibi,Brian Van Essen,Brandon M. Wood,C. Lawrence Zitnick,Samuel M. Blau,Evan R. Antoniuk*

Main category: physics.chem-ph

TL;DR: The OPoly26 dataset provides over 6.57 million DFT calculations on polymer-derived clusters to address the lack of polymer data in quantum chemical ML training, improving model performance for polymer property predictions.


<details>
  <summary>Details</summary>
Motivation: Polymers are fundamental to biology and technology, but ML models have been trained primarily on small molecules and materials, not polymers, due to the computational expense of high-quality electronic structure calculations on polymeric systems.

Method: Created the Open Polymers 2026 (OPoly26) dataset containing 6.57 million density functional theory (DFT) calculations on up to 360-atom clusters derived from polymeric systems, capturing chemical diversity including monomer composition, polymerization degree, chain architectures, and solvation environments.

Result: The dataset comprises over 1.2 billion total atoms and shows that augmenting ML model training with OPoly26 improves model performance for polymer prediction tasks.

Conclusion: OPoly26 addresses the polymer data gap in quantum chemical ML, enables better polymer property predictions, and is publicly released to advance ML models for polymers and universal atomistic models.

Abstract: Polymers-macromolecular systems composed of repeating chemical units-constitute the molecular foundation of living organisms, while their synthetic counterparts drive transformative advances across medicine, consumer products, and energy technologies. While machine learning (ML) models have been trained on millions of quantum chemical atomistic simulations for materials and/or small molecular structures to enable efficient, accurate, and transferable predictions of chemical properties, polymers have largely not been included in prior datasets due to the computational expense of high quality electronic structure calculations on representative polymeric structures. Here, we address this shortcoming with the creation of the Open Polymers 2026 (OPoly26) dataset, which contains more than 6.57 million density functional theory (DFT) calculations on up to 360 atom clusters derived from polymeric systems, comprising over 1.2 billion total atoms. OPoly26 captures the chemical diversity that makes polymers intrinsically tunable and versatile materials, encompassing variations in monomer composition, degree of polymerization, chain architectures, and solvation environments. We show that augmenting ML model training with the OPoly26 dataset improves model performance for polymer prediction tasks. We also publicly release the OPoly26 dataset to help further the development of ML models for polymers, and more broadly, strive towards universal atomistic models.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [84] [Inverse scattering for waveguides in topological insulators](https://arxiv.org/abs/2512.22480)
*Guillaume Bal,Xixian Wang,Zhongjian Wang*

Main category: math-ph

TL;DR: Inverse scattering problem for topological waveguide in 2D topological insulators solved with reconstruction from scattering data, stability analysis, and numerical implementation.


<details>
  <summary>Details</summary>
Motivation: To address the inverse scattering problem for topologically non-trivial waveguides in 2D topological insulators, which is important for understanding and characterizing topological materials through scattering measurements.

Method: Uses Dirac system model, linearized reconstruction approach, finite-dimensional setting with smallness constraint, stability analysis in appropriate topologies, and numerical solution via standard adjoint method.

Result: Demonstrates that short-range perturbations can be fully reconstructed from scattering data in both linearized and finite-dimensional settings under smallness constraints, with stability guarantees, and validates through numerical simulations.

Conclusion: The inverse scattering problem for topological waveguides is solvable with theoretical guarantees and practical numerical implementation, enabling characterization of topological materials through scattering measurements.

Abstract: This paper concerns the inverse scattering problem of a topologically non-trivial waveguide separating two-dimensional topological insulators. We consider the specific model of a Dirac system. We show that a short-range perturbation can be fully reconstructed from scattering data in a linearized setting and in a finite-dimensional setting under a smallness constraint. We also provide a stability result in appropriate topologies. We then solve the problem numerically by means of a standard adjoint method and illustrate our theoretical findings with several numerical simulations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [85] [PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations](https://arxiv.org/abs/2512.23056)
*Min Zhu,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer,Lu Lu*

Main category: cs.LG

TL;DR: PI-MFM is a physics-informed multimodal foundation model framework that enforces PDE governing equations during training, enabling data-efficient learning of PDE solution operators across diverse equation families.


<details>
  <summary>Details</summary>
Motivation: Existing multi-operator learning approaches for PDEs are data-hungry and neglect physics during training, limiting their practical application and data efficiency.

Method: PI-MFM takes symbolic PDE representations as input, automatically assembles PDE residual losses via vectorized derivative computation, and enables unified physics-informed training across equation families.

Result: PI-MFM outperforms data-driven counterparts on 13 parametric 1D time-dependent PDE families, especially with sparse data, partial observations, or few labeled pairs. It shows robustness to noise and enables zero-shot fine-tuning to unseen PDE families with only 1% test error.

Conclusion: PI-MFM provides a practical, scalable path toward data-efficient, transferable PDE solvers by integrating physics directly into multimodal foundation model training.

Abstract: Partial differential equations (PDEs) govern a wide range of physical systems, and recent multimodal foundation models have shown promise for learning PDE solution operators across diverse equation families. However, existing multi-operator learning approaches are data-hungry and neglect physics during training. Here, we propose a physics-informed multimodal foundation model (PI-MFM) framework that directly enforces governing equations during pretraining and adaptation. PI-MFM takes symbolic representations of PDEs as the input, and automatically assembles PDE residual losses from the input expression via a vectorized derivative computation. These designs enable any PDE-encoding multimodal foundation model to be trained or adapted with unified physics-informed objectives across equation families. On a benchmark of 13 parametric one-dimensional time-dependent PDE families, PI-MFM consistently outperforms purely data-driven counterparts, especially with sparse labeled spatiotemporal points, partially observed time domains, or few labeled function pairs. Physics losses further improve robustness against noise, and simple strategies such as resampling collocation points substantially improve accuracy. We also analyze the accuracy, precision, and computational cost of automatic differentiation and finite differences for derivative computation within PI-MFM. Finally, we demonstrate zero-shot physics-informed fine-tuning to unseen PDE families: starting from a physics-informed pretrained model, adapting using only PDE residuals and initial/boundary conditions, without any labeled solution data, rapidly reduces test errors to around 1% and clearly outperforms physics-only training from scratch. These results show that PI-MFM provides a practical and scalable path toward data-efficient, transferable PDE solvers.

</details>


### [86] [Spectral Analysis of Hard-Constraint PINNs: The Spatial Modulation Mechanism of Boundary Functions](https://arxiv.org/abs/2512.23295)
*Yuchen Xie,Honghang Chi,Haopeng Quan,Yahui Wang,Wei Wang,Yu Ma*

Main category: cs.LG

TL;DR: HC-PINNs use hard constraints via trial function ansatz, but training dynamics were unexplored. This work establishes NTK framework showing boundary function acts as spectral filter, with effective rank predicting convergence better than condition numbers.


<details>
  <summary>Details</summary>
Motivation: Physics-Informed Neural Networks with hard constraints (HC-PINNs) are increasingly used but their theoretical training mechanisms remained unexplored. Unlike soft constraints with additive penalties, hard constraints introduce multiplicative spatial modulation that fundamentally changes learning dynamics.

Method: Established rigorous Neural Tangent Kernel (NTK) framework for HC-PINNs, deriving explicit kernel composition law. Performed spectral analysis to show boundary function acts as spectral filter reshaping eigenspectrum. Identified effective rank of residual kernel as predictor of training convergence.

Result: Boundary function B(x) functions as spectral filter reshaping neural network's native kernel eigenspectrum. Effective rank of residual kernel is deterministic predictor of training convergence, superior to classical condition numbers. Widely used boundary functions can inadvertently induce spectral collapse leading to optimization stagnation.

Conclusion: This framework transforms boundary function design from heuristic choice into principled spectral optimization problem, providing solid theoretical foundation for geometric hard constraints in scientific machine learning. Validated across multi-dimensional benchmarks.

Abstract: Physics-Informed Neural Networks with hard constraints (HC-PINNs) are increasingly favored for their ability to strictly enforce boundary conditions via a trial function ansatz $\tilde{u} = A + B \cdot N$, yet the theoretical mechanisms governing their training dynamics have remained unexplored.
  Unlike soft-constrained formulations where boundary terms act as additive penalties, this work reveals that the boundary function $B$ introduces a multiplicative spatial modulation that fundamentally alters the learning landscape.
  A rigorous Neural Tangent Kernel (NTK) framework for HC-PINNs is established, deriving the explicit kernel composition law.
  This relationship demonstrates that the boundary function $B(\vec{x})$ functions as a spectral filter, reshaping the eigenspectrum of the neural network's native kernel.
  Through spectral analysis, the effective rank of the residual kernel is identified as a deterministic predictor of training convergence, superior to classical condition numbers.
  It is shown that widely used boundary functions can inadvertently induce spectral collapse, leading to optimization stagnation despite exact boundary satisfaction.
  Validated across multi-dimensional benchmarks, this framework transforms the design of boundary functions from a heuristic choice into a principled spectral optimization problem, providing a solid theoretical foundation for geometric hard constraints in scientific machine learning.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [87] [Active-Absorbing Phase Transitions in the Parallel Minority Game](https://arxiv.org/abs/2512.22826)
*Aryan Tyagi,Soumyajyoti Biswas,Anirban Chakraborti*

Main category: cond-mat.stat-mech

TL;DR: The Parallel Minority Game shows different critical behavior depending on agent decision rules: instantaneous rules follow mean-field directed percolation scaling, while threshold rules create a distinct non-mean-field universality class.


<details>
  <summary>Details</summary>
Motivation: To understand how microscopic decision rules in adaptive multi-agent systems affect large-scale critical behavior, particularly in socio-economic and active systems, by studying the Parallel Minority Game under different agent update mechanisms.

Method: Comprehensive numerical study of PMG with two families of microscopic decision rules: (1) instantaneous population-based updates, and (2) threshold-based activation where agents move only after overcrowding density crosses a threshold. Measured time-dependent and steady-state activity A(t) and overcrowding fraction F(t) as functions of control parameter g=N/D.

Result: Instantaneous rules display mean-field directed-percolation (MF-DP) scaling with β≈1.00, δ≈0.5, and ν∥≈2.0. Threshold rules produce a distinct non-mean-field universality class with β≈0.75 and systematic failure of MF-DP dynamical scaling. Thresholding acts as a relevant perturbation to DP.

Conclusion: Minimal cognitive features at the agent level (like threshold-based decision making) can fundamentally alter large-scale critical behavior in socio-economic and active systems, moving systems from mean-field to non-mean-field universality classes.

Abstract: The Parallel Minority Game (PMG) is a synchronous adaptive multi-agent model that exhibits active-absorbing transitions characteristic of non-equilibrium statistical systems. We perform a comprehensive numerical study of the PMG under two families of microscopic decision rules: (i) agents update their choices based on instantaneous population in their alternative choices, and (ii) threshold-based activation that activates agents movement only after overcrowding density crossing a threshold. We measure time-dependent and steady state limits of activity $A(t)$, overcrowding fraction $F(t)$ as functions of the control parameter $g=N/D$, where $N$ is the number of agents and $D$ is the total number of sites. Instantaneous rules display mean-field directed-percolation (MF-DP) scaling with $β\approx1.00$, $δ\approx0.5$, and $ν_{\parallel}\approx2.0$. Threshold rules, however, produce a distinct non-mean-field universality class with $β\approx0.75$ and a systematic failure of MF-DP dynamical scaling. We show that thresholding acts as a relevant perturbation to DP. The results highlight how minimal cognitive features at the agent level fundamentally alter large-scale critical behaviour in socio-economic and active systems.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [88] [Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space](https://arxiv.org/abs/2512.22676)
*Sergey Salishev*

Main category: eess.SP

TL;DR: This thesis develops energy-efficient signal processing algorithms with minimal parallelism and memory constraints, focusing on power modeling, function approximations, FFT scheduling, and Toeplitz system solving.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve energy efficiency of low-power computing hardware by developing signal-processing algorithms that work under constraints of minimal parallelism and memory space, enabling efficient specialized accelerators for embedded and low-power applications.

Method: Four main approaches: (1) power/energy consumption model for clocked CMOS logic to select optimal parallelism, (2) integer-friendly approximation methods for elementary functions using constrained piecewise-polynomial (quasi-spline) constructions, (3) provably conflict-free data placement and execution order for mixed-radix streaming FFT on multi-bank/single-port memories, (4) parallelism/memory analysis of fast Schur algorithm for superfast Toeplitz system solving.

Result: The thesis provides constructive theorems, schedules, and design trade-offs that enable efficient specialized accelerators. The methods reduce lookup-table size for function approximations, enable conflict-free FFT execution on constrained memory architectures, and analyze parallelism/memory trade-offs for Toeplitz solvers.

Conclusion: The developed algorithms and implementation schemes successfully address energy efficiency challenges in low-power computing hardware by optimizing parallelism, reducing memory requirements, and providing provably correct scheduling methods for signal processing workloads like FFT and echo cancellation.

Abstract: This thesis develops signal-processing algorithms and implementation schemes under constraints of minimal parallelism and memory space, with the goal of improving energy efficiency of low-power computing hardware. We propose (i) a power/energy consumption model for clocked CMOS logic that supports selecting optimal parallelism, (ii) integer-friendly approximation methods for elementary functions that reduce lookup-table size via constrained piecewise-polynomial (quasi-spline) constructions with accuracy guarantees, (iii) provably conflict-free data placement and execution order for mixed-radix streaming FFT on multi-bank and single-port memories, including a self-sorting FFT variant, and (iv) a parallelism/memory analysis of the fast Schur algorithm for superfast Toeplitz system solving, motivated by echo-cancellation workloads. The results provide constructive theorems, schedules, and design trade-offs enabling efficient specialized accelerators.

</details>
