<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 18]
- [math.AP](#math.AP) [Total: 24]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [math.OC](#math.OC) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [math.NT](#math.NT) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [nlin.PS](#nlin.PS) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math.DG](#math.DG) [Total: 4]
- [math.CA](#math.CA) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A note on the space-time variational formulation for the wave equation with source term in $L^2(Q)$](https://arxiv.org/abs/2512.23807)
*Marco Zank*

Main category: math.NA

TL;DR: A variational formulation for the scalar wave equation with homogeneous initial conditions on bounded Lipschitz domains, using a new solution space with L² test functions, proven to satisfy inf-sup conditions and provide unique solvability.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous variational framework for the second-order scalar wave equation that supports space-time discretization methods (including least-squares approaches) and provides foundations for analyzing space-time boundary integral equations and boundary element methods.

Method: Derived a variational formulation in a bounded space-time cylinder Q with homogeneous initial conditions. Introduced a new solution space paired with test space L²(Q) for L² source terms. Used existence/uniqueness results in H¹(Q) to prove the formulation satisfies inf-sup theory and establishes an isomorphism as solution operator.

Result: Proved the variational setting fits inf-sup theory with an isomorphism as solution operator. Showed the new solution space is not a subspace of H²(Q). Established new uniqueness and solvability results crucial for space-time discretizations and regularity analysis.

Conclusion: The new variational framework provides rigorous foundations for space-time methods, including least-squares approaches and boundary element methods, with important implications for regularity results and analysis of space-time boundary integral equations.

Abstract: We derive a variational formulation for the scalar wave equation in the second-order formulation on bounded Lipschitz domains and homogeneous initial conditions. We investigate a variational framework in a bounded space-time cylinder $Q$ with a new solution space and the test space $L^2(Q)$ for source terms in $L^2(Q)$. Using existence and uniqueness results in $H^1(Q)$, we prove that this variational setting fits the inf-sup theory, including an isomorphism as solution operator. Moreover, we show that the new solution space is not a subspace of $H^2(Q)$. This new uniqueness and solvability result is not only crucial for discretizations using space-time methods, including least-squares approaches, but also important for regularity results and the analysis of related space-time boundary integral equations, which form the basis for space-time boundary element methods.

</details>


### [2] [Greedy Rational Approximation for Frequency-Domain Model Reduction of Parametric LTI Systems](https://arxiv.org/abs/2512.23814)
*Filip Bělík,Yanlai Chen,Akil Narayan*

Main category: math.NA

TL;DR: The paper proposes a reduced basis method for model reduction of parametric LTI systems by constructing low-order rational approximations of high-order rational functions in the frequency domain.


<details>
  <summary>Details</summary>
Motivation: Need for efficient model reduction of parametric linear time-invariant dynamical systems, which are often high-order and computationally expensive to simulate.

Method: Uses a standard reduced basis method (RBM) with iterative greedy approach, where greedy objective is evaluated through an error estimator that exploits linearity of frequency domain representation.

Result: Provides a principled framework for rational compression of high-order rational functions and computational pathway for parametric LTI system model reduction.

Conclusion: The proposed greedy framework offers a systematic approach to model reduction of parametric LTI systems through rational function approximation in the frequency domain.

Abstract: We investigate model reduction of parametric linear time-invariant (LTI) dynamical systems. When posed in the frequency domain, this problem can be formulated as seeking a low-order rational function approximation of a high-order rational function. We propose to use a standard reduced basis method (RBM) to construct this low-order rational function. Algorithmically, this procedure is an iterative greedy approach, where the greedy objective is evaluated through an error estimator that exploits the linearity of the frequency domain representation. The greedy framework is motivated through theoretical results of rational approximability of functions. This framework provides a principled approach to rational compression of high-order rational functions, and provides a computational pathway for model reduction of parametric LTI systems.

</details>


### [3] [Deep learning methods for inverse problems using connections between proximal operators and Hamilton-Jacobi equations](https://arxiv.org/abs/2512.23829)
*Oluwatosin Akande,Gabriel P. Langlois,Akwum Onwunta*

Main category: math.NA

TL;DR: The paper proposes using Hamilton-Jacobi PDEs to develop deep learning architectures for learning priors in inverse problems, enabling direct prior learning without inversion after training.


<details>
  <summary>Details</summary>
Motivation: Inverse problems require regularization/priors due to ill-posedness. Proximal operators are central for encoding priors and building efficient algorithms. Recent work connects proximal operators to convex potentials, and there's growing interest in learning priors via deep learning, but existing methods often require inverting the learned prior after training.

Method: Leverage connections between proximal operators and Hamilton-Jacobi PDEs to develop novel deep learning architectures for learning priors directly. The approach learns the prior without needing to invert it after training.

Result: Several numerical results demonstrate the efficiency of the proposed method in high-dimensional settings.

Conclusion: The Hamilton-Jacobi PDE framework provides an effective approach for direct prior learning in inverse problems, offering advantages over methods that require post-training inversion of learned priors.

Abstract: Inverse problems are important mathematical problems that seek to recover model parameters from noisy data. Since inverse problems are often ill-posed, they require regularization or incorporation of prior information about the underlying model or unknown variables. Proximal operators, ubiquitous in nonsmooth optimization, are central to this because they provide a flexible and convenient way to encode priors and build efficient iterative algorithms. They have also recently become key to modern machine learning methods, e.g., for plug-and-play methods for learned denoisers and deep neural architectures for learning priors of proximal operators. The latter was developed partly due to recent work characterizing proximal operators of nonconvex priors as subdifferential of convex potentials. In this work, we propose to leverage connections between proximal operators and Hamilton-Jacobi partial differential equations (HJ PDEs) to develop novel deep learning architectures for learning the prior. In contrast to other existing methods, we learn the prior directly without recourse to inverting the prior after training. We present several numerical results that demonstrate the efficiency of the proposed method in high dimensions.

</details>


### [4] [Multimodal sampling via Schrödinger-Föllmer samplers with temperatures](https://arxiv.org/abs/2512.23965)
*Xiaojie Wang,Xiaoyan Zhang*

Main category: math.NA

TL;DR: The paper introduces temperature-parameterized Schrödinger-Föllmer samplers (SFS) that achieve improved O(h) convergence rate in Wasserstein distance, outperforming Langevin samplers especially for multimodal distributions.


<details>
  <summary>Details</summary>
Motivation: To improve upon existing Schrödinger-Föllmer samplers which had O(√h) convergence rate, and to develop more effective gradient-free samplers that work on unit intervals without requiring ergodicity, particularly for challenging multimodal distributions.

Method: Introduces temperature-parameterized SFS based on Euler discretization of Schrödinger-Föllmer processes with temperatures. Develops novel error analysis approach for time discretization to prove improved convergence rates.

Result: Achieves enhanced O(h) convergence rate in L²-Wasserstein distance under smoothness conditions, significantly improving the previous O(√h) rate. Numerical experiments confirm the rate and show SFS substantially outperforms vanilla Langevin samplers for multimodal distributions.

Conclusion: Temperature-parameterized SFS provides a gradient-free, non-ergodic sampling method with improved convergence rates that excels at sampling from multimodal distributions, offering advantages over traditional Langevin samplers.

Abstract: Generating samples from complex and high-dimensional distributions is ubiquitous in various scientific fields of statistical physics, Bayesian inference, scientific computing and machine learning. Very recently, Huang et al. (IEEE Trans. Inform. Theory, 2025) proposed new Schrödinger-Föllmer samplers (SFS), based on the Euler discretization of the Schrödinger-Föllmer diffusion evolving on the unit interval $[0, 1]$. There, a convergence rate of order $\mathcal{O}(\sqrt{h})$ in the $L^2$-Wasserstein distance was obtained for the Euler discretization with a uniform time step-size $h>0$.
  By incorporating a temperature parameter, different samplers are introduced in this paper, based on the Euler discretization of the Schrödinger-Föllmer process with temperatures. As revealed by numerical experiments, high temperatures are vital, particularly in sampling from multimodal distributions. Further, a novel approach of error analysis is developed for the time discretization and an enhanced convergence rate of order ${ \mathcal{O}(h)}$ is obtained in the $L^2$-Wasserstein distance, under certain smoothness conditions on the drift. This significantly improves the existing order-half convergence in the aforementioned paper. Unlike Langevin samplers, SFS is of gradient-free, works in a unit interval $[0, 1]$ and does not require any ergodicity. Numerical experiments confirm the convergence rate and show that, the SFS substantially outperforms vanilla Langevin samplers, particularly in sampling from multimodal distributions.

</details>


### [5] [High order numerical discretizations of the Einstein-Euler equations in the Generalized Harmonic formulation](https://arxiv.org/abs/2512.24121)
*Stefano Muzzolon,Michael Dumbser,Olindo Zanotti,Elena Gaburro*

Main category: math.NA

TL;DR: Two new numerical schemes for solving coupled Einstein-Euler equations: a finite difference CWENO scheme on Cartesian meshes and an ADER discontinuous Galerkin scheme on 2D unstructured polygonal meshes, both with well-balancing properties.


<details>
  <summary>Details</summary>
Motivation: To develop robust numerical methods for solving the coupled Einstein-Euler equations in numerical relativity, with particular focus on creating a foundation for future 3D simulations on unstructured moving meshes.

Method: 1) Finite difference Central Weighted Essentially Non-Oscillatory (CWENO) scheme on Cartesian meshes; 2) ADER discontinuous Galerkin scheme on 2D unstructured polygonal meshes. Both schemes incorporate well-balancing to preserve equilibrium of known stationary solutions.

Result: Successful validation through standard vacuum tests (robust stability, linearized wave, gauge wave), long-term stable evolutions of stationary black holes (including extreme spin Kerr), and matter coupling tests (spherical accretion onto Schwarzschild black hole, perturbed non-rotating neutron star).

Conclusion: The developed schemes provide a solid foundation for more complex astrophysical simulations using DG schemes on unstructured 3D meshes, demonstrating capability to handle full Einstein-Euler systems.

Abstract: We propose two new alternative numerical schemes to solve the coupled Einstein-Euler equations in the Generalized Harmonic formulation. The first one is a finite difference (FD) Central Weighted Essentially Non-Oscillatory (CWENO) scheme on a traditional Cartesian mesh, while the second one is an ADER (Arbitrary high order Derivatives) discontinuous Galerkin (DG) scheme on 2D unstructured polygonal meshes. The latter, in particular, represents a preliminary step in view of a full 3D numerical relativity calculation on moving meshes. Both schemes are equipped with a well-balancing (WB) property, which allows to preserve the equilibrium of a priori known stationary solutions exactly at the discrete level. We validate our numerical approaches by successfully reproducing standard vacuum test cases, such as the robust stability, the linearized wave, and the gauge wave tests, as well as achieving long-term stable evolutions of stationary black holes, including Kerr black holes with extreme spin. Concerning the coupling with matter, modeled by the relativistic Euler equations, we perform a classical test of spherical accretion onto a Schwarzschild black hole, as well as an evolution of a perturbed non-rotating neutron star, demonstrating the capability of our schemes to operate also on the full Einstein-Euler system. Altogether, these results provide a solid foundation for addressing more complex and challenging simulations of astrophysical sources through DG schemes on unstructured 3D meshes.

</details>


### [6] [Structure-preserving schemes for nonlinear symmetric hyperbolic and thermodynamically compatible systems of partial differential equations](https://arxiv.org/abs/2512.24127)
*Alessia Lucca,Michael Dumbser*

Main category: math.NA

TL;DR: Develops exactly energy-conservative and structure-preserving finite volume schemes for SHTC systems using both collocated and staggered semi-implicit approaches.


<details>
  <summary>Details</summary>
Motivation: SHTC systems in continuum physics satisfy total energy conservation and often have stationary differential constraints (involutions), but existing discretizations may not preserve these properties exactly.

Method: Two approaches: 1) Simple semi-discrete cell-centered HTC finite volume scheme with collocated grids (energy-conservative but not involution-preserving). 2) Fully discrete semi-implicit staggered vertex-based scheme that exactly preserves both energy conservation and involution constraints using discrete symmetric-hyperbolic Godunov-form.

Result: The proposed schemes are applied to three SHTC systems (nonlinear acoustics, nonlinear Maxwell equations, nonlinear Maxwell-GLM system) with numerical results demonstrating exact energy conservation and involution preservation.

Conclusion: The paper successfully develops structure-preserving finite volume schemes that exactly conserve total energy and satisfy involution constraints for SHTC systems, using a discrete symmetric-hyperbolic formulation that leads to symmetric positive definite linear systems.

Abstract: This paper aims at developing exactly energy-conservative and structure-preserving finite volume schemes for the discretisation of first-order symmetric-hyperbolic and thermodynamically compatible (SHTC) systems of partial differential equations in continuum physics. Due to their thermodynamic compatibility the class of SHTC systems satisfies an additional conservation law for the total energy and many PDE in this class of equations also satisfy stationary differential constraints (involutions). First, we propose a simple semi-discrete cell-centered HTC finite volume scheme that employs collocated grids and that is compatible with the total energy conservation law, but which does not satisfy the involutions. Second, we develop a fully discrete semi-implicit finite volume scheme that conserves total energy and which can be proven to satisfy also the involution constraints exactly at the discrete level. This method is a vertex-based staggered semi-implicit scheme that preserves the basic vector calculus identities $\nabla \cdot \nabla \times A = 0$ and $\nabla \times \nabla φ= 0$ for any vector and scalar field, respectively, exactly at the discrete level and which is also exactly totally energy conservative. The main key ingredient of the proposed implicit scheme is the fact that it uses a discrete version of the symmetric-hyperbolic Godunov-form of the governing PDE system. This leads naturally to sequences of symmetric and positive definite linear algebraic systems to be solved inside an iterative fixed-point method used in each time step. We apply our new schemes to three different SHTC systems. In particular, we consider the equations of nonlinear acoustics, the nonlinear Maxwell equations in the absence of charges and a nonlinear version of the Maxwell-GLM system. We also show some numerical results to provide evidence of the stated properties of the proposed schemes.

</details>


### [7] [Sufficient and Necessary Conditions for Eckart-Young-like Result for Tubal Tensors](https://arxiv.org/abs/2512.24405)
*Uria Mor*

Main category: math.NA

TL;DR: Characterization of tubal tensor products that satisfy Eckart-Young theorem for low-rank approximation.


<details>
  <summary>Details</summary>
Motivation: The tubal tensor framework extends matrix algebra concepts to tensors, including SVD and rank notions. While some tubal products have been shown to satisfy an Eckart-Young theorem (best low-rank approximation via truncated SVD), the complete family of such products remains uncharacterized.

Method: Theoretical characterization of tubal products that yield Eckart-Young type results, followed by experimental validation using video data and data-driven dynamical systems.

Result: Complete characterization of the family of tubal products that satisfy the Eckart-Young theorem for tensor low-rank approximation.

Conclusion: Identifies the specific tubal products that preserve the Eckart-Young property, enabling reliable low-rank tensor approximation via truncated tubal SVD, with practical applications demonstrated in video processing and dynamical systems.

Abstract: A valuable feature of the tubal tensor framework is that many familiar constructions from matrix algebra carry over to tensors, including SVD and notions of rank. Most importantly, it has been shown that for a specific family of tubal products, an Eckart-Young type theorem holds, i.e., the best low-rank approximation of a tensor under the Frobenius norm is obtained by truncating its tubal SVD. In this paper, we provide a complete characterization of the family of tubal products that yield an Eckart-Young type result. We demonstrate the practical implications of our theoretical findings by conducting experiments with video data and data-driven dynamical systems.

</details>


### [8] [Fast high-order spectral solvers for PDEs on triangulated surfaces with applications to deforming surfaces](https://arxiv.org/abs/2512.24456)
*Gentian Zavalani*

Main category: math.NA

TL;DR: Extends quadrilateral-based hierarchical Poincaré-Steklov (HPS) framework to triangular geometries using two high-order strategies while preserving spectral accuracy and fast direct-solver structure.


<details>
  <summary>Details</summary>
Motivation: The classical HPS method is limited to quadrilateral meshes with tensor-product spectral discretizations, restricting its application to triangulated geometries common in many computational problems.

Method: Two complementary high-order strategies: 1) reduced quadrilateralization approach (straightforward implementation), and 2) triangle-based spectral element method using Dubiner polynomials.

Result: Numerical demonstrations show the extensions preserve spectral accuracy, efficiency, and fast direct-solver structure of HPS framework. Successfully applied to time-dependent and evolving surfaces, reaction-diffusion systems, and geometry-driven surface evolution.

Conclusion: Successfully extended HPS framework to triangular geometries while maintaining its key advantages, enabling broader application to complex geometries and time-dependent surface problems.

Abstract: In this paper, we extend the classical quadrilateral based hierarchical Poincaré-Steklov (HPS) framework to triangulated geometries. Traditionally, the HPS method takes as input an unstructured, high-order quadrilateral mesh and relies on tensor-product spectral discretizations on each element. To overcome this restriction, we introduce two complementary high-order strategies for triangular elements: a reduced quadrilateralization approach which is straightforward to implement, and triangle based spectral element method based on Dubiner polynomials. We show numerically that these extensions preserve the spectral accuracy, efficiency, and fast direct-solver structure of the HPS framework. The method is further extended to time dependent and evolving surfaces, and its performance is demonstrated through numerical experiments on reaction-diffusion systems, and geometry driven surface evolution.

</details>


### [9] [Exponential Convergence of Deep Composite Polynomial Approximation for Cusp-Type Functions](https://arxiv.org/abs/2512.24523)
*Kingsley Yeon,Steven B. Damelin,Michael Werman*

Main category: math.NA

TL;DR: Deep composite polynomial approximations achieve exponential convergence for functions with algebraic cusp singularities, outperforming classical polynomial methods that only achieve algebraic rates.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of approximating continuous but non-differentiable functions with algebraic cusp singularities. Classical polynomial approximations for such functions only achieve algebraic convergence rates, which is inefficient. The authors aim to develop a more efficient approximation scheme that can handle functions with finitely many cusp terms of the form |x-a_j|^{α_j} with rational exponents α_j∈(0,1) on a real-analytic background.

Method: The authors propose a constructive approximation scheme using deep composite polynomial structures. The method combines two layers: (1) an inner layer using a division-free polynomial iteration for fractional powers to handle the cusp singularities, and (2) an outer layer for analytic polynomial fitting to handle the real-analytic background. This composite structure allows for efficient approximation of functions with both singular and smooth components.

Result: The main theoretical result shows that the deep composite polynomial approximation achieves exponential convergence in the L^p([-1,1]) norm with respect to the parameter budget (number of scalar coefficients). This is in sharp contrast to classical single-layer polynomial approximations which only achieve algebraic convergence rates for cusp-type functions. Numerical experiments confirm the theoretical rates and demonstrate the parameter efficiency of the proposed approach for both single and multiple cusp configurations.

Conclusion: Deep composite polynomial constructions provide an efficient framework for approximating functions with algebraic cusp singularities, achieving exponential convergence rates that significantly outperform classical polynomial approximation methods. The proposed two-layer structure effectively handles both the singular and smooth components of such functions, making it a promising approach for practical applications involving non-differentiable functions with cusp singularities.

Abstract: We investigate deep composite polynomial approximations of continuous but non-differentiable functions with algebraic cusp singularities. The functions in focus consist of finitely many cusp terms of the form $|x-a_j|^{α_j}$ with rational exponents $α_j\in(0,1)$ on a real-analytic background. We propose a constructive approximation scheme that combines a division-free polynomial iteration for fractional powers with an outer layer for the analytic polynomial fitting. Our main result shows that this composite structure achieves exponential convergence in the the number of scalar coefficients in the inner and outer polynomial layers. Specifically, the $L^p([-1,1])$ approximation error, decays exponentially with respect to the parameter budget, in contrast to the algebraic rates obtained by classical single-layer polynomial approximation for cusp-type functions. Numerical experiments for both single and multiple cusp configurations confirm the theoretical rates and demonstrate the parameter efficiency of deep composite polynomial constructions.

</details>


### [10] [Newton-Krylov Methods for Computing Steady States of Particle Timesteppers via Optimal Transport](https://arxiv.org/abs/2512.24567)
*Hannes Vandecasteele,Nicholas Karris,Alexander Cloninger,Ioannis G. Kevrekidis*

Main category: math.NA

TL;DR: A matrix-free framework for computing steady-state distributions of stochastic particle systems using timesteppers reformulated as operators on probability measures via optimal transport.


<details>
  <summary>Details</summary>
Motivation: Timesteppers are powerful computational tools that implicitly encode steady states, but stochastic particle simulations have intrinsic randomness that prevents direct steady state extraction. There's a need to extend deterministic steady-state computation methods to stochastic systems.

Method: Reformulate stochastic timesteppers as operators acting on probability measures using optimal transport theory. Construct smooth cumulative- and inverse-cumulative-distribution-function (CDF/ICDF) timesteppers that evolve distributions rather than individual particles. Combine with matrix-free Newton-Krylov solvers for efficient computation.

Result: The framework enables efficient computation of steady-state distributions even under high stochastic noise. Error analysis quantifies noise effects on finite-difference Jacobian approximations, showing convergence in high noise regimes. Higher-dimensional generalizations work for non-trivial 2D distributions.

Conclusion: Establishes a unified variational framework for computing meaningful steady states of both deterministic and stochastic timesteppers, bridging the gap between deterministic and stochastic computational methods.

Abstract: Timesteppers constitute a powerful tool in modern computational science and engineering. Although they are typically used to advance the system forward in time, they can also be viewed as nonlinear mappings that implicitly encode steady states and stability information. In this work, we present an extension of the matrix-free framework for calculating, via timesteppers, steady states of deterministic systems to stochastic particle simulations, where intrinsic randomness prevents direct steady state extraction. By formulating stochastic timesteppers in the language of optimal transport, we reinterpret them as operators acting on probability measures rather than on individual particle trajectories. This perspective enables the construction of smooth cumulative- and inverse-cumulative-distribution-function ((I)CDF) timesteppers that evolve distributions rather than particles. Combined with matrix-free Newton-Krylov solvers, these smooth timesteppers allow efficient computation of steady-state distributions even under high stochastic noise. We perform an error analysis quantifying how noise affects finite-difference Jacobian action approximations, and demonstrate that convergence can be obtained even in high noise regimes. Finally, we introduce higher-dimensional generalizations based on smooth CDF-related representations of particles and validate their performance on a non-trivial two-dimensional distribution. Together, these developments establish a unified variational framework for computing meaningful steady states of both deterministic and stochastic timesteppers.

</details>


### [11] [Solving the inverse Source Problems for wave equation with final time measurements by a data driven approach](https://arxiv.org/abs/2512.24647)
*Qiling Gu,Wenlong Zhang,Zhidong Zhang*

Main category: math.NA

TL;DR: A discrete data-driven approach for solving wave equation inverse source problems with final time measurements using L²-Tikhonov regularization, establishing error bounds without source conditions and providing optimal parameter selection.


<details>
  <summary>Details</summary>
Motivation: To develop a practical, data-driven method for solving inverse source problems of wave equations using discrete measurements, without requiring classical source conditions that are often difficult to verify in practice.

Method: Uses L²-Tikhonov regularization with spectral decomposition of forward operator and noise separation technique; analyzes convergence under two noise models; extends to fully discrete case with finite element discretization.

Result: Establishes error bounds for reconstructed solution u and source term f without source conditions; derives expected convergence rate for source error in weaker topology; shows overall error depends on noise level, regularization parameter, time step, and mesh size.

Conclusion: The proposed data-driven approach effectively solves wave equation inverse source problems, provides theoretical error bounds without source conditions, enables optimal regularization parameter selection, and is validated by numerical experiments.

Abstract: This paper develops a discrete data-driven approach for solving the inverse source problem of the wave equation with final time measurements. Focusing on the $L^2$-Tikhonov regularization method, we analyze its convergence under two different noise models, using noisy discrete spatial observations. By exploiting the spectral decomposition of the forward operator and introducing a noise separation technique into the variational framework, we establish error bounds for the reconstructed solution $u$ and the source term $f$ without requiring classical source conditions. Moreover, an expected convergence rate for the source error is derived in a weaker topology. We also extend the analysis to the fully discrete case with finite element discretization, showing that the overall error depends only on the noise level, regularization parameter, time step size, and spatial mesh size. These estimates provide a basis for selecting the optimal regularization parameter in a data-driven manner, without a priori information. Numerical experiments validate the theoretical results and demonstrate the efficiency of the proposed algorithm.

</details>


### [12] [Boundary error control for numerical solution of BSDEs by the convolution-FFT method](https://arxiv.org/abs/2512.24714)
*Xiang Gao,Cody Hyndman*

Main category: math.NA

TL;DR: Improved CFFT method for BSDEs with better boundary error handling for option pricing using time-dependent shifting schemes.


<details>
  <summary>Details</summary>
Motivation: The original CFFT approach for solving BSDEs in option valuation suffers from boundary errors when transforming functions for Fourier analysis. These boundary errors reduce accuracy in practical applications.

Method: Modified the damping and shifting schemes from the original CFFT formulation. Introduced time-dependent shifting to transform target functions into bounded periodic functions suitable for Fourier transforms while minimizing boundary errors.

Result: Time-dependent shifting significantly reduces boundary errors. Numerical results demonstrate improved accuracy and convergence of the modified convolution method compared to the original approach.

Conclusion: The proposed modifications to the CFFT method effectively address boundary error issues in BSDE-based option pricing, leading to more accurate and reliable numerical solutions.

Abstract: We first review the convolution fast-Fourier-transform (CFFT) approach for the numerical solution of backward stochastic differential equations (BSDEs) introduced in (Hyndman and Oyono Ngou, 2017). We then propose a method for improving the boundary errors obtained when valuing options using this approach. We modify the damping and shifting schemes used in the original formulation, which transforms the target function into a bounded periodic function so that Fourier transforms can be applied successfully. Time-dependent shifting reduces boundary error significantly. We present numerical results for our implementation and provide a detailed error analysis showing the improved accuracy and convergence of the modified convolution method.

</details>


### [13] [A structure-preserving parametric approximation for anisotropic geometric flows via an $α$-surface energy matrix](https://arxiv.org/abs/2512.24875)
*Weizhu Bao,Yifei Li,Wenjun Ying,Yulin Zhang*

Main category: math.NA

TL;DR: Proposes structure-preserving parametric approximation for anisotropic geometric flows with optimal energy stability at α=-1


<details>
  <summary>Details</summary>
Motivation: Existing formulations for anisotropic geometric flows lack a unified framework with optimal energy stability guarantees. The paper aims to develop a structure-preserving approximation that can handle general anisotropic effects while achieving optimal stability conditions.

Method: Introduces hyperparameter α to construct unified surface energy matrix Ĝₖᵅ(θ) that encompasses all existing formulations. Applies this to anisotropic curvature flow and analyzes energy stability conditions. Extends framework to general anisotropic geometric flows through unified velocity discretization.

Result: Proves α=-1 is unique choice achieving optimal energy stability under necessary and sufficient condition 3γ̂(θ)≥γ̂(θ-π). Other α≠-1 require strictly stronger conditions. Numerical experiments validate theoretical optimality of α=-1 and demonstrate effectiveness and robustness.

Conclusion: The proposed framework provides a unified structure-preserving approach for anisotropic geometric flows with optimal energy stability at α=-1, offering improved theoretical guarantees and practical performance over existing formulations.

Abstract: We propose a structure-preserving parametric approximation for geometric flows with general anisotropic effects. By introducing a hyperparameter $α$, we construct a unified surface energy matrix $\hat{\boldsymbol{G}}_k^α(θ)$ that encompasses all existing formulations of surface energy matrices, and apply it to anisotropic curvature flow. We prove that $α=-1$ is the unique choice achieving optimal energy stability under the necessary and sufficient condition $3\hatγ(θ)\geq\hatγ(θ-π)$, while all other $α\neq-1$ require strictly stronger conditions. The framework extends naturally to general anisotropic geometric flows through a unified velocity discretization that ensures energy stability. Numerical experiments validate the theoretical optimality of $α=-1$ and demonstrate the effectiveness and robustness.

</details>


### [14] [Random compressible Euler flows](https://arxiv.org/abs/2512.24879)
*Maria Lukacova-Medvidova,Simon Schneider*

Main category: math.NA

TL;DR: Finite volume stochastic collocation method for random Euler system with rigorous convergence proof under bounded discrete differential quotients assumption.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method for solving random Euler systems (compressible fluid dynamics with uncertainty) that combines finite volume discretization with stochastic collocation, and to provide rigorous mathematical convergence guarantees.

Method: Finite volume stochastic collocation method combining deterministic finite volume discretization with stochastic collocation techniques for uncertainty quantification. Convergence analysis uses stochastic compactness arguments (Skorokhod and Gyöngy-Krylov theorems).

Result: Rigorous proof of convergence for random finite volume solutions under the assumption that discrete differential quotients remain bounded in probability.

Conclusion: The proposed method provides a mathematically sound approach for solving random Euler systems with guaranteed convergence properties, combining deterministic finite volume techniques with stochastic analysis tools.

Abstract: We propose a finite volume stochastic collocation method for the random Euler system. We rigorously prove the convergence of random finite volume solutions under the assumption that the discrete differential quotients remain bounded in probability. Convergence analysis combines results on the convergence of a deterministic FV method with stochastic compactness arguments due to Skorokhod and Gyöngy-Krylov.

</details>


### [15] [A finite element approach for minimizing line and surface energies arising in the study of singularities in liquid crystals](https://arxiv.org/abs/2512.24928)
*Dominik Stantejsky*

Main category: math.NA

TL;DR: Numerical algorithm for Plateau-like problem with area, boundary length, and obstacle constraints, applied to defect structures in nematic liquid crystals.


<details>
  <summary>Details</summary>
Motivation: Study of defect structures in nematic liquid crystals, specifically colloidal particles, requires solving a Plateau-like problem with geometric constraints.

Method: ADMM-based algorithm using finite elements to minimize discretized energy containing surface area, boundary length, and obstacle constraints with surface energy on obstacle.

Result: Algorithm successfully handles different inclusion shapes, revealing rich minimizing configurations with physical interpretation for colloidal particles in nematic liquid crystals.

Conclusion: Generalized TV-minimization approach effectively solves complex geometric optimization problems relevant to liquid crystal physics, providing insights into defect structures.

Abstract: Motivated by a problem originating in the study of defect structures in nematic liquid crystals, we describe and study a numerical algorithm for the resolution of a Plateau-like problem. The energy contains the area of a two-dimensional surface $T$ and the length of its boundary $\partial T$ reduced by a prescribed curve to make our problem non-trivial. We additionally include an obstacle $E$ for $T$ and pose a surface energy on $E$. We present an algorithm based on the Alternating Direction Method of Multipliers that minimizes a discretized version of the energy using finite elements, generalizing existing TV-minimization methods. We study different inclusion shapes demonstrating the rich structure of minimizing configurations and provide physical interpretation of our findings for colloidal particles in nematic liquid crystal.

</details>


### [16] [Approximating evolution operators of linear delay equations: a general framework for the convergence analysis](https://arxiv.org/abs/2512.24964)
*Alessia andò,Giusy Bosco,Dimitri Breda,Davide Liessi*

Main category: math.NA

TL;DR: General framework for discretizing linear delay equation evolution operators to approximate spectra for stability analysis, unifying existing methods and providing new convergence proofs.


<details>
  <summary>Details</summary>
Motivation: To investigate stability properties of nonlinear delay equations via linearized stability principle, requiring accurate approximation of spectra of linear delay evolution operators.

Method: Develops general convergence analysis framework using fixed-point equation reformulation, with hypotheses on regularization properties and approximation convergence on suitable subspaces. Unifies pseudospectral discretization methods and applies to weighted residuals method.

Result: Provides unified convergence proofs for existing pseudospectral discretization methods and gives first formal convergence analysis for weighted residuals method previously lacking such analysis.

Conclusion: The framework successfully unifies convergence analysis for various discretization methods of linear delay evolution operators, demonstrating generality by covering both pseudospectral methods and weighted residuals approach.

Abstract: We consider the problem of discretizing evolution operators of linear delay equations with the aim of approximating their spectra, which is useful in investigating the stability properties of (nonlinear) equations via the principle of linearized stability. We develop a general convergence analysis based on a reformulation of the operators by means of a fixed-point equation, providing a list of hypotheses related to the regularization properties of the equation and the convergence of the chosen approximation techniques on suitable subspaces. This framework unifies the proofs for some methods based on pseudospectral discretization, which we present here in this new form. To exemplify the generality of the framework, we also apply it to a method of weighted residuals found in the literature, which was previously lacking a formal convergence analysis.

</details>


### [17] [At the intersection of Numerical Analysis and Spectral Geometry](https://arxiv.org/abs/2512.25012)
*Nilima Nigam*

Main category: math.NA

TL;DR: Survey paper exploring the intersection of spectral geometry and numerical analysis, focusing on eigenvalue approximation methods and their applications in both conjecture formulation and proof strategies.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between spectral geometry (studying how domain geometry affects operator spectra) and numerical analysis (developing methods for accurate eigenvalue computation), examining how computational tools can both guide conjectures and serve as proof components.

Method: Expository survey approach reviewing existing discretization methods and approximation strategies, comparing specialized methods for efficiency vs. rigorous methods with error bounds for proofs.

Result: Identifies that computational spectral geometry requires different approaches depending on goals: rapid specialized methods for conjecture formulation vs. validated computations with error bounds for proof strategies.

Conclusion: The intersection of spectral geometry and numerical analysis is mutually beneficial - computational tools advance spectral geometry research while the demanding requirements of spectral geometry drive new developments in numerical analysis.

Abstract: How do the geometric properties of a domain impact the spectrum of an operator defined on it? How do we compute accurate and reliable approximations of these spectra? The former question is studied in spectral geometry, and the latter is a central concern in numerical analysis. In this short expository survey we revisit the process of eigenvalue approximation, from the perspective of computational spectral geometry. Over the years a multitude of methods -- for discretizing the operator and for the resultant discrete system -- have been developed and analyzed in the field of numerical analysis. High-accuracy and provably convergent discretization approaches can be used to examine the interplay between the spectrum of an operator and the geometric properties of the spatial domain or manifold it is defined on. While computations have been used to guide conjectures in spectral geometry, in recent years approximation-theoretic tools and validated computations are also being used as part of proof strategies in spectral geometry.
  Given a particular spectral feature of interest, should we discretize the original problem, or seek a reformulation? Of the many possible approximation strategies, which should we choose? These choices are inextricably linked to the objective: on the one hand, rapid, specialized methods are often ideal for conjecture formulation (prioritizing efficiency and accuracy), whereas schemes with guaranteed, computable error bounds are needed when computation is incorporated into a proof strategy. We also review instances where the demanding requirements of spectral geometry -- the need for rigorous error control or the robust calculation of higher eigenvalues -- motivate new developments in numerical analysis.

</details>


### [18] [Convergence of the generalization error for deep gradient flow methods for PDEs](https://arxiv.org/abs/2512.25017)
*Chenguang Liu,Antonis Papapantoleon,Jasper Rou*

Main category: math.NA

TL;DR: DGFMs provide rigorous mathematical foundation for solving high-dimensional PDEs using neural networks, with generalization error converging to zero as neurons and training time increase.


<details>
  <summary>Details</summary>
Motivation: To establish a firm mathematical foundation for deep gradient flow methods (DGFMs) in solving high-dimensional PDEs, addressing the lack of rigorous theoretical guarantees for these neural network-based PDE solvers.

Method: Decompose generalization error into approximation and training errors. First prove neural networks can approximate PDE solutions (approximation error → 0 as neurons → ∞). Then derive gradient flow in "wide network limit" and analyze its limit as training time → ∞.

Result: Theoretical proof that generalization error of DGFMs tends to zero as both number of neurons and training time tend to infinity, providing rigorous convergence guarantees for neural network PDE solvers.

Conclusion: DGFMs offer mathematically sound approach for solving high-dimensional PDEs with provable convergence properties, bridging the gap between practical neural network methods and rigorous mathematical analysis.

Abstract: The aim of this article is to provide a firm mathematical foundation for the application of deep gradient flow methods (DGFMs) for the solution of (high-dimensional) partial differential equations (PDEs). We decompose the generalization error of DGFMs into an approximation and a training error. We first show that the solution of PDEs that satisfy reasonable and verifiable assumptions can be approximated by neural networks, thus the approximation error tends to zero as the number of neurons tends to infinity. Then, we derive the gradient flow that the training process follows in the ``wide network limit'' and analyze the limit of this flow as the training time tends to infinity. These results combined show that the generalization error of DGFMs tends to zero as the number of neurons and the training time tend to infinity.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [19] [Fractal Mehler kernels and nonlinear geometric flows](https://arxiv.org/abs/2512.23830)
*Nicola Garofalo*

Main category: math.AP

TL;DR: The paper introduces a two-parameter family of Mehler kernels connected to Baouendi-Grushin flows in fractal dimensions, with links to geometric fully nonlinear equations and open questions.


<details>
  <summary>Details</summary>
Motivation: To explore connections between Mehler kernels, Baouendi-Grushin flows in fractal dimensions, and geometric fully nonlinear equations, establishing new mathematical relationships.

Method: Introduces a two-parameter family of Mehler kernels and demonstrates their connection to Baouendi-Grushin flows in fractal dimensions, while highlighting links to geometric fully nonlinear equations.

Result: Establishes connections between Mehler kernels and Baouendi-Grushin flows in fractal dimensions, identifies relationships with geometric fully nonlinear equations, and formulates two open questions.

Conclusion: The paper successfully bridges different mathematical areas by connecting Mehler kernels to Baouendi-Grushin flows in fractal dimensions and geometric fully nonlinear equations, while identifying important open problems for future research.

Abstract: In this paper we introduce a two-parameter family of Mehler kernels and connect them to a class of Baouendi-Grushin flows in fractal dimension. We also highlight a link with a geometric fully nonlinear equation and formulate two questions.

</details>


### [20] [A nonlinear instability result to the Navier-Stokes equations with Navier slip boundary conditions](https://arxiv.org/abs/2512.23946)
*Tien-Tai Nguyen*

Main category: math.AP

TL;DR: The paper proves linear and nonlinear instability of trivial steady states for incompressible viscous fluid with Navier-slip boundary conditions using operator methods and boundary layer analysis.


<details>
  <summary>Details</summary>
Motivation: To investigate instability of trivial steady states in viscous fluids with Navier-slip boundary conditions, providing an alternative approach to previous work by Ding, Li and Xin (2018).

Method: Uses operator method of Lafitte and Nguyen (2022) to show existence of infinitely many normal mode solutions for linear instability, then adapts framework of Desjardins and Grenier (2003) for boundary layer analysis to prove nonlinear instability by obtaining two separated solutions at escaping time.

Result: Proves both linear and nonlinear instability of trivial steady states for incompressible viscous fluid with Navier-slip boundary conditions.

Conclusion: The paper successfully demonstrates instability using a different approach than previous work, combining operator methods with boundary layer analysis to handle Navier-slip boundary conditions.

Abstract: In this paper, we investigate the instability of the trivial steady states to the incompressible viscous fluid with Navier-slip boundary conditions. For the linear instability, the existence of infinitely many normal mode solutions to the linearized equations is shown via the operator method of Lafitte and Nguyen (2022). Hence, we prove the nonlinear instability by adapting the framework of Desjardins and Grenier (2003) studying some classes of viscous boundary layers to obtain two separated solutions at escaping time. Our work performs a different approach from that of Ding, Li and Xin (2018).

</details>


### [21] [A regularity theory for second-order parabolic partial differential equations in weighted mixed norm Sobolev-Zygmund spaces](https://arxiv.org/abs/2512.24020)
*Jae-Hwan Choi,Junhee Ryu*

Main category: math.AP

TL;DR: Optimal regularity theory for parabolic PDEs in weighted mixed norm Sobolev-Zygmund spaces, extending Schauder estimates to time-measurable coefficients and integer-order regularity, with sharp trace theorem for initial data.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive regularity theory for parabolic PDEs that goes beyond classical Schauder estimates, handling coefficients that are only measurable in time and addressing the critical case of integer-order regularity, while properly treating nonzero initial data.

Method: Develops theory in weighted mixed norm Sobolev-Zygmund spaces, extends classical Schauder estimates to accommodate coefficients measurable in time, handles integer-order regularity cases, and establishes sharp trace theorem for initial data treatment.

Result: Establishes optimal regularity theory for parabolic PDEs, successfully extending Schauder estimates to time-measurable coefficients and integer-order regularity, with proper treatment of nonzero initial data via sharp trace theorem.

Conclusion: The paper provides a comprehensive regularity framework for parabolic PDEs that significantly generalizes classical results, enabling analysis of equations with less regular coefficients and proper handling of initial conditions in optimal trace spaces.

Abstract: We develop an optimal regularity theory for parabolic partial differential equations in weighted mixed norm Sobolev-Zygmund spaces. The results extend the classical Schauder estimates to coefficients that are merely measurable in time and to the critical case of integer-order regularity. In addition, nonzero initial data are treated in the optimal trace space via a sharp trace theorem.

</details>


### [22] [$L^p$ Estimates for Numerical Approximation of Hamilton-Jacobi Equations](https://arxiv.org/abs/2512.24051)
*Alessio Basti,Fabio Camilli*

Main category: math.AP

TL;DR: L^p error estimates for monotone schemes approximating Hamilton-Jacobi equations on torus using adjoint method


<details>
  <summary>Details</summary>
Motivation: To establish rigorous error estimates for numerical schemes solving Hamilton-Jacobi equations, which are important in optimal control, differential games, and other applications. Existing results need improvement and unification.

Method: Uses adjoint method to prove L^1 error bound of order one for finite-difference and semi-Lagrangian schemes under convexity assumptions on Hamiltonian. Then uses interpolation to obtain L^p estimates for all finite p>1.

Result: Proves L^1 error bound of order one, obtains L^p estimates for all finite p>1 via interpolation, covers broad class of schemes, improves existing results, provides unified framework for discrete error estimates.

Conclusion: The adjoint method provides a powerful unified framework for establishing L^p error estimates for monotone numerical schemes approximating Hamilton-Jacobi equations, with improved results over previous approaches.

Abstract: We establish $L^p$ error estimates for monotone numerical schemes approximating Hamilton-Jacobi equations on the $d$-dimensional torus. Using the adjoint method, we first prove a $L^1$ error bound of order one for finite-difference and semi-Lagrangian schemes under standard convexity assumptions on the Hamiltonian. By interpolation, we also obtain $L^p$ estimates for every finite $p>1$. Our analysis covers a broad class of schemes, improves several existing results, and provides a unified framework for discrete error estimates.

</details>


### [23] [Propagation of chaos for the homogeneous Boltzmann equation with moderately soft potentials](https://arxiv.org/abs/2512.24065)
*Nicolas Fournier,Stéphane Mischler*

Main category: math.AP

TL;DR: Kac particle system converges to Boltzmann equation for moderately soft potentials, proving propagation of chaos via Fisher information control.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous connection between microscopic particle systems (Kac model) and macroscopic kinetic theory (Boltzmann equation) for moderately soft potentials, proving propagation of chaos.

Method: Adapt recent work by Imbert, Silvestre and Villani to show Fisher information is nonincreasing along solutions to Kac master equation, using this estimate to control interaction singularity.

Result: Proves convergence of Kac particle system to homogeneous Boltzmann equation as particle number → ∞ for moderately soft potentials (γ ∈ (-2,0)), establishing propagation of chaos.

Conclusion: Fisher information monotonicity provides key analytical tool to handle singular interactions in moderately soft potentials, enabling rigorous derivation of Boltzmann equation from particle dynamics.

Abstract: We show that the Kac particle system converges, as the number of particles tends to infinity, to the solution of the homogeneous Boltzmann equation, in the regime of moderately soft potentials, $γ\in (-2,0)$ with the common notation. This proves the propagation of chaos. We adapt the recent work of Imbert, Silvestre and Villani, to show that the Fisher information is nonincreasing in time along solutions to the Kac master equation. This estimate allows us to control the singularity of the interaction.

</details>


### [24] [Dirac solitons in one-dimensional nonlinear Schrödinger equations](https://arxiv.org/abs/2512.24089)
*William Borrelli,Elena Danesi,Simone Dovetta,Lorenzo Tentarelli*

Main category: math.AP

TL;DR: The paper studies Dirac solitons in 1D cubic NLS equations with periodic potentials, showing that NLD equations serve as effective models for NLS near Dirac points.


<details>
  <summary>Details</summary>
Motivation: To understand and rigorously justify the relationship between nonlinear Schrödinger equations with periodic potentials (displaying Dirac points) and nonlinear Dirac equations as effective models for describing standing wave solutions.

Method: Study 1D stationary cubic NLS equations with periodic potentials having Dirac points in dispersion relation. Introduce periodic perturbations to open spectral gaps around Dirac-point energy, then construct standing waves whose leading-order profile is a modulation of Bloch waves using spinor components solving cubic NLD equations.

Result: Successfully construct Dirac solitons - standing waves of NLS equation whose leading-order profile is described by solutions to an appropriate cubic NLD equation. This provides rigorous justification for using NLD as an effective model for the original NLS equation.

Conclusion: The nonlinear Dirac equation serves as a valid effective model for the nonlinear Schrödinger equation near Dirac points in periodic potentials, with Dirac solitons providing the connection between these two mathematical frameworks.

Abstract: In this paper we study a family of one-dimensional stationary cubic nonlinear Schrödinger (NLS) equations with periodic potentials and linear part displaying Dirac points in the dispersion relation. By introducing a suitable periodic perturbation, one can open a spectral gap around the Dirac-point energy. This allows to construct standing waves of the NLS equation whose leading-order profile is a modulation of Bloch waves by means of the components of a spinor solving an appropriate cubic nonlinear Dirac (NLD) equation. We refer to these solutions as Dirac solitons. Our analysis thus provides a rigorous justification for the use of the NLD equation as an effective model for the original NLS equation.

</details>


### [25] [An Equivalence Result on the Order of Differentiability in Frobenius' Theorem](https://arxiv.org/abs/2512.24218)
*Yuhki Hosoya*

Main category: math.AP

TL;DR: This paper analyzes total differential equations in foliation theory without smoothness assumptions, revealing asymmetry in solution differentiability. It shows integral manifolds have higher regularity than solutions, provides counterexamples, and characterizes quasi-convex solutions to optimization problems.


<details>
  <summary>Details</summary>
Motivation: To examine the simplest case of total differential equations in foliation theory without imposing smoothness assumptions, which reveals a peculiar asymmetry in the differentiability of solutions that needs to be resolved.

Method: Focuses on the differentiability of integral manifolds rather than solutions directly. Analyzes regularity under local Lipschitz and C^k conditions, provides counterexamples, and characterizes minimizers of optimization problems with quasi-convex solutions to total differential equations.

Result: When system is locally Lipschitz, solutions are only locally Lipschitz but integral manifolds must be C^1. When system is C^k, solutions are C^k but integral manifolds must be C^{k+1}. Provides counterexample where system is C^1 but no C^2 solution exists. Characterizes minimizers for optimization with quasi-convex solutions.

Conclusion: Integral manifolds exhibit higher regularity than solutions to total differential equations in foliation theory. The asymmetry in differentiability can be resolved by focusing on integral manifold regularity. The analysis provides insights into quasi-convex solutions and optimization problems in this context.

Abstract: This paper examines the simplest case of total differential equations that appears in the theory of foliation structures, without imposing the smoothness assumptions. This leads to a peculiar asymmetry in the differentiability of solutions. To resolve this asymmetry, this paper focuses on the differentiability of the integral manifold. When the system is locally Lipschitz, a solution is ensured to be only locally Lipschitz, but the integral manifolds must be $C^1$. When the system is $C^k$, we can only ensure the existence of a $C^k$ solution, but the integral manifolds must be $C^{k+1}$. In addition, we see a counterexample in which the system is $C^1$, but there is no $C^2$ solution. Moreover, we characterize a minimizer of an optimization problem whose objective function is a quasi-convex solution to a total differential equation. In this connection, we examine two necessary and sufficient conditions for the system in which any solution is quasi-convex.

</details>


### [26] [Multi-bump solutions for sublinear elliptic equations with nonsymmetric coefficients](https://arxiv.org/abs/2512.24234)
*Chengxiang Zhang,Xu Zhang*

Main category: math.AP

TL;DR: The paper proves existence of infinitely many nonnegative bump solutions to a sublinear elliptic equation with asymmetric potential, using sharp support estimates to control bump interactions.


<details>
  <summary>Details</summary>
Motivation: To study existence of multiple bump solutions for sublinear elliptic equations with asymmetric potentials, overcoming challenges from sensitive bump interactions due to compact support of limiting profiles.

Method: Uses truncated functional space approach to obtain sharp estimates of support sets, with qualitative local stability estimates in region-wise maximum norms that confine each bump's core to designated regions and minimize overlap.

Result: Constructs infinitely many solutions with arbitrarily many bumps when ||K-1||_{L^p_loc} is sufficiently small, with uniform estimates in number of bumps.

Conclusion: The method provides precise control over bump interactions through sharp support estimates, enabling construction of solutions with infinitely many bumps despite asymmetric potentials and compact support challenges.

Abstract: We investigate the existence of nonnegative bump solutions to the sublinear elliptic equation \[ \begin{cases} -Δv - K(x)v + |v|^{q-2}v = 0 & \text{in } \mathbb{R}^N, \\ v(x) \to 0 & \text{as } |x| \to \infty, \end{cases} \] where $q \in (1,2)$, $ N \geq 2$, and the potential $K \in L^p_{\mathrm{loc}}(\mathbb{R}^N)$ with $p > N/2$ is a function without any symmetry assumptions. Under the condition that $\|K - 1\|_{L^p_{\mathrm{loc}}}$ is sufficiently small, we construct infinitely many solutions with arbitrarily many bumps. The construction is challenged by the sensitive interaction between bumps, whose limiting profiles have compact support. The key to ensuring their effective separation lies in obtaining sharp estimates of the support sets. Our method, based on a truncated functional space, provides precisely such control. We derive qualitative local stability estimates in region-wise maximum norms that govern the size of each bump's essential support, confining its core to a designated region and minimizing overlap. Crucially, these estimates are uniform in the number of bumps, which is the pivotal step in establishing the existence of solutions with infinitely many bumps.

</details>


### [27] [Mean-Field Limits of Deterministic and Stochastic Flocking Models with Nonlinear Velocity Alignment](https://arxiv.org/abs/2512.24383)
*Vinh Nguyen,Roman Shvydkoy,Changhui Tan*

Main category: math.AP

TL;DR: Mean-field limit analysis for flocking models with nonlinear velocity alignment and power-law coupling, extended to both deterministic and stochastic settings with improved convergence rates.


<details>
  <summary>Details</summary>
Motivation: Extend classical Cucker-Smale flocking theory to nonlinear frameworks that have gained recent attention, addressing models with power-law velocity coupling and fat-tailed communication kernels.

Method: Prove mean-field limits for agent-based flocking models with nonlinear velocity alignment (A(v) = |v|^{p-2}v, p>2) in both deterministic and stochastic settings. For deterministic case, provide quantitative propagation of chaos estimates with improved convergence rates for fat-tailed kernels. For stochastic case, incorporate multiplicative noise dependent on local interaction intensity.

Result: Successfully establish mean-field limits for nonlinear flocking models, showing improved convergence rates for deterministic systems with fat-tailed kernels, and derive corresponding Vlasov and Fokker-Planck-Alignment equations for deterministic and stochastic cases respectively.

Conclusion: The work extends classical flocking theory to nonlinear velocity alignment frameworks, providing rigorous mathematical foundations for both deterministic and stochastic agent-based models with improved convergence properties.

Abstract: We study the mean-field limit for a class of agent-based models describing flocking with nonlinear velocity alignment. Each agent interacts through a communication protocol $φ$ and a non-linear coupling of velocities given by the power law $A(\bv) = |\bv|^{p-2}\bv$, $p > 2$. The mean-field limit is proved in two settings -- deterministic and stochastic. We then provide quantitative estimates on propagation of chaos for deterministic case in the case of the classical fat-tailed kernels, showing an improved convergence rate of the $k$-particle marginals to a solution of the corresponding Vlasov equation. The stochastic version is addressed with multiplicative noise depending on the local interaction intensity, which leads to the associated Fokker-Planck-Alignment equation.
  Our results extend the classical Cucker-Smale theory to the nonlinear framework which has received considerable attention in the literature recently.

</details>


### [28] [Stability of the reconstruction of the heat reflection coefficient in the phonon transport equation](https://arxiv.org/abs/2512.24394)
*Peiyi Chen,Irene M. Gamba,Qin Li,Anjali Nair*

Main category: math.AP

TL;DR: The paper analyzes stability of inverse problem for inferring reflection coefficient in phonon transport, showing ill-posedness as system transitions from ballistic to diffusive regime.


<details>
  <summary>Details</summary>
Motivation: The reflection coefficient is a crucial thermal property at nanoscale, requiring inverse problem solving from macroscopic measurements. Previous studies show discrepancies about well-posedness of this inverse problem.

Method: Investigates stability of inverse problem for reflection coefficient in phonon transport equation, analyzing how stability changes with Knudsen number as system transitions between ballistic and diffusive regimes.

Result: The inverse problem becomes ill-posed as system transitions from ballistic to diffusive regime (Knudsen number → 0). Quantifies rate of stability deterioration with respect to Knudsen number and provides numerical confirmation.

Conclusion: The stability analysis clarifies previous discrepancies about well-posedness and provides theoretical framework for understanding limitations in inferring reflection coefficients from temperature measurements.

Abstract: The reflection coefficient is an important thermal property of materials, especially at the nanoscale, and determining this property requires solving an inverse problem based on macroscopic temperature measurements. In this manuscript, we investigate the stability of this inverse problem to infer the reflection coefficient in the phonon transport equation. We show that the problem becomes ill-posed as the system transitions from the ballistic to the diffusive regime, characterized by the Knudsen number converging to zero. Such a stability estimate clarifies the discrepancy observed in previous studies on the well-posedness of this inverse problem. Furthermore, we quantify the rate at which the stability deteriorates with respect to the Knudsen number and confirm the theoretical result with numerical evidence.

</details>


### [29] [Solvability conditions for some non-Fredholm operators with shifted arguments](https://arxiv.org/abs/2512.24476)
*Vitali Vougalter,Vitaly Volpert*

Main category: math.AP

TL;DR: Existence and convergence results for solutions in H²(R) of nonhomogeneous linear differential and integro-differential equations with argument translation, showing convergence of source terms/kernels implies convergence of solutions.


<details>
  <summary>Details</summary>
Motivation: To establish existence and convergence properties for solutions to differential equations with translated arguments, which arise in various applications and present analytical challenges due to the argument shift affecting the operator's Fredholm properties.

Method: Two-part approach: 1) For linear differential equations with argument translation, show L² convergence of source terms implies H² convergence of solutions. 2) For integro-differential equations with argument shift, demonstrate L¹ convergence of integral kernels yields H² convergence of solutions.

Result: Proved existence in the sense of sequences for both equation types in H²(R). Established that under reasonable technical conditions, convergence of source terms/kernels in appropriate function spaces implies existence and convergence of solutions in H²(R).

Conclusion: The paper successfully establishes solvability and convergence results for differential and integro-differential equations with argument translation, providing analytical tools for studying such equations where the translation constant affects the operator's Fredholm properties.

Abstract: In the first part of the article we establish the existence in the sense of sequences of solutions in $H^{2}(R)$ for some nonhomogeneous linear differential equation in which one of the terms has the argument translated by a constant. It is shown that under the reasonable technical conditions the convergence in $L^{2}(R)$ of the source terms implies the existence and the convergence in $H^{2}(R)$ of the solutions. The second part of the work deals with the solvability in the sense of sequences in $H^{2}(R)$ of the integro-differential equation in which one of the terms has the argument shifted by a constant. It is demonstrated that under the appropriate auxiliary assumptions the convergence in $L^{1}(R)$ of the integral kernels yields the existence and the convergence in $H^{2}(R)$ of the solutions. Both equations considered involve the second order differential operator with or without the Fredholm property depending on the value of the constant by which the argument gets translated.

</details>


### [30] [Steady Self-Propelled Motion of a Rigid Body in a Viscous Fluid with Navier-Slip Boundary Conditions](https://arxiv.org/abs/2512.24510)
*Sarka Necasova,Arnab Roy,Ana Leonor Silvestre*

Main category: math.AP

TL;DR: Existence of steady self-propelled motion for a rigid body in viscous fluid with Navier-slip boundary conditions, establishing propulsion mechanisms through boundary-driven flows.


<details>
  <summary>Details</summary>
Motivation: To understand propulsion in microfluidic and rough-surface regimes where partial slip effects are significant, extending classical Dirichlet-based theory to more realistic Navier-slip boundary conditions.

Method: Analysis in body-fixed reference frame with fluid in exterior domain; modeling propulsion via nonhomogeneous Navier-slip boundary conditions; deriving Korn-type inequality for exterior domains with rigid-body motion; introducing finite-dimensional thrust space via auxiliary Stokes problems.

Result: Established existence of weak steady solutions under smallness assumptions; derived necessary and sufficient condition for slip velocity to induce nontrivial translational/rotational motion; clarified how boundary effects generate propulsion.

Conclusion: The paper extends fluid-structure interaction theory to Navier-slip settings, providing mathematical framework for understanding propulsion mechanisms in microfluidic applications with partial slip effects.

Abstract: We investigate the steady self-propelled motion of a rigid body immersed in a three-dimensional incompressible viscous fluid governed by the Navier-Stokes equations. The analysis is performed in a body-fixed reference frame, so that the fluid occupies an exterior domain and the propulsion mechanism is modeled through nonhomogeneous Navier-slip boundary conditions at the fluid-body interface. Such conditions provide a realistic description of propulsion in microfluidic and rough-surface regimes, where partial slip effects are significant. Under suitable smallness assumptions on the boundary flux and on the normal component of the prescribed surface velocity, we establish the existence of weak steady solutions to the coupled fluid-structure system. A key analytical ingredient is the derivation of a Korn-type inequality adapted to exterior domains with rigid-body motion and Navier-slip interfaces, which yields uniform control of both the fluid velocity and the translational and rotational velocities of the body. Beyond existence, we provide a necessary and sufficient condition under which a prescribed slip velocity on the body surface induces nontrivial translational or rotational motion of the rigid body. This is achieved through the introduction of a finite-dimensional thrust space, defined via auxiliary exterior Stokes problems with Navier boundary conditions, which captures the effective contribution of boundary-driven flows to the rigid-body motion. Our results clarify how boundary effects generate propulsion and extend the classical Dirichlet-based theory to the Navier-slip setting.

</details>


### [31] [Anomalous Dissipation at Onsager-Critical Regularity](https://arxiv.org/abs/2512.24568)
*Alexey Cheskidov,Qirui Peng*

Main category: math.AP

TL;DR: The paper constructs 3D Euler equation solutions with anomalous dissipation in finite time via vanishing viscosity limits, extending previous 2.5D constructions and establishing sharp Onsager-critical energy criteria.


<details>
  <summary>Details</summary>
Motivation: To demonstrate existence of dissipative Euler flows in three dimensions that violate Onsager's conjecture in a controlled way, showing anomalous dissipation can occur even at the critical regularity threshold.

Method: Extends 2.5-dimensional constructions from previous works, uses vanishing viscosity limits to obtain Euler solutions, establishes Onsager-critical energy criterion adapted to such flows, and provides fully 3D dissipative example with slightly rough external force.

Result: Successfully constructs 3D Euler solutions exhibiting anomalous dissipation in finite time, proves sharpness of the Onsager-critical energy criterion, and provides explicit dissipative example that is sharp in Onsager's sense.

Conclusion: The work demonstrates that anomalous dissipation can occur in 3D Euler equations at the critical Onsager regularity, providing concrete examples and sharp criteria for such dissipative behavior.

Abstract: We construct solutions to the three-dimensional Euler equations exhibiting anomalous dissipation in finite time through a vanishing viscosity limit. Inspired by \cite{BDL23} and \cite{cheskidov2023dissipation}, we extend the \(2\frac{1}{2}\)-dimensional constructions and establish an Onsager-critical energy criterion adapted to such flows, showing its sharpness. Moreover, we provide a fully three-dimensional dissipative Euler example, sharp in Onsager's sense, driven by a slightly rough external force, following the framework of \cite{CL21}.

</details>


### [32] [Propagation of space-time singularities for perturbed harmonic oscillators](https://arxiv.org/abs/2512.24582)
*Kenichi Ito,Tomoya Tagawa*

Main category: math.AP

TL;DR: Analysis of space-time singularity propagation for quantum harmonic oscillator with time-dependent perturbations using semiclassical wave front set methods.


<details>
  <summary>Details</summary>
Motivation: To understand how space-time singularities propagate in quantum systems with time-dependent perturbations, particularly for the quantum harmonic oscillator, which is a fundamental model in quantum mechanics.

Method: Reformulate Lascar's quasi-homogeneous wave front set in a semiclassical manner and apply Nakamura's argument (originally for spatial singularities of Schrödinger equation) to analyze space-time singularities, addressing the non-trivial challenge that time is now a base variable rather than just a parameter.

Result: Obtain characterization of singularity appearance in perturbed system compared to unperturbed system, showing how singularities propagate under time-dependent metric and potential perturbations.

Conclusion: Successfully extend Nakamura's method to handle space-time singularities where time participates as a base variable, providing analytical tools for studying singularity propagation in time-dependent quantum systems.

Abstract: We discuss propagation of space-time singularities for the quantum harmonic oscillator with time-dependent metric and potential perturbations. Reformulating the quasi-homogeneous wave front set according to Lascar (1977) in a semiclassical manner, we obtain a characterization of its appearance in comparison with the unperturbed system. The idea of our proof is based on the argument of Nakamura (2009), which was originally devised for the analysis of spatial singularities of the Schrödinger equation, however, the application is non-trivial since the time is no more a parameter, but takes a part in the base variables.

</details>


### [33] [Phase transition thresholds and chiral magnetic fields of general degree](https://arxiv.org/abs/2512.24598)
*Slim Ibrahim,Tatsuya Miura,Carlos Román,Ikkei Shimizu*

Main category: math.AP

TL;DR: The paper analyzes the Landau-Lifshitz energy with Dzyaloshinskii-Moriya interactions in 2D micromagnetics, focusing on the Bogomol'nyi regime. It determines minimal energy for arbitrary topological degree, reveals phase transitions, proves uniqueness/nonexistence of minimizers, and shows stability properties of homogeneous states beyond skyrmion instability thresholds.


<details>
  <summary>Details</summary>
Motivation: To understand the variational properties of the Landau-Lifshitz energy with Dzyaloshinskii-Moriya interactions in 2D micromagnetics, particularly in the Bogomol'nyi regime. The research aims to characterize energy minimization, phase transitions, and stability properties relevant to magnetic skyrmions and their applications.

Method: The authors use variational analysis and mathematical techniques to study the Landau-Lifshitz energy functional with Dzyaloshinskii-Moriya interactions. They focus on the Bogomol'nyi regime, employing topological degree analysis, energy minimization methods, and stability analysis to examine phase transitions and minimizer properties.

Result: 1) Determined minimal energy for arbitrary topological degree, revealing two types of phase transitions consistent with physical observations. 2) Proved uniqueness of energy minimizers for degrees 0 and -1, and nonexistence of minimizers for all other degrees. 3) Showed that homogeneous state remains stable even beyond the threshold where skyrmions lose stability. 4) Uncovered a new stability transition driven by Zeeman energy.

Conclusion: The study provides a comprehensive mathematical understanding of the Landau-Lifshitz energy with Dzyaloshinskii-Moriya interactions in the Bogomol'nyi regime. The results characterize energy minimization properties, phase transitions, and stability regimes, offering insights into magnetic skyrmion behavior and potential applications in spintronics and magnetic storage technologies.

Abstract: We study a variational problem for the Landau--Lifshitz energy with Dzyaloshinskii--Moriya interactions arising in 2D micromagnetics, focusing on the Bogomol'nyi regime. We first determine the minimal energy for arbitrary topological degree, thereby revealing two types of phase transitions consistent with physical observations. In addition, we prove the uniqueness of the energy minimizer in degrees $0$ and $-1$, and nonexistence of minimizers for all other degrees. Finally, we show that the homogeneous state remains stable even beyond the threshold at which the skyrmion loses stability, and we uncover a new stability transition driven by the Zeeman energy.

</details>


### [34] [Half-space minimizing solutions of a two dimensional Allen-Cahn system](https://arxiv.org/abs/2512.24610)
*Zhiyuan Geng*

Main category: math.AP

TL;DR: Complete classification of half-space minimizing solutions to 2D Allen-Cahn system with Dirichlet boundary conditions, based on blow-down limits at infinity, plus characterization of asymptotic behavior near sharp interfaces.


<details>
  <summary>Details</summary>
Motivation: To understand and classify minimizing solutions to the Allen-Cahn system on the upper half plane with Dirichlet boundary conditions, which is important for studying phase transitions and interface phenomena in bounded domains with boundaries.

Method: Analysis of two-dimensional Allen-Cahn system on upper half plane using blow-down limits at infinity to classify minimizing solutions, and studying asymptotic behavior near sharp interfaces through mathematical analysis techniques.

Result: Complete classification of half-space minimizing solutions in terms of their blow-down limits at infinity, and characterization of asymptotic behavior near sharp interfaces.

Conclusion: The paper provides a comprehensive understanding of minimizing solutions to the Allen-Cahn system on half-spaces, establishing connections between boundary conditions, blow-down limits, and interface behavior.

Abstract: This paper studies minimizing solutions to a two dimensional Allen-Cahn system on the upper half plane, subject to Dirichlet boundary conditions, \begin{equation*}
  Δu-\nabla_u W(u)=0, \quad u: \mathbb{R}_+^2\to \mathbb{R}^2,\ u=u_0 \text{ on } \partial \mathbb{R}_+^2, \end{equation*} where $W: \mathbb{R}^2\to [0,\infty)$ is a multi-well potential. We give a complete classification of such half-space minimizing solutions in terms of their blow-down limits at infinity. In addition, we characterize the asymptotic behavior of solutions near the associated sharp interfaces.

</details>


### [35] [A unified spatiotemporal formulation with physics-preserving structure for time-dependent convection-diffusion problems](https://arxiv.org/abs/2512.24650)
*James H. Adler,Xiaozhe Hu,Seulip Lee*

Main category: math.AP

TL;DR: A 4D spatiotemporal framework for time-dependent convection-diffusion problems that treats time as a space-like coordinate, enabling structure-preserving formulations using exterior calculus.


<details>
  <summary>Details</summary>
Motivation: To develop a unified formulation for time-dependent convection-diffusion problems that preserves underlying physical structures and can handle the full family of problems posed on H(grad), H(curl), and H(div) spaces.

Method: Treat time as an additional space-like coordinate to reformulate evolution problems as stationary convection-diffusion equations on 4D space-time domains. Use exterior calculus to extend to H(grad), H(curl), and H(div) problems. Introduce a 4D Hodge-Laplacian operator with spatiotemporal diffusion tensor and convection field, augmented by temporal perturbation for nondegeneracy. Develop an exponentially-fitted 4D spatiotemporal flux operator for symmetrization.

Result: A unified 4D formulation that naturally incorporates fundamental physical constraints (divergence-free and curl-free conditions) and enables well-posed variational formulations through symmetrization of the convection-diffusion operator.

Conclusion: The temporally-perturbed formulation converges to the original time-dependent convection-diffusion model as the perturbation parameter tends to zero, providing a mathematically rigorous and physically consistent framework for structure-preserving analysis of convection-diffusion problems.

Abstract: We propose a unified four-dimensional (4D) spatiotemporal formulation for time-dependent convection-diffusion problems that preserves underlying physical structures. By treating time as an additional space-like coordinate, the evolution problem is reformulated as a stationary convection-diffusion equation on a 4D space-time domain. Using exterior calculus, we extend this framework to the full family of convection-diffusion problems posed on $H(\textbf{grad})$, $H(\textbf{curl})$, and $H(\text{div})$. The resulting formulation is based on a 4D Hodge-Laplacian operator with a spatiotemporal diffusion tensor and convection field, augmented by a small temporal perturbation to ensure nondegeneracy. This formulation naturally incorporates fundamental physical constraints, including divergence-free and curl-free conditions. We further introduce an exponentially-fitted 4D spatiotemporal flux operator that symmetrizes the convection-diffusion operator and enables a well-posed variational formulation. Finally, we prove that the temporally-perturbed formulation converges to the original time-dependent convection-diffusion model as the perturbation parameter tends to zero.

</details>


### [36] [$L_p$-estimates for nonlocal equations with general Lévy measures](https://arxiv.org/abs/2512.24704)
*Hongjie Dong,Junhee Ryu*

Main category: math.AP

TL;DR: The paper establishes continuity of nonlocal operators with singular Lévy measures and unique strong solvability of corresponding parabolic equations in Lp spaces, with analysis of weighted mixed-norm space applicability.


<details>
  <summary>Details</summary>
Motivation: To study nonlocal parabolic equations with very singular Lévy measures that lack time regularity assumptions, extending analysis beyond classical settings to more general singular cases.

Method: Analyzes nonlocal operators with general Lévy measures of order σ∈(0,2), establishes operator continuity and unique strong solvability in Lp spaces, and investigates applicability in weighted mixed-norm spaces based on σ and dimension d.

Result: Proves continuity of operators and unique strong solvability of corresponding nonlocal parabolic equations in Lp spaces, and determines conditions under which operators can be treated in weighted mixed-norm spaces depending on σ and d ranges.

Conclusion: The framework handles very singular Lévy measures without time regularity, with operator properties and solvability established in Lp spaces, and weighted mixed-norm space applicability characterized by σ and dimension relationships.

Abstract: We consider nonlocal operators of the form \begin{equation*}
  L_t u(x) = \int_{\mathbb{R}^d} \left( u(x+y)-u(x)-\nabla u(x)\cdot y^{(σ)} \right) ν_t(dy), \end{equation*} where $ν_t$ is a general Lévy measure of order $σ\in(0,2)$. We allow this class of Lévy measures to be very singular and impose no regularity assumptions in the time variable. Continuity of the operators and the unique strong solvability of the corresponding nonlocal parabolic equations in $L_p$ spaces are established. We also demonstrate that, depending on the ranges of $σ$ and $d$, the operator can or cannot be treated in weighted mixed-norm spaces.

</details>


### [37] [Global spherically symmetric classical solutions for arbitrary large initial data of the multi-dimensional non-isentropic compressible Navier-Stokes equations](https://arxiv.org/abs/2512.24799)
*Yongteng Gu,Xiangdi Huang*

Main category: math.AP

TL;DR: This paper proves global classical solutions for arbitrary large initial data to the viscous shallow water system with transported entropy in spherically symmetric 2D and 3D settings, extending previous results on isentropic compressible Navier-Stokes equations.


<details>
  <summary>Details</summary>
Motivation: The global classical solutions for arbitrary large initial data of multi-dimensional viscous Saint-Venant system (shallow water equations) has been an open problem since 1871. Recent breakthroughs by Huang-Meng-Zhang and Chen-Zhang-Zhu solved this for isentropic cases under radial symmetry. This work aims to extend these results to non-isentropic compressible fluids with transported entropy.

Method: The authors prove a new BD entropy inequality for a class of non-isentropic compressible fluids (generalization of shallow water equations with transported entropy). They employ new estimates on the lower bound of density different from Huang-Meng-Zhang's approach, and analyze the spherically symmetric initial-boundary value problem.

Result: The viscous shallow water system with transport entropy admits global classical solutions for arbitrary large initial data in both two and three dimensions under spherically symmetry. The results relax previous restrictions: from N=2, γ≥3/2 to N=2, γ>1 and N=3, 1<γ<3.

Conclusion: This work successfully extends the existence theory for global classical solutions of shallow water equations to non-isentropic cases with transported entropy, overcoming limitations in dimension and adiabatic index that constrained previous results.

Abstract: In 1871, Saint-Venant introduced the shallow water equations. Since then, the global classical solutions for arbitrary large initial data of the multi-dimensional viscous Saint-Venant system have remained a well-known open problem. It was only recently that [Huang-Meng-Zhang, http:arXiv:2512.15029, 2025], under the assumption of radial symmetry, first proved the existence of global classical solutions for arbitrary large initial data to the initial-boundary value problem of the two-dimensional viscous shallow water equations. At the same time, [Chen-Zhang-Zhu, http:arXiv:2512.18545, 2025] also independently proved the existence of global large solutions to the Cauchy problem of this system. Notably, in the work of Huang-Meng-Zhang, they also established the existence of global classical solutions for arbitrary large initial data to the isentropic compressible Navier-Stokes equations satisfying the BD entropy equality in both two and three dimensions, and the viscous shallow water equations are precisely a specific class of isentropic compressible fluids subject to the BD entropy equality. In this paper, we prove a new BD entropy inequality for a class of non-isentropic compressible fluids, which can be regarded as a generalization of the shallow water equations with transported entropy. Employing new estimates on the lower bound of density different from that of Huang-Meng-Zhang's work, we show the "viscous shallow water system with transport entropy" will admit global classical solutions for arbitrary large initial data to the spherically symmetric initial-boundary value problem in both two and three dimensions. Our results also relax the restrictions on the dimension and adiabatic index imposed in Huang-Meng-Zhang's work on the shallow water equations, extending the range from $N=2,\ γ\ge \frac{3}{2}$ to $N=2,\ γ> 1$ and $N=3,\ 1<γ<3$.

</details>


### [38] [Hölder continuity of weak solutions to the thin-film equation in $d=2$](https://arxiv.org/abs/2512.24809)
*Federico Cornalba,Julian Fischer,Erika Maringová Kokavcová*

Main category: math.AP

TL;DR: The paper proves Hölder continuity of energy-dissipating weak solutions to the 2D thin-film equation, solving a major open problem about solution regularity.


<details>
  <summary>Details</summary>
Motivation: The thin-film equation models viscous liquid film spreading, but despite extensive existence theory for weak solutions, boundedness and regularity in 2D remained unsolved for decades. The fourth-order degenerate parabolic structure prevents standard regularity techniques.

Method: Uses the hole-filling technique adapted to handle the degenerate parabolicity of the fourth-order PDE, overcoming challenges where De Giorgi-Nash-Moser theory doesn't apply.

Result: Proves Hölder continuity of energy-dissipating weak solutions to the thin-film equation in two spatial dimensions (d=2), establishing crucial regularity properties.

Conclusion: Solves a major open problem in thin-film equation theory by establishing regularity of weak solutions in the physically most relevant 2D case, advancing mathematical understanding of these important physical models.

Abstract: The thin-film equation $\partial_t u = -\nabla \cdot (u^n \nabla Δu)$ describes the evolution of the height $u=u(x,t)\geq 0$ of a viscous thin liquid film spreading on a flat solid surface. We prove Hölder continuity of energy-dissipating weak solutions to the thin-film equation in the physically most relevant case of two spatial dimensions $d=2$. While an extensive existence theory of weak solutions to the thin-film equation was established more than two decades ago, even boundedness of weak solutions in $d=2$ has remained a major unsolved problem in the theory of the thin-film equation. Due the fourth-order structure of the thin-film equation, De Giorgi-Nash-Moser theory is not applicable. Our proof is based on the hole-filling technique, the challenge being posed by the degenerate parabolicity of the fourth-order PDE.

</details>


### [39] [Bol's type inequality for singular metrics and its application to prescribing $Q$-curvature problems](https://arxiv.org/abs/2512.24828)
*Mrityunjoy Ghosh,Ali Hyder*

Main category: math.AP

TL;DR: The paper studies higher-order Bol's inequality for radial normal solutions to singular Liouville equations, using these inequalities with compactness arguments to establish existence conditions for radial normal solutions to singular Q-curvature problems and obtain uniform bounds on total Q-curvature.


<details>
  <summary>Details</summary>
Motivation: To understand existence conditions for radial normal solutions to singular Q-curvature problems and establish uniform bounds on total Q-curvature through the application of higher-order Bol's inequalities.

Method: Apply higher-order Bol's inequality for radial normal solutions to singular Liouville equations, combined with compactness arguments, to analyze existence conditions and derive bounds.

Result: Derived necessary and sufficient conditions for existence of radial normal solutions to singular Q-curvature problems, and obtained uniform bounds on total Q-curvature under suitable assumptions.

Conclusion: Higher-order Bol's inequalities combined with compactness arguments provide powerful tools for establishing existence conditions and uniform bounds in singular Q-curvature problems with radial normal solutions.

Abstract: In this article, we study higher-order Bol's inequality for radial normal solutions to a singular Liouville equation. By applying these inequalities along with compactness arguments, we derive necessary and sufficient conditions for the existence of radial normal solutions to a singular $Q$-curvature problem. Moreover, under suitable assumptions on the $Q$-curvature, we obtain uniform bounds on the total $Q$-curvature.

</details>


### [40] [Boundedness of Fourier Integral Operators with complex phases on Fourier Lebesgue spaces](https://arxiv.org/abs/2512.24854)
*Duván Cardona,William Obeng-Denteh,Frederick Opoku*

Main category: math.AP

TL;DR: Boundedness estimates for Fourier integral operators on Fourier Lebesgue spaces with complex phase functions, extending real-phase results to complex case.


<details>
  <summary>Details</summary>
Motivation: Extend existing boundedness results for Fourier integral operators from real canonical relations to complex canonical relations, establishing the complex analogue of Rodino, Nicola, and Cordero's work.

Method: Develop boundedness estimates under spatial factorization condition of rank ϰ, analyzing operators with complex phase functions on Fourier Lebesgue spaces.

Result: Fourier integral operator is bounded on Fourier Lebesgue space F L^p when order m ≤ -ϰ|1/p - 1/2|, for 1 ≤ p ≤ ∞, and this condition is sharp.

Conclusion: Successfully established sharp boundedness conditions for Fourier integral operators with complex phase functions, completing the complex analogue of real-phase results.

Abstract: In this paper, we develop boundedness estimates for Fourier integral operators on Fourier Lebesgue spaces when the associated canonical relation is parametrised by a complex phase function. Our result constitutes the complex analogue of those obtained for real canonical relations by Rodino, Nicola, and Cordero. We prove that, under the spatial factorization condition of rank $\varkappa$, the corresponding Fourier integral operator is bounded on the Fourier Lebesgue space $\mathcal{F}L^p,$ provided that the order $m$ of the operator satisfies that $ m \leq -\varkappa\left|\frac{1}{p}-\frac{1}{2}\right|, 1 \leq p \leq \infty. $ This condition on the order $m$ is sharp.

</details>


### [41] [Global boundedness and absorbing sets in two-dimensional chemotaxis-Navier-Stokes systems with weakly singular sensitivity and a sub-logistic source](https://arxiv.org/abs/2512.24892)
*Minh Le,Alexey Cheskidov*

Main category: math.AP

TL;DR: The paper proves global existence and boundedness of classical solutions for a chemotaxis-fluid system with logistic growth and nonlinear sensitivity in 2D bounded domains.


<details>
  <summary>Details</summary>
Motivation: To establish well-posedness and long-time behavior for a complex chemotaxis-fluid system with nonlinear sensitivity (n∇c/c^k) and logistic growth modified by logarithmic damping, which extends previous results on chemotaxis-fluid models.

Method: Uses energy estimates and functional analysis techniques to establish a priori bounds, proving global existence of classical solutions via fixed-point arguments and semigroup theory under no-flux/no-flux/Dirichlet boundary conditions.

Result: Shows the system admits globally bounded classical solutions under suitable initial conditions, and possesses an absorbing set in C⁰(Ω̄)×W¹,∞(Ω)×C⁰(Ω̄;ℝ²) topology, indicating long-time stability.

Conclusion: The chemotaxis-fluid system with nonlinear sensitivity and modified logistic growth is well-posed in 2D, with solutions remaining bounded globally in time and exhibiting dissipative dynamics.

Abstract: This paper studies the following chemotaxis-fluid system in a two-dimensional bounded domain $Ω$: \begin{equation*}
  \begin{cases}
  n_t + u \cdot \nabla n &= Δn - χ\nabla \cdot \left (n \frac{\nabla c}{c^k} \right ) + r n - \frac{μn^2}{\log^η(n+e)},
  c_t + u \cdot \nabla c &= Δc - αc + βn,
  u_t + u \cdot \nabla u &= Δu - \nabla P + n \nabla φ+ f,
  \nabla \cdot u &= 0,
  \end{cases} \end{equation*} where $r, μ, α, β, χ$ are positive parameters, $k, η\in (0,1)$, $φ\in W^{2,\infty}(Ω)$, and $f \in C^1\left(\barΩ\times [0, \infty)\right) \cap L^\infty\left(Ω\times (0, \infty)\right)$. We show that, under suitable conditions on the initial data and with no-flux/no-flux/Dirichlet boundary conditions, this system admits a globally bounded classical solution. Furthermore, the system possesses an absorbing set in the topology of $C^0(\barΩ) \times W^{1, \infty}(Ω) \times C^0(\barΩ; \mathbb{R}^2)$.

</details>


### [42] [On exact Observability for Compactly perturbed infinite dimension system](https://arxiv.org/abs/2512.25041)
*Nisrine Charaf,Faouzi Triki*

Main category: math.AP

TL;DR: Study of observability preservation for compactly perturbed infinite-dimensional systems with self-adjoint generators


<details>
  <summary>Details</summary>
Motivation: To understand how observability properties are preserved when infinite-dimensional systems with self-adjoint generators are perturbed by compact operators

Method: Assuming exact observability of the original system, derive sufficient conditions on compact self-adjoint perturbations to maintain exact observability. Analysis based on asymptotic estimation of spectral elements of perturbed unbounded operators.

Result: Developed sufficient conditions for observability preservation under compact perturbations. Derived important intermediate results on asymptotic estimation of spectral elements of perturbed operators.

Conclusion: Observability can be preserved under certain compact perturbations of infinite-dimensional systems with self-adjoint generators, with spectral analysis providing key insights.

Abstract: In this paper, we study the observability of compactly perturbed infinite dimensional systems. Assuming that a given infinite-dimensional system with self-adjoint generator is exactly observable we derive sufficient conditions on a compact self adjoint perturbation to guarantee that the perturbed system stays exactly observable. The analysis is based on a careful asymptotic estimation of the spectral elements of the perturbed unbounded operator in terms of the compact perturbation. These intermediate results are of importance themselves.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [43] [Learning Density Functionals to Bridge Particle and Continuum Scales](https://arxiv.org/abs/2512.23840)
*Edoardo Monti,Peter Yatsyshin,Konstantinos Gkagkas,Andrew B. Duncan*

Main category: physics.comp-ph

TL;DR: A physics-informed learning framework that augments classical density functional theory (cDFT) with neural corrections trained against molecular dynamics data, preserving thermodynamic consistency while improving predictive accuracy for interfacial phenomena.


<details>
  <summary>Details</summary>
Motivation: Predicting interfacial thermodynamics across molecular and continuum scales is challenging. Classical density functional theory (cDFT) provides a first-principles approach but suffers from approximate free-energy functionals that are difficult to generalize and lack accuracy.

Method: Embed compact neural networks within the Helmholtz free-energy functional of cDFT, learning local and nonlocal corrections through adjoint optimization trained directly against molecular dynamics data. This preserves thermodynamic consistency while capturing missing correlations.

Result: The augmented excess free-energy functional quantitatively reproduces equilibrium density profiles, coexistence curves, and surface tensions across broad temperature ranges for Lennard-Jones fluids. It accurately predicts contact angles and droplet shapes far beyond the training regime.

Conclusion: This approach combines interpretability of statistical mechanics with adaptability of machine learning, establishing a general route to learned thermodynamic functionals that bridge molecular simulations and continuum-scale models.

Abstract: Predicting interfacial thermodynamics across molecular and continuum scales remains a central challenge in computational science. Classical density functional theory (cDFT) provides a first-principles route to connect microscopic interactions with macroscopic observables, but its predictive accuracy depends on approximate free-energy functionals that are difficult to generalize. Here we introduce a physics-informed learning framework that augments cDFT with neural corrections trained directly against molecular-dynamics data through adjoint optimization. Rather than replacing the theory with a black-box surrogate, we embed compact neural networks within the Helmholtz free-energy functional, learning local and nonlocal corrections that preserve thermodynamic consistency while capturing missing correlations. Applied to Lennard-Jones fluids, the resulting augmented excess free-energy functional quantitatively reproduces equilibrium density profiles, coexistence curves, and surface tensions across a broad temperature range, and accurately predicts contact angles and droplet shapes far beyond the training regime. This approach combines the interpretability of statistical mechanics with the adaptability of modern machine learning, establishing a general route to learned thermodynamic functionals that bridge molecular simulations and continuum-scale models.

</details>


### [44] [BF-APNN: A Low-Memory Method for Accelerating the Solution of Radiative Transfer Equations](https://arxiv.org/abs/2512.24534)
*Xizhe Xie,Wengu Chen,Weiming Li,Peng Song,Han Wang*

Main category: physics.comp-ph

TL;DR: BF-APNN is a neural network framework that accelerates radiative transfer equation solutions by using basis function expansion to reduce computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Radiative Transfer Equations are high-dimensional and multiscale, making traditional methods computationally expensive. Existing deep learning methods struggle with high-dimensional or nonlinear RTEs.

Method: Proposes Basis Function Asymptotically Preserving Neural Network (BF-APNN) that uses basis function expansion on the microscopic component from micro-macro decomposition to reduce high-dimensional integral evaluations during training.

Result: BF-APNN substantially reduces training time compared to RT-APNN while preserving high solution accuracy. It shows superior performance on complex, high-dimensional RTE problems with nonlinearity, discontinuities, and multiscale behavior.

Conclusion: BF-APNN is a robust tool for radiative transfer computations that effectively addresses computational challenges of high-dimensional, nonlinear RTEs while maintaining accuracy.

Abstract: The Radiative Transfer Equations (RTEs) exhibit high dimensionality and multiscale characteristics, rendering conventional numerical methods computationally intensive. Existing deep learning methods perform well in low-dimensional or linear RTEs, but still face many challenges with high-dimensional or nonlinear RTEs. To overcome these challenges, we propose the Basis Function Asymptotically Preserving Neural Network (BF-APNN), a framework that inherits the advantages of Radiative Transfer Asymptotically Preserving Neural Network (RT-APNN) and accelerates the solution process. By employing basis function expansion on the microscopic component, derived from micro-macro decomposition, BF-APNN effectively mitigates the computational burden associated with evaluating high-dimensional integrals during training. Numerical experiments, which involve challenging RTE scenarios featuring, nonlinearity, discontinuities, and multiscale behavior, demonstrate that BF-APNN substantially reduces training time compared to RT-APNN while preserving high solution accuracy. Moreover, BF-APNN exhibits superior performance in addressing complex, high-dimensional RTE problems, underscoring its potential as a robust tool for radiative transfer computations.

</details>


### [45] [Random Batch Sum-of-Gaussians Method for Molecular Dynamics of Born-Mayer-Huggins Systems](https://arxiv.org/abs/2512.24970)
*Chen Chen,Jiuyang Liang,Zhenli Xu,Qianru Zhang*

Main category: physics.comp-ph

TL;DR: RBSOG+RBL method accelerates BMH potential simulations by combining SOG decomposition for Coulomb interactions with random batch list for short-range parts, achieving 4-10x speedup over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Large-scale MD simulations of ionic materials using BMH potential are computationally expensive due to long-range Coulomb interactions and short-range exponential repulsion/dispersion terms, limiting simulation scale and efficiency.

Method: Extends RBSOG method to BMH systems by: 1) Using SOG decomposition to split potential into short- and long-range parts, 2) Applying importance sampling in Fourier space for long-range Coulomb, 3) Introducing random batch list (RBL) scheme to accelerate short-range interactions including exponential repulsion and dispersion.

Result: Achieved 4-10x speedup over Ewald-based PPPM and 2x speedup over RBSOG-only method on 1000 CPU cores for molten NaCl and mixed alkali halide systems with up to 5 million atoms. Reduced memory usage while maintaining structural and thermodynamic accuracy.

Conclusion: The unified RBSOG+RBL framework provides efficient, scalable, and accurate MD simulations for BMH systems with long-range interactions, demonstrating attractive performance for large-scale ionic material simulations.

Abstract: The Born-Mayer-Huggins (BMH) potential, which combines Coulomb interactions with dispersion and short-range exponential repulsion, is widely used for ionic materials such as molten salts. However, large-scale molecular dynamics simulations of BMH systems are often limited by computation, communication, and memory costs. We recently proposed the random batch sum-of-Gaussians (RBSOG) method, which accelerates Coulomb calculations by using a sum-of-Gaussians (SOG) decomposition to split the potential into short- and long-range parts and by applying importance sampling in Fourier space for the long-range part. In this work, we extend the RBSOG to BMH systems and incorporate a random batch list (RBL) scheme to further accelerate the short-range part, yielding a unified framework for efficient simulations with the BMH potential. The combination of the SOG decomposition and the RBL enables an efficient and scalable treatment of both long- and short-range interactions in BMH system, particularly the RBL well handles the medium-range exponential repulsion and dispersion by the random batch neighbor list. Error estimate is provided to show the theoretical convergence of the RBL force. We evaluate the framework on molten NaCl and mixed alkali halide with up to $5\times10^6$ atoms on $2048$ CPU cores. Compared to the Ewald-based particle-particle particle-mesh method and the RBSOG-only method, our method achieves approximately $4\sim10\times$ and $2\times$ speedups while using $1000$ cores, respectively, under the same level of structural and thermodynamic accuracy and with a reduced memory usage. These results demonstrate the attractive performance of our method in accuracy and scalability for MD simulations with long-range interactions.

</details>


### [46] [Fast Poisson brackets and constraint algebras in canonical gravity](https://arxiv.org/abs/2512.25007)
*Will Barker*

Main category: physics.comp-ph

TL;DR: A computer algebra package for efficiently computing Poisson brackets and constraint algebras in gravity theories, tested on GR and modified gravity.


<details>
  <summary>Details</summary>
Motivation: Dirac's Hamiltonian constraint algorithm is essential for analyzing propagating modes and gauge symmetries in gravity theories, but its implementation is notoriously difficult and time-consuming.

Method: Developed a computer algebra package that automates the computation of Poisson brackets and reconstruction of constraint algebras, making the Dirac algorithm more efficient and accessible.

Result: Successfully stress-tested the package on pure general relativity and various modified gravity theories, including order reduction of general relativity at two loops.

Conclusion: The presented computer algebra package provides a practical solution to the arduous implementation of Dirac's Hamiltonian constraint algorithm, facilitating analysis of gravity theories and their pathologies.

Abstract: In the study of alternative or extended theories of gravity, Dirac's Hamiltonian constraint algorithm is invaluable for enumerating the propagating modes and gauge symmetries. For gravity, this canonical approach is frequently applied as a means for finding pathologies such as strongly coupled modes; more generally it facilitates the reconstruction of gauge symmetries and the quantization of gauge theories. For gravity, however, the algorithm can become notoriously arduous to implement. We present a simple computer algebra package for efficiently computing Poisson brackets and reconstructing constraint algebras. The tools are stress-tested against pure general relativity and modified gravity, including the order reduction of general relativity at two loops.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [47] [Autoregressive long-horizon prediction of plasma edge dynamics](https://arxiv.org/abs/2512.23884)
*Hunor Csala,Sebastian De Pascuale,Paul Laiu,Jeremy Lore,Jae-Sun Park,Pei Zhang*

Main category: physics.plasm-ph

TL;DR: Transformer-based autoregressive surrogate models for fast prediction of 2D time-dependent plasma edge states, trained on SOLPS-ITER data, enabling orders-of-magnitude faster simulations than high-fidelity codes.


<details>
  <summary>Details</summary>
Motivation: High-fidelity edge fluid/neutral codes like SOLPS-ITER are computationally expensive, limiting parameter scans and long transient studies needed for fusion device design. There's a need for faster surrogate models to enable rapid exploration of plasma edge dynamics.

Method: Transformer-based autoregressive surrogate models trained on SOLPS-ITER spatiotemporal data to forecast electron temperature, electron density, and radiated power. Models evaluated with increasing autoregressive horizons (1-100 steps) on short- and long-horizon prediction tasks.

Result: Longer-horizon training improves rollout stability and reduces error accumulation, enabling stable predictions over hundreds to thousands of steps while reproducing key dynamical features. Surrogate is orders of magnitude faster than SOLPS-ITER in wall-clock time. Accuracy degrades when predicting outside training data regimes.

Conclusion: Transformer-based surrogates provide fast, accurate alternatives to computationally intensive plasma edge simulations, supporting rapid scenario exploration, control-oriented studies, and progress toward real-time applications in fusion devices. Future work needed for data enrichment and physics-informed constraints.

Abstract: Accurate modeling of scrape-off layer (SOL) and divertor-edge dynamics is vital for designing plasma-facing components in fusion devices. High-fidelity edge fluid/neutral codes such as SOLPS-ITER capture SOL physics with high accuracy, but their computational cost limits broad parameter scans and long transient studies. We present transformer-based, autoregressive surrogates for efficient prediction of 2D, time-dependent plasma edge state fields. Trained on SOLPS-ITER spatiotemporal data, the surrogates forecast electron temperature, electron density, and radiated power over extended horizons. We evaluate model variants trained with increasing autoregressive horizons (1-100 steps) on short- and long-horizon prediction tasks. Longer-horizon training systematically improves rollout stability and mitigates error accumulation, enabling stable predictions over hundreds to thousands of steps and reproducing key dynamical features such as the motion of high-radiation regions. Measured end-to-end wall-clock times show the surrogate is orders of magnitude faster than SOLPS-ITER, enabling rapid parameter exploration. Prediction accuracy degrades when the surrogate enters physical regimes not represented in the training dataset, motivating future work on data enrichment and physics-informed constraints. Overall, this approach provides a fast, accurate surrogate for computationally intensive plasma edge simulations, supporting rapid scenario exploration, control-oriented studies, and progress toward real-time applications in fusion devices.

</details>


### [48] [The role of particle feedback on particle acceleration in magnetic reconnection](https://arxiv.org/abs/2512.24054)
*Shimin Liang,Nianyu Yi*

Main category: physics.plasm-ph

TL;DR: Particle feedback in magnetic reconnection amplifies shear flows in magnetic islands, strengthening convective electric fields and boosting particle acceleration, resulting in higher maximum energies and harder spectra, while guide fields suppress these effects.


<details>
  <summary>Details</summary>
Motivation: To understand how particle feedback affects magnetic reconnection and particle acceleration processes in astrophysical plasmas, particularly the complex interplay between feedback mechanisms, guide fields, and reconnection dynamics.

Method: 2.5D magnetohydrodynamic (MHD) simulations using a co-evolving fluid-particle framework to investigate particle feedback effects on reconnection and acceleration.

Result: Particle feedback amplifies shear flows within magnetic islands, strengthening convective electric fields and boosting particle acceleration, leading to higher maximum particle energies and harder non-thermal energy spectra. Guide fields suppress both gas internal energy increase and particle acceleration.

Conclusion: The study reveals a complex feedback mechanism where particles influence reconnection dynamics, highlighting the importance of considering particle feedback in understanding magnetic reconnection and particle acceleration in astrophysical contexts.

Abstract: Magnetic reconnection is a ubiquitous process in astrophysical plasmas and an efficient mechanism for particle acceleration. Using 2.5D magnetohydrodynamic (MHD) simulations with a co-evolving fluid-particle framework, we investigate how particle feedback affects reconnection and acceleration. Our simulations demonstrate that particle feedback to the fluid amplifies shear flows within magnetic islands, which strengthens the convective electric field and thereby boosts particle acceleration. This mechanism results in a higher maximum particle energy and a harder non-thermal energy spectrum. The guide field suppresses both the increase in gas internal energy and particle acceleration. These findings highlight the complex interplay between feedback, guide fields, and reconnection dynamics.

</details>


### [49] [Coordinates based on a magnetic mirror field](https://arxiv.org/abs/2512.24305)
*R. D. Hazeltine*

Main category: physics.plasm-ph

TL;DR: Constructing a coordinate system tailored to cylindrically symmetric magnetic field geometry


<details>
  <summary>Details</summary>
Motivation: Standard coordinate systems may not align well with complex magnetic field geometries, making analysis and calculations difficult. A coordinate system that naturally fits the magnetic field structure would simplify physical descriptions and computations.

Method: Develop a specialized coordinate system construction method that adapts to the cylindrical symmetry of magnetic fields, likely using field lines as coordinate curves or creating orthogonal/non-orthogonal systems aligned with field geometry.

Result: Successfully created a coordinate system that naturally conforms to cylindrically symmetric magnetic field configurations, providing a more natural framework for describing field properties and particle dynamics.

Conclusion: The constructed coordinate system offers improved analytical and computational advantages for studying cylindrically symmetric magnetic fields compared to conventional coordinate systems.

Abstract: We construct a coordinate system fitting the geometry of a given, cylindrically symmetric, magnetic field.

</details>


### [50] [Computing Flux-Surface Shapes in Tokamaks and Stellarators](https://arxiv.org/abs/2512.24544)
*M. J. Gerard,M. J. Pueschel,S. Stewart,H. O. M. Hillebrecht,B. Geiger*

Main category: physics.plasm-ph

TL;DR: A new method for characterizing stellarator magnetic field geometry using Fourier analysis of flux-surface shaping modes, revealing that quasi-symmetry emerges from spatial resonance between shape complexity and rotation about the magnetic axis.


<details>
  <summary>Details</summary>
Motivation: There is currently no agreed-upon methodology for characterizing stellarator magnetic field geometry, despite modern stellarator designs achieving high levels of magnetic-field quasi-symmetry through careful flux-surface shaping. The lack of standardized characterization methods hinders systematic investigations of flux-surface geometries.

Method: A general Fourier mode analysis framework for computing ideal-MHD equilibrium shapes applicable to both axisymmetric and non-axisymmetric configurations. The method defines shaping modes (elongation, triangularity, squareness, etc.) for non-planar cross-sections, with the additional degree of freedom in non-axisymmetric equilibria manifesting as rotation of each shaping mode about the magnetic axis.

Result: Analysis of non-axisymmetric configurations with precise quasi-symmetry and QUASR database cases shows that quasi-symmetry results from spatial resonance between shape complexity and shape rotation about the magnetic axis. The quantitative features of this resonance correlate closely with rotational transform and number of field periods.

Conclusion: The proposed shaping paradigm can facilitate systematic investigations into the relationship between general flux-surface geometries and other figures of merit, providing a standardized methodology for characterizing stellarator magnetic field geometry that reveals fundamental connections between shaping parameters and quasi-symmetry.

Abstract: There is currently no agreed-upon methodology for characterizing a stellarator magnetic field geometry, and yet modern stellarator designs routinely attain high levels of magnetic-field quasi-symmetry through careful flux-surface shaping. Here, we introduce a general method for computing the shape of an ideal-MHD equilibrium that can be used in both axisymmetric and non-axisymmetric configurations. This framework uses a Fourier mode analysis to define the shaping modes (e.g. elongation, triangularity, squareness, etc.) of cross-sections that can be non-planar. Relative to an axisymmetric equilibrium, the additional degree of freedom in a non-axisymmetric equilibrium manifests as a rotation of each shaping mode about the magnetic axis. Using this method, a shaping analysis is performed on non-axisymmetric configurations with precise quasi-symmetry and select cases from the QUASR database spanning a range of quasi-symmetry quality. Empirically, we find that quasi-symmetry results from a spatial resonance between shape complexity and shape rotation about the magnetic axis. The quantitative features of this resonance correlate closely with a configuration's rotational transform and number of field periods. Based on these observations, it is conjectured that this shaping paradigm can facilitate systematic investigations into the relationship between general flux-surface geometries and other figures of merit.

</details>


### [51] [Cataloging the nonlinear waves excited by moving a charged body in the dusty plasma medium](https://arxiv.org/abs/2512.24723)
*Swathi S Krishna,S. K. Mishra,S. Jaiswal*

Main category: physics.plasm-ph

TL;DR: Study examines nonlinear waves generated by charged body moving through dusty plasma, described by forced KdV equation, showing wave evolution depends on source parameters beyond just Mach number.


<details>
  <summary>Details</summary>
Motivation: To understand how charged bodies moving through dusty plasma generate diverse nonlinear waves (precursors, pinned solitons) and investigate the role of source parameters in shaping these wave excitations.

Method: Theoretical analysis using forced Korteweg-de Vries (fKdV) equation under weakly nonlinear and dispersive limits, examining effects of three source parameters: amplitude, width, and flow speed.

Result: Nonlinear structure excitation depends not just on Mach number but also on source features (amplitude, width). Discovery of novel lagging structures that maintain shape and speed while propagating behind the source.

Conclusion: Moving charged bodies in dusty plasma generate complex nonlinear waves influenced by multiple source parameters, with first theoretical depiction of lagging structures that propagate behind the source.

Abstract: The nonlinear waves excited by the movement of a charged body in the dusty plasma medium are studied. A charged body moving through a dusty plasma medium can generate diverse nonlinear waves, such as precursors and pinned solitons. These wave excitations under weakly nonlinear and dispersive limits are described theoretically by the forced Korteweg-de Vries (fKdV) type equation. We have examined the role of the driver in shaping and evolving these wave excitations. In particular we studied the effect of primarily three source parameters, namely, amplitude, width, and flow speed, on the evolution of nonlinear structures. The driver generates a perturbation in the stable system configuration, which couples with medium characteristics and eventually evolves into propagating excitations. Our finding shows that the excitation of nonlinear structure by a moving body in a plasma medium is not just dictated by the mach number but also the features of the source such as amplitude and width. As a novel finding apart from pinned and precursor solitons, we observe another nonlinear structure that lags behind the source term, maintaining its shape and speed as it propagates. These features are the first ever theoretical depiction of such lagging structures.

</details>


### [52] [Runaway electron avalanche and macroscopic beam formation: simulations of the DTT full power scenario](https://arxiv.org/abs/2512.24760)
*E. Emanuelli,F. Vannini,M. Hoelzl,E. Nardon,V. Bandaru,N. Schwarz,D. Bonfiglio,G. Ramogida,F. Subba,JOREK Team*

Main category: physics.plasm-ph

TL;DR: DTT facility's transition from low-current (2 MA) to full-power (5.5 MA) operation dramatically increases runaway electron avalanche risk during disruptions, requiring careful impurity injection balancing to mitigate thermal loads while avoiding dangerous RE beam formation.


<details>
  <summary>Details</summary>
Motivation: The transition from DTT's initial commissioning phase (2 MA) to full power operation (5.5 MA) introduces critical changes in runaway electron dynamics during disruptions. Previous studies showed safety margins at low currents, but the exponential scaling of RE avalanche gain with plasma current raises concerns about operational safety in the full power scenario.

Method: Used the non-linear magnetohydrodynamic code JOREK to perform comprehensive 2D simulations of the current quench phase during disruptions. Systematically scanned initial RE seed currents and injected impurity levels to analyze RE avalanche behavior in different operational scenarios.

Result: In the full power scenario, avalanche multiplication factor reaches G_av ≈ 1.3×10^5, allowing a tiny 5.5 A seed current to grow into macroscopic RE beams of ≈0.7 MA with high impurity levels. For higher RE seeds, RE current can peak at ≈3.2 MA, constituting up to 80% of total plasma current during current quench.

Conclusion: Unlike the Day-0 phase, DTT's full power scenario requires careful balancing between thermal load mitigation and RE avoidance through precisely chosen impurity injection quantities. This work establishes the baseline needed for future RE load estimations on plasma-facing components and design of mitigation systems like sacrificial limiters.

Abstract: The transition of the Divertor Tokamak Test (DTT) facility from its initial commissioning phase (Day-0, plasma current $I_{p}=2$ MA) to the full power scenario ($I_{p}=5.5$ MA) introduces a critical shift in the dynamics of runaway electrons (REs) generation. While previous predictive studies of the low-current scenario indicated a robust safety margin against RE beam formation, this work reveals that the exponential scaling of the RE avalanche gain with plasma current severely narrows the safe operational window in the full power scenario. Using the non-linear magnetohydrodynamic code JOREK, we perform comprehensive 2D simulations of the current quench (CQ) phase of several disruption scenarios, systematically scanning initial RE seed currents and injected impurity levels. The results demonstrate that in the full power scenario, the avalanche multiplication factor is sufficiently high ($G_\text{av} \approx 1.3 \cdot 10^5$) to convert a mere 5.5 A seed current into macroscopic RE beams of $\approx 0.7$ MA when large amounts of impurities are present. For even higher RE seeds, the RE current can peak at $ \approx 3.2$ MA, constituting up to $\approx$ 80% of the total plasma current during the CQ. These findings suggest that, unlike the Day-0 phase, the disruption mitigation strategy for the full power scenario involves a careful balance between thermal load mitigation and RE avoidance, necessitating a well-chosen quantity of injected impurities. This work provides the baseline needed for future estimations of RE loads on the plasma-facing components of DTT, which will be essential for designing and positioning mitigation components like sacrificial limiters.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [53] [Computational Analysis of Disease Progression in Pediatric Pulmonary Arterial Hypertension](https://arxiv.org/abs/2512.25027)
*Omar Said,Christopher Tossas-Betancourt,Mary K. Olive,Jimmy C. Lu,Adam Dorfman,C. Alberto Figueroa*

Main category: physics.med-ph

TL;DR: Researchers developed patient-specific computational models for pediatric pulmonary arterial hypertension using MRI and catheterization data, creating an automated calibration system that reduces setup time from weeks to days.


<details>
  <summary>Details</summary>
Motivation: Pediatric PAH is understudied due to limited data and lack of targeted diagnostic/therapeutic strategies. There's a need for better tools to understand disease progression and inform treatment decisions.

Method: Created multi-scale patient-specific cardiovascular models for 4 pediatric PAH patients using longitudinal MRI and catheterization data. Used CRIMSON framework to couple 3D FSI models of pulmonary arteries with 0D lumped-parameter heart and Windkessel models. Developed automated Python-based optimizer to calibrate boundary conditions.

Result: Successfully calibrated models with reduced calibration time from weeks to days. Model-derived metrics (arterial stiffness, pulse wave velocity, resistance, compliance) aligned with clinical indicators of disease severity and progression.

Conclusion: Computational modeling can non-invasively capture patient-specific hemodynamic adaptation over time, offering a promising tool for monitoring pediatric PAH and informing future treatment strategies.

Abstract: Pulmonary arterial hypertension (PAH) is a progressive cardiopulmonary disease that leads to increased pulmonary pressures, vascular remodeling, and eventual right ventricular (RV) failure. Pediatric PAH remains understudied due to limited data and the lack of targeted diagnostic and therapeutic strategies. In this study, we developed and calibrated multi-scale, patient-specific cardiovascular models for four pediatric PAH patients using longitudinal MRI and catheterization data collected approximately two years apart. Using the CRIMSON simulation framework, we coupled three-dimensional fluid-structure interaction (FSI) models of the pulmonary arteries with zero-dimensional (0D) lumped-parameter heart and Windkessel models to simulate patient hemodynamics. An automated Python-based optimizer was developed to calibrate boundary conditions by minimizing discrepancies between simulated and clinical metrics, reducing calibration time from weeks to days. Model-derived metrics such as arterial stiffness, pulse wave velocity, resistance, and compliance were found to align with clinical indicators of disease severity and progression. Our findings demonstrate that computational modeling can non-invasively capture patient-specific hemodynamic adaptation over time, offering a promising tool for monitoring pediatric PAH and informing future treatment strategies.

</details>


### [54] [Finite element analysis of very large bone models based on micro-CT scans](https://arxiv.org/abs/2512.24401)
*Shani Martinez-Weissberg,Will Pazner,Zohar Yosibash*

Main category: physics.med-ph

TL;DR: Open-source μFE framework enables large-scale biomechanical analysis of intact rabbit femur using μCT data, validated with experiments, showing 40μm resolution balances accuracy and computational cost.


<details>
  <summary>Details</summary>
Motivation: High-resolution voxel-based μFE models from μCT imaging are computationally challenging at anatomically relevant scales, requiring scalable solutions for realistic bone mechanics analysis.

Method: Integrated framework using MIA clustering for segmentation, MFEM library for solving large-scale linear elasticity systems, validated with commercial solver and Digital Image Correlation experiments on NZW rabbit femur.

Result: Successfully solved models with over 800M DOFs using moderate HPC; 40μm voxel size preserves accuracy while reducing cost; segmentation parameters affect mechanical response; calibrated effective bone material properties.

Conclusion: Large-scale, experimentally informed μFE modeling is feasible with open-source tools, providing robust foundation for preclinical bone mechanics assessment and treatment risk evaluation.

Abstract: High-resolution voxel-based micro-finite element ($μ$FE) models derived from $μ$CT imaging enable detailed investigation of bone mechanics but remain computationally challenging at anatomically relevant scales. This study presents a comprehensive $μ$FE framework for large-scale biomechanical analysis of an intact New Zealand White (NZW) rabbit femur, integrating advanced segmentation, scalable finite element solvers, and experimental validation using predominantly open-source libraries. Bone geometries were segmented from $μ$CT data using the MIA clustering algorithm and converted into voxel-based $μ$FE meshes, which were solved using the open-source MFEM library with algorithms designed for large-scale linear elasticity systems.
  The numerical solutions were verified by comparing with a commercial finite element solver, and by evaluating the performance of full assembly and element-by-element formulations within MFEM. Models containing over $8\times10^{8}$ DOFs were solved using moderate HPC resources, demonstrating the feasibility of anatomically realistic $μ$FE simulations at this scale. Resolution effects were investigated by comparing models with voxel sizes of 20, 40, and 80 $μ$m, revealing that 40 $μ$m preserves boundary displacement and principal strain distributions with minimal bias while significantly reducing computational cost. Sensitivity analyses further showed that segmentation parameters influence the global mechanical response.
  Finally, $μ$FE predictions were coupled with Digital Image Correlation measurements on an NZW rabbit femur under compression to calibrate effective bone material properties at the micron scale. The results demonstrate that large-scale, experimentally informed $μ$FE modeling can be achieved using open-source tools, providing a robust foundation for preclinical assessment of bone mechanics and treatment-related risks.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [55] [Stochastic Galerkin Method and Hierarchical Preconditioning for PDE-constrained Optimization](https://arxiv.org/abs/2512.23804)
*Zhendong Li,Akwum Onwunta,Bedřich Sousedík*

Main category: math.OC

TL;DR: Hierarchical preconditioners for PDE-constrained optimal control under uncertainty, using stochastic Galerkin and polynomial chaos expansions to accelerate iterative solvers.


<details>
  <summary>Details</summary>
Motivation: Address computational challenges of large-scale, ill-conditioned linear systems in uncertainty quantification for PDE-constrained optimal control problems with uncertain coefficients.

Method: Discretize-then-optimize framework combining finite elements, stochastic Galerkin approximation, advanced time-discretization, and hierarchical preconditioners exploiting sparsity in polynomial chaos expansions.

Result: Proposed preconditioners significantly accelerate iterative solver convergence compared to existing methods, providing robust and efficient solvers for steady-state and time-dependent problems.

Conclusion: Hierarchical preconditioners based on truncated stochastic expansions effectively balance computational cost and preconditioning quality for uncertainty quantification in optimal control.

Abstract: We develop efficient hierarchical preconditioners for optimal control problems governed by partial differential equations with uncertain coefficients. Adopting a discretize-then-optimize framework that integrates finite element discretization, stochastic Galerkin approximation, and advanced time-discretization schemes, the approach addresses the challenge of large-scale, ill-conditioned linear systems arising in uncertainty quantification. By exploiting the sparsity inherent in generalized polynomial chaos expansions, we derive hierarchical preconditioners based on truncated stochastic expansion that strike an effective balance between computational cost and preconditioning quality. Numerical experiments demonstrate that the proposed preconditioners significantly accelerate the convergence of iterative solvers compared to existing methods, providing robust and efficient solvers for both steady-state and time-dependent optimal control applications under uncertainty.

</details>


### [56] [The Flow-Limit of Reflect-Reflect-Relax: Existence, Stability, and Discrete-Time Behavior](https://arxiv.org/abs/2512.23843)
*Manish Krishan Lal*

Main category: math.OC

TL;DR: The paper analyzes the Reflect-Reflect-Relax (RRR) algorithm in small-step regime, showing exponential convergence via hyperbolic sink dynamics, constructing Lyapunov functions, proving finite-time capture, and explaining optimal relaxation parameters through flow discretization.


<details>
  <summary>Details</summary>
Motivation: To understand the convergence behavior and performance characteristics of the Reflect-Reflect-Relax (RRR) algorithm in its small-step regime, particularly explaining the emergence of iteration-optimal relaxation parameters and performance deterioration near the Douglas-Rachford limit.

Method: The study uses dynamical systems analysis in both continuous (flow-limit) and discrete settings. In the continuous setting, it analyzes transverse dynamics as hyperbolic sinks and constructs Lyapunov functions. In the discrete setting, it shows RRR is a forward-Euler discretization of the flow, and introduces a mesoscopic framework using percolation and renormalization group theory.

Result: Proves exponential decay of gap measure via hyperbolic sink dynamics, constructs strict Lyapunov function excluding recurrent/chaotic behavior, shows finite-time capture into solution domains, and demonstrates that solution times converge to finite limits while iteration counts diverge, explaining optimal relaxation parameters.

Conclusion: The small-step RRR algorithm exhibits well-behaved convergence dynamics with exponential decay, finite-time capture properties, and its discretization properties explain the emergence of optimal relaxation parameters, with performance deterioration near the Douglas-Rachford limit organized by mesoscopic percolation theory.

Abstract: We study the Reflect-Reflect-Relax (RRR) algorithm in its small-step (flow-limit) regime. In the smooth transversal setting, we show that the transverse dynamics form a hyperbolic sink, yielding exponential decay of a natural gap measure. Under uniform geometric assumptions, we construct a tubular neighborhood of the feasible manifold on which the squared gap defines a strict Lyapunov function, excluding recurrent dynamics and chaotic behavior within this basin.
  In the discrete setting, the induced flow is piecewise constant on W-domains and supports Filippov sliding along convergent boundaries, leading to finite-time capture into a solution domain. We prove that small-step RRR is a forward-Euler discretization of this flow, so that solution times measured in rescaled units converge to a finite limit while iteration counts diverge, explaining the emergence of iteration-optimal relaxation parameters. Finally, we introduce a heuristic mesoscopic framework based on percolation and renormalization group to organize performance deterioration near the Douglas-Rachford limit.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [57] [Basic Inequalities for First-Order Optimization with Applications to Statistical Risk Analysis](https://arxiv.org/abs/2512.24999)
*Seunghoon Paik,Kangjie Zhou,Matus Telgarsky,Ryan J. Tibshirani*

Main category: math.ST

TL;DR: The paper introduces "basic inequalities" as a unified framework connecting implicit and explicit regularization in first-order optimization algorithms, providing a tool for statistical analysis of training dynamics and prediction risk.


<details>
  <summary>Details</summary>
Motivation: To develop a simple, versatile framework that connects implicit and explicit regularization in optimization algorithms, providing a unified tool for statistical analysis that can handle various algorithms beyond just gradient descent.

Method: Introduces "basic inequalities" that bound the objective function difference f(θ_T)-f(z) in terms of accumulated step sizes and distances between initial point, current iterate, and reference point z. This translates iteration count into an effective regularization coefficient.

Result: The framework successfully analyzes training dynamics and provides prediction risk bounds for gradient descent, mirror descent with Bregman divergence projection, generalized linear models trained by gradient descent and exponentiated gradient descent, and randomized predictors.

Conclusion: The basic inequalities framework provides a powerful, unified approach to understanding regularization in optimization algorithms, connecting implicit and explicit regularization, with applications across various algorithms and models, supported by both theoretical analysis and experimental validation.

Abstract: We introduce \textit{basic inequalities} for first-order iterative optimization algorithms, forming a simple and versatile framework that connects implicit and explicit regularization. While related inequalities appear in the literature, we isolate and highlight a specific form and develop it as a well-rounded tool for statistical analysis. Let $f$ denote the objective function to be optimized. Given a first-order iterative algorithm initialized at $θ_0$ with current iterate $θ_T$, the basic inequality upper bounds $f(θ_T)-f(z)$ for any reference point $z$ in terms of the accumulated step sizes and the distances between $θ_0$, $θ_T$, and $z$. The bound translates the number of iterations into an effective regularization coefficient in the loss function. We demonstrate this framework through analyses of training dynamics and prediction risk bounds. In addition to revisiting and refining known results on gradient descent, we provide new results for mirror descent with Bregman divergence projection, for generalized linear models trained by gradient descent and exponentiated gradient descent, and for randomized predictors. We illustrate and supplement these theoretical findings with experiments on generalized linear models.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [58] [Green's function on the Tate curve](https://arxiv.org/abs/2512.24935)
*An Huang,Rebecca Rohrlich,Yaojia Sun,Eric Whyman*

Main category: math.NT

TL;DR: The paper defines a Laplacian operator on the Tate curve and proves the existence and provides an explicit formula for its Green's function, establishing a p-adic counterpart to the Archimedean Green's function on a flat torus.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the need to define a p-adic string worldsheet action in genus one, which requires understanding the Laplacian operator and its Green's function on the Tate curve.

Method: The authors define a Laplacian operator on the Tate curve and study its Green's function. They prove the existence of this Green's function and derive an explicit formula for it.

Result: The main results are: (1) The Green's function exists on the Tate curve, and (2) An explicit formula is provided for this Green's function, which serves as a non-Archimedean counterpart to the Archimedean Green's function on a flat torus.

Conclusion: The paper successfully establishes the mathematical foundation for p-adic string theory in genus one by defining and analyzing the Laplacian operator and its Green's function on the Tate curve, providing a non-Archimedean analog to the familiar Archimedean case.

Abstract: Motivated by the question of defining a $p$-adic string worldsheet action in genus one, we define a Laplacian operator on the Tate curve, and study its Green's function. We show that the Green's function exists. We provide an explicit formula for the Green's function, which turns out to be a non-Archimedean counterpart of the Archimedean Green's function on a flat torus.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [59] [Assessing generative modeling approaches for free energy estimates in condensed matter](https://arxiv.org/abs/2512.23930)
*Maximilian Schebek,Jiajun He,Emil Hoffmann,Yuanqi Du,Frank Noé,Jutta Rogal*

Main category: cond-mat.stat-mech

TL;DR: Systematic review and benchmarking of generative-model-based free energy estimation methods for condensed-matter systems, comparing discrete/continuous normalizing flows, FEAT, and escorted Jarzynski equality.


<details>
  <summary>Details</summary>
Motivation: Traditional free energy estimation methods require sampling multiple intermediate states and are computationally expensive. Recent generative-model-based methods bypass intermediate states but lack systematic comparison of their trade-offs between efficiency, accuracy, and scalability.

Method: Systematic review of generative-model-based methods and benchmarking of selected approaches: discrete/continuous normalizing flows with targeted free energy perturbation, FEAT (Free energy Estimators with Adaptive Transport), and escorted Jarzynski equality. Evaluated on coarse-grained monatomic ice and Lennard-Jones solids.

Result: Evaluation of accuracy, data efficiency, computational cost, and scalability with system size. Provides quantitative framework for selecting effective free energy estimation strategies in condensed-phase systems.

Conclusion: The study offers systematic comparison and benchmarking results to guide selection of optimal free energy estimation methods based on specific requirements of efficiency, accuracy, and scalability for condensed-matter systems.

Abstract: The accurate estimation of free energy differences between two states is a long-standing challenge in molecular simulations. Traditional approaches generally rely on sampling multiple intermediate states to ensure sufficient overlap in phase space and are, consequently, computationally expensive. Several generative-model-based methods have recently addressed this challenge by learning a direct bridge between distributions, bypassing the need for intermediate states. However, it remains unclear which approaches provide the best trade-off between efficiency, accuracy, and scalability. In this work, we systematically review these methods and benchmark selected approaches with a focus on condensed-matter systems. In particular, we investigate the performance of discrete and continuous normalizing flows in the context of targeted free energy perturbation as well as FEAT (Free energy Estimators with Adaptive Transport) together with the escorted Jarzynski equality, using coarse-grained monatomic ice and Lennard-Jones solids as benchmark systems. We evaluate accuracy, data efficiency, computational cost, and scalability with system size. Our results provide a quantitative framework for selecting effective free energy estimation strategies in condensed-phase systems.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [60] [Bridging Visual Intuition and Chemical Expertise: An Autonomous Analysis Framework for Nonadiabatic Dynamics Simulations via Mentor-Engineer-Student Collaboration](https://arxiv.org/abs/2512.24133)
*Yifei Zhu,Jiahui Zhang,Binni Huang,Zhenggang Lan*

Main category: physics.chem-ph

TL;DR: VisU is a vision-driven AI framework that uses large language models to autonomously analyze nonadiabatic molecular dynamics trajectories through a virtual research collective approach, reducing reliance on expert intuition.


<details>
  <summary>Details</summary>
Motivation: Traditional analysis of nonadiabatic molecular dynamics trajectories heavily depends on expert intuition and visual pattern recognition, which is difficult to formalize and scale. There's a need for more systematic, automated approaches that can bridge visual insight with chemical expertise.

Method: VisU uses a "virtual research collective" with a "Mentor-Engineer-Student" paradigm leveraging two state-of-the-art large language models. The Mentor provides physical intuition through visual reasoning, the Engineer constructs analysis scripts adaptively, and the Student executes pipelines and manages data. The framework autonomously orchestrates a four-stage workflow: Preprocessing, Recursive Channel Discovery, Important-Motion Identification, and Validation/Summary.

Result: The framework identifies reaction channels and key nuclear motions while generating professional academic reports. It establishes a new paradigm for human-AI collaboration in analyzing excited-state dynamics simulation results.

Conclusion: VisU significantly reduces dependence on manual interpretation and enables more intuitive, scalable mechanistic discovery by bridging visual insight with chemical expertise through AI-driven collaborative analysis.

Abstract: Analyzing nonadiabatic molecular dynamics trajectories traditionally heavily relies on expert intuition and visual pattern recognition, a process that is difficult to formalize. We present VisU, a vision-driven framework that leverages the complementary strengths of two state-of-the-art large language models to establish a "virtual research collective." This collective operates through a "Mentor-Engineer-Student" paradigm that mimics the collaborative intelligence of a professional chemistry laboratory. Within this ecosystem, the Mentor provides physical intuition through visual reasoning, while the Engineer adaptively constructs analysis scripts, and the Student executes the pipeline and manages the data and results. VisU autonomously orchestrates a four-stage workflow comprising Preprocessing, Recursive Channel Discovery, Important-Motion Identification, and Validation/Summary. This systematic approach identifies reaction channels and key nuclear motions while generating professional academic reports. By bridging visual insight with chemical expertise, VisU establishes a new paradigm for human-AI collaboration in the analysis of excited-state dynamics simulation results, significantly reducing dependence on manual interpretation and enabling more intuitive, scalable mechanistic discovery.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [61] [NeuralCrop: Combining physics and machine learning for improved crop yield predictions](https://arxiv.org/abs/2512.20177)
*Yunan Lin,Sebastian Bathiany,Maha Badri,Maximilian Gelbrecht,Philipp Hess,Brian Groenke,Jens Heinke,Christoph Müller,Niklas Boers*

Main category: cs.LG

TL;DR: NeuralCrop is a hybrid crop model combining process-based GGCMs with machine learning that outperforms state-of-the-art models in yield prediction, especially under drought conditions, while maintaining robustness to climate change.


<details>
  <summary>Details</summary>
Motivation: Traditional GGCMs have substantial uncertainties due to limited process understanding, while pure machine learning models fail to generalize to changing climate conditions outside their training distributions.

Method: Hybrid approach combining advanced process-based GGCM with data-driven ML components, first trained to emulate a competitive GGCM then fine-tuned on observational data.

Result: NeuralCrop outperforms state-of-the-art GGCMs across site-level and large-scale cropping regions, particularly improving drought prediction accuracy. Maintains robust projections under unseen conditions where pure ML models degrade.

Conclusion: Hybrid crop modeling approach offers improved crop modeling and more reliable yield projections under climate change and extreme weather conditions.

Abstract: Global gridded crop models (GGCMs) simulate daily crop growth by explicitly representing key biophysical processes and project end-of-season yield time series. They are a primary tool to quantify the impacts of climate change on agricultural productivity and assess associated risks for food security. Despite decades of development, state-of-the-art GGCMs still have substantial uncertainties in simulating complex biophysical processes due to limited process understanding. Recently, machine learning approaches trained on observational data have shown great potential in crop yield predictions. However, these models have not demonstrated improved performance over classical GGCMs and are not suitable for simulating crop yields under changing climate conditions due to problems in generalizing outside their training distributions. Here we introduce NeuralCrop, a hybrid GGCM that combines the strengths of an advanced process-based GGCM, resolving important processes explicitly, with data-driven machine learning components. The model is first trained to emulate a competitive GGCM before it is fine-tuned on observational data. We show that NeuralCrop outperforms state-of-the-art GGCMs across site-level and large-scale cropping regions. Across moisture conditions, NeuralCrop reproduces the interannual yield anomalies in European wheat regions and the US Corn Belt more accurately during the period from 2000 to 2019 with particularly strong improvements under drought extremes. When generalizing to conditions unseen during training, NeuralCrop continues to make robust projections, while pure machine learning models exhibit substantial performance degradation. Our results show that our hybrid crop modelling approach offers overall improved crop modeling and more reliable yield projections under climate change and intensifying extreme weather conditions.

</details>


### [62] [Generative forecasting with joint probability models](https://arxiv.org/abs/2512.24446)
*Patrick Wyrod,Ashesh Chattopadhyay,Daniele Venturi*

Main category: cs.LG

TL;DR: Joint generative forecasting learns probability distributions over lagged system states to improve chaotic system prediction, outperforming conventional next-step models in short-term skill and long-range statistics.


<details>
  <summary>Details</summary>
Motivation: Chaotic systems have fundamental forecasting limits due to sensitivity to initial conditions and unresolved multiscale processes. Existing generative models focus on next-step prediction rather than capturing underlying dynamic structure.

Method: Reframe forecasting as learning joint probability distributions over lagged system states in temporal windows, with forecasts obtained through marginalization. Introduce model-agnostic training/inference framework with three uncertainty metrics: ensemble variance, short-horizon autocorrelation, and cumulative Wasserstein drift.

Result: Joint generative models show improved short-term predictive skill, preserve attractor geometry, and achieve substantially more accurate long-range statistical behavior than conventional next-step models on Lorenz-63 and Kuramoto-Sivashinsky systems.

Conclusion: Learning joint distributions over system states provides better chaotic system forecasting by capturing nonlinear temporal dependencies and enabling robust uncertainty quantification without ground truth access.

Abstract: Chaotic dynamical systems exhibit strong sensitivity to initial conditions and often contain unresolved multiscale processes, making deterministic forecasting fundamentally limited. Generative models offer an appealing alternative by learning distributions over plausible system evolutions; yet, most existing approaches focus on next-step conditional prediction rather than the structure of the underlying dynamics. In this work, we reframe forecasting as a fully generative problem by learning the joint probability distribution of lagged system states over short temporal windows and obtaining forecasts through marginalization. This new perspective allows the model to capture nonlinear temporal dependencies, represent multistep trajectory segments, and produce next-step predictions consistent with the learned joint distribution. We also introduce a general, model-agnostic training and inference framework for joint generative forecasting and show how it enables assessment of forecast robustness and reliability using three complementary uncertainty quantification metrics (ensemble variance, short-horizon autocorrelation, and cumulative Wasserstein drift), without access to ground truth. We evaluate the performance of the proposed method on two canonical chaotic dynamical systems, the Lorenz-63 system and the Kuramoto-Sivashinsky equation, and show that joint generative models yield improved short-term predictive skill, preserve attractor geometry, and achieve substantially more accurate long-range statistical behaviour than conventional conditional next-step models.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [63] [Achieving High Efficiency And Enhanced Beam Quality In Laser Wakefield Acceleration](https://arxiv.org/abs/2512.24719)
*Jia Wang,Ming Zeng,Dazhang Li,Wentao Wang,Song Li,Ke Feng,Jie Gao*

Main category: physics.acc-ph

TL;DR: Shorter laser pulses enable two-step dechirping for high-charge electron beams with 1% energy spread and 10-30% energy transfer efficiency in laser wakefield acceleration.


<details>
  <summary>Details</summary>
Motivation: Laser wakefield acceleration offers compact, cost-effective particle acceleration with extremely high gradients (>100GV/m), but faces challenges in improving energy transfer efficiency while maintaining beam quality suitable for practical applications like particle colliders and light sources.

Method: Using shorter laser pulse durations to enable a two-step dechirping process for accelerated electron beams with nanocoulomb-level charge. The approach works across a large parameter space.

Result: Demonstrated generation of electron beams with 1% energy spread and 10-30% energy transfer efficiency. Specific example: produced 420MeV electron beam with 5.5nC charge and 2% RMS energy spread using 8.3J laser pulse with 7.2fs duration.

Conclusion: Shorter laser pulses enable efficient two-step dechirping, achieving both high beam quality (low energy spread) and high energy transfer efficiency in laser wakefield acceleration, advancing its potential for practical applications.

Abstract: Laser wakefield acceleration, characterized by the extremely high electric field gradient exceeding 100GV/m, is regarded as a compact and cost affordable technology for the next generation of particle colliders and light sources. However, it has always been a major challenge to effectively increase the energy transfer efficiency from the laser to the accelerated beam, while ensuring the beam quality remains suitable for practical applications. This study demonstrates that the laser with shorter pulse duration allows for a two-step dechirping process of the accelerated electron beam with charge of nanocoulomb level. The electron beams with an energy spread of 1% can be generated with the energy transfer efficiency of 10% to 30% in a large parameter space. For example, one electron beam with the energy of 420MeV, the charge of 5.5nC and the RMS energy spread of 2% can be produced using an 8.3J laser pulse with 7.2fs duration.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [64] [Polynomial mixing for the stochastic Schrödinger equation with large damping in the whole space](https://arxiv.org/abs/2512.24599)
*Hung D. Nguyen,Kihoon Seong*

Main category: math.PR

TL;DR: The paper establishes polynomial mixing rates for the stochastic nonlinear Schrödinger equation with large damping in dimensions d≤3.


<details>
  <summary>Details</summary>
Motivation: While unique ergodicity is known for the stochastic nonlinear Schrödinger equation under strong damping, the rate of convergence to equilibrium has remained unknown. The authors aim to quantify this mixing behavior.

Method: The approach uses a coupling strategy combined with pathwise Strichartz estimates to analyze the long-time behavior of solutions.

Result: In the regime of large damping, solutions are attracted toward the unique invariant probability measure at polynomial rates of arbitrary order.

Conclusion: The work successfully establishes quantitative mixing rates for the stochastic nonlinear Schrödinger equation, addressing a previously open problem about convergence speed to equilibrium.

Abstract: We study the long-time mixing behavior of the stochastic nonlinear Schrödinger equation in $\mathbb{R}^d$, $d\le 3$. It is well known that, under a sufficiently strong damping force, the system admits unique ergodicity, although the rate of convergence toward equilibrium has remained unknown. In this work, we address the mixing property in the regime of large damping and establish that solutions are attracted toward the unique invariant probability measure at polynomial rates of arbitrary order. Our approach is based on a coupling strategy with pathwise Strichartz estimates.

</details>


### [65] [Heat kernel estimates for Markov processes with jump kernels blowing-up at the boundary](https://arxiv.org/abs/2512.24807)
*Soobin Cho,Panki Kim,Renming Song,Zoran Vondraček*

Main category: math.PR

TL;DR: The paper establishes sharp two-sided heat kernel estimates for symmetric Markov jump processes with boundary-blowup kernels on closed subsets of ℝ^d.


<details>
  <summary>Details</summary>
Motivation: To extend the framework for conservative self-similar Markov processes to more general geometric settings where jump kernels blow up at the boundary, addressing processes that don't have uniformly bounded jump measure tails.

Method: Employ recently developed weighted functional inequalities specifically designed for jump kernels that blow up at the boundary, overcoming the limitation of standard techniques that require uniformly bounded jump measure tails.

Result: Establish sharp two-sided heat kernel estimates for symmetric Markov processes with jump kernels of the form J(x,y)=|x-y|^{-d-α}ℬ(x,y) where ℬ(x,y) may blow up at the boundary.

Conclusion: The developed framework successfully handles a broad class of Markov processes including traces of isotropic α-stable processes, processes in Lipschitz sets related to nonlocal Neumann problems, and resurrected self-similar processes, providing sharp heat kernel estimates despite the boundary-blowup behavior.

Abstract: In this paper, we study purely discontinuous symmetric Markov processes on closed subsets of ${\mathbb R}^d$, $d\ge 1$, with jump kernels of the form $J(x,y)=|x-y|^{-d-α}{\mathcal B}(x,y)$, $α\in (0,2)$, where the function ${\mathcal B}(x,y)$ may blow up at the boundary of the state space. This extends the framework developed recently for conservative self-similar Markov processes on the upper half-space to a broader geometric setting. Examples of Markov processes that fall into our general framework include traces of isotropic $α$-stable processes in $C^{1,\rm Dini}$ sets, processes in Lipschitz sets arising in connection with the nonlocal Neumann problem, and a large class of resurrected self-similar processes in the closed upper half-space.
  We establish sharp two-sided heat kernel estimates for these Markov processes. A fundamental difficulty in accomplishing this task is that, in contrast to the existing literature on heat kernels for jump processes, the tails of the associated jump measures in our setting are not uniformly bounded. Thus, standard techniques in the existing literature used to study heat kernels are not applicable. To overcome this obstacle, we employ recently developed weighted functional inequalities specifically designed for jump kernels blowing up at the boundary.

</details>


### [66] [Uniqueness for stochastic differential equations in Hilbert spaces with irregular drift](https://arxiv.org/abs/2512.25003)
*Lukas Anzeletti,Oleg Butkovsky,Máté Gerencsér,Alexander Shaposhnikov*

Main category: math.PR

TL;DR: The paper presents a framework for proving strong existence and uniqueness of SDEs in Hilbert spaces with irregular drift, extending previous work by removing structural assumptions on the drift function.


<details>
  <summary>Details</summary>
Motivation: To establish strong existence and uniqueness for stochastic differential equations in Hilbert spaces with irregular drift coefficients, extending beyond the structural assumptions required in previous seminal work by Da Prato and Flandoli (2010).

Method: Develops a new technique combining Lê's theory of stochastic sewing in Hilbert spaces, Gaussian analysis, and a method of Lasry and Lions for approximation in Hilbert spaces, avoiding the use of infinite-dimensional Kolmogorov equations.

Result: Proves that the SDE has a unique strong solution provided that α > 2γ/(1+γ), where α is the Hölder exponent of the drift function b and γ is a parameter related to the stochastic convolution.

Conclusion: The paper substantially extends previous results by establishing strong existence and uniqueness for SDEs in Hilbert spaces with irregular drift without imposing structural assumptions on the drift function, using novel analytical techniques.

Abstract: We present a versatile framework to study strong existence and uniqueness for stochastic differential equations (SDEs) in Hilbert spaces with irregular drift. We consider an SDE in a separable Hilbert space $H$ \begin{equation*} dX_t= (A X_t + b(X_t))dt +(-A)^{-γ/2}dW_t,\quad X_0=x_0 \in H, \end{equation*} where $A$ is a self-adjoint negative definite operator with purely atomic spectrum, $W$ is a cylindrical Wiener process, $b$ is $α$-Hölder continuous function $H\to H$, and a nonnegative parameter $γ$ such that the stochastic convolution takes values in $H$. We show that this equation has a unique strong solution provided that $α> 2γ/(1+γ)$. This substantially extends the seminal work of Da Prato and Flandoli (2010) as no structural assumption on $b$ is imposed. To obtain this result, we do not use infinite-dimensional Kolmogorov equations but instead develop a new technique combining Lê's theory of stochastic sewing in Hilbert spaces, Gaussian analysis, and a method of Lasry and Lions for approximation in Hilbert spaces.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [67] [Upscaling from ab initio atomistic simulations to electrode scale: The case of manganese hexacyanoferrate, a cathode material for Na-ion batteries](https://arxiv.org/abs/2512.24816)
*Yuan-Chi Yang,Eric Woillez,Quentin Jacquet,Ambroise van Roekeghem*

Main category: cond-mat.mtrl-sci

TL;DR: A multiscale computational framework bridges atomistic to device scales for predictive modeling of insertion-type electrode materials, demonstrated on sodium manganese hexacyanoferrate cathode for sodium-ion batteries.


<details>
  <summary>Details</summary>
Motivation: To enable rational computational design of next-generation insertion-type materials (like battery electrodes) by systematically translating atomistic insights into continuum-scale predictions, overcoming scale-bridging challenges in materials modeling.

Method: Active-learning strategy trains a Moment Tensor Potential through iterative hybrid grand-canonical Monte Carlo-molecular dynamics sampling. The machine learning interatomic potential captures configuration spaces at all sodiation levels, then feeds computed parameters (diffusivities, interfacial/strain energies, free-energy landscapes) into pseudo-2D phase-field simulations for electrode-scale predictions.

Result: The framework accurately reproduces experimental properties (volume expansion, operating voltage, structural transformations) and reveals a four-order-of-magnitude difference in sodium diffusivity between rhombohedral (sodium-rich) and tetragonal (sodium-poor) phases at 300 K. Phase-field simulations successfully predict phase-boundary propagation and rate-dependent performance across electrode scales.

Conclusion: The multiscale workflow establishes a blueprint for computational design of insertion-type materials, demonstrating systematic translation of atomistic insights into continuum-scale predictions for next-generation battery electrode materials and beyond.

Abstract: We present a generalizable scale-bridging computational framework that enables predictive modeling of insertion-type electrode materials from atomistic to device scales. Applied to sodium manganese hexacyanoferrate, a promising cathode material for grid-scale sodium-ion batteries, our methodology employs an active-learning strategy to train a Moment Tensor Potential through iterative hybrid grand-canonical Monte Carlo--molecular dynamics sampling, robustly capturing configuration spaces at all sodiation levels. The resulting machine learning interatomic potential accurately reproduces experimental properties including volume expansion, operating voltage, and sodium concentration-dependent structural transformations, while revealing a four-order-of-magnitude difference in sodium diffusivity between the rhombohedral (sodium-rich) and tetragonal (sodium-poor) phases at 300 K. We directly compute all critical parameters -- temperature- and concentration-dependent diffusivities, interfacial and strain energies, and complete free-energy landscapes -- to feed them into pseudo-2D phase-field simulations that predict phase-boundary propagation and rate-dependent performances across electrode length scales. This multiscale workflow establishes a blueprint for rational computational design of next-generation insertion-type materials, such as battery electrode materials, demonstrating how atomistic insights can be systematically translated into continuum-scale predictions.

</details>


<div id='nlin.PS'></div>

# nlin.PS [[Back]](#toc)

### [68] [Soliton profiles: Classical Numerical Schemes vs. Neural Network - Based Solvers](https://arxiv.org/abs/2512.24634)
*Chandler Haight,Svetlana Roudenko,Zhongming Wang*

Main category: nlin.PS

TL;DR: Comparative study shows classical numerical solvers outperform neural network methods for single-instance 1D solitary wave problems in accuracy and efficiency, though operator-learning methods offer advantages for repeated simulations.


<details>
  <summary>Details</summary>
Motivation: To compare the performance of classical numerical methods versus neural network-based approaches for computing ground states/profiles of solitary-wave solutions to 1D dispersive PDEs, assessing their relative strengths and limitations.

Method: Comparative analysis of three approaches: 1) Classical numerical solvers (Petviashvili's method, finite difference with Newton iterations), 2) Physics-informed neural networks (PINNs), and 3) Operator-learning methods, applied to nonlinear Schrödinger, nonlinear Klein-Gordon, and generalized KdV equations.

Result: Classical methods maintain high-order accuracy and computational efficiency for single-instance 1D problems. PINNs produce qualitative solutions but are less accurate and efficient due to expensive training. Operator-learning methods, while computationally intensive during training, enable rapid inference across parameter instances after pretraining.

Conclusion: Classical solvers remain superior for single-instance computations in 1D, while operator-learning methods become attractive for applications requiring repeated simulations or real-time predictions due to their reusability across parameter instances.

Abstract: We present a comparative study of classical numerical solvers, such as Petviashvili's method or finite difference with Newton iterations, and neural network-based methods for computing ground states or profiles of solitary-wave solutions to the one-dimensional dispersive PDEs that include the nonlinear Schrödinger, the nonlinear Klein-Gordon and the generalized KdV equations. We confirm that classical approaches retain high-order accuracy and strong computational efficiency for single-instance problems in the one-dimensional setting. Physics-informed neural networks (PINNs) are also able to reproduce qualitative solutions but are generally less accurate and less efficient in low dimensions than classical solvers due to expensive training and slow convergence. We also investigate the operator-learning methods, which, although computationally intensive during training, can be reused across many parameter instances, providing rapid inference after pretraining, making them attractive for applications involving repeated simulations or real-time predictions. For single-instance computations, however, the accuracy of operator-learning methods remains lower than that of classical methods or PINNs, in general.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [69] [Mathematical Theory for Photonic Hall Effect in Honeycomb Photonic Crystals](https://arxiv.org/abs/2512.24477)
*Wei Li,Junshan Lin,Jiayu Qiu,Hai Zhang*

Main category: physics.optics

TL;DR: The paper develops a mathematical theory for the photonic Hall effect and proves existence of guided electromagnetic waves at interfaces between honeycomb photonic crystals, analogous to electronic edge states.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous mathematical foundation for the photonic Hall effect and demonstrate the existence of topological interface modes in photonic systems, similar to edge states in electronic topological insulators.

Method: Start from symmetric honeycomb photonic crystals with Dirac points at K and K' points. Introduce two classes of perturbations that lift Dirac degeneracy, creating spectral band valleys with well-defined topological phases. Use layer potential techniques and spectral analysis to study guided waves at interfaces between two perturbed crystals.

Result: Proves existence of guided electromagnetic waves propagating along interfaces but not in bulk media. Shows these interface modes are induced by topological Hall effect and their existence depends on the nature of perturbations in the two periodic media.

Conclusion: Provides mathematical proof for photonic Hall effect and topological interface states in honeycomb photonic crystals, establishing connection between perturbation types and existence of guided waves at interfaces.

Abstract: In this work, we develop a mathematical theory for the photonic Hall effect and prove the existence of guided electromagnetic waves at the interface of two honeycomb photonic crystals. The guided wave resembles the edge states in electronic systems: it is induced by the topological Hall effect, and the wave propagates along the interface but not in the bulk media. Starting from a symmetric honeycomb photonic crystal that attains Dirac points at the high-symmetry points of the Brillouin zone, $K$ and $K'$, we introduce two classes of perturbations for the periodic medium. The perturbations lift the Dirac degeneracy, forming a spectral band valley at the points $K$ and $K'$ with well-defined topological phase that depends on the sign of the perturbation parameters. By employing the layer potential techniques and spectral analysis, we investigate the existence of guided wave along an interface when two honeycomb photonic crystals are glued together. In particular, we elucidate the relationship between the existence of the interface mode and the nature of perturbations imposed on the two periodic media separated by the interface.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [70] [Classification of ancient cylindrical mean curvature flows and the Mean Convex Neighborhood Conjecture](https://arxiv.org/abs/2512.24524)
*Richard H. Bamler,Yi Lai*

Main category: math.DG

TL;DR: The paper resolves the Mean Convex Neighborhood Conjecture for mean curvature flows, showing that near cylindrical singularities the flow is mean-convex with level set structure, and establishes a canonical neighborhood theorem with quantitative results.


<details>
  <summary>Details</summary>
Motivation: To prove the Mean Convex Neighborhood Conjecture for mean curvature flows in all dimensions and for all cylindrical singularities, which has been a longstanding open problem in geometric analysis.

Method: Complete classification of ancient, asymptotically cylindrical flows using refined asymptotic analysis, novel leading mode condition, and new "induction over thresholds" argument. The approach provides full parameterization of asymptotically cylindrical flows and is largely self-contained.

Result: Resolved the conjecture: near multiplicity-one cylindrical singularities, flows are mean-convex with time-slices as level sets, all nearby tangent flows are cylindrical, and established canonical neighborhood theorem with quantitative structural description.

Conclusion: The paper provides a complete solution to the Mean Convex Neighborhood Conjecture, characterizes flows near cylindrical singularities, classifies ancient asymptotically cylindrical flows into three canonical families, and develops new analytical techniques applicable to geometric flows.

Abstract: We resolve the Mean Convex Neighborhood Conjecture for mean curvature flows in all dimensions and for all types of cylindrical singularities. Specifically, we show that if the tangent flow at a singular point is a multiplicity-one cylinder, then in a neighborhood of that point the flow is mean-convex, its time-slices arise as level sets of a continuous function, and all nearby tangent flows are cylindrical. Moreover, we establish a canonical neighborhood theorem near such points, which characterizes the flow via local models. We also obtain a more uniform version of the Mean Convex Neighborhood Conjecture, which only requires closeness to a cylinder at some initial time and yields a quantitative version of this structural description.
  Our proof relies on a complete classification of ancient, asymptotically cylindrical flows. We prove that any such flow is non-collapsed, convex, rotationally symmetric, and belongs to one of three canonical families: ancient ovals, the bowl soliton, or the flying wing translating solitons. Central to our method is a refined asymptotic analysis and a novel \emph{leading mode condition,} together with a new ``induction over thresholds'' argument. In addition, our approach provides a full parameterization of the space of asymptotically cylindrical flows and gives a new proof of the existence of flying wing solitons.
  Our method is independent of prior work and, together with our prequel paper, this work is largely self-contained.

</details>


### [71] [Isocapacitary constants for the $p$-Laplacian on compact manifolds](https://arxiv.org/abs/2512.24725)
*Lili Wang,Tao Wang*

Main category: math.DG

TL;DR: Introduces Steklov and Neumann isocapacitary constants for p-Laplacian on compact manifolds that provide two-sided bounds for (p,α)-Sobolev constants and degenerate to bounds for first nontrivial eigenvalues when α=1.


<details>
  <summary>Details</summary>
Motivation: To develop new isocapacitary constants that can provide quantitative bounds for Sobolev constants and eigenvalues of the p-Laplacian on compact manifolds, extending classical results to the nonlinear p-Laplacian setting.

Method: Introduces Steklov and Neumann isocapacitary constants specifically for the p-Laplacian operator on compact manifolds, analyzing their relationship with (p,α)-Sobolev constants and showing how they degenerate to eigenvalue bounds when α=1.

Result: The new isocapacitary constants yield two-sided bounds for (p,α)-Sobolev constants, and when α=1, these bounds become upper and lower bounds for the first nontrivial Steklov and Neumann eigenvalues of the p-Laplacian.

Conclusion: The introduced Steklov and Neumann isocapacitary constants provide a unified framework for obtaining quantitative bounds on Sobolev constants and eigenvalues of the p-Laplacian on compact manifolds, with special cases recovering classical eigenvalue bounds.

Abstract: In this paper, we introduce Steklov and Neumann isocapacitary constants for the $p$-Laplacian on compact manifolds. These constants yield two-sided bounds for the $(p,α)$-Sobolev constants, which degenerate to upper and lower bounds for the first nontrivial Steklov and Neumann eigenvalues of the $p$-Laplacian when $α= 1$.

</details>


### [72] [A Liouville-Weierstrass correspondence for Spacelike and Timelike Minimal Surfaces in $\mathbb{L}^3$](https://arxiv.org/abs/2512.24908)
*Adriana A. Cintra,Iury Domingos,Irene I. Onnis*

Main category: math.DG

TL;DR: The paper establishes a correspondence between solutions of the Liouville equation and minimal surfaces in Lorentz-Minkowski space, providing a unified treatment of both spacelike and timelike surfaces using complex/paracomplex analysis.


<details>
  <summary>Details</summary>
Motivation: To investigate the relationship between solutions of the Liouville equation and minimal surfaces in Lorentz-Minkowski space, providing a unified framework for both spacelike and timelike cases using complex and paracomplex analysis.

Method: Using complex analysis for spacelike surfaces and paracomplex analysis for timelike surfaces, studying the action of pseudo-isometries via Möbius-type transformations, and establishing connections between Liouville equation solutions and Weierstrass data.

Result: Established a correspondence between Liouville equation solutions and minimal surfaces in L^3, showed how pseudo-isometries act via Möbius transformations, demonstrated how local solutions determine Gauss maps and Weierstrass data, and provided explicit examples of both spacelike and timelike minimal surfaces.

Conclusion: The paper successfully provides a unified treatment of spacelike and timelike minimal surfaces in Lorentz-Minkowski space through their connection to Liouville equation solutions, using complex/paracomplex analysis and establishing important transformation properties.

Abstract: We investigate a correspondence between solutions $λ(x,y)$ of the Liouville equation \[ Δλ= -\varepsilon e^{-4λ}, \] and the Weierstrass representations of spacelike ($\varepsilon = 1$) and timelike ($\varepsilon = -1$) minimal surfaces with diagonalizable Weingarten map in the three-dimensional Lorentz--Minkowski space $\mathbb{L}^3$. Using complex and paracomplex analysis, we provide a unified treatment of both causal types. We study the action of pseudo-isometries of $\mathbb{L}^3$ on minimal surfaces via Möbius-type transformations, establishing a correspondence between these transformations and rotations in the special orthochronous Lorentz group. Furthermore, we show how local solutions of the Liouville equation determine the Gauss map and the associated Weierstrass data. Finally, we present explicit examples of spacelike and timelike minimal surfaces in $\mathbb{L}^3$ arising from solutions of the Liouville equation.

</details>


### [73] [The PDE-ODI principle and cylindrical mean curvature flows](https://arxiv.org/abs/2512.25050)
*Richard H. Bamler,Yi Lai*

Main category: math.DG

TL;DR: The paper introduces a new PDE-ODI principle that converts parabolic PDEs into systems of ordinary differential inequalities, enabling high-order asymptotic expansions for ancient solutions and singularities of mean curvature flow modeled on cylinders.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient approach for analyzing ancient solutions and singularities of mean curvature flow that avoids delicate analytic estimates used in previous work, and to establish stronger asymptotic control for cylindrical flows.

Method: Introduces the PDE-ODI principle that converts parabolic differential equations into systems of ordinary differential inequalities, bypassing complex analytic estimates. This framework is independent of prior work and largely self-contained.

Result: 1) Uniqueness of bowl soliton times Euclidean factor among ancient cylindrical flows with dominant linear mode. 2) Complete asymptotic expansion to arbitrary polynomial order when quadratic mode dominates. 3) New proofs of uniqueness of tangent flows and rigidity of cylinders among shrinkers using a single ODI.

Conclusion: The PDE-ODI principle provides a powerful new framework for analyzing mean curvature flow singularities, offering stronger asymptotic control, unifying classical results, and enabling high-order expansions while avoiding complex analytic estimates.

Abstract: We introduce a new approach for analyzing ancient solutions and singularities of mean curvature flow that are locally modeled on a cylinder. Its key ingredient is a general mechanism, called the \emph{PDE--ODI principle}, which converts a broad class of parabolic differential equations into systems of ordinary differential inequalities. This principle bypasses many delicate analytic estimates used in previous work, and yields asymptotic expansions to arbitrarily high order.
  As an application, we establish the uniqueness of the bowl soliton times a Euclidean factor among ancient, cylindrical flows with dominant linear mode. This extends previous results on this problem to the most general setting and is made possible by the stronger asymptotic control provided by our analysis. In the other case, when the quadratic mode dominates, we obtain a complete asymptotic expansion to arbitrary polynomial order, which will form the basis for a subsequent paper. Our framework also recovers and unifies several classical results. In particular, we give new proofs of the uniqueness of tangent flows (due to Colding-Minicozzi) and the rigidity of cylinders among shrinkers (due to Colding-Ilmanen-Minicozzi) by reducing both problems to a single ordinary differential inequality, without using the Łojasiewicz-Simon inequality.
  Our approach is independent of prior work and the paper is largely self-contained.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [74] [A positive eigenvalue result for semilinear differential equations in Banach spaces with functional initial conditions](https://arxiv.org/abs/2512.23876)
*Gennaro Infante,Paola Rubbioni*

Main category: math.CA

TL;DR: Existence of positive eigenvalues with nonnegative eigenfunctions for abstract initial value problems in Banach spaces with functional/nonlocal initial conditions, applied to reaction-diffusion equations.


<details>
  <summary>Details</summary>
Motivation: To establish existence results for positive eigenvalues with associated nonnegative mild eigenfunctions in Banach spaces with various types of functional initial conditions (periodic, multipoint, integral average), which arise in practical applications like heat flow problems.

Method: Uses nonlinear analysis, topological methods, and strongly continuous semigroup theory to develop abstract framework applicable to wide range of models.

Result: Develops abstract theory for existence of positive eigenvalues with nonnegative eigenfunctions, then applies it to reaction-diffusion equation with nonlocal initial condition from heat flow problem.

Conclusion: Provides general framework for studying eigenvalue problems with functional initial conditions, with concrete application to reaction-diffusion equations demonstrating practical utility of the abstract theory.

Abstract: We study the existence of positive eigenvalues with associated nonnegative mild eigenfunctions for a class of abstract initial value problems in Banach spaces with functional, possibly nonlocal, initial conditions. The framework includes periodic, multipoint, and integral average conditions. Our approach relies on nonlinear analysis, topological methods, and the theory of strongly continuous semigroups, yielding results applicable to a wide range of models. As an illustration, we apply the abstract theory to a reaction-diffusion equation with a nonlocal initial condition arising from a heat flow problem.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [75] [Numerical study of solitary waves in Dirac--Klein--Gordon system](https://arxiv.org/abs/2512.24954)
*Andrew Comech,Julien Ricaud,Marco Roque*

Main category: math-ph

TL;DR: Numerical construction of solitary waves in Dirac-Klein-Gordon systems in 1D and 3D, studying energy/charge dependence on frequency ω, using iterative methods and comparing with shooting for massless case.


<details>
  <summary>Details</summary>
Motivation: To study solitary wave solutions in Dirac-Klein-Gordon systems and understand their properties, particularly the dependence of energy and charge on frequency, which has implications for spectral stability analysis.

Method: Numerical construction using iterative procedure starting from nonlinear Dirac solitary waves, computing corresponding scalar field, and adjusting coupling constant. For massless scalar field, compared iterative method with shooting method. Used virial identities to control simulation errors.

Result: Constructed solitary waves in both 1D and 3D Dirac-Klein-Gordon systems, analyzed energy and charge dependence on ω parameter, validated methods through error control with virial identities, and compared iterative vs shooting approaches for massless case.

Conclusion: Successfully developed numerical methods for constructing solitary waves in Dirac-Klein-Gordon systems, providing foundation for analyzing spectral stability properties of these solutions based on energy/charge frequency dependence.

Abstract: We use numerics to construct solitary waves in Dirac--Klein--Gordon (in one and three spatial dimensions) and study the dependence of energy and charge on $ω$. For the construction, we use the iterative procedure, starting from solitary waves of nonlinear Dirac equation, computing the corresponding scalar field, and adjusting the coupling constant. We also consider the case of massless scalar field, when the iteration procedure could be compared with the shooting method. We use the virial identities to control the error of simulations. We also discuss possible implications from the obtained results for the spectral stability of solitary waves.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [76] [Towards mechanistic understanding in a data-driven weather model: internal activations reveal interpretable physical features](https://arxiv.org/abs/2512.24440)
*Theodore MacMillan,Nicholas T. Ouellette*

Main category: physics.ao-ph

TL;DR: Researchers adapt LLM interpretability tools to analyze GraphCast's internal representations, discovering interpretable features corresponding to real-world weather phenomena like tropical cyclones and atmospheric rivers, and demonstrating physically consistent interventions.


<details>
  <summary>Details</summary>
Motivation: While data-driven physics models like GraphCast have shown impressive predictive accuracy, their internal computations remain largely unknown black boxes. The researchers want to understand whether these models develop interpretable, physically consistent representations of weather phenomena.

Method: Adapt interpretability tools from Large Language Models to GraphCast, specifically using sparse autoencoders to discover interpretable features in the model's neuron space. Then probe these features through interventions on the model's prediction steps.

Result: Discovered distinct interpretable features corresponding to real-world weather phenomena across various scales: tropical cyclones, atmospheric rivers, diurnal/seasonal behavior, precipitation patterns, geographical coding, and sea-ice extent. Successfully demonstrated physically consistent modifications by intervening on tropical cyclone features.

Conclusion: The approach provides a window into black-box data-driven physics models, moving toward making them trustworthy predictors and scientifically valuable discovery tools by revealing their interpretable, physically consistent internal representations.

Abstract: Large data-driven physics models like DeepMind's weather model GraphCast have empirically succeeded in parameterizing time operators for complex dynamical systems with an accuracy reaching or in some cases exceeding that of traditional physics-based solvers. Unfortunately, how these data-driven models perform computations is largely unknown and whether their internal representations are interpretable or physically consistent is an open question. Here, we adapt tools from interpretability research in Large Language Models to analyze intermediate computational layers in GraphCast, leveraging sparse autoencoders to discover interpretable features in the neuron space of the model. We uncover distinct features on a wide range of length and time scales that correspond to tropical cyclones, atmospheric rivers, diurnal and seasonal behavior, large-scale precipitation patterns, specific geographical coding, and sea-ice extent, among others. We further demonstrate how the precise abstraction of these features can be probed via interventions on the prediction steps of the model. As a case study, we sparsely modify a feature corresponding to tropical cyclones in GraphCast and observe interpretable and physically consistent modifications to evolving hurricanes. Such methods offer a window into the black-box behavior of data-driven physics models and are a step towards realizing their potential as trustworthy predictors and scientifically valuable tools for discovery.

</details>
