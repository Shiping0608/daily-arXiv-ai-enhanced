<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 18]
- [math.AP](#math.AP) [Total: 24]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [math.PR](#math.PR) [Total: 3]
- [physics.optics](#physics.optics) [Total: 1]
- [math.DG](#math.DG) [Total: 4]
- [math-ph](#math-ph) [Total: 1]
- [math.NT](#math.NT) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [nlin.PS](#nlin.PS) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [math.OC](#math.OC) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A note on the space-time variational formulation for the wave equation with source term in $L^2(Q)$](https://arxiv.org/abs/2512.23807)
*Marco Zank*

Main category: math.NA

TL;DR: A variational formulation for the scalar wave equation in second-order form with homogeneous initial conditions, establishing existence, uniqueness, and solvability results in a new solution space that's not a subspace of H²(Q).


<details>
  <summary>Details</summary>
Motivation: To develop a robust variational framework for the scalar wave equation that supports space-time discretization methods, including least-squares approaches and boundary element methods, while addressing regularity issues and providing a foundation for analyzing related space-time boundary integral equations.

Method: Derived a variational formulation for the scalar wave equation on bounded Lipschitz domains with homogeneous initial conditions. Investigated a variational framework in a bounded space-time cylinder Q with a new solution space and test space L²(Q) for source terms in L²(Q). Used existence and uniqueness results in H¹(Q) to prove the variational setting fits inf-sup theory, including an isomorphism as solution operator.

Result: Proved that the variational setting fits the inf-sup theory with an isomorphism as solution operator. Showed that the new solution space is not a subspace of H²(Q). Established new uniqueness and solvability results crucial for space-time discretizations.

Conclusion: The new variational framework provides essential mathematical foundations for space-time methods, including least-squares approaches and boundary element methods, with important implications for regularity results and analysis of space-time boundary integral equations.

Abstract: We derive a variational formulation for the scalar wave equation in the second-order formulation on bounded Lipschitz domains and homogeneous initial conditions. We investigate a variational framework in a bounded space-time cylinder $Q$ with a new solution space and the test space $L^2(Q)$ for source terms in $L^2(Q)$. Using existence and uniqueness results in $H^1(Q)$, we prove that this variational setting fits the inf-sup theory, including an isomorphism as solution operator. Moreover, we show that the new solution space is not a subspace of $H^2(Q)$. This new uniqueness and solvability result is not only crucial for discretizations using space-time methods, including least-squares approaches, but also important for regularity results and the analysis of related space-time boundary integral equations, which form the basis for space-time boundary element methods.

</details>


### [2] [Greedy Rational Approximation for Frequency-Domain Model Reduction of Parametric LTI Systems](https://arxiv.org/abs/2512.23814)
*Filip Bělík,Yanlai Chen,Akil Narayan*

Main category: math.NA

TL;DR: A reduced basis method for parametric LTI system model reduction via rational function approximation in frequency domain.


<details>
  <summary>Details</summary>
Motivation: Need efficient model reduction for parametric linear time-invariant dynamical systems, which in frequency domain becomes rational function approximation problem.

Method: Use standard reduced basis method (RBM) with iterative greedy approach, exploiting linearity of frequency domain representation for error estimation.

Result: Provides principled framework for rational compression of high-order rational functions and computational pathway for parametric LTI system model reduction.

Conclusion: RBM with greedy approach offers effective method for parametric LTI system reduction through rational function approximation in frequency domain.

Abstract: We investigate model reduction of parametric linear time-invariant (LTI) dynamical systems. When posed in the frequency domain, this problem can be formulated as seeking a low-order rational function approximation of a high-order rational function. We propose to use a standard reduced basis method (RBM) to construct this low-order rational function. Algorithmically, this procedure is an iterative greedy approach, where the greedy objective is evaluated through an error estimator that exploits the linearity of the frequency domain representation. The greedy framework is motivated through theoretical results of rational approximability of functions. This framework provides a principled approach to rational compression of high-order rational functions, and provides a computational pathway for model reduction of parametric LTI systems.

</details>


### [3] [Deep learning methods for inverse problems using connections between proximal operators and Hamilton-Jacobi equations](https://arxiv.org/abs/2512.23829)
*Oluwatosin Akande,Gabriel P. Langlois,Akwum Onwunta*

Main category: math.NA

TL;DR: The paper proposes using Hamilton-Jacobi PDEs to develop deep learning architectures for learning priors in inverse problems, directly learning the prior without needing to invert it after training.


<details>
  <summary>Details</summary>
Motivation: Inverse problems require regularization/priors due to ill-posedness. Proximal operators are central for encoding priors and building efficient algorithms, and recent work has connected them to convex potentials. The authors want to leverage connections between proximal operators and Hamilton-Jacobi PDEs to develop novel deep learning architectures for learning priors directly.

Method: The method leverages connections between proximal operators and Hamilton-Jacobi partial differential equations (HJ PDEs) to develop novel deep learning architectures for learning the prior. Unlike existing methods, it learns the prior directly without needing to invert the prior after training.

Result: The paper presents several numerical results demonstrating the efficiency of the proposed method in high dimensions.

Conclusion: The proposed approach using Hamilton-Jacobi PDEs provides an effective way to learn priors directly for inverse problems, with demonstrated efficiency in high-dimensional settings.

Abstract: Inverse problems are important mathematical problems that seek to recover model parameters from noisy data. Since inverse problems are often ill-posed, they require regularization or incorporation of prior information about the underlying model or unknown variables. Proximal operators, ubiquitous in nonsmooth optimization, are central to this because they provide a flexible and convenient way to encode priors and build efficient iterative algorithms. They have also recently become key to modern machine learning methods, e.g., for plug-and-play methods for learned denoisers and deep neural architectures for learning priors of proximal operators. The latter was developed partly due to recent work characterizing proximal operators of nonconvex priors as subdifferential of convex potentials. In this work, we propose to leverage connections between proximal operators and Hamilton-Jacobi partial differential equations (HJ PDEs) to develop novel deep learning architectures for learning the prior. In contrast to other existing methods, we learn the prior directly without recourse to inverting the prior after training. We present several numerical results that demonstrate the efficiency of the proposed method in high dimensions.

</details>


### [4] [Multimodal sampling via Schrödinger-Föllmer samplers with temperatures](https://arxiv.org/abs/2512.23965)
*Xiaojie Wang,Xiaoyan Zhang*

Main category: math.NA

TL;DR: This paper introduces temperature-parameterized Schrödinger-Föllmer samplers (SFS) that achieve O(h) convergence rate in Wasserstein distance, improving upon previous O(√h) results, and demonstrates superior performance over Langevin samplers for multimodal distributions.


<details>
  <summary>Details</summary>
Motivation: To improve sampling from complex high-dimensional distributions by enhancing the convergence rate of Schrödinger-Föllmer samplers and addressing challenges with multimodal distributions through temperature parameterization.

Method: Introduces temperature-parameterized Schrödinger-Föllmer samplers based on Euler discretization of the Schrödinger-Föllmer process with temperatures. Develops novel error analysis for time discretization and incorporates temperature parameters to handle multimodal distributions.

Result: Achieves enhanced convergence rate of O(h) in L²-Wasserstein distance (improving from O(√h)), with numerical experiments confirming the rate and showing SFS substantially outperforms vanilla Langevin samplers, especially for multimodal distributions.

Conclusion: Temperature-parameterized SFS provides a gradient-free, unit-interval-based sampling method with improved convergence rates and superior performance for multimodal distributions compared to traditional Langevin samplers.

Abstract: Generating samples from complex and high-dimensional distributions is ubiquitous in various scientific fields of statistical physics, Bayesian inference, scientific computing and machine learning. Very recently, Huang et al. (IEEE Trans. Inform. Theory, 2025) proposed new Schrödinger-Föllmer samplers (SFS), based on the Euler discretization of the Schrödinger-Föllmer diffusion evolving on the unit interval $[0, 1]$. There, a convergence rate of order $\mathcal{O}(\sqrt{h})$ in the $L^2$-Wasserstein distance was obtained for the Euler discretization with a uniform time step-size $h>0$.
  By incorporating a temperature parameter, different samplers are introduced in this paper, based on the Euler discretization of the Schrödinger-Föllmer process with temperatures. As revealed by numerical experiments, high temperatures are vital, particularly in sampling from multimodal distributions. Further, a novel approach of error analysis is developed for the time discretization and an enhanced convergence rate of order ${ \mathcal{O}(h)}$ is obtained in the $L^2$-Wasserstein distance, under certain smoothness conditions on the drift. This significantly improves the existing order-half convergence in the aforementioned paper. Unlike Langevin samplers, SFS is of gradient-free, works in a unit interval $[0, 1]$ and does not require any ergodicity. Numerical experiments confirm the convergence rate and show that, the SFS substantially outperforms vanilla Langevin samplers, particularly in sampling from multimodal distributions.

</details>


### [5] [High order numerical discretizations of the Einstein-Euler equations in the Generalized Harmonic formulation](https://arxiv.org/abs/2512.24121)
*Stefano Muzzolon,Michael Dumbser,Olindo Zanotti,Elena Gaburro*

Main category: math.NA

TL;DR: Two new numerical schemes for solving Einstein-Euler equations: FD-CWENO on Cartesian meshes and ADER-DG on unstructured polygonal meshes, both with well-balancing properties.


<details>
  <summary>Details</summary>
Motivation: To develop robust numerical methods for solving coupled Einstein-Euler equations in numerical relativity, enabling accurate simulations of astrophysical sources like black holes and neutron stars.

Method: Two approaches: 1) Finite Difference Central Weighted Essentially Non-Oscillatory (FD-CWENO) scheme on Cartesian meshes; 2) ADER discontinuous Galerkin (DG) scheme on 2D unstructured polygonal meshes, both with well-balancing properties to preserve equilibrium solutions.

Result: Successful validation through standard vacuum tests (robust stability, linearized wave, gauge wave), long-term stable evolutions of stationary black holes (including extreme spin Kerr), and matter coupling tests (spherical accretion onto Schwarzschild black hole, perturbed non-rotating neutron star).

Conclusion: The schemes provide a solid foundation for more complex astrophysical simulations using DG methods on unstructured 3D meshes, representing a preliminary step toward full 3D numerical relativity calculations on moving meshes.

Abstract: We propose two new alternative numerical schemes to solve the coupled Einstein-Euler equations in the Generalized Harmonic formulation. The first one is a finite difference (FD) Central Weighted Essentially Non-Oscillatory (CWENO) scheme on a traditional Cartesian mesh, while the second one is an ADER (Arbitrary high order Derivatives) discontinuous Galerkin (DG) scheme on 2D unstructured polygonal meshes. The latter, in particular, represents a preliminary step in view of a full 3D numerical relativity calculation on moving meshes. Both schemes are equipped with a well-balancing (WB) property, which allows to preserve the equilibrium of a priori known stationary solutions exactly at the discrete level. We validate our numerical approaches by successfully reproducing standard vacuum test cases, such as the robust stability, the linearized wave, and the gauge wave tests, as well as achieving long-term stable evolutions of stationary black holes, including Kerr black holes with extreme spin. Concerning the coupling with matter, modeled by the relativistic Euler equations, we perform a classical test of spherical accretion onto a Schwarzschild black hole, as well as an evolution of a perturbed non-rotating neutron star, demonstrating the capability of our schemes to operate also on the full Einstein-Euler system. Altogether, these results provide a solid foundation for addressing more complex and challenging simulations of astrophysical sources through DG schemes on unstructured 3D meshes.

</details>


### [6] [Structure-preserving schemes for nonlinear symmetric hyperbolic and thermodynamically compatible systems of partial differential equations](https://arxiv.org/abs/2512.24127)
*Alessia Lucca,Michael Dumbser*

Main category: math.NA

TL;DR: This paper develops exactly energy-conservative and structure-preserving finite volume schemes for symmetric-hyperbolic and thermodynamically compatible (SHTC) systems in continuum physics.


<details>
  <summary>Details</summary>
Motivation: To create numerical schemes that preserve both total energy conservation and involution constraints (stationary differential constraints) for SHTC systems, which are important in continuum physics but challenging to discretize properly.

Method: Two approaches: 1) A simple semi-discrete cell-centered HTC finite volume scheme with collocated grids that conserves total energy but not involutions; 2) A fully discrete semi-implicit vertex-based staggered scheme that exactly preserves both energy conservation and involution constraints (∇·∇×A=0 and ∇×∇φ=0) using a discrete symmetric-hyperbolic Godunov-form approach.

Result: The paper successfully develops schemes that are exactly energy-conservative and structure-preserving. The implicit scheme naturally leads to symmetric positive definite linear systems solved via iterative fixed-point methods. Applications to nonlinear acoustics, nonlinear Maxwell equations, and Maxwell-GLM systems demonstrate the schemes' properties.

Conclusion: The paper presents novel finite volume schemes that exactly preserve both energy conservation and involution constraints for SHTC systems, addressing a significant challenge in numerical continuum physics through careful discretization of the symmetric-hyperbolic Godunov-form.

Abstract: This paper aims at developing exactly energy-conservative and structure-preserving finite volume schemes for the discretisation of first-order symmetric-hyperbolic and thermodynamically compatible (SHTC) systems of partial differential equations in continuum physics. Due to their thermodynamic compatibility the class of SHTC systems satisfies an additional conservation law for the total energy and many PDE in this class of equations also satisfy stationary differential constraints (involutions). First, we propose a simple semi-discrete cell-centered HTC finite volume scheme that employs collocated grids and that is compatible with the total energy conservation law, but which does not satisfy the involutions. Second, we develop a fully discrete semi-implicit finite volume scheme that conserves total energy and which can be proven to satisfy also the involution constraints exactly at the discrete level. This method is a vertex-based staggered semi-implicit scheme that preserves the basic vector calculus identities $\nabla \cdot \nabla \times A = 0$ and $\nabla \times \nabla φ= 0$ for any vector and scalar field, respectively, exactly at the discrete level and which is also exactly totally energy conservative. The main key ingredient of the proposed implicit scheme is the fact that it uses a discrete version of the symmetric-hyperbolic Godunov-form of the governing PDE system. This leads naturally to sequences of symmetric and positive definite linear algebraic systems to be solved inside an iterative fixed-point method used in each time step. We apply our new schemes to three different SHTC systems. In particular, we consider the equations of nonlinear acoustics, the nonlinear Maxwell equations in the absence of charges and a nonlinear version of the Maxwell-GLM system. We also show some numerical results to provide evidence of the stated properties of the proposed schemes.

</details>


### [7] [Sufficient and Necessary Conditions for Eckart-Young-like Result for Tubal Tensors](https://arxiv.org/abs/2512.24405)
*Uria Mor*

Main category: math.NA

TL;DR: Characterization of tubal products that yield Eckart-Young theorem for tensor low-rank approximation.


<details>
  <summary>Details</summary>
Motivation: The tubal tensor framework allows matrix algebra concepts like SVD and rank to extend to tensors, but only specific tubal products guarantee Eckart-Young results (best low-rank approximation via truncated SVD). The paper aims to identify which tubal products preserve this important property.

Method: Theoretical characterization of the family of tubal products that yield Eckart-Young type results, followed by experimental validation using video data and data-driven dynamical systems.

Result: Complete characterization of tubal products that guarantee Eckart-Young theorem for tensor low-rank approximation, with practical demonstrations showing the implications of these theoretical findings.

Conclusion: Identifies specific tubal products that preserve the Eckart-Young property for tensor approximation, providing theoretical foundation and practical validation for tensor decomposition methods.

Abstract: A valuable feature of the tubal tensor framework is that many familiar constructions from matrix algebra carry over to tensors, including SVD and notions of rank. Most importantly, it has been shown that for a specific family of tubal products, an Eckart-Young type theorem holds, i.e., the best low-rank approximation of a tensor under the Frobenius norm is obtained by truncating its tubal SVD. In this paper, we provide a complete characterization of the family of tubal products that yield an Eckart-Young type result. We demonstrate the practical implications of our theoretical findings by conducting experiments with video data and data-driven dynamical systems.

</details>


### [8] [Fast high-order spectral solvers for PDEs on triangulated surfaces with applications to deforming surfaces](https://arxiv.org/abs/2512.24456)
*Gentian Zavalani*

Main category: math.NA

TL;DR: Extends quadrilateral-based hierarchical Poincaré-Steklov (HPS) method to triangles via two high-order strategies: reduced quadrilateralization and triangle spectral elements using Dubiner polynomials.


<details>
  <summary>Details</summary>
Motivation: Traditional HPS method is limited to quadrilateral meshes with tensor-product spectral discretizations, restricting its application to triangular geometries which are common in computational practice.

Method: Two complementary approaches: 1) Reduced quadrilateralization (straightforward implementation), 2) Triangle-based spectral element method using Dubiner polynomials. Both preserve HPS framework's structure.

Result: Numerical demonstration shows preserved spectral accuracy, efficiency, and fast direct-solver structure. Extended to time-dependent and evolving surfaces, tested on reaction-diffusion systems and geometry-driven surface evolution.

Conclusion: Successfully extends HPS framework to triangular geometries while maintaining its desirable properties, enabling broader application to complex geometries and time-dependent surface problems.

Abstract: In this paper, we extend the classical quadrilateral based hierarchical Poincaré-Steklov (HPS) framework to triangulated geometries. Traditionally, the HPS method takes as input an unstructured, high-order quadrilateral mesh and relies on tensor-product spectral discretizations on each element. To overcome this restriction, we introduce two complementary high-order strategies for triangular elements: a reduced quadrilateralization approach which is straightforward to implement, and triangle based spectral element method based on Dubiner polynomials. We show numerically that these extensions preserve the spectral accuracy, efficiency, and fast direct-solver structure of the HPS framework. The method is further extended to time dependent and evolving surfaces, and its performance is demonstrated through numerical experiments on reaction-diffusion systems, and geometry driven surface evolution.

</details>


### [9] [Exponential Convergence of Deep Composite Polynomial Approximation for Cusp-Type Functions](https://arxiv.org/abs/2512.24523)
*Kingsley Yeon,Steven B. Damelin,Michael Werman*

Main category: math.NA

TL;DR: Deep composite polynomial approximations achieve exponential convergence for functions with algebraic cusp singularities, outperforming classical polynomial methods with algebraic rates.


<details>
  <summary>Details</summary>
Motivation: Continuous but non-differentiable functions with algebraic cusp singularities (like |x-a|^α with α∈(0,1)) are challenging for classical polynomial approximations which only achieve algebraic convergence rates. There's a need for more efficient approximation methods for such functions.

Method: Constructive approximation scheme combining: 1) division-free polynomial iteration for fractional powers (inner layer), and 2) analytic polynomial fitting (outer layer). The deep composite structure uses multiple polynomial layers to handle cusp singularities.

Result: Exponential convergence in L^p([-1,1]) approximation error with respect to parameter budget (number of scalar coefficients). Numerical experiments confirm theoretical rates for both single and multiple cusp configurations, demonstrating parameter efficiency.

Conclusion: Deep composite polynomial constructions provide an efficient framework for approximating functions with algebraic cusp singularities, achieving exponential convergence that significantly outperforms classical single-layer polynomial approximations.

Abstract: We investigate deep composite polynomial approximations of continuous but non-differentiable functions with algebraic cusp singularities. The functions in focus consist of finitely many cusp terms of the form $|x-a_j|^{α_j}$ with rational exponents $α_j\in(0,1)$ on a real-analytic background. We propose a constructive approximation scheme that combines a division-free polynomial iteration for fractional powers with an outer layer for the analytic polynomial fitting. Our main result shows that this composite structure achieves exponential convergence in the the number of scalar coefficients in the inner and outer polynomial layers. Specifically, the $L^p([-1,1])$ approximation error, decays exponentially with respect to the parameter budget, in contrast to the algebraic rates obtained by classical single-layer polynomial approximation for cusp-type functions. Numerical experiments for both single and multiple cusp configurations confirm the theoretical rates and demonstrate the parameter efficiency of deep composite polynomial constructions.

</details>


### [10] [Newton-Krylov Methods for Computing Steady States of Particle Timesteppers via Optimal Transport](https://arxiv.org/abs/2512.24567)
*Hannes Vandecasteele,Nicholas Karris,Alexander Cloninger,Ioannis G. Kevrekidis*

Main category: math.NA

TL;DR: Matrix-free framework extended to compute steady-state distributions from stochastic particle simulations using timesteppers reinterpreted as optimal transport operators on probability measures.


<details>
  <summary>Details</summary>
Motivation: Stochastic particle simulations have intrinsic randomness that prevents direct steady state extraction, unlike deterministic systems where timesteppers can implicitly encode steady states and stability information.

Method: Reinterpret stochastic timesteppers as operators acting on probability measures via optimal transport framework, construct smooth (I)CDF timesteppers that evolve distributions rather than particles, combine with matrix-free Newton-Krylov solvers, perform error analysis of noise effects on Jacobian approximations, and develop higher-dimensional generalizations.

Result: Efficient computation of steady-state distributions even under high stochastic noise, convergence demonstrated in high noise regimes, successful validation on non-trivial two-dimensional distributions.

Conclusion: Establishes unified variational framework for computing meaningful steady states of both deterministic and stochastic timesteppers through probability measure perspective.

Abstract: Timesteppers constitute a powerful tool in modern computational science and engineering. Although they are typically used to advance the system forward in time, they can also be viewed as nonlinear mappings that implicitly encode steady states and stability information. In this work, we present an extension of the matrix-free framework for calculating, via timesteppers, steady states of deterministic systems to stochastic particle simulations, where intrinsic randomness prevents direct steady state extraction. By formulating stochastic timesteppers in the language of optimal transport, we reinterpret them as operators acting on probability measures rather than on individual particle trajectories. This perspective enables the construction of smooth cumulative- and inverse-cumulative-distribution-function ((I)CDF) timesteppers that evolve distributions rather than particles. Combined with matrix-free Newton-Krylov solvers, these smooth timesteppers allow efficient computation of steady-state distributions even under high stochastic noise. We perform an error analysis quantifying how noise affects finite-difference Jacobian action approximations, and demonstrate that convergence can be obtained even in high noise regimes. Finally, we introduce higher-dimensional generalizations based on smooth CDF-related representations of particles and validate their performance on a non-trivial two-dimensional distribution. Together, these developments establish a unified variational framework for computing meaningful steady states of both deterministic and stochastic timesteppers.

</details>


### [11] [Solving the inverse Source Problems for wave equation with final time measurements by a data driven approach](https://arxiv.org/abs/2512.24647)
*Qiling Gu,Wenlong Zhang,Zhidong Zhang*

Main category: math.NA

TL;DR: A discrete data-driven approach for solving wave equation inverse source problems using L²-Tikhonov regularization with final time measurements, establishing error bounds without classical source conditions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inverse source problem for wave equations with final time measurements, which is ill-posed and requires regularization. The motivation is to develop a discrete data-driven approach that can handle noisy measurements and provide convergence guarantees without relying on restrictive source conditions.

Method: Uses L²-Tikhonov regularization with spectral decomposition analysis and noise separation technique. Analyzes convergence under two noise models, establishes error bounds for reconstructed solution and source term, extends to fully discrete case with finite element discretization, and provides data-driven parameter selection.

Result: Established error bounds for reconstructed solution u and source term f without requiring classical source conditions. Derived expected convergence rate for source error in weaker topology. Showed overall error depends only on noise level, regularization parameter, time step size, and spatial mesh size in fully discrete case.

Conclusion: The proposed discrete data-driven approach effectively solves the wave equation inverse source problem, provides rigorous convergence analysis under various noise models, enables optimal parameter selection without a priori information, and is validated by numerical experiments demonstrating efficiency.

Abstract: This paper develops a discrete data-driven approach for solving the inverse source problem of the wave equation with final time measurements. Focusing on the $L^2$-Tikhonov regularization method, we analyze its convergence under two different noise models, using noisy discrete spatial observations. By exploiting the spectral decomposition of the forward operator and introducing a noise separation technique into the variational framework, we establish error bounds for the reconstructed solution $u$ and the source term $f$ without requiring classical source conditions. Moreover, an expected convergence rate for the source error is derived in a weaker topology. We also extend the analysis to the fully discrete case with finite element discretization, showing that the overall error depends only on the noise level, regularization parameter, time step size, and spatial mesh size. These estimates provide a basis for selecting the optimal regularization parameter in a data-driven manner, without a priori information. Numerical experiments validate the theoretical results and demonstrate the efficiency of the proposed algorithm.

</details>


### [12] [Boundary error control for numerical solution of BSDEs by the convolution-FFT method](https://arxiv.org/abs/2512.24714)
*Xiang Gao,Cody Hyndman*

Main category: math.NA

TL;DR: Improved CFFT method for BSDEs with better boundary error handling through modified damping and shifting schemes.


<details>
  <summary>Details</summary>
Motivation: The original CFFT approach for solving BSDEs suffers from boundary errors when valuing options, which needs improvement for better accuracy.

Method: Modified the damping and shifting schemes in the original CFFT formulation, introducing time-dependent shifting to transform target functions into bounded periodic functions suitable for Fourier transforms.

Result: Time-dependent shifting significantly reduces boundary errors, leading to improved accuracy and convergence of the modified convolution method.

Conclusion: The proposed modifications to the CFFT approach effectively address boundary error issues, providing a more accurate numerical solution for BSDEs in option pricing applications.

Abstract: We first review the convolution fast-Fourier-transform (CFFT) approach for the numerical solution of backward stochastic differential equations (BSDEs) introduced in (Hyndman and Oyono Ngou, 2017). We then propose a method for improving the boundary errors obtained when valuing options using this approach. We modify the damping and shifting schemes used in the original formulation, which transforms the target function into a bounded periodic function so that Fourier transforms can be applied successfully. Time-dependent shifting reduces boundary error significantly. We present numerical results for our implementation and provide a detailed error analysis showing the improved accuracy and convergence of the modified convolution method.

</details>


### [13] [A structure-preserving parametric approximation for anisotropic geometric flows via an $α$-surface energy matrix](https://arxiv.org/abs/2512.24875)
*Weizhu Bao,Yifei Li,Wenjun Ying,Yulin Zhang*

Main category: math.NA

TL;DR: Proposes a structure-preserving parametric approximation for anisotropic geometric flows with optimal energy stability at α=-1.


<details>
  <summary>Details</summary>
Motivation: Existing formulations for anisotropic geometric flows lack a unified framework with optimal energy stability properties. The paper aims to develop a general parametric approximation that preserves structure and achieves optimal stability conditions.

Method: Introduces a hyperparameter α to construct a unified surface energy matrix that encompasses all existing formulations. Applies this to anisotropic curvature flow and analyzes energy stability conditions. Extends framework to general anisotropic geometric flows through unified velocity discretization.

Result: Proves that α=-1 is the unique choice achieving optimal energy stability under the necessary and sufficient condition 3γ̂(θ)≥γ̂(θ-π). All other α≠-1 require strictly stronger conditions. Numerical experiments validate theoretical optimality and demonstrate effectiveness.

Conclusion: The proposed framework provides a unified approach for anisotropic geometric flows with optimal energy stability at α=-1, offering both theoretical guarantees and practical effectiveness for structure-preserving approximations.

Abstract: We propose a structure-preserving parametric approximation for geometric flows with general anisotropic effects. By introducing a hyperparameter $α$, we construct a unified surface energy matrix $\hat{\boldsymbol{G}}_k^α(θ)$ that encompasses all existing formulations of surface energy matrices, and apply it to anisotropic curvature flow. We prove that $α=-1$ is the unique choice achieving optimal energy stability under the necessary and sufficient condition $3\hatγ(θ)\geq\hatγ(θ-π)$, while all other $α\neq-1$ require strictly stronger conditions. The framework extends naturally to general anisotropic geometric flows through a unified velocity discretization that ensures energy stability. Numerical experiments validate the theoretical optimality of $α=-1$ and demonstrate the effectiveness and robustness.

</details>


### [14] [Random compressible Euler flows](https://arxiv.org/abs/2512.24879)
*Maria Lukacova-Medvidova,Simon Schneider*

Main category: math.NA

TL;DR: Finite volume stochastic collocation method for random Euler system with convergence proof under bounded discrete differential quotients assumption.


<details>
  <summary>Details</summary>
Motivation: Need robust numerical methods for random Euler systems (compressible fluid dynamics with uncertainty) that can handle stochasticity while maintaining mathematical rigor and convergence guarantees.

Method: Finite volume stochastic collocation method combining deterministic FV discretization with stochastic sampling. Convergence analysis uses deterministic FV convergence results plus stochastic compactness arguments (Skorokhod and Gyöngy-Krylov theorems).

Result: Rigorous proof of convergence for random finite volume solutions under the assumption that discrete differential quotients remain bounded in probability.

Conclusion: The proposed method provides a mathematically sound approach for solving random Euler systems with guaranteed convergence under reasonable assumptions, combining deterministic numerical analysis with stochastic analysis tools.

Abstract: We propose a finite volume stochastic collocation method for the random Euler system. We rigorously prove the convergence of random finite volume solutions under the assumption that the discrete differential quotients remain bounded in probability. Convergence analysis combines results on the convergence of a deterministic FV method with stochastic compactness arguments due to Skorokhod and Gyöngy-Krylov.

</details>


### [15] [A finite element approach for minimizing line and surface energies arising in the study of singularities in liquid crystals](https://arxiv.org/abs/2512.24928)
*Dominik Stantejsky*

Main category: math.NA

TL;DR: Numerical algorithm for Plateau-like problem with area, boundary length, and obstacle constraints, applied to defect structures in nematic liquid crystals.


<details>
  <summary>Details</summary>
Motivation: Study defect structures in nematic liquid crystals, specifically colloidal particles, requiring solution of Plateau-like problem with geometric constraints.

Method: ADMM-based algorithm using finite elements to minimize discretized energy containing surface area, boundary length, and obstacle constraints with surface energy on obstacle.

Result: Algorithm successfully handles various inclusion shapes, revealing rich minimizing configurations with physical interpretation for colloidal particles in nematic liquid crystals.

Conclusion: Generalized TV-minimization approach effectively solves complex geometric optimization problems relevant to liquid crystal physics, providing insights into defect structures.

Abstract: Motivated by a problem originating in the study of defect structures in nematic liquid crystals, we describe and study a numerical algorithm for the resolution of a Plateau-like problem. The energy contains the area of a two-dimensional surface $T$ and the length of its boundary $\partial T$ reduced by a prescribed curve to make our problem non-trivial. We additionally include an obstacle $E$ for $T$ and pose a surface energy on $E$. We present an algorithm based on the Alternating Direction Method of Multipliers that minimizes a discretized version of the energy using finite elements, generalizing existing TV-minimization methods. We study different inclusion shapes demonstrating the rich structure of minimizing configurations and provide physical interpretation of our findings for colloidal particles in nematic liquid crystal.

</details>


### [16] [Approximating evolution operators of linear delay equations: a general framework for the convergence analysis](https://arxiv.org/abs/2512.24964)
*Alessia andò,Giusy Bosco,Dimitri Breda,Davide Liessi*

Main category: math.NA

TL;DR: A framework for discretizing linear delay equation evolution operators to approximate spectra, with unified convergence analysis and application to pseudospectral and weighted residual methods.


<details>
  <summary>Details</summary>
Motivation: To investigate stability properties of nonlinear delay equations via linearized stability principle by approximating spectra of linear delay equation evolution operators.

Method: Develops general convergence analysis framework using fixed-point equation reformulation, with hypotheses about regularization properties and approximation convergence on suitable subspaces.

Result: Unifies proofs for pseudospectral discretization methods and provides formal convergence analysis for weighted residual method that previously lacked it.

Conclusion: The framework provides a general approach for analyzing convergence of discretization methods for linear delay equation spectra approximation.

Abstract: We consider the problem of discretizing evolution operators of linear delay equations with the aim of approximating their spectra, which is useful in investigating the stability properties of (nonlinear) equations via the principle of linearized stability. We develop a general convergence analysis based on a reformulation of the operators by means of a fixed-point equation, providing a list of hypotheses related to the regularization properties of the equation and the convergence of the chosen approximation techniques on suitable subspaces. This framework unifies the proofs for some methods based on pseudospectral discretization, which we present here in this new form. To exemplify the generality of the framework, we also apply it to a method of weighted residuals found in the literature, which was previously lacking a formal convergence analysis.

</details>


### [17] [At the intersection of Numerical Analysis and Spectral Geometry](https://arxiv.org/abs/2512.25012)
*Nilima Nigam*

Main category: math.NA

TL;DR: Survey paper on computational spectral geometry, examining how to compute operator spectra on geometric domains and how these computations guide spectral geometry research.


<details>
  <summary>Details</summary>
Motivation: Bridge spectral geometry (studying how domain geometry affects operator spectra) with numerical analysis (computing accurate approximations), exploring how computations can both guide conjectures and be incorporated into proofs.

Method: Expository survey reviewing discretization methods for operators, approximation strategies, and error control techniques, with focus on trade-offs between efficiency for conjecture formulation versus rigorous error bounds for proof strategies.

Result: Identifies that computational requirements of spectral geometry (rigorous error control, robust calculation of higher eigenvalues) motivate new developments in numerical analysis, creating a symbiotic relationship between the fields.

Conclusion: Computational spectral geometry represents a productive intersection where numerical methods inform geometric insights and geometric problems drive numerical innovation, with choice of approximation strategy depending on research objective (conjecture vs proof).

Abstract: How do the geometric properties of a domain impact the spectrum of an operator defined on it? How do we compute accurate and reliable approximations of these spectra? The former question is studied in spectral geometry, and the latter is a central concern in numerical analysis. In this short expository survey we revisit the process of eigenvalue approximation, from the perspective of computational spectral geometry. Over the years a multitude of methods -- for discretizing the operator and for the resultant discrete system -- have been developed and analyzed in the field of numerical analysis. High-accuracy and provably convergent discretization approaches can be used to examine the interplay between the spectrum of an operator and the geometric properties of the spatial domain or manifold it is defined on. While computations have been used to guide conjectures in spectral geometry, in recent years approximation-theoretic tools and validated computations are also being used as part of proof strategies in spectral geometry.
  Given a particular spectral feature of interest, should we discretize the original problem, or seek a reformulation? Of the many possible approximation strategies, which should we choose? These choices are inextricably linked to the objective: on the one hand, rapid, specialized methods are often ideal for conjecture formulation (prioritizing efficiency and accuracy), whereas schemes with guaranteed, computable error bounds are needed when computation is incorporated into a proof strategy. We also review instances where the demanding requirements of spectral geometry -- the need for rigorous error control or the robust calculation of higher eigenvalues -- motivate new developments in numerical analysis.

</details>


### [18] [Convergence of the generalization error for deep gradient flow methods for PDEs](https://arxiv.org/abs/2512.25017)
*Chenguang Liu,Antonis Papapantoleon,Jasper Rou*

Main category: math.NA

TL;DR: DGFMs for PDEs: generalization error → 0 as neurons & training time → ∞ via approximation error analysis and gradient flow convergence.


<details>
  <summary>Details</summary>
Motivation: Provide rigorous mathematical foundation for deep gradient flow methods (DGFMs) solving high-dimensional PDEs, addressing generalization error decomposition and convergence guarantees.

Method: Decompose generalization error into approximation and training errors. Show PDE solutions approximated by neural networks (approximation error → 0). Derive gradient flow in wide network limit and analyze its convergence as training time → ∞.

Result: Approximation error → 0 as neurons → ∞. Gradient flow converges in wide network limit. Combined: generalization error → 0 as neurons & training time → ∞.

Conclusion: DGFMs provide mathematically sound approach for solving PDEs with provable convergence guarantees for generalization error under reasonable assumptions.

Abstract: The aim of this article is to provide a firm mathematical foundation for the application of deep gradient flow methods (DGFMs) for the solution of (high-dimensional) partial differential equations (PDEs). We decompose the generalization error of DGFMs into an approximation and a training error. We first show that the solution of PDEs that satisfy reasonable and verifiable assumptions can be approximated by neural networks, thus the approximation error tends to zero as the number of neurons tends to infinity. Then, we derive the gradient flow that the training process follows in the ``wide network limit'' and analyze the limit of this flow as the training time tends to infinity. These results combined show that the generalization error of DGFMs tends to zero as the number of neurons and the training time tend to infinity.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [19] [Fractal Mehler kernels and nonlinear geometric flows](https://arxiv.org/abs/2512.23830)
*Nicola Garofalo*

Main category: math.AP

TL;DR: Introduces two-parameter Mehler kernels linked to Baouendi-Grushin flows in fractal dimensions, with connections to geometric fully nonlinear equations and open questions.


<details>
  <summary>Details</summary>
Motivation: To establish connections between Mehler kernels, Baouendi-Grushin flows in fractal dimensions, and geometric fully nonlinear equations, exploring new mathematical relationships.

Method: Introduces a two-parameter family of Mehler kernels and connects them to Baouendi-Grushin flows operating in fractal dimensions.

Result: Establishes a link between the introduced Mehler kernels and Baouendi-Grushin flows in fractal dimensions, and connects this framework to geometric fully nonlinear equations.

Conclusion: The paper successfully connects Mehler kernels, Baouendi-Grushin flows in fractal dimensions, and geometric fully nonlinear equations, while formulating two open questions for further research.

Abstract: In this paper we introduce a two-parameter family of Mehler kernels and connect them to a class of Baouendi-Grushin flows in fractal dimension. We also highlight a link with a geometric fully nonlinear equation and formulate two questions.

</details>


### [20] [A nonlinear instability result to the Navier-Stokes equations with Navier slip boundary conditions](https://arxiv.org/abs/2512.23946)
*Tien-Tai Nguyen*

Main category: math.AP

TL;DR: The paper investigates instability of trivial steady states in incompressible viscous fluids with Navier-slip boundary conditions, proving both linear and nonlinear instability using operator methods and adapting existing frameworks.


<details>
  <summary>Details</summary>
Motivation: To study the instability of trivial steady states in incompressible viscous fluids with Navier-slip boundary conditions, providing a different approach from previous work by Ding, Li and Xin (2018).

Method: Uses operator method of Lafitte and Nguyen (2022) to show existence of infinitely many normal mode solutions for linear instability, then adapts framework of Desjardins and Grenier (2003) for viscous boundary layers to prove nonlinear instability by obtaining two separated solutions at escaping time.

Result: Proves both linear instability (existence of infinitely many normal mode solutions) and nonlinear instability for trivial steady states in incompressible viscous fluids with Navier-slip boundary conditions.

Conclusion: The paper successfully demonstrates instability of trivial steady states using a novel approach different from previous work, combining operator methods for linear analysis with boundary layer frameworks for nonlinear analysis.

Abstract: In this paper, we investigate the instability of the trivial steady states to the incompressible viscous fluid with Navier-slip boundary conditions. For the linear instability, the existence of infinitely many normal mode solutions to the linearized equations is shown via the operator method of Lafitte and Nguyen (2022). Hence, we prove the nonlinear instability by adapting the framework of Desjardins and Grenier (2003) studying some classes of viscous boundary layers to obtain two separated solutions at escaping time. Our work performs a different approach from that of Ding, Li and Xin (2018).

</details>


### [21] [A regularity theory for second-order parabolic partial differential equations in weighted mixed norm Sobolev-Zygmund spaces](https://arxiv.org/abs/2512.24020)
*Jae-Hwan Choi,Junhee Ryu*

Main category: math.AP

TL;DR: Optimal regularity theory for parabolic PDEs in weighted mixed norm Sobolev-Zygmund spaces, extending Schauder estimates to time-measurable coefficients and integer-order regularity cases.


<details>
  <summary>Details</summary>
Motivation: Classical Schauder estimates have limitations: they typically require Hölder continuous coefficients and don't handle the critical case of integer-order regularity optimally. There's a need to extend regularity theory to more general coefficient classes (merely measurable in time) and handle nonzero initial data in optimal trace spaces.

Method: Develops regularity theory using weighted mixed norm Sobolev-Zygmund spaces. Employs a sharp trace theorem to handle nonzero initial data in optimal trace spaces. Extends classical parabolic theory to coefficients that are only measurable in time rather than requiring Hölder continuity.

Result: Establishes optimal regularity estimates for parabolic PDEs in the new functional framework. Successfully handles the critical case of integer-order regularity. Provides sharp trace results for initial data. Extends Schauder theory to more general coefficient classes (time-measurable coefficients).

Conclusion: The paper develops a comprehensive optimal regularity theory for parabolic equations in weighted mixed norm Sobolev-Zygmund spaces, significantly extending classical results by allowing more general coefficients and providing sharp trace theorems for initial data.

Abstract: We develop an optimal regularity theory for parabolic partial differential equations in weighted mixed norm Sobolev-Zygmund spaces. The results extend the classical Schauder estimates to coefficients that are merely measurable in time and to the critical case of integer-order regularity. In addition, nonzero initial data are treated in the optimal trace space via a sharp trace theorem.

</details>


### [22] [$L^p$ Estimates for Numerical Approximation of Hamilton-Jacobi Equations](https://arxiv.org/abs/2512.24051)
*Alessio Basti,Fabio Camilli*

Main category: math.AP

TL;DR: L^p error estimates for monotone schemes approximating Hamilton-Jacobi equations on torus, with L^1 error bound of order one and interpolation to L^p estimates for p>1.


<details>
  <summary>Details</summary>
Motivation: Need for rigorous error estimates for numerical schemes approximating Hamilton-Jacobi equations, improving existing results and providing unified framework.

Method: Adjoint method to prove L^1 error bound of order one for finite-difference and semi-Lagrangian schemes under convexity assumptions, then interpolation to obtain L^p estimates.

Result: Established L^p error estimates for monotone schemes, with L^1 error bound of order one, covering broad class of schemes and improving existing results.

Conclusion: Provides unified framework for discrete error estimates of Hamilton-Jacobi equations on torus, with improved error bounds applicable to various numerical schemes.

Abstract: We establish $L^p$ error estimates for monotone numerical schemes approximating Hamilton-Jacobi equations on the $d$-dimensional torus. Using the adjoint method, we first prove a $L^1$ error bound of order one for finite-difference and semi-Lagrangian schemes under standard convexity assumptions on the Hamiltonian. By interpolation, we also obtain $L^p$ estimates for every finite $p>1$. Our analysis covers a broad class of schemes, improves several existing results, and provides a unified framework for discrete error estimates.

</details>


### [23] [Propagation of chaos for the homogeneous Boltzmann equation with moderately soft potentials](https://arxiv.org/abs/2512.24065)
*Nicolas Fournier,Stéphane Mischler*

Main category: math.AP

TL;DR: Kac particle system converges to Boltzmann equation for moderately soft potentials, proving propagation of chaos via Fisher information control.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous connection between microscopic particle dynamics (Kac system) and macroscopic kinetic theory (Boltzmann equation) for moderately soft potentials, proving propagation of chaos.

Method: Adapt recent work by Imbert, Silvestre and Villani to show Fisher information is nonincreasing along solutions to Kac master equation, using this to control interaction singularity.

Result: Kac particle system converges to homogeneous Boltzmann equation as particle number → ∞ for moderately soft potentials (γ ∈ (-2,0)), establishing propagation of chaos.

Conclusion: Fisher information monotonicity provides key analytical tool to handle singular interactions, enabling rigorous proof of convergence from microscopic to macroscopic descriptions in kinetic theory.

Abstract: We show that the Kac particle system converges, as the number of particles tends to infinity, to the solution of the homogeneous Boltzmann equation, in the regime of moderately soft potentials, $γ\in (-2,0)$ with the common notation. This proves the propagation of chaos. We adapt the recent work of Imbert, Silvestre and Villani, to show that the Fisher information is nonincreasing in time along solutions to the Kac master equation. This estimate allows us to control the singularity of the interaction.

</details>


### [24] [Dirac solitons in one-dimensional nonlinear Schrödinger equations](https://arxiv.org/abs/2512.24089)
*William Borrelli,Elena Danesi,Simone Dovetta,Lorenzo Tentarelli*

Main category: math.AP

TL;DR: The paper studies stationary cubic NLS equations with periodic potentials featuring Dirac points, constructs Dirac solitons via NLD equations, and justifies NLD as an effective model for NLS.


<details>
  <summary>Details</summary>
Motivation: To understand and construct standing wave solutions (Dirac solitons) in nonlinear Schrödinger equations with periodic potentials that have Dirac points in their dispersion relation, and to rigorously justify the use of nonlinear Dirac equations as effective models.

Method: Introduce periodic perturbations to open spectral gaps around Dirac-point energies, then construct standing waves whose leading-order profile is a modulation of Bloch waves using spinor components solving an appropriate cubic nonlinear Dirac equation.

Result: Successfully construct Dirac solitons - standing wave solutions of the NLS equation - and provide rigorous justification for using the nonlinear Dirac equation as an effective model for the original nonlinear Schrödinger equation.

Conclusion: The nonlinear Dirac equation serves as a valid effective model for the nonlinear Schrödinger equation with periodic potentials featuring Dirac points, enabling construction of Dirac soliton solutions through modulation of Bloch waves.

Abstract: In this paper we study a family of one-dimensional stationary cubic nonlinear Schrödinger (NLS) equations with periodic potentials and linear part displaying Dirac points in the dispersion relation. By introducing a suitable periodic perturbation, one can open a spectral gap around the Dirac-point energy. This allows to construct standing waves of the NLS equation whose leading-order profile is a modulation of Bloch waves by means of the components of a spinor solving an appropriate cubic nonlinear Dirac (NLD) equation. We refer to these solutions as Dirac solitons. Our analysis thus provides a rigorous justification for the use of the NLD equation as an effective model for the original NLS equation.

</details>


### [25] [An Equivalence Result on the Order of Differentiability in Frobenius' Theorem](https://arxiv.org/abs/2512.24218)
*Yuhki Hosoya*

Main category: math.AP

TL;DR: The paper analyzes total differential equations in foliation theory without smoothness assumptions, revealing asymmetry in solution differentiability and establishing regularity results for integral manifolds.


<details>
  <summary>Details</summary>
Motivation: To study total differential equations in foliation theory without imposing smoothness assumptions, addressing the peculiar asymmetry in differentiability of solutions that arises in this context.

Method: Focuses on analyzing the differentiability of integral manifolds for total differential equations, examining regularity results under various smoothness conditions (locally Lipschitz, C^k), and providing counterexamples to illustrate limitations.

Result: When system is locally Lipschitz, solutions are only locally Lipschitz but integral manifolds must be C^1. For C^k systems, solutions are C^k but integral manifolds must be C^{k+1}. A counterexample shows C^1 system without C^2 solution. Characterizes minimizers for optimization with quasi-convex solutions.

Conclusion: The paper reveals fundamental asymmetry in regularity between solutions and integral manifolds in total differential equations, establishes precise regularity relationships, and provides conditions for quasi-convex solutions in optimization contexts.

Abstract: This paper examines the simplest case of total differential equations that appears in the theory of foliation structures, without imposing the smoothness assumptions. This leads to a peculiar asymmetry in the differentiability of solutions. To resolve this asymmetry, this paper focuses on the differentiability of the integral manifold. When the system is locally Lipschitz, a solution is ensured to be only locally Lipschitz, but the integral manifolds must be $C^1$. When the system is $C^k$, we can only ensure the existence of a $C^k$ solution, but the integral manifolds must be $C^{k+1}$. In addition, we see a counterexample in which the system is $C^1$, but there is no $C^2$ solution. Moreover, we characterize a minimizer of an optimization problem whose objective function is a quasi-convex solution to a total differential equation. In this connection, we examine two necessary and sufficient conditions for the system in which any solution is quasi-convex.

</details>


### [26] [Multi-bump solutions for sublinear elliptic equations with nonsymmetric coefficients](https://arxiv.org/abs/2512.24234)
*Chengxiang Zhang,Xu Zhang*

Main category: math.AP

TL;DR: The paper proves existence of infinitely many nonnegative bump solutions to a sublinear elliptic equation with nonsymmetric potential, using sharp support estimates to control bump interactions.


<details>
  <summary>Details</summary>
Motivation: To study existence of multiple bump solutions for sublinear elliptic equations with nonsymmetric potentials, overcoming challenges from sensitive bump interactions due to compact support of limiting profiles.

Method: Uses truncated functional space approach to obtain sharp estimates of support sets. Derives qualitative local stability estimates in region-wise maximum norms to control each bump's essential support and minimize overlap.

Result: Constructs infinitely many solutions with arbitrarily many bumps when ‖K-1‖_{L^p_loc} is sufficiently small. Estimates are uniform in number of bumps, enabling existence of solutions with infinitely many bumps.

Conclusion: The method provides precise control over bump interactions through sharp support estimates, establishing existence of infinitely many bump solutions for sublinear elliptic equations with nonsymmetric potentials.

Abstract: We investigate the existence of nonnegative bump solutions to the sublinear elliptic equation \[ \begin{cases} -Δv - K(x)v + |v|^{q-2}v = 0 & \text{in } \mathbb{R}^N, \\ v(x) \to 0 & \text{as } |x| \to \infty, \end{cases} \] where $q \in (1,2)$, $ N \geq 2$, and the potential $K \in L^p_{\mathrm{loc}}(\mathbb{R}^N)$ with $p > N/2$ is a function without any symmetry assumptions. Under the condition that $\|K - 1\|_{L^p_{\mathrm{loc}}}$ is sufficiently small, we construct infinitely many solutions with arbitrarily many bumps. The construction is challenged by the sensitive interaction between bumps, whose limiting profiles have compact support. The key to ensuring their effective separation lies in obtaining sharp estimates of the support sets. Our method, based on a truncated functional space, provides precisely such control. We derive qualitative local stability estimates in region-wise maximum norms that govern the size of each bump's essential support, confining its core to a designated region and minimizing overlap. Crucially, these estimates are uniform in the number of bumps, which is the pivotal step in establishing the existence of solutions with infinitely many bumps.

</details>


### [27] [Mean-Field Limits of Deterministic and Stochastic Flocking Models with Nonlinear Velocity Alignment](https://arxiv.org/abs/2512.24383)
*Vinh Nguyen,Roman Shvydkoy,Changhui Tan*

Main category: math.AP

TL;DR: The paper studies mean-field limits for agent-based flocking models with nonlinear velocity alignment, proving convergence in both deterministic and stochastic settings with improved propagation of chaos rates.


<details>
  <summary>Details</summary>
Motivation: To extend classical Cucker-Smale flocking theory to nonlinear velocity alignment frameworks that have gained recent attention, addressing both deterministic and stochastic agent interactions.

Method: Analyzes agent-based models with nonlinear velocity coupling A(v) = |v|^{p-2}v (p>2) and communication protocol φ. Proves mean-field limits using propagation of chaos techniques for deterministic case with fat-tailed kernels, and handles stochastic version with multiplicative noise dependent on local interaction intensity.

Result: Establishes mean-field convergence in both deterministic and stochastic settings. Provides quantitative propagation of chaos estimates showing improved convergence rates for k-particle marginals to Vlasov equation solutions in deterministic case. For stochastic case, derives associated Fokker-Planck-Alignment equation.

Conclusion: Successfully extends Cucker-Smale theory to nonlinear alignment frameworks, providing rigorous mathematical foundation for both deterministic and stochastic flocking models with nonlinear velocity coupling.

Abstract: We study the mean-field limit for a class of agent-based models describing flocking with nonlinear velocity alignment. Each agent interacts through a communication protocol $φ$ and a non-linear coupling of velocities given by the power law $A(\bv) = |\bv|^{p-2}\bv$, $p > 2$. The mean-field limit is proved in two settings -- deterministic and stochastic. We then provide quantitative estimates on propagation of chaos for deterministic case in the case of the classical fat-tailed kernels, showing an improved convergence rate of the $k$-particle marginals to a solution of the corresponding Vlasov equation. The stochastic version is addressed with multiplicative noise depending on the local interaction intensity, which leads to the associated Fokker-Planck-Alignment equation.
  Our results extend the classical Cucker-Smale theory to the nonlinear framework which has received considerable attention in the literature recently.

</details>


### [28] [Stability of the reconstruction of the heat reflection coefficient in the phonon transport equation](https://arxiv.org/abs/2512.24394)
*Peiyi Chen,Irene M. Gamba,Qin Li,Anjali Nair*

Main category: math.AP

TL;DR: The paper analyzes stability of inverse problem for inferring reflection coefficient in phonon transport, showing ill-posedness as system transitions from ballistic to diffusive regime (Knudsen number → 0).


<details>
  <summary>Details</summary>
Motivation: Reflection coefficient is crucial thermal property at nanoscale, but determining it requires solving inverse problem from macroscopic temperature measurements. Previous studies show discrepancy about well-posedness of this inverse problem.

Method: Investigate stability of inverse problem for reflection coefficient in phonon transport equation. Analyze how stability changes as system transitions between transport regimes characterized by Knudsen number.

Result: Problem becomes ill-posed as system transitions from ballistic to diffusive regime (Knudsen number → 0). Quantify rate at which stability deteriorates with respect to Knudsen number. Provide theoretical analysis confirmed by numerical evidence.

Conclusion: Stability analysis clarifies discrepancy in previous studies about well-posedness of inverse problem for reflection coefficient. The work provides fundamental understanding of when and why this inverse problem becomes ill-posed in different transport regimes.

Abstract: The reflection coefficient is an important thermal property of materials, especially at the nanoscale, and determining this property requires solving an inverse problem based on macroscopic temperature measurements. In this manuscript, we investigate the stability of this inverse problem to infer the reflection coefficient in the phonon transport equation. We show that the problem becomes ill-posed as the system transitions from the ballistic to the diffusive regime, characterized by the Knudsen number converging to zero. Such a stability estimate clarifies the discrepancy observed in previous studies on the well-posedness of this inverse problem. Furthermore, we quantify the rate at which the stability deteriorates with respect to the Knudsen number and confirm the theoretical result with numerical evidence.

</details>


### [29] [Solvability conditions for some non-Fredholm operators with shifted arguments](https://arxiv.org/abs/2512.24476)
*Vitali Vougalter,Vitaly Volpert*

Main category: math.AP

TL;DR: Existence and convergence results for solutions in H²(R) of linear differential and integro-differential equations with argument translation/constant shift, showing convergence of source terms/kernels implies solution convergence.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous existence and convergence results for solutions to differential equations with argument translation/shift, which are important in applications but present analytical challenges due to the non-local nature of the translation operator.

Method: Two-part approach: 1) For linear differential equations with argument translation, use sequence-based existence theory in H²(R) with L² convergence of source terms. 2) For integro-differential equations with argument shift, use sequence-based solvability in H²(R) with L¹ convergence of integral kernels. Both analyze second-order differential operators with/without Fredholm property depending on translation constant.

Result: 1) Under reasonable technical conditions, L² convergence of source terms implies existence and H² convergence of solutions for linear differential equations with argument translation. 2) Under appropriate auxiliary assumptions, L¹ convergence of integral kernels yields existence and H² convergence of solutions for integro-differential equations with argument shift.

Conclusion: The paper successfully establishes sequence-based existence and convergence results for both linear differential and integro-differential equations involving argument translation/shift, providing rigorous analytical foundations for these non-local problems with practical applications.

Abstract: In the first part of the article we establish the existence in the sense of sequences of solutions in $H^{2}(R)$ for some nonhomogeneous linear differential equation in which one of the terms has the argument translated by a constant. It is shown that under the reasonable technical conditions the convergence in $L^{2}(R)$ of the source terms implies the existence and the convergence in $H^{2}(R)$ of the solutions. The second part of the work deals with the solvability in the sense of sequences in $H^{2}(R)$ of the integro-differential equation in which one of the terms has the argument shifted by a constant. It is demonstrated that under the appropriate auxiliary assumptions the convergence in $L^{1}(R)$ of the integral kernels yields the existence and the convergence in $H^{2}(R)$ of the solutions. Both equations considered involve the second order differential operator with or without the Fredholm property depending on the value of the constant by which the argument gets translated.

</details>


### [30] [Steady Self-Propelled Motion of a Rigid Body in a Viscous Fluid with Navier-Slip Boundary Conditions](https://arxiv.org/abs/2512.24510)
*Sarka Necasova,Arnab Roy,Ana Leonor Silvestre*

Main category: math.AP

TL;DR: Existence of steady self-propelled motion for a rigid body in viscous fluid with Navier-slip boundary conditions, establishing weak solutions and characterizing propulsion through boundary effects.


<details>
  <summary>Details</summary>
Motivation: To understand propulsion mechanisms in microfluidic and rough-surface regimes where partial slip effects are significant, extending classical Dirichlet-based theory to more realistic Navier-slip boundary conditions.

Method: Analysis in body-fixed reference frame with fluid in exterior domain; use of Navier-slip boundary conditions at fluid-body interface; derivation of Korn-type inequality for exterior domains with rigid-body motion; introduction of finite-dimensional thrust space via auxiliary Stokes problems.

Result: Established existence of weak steady solutions under smallness assumptions on boundary flux and normal surface velocity; provided necessary and sufficient condition for slip velocity to induce nontrivial translational/rotational motion through thrust space analysis.

Conclusion: The work extends classical fluid-structure interaction theory to Navier-slip settings, clarifying how boundary effects generate propulsion and providing analytical framework for microfluidic propulsion mechanisms.

Abstract: We investigate the steady self-propelled motion of a rigid body immersed in a three-dimensional incompressible viscous fluid governed by the Navier-Stokes equations. The analysis is performed in a body-fixed reference frame, so that the fluid occupies an exterior domain and the propulsion mechanism is modeled through nonhomogeneous Navier-slip boundary conditions at the fluid-body interface. Such conditions provide a realistic description of propulsion in microfluidic and rough-surface regimes, where partial slip effects are significant. Under suitable smallness assumptions on the boundary flux and on the normal component of the prescribed surface velocity, we establish the existence of weak steady solutions to the coupled fluid-structure system. A key analytical ingredient is the derivation of a Korn-type inequality adapted to exterior domains with rigid-body motion and Navier-slip interfaces, which yields uniform control of both the fluid velocity and the translational and rotational velocities of the body. Beyond existence, we provide a necessary and sufficient condition under which a prescribed slip velocity on the body surface induces nontrivial translational or rotational motion of the rigid body. This is achieved through the introduction of a finite-dimensional thrust space, defined via auxiliary exterior Stokes problems with Navier boundary conditions, which captures the effective contribution of boundary-driven flows to the rigid-body motion. Our results clarify how boundary effects generate propulsion and extend the classical Dirichlet-based theory to the Navier-slip setting.

</details>


### [31] [Anomalous Dissipation at Onsager-Critical Regularity](https://arxiv.org/abs/2512.24568)
*Alexey Cheskidov,Qirui Peng*

Main category: math.AP

TL;DR: The paper constructs 3D Euler equation solutions showing anomalous dissipation in finite time via vanishing viscosity, extending previous 2.5D constructions and establishing sharp Onsager-critical energy criteria.


<details>
  <summary>Details</summary>
Motivation: To demonstrate anomalous dissipation (energy dissipation without viscosity) in 3D Euler equations through explicit constructions, extending previous work on 2.5-dimensional flows and establishing sharp criteria for such phenomena.

Method: Extends 2.5-dimensional constructions from previous works, uses vanishing viscosity limit approach, establishes Onsager-critical energy criterion adapted to such flows, and provides fully 3D dissipative Euler example with slightly rough external force following specific framework.

Result: Successfully constructs 3D Euler solutions exhibiting anomalous dissipation in finite time, shows sharpness of the established Onsager-critical energy criterion, and provides a fully 3D dissipative example that is sharp in Onsager's sense.

Conclusion: The work demonstrates explicit constructions of 3D Euler flows with anomalous dissipation, establishes sharp criteria for such phenomena, and extends understanding of dissipative mechanisms in ideal fluid dynamics beyond classical viscosity assumptions.

Abstract: We construct solutions to the three-dimensional Euler equations exhibiting anomalous dissipation in finite time through a vanishing viscosity limit. Inspired by \cite{BDL23} and \cite{cheskidov2023dissipation}, we extend the \(2\frac{1}{2}\)-dimensional constructions and establish an Onsager-critical energy criterion adapted to such flows, showing its sharpness. Moreover, we provide a fully three-dimensional dissipative Euler example, sharp in Onsager's sense, driven by a slightly rough external force, following the framework of \cite{CL21}.

</details>


### [32] [Propagation of space-time singularities for perturbed harmonic oscillators](https://arxiv.org/abs/2512.24582)
*Kenichi Ito,Tomoya Tagawa*

Main category: math.AP

TL;DR: The paper analyzes singularity propagation for quantum harmonic oscillators with time-dependent perturbations, extending Nakamura's spatial singularity analysis to include time as a base variable.


<details>
  <summary>Details</summary>
Motivation: To understand how space-time singularities propagate in quantum harmonic oscillators when both the metric and potential are perturbed in a time-dependent manner, extending previous spatial-only analyses to include temporal components.

Method: Reformulates Lascar's quasi-homogeneous wave front set in a semiclassical framework and adapts Nakamura's 2009 argument for spatial singularities to handle time as a base variable rather than just a parameter.

Result: Obtains a characterization of how singularities appear in the perturbed system compared to the unperturbed one, addressing the non-trivial challenge of including time in the base variables.

Conclusion: Successfully extends singularity analysis to time-dependent quantum harmonic oscillators by adapting existing methods to handle the temporal dimension as an integral part of the base variables.

Abstract: We discuss propagation of space-time singularities for the quantum harmonic oscillator with time-dependent metric and potential perturbations. Reformulating the quasi-homogeneous wave front set according to Lascar (1977) in a semiclassical manner, we obtain a characterization of its appearance in comparison with the unperturbed system. The idea of our proof is based on the argument of Nakamura (2009), which was originally devised for the analysis of spatial singularities of the Schrödinger equation, however, the application is non-trivial since the time is no more a parameter, but takes a part in the base variables.

</details>


### [33] [Phase transition thresholds and chiral magnetic fields of general degree](https://arxiv.org/abs/2512.24598)
*Slim Ibrahim,Tatsuya Miura,Carlos Román,Ikkei Shimizu*

Main category: math.AP

TL;DR: The paper studies variational problems for Landau-Lifshitz energy with Dzyaloshinskii-Moriya interactions in 2D micromagnetics, focusing on the Bogomol'nyi regime. It determines minimal energy for arbitrary topological degree, reveals phase transitions, proves uniqueness of minimizers in specific degrees, and analyzes stability transitions.


<details>
  <summary>Details</summary>
Motivation: To understand the variational properties of the Landau-Lifshitz energy with Dzyaloshinskii-Moriya interactions in 2D micromagnetics, particularly in the Bogomol'nyi regime. The research aims to characterize energy minimization, phase transitions, and stability properties relevant to skyrmion physics.

Method: The authors use variational analysis techniques to study the Landau-Lifshitz energy functional with Dzyaloshinskii-Moriya interactions. They focus on the Bogomol'nyi regime and employ mathematical methods to determine minimal energy configurations, prove uniqueness/nonexistence results for minimizers, and analyze stability transitions.

Result: 1) Determined minimal energy for arbitrary topological degree, revealing two types of phase transitions consistent with physical observations. 2) Proved uniqueness of energy minimizers in degrees 0 and -1, and nonexistence of minimizers for all other degrees. 3) Showed the homogeneous state remains stable beyond the threshold where skyrmion loses stability. 4) Uncovered a new stability transition driven by Zeeman energy.

Conclusion: The study provides a comprehensive mathematical analysis of energy minimization in the Bogomol'nyi regime of 2D micromagnetics. The results reveal important phase transitions, establish precise conditions for existence/uniqueness of minimizers, and identify novel stability phenomena driven by Zeeman energy, contributing to the fundamental understanding of skyrmion physics.

Abstract: We study a variational problem for the Landau--Lifshitz energy with Dzyaloshinskii--Moriya interactions arising in 2D micromagnetics, focusing on the Bogomol'nyi regime. We first determine the minimal energy for arbitrary topological degree, thereby revealing two types of phase transitions consistent with physical observations. In addition, we prove the uniqueness of the energy minimizer in degrees $0$ and $-1$, and nonexistence of minimizers for all other degrees. Finally, we show that the homogeneous state remains stable even beyond the threshold at which the skyrmion loses stability, and we uncover a new stability transition driven by the Zeeman energy.

</details>


### [34] [Half-space minimizing solutions of a two dimensional Allen-Cahn system](https://arxiv.org/abs/2512.24610)
*Zhiyuan Geng*

Main category: math.AP

TL;DR: Complete classification of half-space minimizing solutions to 2D Allen-Cahn system with Dirichlet boundary conditions in terms of blow-down limits at infinity, plus characterization of asymptotic behavior near sharp interfaces.


<details>
  <summary>Details</summary>
Motivation: To understand the structure and behavior of minimizing solutions to the Allen-Cahn system on the upper half plane with Dirichlet boundary conditions, which is important for phase transition problems and interface dynamics in bounded domains.

Method: Study minimizing solutions to the 2D Allen-Cahn system on the upper half plane with Dirichlet boundary conditions, using blow-down analysis at infinity to classify solutions, and analyzing asymptotic behavior near sharp interfaces.

Result: Complete classification of half-space minimizing solutions in terms of their blow-down limits at infinity, and characterization of asymptotic behavior near sharp interfaces.

Conclusion: The paper provides a comprehensive understanding of minimizing solutions to the Allen-Cahn system on the upper half plane, establishing connections between boundary conditions, blow-down limits, and interface behavior.

Abstract: This paper studies minimizing solutions to a two dimensional Allen-Cahn system on the upper half plane, subject to Dirichlet boundary conditions, \begin{equation*}
  Δu-\nabla_u W(u)=0, \quad u: \mathbb{R}_+^2\to \mathbb{R}^2,\ u=u_0 \text{ on } \partial \mathbb{R}_+^2, \end{equation*} where $W: \mathbb{R}^2\to [0,\infty)$ is a multi-well potential. We give a complete classification of such half-space minimizing solutions in terms of their blow-down limits at infinity. In addition, we characterize the asymptotic behavior of solutions near the associated sharp interfaces.

</details>


### [35] [A unified spatiotemporal formulation with physics-preserving structure for time-dependent convection-diffusion problems](https://arxiv.org/abs/2512.24650)
*James H. Adler,Xiaozhe Hu,Seulip Lee*

Main category: math.AP

TL;DR: A 4D spacetime formulation for time-dependent convection-diffusion problems using exterior calculus that preserves physical structures and enables well-posed variational formulations.


<details>
  <summary>Details</summary>
Motivation: To develop a unified framework for time-dependent convection-diffusion problems that preserves underlying physical structures (divergence-free and curl-free conditions) and enables robust numerical treatment by treating time as an additional space-like coordinate.

Method: Reformulate evolution problems as stationary convection-diffusion equations on 4D spacetime domains using exterior calculus. Extend to H(grad), H(curl), and H(div) problems. Introduce 4D Hodge-Laplacian operator with spatiotemporal diffusion tensor and convection field, augmented by small temporal perturbation for nondegeneracy. Develop exponentially-fitted 4D spatiotemporal flux operator that symmetrizes the convection-diffusion operator.

Result: The formulation naturally incorporates fundamental physical constraints including divergence-free and curl-free conditions. The exponentially-fitted flux operator enables well-posed variational formulation. The temporally-perturbed formulation converges to the original time-dependent convection-diffusion model as perturbation parameter tends to zero.

Conclusion: The proposed 4D spacetime formulation provides a unified, structure-preserving framework for time-dependent convection-diffusion problems that maintains physical constraints and enables robust mathematical analysis and numerical implementation through well-posed variational formulations.

Abstract: We propose a unified four-dimensional (4D) spatiotemporal formulation for time-dependent convection-diffusion problems that preserves underlying physical structures. By treating time as an additional space-like coordinate, the evolution problem is reformulated as a stationary convection-diffusion equation on a 4D space-time domain. Using exterior calculus, we extend this framework to the full family of convection-diffusion problems posed on $H(\textbf{grad})$, $H(\textbf{curl})$, and $H(\text{div})$. The resulting formulation is based on a 4D Hodge-Laplacian operator with a spatiotemporal diffusion tensor and convection field, augmented by a small temporal perturbation to ensure nondegeneracy. This formulation naturally incorporates fundamental physical constraints, including divergence-free and curl-free conditions. We further introduce an exponentially-fitted 4D spatiotemporal flux operator that symmetrizes the convection-diffusion operator and enables a well-posed variational formulation. Finally, we prove that the temporally-perturbed formulation converges to the original time-dependent convection-diffusion model as the perturbation parameter tends to zero.

</details>


### [36] [$L_p$-estimates for nonlocal equations with general Lévy measures](https://arxiv.org/abs/2512.24704)
*Hongjie Dong,Junhee Ryu*

Main category: math.AP

TL;DR: The paper establishes continuity of nonlocal operators with singular Lévy measures and unique strong solvability of corresponding parabolic equations in Lp spaces, with analysis of weighted mixed-norm space applicability.


<details>
  <summary>Details</summary>
Motivation: To study nonlocal operators with very singular Lévy measures without time regularity assumptions, addressing the gap in understanding such operators' properties and the solvability of corresponding parabolic equations.

Method: Analysis of nonlocal operators with general Lévy measures of order σ∈(0,2), allowing singular measures and no time regularity. The approach involves establishing operator continuity and proving unique strong solvability in Lp spaces.

Result: Continuity of the operators and unique strong solvability of corresponding nonlocal parabolic equations in Lp spaces are established. The applicability in weighted mixed-norm spaces depends on ranges of σ and d.

Conclusion: The paper successfully analyzes nonlocal operators with singular Lévy measures, establishing fundamental properties and solvability results, while characterizing when weighted mixed-norm space treatment is possible based on parameter ranges.

Abstract: We consider nonlocal operators of the form \begin{equation*}
  L_t u(x) = \int_{\mathbb{R}^d} \left( u(x+y)-u(x)-\nabla u(x)\cdot y^{(σ)} \right) ν_t(dy), \end{equation*} where $ν_t$ is a general Lévy measure of order $σ\in(0,2)$. We allow this class of Lévy measures to be very singular and impose no regularity assumptions in the time variable. Continuity of the operators and the unique strong solvability of the corresponding nonlocal parabolic equations in $L_p$ spaces are established. We also demonstrate that, depending on the ranges of $σ$ and $d$, the operator can or cannot be treated in weighted mixed-norm spaces.

</details>


### [37] [Global spherically symmetric classical solutions for arbitrary large initial data of the multi-dimensional non-isentropic compressible Navier-Stokes equations](https://arxiv.org/abs/2512.24799)
*Yongteng Gu,Xiangdi Huang*

Main category: math.AP

TL;DR: This paper proves global classical solutions for arbitrary large initial data to the viscous shallow water system with transport entropy in spherically symmetric 2D and 3D cases, extending previous results on shallow water equations.


<details>
  <summary>Details</summary>
Motivation: The global classical solutions for arbitrary large initial data of multi-dimensional viscous Saint-Venant (shallow water) equations have been an open problem since 1871. Recent breakthroughs by Huang-Meng-Zhang (2025) and Chen-Zhang-Zhu (2025) solved this for radial symmetry, but with dimensional and adiabatic index restrictions. This paper aims to generalize these results to non-isentropic compressible fluids with transported entropy.

Method: The authors prove a new BD entropy inequality for a class of non-isentropic compressible fluids (generalization of shallow water equations with transported entropy). They employ new estimates on the lower bound of density different from Huang-Meng-Zhang's approach, and analyze the spherically symmetric initial-boundary value problem.

Result: The paper shows that the "viscous shallow water system with transport entropy" admits global classical solutions for arbitrary large initial data in both two and three dimensions under spherical symmetry. The results relax previous restrictions: from N=2, γ≥3/2 to N=2, γ>1 and N=3, 1<γ<3.

Conclusion: This work extends the existence theory for global classical solutions of viscous shallow water equations to include transported entropy and broader parameter ranges, representing significant progress on a long-standing open problem in fluid dynamics.

Abstract: In 1871, Saint-Venant introduced the shallow water equations. Since then, the global classical solutions for arbitrary large initial data of the multi-dimensional viscous Saint-Venant system have remained a well-known open problem. It was only recently that [Huang-Meng-Zhang, http:arXiv:2512.15029, 2025], under the assumption of radial symmetry, first proved the existence of global classical solutions for arbitrary large initial data to the initial-boundary value problem of the two-dimensional viscous shallow water equations. At the same time, [Chen-Zhang-Zhu, http:arXiv:2512.18545, 2025] also independently proved the existence of global large solutions to the Cauchy problem of this system. Notably, in the work of Huang-Meng-Zhang, they also established the existence of global classical solutions for arbitrary large initial data to the isentropic compressible Navier-Stokes equations satisfying the BD entropy equality in both two and three dimensions, and the viscous shallow water equations are precisely a specific class of isentropic compressible fluids subject to the BD entropy equality. In this paper, we prove a new BD entropy inequality for a class of non-isentropic compressible fluids, which can be regarded as a generalization of the shallow water equations with transported entropy. Employing new estimates on the lower bound of density different from that of Huang-Meng-Zhang's work, we show the "viscous shallow water system with transport entropy" will admit global classical solutions for arbitrary large initial data to the spherically symmetric initial-boundary value problem in both two and three dimensions. Our results also relax the restrictions on the dimension and adiabatic index imposed in Huang-Meng-Zhang's work on the shallow water equations, extending the range from $N=2,\ γ\ge \frac{3}{2}$ to $N=2,\ γ> 1$ and $N=3,\ 1<γ<3$.

</details>


### [38] [Hölder continuity of weak solutions to the thin-film equation in $d=2$](https://arxiv.org/abs/2512.24809)
*Federico Cornalba,Julian Fischer,Erika Maringová Kokavcová*

Main category: math.AP

TL;DR: The paper proves Hölder continuity of energy-dissipating weak solutions to the thin-film equation in two spatial dimensions, solving a major open problem in the field.


<details>
  <summary>Details</summary>
Motivation: The thin-film equation models viscous liquid film spreading on surfaces. While existence theory for weak solutions was established decades ago, proving regularity properties like boundedness and Hölder continuity in 2D remained a major unsolved problem due to the equation's fourth-order degenerate parabolic structure.

Method: The proof uses the hole-filling technique, which is adapted to handle the challenges posed by the degenerate parabolicity of the fourth-order PDE. The authors overcome the limitation that standard De Giorgi-Nash-Moser theory doesn't apply to fourth-order equations.

Result: The main result is the proof of Hölder continuity for energy-dissipating weak solutions to the thin-film equation in two spatial dimensions (d=2), which is the physically most relevant case.

Conclusion: This work resolves a longstanding open problem in thin-film equation theory by establishing regularity properties for weak solutions in 2D, providing important mathematical foundations for understanding thin liquid film dynamics.

Abstract: The thin-film equation $\partial_t u = -\nabla \cdot (u^n \nabla Δu)$ describes the evolution of the height $u=u(x,t)\geq 0$ of a viscous thin liquid film spreading on a flat solid surface. We prove Hölder continuity of energy-dissipating weak solutions to the thin-film equation in the physically most relevant case of two spatial dimensions $d=2$. While an extensive existence theory of weak solutions to the thin-film equation was established more than two decades ago, even boundedness of weak solutions in $d=2$ has remained a major unsolved problem in the theory of the thin-film equation. Due the fourth-order structure of the thin-film equation, De Giorgi-Nash-Moser theory is not applicable. Our proof is based on the hole-filling technique, the challenge being posed by the degenerate parabolicity of the fourth-order PDE.

</details>


### [39] [Bol's type inequality for singular metrics and its application to prescribing $Q$-curvature problems](https://arxiv.org/abs/2512.24828)
*Mrityunjoy Ghosh,Ali Hyder*

Main category: math.AP

TL;DR: Higher-order Bol's inequality applied to radial normal solutions of singular Liouville equations yields existence criteria for singular Q-curvature problems and uniform bounds on total Q-curvature.


<details>
  <summary>Details</summary>
Motivation: To establish necessary and sufficient conditions for existence of radial normal solutions to singular Q-curvature problems, and to obtain uniform bounds on total Q-curvature under appropriate assumptions.

Method: Apply higher-order Bol's inequality to radial normal solutions of singular Liouville equations, combined with compactness arguments.

Result: Derived necessary and sufficient conditions for existence of radial normal solutions to singular Q-curvature problems, and obtained uniform bounds on total Q-curvature under suitable assumptions.

Conclusion: Higher-order Bol's inequality combined with compactness arguments provides powerful tools for analyzing existence and bounds in singular Q-curvature problems with radial normal solutions.

Abstract: In this article, we study higher-order Bol's inequality for radial normal solutions to a singular Liouville equation. By applying these inequalities along with compactness arguments, we derive necessary and sufficient conditions for the existence of radial normal solutions to a singular $Q$-curvature problem. Moreover, under suitable assumptions on the $Q$-curvature, we obtain uniform bounds on the total $Q$-curvature.

</details>


### [40] [Boundedness of Fourier Integral Operators with complex phases on Fourier Lebesgue spaces](https://arxiv.org/abs/2512.24854)
*Duván Cardona,William Obeng-Denteh,Frederick Opoku*

Main category: math.AP

TL;DR: Complex phase Fourier integral operators are bounded on Fourier Lebesgue spaces under spatial factorization condition with sharp order bound.


<details>
  <summary>Details</summary>
Motivation: Extend boundedness results for Fourier integral operators from real canonical relations (Rodino, Nicola, Cordero) to complex canonical relations, establishing complex analogues of known estimates.

Method: Develop boundedness estimates for Fourier integral operators on Fourier Lebesgue spaces when the canonical relation is parametrized by a complex phase function, under spatial factorization condition of rank ϰ.

Result: The Fourier integral operator is bounded on Fourier Lebesgue space F L^p when order m ≤ -ϰ|1/p - 1/2|, for 1 ≤ p ≤ ∞. This condition on m is sharp.

Conclusion: Successfully established complex analogue of real canonical relation results with sharp order bounds, extending boundedness theory to complex phase Fourier integral operators.

Abstract: In this paper, we develop boundedness estimates for Fourier integral operators on Fourier Lebesgue spaces when the associated canonical relation is parametrised by a complex phase function. Our result constitutes the complex analogue of those obtained for real canonical relations by Rodino, Nicola, and Cordero. We prove that, under the spatial factorization condition of rank $\varkappa$, the corresponding Fourier integral operator is bounded on the Fourier Lebesgue space $\mathcal{F}L^p,$ provided that the order $m$ of the operator satisfies that $ m \leq -\varkappa\left|\frac{1}{p}-\frac{1}{2}\right|, 1 \leq p \leq \infty. $ This condition on the order $m$ is sharp.

</details>


### [41] [Global boundedness and absorbing sets in two-dimensional chemotaxis-Navier-Stokes systems with weakly singular sensitivity and a sub-logistic source](https://arxiv.org/abs/2512.24892)
*Minh Le,Alexey Cheskidov*

Main category: math.AP

TL;DR: The paper proves global existence and boundedness of classical solutions for a chemotaxis-fluid system with logistic growth and modified chemotactic sensitivity in 2D bounded domains.


<details>
  <summary>Details</summary>
Motivation: To establish global well-posedness for a chemotaxis-fluid model with modified chemotactic sensitivity (n∇c/c^k) and logistic growth term (rn - μn²/log^η(n+e)), which prevents blow-up phenomena common in chemotaxis systems.

Method: Uses energy estimates and functional analysis techniques to establish a priori bounds for the solution components (n, c, u). Applies no-flux boundary conditions for n and c, Dirichlet boundary conditions for u. Leverages the modified chemotactic sensitivity and logistic damping to control solution growth.

Result: Proves existence of globally bounded classical solutions under suitable initial conditions. Shows the system possesses an absorbing set in C⁰(Ω̄) × W¹,∞(Ω) × C⁰(Ω̄; ℝ²) topology, indicating long-time behavior control.

Conclusion: The modified chemotaxis-fluid system with logistic damping admits global classical solutions that remain bounded, demonstrating that the modifications prevent finite-time blow-up while maintaining biological relevance.

Abstract: This paper studies the following chemotaxis-fluid system in a two-dimensional bounded domain $Ω$: \begin{equation*}
  \begin{cases}
  n_t + u \cdot \nabla n &= Δn - χ\nabla \cdot \left (n \frac{\nabla c}{c^k} \right ) + r n - \frac{μn^2}{\log^η(n+e)},
  c_t + u \cdot \nabla c &= Δc - αc + βn,
  u_t + u \cdot \nabla u &= Δu - \nabla P + n \nabla φ+ f,
  \nabla \cdot u &= 0,
  \end{cases} \end{equation*} where $r, μ, α, β, χ$ are positive parameters, $k, η\in (0,1)$, $φ\in W^{2,\infty}(Ω)$, and $f \in C^1\left(\barΩ\times [0, \infty)\right) \cap L^\infty\left(Ω\times (0, \infty)\right)$. We show that, under suitable conditions on the initial data and with no-flux/no-flux/Dirichlet boundary conditions, this system admits a globally bounded classical solution. Furthermore, the system possesses an absorbing set in the topology of $C^0(\barΩ) \times W^{1, \infty}(Ω) \times C^0(\barΩ; \mathbb{R}^2)$.

</details>


### [42] [On exact Observability for Compactly perturbed infinite dimension system](https://arxiv.org/abs/2512.25041)
*Nisrine Charaf,Faouzi Triki*

Main category: math.AP

TL;DR: Study of observability preservation for compactly perturbed infinite-dimensional systems with self-adjoint generators


<details>
  <summary>Details</summary>
Motivation: To understand how observability properties are affected when infinite-dimensional systems with self-adjoint generators are perturbed by compact operators

Method: Asymptotic estimation of spectral elements of perturbed unbounded operators in terms of compact perturbations, deriving sufficient conditions on perturbations

Result: Established sufficient conditions on compact self-adjoint perturbations to guarantee that perturbed systems remain exactly observable when original systems are exactly observable

Conclusion: Observability can be preserved under compact perturbations with appropriate conditions, with spectral analysis techniques providing important intermediate results

Abstract: In this paper, we study the observability of compactly perturbed infinite dimensional systems. Assuming that a given infinite-dimensional system with self-adjoint generator is exactly observable we derive sufficient conditions on a compact self adjoint perturbation to guarantee that the perturbed system stays exactly observable. The analysis is based on a careful asymptotic estimation of the spectral elements of the perturbed unbounded operator in terms of the compact perturbation. These intermediate results are of importance themselves.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [43] [Learning Density Functionals to Bridge Particle and Continuum Scales](https://arxiv.org/abs/2512.23840)
*Edoardo Monti,Peter Yatsyshin,Konstantinos Gkagkas,Andrew B. Duncan*

Main category: physics.comp-ph

TL;DR: A physics-informed learning framework that augments classical density functional theory (cDFT) with neural corrections trained against molecular dynamics data, preserving thermodynamic consistency while improving predictive accuracy for interfacial thermodynamics.


<details>
  <summary>Details</summary>
Motivation: Predicting interfacial thermodynamics across molecular and continuum scales is challenging. Classical density functional theory (cDFT) provides a first-principles approach but its accuracy depends on approximate free-energy functionals that are difficult to generalize.

Method: Introduces a physics-informed learning framework that augments cDFT with neural corrections trained directly against molecular dynamics data through adjoint optimization. Rather than replacing theory with black-box surrogates, it embeds compact neural networks within the Helmholtz free-energy functional to learn local and nonlocal corrections that preserve thermodynamic consistency.

Result: Applied to Lennard-Jones fluids, the augmented excess free-energy functional quantitatively reproduces equilibrium density profiles, coexistence curves, and surface tensions across a broad temperature range, and accurately predicts contact angles and droplet shapes far beyond the training regime.

Conclusion: This approach combines the interpretability of statistical mechanics with the adaptability of modern machine learning, establishing a general route to learned thermodynamic functionals that bridge molecular simulations and continuum-scale models.

Abstract: Predicting interfacial thermodynamics across molecular and continuum scales remains a central challenge in computational science. Classical density functional theory (cDFT) provides a first-principles route to connect microscopic interactions with macroscopic observables, but its predictive accuracy depends on approximate free-energy functionals that are difficult to generalize. Here we introduce a physics-informed learning framework that augments cDFT with neural corrections trained directly against molecular-dynamics data through adjoint optimization. Rather than replacing the theory with a black-box surrogate, we embed compact neural networks within the Helmholtz free-energy functional, learning local and nonlocal corrections that preserve thermodynamic consistency while capturing missing correlations. Applied to Lennard-Jones fluids, the resulting augmented excess free-energy functional quantitatively reproduces equilibrium density profiles, coexistence curves, and surface tensions across a broad temperature range, and accurately predicts contact angles and droplet shapes far beyond the training regime. This approach combines the interpretability of statistical mechanics with the adaptability of modern machine learning, establishing a general route to learned thermodynamic functionals that bridge molecular simulations and continuum-scale models.

</details>


### [44] [BF-APNN: A Low-Memory Method for Accelerating the Solution of Radiative Transfer Equations](https://arxiv.org/abs/2512.24534)
*Xizhe Xie,Wengu Chen,Weiming Li,Peng Song,Han Wang*

Main category: physics.comp-ph

TL;DR: BF-APNN is a neural network framework that accelerates radiative transfer equation solutions by using basis function expansion to reduce computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Radiative Transfer Equations (RTEs) are high-dimensional and multiscale, making conventional numerical methods computationally intensive. Existing deep learning methods struggle with high-dimensional or nonlinear RTEs.

Method: BF-APNN combines basis function expansion with micro-macro decomposition to reduce computational burden of high-dimensional integrals during training, inheriting advantages from RT-APNN framework.

Result: BF-APNN substantially reduces training time compared to RT-APNN while preserving high solution accuracy, and shows superior performance on complex, high-dimensional RTE problems with nonlinearity, discontinuities, and multiscale behavior.

Conclusion: BF-APNN is a robust tool for radiative transfer computations that effectively addresses computational challenges of high-dimensional, nonlinear RTEs while maintaining accuracy.

Abstract: The Radiative Transfer Equations (RTEs) exhibit high dimensionality and multiscale characteristics, rendering conventional numerical methods computationally intensive. Existing deep learning methods perform well in low-dimensional or linear RTEs, but still face many challenges with high-dimensional or nonlinear RTEs. To overcome these challenges, we propose the Basis Function Asymptotically Preserving Neural Network (BF-APNN), a framework that inherits the advantages of Radiative Transfer Asymptotically Preserving Neural Network (RT-APNN) and accelerates the solution process. By employing basis function expansion on the microscopic component, derived from micro-macro decomposition, BF-APNN effectively mitigates the computational burden associated with evaluating high-dimensional integrals during training. Numerical experiments, which involve challenging RTE scenarios featuring, nonlinearity, discontinuities, and multiscale behavior, demonstrate that BF-APNN substantially reduces training time compared to RT-APNN while preserving high solution accuracy. Moreover, BF-APNN exhibits superior performance in addressing complex, high-dimensional RTE problems, underscoring its potential as a robust tool for radiative transfer computations.

</details>


### [45] [Random Batch Sum-of-Gaussians Method for Molecular Dynamics of Born-Mayer-Huggins Systems](https://arxiv.org/abs/2512.24970)
*Chen Chen,Jiuyang Liang,Zhenli Xu,Qianru Zhang*

Main category: physics.comp-ph

TL;DR: A unified framework combining random batch sum-of-Gaussians (RBSOG) with random batch list (RBL) scheme accelerates Born-Mayer-Huggins potential simulations for ionic materials, achieving 4-10x speedup over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Large-scale molecular dynamics simulations of BMH systems for ionic materials like molten salts are computationally expensive due to Coulomb, dispersion, and short-range exponential repulsion interactions, limiting simulation scale and efficiency.

Method: Extends RBSOG method to BMH systems by combining sum-of-Gaussians decomposition (splitting potential into short- and long-range parts) with random batch list scheme for short-range acceleration, creating unified framework with importance sampling in Fourier space for long-range interactions.

Result: Achieves 4-10x speedup over Ewald-based particle-particle particle-mesh method and 2x speedup over RBSOG-only method on 1000 CPU cores for molten NaCl and mixed alkali halide systems with up to 5×10⁶ atoms, while maintaining structural/thermodynamic accuracy and reducing memory usage.

Conclusion: The combined RBSOG+RBL framework provides an efficient, scalable, and memory-friendly approach for large-scale MD simulations with BMH potentials, demonstrating attractive performance for long-range interaction systems.

Abstract: The Born-Mayer-Huggins (BMH) potential, which combines Coulomb interactions with dispersion and short-range exponential repulsion, is widely used for ionic materials such as molten salts. However, large-scale molecular dynamics simulations of BMH systems are often limited by computation, communication, and memory costs. We recently proposed the random batch sum-of-Gaussians (RBSOG) method, which accelerates Coulomb calculations by using a sum-of-Gaussians (SOG) decomposition to split the potential into short- and long-range parts and by applying importance sampling in Fourier space for the long-range part. In this work, we extend the RBSOG to BMH systems and incorporate a random batch list (RBL) scheme to further accelerate the short-range part, yielding a unified framework for efficient simulations with the BMH potential. The combination of the SOG decomposition and the RBL enables an efficient and scalable treatment of both long- and short-range interactions in BMH system, particularly the RBL well handles the medium-range exponential repulsion and dispersion by the random batch neighbor list. Error estimate is provided to show the theoretical convergence of the RBL force. We evaluate the framework on molten NaCl and mixed alkali halide with up to $5\times10^6$ atoms on $2048$ CPU cores. Compared to the Ewald-based particle-particle particle-mesh method and the RBSOG-only method, our method achieves approximately $4\sim10\times$ and $2\times$ speedups while using $1000$ cores, respectively, under the same level of structural and thermodynamic accuracy and with a reduced memory usage. These results demonstrate the attractive performance of our method in accuracy and scalability for MD simulations with long-range interactions.

</details>


### [46] [Fast Poisson brackets and constraint algebras in canonical gravity](https://arxiv.org/abs/2512.25007)
*Will Barker*

Main category: physics.comp-ph

TL;DR: A computer algebra package for efficiently computing Poisson brackets and constraint algebras in gravity theories, tested on GR and modified gravity.


<details>
  <summary>Details</summary>
Motivation: Dirac's Hamiltonian constraint algorithm is crucial for analyzing gravity theories but is notoriously arduous to implement manually, requiring computational tools to simplify the process.

Method: Developed a computer algebra package that automates the computation of Poisson brackets and reconstruction of constraint algebras for gravity theories.

Result: Successfully stress-tested the package against pure general relativity and modified gravity, including order reduction of general relativity at two loops.

Conclusion: The package provides an efficient computational tool that simplifies the implementation of Dirac's Hamiltonian constraint algorithm for gravity theories, facilitating analysis of propagating modes, gauge symmetries, and pathologies.

Abstract: In the study of alternative or extended theories of gravity, Dirac's Hamiltonian constraint algorithm is invaluable for enumerating the propagating modes and gauge symmetries. For gravity, this canonical approach is frequently applied as a means for finding pathologies such as strongly coupled modes; more generally it facilitates the reconstruction of gauge symmetries and the quantization of gauge theories. For gravity, however, the algorithm can become notoriously arduous to implement. We present a simple computer algebra package for efficiently computing Poisson brackets and reconstructing constraint algebras. The tools are stress-tested against pure general relativity and modified gravity, including the order reduction of general relativity at two loops.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [47] [Autoregressive long-horizon prediction of plasma edge dynamics](https://arxiv.org/abs/2512.23884)
*Hunor Csala,Sebastian De Pascuale,Paul Laiu,Jeremy Lore,Jae-Sun Park,Pei Zhang*

Main category: physics.plasm-ph

TL;DR: Transformer-based autoregressive surrogates for fast prediction of 2D time-dependent plasma edge states, trained on SOLPS-ITER data, enabling orders-of-magnitude faster simulations for fusion device design.


<details>
  <summary>Details</summary>
Motivation: High-fidelity edge fluid/neutral codes like SOLPS-ITER are computationally expensive, limiting parameter scans and long transient studies needed for designing plasma-facing components in fusion devices.

Method: Transformer-based autoregressive surrogates trained on SOLPS-ITER spatiotemporal data to forecast electron temperature, electron density, and radiated power. Models evaluated with increasing autoregressive horizons (1-100 steps) for short- and long-horizon prediction tasks.

Result: Longer-horizon training improves rollout stability and reduces error accumulation, enabling stable predictions over hundreds to thousands of steps. Surrogates are orders of magnitude faster than SOLPS-ITER while reproducing key dynamical features like motion of high-radiation regions.

Conclusion: Transformer surrogates provide fast, accurate alternatives to computationally intensive plasma edge simulations, supporting rapid scenario exploration and control-oriented studies. Future work needed for data enrichment and physics-informed constraints to handle unseen physical regimes.

Abstract: Accurate modeling of scrape-off layer (SOL) and divertor-edge dynamics is vital for designing plasma-facing components in fusion devices. High-fidelity edge fluid/neutral codes such as SOLPS-ITER capture SOL physics with high accuracy, but their computational cost limits broad parameter scans and long transient studies. We present transformer-based, autoregressive surrogates for efficient prediction of 2D, time-dependent plasma edge state fields. Trained on SOLPS-ITER spatiotemporal data, the surrogates forecast electron temperature, electron density, and radiated power over extended horizons. We evaluate model variants trained with increasing autoregressive horizons (1-100 steps) on short- and long-horizon prediction tasks. Longer-horizon training systematically improves rollout stability and mitigates error accumulation, enabling stable predictions over hundreds to thousands of steps and reproducing key dynamical features such as the motion of high-radiation regions. Measured end-to-end wall-clock times show the surrogate is orders of magnitude faster than SOLPS-ITER, enabling rapid parameter exploration. Prediction accuracy degrades when the surrogate enters physical regimes not represented in the training dataset, motivating future work on data enrichment and physics-informed constraints. Overall, this approach provides a fast, accurate surrogate for computationally intensive plasma edge simulations, supporting rapid scenario exploration, control-oriented studies, and progress toward real-time applications in fusion devices.

</details>


### [48] [The role of particle feedback on particle acceleration in magnetic reconnection](https://arxiv.org/abs/2512.24054)
*Shimin Liang,Nianyu Yi*

Main category: physics.plasm-ph

TL;DR: Particle feedback in magnetic reconnection amplifies shear flows, strengthening convective electric fields and boosting particle acceleration, resulting in higher maximum energies and harder spectra, while guide fields suppress these effects.


<details>
  <summary>Details</summary>
Motivation: To understand how particle feedback affects magnetic reconnection dynamics and particle acceleration in astrophysical plasmas, particularly the interplay between feedback, guide fields, and reconnection processes.

Method: 2.5D magnetohydrodynamic (MHD) simulations with a co-evolving fluid-particle framework to investigate particle feedback effects on reconnection and acceleration.

Result: Particle feedback amplifies shear flows within magnetic islands, strengthening convective electric fields and boosting particle acceleration, leading to higher maximum particle energies and harder non-thermal energy spectra. Guide fields suppress both gas internal energy increase and particle acceleration.

Conclusion: The study reveals complex interplay between particle feedback, guide fields, and reconnection dynamics, showing that feedback enhances acceleration through shear flow amplification while guide fields suppress these effects.

Abstract: Magnetic reconnection is a ubiquitous process in astrophysical plasmas and an efficient mechanism for particle acceleration. Using 2.5D magnetohydrodynamic (MHD) simulations with a co-evolving fluid-particle framework, we investigate how particle feedback affects reconnection and acceleration. Our simulations demonstrate that particle feedback to the fluid amplifies shear flows within magnetic islands, which strengthens the convective electric field and thereby boosts particle acceleration. This mechanism results in a higher maximum particle energy and a harder non-thermal energy spectrum. The guide field suppresses both the increase in gas internal energy and particle acceleration. These findings highlight the complex interplay between feedback, guide fields, and reconnection dynamics.

</details>


### [49] [Coordinates based on a magnetic mirror field](https://arxiv.org/abs/2512.24305)
*R. D. Hazeltine*

Main category: physics.plasm-ph

TL;DR: Constructing a coordinate system tailored to cylindrically symmetric magnetic field geometry


<details>
  <summary>Details</summary>
Motivation: Standard coordinate systems may not optimally represent the geometry of cylindrically symmetric magnetic fields, making analysis and calculations inefficient

Method: Develop a specialized coordinate system that aligns with the symmetry and structure of cylindrically symmetric magnetic fields

Result: A new coordinate system that naturally fits the magnetic field geometry, potentially simplifying field analysis and calculations

Conclusion: Tailored coordinate systems can provide more efficient representations for analyzing specific magnetic field geometries

Abstract: We construct a coordinate system fitting the geometry of a given, cylindrically symmetric, magnetic field.

</details>


### [50] [Computing Flux-Surface Shapes in Tokamaks and Stellarators](https://arxiv.org/abs/2512.24544)
*M. J. Gerard,M. J. Pueschel,S. Stewart,H. O. M. Hillebrecht,B. Geiger*

Main category: physics.plasm-ph

TL;DR: The paper introduces a general Fourier-based method to characterize stellarator magnetic field geometry, revealing that quasi-symmetry emerges from a spatial resonance between shape complexity and shape rotation about the magnetic axis.


<details>
  <summary>Details</summary>
Motivation: There is currently no agreed-upon methodology for characterizing stellarator magnetic field geometry, despite modern stellarator designs achieving high levels of magnetic-field quasi-symmetry through careful flux-surface shaping. This lack of standardized characterization methods hinders systematic investigations of flux-surface geometries.

Method: The authors introduce a general Fourier mode analysis method to define shaping modes (elongation, triangularity, squareness, etc.) of cross-sections that can be non-planar. The framework works for both axisymmetric and non-axisymmetric configurations, with the additional degree of freedom in non-axisymmetric equilibria manifesting as rotation of each shaping mode about the magnetic axis.

Result: Analysis of non-axisymmetric configurations with precise quasi-symmetry and cases from the QUASR database shows that quasi-symmetry results from a spatial resonance between shape complexity and shape rotation about the magnetic axis. The quantitative features of this resonance correlate closely with a configuration's rotational transform and number of field periods.

Conclusion: The proposed shaping paradigm can facilitate systematic investigations into the relationship between general flux-surface geometries and other figures of merit, providing a standardized methodology for characterizing stellarator magnetic field geometry.

Abstract: There is currently no agreed-upon methodology for characterizing a stellarator magnetic field geometry, and yet modern stellarator designs routinely attain high levels of magnetic-field quasi-symmetry through careful flux-surface shaping. Here, we introduce a general method for computing the shape of an ideal-MHD equilibrium that can be used in both axisymmetric and non-axisymmetric configurations. This framework uses a Fourier mode analysis to define the shaping modes (e.g. elongation, triangularity, squareness, etc.) of cross-sections that can be non-planar. Relative to an axisymmetric equilibrium, the additional degree of freedom in a non-axisymmetric equilibrium manifests as a rotation of each shaping mode about the magnetic axis. Using this method, a shaping analysis is performed on non-axisymmetric configurations with precise quasi-symmetry and select cases from the QUASR database spanning a range of quasi-symmetry quality. Empirically, we find that quasi-symmetry results from a spatial resonance between shape complexity and shape rotation about the magnetic axis. The quantitative features of this resonance correlate closely with a configuration's rotational transform and number of field periods. Based on these observations, it is conjectured that this shaping paradigm can facilitate systematic investigations into the relationship between general flux-surface geometries and other figures of merit.

</details>


### [51] [Cataloging the nonlinear waves excited by moving a charged body in the dusty plasma medium](https://arxiv.org/abs/2512.24723)
*Swathi S Krishna,S. K. Mishra,S. Jaiswal*

Main category: physics.plasm-ph

TL;DR: Study examines nonlinear waves generated by charged body moving in dusty plasma, described by forced KdV equation, showing source parameters (amplitude, width, speed) crucially shape wave evolution beyond just Mach number.


<details>
  <summary>Details</summary>
Motivation: To understand how charged bodies moving through dusty plasma generate diverse nonlinear waves (precursors, pinned solitons) and investigate the role of source parameters in shaping these wave excitations.

Method: Theoretical analysis using forced Korteweg-de Vries (fKdV) equation under weakly nonlinear and dispersive limits, examining effects of three source parameters: amplitude, width, and flow speed.

Result: Nonlinear wave excitation depends not just on Mach number but also on source amplitude and width. Discovery of novel lagging nonlinear structures that maintain shape and speed while propagating behind the source.

Conclusion: Moving charged bodies in dusty plasma generate complex nonlinear structures influenced by multiple source parameters, with first theoretical depiction of lagging structures that propagate behind the source.

Abstract: The nonlinear waves excited by the movement of a charged body in the dusty plasma medium are studied. A charged body moving through a dusty plasma medium can generate diverse nonlinear waves, such as precursors and pinned solitons. These wave excitations under weakly nonlinear and dispersive limits are described theoretically by the forced Korteweg-de Vries (fKdV) type equation. We have examined the role of the driver in shaping and evolving these wave excitations. In particular we studied the effect of primarily three source parameters, namely, amplitude, width, and flow speed, on the evolution of nonlinear structures. The driver generates a perturbation in the stable system configuration, which couples with medium characteristics and eventually evolves into propagating excitations. Our finding shows that the excitation of nonlinear structure by a moving body in a plasma medium is not just dictated by the mach number but also the features of the source such as amplitude and width. As a novel finding apart from pinned and precursor solitons, we observe another nonlinear structure that lags behind the source term, maintaining its shape and speed as it propagates. These features are the first ever theoretical depiction of such lagging structures.

</details>


### [52] [Runaway electron avalanche and macroscopic beam formation: simulations of the DTT full power scenario](https://arxiv.org/abs/2512.24760)
*E. Emanuelli,F. Vannini,M. Hoelzl,E. Nardon,V. Bandaru,N. Schwarz,D. Bonfiglio,G. Ramogida,F. Subba,JOREK Team*

Main category: physics.plasm-ph

TL;DR: DTT's transition to full power (5.5 MA) dramatically increases runaway electron avalanche risk compared to initial commissioning (2 MA), requiring careful impurity injection balancing between thermal load mitigation and RE avoidance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical safety concern of runaway electron (RE) generation during disruptions in the DTT facility's transition from initial commissioning (2 MA) to full power operation (5.5 MA). Previous studies showed safety margins at low current, but the exponential scaling of RE avalanche gain with plasma current creates new risks that must be understood for safe operation.

Method: Used the non-linear magnetohydrodynamic code JOREK to perform comprehensive 2D simulations of the current quench phase during disruptions. Systematically scanned initial RE seed currents and injected impurity levels across different disruption scenarios to analyze RE avalanche behavior.

Result: In the full power scenario (5.5 MA), avalanche multiplication factor reaches ~1.3×10⁵, allowing tiny 5.5 A seed currents to grow into 0.7 MA RE beams with high impurity levels. Higher RE seeds can produce RE currents up to 3.2 MA (≈80% of total plasma current). This contrasts sharply with the safer Day-0 scenario where RE formation was unlikely.

Conclusion: DTT's full power operation requires a fundamentally different disruption mitigation strategy that carefully balances thermal load reduction with RE avoidance through precisely controlled impurity injection. This work establishes the baseline for future RE load estimations on plasma-facing components and informs the design/placement of mitigation systems like sacrificial limiters.

Abstract: The transition of the Divertor Tokamak Test (DTT) facility from its initial commissioning phase (Day-0, plasma current $I_{p}=2$ MA) to the full power scenario ($I_{p}=5.5$ MA) introduces a critical shift in the dynamics of runaway electrons (REs) generation. While previous predictive studies of the low-current scenario indicated a robust safety margin against RE beam formation, this work reveals that the exponential scaling of the RE avalanche gain with plasma current severely narrows the safe operational window in the full power scenario. Using the non-linear magnetohydrodynamic code JOREK, we perform comprehensive 2D simulations of the current quench (CQ) phase of several disruption scenarios, systematically scanning initial RE seed currents and injected impurity levels. The results demonstrate that in the full power scenario, the avalanche multiplication factor is sufficiently high ($G_\text{av} \approx 1.3 \cdot 10^5$) to convert a mere 5.5 A seed current into macroscopic RE beams of $\approx 0.7$ MA when large amounts of impurities are present. For even higher RE seeds, the RE current can peak at $ \approx 3.2$ MA, constituting up to $\approx$ 80% of the total plasma current during the CQ. These findings suggest that, unlike the Day-0 phase, the disruption mitigation strategy for the full power scenario involves a careful balance between thermal load mitigation and RE avoidance, necessitating a well-chosen quantity of injected impurities. This work provides the baseline needed for future estimations of RE loads on the plasma-facing components of DTT, which will be essential for designing and positioning mitigation components like sacrificial limiters.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [53] [Towards mechanistic understanding in a data-driven weather model: internal activations reveal interpretable physical features](https://arxiv.org/abs/2512.24440)
*Theodore MacMillan,Nicholas T. Ouellette*

Main category: physics.ao-ph

TL;DR: Researchers adapt LLM interpretability tools to analyze GraphCast, discovering interpretable weather features like tropical cyclones and atmospheric rivers, and demonstrate feature manipulation for physically consistent hurricane predictions.


<details>
  <summary>Details</summary>
Motivation: While data-driven physics models like GraphCast show impressive predictive accuracy, their internal workings remain black boxes. The paper aims to understand whether these models' internal representations are interpretable and physically consistent, addressing the lack of transparency in large data-driven weather models.

Method: Adapt interpretability tools from Large Language Models to GraphCast, specifically using sparse autoencoders to discover interpretable features in the model's neuron space. Then perform interventions by sparsely modifying identified features during prediction steps to probe their abstract representations.

Result: Discovered distinct interpretable features corresponding to real-world weather phenomena: tropical cyclones, atmospheric rivers, diurnal/seasonal behavior, large-scale precipitation patterns, geographical coding, and sea-ice extent. Successfully demonstrated feature manipulation by modifying tropical cyclone features to produce interpretable and physically consistent hurricane predictions.

Conclusion: Data-driven physics models like GraphCast contain interpretable, physically consistent internal representations. The adapted interpretability methods provide a window into black-box behavior, moving toward trustworthy predictors and scientifically valuable discovery tools.

Abstract: Large data-driven physics models like DeepMind's weather model GraphCast have empirically succeeded in parameterizing time operators for complex dynamical systems with an accuracy reaching or in some cases exceeding that of traditional physics-based solvers. Unfortunately, how these data-driven models perform computations is largely unknown and whether their internal representations are interpretable or physically consistent is an open question. Here, we adapt tools from interpretability research in Large Language Models to analyze intermediate computational layers in GraphCast, leveraging sparse autoencoders to discover interpretable features in the neuron space of the model. We uncover distinct features on a wide range of length and time scales that correspond to tropical cyclones, atmospheric rivers, diurnal and seasonal behavior, large-scale precipitation patterns, specific geographical coding, and sea-ice extent, among others. We further demonstrate how the precise abstraction of these features can be probed via interventions on the prediction steps of the model. As a case study, we sparsely modify a feature corresponding to tropical cyclones in GraphCast and observe interpretable and physically consistent modifications to evolving hurricanes. Such methods offer a window into the black-box behavior of data-driven physics models and are a step towards realizing their potential as trustworthy predictors and scientifically valuable tools for discovery.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [54] [Achieving High Efficiency And Enhanced Beam Quality In Laser Wakefield Acceleration](https://arxiv.org/abs/2512.24719)
*Jia Wang,Ming Zeng,Dazhang Li,Wentao Wang,Song Li,Ke Feng,Jie Gao*

Main category: physics.acc-ph

TL;DR: Shorter laser pulses enable two-step dechirping for high-charge electron beams with 1% energy spread and 10-30% energy transfer efficiency in laser wakefield acceleration.


<details>
  <summary>Details</summary>
Motivation: Laser wakefield acceleration offers compact, cost-effective particle acceleration with extremely high gradients (>100GV/m), but faces challenges in improving energy transfer efficiency while maintaining beam quality suitable for practical applications like particle colliders and light sources.

Method: Using shorter laser pulse duration to enable a two-step dechirping process for accelerated electron beams with nanocoulomb-level charge. The approach works across a large parameter space.

Result: Achieved electron beams with 1% energy spread and 10-30% energy transfer efficiency. Demonstrated example: 420MeV electron beam with 5.5nC charge and 2% RMS energy spread using 8.3J, 7.2fs laser pulse.

Conclusion: Shorter laser pulses facilitate effective two-step dechirping, enabling high-quality electron beams with improved energy transfer efficiency, advancing laser wakefield acceleration toward practical applications.

Abstract: Laser wakefield acceleration, characterized by the extremely high electric field gradient exceeding 100GV/m, is regarded as a compact and cost affordable technology for the next generation of particle colliders and light sources. However, it has always been a major challenge to effectively increase the energy transfer efficiency from the laser to the accelerated beam, while ensuring the beam quality remains suitable for practical applications. This study demonstrates that the laser with shorter pulse duration allows for a two-step dechirping process of the accelerated electron beam with charge of nanocoulomb level. The electron beams with an energy spread of 1% can be generated with the energy transfer efficiency of 10% to 30% in a large parameter space. For example, one electron beam with the energy of 420MeV, the charge of 5.5nC and the RMS energy spread of 2% can be produced using an 8.3J laser pulse with 7.2fs duration.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [55] [Basic Inequalities for First-Order Optimization with Applications to Statistical Risk Analysis](https://arxiv.org/abs/2512.24999)
*Seunghoon Paik,Kangjie Zhou,Matus Telgarsky,Ryan J. Tibshirani*

Main category: math.ST

TL;DR: The paper introduces "basic inequalities" as a unified framework connecting implicit and explicit regularization in first-order optimization algorithms, providing a tool for statistical analysis of training dynamics and prediction risk.


<details>
  <summary>Details</summary>
Motivation: To develop a simple and versatile framework that connects implicit and explicit regularization in first-order optimization algorithms, providing a unified tool for statistical analysis that can handle various algorithms beyond just gradient descent.

Method: The authors introduce "basic inequalities" - a specific form of inequality that bounds the objective function difference f(θ_T)-f(z) for any reference point z in terms of accumulated step sizes and distances between initial point θ_0, current iterate θ_T, and z. This framework translates iteration count into an effective regularization coefficient.

Result: The framework provides new results for mirror descent with Bregman divergence projection, generalized linear models trained by gradient descent and exponentiated gradient descent, and randomized predictors, while also refining known results on gradient descent. Experimental validation on generalized linear models supplements the theoretical findings.

Conclusion: The basic inequalities framework offers a well-rounded tool for statistical analysis of optimization algorithms, connecting implicit and explicit regularization, and demonstrating versatility across various first-order methods including gradient descent, mirror descent, and specialized algorithms for generalized linear models.

Abstract: We introduce \textit{basic inequalities} for first-order iterative optimization algorithms, forming a simple and versatile framework that connects implicit and explicit regularization. While related inequalities appear in the literature, we isolate and highlight a specific form and develop it as a well-rounded tool for statistical analysis. Let $f$ denote the objective function to be optimized. Given a first-order iterative algorithm initialized at $θ_0$ with current iterate $θ_T$, the basic inequality upper bounds $f(θ_T)-f(z)$ for any reference point $z$ in terms of the accumulated step sizes and the distances between $θ_0$, $θ_T$, and $z$. The bound translates the number of iterations into an effective regularization coefficient in the loss function. We demonstrate this framework through analyses of training dynamics and prediction risk bounds. In addition to revisiting and refining known results on gradient descent, we provide new results for mirror descent with Bregman divergence projection, for generalized linear models trained by gradient descent and exponentiated gradient descent, and for randomized predictors. We illustrate and supplement these theoretical findings with experiments on generalized linear models.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [56] [Polynomial mixing for the stochastic Schrödinger equation with large damping in the whole space](https://arxiv.org/abs/2512.24599)
*Hung D. Nguyen,Kihoon Seong*

Main category: math.PR

TL;DR: The paper establishes polynomial mixing rates for the stochastic nonlinear Schrödinger equation with large damping in dimensions d≤3.


<details>
  <summary>Details</summary>
Motivation: While unique ergodicity is known for the damped stochastic nonlinear Schrödinger equation, the rate of convergence to equilibrium has remained unknown. The authors aim to quantify this mixing behavior.

Method: The approach uses a coupling strategy combined with pathwise Strichartz estimates to analyze the long-time behavior in the regime of large damping.

Result: Solutions are attracted toward the unique invariant probability measure at polynomial rates of arbitrary order when damping is sufficiently strong.

Conclusion: The work provides quantitative mixing rates for the stochastic nonlinear Schrödinger equation with large damping, resolving a previously open problem about convergence speed to equilibrium.

Abstract: We study the long-time mixing behavior of the stochastic nonlinear Schrödinger equation in $\mathbb{R}^d$, $d\le 3$. It is well known that, under a sufficiently strong damping force, the system admits unique ergodicity, although the rate of convergence toward equilibrium has remained unknown. In this work, we address the mixing property in the regime of large damping and establish that solutions are attracted toward the unique invariant probability measure at polynomial rates of arbitrary order. Our approach is based on a coupling strategy with pathwise Strichartz estimates.

</details>


### [57] [Heat kernel estimates for Markov processes with jump kernels blowing-up at the boundary](https://arxiv.org/abs/2512.24807)
*Soobin Cho,Panki Kim,Renming Song,Zoran Vondraček*

Main category: math.PR

TL;DR: The paper establishes sharp two-sided heat kernel estimates for symmetric Markov jump processes with boundary-blowing jump kernels on closed subsets of ℝ^d.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend heat kernel analysis to Markov processes with jump kernels that blow up at the boundary, which occurs in various geometric settings including traces of stable processes, nonlocal Neumann problems, and resurrected self-similar processes.

Method: The authors employ recently developed weighted functional inequalities specifically designed for jump kernels that blow up at the boundary, overcoming the limitation that standard techniques require uniformly bounded jump measure tails.

Result: The main result is establishing sharp two-sided heat kernel estimates for symmetric Markov processes with jump kernels of the form J(x,y)=|x-y|^{-d-α}ℬ(x,y) where ℬ(x,y) may blow up at the boundary.

Conclusion: The paper successfully extends heat kernel analysis to a broader class of Markov processes with boundary-blowing jump kernels, providing a framework that includes important examples from probability theory and analysis.

Abstract: In this paper, we study purely discontinuous symmetric Markov processes on closed subsets of ${\mathbb R}^d$, $d\ge 1$, with jump kernels of the form $J(x,y)=|x-y|^{-d-α}{\mathcal B}(x,y)$, $α\in (0,2)$, where the function ${\mathcal B}(x,y)$ may blow up at the boundary of the state space. This extends the framework developed recently for conservative self-similar Markov processes on the upper half-space to a broader geometric setting. Examples of Markov processes that fall into our general framework include traces of isotropic $α$-stable processes in $C^{1,\rm Dini}$ sets, processes in Lipschitz sets arising in connection with the nonlocal Neumann problem, and a large class of resurrected self-similar processes in the closed upper half-space.
  We establish sharp two-sided heat kernel estimates for these Markov processes. A fundamental difficulty in accomplishing this task is that, in contrast to the existing literature on heat kernels for jump processes, the tails of the associated jump measures in our setting are not uniformly bounded. Thus, standard techniques in the existing literature used to study heat kernels are not applicable. To overcome this obstacle, we employ recently developed weighted functional inequalities specifically designed for jump kernels blowing up at the boundary.

</details>


### [58] [Uniqueness for stochastic differential equations in Hilbert spaces with irregular drift](https://arxiv.org/abs/2512.25003)
*Lukas Anzeletti,Oleg Butkovsky,Máté Gerencsér,Alexander Shaposhnikov*

Main category: math.PR

TL;DR: The paper presents a framework for proving strong existence and uniqueness of SDEs in Hilbert spaces with irregular drift, extending previous work by removing structural assumptions on the drift function.


<details>
  <summary>Details</summary>
Motivation: To study strong existence and uniqueness for stochastic differential equations in Hilbert spaces with irregular drift, extending beyond the structural assumptions required in previous seminal work by Da Prato and Flandoli (2010).

Method: Develops a new technique combining Lê's theory of stochastic sewing in Hilbert spaces, Gaussian analysis, and a method of Lasry and Lions for approximation in Hilbert spaces, avoiding the use of infinite-dimensional Kolmogorov equations.

Result: Proves that the SDE has a unique strong solution provided that α > 2γ/(1+γ), where α is the Hölder continuity exponent of the drift function b and γ is a parameter related to the stochastic convolution.

Conclusion: The framework substantially extends previous results by removing structural assumptions on the drift function, providing a more general approach to studying SDEs with irregular drift in Hilbert spaces.

Abstract: We present a versatile framework to study strong existence and uniqueness for stochastic differential equations (SDEs) in Hilbert spaces with irregular drift. We consider an SDE in a separable Hilbert space $H$ \begin{equation*} dX_t= (A X_t + b(X_t))dt +(-A)^{-γ/2}dW_t,\quad X_0=x_0 \in H, \end{equation*} where $A$ is a self-adjoint negative definite operator with purely atomic spectrum, $W$ is a cylindrical Wiener process, $b$ is $α$-Hölder continuous function $H\to H$, and a nonnegative parameter $γ$ such that the stochastic convolution takes values in $H$. We show that this equation has a unique strong solution provided that $α> 2γ/(1+γ)$. This substantially extends the seminal work of Da Prato and Flandoli (2010) as no structural assumption on $b$ is imposed. To obtain this result, we do not use infinite-dimensional Kolmogorov equations but instead develop a new technique combining Lê's theory of stochastic sewing in Hilbert spaces, Gaussian analysis, and a method of Lasry and Lions for approximation in Hilbert spaces.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [59] [Mathematical Theory for Photonic Hall Effect in Honeycomb Photonic Crystals](https://arxiv.org/abs/2512.24477)
*Wei Li,Junshan Lin,Jiayu Qiu,Hai Zhang*

Main category: physics.optics

TL;DR: The paper develops a mathematical theory for the photonic Hall effect, proving existence of guided electromagnetic waves at interfaces between honeycomb photonic crystals, analogous to electronic edge states.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous mathematical foundation for the photonic Hall effect and demonstrate the existence of topological edge states in photonic systems, similar to those in electronic topological insulators.

Method: Start with symmetric honeycomb photonic crystals with Dirac points at K and K' points. Introduce two classes of perturbations to lift Dirac degeneracy and create spectral band valleys with well-defined topological phases. Use layer potential techniques and spectral analysis to study guided waves at interfaces between perturbed crystals.

Result: Proves existence of guided electromagnetic waves propagating along interfaces between honeycomb photonic crystals, induced by topological Hall effect. Shows relationship between interface mode existence and nature of perturbations on the two periodic media.

Conclusion: Provides rigorous mathematical framework for photonic Hall effect, demonstrating topological edge states in photonic systems analogous to electronic systems, with interface modes dependent on perturbation characteristics.

Abstract: In this work, we develop a mathematical theory for the photonic Hall effect and prove the existence of guided electromagnetic waves at the interface of two honeycomb photonic crystals. The guided wave resembles the edge states in electronic systems: it is induced by the topological Hall effect, and the wave propagates along the interface but not in the bulk media. Starting from a symmetric honeycomb photonic crystal that attains Dirac points at the high-symmetry points of the Brillouin zone, $K$ and $K'$, we introduce two classes of perturbations for the periodic medium. The perturbations lift the Dirac degeneracy, forming a spectral band valley at the points $K$ and $K'$ with well-defined topological phase that depends on the sign of the perturbation parameters. By employing the layer potential techniques and spectral analysis, we investigate the existence of guided wave along an interface when two honeycomb photonic crystals are glued together. In particular, we elucidate the relationship between the existence of the interface mode and the nature of perturbations imposed on the two periodic media separated by the interface.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [60] [Classification of ancient cylindrical mean curvature flows and the Mean Convex Neighborhood Conjecture](https://arxiv.org/abs/2512.24524)
*Richard H. Bamler,Yi Lai*

Main category: math.DG

TL;DR: The paper resolves the Mean Convex Neighborhood Conjecture for mean curvature flows, proving that near cylindrical singularities the flow is mean-convex and can be characterized by local models, with applications to classifying ancient asymptotically cylindrical flows.


<details>
  <summary>Details</summary>
Motivation: To prove the Mean Convex Neighborhood Conjecture for mean curvature flows in all dimensions, which concerns the local behavior near cylindrical singularities and provides a structural understanding of the flow near such points.

Method: Complete classification of ancient, asymptotically cylindrical flows; refined asymptotic analysis with a novel "leading mode condition"; "induction over thresholds" argument; independent approach from prior work.

Result: Resolved the conjecture: near multiplicity-one cylindrical singularities, the flow is mean-convex, time-slices are level sets of continuous functions, all nearby tangent flows are cylindrical; established canonical neighborhood theorem; classified ancient asymptotically cylindrical flows as non-collapsed, convex, rotationally symmetric belonging to three canonical families.

Conclusion: The work provides a complete resolution of the Mean Convex Neighborhood Conjecture, offers new tools for analyzing mean curvature flows near singularities, and gives a full parameterization of asymptotically cylindrical flows with applications to soliton existence proofs.

Abstract: We resolve the Mean Convex Neighborhood Conjecture for mean curvature flows in all dimensions and for all types of cylindrical singularities. Specifically, we show that if the tangent flow at a singular point is a multiplicity-one cylinder, then in a neighborhood of that point the flow is mean-convex, its time-slices arise as level sets of a continuous function, and all nearby tangent flows are cylindrical. Moreover, we establish a canonical neighborhood theorem near such points, which characterizes the flow via local models. We also obtain a more uniform version of the Mean Convex Neighborhood Conjecture, which only requires closeness to a cylinder at some initial time and yields a quantitative version of this structural description.
  Our proof relies on a complete classification of ancient, asymptotically cylindrical flows. We prove that any such flow is non-collapsed, convex, rotationally symmetric, and belongs to one of three canonical families: ancient ovals, the bowl soliton, or the flying wing translating solitons. Central to our method is a refined asymptotic analysis and a novel \emph{leading mode condition,} together with a new ``induction over thresholds'' argument. In addition, our approach provides a full parameterization of the space of asymptotically cylindrical flows and gives a new proof of the existence of flying wing solitons.
  Our method is independent of prior work and, together with our prequel paper, this work is largely self-contained.

</details>


### [61] [Isocapacitary constants for the $p$-Laplacian on compact manifolds](https://arxiv.org/abs/2512.24725)
*Lili Wang,Tao Wang*

Main category: math.DG

TL;DR: The paper introduces Steklov and Neumann isocapacitary constants for p-Laplacian on compact manifolds, which provide two-sided bounds for Sobolev constants and degenerate to bounds for eigenvalues when α=1.


<details>
  <summary>Details</summary>
Motivation: To develop new isocapacitary constants that can provide effective bounds for Sobolev constants and eigenvalues of the p-Laplacian on compact manifolds, extending classical results to the nonlinear setting.

Method: Introduce Steklov and Neumann isocapacitary constants specifically designed for the p-Laplacian operator on compact manifolds, and analyze their relationship with (p,α)-Sobolev constants.

Result: The new constants provide two-sided bounds for (p,α)-Sobolev constants, and when α=1, these bounds degenerate to upper and lower bounds for the first nontrivial Steklov and Neumann eigenvalues of the p-Laplacian.

Conclusion: The introduced isocapacitary constants serve as effective tools for bounding Sobolev constants and eigenvalues in the nonlinear p-Laplacian setting on compact manifolds, generalizing classical eigenvalue bounds to the nonlinear case.

Abstract: In this paper, we introduce Steklov and Neumann isocapacitary constants for the $p$-Laplacian on compact manifolds. These constants yield two-sided bounds for the $(p,α)$-Sobolev constants, which degenerate to upper and lower bounds for the first nontrivial Steklov and Neumann eigenvalues of the $p$-Laplacian when $α= 1$.

</details>


### [62] [A Liouville-Weierstrass correspondence for Spacelike and Timelike Minimal Surfaces in $\mathbb{L}^3$](https://arxiv.org/abs/2512.24908)
*Adriana A. Cintra,Iury Domingos,Irene I. Onnis*

Main category: math.DG

TL;DR: The paper establishes a correspondence between solutions of the Liouville equation and Weierstrass representations of spacelike/timelike minimal surfaces in Lorentz-Minkowski space, using complex/paracomplex analysis to unify both causal types.


<details>
  <summary>Details</summary>
Motivation: To investigate the relationship between solutions of the Liouville equation and minimal surfaces in Lorentz-Minkowski space, providing a unified framework for both spacelike and timelike surfaces through complex and paracomplex analysis.

Method: Using complex analysis for spacelike surfaces and paracomplex analysis for timelike surfaces, studying the action of pseudo-isometries via Möbius-type transformations, establishing correspondences with Lorentz group rotations, and deriving Gauss maps and Weierstrass data from Liouville equation solutions.

Result: Established a unified correspondence between Liouville equation solutions and Weierstrass representations of minimal surfaces in L^3, showed how pseudo-isometries act via Möbius transformations corresponding to Lorentz group rotations, and provided explicit examples of both spacelike and timelike minimal surfaces from Liouville solutions.

Conclusion: The Liouville equation provides a powerful framework for constructing and analyzing both spacelike and timelike minimal surfaces in Lorentz-Minkowski space, with complex/paracomplex analysis offering a unified treatment and Möbius transformations revealing symmetry structures.

Abstract: We investigate a correspondence between solutions $λ(x,y)$ of the Liouville equation \[ Δλ= -\varepsilon e^{-4λ}, \] and the Weierstrass representations of spacelike ($\varepsilon = 1$) and timelike ($\varepsilon = -1$) minimal surfaces with diagonalizable Weingarten map in the three-dimensional Lorentz--Minkowski space $\mathbb{L}^3$. Using complex and paracomplex analysis, we provide a unified treatment of both causal types. We study the action of pseudo-isometries of $\mathbb{L}^3$ on minimal surfaces via Möbius-type transformations, establishing a correspondence between these transformations and rotations in the special orthochronous Lorentz group. Furthermore, we show how local solutions of the Liouville equation determine the Gauss map and the associated Weierstrass data. Finally, we present explicit examples of spacelike and timelike minimal surfaces in $\mathbb{L}^3$ arising from solutions of the Liouville equation.

</details>


### [63] [The PDE-ODI principle and cylindrical mean curvature flows](https://arxiv.org/abs/2512.25050)
*Richard H. Bamler,Yi Lai*

Main category: math.DG

TL;DR: New PDE-ODI principle converts parabolic PDEs to ODE inequalities, enabling high-order asymptotic expansions for mean curvature flow singularities without delicate analytic estimates.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient framework for analyzing ancient solutions and singularities of mean curvature flow, particularly those modeled on cylinders, bypassing complex analytic estimates used in previous work.

Method: Introduces the PDE-ODI principle that converts parabolic differential equations into systems of ordinary differential inequalities, providing stronger asymptotic control and enabling arbitrary high-order expansions.

Result: 1) Proves uniqueness of bowl soliton × Euclidean factor among ancient cylindrical flows with dominant linear mode; 2) Obtains complete asymptotic expansions for quadratic mode case; 3) Recovers classical results (tangent flow uniqueness, cylinder rigidity) with simpler proofs using single ODE inequality.

Conclusion: The PDE-ODI principle provides a powerful, self-contained framework for mean curvature flow analysis that simplifies proofs, yields stronger asymptotic control, and unifies classical results while opening avenues for further research.

Abstract: We introduce a new approach for analyzing ancient solutions and singularities of mean curvature flow that are locally modeled on a cylinder. Its key ingredient is a general mechanism, called the \emph{PDE--ODI principle}, which converts a broad class of parabolic differential equations into systems of ordinary differential inequalities. This principle bypasses many delicate analytic estimates used in previous work, and yields asymptotic expansions to arbitrarily high order.
  As an application, we establish the uniqueness of the bowl soliton times a Euclidean factor among ancient, cylindrical flows with dominant linear mode. This extends previous results on this problem to the most general setting and is made possible by the stronger asymptotic control provided by our analysis. In the other case, when the quadratic mode dominates, we obtain a complete asymptotic expansion to arbitrary polynomial order, which will form the basis for a subsequent paper. Our framework also recovers and unifies several classical results. In particular, we give new proofs of the uniqueness of tangent flows (due to Colding-Minicozzi) and the rigidity of cylinders among shrinkers (due to Colding-Ilmanen-Minicozzi) by reducing both problems to a single ordinary differential inequality, without using the Łojasiewicz-Simon inequality.
  Our approach is independent of prior work and the paper is largely self-contained.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [64] [Numerical study of solitary waves in Dirac--Klein--Gordon system](https://arxiv.org/abs/2512.24954)
*Andrew Comech,Julien Ricaud,Marco Roque*

Main category: math-ph

TL;DR: Numerical construction of solitary waves in Dirac-Klein-Gordon systems in 1D and 3D, studying energy/charge dependence on ω, using iterative methods from nonlinear Dirac solutions, with error control via virial identities and stability implications.


<details>
  <summary>Details</summary>
Motivation: To understand the properties of solitary waves in Dirac-Klein-Gordon systems, particularly their energy and charge dependence on the frequency parameter ω, and to investigate their spectral stability through numerical analysis.

Method: Numerical construction using iterative procedure starting from nonlinear Dirac equation solitary waves, computing corresponding scalar field, adjusting coupling constant. Comparison with shooting method for massless scalar field case. Error control using virial identities.

Result: Successfully constructed solitary waves in both 1D and 3D Dirac-Klein-Gordon systems, obtained dependence of energy and charge on ω, validated methods through comparison with shooting approach for massless case.

Conclusion: The numerical approach effectively constructs solitary waves in Dirac-Klein-Gordon systems, with virial identities providing reliable error control. The obtained energy/charge dependence on ω provides insights for analyzing spectral stability of these solitary waves.

Abstract: We use numerics to construct solitary waves in Dirac--Klein--Gordon (in one and three spatial dimensions) and study the dependence of energy and charge on $ω$. For the construction, we use the iterative procedure, starting from solitary waves of nonlinear Dirac equation, computing the corresponding scalar field, and adjusting the coupling constant. We also consider the case of massless scalar field, when the iteration procedure could be compared with the shooting method. We use the virial identities to control the error of simulations. We also discuss possible implications from the obtained results for the spectral stability of solitary waves.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [65] [Green's function on the Tate curve](https://arxiv.org/abs/2512.24935)
*An Huang,Rebecca Rohrlich,Yaojia Sun,Eric Whyman*

Main category: math.NT

TL;DR: The paper defines a Laplacian operator on the Tate curve and proves the existence and provides an explicit formula for its Green's function, establishing a p-adic counterpart to the Archimedean Green's function on a flat torus.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the problem of defining a p-adic string worldsheet action in genus one (torus topology), which requires understanding the corresponding Laplacian and Green's function in the p-adic setting.

Method: The authors define a Laplacian operator on the Tate curve (which is a p-adic analogue of an elliptic curve/torus) and study its Green's function through mathematical analysis in the p-adic setting.

Result: The main results are: (1) proof that the Green's function exists on the Tate curve, and (2) derivation of an explicit formula for this Green's function, which serves as a non-Archimedean counterpart to the Archimedean Green's function on a flat torus.

Conclusion: The work successfully establishes the mathematical foundation for p-adic string theory on genus one surfaces by providing the essential Green's function needed for worldsheet actions, bridging the gap between Archimedean and non-Archimedean formulations.

Abstract: Motivated by the question of defining a $p$-adic string worldsheet action in genus one, we define a Laplacian operator on the Tate curve, and study its Green's function. We show that the Green's function exists. We provide an explicit formula for the Green's function, which turns out to be a non-Archimedean counterpart of the Archimedean Green's function on a flat torus.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [66] [A positive eigenvalue result for semilinear differential equations in Banach spaces with functional initial conditions](https://arxiv.org/abs/2512.23876)
*Gennaro Infante,Paola Rubbioni*

Main category: math.CA

TL;DR: Existence of positive eigenvalues with nonnegative eigenfunctions for abstract initial value problems in Banach spaces with functional/nonlocal initial conditions.


<details>
  <summary>Details</summary>
Motivation: To establish existence results for positive eigenvalues and associated nonnegative eigenfunctions in abstract evolution problems with various types of functional initial conditions (periodic, multipoint, integral averages), which are important for applications in PDEs and dynamical systems.

Method: Combines nonlinear analysis, topological methods, and strongly continuous semigroup theory to develop an abstract framework applicable to a wide range of models.

Result: Develops abstract theory proving existence of positive eigenvalues with nonnegative mild eigenfunctions for problems with functional/nonlocal initial conditions, and demonstrates application to a reaction-diffusion equation with nonlocal initial condition from heat flow.

Conclusion: The abstract framework successfully establishes existence results for positive eigenvalues in Banach space problems with various functional initial conditions, with practical applications to PDE models like reaction-diffusion equations with nonlocal conditions.

Abstract: We study the existence of positive eigenvalues with associated nonnegative mild eigenfunctions for a class of abstract initial value problems in Banach spaces with functional, possibly nonlocal, initial conditions. The framework includes periodic, multipoint, and integral average conditions. Our approach relies on nonlinear analysis, topological methods, and the theory of strongly continuous semigroups, yielding results applicable to a wide range of models. As an illustration, we apply the abstract theory to a reaction-diffusion equation with a nonlocal initial condition arising from a heat flow problem.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [67] [Bridging Visual Intuition and Chemical Expertise: An Autonomous Analysis Framework for Nonadiabatic Dynamics Simulations via Mentor-Engineer-Student Collaboration](https://arxiv.org/abs/2512.24133)
*Yifei Zhu,Jiahui Zhang,Binni Huang,Zhenggang Lan*

Main category: physics.chem-ph

TL;DR: VisU is a vision-driven AI framework that uses large language models to autonomously analyze nonadiabatic molecular dynamics trajectories through a virtual research collective approach, reducing reliance on expert intuition.


<details>
  <summary>Details</summary>
Motivation: Traditional analysis of nonadiabatic molecular dynamics trajectories heavily relies on expert intuition and visual pattern recognition, which is difficult to formalize and scale. There's a need to reduce dependence on manual interpretation and enable more systematic, scalable mechanistic discovery.

Method: VisU uses a "virtual research collective" with a "Mentor-Engineer-Student" paradigm using two state-of-the-art large language models. The Mentor provides physical intuition through visual reasoning, the Engineer constructs analysis scripts, and the Student executes pipelines and manages data. The framework orchestrates a four-stage workflow: Preprocessing, Recursive Channel Discovery, Important-Motion Identification, and Validation/Summary.

Result: VisU autonomously identifies reaction channels and key nuclear motions while generating professional academic reports. It bridges visual insight with chemical expertise to analyze excited-state dynamics simulation results.

Conclusion: VisU establishes a new paradigm for human-AI collaboration in analyzing excited-state dynamics, significantly reducing dependence on manual interpretation and enabling more intuitive, scalable mechanistic discovery.

Abstract: Analyzing nonadiabatic molecular dynamics trajectories traditionally heavily relies on expert intuition and visual pattern recognition, a process that is difficult to formalize. We present VisU, a vision-driven framework that leverages the complementary strengths of two state-of-the-art large language models to establish a "virtual research collective." This collective operates through a "Mentor-Engineer-Student" paradigm that mimics the collaborative intelligence of a professional chemistry laboratory. Within this ecosystem, the Mentor provides physical intuition through visual reasoning, while the Engineer adaptively constructs analysis scripts, and the Student executes the pipeline and manages the data and results. VisU autonomously orchestrates a four-stage workflow comprising Preprocessing, Recursive Channel Discovery, Important-Motion Identification, and Validation/Summary. This systematic approach identifies reaction channels and key nuclear motions while generating professional academic reports. By bridging visual insight with chemical expertise, VisU establishes a new paradigm for human-AI collaboration in the analysis of excited-state dynamics simulation results, significantly reducing dependence on manual interpretation and enabling more intuitive, scalable mechanistic discovery.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [68] [NeuralCrop: Combining physics and machine learning for improved crop yield predictions](https://arxiv.org/abs/2512.20177)
*Yunan Lin,Sebastian Bathiany,Maha Badri,Maximilian Gelbrecht,Philipp Hess,Brian Groenke,Jens Heinke,Christoph Müller,Niklas Boers*

Main category: cs.LG

TL;DR: NeuralCrop is a hybrid crop model combining process-based GGCMs with machine learning, outperforming state-of-the-art models in yield prediction and showing robust generalization to unseen climate conditions.


<details>
  <summary>Details</summary>
Motivation: Traditional GGCMs have substantial uncertainties due to limited process understanding, while pure ML models fail to generalize to changing climate conditions. There's a need for models that combine process understanding with data-driven approaches for reliable yield projections under climate change.

Method: NeuralCrop combines an advanced process-based GGCM with data-driven ML components. It's first trained to emulate a competitive GGCM, then fine-tuned on observational data, creating a hybrid approach that leverages both process understanding and data-driven learning.

Result: NeuralCrop outperforms state-of-the-art GGCMs across site-level and large-scale cropping regions, particularly improving drought extreme predictions. It maintains robust projections under unseen conditions while pure ML models show substantial performance degradation.

Conclusion: The hybrid crop modeling approach offers improved crop modeling and more reliable yield projections under climate change and intensifying extreme weather conditions, demonstrating the value of combining process-based and data-driven methods.

Abstract: Global gridded crop models (GGCMs) simulate daily crop growth by explicitly representing key biophysical processes and project end-of-season yield time series. They are a primary tool to quantify the impacts of climate change on agricultural productivity and assess associated risks for food security. Despite decades of development, state-of-the-art GGCMs still have substantial uncertainties in simulating complex biophysical processes due to limited process understanding. Recently, machine learning approaches trained on observational data have shown great potential in crop yield predictions. However, these models have not demonstrated improved performance over classical GGCMs and are not suitable for simulating crop yields under changing climate conditions due to problems in generalizing outside their training distributions. Here we introduce NeuralCrop, a hybrid GGCM that combines the strengths of an advanced process-based GGCM, resolving important processes explicitly, with data-driven machine learning components. The model is first trained to emulate a competitive GGCM before it is fine-tuned on observational data. We show that NeuralCrop outperforms state-of-the-art GGCMs across site-level and large-scale cropping regions. Across moisture conditions, NeuralCrop reproduces the interannual yield anomalies in European wheat regions and the US Corn Belt more accurately during the period from 2000 to 2019 with particularly strong improvements under drought extremes. When generalizing to conditions unseen during training, NeuralCrop continues to make robust projections, while pure machine learning models exhibit substantial performance degradation. Our results show that our hybrid crop modelling approach offers overall improved crop modeling and more reliable yield projections under climate change and intensifying extreme weather conditions.

</details>


### [69] [Generative forecasting with joint probability models](https://arxiv.org/abs/2512.24446)
*Patrick Wyrod,Ashesh Chattopadhyay,Daniele Venturi*

Main category: cs.LG

TL;DR: Joint generative forecasting learns probability distributions over lagged system states, enabling improved chaotic system prediction with better uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Chaotic systems have fundamental forecasting limits due to sensitivity to initial conditions and unresolved multiscale processes. Existing generative models focus on next-step prediction rather than capturing underlying dynamics structure.

Method: Reframe forecasting as learning joint probability distribution of lagged system states over temporal windows, with forecasts obtained through marginalization. Introduce model-agnostic training/inference framework with three uncertainty metrics: ensemble variance, short-horizon autocorrelation, and cumulative Wasserstein drift.

Result: Joint generative models show improved short-term predictive skill, preserve attractor geometry, and achieve substantially more accurate long-range statistical behavior than conventional conditional next-step models on Lorenz-63 and Kuramoto-Sivashinsky systems.

Conclusion: Joint generative forecasting captures nonlinear temporal dependencies and enables robust uncertainty quantification without ground truth, offering superior performance for chaotic dynamical systems.

Abstract: Chaotic dynamical systems exhibit strong sensitivity to initial conditions and often contain unresolved multiscale processes, making deterministic forecasting fundamentally limited. Generative models offer an appealing alternative by learning distributions over plausible system evolutions; yet, most existing approaches focus on next-step conditional prediction rather than the structure of the underlying dynamics. In this work, we reframe forecasting as a fully generative problem by learning the joint probability distribution of lagged system states over short temporal windows and obtaining forecasts through marginalization. This new perspective allows the model to capture nonlinear temporal dependencies, represent multistep trajectory segments, and produce next-step predictions consistent with the learned joint distribution. We also introduce a general, model-agnostic training and inference framework for joint generative forecasting and show how it enables assessment of forecast robustness and reliability using three complementary uncertainty quantification metrics (ensemble variance, short-horizon autocorrelation, and cumulative Wasserstein drift), without access to ground truth. We evaluate the performance of the proposed method on two canonical chaotic dynamical systems, the Lorenz-63 system and the Kuramoto-Sivashinsky equation, and show that joint generative models yield improved short-term predictive skill, preserve attractor geometry, and achieve substantially more accurate long-range statistical behaviour than conventional conditional next-step models.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [70] [Upscaling from ab initio atomistic simulations to electrode scale: The case of manganese hexacyanoferrate, a cathode material for Na-ion batteries](https://arxiv.org/abs/2512.24816)
*Yuan-Chi Yang,Eric Woillez,Quentin Jacquet,Ambroise van Roekeghem*

Main category: cond-mat.mtrl-sci

TL;DR: A multiscale computational framework bridges atomistic to device scales for predictive modeling of insertion-type electrode materials, demonstrated on sodium manganese hexacyanoferrate cathode for sodium-ion batteries.


<details>
  <summary>Details</summary>
Motivation: To enable rational computational design of next-generation insertion-type materials (like battery electrodes) by systematically translating atomistic insights into continuum-scale predictions, addressing the scale-bridging challenge in materials modeling.

Method: Active-learning strategy trains Moment Tensor Potential through iterative hybrid grand-canonical Monte Carlo-molecular dynamics sampling. Machine learning interatomic potential captures configuration spaces at all sodiation levels. Critical parameters (diffusivities, interfacial/strain energies, free-energy landscapes) feed into pseudo-2D phase-field simulations for electrode-scale predictions.

Result: Accurately reproduces experimental properties (volume expansion, operating voltage, structural transformations). Reveals four-order-of-magnitude difference in sodium diffusivity between rhombohedral (sodium-rich) and tetragonal (sodium-poor) phases at 300K. Successfully predicts phase-boundary propagation and rate-dependent performances across electrode length scales.

Conclusion: The multiscale workflow establishes a blueprint for rational computational design of insertion-type materials, demonstrating systematic translation of atomistic insights into continuum-scale predictions for battery electrode materials and beyond.

Abstract: We present a generalizable scale-bridging computational framework that enables predictive modeling of insertion-type electrode materials from atomistic to device scales. Applied to sodium manganese hexacyanoferrate, a promising cathode material for grid-scale sodium-ion batteries, our methodology employs an active-learning strategy to train a Moment Tensor Potential through iterative hybrid grand-canonical Monte Carlo--molecular dynamics sampling, robustly capturing configuration spaces at all sodiation levels. The resulting machine learning interatomic potential accurately reproduces experimental properties including volume expansion, operating voltage, and sodium concentration-dependent structural transformations, while revealing a four-order-of-magnitude difference in sodium diffusivity between the rhombohedral (sodium-rich) and tetragonal (sodium-poor) phases at 300 K. We directly compute all critical parameters -- temperature- and concentration-dependent diffusivities, interfacial and strain energies, and complete free-energy landscapes -- to feed them into pseudo-2D phase-field simulations that predict phase-boundary propagation and rate-dependent performances across electrode length scales. This multiscale workflow establishes a blueprint for rational computational design of next-generation insertion-type materials, such as battery electrode materials, demonstrating how atomistic insights can be systematically translated into continuum-scale predictions.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [71] [Assessing generative modeling approaches for free energy estimates in condensed matter](https://arxiv.org/abs/2512.23930)
*Maximilian Schebek,Jiajun He,Emil Hoffmann,Yuanqi Du,Frank Noé,Jutta Rogal*

Main category: cond-mat.stat-mech

TL;DR: Systematic review and benchmarking of generative-model-based free energy estimation methods for condensed-matter systems, comparing discrete/continuous normalizing flows, FEAT, and escorted Jarzynski equality.


<details>
  <summary>Details</summary>
Motivation: Traditional free energy estimation methods require sampling multiple intermediate states, making them computationally expensive. Recent generative-model-based methods bypass intermediates but lack systematic comparison of their trade-offs between efficiency, accuracy, and scalability.

Method: Systematic review of generative-model-based methods and benchmarking of selected approaches: discrete/continuous normalizing flows in targeted free energy perturbation, FEAT (Free energy Estimators with Adaptive Transport), and escorted Jarzynski equality. Evaluated on coarse-grained monatomic ice and Lennard-Jones solids.

Result: Evaluation of accuracy, data efficiency, computational cost, and scalability with system size. Provides quantitative framework for selecting effective free energy estimation strategies in condensed-phase systems.

Conclusion: The study offers a systematic comparison that helps researchers choose appropriate free energy estimation methods based on specific needs for accuracy, efficiency, and scalability in condensed-matter simulations.

Abstract: The accurate estimation of free energy differences between two states is a long-standing challenge in molecular simulations. Traditional approaches generally rely on sampling multiple intermediate states to ensure sufficient overlap in phase space and are, consequently, computationally expensive. Several generative-model-based methods have recently addressed this challenge by learning a direct bridge between distributions, bypassing the need for intermediate states. However, it remains unclear which approaches provide the best trade-off between efficiency, accuracy, and scalability. In this work, we systematically review these methods and benchmark selected approaches with a focus on condensed-matter systems. In particular, we investigate the performance of discrete and continuous normalizing flows in the context of targeted free energy perturbation as well as FEAT (Free energy Estimators with Adaptive Transport) together with the escorted Jarzynski equality, using coarse-grained monatomic ice and Lennard-Jones solids as benchmark systems. We evaluate accuracy, data efficiency, computational cost, and scalability with system size. Our results provide a quantitative framework for selecting effective free energy estimation strategies in condensed-phase systems.

</details>


<div id='nlin.PS'></div>

# nlin.PS [[Back]](#toc)

### [72] [Soliton profiles: Classical Numerical Schemes vs. Neural Network - Based Solvers](https://arxiv.org/abs/2512.24634)
*Chandler Haight,Svetlana Roudenko,Zhongming Wang*

Main category: nlin.PS

TL;DR: Classical numerical solvers outperform neural network methods for single-instance 1D solitary wave computations, while operator-learning methods offer advantages for repeated simulations.


<details>
  <summary>Details</summary>
Motivation: To compare the performance of classical numerical methods versus neural network approaches for computing ground states/profiles of solitary-wave solutions in 1D dispersive PDEs.

Method: Comparative study of classical methods (Petviashvili's method, finite difference with Newton iterations) vs. neural network methods (PINNs, operator-learning) for solving nonlinear Schrödinger, nonlinear Klein-Gordon, and generalized KdV equations.

Result: Classical methods maintain high-order accuracy and computational efficiency for single-instance problems. PINNs produce qualitative solutions but are less accurate and efficient. Operator-learning methods are computationally intensive during training but provide rapid inference for repeated simulations.

Conclusion: Classical solvers remain superior for single-instance computations in 1D, while operator-learning methods are attractive for applications requiring repeated simulations or real-time predictions despite lower single-instance accuracy.

Abstract: We present a comparative study of classical numerical solvers, such as Petviashvili's method or finite difference with Newton iterations, and neural network-based methods for computing ground states or profiles of solitary-wave solutions to the one-dimensional dispersive PDEs that include the nonlinear Schrödinger, the nonlinear Klein-Gordon and the generalized KdV equations. We confirm that classical approaches retain high-order accuracy and strong computational efficiency for single-instance problems in the one-dimensional setting. Physics-informed neural networks (PINNs) are also able to reproduce qualitative solutions but are generally less accurate and less efficient in low dimensions than classical solvers due to expensive training and slow convergence. We also investigate the operator-learning methods, which, although computationally intensive during training, can be reused across many parameter instances, providing rapid inference after pretraining, making them attractive for applications involving repeated simulations or real-time predictions. For single-instance computations, however, the accuracy of operator-learning methods remains lower than that of classical methods or PINNs, in general.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [73] [Computational Analysis of Disease Progression in Pediatric Pulmonary Arterial Hypertension](https://arxiv.org/abs/2512.25027)
*Omar Said,Christopher Tossas-Betancourt,Mary K. Olive,Jimmy C. Lu,Adam Dorfman,C. Alberto Figueroa*

Main category: physics.med-ph

TL;DR: Researchers developed patient-specific computational models for pediatric pulmonary arterial hypertension using longitudinal MRI/catheterization data, creating multi-scale cardiovascular simulations that track disease progression over time.


<details>
  <summary>Details</summary>
Motivation: Pediatric PAH is understudied due to limited data and lack of targeted diagnostic/therapeutic strategies. There's a need for better tools to understand disease progression and inform treatment in children with PAH.

Method: Developed multi-scale patient-specific cardiovascular models for 4 pediatric PAH patients using longitudinal MRI/catheterization data collected ~2 years apart. Used CRIMSON framework to couple 3D FSI pulmonary artery models with 0D lumped-parameter heart and Windkessel models. Created automated Python-based optimizer to calibrate boundary conditions, reducing calibration time from weeks to days.

Result: Model-derived metrics (arterial stiffness, pulse wave velocity, resistance, compliance) aligned with clinical indicators of disease severity and progression. Computational modeling successfully captured patient-specific hemodynamic adaptation over time.

Conclusion: Computational modeling offers a promising non-invasive tool for monitoring pediatric PAH progression and informing future treatment strategies, potentially addressing the current limitations in pediatric PAH management.

Abstract: Pulmonary arterial hypertension (PAH) is a progressive cardiopulmonary disease that leads to increased pulmonary pressures, vascular remodeling, and eventual right ventricular (RV) failure. Pediatric PAH remains understudied due to limited data and the lack of targeted diagnostic and therapeutic strategies. In this study, we developed and calibrated multi-scale, patient-specific cardiovascular models for four pediatric PAH patients using longitudinal MRI and catheterization data collected approximately two years apart. Using the CRIMSON simulation framework, we coupled three-dimensional fluid-structure interaction (FSI) models of the pulmonary arteries with zero-dimensional (0D) lumped-parameter heart and Windkessel models to simulate patient hemodynamics. An automated Python-based optimizer was developed to calibrate boundary conditions by minimizing discrepancies between simulated and clinical metrics, reducing calibration time from weeks to days. Model-derived metrics such as arterial stiffness, pulse wave velocity, resistance, and compliance were found to align with clinical indicators of disease severity and progression. Our findings demonstrate that computational modeling can non-invasively capture patient-specific hemodynamic adaptation over time, offering a promising tool for monitoring pediatric PAH and informing future treatment strategies.

</details>


### [74] [Finite element analysis of very large bone models based on micro-CT scans](https://arxiv.org/abs/2512.24401)
*Shani Martinez-Weissberg,Will Pazner,Zohar Yosibash*

Main category: physics.med-ph

TL;DR: Open-source μFE framework enables large-scale biomechanical analysis of intact rabbit femur using μCT data, validated with experiments, showing 40μm resolution balances accuracy and computational cost.


<details>
  <summary>Details</summary>
Motivation: High-resolution μFE models from μCT imaging provide detailed bone mechanics but face computational challenges at anatomically relevant scales, requiring scalable open-source solutions.

Method: Integrated μCT segmentation using MIA clustering, voxel-based μFE meshing with MFEM library, verification with commercial solver, resolution studies (20/40/80μm), and experimental validation via Digital Image Correlation on rabbit femur compression.

Result: Models with over 800M DOFs solved on moderate HPC; 40μm resolution preserves boundary displacement and strain distributions while reducing computational cost; segmentation parameters affect global mechanical response; effective bone material properties calibrated.

Conclusion: Large-scale experimentally informed μFE modeling is feasible with open-source tools, providing robust foundation for preclinical bone mechanics assessment and treatment-related risk evaluation.

Abstract: High-resolution voxel-based micro-finite element ($μ$FE) models derived from $μ$CT imaging enable detailed investigation of bone mechanics but remain computationally challenging at anatomically relevant scales. This study presents a comprehensive $μ$FE framework for large-scale biomechanical analysis of an intact New Zealand White (NZW) rabbit femur, integrating advanced segmentation, scalable finite element solvers, and experimental validation using predominantly open-source libraries. Bone geometries were segmented from $μ$CT data using the MIA clustering algorithm and converted into voxel-based $μ$FE meshes, which were solved using the open-source MFEM library with algorithms designed for large-scale linear elasticity systems.
  The numerical solutions were verified by comparing with a commercial finite element solver, and by evaluating the performance of full assembly and element-by-element formulations within MFEM. Models containing over $8\times10^{8}$ DOFs were solved using moderate HPC resources, demonstrating the feasibility of anatomically realistic $μ$FE simulations at this scale. Resolution effects were investigated by comparing models with voxel sizes of 20, 40, and 80 $μ$m, revealing that 40 $μ$m preserves boundary displacement and principal strain distributions with minimal bias while significantly reducing computational cost. Sensitivity analyses further showed that segmentation parameters influence the global mechanical response.
  Finally, $μ$FE predictions were coupled with Digital Image Correlation measurements on an NZW rabbit femur under compression to calibrate effective bone material properties at the micron scale. The results demonstrate that large-scale, experimentally informed $μ$FE modeling can be achieved using open-source tools, providing a robust foundation for preclinical assessment of bone mechanics and treatment-related risks.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [75] [Stochastic Galerkin Method and Hierarchical Preconditioning for PDE-constrained Optimization](https://arxiv.org/abs/2512.23804)
*Zhendong Li,Akwum Onwunta,Bedřich Sousedík*

Main category: math.OC

TL;DR: Efficient hierarchical preconditioners for PDE-constrained optimal control with uncertain coefficients, using stochastic Galerkin and polynomial chaos to accelerate iterative solvers.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of solving large-scale, ill-conditioned linear systems arising in uncertainty quantification for PDE-constrained optimal control problems with uncertain coefficients.

Method: Discretize-then-optimize framework combining finite elements, stochastic Galerkin approximation, and advanced time-discretization. Exploits sparsity in generalized polynomial chaos expansions to derive hierarchical preconditioners based on truncated stochastic expansions.

Result: Proposed preconditioners significantly accelerate convergence of iterative solvers compared to existing methods, providing robust and efficient solvers for both steady-state and time-dependent optimal control under uncertainty.

Conclusion: The hierarchical preconditioning approach effectively balances computational cost and preconditioning quality, offering practical computational tools for uncertainty quantification in PDE-constrained optimal control.

Abstract: We develop efficient hierarchical preconditioners for optimal control problems governed by partial differential equations with uncertain coefficients. Adopting a discretize-then-optimize framework that integrates finite element discretization, stochastic Galerkin approximation, and advanced time-discretization schemes, the approach addresses the challenge of large-scale, ill-conditioned linear systems arising in uncertainty quantification. By exploiting the sparsity inherent in generalized polynomial chaos expansions, we derive hierarchical preconditioners based on truncated stochastic expansion that strike an effective balance between computational cost and preconditioning quality. Numerical experiments demonstrate that the proposed preconditioners significantly accelerate the convergence of iterative solvers compared to existing methods, providing robust and efficient solvers for both steady-state and time-dependent optimal control applications under uncertainty.

</details>


### [76] [The Flow-Limit of Reflect-Reflect-Relax: Existence, Stability, and Discrete-Time Behavior](https://arxiv.org/abs/2512.23843)
*Manish Krishan Lal*

Main category: math.OC

TL;DR: RRR algorithm's small-step regime analyzed: smooth transverse dynamics form hyperbolic sink with exponential gap decay, finite-time capture in discrete setting, and heuristic mesoscopic framework for performance near Douglas-Rachford limit.


<details>
  <summary>Details</summary>
Motivation: To understand the Reflect-Reflect-Relax (RRR) algorithm's behavior in its small-step (flow-limit) regime, particularly its convergence properties, dynamics, and the emergence of iteration-optimal relaxation parameters.

Method: Analyze RRR in smooth transversal setting showing transverse dynamics form hyperbolic sink; construct Lyapunov function using squared gap measure; study discrete setting with Filippov sliding dynamics; prove small-step RRR is forward-Euler discretization; introduce heuristic mesoscopic framework using percolation and renormalization group theory.

Result: Exponential decay of gap measure in smooth setting; finite-time capture into solution domain in discrete setting; solution times converge to finite limit while iteration counts diverge; explains emergence of iteration-optimal relaxation parameters.

Conclusion: RRR algorithm exhibits well-behaved dynamics with exponential convergence in smooth settings, finite-time capture in discrete implementations, and the analysis provides theoretical explanation for optimal parameter selection and performance degradation near Douglas-Rachford limit.

Abstract: We study the Reflect-Reflect-Relax (RRR) algorithm in its small-step (flow-limit) regime. In the smooth transversal setting, we show that the transverse dynamics form a hyperbolic sink, yielding exponential decay of a natural gap measure. Under uniform geometric assumptions, we construct a tubular neighborhood of the feasible manifold on which the squared gap defines a strict Lyapunov function, excluding recurrent dynamics and chaotic behavior within this basin.
  In the discrete setting, the induced flow is piecewise constant on W-domains and supports Filippov sliding along convergent boundaries, leading to finite-time capture into a solution domain. We prove that small-step RRR is a forward-Euler discretization of this flow, so that solution times measured in rescaled units converge to a finite limit while iteration counts diverge, explaining the emergence of iteration-optimal relaxation parameters. Finally, we introduce a heuristic mesoscopic framework based on percolation and renormalization group to organize performance deterioration near the Douglas-Rachford limit.

</details>
