<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 18]
- [math.AP](#math.AP) [Total: 24]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [math.OC](#math.OC) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.NT](#math.NT) [Total: 1]
- [math.PR](#math.PR) [Total: 3]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [nlin.PS](#nlin.PS) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [math.DG](#math.DG) [Total: 4]
- [math-ph](#math-ph) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A note on the space-time variational formulation for the wave equation with source term in $L^2(Q)$](https://arxiv.org/abs/2512.23807)
*Marco Zank*

Main category: math.NA

TL;DR: A variational formulation for the scalar wave equation with homogeneous initial conditions on bounded Lipschitz domains, using a new solution space and L² test space, proven to satisfy inf-sup conditions with an isomorphism as solution operator.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous variational framework for the second-order scalar wave equation that supports space-time discretization methods, including least-squares approaches and boundary element methods, while addressing uniqueness and solvability issues.

Method: Derived a variational formulation in a bounded space-time cylinder Q with homogeneous initial conditions, using a new solution space and test space L²(Q) for L² source terms. Applied inf-sup theory and proved the variational setting satisfies inf-sup conditions with an isomorphism as solution operator.

Result: Proved the variational framework fits inf-sup theory with an isomorphism as solution operator. Showed the new solution space is not a subspace of H²(Q). Established uniqueness and solvability results crucial for space-time discretizations and boundary integral equations.

Conclusion: The new variational formulation provides a solid theoretical foundation for space-time methods, including least-squares approaches and boundary element methods, with proven uniqueness and solvability properties that are essential for numerical analysis and regularity results.

Abstract: We derive a variational formulation for the scalar wave equation in the second-order formulation on bounded Lipschitz domains and homogeneous initial conditions. We investigate a variational framework in a bounded space-time cylinder $Q$ with a new solution space and the test space $L^2(Q)$ for source terms in $L^2(Q)$. Using existence and uniqueness results in $H^1(Q)$, we prove that this variational setting fits the inf-sup theory, including an isomorphism as solution operator. Moreover, we show that the new solution space is not a subspace of $H^2(Q)$. This new uniqueness and solvability result is not only crucial for discretizations using space-time methods, including least-squares approaches, but also important for regularity results and the analysis of related space-time boundary integral equations, which form the basis for space-time boundary element methods.

</details>


### [2] [Greedy Rational Approximation for Frequency-Domain Model Reduction of Parametric LTI Systems](https://arxiv.org/abs/2512.23814)
*Filip Bělík,Yanlai Chen,Akil Narayan*

Main category: math.NA

TL;DR: The paper proposes using reduced basis methods for model reduction of parametric LTI systems by constructing low-order rational approximations of high-order rational functions through an iterative greedy approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a principled approach for model reduction of parametric linear time-invariant dynamical systems, which can be formulated as seeking low-order rational function approximations of high-order rational functions in the frequency domain.

Method: The method uses a standard reduced basis method (RBM) with an iterative greedy approach. The greedy objective is evaluated through an error estimator that exploits the linearity of the frequency domain representation. The framework is theoretically motivated by rational approximability results.

Result: The paper presents a computational framework for rational compression of high-order rational functions, providing a pathway for model reduction of parametric LTI systems.

Conclusion: The proposed reduced basis method with greedy approach offers a principled and computationally viable method for model reduction of parametric LTI systems through rational function approximation in the frequency domain.

Abstract: We investigate model reduction of parametric linear time-invariant (LTI) dynamical systems. When posed in the frequency domain, this problem can be formulated as seeking a low-order rational function approximation of a high-order rational function. We propose to use a standard reduced basis method (RBM) to construct this low-order rational function. Algorithmically, this procedure is an iterative greedy approach, where the greedy objective is evaluated through an error estimator that exploits the linearity of the frequency domain representation. The greedy framework is motivated through theoretical results of rational approximability of functions. This framework provides a principled approach to rational compression of high-order rational functions, and provides a computational pathway for model reduction of parametric LTI systems.

</details>


### [3] [Deep learning methods for inverse problems using connections between proximal operators and Hamilton-Jacobi equations](https://arxiv.org/abs/2512.23829)
*Oluwatosin Akande,Gabriel P. Langlois,Akwum Onwunta*

Main category: math.NA

TL;DR: The paper proposes using Hamilton-Jacobi PDEs to develop deep learning architectures for learning priors in inverse problems, directly learning the prior without needing to invert it after training.


<details>
  <summary>Details</summary>
Motivation: Inverse problems are ill-posed and require regularization/priors. Proximal operators are central for encoding priors and building efficient algorithms. Recent work connects proximal operators to convex potentials, and there's a need for methods that learn priors directly without inversion after training.

Method: Leverage connections between proximal operators and Hamilton-Jacobi PDEs to develop novel deep learning architectures for learning the prior. The approach learns the prior directly without recourse to inverting the prior after training.

Result: Several numerical results demonstrate the efficiency of the proposed method in high dimensions.

Conclusion: The Hamilton-Jacobi PDE connection provides a novel approach for learning priors in inverse problems, offering direct prior learning without post-training inversion and demonstrating effectiveness in high-dimensional settings.

Abstract: Inverse problems are important mathematical problems that seek to recover model parameters from noisy data. Since inverse problems are often ill-posed, they require regularization or incorporation of prior information about the underlying model or unknown variables. Proximal operators, ubiquitous in nonsmooth optimization, are central to this because they provide a flexible and convenient way to encode priors and build efficient iterative algorithms. They have also recently become key to modern machine learning methods, e.g., for plug-and-play methods for learned denoisers and deep neural architectures for learning priors of proximal operators. The latter was developed partly due to recent work characterizing proximal operators of nonconvex priors as subdifferential of convex potentials. In this work, we propose to leverage connections between proximal operators and Hamilton-Jacobi partial differential equations (HJ PDEs) to develop novel deep learning architectures for learning the prior. In contrast to other existing methods, we learn the prior directly without recourse to inverting the prior after training. We present several numerical results that demonstrate the efficiency of the proposed method in high dimensions.

</details>


### [4] [Multimodal sampling via Schrödinger-Föllmer samplers with temperatures](https://arxiv.org/abs/2512.23965)
*Xiaojie Wang,Xiaoyan Zhang*

Main category: math.NA

TL;DR: The paper introduces temperature-parameterized Schrödinger-Föllmer samplers with improved O(h) convergence rate in Wasserstein distance, outperforming Langevin samplers for multimodal distributions.


<details>
  <summary>Details</summary>
Motivation: Existing Schrödinger-Föllmer samplers have only O(√h) convergence rate. The authors aim to improve convergence and enhance performance for sampling from complex, high-dimensional multimodal distributions.

Method: Introduce temperature parameter into Schrödinger-Föllmer process, develop novel error analysis for time discretization, and analyze Euler discretization of the temperature-parameterized diffusion process.

Result: Achieve enhanced O(h) convergence rate in L²-Wasserstein distance (improving from O(√h)), demonstrate superior performance over Langevin samplers for multimodal distributions, and show temperature's importance for multimodal sampling.

Conclusion: Temperature-parameterized Schrödinger-Föllmer samplers provide gradient-free, non-ergodic sampling with improved convergence rates, particularly effective for challenging multimodal distributions compared to traditional Langevin methods.

Abstract: Generating samples from complex and high-dimensional distributions is ubiquitous in various scientific fields of statistical physics, Bayesian inference, scientific computing and machine learning. Very recently, Huang et al. (IEEE Trans. Inform. Theory, 2025) proposed new Schrödinger-Föllmer samplers (SFS), based on the Euler discretization of the Schrödinger-Föllmer diffusion evolving on the unit interval $[0, 1]$. There, a convergence rate of order $\mathcal{O}(\sqrt{h})$ in the $L^2$-Wasserstein distance was obtained for the Euler discretization with a uniform time step-size $h>0$.
  By incorporating a temperature parameter, different samplers are introduced in this paper, based on the Euler discretization of the Schrödinger-Föllmer process with temperatures. As revealed by numerical experiments, high temperatures are vital, particularly in sampling from multimodal distributions. Further, a novel approach of error analysis is developed for the time discretization and an enhanced convergence rate of order ${ \mathcal{O}(h)}$ is obtained in the $L^2$-Wasserstein distance, under certain smoothness conditions on the drift. This significantly improves the existing order-half convergence in the aforementioned paper. Unlike Langevin samplers, SFS is of gradient-free, works in a unit interval $[0, 1]$ and does not require any ergodicity. Numerical experiments confirm the convergence rate and show that, the SFS substantially outperforms vanilla Langevin samplers, particularly in sampling from multimodal distributions.

</details>


### [5] [High order numerical discretizations of the Einstein-Euler equations in the Generalized Harmonic formulation](https://arxiv.org/abs/2512.24121)
*Stefano Muzzolon,Michael Dumbser,Olindo Zanotti,Elena Gaburro*

Main category: math.NA

TL;DR: Two new numerical schemes (FD-CWENO and ADER-DG) for solving Einstein-Euler equations with well-balancing properties, validated on vacuum and matter-coupled tests including black holes and neutron stars.


<details>
  <summary>Details</summary>
Motivation: To develop robust numerical methods for solving coupled Einstein-Euler equations in General Relativity, particularly for astrophysical simulations involving black holes and neutron stars, with a focus on unstructured mesh capabilities for future 3D moving mesh applications.

Method: Two numerical schemes: 1) Finite Difference Central Weighted Essentially Non-Oscillatory (FD-CWENO) on Cartesian meshes, and 2) ADER discontinuous Galerkin (DG) scheme on 2D unstructured polygonal meshes. Both incorporate well-balancing to preserve stationary solutions exactly at discrete level.

Result: Successful validation on standard vacuum tests (robust stability, linearized wave, gauge wave), long-term stable evolutions of stationary black holes (including extreme spin Kerr), and matter-coupled tests (spherical accretion onto Schwarzschild black hole, perturbed non-rotating neutron star).

Conclusion: The developed schemes provide a solid foundation for more complex astrophysical simulations using DG methods on unstructured 3D meshes, demonstrating capability to handle both vacuum and matter-coupled Einstein-Euler systems.

Abstract: We propose two new alternative numerical schemes to solve the coupled Einstein-Euler equations in the Generalized Harmonic formulation. The first one is a finite difference (FD) Central Weighted Essentially Non-Oscillatory (CWENO) scheme on a traditional Cartesian mesh, while the second one is an ADER (Arbitrary high order Derivatives) discontinuous Galerkin (DG) scheme on 2D unstructured polygonal meshes. The latter, in particular, represents a preliminary step in view of a full 3D numerical relativity calculation on moving meshes. Both schemes are equipped with a well-balancing (WB) property, which allows to preserve the equilibrium of a priori known stationary solutions exactly at the discrete level. We validate our numerical approaches by successfully reproducing standard vacuum test cases, such as the robust stability, the linearized wave, and the gauge wave tests, as well as achieving long-term stable evolutions of stationary black holes, including Kerr black holes with extreme spin. Concerning the coupling with matter, modeled by the relativistic Euler equations, we perform a classical test of spherical accretion onto a Schwarzschild black hole, as well as an evolution of a perturbed non-rotating neutron star, demonstrating the capability of our schemes to operate also on the full Einstein-Euler system. Altogether, these results provide a solid foundation for addressing more complex and challenging simulations of astrophysical sources through DG schemes on unstructured 3D meshes.

</details>


### [6] [Structure-preserving schemes for nonlinear symmetric hyperbolic and thermodynamically compatible systems of partial differential equations](https://arxiv.org/abs/2512.24127)
*Alessia Lucca,Michael Dumbser*

Main category: math.NA

TL;DR: Development of exactly energy-conservative and structure-preserving finite volume schemes for SHTC systems in continuum physics.


<details>
  <summary>Details</summary>
Motivation: SHTC systems have thermodynamic compatibility that ensures total energy conservation and often satisfy stationary differential constraints (involutions). Existing schemes may not preserve these properties at discrete level.

Method: Two approaches: 1) Simple semi-discrete cell-centered HTC finite volume scheme with collocated grids (energy-conservative but not involution-preserving). 2) Fully discrete semi-implicit vertex-based staggered scheme that exactly preserves both energy conservation and involution constraints (∇·∇×A=0 and ∇×∇φ=0). Uses discrete symmetric-hyperbolic Godunov-form leading to symmetric positive definite linear systems solved via iterative fixed-point method.

Result: Applied to three SHTC systems: nonlinear acoustics, nonlinear Maxwell equations (no charges), and nonlinear Maxwell-GLM system. Numerical results demonstrate the claimed properties of the schemes.

Conclusion: Successfully developed exactly energy-conservative and structure-preserving finite volume schemes for SHTC systems, with the staggered semi-implicit scheme achieving both exact energy conservation and exact preservation of involution constraints at discrete level.

Abstract: This paper aims at developing exactly energy-conservative and structure-preserving finite volume schemes for the discretisation of first-order symmetric-hyperbolic and thermodynamically compatible (SHTC) systems of partial differential equations in continuum physics. Due to their thermodynamic compatibility the class of SHTC systems satisfies an additional conservation law for the total energy and many PDE in this class of equations also satisfy stationary differential constraints (involutions). First, we propose a simple semi-discrete cell-centered HTC finite volume scheme that employs collocated grids and that is compatible with the total energy conservation law, but which does not satisfy the involutions. Second, we develop a fully discrete semi-implicit finite volume scheme that conserves total energy and which can be proven to satisfy also the involution constraints exactly at the discrete level. This method is a vertex-based staggered semi-implicit scheme that preserves the basic vector calculus identities $\nabla \cdot \nabla \times A = 0$ and $\nabla \times \nabla φ= 0$ for any vector and scalar field, respectively, exactly at the discrete level and which is also exactly totally energy conservative. The main key ingredient of the proposed implicit scheme is the fact that it uses a discrete version of the symmetric-hyperbolic Godunov-form of the governing PDE system. This leads naturally to sequences of symmetric and positive definite linear algebraic systems to be solved inside an iterative fixed-point method used in each time step. We apply our new schemes to three different SHTC systems. In particular, we consider the equations of nonlinear acoustics, the nonlinear Maxwell equations in the absence of charges and a nonlinear version of the Maxwell-GLM system. We also show some numerical results to provide evidence of the stated properties of the proposed schemes.

</details>


### [7] [Sufficient and Necessary Conditions for Eckart-Young-like Result for Tubal Tensors](https://arxiv.org/abs/2512.24405)
*Uria Mor*

Main category: math.NA

TL;DR: The paper characterizes which tubal tensor products yield Eckart-Young type theorems for low-rank approximation.


<details>
  <summary>Details</summary>
Motivation: While tubal tensor frameworks allow matrix-like operations including SVD and rank concepts, only specific tubal products have been shown to support Eckart-Young type results (best low-rank approximation via truncated SVD). The paper aims to fully characterize which tubal products yield this desirable property.

Method: Theoretical analysis to characterize the family of tubal products that satisfy Eckart-Young type theorems. Experimental validation using video data and data-driven dynamical systems to demonstrate practical implications.

Result: Complete characterization of tubal products that yield Eckart-Young results. Experimental demonstration showing practical benefits of these theoretical findings in real applications.

Conclusion: The paper provides a comprehensive theoretical framework identifying which tubal tensor products support Eckart-Young type low-rank approximation, with practical validation showing these properties are useful for real-world data analysis tasks.

Abstract: A valuable feature of the tubal tensor framework is that many familiar constructions from matrix algebra carry over to tensors, including SVD and notions of rank. Most importantly, it has been shown that for a specific family of tubal products, an Eckart-Young type theorem holds, i.e., the best low-rank approximation of a tensor under the Frobenius norm is obtained by truncating its tubal SVD. In this paper, we provide a complete characterization of the family of tubal products that yield an Eckart-Young type result. We demonstrate the practical implications of our theoretical findings by conducting experiments with video data and data-driven dynamical systems.

</details>


### [8] [Fast high-order spectral solvers for PDEs on triangulated surfaces with applications to deforming surfaces](https://arxiv.org/abs/2512.24456)
*Gentian Zavalani*

Main category: math.NA

TL;DR: Extends quadrilateral-based HPS method to triangles using two high-order strategies: reduced quadrilateralization and Dubiner polynomial-based spectral elements, preserving spectral accuracy and fast solver structure.


<details>
  <summary>Details</summary>
Motivation: The classical HPS framework is limited to quadrilateral meshes, restricting its application to triangulated geometries. There's a need to extend this efficient framework to triangular elements while maintaining its spectral accuracy and fast direct-solver properties.

Method: Two complementary high-order strategies: 1) Reduced quadrilateralization approach (straightforward implementation), and 2) Triangle-based spectral element method using Dubiner polynomials. Both methods extend the HPS framework to triangular elements.

Result: Numerical demonstrations show the extensions preserve spectral accuracy, efficiency, and fast direct-solver structure of HPS. The method successfully handles time-dependent and evolving surfaces, with applications to reaction-diffusion systems and geometry-driven surface evolution.

Conclusion: The paper successfully extends the HPS framework to triangular geometries while maintaining its key advantages, enabling broader application to complex geometries and time-dependent surface problems.

Abstract: In this paper, we extend the classical quadrilateral based hierarchical Poincaré-Steklov (HPS) framework to triangulated geometries. Traditionally, the HPS method takes as input an unstructured, high-order quadrilateral mesh and relies on tensor-product spectral discretizations on each element. To overcome this restriction, we introduce two complementary high-order strategies for triangular elements: a reduced quadrilateralization approach which is straightforward to implement, and triangle based spectral element method based on Dubiner polynomials. We show numerically that these extensions preserve the spectral accuracy, efficiency, and fast direct-solver structure of the HPS framework. The method is further extended to time dependent and evolving surfaces, and its performance is demonstrated through numerical experiments on reaction-diffusion systems, and geometry driven surface evolution.

</details>


### [9] [Exponential Convergence of Deep Composite Polynomial Approximation for Cusp-Type Functions](https://arxiv.org/abs/2512.24523)
*Kingsley Yeon,Steven B. Damelin,Michael Werman*

Main category: math.NA

TL;DR: Deep composite polynomial approximations achieve exponential convergence for functions with algebraic cusp singularities, outperforming classical polynomial methods.


<details>
  <summary>Details</summary>
Motivation: Continuous but non-differentiable functions with algebraic cusp singularities (like |x-a|^α with α∈(0,1)) are challenging for classical polynomial approximations which only achieve algebraic convergence rates.

Method: Constructive scheme combining division-free polynomial iteration for fractional powers (inner layer) with analytic polynomial fitting (outer layer). The composite structure has two polynomial layers working together.

Result: Exponential convergence in L^p([-1,1]) approximation error with respect to parameter budget (number of scalar coefficients). Numerical experiments confirm theoretical rates for single and multiple cusp configurations.

Conclusion: Deep composite polynomial constructions are parameter-efficient and achieve exponential convergence for cusp-type functions, significantly outperforming classical single-layer polynomial approximations.

Abstract: We investigate deep composite polynomial approximations of continuous but non-differentiable functions with algebraic cusp singularities. The functions in focus consist of finitely many cusp terms of the form $|x-a_j|^{α_j}$ with rational exponents $α_j\in(0,1)$ on a real-analytic background. We propose a constructive approximation scheme that combines a division-free polynomial iteration for fractional powers with an outer layer for the analytic polynomial fitting. Our main result shows that this composite structure achieves exponential convergence in the the number of scalar coefficients in the inner and outer polynomial layers. Specifically, the $L^p([-1,1])$ approximation error, decays exponentially with respect to the parameter budget, in contrast to the algebraic rates obtained by classical single-layer polynomial approximation for cusp-type functions. Numerical experiments for both single and multiple cusp configurations confirm the theoretical rates and demonstrate the parameter efficiency of deep composite polynomial constructions.

</details>


### [10] [Newton-Krylov Methods for Computing Steady States of Particle Timesteppers via Optimal Transport](https://arxiv.org/abs/2512.24567)
*Hannes Vandecasteele,Nicholas Karris,Alexander Cloninger,Ioannis G. Kevrekidis*

Main category: math.NA

TL;DR: A matrix-free framework for computing steady-state distributions of stochastic particle systems using timesteppers reformulated as operators on probability measures via optimal transport, enabling efficient computation even under high noise.


<details>
  <summary>Details</summary>
Motivation: Stochastic particle simulations have intrinsic randomness that prevents direct steady state extraction, unlike deterministic systems where timesteppers can be used to compute steady states. There's a need to extend matrix-free steady state computation methods to handle stochastic systems with high noise.

Method: Reformulate stochastic timesteppers as operators acting on probability measures using optimal transport theory. Construct smooth cumulative- and inverse-cumulative-distribution-function ((I)CDF) timesteppers that evolve distributions rather than individual particles. Combine with matrix-free Newton-Krylov solvers for efficient computation.

Result: The framework enables efficient computation of steady-state distributions even under high stochastic noise. Error analysis quantifies how noise affects finite-difference Jacobian approximations, showing convergence is possible in high noise regimes. Higher-dimensional generalizations work on non-trivial two-dimensional distributions.

Conclusion: Establishes a unified variational framework for computing meaningful steady states of both deterministic and stochastic timesteppers, bridging the gap between deterministic steady state computation and stochastic particle simulations.

Abstract: Timesteppers constitute a powerful tool in modern computational science and engineering. Although they are typically used to advance the system forward in time, they can also be viewed as nonlinear mappings that implicitly encode steady states and stability information. In this work, we present an extension of the matrix-free framework for calculating, via timesteppers, steady states of deterministic systems to stochastic particle simulations, where intrinsic randomness prevents direct steady state extraction. By formulating stochastic timesteppers in the language of optimal transport, we reinterpret them as operators acting on probability measures rather than on individual particle trajectories. This perspective enables the construction of smooth cumulative- and inverse-cumulative-distribution-function ((I)CDF) timesteppers that evolve distributions rather than particles. Combined with matrix-free Newton-Krylov solvers, these smooth timesteppers allow efficient computation of steady-state distributions even under high stochastic noise. We perform an error analysis quantifying how noise affects finite-difference Jacobian action approximations, and demonstrate that convergence can be obtained even in high noise regimes. Finally, we introduce higher-dimensional generalizations based on smooth CDF-related representations of particles and validate their performance on a non-trivial two-dimensional distribution. Together, these developments establish a unified variational framework for computing meaningful steady states of both deterministic and stochastic timesteppers.

</details>


### [11] [Solving the inverse Source Problems for wave equation with final time measurements by a data driven approach](https://arxiv.org/abs/2512.24647)
*Qiling Gu,Wenlong Zhang,Zhidong Zhang*

Main category: math.NA

TL;DR: A discrete data-driven approach for solving wave equation inverse source problems using L²-Tikhonov regularization with final time measurements, establishing convergence rates under different noise models without requiring classical source conditions.


<details>
  <summary>Details</summary>
Motivation: To develop a robust data-driven method for solving inverse source problems in wave equations using discrete observations, addressing challenges with noise in measurements and avoiding restrictive classical source conditions.

Method: Uses L²-Tikhonov regularization with spectral decomposition analysis, introduces noise separation technique in variational framework, extends to fully discrete case with finite element discretization, and provides error bounds for both solution and source term reconstruction.

Result: Establishes error bounds for reconstructed solution u and source term f, derives expected convergence rate for source error in weaker topology, shows overall error depends on noise level, regularization parameter, time step size, and spatial mesh size, enabling data-driven parameter selection.

Conclusion: The proposed discrete data-driven approach effectively solves wave equation inverse source problems with final time measurements, providing rigorous convergence analysis and practical parameter selection methods validated by numerical experiments.

Abstract: This paper develops a discrete data-driven approach for solving the inverse source problem of the wave equation with final time measurements. Focusing on the $L^2$-Tikhonov regularization method, we analyze its convergence under two different noise models, using noisy discrete spatial observations. By exploiting the spectral decomposition of the forward operator and introducing a noise separation technique into the variational framework, we establish error bounds for the reconstructed solution $u$ and the source term $f$ without requiring classical source conditions. Moreover, an expected convergence rate for the source error is derived in a weaker topology. We also extend the analysis to the fully discrete case with finite element discretization, showing that the overall error depends only on the noise level, regularization parameter, time step size, and spatial mesh size. These estimates provide a basis for selecting the optimal regularization parameter in a data-driven manner, without a priori information. Numerical experiments validate the theoretical results and demonstrate the efficiency of the proposed algorithm.

</details>


### [12] [Boundary error control for numerical solution of BSDEs by the convolution-FFT method](https://arxiv.org/abs/2512.24714)
*Xiang Gao,Cody Hyndman*

Main category: math.NA

TL;DR: Improved CFFT method for BSDEs with better boundary error handling through modified damping and shifting schemes


<details>
  <summary>Details</summary>
Motivation: The original CFFT approach for solving BSDEs suffers from boundary errors when valuing options, which need to be reduced for better accuracy

Method: Modified damping and shifting schemes in the CFFT formulation, specifically introducing time-dependent shifting to transform target functions into bounded periodic functions suitable for Fourier transforms

Result: Significant reduction in boundary errors, improved accuracy and convergence demonstrated through numerical results and detailed error analysis

Conclusion: The modified convolution method with time-dependent shifting provides superior performance for option valuation using BSDEs compared to the original CFFT approach

Abstract: We first review the convolution fast-Fourier-transform (CFFT) approach for the numerical solution of backward stochastic differential equations (BSDEs) introduced in (Hyndman and Oyono Ngou, 2017). We then propose a method for improving the boundary errors obtained when valuing options using this approach. We modify the damping and shifting schemes used in the original formulation, which transforms the target function into a bounded periodic function so that Fourier transforms can be applied successfully. Time-dependent shifting reduces boundary error significantly. We present numerical results for our implementation and provide a detailed error analysis showing the improved accuracy and convergence of the modified convolution method.

</details>


### [13] [A structure-preserving parametric approximation for anisotropic geometric flows via an $α$-surface energy matrix](https://arxiv.org/abs/2512.24875)
*Weizhu Bao,Yifei Li,Wenjun Ying,Yulin Zhang*

Main category: math.NA

TL;DR: Proposes structure-preserving parametric approximation for anisotropic geometric flows with optimal energy stability at α=-1


<details>
  <summary>Details</summary>
Motivation: Existing formulations for anisotropic geometric flows lack unified framework with optimal energy stability properties. Need systematic approach to handle general anisotropic effects in geometric flows while preserving structure.

Method: Introduces hyperparameter α to construct unified surface energy matrix Ĝₖᵅ(θ) encompassing all existing formulations. Applies to anisotropic curvature flow and proves α=-1 achieves optimal energy stability under minimal condition 3γ̂(θ)≥γ̂(θ-π). Extends to general anisotropic flows via unified velocity discretization.

Result: Proves α=-1 is unique choice achieving optimal energy stability with weakest condition. Other α≠-1 require strictly stronger conditions. Numerical experiments validate theoretical optimality of α=-1 and demonstrate effectiveness and robustness.

Conclusion: Provides unified framework for anisotropic geometric flows with structure-preserving properties. α=-1 is theoretically optimal for energy stability, offering practical advantages for numerical simulations of anisotropic geometric flows.

Abstract: We propose a structure-preserving parametric approximation for geometric flows with general anisotropic effects. By introducing a hyperparameter $α$, we construct a unified surface energy matrix $\hat{\boldsymbol{G}}_k^α(θ)$ that encompasses all existing formulations of surface energy matrices, and apply it to anisotropic curvature flow. We prove that $α=-1$ is the unique choice achieving optimal energy stability under the necessary and sufficient condition $3\hatγ(θ)\geq\hatγ(θ-π)$, while all other $α\neq-1$ require strictly stronger conditions. The framework extends naturally to general anisotropic geometric flows through a unified velocity discretization that ensures energy stability. Numerical experiments validate the theoretical optimality of $α=-1$ and demonstrate the effectiveness and robustness.

</details>


### [14] [Random compressible Euler flows](https://arxiv.org/abs/2512.24879)
*Maria Lukacova-Medvidova,Simon Schneider*

Main category: math.NA

TL;DR: Finite volume stochastic collocation method for random Euler system with rigorous convergence proof under bounded discrete differential quotients assumption.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method for solving random Euler systems (compressible fluid dynamics with uncertainty) that combines deterministic finite volume methods with stochastic collocation techniques, and to provide rigorous mathematical convergence analysis for such methods.

Method: Finite volume stochastic collocation method that combines deterministic finite volume discretization with stochastic collocation approach. Convergence analysis uses deterministic FV convergence results combined with stochastic compactness arguments (Skorokhod and Gyöngy-Krylov theorems).

Result: Rigorous proof of convergence for random finite volume solutions under the assumption that discrete differential quotients remain bounded in probability.

Conclusion: The proposed method provides a mathematically sound approach for solving random Euler systems with guaranteed convergence under appropriate conditions, bridging deterministic numerical methods with stochastic analysis techniques.

Abstract: We propose a finite volume stochastic collocation method for the random Euler system. We rigorously prove the convergence of random finite volume solutions under the assumption that the discrete differential quotients remain bounded in probability. Convergence analysis combines results on the convergence of a deterministic FV method with stochastic compactness arguments due to Skorokhod and Gyöngy-Krylov.

</details>


### [15] [A finite element approach for minimizing line and surface energies arising in the study of singularities in liquid crystals](https://arxiv.org/abs/2512.24928)
*Dominik Stantejsky*

Main category: math.NA

TL;DR: Numerical algorithm for Plateau-like problem with area, boundary length, and obstacle constraints, applied to defect structures in nematic liquid crystals.


<details>
  <summary>Details</summary>
Motivation: Study of defect structures in nematic liquid crystals motivates development of numerical algorithm for Plateau-like problems with physical constraints.

Method: ADMM-based algorithm using finite elements to minimize discretized energy containing surface area, boundary length, obstacle constraints, and surface energy on obstacle.

Result: Algorithm successfully handles different inclusion shapes, reveals rich minimizing configurations, and provides physical interpretation for colloidal particles in nematic liquid crystals.

Conclusion: Generalized TV-minimization approach effectively solves complex Plateau-like problems with applications in liquid crystal physics and colloidal systems.

Abstract: Motivated by a problem originating in the study of defect structures in nematic liquid crystals, we describe and study a numerical algorithm for the resolution of a Plateau-like problem. The energy contains the area of a two-dimensional surface $T$ and the length of its boundary $\partial T$ reduced by a prescribed curve to make our problem non-trivial. We additionally include an obstacle $E$ for $T$ and pose a surface energy on $E$. We present an algorithm based on the Alternating Direction Method of Multipliers that minimizes a discretized version of the energy using finite elements, generalizing existing TV-minimization methods. We study different inclusion shapes demonstrating the rich structure of minimizing configurations and provide physical interpretation of our findings for colloidal particles in nematic liquid crystal.

</details>


### [16] [Approximating evolution operators of linear delay equations: a general framework for the convergence analysis](https://arxiv.org/abs/2512.24964)
*Alessia andò,Giusy Bosco,Dimitri Breda,Davide Liessi*

Main category: math.NA

TL;DR: A general convergence analysis framework for discretizing evolution operators of linear delay equations to approximate their spectra, unifying existing methods and providing formal convergence analysis for previously unanalyzed methods.


<details>
  <summary>Details</summary>
Motivation: To investigate stability properties of nonlinear delay equations via linearized stability principle by approximating spectra of linear delay equations, and to provide unified convergence analysis for various discretization methods.

Method: Develops a general convergence analysis framework based on reformulating evolution operators using fixed-point equations, with hypotheses related to regularization properties and convergence of approximation techniques on suitable subspaces.

Result: The framework unifies proofs for pseudospectral discretization methods and provides formal convergence analysis for a weighted residuals method that previously lacked such analysis.

Conclusion: The proposed framework offers a general approach for analyzing convergence of discretization methods for linear delay equations, demonstrating its generality by applying it to both existing and previously unanalyzed methods.

Abstract: We consider the problem of discretizing evolution operators of linear delay equations with the aim of approximating their spectra, which is useful in investigating the stability properties of (nonlinear) equations via the principle of linearized stability. We develop a general convergence analysis based on a reformulation of the operators by means of a fixed-point equation, providing a list of hypotheses related to the regularization properties of the equation and the convergence of the chosen approximation techniques on suitable subspaces. This framework unifies the proofs for some methods based on pseudospectral discretization, which we present here in this new form. To exemplify the generality of the framework, we also apply it to a method of weighted residuals found in the literature, which was previously lacking a formal convergence analysis.

</details>


### [17] [At the intersection of Numerical Analysis and Spectral Geometry](https://arxiv.org/abs/2512.25012)
*Nilima Nigam*

Main category: math.NA

TL;DR: Survey paper exploring the intersection of spectral geometry and numerical analysis, focusing on eigenvalue approximation methods and their applications in both conjecture formulation and proof strategies.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between spectral geometry (studying how domain geometry affects operator spectra) and numerical analysis (computing accurate approximations), examining how computational methods can both guide conjectures and serve as proof tools.

Method: Expository survey approach reviewing existing discretization methods and approximation strategies, analyzing trade-offs between efficiency/accuracy for conjecture formulation versus rigorous error bounds for proof strategies.

Result: Identifies that computational spectral geometry requires different approaches depending on objectives: rapid specialized methods for conjecture formulation versus schemes with guaranteed error bounds for proof strategies.

Conclusion: Computational spectral geometry represents a fruitful intersection where spectral geometry's demanding requirements motivate new numerical analysis developments, while validated computations enable new proof strategies in spectral geometry.

Abstract: How do the geometric properties of a domain impact the spectrum of an operator defined on it? How do we compute accurate and reliable approximations of these spectra? The former question is studied in spectral geometry, and the latter is a central concern in numerical analysis. In this short expository survey we revisit the process of eigenvalue approximation, from the perspective of computational spectral geometry. Over the years a multitude of methods -- for discretizing the operator and for the resultant discrete system -- have been developed and analyzed in the field of numerical analysis. High-accuracy and provably convergent discretization approaches can be used to examine the interplay between the spectrum of an operator and the geometric properties of the spatial domain or manifold it is defined on. While computations have been used to guide conjectures in spectral geometry, in recent years approximation-theoretic tools and validated computations are also being used as part of proof strategies in spectral geometry.
  Given a particular spectral feature of interest, should we discretize the original problem, or seek a reformulation? Of the many possible approximation strategies, which should we choose? These choices are inextricably linked to the objective: on the one hand, rapid, specialized methods are often ideal for conjecture formulation (prioritizing efficiency and accuracy), whereas schemes with guaranteed, computable error bounds are needed when computation is incorporated into a proof strategy. We also review instances where the demanding requirements of spectral geometry -- the need for rigorous error control or the robust calculation of higher eigenvalues -- motivate new developments in numerical analysis.

</details>


### [18] [Convergence of the generalization error for deep gradient flow methods for PDEs](https://arxiv.org/abs/2512.25017)
*Chenguang Liu,Antonis Papapantoleon,Jasper Rou*

Main category: math.NA

TL;DR: DGFMs for PDEs: generalization error → 0 as neurons & training time → ∞ via approximation error (neural networks can approximate PDE solutions) and training error analysis (gradient flow convergence).


<details>
  <summary>Details</summary>
Motivation: Provide rigorous mathematical foundation for deep gradient flow methods (DGFMs) solving high-dimensional PDEs, addressing lack of theoretical guarantees for these methods.

Method: Decompose generalization error into approximation error (neural networks approximating PDE solutions) and training error (analyzing gradient flow dynamics in wide network limit with infinite training time).

Result: 1) PDE solutions can be approximated by neural networks (approximation error → 0 as neurons → ∞). 2) Training gradient flow converges in wide network limit. 3) Combined: generalization error → 0 as neurons & training time → ∞.

Conclusion: DGFMs provide theoretically sound approach for solving PDEs with provable convergence guarantees, establishing mathematical foundation for these methods in high-dimensional settings.

Abstract: The aim of this article is to provide a firm mathematical foundation for the application of deep gradient flow methods (DGFMs) for the solution of (high-dimensional) partial differential equations (PDEs). We decompose the generalization error of DGFMs into an approximation and a training error. We first show that the solution of PDEs that satisfy reasonable and verifiable assumptions can be approximated by neural networks, thus the approximation error tends to zero as the number of neurons tends to infinity. Then, we derive the gradient flow that the training process follows in the ``wide network limit'' and analyze the limit of this flow as the training time tends to infinity. These results combined show that the generalization error of DGFMs tends to zero as the number of neurons and the training time tend to infinity.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [19] [Fractal Mehler kernels and nonlinear geometric flows](https://arxiv.org/abs/2512.23830)
*Nicola Garofalo*

Main category: math.AP

TL;DR: The paper introduces a new two-parameter family of Mehler kernels and establishes connections to Baouendi-Grushin flows in fractal dimensions, plus links to geometric fully nonlinear equations with open questions.


<details>
  <summary>Details</summary>
Motivation: To develop mathematical connections between Mehler kernels, Baouendi-Grushin flows in fractal settings, and geometric fully nonlinear equations, exploring new relationships in analysis and geometry.

Method: Introduces a two-parameter family of Mehler kernels and establishes theoretical connections to Baouendi-Grushin flows in fractal dimensions, while also identifying links with geometric fully nonlinear equations.

Result: Establishes novel connections between Mehler kernels, Baouendi-Grushin flows in fractal dimensions, and geometric fully nonlinear equations, presenting two open questions for further research.

Conclusion: The paper successfully bridges several mathematical areas through the introduction of new Mehler kernels and their connections to Baouendi-Grushin flows and geometric equations, while identifying important open problems for future investigation.

Abstract: In this paper we introduce a two-parameter family of Mehler kernels and connect them to a class of Baouendi-Grushin flows in fractal dimension. We also highlight a link with a geometric fully nonlinear equation and formulate two questions.

</details>


### [20] [A nonlinear instability result to the Navier-Stokes equations with Navier slip boundary conditions](https://arxiv.org/abs/2512.23946)
*Tien-Tai Nguyen*

Main category: math.AP

TL;DR: The paper proves linear and nonlinear instability of trivial steady states for incompressible viscous fluids with Navier-slip boundary conditions using operator methods and adapting viscous boundary layer frameworks.


<details>
  <summary>Details</summary>
Motivation: To investigate instability of trivial steady states in incompressible viscous fluids with Navier-slip boundary conditions, providing a different approach from previous work by Ding, Li and Xin (2018).

Method: Uses operator method of Lafitte and Nguyen (2022) to show existence of infinitely many normal mode solutions for linear instability, then adapts framework of Desjardins and Grenier (2003) for viscous boundary layers to obtain two separated solutions at escaping time for nonlinear instability.

Result: Proves both linear instability (existence of infinitely many normal mode solutions) and nonlinear instability of trivial steady states for the fluid system with Navier-slip boundary conditions.

Conclusion: The paper establishes instability results for incompressible viscous fluids with Navier-slip boundary conditions using a novel approach that differs from previous work, successfully demonstrating both linear and nonlinear instability.

Abstract: In this paper, we investigate the instability of the trivial steady states to the incompressible viscous fluid with Navier-slip boundary conditions. For the linear instability, the existence of infinitely many normal mode solutions to the linearized equations is shown via the operator method of Lafitte and Nguyen (2022). Hence, we prove the nonlinear instability by adapting the framework of Desjardins and Grenier (2003) studying some classes of viscous boundary layers to obtain two separated solutions at escaping time. Our work performs a different approach from that of Ding, Li and Xin (2018).

</details>


### [21] [A regularity theory for second-order parabolic partial differential equations in weighted mixed norm Sobolev-Zygmund spaces](https://arxiv.org/abs/2512.24020)
*Jae-Hwan Choi,Junhee Ryu*

Main category: math.AP

TL;DR: Optimal regularity theory for parabolic PDEs in weighted mixed norm Sobolev-Zygmund spaces, extending Schauder estimates to time-measurable coefficients and integer-order regularity, with sharp trace theorem for initial data.


<details>
  <summary>Details</summary>
Motivation: To extend classical Schauder estimates for parabolic PDEs to more general settings: coefficients that are only measurable in time (not necessarily continuous), the critical case of integer-order regularity, and proper treatment of nonzero initial data in optimal trace spaces.

Method: Develops regularity theory in weighted mixed norm Sobolev-Zygmund spaces, uses sharp trace theorem to handle initial data, and extends classical parabolic Schauder estimates to more general coefficient conditions.

Result: Establishes optimal regularity results for parabolic PDEs with time-measurable coefficients, covers the critical integer-order regularity case, and provides a sharp trace theorem for initial data treatment.

Conclusion: The paper successfully extends parabolic Schauder theory to broader coefficient conditions and regularity cases while providing optimal trace space treatment for initial data, advancing the regularity theory for parabolic PDEs.

Abstract: We develop an optimal regularity theory for parabolic partial differential equations in weighted mixed norm Sobolev-Zygmund spaces. The results extend the classical Schauder estimates to coefficients that are merely measurable in time and to the critical case of integer-order regularity. In addition, nonzero initial data are treated in the optimal trace space via a sharp trace theorem.

</details>


### [22] [$L^p$ Estimates for Numerical Approximation of Hamilton-Jacobi Equations](https://arxiv.org/abs/2512.24051)
*Alessio Basti,Fabio Camilli*

Main category: math.AP

TL;DR: L^p error estimates for monotone schemes approximating Hamilton-Jacobi equations on torus using adjoint method


<details>
  <summary>Details</summary>
Motivation: Need better error estimates for numerical schemes solving Hamilton-Jacobi equations, improving existing results and providing unified framework

Method: Adjoint method to prove L^1 error bound of order one for finite-difference and semi-Lagrangian schemes under convexity assumptions, then interpolation for L^p estimates

Result: Established L^p error estimates for monotone schemes, improved existing results, provided unified framework covering broad class of schemes

Conclusion: Adjoint method provides effective unified approach for discrete error estimates of Hamilton-Jacobi equations on torus with improved results

Abstract: We establish $L^p$ error estimates for monotone numerical schemes approximating Hamilton-Jacobi equations on the $d$-dimensional torus. Using the adjoint method, we first prove a $L^1$ error bound of order one for finite-difference and semi-Lagrangian schemes under standard convexity assumptions on the Hamiltonian. By interpolation, we also obtain $L^p$ estimates for every finite $p>1$. Our analysis covers a broad class of schemes, improves several existing results, and provides a unified framework for discrete error estimates.

</details>


### [23] [Propagation of chaos for the homogeneous Boltzmann equation with moderately soft potentials](https://arxiv.org/abs/2512.24065)
*Nicolas Fournier,Stéphane Mischler*

Main category: math.AP

TL;DR: The paper proves that the Kac particle system converges to the Boltzmann equation for moderately soft potentials, establishing propagation of chaos.


<details>
  <summary>Details</summary>
Motivation: To rigorously connect microscopic particle systems (Kac model) with macroscopic kinetic theory (Boltzmann equation) for moderately soft potentials, proving propagation of chaos.

Method: Adapts recent work by Imbert, Silvestre and Villani to show Fisher information is nonincreasing along solutions to the Kac master equation, using this estimate to control interaction singularities.

Result: Proves convergence of Kac particle system to homogeneous Boltzmann equation as number of particles tends to infinity for moderately soft potentials (γ ∈ (-2,0)), establishing propagation of chaos.

Conclusion: The Fisher information approach successfully controls singularity issues, enabling rigorous proof of convergence from microscopic particle systems to macroscopic kinetic equations for moderately soft potentials.

Abstract: We show that the Kac particle system converges, as the number of particles tends to infinity, to the solution of the homogeneous Boltzmann equation, in the regime of moderately soft potentials, $γ\in (-2,0)$ with the common notation. This proves the propagation of chaos. We adapt the recent work of Imbert, Silvestre and Villani, to show that the Fisher information is nonincreasing in time along solutions to the Kac master equation. This estimate allows us to control the singularity of the interaction.

</details>


### [24] [Dirac solitons in one-dimensional nonlinear Schrödinger equations](https://arxiv.org/abs/2512.24089)
*William Borrelli,Elena Danesi,Simone Dovetta,Lorenzo Tentarelli*

Main category: math.AP

TL;DR: The paper studies stationary cubic NLS equations with periodic potentials featuring Dirac points, constructs "Dirac solitons" via perturbation theory, and rigorously justifies the NLD equation as an effective model for NLS.


<details>
  <summary>Details</summary>
Motivation: To understand how nonlinear standing waves (solitons) can be constructed in periodic potentials with Dirac points, and to provide rigorous justification for using the nonlinear Dirac equation as an effective model for the original nonlinear Schrödinger equation.

Method: Introduce periodic perturbations to open spectral gaps around Dirac-point energies, then construct standing waves whose leading-order profile is a modulation of Bloch waves using components of a spinor solving an appropriate cubic nonlinear Dirac equation.

Result: Successfully construct "Dirac solitons" - standing wave solutions to the NLS equation with periodic potentials, and rigorously demonstrate that the nonlinear Dirac equation serves as an effective model for the original NLS equation.

Conclusion: The analysis provides rigorous mathematical justification for using the nonlinear Dirac equation as an effective model for studying nonlinear Schrödinger equations with periodic potentials featuring Dirac points, enabling construction of Dirac solitons through perturbation theory.

Abstract: In this paper we study a family of one-dimensional stationary cubic nonlinear Schrödinger (NLS) equations with periodic potentials and linear part displaying Dirac points in the dispersion relation. By introducing a suitable periodic perturbation, one can open a spectral gap around the Dirac-point energy. This allows to construct standing waves of the NLS equation whose leading-order profile is a modulation of Bloch waves by means of the components of a spinor solving an appropriate cubic nonlinear Dirac (NLD) equation. We refer to these solutions as Dirac solitons. Our analysis thus provides a rigorous justification for the use of the NLD equation as an effective model for the original NLS equation.

</details>


### [25] [An Equivalence Result on the Order of Differentiability in Frobenius' Theorem](https://arxiv.org/abs/2512.24218)
*Yuhki Hosoya*

Main category: math.AP

TL;DR: This paper analyzes total differential equations in foliation theory without smoothness assumptions, revealing an asymmetry in solution differentiability. It shows that integral manifolds have higher regularity than solutions, provides counterexamples, and characterizes quasi-convex solutions to optimization problems.


<details>
  <summary>Details</summary>
Motivation: To understand the differentiability properties of solutions to total differential equations in foliation theory when smoothness assumptions are relaxed, addressing the peculiar asymmetry that arises between solution regularity and integral manifold regularity.

Method: Examines total differential equations without smoothness assumptions, analyzes differentiability of integral manifolds, provides counterexamples for regularity gaps, and characterizes quasi-convex solutions through necessary and sufficient conditions.

Result: When system is locally Lipschitz, solutions are only locally Lipschitz but integral manifolds must be C¹. For C^k systems, solutions are C^k but integral manifolds must be C^{k+1}. Counterexample shows C¹ system without C² solution. Characterizes quasi-convex solutions to optimization problems.

Conclusion: There exists a fundamental asymmetry in differentiability between solutions and integral manifolds in total differential equations. Integral manifolds exhibit higher regularity than solutions, and this regularity gap is characterized. The analysis provides conditions for quasi-convex solutions in optimization problems involving total differential equations.

Abstract: This paper examines the simplest case of total differential equations that appears in the theory of foliation structures, without imposing the smoothness assumptions. This leads to a peculiar asymmetry in the differentiability of solutions. To resolve this asymmetry, this paper focuses on the differentiability of the integral manifold. When the system is locally Lipschitz, a solution is ensured to be only locally Lipschitz, but the integral manifolds must be $C^1$. When the system is $C^k$, we can only ensure the existence of a $C^k$ solution, but the integral manifolds must be $C^{k+1}$. In addition, we see a counterexample in which the system is $C^1$, but there is no $C^2$ solution. Moreover, we characterize a minimizer of an optimization problem whose objective function is a quasi-convex solution to a total differential equation. In this connection, we examine two necessary and sufficient conditions for the system in which any solution is quasi-convex.

</details>


### [26] [Multi-bump solutions for sublinear elliptic equations with nonsymmetric coefficients](https://arxiv.org/abs/2512.24234)
*Chengxiang Zhang,Xu Zhang*

Main category: math.AP

TL;DR: The paper proves existence of infinitely many nonnegative bump solutions to a sublinear elliptic equation with nonsymmetric potential, using sharp support estimates to control bump interactions.


<details>
  <summary>Details</summary>
Motivation: To study existence of multiple bump solutions for sublinear elliptic equations with nonsymmetric potentials, overcoming challenges from sensitive bump interactions due to compact support of limiting profiles.

Method: Uses truncated functional space approach with qualitative local stability estimates in region-wise maximum norms to control bump support sets, ensuring effective separation and minimizing overlap between bumps.

Result: Constructs infinitely many nonnegative bump solutions with arbitrarily many bumps when the potential K is close to 1 in local L^p norm, with uniform estimates independent of bump number.

Conclusion: The method successfully handles sensitive bump interactions in sublinear elliptic equations with nonsymmetric potentials, establishing existence of solutions with infinitely many bumps through precise control of support sets.

Abstract: We investigate the existence of nonnegative bump solutions to the sublinear elliptic equation \[ \begin{cases} -Δv - K(x)v + |v|^{q-2}v = 0 & \text{in } \mathbb{R}^N, \\ v(x) \to 0 & \text{as } |x| \to \infty, \end{cases} \] where $q \in (1,2)$, $ N \geq 2$, and the potential $K \in L^p_{\mathrm{loc}}(\mathbb{R}^N)$ with $p > N/2$ is a function without any symmetry assumptions. Under the condition that $\|K - 1\|_{L^p_{\mathrm{loc}}}$ is sufficiently small, we construct infinitely many solutions with arbitrarily many bumps. The construction is challenged by the sensitive interaction between bumps, whose limiting profiles have compact support. The key to ensuring their effective separation lies in obtaining sharp estimates of the support sets. Our method, based on a truncated functional space, provides precisely such control. We derive qualitative local stability estimates in region-wise maximum norms that govern the size of each bump's essential support, confining its core to a designated region and minimizing overlap. Crucially, these estimates are uniform in the number of bumps, which is the pivotal step in establishing the existence of solutions with infinitely many bumps.

</details>


### [27] [Mean-Field Limits of Deterministic and Stochastic Flocking Models with Nonlinear Velocity Alignment](https://arxiv.org/abs/2512.24383)
*Vinh Nguyen,Roman Shvydkoy,Changhui Tan*

Main category: math.AP

TL;DR: Mean-field limit analysis for flocking models with nonlinear velocity alignment, extending Cucker-Smale theory to nonlinear framework with improved convergence rates.


<details>
  <summary>Details</summary>
Motivation: Extend classical Cucker-Smale flocking theory to nonlinear velocity alignment framework, which has received considerable recent attention in literature.

Method: Study agent-based models with nonlinear velocity coupling A(v)=|v|^{p-2}v (p>2) and communication protocol φ. Prove mean-field limits in deterministic and stochastic settings, with quantitative propagation of chaos estimates for deterministic case using fat-tailed kernels.

Result: Established mean-field limits for both deterministic and stochastic cases. For deterministic case with fat-tailed kernels, obtained improved convergence rates for k-particle marginals to Vlasov equation solutions. Stochastic version leads to Fokker-Planck-Alignment equation with multiplicative noise depending on local interaction intensity.

Conclusion: Successfully extended Cucker-Smale theory to nonlinear framework, providing rigorous mean-field analysis with quantitative convergence results for flocking models with nonlinear velocity alignment.

Abstract: We study the mean-field limit for a class of agent-based models describing flocking with nonlinear velocity alignment. Each agent interacts through a communication protocol $φ$ and a non-linear coupling of velocities given by the power law $A(\bv) = |\bv|^{p-2}\bv$, $p > 2$. The mean-field limit is proved in two settings -- deterministic and stochastic. We then provide quantitative estimates on propagation of chaos for deterministic case in the case of the classical fat-tailed kernels, showing an improved convergence rate of the $k$-particle marginals to a solution of the corresponding Vlasov equation. The stochastic version is addressed with multiplicative noise depending on the local interaction intensity, which leads to the associated Fokker-Planck-Alignment equation.
  Our results extend the classical Cucker-Smale theory to the nonlinear framework which has received considerable attention in the literature recently.

</details>


### [28] [Stability of the reconstruction of the heat reflection coefficient in the phonon transport equation](https://arxiv.org/abs/2512.24394)
*Peiyi Chen,Irene M. Gamba,Qin Li,Anjali Nair*

Main category: math.AP

TL;DR: The paper analyzes stability of inverse problem for determining reflection coefficient in phonon transport, showing ill-posedness in ballistic-to-diffusive transition as Knudsen number approaches zero.


<details>
  <summary>Details</summary>
Motivation: The reflection coefficient is crucial for thermal properties at nanoscale, but determining it requires solving inverse problems from temperature measurements. Previous studies show discrepancies about well-posedness of this inverse problem.

Method: Investigates stability of inverse problem for inferring reflection coefficient in phonon transport equation. Analyzes how stability changes as system transitions from ballistic to diffusive regime using Knudsen number analysis.

Result: The inverse problem becomes ill-posed as Knudsen number converges to zero (transition to diffusive regime). Quantifies rate of stability deterioration with respect to Knudsen number and provides numerical evidence confirming theoretical findings.

Conclusion: The stability analysis clarifies discrepancies in previous studies about well-posedness of the inverse problem for reflection coefficient determination, showing systematic deterioration of stability in diffusive regime.

Abstract: The reflection coefficient is an important thermal property of materials, especially at the nanoscale, and determining this property requires solving an inverse problem based on macroscopic temperature measurements. In this manuscript, we investigate the stability of this inverse problem to infer the reflection coefficient in the phonon transport equation. We show that the problem becomes ill-posed as the system transitions from the ballistic to the diffusive regime, characterized by the Knudsen number converging to zero. Such a stability estimate clarifies the discrepancy observed in previous studies on the well-posedness of this inverse problem. Furthermore, we quantify the rate at which the stability deteriorates with respect to the Knudsen number and confirm the theoretical result with numerical evidence.

</details>


### [29] [Solvability conditions for some non-Fredholm operators with shifted arguments](https://arxiv.org/abs/2512.24476)
*Vitali Vougalter,Vitaly Volpert*

Main category: math.AP

TL;DR: Existence and convergence results for solutions of nonhomogeneous linear differential and integro-differential equations with argument translation in H²(R) space.


<details>
  <summary>Details</summary>
Motivation: To establish existence and convergence properties for solutions of equations involving argument translation (shifted arguments), which is important for understanding the behavior of differential and integro-differential equations with delayed or advanced terms.

Method: Two-part approach: 1) For linear differential equations, show that L² convergence of source terms implies H² convergence of solutions. 2) For integro-differential equations, demonstrate that L¹ convergence of integral kernels yields H² convergence of solutions. Both involve second-order differential operators whose Fredholm property depends on the translation constant.

Result: Proved existence in the sense of sequences of solutions in H²(R) for both types of equations. Established that under reasonable technical conditions, convergence of source terms/kernels in appropriate spaces (L² or L¹) implies existence and convergence of solutions in H²(R).

Conclusion: The paper successfully establishes solvability and convergence results for differential and integro-differential equations with argument translation, providing important theoretical foundations for analyzing such equations in Sobolev spaces.

Abstract: In the first part of the article we establish the existence in the sense of sequences of solutions in $H^{2}(R)$ for some nonhomogeneous linear differential equation in which one of the terms has the argument translated by a constant. It is shown that under the reasonable technical conditions the convergence in $L^{2}(R)$ of the source terms implies the existence and the convergence in $H^{2}(R)$ of the solutions. The second part of the work deals with the solvability in the sense of sequences in $H^{2}(R)$ of the integro-differential equation in which one of the terms has the argument shifted by a constant. It is demonstrated that under the appropriate auxiliary assumptions the convergence in $L^{1}(R)$ of the integral kernels yields the existence and the convergence in $H^{2}(R)$ of the solutions. Both equations considered involve the second order differential operator with or without the Fredholm property depending on the value of the constant by which the argument gets translated.

</details>


### [30] [Steady Self-Propelled Motion of a Rigid Body in a Viscous Fluid with Navier-Slip Boundary Conditions](https://arxiv.org/abs/2512.24510)
*Sarka Necasova,Arnab Roy,Ana Leonor Silvestre*

Main category: math.AP

TL;DR: Existence of steady self-propelled motion for a rigid body in viscous fluid with Navier-slip boundary conditions, establishing weak solutions and propulsion conditions.


<details>
  <summary>Details</summary>
Motivation: To understand propulsion mechanisms in microfluidic and rough-surface regimes where partial slip effects are significant, extending classical Dirichlet-based theory to more realistic Navier-slip boundary conditions.

Method: Analysis in body-fixed reference frame with fluid in exterior domain, using nonhomogeneous Navier-slip boundary conditions. Derivation of Korn-type inequality for exterior domains with rigid-body motion and Navier-slip interfaces. Introduction of finite-dimensional thrust space via auxiliary exterior Stokes problems with Navier boundary conditions.

Result: Established existence of weak steady solutions under smallness assumptions on boundary flux and normal surface velocity. Provided necessary and sufficient condition for slip velocity to induce nontrivial translational/rotational motion through thrust space analysis.

Conclusion: The work clarifies how boundary effects generate propulsion and extends classical Dirichlet-based theory to Navier-slip setting, providing analytical framework for understanding propulsion in microfluidic and rough-surface regimes.

Abstract: We investigate the steady self-propelled motion of a rigid body immersed in a three-dimensional incompressible viscous fluid governed by the Navier-Stokes equations. The analysis is performed in a body-fixed reference frame, so that the fluid occupies an exterior domain and the propulsion mechanism is modeled through nonhomogeneous Navier-slip boundary conditions at the fluid-body interface. Such conditions provide a realistic description of propulsion in microfluidic and rough-surface regimes, where partial slip effects are significant. Under suitable smallness assumptions on the boundary flux and on the normal component of the prescribed surface velocity, we establish the existence of weak steady solutions to the coupled fluid-structure system. A key analytical ingredient is the derivation of a Korn-type inequality adapted to exterior domains with rigid-body motion and Navier-slip interfaces, which yields uniform control of both the fluid velocity and the translational and rotational velocities of the body. Beyond existence, we provide a necessary and sufficient condition under which a prescribed slip velocity on the body surface induces nontrivial translational or rotational motion of the rigid body. This is achieved through the introduction of a finite-dimensional thrust space, defined via auxiliary exterior Stokes problems with Navier boundary conditions, which captures the effective contribution of boundary-driven flows to the rigid-body motion. Our results clarify how boundary effects generate propulsion and extend the classical Dirichlet-based theory to the Navier-slip setting.

</details>


### [31] [Anomalous Dissipation at Onsager-Critical Regularity](https://arxiv.org/abs/2512.24568)
*Alexey Cheskidov,Qirui Peng*

Main category: math.AP

TL;DR: Construction of 3D Euler solutions with anomalous dissipation via vanishing viscosity limit, extending 2.5D constructions and establishing sharp Onsager-critical energy criterion.


<details>
  <summary>Details</summary>
Motivation: To demonstrate existence of dissipative Euler solutions in finite time through vanishing viscosity limit, addressing Onsager's conjecture and anomalous dissipation phenomena in 3D turbulence.

Method: Extends 2.5-dimensional constructions from previous works, establishes Onsager-critical energy criterion adapted to such flows, and provides fully 3D dissipative Euler example with slightly rough external force following CL21 framework.

Result: Successfully constructs solutions exhibiting anomalous dissipation in finite time, shows sharpness of Onsager-critical energy criterion, and provides fully 3D dissipative example that is sharp in Onsager's sense.

Conclusion: Demonstrates existence of dissipative Euler solutions through vanishing viscosity approach, establishes sharp energy criterion, and provides concrete 3D examples advancing understanding of anomalous dissipation in turbulence.

Abstract: We construct solutions to the three-dimensional Euler equations exhibiting anomalous dissipation in finite time through a vanishing viscosity limit. Inspired by \cite{BDL23} and \cite{cheskidov2023dissipation}, we extend the \(2\frac{1}{2}\)-dimensional constructions and establish an Onsager-critical energy criterion adapted to such flows, showing its sharpness. Moreover, we provide a fully three-dimensional dissipative Euler example, sharp in Onsager's sense, driven by a slightly rough external force, following the framework of \cite{CL21}.

</details>


### [32] [Propagation of space-time singularities for perturbed harmonic oscillators](https://arxiv.org/abs/2512.24582)
*Kenichi Ito,Tomoya Tagawa*

Main category: math.AP

TL;DR: The paper analyzes singularity propagation for quantum harmonic oscillators with time-dependent perturbations, adapting semiclassical methods to characterize wave front sets compared to unperturbed systems.


<details>
  <summary>Details</summary>
Motivation: To understand how space-time singularities propagate in quantum harmonic oscillators when both the metric and potential become time-dependent, which is important for studying quantum systems in dynamic backgrounds.

Method: Reformulates Lascar's quasi-homogeneous wave front set in a semiclassical manner and adapts Nakamura's (2009) argument for spatial singularities of Schrödinger equations to handle time-dependent cases where time becomes a base variable rather than just a parameter.

Result: Obtains a characterization of singularity appearance in perturbed quantum harmonic oscillators compared to unperturbed systems, successfully applying Nakamura's method despite the non-trivial challenge of time being part of the base variables.

Conclusion: The paper successfully extends singularity analysis methods to time-dependent quantum harmonic oscillators, providing tools to characterize wave front sets and singularity propagation in perturbed quantum systems.

Abstract: We discuss propagation of space-time singularities for the quantum harmonic oscillator with time-dependent metric and potential perturbations. Reformulating the quasi-homogeneous wave front set according to Lascar (1977) in a semiclassical manner, we obtain a characterization of its appearance in comparison with the unperturbed system. The idea of our proof is based on the argument of Nakamura (2009), which was originally devised for the analysis of spatial singularities of the Schrödinger equation, however, the application is non-trivial since the time is no more a parameter, but takes a part in the base variables.

</details>


### [33] [Phase transition thresholds and chiral magnetic fields of general degree](https://arxiv.org/abs/2512.24598)
*Slim Ibrahim,Tatsuya Miura,Carlos Román,Ikkei Shimizu*

Main category: math.AP

TL;DR: Study of Landau-Lifshitz energy with Dzyaloshinskii-Moriya interactions in 2D micromagnetics, focusing on Bogomol'nyi regime, minimal energy determination, phase transitions, uniqueness/nonexistence of minimizers, and stability analysis.


<details>
  <summary>Details</summary>
Motivation: To understand the variational properties of the Landau-Lifshitz energy with Dzyaloshinskii-Moriya interactions in 2D micromagnetics, particularly in the Bogomol'nyi regime, and to characterize phase transitions, minimizer existence/uniqueness, and stability properties relevant to physical observations.

Method: Variational analysis of the Landau-Lifshitz energy functional with Dzyaloshinskii-Moriya interactions in the Bogomol'nyi regime, using mathematical techniques to determine minimal energy for arbitrary topological degree, prove uniqueness/nonexistence results, and analyze stability transitions.

Result: 1) Determined minimal energy for arbitrary topological degree, revealing two types of phase transitions consistent with physical observations. 2) Proved uniqueness of energy minimizer for degrees 0 and -1, and nonexistence of minimizers for all other degrees. 3) Showed homogeneous state remains stable beyond skyrmion stability threshold, and uncovered new stability transition driven by Zeeman energy.

Conclusion: The study provides comprehensive understanding of the variational landscape for Landau-Lifshitz energy with DMI in Bogomol'nyi regime, characterizing phase transitions, establishing precise conditions for minimizer existence/uniqueness, and revealing novel stability phenomena including Zeeman-driven transitions.

Abstract: We study a variational problem for the Landau--Lifshitz energy with Dzyaloshinskii--Moriya interactions arising in 2D micromagnetics, focusing on the Bogomol'nyi regime. We first determine the minimal energy for arbitrary topological degree, thereby revealing two types of phase transitions consistent with physical observations. In addition, we prove the uniqueness of the energy minimizer in degrees $0$ and $-1$, and nonexistence of minimizers for all other degrees. Finally, we show that the homogeneous state remains stable even beyond the threshold at which the skyrmion loses stability, and we uncover a new stability transition driven by the Zeeman energy.

</details>


### [34] [Half-space minimizing solutions of a two dimensional Allen-Cahn system](https://arxiv.org/abs/2512.24610)
*Zhiyuan Geng*

Main category: math.AP

TL;DR: Complete classification of half-space minimizing solutions to 2D Allen-Cahn system with Dirichlet boundary conditions, based on blow-down limits at infinity and characterization of interface asymptotics.


<details>
  <summary>Details</summary>
Motivation: To understand and classify minimizing solutions to the Allen-Cahn system on the upper half plane with Dirichlet boundary conditions, particularly focusing on their behavior at infinity and near interfaces.

Method: Analysis of two-dimensional Allen-Cahn system on upper half plane with multi-well potential, using blow-down limits at infinity to classify solutions and characterizing asymptotic behavior near sharp interfaces.

Result: Complete classification of half-space minimizing solutions in terms of their blow-down limits at infinity, plus characterization of asymptotic behavior near associated sharp interfaces.

Conclusion: The paper provides a comprehensive understanding of minimizing solutions to the Allen-Cahn system on half-space, establishing classification framework based on asymptotic behavior and interface properties.

Abstract: This paper studies minimizing solutions to a two dimensional Allen-Cahn system on the upper half plane, subject to Dirichlet boundary conditions, \begin{equation*}
  Δu-\nabla_u W(u)=0, \quad u: \mathbb{R}_+^2\to \mathbb{R}^2,\ u=u_0 \text{ on } \partial \mathbb{R}_+^2, \end{equation*} where $W: \mathbb{R}^2\to [0,\infty)$ is a multi-well potential. We give a complete classification of such half-space minimizing solutions in terms of their blow-down limits at infinity. In addition, we characterize the asymptotic behavior of solutions near the associated sharp interfaces.

</details>


### [35] [A unified spatiotemporal formulation with physics-preserving structure for time-dependent convection-diffusion problems](https://arxiv.org/abs/2512.24650)
*James H. Adler,Xiaozhe Hu,Seulip Lee*

Main category: math.AP

TL;DR: A 4D spatiotemporal formulation for time-dependent convection-diffusion problems using exterior calculus, treating time as space-like coordinate with Hodge-Laplacian operator and exponential fitting for well-posed variational formulation.


<details>
  <summary>Details</summary>
Motivation: To develop a unified framework for time-dependent convection-diffusion problems that preserves underlying physical structures and naturally incorporates fundamental physical constraints like divergence-free and curl-free conditions.

Method: Treat time as additional space-like coordinate to reformulate evolution problem as stationary convection-diffusion equation on 4D space-time domain. Use exterior calculus to extend to H(grad), H(curl), and H(div) problems. Introduce 4D Hodge-Laplacian operator with spatiotemporal diffusion tensor and convection field, augmented by small temporal perturbation. Develop exponentially-fitted 4D spatiotemporal flux operator for symmetrization and well-posed variational formulation.

Result: The formulation naturally incorporates physical constraints (divergence-free, curl-free conditions) and enables well-posed variational formulation through symmetrization. Proved convergence of temporally-perturbed formulation to original time-dependent model as perturbation parameter tends to zero.

Conclusion: A unified 4D spatiotemporal framework successfully reformulates time-dependent convection-diffusion problems while preserving physical structures, providing mathematical foundation for numerical methods that respect underlying physics.

Abstract: We propose a unified four-dimensional (4D) spatiotemporal formulation for time-dependent convection-diffusion problems that preserves underlying physical structures. By treating time as an additional space-like coordinate, the evolution problem is reformulated as a stationary convection-diffusion equation on a 4D space-time domain. Using exterior calculus, we extend this framework to the full family of convection-diffusion problems posed on $H(\textbf{grad})$, $H(\textbf{curl})$, and $H(\text{div})$. The resulting formulation is based on a 4D Hodge-Laplacian operator with a spatiotemporal diffusion tensor and convection field, augmented by a small temporal perturbation to ensure nondegeneracy. This formulation naturally incorporates fundamental physical constraints, including divergence-free and curl-free conditions. We further introduce an exponentially-fitted 4D spatiotemporal flux operator that symmetrizes the convection-diffusion operator and enables a well-posed variational formulation. Finally, we prove that the temporally-perturbed formulation converges to the original time-dependent convection-diffusion model as the perturbation parameter tends to zero.

</details>


### [36] [$L_p$-estimates for nonlocal equations with general Lévy measures](https://arxiv.org/abs/2512.24704)
*Hongjie Dong,Junhee Ryu*

Main category: math.AP

TL;DR: Study of nonlocal operators with singular Lévy measures, establishing continuity and unique strong solvability of corresponding parabolic equations in L_p spaces, with analysis of weighted mixed-norm space applicability.


<details>
  <summary>Details</summary>
Motivation: To analyze nonlocal operators with very singular Lévy measures without time regularity assumptions, extending the theory of nonlocal parabolic equations to more general and singular settings.

Method: Consider nonlocal operators defined by Lévy measures of order σ∈(0,2), allow measures to be very singular, impose no time regularity, establish operator continuity and unique strong solvability in L_p spaces.

Result: Proved continuity of operators and unique strong solvability of corresponding nonlocal parabolic equations in L_p spaces; demonstrated that applicability in weighted mixed-norm spaces depends on ranges of σ and d.

Conclusion: The paper establishes a comprehensive theory for nonlocal operators with singular Lévy measures, providing solvability results and clarifying when weighted mixed-norm space approaches are feasible based on parameter ranges.

Abstract: We consider nonlocal operators of the form \begin{equation*}
  L_t u(x) = \int_{\mathbb{R}^d} \left( u(x+y)-u(x)-\nabla u(x)\cdot y^{(σ)} \right) ν_t(dy), \end{equation*} where $ν_t$ is a general Lévy measure of order $σ\in(0,2)$. We allow this class of Lévy measures to be very singular and impose no regularity assumptions in the time variable. Continuity of the operators and the unique strong solvability of the corresponding nonlocal parabolic equations in $L_p$ spaces are established. We also demonstrate that, depending on the ranges of $σ$ and $d$, the operator can or cannot be treated in weighted mixed-norm spaces.

</details>


### [37] [Global spherically symmetric classical solutions for arbitrary large initial data of the multi-dimensional non-isentropic compressible Navier-Stokes equations](https://arxiv.org/abs/2512.24799)
*Yongteng Gu,Xiangdi Huang*

Main category: math.AP

TL;DR: The paper proves global classical solutions for arbitrary large initial data to the viscous shallow water system with transported entropy in spherically symmetric 2D and 3D cases, extending previous results on shallow water equations.


<details>
  <summary>Details</summary>
Motivation: The global classical solutions for arbitrary large initial data of multi-dimensional viscous Saint-Venant (shallow water) equations have been an open problem since 1871. Recent works by Huang-Meng-Zhang (2025) and Chen-Zhang-Zhu (2025) made progress on this problem, but with limitations on dimension and adiabatic index. This paper aims to extend these results to a more general class of non-isentropic compressible fluids with transported entropy.

Method: The authors prove a new BD entropy inequality for a class of non-isentropic compressible fluids (generalization of shallow water equations with transported entropy). They employ new estimates on the lower bound of density that differ from Huang-Meng-Zhang's approach, and analyze the spherically symmetric initial-boundary value problem in both two and three dimensions.

Result: The paper shows that the "viscous shallow water system with transport entropy" admits global classical solutions for arbitrary large initial data in spherically symmetric initial-boundary value problems in both 2D and 3D. The results relax restrictions on dimension and adiabatic index from previous work (N=2, γ≥3/2) to N=2, γ>1 and N=3, 1<γ<3.

Conclusion: The work successfully extends the existence theory for global classical solutions of large initial data to a more general class of compressible fluids with transported entropy, overcoming limitations in previous results and providing a broader mathematical framework for analyzing viscous shallow water systems.

Abstract: In 1871, Saint-Venant introduced the shallow water equations. Since then, the global classical solutions for arbitrary large initial data of the multi-dimensional viscous Saint-Venant system have remained a well-known open problem. It was only recently that [Huang-Meng-Zhang, http:arXiv:2512.15029, 2025], under the assumption of radial symmetry, first proved the existence of global classical solutions for arbitrary large initial data to the initial-boundary value problem of the two-dimensional viscous shallow water equations. At the same time, [Chen-Zhang-Zhu, http:arXiv:2512.18545, 2025] also independently proved the existence of global large solutions to the Cauchy problem of this system. Notably, in the work of Huang-Meng-Zhang, they also established the existence of global classical solutions for arbitrary large initial data to the isentropic compressible Navier-Stokes equations satisfying the BD entropy equality in both two and three dimensions, and the viscous shallow water equations are precisely a specific class of isentropic compressible fluids subject to the BD entropy equality. In this paper, we prove a new BD entropy inequality for a class of non-isentropic compressible fluids, which can be regarded as a generalization of the shallow water equations with transported entropy. Employing new estimates on the lower bound of density different from that of Huang-Meng-Zhang's work, we show the "viscous shallow water system with transport entropy" will admit global classical solutions for arbitrary large initial data to the spherically symmetric initial-boundary value problem in both two and three dimensions. Our results also relax the restrictions on the dimension and adiabatic index imposed in Huang-Meng-Zhang's work on the shallow water equations, extending the range from $N=2,\ γ\ge \frac{3}{2}$ to $N=2,\ γ> 1$ and $N=3,\ 1<γ<3$.

</details>


### [38] [Hölder continuity of weak solutions to the thin-film equation in $d=2$](https://arxiv.org/abs/2512.24809)
*Federico Cornalba,Julian Fischer,Erika Maringová Kokavcová*

Main category: math.AP

TL;DR: Proves Hölder continuity of energy-dissipating weak solutions to the thin-film equation in 2D, solving a major open problem about boundedness of solutions.


<details>
  <summary>Details</summary>
Motivation: The thin-film equation models viscous liquid film spreading on surfaces. While existence theory for weak solutions was established decades ago, boundedness of solutions in 2D remained a major unsolved problem due to the equation's fourth-order structure preventing application of standard regularity theory.

Method: Uses the hole-filling technique to overcome challenges posed by the degenerate parabolicity of the fourth-order PDE, since De Giorgi-Nash-Moser theory doesn't apply to fourth-order equations.

Result: Successfully proves Hölder continuity of energy-dissipating weak solutions to the thin-film equation in two spatial dimensions (d=2), establishing boundedness that was previously unknown.

Conclusion: Resolves a long-standing open problem in thin-film equation theory by establishing regularity properties of weak solutions in 2D, which is the physically most relevant case for modeling liquid films on surfaces.

Abstract: The thin-film equation $\partial_t u = -\nabla \cdot (u^n \nabla Δu)$ describes the evolution of the height $u=u(x,t)\geq 0$ of a viscous thin liquid film spreading on a flat solid surface. We prove Hölder continuity of energy-dissipating weak solutions to the thin-film equation in the physically most relevant case of two spatial dimensions $d=2$. While an extensive existence theory of weak solutions to the thin-film equation was established more than two decades ago, even boundedness of weak solutions in $d=2$ has remained a major unsolved problem in the theory of the thin-film equation. Due the fourth-order structure of the thin-film equation, De Giorgi-Nash-Moser theory is not applicable. Our proof is based on the hole-filling technique, the challenge being posed by the degenerate parabolicity of the fourth-order PDE.

</details>


### [39] [Bol's type inequality for singular metrics and its application to prescribing $Q$-curvature problems](https://arxiv.org/abs/2512.24828)
*Mrityunjoy Ghosh,Ali Hyder*

Main category: math.AP

TL;DR: The paper studies higher-order Bol's inequality for radial normal solutions to a singular Liouville equation, using these inequalities with compactness arguments to derive existence conditions for radial normal solutions to a singular Q-curvature problem, and obtaining uniform bounds on total Q-curvature.


<details>
  <summary>Details</summary>
Motivation: To establish existence conditions for radial normal solutions to singular Q-curvature problems and obtain uniform bounds on total Q-curvature, building on higher-order generalizations of Bol's inequality.

Method: Apply higher-order Bol's inequality for radial normal solutions to singular Liouville equations, combined with compactness arguments, to analyze singular Q-curvature problems.

Result: Derive necessary and sufficient conditions for existence of radial normal solutions to singular Q-curvature problem, and obtain uniform bounds on total Q-curvature under suitable assumptions.

Conclusion: Higher-order Bol's inequality combined with compactness arguments provides effective tools for establishing existence conditions and uniform bounds in singular Q-curvature problems with radial normal solutions.

Abstract: In this article, we study higher-order Bol's inequality for radial normal solutions to a singular Liouville equation. By applying these inequalities along with compactness arguments, we derive necessary and sufficient conditions for the existence of radial normal solutions to a singular $Q$-curvature problem. Moreover, under suitable assumptions on the $Q$-curvature, we obtain uniform bounds on the total $Q$-curvature.

</details>


### [40] [Boundedness of Fourier Integral Operators with complex phases on Fourier Lebesgue spaces](https://arxiv.org/abs/2512.24854)
*Duván Cardona,William Obeng-Denteh,Frederick Opoku*

Main category: math.AP

TL;DR: Boundedness estimates for Fourier integral operators with complex phase functions on Fourier Lebesgue spaces, extending real case results to complex canonical relations.


<details>
  <summary>Details</summary>
Motivation: Extend boundedness results for Fourier integral operators from real canonical relations (Rodino, Nicola, Cordero) to complex canonical relations, establishing the complex analogue of existing theory.

Method: Develop boundedness estimates under spatial factorization condition of rank ϰ, analyzing operators on Fourier Lebesgue spaces ℱL^p with complex phase functions parametrizing the canonical relation.

Result: Fourier integral operator is bounded on ℱL^p when order m satisfies m ≤ -ϰ|1/p - 1/2| for 1 ≤ p ≤ ∞, and this condition on m is sharp.

Conclusion: Successfully established complex analogue of boundedness results for Fourier integral operators, providing sharp order conditions for boundedness on Fourier Lebesgue spaces with complex phase functions.

Abstract: In this paper, we develop boundedness estimates for Fourier integral operators on Fourier Lebesgue spaces when the associated canonical relation is parametrised by a complex phase function. Our result constitutes the complex analogue of those obtained for real canonical relations by Rodino, Nicola, and Cordero. We prove that, under the spatial factorization condition of rank $\varkappa$, the corresponding Fourier integral operator is bounded on the Fourier Lebesgue space $\mathcal{F}L^p,$ provided that the order $m$ of the operator satisfies that $ m \leq -\varkappa\left|\frac{1}{p}-\frac{1}{2}\right|, 1 \leq p \leq \infty. $ This condition on the order $m$ is sharp.

</details>


### [41] [Global boundedness and absorbing sets in two-dimensional chemotaxis-Navier-Stokes systems with weakly singular sensitivity and a sub-logistic source](https://arxiv.org/abs/2512.24892)
*Minh Le,Alexey Cheskidov*

Main category: math.AP

TL;DR: The paper proves global existence and boundedness of classical solutions for a chemotaxis-fluid system with logistic growth and nonlinear diffusion in 2D bounded domains.


<details>
  <summary>Details</summary>
Motivation: To establish global well-posedness for a complex chemotaxis-fluid system with nonlinear diffusion terms and logistic growth, which combines biological transport phenomena with fluid dynamics.

Method: Mathematical analysis of PDE system using energy estimates, a priori bounds, and functional analysis techniques to prove existence of globally bounded classical solutions under appropriate initial conditions and boundary conditions.

Result: The system admits globally bounded classical solutions and possesses an absorbing set in the topology of C⁰(Ω̄) × W¹,∞(Ω) × C⁰(Ω̄; ℝ²).

Conclusion: The chemotaxis-fluid system with logistic growth and nonlinear diffusion is well-posed in two dimensions, with solutions remaining bounded globally in time and exhibiting dissipative dynamics.

Abstract: This paper studies the following chemotaxis-fluid system in a two-dimensional bounded domain $Ω$: \begin{equation*}
  \begin{cases}
  n_t + u \cdot \nabla n &= Δn - χ\nabla \cdot \left (n \frac{\nabla c}{c^k} \right ) + r n - \frac{μn^2}{\log^η(n+e)},
  c_t + u \cdot \nabla c &= Δc - αc + βn,
  u_t + u \cdot \nabla u &= Δu - \nabla P + n \nabla φ+ f,
  \nabla \cdot u &= 0,
  \end{cases} \end{equation*} where $r, μ, α, β, χ$ are positive parameters, $k, η\in (0,1)$, $φ\in W^{2,\infty}(Ω)$, and $f \in C^1\left(\barΩ\times [0, \infty)\right) \cap L^\infty\left(Ω\times (0, \infty)\right)$. We show that, under suitable conditions on the initial data and with no-flux/no-flux/Dirichlet boundary conditions, this system admits a globally bounded classical solution. Furthermore, the system possesses an absorbing set in the topology of $C^0(\barΩ) \times W^{1, \infty}(Ω) \times C^0(\barΩ; \mathbb{R}^2)$.

</details>


### [42] [On exact Observability for Compactly perturbed infinite dimension system](https://arxiv.org/abs/2512.25041)
*Nisrine Charaf,Faouzi Triki*

Main category: math.AP

TL;DR: Study of observability preservation for compactly perturbed infinite-dimensional systems with self-adjoint generators.


<details>
  <summary>Details</summary>
Motivation: To understand how compact perturbations affect the exact observability of infinite-dimensional systems, which is crucial for control theory applications where systems are often subject to perturbations.

Method: Assuming an infinite-dimensional system with self-adjoint generator is exactly observable, derive sufficient conditions on compact self-adjoint perturbations to preserve exact observability. Analysis based on asymptotic estimation of spectral elements of perturbed unbounded operators.

Result: Established sufficient conditions for compact perturbations to maintain exact observability, with intermediate results on spectral element estimation that are valuable in their own right.

Conclusion: Compact perturbations can preserve exact observability under certain conditions, with spectral analysis providing key insights into perturbation effects on infinite-dimensional systems.

Abstract: In this paper, we study the observability of compactly perturbed infinite dimensional systems. Assuming that a given infinite-dimensional system with self-adjoint generator is exactly observable we derive sufficient conditions on a compact self adjoint perturbation to guarantee that the perturbed system stays exactly observable. The analysis is based on a careful asymptotic estimation of the spectral elements of the perturbed unbounded operator in terms of the compact perturbation. These intermediate results are of importance themselves.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [43] [Learning Density Functionals to Bridge Particle and Continuum Scales](https://arxiv.org/abs/2512.23840)
*Edoardo Monti,Peter Yatsyshin,Konstantinos Gkagkas,Andrew B. Duncan*

Main category: physics.comp-ph

TL;DR: A physics-informed learning framework that augments classical density functional theory with neural corrections trained against molecular dynamics data, preserving thermodynamic consistency while improving predictive accuracy for interfacial thermodynamics.


<details>
  <summary>Details</summary>
Motivation: Predicting interfacial thermodynamics across molecular and continuum scales is challenging. Classical density functional theory (cDFT) provides first-principles connection but its accuracy depends on approximate free-energy functionals that are difficult to generalize.

Method: Introduces a physics-informed learning framework that augments cDFT with neural corrections trained directly against molecular dynamics data through adjoint optimization. Embeds compact neural networks within the Helmholtz free-energy functional to learn local and nonlocal corrections that preserve thermodynamic consistency.

Result: Applied to Lennard-Jones fluids, the augmented excess free-energy functional quantitatively reproduces equilibrium density profiles, coexistence curves, and surface tensions across broad temperature ranges, and accurately predicts contact angles and droplet shapes far beyond the training regime.

Conclusion: This approach combines interpretability of statistical mechanics with adaptability of modern machine learning, establishing a general route to learned thermodynamic functionals that bridge molecular simulations and continuum-scale models.

Abstract: Predicting interfacial thermodynamics across molecular and continuum scales remains a central challenge in computational science. Classical density functional theory (cDFT) provides a first-principles route to connect microscopic interactions with macroscopic observables, but its predictive accuracy depends on approximate free-energy functionals that are difficult to generalize. Here we introduce a physics-informed learning framework that augments cDFT with neural corrections trained directly against molecular-dynamics data through adjoint optimization. Rather than replacing the theory with a black-box surrogate, we embed compact neural networks within the Helmholtz free-energy functional, learning local and nonlocal corrections that preserve thermodynamic consistency while capturing missing correlations. Applied to Lennard-Jones fluids, the resulting augmented excess free-energy functional quantitatively reproduces equilibrium density profiles, coexistence curves, and surface tensions across a broad temperature range, and accurately predicts contact angles and droplet shapes far beyond the training regime. This approach combines the interpretability of statistical mechanics with the adaptability of modern machine learning, establishing a general route to learned thermodynamic functionals that bridge molecular simulations and continuum-scale models.

</details>


### [44] [BF-APNN: A Low-Memory Method for Accelerating the Solution of Radiative Transfer Equations](https://arxiv.org/abs/2512.24534)
*Xizhe Xie,Wengu Chen,Weiming Li,Peng Song,Han Wang*

Main category: physics.comp-ph

TL;DR: BF-APNN accelerates radiative transfer equation solutions by using basis function expansion to reduce high-dimensional integral computations, cutting training time while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Radiative Transfer Equations (RTEs) are high-dimensional and multiscale, making conventional methods computationally intensive. Existing deep learning methods struggle with high-dimensional or nonlinear RTEs, creating a need for more efficient approaches.

Method: BF-APNN combines basis function expansion with micro-macro decomposition. It expands the microscopic component using basis functions to reduce computational burden from high-dimensional integrals during training, building on the RT-APNN framework.

Result: BF-APNN significantly reduces training time compared to RT-APNN while maintaining high solution accuracy. It handles challenging RTE scenarios with nonlinearity, discontinuities, and multiscale behavior, and performs well on complex, high-dimensional problems.

Conclusion: BF-APNN is a robust tool for radiative transfer computations that effectively addresses computational challenges of high-dimensional RTEs through basis function expansion, offering both efficiency and accuracy improvements.

Abstract: The Radiative Transfer Equations (RTEs) exhibit high dimensionality and multiscale characteristics, rendering conventional numerical methods computationally intensive. Existing deep learning methods perform well in low-dimensional or linear RTEs, but still face many challenges with high-dimensional or nonlinear RTEs. To overcome these challenges, we propose the Basis Function Asymptotically Preserving Neural Network (BF-APNN), a framework that inherits the advantages of Radiative Transfer Asymptotically Preserving Neural Network (RT-APNN) and accelerates the solution process. By employing basis function expansion on the microscopic component, derived from micro-macro decomposition, BF-APNN effectively mitigates the computational burden associated with evaluating high-dimensional integrals during training. Numerical experiments, which involve challenging RTE scenarios featuring, nonlinearity, discontinuities, and multiscale behavior, demonstrate that BF-APNN substantially reduces training time compared to RT-APNN while preserving high solution accuracy. Moreover, BF-APNN exhibits superior performance in addressing complex, high-dimensional RTE problems, underscoring its potential as a robust tool for radiative transfer computations.

</details>


### [45] [Random Batch Sum-of-Gaussians Method for Molecular Dynamics of Born-Mayer-Huggins Systems](https://arxiv.org/abs/2512.24970)
*Chen Chen,Jiuyang Liang,Zhenli Xu,Qianru Zhang*

Main category: physics.comp-ph

TL;DR: Extended random batch sum-of-Gaussians (RBSOG) method with random batch list (RBL) for efficient large-scale MD simulations of Born-Mayer-Huggins potential systems.


<details>
  <summary>Details</summary>
Motivation: Large-scale MD simulations of BMH systems are limited by computational, communication, and memory costs. Existing methods need improvement for better scalability and efficiency.

Method: Extends RBSOG method to BMH systems, adds random batch list (RBL) scheme for short-range interactions. Uses SOG decomposition to split potential into short- and long-range parts, applies importance sampling in Fourier space for long-range part, and RBL handles medium-range exponential repulsion and dispersion.

Result: Achieves 4-10× speedup vs Ewald-based PPPM and 2× speedup vs RBSOG-only method on 1000 cores for systems up to 5×10⁶ atoms. Reduced memory usage while maintaining structural and thermodynamic accuracy.

Conclusion: The unified RBSOG+RBL framework provides attractive performance in accuracy and scalability for MD simulations with long-range interactions in BMH systems.

Abstract: The Born-Mayer-Huggins (BMH) potential, which combines Coulomb interactions with dispersion and short-range exponential repulsion, is widely used for ionic materials such as molten salts. However, large-scale molecular dynamics simulations of BMH systems are often limited by computation, communication, and memory costs. We recently proposed the random batch sum-of-Gaussians (RBSOG) method, which accelerates Coulomb calculations by using a sum-of-Gaussians (SOG) decomposition to split the potential into short- and long-range parts and by applying importance sampling in Fourier space for the long-range part. In this work, we extend the RBSOG to BMH systems and incorporate a random batch list (RBL) scheme to further accelerate the short-range part, yielding a unified framework for efficient simulations with the BMH potential. The combination of the SOG decomposition and the RBL enables an efficient and scalable treatment of both long- and short-range interactions in BMH system, particularly the RBL well handles the medium-range exponential repulsion and dispersion by the random batch neighbor list. Error estimate is provided to show the theoretical convergence of the RBL force. We evaluate the framework on molten NaCl and mixed alkali halide with up to $5\times10^6$ atoms on $2048$ CPU cores. Compared to the Ewald-based particle-particle particle-mesh method and the RBSOG-only method, our method achieves approximately $4\sim10\times$ and $2\times$ speedups while using $1000$ cores, respectively, under the same level of structural and thermodynamic accuracy and with a reduced memory usage. These results demonstrate the attractive performance of our method in accuracy and scalability for MD simulations with long-range interactions.

</details>


### [46] [Fast Poisson brackets and constraint algebras in canonical gravity](https://arxiv.org/abs/2512.25007)
*Will Barker*

Main category: physics.comp-ph

TL;DR: A computer algebra package for efficiently computing Poisson brackets and constraint algebras in gravity theories, tested on GR and modified gravity.


<details>
  <summary>Details</summary>
Motivation: Dirac's Hamiltonian constraint algorithm is crucial for analyzing gravity theories but is notoriously difficult to implement manually, requiring computational tools to handle the complexity.

Method: Developed a computer algebra package that automates the computation of Poisson brackets and reconstruction of constraint algebras for gravity theories.

Result: Successfully stress-tested the package against pure general relativity and modified gravity, including order reduction of general relativity at two loops.

Conclusion: The package provides an efficient computational tool to overcome the arduous implementation of Dirac's Hamiltonian constraint algorithm in gravity theories.

Abstract: In the study of alternative or extended theories of gravity, Dirac's Hamiltonian constraint algorithm is invaluable for enumerating the propagating modes and gauge symmetries. For gravity, this canonical approach is frequently applied as a means for finding pathologies such as strongly coupled modes; more generally it facilitates the reconstruction of gauge symmetries and the quantization of gauge theories. For gravity, however, the algorithm can become notoriously arduous to implement. We present a simple computer algebra package for efficiently computing Poisson brackets and reconstructing constraint algebras. The tools are stress-tested against pure general relativity and modified gravity, including the order reduction of general relativity at two loops.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [47] [Autoregressive long-horizon prediction of plasma edge dynamics](https://arxiv.org/abs/2512.23884)
*Hunor Csala,Sebastian De Pascuale,Paul Laiu,Jeremy Lore,Jae-Sun Park,Pei Zhang*

Main category: physics.plasm-ph

TL;DR: Transformer-based autoregressive surrogates for fast prediction of 2D time-dependent plasma edge states, trained on SOLPS-ITER data, enabling orders-of-magnitude speedup for fusion device design.


<details>
  <summary>Details</summary>
Motivation: High-fidelity edge fluid/neutral codes like SOLPS-ITER are computationally expensive, limiting parameter scans and long transient studies for fusion device design. There's a need for faster surrogates to enable rapid scenario exploration and control-oriented studies.

Method: Transformer-based autoregressive surrogates trained on SOLPS-ITER spatiotemporal data to predict 2D time-dependent plasma edge state fields (electron temperature, electron density, radiated power). Models trained with increasing autoregressive horizons (1-100 steps) and evaluated on short- and long-horizon prediction tasks.

Result: Longer-horizon training systematically improves rollout stability and mitigates error accumulation, enabling stable predictions over hundreds to thousands of steps while reproducing key dynamical features. The surrogate is orders of magnitude faster than SOLPS-ITER in wall-clock time. Accuracy degrades when entering physical regimes not represented in training data.

Conclusion: Transformer-based surrogates provide fast, accurate alternatives to computationally intensive plasma edge simulations, supporting rapid parameter exploration, control-oriented studies, and progress toward real-time applications in fusion devices. Future work needed on data enrichment and physics-informed constraints to handle unseen physical regimes.

Abstract: Accurate modeling of scrape-off layer (SOL) and divertor-edge dynamics is vital for designing plasma-facing components in fusion devices. High-fidelity edge fluid/neutral codes such as SOLPS-ITER capture SOL physics with high accuracy, but their computational cost limits broad parameter scans and long transient studies. We present transformer-based, autoregressive surrogates for efficient prediction of 2D, time-dependent plasma edge state fields. Trained on SOLPS-ITER spatiotemporal data, the surrogates forecast electron temperature, electron density, and radiated power over extended horizons. We evaluate model variants trained with increasing autoregressive horizons (1-100 steps) on short- and long-horizon prediction tasks. Longer-horizon training systematically improves rollout stability and mitigates error accumulation, enabling stable predictions over hundreds to thousands of steps and reproducing key dynamical features such as the motion of high-radiation regions. Measured end-to-end wall-clock times show the surrogate is orders of magnitude faster than SOLPS-ITER, enabling rapid parameter exploration. Prediction accuracy degrades when the surrogate enters physical regimes not represented in the training dataset, motivating future work on data enrichment and physics-informed constraints. Overall, this approach provides a fast, accurate surrogate for computationally intensive plasma edge simulations, supporting rapid scenario exploration, control-oriented studies, and progress toward real-time applications in fusion devices.

</details>


### [48] [The role of particle feedback on particle acceleration in magnetic reconnection](https://arxiv.org/abs/2512.24054)
*Shimin Liang,Nianyu Yi*

Main category: physics.plasm-ph

TL;DR: Particle feedback in magnetic reconnection amplifies shear flows, strengthening electric fields and boosting particle acceleration, while guide fields suppress these effects.


<details>
  <summary>Details</summary>
Motivation: To understand how particle feedback affects magnetic reconnection dynamics and particle acceleration processes in astrophysical plasmas.

Method: 2.5D magnetohydrodynamic (MHD) simulations using a co-evolving fluid-particle framework to study particle feedback effects on reconnection.

Result: Particle feedback amplifies shear flows within magnetic islands, strengthening convective electric fields and boosting particle acceleration, resulting in higher maximum particle energies and harder non-thermal spectra. Guide fields suppress both gas internal energy increase and particle acceleration.

Conclusion: The study reveals complex interplay between particle feedback, guide fields, and reconnection dynamics, showing feedback enhances acceleration while guide fields suppress it.

Abstract: Magnetic reconnection is a ubiquitous process in astrophysical plasmas and an efficient mechanism for particle acceleration. Using 2.5D magnetohydrodynamic (MHD) simulations with a co-evolving fluid-particle framework, we investigate how particle feedback affects reconnection and acceleration. Our simulations demonstrate that particle feedback to the fluid amplifies shear flows within magnetic islands, which strengthens the convective electric field and thereby boosts particle acceleration. This mechanism results in a higher maximum particle energy and a harder non-thermal energy spectrum. The guide field suppresses both the increase in gas internal energy and particle acceleration. These findings highlight the complex interplay between feedback, guide fields, and reconnection dynamics.

</details>


### [49] [Coordinates based on a magnetic mirror field](https://arxiv.org/abs/2512.24305)
*R. D. Hazeltine*

Main category: physics.plasm-ph

TL;DR: Constructs a specialized coordinate system tailored to cylindrically symmetric magnetic field geometry


<details>
  <summary>Details</summary>
Motivation: Standard coordinate systems don't naturally align with the geometry of cylindrically symmetric magnetic fields, making analysis and calculations more complex

Method: Develops a mathematical framework to construct coordinate systems that fit the specific geometry of cylindrically symmetric magnetic fields

Result: Successfully creates a coordinate system that naturally aligns with magnetic field geometry, simplifying field analysis and calculations

Conclusion: The constructed coordinate system provides a more natural framework for studying cylindrically symmetric magnetic fields, potentially enabling easier analysis and applications in plasma physics and related fields

Abstract: We construct a coordinate system fitting the geometry of a given, cylindrically symmetric, magnetic field.

</details>


### [50] [Computing Flux-Surface Shapes in Tokamaks and Stellarators](https://arxiv.org/abs/2512.24544)
*M. J. Gerard,M. J. Pueschel,S. Stewart,H. O. M. Hillebrecht,B. Geiger*

Main category: physics.plasm-ph

TL;DR: A new method for characterizing stellarator magnetic field geometry using Fourier analysis to define shaping modes, revealing that quasi-symmetry emerges from spatial resonance between shape complexity and rotation about the magnetic axis.


<details>
  <summary>Details</summary>
Motivation: There is currently no agreed-upon methodology for characterizing stellarator magnetic field geometry, despite modern designs achieving high levels of magnetic-field quasi-symmetry through careful flux-surface shaping. The lack of standardized characterization methods hinders systematic investigations into the relationship between flux-surface geometries and other performance metrics.

Method: The authors introduce a general method using Fourier mode analysis to define shaping modes (elongation, triangularity, squareness, etc.) of cross-sections that can be non-planar. The framework works for both axisymmetric and non-axisymmetric configurations, with the additional degree of freedom in non-axisymmetric equilibria manifesting as rotation of each shaping mode about the magnetic axis.

Result: Analysis of non-axisymmetric configurations with precise quasi-symmetry and cases from the QUASR database reveals that quasi-symmetry results from a spatial resonance between shape complexity and shape rotation about the magnetic axis. The quantitative features of this resonance correlate closely with a configuration's rotational transform and number of field periods.

Conclusion: The proposed shaping paradigm can facilitate systematic investigations into the relationship between general flux-surface geometries and other figures of merit, providing a standardized methodology for characterizing stellarator magnetic field geometry that was previously lacking.

Abstract: There is currently no agreed-upon methodology for characterizing a stellarator magnetic field geometry, and yet modern stellarator designs routinely attain high levels of magnetic-field quasi-symmetry through careful flux-surface shaping. Here, we introduce a general method for computing the shape of an ideal-MHD equilibrium that can be used in both axisymmetric and non-axisymmetric configurations. This framework uses a Fourier mode analysis to define the shaping modes (e.g. elongation, triangularity, squareness, etc.) of cross-sections that can be non-planar. Relative to an axisymmetric equilibrium, the additional degree of freedom in a non-axisymmetric equilibrium manifests as a rotation of each shaping mode about the magnetic axis. Using this method, a shaping analysis is performed on non-axisymmetric configurations with precise quasi-symmetry and select cases from the QUASR database spanning a range of quasi-symmetry quality. Empirically, we find that quasi-symmetry results from a spatial resonance between shape complexity and shape rotation about the magnetic axis. The quantitative features of this resonance correlate closely with a configuration's rotational transform and number of field periods. Based on these observations, it is conjectured that this shaping paradigm can facilitate systematic investigations into the relationship between general flux-surface geometries and other figures of merit.

</details>


### [51] [Cataloging the nonlinear waves excited by moving a charged body in the dusty plasma medium](https://arxiv.org/abs/2512.24723)
*Swathi S Krishna,S. K. Mishra,S. Jaiswal*

Main category: physics.plasm-ph

TL;DR: Study of nonlinear waves generated by charged body movement in dusty plasma, described by forced KdV equation, showing source parameters (amplitude, width, speed) affect wave evolution beyond just Mach number.


<details>
  <summary>Details</summary>
Motivation: To understand how charged bodies moving through dusty plasma generate diverse nonlinear waves (precursors, pinned solitons) and investigate how source characteristics shape these wave excitations beyond traditional Mach number considerations.

Method: Theoretical analysis using forced Korteweg-de Vries (fKdV) equation under weakly nonlinear and dispersive limits. Examined effects of three source parameters: amplitude, width, and flow speed on nonlinear wave evolution.

Result: Found that nonlinear structure excitation depends not just on Mach number but also on source features (amplitude, width). Discovered novel lagging nonlinear structures that maintain shape and speed while propagating behind the source term.

Conclusion: The study provides first theoretical depiction of lagging structures in dusty plasma, demonstrating that source characteristics significantly influence nonlinear wave evolution beyond traditional Mach number considerations in plasma physics.

Abstract: The nonlinear waves excited by the movement of a charged body in the dusty plasma medium are studied. A charged body moving through a dusty plasma medium can generate diverse nonlinear waves, such as precursors and pinned solitons. These wave excitations under weakly nonlinear and dispersive limits are described theoretically by the forced Korteweg-de Vries (fKdV) type equation. We have examined the role of the driver in shaping and evolving these wave excitations. In particular we studied the effect of primarily three source parameters, namely, amplitude, width, and flow speed, on the evolution of nonlinear structures. The driver generates a perturbation in the stable system configuration, which couples with medium characteristics and eventually evolves into propagating excitations. Our finding shows that the excitation of nonlinear structure by a moving body in a plasma medium is not just dictated by the mach number but also the features of the source such as amplitude and width. As a novel finding apart from pinned and precursor solitons, we observe another nonlinear structure that lags behind the source term, maintaining its shape and speed as it propagates. These features are the first ever theoretical depiction of such lagging structures.

</details>


### [52] [Runaway electron avalanche and macroscopic beam formation: simulations of the DTT full power scenario](https://arxiv.org/abs/2512.24760)
*E. Emanuelli,F. Vannini,M. Hoelzl,E. Nardon,V. Bandaru,N. Schwarz,D. Bonfiglio,G. Ramogida,F. Subba,JOREK Team*

Main category: physics.plasm-ph

TL;DR: DTT facility's transition from low-current (2 MA) to full-power (5.5 MA) scenarios dramatically increases runaway electron avalanche risk, narrowing safe operational windows and requiring careful impurity injection balancing.


<details>
  <summary>Details</summary>
Motivation: To understand how the transition from DTT's initial commissioning phase (2 MA) to full power operation (5.5 MA) affects runaway electron generation dynamics during disruptions, as previous studies showed safety margins for low-current scenarios but the scaling with plasma current may create new risks.

Method: Used non-linear magnetohydrodynamic code JOREK to perform comprehensive 2D simulations of current quench phase during disruptions, systematically scanning initial RE seed currents and injected impurity levels for different disruption scenarios.

Result: In full power scenario, avalanche multiplication factor is extremely high (G_av ≈ 1.3×10^5), converting just 5.5 A seed current into 0.7 MA RE beams with large impurities. Higher seeds can produce RE currents up to 3.2 MA (≈80% of total plasma current). Unlike Day-0 phase, full power scenario requires careful balance between thermal load mitigation and RE avoidance.

Conclusion: The disruption mitigation strategy for DTT's full power scenario must carefully balance impurity injection to manage both thermal loads and runaway electron generation, providing essential baseline for future RE load estimations on plasma-facing components and design of mitigation systems.

Abstract: The transition of the Divertor Tokamak Test (DTT) facility from its initial commissioning phase (Day-0, plasma current $I_{p}=2$ MA) to the full power scenario ($I_{p}=5.5$ MA) introduces a critical shift in the dynamics of runaway electrons (REs) generation. While previous predictive studies of the low-current scenario indicated a robust safety margin against RE beam formation, this work reveals that the exponential scaling of the RE avalanche gain with plasma current severely narrows the safe operational window in the full power scenario. Using the non-linear magnetohydrodynamic code JOREK, we perform comprehensive 2D simulations of the current quench (CQ) phase of several disruption scenarios, systematically scanning initial RE seed currents and injected impurity levels. The results demonstrate that in the full power scenario, the avalanche multiplication factor is sufficiently high ($G_\text{av} \approx 1.3 \cdot 10^5$) to convert a mere 5.5 A seed current into macroscopic RE beams of $\approx 0.7$ MA when large amounts of impurities are present. For even higher RE seeds, the RE current can peak at $ \approx 3.2$ MA, constituting up to $\approx$ 80% of the total plasma current during the CQ. These findings suggest that, unlike the Day-0 phase, the disruption mitigation strategy for the full power scenario involves a careful balance between thermal load mitigation and RE avoidance, necessitating a well-chosen quantity of injected impurities. This work provides the baseline needed for future estimations of RE loads on the plasma-facing components of DTT, which will be essential for designing and positioning mitigation components like sacrificial limiters.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [53] [Stochastic Galerkin Method and Hierarchical Preconditioning for PDE-constrained Optimization](https://arxiv.org/abs/2512.23804)
*Zhendong Li,Akwum Onwunta,Bedřich Sousedík*

Main category: math.OC

TL;DR: Efficient hierarchical preconditioners for PDE-constrained optimal control with uncertain coefficients, using truncated stochastic expansions to balance computational cost and preconditioning quality.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of solving large-scale, ill-conditioned linear systems arising in uncertainty quantification for optimal control problems governed by PDEs with uncertain coefficients.

Method: Discretize-then-optimize framework combining finite element discretization, stochastic Galerkin approximation, and advanced time-discretization schemes. Hierarchical preconditioners exploit sparsity in generalized polynomial chaos expansions using truncated stochastic expansions.

Result: Proposed preconditioners significantly accelerate convergence of iterative solvers compared to existing methods, providing robust and efficient solvers for both steady-state and time-dependent optimal control under uncertainty.

Conclusion: The hierarchical preconditioners based on truncated stochastic expansions offer an effective balance between computational cost and preconditioning quality, enabling efficient solution of large-scale optimal control problems with uncertain parameters.

Abstract: We develop efficient hierarchical preconditioners for optimal control problems governed by partial differential equations with uncertain coefficients. Adopting a discretize-then-optimize framework that integrates finite element discretization, stochastic Galerkin approximation, and advanced time-discretization schemes, the approach addresses the challenge of large-scale, ill-conditioned linear systems arising in uncertainty quantification. By exploiting the sparsity inherent in generalized polynomial chaos expansions, we derive hierarchical preconditioners based on truncated stochastic expansion that strike an effective balance between computational cost and preconditioning quality. Numerical experiments demonstrate that the proposed preconditioners significantly accelerate the convergence of iterative solvers compared to existing methods, providing robust and efficient solvers for both steady-state and time-dependent optimal control applications under uncertainty.

</details>


### [54] [The Flow-Limit of Reflect-Reflect-Relax: Existence, Stability, and Discrete-Time Behavior](https://arxiv.org/abs/2512.23843)
*Manish Krishan Lal*

Main category: math.OC

TL;DR: The paper analyzes the Reflect-Reflect-Relax (RRR) algorithm in small-step regime, showing it forms a hyperbolic sink with exponential convergence, has finite-time capture properties, and connects to forward-Euler discretization with emergent optimal relaxation parameters.


<details>
  <summary>Details</summary>
Motivation: To understand the convergence behavior and dynamics of the Reflect-Reflect-Relax (RRR) algorithm in the small-step regime, particularly its geometric properties, convergence rates, and the emergence of optimal relaxation parameters.

Method: Uses dynamical systems analysis to study RRR as a flow, constructs Lyapunov functions, analyzes Filippov sliding dynamics, connects discrete algorithm to forward-Euler discretization, and applies percolation and renormalization group heuristics for performance analysis near limits.

Result: Shows RRR forms hyperbolic sink with exponential decay, has finite-time capture into solution domains, proves small-step RRR is forward-Euler discretization of the flow, explains emergence of iteration-optimal relaxation parameters, and provides framework for performance deterioration analysis.

Conclusion: The small-step RRR algorithm exhibits well-behaved geometric dynamics with exponential convergence, finite-time capture properties, and emergent optimal parameters, with performance deterioration near limits explainable through statistical physics-inspired frameworks.

Abstract: We study the Reflect-Reflect-Relax (RRR) algorithm in its small-step (flow-limit) regime. In the smooth transversal setting, we show that the transverse dynamics form a hyperbolic sink, yielding exponential decay of a natural gap measure. Under uniform geometric assumptions, we construct a tubular neighborhood of the feasible manifold on which the squared gap defines a strict Lyapunov function, excluding recurrent dynamics and chaotic behavior within this basin.
  In the discrete setting, the induced flow is piecewise constant on W-domains and supports Filippov sliding along convergent boundaries, leading to finite-time capture into a solution domain. We prove that small-step RRR is a forward-Euler discretization of this flow, so that solution times measured in rescaled units converge to a finite limit while iteration counts diverge, explaining the emergence of iteration-optimal relaxation parameters. Finally, we introduce a heuristic mesoscopic framework based on percolation and renormalization group to organize performance deterioration near the Douglas-Rachford limit.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [55] [Upscaling from ab initio atomistic simulations to electrode scale: The case of manganese hexacyanoferrate, a cathode material for Na-ion batteries](https://arxiv.org/abs/2512.24816)
*Yuan-Chi Yang,Eric Woillez,Quentin Jacquet,Ambroise van Roekeghem*

Main category: cond-mat.mtrl-sci

TL;DR: A multiscale computational framework bridges atomistic to device scales for predictive modeling of insertion-type electrode materials, demonstrated on sodium manganese hexacyanoferrate cathode for sodium-ion batteries.


<details>
  <summary>Details</summary>
Motivation: To enable rational computational design of next-generation insertion-type materials (like battery electrodes) by systematically translating atomistic insights into continuum-scale predictions, addressing the scale-bridging challenge in materials modeling.

Method: Active-learning strategy trains Moment Tensor Potential via iterative hybrid grand-canonical Monte Carlo-molecular dynamics sampling. Machine learning interatomic potential captures configuration spaces at all sodiation levels. Critical parameters (diffusivities, interfacial/strain energies, free-energy landscapes) feed into pseudo-2D phase-field simulations for electrode-scale predictions.

Result: Accurately reproduces experimental properties (volume expansion, operating voltage, structural transformations). Reveals 4-order-of-magnitude sodium diffusivity difference between rhombohedral (sodium-rich) and tetragonal (sodium-poor) phases at 300K. Successfully predicts phase-boundary propagation and rate-dependent performance across electrode scales.

Conclusion: The multiscale workflow establishes a blueprint for computational design of insertion-type materials, demonstrating systematic translation of atomistic insights into continuum-scale predictions for rational materials development.

Abstract: We present a generalizable scale-bridging computational framework that enables predictive modeling of insertion-type electrode materials from atomistic to device scales. Applied to sodium manganese hexacyanoferrate, a promising cathode material for grid-scale sodium-ion batteries, our methodology employs an active-learning strategy to train a Moment Tensor Potential through iterative hybrid grand-canonical Monte Carlo--molecular dynamics sampling, robustly capturing configuration spaces at all sodiation levels. The resulting machine learning interatomic potential accurately reproduces experimental properties including volume expansion, operating voltage, and sodium concentration-dependent structural transformations, while revealing a four-order-of-magnitude difference in sodium diffusivity between the rhombohedral (sodium-rich) and tetragonal (sodium-poor) phases at 300 K. We directly compute all critical parameters -- temperature- and concentration-dependent diffusivities, interfacial and strain energies, and complete free-energy landscapes -- to feed them into pseudo-2D phase-field simulations that predict phase-boundary propagation and rate-dependent performances across electrode length scales. This multiscale workflow establishes a blueprint for rational computational design of next-generation insertion-type materials, such as battery electrode materials, demonstrating how atomistic insights can be systematically translated into continuum-scale predictions.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [56] [A positive eigenvalue result for semilinear differential equations in Banach spaces with functional initial conditions](https://arxiv.org/abs/2512.23876)
*Gennaro Infante,Paola Rubbioni*

Main category: math.CA

TL;DR: The paper studies existence of positive eigenvalues with nonnegative eigenfunctions for abstract initial value problems in Banach spaces with functional/nonlocal initial conditions, applying the theory to reaction-diffusion equations.


<details>
  <summary>Details</summary>
Motivation: To establish existence results for positive eigenvalues with associated nonnegative eigenfunctions in abstract evolution problems with functional initial conditions, which arise in various applications including periodic, multipoint, and integral average conditions.

Method: Uses nonlinear analysis, topological methods, and strongly continuous semigroup theory to develop abstract framework applicable to wide range of models with functional/nonlocal initial conditions.

Result: Develops abstract theory for existence of positive eigenvalues with nonnegative eigenfunctions, then applies it to reaction-diffusion equation with nonlocal initial condition from heat flow problem.

Conclusion: Provides general framework for studying eigenvalue problems with functional initial conditions, demonstrating applicability to concrete PDE models like reaction-diffusion equations with nonlocal conditions.

Abstract: We study the existence of positive eigenvalues with associated nonnegative mild eigenfunctions for a class of abstract initial value problems in Banach spaces with functional, possibly nonlocal, initial conditions. The framework includes periodic, multipoint, and integral average conditions. Our approach relies on nonlinear analysis, topological methods, and the theory of strongly continuous semigroups, yielding results applicable to a wide range of models. As an illustration, we apply the abstract theory to a reaction-diffusion equation with a nonlocal initial condition arising from a heat flow problem.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [57] [Computational Analysis of Disease Progression in Pediatric Pulmonary Arterial Hypertension](https://arxiv.org/abs/2512.25027)
*Omar Said,Christopher Tossas-Betancourt,Mary K. Olive,Jimmy C. Lu,Adam Dorfman,C. Alberto Figueroa*

Main category: physics.med-ph

TL;DR: Researchers developed patient-specific computational models for pediatric pulmonary arterial hypertension using longitudinal MRI/catheterization data, creating an automated calibration system that reduces setup time from weeks to days.


<details>
  <summary>Details</summary>
Motivation: Pediatric PAH is understudied due to limited data and lack of targeted diagnostic/therapeutic strategies. There's a need for non-invasive methods to monitor disease progression and inform treatment decisions.

Method: Created multi-scale patient-specific cardiovascular models for 4 pediatric PAH patients using longitudinal MRI/catheterization data. Used CRIMSON framework to couple 3D FSI pulmonary artery models with 0D lumped-parameter heart/Windkessel models. Developed automated Python-based optimizer to calibrate boundary conditions by minimizing simulation-clinical metric discrepancies.

Result: Successfully calibrated models with reduced setup time from weeks to days. Model-derived metrics (arterial stiffness, pulse wave velocity, resistance, compliance) aligned with clinical indicators of disease severity and progression.

Conclusion: Computational modeling can non-invasively capture patient-specific hemodynamic adaptation over time, offering a promising tool for monitoring pediatric PAH and informing future treatment strategies.

Abstract: Pulmonary arterial hypertension (PAH) is a progressive cardiopulmonary disease that leads to increased pulmonary pressures, vascular remodeling, and eventual right ventricular (RV) failure. Pediatric PAH remains understudied due to limited data and the lack of targeted diagnostic and therapeutic strategies. In this study, we developed and calibrated multi-scale, patient-specific cardiovascular models for four pediatric PAH patients using longitudinal MRI and catheterization data collected approximately two years apart. Using the CRIMSON simulation framework, we coupled three-dimensional fluid-structure interaction (FSI) models of the pulmonary arteries with zero-dimensional (0D) lumped-parameter heart and Windkessel models to simulate patient hemodynamics. An automated Python-based optimizer was developed to calibrate boundary conditions by minimizing discrepancies between simulated and clinical metrics, reducing calibration time from weeks to days. Model-derived metrics such as arterial stiffness, pulse wave velocity, resistance, and compliance were found to align with clinical indicators of disease severity and progression. Our findings demonstrate that computational modeling can non-invasively capture patient-specific hemodynamic adaptation over time, offering a promising tool for monitoring pediatric PAH and informing future treatment strategies.

</details>


### [58] [Finite element analysis of very large bone models based on micro-CT scans](https://arxiv.org/abs/2512.24401)
*Shani Martinez-Weissberg,Will Pazner,Zohar Yosibash*

Main category: physics.med-ph

TL;DR: Open-source μFE framework enables large-scale biomechanical analysis of intact rabbit femur using μCT data, validated with experiments, showing 40μm resolution balances accuracy and computational cost.


<details>
  <summary>Details</summary>
Motivation: High-resolution voxel-based μFE models from μCT imaging provide detailed bone mechanics but are computationally challenging at anatomically relevant scales. Need for scalable, experimentally validated open-source framework for preclinical bone assessment.

Method: Integrated framework using MIA clustering for segmentation from μCT data, MFEM library for solving voxel-based μFE meshes with full assembly and element-by-element formulations. Models with over 800M DOFs solved on HPC. Resolution effects studied at 20, 40, 80μm. Experimental validation via Digital Image Correlation on rabbit femur under compression.

Result: Successfully solved models with 800M+ DOFs using moderate HPC. 40μm resolution preserves boundary displacement and principal strain distributions with minimal bias while reducing computational cost. Segmentation parameters influence global mechanical response. Calibrated effective bone material properties at micron scale through experimental coupling.

Conclusion: Large-scale, experimentally informed μFE modeling achievable with open-source tools, providing robust foundation for preclinical assessment of bone mechanics and treatment-related risks. Framework enables anatomically realistic simulations at scale with validated accuracy.

Abstract: High-resolution voxel-based micro-finite element ($μ$FE) models derived from $μ$CT imaging enable detailed investigation of bone mechanics but remain computationally challenging at anatomically relevant scales. This study presents a comprehensive $μ$FE framework for large-scale biomechanical analysis of an intact New Zealand White (NZW) rabbit femur, integrating advanced segmentation, scalable finite element solvers, and experimental validation using predominantly open-source libraries. Bone geometries were segmented from $μ$CT data using the MIA clustering algorithm and converted into voxel-based $μ$FE meshes, which were solved using the open-source MFEM library with algorithms designed for large-scale linear elasticity systems.
  The numerical solutions were verified by comparing with a commercial finite element solver, and by evaluating the performance of full assembly and element-by-element formulations within MFEM. Models containing over $8\times10^{8}$ DOFs were solved using moderate HPC resources, demonstrating the feasibility of anatomically realistic $μ$FE simulations at this scale. Resolution effects were investigated by comparing models with voxel sizes of 20, 40, and 80 $μ$m, revealing that 40 $μ$m preserves boundary displacement and principal strain distributions with minimal bias while significantly reducing computational cost. Sensitivity analyses further showed that segmentation parameters influence the global mechanical response.
  Finally, $μ$FE predictions were coupled with Digital Image Correlation measurements on an NZW rabbit femur under compression to calibrate effective bone material properties at the micron scale. The results demonstrate that large-scale, experimentally informed $μ$FE modeling can be achieved using open-source tools, providing a robust foundation for preclinical assessment of bone mechanics and treatment-related risks.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [59] [Basic Inequalities for First-Order Optimization with Applications to Statistical Risk Analysis](https://arxiv.org/abs/2512.24999)
*Seunghoon Paik,Kangjie Zhou,Matus Telgarsky,Ryan J. Tibshirani*

Main category: math.ST

TL;DR: The paper introduces "basic inequalities" - a framework connecting implicit and explicit regularization in first-order optimization algorithms, providing a tool for statistical analysis of training dynamics and prediction risk.


<details>
  <summary>Details</summary>
Motivation: To develop a unified framework that connects implicit and explicit regularization in optimization algorithms, providing a versatile tool for statistical analysis that can be applied to various first-order methods.

Method: Introduces basic inequalities that bound the objective function difference f(θ_T)-f(z) in terms of accumulated step sizes and distances between initial point, current iterate, and reference point. This translates iteration count into effective regularization.

Result: The framework is demonstrated through analyses of training dynamics and prediction risk bounds, providing new results for mirror descent with Bregman divergence projection, generalized linear models trained by gradient descent and exponentiated gradient descent, and randomized predictors.

Conclusion: Basic inequalities provide a simple yet powerful framework for analyzing optimization algorithms, connecting iteration count to regularization effects, with applications to various optimization methods and theoretical results supplemented by experiments on generalized linear models.

Abstract: We introduce \textit{basic inequalities} for first-order iterative optimization algorithms, forming a simple and versatile framework that connects implicit and explicit regularization. While related inequalities appear in the literature, we isolate and highlight a specific form and develop it as a well-rounded tool for statistical analysis. Let $f$ denote the objective function to be optimized. Given a first-order iterative algorithm initialized at $θ_0$ with current iterate $θ_T$, the basic inequality upper bounds $f(θ_T)-f(z)$ for any reference point $z$ in terms of the accumulated step sizes and the distances between $θ_0$, $θ_T$, and $z$. The bound translates the number of iterations into an effective regularization coefficient in the loss function. We demonstrate this framework through analyses of training dynamics and prediction risk bounds. In addition to revisiting and refining known results on gradient descent, we provide new results for mirror descent with Bregman divergence projection, for generalized linear models trained by gradient descent and exponentiated gradient descent, and for randomized predictors. We illustrate and supplement these theoretical findings with experiments on generalized linear models.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [60] [Assessing generative modeling approaches for free energy estimates in condensed matter](https://arxiv.org/abs/2512.23930)
*Maximilian Schebek,Jiajun He,Emil Hoffmann,Yuanqi Du,Frank Noé,Jutta Rogal*

Main category: cond-mat.stat-mech

TL;DR: Systematic review and benchmarking of generative-model-based methods for free energy estimation in condensed-matter systems, comparing normalizing flows and adaptive transport approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional free energy estimation methods require sampling multiple intermediate states and are computationally expensive. Recent generative-model-based methods offer direct bridging between distributions, but there's no clear guidance on which approaches provide the best trade-off between efficiency, accuracy, and scalability.

Method: Systematic review of generative-model-based methods followed by benchmarking of selected approaches: discrete and continuous normalizing flows in targeted free energy perturbation, FEAT (Free energy Estimators with Adaptive Transport) with escorted Jarzynski equality. Tested on coarse-grained monatomic ice and Lennard-Jones solids.

Result: Evaluation of accuracy, data efficiency, computational cost, and scalability with system size. Provides quantitative framework for selecting effective free energy estimation strategies in condensed-phase systems.

Conclusion: The study offers systematic guidance for selecting appropriate free energy estimation methods based on performance metrics, addressing the trade-off between computational efficiency and accuracy in condensed-matter simulations.

Abstract: The accurate estimation of free energy differences between two states is a long-standing challenge in molecular simulations. Traditional approaches generally rely on sampling multiple intermediate states to ensure sufficient overlap in phase space and are, consequently, computationally expensive. Several generative-model-based methods have recently addressed this challenge by learning a direct bridge between distributions, bypassing the need for intermediate states. However, it remains unclear which approaches provide the best trade-off between efficiency, accuracy, and scalability. In this work, we systematically review these methods and benchmark selected approaches with a focus on condensed-matter systems. In particular, we investigate the performance of discrete and continuous normalizing flows in the context of targeted free energy perturbation as well as FEAT (Free energy Estimators with Adaptive Transport) together with the escorted Jarzynski equality, using coarse-grained monatomic ice and Lennard-Jones solids as benchmark systems. We evaluate accuracy, data efficiency, computational cost, and scalability with system size. Our results provide a quantitative framework for selecting effective free energy estimation strategies in condensed-phase systems.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [61] [Bridging Visual Intuition and Chemical Expertise: An Autonomous Analysis Framework for Nonadiabatic Dynamics Simulations via Mentor-Engineer-Student Collaboration](https://arxiv.org/abs/2512.24133)
*Yifei Zhu,Jiahui Zhang,Binni Huang,Zhenggang Lan*

Main category: physics.chem-ph

TL;DR: VisU is a vision-driven AI framework that uses large language models to autonomously analyze nonadiabatic molecular dynamics trajectories through a virtual research collective approach, reducing reliance on expert intuition.


<details>
  <summary>Details</summary>
Motivation: Traditional analysis of nonadiabatic molecular dynamics trajectories heavily depends on expert intuition and visual pattern recognition, which is difficult to formalize and scale. There's a need for more systematic, automated approaches to analyze excited-state dynamics simulation results.

Method: VisU uses two state-of-the-art large language models in a "Mentor-Engineer-Student" paradigm that mimics a professional chemistry lab. The Mentor provides physical intuition through visual reasoning, the Engineer constructs analysis scripts, and the Student executes pipelines and manages data. The framework orchestrates a four-stage workflow: Preprocessing, Recursive Channel Discovery, Important-Motion Identification, and Validation/Summary.

Result: VisU autonomously identifies reaction channels and key nuclear motions while generating professional academic reports. It bridges visual insight with chemical expertise to analyze excited-state dynamics simulation results.

Conclusion: VisU establishes a new paradigm for human-AI collaboration in analyzing excited-state dynamics, significantly reducing dependence on manual interpretation and enabling more intuitive, scalable mechanistic discovery.

Abstract: Analyzing nonadiabatic molecular dynamics trajectories traditionally heavily relies on expert intuition and visual pattern recognition, a process that is difficult to formalize. We present VisU, a vision-driven framework that leverages the complementary strengths of two state-of-the-art large language models to establish a "virtual research collective." This collective operates through a "Mentor-Engineer-Student" paradigm that mimics the collaborative intelligence of a professional chemistry laboratory. Within this ecosystem, the Mentor provides physical intuition through visual reasoning, while the Engineer adaptively constructs analysis scripts, and the Student executes the pipeline and manages the data and results. VisU autonomously orchestrates a four-stage workflow comprising Preprocessing, Recursive Channel Discovery, Important-Motion Identification, and Validation/Summary. This systematic approach identifies reaction channels and key nuclear motions while generating professional academic reports. By bridging visual insight with chemical expertise, VisU establishes a new paradigm for human-AI collaboration in the analysis of excited-state dynamics simulation results, significantly reducing dependence on manual interpretation and enabling more intuitive, scalable mechanistic discovery.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [62] [Green's function on the Tate curve](https://arxiv.org/abs/2512.24935)
*An Huang,Rebecca Rohrlich,Yaojia Sun,Eric Whyman*

Main category: math.NT

TL;DR: Defined a Laplacian operator on the Tate curve and proved existence of its Green's function, providing explicit formula as p-adic counterpart to Archimedean Green's function on flat torus.


<details>
  <summary>Details</summary>
Motivation: Motivated by defining a p-adic string worldsheet action in genus one (torus topology).

Method: Defined a Laplacian operator on the Tate curve (p-adic analogue of elliptic curve/torus) and studied its Green's function.

Result: Proved existence of the Green's function and provided explicit formula, showing it's a non-Archimedean counterpart of the Archimedean Green's function on a flat torus.

Conclusion: Successfully defined and analyzed Green's function for Laplacian on Tate curve, establishing p-adic analogue of torus Green's function relevant for string theory.

Abstract: Motivated by the question of defining a $p$-adic string worldsheet action in genus one, we define a Laplacian operator on the Tate curve, and study its Green's function. We show that the Green's function exists. We provide an explicit formula for the Green's function, which turns out to be a non-Archimedean counterpart of the Archimedean Green's function on a flat torus.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [63] [Polynomial mixing for the stochastic Schrödinger equation with large damping in the whole space](https://arxiv.org/abs/2512.24599)
*Hung D. Nguyen,Kihoon Seong*

Main category: math.PR

TL;DR: The paper establishes polynomial mixing rates for the stochastic nonlinear Schrödinger equation with large damping in dimensions d≤3.


<details>
  <summary>Details</summary>
Motivation: While unique ergodicity is known for sufficiently damped stochastic nonlinear Schrödinger equations, the actual rate of convergence to equilibrium has remained unknown. The authors aim to quantify this mixing behavior.

Method: The approach uses a coupling strategy combined with pathwise Strichartz estimates to analyze the long-time behavior in the regime of large damping.

Result: Solutions are attracted toward the unique invariant probability measure at polynomial rates of arbitrary order, establishing quantitative mixing properties.

Conclusion: The work provides the first quantitative mixing rates for stochastic nonlinear Schrödinger equations with damping, showing polynomial convergence to equilibrium in low dimensions.

Abstract: We study the long-time mixing behavior of the stochastic nonlinear Schrödinger equation in $\mathbb{R}^d$, $d\le 3$. It is well known that, under a sufficiently strong damping force, the system admits unique ergodicity, although the rate of convergence toward equilibrium has remained unknown. In this work, we address the mixing property in the regime of large damping and establish that solutions are attracted toward the unique invariant probability measure at polynomial rates of arbitrary order. Our approach is based on a coupling strategy with pathwise Strichartz estimates.

</details>


### [64] [Heat kernel estimates for Markov processes with jump kernels blowing-up at the boundary](https://arxiv.org/abs/2512.24807)
*Soobin Cho,Panki Kim,Renming Song,Zoran Vondraček*

Main category: math.PR

TL;DR: The paper establishes sharp two-sided heat kernel estimates for symmetric Markov processes with jump kernels that blow up at the boundary, overcoming challenges from unbounded jump measure tails.


<details>
  <summary>Details</summary>
Motivation: To extend the framework for conservative self-similar Markov processes to a broader geometric setting where jump kernels blow up at the boundary, covering important examples like traces of isotropic α-stable processes and processes related to nonlocal Neumann problems.

Method: Employ recently developed weighted functional inequalities specifically designed for jump kernels blowing up at the boundary, as standard techniques for jump processes with uniformly bounded jump measure tails are not applicable.

Result: Establish sharp two-sided heat kernel estimates for these Markov processes, overcoming the fundamental difficulty of dealing with jump measures whose tails are not uniformly bounded.

Conclusion: The paper successfully extends heat kernel analysis to a broader class of symmetric Markov processes with boundary-blowing jump kernels, providing a framework that covers several important examples in nonlocal analysis and stochastic processes.

Abstract: In this paper, we study purely discontinuous symmetric Markov processes on closed subsets of ${\mathbb R}^d$, $d\ge 1$, with jump kernels of the form $J(x,y)=|x-y|^{-d-α}{\mathcal B}(x,y)$, $α\in (0,2)$, where the function ${\mathcal B}(x,y)$ may blow up at the boundary of the state space. This extends the framework developed recently for conservative self-similar Markov processes on the upper half-space to a broader geometric setting. Examples of Markov processes that fall into our general framework include traces of isotropic $α$-stable processes in $C^{1,\rm Dini}$ sets, processes in Lipschitz sets arising in connection with the nonlocal Neumann problem, and a large class of resurrected self-similar processes in the closed upper half-space.
  We establish sharp two-sided heat kernel estimates for these Markov processes. A fundamental difficulty in accomplishing this task is that, in contrast to the existing literature on heat kernels for jump processes, the tails of the associated jump measures in our setting are not uniformly bounded. Thus, standard techniques in the existing literature used to study heat kernels are not applicable. To overcome this obstacle, we employ recently developed weighted functional inequalities specifically designed for jump kernels blowing up at the boundary.

</details>


### [65] [Uniqueness for stochastic differential equations in Hilbert spaces with irregular drift](https://arxiv.org/abs/2512.25003)
*Lukas Anzeletti,Oleg Butkovsky,Máté Gerencsér,Alexander Shaposhnikov*

Main category: math.PR

TL;DR: The paper presents a framework for proving strong existence and uniqueness of SDEs in Hilbert spaces with irregular drift, extending previous work by removing structural assumptions on the drift term.


<details>
  <summary>Details</summary>
Motivation: To establish strong existence and uniqueness results for stochastic differential equations in Hilbert spaces with irregular drift functions, going beyond previous work that required structural assumptions on the drift term.

Method: Develops a new technique combining Lê's theory of stochastic sewing in Hilbert spaces, Gaussian analysis, and Lasry-Lions approximation methods, avoiding traditional infinite-dimensional Kolmogorov equations.

Result: Proves unique strong solution existence for SDEs in Hilbert spaces with α-Hölder continuous drift, provided α > 2γ/(1+γ), where γ relates to the stochastic convolution's regularity.

Conclusion: The framework substantially extends Da Prato and Flandoli's seminal work by removing structural assumptions on the drift, offering a more general approach to studying SDEs with irregular coefficients in infinite dimensions.

Abstract: We present a versatile framework to study strong existence and uniqueness for stochastic differential equations (SDEs) in Hilbert spaces with irregular drift. We consider an SDE in a separable Hilbert space $H$ \begin{equation*} dX_t= (A X_t + b(X_t))dt +(-A)^{-γ/2}dW_t,\quad X_0=x_0 \in H, \end{equation*} where $A$ is a self-adjoint negative definite operator with purely atomic spectrum, $W$ is a cylindrical Wiener process, $b$ is $α$-Hölder continuous function $H\to H$, and a nonnegative parameter $γ$ such that the stochastic convolution takes values in $H$. We show that this equation has a unique strong solution provided that $α> 2γ/(1+γ)$. This substantially extends the seminal work of Da Prato and Flandoli (2010) as no structural assumption on $b$ is imposed. To obtain this result, we do not use infinite-dimensional Kolmogorov equations but instead develop a new technique combining Lê's theory of stochastic sewing in Hilbert spaces, Gaussian analysis, and a method of Lasry and Lions for approximation in Hilbert spaces.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [66] [Towards mechanistic understanding in a data-driven weather model: internal activations reveal interpretable physical features](https://arxiv.org/abs/2512.24440)
*Theodore MacMillan,Nicholas T. Ouellette*

Main category: physics.ao-ph

TL;DR: Researchers adapt LLM interpretability tools to analyze GraphCast, discovering interpretable weather features like tropical cyclones and atmospheric rivers, and show how interventions can modify predictions in physically consistent ways.


<details>
  <summary>Details</summary>
Motivation: While data-driven physics models like GraphCast perform well, their internal computations are largely unknown and it's unclear whether their representations are interpretable or physically consistent. The paper aims to open the black box of these models.

Method: Adapt tools from LLM interpretability research, specifically using sparse autoencoders to discover interpretable features in GraphCast's neuron space. Perform interventions on prediction steps to probe feature abstractions.

Result: Discovered distinct interpretable features corresponding to tropical cyclones, atmospheric rivers, diurnal/seasonal behavior, precipitation patterns, geographical coding, and sea-ice extent. Interventions on tropical cyclone features produced interpretable, physically consistent modifications to hurricane predictions.

Conclusion: These interpretability methods provide insight into data-driven physics models, moving toward trustworthy predictors and scientifically valuable discovery tools by making their black-box behavior more transparent.

Abstract: Large data-driven physics models like DeepMind's weather model GraphCast have empirically succeeded in parameterizing time operators for complex dynamical systems with an accuracy reaching or in some cases exceeding that of traditional physics-based solvers. Unfortunately, how these data-driven models perform computations is largely unknown and whether their internal representations are interpretable or physically consistent is an open question. Here, we adapt tools from interpretability research in Large Language Models to analyze intermediate computational layers in GraphCast, leveraging sparse autoencoders to discover interpretable features in the neuron space of the model. We uncover distinct features on a wide range of length and time scales that correspond to tropical cyclones, atmospheric rivers, diurnal and seasonal behavior, large-scale precipitation patterns, specific geographical coding, and sea-ice extent, among others. We further demonstrate how the precise abstraction of these features can be probed via interventions on the prediction steps of the model. As a case study, we sparsely modify a feature corresponding to tropical cyclones in GraphCast and observe interpretable and physically consistent modifications to evolving hurricanes. Such methods offer a window into the black-box behavior of data-driven physics models and are a step towards realizing their potential as trustworthy predictors and scientifically valuable tools for discovery.

</details>


<div id='nlin.PS'></div>

# nlin.PS [[Back]](#toc)

### [67] [Soliton profiles: Classical Numerical Schemes vs. Neural Network - Based Solvers](https://arxiv.org/abs/2512.24634)
*Chandler Haight,Svetlana Roudenko,Zhongming Wang*

Main category: nlin.PS

TL;DR: Comparative study shows classical numerical solvers outperform neural network methods for single-instance 1D solitary-wave computations, while operator-learning methods offer advantages for repeated simulations.


<details>
  <summary>Details</summary>
Motivation: To compare the performance of classical numerical methods versus neural network approaches for computing ground states/profiles of solitary-wave solutions to 1D dispersive PDEs (NLS, NLKG, gKdV).

Method: Comparative analysis of classical methods (Petviashvili's method, finite difference with Newton iterations) vs. neural network methods (PINNs and operator-learning approaches) across three 1D dispersive PDEs.

Result: Classical methods maintain high-order accuracy and computational efficiency for single-instance problems. PINNs produce qualitative solutions but are less accurate and efficient. Operator-learning methods are computationally intensive during training but enable rapid inference across parameter instances.

Conclusion: Classical solvers remain superior for single-instance computations, while operator-learning methods become attractive for applications requiring repeated simulations or real-time predictions due to their reusability.

Abstract: We present a comparative study of classical numerical solvers, such as Petviashvili's method or finite difference with Newton iterations, and neural network-based methods for computing ground states or profiles of solitary-wave solutions to the one-dimensional dispersive PDEs that include the nonlinear Schrödinger, the nonlinear Klein-Gordon and the generalized KdV equations. We confirm that classical approaches retain high-order accuracy and strong computational efficiency for single-instance problems in the one-dimensional setting. Physics-informed neural networks (PINNs) are also able to reproduce qualitative solutions but are generally less accurate and less efficient in low dimensions than classical solvers due to expensive training and slow convergence. We also investigate the operator-learning methods, which, although computationally intensive during training, can be reused across many parameter instances, providing rapid inference after pretraining, making them attractive for applications involving repeated simulations or real-time predictions. For single-instance computations, however, the accuracy of operator-learning methods remains lower than that of classical methods or PINNs, in general.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [68] [Mathematical Theory for Photonic Hall Effect in Honeycomb Photonic Crystals](https://arxiv.org/abs/2512.24477)
*Wei Li,Junshan Lin,Jiayu Qiu,Hai Zhang*

Main category: physics.optics

TL;DR: The paper develops a mathematical theory for the photonic Hall effect, proving existence of guided electromagnetic waves at interfaces of honeycomb photonic crystals, analogous to electronic edge states.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous mathematical foundation for the photonic Hall effect and understand how topological properties can create guided electromagnetic waves at interfaces between photonic crystals, similar to topological edge states in electronic systems.

Method: Starting from symmetric honeycomb photonic crystals with Dirac points at K and K' points, introducing two classes of perturbations that lift Dirac degeneracy to create spectral band valleys with well-defined topological phases. Using layer potential techniques and spectral analysis to investigate guided waves at interfaces between perturbed crystals.

Result: Proves existence of guided electromagnetic waves propagating along interfaces between honeycomb photonic crystals, induced by topological Hall effect. Shows relationship between interface mode existence and nature of perturbations on the two periodic media.

Conclusion: Establishes mathematical theory for photonic Hall effect, demonstrating that topological properties can create guided electromagnetic interface waves in photonic crystals, analogous to electronic topological edge states.

Abstract: In this work, we develop a mathematical theory for the photonic Hall effect and prove the existence of guided electromagnetic waves at the interface of two honeycomb photonic crystals. The guided wave resembles the edge states in electronic systems: it is induced by the topological Hall effect, and the wave propagates along the interface but not in the bulk media. Starting from a symmetric honeycomb photonic crystal that attains Dirac points at the high-symmetry points of the Brillouin zone, $K$ and $K'$, we introduce two classes of perturbations for the periodic medium. The perturbations lift the Dirac degeneracy, forming a spectral band valley at the points $K$ and $K'$ with well-defined topological phase that depends on the sign of the perturbation parameters. By employing the layer potential techniques and spectral analysis, we investigate the existence of guided wave along an interface when two honeycomb photonic crystals are glued together. In particular, we elucidate the relationship between the existence of the interface mode and the nature of perturbations imposed on the two periodic media separated by the interface.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [69] [NeuralCrop: Combining physics and machine learning for improved crop yield predictions](https://arxiv.org/abs/2512.20177)
*Yunan Lin,Sebastian Bathiany,Maha Badri,Maximilian Gelbrecht,Philipp Hess,Brian Groenke,Jens Heinke,Christoph Müller,Niklas Boers*

Main category: cs.LG

TL;DR: NeuralCrop is a hybrid crop model combining process-based GGCMs with machine learning that outperforms state-of-the-art models in yield prediction, especially under drought extremes, while maintaining robustness for climate change projections.


<details>
  <summary>Details</summary>
Motivation: Traditional GGCMs have substantial uncertainties due to limited process understanding, while pure machine learning models fail to generalize to changing climate conditions outside their training distributions.

Method: NeuralCrop combines an advanced process-based GGCM with data-driven ML components, first trained to emulate a competitive GGCM then fine-tuned on observational data.

Result: NeuralCrop outperforms state-of-the-art GGCMs across site-level and large-scale cropping regions, reproduces interannual yield anomalies more accurately (2000-2019), with strong improvements under drought extremes, and maintains robust projections for unseen conditions.

Conclusion: The hybrid crop modeling approach offers overall improved crop modeling and more reliable yield projections under climate change and intensifying extreme weather conditions.

Abstract: Global gridded crop models (GGCMs) simulate daily crop growth by explicitly representing key biophysical processes and project end-of-season yield time series. They are a primary tool to quantify the impacts of climate change on agricultural productivity and assess associated risks for food security. Despite decades of development, state-of-the-art GGCMs still have substantial uncertainties in simulating complex biophysical processes due to limited process understanding. Recently, machine learning approaches trained on observational data have shown great potential in crop yield predictions. However, these models have not demonstrated improved performance over classical GGCMs and are not suitable for simulating crop yields under changing climate conditions due to problems in generalizing outside their training distributions. Here we introduce NeuralCrop, a hybrid GGCM that combines the strengths of an advanced process-based GGCM, resolving important processes explicitly, with data-driven machine learning components. The model is first trained to emulate a competitive GGCM before it is fine-tuned on observational data. We show that NeuralCrop outperforms state-of-the-art GGCMs across site-level and large-scale cropping regions. Across moisture conditions, NeuralCrop reproduces the interannual yield anomalies in European wheat regions and the US Corn Belt more accurately during the period from 2000 to 2019 with particularly strong improvements under drought extremes. When generalizing to conditions unseen during training, NeuralCrop continues to make robust projections, while pure machine learning models exhibit substantial performance degradation. Our results show that our hybrid crop modelling approach offers overall improved crop modeling and more reliable yield projections under climate change and intensifying extreme weather conditions.

</details>


### [70] [Generative forecasting with joint probability models](https://arxiv.org/abs/2512.24446)
*Patrick Wyrod,Ashesh Chattopadhyay,Daniele Venturi*

Main category: cs.LG

TL;DR: The paper proposes reframing chaotic system forecasting as a joint generative modeling problem, learning distributions over lagged state windows rather than just next-step predictions, enabling better uncertainty quantification and long-term statistical accuracy.


<details>
  <summary>Details</summary>
Motivation: Chaotic systems have fundamental limitations for deterministic forecasting due to sensitivity to initial conditions and unresolved multiscale processes. Existing generative approaches focus too narrowly on next-step conditional prediction rather than capturing the underlying dynamic structure.

Method: Reframe forecasting as learning joint probability distributions over lagged system states within temporal windows, with forecasts obtained through marginalization. Introduce model-agnostic training/inference framework for joint generative forecasting with three uncertainty quantification metrics (ensemble variance, short-horizon autocorrelation, cumulative Wasserstein drift).

Result: Joint generative models outperform conventional conditional next-step models on Lorenz-63 and Kuramoto-Sivashinsky systems, showing improved short-term predictive skill, preserved attractor geometry, and substantially more accurate long-range statistical behavior.

Conclusion: The joint generative forecasting approach captures nonlinear temporal dependencies better than next-step methods, enables robust uncertainty quantification without ground truth, and produces more reliable long-term statistical predictions for chaotic dynamical systems.

Abstract: Chaotic dynamical systems exhibit strong sensitivity to initial conditions and often contain unresolved multiscale processes, making deterministic forecasting fundamentally limited. Generative models offer an appealing alternative by learning distributions over plausible system evolutions; yet, most existing approaches focus on next-step conditional prediction rather than the structure of the underlying dynamics. In this work, we reframe forecasting as a fully generative problem by learning the joint probability distribution of lagged system states over short temporal windows and obtaining forecasts through marginalization. This new perspective allows the model to capture nonlinear temporal dependencies, represent multistep trajectory segments, and produce next-step predictions consistent with the learned joint distribution. We also introduce a general, model-agnostic training and inference framework for joint generative forecasting and show how it enables assessment of forecast robustness and reliability using three complementary uncertainty quantification metrics (ensemble variance, short-horizon autocorrelation, and cumulative Wasserstein drift), without access to ground truth. We evaluate the performance of the proposed method on two canonical chaotic dynamical systems, the Lorenz-63 system and the Kuramoto-Sivashinsky equation, and show that joint generative models yield improved short-term predictive skill, preserve attractor geometry, and achieve substantially more accurate long-range statistical behaviour than conventional conditional next-step models.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [71] [Classification of ancient cylindrical mean curvature flows and the Mean Convex Neighborhood Conjecture](https://arxiv.org/abs/2512.24524)
*Richard H. Bamler,Yi Lai*

Main category: math.DG

TL;DR: The paper resolves the Mean Convex Neighborhood Conjecture for mean curvature flows, proving that near cylindrical singularities the flow is mean-convex and can be characterized by local models, with applications to classifying ancient asymptotically cylindrical flows.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve the long-standing Mean Convex Neighborhood Conjecture in mean curvature flow theory, which concerns the local structure of flows near singularities where the tangent flow is cylindrical. This conjecture has been a fundamental open problem in geometric analysis.

Method: The method involves: 1) Complete classification of ancient asymptotically cylindrical flows into three canonical families (ancient ovals, bowl soliton, flying wing solitons), 2) Refined asymptotic analysis with a novel "leading mode condition", 3) "Induction over thresholds" argument, 4) Parameterization of asymptotically cylindrical flows space, 5) Independent approach from prior work with self-contained proofs.

Result: Main results: 1) Proof of Mean Convex Neighborhood Conjecture for all dimensions and cylindrical singularities, 2) Canonical neighborhood theorem near cylindrical points, 3) Uniform version with quantitative structural description, 4) Complete classification of ancient asymptotically cylindrical flows as non-collapsed, convex, rotationally symmetric flows belonging to three families, 5) New proof of existence of flying wing solitons.

Conclusion: The paper successfully resolves the Mean Convex Neighborhood Conjecture, providing comprehensive understanding of mean curvature flows near cylindrical singularities. The work establishes fundamental structural results, classifies ancient flows, and develops new analytical techniques that advance the field of geometric analysis.

Abstract: We resolve the Mean Convex Neighborhood Conjecture for mean curvature flows in all dimensions and for all types of cylindrical singularities. Specifically, we show that if the tangent flow at a singular point is a multiplicity-one cylinder, then in a neighborhood of that point the flow is mean-convex, its time-slices arise as level sets of a continuous function, and all nearby tangent flows are cylindrical. Moreover, we establish a canonical neighborhood theorem near such points, which characterizes the flow via local models. We also obtain a more uniform version of the Mean Convex Neighborhood Conjecture, which only requires closeness to a cylinder at some initial time and yields a quantitative version of this structural description.
  Our proof relies on a complete classification of ancient, asymptotically cylindrical flows. We prove that any such flow is non-collapsed, convex, rotationally symmetric, and belongs to one of three canonical families: ancient ovals, the bowl soliton, or the flying wing translating solitons. Central to our method is a refined asymptotic analysis and a novel \emph{leading mode condition,} together with a new ``induction over thresholds'' argument. In addition, our approach provides a full parameterization of the space of asymptotically cylindrical flows and gives a new proof of the existence of flying wing solitons.
  Our method is independent of prior work and, together with our prequel paper, this work is largely self-contained.

</details>


### [72] [Isocapacitary constants for the $p$-Laplacian on compact manifolds](https://arxiv.org/abs/2512.24725)
*Lili Wang,Tao Wang*

Main category: math.DG

TL;DR: The paper introduces Steklov and Neumann isocapacitary constants for p-Laplacian on compact manifolds, which provide two-sided bounds for (p,α)-Sobolev constants and degenerate to bounds for first nontrivial eigenvalues when α=1.


<details>
  <summary>Details</summary>
Motivation: To develop new constants (Steklov and Neumann isocapacitary constants) that can provide bounds for Sobolev constants and eigenvalues of the p-Laplacian on compact manifolds, extending existing results to more general settings.

Method: Introduces Steklov and Neumann isocapacitary constants specifically for the p-Laplacian on compact manifolds, showing how these constants relate to (p,α)-Sobolev constants and degenerate to eigenvalue bounds when α=1.

Result: The new constants yield two-sided bounds for (p,α)-Sobolev constants, and when α=1, they provide upper and lower bounds for the first nontrivial Steklov and Neumann eigenvalues of the p-Laplacian.

Conclusion: Steklov and Neumann isocapacitary constants provide a useful framework for bounding Sobolev constants and eigenvalues in p-Laplacian problems on compact manifolds, with applications to spectral geometry and partial differential equations.

Abstract: In this paper, we introduce Steklov and Neumann isocapacitary constants for the $p$-Laplacian on compact manifolds. These constants yield two-sided bounds for the $(p,α)$-Sobolev constants, which degenerate to upper and lower bounds for the first nontrivial Steklov and Neumann eigenvalues of the $p$-Laplacian when $α= 1$.

</details>


### [73] [A Liouville-Weierstrass correspondence for Spacelike and Timelike Minimal Surfaces in $\mathbb{L}^3$](https://arxiv.org/abs/2512.24908)
*Adriana A. Cintra,Iury Domingos,Irene I. Onnis*

Main category: math.DG

TL;DR: The paper establishes a correspondence between solutions of the Liouville equation and Weierstrass representations of minimal surfaces in Lorentz-Minkowski space, providing a unified treatment for both spacelike and timelike surfaces using complex/paracomplex analysis.


<details>
  <summary>Details</summary>
Motivation: To investigate the relationship between solutions of the Liouville equation and minimal surfaces in Lorentz-Minkowski space, providing a unified framework for both spacelike and timelike surfaces using analytic methods.

Method: Using complex and paracomplex analysis to study the correspondence between Liouville equation solutions and Weierstrass representations of minimal surfaces with diagonalizable Weingarten map in L³. Analyzing pseudo-isometries via Möbius-type transformations and their relation to Lorentz group rotations.

Result: Established correspondence between Liouville equation solutions and minimal surfaces in L³, showed how pseudo-isometries act via Möbius transformations corresponding to Lorentz rotations, demonstrated how local solutions determine Gauss map and Weierstrass data, and provided explicit examples of both spacelike and timelike minimal surfaces.

Conclusion: The paper successfully develops a unified analytic framework connecting Liouville equation solutions to minimal surface theory in Lorentz-Minkowski space, with applications to both spacelike and timelike cases through complex/paracomplex methods.

Abstract: We investigate a correspondence between solutions $λ(x,y)$ of the Liouville equation \[ Δλ= -\varepsilon e^{-4λ}, \] and the Weierstrass representations of spacelike ($\varepsilon = 1$) and timelike ($\varepsilon = -1$) minimal surfaces with diagonalizable Weingarten map in the three-dimensional Lorentz--Minkowski space $\mathbb{L}^3$. Using complex and paracomplex analysis, we provide a unified treatment of both causal types. We study the action of pseudo-isometries of $\mathbb{L}^3$ on minimal surfaces via Möbius-type transformations, establishing a correspondence between these transformations and rotations in the special orthochronous Lorentz group. Furthermore, we show how local solutions of the Liouville equation determine the Gauss map and the associated Weierstrass data. Finally, we present explicit examples of spacelike and timelike minimal surfaces in $\mathbb{L}^3$ arising from solutions of the Liouville equation.

</details>


### [74] [The PDE-ODI principle and cylindrical mean curvature flows](https://arxiv.org/abs/2512.25050)
*Richard H. Bamler,Yi Lai*

Main category: math.DG

TL;DR: New PDE-ODI principle converts parabolic PDEs to ODE inequalities, enabling high-order asymptotic expansions for ancient MCF solutions, proving uniqueness of bowl soliton × ℝ^k among cylindrical flows and recovering classical results with simpler proofs.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient framework for analyzing ancient solutions and singularities of mean curvature flow, particularly those modeled on cylinders, bypassing delicate analytic estimates used in previous work and providing stronger asymptotic control.

Method: Introduces the PDE-ODI principle that converts parabolic differential equations into systems of ordinary differential inequalities, enabling high-order asymptotic expansions without complex analytic estimates.

Result: 1) Proves uniqueness of bowl soliton × ℝ^k among ancient cylindrical flows with dominant linear mode; 2) Obtains complete asymptotic expansions for quadratic-dominant case; 3) Recovers classical results (Colding-Minicozzi uniqueness of tangent flows, Colding-Ilmanen-Minicozzi cylinder rigidity) with simpler proofs using single ODE inequality.

Conclusion: The PDE-ODI principle provides a powerful, self-contained framework for analyzing mean curvature flow singularities, yielding stronger asymptotic control and unifying classical results while bypassing technical analytic estimates like Łojasiewicz-Simon inequality.

Abstract: We introduce a new approach for analyzing ancient solutions and singularities of mean curvature flow that are locally modeled on a cylinder. Its key ingredient is a general mechanism, called the \emph{PDE--ODI principle}, which converts a broad class of parabolic differential equations into systems of ordinary differential inequalities. This principle bypasses many delicate analytic estimates used in previous work, and yields asymptotic expansions to arbitrarily high order.
  As an application, we establish the uniqueness of the bowl soliton times a Euclidean factor among ancient, cylindrical flows with dominant linear mode. This extends previous results on this problem to the most general setting and is made possible by the stronger asymptotic control provided by our analysis. In the other case, when the quadratic mode dominates, we obtain a complete asymptotic expansion to arbitrary polynomial order, which will form the basis for a subsequent paper. Our framework also recovers and unifies several classical results. In particular, we give new proofs of the uniqueness of tangent flows (due to Colding-Minicozzi) and the rigidity of cylinders among shrinkers (due to Colding-Ilmanen-Minicozzi) by reducing both problems to a single ordinary differential inequality, without using the Łojasiewicz-Simon inequality.
  Our approach is independent of prior work and the paper is largely self-contained.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [75] [Numerical study of solitary waves in Dirac--Klein--Gordon system](https://arxiv.org/abs/2512.24954)
*Andrew Comech,Julien Ricaud,Marco Roque*

Main category: math-ph

TL;DR: Numerical construction of solitary waves in Dirac-Klein-Gordon systems across 1D and 3D, analyzing energy/charge dependence on frequency ω, using iterative methods starting from nonlinear Dirac solutions and comparing with shooting methods for massless cases.


<details>
  <summary>Details</summary>
Motivation: To systematically construct and analyze solitary wave solutions in Dirac-Klein-Gordon systems, understand their energy and charge characteristics as functions of frequency ω, and investigate implications for spectral stability of these waves.

Method: Numerical construction using iterative procedure starting from solitary waves of nonlinear Dirac equation, computing corresponding scalar field, and adjusting coupling constant. Comparison with shooting method for massless scalar field cases. Use of virial identities to control simulation errors.

Result: Successfully constructed solitary waves in both 1D and 3D Dirac-Klein-Gordon systems. Obtained dependence of energy and charge on frequency ω. Validated iterative approach against shooting method for massless cases. Established error control through virial identities.

Conclusion: The numerical methods effectively construct solitary waves in Dirac-Klein-Gordon systems, revealing important relationships between energy, charge, and frequency. The results provide insights for further analysis of spectral stability properties of these solitary waves.

Abstract: We use numerics to construct solitary waves in Dirac--Klein--Gordon (in one and three spatial dimensions) and study the dependence of energy and charge on $ω$. For the construction, we use the iterative procedure, starting from solitary waves of nonlinear Dirac equation, computing the corresponding scalar field, and adjusting the coupling constant. We also consider the case of massless scalar field, when the iteration procedure could be compared with the shooting method. We use the virial identities to control the error of simulations. We also discuss possible implications from the obtained results for the spectral stability of solitary waves.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [76] [Achieving High Efficiency And Enhanced Beam Quality In Laser Wakefield Acceleration](https://arxiv.org/abs/2512.24719)
*Jia Wang,Ming Zeng,Dazhang Li,Wentao Wang,Song Li,Ke Feng,Jie Gao*

Main category: physics.acc-ph

TL;DR: Shorter laser pulses enable two-step dechirping for efficient, high-quality electron beam generation in laser wakefield acceleration.


<details>
  <summary>Details</summary>
Motivation: Laser wakefield acceleration offers compact, affordable particle acceleration with extremely high gradients, but faces challenges in improving energy transfer efficiency while maintaining beam quality suitable for practical applications.

Method: Using lasers with shorter pulse durations to enable a two-step dechirping process for accelerated electron beams with nanocoulomb-level charge.

Result: Electron beams with 1% energy spread can be generated with 10-30% energy transfer efficiency across large parameter space. Demonstrated example: 420MeV beam with 5.5nC charge and 2% RMS energy spread using 8.3J, 7.2fs laser pulse.

Conclusion: Shorter laser pulse durations facilitate efficient two-step dechirping, enabling high-quality electron beams with improved energy transfer efficiency in laser wakefield acceleration systems.

Abstract: Laser wakefield acceleration, characterized by the extremely high electric field gradient exceeding 100GV/m, is regarded as a compact and cost affordable technology for the next generation of particle colliders and light sources. However, it has always been a major challenge to effectively increase the energy transfer efficiency from the laser to the accelerated beam, while ensuring the beam quality remains suitable for practical applications. This study demonstrates that the laser with shorter pulse duration allows for a two-step dechirping process of the accelerated electron beam with charge of nanocoulomb level. The electron beams with an energy spread of 1% can be generated with the energy transfer efficiency of 10% to 30% in a large parameter space. For example, one electron beam with the energy of 420MeV, the charge of 5.5nC and the RMS energy spread of 2% can be produced using an 8.3J laser pulse with 7.2fs duration.

</details>
